[{"id": "2010.00087", "submitter": "Karthik C. S.", "authors": "Vincent Cohen-Addad, Karthik C. S., and Euiwoong Lee", "title": "On Approximability of Clustering Problems Without Candidate Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means objective is arguably the most widely-used cost function for\nmodeling clustering tasks in a metric space. In practice and historically,\nk-means is thought of in a continuous setting, namely where the centers can be\nlocated anywhere in the metric space. For example, the popular Lloyd's\nheuristic locates a center at the mean of each cluster.\n  Despite persistent efforts on understanding the approximability of k-means,\nand other classic clustering problems such as k-median and k-minsum, our\nknowledge of the hardness of approximation factors of these problems remains\nquite poor. In this paper, we significantly improve upon the hardness of\napproximation factors known in the literature for these objectives. We show\nthat if the input lies in a general metric space, it is NP-hard to approximate:\n  $\\bullet$ Continuous k-median to a factor of $2-o(1)$; this improves upon the\nprevious inapproximability factor of 1.36 shown by Guha and Khuller (J.\nAlgorithms '99).\n  $\\bullet$ Continuous k-means to a factor of $4- o(1)$; this improves upon the\nprevious inapproximability factor of 2.10 shown by Guha and Khuller (J.\nAlgorithms '99).\n  $\\bullet$ k-minsum to a factor of $1.415$; this improves upon the\nAPX-hardness shown by Guruswami and Indyk (SODA '03).\n  Our results shed new and perhaps counter-intuitive light on the differences\nbetween clustering problems in the continuous setting versus the discrete\nsetting (where the candidate centers are given as part of the input).\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 20:05:46 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 17:35:55 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["S.", "Karthik C.", ""], ["Lee", "Euiwoong", ""]]}, {"id": "2010.00239", "submitter": "Laercio Lima Pilla", "authors": "La\\'ercio Lima Pilla (ParSys - LRI)", "title": "Optimal Task Assignment to Heterogeneous Federated Learning Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning provides new opportunities for training machine learning\nmodels while respecting data privacy. This technique is based on heterogeneous\ndevices that work together to iteratively train a model while never sharing\ntheir own data. Given the synchronous nature of this training, the performance\nof Federated Learning systems is dictated by the slowest devices, also known as\nstragglers. In this paper, we investigate the problem of minimizing the\nduration of Federated Learning rounds by controlling how much data each device\nuses for training. We formulate this problem as a makespan minimization problem\nwith identical, independent, and atomic tasks that have to be assigned to\nheterogeneous resources with non-decreasing cost functions while respecting\nlower and upper limits of tasks per resource. Based on this formulation, we\npropose a polynomial-time algorithm named OLAR and prove that it provides\noptimal schedules. We evaluate OLAR in an extensive experimental evaluation\nusing simulation that includes comparisons to other algorithms from the state\nof the art and new extensions to them. Our results indicate that OLAR provides\noptimal solutions with a small execution time. They also show that the presence\nof lower and upper limits of tasks per resource erase any benefits that\nsuboptimal heuristics could provide in terms of algorithm execution time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 07:58:48 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Pilla", "La\u00e9rcio Lima", "", "ParSys - LRI"]]}, {"id": "2010.00334", "submitter": "Marco Pegoraro", "authors": "Marco Pegoraro, Merih Seran Uysal and Wil M.P. van der Aalst", "title": "Efficient Time and Space Representation of Uncertain Event Data", "comments": "34 pages, 16 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a discipline which concerns the analysis of execution data\nof operational processes, the extraction of models from event data, the\nmeasurement of the conformance between event data and normative models, and the\nenhancement of all aspects of processes. Most approaches assume that event data\nis accurately capture behavior. However, this is not realistic in many\napplications: data can contain uncertainty, generated from errors in recording,\nimprecise measurements, and other factors. Recently, new methods have been\ndeveloped to analyze event data containing uncertainty; these techniques\nprominently rely on representing uncertain event data by means of graph-based\nmodels explicitly capturing uncertainty. In this paper, we introduce a new\napproach to efficiently calculate a graph representation of the behavior\ncontained in an uncertain process trace. We present our novel algorithm, prove\nits asymptotic time complexity, and show experimental results that highlight\norder-of-magnitude performance improvements for the behavior graph\nconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:03:56 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 10:42:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Pegoraro", "Marco", ""], ["Uysal", "Merih Seran", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2010.00348", "submitter": "Bartlomiej Dudek", "authors": "Bart{\\l}omiej Dudek and Pawe{\\l} Gawrychowski", "title": "Counting 4-Patterns in Permutations Is Equivalent to Counting 4-Cycles\n  in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation $\\sigma$ appears in permutation $\\pi$ if there exists a\nsubsequence of $\\pi$ that is order-isomorphic to $\\sigma$. The natural question\nis to check if $\\sigma$ appears in $\\pi$, and if so count the number of\noccurrences. We know that for any fixed length~k, we can check if a given\npattern of length k appears in a permutation of length n in time linear in n,\nbut being able to count all such occurrences in $f(k)\\cdot n^{o(k/\\log k)}$\ntime would refute the exponential time hypothesis (ETH). This motivates a\nsystematic study of the complexity of counting occurrences for different\npatterns of fixed small length k. We investigate this question for k=4. Very\nrecently, Even-Zohar and Leng [arXiv 2019] identified two types of 4-patterns.\nFor the first type they designed an $\\~O(n)$ time algorithm, while for the\nsecond they were able to provide an $\\~O(n^{1.5})$ time algorithm. This brings\nup the question whether the permutations of the second type are inherently\nharder than the first type.\n  We establish a connection between counting 4-patterns of the second type and\ncounting 4-cycles in a sparse undirected graph. By designing two-way reductions\nwe show that the complexities of both problems are the same, up to\npolylogarithmic factors. This allows us to provide a reasonable argument for\nwhy there is a difference in the complexities for counting 4-patterns of the\ntwo types. In particular, even for the simpler problem of detecting a 4-cycle\nin a graph on m edges, the best known algorithm works in $O(m^{4/3})$ time. Our\nreductions imply that an $O(n^{4/3-\\varepsilon})$ time algorithm for counting\noccurrences would imply an exciting breakthrough for counting (and hence also\ndetecting) 4-cycles. In the other direction, by plugging in the fastest known\nalgorithm for counting 4-cycles, we obtain an algorithm for counting\noccurrences of any 4-pattern in $O(n^{1.48})$ time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:25:52 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "2010.00402", "submitter": "Ines Chami", "authors": "Ines Chami, Albert Gu, Vaggos Chatziafratis and Christopher R\\'e", "title": "From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-based Hierarchical Clustering (HC) is a classical unsupervised\nmachine learning algorithm that has traditionally been solved with heuristic\nalgorithms like Average-Linkage. Recently, Dasgupta reframed HC as a discrete\noptimization problem by introducing a global cost function measuring the\nquality of a given tree. In this work, we provide the first continuous\nrelaxation of Dasgupta's discrete optimization problem with provable quality\nguarantees. The key idea of our method, HypHC, is showing a direct\ncorrespondence from discrete trees to continuous representations (via the\nhyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm\nthat maps leaf embeddings to a dendrogram), allowing us to search the space of\ndiscrete binary trees with continuous optimization. Building on analogies\nbetween trees and hyperbolic space, we derive a continuous analogue for the\nnotion of lowest common ancestor, which leads to a continuous relaxation of\nDasgupta's discrete objective. We can show that after decoding, the global\nminimizer of our continuous relaxation yields a discrete tree with a (1 +\nepsilon)-factor approximation for Dasgupta's optimal tree, where epsilon can be\nmade arbitrarily small and controls optimization challenges. We experimentally\nevaluate HypHC on a variety of HC benchmarks and find that even approximate\nsolutions found with gradient descent have superior clustering quality than\nagglomerative heuristics or other gradient based algorithms. Finally, we\nhighlight the flexibility of HypHC using end-to-end training in a downstream\nclassification task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:43:19 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Chami", "Ines", ""], ["Gu", "Albert", ""], ["Chatziafratis", "Vaggos", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2010.00412", "submitter": "Bo Sun", "authors": "Bo Sun, Ali Zeynali, Tongxin Li, Mohammad Hajiesmaili, Adam Wierman,\n  Danny H.K. Tsang", "title": "Competitive Algorithms for the Online Multiple Knapsack Problem with\n  Application to Electric Vehicle Charging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a general version of the fractional online knapsack\nproblem with multiple knapsacks, heterogeneous constraints on which items can\nbe assigned to which knapsack, and rate-limiting constraints on the assignment\nof items to knapsacks. This problem generalizes variations of the knapsack\nproblem and of the one-way trading problem that have previously been treated\nseparately, and additionally finds application to the real-time control of\nelectric vehicle (EV) charging. We introduce a new algorithm that achieves a\ncompetitive ratio within an additive factor of one of the best achievable\ncompetitive ratios for the general problem and matches or improves upon the\nbest-known competitive ratio for special cases in the knapsack and one-way\ntrading literatures. Moreover, our analysis provides a novel approach to online\nalgorithm design based on an instance-dependent primal-dual analysis that\nconnects the identification of worst-case instances to the design of\nalgorithms. Finally, we illustrate the proposed algorithm via trace-based\nexperiments of EV charging.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:57:29 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 09:51:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sun", "Bo", ""], ["Zeynali", "Ali", ""], ["Li", "Tongxin", ""], ["Hajiesmaili", "Mohammad", ""], ["Wierman", "Adam", ""], ["Tsang", "Danny H. K.", ""]]}, {"id": "2010.00917", "submitter": "Uri Stemmer", "authors": "Haim Kaplan, Yishay Mansour, Uri Stemmer", "title": "The Sparse Vector Technique, Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit one of the most basic and widely applicable techniques in the\nliterature of differential privacy - the sparse vector technique [Dwork et al.,\nSTOC 2009]. This simple algorithm privately tests whether the value of a given\nquery on a database is close to what we expect it to be. It allows to ask an\nunbounded number of queries as long as the answer is close to what we expect,\nand halts following the first query for which this is not the case.\n  We suggest an alternative, equally simple, algorithm that can continue\ntesting queries as long as any single individual does not contribute to the\nanswer of too many queries whose answer deviates substantially form what we\nexpect. Our analysis is subtle and some of its ingredients may be more widely\napplicable. In some cases our new algorithm allows to privately extract much\nmore information from the database than the original.\n  We demonstrate this by applying our algorithm to the shifting heavy-hitters\nproblem: On every time step, each of $n$ users gets a new input, and the task\nis to privately identify all the current heavy-hitters. That is, on time step\n$i$, the goal is to identify all data elements $x$ such that many of the users\nhave $x$ as their current input. We present an algorithm for this problem with\nimproved error guarantees over what can be obtained using existing techniques.\nSpecifically, the error of our algorithm depends on the maximal number of times\nthat a single user holds a heavy-hitter as input, rather than the total number\nof times in which a heavy-hitter exists.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:50:52 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:15:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""], ["Stemmer", "Uri", ""]]}, {"id": "2010.00931", "submitter": "L\\'aszl\\'o Kozma", "authors": "Benjamin Aram Berendsohn, L\\'aszl\\'o Kozma", "title": "Splay trees on trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search trees on trees (STTs) are a far-reaching generalization of binary\nsearch trees (BSTs), allowing the efficient exploration of tree-structured\ndomains. (BSTs are the special case in which the underlying domain is a path.)\nTrees on trees have been extensively studied under various guises in computer\nscience and discrete mathematics.\n  Recently Bose, Cardinal, Iacono, Koumoutsos, and Langerman (SODA 2020)\nconsidered adaptive STTs and observed that, apart from notable exceptions, the\nmachinery developed for BSTs in the past decades does not readily transfer to\nSTTs. In particular, they asked whether the optimal STT can be efficiently\ncomputed or approximated (by analogy to Knuth's algorithm for optimal BSTs),\nand whether natural self-adjusting BSTs such as Splay trees (Sleator, Tarjan,\n1983) can be extended to this more general setting.\n  We answer both questions affirmatively. First, we show that a $(1 +\n\\frac{1}{t})$-approximation of an optimal size-$n$ STT for a given search\ndistribution can be computed in time $O(n^{2t + 1})$ for all integers $t \\geq\n1$. Second, we identify a broad family of STTs with linear rotation-distance,\nallowing the generalization of Splay trees to the STT setting. We show that our\ngeneralized Splay satisfies a static optimality theorem, asymptotically\nmatching the cost of the optimal STT in an online fashion, i.e. without\nknowledge of the search distribution. Our results suggest an extension of the\ndynamic optimality conjecture for Splay trees to the broader setting of trees\non trees.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 11:51:03 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 11:19:31 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Berendsohn", "Benjamin Aram", ""], ["Kozma", "L\u00e1szl\u00f3", ""]]}, {"id": "2010.00937", "submitter": "Maximilian Probst Gutenberg", "authors": "Jacob Evald, Viktor Fredslund-Hansen, Maximilian Probst Gutenberg,\n  Christian Wulff-Nilsen", "title": "Decremental APSP in Directed Graphs Versus an Adaptive Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G = (V,E)$, undergoing an online sequence of edge\ndeletions with $m$ edges in the initial version of $G$ and $n = |V|$, we\nconsider the problem of maintaining all-pairs shortest paths (APSP) in $G$.\n  Whilst this problem has been studied in a long line of research [ACM'81,\nFOCS'99, FOCS'01, STOC'02, STOC'03, SWAT'04, STOC'13] and the problem of\n$(1+\\epsilon)$-approximate, weighted APSP was solved to near-optimal update\ntime $\\tilde{O}(mn)$ by Bernstein [STOC'13], the problem has mainly been\nstudied in the context of oblivious adversaries, which assumes that the\nadversary fixes the update sequence before the algorithm is started.\n  In this paper, we make significant progress on the problem in the setting\nwhere the adversary is adaptive, i.e. can base the update sequence on the\noutput of the data structure queries. We present three new data structures that\nfit different settings:\n  We first present a deterministic data structure that maintains exact\ndistances with total update time $\\tilde{O}(n^3)$.\n  We also present a deterministic data structure that maintains\n$(1+\\epsilon)$-approximate distance estimates with total update time $\\tilde\nO(\\sqrt{m} n^2/\\epsilon)$ which for sparse graphs is $\\tilde\nO(n^{2+1/2}/\\epsilon)$.\n  Finally, we present a randomized $(1+\\epsilon)$-approximate data structure\nwhich works against an adaptive adversary; its total update time is $\\tilde\nO(m^{2/3}n^{5/3} + n^{8/3}/(m^{1/3}\\epsilon^2))$ which for sparse graphs is\n$\\tilde O(n^{2+1/3})$.\n  Our exact data structure matches the total update time of the best randomized\ndata structure by Baswana et al. [STOC'02] and maintains the distance matrix in\nnear-optimal time. Our approximate data structures improve upon the best data\nstructures against an adaptive adversary which have $\\tilde{O}(mn^2)$ total\nupdate time [JACM'81, STOC'03].\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:06:33 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Evald", "Jacob", ""], ["Fredslund-Hansen", "Viktor", ""], ["Gutenberg", "Maximilian Probst", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "2010.00967", "submitter": "Alessio Conte", "authors": "Alessio Conte, Roberto Grossi, Andrea Marino, Luca Versari", "title": "Efficient Estimation of Graph Trussness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-truss is an edge-induced subgraph $H$ such that each of its edges\nbelongs to at least $k-2$ triangles of $H$. This notion has been introduced\naround ten years ago in social network analysis and security, as a form of\ncohesive subgraph that is rich of triangles and less stringent than the clique.\nThe \\emph{trussness} of a graph is the maximum $k$ such that a $k$-truss\nexists.\n  The problem of computing $k$-trusses has been largely investigated from the\npractical and engineering point of view. On the other hand, the theoretical\nside of the problem has received much less attention, despite presenting\ninteresting challenges. The existing methods share a common design, based on\niteratively removing the edge with smallest support, where the support of an\nedge is the number of triangles containing it.\n  The aim of this paper is studying algorithmic aspects of graph trussness.\nWhile it is possible to show that the time complexity of computing exactly the\ngraph trussness and that of counting/listing all triangles is inherently the\nsame, we provide efficient algorithms for estimating its value, under suitable\nconditions, with significantly lower complexity than the exact approach. In\nparticular, we provide a $(1 \\pm \\epsilon)$-approximation algorithm that is\nasymptotically faster than the exact approach, on graphs which contain\n$\\omega(m \\, polylog(n))$ triangles, and has the same running time on graphs\nthat do not. For the latter case, we also show that it is impossible to obtain\nan approximation algorithm with faster running time than the one of the exact\napproach when the number of triangles is $O(m)$, unless well known conjectures\non triangle-freeness and Boolean matrix multiplication are false.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:55:28 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Conte", "Alessio", ""], ["Grossi", "Roberto", ""], ["Marino", "Andrea", ""], ["Versari", "Luca", ""]]}, {"id": "2010.00970", "submitter": "Paul Ferm\\'e", "authors": "Siddharth Barman, Omar Fawzi, Paul Ferm\\'e", "title": "Tight Approximation Guarantees for Concave Coverage Problems", "comments": "33 pages. v3 minor corrections and added FPT hardness", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2021.9", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the maximum coverage problem, we are given subsets $T_1, \\ldots, T_m$ of a\nuniverse $[n]$ along with an integer $k$ and the objective is to find a subset\n$S \\subseteq [m]$ of size $k$ that maximizes $C(S) := \\Big|\\bigcup_{i \\in S}\nT_i\\Big|$. It is a classic result that the greedy algorithm for this problem\nachieves an optimal approximation ratio of $1-e^{-1}$.\n  In this work we consider a generalization of this problem wherein an element\n$a$ can contribute by an amount that depends on the number of times it is\ncovered. Given a concave, nondecreasing function $\\varphi$, we define\n$C^{\\varphi}(S) := \\sum_{a \\in [n]}w_a\\varphi(|S|_a)$, where $|S|_a = |\\{i \\in\nS : a \\in T_i\\}|$. The standard maximum coverage problem corresponds to taking\n$\\varphi(j) = \\min\\{j,1\\}$. For any such $\\varphi$, we provide an efficient\nalgorithm that achieves an approximation ratio equal to the Poisson concavity\nratio of $\\varphi$, defined by $\\alpha_{\\varphi} := \\min_{x \\in \\mathbb{N}^*}\n\\frac{\\mathbb{E}[\\varphi(\\text{Poi}(x))]}{\\varphi(\\mathbb{E}[\\text{Poi}(x)])}$.\nComplementing this approximation guarantee, we establish a matching NP-hardness\nresult when $\\varphi$ grows in a sublinear way.\n  As special cases, we improve the result of [Barman et al., IPCO, 2020] about\nmaximum multi-coverage, that was based on the unique games conjecture, and we\nrecover the result of [Dudycz et al., IJCAI, 2020] on multi-winner\napproval-based voting for geometrically dominant rules. Our result goes beyond\nthese special cases and we illustrate it with applications to distributed\nresource allocation problems, welfare maximization problems and approval-based\nvoting for general rules.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:03:04 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 13:19:35 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 10:36:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Barman", "Siddharth", ""], ["Fawzi", "Omar", ""], ["Ferm\u00e9", "Paul", ""]]}, {"id": "2010.01102", "submitter": "Harold Gabow", "authors": "Harold Gabow", "title": "A Weight-scaling Algorithm for $f$-factors of Multigraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss combinatorial algorithms for finding a maximum weight $f$-factor\non an arbitrary multigraph, for given integral weights of magnitude at most\n$W$. For simple bipartite graphs the best-known time bound is $O(n^{2/3}\\, m\\,\n\\log nW)$ (\\cite{GT89}; $n$ and $m$ are respectively the number of vertices and\nedges). A recent algorithm of Duan and He et al. \\cite{DHZ} for $f$-factors of\nsimple graphs comes within logarithmic factors of this bound, $\\widetilde{O}\n(n^{2/3}\\, m\\, \\log W)$. The best-known bound for bipartite multigraphs is\n$O(\\sqrt {\\Phi}\\, m\\, \\log \\Phi W)$ ($\\Phi\\le m$ is the size of the $f$-factor,\n$\\Phi=\\sum_{v\\in V}f(v)/2$). This bound is more general than the restriction to\nsimple graphs, and is even superior on \"small\" simple graphs, i.e.,\n$\\Phi=o(n^{4/3})$. We present an algorithm that comes within a $\\sqrt {\\log\n\\Phi}$ factor of this bound, i.e., $O(\\sqrt {\\Phi \\log \\Phi}\\,m \\,\\log \\Phi\nW)$.\n  The algorithm is a direct generalization of the algorithm of Gabow and Tarjan\n\\cite{GT} for the special case of ordinary matching ($f\\equiv 1$). We present\nour algorithm first for ordinary matching, as the analysis is a simplified\nversion of \\cite{GT}. Furthermore the algorithm and analysis both get\nincorporated without modification into the multigraph algorithm.\n  To extend these ideas to $f$-factors, the first step is \"expanding\" edges\n(i.e., replacing an edge by a length 3 alternating path). \\cite{DHZ} uses a\none-time expansion of the entire graph. Our algorithm keeps the graph small by\nonly expanding selected edges, and \"compressing\" them back to their original\nsource when no longer needed. Several other ideas are needed, including a\nrelaxation of the notion of \"blossom\" to e-blossom (\"expanded blossom\").\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:58:19 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gabow", "Harold", ""]]}, {"id": "2010.01306", "submitter": "Rafael Melo", "authors": "Jesus O. Cunha and Rafael A. Melo", "title": "Valid inequalities, preprocessing, and an effective heuristic for the\n  uncapacitated three-level lot-sizing and replenishment problem with a\n  distribution structure", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2021.03.029", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the uncapacitated three-level lot-sizing and replenishment\nproblem with a distribution structure. In this NP-hard problem, a single\nproduction plant sends the produced items to replenish warehouses from where\nthey are dispatched to the retailers in order to satisfy their demands over a\nfinite planning horizon. The goal of the problem is to determine an integrated\nproduction and distribution plan minimizing the total costs, which comprehends\nfixed production and transportation setup as well as variable inventory holding\ncosts. We describe new valid inequalities both in the space of a standard mixed\ninteger programming (MIP) formulation and in that of a new alternative extended\nMIP formulation. We show that using such extended formulation, valid\ninequalities having similar structures to those in the standard one allow\nachieving tighter linear relaxation bounds. Furthermore, we propose a\npreprocessing approach to reduce the size of a multi-commodity MIP formulation\nand a multi-start randomized bottom-up dynamic programming based heuristic.\nComputational experiments indicate that the use of the valid inequalities in a\nbranch-and-cut approach significantly increase the ability of a MIP solver to\nsolve instances to optimality. Additionally, the valid inequalities for the\nextended formulation outperform those for the standard one in terms of number\nof solved instances, running time and number of enumerated nodes. Moreover, the\nproposed heuristic is able to generate solutions with considerably low\noptimality gaps within very short computational times even for large instances.\nCombining the preprocessing approach with the heuristic, one can achieve an\nincrease in the number of solutions solved to optimality within the time limit\ntogether with significant reductions on the average times for solving them.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 09:09:54 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 01:22:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cunha", "Jesus O.", ""], ["Melo", "Rafael A.", ""]]}, {"id": "2010.01331", "submitter": "Chen Feng", "authors": "Chen Feng, Luoyi Fu, Bo Jiang, Haisong Zhang, Xinbing Wang, Feilong\n  Tang and Guihai Chen", "title": "Neighborhood Matters: Influence Maximization in Social Networks with\n  Limited Access", "comments": "Already accepted by IEEE Transactions on Knowledge and Data\n  Engineering, 21 pages including 15 pages main paper and 6 pages supplemental\n  file", "journal-ref": null, "doi": "10.1109/TKDE.2020.3015387", "report-no": null, "categories": "cs.SI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization (IM) aims at maximizing the spread of influence by\noffering discounts to influential users (called seeding). In many applications,\ndue to user's privacy concern, overwhelming network scale etc., it is hard to\ntarget any user in the network as one wishes. Instead, only a small subset of\nusers is initially accessible. Such access limitation would significantly\nimpair the influence spread, since IM often relies on seeding high degree\nusers, which are particularly rare in such a small subset due to the power-law\nstructure of social networks. In this paper, we attempt to solve the limited IM\nin real-world scenarios by the adaptive approach with seeding and diffusion\nuncertainty considered. Specifically, we consider fine-grained discounts and\nassume users accept the discount probabilistically. The diffusion process is\ndepicted by the independent cascade model. To overcome the access limitation,\nwe prove the set-wise friendship paradox (FP) phenomenon that neighbors have\nhigher degree in expectation, and propose a two-stage seeding model with the FP\nembedded, where neighbors are seeded. On this basis, for comparison we\nformulate the non-adaptive case and adaptive case, both proven to be NP-hard.\nIn the non-adaptive case, discounts are allocated to users all at once. We show\nthe monotonicity of influence spread w.r.t. discount allocation and design a\ntwo-stage coordinate descent framework to decide the discount allocation. In\nthe adaptive case, users are sequentially seeded based on observations of\nexisting seeding and diffusion results. We prove the adaptive submodularity and\nsubmodularity of the influence spread function in two stages. Then, a series of\nadaptive greedy algorithms are proposed with constant approximation ratio.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:47:46 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Feng", "Chen", ""], ["Fu", "Luoyi", ""], ["Jiang", "Bo", ""], ["Zhang", "Haisong", ""], ["Wang", "Xinbing", ""], ["Tang", "Feilong", ""], ["Chen", "Guihai", ""]]}, {"id": "2010.01420", "submitter": "Sahil Singla", "authors": "Sepehr Assadi, Thomas Kesselheim, and Sahil Singla", "title": "Improved Truthful Mechanisms for Subadditive Combinatorial Auctions:\n  Breaking the Logarithmic Barrier", "comments": "SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computationally-efficient truthful mechanism for combinatorial\nauctions with subadditive bidders that achieves an\n$O((\\log\\!\\log{m})^3)$-approximation to the maximum welfare in expectation\nusing $O(n)$ demand queries; here $m$ and $n$ are the number of items and\nbidders, respectively. This breaks the longstanding logarithmic barrier for the\nproblem dating back to the $O(\\log{m}\\cdot\\log\\!\\log{m})$-approximation\nmechanism of Dobzinski from 2007. Along the way, we also improve and\nconsiderably simplify the state-of-the-art mechanisms for submodular bidders.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 20:12:28 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Assadi", "Sepehr", ""], ["Kesselheim", "Thomas", ""], ["Singla", "Sahil", ""]]}, {"id": "2010.01439", "submitter": "Arya Tanmay Gupta", "authors": "Arya Tanmay Gupta", "title": "Burning Geometric Graphs", "comments": "Masters' dissertation", "journal-ref": null, "doi": null, "report-no": "201861003", "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A procedure called \\textit{graph burning} was introduced to facilitate the\nmodelling of spread of an alarm, a social contagion, or a social influence or\nemotion on graphs and networks.\n  Graph burning runs on discrete time-steps (or rounds). At each step $t$,\nfirst (a) an unburned vertex is burned (as a \\textit{fire source}) from\n\"outside\", and then (b) the fire spreads to vertices adjacent to the vertices\nwhich are burned till step $t-1$. This process stops after all the vertices of\n$G$ have been burned. The aim is to burn all the vertices in a given graph in\nminimum time-steps. The least number of time-steps required to burn a graph is\ncalled its \\textit{burning number}. The less the burning number is, the faster\na graph can be burned.\n  Burning a general graph optimally is an NP-Complete problem. It has been\nproved that optimal burning of path forests, spider graphs, and trees with\nmaximum degree three is NP-Complete. We study the \\textit{graph burning\nproblem} on several sub-classes of \\textit{geometric graphs}.\n  We show that burning interval graphs (Section 7.1, Theorem 7.1), permutation\ngraphs (Section 7.2, Theorem 7.2) and disk graphs (Section 7.3, Theorem 7.3)\noptimally is NP-Complete. In addition, we opine that optimal burning of general\ngraphs (Section 9.2, Conjecture 9.1) cannot be approximated better than\n3-approximation factor.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 23:03:11 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gupta", "Arya Tanmay", ""]]}, {"id": "2010.01457", "submitter": "Arun Ganesh", "authors": "Arun Ganesh, Jiazheng Zhao", "title": "Privately Answering Counting Queries with Generalized Gaussian\n  Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of answering $k$ counting (i.e. sensitivity-1)\nqueries about a database with $(\\epsilon, \\delta)$-differential privacy. We\ngive a mechanism such that if the true answers to the queries are the vector\n$d$, the mechanism outputs answers $\\tilde{d}$ with the $\\ell_\\infty$-error\nguarantee:\n  $$\\mathcal{E}\\left[||\\tilde{d} - d||_\\infty\\right] = O\\left(\\frac{\\sqrt{k\n\\log \\log \\log k \\log(1/\\delta)}}{\\epsilon}\\right).$$\n  This reduces the multiplicative gap between the best known upper and lower\nbounds on $\\ell_\\infty$-error from $O(\\sqrt{\\log \\log k})$ to $O(\\sqrt{\\log\n\\log \\log k})$. Our main technical contribution is an analysis of the family of\nmechanisms of the following form for answering counting queries: Sample $x$\nfrom a \\textit{Generalized Gaussian}, i.e. with probability proportional to\n$\\exp(-(||x||_p/\\sigma)^p)$, and output $\\tilde{d} = d + x$. This family of\nmechanisms offers a tradeoff between $\\ell_1$ and $\\ell_\\infty$-error\nguarantees and may be of independent interest. For $p = O(\\log \\log k)$, this\nmechanism already matches the previous best known $\\ell_\\infty$-error bound. We\narrive at our main result by composing this mechanism for $p = O(\\log \\log \\log\nk)$ with the sparse vector mechanism, generalizing a technique of Steinke and\nUllman.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 01:06:19 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ganesh", "Arun", ""], ["Zhao", "Jiazheng", ""]]}, {"id": "2010.01459", "submitter": "Vaggos Chatziafratis", "authors": "Vaggos Chatziafratis, Neha Gupta, Euiwoong Lee", "title": "Inapproximability for Local Correlation Clustering and Dissimilarity\n  Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present hardness of approximation results for Correlation Clustering with\nlocal objectives and for Hierarchical Clustering with dissimilarity\ninformation. For the former, we study the local objective of Puleo and\nMilenkovic (ICML '16) that prioritizes reducing the disagreements at data\npoints that are worst off and for the latter we study the maximization version\nof Dasgupta's cost function (STOC '16). Our APX hardness results imply that the\ntwo problems are hard to approximate within a constant of 4/3 ~ 1.33 (assuming\nP vs NP) and 9159/9189 ~ 0.9967 (assuming the Unique Games Conjecture)\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 01:16:07 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chatziafratis", "Vaggos", ""], ["Gupta", "Neha", ""], ["Lee", "Euiwoong", ""]]}, {"id": "2010.01503", "submitter": "Merav Parter", "authors": "Merav Parter", "title": "Distributed Constructions of Dual-Failure Fault-Tolerant Distance\n  Preservers", "comments": "To appear in DISC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault tolerant distance preservers (spanners) are sparse subgraphs that\npreserve (approximate) distances between given pairs of vertices under edge or\nvertex failures. So-far, these structures have been studied mainly from a\ncentralized viewpoint. Despite the fact fault tolerant preservers are mainly\nmotivated by the error-prone nature of distributed networks, not much is known\non the distributed computational aspects of these structures.\n  In this paper, we present distributed algorithms for constructing fault\ntolerant distance preservers and $+2$ additive spanners that are resilient to\nat most \\emph{two edge} faults. Prior to our work, the only non-trivial\nconstructions known were for the \\emph{single} fault and \\emph{single source}\nsetting by [Ghaffari and Parter SPAA'16].\n  Our key technical contribution is a distributed algorithm for computing\ndistance preservers w.r.t. a subset $S$ of source vertices, resilient to two\nedge faults. The output structure contains a BFS tree $BFS(s,G \\setminus\n\\{e_1,e_2\\})$ for every $s \\in S$ and every $e_1,e_2 \\in G$. The distributed\nconstruction of this structure is based on a delicate balance between the edge\ncongestion (formed by running multiple BFS trees simultaneously) and the\nsparsity of the output subgraph. No sublinear-round algorithms for constructing\nthese structures have been known before.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:04:07 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "2010.01631", "submitter": "Xu Rao", "authors": "Dorit S. Hochbaum and Xu Rao", "title": "A Fully Polynomial Time Approximation Scheme for the Replenishment\n  Storage Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Replenishment Storage problem (RSP) is to minimize the storage capacity\nrequirement for a deterministic demand, multi-item inventory system where each\nitem has a given reorder size and cycle length. The reorders can only take\nplace at integer time units within the cycle. This problem was shown to be\nweakly NP-hard for constant joint cycle length (the least common multiple of\nthe lengths of all individual cycles). When all items have the same constant\ncycle length, there exists a Fully Polynomial Time Approximation Scheme\n(FPTAS), but no FPTAS has been known for the case when the individual cycles\nare different. Here we devise the first known FPTAS for the RSP with different\nindividual cycles and constant joint cycle length.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 17:07:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hochbaum", "Dorit S.", ""], ["Rao", "Xu", ""]]}, {"id": "2010.01705", "submitter": "Vasilis Kontonis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Christos Tzamos,\n  Nikos Zarifis", "title": "A Polynomial Time Algorithm for Learning Halfspaces with Tsybakov Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of PAC learning homogeneous halfspaces in the presence\nof Tsybakov noise. In the Tsybakov noise model, the label of every sample is\nindependently flipped with an adversarially controlled probability that can be\narbitrarily close to $1/2$ for a fraction of the samples. {\\em We give the\nfirst polynomial-time algorithm for this fundamental learning problem.} Our\nalgorithm learns the true halfspace within any desired accuracy $\\epsilon$ and\nsucceeds under a broad family of well-behaved distributions including\nlog-concave distributions. Prior to our work, the only previous algorithm for\nthis problem required quasi-polynomial runtime in $1/\\epsilon$.\n  Our algorithm employs a recently developed reduction \\cite{DKTZ20b} from\nlearning to certifying the non-optimality of a candidate halfspace. This prior\nwork developed a quasi-polynomial time certificate algorithm based on\npolynomial regression. {\\em The main technical contribution of the current\npaper is the first polynomial-time certificate algorithm.} Starting from a\nnon-trivial warm-start, our algorithm performs a novel \"win-win\" iterative\nprocess which, at each step, either finds a valid certificate or improves the\nangle between the current halfspace and the true one. Our warm-start algorithm\nfor isotropic log-concave distributions involves a number of analytic tools\nthat may be of broader interest. These include a new efficient method for\nreweighting the distribution in order to recenter it and a novel\ncharacterization of the spectrum of the degree-$2$ Chow parameters.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:19:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2010.01801", "submitter": "Robin Kothari", "authors": "Ankit Garg, Robin Kothari, Praneeth Netrapalli, Suhail Sherif", "title": "No quantum speedup over gradient descent for non-smooth convex\n  optimization", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the first-order convex optimization problem, where we have black-box\naccess to a (not necessarily smooth) function $f:\\mathbb{R}^n \\to \\mathbb{R}$\nand its (sub)gradient. Our goal is to find an $\\epsilon$-approximate minimum of\n$f$ starting from a point that is distance at most $R$ from the true minimum.\nIf $f$ is $G$-Lipschitz, then the classic gradient descent algorithm solves\nthis problem with $O((GR/\\epsilon)^{2})$ queries. Importantly, the number of\nqueries is independent of the dimension $n$ and gradient descent is optimal in\nthis regard: No deterministic or randomized algorithm can achieve better\ncomplexity that is still independent of the dimension $n$.\n  In this paper we reprove the randomized lower bound of\n$\\Omega((GR/\\epsilon)^{2})$ using a simpler argument than previous lower\nbounds. We then show that although the function family used in the lower bound\nis hard for randomized algorithms, it can be solved using $O(GR/\\epsilon)$\nquantum queries. We then show an improved lower bound against quantum\nalgorithms using a different set of instances and establish our main result\nthat in general even quantum algorithms need $\\Omega((GR/\\epsilon)^2)$ queries\nto solve the problem. Hence there is no quantum speedup over gradient descent\nfor black-box first-order convex optimization without further assumptions on\nthe function family.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 06:32:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Garg", "Ankit", ""], ["Kothari", "Robin", ""], ["Netrapalli", "Praneeth", ""], ["Sherif", "Suhail", ""]]}, {"id": "2010.01890", "submitter": "Brice Minaud", "authors": "Brice Minaud, Charalampos Papamanthou", "title": "Note on Generalized Cuckoo Hashing with a Stash", "comments": "6 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo hashing is a common hashing technique, guaranteeing constant-time\nlookups in the worst case. Adding a stash was proposed by Kirsch, Mitzenmacher,\nand Wieder at SICOMP 2010, as a way to reduce the probability of rehash. It has\nsince become a standard technique in areas such as cryptography, where a\nsuperpolynomially low probability of rehash is often required. Another\nextension of cuckoo hashing is to allow multiple items per bucket, improving\nthe load factor. That extension was also analyzed by Kirsch et al. in the\npresence of a stash. The purpose of this note is to repair a bug in that\nanalysis. Letting $d$ be the number of items per bucket, and $s$ be the stash\nsize, the original claim was that the probability that a valid cuckoo\nassignment fails to exist is $O(n^{(1-d)(s+1)})$. We point to an error in the\nargument, and show that it is $\\Theta(n^{-d-s})$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:54:26 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Minaud", "Brice", ""], ["Papamanthou", "Charalampos", ""]]}, {"id": "2010.01901", "submitter": "Mohammad Shadravan", "authors": "Mohammad Shadravan", "title": "Improved Submodular Secretary Problem with Shortlists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First, for the for the submodular $k$-secretary problem with shortlists [1],\nwe provide a near optimal $1-1/e-\\epsilon$ approximation using shortlist of\nsize $O(k poly(1/\\epsilon))$. In particular, we improve the size of shortlist\nused in \\cite{us} from $O(k 2^{poly(1/\\epsilon)})$ to $O(k poly(1/\\epsilon))$.\nAs a result, we provide a near optimal approximation algorithm for random-order\nstreaming of monotone submodular functions under cardinality constraints, using\nmemory $O(k poly(1/\\epsilon))$. It exponentially improves the running time and\nmemory of \\cite{us} in terms of $1/\\epsilon$.\n  Next we generalize the problem to matroid constraints, which we refer to as\nsubmodular matroid secretary problem with shortlists. It is a variant of the\n\\textit{matroid secretary problem} \\cite{feldman2014simple}, in which the\nalgorithm is allowed to have a shortlist. We design an algorithm that achieves\na $\\frac{1}{2}(1-1/e^2 -\\epsilon)$ competitive ratio for any constant\n$\\epsilon>0$, using a shortlist of size $O(k poly(\\frac{1}{\\epsilon}))$.\nMoreover, we generalize our results to the case of $p$-matchoid constraints and\ngive a $\\frac{1}{p+1}(1-1/e^{p+1}-\\epsilon )$ approximation using shortlist of\nsize $O(k poly(\\frac{1}{\\epsilon}))$. It asymptotically approaches the best\nknown offline guarantee $\\frac{1}{p+1}$ [22]. Furthermore, we show that our\nalgorithms can be implemented in the streaming setting using $O(k\npoly(\\frac{1}{\\epsilon}))$ memory.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 17:59:42 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:59:02 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Shadravan", "Mohammad", ""]]}, {"id": "2010.02116", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, Huacheng Yu", "title": "Optimal bounds for approximate counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storing a counter incremented $N$ times would naively consume $O(\\log N)$\nbits of memory. In 1978 Morris described the very first streaming algorithm:\nthe \"Morris Counter\" [Morris78]. His algorithm has been shown to use\n$O(\\log\\log N + \\log(1/\\varepsilon) + \\log(1/\\delta))$ bits of memory to\nprovide a $(1+\\varepsilon)$-approximation with probability $1-\\delta$ to the\ncounter's value. We provide a new simple algorithm with a simple analysis\nshowing that $O(\\log\\log N + \\log(1/\\varepsilon) + \\log\\log(1/\\delta))$ bits\nsuffice for the same task, i.e. an exponentially improved dependence on the\ninverse failure probability. We then provide a more technical analysis showing\nthat the original Morris Counter itself, after a minor but necessary tweak,\nactually also enjoys this same improved upper bound. Lastly, we prove a new\nlower bound for this task showing optimality of our upper bound. We thus\ncompletely resolve the asymptotic space complexity of approximate counting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:58:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nelson", "Jelani", ""], ["Yu", "Huacheng", ""]]}, {"id": "2010.02264", "submitter": "Aarshvi Gajjar", "authors": "Aarshvi Gajjar, Cameron Musco", "title": "Subspace Embeddings Under Nonlinear Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider low-distortion embeddings for subspaces under \\emph{entrywise\nnonlinear transformations}. In particular we seek embeddings that preserve the\nnorm of all vectors in a space $S = \\{y: y = f(x)\\text{ for }x \\in Z\\}$, where\n$Z$ is a $k$-dimensional subspace of $\\mathbb{R}^n$ and $f(x)$ is a nonlinear\nactivation function applied entrywise to $x$. When $f$ is the identity, and so\n$S$ is just a $k$-dimensional subspace, it is known that, with high\nprobability, a random embedding into $O(k/\\epsilon^2)$ dimensions preserves the\nnorm of all $y \\in S$ up to $(1\\pm \\epsilon)$ relative error. Such embeddings\nare known as \\emph{subspace embeddings}, and have found widespread use in\ncompressed sensing and approximation algorithms. We give the first\nlow-distortion embeddings for a wide class of nonlinear functions $f$. In\nparticular, we give additive $\\epsilon$ error embeddings into $O(\\frac{k\\log\n(n/\\epsilon)}{\\epsilon^2})$ dimensions for a class of nonlinearities that\nincludes the popular Sigmoid SoftPlus, and Gaussian functions. We strengthen\nthis result to give relative error embeddings under some further restrictions,\nwhich are satisfied e.g., by the Tanh, SoftSign, Exponential Linear Unit, and\nmany other `soft' step functions and rectifying units. Understanding embeddings\nfor subspaces under nonlinear transformations is a key step towards extending\nrandom sketching and compressing sensing techniques for linear problems to\nnonlinear ones. We discuss example applications of our results to improved\nbounds for compressed sensing via generative neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:18:04 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Gajjar", "Aarshvi", ""], ["Musco", "Cameron", ""]]}, {"id": "2010.02324", "submitter": "R. Teal Witter", "authors": "Shelby Kimmel and R. Teal Witter", "title": "A Query-Efficient Quantum Algorithm for Maximum Matching on General\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design quantum algorithms for maximum matching. Working in the query\nmodel, in both adjacency matrix and adjacency list settings, we improve on the\nbest known algorithms for general graphs, matching previously obtained results\nfor bipartite graphs. In particular, for a graph with $n$ nodes and $m$ edges,\nour algorithm makes $O(n^{7/4})$ queries in the matrix model and\n$O(n^{3/4}(m+n)^{1/2})$ queries in the list model. Our approach combines\nGabow's classical maximum matching algorithm [Gabow, Fundamenta Informaticae,\n'17] with the guessing tree method of Beigi and Taghavi [Beigi and Taghavi,\nQuantum, '20].\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:34:39 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:59:41 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kimmel", "Shelby", ""], ["Witter", "R. Teal", ""]]}, {"id": "2010.02331", "submitter": "Ran Ben Basat", "authors": "Ran Ben-Basat, Michael Mitzenmacher, Shay Vargaftik", "title": "How to send a real number using a single bit (and some shared\n  randomness)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of communicating an estimate of a real\nnumber $x\\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value\n$X\\in\\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the\nvalue of $X$. We consider both the biased and unbiased estimation problems and\naim to minimize the cost. For the biased case, the cost is the worst-case (over\nthe choice of $x$) expected squared error, which coincides with the variance if\nthe algorithm is required to be unbiased.\n  We first overview common biased and unbiased estimation approaches and prove\ntheir optimality when no shared randomness is allowed. We then show how a small\namount of shared randomness, which can be as low as a single bit, reduces the\ncost in both cases. Specifically, we derive lower bounds on the cost attainable\nby any algorithm with unrestricted use of shared randomness and propose\nnear-optimal solutions that use a small number of shared random bits. Finally,\nwe discuss open problems and future directions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:52:06 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 19:29:26 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 11:33:46 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 09:41:14 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Mitzenmacher", "Michael", ""], ["Vargaftik", "Shay", ""]]}, {"id": "2010.02379", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Shangdi Yu, Yan Gu, Julian Shun", "title": "A Parallel Batch-Dynamic Data Structure for the Closest Pair Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theoretically-efficient and practical parallel batch-dynamic\ndata structure for the closest pair problem. Our solution is based on a serial\ndynamic closest pair data structure by Golin et al., and supports batches of\ninsertions and deletions in parallel. For a data set of size $n$, our data\nstructure supports a batch of insertions or deletions of size $m$ in\n$O(m(1+\\log ((n+m)/m)))$ expected work and $O(\\log (n+m)\\log^*(n+m))$ depth\nwith high probability, and takes linear space. The key techniques for achieving\nthese bounds are a new work-efficient parallel batch-dynamic binary heap, and\ncareful management of the computation across sets of points to minimize work\nand depth.\n  We provide an optimized multicore implementation of our data structure using\ndynamic hash tables, parallel heaps, and dynamic $k$-d trees. Our experiments\non a variety of synthetic and real-world data sets show that it achieves a\nparallel speedup of up to 38.57x (15.10x on average) on 48 cores with\nhyper-threading. In addition, we also implement and compare four parallel\nalgorithms for static closest pair problem, for which we are not aware of any\nexisting practical implementations. On 48 cores with hyper-threading, the\nstatic algorithms achieve up to 51.45x (29.42x on average) speedup, and Rabin's\nalgorithm performs the best on average. Comparing our dynamic algorithm to the\nfastest static algorithm, we find that it is advantageous to use the dynamic\nalgorithm for batch sizes of up to 20\\% of the data set. As far as we know, our\nwork is the first to experimentally evaluate parallel closest pair algorithms,\nin both the static and the dynamic settings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:50:33 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 22:30:41 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 23:26:51 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wang", "Yiqiu", ""], ["Yu", "Shangdi", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "2010.02388", "submitter": "Yasuaki Kobayashi", "authors": "Yasuaki Kobayashi, Yu Nakahata", "title": "A Note on Exponential-Time Algorithms for Linearwidth", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we give an algorithm that computes the linearwidth of input\n$n$-vertex graphs in time $O^*(2^n)$, which improves a trivial $O^*(2^m)$-time\nalgorithm, where $n$ and $m$ the number of vertices and edges, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:12:05 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 09:53:42 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kobayashi", "Yasuaki", ""], ["Nakahata", "Yu", ""]]}, {"id": "2010.02507", "submitter": "Vladan Majerech Dr.", "authors": "Vladan Majerech", "title": "Fast DecreaseKey Heaps with worst-case variants", "comments": "9 pages+1 citation, 1 figure, 3 tables. arXiv admin note: text\n  overlap with arXiv:1911.11637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper \"Fast Fibonacci heaps with worst case extensions\", we have\ndescribed heaps with both Meld-DecreaseKey and DecreaseKey interfaces, allowing\noperations with guaranteed worst-case asymptotically optimal times. The paper\nwas intended to concentrate on the DecreaseKey interface, but it could be hard\nto separate the two described data structures without careful reading. The\ncurrent paper's goal is not to invent a novel data structure, but to describe a\nrather easy DecreaseKey version in a hopefully readable form. The paper is\nintended not to require reference to other papers.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:55:50 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 20:14:36 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Majerech", "Vladan", ""]]}, {"id": "2010.02583", "submitter": "Stefan Hougardy", "authors": "Ulrich A. Brodowsky and Stefan Hougardy", "title": "The Approximation Ratio of the 2-Opt Heuristic for the Euclidean\n  Traveling Salesman Problem", "comments": "revised version, to appear in: 38th International Symposium on\n  Theoretical Aspects of Computer Science (STACS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2-Opt heuristic is a simple improvement heuristic for the Traveling\nSalesman Problem. It starts with an arbitrary tour and then repeatedly replaces\ntwo edges of the tour by two other edges, as long as this yields a shorter\ntour. We will prove that for Euclidean Traveling Salesman Problems with $n$\ncities the approximation ratio of the 2-Opt heuristic is $\\Theta(\\log n/ \\log\n\\log n)$. This improves the upper bound of $O(\\log n$) given by Chandra,\nKarloff, and Tovey [3] in 1999.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:49:13 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 08:24:14 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 20:11:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Brodowsky", "Ulrich A.", ""], ["Hougardy", "Stefan", ""]]}, {"id": "2010.02618", "submitter": "John Fearnley", "authors": "John Fearnley, D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi, and Rahul Savani", "title": "A faster algorithm for finding Tarski fixed points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dang et al. have given an algorithm that can find a Tarski fixed point in a\n$k$-dimensional lattice of width $n$ using $O(\\log^{k} n)$ queries. Multiple\nauthors have conjectured that this algorithm is optimal [Dang et al., Etessami\net al.], and indeed this has been proven for two-dimensional instances\n[Etessami et al.]. We show that these conjectures are false in dimension three\nor higher by giving an $O(\\log^2 n)$ query algorithm for the three-dimensional\nTarski problem. We also give a new decomposition theorem for $k$-dimensional\nTarski problems which, in combination with our new algorithm for three\ndimensions, gives an $O(\\log^{2 \\lceil k/3 \\rceil} n)$ query algorithm for the\n$k$-dimensional problem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:56:42 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 09:05:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Fearnley", "John", ""], ["P\u00e1lv\u00f6lgyi", "D\u00f6m\u00f6t\u00f6r", ""], ["Savani", "Rahul", ""]]}, {"id": "2010.02772", "submitter": "Yangsibo Huang", "authors": "Yangsibo Huang, Zhao Song, Kai Li, Sanjeev Arora", "title": "InstaHide: Instance-hiding Schemes for Private Distributed Learning", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  How can multiple distributed entities collaboratively train a shared deep net\non their private data while preserving privacy? This paper introduces\nInstaHide, a simple encryption of training images, which can be plugged into\nexisting distributed deep learning pipelines. The encryption is efficient and\napplying it during training has minor effect on test accuracy.\n  InstaHide encrypts each training image with a \"one-time secret key\" which\nconsists of mixing a number of randomly chosen images and applying a random\npixel-wise mask. Other contributions of this paper include: (a) Using a large\npublic dataset (e.g. ImageNet) for mixing during its encryption, which improves\nsecurity. (b) Experimental results to show effectiveness in preserving privacy\nagainst known attacks with only minor effects on accuracy. (c) Theoretical\nanalysis showing that successfully attacking privacy requires attackers to\nsolve a difficult computational problem. (d) Demonstrating that use of the\npixel-wise mask is important for security, since Mixup alone is shown to be\ninsecure to some some efficient attacks. (e) Release of a challenge dataset\nhttps://github.com/Hazelsuko07/InstaHide_Challenge\n  Our code is available at https://github.com/Hazelsuko07/InstaHide\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:43:23 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 18:54:19 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Huang", "Yangsibo", ""], ["Song", "Zhao", ""], ["Li", "Kai", ""], ["Arora", "Sanjeev", ""]]}, {"id": "2010.02787", "submitter": "Maximilian Katzmann", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Maximilian Katzmann", "title": "Efficiently Approximating Vertex Cover on Scale-Free Networks with\n  Underlying Hyperbolic Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding a minimum vertex cover in a network is a fundamental NP-complete\ngraph problem. One way to deal with its computational hardness, is to trade the\nqualitative performance of an algorithm (allowing non-optimal outputs) for an\nimproved running time. For the vertex cover problem, there is a gap between\ntheory and practice when it comes to understanding this tradeoff. On the one\nhand, it is known that it is NP-hard to approximate a minimum vertex cover\nwithin a factor of $\\sqrt{2}$. On the other hand, a simple greedy algorithm\nyields close to optimal approximations in practice.\n  A promising approach towards understanding this discrepancy is to recognize\nthe differences between theoretical worst-case instances and real-world\nnetworks. Following this direction, we close the gap between theory and\npractice by providing an algorithm that efficiently computes nearly optimal\nvertex cover approximations on hyperbolic random graphs; a network model that\nclosely resembles real-world networks in terms of degree distribution,\nclustering, and the small-world property. More precisely, our algorithm\ncomputes a $(1 + o(1))$-approximation, asymptotically almost surely, and has a\nrunning time of $\\mathcal{O}(m \\log(n))$.\n  The proposed algorithm is an adaption of the successful greedy approach,\nenhanced with a procedure that improves on parts of the graph where greedy is\nnot optimal. This makes it possible to introduce a parameter that can be used\nto tune the tradeoff between approximation performance and running time. Our\nempirical evaluation on real-world networks shows that this allows for\nimproving over the near-optimal results of the greedy approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:56:48 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 11:04:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["Katzmann", "Maximilian", ""]]}, {"id": "2010.02841", "submitter": "Aidao Chen", "authors": "Aidao Chen, Anindya De, Aravindan Vijayaraghavan", "title": "Learning a mixture of two subspaces over finite fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a mixture of two subspaces over\n$\\mathbb{F}_2^n$. The goal is to recover the individual subspaces, given\nsamples from a (weighted) mixture of samples drawn uniformly from the two\nsubspaces $A_0$ and $A_1$.\n  This problem is computationally challenging, as it captures the notorious\nproblem of \"learning parities with noise\" in the degenerate setting when $A_1\n\\subseteq A_0$. This is in contrast to the analogous problem over the reals\nthat can be solved in polynomial time (Vidal'03). This leads to the following\nnatural question: is Learning Parities with Noise the only computational\nbarrier in obtaining efficient algorithms for learning mixtures of subspaces\nover $\\mathbb{F}_2^n$?\n  The main result of this paper is an affirmative answer to the above question.\nNamely, we show the following results: 1. When the subspaces $A_0$ and $A_1$\nare incomparable, i.e., $A_0$ and $A_1$ are not contained inside each other,\nthen there is a polynomial time algorithm to recover the subspaces $A_0$ and\n$A_1$. 2. In the case when $A_1$ is a subspace of $A_0$ with a significant gap\nin the dimension i.e., $dim(A_1) \\le \\alpha dim(A_0)$ for $\\alpha<1$, there is\na $n^{O(1/(1-\\alpha))}$ time algorithm to recover the subspaces $A_0$ and\n$A_1$.\n  Thus, our algorithms imply computational tractability of the problem of\nlearning mixtures of two subspaces, except in the degenerate setting captured\nby learning parities with noise.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:04:42 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 17:49:45 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Chen", "Aidao", ""], ["De", "Anindya", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2010.02912", "submitter": "Flavio Chierichetti", "authors": "Flavio Chierichetti, Anirban Dasgupta, Ravi Kumar", "title": "On Additive Approximate Submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A real-valued set function is (additively) approximately submodular if it\nsatisfies the submodularity conditions with an additive error. Approximate\nsubmodularity arises in many settings, especially in machine learning, where\nthe function evaluation might not be exact. In this paper we study how close\nsuch approximately submodular functions are to truly submodular functions.\n  We show that an approximately submodular function defined on a ground set of\n$n$ elements is $O(n^2)$ pointwise-close to a submodular function. This result\nalso provides an algorithmic tool that can be used to adapt existing submodular\noptimization algorithms to approximately submodular functions. To complement,\nwe show an $\\Omega(\\sqrt{n})$ lower bound on the distance to submodularity.\n  These results stand in contrast to the case of approximate modularity, where\nthe distance to modularity is a constant, and approximate convexity, where the\ndistance to convexity is logarithmic.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:48:28 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 16:59:21 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Chierichetti", "Flavio", ""], ["Dasgupta", "Anirban", ""], ["Kumar", "Ravi", ""]]}, {"id": "2010.02982", "submitter": "Alexandre Vigny", "authors": "Alexandre Vigny", "title": "Dynamic Query Evaluation Over Structures with Low Degree", "comments": "21 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the evaluation of first-order queries over classes of databases\nthat have bounded degree and low degree. More precisely, given a query and a\ndatabase, we want to efficiently test whether there is a solution, count how\nmany solutions there are, or be able to enumerate the set of all solutions.\n  Bounded and low degree are rather natural notions and both yield efficient\nalgorithms. For example, Berkholz, Keppeler, and Schweikardt showed in 2017\nthat over databases of bounded degree, not only any first order query can\nefficiently be tested, counted and enumerated, but the data structure used can\nbe updated when the database itself is updated.\n  This paper extends existing results in two directions. First, we show that\nover classes of databases with low degree, there is a data structure that\nenables us to test, count and enumerate the solutions of first order queries.\nThis data structure can also be efficiently recomputed when the database is\nupdated. Secondly, for classes of databases with bounded degree we show that,\nwithout increasing the preprocessing time, we can compute a data structure that\ndoes not depend on the query but only on its quantifier rank. We can therefore\nperform a single preprocessing that can later be used for many queries.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:16:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Vigny", "Alexandre", ""]]}, {"id": "2010.02988", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Elke A. Rundensteiner, David Maier", "title": "GRETA: Graph-based Real-time Event Trend Aggregation", "comments": "Technical report for the paper in VLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming applications from algorithmic trading to traffic management deploy\nKleene patterns to detect and aggregate arbitrarily-long event sequences,\ncalled event trends. State-of-the-art systems process such queries in two\nsteps. Namely, they first construct all trends and then aggregate them. Due to\nthe exponential costs of trend construction, this two-step approach suffers\nfrom both a long delays and high memory costs. To overcome these limitations,\nwe propose the Graph-based Real-time Event Trend Aggregation (Greta) approach\nthat dynamically computes event trend aggregation without first constructing\nthese trends. We define the Greta graph to compactly encode all trends. Our\nGreta runtime incrementally maintains the graph, while dynamically propagating\naggregates along its edges. Based on the graph, the final aggregate is\nincrementally updated and instantaneously returned at the end of each query\nwindow. Our Greta runtime represents a win-win solution, reducing both the time\ncomplexity from exponential to quadratic and the space complexity from\nexponential to linear in the number of events. Our experiments demonstrate that\nGreta achieves up to four orders of magnitude speed-up and up to 50--fold\nmemory reduction compared to the state-of-the-art two-step approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:26:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Rundensteiner", "Elke A.", ""], ["Maier", "David", ""]]}, {"id": "2010.03105", "submitter": "Mahdi Belbasi", "authors": "Mahdi Belbasi, Martin F\\\"urer", "title": "An Improvement of Reed's Treewidth Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approximation algorithm for the treewidth problem which\nconstructs a corresponding tree decomposition as well. Our algorithm is a\nfaster variation of Reed's classical algorithm. For the benefit of the reader,\nand to be able to compare these two algorithms, we start with a detailed time\nanalysis for Reed's algorithm. We fill in many details that have been omitted\nin Reed's paper. Computing tree decompositions parameterized by the treewidth\n$k$ is fixed parameter tractable (FPT), meaning that there are algorithms\nrunning in time $O(f(k) g(n))$ where $f$ is a computable function, $g$ is a\npolynomial function, and $n$ is the number of vertices. An analysis of Reed's\nalgorithm shows $f(k) = 2^{O(k \\log k)}$ and $g(n) = n \\log n$ for a\n5-approximation. Reed simply claims time $O(n \\log n)$ for bounded $k$ for his\nconstant factor approximation algorithm, but the bound of $2^{\\Omega(k \\log k)}\nn \\log n$ is well known. From a practical point of view, we notice that the\ntime of Reed's algorithm also contains a term of $O(k^2 2^{24k} n \\log n)$,\nwhich for small $k$ is much worse than the asymptotically leading term of\n$2^{O(k \\log k)} n \\log n$. We analyze $f(k)$ more precisely, because the\npurpose of this paper is to improve the running times for all reasonably small\nvalues of $k$.\n  Our algorithm runs in $\\mathcal{O}(f(k)n\\log{n})$ too, but with a much\nsmaller dependence on $k$. In our case, $f(k) = 2^{\\mathcal{O}(k)}$. This\nalgorithm is simple and fast, especially for small values of $k$. We should\nmention that Bodlaender et al.\\ [2016] have an asymptotically faster algorithm\nrunning in time $2^{\\mathcal{O}(k)} n$. It relies on a very sophisticated data\nstructure and does not claim to be useful for small values of $k$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 01:38:40 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Belbasi", "Mahdi", ""], ["F\u00fcrer", "Martin", ""]]}, {"id": "2010.03106", "submitter": "Kevin Tian", "authors": "Yin Tat Lee, Ruoqi Shen, Kevin Tian", "title": "Structured Logconcave Sampling with a Restricted Gaussian Oracle", "comments": "56 pages. The results of Section 5 of this paper, as well as an\n  empirical evaluation, appeared earlier as arXiv:2006.05976. Updated version\n  polishes exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms for sampling several structured logconcave families to\nhigh accuracy. We further develop a reduction framework, inspired by proximal\npoint methods in convex optimization, which bootstraps samplers for regularized\ndensities to improve dependences on problem conditioning. A key ingredient in\nour framework is the notion of a \"restricted Gaussian oracle\" (RGO) for $g:\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$, which is a sampler for distributions\nwhose negative log-likelihood sums a quadratic and $g$. By combining our\nreduction framework with our new samplers, we obtain the following bounds for\nsampling structured distributions to total variation distance $\\epsilon$. For\ncomposite densities $\\exp(-f(x) - g(x))$, where $f$ has condition number\n$\\kappa$ and convex (but possibly non-smooth) $g$ admits an RGO, we obtain a\nmixing time of $O(\\kappa d \\log^3\\frac{\\kappa d}{\\epsilon})$, matching the\nstate-of-the-art non-composite bound; no composite samplers with better mixing\nthan general-purpose logconcave samplers were previously known. For logconcave\nfinite sums $\\exp(-F(x))$, where $F(x) = \\frac{1}{n}\\sum_{i \\in [n]} f_i(x)$\nhas condition number $\\kappa$, we give a sampler querying $\\widetilde{O}(n +\n\\kappa\\max(d, \\sqrt{nd}))$ gradient oracles to $\\{f_i\\}_{i \\in [n]}$; no\nhigh-accuracy samplers with nontrivial gradient query complexity were\npreviously known. For densities with condition number $\\kappa$, we give an\nalgorithm obtaining mixing time $O(\\kappa d \\log^2\\frac{\\kappa d}{\\epsilon})$,\nimproving the prior state-of-the-art by a logarithmic factor with a\nsignificantly simpler analysis; we also show a zeroth-order algorithm attains\nthe same query complexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 01:43:07 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 20:17:48 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 02:19:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lee", "Yin Tat", ""], ["Shen", "Ruoqi", ""], ["Tian", "Kevin", ""]]}, {"id": "2010.03287", "submitter": "Prantar Ghosh", "authors": "Prantar Ghosh", "title": "New Verification Schemes for Frequency-Based Functions on Data Streams", "comments": "To appear in FSTTCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the general problem of computing frequency-based functions, i.e.,\nthe sum of any given function of data stream frequencies. Special cases include\nfundamental data stream problems such as computing the number of distinct\nelements ($F_0$), frequency moments ($F_k$), and heavy-hitters. It can also be\napplied to calculate the maximum frequency of an element ($F_{\\infty}$).\n  Given that exact computation of most of these special cases provably do not\nadmit any sublinear space algorithm, a natural approach is to consider them in\nan enhanced data streaming model, where we have a computationally unbounded but\nuntrusted prover sending proofs or help messages to ease the computation. Think\nof a memory-restricted client delegating the computation to a stronger cloud\nservice whom it doesn't want to trust blindly. Using its limited memory, it\nwants to verify the proof that the cloud sends. Chakrabarti et al.~(ICALP '09)\nintroduced this setting as the \"annotated data streaming model\" and showed that\nmultiple problems including exact computation of frequency-based\nfunctions---that have no sublinear algorithms in basic streaming---do have\nannotated streaming algorithms, also called \"schemes\", with both space and\nproof-length sublinear in the input size.\n  We give a general scheme for computing any frequency-based function with both\nspace usage and proof-size of $O(n^{2/3}\\log n)$ bits, where $n$ is the size of\nthe universe. This improves upon the best known bound of $O(n^{2/3}\\log^{4/3}\nn)$ given by the seminal paper of Chakrabarti et al.~and as a result, also\nimproves upon the best known bounds for the important special cases of\ncomputing $F_0$ and $F_{\\infty}$. We emphasize that while being quantitatively\nbetter, our scheme is also qualitatively better in the sense that it is simpler\nthan the previously best scheme that uses intricate data structures and\nelaborate subroutines.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:08:17 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ghosh", "Prantar", ""]]}, {"id": "2010.03354", "submitter": "Yixin Cao", "authors": "Yixin Cao", "title": "Recognizing (Unit) Interval Graphs by Zigzag Graph Searches", "comments": "To appear in SOSA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corneil, Olariu, and Stewart [SODA 1998; SIAM Journal on Discrete Mathematics\n2009] presented a recognition algorithm for interval graphs by six graph\nsearches. Li and Wu [Discrete Mathematics \\& Theoretical Computer Science 2014]\nsimplified it to only four. The great simplicity of the latter algorithm is\nhowever eclipsed by the complicated and long proofs. The main purpose of this\npaper is to present a new and significantly short proof for Li and Wu's\nalgorithm, as well as a simpler implementation. We also give a self-contained\nsimpler interpretation of the recognition algorithm of Corneil [Discrete\nApplied Mathematics 2004] for unit interval graphs, based on three sweeps of\ngraph searches. Moreover, we show that two sweeps are already sufficient.\nToward the proofs of the main results, we make several new structural\nobservations that might be of independent interests.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:00:56 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cao", "Yixin", ""]]}, {"id": "2010.03438", "submitter": "Sajjad Ghobadi", "authors": "Ruben Becker, Gianlorenzo D'Angelo, Sajjad Ghobadi, Hugo Gilbert", "title": "Fairness in Influence Maximization through Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence maximization paradigm has been used by researchers in various\nfields in order to study how information spreads in social networks. While\npreviously the attention was mostly on efficiency, more recently fairness\nissues have been taken into account in this scope. In this paper, we propose to\nuse randomization as a mean for achieving fairness. Similar to previous works\nlike Fish et al. (WWW '19) and Tsang et al. (IJCAI '19), we study the maximin\ncriterion for (group) fairness. In contrast to their work however, we model the\nproblem in such a way that, when choosing the seed sets, probabilistic\nstrategies are possible rather than only deterministic ones. We introduce two\ndifferent variants of this probabilistic problem, one that entails\nprobabilistic strategies over nodes (node-based problem) and a second one that\nentails probabilistic strategies over sets of nodes (set-based problem). While\nthe original deterministic problem involving the maximin criterion has been\nshown to be inapproximable, interestingly, we show that both probabilistic\nvariants permit approximation algorithms that achieve a constant multiplicative\nfactor of 1-1/e plus an additive arbitrarily small error that is due to the\nsimulation of the information spread. For an experimental study, we provide\nimplementations of multiplicative-weight routines for both problems and compare\nthe achieved fairness values to existing methods. Maybe non-surprisingly, we\nshow that the ex-ante values of the computed probabilistic strategies are\nsignificantly larger than the (ex-post) fairness values of previous methods.\nThis indicates that studying fairness via randomization is a worthwhile path to\nfollow. Interestingly and maybe more surprisingly, we observe that even the\nex-post fairness values computed by our routines, dominate over the fairness\nachieved by previous methods on most of the instances tested.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:28:40 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 15:25:47 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 12:48:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Becker", "Ruben", ""], ["D'Angelo", "Gianlorenzo", ""], ["Ghobadi", "Sajjad", ""], ["Gilbert", "Hugo", ""]]}, {"id": "2010.03517", "submitter": "Murilo Santos de Lima", "authors": "Steven Chaplick (1), Magn\\'us M. Halld\\'orsson (2), Murilo S. de Lima\n  (3), Tigran Tonoyan (4) ((1) Department of Data Science and Knowledge\n  Engineering, Maastricht University, the Netherlands, (2) ICE-TCS, Department\n  of Computer Science, Reykjavik University, Iceland, (3) K\\'opavogur, Iceland,\n  (4) Computer Science Department, Technion Institute of Technology, Israel)", "title": "Query Minimization under Stochastic Uncertainty", "comments": "Partially supported by Icelandic Research Fund grant 174484-051 and\n  by EPSRC grant EP/S033483/1. A preliminary version of this paper appeared in\n  volume 12118 of LNCS (LATIN 2020), pp. 181--193, 2020. DOI:\n  10.1007/978-3-030-61792-9_15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study problems with stochastic uncertainty information on intervals for\nwhich the precise value can be queried by paying a cost. The goal is to devise\nan adaptive decision tree to find a correct solution to the problem in\nconsideration while minimizing the expected total query cost. We show that, for\nthe sorting problem, such a decision tree can be found in polynomial time. For\nthe problem of finding the data item with minimum value, we have some evidence\nfor hardness. This contradicts intuition, since the minimum problem is easier\nboth in the online setting with adversarial inputs and in the offline\nverification setting. However, the stochastic assumption can be leveraged to\nbeat both deterministic and randomized approximation lower bounds for the\nonline setting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:45:05 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:15:13 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chaplick", "Steven", ""], ["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["de Lima", "Murilo S.", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "2010.03653", "submitter": "Nguyen Ho Ms.", "authors": "Van Long Ho, Nguyen Ho, Torben Bach Pedersen", "title": "Efficient Temporal Pattern Mining in Big Time Series Using Mutual\n  Information -- Full Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large time series are increasingly available from an ever wider range of\nIoT-enabled sensors deployed in different environments. Significant insights\ncan be obtained through mining temporal patterns from these time series. Unlike\ntraditional pattern mining, temporal pattern mining (TPM) adds additional\ntemporal aspect, e.g., the time intervals of the events, into extracted\npatterns, making them more expressive. However, adding this extra temporal\ninformation to the patterns also adds an additional exponential factor to the\nexponential growth of the search space, increases significantly the mining\ncomplexity. This raises an imperative need to have a more efficient and\nscalable method for temporal pattern mining. Existing TPM methods either cannot\nscale to large datasets, or work only on pre-processed temporal events rather\nthan on time series. This paper presents our comprehensive Frequent Temporal\nPattern Mining from Time Series (FTPMfTS) with the following contributions: (1)\nThe end-to-end FTPMfTS process that directly takes time series as input and\nproduces frequent temporal patterns as output. (2) The efficient Hierarchical\nTemporal Pattern Graph Mining (HTPGM) algorithm that uses efficient data\nstructures to enable fast computations of support and confidence, and employs\neffective pruning techniques to achieve significantly faster mining. (3) An\napproximate version of HTPGM which relies on mutual information to prune\nunpromising time series, and thus significantly reduce the search space. (4) An\nextensive experimental evaluation on synthetic and real-world datasets shows\nthat HTPGM outperforms the baselines in runtime and memory consumption, and can\nscale to big datasets. The approximate HTPGM achieves up to 2 orders of\nmagnitude speedup compared to the baselines while having high accuracy compared\nto the exact HTPGM.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:03:35 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 17:06:49 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 17:44:38 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 09:50:12 GMT"}, {"version": "v5", "created": "Thu, 22 Oct 2020 11:24:19 GMT"}, {"version": "v6", "created": "Sun, 2 May 2021 20:04:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ho", "Van Long", ""], ["Ho", "Nguyen", ""], ["Pedersen", "Torben Bach", ""]]}, {"id": "2010.03820", "submitter": "Naoyuki Kamiyama", "authors": "Naoyuki Kamiyama", "title": "A Matroid Generalization of the Super-Stable Matching Problem", "comments": "v2: small mistakes and typos are fixed, and proofs are improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A super-stable matching, which was introduced by Irving, is a solution\nconcept in a variant of the stable matching problem in which the preferences\nmay contain ties. Irving proposed a polynomial-time algorithm for the problem\nof finding a super-stable matching if a super-stable matching exists. In this\npaper, we consider a matroid generalization of a super-stable matching. We call\nour generalization of a super-stable matching a super-stable common independent\nset. This can be considered as a generalization of the matroid generalization\nof a stable matching for strict preferences proposed by Fleiner. We propose a\npolynomial-time algorithm for the problem of finding a super-stable common\nindependent set if a super-stable common independent set exists.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:43:39 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 04:01:31 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kamiyama", "Naoyuki", ""]]}, {"id": "2010.03850", "submitter": "Gordon Hoi", "authors": "Gordon Hoi", "title": "An Improved Exact Algorithm for the Exact Satisfiability Problem", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-64843-5_21", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Exact Satisfiability problem, XSAT, is defined as the problem of finding\na satisfying assignment to a formula $\\varphi$ in CNF such that exactly one\nliteral in each clause is assigned to be \"1\" and the other literals in the same\nclause are set to \"0\". Since it is an important variant of the satisfiability\nproblem, XSAT has also been studied heavily and has seen numerous improvements\nto the development of its exact algorithms over the years.\n  The fastest known exact algorithm to solve XSAT runs in $O(1.1730^n)$ time,\nwhere $n$ is the number of variables in the formula. In this paper, we propose\na faster exact algorithm that solves the problem in $O(1.1674^n)$ time. Like\nmany of the authors working on this problem, we give a DPLL algorithm to solve\nit. The novelty of this paper lies on the design of the nonstandard measure, to\nhelp us to tighten the analysis of the algorithm further.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:08:01 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 10:09:08 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 12:30:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hoi", "Gordon", ""]]}, {"id": "2010.03910", "submitter": "Huan Li", "authors": "Tiantian Liu, Huan Li, Hua Lu, Muhammad Aamir Cheema, and Lidan Shou", "title": "An Experimental Analysis of Indoor Spatial Queries: Modeling, Indexing,\n  and Processing", "comments": "An Experiment and Analysis Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Indoor location-based services (LBS), such as POI search and routing, are\noften built on top of typical indoor spatial queries. To support such queries\nand indoor LBS, multiple techniques including model/indexes and search\nalgorithms have been proposed. In this work, we conduct an extensive\nexperimental study on existing proposals for indoor spatial queries. We survey\nfive model/indexes, compare their algorithmic characteristics, and analyze\ntheir space and time complexities. We also design an in-depth benchmark with\nreal and synthetic datasets, evaluation tasks and performance metrics. Enabled\nby the benchmark, we obtain and report the performance results of all\nmodel/indexes under investigation. By analyzing the results, we summarize the\npros and cons of all techniques and suggest the best choice for typical\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 11:48:35 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Liu", "Tiantian", ""], ["Li", "Huan", ""], ["Lu", "Hua", ""], ["Cheema", "Muhammad Aamir", ""], ["Shou", "Lidan", ""]]}, {"id": "2010.03983", "submitter": "Rajan Udwani", "authors": "Vineet Goyal, Garud Iyengar, Rajan Udwani", "title": "Online Allocation of Reusable Resources via Algorithms Guided by Fluid\n  Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online allocation (matching and assortments) of\nreusable resources where customers arrive sequentially in an adversarial\nfashion and allocated resources are used or rented for a stochastic duration\nthat is drawn independently from known distributions. Focusing on the case of\nlarge inventory, we give an algorithm that is $(1-1/e)$ competitive for general\nusage distributions. At the heart of our result is the notion of a relaxed\nonline algorithm that is only subjected to fluid approximations of the\nstochastic elements in the problem. The output of this algorithm serves as a\nguide for the final algorithm. This leads to a principled approach for\nseamlessly addressing stochastic elements (such as reusability, customer\nchoice, and combinations thereof) in online resource allocation problems, that\nmay be useful more broadly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 13:56:11 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Goyal", "Vineet", ""], ["Iyengar", "Garud", ""], ["Udwani", "Rajan", ""]]}, {"id": "2010.04108", "submitter": "Sebastian Wild", "authors": "Konstantinos Tsakalidis, Sebastian Wild, Viktor Zamaraev", "title": "Succinct Permutation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a succinct, i.e., asymptotically space-optimal, data structure for\npermutation graphs that supports distance, adjacency, neighborhood and\nshortest-path queries in optimal time; a variant of our data structure also\nsupports degree queries in time independent of the neighborhood's size at the\nexpense of an $O(\\log n/\\log \\log n)$-factor overhead in all running times. We\nshow how to generalize our data structure to the class of circular permutation\ngraphs with asymptotically no extra space, while supporting the same queries in\noptimal time. Furthermore, we develop a similar compact data structure for the\nspecial case of bipartite permutation graphs and conjecture that it is succinct\nfor this class. We demonstrate how to execute algorithms directly over our\nsuccinct representations for several combinatorial problems on permutation\ngraphs: Clique, Coloring, Independent Set, Hamiltonian Cycle, All-Pair Shortest\nPaths, and others.\n  Moreover, we initiate the study of semi-local graph representations; a\nconcept that \"interpolates\" between local labeling schemes and standard\n\"centralized\" data structures. We show how to turn some of our data structures\ninto semi-local representations by storing only $O(n)$ bits of additional\nglobal information, beating the lower bound on distance labeling schemes for\npermutation graphs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 16:47:10 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Tsakalidis", "Konstantinos", ""], ["Wild", "Sebastian", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "2010.04157", "submitter": "Sitan Chen", "authors": "Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau", "title": "Online and Distribution-Free Robustness: Regression and Contextual\n  Bandits with Huber Contamination", "comments": "66 pages, 1 figure, v3: refined exposition and improved rates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we revisit two classic high-dimensional online learning\nproblems, namely linear regression and contextual bandits, from the perspective\nof adversarial robustness. Existing works in algorithmic robust statistics make\nstrong distributional assumptions that ensure that the input data is evenly\nspread out or comes from a nice generative model. Is it possible to achieve\nstrong robustness guarantees even without distributional assumptions\naltogether, where the sequence of tasks we are asked to solve is adaptively and\nadversarially chosen?\n  We answer this question in the affirmative for both linear regression and\ncontextual bandits. In fact our algorithms succeed where conventional methods\nfail. In particular we show strong lower bounds against Huber regression and\nmore generally any convex M-estimator. Our approach is based on a novel\nalternating minimization scheme that interleaves ordinary least-squares with a\nsimple convex program that finds the optimal reweighting of the distribution\nunder a spectral constraint. Our results obtain essentially optimal dependence\non the contamination level $\\eta$, reach the optimal breakdown point, and\nnaturally apply to infinite dimensional settings where the feature vectors are\nrepresented implicitly via a kernel map.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:59:05 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 02:12:07 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 22:54:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chen", "Sitan", ""], ["Koehler", "Frederic", ""], ["Moitra", "Ankur", ""], ["Yau", "Morris", ""]]}, {"id": "2010.04281", "submitter": "Conor McMeel", "authors": "Conor McMeel and Yuichi Yoshida", "title": "Sensitivity Analysis of Submodular Function Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the recently introduced idea of worst-case sensitivity for monotone\nsubmodular maximization with cardinality constraint $k$, which captures the\ndegree to which the output argument changes on deletion of an element in the\ninput. We find that for large classes of algorithms that non-trivial\nsensitivity of $o(k)$ is not possible, even with bounded curvature, and that\nthese results also hold in the distributed framework. However, we also show\nthat in the regime $k = \\Omega(n)$ that we can obtain $O(1)$ sensitivity for\nsufficiently low curvature.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:08:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["McMeel", "Conor", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2010.04333", "submitter": "Sankardeep Chakraborty", "authors": "H\\\"useyin Acan, Sankardeep Chakraborty, Seungbum Jo, Kei Nakashima,\n  Kunihiko Sadakane, Srinivasa Rao Satti", "title": "Succinct Navigational Oracles for Families of Intersection Graphs on a\n  Circle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing succinct navigational oracles, i.e.,\nsuccinct data structures supporting basic navigational queries such as degree,\nadjacency, and neighborhood efficiently for intersection graphs on a circle,\nwhich include graph classes such as {\\it circle graphs}, {\\it\n$k$-polygon-circle graphs}, {\\it circle-trapezoid graphs}, {\\it trapezoid\ngraphs}. The degree query reports the number of incident edges to a given\nvertex, the adjacency query asks if there is an edge between two given\nvertices, and the neighborhood query enumerates all the neighbors of a given\nvertex. We first prove a general lower bound for these intersection graph\nclasses and then present a uniform approach that lets us obtain matching lower\nand upper bounds for representing each of these graph classes. More\nspecifically, our lower bound proofs use a unified technique to produce tight\nbounds for all these classes, and this is followed by our data structures which\nare also obtained from a unified representation method to achieve succinctness\nfor each class. In addition, we prove a lower bound of space for representing\n{\\it trapezoid} graphs and give a succinct navigational oracle for this class\nof graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 02:35:58 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Acan", "H\u00fcseyin", ""], ["Chakraborty", "Sankardeep", ""], ["Jo", "Seungbum", ""], ["Nakashima", "Kei", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "2010.04400", "submitter": "Quentin Bramas", "authors": "Quentin Bramas (ICube, UNISTRA, ICUBE-R\\'eseaux), Anissa Lamani\n  (ICube, UNISTRA, ICUBE-R\\'eseaux), S\\'ebastien Tixeuil (SU, NPA)", "title": "Stand Up Indulgent Rendezvous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two mobile oblivious robots that evolve in a continuous Euclidean\nspace. We require the two robots to solve the rendezvous problem (meeting in\nfinite time at the same location, not known beforehand) despite the possibility\nthat one of those robots crashes unpredictably. The rendezvous is stand up\nindulgent in the sense that when a crash occurs, the correct robot must still\nmeet the crashed robot on its last position. We characterize the system\nassumptions that enable problem solvability, and present a series of algorithms\nthat solve the problem for the possible cases.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:12:36 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bramas", "Quentin", "", "ICube, UNISTRA, ICUBE-R\u00e9seaux"], ["Lamani", "Anissa", "", "ICube, UNISTRA, ICUBE-R\u00e9seaux"], ["Tixeuil", "S\u00e9bastien", "", "SU, NPA"]]}, {"id": "2010.04412", "submitter": "Yanhao Wang", "authors": "Yanhao Wang and Francesco Fabbri and Michael Mathioudakis", "title": "Fair and Representative Subset Selection from Data Streams", "comments": "11 pages, 8 figures, to appear in the Web conference 2021 (WWW '21)", "journal-ref": null, "doi": "10.1145/3442381.3449799", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study the problem of extracting a small subset of representative items\nfrom a large data stream. In many data mining and machine learning applications\nsuch as social network analysis and recommender systems, this problem can be\nformulated as maximizing a monotone submodular function subject to a\ncardinality constraint $k$. In this work, we consider the setting where data\nitems in the stream belong to one of several disjoint groups and investigate\nthe optimization problem with an additional \\emph{fairness} constraint that\nlimits selection to a given number of items from each group. We then propose\nefficient algorithms for the fairness-aware variant of the streaming submodular\nmaximization problem. In particular, we first give a $\n(\\frac{1}{2}-\\varepsilon) $-approximation algorithm that requires $\nO(\\frac{1}{\\varepsilon} \\log \\frac{k}{\\varepsilon}) $ passes over the stream\nfor any constant $ \\varepsilon>0 $. Moreover, we give a single-pass streaming\nalgorithm that has the same approximation ratio of $(\\frac{1}{2}-\\varepsilon)$\nwhen unlimited buffer sizes and post-processing time are permitted, and discuss\nhow to adapt it to more practical settings where the buffer sizes are bounded.\nFinally, we demonstrate the efficiency and effectiveness of our proposed\nalgorithms on two real-world applications, namely \\emph{maximum coverage on\nlarge graphs} and \\emph{personalized recommendation}.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:49:13 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 09:04:12 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Wang", "Yanhao", ""], ["Fabbri", "Francesco", ""], ["Mathioudakis", "Michael", ""]]}, {"id": "2010.04527", "submitter": "Philipp Krause", "authors": "Philipp Klaus Krause", "title": "Constant-time connectivity tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present implementations of constant-time algorithms for connectivity tests\nand related problems. Some are implementations of slightly improved variants of\npreviously known algorithms; for other problems we present new algorithms that\nhave substantially better runtime than previously known algorithms (estimates\nof the distance to and tolerant testers for connectivity, 2-edge-connectivity,\n3-edge-connectivity, eulerianity).\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:32:20 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Krause", "Philipp Klaus", ""]]}, {"id": "2010.04643", "submitter": "Hendrik Molter", "authors": "Klaus Heeger, Danny Hermelin, George B. Mertzios, Hendrik Molter, Rolf\n  Niedermeier, Dvir Shabtay", "title": "Equitable Scheduling on a Single Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a natural but seemingly yet unstudied generalization of the\nproblem of scheduling jobs on a single machine so as to minimize the number of\ntardy jobs. Our generalization lies in simultaneously considering several\ninstances of the problem at once. In particular, we have $n$ clients over a\nperiod of $m$ days, where each client has a single job with its own processing\ntime and deadline per day. Our goal is to provide a schedule for each of the\n$m$ days, so that each client is guaranteed to have their job meet its deadline\nin at least $k \\le m$ days. This corresponds to an equitable schedule where\neach client is guaranteed a minimal level of service throughout the period of\n$m$ days. We provide a thorough analysis of the computational complexity of\nthree main variants of this problem, identifying both efficient algorithms and\nworst-case intractability results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:34:39 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:38:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Heeger", "Klaus", ""], ["Hermelin", "Danny", ""], ["Mertzios", "George B.", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Shabtay", "Dvir", ""]]}, {"id": "2010.04752", "submitter": "Russel Villacarlos", "authors": "Russel L. Villacarlos, Jaime M. Samaniego, Arian J. Jacildo, Maria Art\n  Antonette D. Clari\\~no", "title": "A Tale of Two Trees: New Analysis for AVL Tree and Binary Heap", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide new insights and analysis for the two elementary\ntree-based data structures - the AVL tree and binary heap. We presented two\nsimple properties that gives a more direct way of relating the size of an AVL\ntree and the Fibonacci recurrence to establish the AVL tree's logarithmic\nheight. We then give a potential function-based analysis of the bottom-up heap\nconstruction to get a simpler and tight bound for its worst-case running-time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:20:14 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Villacarlos", "Russel L.", ""], ["Samaniego", "Jaime M.", ""], ["Jacildo", "Arian J.", ""], ["Clari\u00f1o", "Maria Art Antonette D.", ""]]}, {"id": "2010.04799", "submitter": "Sergio Cabello", "authors": "\\'Edouard Bonnet and Sergio Cabello", "title": "The Complexity of Mixed-Connectivity", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity in $a$ and $b$ of determining\nwhether a graph~$G$ has a subset of $a$ vertices and $b$ edges whose removal\ndisconnects $G$, or disconnects two prescribed vertices $s, t \\in V(G)$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 20:46:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Cabello", "Sergio", ""]]}, {"id": "2010.04809", "submitter": "Chris Peikert", "authors": "Ethan Mook and Chris Peikert", "title": "Lattice (List) Decoding Near Minkowski's Inequality", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minkowski proved that any $n$-dimensional lattice of unit determinant has a\nnonzero vector of Euclidean norm at most $\\sqrt{n}$; in fact, there are\n$2^{\\Omega(n)}$ such lattice vectors. Lattices whose minimum distances come\nclose to Minkowski's bound provide excellent sphere packings and\nerror-correcting codes in $\\mathbb{R}^{n}$.\n  The focus of this work is a certain family of efficiently constructible\n$n$-dimensional lattices due to Barnes and Sloane, whose minimum distances are\nwithin an $O(\\sqrt{\\log n})$ factor of Minkowski's bound. Our primary\ncontribution is a polynomial-time algorithm that \\emph{list decodes} this\nfamily to distances approaching $1/\\sqrt{2}$ times the minimum distance. The\nmain technique is to decode Reed-Solomon codes under error measured in the\nEuclidean norm, using the Koetter-Vardy \"soft decision\" variant of the\nGuruswami-Sudan list-decoding algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 21:10:40 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Mook", "Ethan", ""], ["Peikert", "Chris", ""]]}, {"id": "2010.05005", "submitter": "Rishikesh Gajjala", "authors": "Shashwat Banchhor, Rishikesh Gajjala, Yogish Sabharwal, Sandeep Sen", "title": "Decode efficient prefix codes", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression is used in a wide variety of tasks, including compression of\ndatabases, large learning models, videos, images, etc. The cost of\ndecompressing (decoding) data can be prohibitive for certain real-time\napplications. In many scenarios, it is acceptable to sacrifice (to some extent)\non compression in the interest of fast decoding. In this work, we introduce and\nstudy a novel problem of finding a prefix tree having the best decode time\nunder the constraint that the code length does not exceed a certain threshold\nfor a natural class of memory access cost functions that use blocking (also\nreferred to as lookup tables), i.e., these decoding schemes access multiple\nprefix tree entries in a single access, using associative memory table\nlook-ups. We present (i) an exact algorithm for this problem that is polynomial\nin the number of characters and the codelength; (ii) a strongly polynomial\npseudo approximation algorithm that achieves the best decode time by relaxing\nthe codelength constraint by a small factor; and (iii) a more efficient version\nof the pseudo approximation algorithm that achieves near optimal decode time by\nrelaxing the codelength constraint by a small factor. All our algorithms are\nbased on dynamic programming and capitalize on an interesting structure of the\noptimal solution. To the best of our knowledge, there is no prior work that\ngives any provable theoretical guarantees for minimizing decode time along with\nthe code length. We also demonstrate the performance benefits of our algorithm\non different types of real-world data sets, namely (i) a deep learning model\n(Mobilenet-V2); (ii) image and (iii) text data. We also implement and evaluate\nthe performance of our algorithms on the GPU.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 14:01:21 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 07:10:25 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Banchhor", "Shashwat", ""], ["Gajjala", "Rishikesh", ""], ["Sabharwal", "Yogish", ""], ["Sen", "Sandeep", ""]]}, {"id": "2010.05080", "submitter": "Nika Haghtalab", "authors": "Maria-Florina Balcan, Nika Haghtalab", "title": "Noise in Classification", "comments": "Chapter 16 of the book Beyond the Worst-Case Analysis of Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter considers the computational and statistical aspects of learning\nlinear thresholds in presence of noise. When there is no noise, several\nalgorithms exist that efficiently learn near-optimal linear thresholds using a\nsmall amount of data. However, even a small amount of adversarial noise makes\nthis problem notoriously hard in the worst-case. We discuss approaches for\ndealing with these negative results by exploiting natural assumptions on the\ndata-generating process.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 19:52:26 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 15:42:05 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Haghtalab", "Nika", ""]]}, {"id": "2010.05127", "submitter": "Chaitanya Swamy", "authors": "Sharat Ibrahimpur, Chaitanya Swamy", "title": "Approximation Algorithms for Stochastic Minimum Norm Combinatorial\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for, and growing interest in, modeling uncertainty in\ndata, we introduce and study {\\em stochastic minimum-norm optimization}. We\nhave an underlying combinatorial optimization problem where the costs involved\nare {\\em random variables} with given distributions; each feasible solution\ninduces a random multidimensional cost vector, and given a certain objective\nfunction, the goal is to find a solution (that does not depend on the\nrealizations of the costs) that minimizes the expected objective value. For\ninstance, in stochastic load balancing, jobs with random processing times need\nto be assigned to machines, and the induced cost vector is the machine-load\nvector. Recently, in the deterministic setting, Chakrabarty and Swamy\n\\cite{ChakrabartyS19a} considered a fairly broad suite of objectives, wherein\nwe seek to minimize the $f$-norm of the cost vector under a given {\\em\narbitrary monotone, symmetric norm} $f$. In stochastic minimum-norm\noptimization, we work with this broad class of objectives, and seek a solution\nthat minimizes the {\\em expected $f$-norm} of the induced cost vector.\n  We give a general framework for devising algorithms for stochastic\nminimum-norm combinatorial optimization, using which we obtain approximation\nalgorithms for the stochastic minimum-norm versions of the load balancing and\nspanning tree problems. Two key technical contributions of this work are: (1) a\nstructural result of independent interest connecting stochastic minimum-norm\noptimization to the simultaneous optimization of a (\\emph{small}) collection of\nexpected $\\mathsf{Top}$-$\\ell$-norms; and (2) showing how to tackle expected\n$\\mathsf{Top}$-$\\ell$-norm minimization by leveraging techniques used to deal\nwith minimizing the expected maximum, circumventing the difficulties posed by\nthe non-separable nature of $\\mathsf{Top}$-$\\ell$ norms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 01:10:40 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 18:20:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Ibrahimpur", "Sharat", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "2010.05186", "submitter": "Vinod Reddy I", "authors": "I. Vinod Reddy", "title": "On Structural Parameterizations of Load Coloring", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and a positive integer $k$, the 2-Load coloring problem is\nto check whether there is a $2$-coloring $f:V(G) \\rightarrow \\{r,b\\}$ of $G$\nsuch that for every $i \\in \\{r,b\\}$, there are at least $k$ edges with both end\nvertices colored $i$. It is known that the problem is NP-complete even on\nspecial classes of graphs like regular graphs. Gutin and Jones (Inf Process\nLett 114:446-449, 2014) showed that the problem is fixed-parameter tractable by\ngiving a kernel with at most $7k$ vertices. Barbero et al. (Algorithmica\n79:211-229, 2017) obtained a kernel with less than $4k$ vertices and $O(k)$\nedges, improving the earlier result.\n  In this paper, we study the parameterized complexity of the problem with\nrespect to structural graph parameters. We show that \\lcp{} cannot be solved in\ntime $f(w)n^{o(w)}$, unless ETH fails and it can be solved in time $n^{O(w)}$,\nwhere $n$ is the size of the input graph, $w$ is the clique-width of the graph\nand $f$ is an arbitrary function of $w$. Next, we consider the parameters\ndistance to cluster graphs, distance to co-cluster graphs and distance to\nthreshold graphs, which are weaker than the parameter clique-width and show\nthat the problem is fixed-parameter tractable (FPT) with respect to these\nparameters. Finally, we show that \\lcp{} is NP-complete even on bipartite\ngraphs and split graphs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 07:53:51 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Reddy", "I. Vinod", ""]]}, {"id": "2010.05450", "submitter": "David Harvey", "authors": "David Harvey", "title": "An exponent one-fifth algorithm for deterministic integer factorisation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hittmeir recently presented a deterministic algorithm that provably computes\nthe prime factorisation of a positive integer $N$ in $N^{2/9+o(1)}$ bit\noperations. Prior to this breakthrough, the best known complexity bound for\nthis problem was $N^{1/4+o(1)}$, a result going back to the 1970s. In this\npaper we push Hittmeir's techniques further, obtaining a rigorous,\ndeterministic factoring algorithm with complexity $N^{1/5+o(1)}$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 04:53:21 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Harvey", "David", ""]]}, {"id": "2010.05589", "submitter": "Nomvelo Sibisi", "authors": "Nomvelo Sibisi", "title": "Growth of Random Trees by Leaf Attachment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the growth of a time-ordered rooted tree by probabilistic attachment\nof new vertices to leaves. We construct a likelihood function of the leaves\nbased on the connectivity of the tree. We take such connectivity to be induced\nby the merging of directed ordered paths from leaves to the root. Combining the\nlikelihood with an assigned prior distribution leads to a posterior leaf\ndistribution from which we sample attachment points for new vertices. We\npresent computational examples of such Bayesian tree growth. Although the\ndiscussion is generic, the initial motivation for the paper is the concept of a\ndistributed ledger, which may be regarded as a time-ordered random tree that\ngrows by probabilistic leaf attachment.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:29:32 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 10:37:39 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 11:32:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sibisi", "Nomvelo", ""]]}, {"id": "2010.05644", "submitter": "Giulia Bernardini", "authors": "Giulia Bernardini and Paola Bonizzoni and Pawe{\\l} Gawrychowski", "title": "Incomplete Directed Perfect Phylogeny in Linear Time", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the evolutionary history of a set of species is a central task\nin computational biology. In real data, it is often the case that some\ninformation is missing: the Incomplete Directed Perfect Phylogeny (IDPP)\nproblem asks, given a collection of species described by a set of binary\ncharacters with some unknown states, to complete the missing states in such a\nway that the result can be explained with a perfect directed phylogeny. Pe'er\net al. proposed a solution that takes $\\tilde{O}(nm)$ time for $n$ species and\n$m$ characters. Their algorithm relies on pre-existing dynamic connectivity\ndata structures: a computational study recently conducted by Fern{\\'a}ndez-Baca\nand Liu showed that, in this context, complex data structures perform worse\nthan simpler ones with worse asymptotic bounds.\n  This gives us the motivation to look into the particular properties of the\ndynamic connectivity problem in this setting, so as to avoid the use of\nsophisticated data structures as a blackbox. Not only are we successful in\ndoing so, and give a much simpler $\\tilde{O}(nm)$-time algorithm for the IDPP\nproblem; our insights into the specific structure of the problem lead to an\nasymptotically faster algorithm, that runs in optimal $O(nm)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:31:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bernardini", "Giulia", ""], ["Bonizzoni", "Paola", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "2010.05652", "submitter": "Soh Kumabe", "authors": "Soh Kumabe", "title": "Interval Query Problem on Cube-free Median Graphs", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the \\emph{interval query problem} on cube-free\nmedian graphs. Let $G$ be a cube-free median graph and $\\mathcal{S}$ be a\ncommutative semigroup. For each vertex $v$ in $G$, we are given an element\n$p(v)$ in $\\mathcal{S}$. For each query, we are given two vertices $u,v$ in $G$\nand asked to calculate the sum of $p(z)$ over all vertices $z$ belonging to a\n$u-v$ shortest path. This is a common generalization of range query problems on\ntrees and grids. In this paper, we provide an algorithm to answer each interval\nquery in $O(\\log^2 n)$ time. The required data structure is constructed in\n$O(n\\log^3 n)$ time and $O(n\\log^2 n)$ space. To obtain our algorithm, we\nintroduce a new technique, named the \\emph{stairs decomposition}, to decompose\nan interval of cube-free median graphs into simpler substructures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:37:51 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumabe", "Soh", ""]]}, {"id": "2010.05729", "submitter": "Junichi Teruyama", "authors": "Yuya Higashikawa, Naoki Katoh, Junichi Teruyama, Koji Watase", "title": "Almost Linear Time Algorithms for Minsum $k$-Sink Problems on Dynamic\n  Flow Path Networks", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the facility location problems on dynamic flow path networks. A\ndynamic flow path network consists of an undirected path with positive edge\nlengths, positive edge capacities, and positive vertex weights. A path can be\nconsidered as a road, an edge length as the distance along the road and a\nvertex weight as the number of people at the site. An edge capacity limits the\nnumber of people that can enter the edge per unit time. In the dynamic flow\nnetwork, given particular points on edges or vertices, called sinks, all the\npeople evacuate from the vertices to the sinks as quickly as possible. The\nproblem is to find the location of sinks on a dynamic flow path network in such\na way that the aggregate evacuation time (i.e., the sum of evacuation times for\nall the people) to sinks is minimized. We consider two models of the problem:\nthe confluent flow model and the non-confluent flow model. In the former model,\nthe way of evacuation is restricted so that all the people at a vertex have to\nevacuate to the same sink, and in the latter model, there is no such\nrestriction. In this paper, for both the models, we develop algorithms which\nrun in almost linear time regardless of the number of sinks. It should be\nstressed that for the confluent flow model, our algorithm improves upon the\nprevious result by Benkoczi et al. [Theoretical Computer Science, 2020], and\none for the non-confluent flow model is the first polynomial time algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:19:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Higashikawa", "Yuya", ""], ["Katoh", "Naoki", ""], ["Teruyama", "Junichi", ""], ["Watase", "Koji", ""]]}, {"id": "2010.05733", "submitter": "Paloma Thome De Lima", "authors": "Petr A. Golovach, Paloma T. Lima and Charis Papadopoulos", "title": "Graph Square Roots of Small Distance from Degree One Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph class $\\mathcal{H}$, the task of the $\\mathcal{H}$-Square Root\nproblem is to decide, whether an input graph $G$ has a square root $H$ from\n$\\mathcal{H}$. We are interested in the parameterized complexity of the problem\nfor classes $\\mathcal{H}$ that are composed by the graphs at vertex deletion\ndistance at most $k$ from graphs of maximum degree at most one, that is, we are\nlooking for a square root $H$ such that there is a modulator $S$ of size $k$\nsuch that $H-S$ is the disjoint union of isolated vertices and disjoint edges.\nWe show that different variants of the problems with constraints on the number\nof isolated vertices and edges in $H-S$ are FPT when parameterized by $k$ by\ndemonstrating algorithms with running time $2^{2^{O(k)}}\\cdot n^{O(1)}$. We\nfurther show that the running time of our algorithms is asymptotically optimal\nand it is unlikely that the double-exponential dependence on $k$ could be\navoided. In particular, we prove that the VC-$k$ Root problem, that asks\nwhether an input graph has a square root with vertex cover of size at most $k$,\ncannot be solved in time $2^{2^{o(k)}}\\cdot n^{O(1)}$ unless Exponential Time\nHypothesis fails. Moreover, we point out that VC-$k$ Root parameterized by $k$\ndoes not admit a subexponential kernel unless $P=NP$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:27:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Golovach", "Petr A.", ""], ["Lima", "Paloma T.", ""], ["Papadopoulos", "Charis", ""]]}, {"id": "2010.05779", "submitter": "Gwena\\\"el Joret", "authors": "Louis Esperet and Gwena\\\"el Joret and Pat Morin", "title": "Sparse universal graphs for planarity", "comments": "v2: added new result about induced-universal graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for every integer $n\\geq 1$ there exists a graph $G_n$ with\n$(1+o(1))n$ vertices and $n^{1 + o(1)}$ edges such that every $n$-vertex planar\ngraph is isomorphic to a subgraph of $G_n$. The best previous bound on the\nnumber of edges was $O(n^{3/2})$, proved by Babai, Chung, Erd\\H{o}s, Graham,\nand Spencer in 1982. We then show that for every integer $n\\geq 1$ there is a\ngraph $U_n$ with $n^{1 + o(1)}$ vertices and edges that contains induced copies\nof every $n$-vertex planar graph. This significantly reduces the number of\nedges in a recent construction of the authors with Dujmovi\\'c, Gavoille, and\nMicek.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 15:15:30 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:08:42 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Esperet", "Louis", ""], ["Joret", "Gwena\u00ebl", ""], ["Morin", "Pat", ""]]}, {"id": "2010.05805", "submitter": "Nithin Varma", "authors": "Ilan Newman, Nithin Varma", "title": "New Sublinear Algorithms and Lower Bounds for LIS Estimation", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the length of the longest increasing subsequence (LIS) in an array\nis a problem of fundamental importance. Despite the significance of the LIS\nestimation problem and the amount of attention it has received, there are\nimportant aspects of the problem that are not yet fully understood. There are\nno better lower bounds for LIS estimation than the obvious bounds implied by\ntesting monotonicity (for adaptive or nonadaptive algorithms). In this paper,\nwe give the first nontrivial lower bound on the complexity of LIS estimation,\nand also provide novel algorithms that complement our lower bound.\n  Specifically, for every constant $\\epsilon \\in (0,1)$, every nonadaptive\nalgorithm that outputs an estimate of the length of the LIS in an array of\nlength $n$ to within an additive error of $\\epsilon \\cdot n$ has to make\n$\\log^{\\Omega(\\log (1/\\epsilon))} n)$ queries. Next, we design nonadaptive LIS\nestimation algorithms whose complexity decreases as the the number of distinct\nvalues, $r$, in the array decreases. We first present a simple algorithm that\nmakes $\\tilde{O}(r/\\epsilon^3)$ queries and approximates the LIS length with an\nadditive error bounded by $\\epsilon n$. We then use it to construct a\nnonadaptive algorithm with query complexity $\\tilde{O}(\\sqrt{r} \\cdot\n\\text{poly}(1/\\lambda))$ that, for an array with LIS length at least $\\lambda\nn$, outputs a multiplicative $\\Omega(\\lambda)$-approximation to the LIS length.\n  Finally, we describe a nonadaptive erasure-resilient tester for sortedness,\nwith query complexity $O(\\log n)$. Our result implies that nonadaptive tolerant\ntesting is strictly harder than nonadaptive erasure-resilient testing for the\nnatural property of monotonicity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:01:42 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 10:05:29 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 14:54:45 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Newman", "Ilan", ""], ["Varma", "Nithin", ""]]}, {"id": "2010.05846", "submitter": "Josh Alman", "authors": "Josh Alman and Virginia Vassilevska Williams", "title": "A Refined Laser Method and Faster Matrix Multiplication", "comments": "29 pages, to appear in the 32nd Annual ACM-SIAM Symposium on Discrete\n  Algorithms (SODA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of matrix multiplication is measured in terms of $\\omega$, the\nsmallest real number such that two $n\\times n$ matrices can be multiplied using\n$O(n^{\\omega+\\epsilon})$ field operations for all $\\epsilon>0$; the best bound\nuntil now is $\\omega<2.37287$ [Le Gall'14]. All bounds on $\\omega$ since 1986\nhave been obtained using the so-called laser method, a way to lower-bound the\n`value' of a tensor in designing matrix multiplication algorithms. The main\nresult of this paper is a refinement of the laser method that improves the\nresulting value bound for most sufficiently large tensors. Thus, even before\ncomputing any specific values, it is clear that we achieve an improved bound on\n$\\omega$, and we indeed obtain the best bound on $\\omega$ to date: $$\\omega <\n2.37286.$$ The improvement is of the same magnitude as the improvement that [Le\nGall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement\nto the laser method is quite general, and we believe it will have further\napplications in arithmetic complexity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:51:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Alman", "Josh", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "2010.05984", "submitter": "Vijay Vazirani", "authors": "Vijay V. Vazirani", "title": "An Extension of the Birkhoff-von Neumann Theorem to Non-Bipartite Graphs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that a fractional perfect matching in a non-bipartite graph can be\nwritten, in polynomial time, as a convex combination of perfect matchings. This\nextends the Birkhoff-von Neumann Theorem from bipartite to non-bipartite\ngraphs.\n  The algorithm of Birkhoff and von Neumann is greedy; it starts with the given\nfractional perfect matching and successively \"removes\" from it perfect\nmatchings, with appropriate coefficients. This fails in non-bipartite graphs --\nremoving perfect matchings arbitrarily can lead to a graph that is non-empty\nbut has no perfect matchings. Using odd cuts appropriately saves the day.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:20:46 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 03:53:57 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Vazirani", "Vijay V.", ""]]}, {"id": "2010.05998", "submitter": "Lior Gishboliner", "authors": "Lior Gishboliner, Yevgeny Levanzov, Asaf Shapira", "title": "Counting Subgraphs in Degenerate Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of counting the number of copies of a fixed graph $H$\nwithin an input graph $G$. This is one of the most well-studied algorithmic\ngraph problems, with many theoretical and practical applications. We focus on\nsolving this problem when the input $G$ has bounded degeneracy. This is a rich\nfamily of graphs, containing all graphs without a fixed minor (e.g. planar\ngraphs), as well as graphs generated by various random processes (e.g.\npreferential attachment graphs). We say that $H$ is easy if there is a\nlinear-time algorithm for counting the number of copies of $H$ in an input $G$\nof bounded degeneracy. A seminal result of Chiba and Nishizeki from '85 states\nthat every $H$ on at most 4 vertices is easy. Bera, Pashanasangi, and Seshadhri\nrecently extended this to all $H$ on 5 vertices, and further proved that for\nevery $k > 5$ there is a $k$-vertex $H$ which is not easy. They left open the\nnatural problem of characterizing all easy graphs $H$.\n  Bressan has recently introduced a framework for counting subgraphs in\ndegenerate graphs, from which one can extract a sufficient condition for a\ngraph $H$ to be easy. Here we show that this sufficient condition is also\nnecessary, thus fully answering the Bera--Pashanasangi--Seshadhri problem. We\nfurther resolve two closely related problems; namely characterizing the graphs\nthat are easy with respect to counting induced copies, and with respect to\ncounting homomorphisms. Our proofs rely on several novel approaches for proving\nhardness results in the context of subgraph-counting.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:53:26 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gishboliner", "Lior", ""], ["Levanzov", "Yevgeny", ""], ["Shapira", "Asaf", ""]]}, {"id": "2010.06037", "submitter": "Mart\\'in Mu\\~noz", "authors": "Mart\\'in Mu\\~noz, Cristian Riveros", "title": "Constant-delay enumeration algorithms for document spanners over nested\n  documents", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the most relevant document schemas used online, such as XML and JSON,\nhave a nested format. In recent years, the task of extracting data from large\nnested documents has become especially relevant. We model queries of this kind\nas Visibly Pushdown Transducers (VPT), a structure that extends visibly\npushdown automata with outputs. Since processing a string through a VPT can\ngenerate a huge number of outputs, we are interested in the task of enumerating\nthem one after another as efficiently as possible. This paper describes an\nalgorithm that enumerates these elements with output-linear delay after\npreprocessing the string in a single pass. We show applications of this result\non recursive document spanners over nested documents and show how our algorithm\ncan be adapted to enumerate the outputs in this context.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:26:31 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mu\u00f1oz", "Mart\u00edn", ""], ["Riveros", "Cristian", ""]]}, {"id": "2010.06053", "submitter": "Zhao Song", "authors": "Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora", "title": "TextHide: Tackling Data Privacy in Language Understanding Tasks", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  An unsolved challenge in distributed or federated learning is to effectively\nmitigate privacy risks without slowing down training or reducing accuracy. In\nthis paper, we propose TextHide aiming at addressing this challenge for natural\nlanguage understanding tasks. It requires all participants to add a simple\nencryption step to prevent an eavesdropping attacker from recovering private\ntext data. Such an encryption step is efficient and only affects the task\nperformance slightly. In addition, TextHide fits well with the popular\nframework of fine-tuning pre-trained language models (e.g., BERT) for any\nsentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and\nour experiments show that TextHide can effectively defend attacks on shared\ngradients or representations and the averaged accuracy reduction is only\n$1.9\\%$. We also present an analysis of the security of TextHide using a\nconjecture about the computational intractability of a mathematical problem.\n  Our code is available at https://github.com/Hazelsuko07/TextHide\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 22:22:15 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Huang", "Yangsibo", ""], ["Song", "Zhao", ""], ["Chen", "Danqi", ""], ["Li", "Kai", ""], ["Arora", "Sanjeev", ""]]}, {"id": "2010.06562", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Ziteng Sun, and Himanshu Tyagi", "title": "Unified lower bounds for interactive high-dimensional estimation under\n  information constraints", "comments": "Significant improvements: handle sparse parameter estimation,\n  simplify and generalize arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of distributed parameter estimation using interactive\nprotocols subject to local information constraints such as bandwidth\nlimitations, local differential privacy, and restricted measurements. We\nprovide a unified framework enabling us to derive a variety of (tight) minimax\nlower bounds for different parametric families of distributions, both\ncontinuous and discrete, under any $\\ell_p$ loss. Our lower bound framework is\nversatile and yields \"plug-and-play\" bounds that are widely applicable to a\nlarge range of estimation problems. In particular, our approach recovers bounds\nobtained using data processing inequalities and Cram\\'er--Rao bounds, two other\nalternative approaches for proving lower bounds in our setting of interest.\nFurther, for the families considered, we complement our lower bounds with\nmatching upper bounds.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:25:19 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:04:18 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 18:49:21 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 13:54:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2010.06563", "submitter": "Alexander Wein", "authors": "Alexander S. Wein", "title": "Optimal Low-Degree Hardness of Maximum Independent Set", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithmic task of finding a large independent set in a sparse\nErd\\H{o}s-R\\'{e}nyi random graph with $n$ vertices and average degree $d$. The\nmaximum independent set is known to have size $(2 \\log d / d)n$ in the double\nlimit $n \\to \\infty$ followed by $d \\to \\infty$, but the best known\npolynomial-time algorithms can only find an independent set of half-optimal\nsize $(\\log d / d)n$. We show that the class of low-degree polynomial\nalgorithms can find independent sets of half-optimal size but no larger,\nimproving upon a result of Gamarnik, Jagannath, and the author. This\ngeneralizes earlier work by Rahman and Vir\\'ag, which proved the analogous\nresult for the weaker class of local algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:26:09 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:44:49 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Wein", "Alexander S.", ""]]}, {"id": "2010.06980", "submitter": "Jan Konecny", "authors": "Radek Janostik, Jan Konecny, Petr Kraj\\v{c}a", "title": "LCM from FCA Point of View: A CbO-style Algorithm with Speed-up Features", "comments": "full version of a conference paper to be published in IJAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LCM is an algorithm for enumeration of frequent closed itemsets in\ntransaction databases. It is well known that when we ignore the required\nfrequency, the closed itemsets are exactly intents of formal concepts in Formal\nConcept Analysis (FCA). We describe LCM in terms of FCA and show that LCM is\nbasically the Close-by-One algorithm with multiple speed-up features for\nprocessing sparse data. We analyze the speed-up features and compare them with\nthose of similar FCA algorithms, like FCbO and algorithms from the In-Close\nfamily.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:52:36 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 07:05:23 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Janostik", "Radek", ""], ["Konecny", "Jan", ""], ["Kraj\u010da", "Petr", ""]]}, {"id": "2010.06986", "submitter": "Sruthi Gorantla", "authors": "Sruthi Gorantla, Amit Deshpande, Anand Louis", "title": "On the Problem of Underranking in Group-Fair Ranking", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search and recommendation systems, such as search engines, recruiting tools,\nonline marketplaces, news, and social media, output ranked lists of content,\nproducts, and sometimes, people. Credit ratings, standardized tests, risk\nassessments output only a score, but are also used implicitly for ranking. Bias\nin such ranking systems, especially among the top ranks, can worsen social and\neconomic inequalities, polarize opinions, and reinforce stereotypes. On the\nother hand, a bias correction for minority groups can cause more harm if\nperceived as favoring group-fair outcomes over meritocracy. In this paper, we\nformulate the problem of underranking in group-fair rankings, which was not\naddressed in previous work. Most group-fair ranking algorithms post-process a\ngiven ranking and output a group-fair ranking. We define underranking based on\nhow close the group-fair rank of each item is to its original rank, and prove a\nlower bound on the trade-off achievable for simultaneous underranking and group\nfairness in ranking. We give a fair ranking algorithm that takes any given\nranking and outputs another ranking with simultaneous underranking and group\nfairness guarantees comparable to the lower bound we prove. Our algorithm works\nwith group fairness constraints for any number of groups. Our experimental\nresults confirm the theoretical trade-off between underranking and group\nfairness, and also show that our algorithm achieves the best of both when\ncompared to the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 14:56:10 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 17:20:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Gorantla", "Sruthi", ""], ["Deshpande", "Amit", ""], ["Louis", "Anand", ""]]}, {"id": "2010.07076", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro", "title": "Contextual Pattern Matching", "comments": "Improvements and corrections over my SPIRE 2020 paper with the same\n  title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on indexing repetitive string collections has focused on the\nsame search problems used for regular string collections, though they can make\nlittle sense in this scenario. For example, the basic pattern matching query\n\"list all the positions where pattern $P$ appears\" can produce huge outputs\nwhen $P$ appears in an area shared by many documents. All those occurrences are\nessentially the same.\n  In this paper we propose a new query that can be more appropriate in these\ncollections, which we call {\\em contextual pattern matching}. The basic query\nof this type gives, in addition to $P$, a context length $\\ell$, and asks to\nreport the occurrences of all {\\em distinct} strings $XPY$, with\n$|X|=|Y|=\\ell$.\n  While this query is easily solved in optimal time and linear space, we focus\non using space related to the repetitiveness of the text collection and present\nthe first solution of this kind. Letting $\\ovr$ be the maximum of the number of\nruns in the BWT of the text $T[1..n]$ and of its reverse, our structure uses\n$O(\\ovr\\log(n/\\ovr))$ space and finds the $c$ contextual occurrences $XPY$ of\n$(P,\\ell)$ in time $O(|P| + c \\log n)$. We give other space/time tradeoffs as\nwell, for compressed and uncompressed indexes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:25:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Navarro", "Gonzalo", ""]]}, {"id": "2010.07119", "submitter": "Leah Epstein", "authors": "J\\'anos Balogh and Leah Epstein and Asaf Levin", "title": "More on ordered open end bin packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Ordered Open End Bin Packing problem. Items of sizes in\n$(0,1]$ are presented one by one, to be assigned to bins in this order. An item\ncan be assigned to any bin for which the current total size strictly below $1$.\nThis means also that the bin can be overloaded by its last packed item. We\nimprove lower and upper bounds on the asymptotic competitive ratio in the\nonline case. Specifically, we design the first algorithm whose asymptotic\ncompetitive ratio is strictly below $2$ and it is close to the lower bound.\nThis is in contrast to the best possible absolute approximation ratio, which is\nequal to $2$. We also study the offline problem where the sequence of items is\nknown in advance, while items are still assigned to bins based on their order\nin the sequence. For this scenario we design an asymptotic polynomial time\napproximation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 14:18:15 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Balogh", "J\u00e1nos", ""], ["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "2010.07211", "submitter": "Paul Brown", "authors": "Paul Brown and Trevor Fenner", "title": "Fast Generation of Unlabelled Free Trees using Weight Sequences", "comments": "21 pages, 1 table and 8 figures", "journal-ref": null, "doi": "10.7155/jgaa.00556", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new representation for ordered trees, the\nweight sequence representation. We then use this to construct new\nrepresentations for both rooted trees and free trees, namely the canonical\nweight sequence representation. We construct algorithms for generating the\nweight sequence representations for all rooted and free trees of order n, and\nthen add a number of modifications to improve the efficiency of the algorithms.\nPython implementations of the algorithms incorporate further improvements by\nusing generators to avoid having to store the long lists of trees returned by\nthe recursive calls, as well as caching the lists for rooted trees of small\norder, thereby eliminating many of the recursive calls. We further show how the\nalgorithm can be modifed to generate adjacency list and adjacency matrix\nrepresentations for free trees. We compared the run-times of our Python\nimplementation for generating free trees with the Python implementation of the\nwell-known WROM algorithm taken from NetworkX. The implementation of our\nalgorithm is over four times as fast as the implementation of the WROM\nalgorithm. The run-times for generating adjacency lists and matrices are\nsomewhat longer than those for weight sequences, but are still over three times\nas fast as the corresponding implementations of the WROM algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:24:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 15:51:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Brown", "Paul", ""], ["Fenner", "Trevor", ""]]}, {"id": "2010.07346", "submitter": "Sahil Singla", "authors": "Thomas Kesselheim and Sahil Singla", "title": "Online Learning with Vector Costs and Bandits with Knapsacks", "comments": "Preliminary version appeared in COLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce online learning with vector costs (\\OLVCp) where in each time\nstep $t \\in \\{1,\\ldots, T\\}$, we need to play an action $i \\in \\{1,\\ldots,n\\}$\nthat incurs an unknown vector cost in $[0,1]^{d}$. The goal of the online\nalgorithm is to minimize the $\\ell_p$ norm of the sum of its cost vectors. This\ncaptures the classical online learning setting for $d=1$, and is interesting\nfor general $d$ because of applications like online scheduling where we want to\nbalance the load between different machines (dimensions).\n  We study \\OLVCp in both stochastic and adversarial arrival settings, and give\na general procedure to reduce the problem from $d$ dimensions to a single\ndimension. This allows us to use classical online learning algorithms in both\nfull and bandit feedback models to obtain (near) optimal results. In\nparticular, we obtain a single algorithm (up to the choice of learning rate)\nthat gives sublinear regret for stochastic arrivals and a tight $O(\\min\\{p,\n\\log d\\})$ competitive ratio for adversarial arrivals.\n  The \\OLVCp problem also occurs as a natural subproblem when trying to solve\nthe popular Bandits with Knapsacks (\\BwK) problem. This connection allows us to\nuse our \\OLVCp techniques to obtain (near) optimal results for \\BwK in both\nstochastic and adversarial settings. In particular, we obtain a tight $O(\\log d\n\\cdot \\log T)$ competitive ratio algorithm for adversarial \\BwK, which improves\nover the $O(d \\cdot \\log T)$ competitive ratio algorithm of Immorlica et al.\n[FOCS'19].\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:28:41 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Kesselheim", "Thomas", ""], ["Singla", "Sahil", ""]]}, {"id": "2010.07431", "submitter": "Slobodan Mitrovi\\'c", "authors": "Marwa El Halabi, Slobodan Mitrovi\\'c, Ashkan Norouzi-Fard, Jakab\n  Tardos, Jakub Tarnawski", "title": "Fairness in Streaming Submodular Maximization: Algorithms and Hardness", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization has become established as the method of choice for\nthe task of selecting representative and diverse summaries of data. However, if\ndatapoints have sensitive attributes such as gender or age, such machine\nlearning algorithms, left unchecked, are known to exhibit bias: under- or\nover-representation of particular groups. This has made the design of fair\nmachine learning algorithms increasingly important. In this work we address the\nquestion: Is it possible to create fair summaries for massive datasets? To this\nend, we develop the first streaming approximation algorithms for submodular\nmaximization under fairness constraints, for both monotone and non-monotone\nfunctions. We validate our findings empirically on exemplar-based clustering,\nmovie recommendation, DPP-based summarization, and maximum coverage in social\nnetworks, showing that fairness constraints do not significantly impact\nutility.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 22:57:07 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 18:05:32 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Halabi", "Marwa El", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Norouzi-Fard", "Ashkan", ""], ["Tardos", "Jakab", ""], ["Tarnawski", "Jakub", ""]]}, {"id": "2010.07633", "submitter": "Ali Aouad", "authors": "Ali Aouad and Danny Segev", "title": "An Approximate Dynamic Programming Approach to The Incremental Knapsack\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the incremental knapsack problem, where one wishes to sequentially\npack items into a knapsack whose capacity expands over a finite planning\nhorizon, with the objective of maximizing time-averaged profits. While various\napproximation algorithms were developed under mitigating structural\nassumptions, obtaining non-trivial performance guarantees for this problem in\nits utmost generality has remained an open question thus far. In this paper, we\ndevise a polynomial-time approximation scheme for general instances of the\nincremental knapsack problem, which is the strongest guarantee possible given\nexisting hardness results. In contrast to earlier work, our algorithmic\napproach exploits an approximate dynamic programming formulation. Starting with\na simple exponentially sized dynamic program, we prove that an appropriate\ncomposition of state pruning ideas yields a polynomially sized state space with\nnegligible loss of optimality. The analysis of this formulation synthesizes\nvarious techniques, including new problem decompositions, parsimonious counting\narguments, and efficient rounding methods, that may be of broader interest.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 10:06:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Aouad", "Ali", ""], ["Segev", "Danny", ""]]}, {"id": "2010.07794", "submitter": "Michel de Rougemont", "authors": "Claire Mathieu and Michel de Rougemont", "title": "Large Very Dense Subgraphs in a Stream of Edges", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection and the reconstruction of a large very dense subgraph\nin a social graph with $n$ nodes and $m$ edges given as a stream of edges, when\nthe graph follows a power law degree distribution, in the regime when $m=O(n.\n\\log n)$. A subgraph $S$ is very dense if it has $\\Omega(|S|^2)$ edges. We\nuniformly sample the edges with a Reservoir of size $k=O(\\sqrt{n}.\\log n)$. Our\ndetection algorithm checks whether the Reservoir has a giant component. We show\nthat if the graph contains a very dense subgraph of size $\\Omega(\\sqrt{n})$,\nthen the detection algorithm is almost surely correct. On the other hand, a\nrandom graph that follows a power law degree distribution almost surely has no\nlarge very dense subgraph, and the detection algorithm is almost surely\ncorrect. We define a new model of random graphs which follow a power law degree\ndistribution and have large very dense subgraphs. We then show that on this\nclass of random graphs we can reconstruct a good approximation of the very\ndense subgraph with high probability. We generalize these results to dynamic\ngraphs defined by sliding windows in a stream of edges.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:36:30 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mathieu", "Claire", ""], ["de Rougemont", "Michel", ""]]}, {"id": "2010.07799", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen", "title": "Adaptive and Universal Algorithms for Variational Inequalities with\n  Optimal Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new adaptive algorithms for variational inequalities with monotone\noperators, which capture many problems of interest, notably convex optimization\nand convex-concave saddle point problems. Our algorithms automatically adapt to\nunknown problem parameters such as the smoothness and the norm of the operator,\nand the variance of the stochastic evaluation oracle. We show that our\nalgorithms are universal and simultaneously achieve the optimal convergence\nrates in the non-smooth, smooth, and stochastic settings. The convergence\nguarantees of our algorithms improve over existing adaptive methods by a\n$\\Omega(\\sqrt{\\ln T})$ factor, matching the optimal non-adaptive algorithms.\nAdditionally, prior works require that the optimization domain is bounded. In\nthis work, we remove this restriction and give algorithms for unbounded domains\nthat are adaptive and universal. Our general proof techniques can be used for\nmany variants of the algorithm using one or two operator evaluations per\niteration. The classical methods based on the ExtraGradient/MirrorProx\nalgorithm require two operator evaluations per iteration, which is the dominant\nfactor in the running time in many settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:44:26 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 15:59:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "2010.07920", "submitter": "Stefan Schmid", "authors": "Janardhan Kulkarni, Stefan Schmid and Pawe{\\l} Schmidt", "title": "Scheduling Opportunistic Links in Two-Tiered Reconfigurable Datacenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfigurable optical topologies are emerging as a promising technology to\nimprove the efficiency of datacenter networks. This paper considers the problem\nof scheduling opportunistic links in such reconfigurable datacenters. We study\nthe online setting and aim to minimize flow completion times. The problem is a\ntwo-tier generalization of classic switch scheduling problems. We present a\nstable-matching algorithm which is $2\\cdot (2/\\varepsilon+1)$-competitive\nagainst an optimal offline algorithm, in a resource augmentation model: the\nonline algorithm runs $2+\\varepsilon$ times faster. Our algorithm and result\nare fairly general and allow for different link delays and also apply to hybrid\ntopologies which combine fixed and reconfigurable links. Our analysis is based\non LP relaxation and dual fitting.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:52:10 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kulkarni", "Janardhan", ""], ["Schmid", "Stefan", ""], ["Schmidt", "Pawe\u0142", ""]]}, {"id": "2010.07960", "submitter": "Joe Sawada", "authors": "Evan Sala, Joe Sawada, Abbas Alhakim", "title": "Efficient constructions of the Prefer-same and Prefer-opposite de Bruijn\n  sequences", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy Prefer-same de Bruijn sequence construction was first presented by\nEldert et al.[AIEE Transactions 77 (1958)]. As a greedy algorithm, it has one\nmajor downside: it requires an exponential amount of space to store the length\n$2^n$ de Bruijn sequence. Though de Bruijn sequences have been heavily studied\nover the last 60 years, finding an efficient construction for the Prefer-same\nde Bruijn sequence has remained a tantalizing open problem. In this paper, we\nunveil the underlying structure of the Prefer-same de Bruijn sequence and solve\nthe open problem by presenting an efficient algorithm to construct it using\n$O(n)$ time per bit and only $O(n)$ space. Following a similar approach, we\nalso present an efficient algorithm to construct the Prefer-opposite de Bruijn\nsequence.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 18:05:52 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Sala", "Evan", ""], ["Sawada", "Joe", ""], ["Alhakim", "Abbas", ""]]}, {"id": "2010.07990", "submitter": "Adrian de Wynter", "authors": "Adrian de Wynter", "title": "An Algorithm for Learning Smaller Representations of Models With Scarce\n  Data", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a greedy algorithm for solving binary classification problems in\nsituations where the dataset is either too small or not fully representative of\nthe problem being solved, and obtaining more data is not possible. This\nalgorithm is of particular interest when training small models that have\ntrouble generalizing. It relies on a trained model with loose accuracy\nconstraints, an iterative hyperparameter pruning procedure, and a function used\nto generate new data. Analysis on correctness and runtime complexity under\nideal conditions and an extension to deep neural networks is provided. In the\nformer case we obtain an asymptotic bound of\n$O\\left(|\\Theta^2|\\left(\\log{|\\Theta|} + |\\theta^2| + T_f\\left(|\nD|\\right)\\right) + \\bar{S}|\\Theta||{E}|\\right)$, where $|{\\Theta}|$ is the\ncardinality of the set of hyperparameters $\\theta$ to be searched; $|{E}|$ and\n$|{D}|$ are the sizes of the evaluation and training datasets, respectively;\n$\\bar{S}$ and $\\bar{f}$ are the inference times for the trained model and the\ncandidate model; and $T_f({|{D}|})$ is a polynomial on $|{D}|$ and $\\bar{f}$.\nUnder these conditions, this algorithm returns a solution that is $1 \\leq r\n\\leq 2(1 - {2^{-|{\\Theta}|}})$ times better than simply enumerating and\ntraining with any $\\theta \\in \\Theta$. As part of our analysis of the\ngenerating function we also prove that, under certain assumptions, if an open\ncover of $D$ has the same homology as the manifold where the support of the\nunderlying probability distribution lies, then $D$ is learnable, and viceversa.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:17:51 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["de Wynter", "Adrian", ""]]}, {"id": "2010.08042", "submitter": "Alejandro Grez", "authors": "Pierre Bourhis, Alejandro Grez, Louis Jachiet and Cristian Riveros", "title": "Ranked enumeration of MSO logic on words", "comments": "29 pages (with appendix), 2 figures, submitted to the ICDT21\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, enumeration algorithms with bounded delay have attracted a\nlot of attention for several data management tasks. Given a query and the data,\nthe task is to preprocess the data and then enumerate all the answers to the\nquery one by one and without repetitions. This enumeration scheme is typically\nuseful when the solutions are treated on the fly or when we want to stop the\nenumeration once the pertinent solutions have been found. However, with the\ncurrent schemes, there is no restriction on the order how the solutions are\ngiven and this order usually depends on the techniques used and not on the\nrelevance for the user.\n  In this paper we study the enumeration of monadic second order logic (MSO)\nover words when the solutions are ranked. We present a framework based on MSO\ncost functions that allows to express MSO formulae on words with a cost\nassociated with each solution. We then demonstrate the generality of our\nframework which subsumes, for instance, document spanners and regular complex\nevent processing queries and adds ranking to them. The main technical result of\nthe paper is an algorithm for enumerating all the solutions of formulae in\nincreasing order of cost efficiently, namely, with a linear preprocessing phase\nand logarithmic delay between solutions. The novelty of this algorithm is based\non using functional data structures, in particular, by extending functional\nBrodal queues to suit with the ranked enumeration of MSO on words.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:12:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bourhis", "Pierre", ""], ["Grez", "Alejandro", ""], ["Jachiet", "Louis", ""], ["Riveros", "Cristian", ""]]}, {"id": "2010.08083", "submitter": "Noujan Pashanasangi", "authors": "Suman K. Bera, Noujan Pashanasangi, C. Seshadhri", "title": "Near-Linear Time Homomorphism Counting in Bounded Degeneracy Graphs: The\n  Barrier of Long Induced Cycles", "comments": "To be published in Symposium on Discrete Algorithms (SODA) 2021 Added\n  conclusion section in the new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting homomorphisms of a constant sized pattern graph $H$ in an input\ngraph $G$ is a fundamental computational problem. There is a rich history of\nstudying the complexity of this problem, under various constraints on the input\n$G$ and the pattern $H$. Given the significance of this problem and the large\nsizes of modern inputs, we investigate when near-linear time algorithms are\npossible. We focus on the case when the input graph has bounded degeneracy, a\ncommonly studied and practically relevant class for homomorphism counting. It\nis known from previous work that for certain classes of $H$, $H$-homomorphisms\ncan be counted exactly in near-linear time in bounded degeneracy graphs. Can we\nprecisely characterize the patterns $H$ for which near-linear time algorithms\nare possible?\n  We completely resolve this problem, discovering a clean dichotomy using\nfine-grained complexity. Let $m$ denote the number of edges in $G$. We prove\nthe following: if the largest induced cycle in $H$ has length at most $5$, then\nthere is an $O(m\\log m)$ algorithm for counting $H$-homomorphisms in bounded\ndegeneracy graphs. If the largest induced cycle in $H$ has length at least $6$,\nthen (assuming standard fine-grained complexity conjectures) there is a\nconstant $\\gamma > 0$, such that there is no $o(m^{1+\\gamma})$ time algorithm\nfor counting $H$-homomorphisms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 00:59:01 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 02:57:37 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bera", "Suman K.", ""], ["Pashanasangi", "Noujan", ""], ["Seshadhri", "C.", ""]]}, {"id": "2010.08142", "submitter": "Marek Adamczyk", "authors": "Marek Adamczyk, Brian Brubach, Fabrizio Grandoni, Karthik A.\n  Sankararaman, Aravind Srinivasan and Pan Xu", "title": "Improved Approximation Algorithms for Stochastic-Matching Problems", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.01439", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Stochastic Matching problem, which is motivated by\napplications in kidney exchange and online dating. In this problem, we are\ngiven an undirected graph. Each edge is assigned a known, independent\nprobability of existence and a positive weight (or profit). We must probe an\nedge to discover whether or not it exists. Each node is assigned a positive\ninteger called a timeout (or a patience). On this random graph we are executing\na process, which probes the edges one-by-one and gradually constructs a\nmatching. The process is constrained in two ways. First, if a probed edge\nexists, it must be added irrevocably to the matching (the query-commit model).\nSecond, the timeout of a node $v$ upper-bounds the number of edges incident to\n$v$ that can be probed. The goal is to maximize the expected weight of the\nconstructed matching.\n  For this problem, Bansal et al. (Algorithmica 2012) provided a\n$0.33$-approximation algorithm for bipartite graphs and a $0.25$-approximation\nfor general graphs. We improve the approximation factors to $0.39$ and $0.269$,\nrespectively.\n  The main technical ingredient in our result is a novel way of probing edges\naccording to a not-uniformly-random permutation. Patching this method with an\nalgorithm that works best for large-probability edges (plus additional ideas)\nleads to our improved approximation factors.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:22:45 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Adamczyk", "Marek", ""], ["Brubach", "Brian", ""], ["Grandoni", "Fabrizio", ""], ["Sankararaman", "Karthik A.", ""], ["Srinivasan", "Aravind", ""], ["Xu", "Pan", ""]]}, {"id": "2010.08288", "submitter": "Pierre Ohlmann", "authors": "Marcin Jurdzi\\'nski, R\\'emi Morvan, Pierre Ohlmann and K. S.\n  Thejaswini", "title": "A symmetric attractor-decomposition lifting algorithm for parity games", "comments": "30 pages, including 10 pages of appendix and 5 figures. Submitted to\n  FoSSaCS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress-measure lifting algorithms for solving parity games have the best\nworst-case asymptotic runtime, but are limited by their asymmetric nature, and\nknown from the work of Czerwi\\'nski et al. (2018) to be subject to a matching\nquasi-polynomial lower bound inherited from the combinatorics of universal\ntrees. Parys (2019) has developed an ingenious quasi-polynomial McNaughton-\nZielonka-style algorithm, and Lehtinen et al. (2019) have improved its\nworst-case runtime. Jurdzi\\'nski and Morvan (2020) have recently brought\nforward a generic attractor-based algorithm, formalizing a second class of\nquasi-polynomial solutions to solving parity games, which have runtime\nquadratic in the size of universal trees. First, we adapt the framework of\niterative lifting algorithms to computing attractor-based strategies. Second,\nwe design a symmetric lifting algorithm in this setting, in which two lifting\niterations, one for each player, accelerate each other in a recursive fashion.\nThe symmetric algorithm performs at least as well as progress-measure liftings\nin the worst-case, whilst bypassing their inherent asymmetric limitation.\nThirdly, we argue that the behaviour of the generic attractor-based algorithm\nof Jurdzinski and Morvan (2020) can be reproduced by a specific deceleration of\nour symmetric lifting algorithm, in which some of the information collected by\nthe algorithm is repeatedly discarded. This yields a novel interpretation of\nMcNaughton-Zielonka-style algorithms as progress-measure lifting iterations\n(with deliberate set-backs), further strengthening the ties between all known\nquasi-polynomial algorithms to date.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:27:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Jurdzi\u0144ski", "Marcin", ""], ["Morvan", "R\u00e9mi", ""], ["Ohlmann", "Pierre", ""], ["Thejaswini", "K. S.", ""]]}, {"id": "2010.08423", "submitter": "Suhas Thejaswi", "authors": "Suhas Thejaswi, Juho Lauri, Aristides Gionis", "title": "Restless reachability problems in temporal graphs", "comments": "The paper is updated with more illustrations for improving\n  readability and directed towards broader audience (non-expert group)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study a family of reachability problems under waiting-time restrictions in\ntemporal and vertex-colored temporal graphs. Given a temporal graph and a set\nof source vertices, we find the set of vertices that are reachable from a\nsource via a time-respecting path, where the difference in timestamps between\nconsecutive edges is at most a resting time. Given a vertex-colored temporal\ngraph and a multiset query of colors, we find the set of vertices reachable\nfrom a source via a time-respecting path such that the vertex colors of the\npath agree with the multiset query and the difference in timestamps between\nconsecutive edges is at most a resting time. These kind of problems have\nseveral applications in understanding the spread of a disease in a network,\ntracing contacts in epidemic outbreaks, finding signaling pathways in the brain\nnetwork, and recommending tours for tourists.\n  We present an algebraic algorithmic framework based on constrained\nmultilinear sieving for solving the restless reachability problems we propose.\nIn particular, parameterized by the length of a path $k$ sought, we show the\nproblems can be solved in $O(2^k k m \\Delta)$ time and $O(n \\tau)$ space, where\n$n$ is the number of vertices, $m$ the number of edges, $\\Delta$ the maximum\nresting time and $\\tau$ the maximum timestamp of an input temporal graph. In\naddition, we prove that the algorithms presented for the restless reachability\nproblems in vertex-colored temporal graphs are optimal under plausible\ncomplexity-theoretic assumptions. Finally, with an open-source implementation,\nwe demonstrate that our algorithm scales to large graphs with up to one billion\ntemporal edges, despite the problems being NP-hard. Specifically, we present\nextensive experiments to evaluate our scalability claims both on synthetic and\nreal-world graphs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:40:26 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 21:05:27 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 11:06:14 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Thejaswi", "Suhas", ""], ["Lauri", "Juho", ""], ["Gionis", "Aristides", ""]]}, {"id": "2010.08512", "submitter": "Adrian de Wynter", "authors": "Adrian de Wynter", "title": "An Approximation Algorithm for Optimal Subarchitecture Extraction", "comments": "Preprint. Under review. Original submission does not present the\n  bibliography issues from this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the set of architectural parameters for a\nchosen deep neural network which is optimal under three metrics: parameter\nsize, inference speed, and error rate. In this paper we state the problem\nformally, and present an approximation algorithm that, for a large subset of\ninstances behaves like an FPTAS with an approximation error of $\\rho \\leq |{1-\n\\epsilon}|$, and that runs in $O(|{\\Xi}| + |{W^*_T}|(1 +\n|{\\Theta}||{B}||{\\Xi}|/({\\epsilon\\, s^{3/2})}))$ steps, where $\\epsilon$ and\n$s$ are input parameters; $|{B}|$ is the batch size; $|{W^*_T}|$ denotes the\ncardinality of the largest weight set assignment; and $|{\\Xi}|$ and\n$|{\\Theta}|$ are the cardinalities of the candidate architecture and\nhyperparameter spaces, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:11:32 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["de Wynter", "Adrian", ""]]}, {"id": "2010.08513", "submitter": "Shihua Zhang", "authors": "Penglong Zhai and Shihua Zhang", "title": "Learnable Graph-regularization for Matrix Decomposition", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank approximation models of data matrices have become important machine\nlearning and data mining tools in many fields including computer vision, text\nmining, bioinformatics and many others. They allow for embedding\nhigh-dimensional data into low-dimensional spaces, which mitigates the effects\nof noise and uncovers latent relations. In order to make the learned\nrepresentations inherit the structures in the original data,\ngraph-regularization terms are often added to the loss function. However, the\nprior graph construction often fails to reflect the true network connectivity\nand the intrinsic relationships. In addition, many graph-regularized methods\nfail to take the dual spaces into account. Probabilistic models are often used\nto model the distribution of the representations, but most of previous methods\noften assume that the hidden variables are independent and identically\ndistributed for simplicity. To this end, we propose a learnable\ngraph-regularization model for matrix decomposition (LGMD), which builds a\nbridge between graph-regularized methods and probabilistic matrix decomposition\nmodels. LGMD learns two graphical structures (i.e., two precision matrices) in\nreal-time in an iterative manner via sparse precision matrix estimation and is\nmore robust to noise and missing entries. Extensive numerical results and\ncomparison with competing methods demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:12:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhai", "Penglong", ""], ["Zhang", "Shihua", ""]]}, {"id": "2010.08576", "submitter": "Karol W\\k{e}grzycki", "authors": "Jesper Nederlof, Karol W\\k{e}grzycki", "title": "Improving Schroeppel and Shamir's Algorithm for Subset Sum via\n  Orthogonal Vectors", "comments": "STOC 2021, 38 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $\\mathcal{O}^\\star(2^{0.5n})$ time and\n$\\mathcal{O}^\\star(2^{0.249999n})$ space randomized algorithm for solving\nworst-case Subset Sum instances with $n$ integers. This is the first\nimprovement over the long-standing $\\mathcal{O}^\\star(2^{n/2})$ time and\n$\\mathcal{O}^\\star(2^{n/4})$ space algorithm due to Schroeppel and Shamir (FOCS\n1979).\n  We breach this gap in two steps: (1) We present a space efficient reduction\nto the Orthogonal Vectors Problem (OV), one of the most central problem in\nFine-Grained Complexity. The reduction is established via an intricate\ncombination of the method of Schroeppel and Shamir, and the representation\ntechnique introduced by Howgrave-Graham and Joux (EUROCRYPT 2010) for designing\nSubset Sum algorithms for the average case regime. (2) We provide an algorithm\nfor OV that detects an orthogonal pair among $N$ given vectors in $\\{0,1\\}^d$\nwith support size $d/4$ in time $\\tilde{O}(N\\cdot2^d/\\binom{d}{d/4})$. Our\nalgorithm for OV is based on and refines the representative families framework\ndeveloped by Fomin, Lokshtanov, Panolan and Saurabh (J. ACM 2016).\n  Our reduction uncovers a curious tight relation between Subset Sum and OV,\nbecause any improvement of our algorithm for OV would imply an improvement over\nthe runtime of Schroeppel and Shamir, which is also a long standing open\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:18:22 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 11:23:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nederlof", "Jesper", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "2010.08633", "submitter": "Neha Gupta", "authors": "Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan", "title": "Universal guarantees for decision tree induction via a higher-order\n  splitting criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple extension of top-down decision tree learning heuristics\nsuch as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all\ntarget functions $f: \\{-1,1\\}^n \\to \\{-1,1\\}$ with respect to the uniform\ndistribution, circumventing impossibility results showing that existing\nheuristics fare poorly even for simple target functions. The crux of our\nextension is a new splitting criterion that takes into account the correlations\nbetween $f$ and small subsets of its attributes. The splitting criteria of\nexisting heuristics (e.g. Gini impurity and information gain), in contrast, are\nbased solely on the correlations between $f$ and its individual attributes.\n  Our algorithm satisfies the following guarantee: for all target functions $f\n: \\{-1,1\\}^n \\to \\{-1,1\\}$, sizes $s\\in \\mathbb{N}$, and error parameters\n$\\epsilon$, it constructs a decision tree of size $s^{\\tilde{O}((\\log\ns)^2/\\epsilon^2)}$ that achieves error $\\le O(\\mathsf{opt}_s) + \\epsilon$,\nwhere $\\mathsf{opt}_s$ denotes the error of the optimal size $s$ decision tree.\nA key technical notion that drives our analysis is the noise stability of $f$,\na well-studied smoothness measure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 21:20:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Blanc", "Guy", ""], ["Gupta", "Neha", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2010.08676", "submitter": "Anar Amgalan", "authors": "Anar Amgalan, Lilianne R. Mujica-Parodi, Steven S. Skiena", "title": "Fast Spatial Autocorrelation", "comments": "To be published in ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical or geographic location proves to be an important feature in many\ndata science models, because many diverse natural and social phenomenon have a\nspatial component. Spatial autocorrelation measures the extent to which locally\nadjacent observations of the same phenomenon are correlated. Although\nstatistics like Moran's $I$ and Geary's $C$ are widely used to measure spatial\nautocorrelation, they are slow: all popular methods run in $\\Omega(n^2)$ time,\nrendering them unusable for large data sets, or long time-courses with moderate\nnumbers of points. We propose a new $S_A$ statistic based on the notion that\nthe variance observed when merging pairs of nearby clusters should increase\nslowly for spatially autocorrelated variables. We give a linear-time algorithm\nto calculate $S_A$ for a variable with an input agglomeration order (available\nat https://github.com/aamgalan/spatial_autocorrelation). For a typical dataset\nof $n \\approx 63,000$ points, our $S_A$ autocorrelation measure can be computed\nin 1 second, versus 2 hours or more for Moran's $I$ and Geary's $C$. Through\nsimulation studies, we demonstrate that $S_A$ identifies spatial correlations\nin variables generated with spatially-dependent model half an order of\nmagnitude earlier than either Moran's $I$ or Geary's $C$. Finally, we prove\nseveral theoretical properties of $S_A$: namely that it behaves as a true\ncorrelation statistic, and is invariant under addition or multiplication by a\nconstant.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:24:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Amgalan", "Anar", ""], ["Mujica-Parodi", "Lilianne R.", ""], ["Skiena", "Steven S.", ""]]}, {"id": "2010.08821", "submitter": "Noah Stephens-Davidowitz", "authors": "Zvika Brakerski, Noah Stephens-Davidowitz, Vinod Vaikuntanathan", "title": "On the Hardness of Average-case k-SUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show the first worst-case to average-case reduction for the\nclassical $k$-SUM problem. A $k$-SUM instance is a collection of $m$ integers,\nand the goal of the $k$-SUM problem is to find a subset of $k$ elements that\nsums to $0$. In the average-case version, the $m$ elements are chosen uniformly\nat random from some interval $[-u,u]$.\n  We consider the total setting where $m$ is sufficiently large (with respect\nto $u$ and $k$), so that we are guaranteed (with high probability) that\nsolutions must exist. Much of the appeal of $k$-SUM, in particular connections\nto problems in computational geometry, extends to the total setting.\n  The best known algorithm in the average-case total setting is due to Wagner\n(following the approach of Blum-Kalai-Wasserman), and achieves a run-time of\n$u^{O(1/\\log k)}$. This beats the known (conditional) lower bounds for\nworst-case $k$-SUM, raising the natural question of whether it can be improved\neven further. However, in this work, we show a matching average-case\nlower-bound, by showing a reduction from worst-case lattice problems, thus\nintroducing a new family of techniques into the field of fine-grained\ncomplexity. In particular, we show that any algorithm solving average-case\n$k$-SUM on $m$ elements in time $u^{o(1/\\log k)}$ will give a super-polynomial\nimprovement in the complexity of algorithms for lattice problems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 16:39:41 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 00:56:30 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Brakerski", "Zvika", ""], ["Stephens-Davidowitz", "Noah", ""], ["Vaikuntanathan", "Vinod", ""]]}, {"id": "2010.08840", "submitter": "Bryce Sandlund", "authors": "Bryce Sandlund and Sebastian Wild", "title": "Lazy Search Trees", "comments": "Accepted for publication in FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the lazy search tree data structure. The lazy search tree is a\ncomparison-based data structure on the pointer machine that supports\norder-based operations such as rank, select, membership, predecessor,\nsuccessor, minimum, and maximum while providing dynamic operations insert,\ndelete, change-key, split, and merge. We analyze the performance of our data\nstructure based on a partition of current elements into a set of gaps\n$\\{\\Delta_i\\}$ based on rank. A query falls into a particular gap and splits\nthe gap into two new gaps at a rank $r$ associated with the query operation. If\nwe define $B = \\sum_i |\\Delta_i| \\log_2(n/|\\Delta_i|)$, our performance over a\nsequence of $n$ insertions and $q$ distinct queries is $O(B + \\min(n \\log \\log\nn, n \\log q))$. We show $B$ is a lower bound.\n  Effectively, we reduce the insertion time of binary search trees from\n$\\Theta(\\log n)$ to $O(\\min(\\log(n/|\\Delta_i|) + \\log \\log |\\Delta_i|, \\; \\log\nq))$, where $\\Delta_i$ is the gap in which the inserted element falls. Over a\nsequence of $n$ insertions and $q$ queries, a time bound of $O(n \\log q + q\n\\log n)$ holds; better bounds are possible when queries are non-uniformly\ndistributed. As an extreme case of non-uniformity, if all queries are for the\nminimum element, the lazy search tree performs as a priority queue with $O(\\log\n\\log n)$ time insert and decrease-key operations. The same data structure\nsupports queries for any rank, interpolating between binary search trees and\nefficient priority queues.\n  Lazy search trees can be implemented to operate mostly on arrays, requiring\nonly $O(\\min(q, n))$ pointers. Via direct reduction, our data structure also\nsupports the efficient access theorems of the splay tree, providing a powerful\ndata structure for non-uniform element access, both when the number of accesses\nis small and large.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 18:24:08 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sandlund", "Bryce", ""], ["Wild", "Sebastian", ""]]}, {"id": "2010.09014", "submitter": "Michiel de Bondt", "authors": "Michiel de Bondt", "title": "Solving Shisen-Sho boards", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple proof of that determining solvability of Shisen-Sho boards\nis NP-complete. Furthermore, we show that under realistic assumptions, one can\ncompute in logarithmic time if two tiles form a playable pair.\n  We combine an implementation of the algoritm to test playability of pairs\nwith my earlier algorithm to solve Mahjong Solitaire boards with peeking, to\nobtain an algorithm to solve Shisen-Sho boards. We sample several Shisen-Sho\nand Mahjong Solitaire layouts for solvability for Shisen-Sho and Mahjong\nSolitaire.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:16:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["de Bondt", "Michiel", ""]]}, {"id": "2010.09096", "submitter": "Karl Bringmann", "authors": "Karl Bringmann and Philip Wellnitz", "title": "On Near-Linear-Time Algorithms for Dense Subset Sum", "comments": "full version of SODA'21 paper, 36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Subset Sum problem we are given a set of $n$ positive integers $X$ and\na target $t$ and are asked whether some subset of $X$ sums to $t$. Natural\nparameters for this problem that have been studied in the literature are $n$\nand $t$ as well as the maximum input number $\\rm{mx}_X$ and the sum of all\ninput numbers $\\Sigma_X$. In this paper we study the dense case of Subset Sum,\nwhere all these parameters are polynomial in $n$. In this regime, standard\npseudo-polynomial algorithms solve Subset Sum in polynomial time $n^{O(1)}$.\n  Our main question is: When can dense Subset Sum be solved in near-linear time\n$\\tilde{O}(n)$? We provide an essentially complete dichotomy by designing\nimproved algorithms and proving conditional lower bounds, thereby determining\nessentially all settings of the parameters $n,t,\\rm{mx}_X,\\Sigma_X$ for which\ndense Subset Sum is in time $\\tilde{O}(n)$. For notational convenience we\nassume without loss of generality that $t \\ge \\rm{mx}_X$ (as larger numbers can\nbe ignored) and $t \\le \\Sigma_X/2$ (using symmetry). Then our dichotomy reads\nas follows:\n  - By reviving and improving an additive-combinatorics-based approach by Galil\nand Margalit [SICOMP'91], we show that Subset Sum is in near-linear time\n$\\tilde{O}(n)$ if $t \\gg \\rm{mx}_X \\Sigma_X/n^2$.\n  - We prove a matching conditional lower bound: If Subset Sum is in\nnear-linear time for any setting with $t \\ll \\rm{mx}_X \\Sigma_X/n^2$, then the\nStrong Exponential Time Hypothesis and the Strong k-Sum Hypothesis fail.\n  We also generalize our algorithm from sets to multi-sets, albeit with\nnon-matching upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 20:35:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bringmann", "Karl", ""], ["Wellnitz", "Philip", ""]]}, {"id": "2010.09115", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "Shortest Paths Among Obstacles in the Plane Revisited", "comments": "Published in SODA 2021. Observation 2 in the previous version (and\n  also in SODA proceedings) is not correct. The issue is addressed in this\n  version with slightly different analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of pairwise disjoint polygonal obstacles in the plane, finding an\nobstacle-avoiding Euclidean shortest path between two points is a classical\nproblem in computational geometry and has been studied extensively. The\nprevious best algorithm was given by Hershberger and Suri [FOCS 1993, SIAM J.\nComput. 1999] and the algorithm runs in $O(n\\log n)$ time and $O(n\\log n)$\nspace, where $n$ is the total number of vertices of all obstacles. The\nalgorithm is time-optimal because $\\Omega(n\\log n)$ is a lower bound. It has\nbeen an open problem for over two decades whether the space can be reduced to\n$O(n)$. In this paper, we settle it by solving the problem in $O(n\\log n)$ time\nand $O(n)$ space, which is optimal in both time and space; we achieve this by\nmodifying the algorithm of Hershberger and Suri. Like their original algorithm,\nour new algorithm can build a shortest path map for a source point $s$ in\n$O(n\\log n)$ time and $O(n)$ space, such that given any query point $t$, the\nlength of a shortest path from $s$ to $t$ can be computed in $O(\\log n)$ time\nand a shortest path can be produced in additional time linear in the number of\nedges of the path.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 21:54:15 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 18:23:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "2010.09141", "submitter": "Zafeiria Moumoulidou", "authors": "Zafeiria Moumoulidou, Andrew McGregor, and Alexandra Meliou", "title": "Diverse Data Selection under Fairness Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity is an important principle in data selection and summarization,\nfacility location, and recommendation systems. Our work focuses on maximizing\ndiversity in data selection, while offering fairness guarantees. In particular,\nwe offer the first study that augments the Max-Min diversification objective\nwith fairness constraints. More specifically, given a universe $U$ of $n$\nelements that can be partitioned into $m$ disjoint groups, we aim to retrieve a\n$k$-sized subset that maximizes the pairwise minimum distance within the set\n(diversity) and contains a pre-specified $k_i$ number of elements from each\ngroup $i$ (fairness). We show that this problem is NP-complete even in metric\nspaces, and we propose three novel algorithms, linear in $n$, that provide\nstrong theoretical approximation guarantees for different values of $m$ and\n$k$. Finally, we extend our algorithms and analysis to the case where groups\ncan be overlapping.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 23:51:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Moumoulidou", "Zafeiria", ""], ["McGregor", "Andrew", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2010.09205", "submitter": "Laurence Clarfeld", "authors": "Laurence A. Clarfeld and Margaret J. Eppstein", "title": "Group Testing for Efficiently Sampling Hypergraphs When Tests Have\n  Variable Costs", "comments": "16 pages, 9 figures, submitted to IEEE Transactions on Network\n  Science and Engineering (in review). arXiv admin note: substantial text\n  overlap with arXiv:1909.04513", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the group-testing literature, efficient algorithms have been developed to\nminimize the number of tests required to identify all minimal \"defective\"\nsub-groups embedded within a larger group, using deterministic group splitting\nwith a generalized binary search. In a separate literature, researchers have\nused a stochastic group splitting approach to efficiently sample from the\nintractable number of minimal defective sets of outages in electrical power\nsystems that trigger large cascading failures, a problem in which positive\ntests can be much more computationally costly than negative tests. In this\nwork, we generate test problems with variable numbers of defective sets and a\ntunable positive:negative test cost ratio to compare the efficiency of\ndeterministic and stochastic adaptive group splitting algorithms for\nidentifying defective edges in hypergraphs. For both algorithms, we show that\nthe optimal initial group size is a function of both the prevalence of\ndefective sets and the positive:negative test cost ratio. We find that\ndeterministic splitting requires fewer total tests but stochastic splitting\nrequires fewer positive tests, such that the relative efficiency of these two\napproaches depends on the positive:negative test cost ratio. We discuss some\nreal-world applications where each of these algorithms is expected to\noutperform the other.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 03:49:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Clarfeld", "Laurence A.", ""], ["Eppstein", "Margaret J.", ""]]}, {"id": "2010.09230", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "The Graphs of Stably Matchable Pairs", "comments": "41 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the graphs formed from instances of the stable matching problem by\nconnecting pairs of elements with an edge when there exists a stable matching\nin which they are matched. Our results include the NP-completeness of\nrecognizing these graphs, an exact recognition algorithm that is singly\nexponential in the number of edges of the given graph, and an algorithm whose\ntime is linear in the number of vertices of the graph but exponential in a\npolynomial of its carving width. We also provide characterizations of graphs of\nstably matchable pairs that belong to certain classes of graphs, and of the\nlattices of stable matchings that can have graphs in these classes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 05:26:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "2010.09286", "submitter": "Nicolas Gastineau", "authors": "Nicolas Gastineau (LIB), Wahabou Abdou (LIB), Nader Mbarek (LIB),\n  Olivier Togni (LIB)", "title": "Leader Election And Local Identifiers For 3D Programmable Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two deterministic leader election algorithms for\nprogrammable matter on the face-centered cubic grid. The face-centered cubic\ngrid is a 3-dimensional 12-regular infinite grid that represents an optimal way\nto pack spheres (i.e., spherical particles or modules in the context of the\nprogrammable matter) in the 3-dimensional space. While the first leader\nelection algorithm requires a strong hypothesis about the initial configuration\nof the particles and no hypothesis on the system configurations that the\nparticles are forming, the second one requires fewer hypothesis about the\ninitial configuration of the particles but does not work for all possible\nparticles' arrangement. We also describe a way to compute and assign-local\nidentifiers to the particles in this grid with a memory space not dependent on\nthe number of particles. A-local identifier is a variable assigned to each\nparticle in such a way that particles at distance at most each have a different\nidentifier.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:58:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Gastineau", "Nicolas", "", "LIB"], ["Abdou", "Wahabou", "", "LIB"], ["Mbarek", "Nader", "", "LIB"], ["Togni", "Olivier", "", "LIB"]]}, {"id": "2010.09474", "submitter": "Lixi Zhou", "authors": "Lixi Zhou, Zijie Wang, Amitabh Das, Jia Zou", "title": "It's the Best Only When It Fits You Most: Finding Related Models for\n  Serving Based on Dynamic Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent, deep learning has become the most popular direction in machine\nlearning and artificial intelligence. However, preparation of training data is\noften a bottleneck in the lifecycle of deploying a deep learning model for\nproduction or research. Reusing models for inferencing a dataset can greatly\nsave the human costs required for training data creation. Although there exist\na number of model sharing platform such as TensorFlow Hub, PyTorch Hub, DLHub,\nmost of these systems require model uploaders to manually specify the details\nof each model and model downloaders to screen keyword search results for\nselecting a model. They are in lack of an automatic model searching tool. This\npaper proposes an end-to-end process of searching related models for serving\nbased on the similarity of the target dataset and the training datasets of the\navailable models. While there exist many similarity measurements, we study how\nto efficiently apply these metrics without pair-wise comparison and compare the\neffectiveness of these metrics. We find that our proposed adaptivity\nmeasurement which is based on Jensen-Shannon (JS) divergence, is an effective\nmeasurement, and its computation can be significantly accelerated by using the\ntechnique of locality sensitive hashing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:52:13 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhou", "Lixi", ""], ["Wang", "Zijie", ""], ["Das", "Amitabh", ""], ["Zou", "Jia", ""]]}, {"id": "2010.09580", "submitter": "William Lochet", "authors": "Eduard Eiben, Fedor V. Fomin, Petr A. Golovach, William Lochet, Fahad\n  Panolan and Kirill Simonov", "title": "EPTAS for $k$-means Clustering of Affine Subspaces", "comments": "To be published in Symposium on Discrete Algorithms (SODA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of the fundamental $k$-means clustering for data\nwith incomplete or corrupted entries. When data objects are represented by\npoints in $\\mathbb{R}^d$, a data point is said to be incomplete when some of\nits entries are missing or unspecified. An incomplete data point with at most\n$\\Delta$ unspecified entries corresponds to an axis-parallel affine subspace of\ndimension at most $\\Delta$, called a $\\Delta$-point. Thus we seek a partition\nof $n$ input $\\Delta$-points into $k$ clusters minimizing the $k$-means\nobjective. For $\\Delta=0$, when all coordinates of each point are specified,\nthis is the usual $k$-means clustering. We give an algorithm that finds an $(1+\n\\epsilon)$-approximate solution in time $f(k,\\epsilon, \\Delta) \\cdot n^2 \\cdot\nd$ for some function $f$ of $k,\\epsilon$, and $\\Delta$ only.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:04:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Eiben", "Eduard", ""], ["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Lochet", "William", ""], ["Panolan", "Fahad", ""], ["Simonov", "Kirill", ""]]}, {"id": "2010.09649", "submitter": "Christopher Musco", "authors": "Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff", "title": "Hutch++: Optimal Stochastic Trace Estimation", "comments": "SIAM Symposium on Simplicity in Algorithms (SOSA21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the trace of a matrix $A$ that can only be\naccessed through matrix-vector multiplication. We introduce a new randomized\nalgorithm, Hutch++, which computes a $(1 \\pm \\epsilon)$ approximation to\n$tr(A)$ for any positive semidefinite (PSD) $A$ using just $O(1/\\epsilon)$\nmatrix-vector products. This improves on the ubiquitous Hutchinson's estimator,\nwhich requires $O(1/\\epsilon^2)$ matrix-vector products. Our approach is based\non a simple technique for reducing the variance of Hutchinson's estimator using\na low-rank approximation step, and is easy to implement and analyze. Moreover,\nwe prove that, up to a logarithmic factor, the complexity of Hutch++ is optimal\namongst all matrix-vector query algorithms, even when queries can be chosen\nadaptively. We show that it significantly outperforms Hutchinson's method in\nexperiments. While our theory mainly requires $A$ to be positive semidefinite,\nwe provide generalized guarantees for general square matrices, and show\nempirical gains in such applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:45:37 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 22:31:50 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 18:01:52 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 18:59:49 GMT"}, {"version": "v5", "created": "Fri, 11 Jun 2021 01:33:47 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Meyer", "Raphael A.", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Woodruff", "David P.", ""]]}, {"id": "2010.09705", "submitter": "Makis Arsenis", "authors": "Makis Arsenis, Odysseas Drosis, and Robert Kleinberg", "title": "Constrained-Order Prophet Inequalities", "comments": "To appear in ACM-SIAM Symposium on Discrete Algorithms (SODA) 2021,\n  19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free order prophet inequalities bound the ratio between the expected value\nobtained by two parties each selecting a value from a set of independent random\nvariables: a \"prophet\" who knows the value of each variable and may select the\nmaximum one, and a \"gambler\" who is free to choose the order in which to\nobserve the values but must select one of them immediately after observing it,\nwithout knowing what values will be sampled for the unobserved variables. It is\nknown that the gambler can always ensure an expected payoff at least\n$0.669\\dots$ times as great as that of the prophet. In fact, there exists a\nthreshold stopping rule which guarantees a gambler-to-prophet ratio of at least\n$1-\\frac1e=0.632\\dots$. In contrast, if the gambler must observe the values in\na predetermined order, the tight bound for the gambler-to-prophet ratio is\n$1/2$.\n  In this work we investigate a model that interpolates between these two\nextremes. We assume there is a predefined set of permutations, and the gambler\nis free to choose the order of observation to be any one of these predefined\npermutations. Surprisingly, we show that even when only two orderings are\nallowed---namely, the forward and reverse orderings---the gambler-to-prophet\nratio improves to $\\varphi^{-1}=0.618\\dots$, the inverse of the golden ratio.\nAs the number of allowed permutations grows beyond 2, a striking \"double\nplateau\" phenomenon emerges: after increasing from $0.5$ to $\\varphi^{-1}$, the\ngambler-to-prophet ratio achievable by threshold stopping rules does not exceed\n$\\varphi^{-1}+o(1)$ until the number of allowed permutations grows to $O(\\log\nn)$. The ratio reaches $1-\\frac1e-\\varepsilon$ for a suitably chosen set of\n$O(\\text{poly}(\\varepsilon^{-1})\\cdot\\log n)$ permutations and does not exceed\n$1-\\frac1e$ even when the full set of $n!$ permutations is allowed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:51:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Arsenis", "Makis", ""], ["Drosis", "Odysseas", ""], ["Kleinberg", "Robert", ""]]}, {"id": "2010.09852", "submitter": "Maciej Besta", "authors": "Hermann Schweizer, Maciej Besta, Torsten Hoefler", "title": "Evaluating the Cost of Atomic Operations on Modern Architectures", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Parallel\n  Architectures and Compilation (PACT'15), 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add\n(FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs\nbetween these operations and various characteristics of such systems, such as\nthe structure of caches, are unclear and have not been thoroughly analyzed. In\nthis paper we establish an evaluation methodology, develop a performance model,\nand present a set of detailed benchmarks for latency and bandwidth of different\natomics. We consider various state-of-the-art x86 architectures: Intel Haswell,\nXeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising\nperformance relationships between the considered atomics and architectural\nproperties such as the coherence state of the accessed cache lines. One key\nfinding is that all the tested atomics have comparable latency and bandwidth\neven if they are characterized by different consensus numbers. Another insight\nis that the hardware implementation of atomics prevents any instruction-level\nparallelism even if there are no dependencies between the issued operations.\nFinally, we discuss solutions to the discovered performance issues in the\nanalyzed architectures. Our analysis enables simpler and more effective\nparallel programming and accelerates data processing on various architectures\ndeployed in both off-the-shelf machines and large compute systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:43:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Schweizer", "Hermann", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09884", "submitter": "Elaine Shi", "authors": "Gilad Asharov, Wei-Kai Lin, Elaine Shi", "title": "Sorting Short Keys in Circuits of Size o(n log n)", "comments": "SODA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of sorting an input array containing $n$\nelements, where each element is described with a $k$-bit comparison-key and a\n$w$-bit payload. A long-standing open problem is whether there exist $(k + w)\n\\cdot o(n \\log n)$-sized boolean circuits for sorting. We show that one can\novercome the $n\\log n$ barrier when the keys to be sorted are short.\nSpecifically, we prove that there is a circuit with $(k + w) \\cdot O(n k) \\cdot\n\\poly(\\log^*n - \\log^* (w + k))$ boolean gates capable of sorting any input\narray containing $n$ elements, each described with a $k$-bit key and a $w$-bit\npayload. Therefore, if the keys to be sorted are short, say, $k < o(\\log n)$,\nour result is asymptotically better than the classical AKS sorting network\n(ignoring $\\poly\\log^*$ terms); and we also overcome the $n \\log n$ barrier in\nsuch cases. Such a result might be surprising initially because it is long\nknown that comparator-based techniques must incur $\\Omega(n \\log n)$ comparator\ngates even when the keys to be sorted are only $1$-bit long (e.g., see Knuth's\n\"Art of Programming\" textbook). To the best of our knowledge, we are the first\nto achieve non-trivial results for sorting circuits using non-comparison-based\ntechniques. We also show that if the Li-Li network coding conjecture is true,\nour upper bound is optimal, barring $\\poly\\log^*$ terms, for every $k$ as long\nas $k = O(\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 13:17:44 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 20:31:54 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Asharov", "Gilad", ""], ["Lin", "Wei-Kai", ""], ["Shi", "Elaine", ""]]}, {"id": "2010.09913", "submitter": "Maciej Besta", "authors": "Maciej Besta, Florian Marending, Edgar Solomonik, Torsten Hoefler", "title": "SlimSell: A Vectorizable Graph Representation for Breadth-First Search", "comments": null, "journal-ref": "Proceedings of the 31st IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS'17), 2017", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectorization and GPUs will profoundly change graph processing. Traditional\ngraph algorithms tuned for 32- or 64-bit based memory accesses will be\ninefficient on architectures with 512-bit wide (or larger) instruction units\nthat are already present in the Intel Knights Landing (KNL) manycore CPU.\nAnticipating this shift, we propose SlimSell: a vectorizable graph\nrepresentation to accelerate Breadth-First Search (BFS) based on sparse-matrix\ndense-vector (SpMV) products. SlimSell extends and combines the\nstate-of-the-art SIMD-friendly Sell-C-sigma matrix storage format with\ntropical, real, boolean, and sel-max semiring operations. The resulting design\nreduces the necessary storage (by up to 50%) and thus pressure on the memory\nsubsystem. We augment SlimSell with the SlimWork and SlimChunk schemes that\nreduce the amount of work and improve load balance, further accelerating BFS.\nWe evaluate all the schemes on Intel Haswell multicore CPUs, the\nstate-of-the-art Intel Xeon Phi KNL manycore CPUs, and NVIDIA Tesla GPUs. Our\nexperiments indicate which semiring offers highest speedups for BFS and\nillustrate that SlimSell accelerates a tuned Graph500 BFS code by up to 33%.\nThis work shows that vectorization can secure high-performance in BFS based on\nSpMV products; the proposed principles and designs can be extended to other\ngraph algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:15:25 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 10:13:40 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Besta", "Maciej", ""], ["Marending", "Florian", ""], ["Solomonik", "Edgar", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.09929", "submitter": "Gautam Kamath", "authors": "Ishaq Aden-Ali, Hassan Ashtiani, Gautam Kamath", "title": "On the Sample Complexity of Privately Learning Unbounded\n  High-Dimensional Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide sample complexity upper bounds for agnostically learning\nmultivariate Gaussians under the constraint of approximate differential\nprivacy. These are the first finite sample upper bounds for general Gaussians\nwhich do not impose restrictions on the parameters of the distribution. Our\nbounds are near-optimal in the case when the covariance is known to be the\nidentity, and conjectured to be near-optimal in the general case. From a\ntechnical standpoint, we provide analytic tools for arguing the existence of\nglobal \"locally small\" covers from local covers of the space. These are\nexploited using modifications of recent techniques for differentially private\nhypothesis selection. Our techniques may prove useful for privately learning\nother distribution classes which do not possess a finite cover.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:55:03 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Aden-Ali", "Ishaq", ""], ["Ashtiani", "Hassan", ""], ["Kamath", "Gautam", ""]]}, {"id": "2010.10134", "submitter": "Maximilian Probst Gutenberg", "authors": "Thiago Bergamaschi, Monika Henzinger, Maximilian Probst Gutenberg,\n  Virginia Vassilevska Williams, Nicole Wein", "title": "New Techniques and Fine-Grained Hardness for Dynamic Near-Additive\n  Spanners", "comments": "Accepted to SODA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining and updating shortest paths information in a graph is a\nfundamental problem with many applications. As computations on dense graphs can\nbe prohibitively expensive, and it is preferable to perform the computations on\na sparse skeleton of the given graph that roughly preserves the shortest paths\ninformation. Spanners and emulators serve this purpose. This paper develops\nfast dynamic algorithms for sparse spanner and emulator maintenance and\nprovides evidence from fine-grained complexity that these algorithms are tight.\n  Under the popular OMv conjecture, we show that there can be no decremental or\nincremental algorithm that maintains an $n^{1+o(1)}$ edge (purely additive)\n$+n^{\\delta}$-emulator for any $\\delta<1/2$ with arbitrary polynomial\npreprocessing time and total update time $m^{1+o(1)}$. Also, under the\nCombinatorial $k$-Clique hypothesis, any fully dynamic combinatorial algorithm\nthat maintains an $n^{1+o(1)}$ edge $(1+\\epsilon,n^{o(1)})$-spanner or emulator\nmust either have preprocessing time $mn^{1-o(1)}$ or amortized update time\n$m^{1-o(1)}$. Both of our conditional lower bounds are tight.\n  As the above fully dynamic lower bound only applies to combinatorial\nalgorithms, we also develop an algebraic spanner algorithm that improves over\nthe $m^{1-o(1)}$ update time for dense graphs. For any constant $\\epsilon\\in\n(0,1]$, there is a fully dynamic algorithm with worst-case update time\n$O(n^{1.529})$ that whp maintains an $n^{1+o(1)}$ edge\n$(1+\\epsilon,n^{o(1)})$-spanner.\n  Our new algebraic techniques and spanner algorithms allow us to also obtain\n(1) a new fully dynamic algorithm for All-Pairs Shortest Paths (APSP) with\nupdate and path query time $O(n^{1.9})$; (2) a fully dynamic\n$(1+\\epsilon)$-approximate APSP algorithm with update time $O(n^{1.529})$; (3)\na fully dynamic algorithm for near-$2$-approximate Steiner tree maintenance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 08:57:52 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 09:08:36 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Bergamaschi", "Thiago", ""], ["Henzinger", "Monika", ""], ["Gutenberg", "Maximilian Probst", ""], ["Williams", "Virginia Vassilevska", ""], ["Wein", "Nicole", ""]]}, {"id": "2010.10272", "submitter": "Lars Gottesb\\\"uren", "authors": "Lars Gottesb\\\"uren, Tobias Heuer, Peter Sanders, Sebastian Schlag", "title": "Scalable Shared-Memory Hypergraph Partitioning", "comments": "Accepted for publication at ALENEX21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph partitioning is an important preprocessing step for optimizing\ndata placement and minimizing communication volumes in high-performance\ncomputing applications. To cope with ever growing problem sizes, it has become\nincreasingly important to develop fast parallel partitioning algorithms whose\nsolution quality is competitive with existing sequential algorithms. To this\nend, we present Mt-KaHyPar, the first shared-memory multilevel hypergraph\npartitioner with parallel implementations of many techniques used by the\nsequential, high-quality partitioning systems: a parallel coarsening algorithm\nthat uses parallel community detection as guidance, initial partitioning via\nparallel recursive bipartitioning with work-stealing, a scalable label\npropagation refinement algorithm, and the first fully-parallel direct $k$-way\nformulation of the classical FM algorithm. Experiments performed on a large\nbenchmark set of instances from various application domains demonstrate the\nscalability and effectiveness of our approach. With 64 cores, we observe\nself-relative speedups of up to 51 and a harmonic mean speedup of 23.5. In\nterms of solution quality, we outperform the distributed hypergraph partitioner\nZoltan on 95% of the instances while also being a factor of 2.1 faster. With\njust four cores,Mt-KaHyPar is also slightly faster than the fastest sequential\nmultilevel partitioner PaToH while producing better solutions on 83% of all\ninstances. The sequential high-quality partitioner KaHyPar still finds better\nsolutions than our parallel approach, especially when using max-flow-based\nrefinement. This, however, comes at the cost of considerably longer running\ntimes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:45:12 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 14:51:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Heuer", "Tobias", ""], ["Sanders", "Peter", ""], ["Schlag", "Sebastian", ""]]}, {"id": "2010.10314", "submitter": "Kitty Meeks", "authors": "Jessica Enright, Duncan Lee, Kitty Meeks, William Pettersson and John\n  Sylvester", "title": "The complexity of finding optimal subgraphs to represent spatial\n  correlation", "comments": "New results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lee, Meeks and Pettersson recently demonstrated that improved inference for\nareal unit count data can be achieved by carrying out modifications to a graph\nrepresenting spatial correlations; specifically, they delete edges of the\nplanar graph derived from border-sharing between geographic regions in order to\nmaximise a specific objective function. In this paper we address the\ncomputational complexity of the associated graph optimisation problem,\ndemonstrating that it cannot be solved in polynomial time unless P = NP; we\nfurther show intractability for a related but simpler problem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 14:24:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:07:39 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 16:39:22 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Enright", "Jessica", ""], ["Lee", "Duncan", ""], ["Meeks", "Kitty", ""], ["Pettersson", "William", ""], ["Sylvester", "John", ""]]}, {"id": "2010.10408", "submitter": "Philipp Zschoche", "authors": "Philipp Zschoche", "title": "A Faster Parameterized Algorithm for Temporal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal graph is a sequence of graphs (called layers) over the same vertex\nset -- describing a graph topology which is subject to discrete changes over\ntime. A $\\Delta$-temporal matching $M$ is a set of time edges $(e,t)$ (an edge\n$e$ paired up with a point in time $t$) such that for all distinct time edges\n$(e,t),(e',t') \\in M$ we have that $e$ and $e'$ do not share an endpoint, or\nthe time-labels $t$ and $t'$ are at least $\\Delta$ time units apart. Mertzios\net al. [STACS '20] provided a $2^{O(\\Delta\\nu)}\\cdot |{\\mathcal\nG}|^{O(1)}$-time algorithm to compute the maximum size of a $\\Delta$-temporal\nmatching in a temporal graph $\\mathcal G$, where $|\\mathcal G|$ denotes the\nsize of $\\mathcal G$, and $\\nu$ is the $\\Delta$-vertex cover number of\n$\\mathcal G$. The $\\Delta$-vertex cover number is the minimum number $\\nu$ such\nthat the classical vertex cover number of the union of any $\\Delta$ consecutive\nlayers of the temporal graph is upper-bounded by $\\nu$. We show an improved\nalgorithm to compute a $\\Delta$-temporal matching of maximum size with a\nrunning time of $\\Delta^{O(\\nu)}\\cdot |\\mathcal G|$ and hence provide an\nexponential speedup in terms of $\\Delta$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:14:01 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 13:52:18 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 08:28:04 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zschoche", "Philipp", ""]]}, {"id": "2010.10454", "submitter": "Milad Bakhshizadeh", "authors": "Milad Bakhshizadeh, Ali Kamalinejad, Mina Latifi", "title": "A practical algorithm to calculate Cap Discrepancy", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform distribution of the points has been of interest to researchers for a\nlong time and has applications in different areas of Mathematics and Computer\nScience. One of the well-known measures to evaluate the uniformity of a given\ndistribution is Discrepancy, which assesses the difference between the Uniform\ndistribution and the empirical distribution given by putting mass points at the\npoints of the given set. While Discrepancy is very useful to measure\nuniformity, it is computationally challenging to be calculated accurately. We\nintroduce the concept of directed Discrepancy based on which we have developed\nan algorithm, called Directional Discrepancy, that can offer accurate\napproximation for the cap Discrepancy of a finite set distributed on the unit\nSphere, $\\mathbb{S}^2.$ We also analyze the time complexity of the Directional\nDiscrepancy algorithm precisely; and practically evaluate its capacity by\ncalculating the Cap Discrepancy of a specific distribution, Polar Coordinates,\nwhich aims to distribute points uniformly on the Sphere.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:11:26 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Bakhshizadeh", "Milad", ""], ["Kamalinejad", "Ali", ""], ["Latifi", "Mina", ""]]}, {"id": "2010.10809", "submitter": "Andreas Emil Feldmann", "authors": "Steffen Borgwardt, Cornelius Brand, Andreas Emil Feldmann, Martin\n  Kouteck\\'y", "title": "A Note on the Approximability of Deepest-Descent Circuit Steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear programs (LPs) can be solved by polynomially many moves along the\ncircuit direction improving the objective the most, so-called deepest-descent\nsteps (dd-steps). Computing these steps is NP-hard (De Loera et al., arXiv,\n2019), a consequence of the hardness of deciding the existence of an optimal\ncircuit-neighbor (OCNP) on LPs with non-unique optima.\n  We prove OCNP is easy under the promise of unique optima, but already\n$O(n^{1-\\varepsilon})$-approximating dd-steps remains hard even for totally\nunimodular $n$-dimensional 0/1-LPs with a unique optimum. We provide a matching\n$n$-approximation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:59:59 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 13:36:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Borgwardt", "Steffen", ""], ["Brand", "Cornelius", ""], ["Feldmann", "Andreas Emil", ""], ["Kouteck\u00fd", "Martin", ""]]}, {"id": "2010.10881", "submitter": "Josep Domingo-Ferrer", "authors": "Josep Domingo-Ferrer and Jordi Soria-Comas", "title": "Multi-Dimensional Randomized Response", "comments": "IEEE Transactions on Knowledge and Data Engineering, to appear.\n  (First version submitted on May 8, 2019 as TKDE-2019-05-0430; first revision\n  submitted on July 13, 2020 as TKDE-2019-05-0430.R1; second revision submitted\n  on Nov. 5, 2020 as TKDE-2019-05-0430.R2 and accepted without changes on Dec.\n  16, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our data world, a host of not necessarily trusted controllers gather data\non individual subjects. To preserve her privacy and, more generally, her\ninformational self-determination, the individual has to be empowered by giving\nher agency on her own data. Maximum agency is afforded by local anonymization,\nthat allows each individual to anonymize her own data before handing them to\nthe data controller. Randomized response (RR) is a local anonymization approach\nable to yield multi-dimensional full sets of anonymized microdata that are\nvalid for exploratory analysis and machine learning. This is so because an\nunbiased estimate of the distribution of the true data of individuals can be\nobtained from their pooled randomized data. Furthermore, RR offers rigorous\nprivacy guarantees. The main weakness of RR is the curse of dimensionality when\napplied to several attributes: as the number of attributes grows, the accuracy\nof the estimated true data distribution quickly degrades. We propose several\ncomplementary approaches to mitigate the dimensionality problem. First, we\npresent two basic protocols, separate RR on each attribute and joint RR for all\nattributes, and discuss their limitations. Then we introduce an algorithm to\nform clusters of attributes so that attributes in different clusters can be\nviewed as independent and joint RR can be performed within each cluster. After\nthat, we introduce an adjustment algorithm for the randomized data set that\nrepairs some of the accuracy loss due to assuming independence between\nattributes when using RR separately on each attribute or due to assuming\nindependence between clusters in cluster-wise RR. We also present empirical\nwork to illustrate the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 10:24:27 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 09:40:11 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 18:26:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Domingo-Ferrer", "Josep", ""], ["Soria-Comas", "Jordi", ""]]}, {"id": "2010.11064", "submitter": "Heiko R\\\"oglin", "authors": "Heiko R\\\"oglin", "title": "Smoothed Analysis of Pareto Curves in Multiobjective Optimization", "comments": "Chapter 15 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multiobjective optimization problem a solution is called Pareto-optimal\nif no criterion can be improved without deteriorating at least one of the other\ncriteria. Computing the set of all Pareto-optimal solutions is a common task in\nmultiobjective optimization to filter out unreasonable trade-offs.\n  For most problems the number of Pareto-optimal solutions increases only\nmoderately with the input size in applications. However, for virtually every\nmultiobjective optimization problem there exist worst-case instances with an\nexponential number of Pareto-optimal solutions. In order to explain this\ndiscrepancy, we analyze a large class of multiobjective optimization problems\nin the model of smoothed analysis and prove a polynomial bound on the expected\nnumber of Pareto-optimal solutions.\n  We also present algorithms for computing the set of Pareto-optimal solutions\nfor different optimization problems and discuss related results on the smoothed\ncomplexity of optimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:11:32 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["R\u00f6glin", "Heiko", ""]]}, {"id": "2010.11252", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Jelani Nelson", "title": "On Adaptive Distance Estimation", "comments": "Minor correction in proof of Lemma B.6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a static data structure for distance estimation which supports\n{\\it adaptive} queries. Concretely, given a dataset $X = \\{x_i\\}_{i = 1}^n$ of\n$n$ points in $\\mathbb{R}^d$ and $0 < p \\leq 2$, we construct a randomized data\nstructure with low memory consumption and query time which, when later given\nany query point $q \\in \\mathbb{R}^d$, outputs a $(1+\\epsilon)$-approximation of\n$\\lVert q - x_i \\rVert_p$ with high probability for all $i\\in[n]$. The main\nnovelty is our data structure's correctness guarantee holds even when the\nsequence of queries can be chosen adaptively: an adversary is allowed to choose\nthe $j$th query point $q_j$ in a way that depends on the answers reported by\nthe data structure for $q_1,\\ldots,q_{j-1}$. Previous randomized Monte Carlo\nmethods do not provide error guarantees in the setting of adaptively chosen\nqueries. Our memory consumption is $\\tilde O((n+d)d/\\epsilon^2)$, slightly more\nthan the $O(nd)$ required to store $X$ in memory explicitly, but with the\nbenefit that our time to answer queries is only $\\tilde O(\\epsilon^{-2}(n +\nd))$, much faster than the naive $\\Theta(nd)$ time obtained from a linear scan\nin the case of $n$ and $d$ very large. Here $\\tilde O$ hides\n$\\log(nd/\\epsilon)$ factors. We discuss applications to nearest neighbor search\nand nonparametric estimation.\n  Our method is simple and likely to be applicable to other domains: we\ndescribe a generic approach for transforming randomized Monte Carlo data\nstructures which do not support adaptive queries to ones that do, and show that\nfor the problem at hand, it can be applied to standard nonadaptive solutions to\n$\\ell_p$ norm estimation with negligible overhead in query time and a factor\n$d$ overhead in memory.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 19:12:57 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 07:16:56 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Nelson", "Jelani", ""]]}, {"id": "2010.11381", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Jane Lange, Li-Yang Tan", "title": "Query strategies for priced information, revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing query strategies for priced information,\nintroduced by Charikar et al. In this problem the algorithm designer is given a\nfunction $f : \\{0,1\\}^n \\to \\{-1,1\\}$ and a price associated with each of the\n$n$ coordinates. The goal is to design a query strategy for determining $f$'s\nvalue on unknown inputs for minimum cost.\n  Prior works on this problem have focused on specific classes of functions. We\nanalyze a simple and natural strategy that applies to all functions $f$, and\nshow that its performance relative to the optimal strategy can be expressed in\nterms of a basic complexity measure of $f$, its influence. For $\\varepsilon \\in\n(0,\\frac1{2})$, writing $\\mathsf{opt}$ to denote the expected cost of the\noptimal strategy that errs on at most an $\\varepsilon$-fraction of inputs, our\nstrategy has expected cost $\\mathsf{opt} \\cdot \\mathrm{Inf}(f)/\\varepsilon^2$\nand also errs on at most an $O(\\varepsilon)$-fraction of inputs. This\nconnection yields new guarantees that complement existing ones for a number of\nfunction classes that have been studied in this context, as well as new\nguarantees for new classes.\n  Finally, we show that improving on the parameters that we achieve will\nrequire making progress on the longstanding open problem of properly learning\ndecision trees.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:10:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2010.11420", "submitter": "Benwei Wu", "authors": "Kai Han, Zongmai Cao, Shuang Cui, Benwei Wu", "title": "Deterministic Approximation for Submodular Maximization over a Matroid\n  in Nearly Linear Time", "comments": "Accepted to appear in the Thirty-Fourth Conference on Neural\n  Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a non-monotone, non-negative submodular\nfunction subject to a matroid constraint. The prior best-known deterministic\napproximation ratio for this problem is $\\frac{1}{4}-\\epsilon$ under\n$\\mathcal{O}(({n^4}/{\\epsilon})\\log n)$ time complexity. We show that this\ndeterministic ratio can be improved to $\\frac{1}{4}$ under $\\mathcal{O}(nr)$\ntime complexity, and then present a more practical algorithm dubbed\nTwinGreedyFast which achieves $\\frac{1}{4}-\\epsilon$ deterministic ratio in\nnearly-linear running time of\n$\\mathcal{O}(\\frac{n}{\\epsilon}\\log\\frac{r}{\\epsilon})$. Our approach is based\non a novel algorithmic framework of simultaneously constructing two candidate\nsolution sets through greedy search, which enables us to get improved\nperformance bounds by fully exploiting the properties of independence systems.\nAs a byproduct of this framework, we also show that TwinGreedyFast achieves\n$\\frac{1}{2p+2}-\\epsilon$ deterministic ratio under a $p$-set system constraint\nwith the same time complexity. To showcase the practicality of our approach, we\nempirically evaluated the performance of TwinGreedyFast on two network\napplications, and observed that it outperforms the state-of-the-art\ndeterministic and randomized algorithms with efficient implementations for our\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:52:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Han", "Kai", ""], ["Cao", "Zongmai", ""], ["Cui", "Shuang", ""], ["Wu", "Benwei", ""]]}, {"id": "2010.11440", "submitter": "Karolina Okrasa", "authors": "{\\L}ukasz Bo\\.zyk, Jan Derbisz, Tomasz Krawczyk, Jana Novotn\\'a and\n  Karolina Okrasa", "title": "Vertex deletion into bipartite permutation graphs", "comments": "Extended abstract accepted to International Symposium on\n  Parameterized and Exact Computation (IPEC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A permutation graph can be defined as an intersection graph of segments whose\nendpoints lie on two parallel lines $l_1$ and $l_2$, one on each. A bipartite\npermutation graph is a permutation graph which is bipartite. In this paper we\nstudy the parameterized complexity of the bipartite permutation vertex deletion\nproblem, which asks, for a given n-vertex graph, whether we can remove at most\nk vertices to obtain a bipartite permutation graph. This problem is NP-complete\nby the classical result of Lewis and Yannakakis. We analyze the structure of\nthe so-called almost bipartite permutation graphs which may contain holes\n(large induced cycles) in contrast to bipartite permutation graphs. We exploit\nthe structural properties of the shortest hole in a such graph. We use it to\nobtain an algorithm for the bipartite permutation vertex deletion problem with\nrunning time $O(9^k\\cdot n^9)$, and also give a polynomial-time 9-approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 04:46:13 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 08:44:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bo\u017cyk", "\u0141ukasz", ""], ["Derbisz", "Jan", ""], ["Krawczyk", "Tomasz", ""], ["Novotn\u00e1", "Jana", ""], ["Okrasa", "Karolina", ""]]}, {"id": "2010.11443", "submitter": "Alexander Wei", "authors": "Alexander Wei and Fred Zhang", "title": "Optimal Robustness-Consistency Trade-offs for Learning-Augmented Online\n  Algorithms", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of improving the performance of online algorithms by\nincorporating machine-learned predictions. The goal is to design algorithms\nthat are both consistent and robust, meaning that the algorithm performs well\nwhen predictions are accurate and maintains worst-case guarantees. Such\nalgorithms have been studied in a recent line of works due to Lykouris and\nVassilvitskii (ICML '18) and Purohit et al (NeurIPS '18). They provide\nrobustness-consistency trade-offs for a variety of online problems. However,\nthey leave open the question of whether these trade-offs are tight, i.e., to\nwhat extent to such trade-offs are necessary. In this paper, we provide the\nfirst set of non-trivial lower bounds for competitive analysis using\nmachine-learned predictions. We focus on the classic problems of ski-rental and\nnon-clairvoyant scheduling and provide optimal trade-offs in various settings.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 04:51:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wei", "Alexander", ""], ["Zhang", "Fred", ""]]}, {"id": "2010.11450", "submitter": "Emmanouil Zampetakis", "authors": "Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, Manolis\n  Zampetakis", "title": "Optimal Approximation -- Smoothness Tradeoffs for Soft-Max Functions", "comments": "Accepted for spotlight presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A soft-max function has two main efficiency measures: (1) approximation -\nwhich corresponds to how well it approximates the maximum function, (2)\nsmoothness - which shows how sensitive it is to changes of its input. Our goal\nis to identify the optimal approximation-smoothness tradeoffs for different\nmeasures of approximation and smoothness. This leads to novel soft-max\nfunctions, each of which is optimal for a different application. The most\ncommonly used soft-max function, called exponential mechanism, has optimal\ntradeoff between approximation measured in terms of expected additive\napproximation and smoothness measured with respect to R\\'enyi Divergence. We\nintroduce a soft-max function, called \"piecewise linear soft-max\", with optimal\ntradeoff between approximation, measured in terms of worst-case additive\napproximation and smoothness, measured with respect to $\\ell_q$-norm. The\nworst-case approximation guarantee of the piecewise linear mechanism enforces\nsparsity in the output of our soft-max function, a property that is known to be\nimportant in Machine Learning applications [Martins et al. '16, Laha et al.\n'18] and is not satisfied by the exponential mechanism. Moreover, the\n$\\ell_q$-smoothness is suitable for applications in Mechanism Design and Game\nTheory where the piecewise linear mechanism outperforms the exponential\nmechanism. Finally, we investigate another soft-max function, called power\nmechanism, with optimal tradeoff between expected \\textit{multiplicative}\napproximation and smoothness with respect to the R\\'enyi Divergence, which\nprovides improved theoretical and practical results in differentially private\nsubmodular optimization.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 05:19:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Epasto", "Alessandro", ""], ["Mahdian", "Mohammad", ""], ["Mirrokni", "Vahab", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2010.11456", "submitter": "Stefan Lendl", "authors": "Dennis Fischer and Tim A. Hartmann and Stefan Lendl and Gerhard J.\n  Woeginger", "title": "An Investigation of the Recoverable Robust Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the so-called recoverable robust assignment problem on\nbalanced bipartite graphs with $2n$ vertices, a mainstream problem in robust\noptimization: For two given linear cost functions $c_1$ and $c_2$ on the edges\nand a given integer $k$, the goal is to find two perfect matchings $M_1$ and\n$M_2$ that minimize the objective value $c_1(M_1)+c_2(M_2)$, subject to the\nconstraint that $M_1$ and $M_2$ have at least $k$ edges in common.\n  We derive a variety of results on this problem. First, we show that the\nproblem is W[1]-hard with respect to the parameter $k$, and also with respect\nto the recoverability parameter $k'=n-k$. This hardness result holds even in\nthe highly restricted special case where both cost functions $c_1$ and $c_2$\nonly take the values $0$ and $1$. (On the other hand, containment of the\nproblem in XP is straightforward to see.) Next, as a positive result we\nconstruct a polynomial time algorithm for the special case where one cost\nfunction is Monge, whereas the other one is Anti-Monge. Finally, we study the\nvariant where matching $M_1$ is frozen, and where the optimization goal is to\ncompute the best corresponding matching $M_2$, the second stage recoverable\nassignment problem. We show that this problem variant is contained in the\nrandomized parallel complexity class $\\text{RNC}_2$, and that it is at least as\nhard as the infamous problem \\probl{Exact Matching in Red-Blue Bipartite\nGraphs} whose computational complexity is a long-standing open problem\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 05:44:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fischer", "Dennis", ""], ["Hartmann", "Tim A.", ""], ["Lendl", "Stefan", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "2010.11462", "submitter": "Kazuhiro Kurita", "authors": "Yasuaki Kobayashi, Kazuhiro Kurita, Kunihiro Wasa", "title": "Linear-Delay Enumeration for Minimal Steiner Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kimelfeld and Sagiv [Kimelfeld and Sagiv, PODS 2006], [Kimelfeld and Sagiv,\nInf. Syst. 2008] pointed out the problem of enumerating $K$-fragments is of\ngreat importance in a keyword search on data graphs. In a graph-theoretic term,\nthe problem corresponds to enumerating minimal Steiner trees in (directed)\ngraphs. In this paper, we propose a linear-delay and polynomial-space algorithm\nfor enumerating all minimal Steiner trees, improving on a previous result in\n[Kimelfeld and Sagiv, Inf. Syst. 2008]. Our enumeration algorithm can be\nextended to other Steiner problems, such as minimal Steiner forests, minimal\nterminal Steiner trees, and minimal directed Steiner trees. As another variant\nof the minimal Steiner tree enumeration problem, we study the problem of\nenumerating minimal induced Steiner subgraphs. We propose a polynomial-delay\nand exponential-space enumeration algorithm of minimal induced Steiner\nsubgraphs on claw-free graphs. Contrary to these tractable results, we show\nthat the problem of enumerating minimal group Steiner trees is at least as hard\nas the minimal transversal enumeration problem on hypergraphs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:00:08 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 05:56:04 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kobayashi", "Yasuaki", ""], ["Kurita", "Kazuhiro", ""], ["Wasa", "Kunihiro", ""]]}, {"id": "2010.11497", "submitter": "Olivier Ruas", "authors": "George Giakkoupis (WIDE), Anne-Marie Kermarrec (EPFL), Olivier Ruas\n  (SPIRALS), Fran\\c{c}ois Ta\\\"iani (WIDE, IRISA)", "title": "Cluster-and-Conquer: When Randomness Meets Graph Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\nand machine-learning applications. Some of the most efficient KNN graph\nalgorithms are incremental and local: they start from a random graph, which\nthey incrementally improve by traversing neighbors-of-neighbors links.\nParadoxically, this random start is also one of the key weaknesses of these\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\naway according to the similarity metric. As a result, incremental algorithms\nmust first laboriously explore spurious potential neighbors before they can\nidentify similar nodes, and start converging. In this paper, we remove this\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\nthe starting configuration of greedy algorithms thanks to a novel lightweight\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\nextensive evaluation on real datasets shows that Cluster-and-Conquer\nsignificantly outperforms existing approaches, including LSH, yielding\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:31:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Giakkoupis", "George", "", "WIDE"], ["Kermarrec", "Anne-Marie", "", "EPFL"], ["Ruas", "Olivier", "", "SPIRALS"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE, IRISA"]]}, {"id": "2010.11620", "submitter": "Lida Kanari", "authors": "Lida Kanari, Ad\\'elie Garin and Kathryn Hess", "title": "From trees to barcodes and back again: theoretical and statistical\n  perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of topological data analysis have been successfully applied in a wide\nrange of fields to provide useful summaries of the structure of complex data\nsets in terms of topological descriptors, such as persistence diagrams. While\nthere are many powerful techniques for computing topological descriptors, the\ninverse problem, i.e., recovering the input data from topological descriptors,\nhas proved to be challenging. In this article we study in detail the\nTopological Morphology Descriptor (TMD), which assigns a persistence diagram to\nany tree embedded in Euclidean space, and a sort of stochastic inverse to the\nTMD, the Topological Neuron Synthesis (TNS) algorithm, gaining both theoretical\nand computational insights into the relation between the two. We propose a new\napproach to classify barcodes using symmetric groups, which provides a concrete\nlanguage to formulate our results. We investigate to what extent the TNS\nrecovers a geometric tree from its TMD and describe the effect of different\ntypes of noise on the process of tree generation from persistence diagrams. We\nprove moreover that the TNS algorithm is stable with respect to specific types\nof noise.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:34:00 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 14:30:13 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kanari", "Lida", ""], ["Garin", "Ad\u00e9lie", ""], ["Hess", "Kathryn", ""]]}, {"id": "2010.11629", "submitter": "Etienne Bamas", "authors": "\\'Etienne Bamas, Andreas Maggiori, Lars Rohwedder, Ola Svensson", "title": "Learning Augmented Energy Minimization via Speed Scaling", "comments": "30 pages, 4 figures. To appear in NeurIPS 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As power management has become a primary concern in modern data centers,\ncomputing resources are being scaled dynamically to minimize energy\nconsumption. We initiate the study of a variant of the classic online speed\nscaling problem, in which machine learning predictions about the future can be\nintegrated naturally. Inspired by recent work on learning-augmented online\nalgorithms, we propose an algorithm which incorporates predictions in a\nblack-box manner and outperforms any online algorithm if the accuracy is high,\nyet maintains provable guarantees if the prediction is very inaccurate. We\nprovide both theoretical and experimental evidence to support our claims.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:58:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Bamas", "\u00c9tienne", ""], ["Maggiori", "Andreas", ""], ["Rohwedder", "Lars", ""], ["Svensson", "Ola", ""]]}, {"id": "2010.11632", "submitter": "\\'Etienne Bamas", "authors": "\\'Etienne Bamas, Andreas Maggiori, Ola Svensson", "title": "The Primal-Dual method for Learning Augmented Algorithms", "comments": "30 pages, 11 figures. To appear in NeurIPS 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extension of classical online algorithms when provided with predictions\nis a new and active research area. In this paper, we extend the primal-dual\nmethod for online algorithms in order to incorporate predictions that advise\nthe online algorithm about the next action to take. We use this framework to\nobtain novel algorithms for a variety of online covering problems. We compare\nour algorithms to the cost of the true and predicted offline optimal solutions\nand show that these algorithms outperform any online algorithm when the\nprediction is accurate while maintaining good guarantees when the prediction is\nmisleading.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:58:47 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Bamas", "\u00c9tienne", ""], ["Maggiori", "Andreas", ""], ["Svensson", "Ola", ""]]}, {"id": "2010.11880", "submitter": "Renchi Yang Dr.", "authors": "Renchi Yang", "title": "Fast Approximate CoSimRanks via Random Projections", "comments": "Some mistakes in the papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G with n nodes, and two nodes u,v in G, the CoSim-Rank value\ns(u,v) quantifies the similarity between u and v based on graph topology.\nCompared to SimRank, CoSimRank has been shown to be more accurate and effective\nin many real-world applications including synonym expansion, lexicon\nextraction, and entity relatedness in knowledge graphs. The computation of\nall-pair CoSimRank values in G is highly expensive and challenging. Existing\nmethods all focus on devising approximate algorithms for the computation of\nall-pair CoSimRanks. To attain the desired absolute error delta, the\nstate-of-the-art approximate algorithm for computing all-pair CoSimRank values\nrequires O(n^3log2(ln(1/delta))) time. In this paper, we propose RP-CoSim, a\nrandomized algorithm for computing all-pair CoSimRank values. The basic idea of\nRP-CoSim is to reduce the n*n matrix multiplications into a k-dimensional(k<<n)\nsubspace via a random projection such that the pairwise inner product is\npreserved within a certain error, and then iteratively approximate CoSimRank\nvalues in the k-dimensional subspace in O(n^2k) time. Theoretically,\nRP-CoSimruns in O(n^2*ln(n)*ln(1/delta)/delta^2) time, and meanwhile ensures an\nabsolute error of at most delta in the CoSimRank value of every two nodes in G\nwith a high probability.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:17:28 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 03:22:13 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yang", "Renchi", ""]]}, {"id": "2010.11925", "submitter": "Aravind Gollakota", "authors": "Aravind Gollakota, Sushrut Karmalkar, Adam Klivans", "title": "The Polynomial Method is Universal for Distribution-Free Correlational\n  SQ Learning", "comments": "We were informed that many of these results can be obtained by\n  combining prior works due to Feldman and Sherstov", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distribution-free learning for Boolean function\nclasses in the PAC and agnostic models. Generalizing a recent beautiful work of\nMalach and Shalev-Shwartz (2020) who gave the first tight correlational SQ\n(CSQ) lower bounds for learning DNF formulas, we show that lower bounds on the\nthreshold or approximate degree of any function class directly imply CSQ lower\nbounds for PAC or agnostic learning respectively. These match corresponding\npositive results using upper bounds on the threshold or approximate degree in\nthe SQ model for PAC or agnostic learning. Many of these results were implicit\nin earlier works of Feldman and Sherstov.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:55:26 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:25:29 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Gollakota", "Aravind", ""], ["Karmalkar", "Sushrut", ""], ["Klivans", "Adam", ""]]}, {"id": "2010.11981", "submitter": "Luis Miralles PHD", "authors": "Luis Miralles-Pechu\\'an and Fernando Jim\\'enez and Jos\\'e Manuel\n  Garc\\'ia", "title": "A novel auction system for selecting advertisements in Real-Time bidding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-Time Bidding is a new Internet advertising system that has become very\npopular in recent years. This system works like a global auction where\nadvertisers bid to display their impressions in the publishers' ad slots. The\nmost popular system to select which advertiser wins each auction is the\nGeneralized second-price auction in which the advertiser that offers the most\nwins the bet and is charged with the price of the second largest bet. In this\npaper, we propose an alternative betting system with a new approach that not\nonly considers the economic aspect but also other relevant factors for the\nfunctioning of the advertising system. The factors that we consider are, among\nothers, the benefit that can be given to each advertiser, the probability of\nconversion from the advertisement, the probability that the visit is\nfraudulent, how balanced are the networks participating in RTB and if the\nadvertisers are not paying over the market price. In addition, we propose a\nmethodology based on genetic algorithms to optimize the selection of each\nadvertiser. We also conducted some experiments to compare the performance of\nthe proposed model with the famous Generalized Second-Price method. We think\nthat this new approach, which considers more relevant aspects besides the\nprice, offers greater benefits for RTB networks in the medium and long-term.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:36:41 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Miralles-Pechu\u00e1n", "Luis", ""], ["Jim\u00e9nez", "Fernando", ""], ["Garc\u00eda", "Jos\u00e9 Manuel", ""]]}, {"id": "2010.12000", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis, Themis Gouleakis, Christos Tzamos, Manolis\n  Zampetakis", "title": "Computationally and Statistically Efficient Truncated Regression", "comments": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computationally and statistically efficient estimator for the\nclassical problem of truncated linear regression, where the dependent variable\n$y = w^T x + \\epsilon$ and its corresponding vector of covariates $x \\in R^k$\nare only revealed if the dependent variable falls in some subset $S \\subseteq\nR$; otherwise the existence of the pair $(x, y)$ is hidden. This problem has\nremained a challenge since the early works of [Tobin 1958, Amemiya 1973,\nHausman and Wise 1977], its applications are abundant, and its history dates\nback even further to the work of Galton, Pearson, Lee, and Fisher. While\nconsistent estimators of the regression coefficients have been identified, the\nerror rates are not well-understood, especially in high dimensions.\n  Under a thickness assumption about the covariance matrix of the covariates in\nthe revealed sample, we provide a computationally efficient estimator for the\ncoefficient vector $w$ from $n$ revealed samples that attains $l_2$ error\n$\\tilde{O}(\\sqrt{k/n})$. Our estimator uses Projected Stochastic Gradient\nDescent (PSGD) without replacement on the negative log-likelihood of the\ntruncated sample. For the statistically efficient estimation we only need\noracle access to the set $S$.In order to achieve computational efficiency we\nneed to assume that $S$ is a union of a finite number of intervals but still\ncan be complicated. PSGD without replacement must be restricted to an\nappropriately defined convex cone to guarantee that the negative log-likelihood\nis strongly convex, which in turn is established using concentration of\nmatrices on variables with sub-exponential tails. We perform experiments on\nsimulated data to illustrate the accuracy of our estimator.\n  As a corollary, we show that SGD learns the parameters of single-layer neural\nnetworks with noisy activation functions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:31:30 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2010.12122", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall and Saeed Seddighin", "title": "Quantum Meets Fine-grained Complexity: Sublinear Time Quantum Algorithms\n  for String Problems", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longest common substring (LCS), longest palindrome substring (LPS), and Ulam\ndistance (UL) are three fundamental string problems that can be classically\nsolved in near linear time. In this work, we present sublinear time quantum\nalgorithms for these problems along with quantum lower bounds. Our results shed\nlight on a very surprising fact: Although the classic solutions for LCS and LPS\nare almost identical (via suffix trees), their quantum computational\ncomplexities are different. While we give an exact $\\tilde O(\\sqrt{n})$ time\nalgorithm for LPS, we prove that LCS needs at least time $\\tilde\n\\Omega(n^{2/3})$ even for 0/1 strings.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:00:50 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Seddighin", "Saeed", ""]]}, {"id": "2010.12397", "submitter": "Paul Wollan", "authors": "Ken-ichi Kawarabayashi, Robin Thomas, Paul Wollan", "title": "Quickly excluding a non-planar graph", "comments": "97 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cornerstone theorem in the Graph Minors series of Robertson and Seymour is\nthe result that every graph $G$ with no minor isomorphic to a fixed graph $H$\nhas a certain structure. The structure can then be exploited to deduce\nfar-reaching consequences. The exact statement requires some explanation, but\nroughly it says that there exist integers $k,n$ depending on $H$ only such that\n$0<k<n$ and for every $n\\times n$ grid minor $J$ of $G$ the graph $G$ has a a\n$k$-near embedding in a surface $\\Sigma$ that does not embed $H$ in such a way\nthat a substantial part of $J$ is embedded in $\\Sigma$. Here a $k$-near\nembedding means that after deleting at most $k$ vertices the graph can be drawn\nin $\\Sigma$ without crossings, except for local areas of non-planarity, where\ncrossings are permitted, but at most $k$ of these areas are attached to the\nrest of the graph by four or more vertices and inside those the graph is\nconstrained in a different way, again depending on the parameter $k$.\n  The original and only proof so far is quite long and uses many results\ndeveloped in the Graph Minors series. We give a proof that uses only our\nearlier paper [A new proof of the flat wall theorem, {\\it J.~Combin.\\ Theory\nSer.\\ B \\bf 129} (2018), 158--203] and results from graduate textbooks.\n  Our proof is constructive and yields a polynomial time algorithm to construct\nsuch a structure. We also give explicit constants for the structure theorem,\nwhereas the original proof only guarantees the existence of such constants.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:32:54 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 12:01:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Thomas", "Robin", ""], ["Wollan", "Paul", ""]]}, {"id": "2010.12633", "submitter": "Seyyid Emre Sofuoglu", "authors": "Seyyid Emre Sofuoglu and Selin Aviyente", "title": "Low-rank on Graphs plus Temporally Smooth Sparse Decomposition for\n  Anomaly Detection in Spatiotemporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in spatiotemporal data is a challenging problem encountered\nin a variety of applications including hyperspectral imaging, video\nsurveillance, and urban traffic monitoring. Existing anomaly detection methods\nare most suited for point anomalies in sequence data and cannot deal with\ntemporal and spatial dependencies that arise in spatiotemporal data. In recent\nyears, tensor-based methods have been proposed for anomaly detection to address\nthis problem. These methods rely on conventional tensor decomposition models,\nnot taking the structure of the anomalies into account, and are supervised or\nsemi-supervised. We introduce an unsupervised tensor-based anomaly detection\nmethod that takes the sparse and temporally continuous nature of anomalies into\naccount. In particular, the anomaly detection problem is formulated as a robust\nlowrank + sparse tensor decomposition with a regularization term that minimizes\nthe temporal variation of the sparse part, so that the extracted anomalies are\ntemporally persistent. We also approximate rank minimization with graph total\nvariation minimization to reduce the complexity of the optimization algorithm.\nThe resulting optimization problem is convex, scalable, and is shown to be\nrobust against missing data and noise. The proposed framework is evaluated on\nboth synthetic and real spatiotemporal urban traffic data and compared with\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:34:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sofuoglu", "Seyyid Emre", ""], ["Aviyente", "Selin", ""]]}, {"id": "2010.12816", "submitter": "Sebastian Perez-Salazar", "authors": "Sebastian Perez-Salazar, Rachel Cummings", "title": "Differentially Private Online Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of online submodular maximization under\na cardinality constraint with differential privacy (DP). A stream of $T$\nsubmodular functions over a common finite ground set $U$ arrives online, and at\neach time-step the decision maker must choose at most $k$ elements of $U$\nbefore observing the function. The decision maker obtains a payoff equal to the\nfunction evaluated on the chosen set, and aims to learn a sequence of sets that\nachieves low expected regret. In the full-information setting, we develop an\n$(\\varepsilon,\\delta)$-DP algorithm with expected $(1-1/e)$-regret bound of\n$\\mathcal{O}\\left( \\frac{k^2\\log |U|\\sqrt{T \\log k/\\delta}}{\\varepsilon}\n\\right)$. This algorithm contains $k$ ordered experts that learn the best\nmarginal increments for each item over the whole time horizon while maintaining\nprivacy of the functions. In the bandit setting, we provide an\n$(\\varepsilon,\\delta+ O(e^{-T^{1/3}}))$-DP algorithm with expected\n$(1-1/e)$-regret bound of $\\mathcal{O}\\left( \\frac{\\sqrt{\\log\nk/\\delta}}{\\varepsilon} (k (|U| \\log |U|)^{1/3})^2 T^{2/3} \\right)$. Our\nalgorithms contains $k$ ordered experts that learn the best marginal item to\nselect given the items chosen her predecessors, while maintaining privacy of\nthe functions. One challenge for privacy in this setting is that the payoff and\nfeedback of expert $i$ depends on the actions taken by her $i-1$ predecessors.\nThis particular type of information leakage is not covered by post-processing,\nand new analysis is required. Our techniques for maintaining privacy with\nfeedforward may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:23:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Perez-Salazar", "Sebastian", ""], ["Cummings", "Rachel", ""]]}, {"id": "2010.12918", "submitter": "Yash Pote", "authors": "Kuldeep S. Meel, Yash Pote, Sourav Chakraborty", "title": "On Testing of Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of items $\\mathcal{F}$ and a weight function $\\mathtt{wt}:\n\\mathcal{F} \\mapsto (0,1)$, the problem of sampling seeks to sample an item\nproportional to its weight. Sampling is a fundamental problem in machine\nlearning. The daunting computational complexity of sampling with formal\nguarantees leads designers to propose heuristics-based techniques for which no\nrigorous theoretical analysis exists to quantify the quality of generated\ndistributions.\n  This poses a challenge in designing a testing methodology to test whether a\nsampler under test generates samples according to a given distribution. Only\nrecently, Chakraborty and Meel (2019) designed the first scalable verifier,\ncalled Barbarik1, for samplers in the special case when the weight function\n$\\mathtt{wt}$ is constant, that is, when the sampler is supposed to sample\nuniformly from $\\mathcal{F}$ . The techniques in Barbarik1, however, fail to\nhandle general weight functions.\n  The primary contribution of this paper is an affirmative answer to the above\nchallenge: motivated by Barbarik1 but using different techniques and analysis,\nwe design Barbarik2 an algorithm to test whether the distribution generated by\na sampler is $\\varepsilon$-close or $\\eta$-far from any target distribution. In\ncontrast to black-box sampling techniques that require a number of samples\nproportional to $|\\mathcal{F}|$ , Barbarik2 requires only\n$\\tilde{O}(tilt(\\mathtt{wt},\\varphi)^2/\\eta(\\eta - 6\\varepsilon)^3)$ samples,\nwhere the $tilt$ is the maximum ratio of weights of two satisfying assignments.\nBarbarik2 can handle any arbitrary weight function. We present a prototype\nimplementation of Barbarik2 and use it to test three state-of-the-art samplers.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:42:34 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Meel", "Kuldeep S.", ""], ["Pote", "Yash", ""], ["Chakraborty", "Sourav", ""]]}, {"id": "2010.13042", "submitter": "Baisakh Baisakh", "authors": "Baisakh, Rakesh Mohanty", "title": "A Novel Move-To-Front-or-Logarithmic Position (MFLP) Online List Update\n  Algorithm", "comments": "11 pages, 2 figures, accepted in International Conference on Discrete\n  Mathematics and its Applications to Network Sciences (ICDMANS) 2018, BITS,\n  Pilani, K.K Birla Campus, Goa, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel online deterministic list update algorithm\nknown as Move-To-Front-or-Logarithmic Position (MFLP). Our proposed algorithm\nMFLP achieves a competitive ratio of 2 for larger list with respect to static\noptimum offline algorithm, whereas MFLP is not competitive for smaller list. We\nalso show that MFLP is 2 competitive with respect to dynamic optimum offline\nalgorithm. Our results open up a new direction of work towards classifying the\nonline algorithms based on competitive and not competitive.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 05:32:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Baisakh", "", ""], ["Mohanty", "Rakesh", ""]]}, {"id": "2010.13048", "submitter": "Edith Cohen", "authors": "Edith Cohen, Ofir Geri, Tamas Sarlos, Uri Stemmer", "title": "Differentially Private Weighted Sampling", "comments": "38 pages, 9 figures", "journal-ref": "AISTATS 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common datasets have the form of elements with keys (e.g., transactions and\nproducts) and the goal is to perform analytics on the aggregated form of key\nand frequency pairs. A weighted sample of keys by (a function of) frequency is\na highly versatile summary that provides a sparse set of representative keys\nand supports approximate evaluations of query statistics. We propose private\nweighted sampling (PWS): A method that ensures element-level differential\nprivacy while retaining, to the extent possible, the utility of a respective\nnon-private weighted sample. PWS maximizes the reporting probabilities of keys\nand estimation quality of a broad family of statistics. PWS improves over the\nstate of the art also for the well-studied special case of private histograms,\nwhen no sampling is performed. We empirically demonstrate significant\nperformance gains compared with prior baselines: 20%-300% increase in key\nreporting for common Zipfian frequency distributions and accuracy for $\\times\n2$-$ 8$ lower frequencies in estimation tasks. Moreover, PWS is applied as a\nsimple post-processing of a non-private sample, without requiring the original\ndata. This allows for seamless integration with existing implementations of\nnon-private schemes and retaining the efficiency of schemes designed for\nresource-constrained settings such as massive distributed or streamed data. We\nbelieve that due to practicality and performance, PWS may become a method of\nchoice in applications where privacy is desired.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 06:54:09 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 06:24:54 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 16:55:09 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Cohen", "Edith", ""], ["Geri", "Ofir", ""], ["Sarlos", "Tamas", ""], ["Stemmer", "Uri", ""]]}, {"id": "2010.13143", "submitter": "Gopinath Mishra", "authors": "Anup Bhattacharya, Arijit Bishnu, Gopinath Mishra and Anannya Upasana", "title": "Even the Easiest(?) Graph Coloring Problem is not Easy in Streaming!", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a graph coloring problem that is otherwise easy but becomes quite\nnon-trivial in the one-pass streaming model. In contrast to previous graph\ncoloring problems in streaming that try to find an assignment of colors to\nvertices, our main work is on estimating the number of conflicting or\nmonochromatic edges given a coloring function that is streaming along with the\ngraph; we call the problem {\\sc Conflict-Est}. The coloring function on a\nvertex can be read or accessed only when the vertex is revealed in the stream.\nIf we need the color on a vertex that has streamed past, then that color, along\nwith its vertex, has to be stored explicitly. We provide algorithms for a graph\nthat is streaming in different variants of the one-pass vertex arrival\nstreaming model, viz. the {\\sc Vertex Arrival} ({\\sc VA}), {Vertex Arrival With\nDegree Oracle} ({\\sc VAdeg}), {\\sc Vertex Arrival in Random Order} ({\\sc\nVArand}) models, with special focus on the random order model. We also provide\nmatching lower bounds for most of the cases. The mainstay of our work is in\nshowing that the properties of a random order stream can be exploited to design\nstreaming algorithms for estimating the number of conflicting edges. We have\nalso obtained a lower bound, though not matching the upper bound, for the\nrandom order model. Among all the three models vis-a-vis this problem, we can\nshow a clear separation of power in favor of the {\\sc VArand} model.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 15:50:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Bishnu", "Arijit", ""], ["Mishra", "Gopinath", ""], ["Upasana", "Anannya", ""]]}, {"id": "2010.13170", "submitter": "Ce Jin", "authors": "Ce Jin, Jelani Nelson, Kewen Wu", "title": "An Improved Sketching Algorithm for Edit Distance", "comments": "Appeared in STACS 2021. Fixed the title to match the conference\n  version", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2021.45", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide improved upper bounds for the simultaneous sketching complexity of\nedit distance. Consider two parties, Alice with input $x\\in\\Sigma^n$ and Bob\nwith input $y\\in\\Sigma^n$, that share public randomness and are given a promise\nthat the edit distance $\\mathsf{ed}(x,y)$ between their two strings is at most\nsome given value $k$. Alice must send a message $sx$ and Bob must send $sy$ to\na third party Charlie, who does not know the inputs but shares the same public\nrandomness and also knows $k$. Charlie must output $\\mathsf{ed}(x,y)$ precisely\nas well as a sequence of $\\mathsf{ed}(x,y)$ edits required to transform $x$\ninto $y$. The goal is to minimize the lengths $|sx|, |sy|$ of the messages\nsent.\n  The protocol of Belazzougui and Zhang (FOCS 2016), building upon the random\nwalk method of Chakraborty, Goldenberg, and Kouck\\'y (STOC 2016), achieves a\nmaximum message length of $\\tilde O(k^8)$ bits, where $\\tilde O(\\cdot)$ hides\n$\\mathrm{poly}(\\log n)$ factors. In this work we build upon Belazzougui and\nZhang's protocol and provide an improved analysis demonstrating that a slight\nmodification of their construction achieves a bound of $\\tilde O(k^3)$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 17:35:05 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 03:50:06 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 09:00:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jin", "Ce", ""], ["Nelson", "Jelani", ""], ["Wu", "Kewen", ""]]}, {"id": "2010.13180", "submitter": "Jason Yang", "authors": "Jason Yang, Jun Wan", "title": "On Updating and Querying Submatrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the $d$-dimensional update-query problem. We provide\nlower bounds on update and query running times, assuming a long-standing\nconjecture on min-plus matrix multiplication, as well as algorithms that are\nclose to the lower bounds. Given a $d$-dimensional matrix, an \\textit{update}\nchanges each element in a given submatrix from $x$ to $x\\bigtriangledown v$,\nwhere $v$ is a given constant. A \\textit{query} returns the $\\bigtriangleup$ of\nall elements in a given submatrix. We study the cases where $\\bigtriangledown$\nand $\\bigtriangleup$ are both commutative and associative binary operators.\nWhen $d = 1$, updates and queries can be performed in $O(\\log N)$ worst-case\ntime for many $(\\bigtriangledown,\\bigtriangleup)$ by using a segment tree with\nlazy propagation. However, when $d\\ge 2$, similar techniques usually cannot be\ngeneralized. We show that if min-plus matrix multiplication cannot be computed\nin $O(N^{3-\\varepsilon})$ time for any $\\varepsilon>0$ (which is widely\nbelieved to be the case), then for\n$(\\bigtriangledown,\\bigtriangleup)=(+,\\min)$, either updates or queries cannot\nboth run in $O(N^{1-\\varepsilon})$ time for any constant $\\varepsilon>0$, or\npreprocessing cannot run in polynomial time. Finally, we show a special case\nwhere lazy propagation can be generalized for $d\\ge 2$ and where updates and\nqueries can run in $O(\\log^d N)$ worst-case time. We present an algorithm that\nmeets this running time and is simpler than similar algorithms of previous\nworks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 18:17:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yang", "Jason", ""], ["Wan", "Jun", ""]]}, {"id": "2010.13207", "submitter": "Tytus Pikies", "authors": "Tytus Pikies, Krzysztof Turowski, Marek Kubale", "title": "Scheduling with Complete Multipartite Incompatibility Graph on Parallel\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of scheduling on parallel machines with\na presence of incompatibilities between jobs. The incompatibility relation can\nbe modeled as a complete multipartite graph in which each edge denotes a pair\nof jobs that cannot be scheduled on the same machine. Our research stems from\nthe work of Bodlaender et al.~[1992, 1993]. In particular, we pursue the line\ninvestigated partially by Mallek et al.~[2019], where the graph is complete\nmultipartite so each machine can do jobs only from one partition. We also tie\nour results to the recent approach for so-called identical machines with class\nconstraints by Jansen et al.~[2019], providing a link between our case and\ntheir generalization.\n  In the paper we provide several algorithms constructing schedules, optimal or\napproximate with respect to the two most popular criteria of optimality: Cmax\n(the makespan) and {\\Sigma}Cj(the total completion time). We consider a variety\nof machine types in our paper: identical, uniform, unrelated, and a natural\nsubcase of unrelated machines. Our results consist of delimitation of the easy\n(polynomial) and NP-hard problems within these constraints. In the case when\nthe problem is hard, we also provide algorithm, either with a guaranteed\nconstant worst-case approximation ratio or even in some cases a PTAS.\n  In particular, we fill the gap on research for the problem of finding a\nschedule with smallest total completion time on uniform machines. We address\nthis problem by developing a linear programming relaxation technique with an\nappropriate rounding, which to our knowledge is a novelty for this criterion in\nthe considered setting.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 20:08:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Pikies", "Tytus", ""], ["Turowski", "Krzysztof", ""], ["Kubale", "Marek", ""]]}, {"id": "2010.13717", "submitter": "Thomas Mu\\~noz", "authors": "Floris Geerts, Thomas Mu\\~noz, Cristian Riveros and Domagoj Vrgo\\v{c}", "title": "Expressive power of linear algebra query languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear algebra algorithms often require some sort of iteration or recursion\nas is illustrated by standard algorithms for Gaussian elimination, matrix\ninversion, and transitive closure. A key characteristic shared by these\nalgorithms is that they allow looping for a number of steps that is bounded by\nthe matrix dimension. In this paper we extend the matrix query language MATLANG\nwith this type of recursion, and show that this suffices to express classical\nlinear algebra algorithms. We study the expressive power of this language and\nshow that it naturally corresponds to arithmetic circuit families, which are\noften said to capture linear algebra. Furthermore, we analyze several\nsub-fragments of our language, and show that their expressive power is closely\ntied to logical formalisms on semiring-annotated relations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:59:09 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Geerts", "Floris", ""], ["Mu\u00f1oz", "Thomas", ""], ["Riveros", "Cristian", ""], ["Vrgo\u010d", "Domagoj", ""]]}, {"id": "2010.13888", "submitter": "Jan van den Brand", "authors": "Jan van den Brand", "title": "Unifying Matrix Data Structures: Simplifying and Speeding up Iterative\n  Algorithms", "comments": "SOSA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms use data structures that maintain properties of matrices\nundergoing some changes. The applications are wide-ranging and include for\nexample matchings, shortest paths, linear programming, semi-definite\nprogramming, convex hull and volume computation. Given the wide range of\napplications, the exact property these data structures must maintain varies\nfrom one application to another, forcing algorithm designers to invent them\nfrom scratch or modify existing ones. Thus it is not surprising that these data\nstructures and their proofs are usually tailor-made for their specific\napplication and that maintaining more complicated properties results in more\ncomplicated proofs.\n  In this paper we present a unifying framework that captures a wide range of\nthese data structures. The simplicity of this framework allows us to give short\nproofs for many existing data structures regardless of how complicated the to\nbe maintained property is. We also show how the framework can be used to speed\nup existing iterative algorithms, such as the simplex algorithm.\n  More formally, consider any rational function $f(A_1,...,A_d)$ with input\nmatrices $A_1,...,A_d$. We show that the task of maintaining $f(A_1,...,A_d)$\nunder updates to $A_1,...,A_d$ can be reduced to the much simpler problem of\nmaintaining some matrix inverse $M^{-1}$ under updates to $M$. The latter is a\nwell studied problem called dynamic matrix inverse. By applying our reduction\nand using known algorithms for dynamic matrix inverse we can obtain fast data\nstructures and iterative algorithms for much more general problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 20:28:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Brand", "Jan van den", ""]]}, {"id": "2010.13949", "submitter": "Elfarouk Harb", "authors": "Elfarouk Harb and Ho Shan Lam", "title": "KFC: A Scalable Approximation Algorithm for $k$-center Fair Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of fair clustering on the $k-$center\nobjective. In fair clustering, the input is $N$ points, each belonging to at\nleast one of $l$ protected groups, e.g. male, female, Asian, Hispanic. The\nobjective is to cluster the $N$ points into $k$ clusters to minimize a\nclassical clustering objective function. However, there is an additional\nconstraint that each cluster needs to be fair, under some notion of fairness.\nThis ensures that no group is either \"over-represented\" or \"under-represented\"\nin any cluster. Our work builds on the work of Chierichetti et al. (NIPS 2017),\nBera et al. (NeurIPS 2019), Ahmadian et al. (KDD 2019), and Bercea et al.\n(APPROX 2019). We obtain a randomized $3-$approximation algorithm for the\n$k-$center objective function, beating the previous state of the art\n($4-$approximation). We test our algorithm on real datasets, and show that our\nalgorithm is effective in finding good clusters without over-representation or\nunder-representation, surpassing the current state of the art in runtime speed,\nclustering cost, while achieving similar fairness violations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:33:34 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 08:33:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Harb", "Elfarouk", ""], ["Lam", "Ho Shan", ""]]}, {"id": "2010.14058", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Nesreen K. Ahmed, Aldo Carranza, David Arbour, Anup\n  Rao, Sungchul Kim, Eunyee Koh", "title": "Heterogeneous Graphlets", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.10026", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a generalization of graphlets to heterogeneous\nnetworks called typed graphlets. Informally, typed graphlets are small typed\ninduced subgraphs. Typed graphlets generalize graphlets to rich heterogeneous\nnetworks as they explicitly capture the higher-order typed connectivity\npatterns in such networks. To address this problem, we describe a general\nframework for counting the occurrences of such typed graphlets. The proposed\nalgorithms leverage a number of combinatorial relationships for different typed\ngraphlets. For each edge, we count a few typed graphlets, and with these counts\nalong with the combinatorial relationships, we obtain the exact counts of the\nother typed graphlets in o(1) constant time. Notably, the worst-case time\ncomplexity of the proposed approach matches the time complexity of the best\nknown untyped algorithm. In addition, the approach lends itself to an efficient\nlock-free and asynchronous parallel implementation. While there are no existing\nmethods for typed graphlets, there has been some work that focused on computing\na different and much simpler notion called colored graphlet. The experiments\nconfirm that our proposed approach is orders of magnitude faster and more\nspace-efficient than methods for computing the simpler notion of colored\ngraphlet. Unlike these methods that take hours on small networks, the proposed\napproach takes only seconds on large networks with millions of edges. Notably,\nsince typed graphlet is more general than colored graphlet (and untyped\ngraphlets), the counts of various typed graphlets can be combined to obtain the\ncounts of the much simpler notion of colored graphlets. The proposed methods\ngive rise to new opportunities and applications for typed graphlets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:22:55 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Carranza", "Aldo", ""], ["Arbour", "David", ""], ["Rao", "Anup", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""]]}, {"id": "2010.14189", "submitter": "Roy Friedman", "authors": "Dolev Adas and Roy Friedman", "title": "Jiffy: A Fast, Memory Efficient, Wait-Free Multi-Producers\n  Single-Consumer Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as sharded data processing systems, sharded in-memory\nkey-value stores, data flow programming and load sharing applications, multiple\nconcurrent data producers are feeding requests into the same data consumer.\nThis can be naturally realized through concurrent queues, where each consumer\npulls its tasks from its dedicated queue. For scalability, wait-free queues are\noften preferred over lock based structures.\n  The vast majority of wait-free queue implementations, and even lock-free\nones, support the multi-producer multi-consumer model. Yet, this comes at a\npremium, since implementing wait-free multi-producer multi-consumer queues\nrequires utilizing complex helper data structures. The latter increases the\nmemory consumption of such queues and limits their performance and scalability.\nAdditionally, many such designs employ (hardware) cache unfriendly memory\naccess patterns.\n  In this work we study the implementation of wait-free multi-producer\nsingle-consumer queues. Specifically, we propose Jiffy, an efficient memory\nfrugal novel wait-free multi-producer single-consumer queue and formally prove\nits correctness. We then compare the performance and memory requirements of\nJiffy with other state of the art lock-free and wait-free queues. We show that\nindeed Jiffy can maintain good performance with up to 128 threads, delivers up\nto 50% better throughput than the next best construction we compared against,\nand consumes ~90% less memory.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:51:05 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 10:42:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Adas", "Dolev", ""], ["Friedman", "Roy", ""]]}, {"id": "2010.14338", "submitter": "Lukas N\\\"olke", "authors": "Antonios Antoniadis, Margarita Capretto, Parinya Chalermsook,\n  Christoph Damerius, Peter Kling, Lukas N\\\"olke, Nidia Obscura, Joachim\n  Spoerhase", "title": "On Minimum Generalized Manhattan Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider minimum-cardinality Manhattan connected sets with arbitrary\ndemands: Given a collection of points $P$ in the plane, together with a subset\nof pairs of points in $P$ (which we call demands), find a minimum-cardinality\nsuperset of $P$ such that every demand pair is connected by a path whose length\nis the $\\ell_1$-distance of the pair. This problem is a variant of three\nwell-studied problems that have arisen in computational geometry, data\nstructures, and network design: (i) It is a node-cost variant of the classical\nManhattan network problem, (ii) it is an extension of the binary search tree\nproblem to arbitrary demands, and (iii) it is a special case of the directed\nSteiner forest problem. Since the problem inherits basic structural properties\nfrom the context of binary search trees, an $O(\\log n)$-approximation is\ntrivial. We show that the problem is NP-hard and present an $O(\\sqrt{\\log\nn})$-approximation algorithm. Moreover, we provide an $O(\\log\\log\nn)$-approximation algorithm for complete bipartite demands as well as improved\nresults for unit-disk demands and several generalizations. Our results\ncrucially rely on a new lower bound on the optimal cost that could potentially\nbe useful in the context of BSTs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:54:13 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Capretto", "Margarita", ""], ["Chalermsook", "Parinya", ""], ["Damerius", "Christoph", ""], ["Kling", "Peter", ""], ["N\u00f6lke", "Lukas", ""], ["Obscura", "Nidia", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "2010.14367", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle", "title": "Simultaenous Sieves: A Deterministic Streaming Algorithm for\n  Non-Monotone Submodular Maximization", "comments": "Withdrawn as essentially the same deterministic algorithm, with the\n  same ratio, was developed in the journal version of the paper: Alaluf et al.\n  Optimal Streaming Algorithms for Submodular Maximization with Cardinality\n  Constraints, submitted to Mathematics of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a combinatorial, deterministic single-pass streaming\nalgorithm for the problem of maximizing a submodular function, not necessarily\nmonotone, with respect to a cardinality constraint (SMCC). In the case the\nfunction is monotone, our algorithm reduces to the optimal streaming algorithm\nof Badanidiyuru et al. (2014). In general, our algorithm achieves ratio $\\alpha\n/ (1 + \\alpha) - \\varepsilon$, for any $\\varepsilon > 0$, where $\\alpha$ is the\nratio of an offline (deterministic) algorithm for SMCC used for\npost-processing. Thus, if exponential computation time is allowed, our\nalgorithm deterministically achieves nearly the optimal $1/2$ ratio. These\nresults nearly match those of a recently proposed, randomized streaming\nalgorithm that achieves the same ratios in expectation. For a deterministic,\nsingle-pass streaming algorithm, our algorithm achieves in polynomial time an\nimprovement of the best approximation factor from $1/9$ of previous literature\nto $\\approx 0.2689$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:22:49 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 20:17:17 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 15:35:57 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kuhnle", "Alan", ""]]}, {"id": "2010.14464", "submitter": "Lukasz Borchmann", "authors": "{\\L}ukasz Borchmann, Dawid Jurkiewicz, Filip Grali\\'nski, Tomasz\n  G\\'orecki", "title": "Dynamic Boundary Time Warping for Sub-sequence Matching with Few\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel method of finding a fragment in a long temporal\nsequence similar to the set of shorter sequences. We are the first to propose\nan algorithm for such a search that does not rely on computing the average\nsequence from query examples. Instead, we use query examples as is, utilizing\nall of them simultaneously. The introduced method based on the Dynamic Time\nWarping (DTW) technique is suited explicitly for few-shot query-by-example\nretrieval tasks. We evaluate it on two different few-shot problems from the\nfield of Natural Language Processing. The results show it either outperforms\nbaselines and previous approaches or achieves comparable results when a low\nnumber of examples is available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:23:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Borchmann", "\u0141ukasz", ""], ["Jurkiewicz", "Dawid", ""], ["Grali\u0144ski", "Filip", ""], ["G\u00f3recki", "Tomasz", ""]]}, {"id": "2010.14487", "submitter": "Liren Shan", "authors": "Konstantin Makarychev, Aravind Reddy, Liren Shan", "title": "Improved Guarantees for k-means++ and k-means++ Parallel", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study k-means++ and k-means++ parallel, the two most\npopular algorithms for the classic k-means clustering problem. We provide novel\nanalyses and show improved approximation and bi-criteria approximation\nguarantees for k-means++ and k-means++ parallel. Our results give a better\ntheoretical justification for why these algorithms perform extremely well in\npractice. We also propose a new variant of k-means++ parallel algorithm\n(Exponential Race k-means++) that has the same approximation guarantees as\nk-means++.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:46:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Reddy", "Aravind", ""], ["Shan", "Liren", ""]]}, {"id": "2010.14560", "submitter": "Paul Liu", "authors": "Moses Charikar and Paul Liu", "title": "Improved Algorithms for Edge Colouring in the W-Streaming Model", "comments": "To appear in SOSA21. Updated to contain references to relevant work\n  from SODA21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the W-streaming model, an algorithm is given $O(n \\mathrm{polylog} n)$\nspace and must process a large graph of up to $O(n^2)$ edges. In this short\nnote we give two algorithms for edge colouring under the W-streaming model. For\nedge colouring in W-streaming, a colour for every edge must be determined by\nthe time all the edges are streamed. Our first algorithm uses $\\Delta +\no(\\Delta)$ colours in $O(n \\log^2 n)$ space when the edges arrive according to\na uniformly random permutation. The second algorithm uses $(1 + o(1))\\Delta^2 /\ns$ colours in $\\tilde{O}(n s)$ space when edges arrival adversarially.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:06:38 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 19:45:33 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Charikar", "Moses", ""], ["Liu", "Paul", ""]]}, {"id": "2010.14620", "submitter": "Divya Padmanabhan", "authors": "Louis Chen, Divya Padmanabhan, Chee Chin Lim, Karthik Natarajan", "title": "Correlation Robust Influence Maximization", "comments": null, "journal-ref": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributionally robust model for the influence maximization\nproblem. Unlike the classic independent cascade model\n\\citep{kempe2003maximizing}, this model's diffusion process is adversarially\nadapted to the choice of seed set. Hence, instead of optimizing under the\nassumption that all influence relationships in the network are independent, we\nseek a seed set whose expected influence under the worst correlation, i.e. the\n\"worst-case, expected influence\", is maximized. We show that this worst-case\ninfluence can be efficiently computed, and though the optimization is NP-hard,\na ($1 - 1/e$) approximation guarantee holds. We also analyze the structure to\nthe adversary's choice of diffusion process, and contrast with established\nmodels. Beyond the key computational advantages, we also highlight the extent\nto which the independence assumption may cost optimality, and provide insights\nfrom numerical experiments comparing the adversarial and independent cascade\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:43:56 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chen", "Louis", ""], ["Padmanabhan", "Divya", ""], ["Lim", "Chee Chin", ""], ["Natarajan", "Karthik", ""]]}, {"id": "2010.14658", "submitter": "Arun Ganesh", "authors": "Arun Ganesh, Kunal Talwar", "title": "Faster Differentially Private Samplers via R\\'enyi Divergence Analysis\n  of Discretized Langevin MCMC", "comments": "Appeared in NeurIPS 2020. Fixed a typo in the proof of Theorem 15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various differentially private algorithms instantiate the exponential\nmechanism, and require sampling from the distribution $\\exp(-f)$ for a suitable\nfunction $f$. When the domain of the distribution is high-dimensional, this\nsampling can be computationally challenging. Using heuristic sampling schemes\nsuch as Gibbs sampling does not necessarily lead to provable privacy. When $f$\nis convex, techniques from log-concave sampling lead to polynomial-time\nalgorithms, albeit with large polynomials. Langevin dynamics-based algorithms\noffer much faster alternatives under some distance measures such as statistical\ndistance. In this work, we establish rapid convergence for these algorithms\nunder distance measures more suitable for differential privacy. For smooth,\nstrongly-convex $f$, we give the first results proving convergence in R\\'enyi\ndivergence. This gives us fast differentially private algorithms for such $f$.\nOur techniques and simple and generic and apply also to underdamped Langevin\ndynamics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 22:52:45 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 07:21:58 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Ganesh", "Arun", ""], ["Talwar", "Kunal", ""]]}, {"id": "2010.14684", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Tal Ben-Nun, Dimitri Stanojevic, Johannes\n  De Fine Licht, Torsten Hoefler", "title": "Substream-Centric Maximum Matchings on FPGA", "comments": "Best Paper finalist at ACM FPGA'19, invited to special issue of ACM\n  TRETS'20", "journal-ref": "Proceedings of the ACM Transactions on Reconfigurable Technology\n  and Systems (TRETS), 2020. Proceedings of the 27th ACM/SIGDA International\n  Symposium on Field-Programmable Gate Arrays (FPGA), 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high-performance and energy-efficient algorithms for maximum\nmatchings is becoming increasingly important in social network analysis,\ncomputational sciences, scheduling, and others. In this work, we propose the\nfirst maximum matching algorithm designed for FPGAs; it is energy-efficient and\nhas provable guarantees on accuracy, performance, and storage utilization. To\nachieve this, we forego popular graph processing paradigms, such as\nvertex-centric programming, that often entail large communication costs.\nInstead, we propose a substream-centric approach, in which the input stream of\ndata is divided into substreams processed independently to enable more\nparallelism while lowering communication costs. We base our work on the theory\nof streaming graph algorithms and analyze 14 models and 28 algorithms. We use\nthis analysis to provide theoretical underpinning that matches the physical\nconstraints of FPGA platforms. Our algorithm delivers high performance (more\nthan 4x speedup over tuned parallel CPU variants), low memory, high accuracy,\nand effective usage of FPGA resources. The substream-centric approach could\neasily be extended to other algorithms to offer low-power and high-performance\ngraph processing on FPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 00:31:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Ben-Nun", "Tal", ""], ["Stanojevic", "Dimitri", ""], ["Licht", "Johannes De Fine", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.14752", "submitter": "Baisakh Baisakh", "authors": "Baisakh, Rakesh Mohanty", "title": "Competitive Analysis of Move-to-Front-or-Middle (MFM) Online List Update\n  Algorithm", "comments": "11 pages, 1 figure, 3 tables. arXiv admin note: text overlap with\n  arXiv:2010.13042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and analysis of efficient algorithms with the knowledge of current\nand past inputs is a non-trivial and challenging research area in computer\nscience. In many practical applications the future inputs are not available to\nthe algorithm at any instance of time. So the algorithm has to make decisions\nbased on a sequence of inputs that are in order and on the fly. Such algorithms\nare known as online algorithms. For measuring the performance of online\nalgorithms, a standard measure, known as competitive analysis, has been\nextensively used in the literature. List update problem is a well studied\nresearch problem in the area of online algorithms since last few decades. One\nof the widely used deterministic online list update algorithm is the\nMove-To-Front (MTF) algorithm, which has been shown to be 2-competitive with\nbest performance in practical real life inputs. In this paper we analyse the\nMove-to-Front-or-Middle (MFM) algorithm using competitive analysis by\naddressing one of an open question raised by Albers that whether dynamic\noffline algorithm can be used in finding the competitiveness of an online\nalgorithm? Move-To-Front-or-Middle (MFM) was experimentally studied and\nobserved to be performing better than MTF algorithm by using the Calgary Corpus\nand Canterbury Corpus data set. However, it is interesting and challenging to\nfind the lower bound and upper bound on the competitive ratio of MFM algorithm.\nWe make a first attempt to find the competitiveness of MFM algorithm. Our new\nresults show that MFM is not 2-competitive with respect to static optimum\noffline algorithm, whereas it is 2-competitive with respect to dynamic optimum\noffline algorithm. Our new theoretical results may open up a new direction of\nresearch in the online list update problem by characterising the structure of\ncompetitive and non competitive deterministic online algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 11:09:51 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Baisakh", "", ""], ["Mohanty", "Rakesh", ""]]}, {"id": "2010.14864", "submitter": "Constantinos Daskalakis", "authors": "Constantinos Daskalakis and Qinxuan Pan", "title": "Sample-Optimal and Efficient Learning of Tree Ising models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that $n$-variable tree-structured Ising models can be learned\ncomputationally-efficiently to within total variation distance $\\epsilon$ from\nan optimal $O(n \\ln n/\\epsilon^2)$ samples, where $O(\\cdot)$ hides an absolute\nconstant which, importantly, does not depend on the model being learned -\nneither its tree nor the magnitude of its edge strengths, on which we place no\nassumptions. Our guarantees hold, in fact, for the celebrated Chow-Liu [1968]\nalgorithm, using the plug-in estimator for estimating mutual information. While\nthis (or any other) algorithm may fail to identify the structure of the\nunderlying model correctly from a finite sample, we show that it will still\nlearn a tree-structured model that is $\\epsilon$-close to the true one in total\nvariation distance, a guarantee called \"proper learning.\"\n  Our guarantees do not follow from known results for the Chow-Liu algorithm\nand the ensuing literature on learning graphical models, including a recent\nrenaissance of algorithms on this learning challenge, which only yield\nasymptotic consistency results, or sample-inefficient and/or time-inefficient\nalgorithms, unless further assumptions are placed on the graphical model, such\nas bounds on the \"strengths\" of the model's edges/hyperedges. While we\nestablish guarantees for a widely known and simple algorithm, the analysis that\nthis algorithm succeeds and is sample-optimal is quite complex, requiring a\nhierarchical classification of the edges into layers with different\nreconstruction guarantees, depending on their strength, combined with delicate\nuses of the subadditivity of the squared Hellinger distance over graphical\nmodels to control the error accumulation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 10:17:48 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 22:50:21 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Pan", "Qinxuan", ""]]}, {"id": "2010.14916", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Arnaud Labourel, Andrzej Pelc", "title": "Almost-Optimal Deterministic Treasure Hunt in Arbitrary Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile agent navigating along edges of a simple connected graph, either\nfinite or countably infinite, has to find an inert target (treasure) hidden in\none of the nodes. This task is known as treasure hunt. The agent has no a\npriori knowledge of the graph, of the location of the treasure or of the\ninitial distance to it. The cost of a treasure hunt algorithm is the worst-case\nnumber of edge traversals performed by the agent until finding the treasure.\nAwerbuch, Betke, Rivest and Singh [3] considered graph exploration and treasure\nhunt for finite graphs in a restricted model where the agent has a fuel tank\nthat can be replenished only at the starting node $s$. The size of the tank is\n$B=2(1+\\alpha)r$, for some positive real constant $\\alpha$, where $r$, called\nthe radius of the graph, is the maximum distance from $s$ to any other node.\nThe tank of size $B$ allows the agent to make at most $\\lfloor B\\rfloor$ edge\ntraversals between two consecutive visits at node $s$.\n  Let $e(d)$ be the number of edges whose at least one extremity is at distance\nless than $d$ from $s$. Awerbuch, Betke, Rivest and Singh [3] conjectured that\nit is impossible to find a treasure hidden in a node at distance at most $d$ at\ncost nearly linear in $e(d)$. We first design a deterministic treasure hunt\nalgorithm working in the model without any restrictions on the moves of the\nagent at cost $\\mathcal{O}(e(d) \\log d)$, and then show how to modify this\nalgorithm to work in the model from [3] with the same complexity. Thus we\nrefute the above twenty-year-old conjecture. We observe that no treasure hunt\nalgorithm can beat cost $\\Theta(e(d))$ for all graphs and thus our algorithms\nare also almost optimal.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:25:23 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 12:17:11 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 14:32:43 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2021 12:43:49 GMT"}, {"version": "v5", "created": "Sat, 13 Feb 2021 10:27:18 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Labourel", "Arnaud", ""], ["Pelc", "Andrzej", ""]]}, {"id": "2010.15210", "submitter": "Xing Hu", "authors": "Vassos Hadzilacos, Xing Hu, Sam Toueg", "title": "On Linearizability and the Termination of Randomized Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of whether the \"termination with probability 1\"\nproperty of a randomized algorithm is preserved when one replaces the atomic\nregisters that the algorithm uses with linearizable (implementations of)\nregisters. We show that in general this is not so: roughly speaking, every\nrandomized algorithm A has a corresponding algorithm A' that solves the same\nproblem if the registers that it uses are atomic or strongly-linearizable, but\ndoes not terminate if these registers are replaced with \"merely\" linearizable\nones. Together with a previous result shown in [15], this implies that one\ncannot use the well-known ABD implementation of registers in message-passing\nsystems to automatically transform any randomized algorithm that works in\nshared-memory systems into a randomized algorithm that works in message-passing\nsystems: with a strong adversary the resulting algorithm may not terminate.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:21:51 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hadzilacos", "Vassos", ""], ["Hu", "Xing", ""], ["Toueg", "Sam", ""]]}, {"id": "2010.15435", "submitter": "Eugenio Angriman", "authors": "Eugenio Angriman, Ruben Becker, Gianlorenzo D'Angelo, Hugo Gilbert,\n  Alexander van der Grinten, Henning Meyerhenke", "title": "Group-Harmonic and Group-Closeness Maximization -- Approximation and\n  Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centrality measures characterize important nodes in networks. Efficiently\ncomputing such nodes has received a lot of attention. When considering the\ngeneralization of computing central groups of nodes, challenging optimization\nproblems occur. In this work, we study two such problems, group-harmonic\nmaximization and group-closeness maximization both from a theoretical and from\nan algorithm engineering perspective.\n  On the theoretical side, we obtain the following results. For group-harmonic\nmaximization, unless $P=NP$, there is no polynomial-time algorithm that\nachieves an approximation factor better than $1-1/e$ (directed) and $1-1/(4e)$\n(undirected), even for unweighted graphs. On the positive side, we show that a\ngreedy algorithm achieves an approximation factor of $\\lambda(1-2/e)$\n(directed) and $\\lambda(1-1/e)/2$ (undirected), where $\\lambda$ is the ratio of\nminimal and maximal edge weights. For group-closeness maximization, the\nundirected case is $NP$-hard to be approximated to within a factor better than\n$1-1/(e+1)$ and a constant approximation factor is achieved by a local-search\nalgorithm. For the directed case, however, we show that, for any\n$\\epsilon<1/2$, the problem is $NP$-hard to be approximated within a factor of\n$4|V|^{-\\epsilon}$.\n  From the algorithm engineering perspective, we provide efficient\nimplementations of the above greedy and local search algorithms. In our\nexperimental study we show that, on small instances where an optimum solution\ncan be computed in reasonable time, the quality of both the greedy and the\nlocal search algorithms come very close to the optimum. On larger instances,\nour local search algorithms yield results with superior quality compared to\nexisting greedy and local search solutions, at the cost of additional running\ntime. We thus advocate local search for scenarios where solution quality is of\nhighest concern.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:13:43 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Angriman", "Eugenio", ""], ["Becker", "Ruben", ""], ["D'Angelo", "Gianlorenzo", ""], ["Gilbert", "Hugo", ""], ["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "2010.15476", "submitter": "Marc Alexa", "authors": "Marc Alexa", "title": "Iteratively reweighted greedy set cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically analyze a simple heuristic for large sparse set cover\nproblems. It uses the weighted greedy algorithm as a basic building block. By\nmultiplicative updates of the weights attached to the elements, the greedy\nsolution is iteratively improved. The implementation of this algorithm is\ntrivial and the algorithm is essentially free of parameters that would require\ntuning. More iterations can only improve the solution. This set of features\nmakes the approach attractive for practical problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 10:47:35 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Alexa", "Marc", ""]]}, {"id": "2010.15671", "submitter": "Linh Anh Nguyen D.Sc.", "authors": "Linh Anh Nguyen and Dat Xuan Tran", "title": "Computing Crisp Bisimulations for Fuzzy Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy structures such as fuzzy automata, fuzzy transition systems, weighted\nsocial networks and fuzzy interpretations in fuzzy description logics have been\nwidely studied. For such structures, bisimulation is a natural notion for\ncharacterizing indiscernibility between states or individuals. There are two\nkinds of bisimulations for fuzzy structures: crisp bisimulations and fuzzy\nbisimulations. While the latter fits to the fuzzy paradigm, the former has also\nattracted attention due to the application of crisp equivalence relations, for\nexample, in minimizing structures. Bisimulations can be formulated for fuzzy\nlabeled graphs and then adapted to other fuzzy structures. In this article, we\npresent an efficient algorithm for computing the partition corresponding to the\nlargest crisp bisimulation of a given finite fuzzy labeled graph. Its\ncomplexity is of order $O((m\\log{l} + n)\\log{n})$, where $n$, $m$ and $l$ are\nthe number of vertices, the number of nonzero edges and the number of different\nfuzzy degrees of edges of the input graph, respectively. We also study a\nsimilar problem for the setting with counting successors, which corresponds to\nthe case with qualified number restrictions in description logics and graded\nmodalities in modal logics. In particular, we provide an efficient algorithm\nwith the complexity $O((m\\log{m} + n)\\log{n})$ for the considered problem in\nthat setting.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 23:15:37 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 21:13:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nguyen", "Linh Anh", ""], ["Tran", "Dat Xuan", ""]]}, {"id": "2010.15755", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff, Manuel P\\\"oter", "title": "A more Pragmatic Implementation of the Lock-free, Ordered, Linked List", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lock-free, ordered, linked list is an important, standard example of a\nconcurrent data structure. An obvious, practical drawback of textbook\nimplementations is that failed compare-and-swap (CAS) operations lead to\nretraversal of the entire list (retries), which is particularly harmful for\nsuch a linear-time data structure. We alleviate this drawback by first\nobserving that failed CAS operations under some conditions do not require a\nfull retry, and second by maintaining approximate backwards pointers that are\nused to find a closer starting position in the list for operation retry.\nExperiments with both a worst-case deterministic benchmark, and a standard,\nrandomized, mixed-operation throughput benchmark on three shared-memory systems\n(Intel Xeon, AMD EPYC, SPARC-T5) show practical improvements ranging from\nsignificant, to dramatic, several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 16:59:49 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 09:57:37 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""], ["P\u00f6ter", "Manuel", ""]]}, {"id": "2010.15770", "submitter": "David Williamson", "authors": "David R. Karger, David P. Williamson", "title": "Recursive Random Contraction Revisited", "comments": "To appear in the Symposium on Simplicity in Algorithms 2021 (SOSA\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we revisit the recursive random contraction algorithm of Karger\nand Stein for finding a minimum cut in a graph. Our revisit is occasioned by a\npaper of Fox, Panigrahi, and Zhang which gives an extension of the Karger-Stein\nalgorithm to minimum cuts and minimum $k$-cuts in hypergraphs. When specialized\nto the case of graphs, the algorithm is somewhat different than the original\nKarger-Stein algorithm. We show that the analysis becomes particularly clean in\nthis case: we can prove that the probability that a fixed minimum cut in an $n$\nnode graph is returned by the algorithm is bounded below by $1/(2H_n-2)$, where\n$H_n$ is the $n$th harmonic number. We also consider other similar variants of\nthe algorithm, and show that no such algorithm can achieve an asymptotically\nbetter probability of finding a fixed minimum cut.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:15:22 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Karger", "David R.", ""], ["Williamson", "David P.", ""]]}, {"id": "2010.15776", "submitter": "Bobak Kiani", "authors": "Bobak T. Kiani, Giacomo De Palma, Dirk Englund, William Kaminsky,\n  Milad Marvian, Seth Lloyd", "title": "Quantum advantage for differential equation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.NA math-ph math.MP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum algorithms for both differential equation solving and for machine\nlearning potentially offer an exponential speedup over all known classical\nalgorithms. However, there also exist obstacles to obtaining this potential\nspeedup in useful problem instances. The essential obstacle for quantum\ndifferential equation solving is that outputting useful information may require\ndifficult post-processing, and the essential obstacle for quantum machine\nlearning is that inputting the training set is a difficult task just by itself.\nIn this paper, we demonstrate, when combined, these difficulties solve one\nanother. We show how the output of quantum differential equation solving can\nserve as the input for quantum machine learning, allowing dynamical analysis in\nterms of principal components, power spectra, and wavelet decompositions. To\nillustrate this, we consider continuous time Markov processes on\nepidemiological and social networks. These quantum algorithms provide an\nexponential advantage over existing classical Monte Carlo methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:19:04 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kiani", "Bobak T.", ""], ["De Palma", "Giacomo", ""], ["Englund", "Dirk", ""], ["Kaminsky", "William", ""], ["Marvian", "Milad", ""], ["Lloyd", "Seth", ""]]}, {"id": "2010.15794", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Eccentricity queries and beyond using Hub Labels", "comments": "Abstract shortened to respect the arXiv limit of 1920 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hub labeling schemes are popular methods for computing distances on road\nnetworks and other large complex networks, often answering to a query within a\nfew microseconds for graphs with millions of edges. In this work, we study\ntheir algorithmic applications beyond distance queries. We focus on\neccentricity queries and distance-sum queries, for several versions of these\nproblems on directed weighted graphs, that is in part motivated by their\nimportance in facility location problems. On the negative side, we show\nconditional lower bounds for these above problems on unweighted undirected\nsparse graphs, via standard constructions from \"Fine-grained\" complexity.\nHowever, things take a different turn when the hub labels have a sublogarithmic\nsize. Indeed, given a hub labeling of maximum label size $\\leq k$, after\npre-processing the labels in total $2^{{O}(k)} \\cdot |V|^{1+o(1)}$ time, we can\ncompute both the eccentricity and the distance-sum of any vertex in $2^{{O}(k)}\n\\cdot |V|^{o(1)}$ time. It can also be applied to the fast global computation\nof some topological indices. Finally, as a by-product of our approach, on any\nfixed class of unweighted graphs with bounded expansion, we can decide whether\nthe diameter of an $n$-vertex graph in the class is at most $k$ in $f(k) \\cdot\nn^{1+o(1)}$ time, for some \"explicit\" function $f$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:34:28 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "2010.15803", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Isometric embeddings in trees and their use in the diameter problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that given a discrete space with $n$ points which is either embedded\nin a system of $k$ trees, or the Cartesian product of $k$ trees, we can compute\nall eccentricities in ${\\cal O}(2^{{\\cal O}(k\\log{k})}(N+n)^{1+o(1)})$ time,\nwhere $N$ is the cumulative total order over all these $k$ trees. This is near\noptimal under the Strong Exponential-Time Hypothesis, even in the very special\ncase of an $n$-vertex graph embedded in a system of $\\omega(\\log{n})$ spanning\ntrees. However, given such an embedding in the strong product of $k$ trees,\nthere is a much faster ${\\cal O}(N + kn)$-time algorithm for this problem. All\nour positive results can be turned into approximation algorithms for the graphs\nand finite spaces with a quasi isometric embedding in trees, if such embedding\nis given as input, where the approximation factor (resp., the approximation\nconstant) depends on the distortion of the embedding (resp., of its stretch).\nThe existence of embeddings in the Cartesian product of finitely many trees has\nbeen thoroughly investigated for cube-free median graphs. We give the\nfirst-known quasi linear-time algorithm for computing the diameter within this\ngraph class. It does not require an embedding in a product of trees to be given\nas part of the input. On our way, being given an $n$-node tree $T$, we propose\na data structure with ${\\cal O}(n\\log{n})$ pre-processing time in order to\ncompute in ${\\cal O}(k\\log^2{n})$ time the eccentricity of any subset of $k$\nnodes. We combine the latter technical contribution, of independent interest,\nwith a recent distance-labeling scheme that was designed for cube-free median\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:42:10 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "2010.15805", "submitter": "Hong Zhou", "authors": "Lap Chi Lau and Hong Zhou", "title": "A Local Search Framework for Experimental Design", "comments": "Improved probability bound in Theorem 1.4. A preliminary version\n  accepted by SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a local search framework to design and analyze both combinatorial\nalgorithms and rounding algorithms for experimental design problems. This\nframework provides a unifying approach to match and improve all known results\nin D/A/E-design and to obtain new results in previously unknown settings.\n  For combinatorial algorithms, we provide a new analysis of the classical\nFedorov's exchange method. We prove that this simple local search algorithm\nworks well as long as there exists an almost optimal solution with good\ncondition number. Moreover, we design a new combinatorial local search\nalgorithm for E-design using the regret minimization framework.\n  For rounding algorithms, we provide a unified randomized exchange algorithm\nto match and improve previous results for D/A/E-design. Furthermore, the\nalgorithm works in the more general setting to approximately satisfy multiple\nknapsack constraints, which can be used for weighted experimental design and\nfor incorporating fairness constraints into experimental design.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:43:06 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:00:04 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Lau", "Lap Chi", ""], ["Zhou", "Hong", ""]]}, {"id": "2010.15811", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui and Mark Sellke", "title": "Algorithmic pure states for the negative spherical perceptron", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the spherical perceptron with Gaussian disorder. This is the set\n$S$ of points $\\sigma \\in \\mathbb{R}^N$ on the sphere of radius $\\sqrt{N}$\nsatisfying $\\langle g_a , \\sigma \\rangle \\ge \\kappa\\sqrt{N}\\,$ for all $1 \\le a\n\\le M$, where $(g_a)_{a=1}^M$ are independent standard gaussian vectors and\n$\\kappa \\in \\mathbb{R}$ is fixed. Various characteristics of $S$ such as its\nsurface measure and the largest $M$ for which it is non-empty, were computed\nheuristically in statistical physics in the asymptotic regime $N \\to \\infty$,\n$M/N \\to \\alpha$. The case $\\kappa<0$ is of special interest as $S$ is\nconjectured to exhibit a hierarchical tree-like geometry known as \"full\nreplica-symmetry breaking\" (FRSB) close to the satisfiability threshold\n$\\alpha_{\\text{SAT}}(\\kappa)$, and whose characteristics are captured by a\nParisi variational principle akin to the one appearing in the\nSherrington-Kirkpatrick model. In this paper we design an efficient algorithm\nwhich, given oracle access to the solution of the Parisi variational principle,\nexploits this conjectured FRSB structure for $\\kappa<0$ and outputs a vector\n$\\hat{\\sigma}$ satisfying $\\langle g_a , \\hat{\\sigma}\\rangle \\ge \\kappa\n\\sqrt{N}$ for all $1\\le a \\le M$ and lying on a sphere of non-trivial radius\n$\\sqrt{\\bar{q} N}$, where $\\bar{q} \\in (0,1)$ is the right-end of the support\nof the associated Parisi measure. We expect $\\hat{\\sigma}$ to be approximately\nthe barycenter of a pure state of the spherical perceptron. Moreover we expect\nthat $\\bar{q} \\to 1$ as $\\alpha \\to \\alpha_{\\text{SAT}}(\\kappa)$, so that\n$\\big\\langle g_a,\\hat{\\sigma}/|\\hat{\\sigma}|\\big\\rangle \\geq\n(\\kappa-o(1))\\sqrt{N}$ near criticality.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:46:21 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Sellke", "Mark", ""]]}, {"id": "2010.15814", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Around the diameter of AT-free graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph algorithm is truly subquadratic if it runs in ${\\cal O}(m^b)$ time on\nconnected $m$-edge graphs, for some positive $b < 2$. Roditty and Vassilevska\nWilliams (STOC'13) proved that under plausible complexity assumptions, there is\nno truly subquadratic algorithm for computing the diameter of general graphs.\nIn this work, we present positive and negative results on the existence of such\nalgorithms for computing the diameter on some special graph classes.\nSpecifically, three vertices in a graph form an asteroidal triple (AT) if\nbetween any two of them there exists a path that avoids the closed\nneighbourhood of the third one. We call a graph AT-free if it does not contain\nan AT. We first prove that for all $m$-edge AT-free graphs, one can compute all\nthe eccentricities in truly subquadratic ${\\cal O}(m^{3/2})$ time. Then, we\nextend our study to several subclasses of chordal graphs -- all of them\ngeneralizing interval graphs in various ways --, as an attempt to understand\nwhich of the properties of AT-free graphs, or natural generalizations of the\nlatter, can help in the design of fast algorithms for the diameter problem on\nbroader graph classes. For instance, for all chordal graphs with a dominating\nshortest path, there is a linear-time algorithm for computing a diametral pair\nif the diameter is at least four. However, already for split graphs with a\ndominating edge, under plausible complexity assumptions, there is no truly\nsubquadratic algorithm for deciding whether the diameter is either $2$ or $3$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:48:03 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "2010.15879", "submitter": "Maciej Besta", "authors": "Maciej Besta, Dimitri Stanojevic, Tijana Zivic, Jagpreet Singh,\n  Maurice Hoerold, Torsten Hoefler", "title": "Log(Graph): A Near-Optimal High-Performance Graph Representation", "comments": null, "journal-ref": "Proceedings of the 27th International Conference on Parallel\n  Architectures and Compilation (PACT'18), 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's graphs used in domains such as machine learning or social network\nanalysis may contain hundreds of billions of edges. Yet, they are not\nnecessarily stored efficiently, and standard graph representations such as\nadjacency lists waste a significant number of bits while graph compression\nschemes such as WebGraph often require time-consuming decompression. To address\nthis, we propose Log(Graph): a graph representation that combines high\ncompression ratios with very low-overhead decompression to enable cheaper and\nfaster graph processing. The key idea is to encode a graph so that the parts of\nthe representation approach or match the respective storage lower bounds. We\ncall our approach \"graph logarithmization\" because these bounds are usually\nlogarithmic. Our high-performance Log(Graph) implementation based on modern\nbitwise operations and state-of-the-art succinct data structures achieves high\ncompression ratios as well as performance. For example, compared to the tuned\nGraph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by\n20-35% while matching GAPBS' performance or even delivering speedups due to\nreducing amounts of transferred data. It approaches the compression ratio of\nthe established WebGraph compression library while enabling speedups of up to\nmore than 2x. Log(Graph) can improve the design of various graph processing\nengines or libraries on single NUMA nodes as well as distributed-memory\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:41:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Stanojevic", "Dimitri", ""], ["Zivic", "Tijana", ""], ["Singh", "Jagpreet", ""], ["Hoerold", "Maurice", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.15951", "submitter": "Zhenwei Dai", "authors": "Zhenwei Dai, Aditya Desai, Reinhard Heckel, Anshumali Shrivastava", "title": "Active Sampling Count Sketch (ASCS) for Online Sparse Estimation of a\n  Trillion Scale Covariance Matrix", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating and storing the covariance (or correlation) matrix of\nhigh-dimensional data is computationally challenging because both memory and\ncomputational requirements scale quadratically with the dimension. Fortunately,\nhigh-dimensional covariance matrices as observed in text, click-through,\nmeta-genomics datasets, etc are often sparse. In this paper, we consider the\nproblem of efficient sparse estimation of covariance matrices with possibly\ntrillions of entries. The size of the datasets we target requires the algorithm\nto be online, as more than one pass over the data is prohibitive. In this\npaper, we propose Active Sampling Count Sketch (ASCS), an online and one-pass\nsketching algorithm, that recovers the large entries of the covariance matrix\naccurately. Count Sketch (CS), and other sub-linear compressed sensing\nalgorithms, offer a natural solution to the problem in theory. However, vanilla\nCS does not work well in practice due to a low signal-to-noise ratio (SNR). At\nthe heart of our approach is a novel active sampling strategy that increases\nthe SNR of classical CS. We demonstrate the practicality of our algorithm with\nsynthetic data and real-world high dimensional datasets. ASCS significantly\nimproves over vanilla CS, demonstrating the merit of our active sampling\nstrategy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 21:20:15 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 02:15:14 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Dai", "Zhenwei", ""], ["Desai", "Aditya", ""], ["Heckel", "Reinhard", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2010.16012", "submitter": "Maciej Besta", "authors": "Maciej Besta, Michal Podstawski, Linus Groner, Edgar Solomonik,\n  Torsten Hoefler", "title": "To Push or To Pull: On Reducing Communication and Synchronization in\n  Graph Computations", "comments": null, "journal-ref": "Proceedings of the 26th ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC'17), 2017", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce the cost of communication and synchronization in graph processing\nby analyzing the fastest way to process graphs: pushing the updates to a shared\nstate or pulling the updates to a private state.We investigate the\napplicability of this push-pull dichotomy to various algorithms and its impact\non complexity, performance, and the amount of used locks, atomics, and\nreads/writes. We consider 11 graph algorithms, 3 programming models, 2 graph\nabstractions, and various families of graphs. The conducted analysis\nillustrates surprising differences between push and pull variants of different\nalgorithms in performance, speed of convergence, and code complexity; the\ninsights are backed up by performance data from hardware counters.We use these\nfindings to illustrate which variant is faster for each algorithm and to\ndevelop generic strategies that enable even higher speedups. Our insights can\nbe used to accelerate graph processing engines or libraries on both\nmassively-parallel shared-memory machines as well as distributed-memory\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 01:04:57 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Podstawski", "Michal", ""], ["Groner", "Linus", ""], ["Solomonik", "Edgar", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.16158", "submitter": "Marc Heinrich", "authors": "Marc Heinrich", "title": "Glauber dynamics for colourings of chordal graphs and graphs of bounded\n  treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.PR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Glauber dynamics on the colourings of a graph is a random process which\nconsists in recolouring at each step a random vertex of a graph with a new\ncolour chosen uniformly at random among the colours not already present in its\nneighbourhood. It is known that when the total number of colours available is\nat least $\\Delta +2$, where $\\Delta$ is the maximum degree of the graph, this\nprocess converges to a uniform distribution on the set of all the colourings.\nMoreover, a well known conjecture is that the time it takes for the convergence\nto happen, called the mixing time, is polynomial in the size of the graph. Many\nweaker variants of this conjecture have been studied in the literature by\nallowing either more colours, or restricting the graphs to particular classes,\nor both. This paper follows this line of research by studying the mixing time\nof the Glauber dynamics on chordal graphs, as well as graphs of bounded\ntreewidth. We show that the mixing time is polynomial in the size of the graph\nin the two following cases:\n  - on graphs with bounded treewidth, and at least $\\Delta +2$ colours,\n  - on chordal graphs if the number of colours is at least $(1+\\varepsilon)\n(\\Delta +1)$, for any fixed constant $\\varepsilon$.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:58:02 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 08:27:27 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Heinrich", "Marc", ""]]}, {"id": "2010.16177", "submitter": "Shiri Antaki", "authors": "Shiri Antaki, Quanquan C. Liu and Shay Solomon", "title": "Near-Optimal Distributed Implementations of Dynamic Algorithms for\n  Symmetry-Breaking Problems", "comments": "Abstract truncated to fit arXiv limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of dynamic graph algorithms aims at achieving a thorough\nunderstanding of real-world networks whose topology evolves with time.\nTraditionally, the focus has been on the classic sequential, centralized\nsetting where the main quality measure of an algorithm is its update time, i.e.\nthe time needed to restore the solution after each update. While real-life\nnetworks are very often distributed across multiple machines, the fundamental\nquestion of finding efficient dynamic, distributed graph algorithms received\nlittle attention to date. The goal in this setting is to optimize both the\nround and message complexities incurred per update step, ideally achieving a\nmessage complexity that matches the centralized update time in $O(1)$ (perhaps\namortized) rounds.\n  Toward initiating a systematic study of dynamic, distributed algorithms, we\nstudy some of the most central symmetry-breaking problems: maximal independent\nset (MIS), maximal matching/(approx-) maximum cardinality matching (MM/MCM),\nand $(\\Delta + 1)$-vertex coloring. This paper focuses on dynamic, distributed\nalgorithms that are deterministic, and in particular -- robust against an\nadaptive adversary. Most of our focus is on the MIS algorithm, which achieves\n$O\\left(m^{2/3}\\log^2 n\\right)$ amortized messages in $O\\left(\\log^2 n\\right)$\namortized rounds in the Congest model. Notably, the amortized message\ncomplexity of our algorithm matches the amortized update time of the best-known\ndeterministic centralized MIS algorithm by Gupta and Khan [SOSA'21] up to a\npolylog $n$ factor. The previous best deterministic distributed MIS algorithm,\nby Assadi et al. [STOC'18], uses $O(m^{3/4})$ amortized messages in $O(1)$\namortized rounds, i.e., we achieve a polynomial improvement in the message\ncomplexity by a polylog $n$ increase to the round complexity; moreover, the\nalgorithm of Assadi et al. makes an implicit assumption that the [...]\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:31:17 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 20:54:50 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Antaki", "Shiri", ""], ["Liu", "Quanquan C.", ""], ["Solomon", "Shay", ""]]}, {"id": "2010.16316", "submitter": "Billy Jin", "authors": "Monika Henzinger, Billy Jin, David P. Williamson", "title": "A Combinatorial Cut-Based Algorithm for Solving Laplacian Linear Systems", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, a significant line of work in theoretical\nalgorithms has been progress in solving linear systems of the form\n$\\mathbf{L}\\mathbf{p} = \\mathbf{b}$, where $\\mathbf{L}$ is the Laplacian matrix\nof a weighted graph with weights $w(i,j)>0$ on the edges. The solution\n$\\mathbf{p}$ of the linear system can be interpreted as the potentials of an\nelectrical flow. Kelner, Orrechia, Sidford, and Zhu \\cite{KOSZ13} give a\ncombinatorial, near-linear time algorithm that maintains the Kirchoff Current\nLaw, and gradually enforces the Kirchoff Potential Law. Here we consider a dual\nversion of the algorithm that maintains the Kirchoff Potential Law, and\ngradually enforces the Kirchoff Current Law. We prove that this dual algorithm\nalso runs in a near-linear number of iterations. Each iteration requires\nupdating all potentials on one side of a fundamental cut of a spanning tree by\na fixed amount. If this update step can be performed in polylogarithmic time,\nwe can also obtain a near-linear time algorithm to solve $\\mathbf{L}\\mathbf{p}\n= \\mathbf{b}$. However, if we abstract this update step as a natural data\nstructure problem, we show that we can use the data structure to solve a\nproblem that has been conjectured to be difficult for dynamic algorithms, the\nonline vector-matrix-vector problem \\cite{HKNS15}. The conjecture implies that\nthe data structure does not have an $O(n^{1-\\epsilon})$ time algorithm for any\n$\\epsilon > 0$. Thus our dual algorithm cannot be near-linear time algorithm\nfor solving $\\mathbf{L}\\mathbf{p} = \\mathbf{b}$ unless we are able to take\nadvantage of the structure of the particular update steps that our algorithm\nuses.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:15:04 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Henzinger", "Monika", ""], ["Jin", "Billy", ""], ["Williamson", "David P.", ""]]}, {"id": "2010.16376", "submitter": "David Wajc", "authors": "Sayan Bhattacharya, Fabrizio Grandoni, David Wajc", "title": "Online Edge Coloring Algorithms via the Nibble Method", "comments": "In SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly thirty years ago, Bar-Noy, Motwani and Naor [IPL'92] conjectured that\nan online $(1+o(1))\\Delta$-edge-coloring algorithm exists for $n$-node graphs\nof maximum degree $\\Delta=\\omega(\\log n)$. This conjecture remains open in\ngeneral, though it was recently proven for bipartite graphs under\n\\emph{one-sided vertex arrivals} by Cohen et al.~[FOCS'19]. In a similar vein,\nwe study edge coloring under widely-studied relaxations of the online model.\n  Our main result is in the \\emph{random-order} online model. For this model,\nknown results fall short of the Bar-Noy et al.~conjecture, either in the degree\nbound [Aggarwal et al.~FOCS'03], or number of colors used [Bahmani et\nal.~SODA'10]. We achieve the best of both worlds, thus resolving the Bar-Noy et\nal.~conjecture in the affirmative for this model.\n  Our second result is in the adversarial online (and dynamic) model with\n\\emph{recourse}. A recent algorithm of Duan et al.~[SODA'19] yields a\n$(1+\\epsilon)\\Delta$-edge-coloring with poly$(\\log n/\\epsilon)$ recourse. We\nachieve the same with poly$(1/\\epsilon)$ recourse, thus removing all dependence\non $n$.\n  Underlying our results is one common offline algorithm, which we show how to\nimplement in these two online models. Our algorithm, based on the R\\\"odl Nibble\nMethod, is an adaptation of the distributed algorithm of Dubhashi et\nal.~[TCS'98]. The Nibble Method has proven successful for distributed edge\ncoloring. We display its usefulness in the context of online algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:12:35 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Grandoni", "Fabrizio", ""], ["Wajc", "David", ""]]}]