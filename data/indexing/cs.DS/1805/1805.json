[{"id": "1805.00060", "submitter": "Mathieu Raffinot", "authors": "Tristan Braquelaire and Marie Gasparoux and Mathieu Raffinot and\n  Raluca Uricaru", "title": "On improving the approximation ratio of the r-shortest common\n  superstring problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shortest Common Superstring problem (SCS) consists, for a set of strings\nS = {s_1,...,s_n}, in finding a minimum length string that contains all s_i,\n1<= i <= n, as substrings. While a 2+11/30 approximation ratio algorithm has\nrecently been published, the general objective is now to break the conceptual\nlower bound barrier of 2. This paper is a step ahead in this direction. Here we\nfocus on a particular instance of the SCS problem, meaning the r-SCS problem,\nwhich requires all input strings to be of the same length, r. Golonev et al.\nproved an approximation ratio which is better than the general one for r<= 6.\nHere we extend their approach and improve their approximation ratio, which is\nnow better than the general one for r<= 7, and less than or equal to 2 up to r\n= 6.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 19:00:22 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Braquelaire", "Tristan", ""], ["Gasparoux", "Marie", ""], ["Raffinot", "Mathieu", ""], ["Uricaru", "Raluca", ""]]}, {"id": "1805.00181", "submitter": "Ioannis Koutis", "authors": "Alexandra Kolla, Ioannis Koutis, Vivek Madan, Ali Kemal Sinop", "title": "Spectrally Robust Graph Isomorphism", "comments": "Extended version of a paper appearing in the proceedings of ICALP\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of spectral generalizations of the graph isomorphism\nproblem.\n  (a)The Spectral Graph Dominance (SGD) problem: On input of two graphs $G$ and\n$H$ does there exist a permutation $\\pi$ such that $G\\preceq \\pi(H)$?\n  (b) The Spectrally Robust Graph Isomorphism (SRGI) problem: On input of two\ngraphs $G$ and $H$, find the smallest number $\\kappa$ over all permutations\n$\\pi$ such that $ \\pi(H) \\preceq G\\preceq \\kappa c \\pi(H)$ for some $c$. SRGI\nis a natural formulation of the network alignment problem that has various\napplications, most notably in computational biology.\n  Here $G\\preceq c H$ means that for all vectors $x$ we have $x^T L_G x \\leq c\nx^T L_H x$, where $L_G$ is the Laplacian $G$.\n  We prove NP-hardness for SGD. We also present a $\\kappa$-approximation\nalgorithm for SRGI for the case when both $G$ and $H$ are bounded-degree trees.\nThe algorithm runs in polynomial time when $\\kappa$ is a constant.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 04:18:15 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Kolla", "Alexandra", ""], ["Koutis", "Ioannis", ""], ["Madan", "Vivek", ""], ["Sinop", "Ali Kemal", ""]]}, {"id": "1805.00190", "submitter": "Manoj Gupta", "authors": "Manoj Gupta and Aditi Singh", "title": "Generic Single Edge Fault Tolerant Exact Distance Oracle", "comments": "21 pages, 8 figures. Accepted in 45th International Colloquium on\n  Automata, Languages, and Programming (ICALP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected unweighted graph $G$ and a source set $S$ of $|S| =\n\\sigma $ sources, we want to build a data structure which can process the\nfollowing query {\\sc Q}$(s,t,e):$ find the shortest distance from $s$ to $t$\navoiding an edge $e$, where $s \\in S$ and $t \\in V$. When $\\sigma=n$,\nDemetrescu, Thorup, Chowdhury and Ramachandran (SIAM Journal of Computing,\n2008) designed an algorithm with $\\tilde O(n^2)$ space ($\\tilde O(\\cdot)$ hides\npoly $\\log n$ factor.) and $O(1)$ query time. A natural open question is to\ngeneralize this result to any number of sources. Recently, Bil{\\`o} et. al.\n(STACS 2018) designed a data-structure of size $\\tilde O(\\sigma^{1/2}n^{3/2})$\nwith the query time of $O(\\sqrt{n\\sigma})$ for the above problem. We improve\ntheir result by designing a data-structure of size $\\tilde O(\\sigma^{1/2}\nn^{3/2})$ that can answer queries in $\\tilde O(1)$ time.\n  In a related problem of finding fault tolerant subgraph, Parter and Peleg\n(ESA 2013) showed that if detours of the {\\em replacement} paths ending at a\nvertex $t$ are disjoint, then the number of such paths is $O(\\sqrt{n\\sigma})$.\nThis eventually gives a bound of $O( n \\sqrt{n \\sigma}) =\nO(\\sigma^{1/2}n^{3/2})$ for their problem. {\\em Disjointness of detours} is a\nvery crucial property used in the above result. We show a similar result for a\nsubset of replacement path which \\textbf{may not} be disjoint. This result is\nthe crux of our paper and may be of independent interest.?\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 05:13:25 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Gupta", "Manoj", ""], ["Singh", "Aditi", ""]]}, {"id": "1805.00212", "submitter": "Samson Zhou", "authors": "Vladimir Braverman, Elena Grigorescu, Harry Lang, David P. Woodruff,\n  Samson Zhou", "title": "Nearly Optimal Distinct Elements and Heavy Hitters on Sliding Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distinct elements and $\\ell_p$-heavy hitters problems in the\nsliding window model, where only the most recent $n$ elements in the data\nstream form the underlying set. We first introduce the composable histogram, a\nsimple twist on the exponential (Datar et al., SODA 2002) and smooth histograms\n(Braverman and Ostrovsky, FOCS 2007) that may be of independent interest. We\nthen show that the composable histogram along with a careful combination of\nexisting techniques to track either the identity or frequency of a few specific\nitems suffices to obtain algorithms for both distinct elements and\n$\\ell_p$-heavy hitters that are nearly optimal in both $n$ and $\\epsilon$.\n  Applying our new composable histogram framework, we provide an algorithm that\noutputs a $(1+\\epsilon)$-approximation to the number of distinct elements in\nthe sliding window model and uses $\\mathcal{O}\\left(\\frac{1}{\\epsilon^2}\\log\nn\\log\\frac{1}{\\epsilon}\\log\\log n+\\frac{1}{\\epsilon}\\log^2 n\\right)$ bits of\nspace. For $\\ell_p$-heavy hitters, we provide an algorithm using space\n$\\mathcal{O}\\left(\\frac{1}{\\epsilon^p}\\log^2 n\\left(\\log^2\\log\nn+\\log\\frac{1}{\\epsilon}\\right)\\right)$ for $0<p\\le 2$, improving upon the\nbest-known algorithm for $\\ell_2$-heavy hitters (Braverman et al., COCOON\n2014), which has space complexity $\\mathcal{O}\\left(\\frac{1}{\\epsilon^4}\\log^3\nn\\right)$. We also show complementing nearly optimal lower bounds of\n$\\Omega\\left(\\frac{1}{\\epsilon}\\log^2 n+\\frac{1}{\\epsilon^2}\\log n\\right)$ for\ndistinct elements and $\\Omega\\left(\\frac{1}{\\epsilon^p}\\log^2 n\\right)$ for\n$\\ell_p$-heavy hitters, both tight up to $\\mathcal{O}\\left(\\log\\log n\\right)$\nand $\\mathcal{O}\\left(\\log\\frac{1}{\\epsilon}\\right)$ factors.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 06:56:10 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 23:15:10 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Braverman", "Vladimir", ""], ["Grigorescu", "Elena", ""], ["Lang", "Harry", ""], ["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "1805.00216", "submitter": "Gautam Kamath", "authors": "Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman", "title": "Privately Learning High-Dimensional Distributions", "comments": "To appear in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel, computationally efficient, and differentially private\nalgorithms for two fundamental high-dimensional learning problems: learning a\nmultivariate Gaussian and learning a product distribution over the Boolean\nhypercube in total variation distance. The sample complexity of our algorithms\nnearly matches the sample complexity of the optimal non-private learners for\nthese tasks in a wide range of parameters, showing that privacy comes\nessentially for free for these problems. In particular, in contrast to previous\napproaches, our algorithm for learning Gaussians does not require strong a\npriori bounds on the range of the parameters. Our algorithms introduce a novel\ntechnical approach to reducing the sensitivity of the estimation procedure that\nwe call recursive private preconditioning.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 07:20:46 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 17:38:53 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 04:23:31 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Kamath", "Gautam", ""], ["Li", "Jerry", ""], ["Singhal", "Vikrant", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1805.00321", "submitter": "Ravi Lanka", "authors": "Ravi Lanka", "title": "PURE: Scalable Phase Unwrapping with Spatial Redundant Arcs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase unwrapping is a key problem in many coherent imaging systems, such as\nsynthetic aperture radar (SAR) interferometry. A general formulation for\nredundant integration of finite differences for phase unwrapping (Costantini et\nal., 2010) was shown to produce a more reliable solution by exploiting\nredundant differential estimates. However, this technique requires a commercial\nlinear programming solver for large-scale problems. For a linear cost function,\nwe propose a method based on Dual Decomposition that breaks the given problem\ndefined over a non-planar graph into tractable sub-problems over planar\nsubgraphs. We also propose a decomposition technique that exploits the\nunderlying graph structure for solving the sub-problems efficiently and\nguarantees asymptotic convergence to the globally optimal solution. The\nexperimental results demonstrate that the proposed approach is comparable to\nthe existing state-of-the-art methods in terms of the estimate with a better\nruntime and memory footprint.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:05:07 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 04:13:51 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Lanka", "Ravi", ""]]}, {"id": "1805.00475", "submitter": "Stefanos Kourtis", "authors": "Stefanos Kourtis, Claudio Chamon, Eduardo R. Mucciolo, Andrei E.\n  Ruckenstein", "title": "Fast counting with tensor networks", "comments": "v2: added results for monotone #1-in-3SAT; published version", "journal-ref": "SciPost Phys. 7, 060 (2019)", "doi": "10.21468/SciPostPhys.7.5.060", "report-no": null, "categories": "cond-mat.stat-mech cs.DS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tensor network contraction algorithms for counting satisfying\nassignments of constraint satisfaction problems (#CSPs). We represent each\narbitrary #CSP formula as a tensor network, whose full contraction yields the\nnumber of satisfying assignments of that formula, and use graph theoretical\nmethods to determine favorable orders of contraction. We employ our heuristics\nfor the solution of #P-hard counting boolean satisfiability (#SAT) problems,\nnamely monotone #1-in-3SAT and #Cubic-Vertex-Cover, and find that they\noutperform state-of-the-art solvers by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 18:00:01 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 21:17:31 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Kourtis", "Stefanos", ""], ["Chamon", "Claudio", ""], ["Mucciolo", "Eduardo R.", ""], ["Ruckenstein", "Andrei E.", ""]]}, {"id": "1805.00578", "submitter": "Denis Pankratov", "authors": "Allan Borodin, Christodoulos Karavasilis, Denis Pankratov", "title": "Greedy Bipartite Matching in Random Type Poisson Arrival Model", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new random input model for bipartite matching which we call\nthe Random Type Poisson Arrival Model. Just like in the known i.i.d. model\n(introduced by Feldman et al. 2009), online nodes have types in our model. In\ncontrast to the adversarial types studied in the known i.i.d. model, following\nthe random graphs studied in Mastin and Jaillet 2016, in our model each type\ngraph is generated randomly by including each offline node in the neighborhood\nof an online node with probability $c/n$ independently. In our model, nodes of\nthe same type appear consecutively in the input and the number of times each\ntype node appears is distributed according to the Poisson distribution with\nparameter 1. We analyze the performance of the simple greedy algorithm under\nthis input model. The performance is controlled by the parameter $c$ and we are\nable to exactly characterize the competitive ratio for the regimes $c = o(1)$\nand $c = \\omega(1)$. We also provide a precise bound on the expected size of\nthe matching in the remaining regime of constant $c$. We compare our results to\nthe previous work of Mastin and Jaillet who analyzed the simple greedy\nalgorithm in the $G_{n,n,p}$ model where each online node type occurs exactly\nonce. We essentially show that the approach of Mastin and Jaillet can be\nextended to work for the Random Type Poisson Arrival Model, although several\nnontrivial technical challenges need to be overcome. Intuitively, one can view\nthe Random Type Poisson Arrival Model as the $G_{n,n,p}$ model with less\nrandomness; that is, instead of each online node having a new type, each online\nnode has a chance of repeating the previous type.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 00:06:14 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Borodin", "Allan", ""], ["Karavasilis", "Christodoulos", ""], ["Pankratov", "Denis", ""]]}, {"id": "1805.00612", "submitter": "J\\'er\\'emie Lumbroso", "authors": "J\\'er\\'emie O. Lumbroso", "title": "The Story of HyperLogLog: How Flajolet Processed Streams with Coin Flips", "comments": "18 pages, 1 figure, longer text is a follow-up to talk given at PFAC\n  on 16/12/2011: http://algo.inria.fr/pfac/PFAC/Program_files/Lumbroso.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is a historical introduction to data streaming algorithms that\nwas written as a companion piece to the talk \"How Philippe Flipped Coins to\nCount Data\", given on December 16th, 2011, in the context of the conference in\nhonor of \"Philippe Flajolet and Analytic Combinatorics.\" The narrative was\npieced together through conversations with Philippe Flajolet during my Ph.D.\nthesis under his supervision, as well as several conversations with\ncollaborators after his death. In particular, I am deeply indebted to Nigel\nMartin for his archival records. This article is intended to serve as an\nintroductory text presenting Flajolet's data streaming articles in a projected\nset of complete works.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 03:47:35 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 06:33:29 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Lumbroso", "J\u00e9r\u00e9mie O.", ""]]}, {"id": "1805.00821", "submitter": "Nils Kriege", "authors": "Andre Droschinsky, Nils M. Kriege, Petra Mutzel", "title": "Largest Weight Common Subtree Embeddings with Distance Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest common embeddable subtree problem asks for the largest possible\ntree embeddable into two input trees and generalizes the classical maximum\ncommon subtree problem. Several variants of the problem in labeled and\nunlabeled rooted trees have been studied, e.g., for the comparison of\nevolutionary trees. We consider a generalization, where the sought embedding is\nmaximal with regard to a weight function on pairs of labels. We support rooted\nand unrooted trees with vertex and edge labels as well as distance penalties\nfor skipping vertices. This variant is important for many applications such as\nthe comparison of chemical structures and evolutionary trees. Our algorithm\ncomputes the solution from a series of bipartite matching instances, which are\nsolved efficiently by exploiting their structural relation and imbalance. Our\nanalysis shows that our approach improves or matches the running time of the\nformally best algorithms for several problem variants. Specifically, we obtain\na running time of $\\mathcal O(|T|\\,|T'|\\Delta)$ for two rooted or unrooted\ntrees $T$ and $T'$, where $\\Delta=\\min\\{\\Delta(T),\\Delta(T')\\}$ with\n$\\Delta(X)$ the maximum degree of $X$. If the weights are integral and at most\n$C$, we obtain a running time of $\\mathcal O(|T|\\,|T'|\\sqrt\\Delta\\log\n(C\\min\\{|T|,|T'|\\}))$ for rooted trees.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 14:00:13 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Droschinsky", "Andre", ""], ["Kriege", "Nils M.", ""], ["Mutzel", "Petra", ""]]}, {"id": "1805.00858", "submitter": "Ignasi Sau", "authors": "Luerbio Faria, Sulamita Klein, Ignasi Sau, U\\'everton S. Souza, Rubens\n  Sucupira", "title": "Maximum cuts in edge-colored graphs", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input of the Maximum Colored Cut problem consists of a graph $G=(V,E)$\nwith an edge-coloring $c:E\\to \\{1,2,3,\\ldots , p\\}$ and a positive integer $k$,\nand the question is whether $G$ has a nontrivial edge cut using at least $k$\ncolors. The Colorful Cut problem has the same input but asks for a nontrivial\nedge cut using all $p$ colors. Unlike what happens for the classical Maximum\nCut problem, we prove that both problems are NP-complete even on complete,\nplanar, or bounded treewidth graphs. Furthermore, we prove that Colorful Cut is\nNP-complete even when each color class induces a clique of size at most 3, but\nis trivially solvable when each color induces a $K_2$. On the positive side, we\nprove that Maximum Colored Cut is fixed-parameter tractable when parameterized\nby either $k$ or $p$, by constructing a cubic kernel in both cases.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 15:08:53 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Faria", "Luerbio", ""], ["Klein", "Sulamita", ""], ["Sau", "Ignasi", ""], ["Souza", "U\u00e9verton S.", ""], ["Sucupira", "Rubens", ""]]}, {"id": "1805.00862", "submitter": "Hadrien Van Lierde", "authors": "H. Van Lierde (1), T. W. S. Chow (1), J.-C. Delvenne (2) ((1) City\n  University of Hong Kong, (2) Universite Catholique de Louvain)", "title": "Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs", "comments": "This is the unrefereed Author's Original Version of the article. A\n  peer-reviewed version has been accepted for publication in the Journal of\n  Complex Networks published by Oxford University Press. The present version is\n  not the Accepted Manuscript", "journal-ref": "Journal of Complex Networks, cny011 (2018)", "doi": "10.1093/comnet/cny011", "report-no": null, "categories": "cs.DS cs.CV cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 15:19:49 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Van Lierde", "H.", ""], ["Chow", "T. W. S.", ""], ["Delvenne", "J. -C.", ""]]}, {"id": "1805.01074", "submitter": "Amit Levi", "authors": "Amit Levi, Erik Waingarten", "title": "Lower Bounds for Tolerant Junta and Unateness Testing via Rejection\n  Sampling of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model for testing graph properties which we call the\n\\emph{rejection sampling model}. We show that testing bipartiteness of\n$n$-nodes graphs using rejection sampling queries requires complexity\n$\\widetilde{\\Omega}(n^2)$. Via reductions from the rejection sampling model, we\ngive three new lower bounds for tolerant testing of Boolean functions of the\nform $f\\colon\\{0,1\\}^n\\to \\{0,1\\}$:\n  $\\bullet$Tolerant $k$-junta testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(k^2)$ queries.\n  $\\bullet$Tolerant unateness testing requires $\\widetilde{\\Omega}(n)$ queries.\n  $\\bullet$Tolerant unateness testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(n^{3/2})$ queries.\n  Given the $\\widetilde{O}(k^{3/2})$-query non-adaptive junta tester of Blais\n\\cite{B08}, we conclude that non-adaptive tolerant junta testing requires more\nqueries than non-tolerant junta testing. In addition, given the\n$\\widetilde{O}(n^{3/4})$-query unateness tester of Chen, Waingarten, and Xie\n\\cite{CWX17b} and the $\\widetilde{O}(n)$-query non-adaptive unateness tester of\nBaleshzar, Chakrabarty, Pallavoor, Raskhodnikova, and Seshadhri \\cite{BCPRS17},\nwe conclude that tolerant unateness testing requires more queries than\nnon-tolerant unateness testing, in both adaptive and non-adaptive settings.\nThese lower bounds provide the first separation between tolerant and\nnon-tolerant testing for a natural property of Boolean functions.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 01:08:59 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Levi", "Amit", ""], ["Waingarten", "Erik", ""]]}, {"id": "1805.01209", "submitter": "Austin Benson", "authors": "Austin R. Benson and Jon Kleinberg", "title": "Found Graph Data and Planted Vertex Covers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical way in which network data is recorded is to measure all the\ninteractions among a specified set of core nodes; this produces a graph\ncontaining this core together with a potentially larger set of fringe nodes\nthat have links to the core. Interactions between pairs of nodes in the fringe,\nhowever, are not recorded by this process, and hence not present in the\nresulting graph data. For example, a phone service provider may only have\nrecords of calls in which at least one of the participants is a customer; this\ncan include calls between a customer and a non-customer, but not between pairs\nof non-customers.\n  Knowledge of which nodes belong to the core is an important piece of metadata\nthat is crucial for interpreting the network dataset. But in many cases, this\nmetadata is not available, either because it has been lost due to difficulties\nin data provenance, or because the network consists of found data obtained in\nsettings such as counter-surveillance. This leads to a natural algorithmic\nproblem, namely the recovery of the core set. Since the core set forms a vertex\ncover of the graph, we essentially have a planted vertex cover problem, but\nwith an arbitrary underlying graph. We develop a theoretical framework for\nanalyzing this planted vertex cover problem, based on results in the theory of\nfixed-parameter tractability, together with algorithms for recovering the core.\nOur algorithms are fast, simple to implement, and out-perform several methods\nbased on network core-periphery structure on various real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 10:24:36 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Benson", "Austin R.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1805.01299", "submitter": "Felix Hommelsheim", "authors": "Felix Hommelsheim and Moritz M\\\"uhlenthaler and Oliver Schaudt", "title": "How to Secure Matchings Against Edge Failures", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are given a bipartite graph that admits a perfect matching and an\nadversary may delete any edge from the graph with the intention of destroying\nall perfect matchings. We consider the task of adding a minimum cost edge-set\nto the graph, such that the adversary never wins. We provide efficient exact\nand approximation algorithms. In particular, for the unit-cost problem, we\nprovide a $\\log_2 n$-factor approximation algorithm and a polynomial-time\nalgorithm for chordal-bipartite graphs. Furthermore, we give a fixed parameter\nalgorithm for the problem parameterized by the treewidth of the input graph.\nFor general non-negative weights we settle the approximability of the problem\nand show a close relation to the Directed Steiner Forest Problem. Additionally\nwe prove a dichotomy theorem characterizing minor-closed graph classes which\nallow for a polynomial-time algorithm. Our methods rely on a close relationship\nto the classical strong connectivity augmentation problem and directed Steiner\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:32:22 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 08:35:45 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 10:38:18 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Hommelsheim", "Felix", ""], ["M\u00fchlenthaler", "Moritz", ""], ["Schaudt", "Oliver", ""]]}, {"id": "1805.01310", "submitter": "Martin Schirneck", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Julius Lischeid, Kitty Meeks, and\n  Martin Schirneck", "title": "Efficiently Enumerating Hitting Sets of Hypergraphs Arising in Data\n  Profiling", "comments": "26 pages, 8 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a method to enumerate the inclusion-wise minimal hitting sets of a\nhypergraph. The algorithm has delay $O( m^{k^*+1} \\, n^2)$ on $n$-vertex,\n$m$-edge hypergraphs, where $k^*$ is the rank of the transversal hypergraph,\ni.e., the cardinality of the largest minimal solution. In particular, on\nclasses of hypergraphs for which $k^*$ is bounded, the delay is polynomial. The\nalgorithm uses space linear in the input size only. The enumeration methods\nsolves the extension problem for minimal hitting sets as a subroutine. We show\nthat this problem, parameterised by the cardinality of the set which is to be\nextended, is one of the first natural W[3]-complete problems. We give an\nalgorithm for the subroutine that is optimal under the assumption that $W[2]\n\\neq \\mathrm{FPT}$ or the exponential time hypothesis (ETH), respectively.\nDespite the hardness of the extension problem, we provide empirical evidence\nindicating that the enumeration outperforms its theoretical worst-case\nguarantee on hypergraphs arising in the profiling of relational databases,\nnamely, in the detection of unique column combinations. Our analysis suggest\nthat these hypergraphs exhibit structure that allows the subroutine to be fast\non average.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:53:08 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 16:01:33 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["Lischeid", "Julius", ""], ["Meeks", "Kitty", ""], ["Schirneck", "Martin", ""]]}, {"id": "1805.01311", "submitter": "Meghana Nasre Ms.", "authors": "Krishnapriya A M, Meghana Nasre, Prajakta Nimbhorkar, Amit Rawat", "title": "How good are Popular Matchings?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Hospital Residents problem (HR) and the\nHospital Residents problem with Lower Quotas (HRLQ). In this model with two\nsided preferences, stability is a well accepted notion of optimality. However,\nin the presence of lower quotas, a stable and feasible matching need not exist.\nFor the HRLQ problem, our goal therefore is to output a good feasible matching\nassuming that a feasible matching exists. Computing matchings with minimum\nnumber of blocking pairs (MinBP) and minimum number of blocking residents\n(MinBR) are known to be NP-Complete. The only approximation algorithms for\nthese problems work under severe restrictions on the preference lists. We\npresent an algorithm which circumvents this restriction and computes a popular\nmatching in the HRLQ instance. We show that on data-sets generated using\nvarious generators, our algorithm performs very well in terms of blocking pairs\nand blocking residents. Yokoi (ISAAC 2017) recently studied envy-free matchings\nfor the HRLQ problem. We propose a simple modification to Yokoi's algorithm to\noutput a maximal envy-free matching. We observe that popular matchings\noutperform envy-free matchings on several parameters of practical importance,\nlike size, number of blocking pairs, number of blocking residents.\n  In the absence of lower quotas, that is, in the Hospital Residents (HR)\nproblem, stable matchings are guaranteed to exist. Even in this case, we show\nthat popularity is a practical alternative to stability. For instance, on\nsynthetic data-sets generated using a particular model, as well as on real\nworld data-sets, a popular matching is on an average 8-10% larger in size,\nmatches more number of residents to their top-choice, and more residents prefer\nthe popular matching as compared to a stable matching. Our comprehensive study\nreveals the practical appeal of popular matchings for the HR and HRLQ problems.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 17:20:34 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["M", "Krishnapriya A", ""], ["Nasre", "Meghana", ""], ["Nimbhorkar", "Prajakta", ""], ["Rawat", "Amit", ""]]}, {"id": "1805.01407", "submitter": "Sebastiano Vigna", "authors": "David Blackman, Sebastiano Vigna", "title": "Scrambled Linear Pseudorandom Number Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear pseudorandom number generators are very popular due to their high\nspeed, to the ease with which generators with a sizable state space can be\ncreated, and to their provable theoretical properties. However, they suffer\nfrom linear artifacts which show as failures in linearity-related statistical\ntests such as the binary-rank and the linear-complexity test. In this paper, we\ngive three new contributions. First, we introduce two new linear\ntransformations that have been handcrafted to have good statistical properties\nand at the same time to be programmable very efficiently on superscalar\nprocessors, or even directly in hardware. Then, we describe a new test for\nHamming-weight dependencies that is able to discover subtle, previously unknown\nbiases in existing generators (in particular, in linear ones). Finally, we\ndescribe a number of scramblers, that is, nonlinear functions applied to the\nstate array that reduce or delete the linear artifacts, and propose\ncombinations of linear transformations and scramblers that give extremely fast\npseudorandom generators of high quality. A novelty in our approach is that we\nuse ideas from the theory of filtered linear-feedback shift register to prove\nsome properties of our scramblers, rather than relying purely on heuristics. In\nthe end, we provide simple, extremely fast generators that use a few hundred\nbits of memory, have provable properties and pass very strong statistical\ntests.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 16:22:23 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 11:01:41 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Blackman", "David", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1805.01419", "submitter": "Eduar Castrillo Velilla", "authors": "Eduar Castrillo, Elizabeth Le\\'on, Jonatan G\\'omez", "title": "Dynamic Structural Similarity on Graphs", "comments": "8 pages, 7 figures, 1 table, Submitted for peer-review to the\n  conference ASONAM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way of characterizing the topological and structural properties of\nvertices and edges in a graph is by using structural similarity measures.\nMeasures like Cosine, Jaccard and Dice compute the similarities restricted to\nthe immediate neighborhood of the vertices, bypassing important structural\nproperties beyond the locality. Others measures, such as the generalized edge\nclustering coefficient, go beyond the locality but with high computational\ncomplexity, making them impractical in large-scale scenarios. In this paper we\npropose a novel similarity measure that determines the structural similarity by\ndynamically diffusing and capturing information beyond the locality. This new\nsimilarity is modeled as an iterated function that can be solved by fixed point\niteration in super-linear time and memory complexity, so it is able to analyze\nlarge-scale graphs. In order to show the advantages of the proposed similarity\nin the community detection task, we replace the local structural similarity\nused in the SCAN algorithm with the proposed similarity measure, improving the\nquality of the detected community structure and also reducing the sensitivity\nto the parameter $\\epsilon$ of the SCAN algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 16:44:54 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Castrillo", "Eduar", ""], ["Le\u00f3n", "Elizabeth", ""], ["G\u00f3mez", "Jonatan", ""]]}, {"id": "1805.01684", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, George B. Mertzios, Felix Reidl", "title": "Lower and Upper Bound for Computing the Size of All Second\n  Neighbourhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the size of each $r$-neighbourhood for\nevery vertex of a graph. Specifically, we ask whether the size of the closed\nsecond neighbourhood can be computed in subquadratic time.\n  Adapting the SETH reductions by Abboud et al. (2016) that exclude\nsubquadratic algorithms to compute the radius of a graph, we find that a\nsubquadratic algorithm would violate the SETH. On the other hand, a linear\nfpt-time algorithm by Demaine et al. (2014) parameterized by a certain\n`sparseness parameter' of the graph is known, where the dependence on the\nparameter is exponential. We show here that a better dependence is unlikely:\nfor any~$\\delta < 2$, no algorithm running in time $O(2^{o({\\rm vc}(G))} \\,\nn^{\\delta})$, where~${\\rm vc}(G)$ is the vertex cover number, is possible\nunless the SETH fails.\n  We supplement these lower bounds with algorithms that solve the problem in\ntime~$O(2^{{\\rm vc}(G)/2} {\\rm vc}(G)^2 \\cdot n)$ and $O(2^w w \\cdot n)$.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 09:37:57 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Gutin", "Gregory", ""], ["Mertzios", "George B.", ""], ["Reidl", "Felix", ""]]}, {"id": "1805.01876", "submitter": "Nicola Prezza", "authors": "Nicola Prezza, Nadia Pisanti, Marinella Sciortino, Giovanna Rosone", "title": "Detecting Mutations by eBWT", "comments": "simplified Proposition 4; extended Thm 2 to ambiguous clusters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a theory describing how the extended Burrows-Wheeler\nTransform (eBWT) of a collection of DNA fragments tends to cluster together the\ncopies of nucleotides sequenced from a genome G. Our theory accurately predicts\nhow many copies of any nucleotide are expected inside each such cluster, and\nhow an elegant and precise LCP array based procedure can locate these clusters\nin the eBWT. Our findings are very general and can be applied to a wide range\nof different problems. In this paper, we consider the case of alignment-free\nand reference-free SNPs discovery in multiple collections of reads. We note\nthat, in accordance with our theoretical results, SNPs are clustered in the\neBWT of the reads collection, and we develop a tool finding SNPs with a simple\nscan of the eBWT and LCP arrays. Preliminary results show that our method\nrequires much less coverage than state-of-the-art tools while drastically\nimproving precision and sensitivity.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:44:48 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 07:02:44 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 07:19:19 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Prezza", "Nicola", ""], ["Pisanti", "Nadia", ""], ["Sciortino", "Marinella", ""], ["Rosone", "Giovanna", ""]]}, {"id": "1805.02014", "submitter": "Mark Velednitsky", "authors": "Minjun Chang, Dorit S. Hochbaum, Quico Spaen, Mark Velednitsky", "title": "DISPATCH: An Optimally-Competitive Algorithm for Maximum Online Perfect\n  Bipartite Matching with i.i.d. Arrivals", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-04693-4_10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an optimally-competitive algorithm for the problem of\nmaximum weighted online perfect bipartite matching with i.i.d. arrivals. In\nthis problem, we are given a known set of workers, a distribution over job\ntypes, and non-negative utility weights for each pair of worker and job types.\nAt each time step, a job is drawn i.i.d. from the distribution over job types.\nUpon arrival, the job must be irrevocably assigned to a worker and cannot be\ndropped. The goal is to maximize the expected sum of utilities after all jobs\nare assigned.\n  We introduce DISPATCH, a 0.5-competitive, randomized algorithm. We also prove\nthat 0.5-competitive is the best possible. DISPATCH first selects a \"preferred\nworker\" and assigns the job to this worker if it is available. The preferred\nworker is determined based on an optimal solution to a fractional\ntransportation problem. If the preferred worker is not available, DISPATCH\nrandomly selects a worker from the available workers. We show that DISPATCH\nmaintains a uniform distribution over the workers even when the distribution\nover the job types is non-uniform.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 07:09:46 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 00:33:44 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 11:08:14 GMT"}, {"version": "v4", "created": "Sat, 13 Oct 2018 06:18:11 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Chang", "Minjun", ""], ["Hochbaum", "Dorit S.", ""], ["Spaen", "Quico", ""], ["Velednitsky", "Mark", ""]]}, {"id": "1805.02026", "submitter": "Fotis Iliopoulos", "authors": "Dimitris Achlioptas, Fotis Iliopoulos, Alistair Sinclair", "title": "Beyond the Lovasz Local Lemma: Point to Set Correlations and Their\n  Algorithmic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the groundbreaking algorithm of Moser and Tardos for the Lovasz\nLocal Lemma (LLL), there has been a plethora of results analyzing local search\nalgorithms for various constraint satisfaction problems. The algorithms\nconsidered fall into two broad categories: resampling algorithms, analyzed via\ndifferent algorithmic LLL conditions; and backtracking algorithms, analyzed via\nentropy compression arguments. This paper introduces a new convergence\ncondition that seamlessly handles resampling, backtracking, and hybrid\nalgorithms, i.e., algorithms that perform both resampling and backtracking\nsteps. Unlike all past LLL work, our condition replaces the notion of a\ndependency or causality graph by quantifying point-to-set correlations between\nbad events. As a result, our condition simultaneously: (i)~captures the most\ngeneral algorithmic LLL condition known as a special case; (ii)~significantly\nsimplifies the analysis of entropy compression applications; (iii)~relates\nbacktracking algorithms, which are conceptually very different from resampling\nalgorithms, to the LLL; and most importantly (iv)~allows for the analysis of\nhybrid algorithms, which were outside the scope of previous techniques. We give\nseveral applications of our condition, including a new hybrid vertex coloring\nalgorithm that extends the recent breakthrough result of Molloy for coloring\ntriangle-free graphs to arbitrary graphs.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 09:15:00 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 01:56:00 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 20:03:20 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 01:24:53 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Achlioptas", "Dimitris", ""], ["Iliopoulos", "Fotis", ""], ["Sinclair", "Alistair", ""]]}, {"id": "1805.02033", "submitter": "Stefano Leucci", "authors": "Stefano Leucci, Chih-Hung Liu", "title": "Approximate Minimum Selection with Unreliable Comparisons in Optimal\n  Expected Time", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the \\emph{approximate minimum selection} problem in presence of\n\\emph{independent random comparison faults}. This problem asks to select one of\nthe smallest $k$ elements in a linearly-ordered collection of $n$ elements by\nonly performing \\emph{unreliable} pairwise comparisons: whenever two elements\nare compared, there is a constant probability that the wrong answer is\nreturned.\n  We design a randomized algorithm that solves this problem with probability\n$1-q \\in [ \\frac{1}{2}, 1)$ and for the whole range of values of $k$ using $O(\n\\frac{n}{k} \\log \\frac{1}{q} )$ expected time. Then, we prove that the expected\nrunning time of any algorithm that succeeds w.h.p. must be\n$\\Omega(\\frac{n}{k}\\log \\frac{1}{q})$, thus implying that our algorithm is\nasymptotically optimal, in expectation. These results are quite surprising in\nthe sense that for $k$ between $\\Omega(\\log \\frac{1}{q})$ and $c \\cdot n$, for\nany constant $c<1$, the expected running time must still be\n$\\Omega(\\frac{n}{k}\\log \\frac{1}{q})$ even in absence of comparison faults.\nInformally speaking, we show how to deal with comparison errors without any\nsubstantial complexity penalty w.r.t.\\ the fault-free case. Moreover, we prove\nthat as soon as $k = O( \\frac{n}{\\log\\log \\frac{1}{q}})$, it is possible to\nachieve the optimal \\emph{worst-case} running time of $\\Theta(\\frac{n}{k}\\log\n\\frac{1}{q})$.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 10:14:51 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 02:50:42 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 00:15:35 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Leucci", "Stefano", ""], ["Liu", "Chih-Hung", ""]]}, {"id": "1805.02200", "submitter": "Xingbo Wu", "authors": "Xingbo Wu, Fan Ni, Song Jiang", "title": "Wormhole: A Fast Ordered Index for In-memory Data Management", "comments": "15 pages; 18 figures; 1 table", "journal-ref": null, "doi": "10.1145/1810479.1810540", "report-no": null, "categories": "cs.DB cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-memory data management systems, such as key-value stores, have become an\nessential infrastructure in today's big-data processing and cloud computing.\nThey rely on efficient index structures to access data. While unordered\nindexes, such as hash tables, can perform point search with O(1) time, they\ncannot be used in many scenarios where range queries must be supported. Many\nordered indexes, such as B+ tree and skip list, have a O(log N) lookup cost,\nwhere N is number of keys in an index. For an ordered index hosting billions of\nkeys, it may take more than 30 key-comparisons in a lookup, which is an order\nof magnitude more expensive than that on a hash table. With availability of\nlarge memory and fast network in today's data centers, this O(log N) time is\ntaking a heavy toll on applications that rely on ordered indexes.\n  In this paper we introduce a new ordered index structure, named Wormhole,\nthat takes O(log L) worst-case time for looking up a key with a length of L.\nThe low cost is achieved by simultaneously leveraging strengths of three\nindexing structures, namely hash table, prefix tree, and B+ tree, to\norchestrate a single fast ordered index. Wormhole's range operations can be\nperformed by a linear scan of a list after an initial lookup. This improvement\nof access efficiency does not come at a price of compromised space efficiency.\nInstead, Wormhole's index space is comparable to those of B+ tree and skip\nlist. Experiment results show that Wormhole outperforms skip list, B+ tree,\nART, and Masstree by up to 8.4x, 4.9x, 4.3x, and 6.6x in terms of key lookup\nthroughput, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 12:31:28 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 03:49:13 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wu", "Xingbo", ""], ["Ni", "Fan", ""], ["Jiang", "Song", ""]]}, {"id": "1805.02206", "submitter": "Arnold Filtser", "authors": "Arnold Filtser and Nimrod Talmon", "title": "Distributed Monitoring of Election Winners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed elections, where there is a center and $k$ sites. In\nsuch distributed elections, each voter has preferences over some set of\ncandidates, and each voter is assigned to exactly one site such that each site\nis aware only of the voters assigned to it. The center is able to directly\ncommunicate with all sites. We are interested in designing\ncommunication-efficient protocols, allowing the center to maintain a candidate\nwhich, with arbitrarily high probability, is guaranteed to be a winner, or at\nleast close to being a winner. We consider various single-winner voting rules,\nsuch as variants of Approval voting and scoring rules, tournament-based voting\nrules, and several round-based voting rules. For the voting rules we consider,\nwe show that, using communication which is logarithmic in the number of voters,\nit is possible for the center to maintain such approximate winners; that is,\nupon a query at any time the center can immediately return a candidate which is\nguaranteed to be an approximate winner with high probability. We complement our\nprotocols with lower bounds. Our results are theoretical in nature and relate\nto various scenarios, such as aggregating customer preferences in online\nshopping websites or supermarket chains and collecting votes from different\npolling stations of political elections.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 13:16:26 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 08:27:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Filtser", "Arnold", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1805.02217", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty, Maryam Negahbani", "title": "Generalized Center Problems with Outliers", "comments": "To appear in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $\\mathcal{F}$-center problem with outliers: given a metric space\n$(X,d)$, a general down-closed family $\\mathcal{F}$ of subsets of $X$, and a\nparameter $m$, we need to locate a subset $S\\in \\mathcal{F}$ of centers such\nthat the maximum distance among the closest $m$ points in $X$ to $S$ is\nminimized. Our main result is a dichotomy theorem. Colloquially, we prove that\nthere is an efficient $3$-approximation for the $\\mathcal{F}$-center problem\nwith outliers if and only if we can efficiently optimize a poly-bounded linear\nfunction over $\\mathcal{F}$ subject to a partition constraint. One concrete\nupshot of our result is a polynomial time $3$-approximation for the knapsack\ncenter problem with outliers for which no (true) approximation algorithm was\nknown.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 14:10:47 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Negahbani", "Maryam", ""]]}, {"id": "1805.02244", "submitter": "Shi Li", "authors": "Shi Li", "title": "On Facility Location with General Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give the first constant approximation algorithm for the\nlower bounded facility location (LBFL) problem with general lower bounds. Prior\nto our work, such algorithms were only known for the special case where all\nfacilities have the same lower bound: Svitkina \\cite{Svi10} gave a\n$448$-approximation for the special case, and subsequently Ahmadian and Swamy\n\\cite{AS13} improved the approximation factor to 82.6.\n  As in \\cite{Svi10} and \\cite{AS13}, our algorithm for LBFL with general lower\nbounds works by reducing the problem to the capacitated facility location (CFL)\nproblem. To handle some challenges caused by the general lower bounds, our\nalgorithm involves more reduction steps. One main complication is that after\naggregation of clients and facilities at a few locations, each of these\nlocations may contain many facilities with different opening costs and lower\nbounds. To handle this issue, we introduce and reduce our LBFL problem to an\nintermediate problem called the transportation with configurable supplies and\ndemands (TCSD) problem, which in turn can be reduced to the CFL problem.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 16:54:45 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Li", "Shi", ""]]}, {"id": "1805.02349", "submitter": "Tselil Schramm", "authors": "Boaz Barak, Chi-Ning Chou, Zhixian Lei, Tselil Schramm, Yueqi Sheng", "title": "(Nearly) Efficient Algorithms for the Graph Matching Problem on\n  Correlated Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a quasipolynomial time algorithm for the graph matching problem (also\nknown as noisy or robust graph isomorphism) on correlated random graphs.\nSpecifically, for every $\\gamma>0$, we give a $n^{O(\\log n)}$ time algorithm\nthat given a pair of $\\gamma$-correlated $G(n,p)$ graphs $G_0,G_1$ with average\ndegree between $n^{\\varepsilon}$ and $n^{1/153}$ for $\\varepsilon = o(1)$,\nrecovers the \"ground truth\" permutation $\\pi\\in S_n$ that matches the vertices\nof $G_0$ to the vertices of $G_n$ in the way that minimizes the number of\nmismatched edges. We also give a recovery algorithm for a denser regime, and a\npolynomial-time algorithm for distinguishing between correlated and\nuncorrelated graphs.\n  Prior work showed that recovery is information-theoretically possible in this\nmodel as long the average degree was at least $\\log n$, but sub-exponential\ntime algorithms were only known in the dense case (i.e., for $p > n^{-o(1)}$).\nMoreover, \"Percolation Graph Matching\", which is the most common heuristic for\nthis problem, has been shown to require knowledge of $n^{\\Omega(1)}$ \"seeds\"\n(i.e., input/output pairs of the permutation $\\pi$) to succeed in this regime.\nIn contrast our algorithms require no seed and succeed for $p$ which is as low\nas $n^{o(1)-1}$.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 05:38:41 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 23:32:37 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Barak", "Boaz", ""], ["Chou", "Chi-Ning", ""], ["Lei", "Zhixian", ""], ["Schramm", "Tselil", ""], ["Sheng", "Yueqi", ""]]}, {"id": "1805.02351", "submitter": "Kaifeng Lyu", "authors": "Lijie Chen, Shafi Goldwasser, Kaifeng Lyu, Guy N. Rothblum, Aviad\n  Rubinstein", "title": "Fine-grained Complexity Meets IP = PSPACE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the fine-grained complexity of finding exact and\napproximate solutions to problems in P. Our main contribution is showing\nreductions from exact to approximate solution for a host of such problems.\n  As one (notable) example, we show that the Closest-LCS-Pair problem (Given\ntwo sets of strings $A$ and $B$, compute exactly the maximum $\\textsf{LCS}(a,\nb)$ with $(a, b) \\in A \\times B$) is equivalent to its approximation version\n(under near-linear time reductions, and with a constant approximation factor).\nMore generally, we identify a class of problems, which we call BP-Pair-Class,\ncomprising both exact and approximate solutions, and show that they are all\nequivalent under near-linear time reductions.\n  Exploring this class and its properties, we also show:\n  $\\bullet$ Under the NC-SETH assumption (a significantly more relaxed\nassumption than SETH), solving any of the problems in this class requires\nessentially quadratic time.\n  $\\bullet$ Modest improvements on the running time of known algorithms\n(shaving log factors) would imply that NEXP is not in non-uniform\n$\\textsf{NC}^1$.\n  $\\bullet$ Finally, we leverage our techniques to show new barriers for\ndeterministic approximation algorithms for LCS.\n  At the heart of these new results is a deep connection between interactive\nproof systems for bounded-space computations and the fine-grained complexity of\nexact and approximate solutions to problems in P. In particular, our results\nbuild on the proof techniques from the classical IP = PSPACE result.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 05:50:12 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 08:05:36 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Chen", "Lijie", ""], ["Goldwasser", "Shafi", ""], ["Lyu", "Kaifeng", ""], ["Rothblum", "Guy N.", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1805.02412", "submitter": "Oscar Defrain", "authors": "Oscar Defrain and Lhouari Nourine", "title": "Neighborhood inclusions for minimal dominating sets enumeration: linear\n  and polynomial delay algorithms in $P_7$-free and $P_8$-free chordal graphs", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [M. M. Kant\\'e, V. Limouzy, A. Mary, and L. Nourine. On the enumeration of\nminimal dominating sets and related notions. SIAM Journal on Discrete\nMathematics, 28(4):1916-1929, 2014] the authors give an $O(n+m)$ delay\nalgorithm based on neighborhood inclusions for the enumeration of minimal\ndominating sets in split and $P_6$-free chordal graphs. In this paper, we\ninvestigate generalizations of this technique to $P_k$-free chordal graphs for\nlarger integers $k$. In particular, we give $O(n+m)$ and $O(n^3\\cdot m)$ delays\nalgorithms in the classes of $P_7$-free and $P_8$-free chordal graphs. As for\n$P_k$-free chordal graphs for $k\\geq 9$, we give evidence that such a technique\nis inefficient as a key step of the algorithm, namely the irredundant extension\nproblem, becomes NP-complete.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 09:34:22 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 13:45:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Defrain", "Oscar", ""], ["Nourine", "Lhouari", ""]]}, {"id": "1805.02457", "submitter": "Parter Merav", "authors": "Merav Parter", "title": "$(\\Delta+1)$ Coloring in the Congested Clique Model", "comments": "Appeared in ICALP'18 (the update version adds a missing part in the\n  deterministic coloring procedure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present improved algorithms for the $(\\Delta+1)$ (vertex)\ncoloring problem in the Congested-Clique model of distributed computing. In\nthis model, the input is a graph on $n$ nodes, initially each node knows only\nits incident edges, and per round each two nodes can exchange $O(\\log n)$ bits\nof information.\n  Our key result is a randomized $(\\Delta+1)$ vertex coloring algorithm that\nworks in $O(\\log\\log \\Delta \\cdot \\log^* \\Delta)$-rounds. This is achieved by\ncombining the recent breakthrough result of [Chang-Li-Pettie, STOC'18] in the\n\\local\\ model and a degree reduction technique. We also get the following\nresults with high probability: (1) $(\\Delta+1)$-coloring for $\\Delta=O((n/\\log\nn)^{1-\\epsilon})$ for any $\\epsilon \\in (0,1)$, within\n$O(\\log(1/\\epsilon)\\log^* \\Delta)$ rounds, and (2)\n$(\\Delta+\\Delta^{1/2+o(1)})$-coloring within $O(\\log^* \\Delta)$ rounds. Turning\nto deterministic algorithms, we show a $(\\Delta+1)$-coloring algorithm that\nworks in $O(\\log \\Delta)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 11:57:15 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 12:24:21 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "1805.02526", "submitter": "Andreas T\\\"onnis", "authors": "Max Klimm, Daniel Schmand, Andreas T\\\"onnis", "title": "The Online Best Reply Algorithm for Resource Allocation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of a best reply algorithm for online resource\nallocation problems with a diseconomy of scale. In an online resource\nallocation problem, we are given a set of resources and a set of requests that\narrive in an online manner. Each request consists of a set of feasible\nallocations and an allocation is a set of resources. The total cost of an\nallocation vector is given by the sum of the resources' costs, where each\nresource's cost depends on the total load on the resource under the allocation\nvector. We analyze the natural online procedure where each request is allocated\ngreedily to a feasible set of resources that minimizes the individual cost of\nthat particular request. In the literature, this algorithm is also known as a\none-round walk-in congestion games starting from the empty state. For\nunweighted resource allocation problems with polynomial cost functions with\nmaximum degree $d$, upper bounds on the competitive ratio of this greedy\nalgorithm were known only for the special cases $d\\in\\{1, 2, 3\\}$. In this\npaper, we show a general upper bound on the competitive ratio of $d(d /\nW(\\frac{1.2d-1}{d+1}))^{d+1}$ for the unweighted case where $W$ denotes the\nLambert-W function on $\\mathbb{R}_{\\geq 0}$. For the weighted case, we show\nthat the competitive ratio of the greedy algorithm is bounded from above by\n$(d/W(\\frac{d}{d+1}))^{d+1}$.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 13:44:15 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 15:46:11 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Klimm", "Max", ""], ["Schmand", "Daniel", ""], ["T\u00f6nnis", "Andreas", ""]]}, {"id": "1805.02851", "submitter": "Meghana Nasre Ms.", "authors": "Meghana Nasre and Prajakta Nimbhorkar and Nada Pulath", "title": "Dichotomy Results for Classified Rank-Maximal Matchings and Popular\n  Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of computing an optimal matching in a\nbipartite graph where elements of one side of the bipartition specify\npreferences over the other side, and one or both sides can have capacities and\nclassifications. The input instance is a bipartite graph G=(A U P,E), where A\nis a set of applicants, P is a set of posts, and each applicant ranks its\nneighbors in an order of preference, possibly involving ties. Moreover, each\nvertex v in A U P has a quota q(v) denoting the maximum number of partners it\ncan have in any allocation of applicants to posts - referred to as a {\\em\nmatching} in this paper. A classification for a vertex u is a collection of\nsubsets of neighbors of u. Each subset (class) has an upper quota denoting the\nmaximum number of vertices from the class that can be matched to u. The goal is\nto find a matching that is optimal amongst all the feasible matchings, which\nare matchings that respect quotas of all the vertices and classes.\n  We consider two well-studied notions of optimality namely popularity and\nrank-maximality. The notion of rank-maximality involves finding a matching in\n$G$ with maximum number of rank-$1$ edges, subject to that, maximum number of\nrank-2 edges and so on. We present an O(|E|^2)-time algorithm for finding a\nfeasible rank-maximal matching, when each classification is a laminar family.\nWe complement this with an NP-hardness result when classes are non-laminar even\nunder strict preference lists, and even when only posts have classifications,\nand each applicant has a quota of one. We show an analogous dichotomy result\nfor computing a popular matching amongst feasible matchings (if one exists) in\na bipartite graph with posts having capacities and classifications and\napplicants having a quota of one.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 06:16:23 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 05:57:10 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Nasre", "Meghana", ""], ["Nimbhorkar", "Prajakta", ""], ["Pulath", "Nada", ""]]}, {"id": "1805.02874", "submitter": "Ferran Alet", "authors": "Ferran Alet, Rohan Chitnis, Leslie P. Kaelbling, Tomas Lozano-Perez", "title": "Finding Frequent Entities in Continuous Data", "comments": null, "journal-ref": "IJCAI 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications that involve processing high-dimensional data, it is\nimportant to identify a small set of entities that account for a significant\nfraction of detections. Rather than formalize this as a clustering problem, in\nwhich all detections must be grouped into hard or soft categories, we formalize\nit as an instance of the frequent items or heavy hitters problem, which finds\ngroups of tightly clustered objects that have a high density in the feature\nspace. We show that the heavy hitters formulation generates solutions that are\nmore accurate and effective than the clustering formulation. In addition, we\npresent a novel online algorithm for heavy hitters, called HAC, which addresses\nproblems in continuous space, and demonstrate its effectiveness on real video\nand household domains.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 07:52:19 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Alet", "Ferran", ""], ["Chitnis", "Rohan", ""], ["Kaelbling", "Leslie P.", ""], ["Lozano-Perez", "Tomas", ""]]}, {"id": "1805.02974", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Xiaorui Sun, Omri Weinstein", "title": "Massively Parallel Algorithms for Finding Well-Connected Components in\n  Sparse Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question that shrouds the emergence of massively parallel\ncomputing (MPC) platforms is how can the additional power of the MPC paradigm\nbe leveraged to achieve faster algorithms compared to classical parallel models\nsuch as PRAM?\n  Previous research has identified the sparse graph connectivity problem as a\nmajor obstacle to such improvement: While classical logarithmic-round PRAM\nalgorithms for finding connected components in any $n$-vertex graph have been\nknown for more than three decades, no $o(\\log{n})$-round MPC algorithms are\nknown for this task with truly sublinear in $n$ memory per machine. This\nproblem arises when processing massive yet sparse graphs with $O(n)$ edges, for\nwhich the interesting setting of parameters is $n^{1-\\Omega(1)}$ memory per\nmachine. It is conjectured that achieving an $o(\\log{n})$-round algorithm for\nconnectivity on general sparse graphs with $n^{1-\\Omega(1)}$ per-machine memory\nmay not be possible, and this conjecture also forms the basis for multiple\nconditional hardness results on the round complexity of other problems in the\nMPC model.\n  We take an opportunistic approach towards the sparse graph connectivity\nproblem, by designing an algorithm with improved performance guarantees in\nterms of the connectivity structure of the input graph. Formally, we design an\nalgorithm that finds all connected components with spectral gap at least\n$\\lambda$ in a graph in $O(\\log\\log{n} + \\log{(1/\\lambda)})$ MPC rounds and\n$n^{\\Omega(1)}$ memory per machine. As such, this algorithm achieves an\nexponential round reduction on sparse \"well-connected\" components (i.e.,\n$\\lambda \\geq 1/\\text{polylog}{(n)}$) using only $n^{\\Omega(1)}$ memory per\nmachine and $\\widetilde{O}(n)$ total memory, and still operates in $o(\\log n)$\nrounds even when $\\lambda = 1/n^{o(1)}$.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 12:29:21 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Assadi", "Sepehr", ""], ["Sun", "Xiaorui", ""], ["Weinstein", "Omri", ""]]}, {"id": "1805.02994", "submitter": "Christine Markarian Dr", "authors": "Christine Markarian", "title": "Online Connected Dominating Set Leasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \\emph{Online Connected Dominating Set Leasing} problem\n(OCDSL) in which we are given an undirected connected graph $G = (V, E)$, a set\n$\\mathcal{L}$ of lease types each characterized by a duration and cost, and a\nsequence of subsets of $V$ arriving over time. A node can be leased using lease\ntype $l$ for cost $c_l$ and remains active for time $d_l$. The adversary gives\nin each step $t$ a subset of nodes that need to be dominated by a connected\nsubgraph consisting of nodes active at time $t$. The goal is to minimize the\ntotal leasing costs. OCDSL contains the \\emph{Parking Permit\nProblem}~\\cite{PPP} as a special subcase and generalizes the classical offline\n\\emph{Connected Dominating Set} problem~\\cite{Guha1998}. It has an $\\Omega(\\log\n^2 n + \\log |\\mathcal{L}|)$ randomized lower bound resulting from lower bounds\nfor the \\emph{Parking Permit Problem} and the \\emph{Online Set Cover}\nproblem~\\cite{Alon:2003:OSC:780542.780558,Korman}, where $|\\mathcal{L}|$ is the\nnumber of available lease types and $n$ is the number of nodes in the input\ngraph. We give a randomized $\\mathcal{O}(\\log ^2 n + \\log |\\mathcal{L}| \\log\nn)$-competitive algorithm for OCDSL. We also give a deterministic algorithm for\na variant of OCDSL in which the dominating subgraph need not be connected, the\n\\emph{Online Dominating Set Leasing} problem. The latter is based on a simple\nprimal-dual approach and has an $\\mathcal{O}(|\\mathcal{L}| \\cdot\n\\Delta)$-competitive ratio, where $\\Delta$ is the maximum degree of the input\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 10:48:30 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Markarian", "Christine", ""]]}, {"id": "1805.03055", "submitter": "Peilin Zhong", "authors": "Alexandr Andoni, Clifford Stein, Zhao Song, Zhengyu Wang, Peilin Zhong", "title": "Parallel Graph Connectivity in Log Diameter Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph connectivity problem in MPC model. On an undirected graph with\n$n$ nodes and $m$ edges, $O(\\log n)$ round connectivity algorithms have been\nknown for over 35 years. However, no algorithms with better complexity bounds\nwere known. In this work, we give fully scalable, faster algorithms for the\nconnectivity problem, by parameterizing the time complexity as a function of\nthe diameter of the graph. Our main result is a $O(\\log D \\log\\log_{m/n} n)$\ntime connectivity algorithm for diameter-$D$ graphs, using $\\Theta(m)$ total\nmemory. If our algorithm can use more memory, it can terminate in fewer rounds,\nand there is no lower bound on the memory per processor.\n  We extend our results to related graph problems such as spanning forest,\nfinding a DFS sequence, exact/approximate minimum spanning forest, and\nbottleneck spanning forest. We also show that achieving similar bounds for\nreachability in directed graphs would imply faster boolean matrix\nmultiplication algorithms.\n  We introduce several new algorithmic ideas. We describe a general technique\ncalled double exponential speed problem size reduction which roughly means that\nif we can use total memory $N$ to reduce a problem from size $n$ to $n/k$, for\n$k=(N/n)^{\\Theta(1)}$ in one phase, then we can solve the problem in\n$O(\\log\\log_{N/n} n)$ phases. In order to achieve this fast reduction for graph\nconnectivity, we use a multistep algorithm. One key step is a carefully\nconstructed truncated broadcasting scheme where each node broadcasts neighbor\nsets to its neighbors in a way that limits the size of the resulting neighbor\nsets. Another key step is random leader contraction, where we choose a smaller\nset of leaders than many previous works do.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 14:33:34 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Andoni", "Alexandr", ""], ["Stein", "Clifford", ""], ["Song", "Zhao", ""], ["Wang", "Zhengyu", ""], ["Zhong", "Peilin", ""]]}, {"id": "1805.03158", "submitter": "Luca Versari", "authors": "Roberto Grossi and Luca Versari", "title": "Round-Hashing for Data Storage: Distributed Servers and External-Memory\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes round-hashing, which is suitable for data storage on\ndistributed servers and for implementing external-memory tables in which each\nlookup retrieves at most a single block of external memory, using a stash. For\ndata storage, round-hashing is like consistent hashing as it avoids a full\nrehashing of the keys when new servers are added. Experiments show that the\nspeed to serve requests is tenfold or more than the state of the art. In\ndistributed data storage, this guarantees better throughput for serving\nrequests and, moreover, greatly reduces decision times for which data should\nmove to new servers as rescanning data is much faster.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:48:01 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Grossi", "Roberto", ""], ["Versari", "Luca", ""]]}, {"id": "1805.03164", "submitter": "Brandon Fain", "authors": "Brandon Fain, Kamesh Munagala, Nisarg Shah", "title": "Fair Allocation of Indivisible Public Goods", "comments": "Published in EC 2018, The 19th ACM Conference on Economics and\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fairly allocating indivisible public goods. We\nmodel the public goods as elements with feasibility constraints on what subsets\nof elements can be chosen, and assume that agents have additive utilities\nacross elements. Our model generalizes existing frameworks such as fair public\ndecision making and participatory budgeting. We study a groupwise fairness\nnotion called the core, which generalizes well-studied notions of\nproportionality and Pareto efficiency, and requires that each subset of agents\nmust receive an outcome that is fair relative to its size.\n  In contrast to the case of divisible public goods (where fractional\nallocations are permitted), the core is not guaranteed to exist when allocating\nindivisible public goods. Our primary contributions are the notion of an\nadditive approximation to the core (with a tiny multiplicative loss), and\npolynomial time algorithms that achieve a small additive approximation, where\nthe additive factor is relative to the largest utility of an agent for an\nelement. If the feasibility constraints define a matroid, we show an additive\napproximation of 2. A similar approach yields a constant additive bound when\nthe feasibility constraints define a matching. More generally, if the\nfeasibility constraints define an arbitrary packing polytope with mild\nrestrictions, we show an additive guarantee that is logarithmic in the width of\nthe polytope. Our algorithms are based on variants of the convex program for\nmaximizing the Nash social welfare, but differ significantly from previous work\nin how it is used. Our guarantees are meaningful even when there are fewer\nelements than the number of agents. As far as we are aware, our work is the\nfirst to approximate the core in indivisible settings.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:59:54 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 00:26:32 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Fain", "Brandon", ""], ["Munagala", "Kamesh", ""], ["Shah", "Nisarg", ""]]}, {"id": "1805.03192", "submitter": "Jayson Lynch", "authors": "Kaiying Hou, Jayson Lynch", "title": "The Computational Complexity of Finding Hamiltonian Cycles in Grid\n  Graphs of Semiregular Tessellations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding Hamitonian Cycles in square grid graphs is a well studied and\nimportant questions. More recent work has extended these results to triangular\nand hexagonal grids, as well as further restricted versions. In this paper, we\nexamine a class of more complex grids, as well as looking at the problem with\nrestricted types of paths. We investigate the hardness of Hamiltonian cycle\nproblem in grid graphs of semiregular tessellations. We give NP-hardness\nreductions for finding Hamiltonian paths in grid graphs based on all eight of\nthe semiregular tessilations. Next, we investigate variations on the problem of\nfinding Hamiltonian Paths in grid graphs when the path is forced to turn at\nevery vertex. We give a polynomial time algorithm for deciding if a square grid\ngraph admits a Hamiltonian cycle which turns at every vertex. We then show\ndeciding if cubic grid graphs, even if the height is restricted to $2$, admit a\nHamiltonian cycle is NP-complete.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:51:03 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hou", "Kaiying", ""], ["Lynch", "Jayson", ""]]}, {"id": "1805.03253", "submitter": "Maximilian Katzmann", "authors": "Thomas Bl\\\"asius, Cedric Freiberger, Tobias Friedrich, Maximilian\n  Katzmann, Felix Montenegro-Retana, Marianne Thieffry", "title": "Efficient Shortest Paths in Scale-Free Networks with Underlying\n  Hyperbolic Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common way to accelerate shortest path algorithms on graphs is the use of a\nbidirectional search, which simultaneously explores the graph from the start\nand the destination. It has been observed recently that this strategy performs\nparticularly well on scale-free real-world networks. Such networks typically\nhave a heterogeneous degree distribution (e.g., a power-law distribution) and\nhigh clustering (i.e., vertices with a common neighbor are likely to be\nconnected themselves). These two properties can be obtained by assuming an\nunderlying hyperbolic geometry.\n  To explain the observed behavior of the bidirectional search, we analyze its\nrunning time on hyperbolic random graphs and prove that it is $\\mathcal {\\tilde\nO}(n^{2 - 1/\\alpha} + n^{1/(2\\alpha)} + \\delta_{\\max})$ with high probability,\nwhere $\\alpha \\in (0.5, 1)$ controls the power-law exponent of the degree\ndistribution, and $\\delta_{\\max}$ is the maximum degree. This bound is\nsublinear, improving the obvious worst-case linear bound. Although our analysis\ndepends on the underlying geometry, the algorithm itself is oblivious to it.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 08:11:30 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Freiberger", "Cedric", ""], ["Friedrich", "Tobias", ""], ["Katzmann", "Maximilian", ""], ["Montenegro-Retana", "Felix", ""], ["Thieffry", "Marianne", ""]]}, {"id": "1805.03350", "submitter": "Timothy Johnson", "authors": "Juan Jose Besa, William E. Devanny, David Eppstein, Michael T.\n  Goodrich, Timothy Johnson", "title": "Optimally Sorting Evolving Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give optimal sorting algorithms in the evolving data framework, where an\nalgorithm's input data is changing while the algorithm is executing. In this\nframework, instead of producing a final output, an algorithm attempts to\nmaintain an output close to the correct output for the current state of the\ndata, repeatedly updating its best estimate of a correct output over time. We\nshow that a simple repeated insertion-sort algorithm can maintain an O(n)\nKendall tau distance, with high probability, between a maintained list and an\nunderlying total order of n items in an evolving data model where each\ncomparison is followed by a swap between a random consecutive pair of items in\nthe underlying total order. This result is asymptotically optpimal, since there\nis an Omega(n) lower bound for Kendall tau distance for this problem. Our\nresult closes the gap between this lower bound and the previous best algorithm\nfor this problem, which maintains a Kendall tau distance of O(n log log n) with\nhigh probability. It also confirms previous experimental results that suggested\nthat insertion sort tends to perform better than quicksort in practice.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 02:13:31 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Besa", "Juan Jose", ""], ["Devanny", "William E.", ""], ["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Johnson", "Timothy", ""]]}, {"id": "1805.03405", "submitter": "Martin Milani\\v{c}", "authors": "Endre Boros and Vladimir Gurvich and Martin Milani\\v{c}", "title": "Characterizing and decomposing classes of threshold, split, and\n  bipartite graphs via 1-Sperner hypergraphs", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hypergraph is said to be $1$-Sperner if for every two hyperedges the\nsmallest of their two set differences is of size one. We present several\napplications of $1$-Sperner hypergraphs and their structure to graphs. In\nparticular, we consider the classical characterizations of threshold and\ndomishold graphs and use them to obtain further characterizations of these\nclasses in terms of $1$-Spernerness, thresholdness, and $2$-asummability of\ntheir vertex cover, clique, dominating set, and closed neighborhood\nhypergraphs. Furthermore, we apply a decomposition property of $1$-Sperner\nhypergraphs to derive decomposition theorems for two classes of split graphs, a\nclass of bipartite graphs, and a class of cobipartite graphs. These\ndecomposition theorems are based on certain matrix partitions of the\ncorresponding graphs, giving rise to new classes of graphs of bounded\nclique-width and new polynomially solvable cases of several domination\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 08:15:55 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 15:51:08 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 06:14:43 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Boros", "Endre", ""], ["Gurvich", "Vladimir", ""], ["Milani\u010d", "Martin", ""]]}, {"id": "1805.03437", "submitter": "Dimitrios Letsios", "authors": "Dimitrios Letsios, Miten Mistry, Ruth Misener", "title": "Exact Lexicographic Scheduling and Approximate Rescheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial resource allocation problems, an initial planning stage may\nsolve a nominal problem instance and a subsequent recovery stage may intervene\nto repair inefficiencies and infeasibilities due to uncertainty, e.g.\\ machine\nfailures and job processing time variations. In this context, we investigate\nthe minimum makespan scheduling problem, a.k.a.\\ $P||C_{\\max}$, under\nuncertainty. We propose a two-stage robust scheduling approach where\nfirst-stage decisions are computed with exact lexicographic scheduling and\nsecond-stage decisions are derived using approximate rescheduling. We explore\nrecovery strategies accounting for planning decisions and constrained by\nlimited permitted deviations from the original schedule. Our approach is\nsubstantiated analytically, with a price of robustness characterization\nparameterized by the degree of uncertainty, and numerically. This analysis is\nbased on optimal substructure imposed by lexicographic optimality. Thus,\nlexicographic optimization enables more efficient rescheduling. Further, we\nrevisit state-of-the-art exact lexicographic optimization methods and propose a\nlexicographic branch-and-bound algorithm whose performance is validated\ncomputationally.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 09:55:01 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 15:16:58 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 12:03:59 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Letsios", "Dimitrios", ""], ["Mistry", "Miten", ""], ["Misener", "Ruth", ""]]}, {"id": "1805.03476", "submitter": "Yann Disser", "authors": "Yann Disser, Jan Hackfeld, and Max Klimm", "title": "Tight bounds for undirected graph exploration with pebbles and multiple\n  agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deterministically exploring an undirected and\ninitially unknown graph with $n$ vertices either by a single agent equipped\nwith a set of pebbles, or by a set of collaborating agents. The vertices of the\ngraph are unlabeled and cannot be distinguished by the agents, but the edges\nincident to a vertex have locally distinct labels. The graph is explored when\nall vertices have been visited by at least one agent. In this setting, it is\nknown that for a single agent without pebbles $\\Theta(\\log n)$ bits of memory\nare necessary and sufficient to explore any graph with at most $n$ vertices. We\nare interested in how the memory requirement decreases as the agent may mark\nvertices by dropping and retrieving distinguishable pebbles, or when multiple\nagents jointly explore the graph. We give tight results for both questions\nshowing that for a single agent with constant memory $\\Theta(\\log \\log n)$\npebbles are necessary and sufficient for exploration. We further prove that the\nsame bound holds for the number of collaborating agents needed for exploration.\n  For the upper bound, we devise an algorithm for a single agent with constant\nmemory that explores any $n$-vertex graph using $\\mathcal{O}(\\log \\log n)$\npebbles, even when $n$ is unknown. The algorithm terminates after polynomial\ntime and returns to the starting vertex. Since an additional agent is at least\nas powerful as a pebble, this implies that $\\mathcal{O}(\\log \\log n)$ agents\nwith constant memory can explore any $n$-vertex graph. For the lower bound, we\nshow that the number of agents needed for exploring any graph of size $n$ is\nalready $\\Omega(\\log \\log n)$ when we allow each agent to have at most\n$\\mathcal{O}( \\log n ^{1-\\varepsilon})$ bits of memory for any $\\varepsilon>0$.\nThis also implies that a single agent with sublogarithmic memory needs\n$\\Theta(\\log \\log n)$ pebbles to explore any $n$-vertex graph.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 12:29:46 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Disser", "Yann", ""], ["Hackfeld", "Jan", ""], ["Klimm", "Max", ""]]}, {"id": "1805.03498", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Janardhan Kulkarni", "title": "Deterministically Maintaining a $(2+\\epsilon)$-Approximate Minimum\n  Vertex Cover in $O(1/\\epsilon^2)$ Amortized Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maintaining an (approximately) minimum vertex\ncover in an $n$-node graph $G = (V, E)$ that is getting updated dynamically via\na sequence of edge insertions/deletions. We show how to maintain a\n$(2+\\epsilon)$-approximate minimum vertex cover, \"deterministically\", in this\nsetting in $O(1/\\epsilon^2)$ amortized update time.\n  Prior to our work, the best known deterministic algorithm for maintaining a\n$(2+\\epsilon)$-approximate minimum vertex cover was due to Bhattacharya,\nHenzinger and Italiano [SODA 2015]. Their algorithm has an update time of\n$O(\\log n/\\epsilon^2)$. Recently, Bhattacharya, Chakrabarty, Henzinger [IPCO\n2017] and Gupta, Krishnaswamy, Kumar, Panigrahi [STOC 2017] showed how to\nmaintain an $O(1)$-approximation in $O(1)$-amortized update time for the same\nproblem. Our result gives an \"exponential\" improvement over the update time of\nBhattacharya et al. [SODA 2015], and nearly matches the performance of the\n\"randomized\" algorithm of Solomon [FOCS 2016] who gets an approximation ratio\nof $2$ and an expected amortized update time of $O(1)$.\n  We derive our result by analyzing, via a novel technique, a variant of the\nalgorithm by Bhattacharya et al. We consider an idealized setting where the\nupdate time of an algorithm can take any arbitrary fractional value, and use\ninsights from this setting to come up with an appropriate potential function.\nConceptually, this framework mimics the idea of an LP-relaxation for an\noptimization problem. The difference is that instead of relaxing an integral\nobjective function, we relax the update time of an algorithm itself. We believe\nthat this technique will find further applications in the analysis of dynamic\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 13:19:06 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 20:07:31 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Kulkarni", "Janardhan", ""]]}, {"id": "1805.03574", "submitter": "Veli M\\\"akinen", "authors": "Tuukka Norri, Bastien Cazaux, Dmitry Kosolobov and Veli M\\\"akinen", "title": "Minimum Segmentation for Pan-genomic Founder Reconstruction in Linear\n  Time", "comments": null, "journal-ref": "In Proc. WABI 2018", "doi": "10.4230/LIPIcs.WABI.2018.15", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a threshold $L$ and a set $\\mathcal{R} = \\{R_1, \\ldots, R_m\\}$ of $m$\nhaplotype sequences, each having length $n$, the minimum segmentation problem\nfor founder reconstruction is to partition the sequences into disjoint segments\n$\\mathcal{R}[i_1{+}1,i_2], \\mathcal{R}[i_2{+}1, i_3], \\ldots,\n\\mathcal{R}[i_{r-1}{+}1, i_r]$, where $0 = i_1 < \\cdots < i_r = n$ and\n$\\mathcal{R}[i_{j-1}{+}1, i_j]$ is the set $\\{R_1[i_{j-1}{+}1, i_j], \\ldots,\nR_m[i_{j-1}{+}1, i_j]\\}$, such that the length of each segment, $i_j -\ni_{j-1}$, is at least $L$ and $K = \\max_j\\{ |\\mathcal{R}[i_{j-1}{+}1, i_j]| \\}$\nis minimized. The distinct substrings in the segments $\\mathcal{R}[i_{j-1}{+}1,\ni_j]$ represent founder blocks that can be concatenated to form $K$ founder\nsequences representing the original $\\mathcal{R}$ such that crossovers happen\nonly at segment boundaries. We give an optimal $O(mn)$ time algorithm to solve\nthe problem, improving over earlier $O(mn^2)$. This improvement enables to\nexploit the algorithm on a pan-genomic setting of haplotypes being complete\nhuman chromosomes, with a goal of finding a representative set of references\nthat can be indexed for read alignment and variant calling.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 15:04:25 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 07:56:43 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Norri", "Tuukka", ""], ["Cazaux", "Bastien", ""], ["Kosolobov", "Dmitry", ""], ["M\u00e4kinen", "Veli", ""]]}, {"id": "1805.03682", "submitter": "Amir Ali Ahmadi", "authors": "Amir Ali Ahmadi, Oktay Gunluk", "title": "Robust-to-Dynamics Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust-to-dynamics optimization (RDO) problem is an optimization problem\nspecified by two pieces of input: (i) a mathematical program (an objective\nfunction $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ and a feasible set\n$\\Omega\\subseteq\\mathbb{R}^n$), and (ii) a dynamical system (a map\n$g:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n$). Its goal is to minimize $f$ over the\nset $\\mathcal{S}\\subseteq\\Omega$ of initial conditions that forever remain in\n$\\Omega$ under $g$. The focus of this paper is on the case where the\nmathematical program is a linear program and the dynamical system is either a\nknown linear map, or an uncertain linear map that can change over time. In both\ncases, we study a converging sequence of polyhedral outer approximations and\n(lifted) spectrahedral inner approximations to $\\mathcal{S}$. Our inner\napproximations are optimized with respect to the objective function $f$ and\ntheir semidefinite characterization---which has a semidefinite constraint of\nfixed size---is obtained by applying polar duality to convex sets that are\ninvariant under (multiple) linear maps. We characterize three barriers that can\nstop convergence of the outer approximations from being finite. We prove that\nonce these barriers are removed, our inner and outer approximating procedures\nfind an optimal solution and a certificate of optimality for the RDO problem in\na finite number of steps. Moreover, in the case where the dynamics are linear,\nwe show that this phenomenon occurs in a number of steps that can be computed\nin time polynomial in the bit size of the input data. Our analysis also leads\nto a polynomial-time algorithm for RDO instances where the spectral radius of\nthe linear map is bounded above by any constant less than one. Finally, in our\nconcluding section, we propose a broader research agenda for studying\noptimization problems with dynamical systems constraints, of which RDO is a\nspecial case.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 18:21:40 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Gunluk", "Oktay", ""]]}, {"id": "1805.03741", "submitter": "Lin Chen", "authors": "Lin Chen, Lei Xu, Weidong Shi, Martin Kouteck\\'y", "title": "New Bounds on Augmenting Steps of Block-structured Integer Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider 4-block $n$-fold integer programs, whose constraint matrix\nconsists of $n$ copies of small matrices $A$, $B$, and $D$, and one copy of\n$C$, in a specific block structure. All existing algorithms along this line of\nresearch follows an iterative augmentation framework, which relies on the\nso-called Graver basis of the constraint matrix that constitutes a set of\nfundamental augmenting steps. Bounding the $\\ell_1$- or $\\ell_\\infty$-norm of\nelements of the Graver basis is the key to these algorithms. Hemmecke et\nal.~[Math. Prog. 2014] showed that 4-block $n$-fold IP has Graver elements of\n$\\ell_\\infty$-norm at most $O_{FPT}(n^{2^{s_{D}}})$, leading to an algorithm\nwith a similar runtime; here, $s_{D}$ is the number of rows of matrix $D$ and $\nO_{FPT}(1)$ hides a multiplicative factor that is only dependent on the small\nmatrices $A,B,C,D$.\n  We prove that the $\\ell_{\\infty}$-norm of the Graver elements of 4-block\n$n$-fold IP is upper bounded by $O_{FPT}(n^{s_{D}})$, improving significantly\nover the previous bound $O_{FPT} (n^{2^{s_{D}}})$. We also provide a matching\nlower bound of $\\Omega(n^{s_{D}})$ which even holds for arbitrary non-zero\nlattice elements, ruling out augmenting algorithm relying on even more\nrestricted notions of augmentation than the Graver basis. We then consider a\nspecial case of 4-block $n$-fold in which $C$ is a zero matrix, called 3-block\n$n$-fold IP. We show that while even there the $\\ell_{\\infty}$-norm of its\nGraver elements is $\\Omega(n^{s_{D}})$, there exists a different decomposition\ninto lattice elements whose $\\ell_{\\infty}$-norm is bounded by $ O_{FPT}(1)$,\nwhich allows us to provide improved upper bounds on the $\\ell_{\\infty}$-norm of\nGraver elements for 3-block $n$-fold IP.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 21:34:28 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 16:01:38 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 03:02:18 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Chen", "Lin", ""], ["Xu", "Lei", ""], ["Shi", "Weidong", ""], ["Kouteck\u00fd", "Martin", ""]]}, {"id": "1805.03765", "submitter": "Samson Zhou", "authors": "Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco,\n  Jalaj Upadhyay, David P. Woodruff, Samson Zhou", "title": "Near Optimal Linear Algebra in the Online and Sliding Window Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of numerical linear algebra in the sliding window\nmodel, where only the most recent $W$ updates in a stream form the underlying\ndata set. We first introduce a unified row-sampling based framework that gives\nrandomized algorithms for spectral approximation, low-rank\napproximation/projection-cost preservation, and $\\ell_1$-subspace embeddings in\nthe sliding window model, which often use nearly optimal space and achieve\nnearly input sparsity runtime. Our algorithms are based on \"reverse online\"\nversions of offline sampling distributions such as (ridge) leverage scores,\n$\\ell_1$ sensitivities, and Lewis weights to quantify both the importance and\nthe recency of a row. Our row-sampling framework rather surprisingly implies\nconnections to the well-studied online model; our structural results also give\nthe first sample optimal (up to lower order terms) online algorithm for\nlow-rank approximation/projection-cost preservation. Using this powerful\nprimitive, we give online algorithms for column/row subset selection and\nprincipal component analysis that resolves the main open question of Bhaskara\net. al.,(FOCS 2019). We also give the first online algorithm for\n$\\ell_1$-subspace embeddings. We further formalize the connection between the\nonline model and the sliding window model by introducing an additional unified\nframework for deterministic algorithms using a merge and reduce paradigm and\nthe concept of online coresets. Our sampling based algorithms in the\nrow-arrival online model yield online coresets, giving deterministic algorithms\nfor spectral approximation, low-rank approximation/projection-cost\npreservation, and $\\ell_1$-subspace embeddings in the sliding window model that\nuse nearly optimal space.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 00:48:28 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 20:34:36 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 06:29:17 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 21:18:04 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Braverman", "Vladimir", ""], ["Drineas", "Petros", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Upadhyay", "Jalaj", ""], ["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "1805.03834", "submitter": "Jouni Sir\\'en", "authors": "Jouni Sir\\'en, Erik Garrison, Adam M. Novak, Benedict Paten, Richard\n  Durbin", "title": "Haplotype-aware graph indexes", "comments": "Accepted to WABI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variation graph toolkit (VG) represents genetic variation as a graph.\nEach path in the graph is a potential haplotype, though most paths are unlikely\nrecombinations of true haplotypes. We augment the VG model with haplotype\ninformation to identify which paths are more likely to be correct. For this\npurpose, we develop a scalable implementation of the graph extension of the\npositional Burrows--Wheeler transform. We demonstrate the scalability of the\nnew implementation by indexing the 1000 Genomes Project haplotypes. We also\ndevelop an algorithm for simplifying variation graphs for k-mer indexing\nwithout losing any k-mers in the haplotypes.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 06:05:26 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 06:47:25 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Sir\u00e9n", "Jouni", ""], ["Garrison", "Erik", ""], ["Novak", "Adam M.", ""], ["Paten", "Benedict", ""], ["Durbin", "Richard", ""]]}, {"id": "1805.04071", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang", "title": "Energy Complexity of Distance Computation in Multi-hop Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is a critical issue for wireless devices operated under\nstringent power constraint (e.g., battery). Following prior works, we measure\nthe energy cost of a device by its transceiver usage, and define the energy\ncomplexity of an algorithm as the maximum number of time slots a device\ntransmits or listens, over all devices. In a recent paper of Chang et al. (PODC\n2018), it was shown that broadcasting in a multi-hop network of unknown\ntopology can be done in $\\text{poly} \\log n$ energy. In this paper, we continue\nthis line of research, and investigate the energy complexity of other\nfundamental graph problems in multi-hop networks. Our results are summarized as\nfollows.\n  1. To avoid spending $\\Omega(D)$ energy, the broadcasting protocols of Chang\net al. (PODC 2018) do not send the message along a BFS tree, and it is open\nwhether BFS could be computed in $o(D)$ energy, for sufficiently large $D$. In\nthis paper we devise an algorithm that attains $\\tilde{O}(\\sqrt{n})$ energy\ncost.\n  2. We show that the framework of the ${\\Omega}(n)$ round lower bound proof\nfor computing diameter in CONGEST of Abboud et al. (DISC 2017) can be adapted\nto give an $\\tilde{\\Omega}(n)$ energy lower bound in the wireless network model\n(with no message size constraint), and this lower bound applies to $O(\\log\nn)$-arboricity graphs. From the upper bound side, we show that the energy\ncomplexity of $\\tilde{O}(\\sqrt{n})$ can be attained for bounded-genus graphs\n(which includes planar graphs).\n  3. Our upper bounds for computing diameter can be extended to other graph\nproblems. We show that exact global minimum cut or approximate $s$--$t$ minimum\ncut can be computed in $\\tilde{O}(\\sqrt{n})$ energy for bounded-genus graphs.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:25:23 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 12:54:17 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Chang", "Yi-Jun", ""]]}, {"id": "1805.04131", "submitter": "Rico Zenklusen", "authors": "Rico Zenklusen", "title": "A 1.5-Approximation for Path TSP", "comments": "Minor update of previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $1.5$-approximation for the Metric Path Traveling Salesman\nProblem (Path TSP). All recent improvements on Path TSP crucially exploit a\nstructural property shown by An, Kleinberg, and Shmoys [Journal of the ACM,\n2015], namely that narrow cuts with respect to a Held-Karp solution form a\nchain. We significantly deviate from these approaches by showing the benefit of\ndealing with larger $s$-$t$ cuts, even though they are much less structured.\nMore precisely, we show that a variation of the dynamic programming idea\nrecently introduced by Traub and Vygen [SODA, 2018] is versatile enough to deal\nwith larger size cuts, by exploiting a seminal result of Karger on the number\nof near-minimum cuts. This avoids a recursive application of dynamic\nprogramming as used by Traub and Vygen, and leads to a considerably simpler\nalgorithm avoiding an additional error term in the approximation guarantee. We\nmatch the still unbeaten $1.5$-approximation guarantee of Christofides'\nalgorithm for TSP. Hence, any further progress on the approximability of Path\nTSP will also lead to an improvement for TSP.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 18:49:49 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 16:37:15 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2018 10:50:46 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zenklusen", "Rico", ""]]}, {"id": "1805.04151", "submitter": "Andrii Riazanov", "authors": "Venkatesan Guruswami, Andrii Riazanov", "title": "Beating Fredman-Koml\\'{o}s for perfect $k$-hashing", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say a subset $C \\subseteq \\{1,2,\\dots,k\\}^n$ is a $k$-hash code (also\ncalled $k$-separated) if for every subset of $k$ codewords from $C$, there\nexists a coordinate where all these codewords have distinct values.\nUnderstanding the largest possible rate (in bits), defined as $(\\log_2 |C|)/n$,\nof a $k$-hash code is a classical problem. It arises in two equivalent\ncontexts: (i) the smallest size possible for a perfect hash family that maps a\nuniverse of $N$ elements into $\\{1,2,\\dots,k\\}$, and (ii) the zero-error\ncapacity for decoding with lists of size less than $k$ for a certain\ncombinatorial channel.\n  A general upper bound of $k!/k^{k-1}$ on the rate of a $k$-hash code (in the\nlimit of large $n$) was obtained by Fredman and Koml\\'{o}s in 1984 for any $k\n\\geq 4$. While better bounds have been obtained for $k=4$, their original bound\nhas remained the best known for each $k \\ge 5$. In this work, we obtain the\nfirst improvement to the Fredman-Koml\\'{o}s bound for every $k \\ge 5$. While we\nget explicit (numerical) bounds for $k=5,6$, for larger $k$ we only show that\nthe FK bound can be improved by a positive, but unspecified, amount. Under a\nconjecture on the optimum value of a certain polynomial optimization problem\nover the simplex, our methods allow an effective bound to be computed for every\n$k$.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 19:53:55 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Riazanov", "Andrii", ""]]}, {"id": "1805.04154", "submitter": "Sebastian Wild", "authors": "J. Ian Munro and Sebastian Wild", "title": "Nearly-Optimal Mergesorts: Fast, Practical Sorting Methods That\n  Optimally Adapt to Existing Runs", "comments": null, "journal-ref": null, "doi": "10.4230/lipics.esa.2018.63", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two stable mergesort variants, \"peeksort\" and \"powersort\", that\nexploit existing runs and find nearly-optimal merging orders with practically\nnegligible overhead. Previous methods either require substantial effort for\ndetermining the merging order (Takaoka 2009; Barbay & Navarro 2013) or do not\nhave a constant-factor optimal worst-case guarantee (Peters 2001; Auger, Nicaud\n& Pivoteau 2015; Buss & Knop 2018). We demonstrate that our methods are\ncompetitive in terms of running time with state-of-the-art implementations of\nstable sorting methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:00:42 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Munro", "J. Ian", ""], ["Wild", "Sebastian", ""]]}, {"id": "1805.04165", "submitter": "Ellis Hershkowitz", "authors": "Keren Censor-Hillel and Bernhard Haeupler and D Ellis Hershkowitz and\n  Goran Zuzic", "title": "Erasure Correction for Noisy Radio Networks", "comments": "We gave significantly more high level intuition of our results in a\n  new section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radio network model is a well-studied model of wireless, multi-hop\nnetworks. However, radio networks make the strong assumption that messages are\ndelivered deterministically. The recently introduced noisy radio network model\nrelaxes this assumption by dropping messages independently at random.\n  In this work we quantify the relative computational power of noisy radio\nnetworks and classic radio networks. In particular, given a non-adaptive\nprotocol for a fixed radio network we show how to reliably simulate this\nprotocol if noise is introduced with a multiplicative cost of\n$\\mathrm{poly}(\\log \\Delta, \\log \\log n)$ rounds where $n$ is the number nodes\nin the network and $\\Delta$ is the max degree. Moreover, we demonstrate that,\neven if the simulated protocol is not non-adaptive, it can be simulated with a\nmultiplicative $O(\\Delta \\log ^2 \\Delta)$ cost in the number of rounds. Lastly,\nwe argue that simulations with a multiplicative overhead of $o(\\log \\Delta)$\nare unlikely to exist by proving that an $\\Omega(\\log \\Delta)$ multiplicative\nround overhead is necessary under certain natural assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:29:03 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 00:42:05 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 18:48:26 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Zuzic", "Goran", ""]]}, {"id": "1805.04272", "submitter": "Hanqing Zhao", "authors": "Hanqing Zhao, Yuehan Luo", "title": "An $O(N)$ Sorting Algorithm: Machine Learning Sort", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 08:28:55 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 16:24:39 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhao", "Hanqing", ""], ["Luo", "Yuehan", ""]]}, {"id": "1805.04375", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, Dimitrios M. Thilikos", "title": "On the Parameterized Complexity of Graph Modification to First-Order\n  Logic Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of deciding whether an input graph can be modified\nby removing/adding at most k vertices/edges such that the result of the\nmodification satisfies some property definable in first-order logic. We\nestablish a number of sufficient and necessary conditions on the quantification\npattern of the first-order formula \\phi for the problem to be fixed-parameter\ntractable or to admit a polynomial kernel.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 13:09:47 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 09:15:46 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 09:26:03 GMT"}, {"version": "v4", "created": "Tue, 26 Feb 2019 12:36:12 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1805.04436", "submitter": "Hanrui Zhang", "authors": "Wei Chen, Shang-Hua Teng, Hanrui Zhang", "title": "Capturing Complementarity in Set Functions by Going Beyond\n  Submodularity/Subadditivity", "comments": "ITCS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two new \"degree of complementarity\" measures, which we refer to,\nrespectively, as supermodular width and superadditive width. Both are\nformulated based on natural witnesses of complementarity. We show that both\nmeasures are robust by proving that they, respectively, characterize the gap of\nmonotone set functions from being submodular and subadditive. Thus, they define\ntwo new hierarchies over monotone set functions, which we will refer to as\nSupermodular Width (SMW) hierarchy and Superadditive Width (SAW) hierarchy,\nwith level 0 of the hierarchies resting exactly on submodular and subadditive\nfunctions, respectively.\n  We present a comprehensive comparative analysis of the SMW hierarchy and the\nSupermodular Degree (SD) hierarchy, defined by Feige and Izsak. We prove that\nthe SMW hierarchy is strictly more expressive than the SD hierarchy. We show\nthat previous results regarding approximation guarantees for welfare and\nconstrained maximization as well as regarding the Price of Anarchy (PoA) of\nsimple auctions can be extended without any loss from the SD hierarchy to the\nSMW hierarchy. We also establish almost matching information-theoretical lower\nbounds. The combination of these approximation and hardness results illustrate\nthat the SMW hierarchy provides an accurate characterization of \"near\nsubmodularity\" needed for maximization approximation. While SD and SMW\nhierarchies support nontrivial bounds on the PoA of simple auctions, we show\nthat our SAW hierarchy seems to capture more intrinsic properties needed to\nrealize the efficiency of simple auctions. So far, the SAW hierarchy provides\nthe best dependency for the PoA of Single-bid Auction, and is nearly as\ncompetitive as the Maximum over Positive Hypergraphs (MPH) hierarchy for\nSimultaneous Item First Price Auction (SIA). We provide almost tight lower\nbounds for the PoA of both auctions with respect to the SAW hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 15:00:33 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 19:25:06 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 17:44:32 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Chen", "Wei", ""], ["Teng", "Shang-Hua", ""], ["Zhang", "Hanrui", ""]]}, {"id": "1805.04544", "submitter": "Viktor Zamaraev", "authors": "Christian Konrad, Viktor Zamaraev", "title": "Distributed Minimum Vertex Coloring and Maximum Independent Set in\n  Chordal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give deterministic distributed $(1+\\epsilon)$-approximation algorithms for\nMinimum Vertex Coloring and Maximum Independent Set on chordal graphs in the\nLOCAL model. Our coloring algorithm runs in $O(\\frac{1}{\\epsilon} \\log n)$\nrounds, and our independent set algorithm has a runtime of\n$O(\\frac{1}{\\epsilon}\\log(\\frac{1}{\\epsilon})\\log^* n)$ rounds. For coloring,\nexisting lower bounds imply that the dependencies on $\\frac{1}{\\epsilon}$ and\n$\\log n$ are best possible. For independent set, we prove that\n$O(\\frac{1}{\\epsilon})$ rounds are necessary.\n  Both our algorithms make use of a tree decomposition of the input chordal\ngraph. They iteratively peel off interval subgraphs, which are identified via\nthe tree decomposition of the input graph, thereby partitioning the vertex set\ninto $O(\\log n)$ layers. For coloring, each interval graph is colored\nindependently, which results in various coloring conflicts between the layers.\nThese conflicts are then resolved in a separate phase, using the particular\nstructure of our partitioning. For independent set, only the first $O( \\log\n\\frac{1}{\\epsilon})$ layers are required as they already contain a large enough\nindependent set. We develop a $(1+\\epsilon)$-approximation maximum independent\nset algorithm for interval graphs, which we then apply to those layers.\n  This work raises the question as to how useful tree decompositions are for\ndistributed computing.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 18:21:02 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Konrad", "Christian", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "1805.04599", "submitter": "Joshua Daymude", "authors": "Sarah Cannon and Joshua J. Daymude and Cem Gokmen and Dana Randall and\n  Andr\\'ea W. Richa", "title": "A Local Stochastic Algorithm for Separation in Heterogeneous\n  Self-Organizing Particle Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.ET math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and rigorously analyze the behavior of a distributed, stochastic\nalgorithm for separation and integration in self-organizing particle systems,\nan abstraction of programmable matter. Such systems are composed of individual\ncomputational particles with limited memory, strictly local communication\nabilities, and modest computational power. We consider heterogeneous particle\nsystems of two different colors and prove that these systems can collectively\nseparate into different color classes or integrate, indifferent to color. We\naccomplish both behaviors with the same fully distributed, local, stochastic\nalgorithm. Achieving separation or integration depends only on a single global\nparameter determining whether particles prefer to be next to other particles of\nthe same color or not; this parameter is meant to represent external,\nenvironmental influences on the particle system. The algorithm is a\ngeneralization of a previous distributed, stochastic algorithm for compression\n(PODC '16), which can be viewed as a special case of separation where all\nparticles have the same color. It is significantly more challenging to prove\nthat the desired behavior is achieved in the heterogeneous setting, however,\neven in the bichromatic case we focus on. This requires combining several new\ntechniques, including the cluster expansion from statistical physics, a new\nvariant of the bridging argument of Miracle, Pascoe and Randall (RANDOM '11),\nthe high-temperature expansion of the Ising model, and careful probabilistic\narguments.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 21:46:59 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 04:41:19 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Cannon", "Sarah", ""], ["Daymude", "Joshua J.", ""], ["Gokmen", "Cem", ""], ["Randall", "Dana", ""], ["Richa", "Andr\u00e9a W.", ""]]}, {"id": "1805.04610", "submitter": "Sung-Il Pae", "authors": "Sung-il Pae", "title": "Peres-Style Recursive Algorithms", "comments": "15 pages. Version 2, as of May 22, 2018, includes a proof of\n  Structure Lemma and a few corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peres algorithm applies the famous von Neumann trick recursively to produce\nunbiased random bits from biased coin tosses. Its recursive nature makes the\nalgorithm simple and elegant, and yet its output rate approaches the\ninformation-theoretic upper bound. However, it is relatively hard to explain\nwhy it works, and it appears partly due to this difficulty that its\ngeneralization to many-valued source was discovered only recently. Binarization\ntree provides a new conceptual tool to understand the innerworkings of the\noriginal Peres algorithm and the recently-found generalizations in both aspects\nof the uniform random number generation and asymptotic optimality. Furthermore,\nit facilitates finding many new Peres-style recursive algorithms that have been\narguably very hard to come by without this new tool.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 22:42:13 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 04:44:17 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Pae", "Sung-il", ""]]}, {"id": "1805.04720", "submitter": "Mingda Qiao", "authors": "Mingda Qiao", "title": "Do Outliers Ruin Collaboration?", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a binary classifier from $n$ different\ndata sources, among which at most an $\\eta$ fraction are adversarial. The\noverhead is defined as the ratio between the sample complexity of learning in\nthis setting and that of learning the same hypothesis class on a single data\ndistribution. We present an algorithm that achieves an $O(\\eta n + \\ln n)$\noverhead, which is proved to be worst-case optimal. We also discuss the\npotential challenges to the design of a computationally efficient learning\nalgorithm with a small overhead.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 13:35:35 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Qiao", "Mingda", ""]]}, {"id": "1805.04764", "submitter": "Jason Li", "authors": "Mohsen Ghaffari, Jason Li", "title": "New Distributed Algorithms in Almost Mixing Time via Transformations\n  from Parallel Algorithms", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that many classical optimization problems --- such as\n$(1\\pm\\epsilon)$-approximate maximum flow, shortest path, and transshipment ---\ncan be computed in $\\newcommand{\\tmix}{{\\tau_{\\text{mix}}}}\\tmix(G)\\cdot\nn^{o(1)}$ rounds of distributed message passing, where $\\tmix(G)$ is the mixing\ntime of the network graph $G$. This extends the result of Ghaffari et al.\\\n[PODC'17], whose main result is a distributed MST algorithm in $\\tmix(G)\\cdot\n2^{O(\\sqrt{\\log n \\log\\log n})}$ rounds in the CONGEST model, to a much wider\nclass of optimization problems. For many practical networks of interest, e.g.,\npeer-to-peer or overlay network structures, the mixing time $\\tmix(G)$ is\nsmall, e.g., polylogarithmic. On these networks, our algorithms bypass the\n$\\tilde\\Omega(\\sqrt n+D)$ lower bound of Das Sarma et al.\\ [STOC'11], which\napplies for worst-case graphs and applies to all of the above optimization\nproblems. For all of the problems except MST, this is the first distributed\nalgorithm which takes $o(\\sqrt n)$ rounds on a (nontrivial) restricted class of\nnetwork graphs.\n  Towards deriving these improved distributed algorithms, our main contribution\nis a general transformation that simulates any work-efficient PRAM algorithm\nrunning in $T$ parallel rounds via a distributed algorithm running in $T\\cdot\n\\tmix(G)\\cdot 2^{O(\\sqrt{\\log n})}$ rounds. Work- and time-efficient parallel\nalgorithms for all of the aforementioned problems follow by combining the work\nof Sherman [FOCS'13, SODA'17] and Peng and Spielman [STOC'14]. Thus, simulating\nthese parallel algorithms using our transformation framework produces the\ndesired distributed algorithms.\n  The core technical component of our transformation is the algorithmic problem\nof solving \\emph{multi-commodity routing}---that is, roughly, routing $n$\npackets each from a given source to a given destination---in random graphs. For\nthis problem, we obtain a...\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 18:58:10 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Li", "Jason", ""]]}, {"id": "1805.04794", "submitter": "Aras Atalar", "authors": "Aras Atalar, Paul Renaud-Goud, Philippas Tsigas", "title": "Lock-Free Search Data Structures: Throughput Modelling with Poisson\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the modelling and the analysis of the performance of\nlock-free concurrent search data structures. Our analysis considers such\nlock-free data structures that are utilized through a sequence of operations\nwhich are generated with a memoryless and stationary access pattern. Our main\ncontribution is a new way of analysing lock-free search data structures: our\nexecution model matches with the behavior that we observe in practice and\nachieves good throughput predictions. Search data structures are formed of\nlinked basic blocks, usually referred as nodes, that can be accessed by two\nkinds of events, characterized by their latencies; (i) CAS events originated as\na result of modifications of the search data structures (ii) Read events\noriginated during traversals. This type of data structures are usually designed\nto accommodate a large number of data nodes, which makes the occurrence of an\nevent on a given node rare at any given time. The throughput is defined by the\nnumber of events per operation in conjunction with the factors that impact the\nlatencies of these events. We frame these impacting factors under capacity and\ncoherence cache misses.\n  In this context, we model the events as Poisson processes that we can merge\nand split to estimate the latencies of the events based on the interleaving of\nevents from different threads, and in turn estimate the throughput. We have\nvalidated our analysis on several fundamental lock-free search data structures\nsuch as linked lists, hash tables, skip lists and binary trees.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 22:13:35 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Atalar", "Aras", ""], ["Renaud-Goud", "Paul", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1805.04984", "submitter": "Dorit Hochbaum", "authors": "Dorit S. Hochbaum", "title": "Algorithms and Complexity of Range Clustering", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel criterion in clustering that seeks clusters with limited\nrange of values associated with each cluster's elements. In clustering or\nclassification the objective is to partition a set of objects into subsets,\ncalled clusters or classes, consisting of similar objects so that different\nclusters are as dissimilar as possible. We propose a number of objective\nfunctions that employ the range of the clusters as part of the objective\nfunction. Several of the proposed objectives mimic objectives based on sums of\nsimilarities. These objective functions are motivated by image segmentation\nproblems, where the diameter, or range of values associated with objects in\neach cluster, should be small. It is demonstrated that range-based problems are\nin general easier, in terms of their complexity, than the analogous\nsimilarity-sum problems. Several of the problems we present could therefore be\nviable alternatives to existing clustering problems which are NP-hard, offering\nthe advantage of efficient algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 01:55:38 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Hochbaum", "Dorit S.", ""]]}, {"id": "1805.05094", "submitter": "Tomer Ezra", "authors": "Tomer Ezra and Michal Feldman and Ilan Nehama", "title": "Prophets and Secretaries with Overbooking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prophet and secretary problems demonstrate online scenarios involving the\noptimal stopping theory. In a typical prophet or secretary problem, selection\ndecisions are assumed to be immediate and irrevocable. However, many online\nsettings accommodate some degree of revocability. To study such scenarios, we\nintroduce the $\\ell-out-of-k$ setting, where the decision maker can select up\nto $k$ elements immediately and irrevocably, but her performance is measured by\nthe top $\\ell$ elements in the selected set. Equivalently, the decision makes\ncan hold up to $\\ell$ elements at any given point in time, but can make up to\n$k-\\ell$ returns as new elements arrive.\n  We give upper and lower bounds on the competitive ratio of $\\ell$-out-of-$k$\nprophet and secretary scenarios. These include a single-sample prophet\nalgorithm that gives a competitive ratio of $1-\\ell\\cdot\ne^{-\\Theta\\left(\\frac{\\left(k-\\ell\\right)^2}{k}\\right)}$, which is\nasymptotically tight for $k-\\ell=\\Theta(\\ell)$. For secretary settings, we\ndevise an algorithm that obtains a competitive ratio of $1-\\ell\ne^{-\\frac{k-8\\ell}{2+2\\ln \\ell}} - e^{-k/6}$, and show that no secretary\nalgorithm obtains a better ratio than $1-e^{-k}$ (up to negligible terms). In\npassing, our results lead to an improvement of the results of Assaf et al.\n[2000] for $1-out-of-k$ prophet scenarios.\n  Beyond the contribution to online algorithms and optimal stopping theory, our\nresults have implications to mechanism design. In particular, we use our\nprophet algorithms to derive {\\em overbooking} mechanisms with good welfare and\nrevenue guarantees; these are mechanisms that sell more items than the seller's\ncapacity, then allocate to the agents with the highest values among the\nselected agents.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 10:00:35 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:15:42 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Ezra", "Tomer", ""], ["Feldman", "Michal", ""], ["Nehama", "Ilan", ""]]}, {"id": "1805.05201", "submitter": "Brice N\\'edelec", "authors": "Brice N\\'edelec, Pascal Molli, Achour Most\\'efaoui", "title": "Breaking the Scalability Barrier of Causal Broadcast for Large and\n  Dynamic Systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed protocols and applications rely on causal broadcast to\nensure consistency criteria. However, none of causality tracking\nstate-of-the-art approaches scale in large and dynamic systems. This paper\npresents a new non-blocking causal broadcast protocol suited for dynamic\nsystems. The proposed protocol outperforms state-of-the-art in size of\nmessages, execution time complexity, and local space complexity. Most\nimportantly, messages piggyback control information the size of which is\nconstant. We prove that for both static and dynamic systems. Consequently,\nlarge and dynamic systems can finally afford causal broadcast.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 08:28:11 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["N\u00e9delec", "Brice", ""], ["Molli", "Pascal", ""], ["Most\u00e9faoui", "Achour", ""]]}, {"id": "1805.05208", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Guy E. Blelloch, Julian Shun", "title": "Theoretically Efficient Parallel Graph Algorithms Can Be Fast and\n  Scalable", "comments": "This is the full version of the paper appearing in the ACM Symposium\n  on Parallelism in Algorithms and Architectures (SPAA), 2018", "journal-ref": null, "doi": "10.1145/3210377.3210414", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest in parallel graph processing due\nto the need to quickly analyze the large graphs available today. Many graph\ncodes have been designed for distributed memory or external memory. However,\ntoday even the largest publicly-available real-world graph (the Hyperlink Web\ngraph with over 3.5 billion vertices and 128 billion edges) can fit in the\nmemory of a single commodity multicore server. Nevertheless, most experimental\nwork in the literature report results on much smaller graphs, and the ones for\nthe Hyperlink graph use distributed or external memory. Therefore, it is\nnatural to ask whether we can efficiently solve a broad class of graph problems\non this graph in memory.\n  This paper shows that theoretically-efficient parallel graph algorithms can\nscale to the largest publicly-available graphs using a single machine with a\nterabyte of RAM, processing them in minutes. We give implementations of\ntheoretically-efficient parallel algorithms for 20 important graph problems. We\nalso present the optimizations and techniques that we used in our\nimplementations, which were crucial in enabling us to process these large\ngraphs quickly. We show that the running times of our implementations\noutperform existing state-of-the-art implementations on the largest real-world\ngraphs. For many of the problems that we consider, this is the first time they\nhave been solved on graphs at this scale. We have made the implementations\ndeveloped in this work publicly-available as the Graph-Based Benchmark Suite\n(GBBS).\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 14:58:56 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 19:18:55 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 01:57:24 GMT"}, {"version": "v4", "created": "Wed, 21 Aug 2019 03:40:34 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Blelloch", "Guy E.", ""], ["Shun", "Julian", ""]]}, {"id": "1805.05228", "submitter": "Travis Gagie", "authors": "Diego D\\'iaz-Dom\\'inguez, Djamal Belazzougui, Travis Gagie, Veli\n  M\\\"akinen, Gonzalo Navarro and Simon J. Puglisi", "title": "Assembling Omnitigs using Hidden-Order de Bruijn Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De novo DNA assembly is a fundamental task in Bioinformatics, and finding\nEulerian paths on de Bruijn graphs is one of the dominant approaches to it. In\nmost of the cases, there may be no one order for the de Bruijn graph that works\nwell for assembling all of the reads. For this reason, some de Bruijn-based\nassemblers try assembling on several graphs of increasing order, in turn.\nBoucher et al. (2015) went further and gave a representation making it possible\nto navigate in the graph and change order on the fly, up to a maximum $K$, but\nthey can use up to $\\lg K$ extra bits per edge because they use an LCP array.\nIn this paper, we replace the LCP array by a succinct representation of that\narray's Cartesian tree, which takes only 2 extra bits per edge and still lets\nus support interesting navigation operations efficiently. These operations are\nnot enough to let us easily extract unitigs and only unitigs from the graph but\nthey do let us extract a set of safe strings that contains all unitigs. Suppose\nwe are navigating in a variable-order de Bruijn graph representation, following\nthese rules: if there are no outgoing edges then we reduce the order, hoping\none appears; if there is exactly one outgoing edge then we take it (increasing\nthe current order, up to $K$); if there are two or more outgoing edges then we\nstop. Then we traverse a (variable-order) path such that we cross edges only\nwhen we have no choice or, equivalently, we generate a string appending\ncharacters only when we have no choice. It follows that the strings we extract\nare safe. Our experiments show we extract a set of strings more informative\nthan the unitigs, while using a reasonable amount of memory.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 15:26:30 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["D\u00edaz-Dom\u00ednguez", "Diego", ""], ["Belazzougui", "Djamal", ""], ["Gagie", "Travis", ""], ["M\u00e4kinen", "Veli", ""], ["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1805.05305", "submitter": "Axel Dahlberg", "authors": "Axel Dahlberg, Stephanie Wehner", "title": "Transforming graph states using single-qubit operations", "comments": "26 pages, 1 figure. For computational complexity and more efficient\n  algorithms for relevant graph classes see 'How to transform graph states\n  using single-qubit operations: computational complexity and algorithms'\n  (1805.05306). For related work see F. Hahn et al (1805.04559)", "journal-ref": "Philosophical Transactions of the Royal Society A 376: 20170325,\n  2018, Special issue: 'Foundations of quantum mechanics and their impact on\n  contemporary society'", "doi": "10.1098/rsta.2017.0325", "report-no": null, "categories": "quant-ph cs.CC cs.DS cs.ET math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stabilizer states form an important class of states in quantum information,\nand are of central importance in quantum error correction. Here, we provide an\nalgorithm for deciding whether one stabilizer (target) state can be obtained\nfrom another stabilizer (source) state by single-qubit Clifford operations\n(LC), single-qubit Pauli measurements (LPM), and classical communication (CC)\nbetween sites holding the individual qubits. What's more, we provide a recipe\nto obtain the sequence of LC+LPM+CC operations which prepare the desired target\nstate from the source state, and show how these operations can be applied in\nparallel to reach the target state in constant time. Our algorithm has\napplications in quantum networks, quantum computing, and can also serve as a\ndesign tool - for example, to find transformations between quantum error\ncorrecting codes. We provide a software implementation of our algorithm that\nmakes this tool easier to apply.\n  A key insight leading to our algorithm is to show that the problem is\nequivalent to one in graph theory, which is to decide whether some graph G' is\na vertex-minor of another graph G. Here we show that the vertex-minor problem\ncan be solved in time O(|G|^3) where |G| is the size of the graph G, whenever\nthe rank-width of G and the size of G' are bounded. Our algorithm is based on\ntechniques by Courcelle for solving fixed parameter tractable problems, where\nhere the relevant fixed parameter is the rank width. The second half of this\npaper serves as an accessible but far from exhausting introduction to these\nconcepts, that could be useful for many other problems in quantum information.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 17:29:21 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 15:07:30 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Dahlberg", "Axel", ""], ["Wehner", "Stephanie", ""]]}, {"id": "1805.05306", "submitter": "Axel Dahlberg", "authors": "Axel Dahlberg, Jonas Helsen, Stephanie Wehner", "title": "How to transform graph states using single-qubit operations:\n  computational complexity and algorithms", "comments": "64 pages, lots of figures. For a gentle introduction to the\n  background and proof of principle algorithms see also our related work\n  'Transforming graph states using single-qubit operations' (1805.05305). For\n  related work see also F. Hahn et al (1805.04559)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS cs.ET math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph states are ubiquitous in quantum information with diverse applications\nranging from quantum network protocols to measurement based quantum computing.\nHere we consider the question whether one graph (source) state can be\ntransformed into another graph (target) state, using a specific set of quantum\noperations (LC+LPM+CC): single-qubit Clifford operations (LC), single-qubit\nPauli measurements (LPM) and classical communication (CC) between sites holding\nthe individual qubits. We first show that deciding whether a graph state |G>\ncan be transformed into another graph state |G'> using LC+LPM+CC is\nNP-Complete, even if |G'> is restricted to be the GHZ-state. However, we also\nprovide efficient algorithms for two situations of practical interest:\n  1. |G> has Schmidt-rank width one and |G'> is a GHZ-state. The Schmidt-rank\nwidth is an entanglement measure of quantum states, meaning this algorithm is\nefficient if the original state has little entanglement. Our algorithm has\nruntime O(|V(G')||V(G)|^3), and is also efficient in practice even on small\ninstances as further showcased by a freely available software implementation.\n  2. |G> is in a certain class of states with unbounded Schmidt-rank width, and\n|G'> is a GHZ-state of a constant size. Here the runtime is O(poly(|V(G)|)),\nshowing that more efficient algorithms can in principle be found even for\nstates holding a large amount of entanglement, as long as the output state has\nconstant size.\n  Our results make use of the insight that deciding whether a graph state |G>\ncan be transformed to another graph state |G'> is equivalent to a known\ndecision problem in graph theory, namely the problem of deciding whether a\ngraph G' is a vertex-minor of a graph G. Many of the technical tools developed\nto obtain our results may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 17:33:09 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 15:04:49 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Dahlberg", "Axel", ""], ["Helsen", "Jonas", ""], ["Wehner", "Stephanie", ""]]}, {"id": "1805.05375", "submitter": "Ugo Vaccaro", "authors": "Ferdinando Cicalese, Ugo Vaccaro", "title": "Maximum Entropy Interval Aggregations", "comments": "To be presented at ISIT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a probability distribution ${\\bf p} = (p_1, \\dots, p_n)$ and an integer\n$1\\leq m < n$, we say that ${\\bf q} = (q_1, \\dots, q_m)$ is a contiguous\n$m$-aggregation of ${\\bf p}$ if there exist indices $0=i_0 < i_1 < \\cdots <\ni_{m-1} < i_m = n$ such that for each $j = 1, \\dots, m$ it holds that $q_j =\n\\sum_{k=i_{j-1}+1}^{i_j} p_k.$ In this paper, we consider the problem of\nefficiently finding the contiguous $m$-aggregation of maximum entropy. We\ndesign a dynamic programming algorithm that solves the problem exactly, and two\nmore time-efficient greedy algorithms that provide slightly sub-optimal\nsolutions. We also discuss a few scenarios where our problem matters.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 18:43:29 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1805.05391", "submitter": "Kanstantsin Pashkovich", "authors": "Felix Bauckholt, Kanstantsin Pashkovich, Laura Sanit\\`a", "title": "On the approximability of the stable marriage problem with one-sided\n  ties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical stable marriage problem asks for a matching between a set of\nmen and a set of women with no blocking pairs, which are pairs formed by a man\nand a woman who would both prefer switching from their current status to be\npaired up together. When both men and women have strict preferences over the\nopposite group, all stable matchings have the same cardinality, and the famous\nGale-Shapley algorithm can be used to find one. Differently, if we allow ties\nin the preference lists, finding a stable matching of maximum cardinality is an\nNP-hard problem, already when the ties are one-sided, that is, they appear only\nin the preferences of one group. For this reason, many researchers have focused\non developing approximation algorithm for this problem.\n  In this paper, we give a refined analysis of an approximation algorithm given\nby Huang and Telikepalli (IPCO14) for the stable marriage problem with\none-sided ties, which shows an improved 13/9 -approximation factor for the\nproblem. Interestingly, our analysis is tight.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 19:16:53 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 15:43:39 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Bauckholt", "Felix", ""], ["Pashkovich", "Kanstantsin", ""], ["Sanit\u00e0", "Laura", ""]]}, {"id": "1805.05404", "submitter": "Parter Merav", "authors": "Merav Parter and Eylon Yogev", "title": "Congested Clique Algorithms for Graph Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph spanners are sparse subgraphs that faithfully preserve the distances in\nthe original graph up to small stretch. Spanner have been studied extensively\nas they have a wide range of applications ranging from distance oracles,\nlabeling schemes and routing to solving linear systems and spectral\nsparsification. A $k$-spanner maintains pairwise distances up to multiplicative\nfactor of $k$. It is a folklore that for every $n$-vertex graph $G$, one can\nconstruct a $(2k-1)$ spanner with $O(n^{1+1/k})$ edges. In a distributed\nsetting, such spanners can be constructed in the standard CONGEST model using\n$O(k^2)$ rounds, when randomization is allowed.\n  In this work, we consider spanner constructions in the congested clique\nmodel, and show: (1) A randomized construction of a $(2k-1)$-spanner with\n$\\widetilde{O}(n^{1+1/k})$ edges in $O(\\log k)$ rounds. The previous best\nalgorithm runs in $O(k)$ rounds. (2) A deterministic construction of a\n$(2k-1)$-spanner with $\\widetilde{O}(n^{1+1/k})$ edges in $O(\\log k +(\\log\\log\nn)^3)$ rounds. The previous best algorithm runs in $O(k\\log n)$ rounds. This\nimprovement is achieved by a new derandomization theorem for hitting sets which\nmight be of independent interest. (3) A deterministic construction of a\n$O(k)$-spanner with $O(k \\cdot n^{1+1/k})$ edges in $O(\\log k)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 19:48:13 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Parter", "Merav", ""], ["Yogev", "Eylon", ""]]}, {"id": "1805.05436", "submitter": "Koji M. Kobayashi", "authors": "Koji M. Kobayashi", "title": "Online interval scheduling to maximize total satisfaction", "comments": "An extended abstract of this paper appears in Proc. of COCOON 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interval scheduling problem is one variant of the scheduling problem. In\nthis paper, we propose a novel variant of the interval scheduling problem,\nwhose definition is as follows: given jobs are specified by their {\\em release\ntimes}, {\\em deadlines} and {\\em profits}. An algorithm must start a job at its\nrelease time on one of $m$ identical machines, and continue processing until\nits deadline on the machine to complete the job. All the jobs must be completed\nand the algorithm can obtain the profit of a completed job as a user's\nsatisfaction. It is possible to process more than one job at a time on one\nmachine. The profit of a job is distributed uniformly between its release time\nand deadline, that is its interval, and the profit gained from a subinterval of\na job decreases in reverse proportion to the number of jobs whose intervals\nintersect with the subinterval on the same machine. The objective of our\nvariant is to maximize the total profit of completed jobs. This formulation is\nnaturally motivated by best-effort requests and responses to them, which appear\nin many situations. In best-effort requests and responses, the total amount of\navailable resources for users is always invariant and the resources are equally\nshared with every user. We study online algorithms for this problem.\nSpecifically, we show that for the case where the profits of jobs are\narbitrary, there does not exist an algorithm whose competitive ratio is\nbounded. Then, we consider the case in which the profit of each job is equal to\nits length, that is, the time interval between its release time and deadline.\nFor this case, we prove that for $m = 2$ and $m \\geq 3$, the competitive ratios\nof a greedy algorithm are at most $4/3$ and at most $3$, respectively. Also,\nfor each $m \\geq 2$, we show a lower bound on the competitive ratio of any\ndeterministic algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 20:48:32 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Kobayashi", "Koji M.", ""]]}, {"id": "1805.05443", "submitter": "Timothy Johnson", "authors": "Juan Jose Besa, William E. Devanny, David Eppstein, Michael Goodrich,\n  Timothy Johnson", "title": "Quadratic Time Algorithms Appear to be Optimal for Sorting Evolving Data", "comments": null, "journal-ref": "2018 Proceedings of the Twentieth Workshop on Algorithm\n  Engineering and Experiments (ALENEX) 87-96", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically study sorting in the evolving data model. In this model, a\nsorting algorithm maintains an approximation to the sorted order of a list of\ndata items while simultaneously, with each comparison made by the algorithm, an\nadversary randomly swaps the order of adjacent items in the true sorted order.\nPrevious work studies only two versions of quicksort, and has a gap between the\nlower bound of Omega(n) and the best upper bound of O(n log log n). The\nexperiments we perform in this paper provide empirical evidence that some\nquadratic-time algorithms such as insertion sort and bubble sort are\nasymptotically optimal for any constant rate of random swaps. In fact, these\nalgorithms perform as well as or better than algorithms such as quicksort that\nare more efficient in the traditional algorithm analysis model.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 20:54:56 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Besa", "Juan Jose", ""], ["Devanny", "William E.", ""], ["Eppstein", "David", ""], ["Goodrich", "Michael", ""], ["Johnson", "Timothy", ""]]}, {"id": "1805.05448", "submitter": "Binhai Zhu", "authors": "Sergey Bereg and Feifei Ma and Wencheng Wang and Jian Zhang and Binhai\n  Zhu", "title": "On the Fixed-Parameter Tractability of Some Matching Problems Under the\n  Color-Spanning Model", "comments": "12 pages, 2 figures, earlier version appeared in FAW'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points $P$ in the plane, each colored with one of the $t$\ngiven colors, a color-spanning set $S\\subset P$ is a subset of $t$ points with\ndistinct colors. The minimum diameter color-spanning set (MDCS) is a\ncolor-spanning set whose diameter is minimum (among all color-spanning sets of\n$P$). Somehow symmetrically, the largest closest pair color-spanning set\n(LCPCS) is a color-spanning set whose closest pair is the largest (among all\ncolor-spanning sets of $P$). Both MDCS and LCPCS have been shown to be\nNP-complete, but whether they are fixed-parameter tractable (FPT) when $t$ is a\nparameter is still open. Motivated by this question, we consider the FPT\ntractability of some matching problems under this color-spanning model, where\n$t=2k$ is the parameter. The problems are summarized as follows: (1) MinSum\nMatching Color-Spanning Set, namely, computing a matching of $2k$ points with\ndistinct colors such that their total edge length is minimized; (2) MaxMin\nMatching Color-Spanning Set, namely, computing a matching of $2k$ points with\ndistinct colors such that the minimum edge length is maximized; (3) MinMax\nMatching Color-Spanning Set, namely, computing a matching of $2k$ points with\ndistinct colors such that the maximum edge length is minimized; and (4)\n$k$-Multicolored Independent Matching, namely, computing a matching of $2k$\nvertices in a graph such that the vertices of the edges in the matching do not\nshare common edges in the graph. We show that the first three problems are\npolynomially solvable (hence in FPT), while problem (4) is W[1]-hard.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 21:14:18 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Bereg", "Sergey", ""], ["Ma", "Feifei", ""], ["Wang", "Wencheng", ""], ["Zhang", "Jian", ""], ["Zhu", "Binhai", ""]]}, {"id": "1805.05592", "submitter": "Yan Gu", "authors": "Guy E. Blelloch and Yan Gu and Yihan Sun and Julian Shun", "title": "Parallel Write-Efficient Algorithms and Data Structures for\n  Computational Geometry", "comments": null, "journal-ref": null, "doi": "10.1145/3210377.3210380", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design parallel write-efficient geometric algorithms that\nperform asymptotically fewer writes than standard algorithms for the same\nproblem. This is motivated by emerging non-volatile memory technologies with\nread performance being close to that of random access memory but writes being\nsignificantly more expensive in terms of energy and latency. We design\nalgorithms for planar Delaunay triangulation, $k$-d trees, and static and\ndynamic augmented trees. Our algorithms are designed in the recently introduced\nAsymmetric Nested-Parallel Model, which captures the parallel setting in which\nthere is a small symmetric memory where reads and writes are unit cost as well\nas a large asymmetric memory where writes are $\\omega$ times more expensive\nthan reads. In designing these algorithms, we introduce several techniques for\nobtaining write-efficiency, including DAG tracing, prefix doubling,\nreconstruction-based rebalancing and $\\alpha$-labeling, which we believe will\nbe useful for designing other parallel write-efficient algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:02:20 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 10:08:57 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Gu", "Yan", ""], ["Sun", "Yihan", ""], ["Shun", "Julian", ""]]}, {"id": "1805.05787", "submitter": "Wei Quan Lim", "authors": "Kunal Agrawal, Seth Gilbert, Wei Quan Lim", "title": "Parallel Working-Set Search Structures", "comments": "Authors' version of a paper accepted to SPAA 2018", "journal-ref": null, "doi": "10.1145/3210377.3210390", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two versions of a parallel working-set map on p\nprocessors that supports searches, insertions and deletions. In both versions,\nthe total work of all operations when the map has size at least p is bounded by\nthe working-set bound, i.e., the cost of an item depends on how recently it was\naccessed (for some linearization): accessing an item in the map with recency r\ntakes O(1+log r) work. In the simpler version each map operation has O((log\np)^2+log n) span (where n is the maximum size of the map). In the pipelined\nversion each map operation on an item with recency r has O((log p)^2+log r)\nspan. (Operations in parallel may have overlapping span; span is additive only\nfor operations in sequence.)\n  Both data structures are designed to be used by a dynamic multithreading\nparallel program that at each step executes a unit-time instruction or makes a\ndata structure call. To achieve the stated bounds, the pipelined data structure\nrequires a weak-priority scheduler, which supports a limited form of 2-level\nprioritization. At the end we explain how the results translate to practical\nimplementations using work-stealing schedulers.\n  To the best of our knowledge, this is the first parallel implementation of a\nself-adjusting search structure where the cost of an operation adapts to the\naccess sequence. A corollary of the working-set bound is that it achieves work\nstatic optimality: the total work is bounded by the access costs in an optimal\nstatic search tree.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 14:14:20 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:17:06 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 11:27:04 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Agrawal", "Kunal", ""], ["Gilbert", "Seth", ""], ["Lim", "Wei Quan", ""]]}, {"id": "1805.05993", "submitter": "Gianni Antichi", "authors": "Jan Ku\\v{c}era, Diana Andreea Popescu, Gianni Antichi, Jan\n  Ko\\v{r}enek, Andrew W. Moore", "title": "Seek and Push: Detecting Large Traffic Aggregates in the Dataplane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High level goals such as bandwidth provisioning, accounting and network\nanomaly detection can be easily met if high-volume traffic clusters are\ndetected in real time. This paper presents Elastic Trie, an alternative to\napproaches leveraging controller-dataplane architectures.\n  Our solution is a novel push-based network monitoring approach that allows\ndetection, within the dataplane, of high-volume traffic clusters. Notifications\nfrom the switch to the controller can be sent only as required, avoiding the\ntransmission or processing of unnecessary data. Furthermore, the dataplane can\niteratively refine the responsible IP prefixes allowing a controller to receive\na flexible granularity information. We report and discuss an evaluation of our\nP4-based prototype, showing our solution to be able to detect (with 95% of\nprecision), hierarchical heavy hitters and superspreaders using less than 8KB\nor 80KB of active memory respectively. Finally, Elastic Trie can identify\nchanges in the network traffic patterns, symptomatic of Denial-of-Service\nattack events.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 18:52:21 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Ku\u010dera", "Jan", ""], ["Popescu", "Diana Andreea", ""], ["Antichi", "Gianni", ""], ["Ko\u0159enek", "Jan", ""], ["Moore", "Andrew W.", ""]]}, {"id": "1805.06151", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz, Ely Porat, Yair Rosenmutter", "title": "Improved Worst-Case Deterministic Parallel Dynamic Minimum Spanning\n  Forest", "comments": "Full version of a paper accepted to SPAA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a new deterministic algorithm for the dynamic Minimum\nSpanning Forest (MSF) problem in the EREW PRAM model, where the goal is to\nmaintain a MSF of a weighted graph with $n$ vertices and $m$ edges while\nsupporting edge insertions and deletions. We show that one can solve the\ndynamic MSF problem using $O(\\sqrt n)$ processors and $O(\\log n)$ worst-case\nupdate time, for a total of $O(\\sqrt n \\log n)$ work. This improves on the work\nof Ferragina [IPPS 1995] which costs $O(\\log n)$ worst-case update time and\n$O(n^{2/3} \\log{\\frac{m}{n}})$ work.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:32:44 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""], ["Rosenmutter", "Yair", ""]]}, {"id": "1805.06177", "submitter": "Neda Tavakoli", "authors": "Sahar Hooshmand, Neda Tavakoli, Paniz Abedin, Sharma V. Thankachan", "title": "On Computing Average Common Substring Over Run Length Encoded Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Average Common Substring (ACS) is a popular alignment-free distance\nmeasure for phylogeny reconstruction. The ACS can be computed in O(n) space and\ntime, where n=x+y is the input size. The compressed string matching is the\nstudy of string matching problems with the following twist: the input data is\nin a compressed format and the underling task must be performed with little or\nno decompression. In this paper, we revisit the ACS problem under this paradigm\nwhere the input sequences are given in their run-length encoded format. We\npresent an algorithm to compute ACS(X,Y) in O(Nlog N) time using O(N) space,\nwhere N is the total length of sequences after run-length encoding.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:56:49 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Hooshmand", "Sahar", ""], ["Tavakoli", "Neda", ""], ["Abedin", "Paniz", ""], ["Thankachan", "Sharma V.", ""]]}, {"id": "1805.06232", "submitter": "Kurt Mehlhorn", "authors": "Bhaskar Chaudhury and Yun Kuen Cheung and Jugal Garg and Naveen Garg\n  and Martin Hoefer and Kurt Mehlhorn", "title": "On Fair Division of Indivisible Items", "comments": null, "journal-ref": "FSTTCS 2018", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of assigning indivisible goods to a set of agents in a\nfair manner. Our notion of fairness is Nash social welfare, i.e., the goal is\nto maximize the geometric mean of the utilities of the agents. Each good comes\nin multiple items or copies, and the utility of an agent diminishes as it\nreceives more items of the same good. The utility of a bundle of items for an\nagent is the sum of the utilities of the items in the bundle. Each agent has a\nutility cap beyond which he does not value additional items. We give a\npolynomial time approximation algorithm that maximizes Nash social welfare up\nto a factor of $e^{1/{e}} \\approx 1.445$. The computed allocation is\nPareto-optimal and approximates envy-freeness up to one item up to a factor of\n$2 + \\eps$\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 10:21:55 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 08:57:17 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 11:40:00 GMT"}, {"version": "v4", "created": "Thu, 4 Oct 2018 07:24:58 GMT"}, {"version": "v5", "created": "Fri, 10 May 2019 08:10:31 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Chaudhury", "Bhaskar", ""], ["Cheung", "Yun Kuen", ""], ["Garg", "Jugal", ""], ["Garg", "Naveen", ""], ["Hoefer", "Martin", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1805.06265", "submitter": "Alon Berger", "authors": "Alon Berger, Idit Keidar, and Alexander Spiegelman", "title": "Integrated Bounds for Disintegrated Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out a somewhat surprising similarity between non-authenticated\nByzantine storage, coded storage, and certain emulations of shared registers\nfrom smaller ones. A common characteristic in all of these is the inability of\nreads to safely return a value obtained in a single atomic access to shared\nstorage. We collectively refer to such systems as disintegrated storage, and\nshow integrated space lower bounds for asynchronous regular wait-free\nemulations in all of them. In a nutshell, if readers are invisible, then the\nstorage cost of such systems is inherently exponential in the size of written\nvalues; otherwise, it is at least linear in the number of readers. Our bounds\nare asymptotically tight to known algorithms, and thus justify their high\ncosts.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 12:15:10 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 12:23:49 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Berger", "Alon", ""], ["Keidar", "Idit", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1805.06282", "submitter": "Mario Holldack", "authors": "Mario Holldack", "title": "Max-Product for Maximum Weight Matching - Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on belief propagation for the assignment problem, also known as the\nmaximum weight bipartite matching problem. We provide a constructive proof that\nthe well-known upper bound on the number of iterations (Bayati, Shah, Sharma\n2008) is tight up to a factor of four. Furthermore, we investigate the behavior\nof belief propagation when convergence is not required. We show that the number\nof iterations required for a sharp approximation consumes a large portion of\nthe convergence time. Finally, we propose an \"approximate belief propagation\"\nalgorithm for the assignment problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 13:06:29 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Holldack", "Mario", ""]]}, {"id": "1805.06315", "submitter": "Mahmoud Parham", "authors": "Saeed Akhoondian Amiri, Szymon Dudycz, Mahmoud Parham, Stefan Schmid,\n  Sebastian Wiederrecht", "title": "Short Schedules for Fast Flow Rerouting", "comments": "arXiv admin note: text overlap with arXiv:1611.09296", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the fundamental problem of how to reroute $k$ unsplittable\nflows of a certain demand in a capacitated network from their current paths to\ntheir respective new paths, in a congestion-free manner and fast. This\nscheduling problem has applications in traffic engineering in communication\nnetworks and has recently received much attention in software-defined networks,\nin which updates are distributed over an asynchronous network by a software\ncontroller. However, existing algorithms for this problem either have a\nsuper-polynomial runtime or only compute feasible schedules, which do not\nprovide any guarantees on the length of the rerouting schedule.\n  This paper presents the first polynomial-time algorithm for computing\nshortest update schedules to reroute flows in a congestion-free manner. We\ncontribute an almost tight characterization of the polynomial-time tractability\nof the problem: We present the first polynomial-time solution for this problem\nfor two flows, but also show that even the question whether a feasible update\nschedule exists, is already NP-hard for six flows.\n  In fact, the presented algorithm runs in linear time and is hence not only\noptimal in terms of scheduling but also asymptotically optimal in terms of\nruntime.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 12:55:49 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 13:49:46 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 13:38:29 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Dudycz", "Szymon", ""], ["Parham", "Mahmoud", ""], ["Schmid", "Stefan", ""], ["Wiederrecht", "Sebastian", ""]]}, {"id": "1805.06420", "submitter": "Ger Yang", "authors": "David Applegate, Aaron Archer, David S. Johnson, Evdokia Nikolova,\n  Mikkel Thorup, Ger Yang", "title": "Wireless coverage prediction via parametric shortest paths", "comments": "12 pages, 8 figures, accepted to MobiHoc 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deciding where to place access points in a wireless network, it is\nuseful to model the signal propagation loss between a proposed antenna location\nand the areas it may cover. The indoor dominant path (IDP) model, introduced by\nW\\\"{o}lfle et al., is shown in the literature to have good validation and\ngeneralization error, is faster to compute than competing methods, and is used\nin commercial software such as WinProp, iBwave Design, and CellTrace.\nPreviously, the algorithms known for computing it involved a worst-case\nexponential-time tree search, with pruning heuristics to speed it up.\n  We prove that the IDP model can be reduced to a parametric shortest path\ncomputation on a graph derived from the walls in the floorplan. It therefore\nadmits a quasipolynomial-time (i.e., $n^{O(\\log n)}$) algorithm. We also give a\npractical approximation algorithm based on running a small constant number of\nshortest path computations. Its provable worst-case additive error (in dB) can\nbe made arbitrarily small via appropriate choices of parameters, and is well\nbelow 1dB for reasonable choices. We evaluate our approximation algorithm\nempirically against the exact IDP model, and show that it consistently beats\nits theoretical worst-case bounds, solving the model exactly (i.e., no error)\nin the vast majority of cases.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:49:49 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Applegate", "David", ""], ["Archer", "Aaron", ""], ["Johnson", "David S.", ""], ["Nikolova", "Evdokia", ""], ["Thorup", "Mikkel", ""], ["Yang", "Ger", ""]]}, {"id": "1805.06699", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Victor A. Campos, Carlos Vin\\'icius G. C. Lima,\n  Vin\\'icius Fernandes dos Santos, Ignasi Sau, Ana Silva", "title": "Dual parameterization of Weighted Coloring", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, a proper $k$-coloring of $G$ is a partition $c =\n(S_i)_{i\\in [1,k]}$ of $V(G)$ into $k$ stable sets $S_1,\\ldots, S_{k}$. Given a\nweight function $w: V(G) \\to \\mathbb{R}^+$, the weight of a color $S_i$ is\ndefined as $w(i) = \\max_{v \\in S_i} w(v)$ and the weight of a coloring $c$ as\n$w(c) = \\sum_{i=1}^{k}w(i)$. Guan and Zhu [Inf. Process. Lett., 1997] defined\nthe weighted chromatic number of a pair $(G,w)$, denoted by $\\sigma(G,w)$, as\nthe minimum weight of a proper coloring of $G$. The problem of determining\n$\\sigma(G,w)$ has received considerable attention during the last years, and\nhas been proved to be notoriously hard: for instance, it is NP-hard on split\ngraphs, unsolvable on $n$-vertex trees in time $n^{o(\\log n)}$ unless the ETH\nfails, and W[1]-hard on forests parameterized by the size of a largest tree. In\nthis article we provide some positive results for the problem, by considering\nits so-called dual parameterization: given a vertex-weighted graph $(G,w)$ and\nan integer $k$, the question is whether $\\sigma(G,w) \\leq \\sum_{v \\in V(G)}\nw(v) - k$. We prove that this problem is FPT by providing an algorithm running\nin time $9^k \\cdot n^{O(1)}$, and it is easy to see that no algorithm in time\n$2^{o(k)} \\cdot n^{O(1)}$ exists under the ETH. On the other hand, we present a\nkernel with at most $(2^{k-1}+1) (k-1)$ vertices, and we rule out the existence\nof polynomial kernels unless ${\\sf NP} \\subseteq {\\sf coNP} / {\\sf poly}$, even\non split graphs with only two different weights. Finally, we identify some\nclasses of graphs on which the problem admits a polynomial kernel, in\nparticular interval graphs and subclasses of split graphs, and in the latter\ncase we present lower bounds on the degrees of the polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 11:15:58 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Campos", "Victor A.", ""], ["Lima", "Carlos Vin\u00edcius G. C.", ""], ["Santos", "Vin\u00edcius Fernandes dos", ""], ["Sau", "Ignasi", ""], ["Silva", "Ana", ""]]}, {"id": "1805.06821", "submitter": "Giovanni Manzini", "authors": "Lavinia Egidi, Felipe A. Louza, Giovanni Manzini, Guilherme P. Telles", "title": "External memory BWT and LCP computation for sequence collections with\n  applications", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.WABI.2018.10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an external memory algorithm for the computation of the BWT and\nLCP array for a collection of sequences. Our algorithm takes the amount of\navailable memory as an input parameter, and tries to make the best use of it by\nsplitting the input collection into subcollections sufficiently small that it\ncan compute their BWT in RAM using an optimal linear time algorithm. Next, it\nmerges the partial BWTs in external memory and in the process it also computes\nthe LCP values. We prove that our algorithm performs O(n AveLcp) sequential\nI/Os, where n is the total length of the collection, and AveLcp is the average\nLongest Common Prefix of the collection. This bound is an improvement over the\nknown algorithms for the same task. The experimental results show that our\nalgorithm outperforms the current best algorithm for collections of sequences\nwith different lengths and for collections with relatively small average\nLongest Common Prefix.\n  In the second part of the paper, we show that our algorithm can be modified\nto output two additional arrays that, used with the BWT and LCP arrays, provide\nsimple, scan based, external memory algorithms for three well known problems in\nbioinformatics: the computation of maximal repeats, the all pairs suffix-prefix\noverlaps, and the construction of succinct de Bruijn graphs. To our knowledge,\nthere are no other known external memory algorithms for these problems.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 15:28:49 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Egidi", "Lavinia", ""], ["Louza", "Felipe A.", ""], ["Manzini", "Giovanni", ""], ["Telles", "Guilherme P.", ""]]}, {"id": "1805.06836", "submitter": "Viktor Zamaraev", "authors": "Jessica Enright, Kitty Meeks, George B. Mertzios, Viktor Zamaraev", "title": "Deleting edges to restrict the size of an epidemic in temporal networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spreading processes on graphs are a natural model for a wide variety of\nreal-world phenomena, including information spread over social networks and\nbiological diseases spreading over contact networks. Often, the networks over\nwhich these processes spread are dynamic in nature, and can be modeled with\ntemporal graphs. Here, we study the problem of deleting edges from a given\ntemporal graph in order to reduce the number of vertices (temporally) reachable\nfrom a given starting point. This could be used to control the spread of a\ndisease, rumour, etc. in a temporal graph. In particular, our aim is to find a\ntemporal subgraph in which a process starting at any single vertex can be\ntransferred to only a limited number of other vertices using a\ntemporally-feasible path. We introduce a natural edge-deletion problem for\ntemporal graphs and provide positive and negative results on its computational\ncomplexity and approximability.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:06:44 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 11:27:44 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Enright", "Jessica", ""], ["Meeks", "Kitty", ""], ["Mertzios", "George B.", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "1805.06869", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en", "title": "Revisiting the tree edit distance and its backtracing: A tutorial", "comments": "Supplementary material for the ICML 2018 paper: Tree Edit Distance\n  Learning via Adaptive Symbol Embeddings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Almost 30 years ago, Zhang and Shasha (1989) published a seminal paper\ndescribing an efficient dynamic programming algorithm computing the tree edit\ndistance, that is, the minimum number of node deletions, insertions, and\nreplacements that are necessary to transform one tree into another. Since then,\nthe tree edit distance has been widely applied, for example in biology and\nintelligent tutoring systems. However, the original paper of Zhang and Shasha\ncan be challenging to read for newcomers and it does not describe how to\nefficiently infer the optimal edit script. In this contribution, we provide a\ncomprehensive tutorial to the tree edit distance algorithm of Zhang and Shasha.\nWe further prove metric properties of the tree edit distance, and describe\nefficient algorithms to infer the cheapest edit script, as well as a summary of\nall cheapest edit scripts between two trees.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 17:16:07 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 12:01:18 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 16:47:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""]]}, {"id": "1805.06872", "submitter": "Nicolas Resch", "authors": "Bernhard Haeupler and Nicolas Resch", "title": "Coding for Interactive Communication with Small Memory and Applications\n  to Robust Circuits", "comments": "There is a problem with the main results, i.e., Theorems 6.4 and 6.5.\n  Specifically, an entire iteration can be corrupted in such a way that both\n  parties arrive at the same incorrect node. Our algorithm cannot detect this\n  error. Moreover, the robust KW-transform (Lemma 7.7) for circuits needs to\n  include the assumption that all the internal nodes correspond to\n  combinatorial rectangles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classically, coding theory has been concerned with the problem of\ntransmitting a single message in a format which is robust to noise. Recently,\nresearchers have turned their attention to designing coding schemes to make\ntwo-way conversations robust to noise. That is, given an interactive\ncommunication protocol $\\Pi$, an \\emph{interactive coding scheme} converts\n$\\Pi$ into another communication protocol $\\Pi'$ such that, even if errors are\nintroduced during the execution of $\\Pi'$, the parties are able to determine\nwhat the outcome of running $\\Pi$ would be in a noise-free setting.\n  We consider the problem of designing interactive coding schemes which allow\nthe parties to simulate the original protocol using little memory.\nSpecifically, given any communication protocol $\\Pi$ we construct robust\nsimulating protocols which tolerate a constant noise rate and require the\nparties to use only $O(\\log d \\log s)$ memory, where $d$ is the depth of $\\Pi$\nand $s$ is a measure of the size of $\\Pi$. Prior to this work, all known coding\nschemes required the parties to use at least $\\Omega(d)$ memory, as the parties\nwere required to remember the transcript of the conversation thus far.\nMoreover, our coding scheme achieves a communication rate of\n$1-O(\\sqrt{\\varepsilon})$ over oblivious channels and\n$1-O(\\sqrt{\\varepsilon\\log\\log\\tfrac{1}{\\varepsilon}})$ over adaptive\nadversarial channels, matching the conjecturally optimal rates. Lastly, we\npoint to connections between fault-tolerant circuits and coding for interactive\ncommunication with small memory.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 17:26:53 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 15:02:48 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Resch", "Nicolas", ""]]}, {"id": "1805.06990", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle, J. David Smith, Victoria G. Crawford, My T. Thai", "title": "Fast Maximization of Non-Submodular, Monotonic Functions on the Integer\n  Lattice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of submodular functions on the integer lattice has received\nmuch attention recently, but the objective functions of many applications are\nnon-submodular. We provide two approximation algorithms for maximizing a\nnon-submodular function on the integer lattice subject to a cardinality\nconstraint; these are the first algorithms for this purpose that have\npolynomial query complexity. We propose a general framework for influence\nmaximization on the integer lattice that generalizes prior works on this topic,\nand we demonstrate the efficiency of our algorithms in this context.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 23:19:29 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Kuhnle", "Alan", ""], ["Smith", "J. David", ""], ["Crawford", "Victoria G.", ""], ["Thai", "My T.", ""]]}, {"id": "1805.07135", "submitter": "Thore Husfeldt", "authors": "Karl Bringmann and Thore Husfeldt and M{\\aa}ns Magnusson", "title": "Multivariate Analysis of Orthogonal Range Searching and Graph Distances\n  Parameterized by Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the eccentricities, diameter, radius, and Wiener index of an\nundirected $n$-vertex graph with nonnegative edge lengths can be computed in\ntime $O(n\\cdot \\binom{k+\\lceil\\log n\\rceil}{k} \\cdot 2^k k^2 \\log n)$, where\n$k$ is the treewidth of the graph. For every $\\epsilon>0$, this bound is\n$n^{1+\\epsilon}\\exp O(k)$, which matches a hardness result of Abboud,\nVassilevska Williams, and Wang (SODA 2015) and closes an open problem in the\nmultivariate analysis of polynomial-time computation. To this end, we show that\nthe analysis of an algorithm of Cabello and Knauer (Comp. Geom., 2009) in the\nregime of non-constant treewidth can be improved by revisiting the analysis of\northogonal range searching, improving bounds of the form $\\log^d n$ to\n$\\binom{d+\\lceil\\log n\\rceil}{d}$, as originally observed by Monier (J. Alg.\n1980).\n  We also investigate the parameterization by vertex cover number.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 10:47:16 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Bringmann", "Karl", ""], ["Husfeldt", "Thore", ""], ["Magnusson", "M\u00e5ns", ""]]}, {"id": "1805.07141", "submitter": "Charis Papadopoulos", "authors": "Charis Papadopoulos and Spyridon Tzimas", "title": "Subset Feedback Vertex Set on Graphs of Bounded Independent Set Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (\\textsc{Weighted}) \\textsc{Subset Feedback Vertex Set} problem is a\ngeneralization of the classical \\textsc{Feedback Vertex Set} problem and asks\nfor a vertex set of minimum (weighted) size that intersects all cycles\ncontaining a vertex of a predescribed set of vertices. Although the two\nproblems exhibit different computational complexity on split graphs, no similar\ncharacterization is known on other classes of graphs. Towards the understanding\nof the complexity difference between the two problems, it is natural to study\nthe importance of a structural graph parameter. Here we consider graphs of\nbounded independent set number for which it is known that \\textsc{Weighted\nFeedback Vertex Set} is solved in polynomial time. We provide a dichotomy\nresult with respect to the size of a maximum independent set. In particular we\nshow that \\textsc{Weighted Subset Feedback Vertex Set} can be solved in\npolynomial time for graphs of independent set number at most three, whereas we\nprove that the problem remains NP-hard for graphs of independent set number\nfour. Moreover, we show that the (unweighted) \\textsc{Subset Feedback Vertex\nSet} problem can be solved in polynomial time on graphs of bounded independent\nset number by giving an algorithm with running time $n^{O(d)}$, where $d$ is\nthe size of a maximum independent set of the input graph. To complement our\nresults, we demonstrate how our ideas can be extended to other terminal set\nproblems on graphs of bounded independent set size. Based on our findings for\n\\textsc{Subset Feedback Vertex Set}, we settle the complexity of \\textsc{Node\nMultiway Cut}, a terminal set problem that asks for a vertex set of minimum\nsize that intersects all paths connecting any two terminals, as well as its\nvariants where nodes are weighted and/or the terminals are deletable, for every\nvalue of the given independent set number.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:05:43 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Papadopoulos", "Charis", ""], ["Tzimas", "Spyridon", ""]]}, {"id": "1805.07209", "submitter": "Simon Weidner", "authors": "Fabian Kuhn, Yannic Maus, Simon Weidner", "title": "Deterministic Distributed Ruling Sets of Line Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(\\alpha,\\beta)$-ruling set of a graph $G=(V,E)$ is a set $R\\subseteq V$\nsuch that for any node $v\\in V$ there is a node $u\\in R$ in distance at most\n$\\beta$ from $v$ and such that any two nodes in $R$ are at distance at least\n$\\alpha$ from each other. The concept of ruling sets can naturally be extended\nto edges, i.e., a subset $F\\subseteq E$ is an $(\\alpha,\\beta)$-ruling edge set\nof a graph $G=(V,E)$ if the corresponding nodes form an $(\\alpha,\\beta)$-ruling\nset in the line graph of $G$. This paper presents a simple deterministic,\ndistributed algorithm, in the $\\mathsf{CONGEST}$ model, for computing\n$(2,2)$-ruling edge sets in $O(\\log^* n)$ rounds. Furthermore, we extend the\nalgorithm to compute ruling sets of graphs with bounded diversity. Roughly\nspeaking, the diversity of a graph is the maximum number of maximal cliques a\nvertex belongs to. We devise $(2,O(\\mathcal{D}))$-ruling sets on graphs with\ndiversity $\\mathcal{D}$ in $O(\\mathcal{D}+\\log^* n)$ rounds. This also implies\na fast, deterministic $(2,O(\\ell))$-ruling edge set algorithm for hypergraphs\nwith rank at most $\\ell$.\n  Furthermore, we provide a ruling set algorithm for general graphs that for\nany $B\\geq 2$ computes an $\\big(\\alpha, \\alpha \\lceil \\log_B n \\rceil\n\\big)$-ruling set in $O(\\alpha \\cdot B \\cdot \\log_B n)$ rounds in the\n$\\mathsf{CONGEST}$ model. The algorithm can be modified to compute a $\\big(2,\n\\beta \\big)$-ruling set in $O(\\beta \\Delta^{2/\\beta} + \\log^* n)$ rounds in the\n$\\mathsf{CONGEST}$~ model, which matches the currently best known such\nalgorithm in the more general $\\mathsf{LOCAL}$ model.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 13:52:42 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Weidner", "Simon", ""]]}, {"id": "1805.07232", "submitter": "Feodor Dragan F", "authors": "Victor Chepoi, Feodor F. Dragan, Michel Habib, Yann Vax\\`es and Hend\n  Al-Rasheed", "title": "Fast approximation of centrality and distances in hyperbolic graphs", "comments": "arXiv admin note: text overlap with arXiv:1506.01799 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the eccentricities (and thus the centrality indices) of all\nvertices of a $\\delta$-hyperbolic graph $G=(V,E)$ can be computed in linear\ntime with an additive one-sided error of at most $c\\delta$, i.e., after a\nlinear time preprocessing, for every vertex $v$ of $G$ one can compute in\n$O(1)$ time an estimate $\\hat{e}(v)$ of its eccentricity $ecc_G(v)$ such that\n$ecc_G(v)\\leq \\hat{e}(v)\\leq ecc_G(v)+ c\\delta$ for a small constant $c$. We\nprove that every $\\delta$-hyperbolic graph $G$ has a shortest path tree,\nconstructible in linear time, such that for every vertex $v$ of $G$,\n$ecc_G(v)\\leq ecc_T(v)\\leq ecc_G(v)+ c\\delta$. These results are based on an\ninteresting monotonicity property of the eccentricity function of hyperbolic\ngraphs: the closer a vertex is to the center of $G$, the smaller its\neccentricity is. We also show that the distance matrix of $G$ with an additive\none-sided error of at most $c'\\delta$ can be computed in $O(|V|^2\\log^2|V|)$\ntime, where $c'< c$ is a small constant. Recent empirical studies show that\nmany real-world graphs (including Internet application networks, web networks,\ncollaboration networks, social networks, biological networks, and others) have\nsmall hyperbolicity. So, we analyze the performance of our algorithms for\napproximating centrality and distance matrix on a number of real-world\nnetworks. Our experimental results show that the obtained estimates are even\nbetter than the theoretical bounds.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 02:29:42 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Chepoi", "Victor", ""], ["Dragan", "Feodor F.", ""], ["Habib", "Michel", ""], ["Vax\u00e8s", "Yann", ""], ["Al-Rasheed", "Hend", ""]]}, {"id": "1805.07392", "submitter": "Ahad N. Zehmakan", "authors": "Clemens Jeger and Ahad N. Zehmakan", "title": "Dynamic Monopolies in Reversible Bootstrap Percolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extremal question for the (reversible) $r-$bootstrap percolation\nprocesses. Given a graph and an initial configuration where each vertex is\nactive or inactive, in the $r-$bootstrap percolation process the following rule\nis applied in discrete-time rounds: each vertex gets active if it has at least\n$r$ active neighbors, and an active vertex stays active forever. In the\nreversible $r$-bootstrap percolation, each vertex gets active if it has at\nleast $r$ active neighbors, and inactive otherwise. We consider the following\nquestion on the $d$-dimensional torus: how many vertices should be initially\nactive so that the whole graph becomes active? Our results settle an open\nproblem by Balister, Bollob\\'as, Johnson, and Walters and generalize the\nresults by Flocchini, Lodi, Luccio, Pagli, and Santoro.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 16:09:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Jeger", "Clemens", ""], ["Zehmakan", "Ahad N.", ""]]}, {"id": "1805.07455", "submitter": "Takanori Maehara", "authors": "So Nakashima, Takanori Maehara", "title": "Subspace Selection via DR-Submodular Maximization on Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subspace selection problem seeks a subspace that maximizes an objective\nfunction under some constraint. This problem includes several important machine\nlearning problems such as the principal component analysis and sparse\ndictionary selection problem. Often, these problems can be solved by greedy\nalgorithms. Here, we are interested in why these problems can be solved by\ngreedy algorithms, and what classes of objective functions and constraints\nadmit this property. To answer this question, we formulate the problems as\noptimization problems on lattices. Then, we introduce a new class of functions,\ndirectional DR-submodular functions, to characterize the approximability of\nproblems. We see that the principal component analysis, sparse dictionary\nselection problem, and these generalizations have directional\nDR-submodularities. We show that, under several constraints, the directional\nDR-submodular function maximization problem can be solved efficiently with\nprovable approximation factors.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 21:57:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Nakashima", "So", ""], ["Maehara", "Takanori", ""]]}, {"id": "1805.07474", "submitter": "Lin Chen", "authors": "Lin Chen, Mingrui Zhang, Amin Karbasi", "title": "Projection-Free Bandit Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first computationally efficient projection-free\nalgorithm for bandit convex optimization (BCO). We show that our algorithm\nachieves a sublinear regret of $O(nT^{4/5})$ (where $T$ is the horizon and $n$\nis the dimension) for any bounded convex functions with uniformly bounded\ngradients. We also evaluate the performance of our algorithm against baselines\non both synthetic and real data sets for quadratic programming, portfolio\nselection and matrix completion problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 23:29:24 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 00:24:32 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Chen", "Lin", ""], ["Zhang", "Mingrui", ""], ["Karbasi", "Amin", ""]]}, {"id": "1805.07491", "submitter": "Assaf Kfoury", "authors": "Assaf Kfoury", "title": "A Compositional Approach to Network Algorithms", "comments": "44 pages, 12 figures, 47 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present elements of a typing theory for flow networks, where \"types\",\n\"typings\", and \"type inference\" are formulated in terms of familiar notions\nfrom polyhedral analysis and convex optimization. Based on this typing theory,\nwe develop an alternative approach to the design and analysis of network\nalgorithms, which we illustrate by applying it to the max-flow problem in\nmultiple-source, multiple-sink, capacited directed planar graphs.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 02:21:31 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Kfoury", "Assaf", ""]]}, {"id": "1805.07535", "submitter": "Gera Weiss", "authors": "Liat Cohen, Dror Fried, Gera Weiss", "title": "An optimal approximation of discrete random variables with respect to\n  the Kolmogorov distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that takes a discrete random variable $X$ and a\nnumber $m$ and computes a random variable whose support (set of possible\noutcomes) is of size at most $m$ and whose Kolmogorov distance from $X$ is\nminimal. In addition to a formal theoretical analysis of the correctness and of\nthe computational complexity of the algorithm, we present a detailed empirical\nevaluation that shows how the proposed approach performs in practice in\ndifferent applications and domains.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 07:38:18 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Cohen", "Liat", ""], ["Fried", "Dror", ""], ["Weiss", "Gera", ""]]}, {"id": "1805.07642", "submitter": "Cosmina Croitoru Ms", "authors": "Cosmina Croitoru, Kurt Mehlhorn", "title": "On testing substitutability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The papers~\\cite{hatfimmokomi11} and~\\cite{azizbrilharr13} propose algorithms\nfor testing whether the choice function induced by a (strict) preference list\nof length $N$ over a universe $U$ is substitutable. The running time of these\nalgorithms is $O(|U|^3\\cdot N^3)$, respectively $O(|U|^2\\cdot N^3)$. In this\nnote we present an algorithm with running time $O(|U|^2\\cdot N^2)$. Note that\n$N$ may be exponential in the size $|U|$ of the universe.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 19:09:27 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Croitoru", "Cosmina", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1805.07651", "submitter": "Felix Beierle", "authors": "Felix Beierle", "title": "Do You Like What I Like? Similarity Estimation in Proximity-based Mobile\n  Social Networks", "comments": "Accepted for publication at the 17th IEEE International Conference on\n  Trust, Security and Privacy in Computing and Communications (IEEE TrustCom\n  2018)", "journal-ref": null, "doi": "10.1109/TrustCom/BigDataSE.2018.00146", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing social networking services tend to connect people who know\neach other, people show a desire to also connect to yet unknown people in\nphysical proximity. Existing research shows that people tend to connect to\nsimilar people. Utilizing technology in order to stimulate human interaction\nbetween strangers, we consider the scenario of two strangers meeting. On the\nexample of similarity in musical taste, we develop a solution for the problem\nof similarity estimation in proximity-based mobile social networks. We show\nthat a single exchange of a probabilistic data structure between two devices\ncan closely estimate the similarity of two users - without the need to contact\na third-party server.We introduce metrics for fast and space-efficient\napproximation of the Dice coefficient of two multisets - based on the\ncomparison of two Counting Bloom Filters or two Count-Min Sketches. Our\nanalysis shows that utilizing a single hash function minimizes the error when\ncomparing these probabilistic data structures. The size that should be chosen\nfor the data structure depends on the expected average number of unique input\nelements. Using real user data, we show that a Counting Bloom Filter with a\nsingle hash function and a length of 128 is sufficient to accurately estimate\nthe similarity between two multisets representing the musical tastes of two\nusers. Our approach is generalizable for any other similarity estimation of\nfrequencies represented as multisets.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 20:39:52 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 13:41:45 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Beierle", "Felix", ""]]}, {"id": "1805.07742", "submitter": "Jian Li", "authors": "Hao Fu, Jian Li, Pan Xu", "title": "A PTAS for a Class of Stochastic Dynamic Programs", "comments": "accepted in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for obtaining polynomial time approximation schemes\n(PTAS) for a class of stochastic dynamic programs. Using our framework, we\nobtain the first PTAS for the following stochastic combinatorial optimization\nproblems: \\probemax: We are given a set of $n$ items, each item $i\\in [n]$ has\na value $X_i$ which is an independent random variable with a known (discrete)\ndistribution $\\pi_i$. We can {\\em probe} a subset $P\\subseteq [n]$ of items\nsequentially. Each time after {probing} an item $i$, we observe its value\nrealization, which follows the distribution $\\pi_i$. We can {\\em adaptively}\nprobe at most $m$ items and each item can be probed at most once. The reward is\nthe maximum among the $m$ realized values. Our goal is to design an adaptive\nprobing policy such that the expected value of the reward is maximized. To the\nbest of our knowledge, the best known approximation ratio is $1-1/e$, due to\nAsadpour \\etal~\\cite{asadpour2015maximizing}. We also obtain PTAS for some\ngeneralizations and variants of the problem and some other problems.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 09:18:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Fu", "Hao", ""], ["Li", "Jian", ""], ["Xu", "Pan", ""]]}, {"id": "1805.07764", "submitter": "Michal Dory", "authors": "Michal Dory", "title": "Distributed Approximation of Minimum $k$-edge-connected Spanning\n  Subgraphs", "comments": "To appear in PODC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the minimum $k$-edge-connected spanning subgraph ($k$-ECSS) problem the\ngoal is to find the minimum weight subgraph resistant to up to $k-1$ edge\nfailures. This is a central problem in network design, and a natural\ngeneralization of the minimum spanning tree (MST) problem. While the MST\nproblem has been studied extensively by the distributed computing community,\nfor $k \\geq 2$ less is known in the distributed setting.\n  In this paper, we present fast randomized distributed approximation\nalgorithms for $k$-ECSS in the CONGEST model. Our first contribution is an\n$\\widetilde{O}(D + \\sqrt{n})$-round $O(\\log{n})$-approximation for 2-ECSS, for\na graph with $n$ vertices and diameter $D$. The time complexity of our\nalgorithm is almost tight and almost matches the time complexity of the MST\nproblem. For larger constant values of $k$ we give an $\\widetilde{O}(n)$-round\n$O(\\log{n})$-approximation. Additionally, in the special case of unweighted\n3-ECSS we show how to improve the time complexity to $O(D \\log^3{n})$ rounds.\nAll our results significantly improve the time complexity of previous\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 12:59:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Dory", "Michal", ""]]}, {"id": "1805.07798", "submitter": "Simon Du", "authors": "Simon S. Du, Surbhi Goel", "title": "Improved Learning of One-hidden-layer Convolutional Neural Networks with\n  Overlaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to learn a one-hidden-layer convolutional neural\nnetwork where both the convolutional weights and the outputs weights are\nparameters to be learned. Our algorithm works for a general class of\n(potentially overlapping) patches, including commonly used structures for\ncomputer vision tasks. Our algorithm draws ideas from (1) isotonic regression\nfor learning neural networks and (2) landscape analysis of non-convex matrix\nfactorization problems. We believe these findings may inspire further\ndevelopment in designing provable algorithms for learning neural networks and\nother complex models.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 17:07:25 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 23:29:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Du", "Simon S.", ""], ["Goel", "Surbhi", ""]]}, {"id": "1805.07809", "submitter": "Hanna Sumita", "authors": "Yasushi Kawase and Hanna Sumita", "title": "Randomized Strategies for Robust Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the following robust optimization problem. Given an\nindependence system and candidate objective functions, we choose an independent\nset, and then an adversary chooses one objective function, knowing our choice.\nOur goal is to find a randomized strategy (i.e., a probability distribution\nover the independent sets) that maximizes the expected objective value. To\nsolve the problem, we propose two types of schemes for designing approximation\nalgorithms. One scheme is for the case when objective functions are linear. It\nfirst finds an approximately optimal aggregated strategy and then retrieves a\ndesired solution with little loss of the objective value. The approximation\nratio depends on a relaxation of an independence system polytope. As\napplications, we provide approximation algorithms for a knapsack constraint or\na matroid intersection by developing appropriate relaxations and retrievals.\nThe other scheme is based on the multiplicative weights update method. A key\ntechnique is to introduce a new concept called $(\\eta,\\gamma)$-reductions for\nobjective functions with parameters $\\eta, \\gamma$. We show that our scheme\noutputs a nearly $\\alpha$-approximate solution if there exists an\n$\\alpha$-approximation algorithm for a subproblem defined by\n$(\\eta,\\gamma)$-reductions. This improves approximation ratio in previous\nresults. Using our result, we provide approximation algorithms when the\nobjective functions are submodular or correspond to the cardinality robustness\nfor the knapsack problem.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 18:03:40 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Kawase", "Yasushi", ""], ["Sumita", "Hanna", ""]]}, {"id": "1805.07867", "submitter": "Anuj Rawat", "authors": "Anuj Rawat", "title": "A $\\frac{5}{2}$-Approximation Algorithm for Coloring Rooted Subtrees of\n  a Degree $3$ Tree", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rooted tree $\\vec{R}$ is a rooted subtree of a tree $T$ if the tree\nobtained by replacing the directed edges of $\\vec{R}$ by undirected edges is a\nsubtree of $T$. We study the problem of assigning minimum number of colors to a\ngiven set of rooted subtrees $\\mathcal{R}$ of a given tree $T$ such that if any\ntwo rooted subtrees share a directed edge, then they are assigned different\ncolors. The problem is NP hard even in the case when the degree of $T$ is\nrestricted to $3$. We present a $\\frac{5}{2}$-approximation algorithm for this\nproblem. The motivation for studying this problem stems from the problem of\nassigning wavelengths to multicast traffic requests in all-optical WDM tree\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 02:09:46 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Rawat", "Anuj", ""]]}, {"id": "1805.08043", "submitter": "Guy Louchard", "authors": "Matthew Drescher and Guy Louchard and Yvik Swan", "title": "The Adaptive Sampling Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the number $n$ of distinct keys of a large\ncollection of $N$ data is well known in computer science. A classical algorithm\nis the adaptive sampling (AS). $n$ can be estimated by $R.2^D$, where $R$ is\nthe final bucket (cache) size and $D$ is the final depth at the end of the\nprocess. Several new interesting questions can be asked about AS (some of them\nwere suggested by P.Flajolet and popularized by J.Lumbroso). The distribution\nof $W=\\log (R2^D/n)$ is known, we rederive this distribution in a simpler way.\nWe provide new results on the moments of $D$ and $W$. We also analyze the final\ncache size $R$ distribution. We consider colored keys: assume that among the\n$n$ distinct keys, $n_C$ do have color $C$. We show how to estimate\n$p=\\frac{n_C}{n}$. We also study colored keys with some multiplicity given by\nsome distribution function. We want to estimate mean an variance of this\ndistribution. Finally, we consider the case where neither colors nor\nmultiplicities are known. There we want to estimate the related parameters. An\nappendix is devoted to the case where the hashing function provides bits with\nprobability different from $1/2$.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 13:36:30 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 14:11:12 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 07:20:41 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Drescher", "Matthew", ""], ["Louchard", "Guy", ""], ["Swan", "Yvik", ""]]}, {"id": "1805.08124", "submitter": "Matteo Pontecorvi", "authors": "Matteo Pontecorvi and Vijaya Ramachandran", "title": "Distributed Algorithms for Directed Betweenness Centrality and All Pairs\n  Shortest Paths", "comments": "The new algorithms for APSP and BC in Section 4 of this manuscript\n  will appear as the theoretical component in the following paper, which will\n  appear in Proc. ACM PPoPP, February 2019: Loc Hoang, Matteo Pontecorvi,\n  Roshan Dathathri, Gurbinder Gill, Bozhi You, Keshav Pingali, and Vijaya\n  Ramachandran - A round-efficient distributed Betweenness Centrality algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The betweenness centrality (BC) of a node in a network (or graph) is a\nmeasure of its importance in the network. BC is widely used in a large number\nof environments such as social networks, transport networks, security/mobile\nnetworks and more. We present an O(n)-round distributed algorithm for computing\nBC of every vertex as well as all pairs shortest paths (APSP) in a directed\nunweighted network, where n is the number of vertices and m is the number of\nedges. We also present O(n)-round distributed algorithms for computing APSP and\nBC in a weighted directed acyclic graph (dag). Our algorithms are in the\nCongest model and our weighted dag algorithms appear to be the first nontrivial\ndistributed algorithms for both APSP and BC. All our algorithms pay careful\nattention to the constant factors in the number of rounds and number of\nmessages sent, and for unweighted graphs they improve on one or both of these\nmeasures by at least a constant factor over previous results for both directed\nand undirected APSP and BC.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:31:23 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 20:08:23 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Pontecorvi", "Matteo", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1805.08187", "submitter": "Andrew Stolman", "authors": "Akash Kumar, C. Seshadhri, Andrew Stolman", "title": "Finding forbidden minors in sublinear time: a $n^{1/2+o(1)}$-query\n  one-sided tester for minor closed properties on bounded degree graphs", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be an undirected, bounded degree graph with $n$ vertices. Fix a\nfinite graph $H$, and suppose one must remove $\\varepsilon n$ edges from $G$ to\nmake it $H$-minor free (for some small constant $\\varepsilon > 0$). We give an\n$n^{1/2+o(1)}$-time randomized procedure that, with high probability, finds an\n$H$-minor in such a graph. As an application, suppose one must remove\n$\\varepsilon n$ edges from a bounded degree graph $G$ to make it planar. This\nresult implies an algorithm, with the same running time, that produces a\n$K_{3,3}$ or $K_5$ minor in $G$. No prior sublinear time bound was known for\nthis problem.\n  By the graph minor theorem, we get an analogous result for any minor-closed\nproperty. Up to $n^{o(1)}$ factors, this resolves a conjecture of\nBenjamini-Schramm-Shapira (STOC 2008) on the existence of one-sided property\ntesters for minor-closed properties. Furthermore, our algorithm is nearly\noptimal, by an $\\Omega(\\sqrt{n})$ lower bound of Czumaj et al (RSA 2014).\n  Prior to this work, the only graphs $H$ for which non-trivial one-sided\nproperty testers were known for $H$-minor freeness are the following: $H$ being\na forest or a cycle (Czumaj et al, RSA 2014), $K_{2,k}$, $(k\\times 2)$-grid,\nand the $k$-circus (Fichtenberger et al, Arxiv 2017).\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 17:16:33 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 01:50:19 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Kumar", "Akash", ""], ["Seshadhri", "C.", ""], ["Stolman", "Andrew", ""]]}, {"id": "1805.08255", "submitter": "Samuele Giraudo", "authors": "Samuele Giraudo and St\\'ephane Vialette", "title": "Algorithmic and algebraic aspects of unshuffling permutations", "comments": "33 pages. Complete version of the extended abstract arXiv:1601.05962", "journal-ref": "Theoretical Computer Science, Volume 729, 12 June 2018, Pages\n  20-41", "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A permutation is said to be a square if it can be obtained by shuffling two\norder-isomorphic patterns. The definition is intended to be the natural\ncounterpart to the ordinary shuffle of words and languages. In this paper, we\ntackle the problem of recognizing square permutations from both the point of\nview of algebra and algorithms. On the one hand, we present some algebraic and\ncombinatorial properties of the shuffle product of permutations. We follow an\nunusual line consisting in defining the shuffle of permutations by means of an\nunshuffling operator, known as a coproduct. This strategy allows to obtain easy\nproofs for algebraic and combinatorial properties of our shuffle product. We\nbesides exhibit a bijection between square $(213,231)$-avoiding permutations\nand square binary words. On the other hand, by using a pattern avoidance\ncriterion on directed perfect matchings, we prove that recognizing square\npermutations is {\\bf NP}-complete.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 18:51:44 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Giraudo", "Samuele", ""], ["Vialette", "St\u00e9phane", ""]]}, {"id": "1805.08270", "submitter": "Shenwei Huang", "authors": "Serge Gaspers and Shenwei Huang and Dani\\\"el Paulusma", "title": "Colouring Square-Free Graphs without Long Induced Paths", "comments": "An extended abstract of this paper appeared in the proceedings of\n  STACS 2018.\n  http://drops.dagstuhl.de/opus/volltexte/2018/8492/pdf/LIPIcs-STACS-2018-35.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of {\\sc Colouring} is fully understood for $H$-free graphs,\nbut there are still major complexity gaps if two induced subgraphs $H_1$ and\n$H_2$ are forbidden. Let $H_1$ be the $s$-vertex cycle $C_s$ and $H_2$ be the\n$t$-vertex path $P_t$. We show that {\\sc Colouring} is polynomial-time solvable\nfor $s=4$ and $t\\leq 6$, strengthening several known results. Our main approach\nis to initiate a study into the boundedness of the clique-width of atoms\n(graphs with no clique cutset) of a hereditary graph class. We first show that\nthe classifications of boundedness of clique-width of $H$-free graphs and\n$H$-free atoms coincide. We then show that this is not the case if two graphs\nare forbidden: we prove that $(C_4,P_6)$-free atoms have clique-width at\nmost~18. Our key proof ingredients are a divide-and-conquer approach for\nbounding the clique-width of a subclass of $C_4$-free graphs and the\nconstruction of a new bound on the clique-width for (general) graphs in terms\nof the clique-width of recursively defined subgraphs induced by homogeneous\npairs and triples of sets. As a complementary result we prove that {\\sc\nColouring} is \\NP-complete for $s=4$ and $t\\geq 9$, which is the first hardness\nresult on {\\sc Colouring} for $(C_4,P_t)$-free graphs. Combining our new\nresults with known results leads to an almost complete dichotomy for \\cn\nrestricted to $(C_s,P_t)$-free graphs.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 19:42:56 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Gaspers", "Serge", ""], ["Huang", "Shenwei", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1805.08278", "submitter": "Jeff Calder", "authors": "Jeff Calder, Charles K Smart", "title": "The limit shape of convex hull peeling", "comments": null, "journal-ref": "Duke Math. J. 169, no. 11 (2020), 2079-2124", "doi": "10.1215/00127094-2020-0013", "report-no": null, "categories": "math.AP cs.DS cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the convex peeling of a random point set in dimension d\napproximates motion by the 1/(d + 1) power of Gaussian curvature. We use\nviscosity solution theory to interpret the limiting partial differential\nequation. We use the Martingale method to solve the cell problem associated to\nconvex peeling. Our proof follows the program of Armstrong-Cardaliaguet for\nhomogenization of geometric motions, but with completely different ingredients.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:10:31 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 03:02:56 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Calder", "Jeff", ""], ["Smart", "Charles K", ""]]}, {"id": "1805.08321", "submitter": "Tavor Baharav", "authors": "Vivek Bagaria, Tavor Z. Baharav, Govinda M. Kamath, David N. Tse", "title": "Bandit-Based Monte Carlo Optimization for Nearest Neighbors", "comments": "Accepted to the IEEE Journal on Selected Areas in Information Theory\n  (JSAIT) - Special Issue on Sequential, Active, and Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Monte Carlo method estimates an expensive-to-compute quantity\nby random sampling. Bandit-based Monte Carlo optimization is a general\ntechnique for computing the minimum of many such expensive-to-compute\nquantities by adaptive random sampling. The technique converts an optimization\nproblem into a statistical estimation problem which is then solved via\nmulti-armed bandits. We apply this technique to solve the problem of\nhigh-dimensional $k$-nearest neighbors, developing an algorithm which we prove\nis able to identify exact nearest neighbors with high probability. We show that\nunder regularity assumptions on a dataset of $n$ points in $d$-dimensional\nspace, the complexity of our algorithm scales logarithmically with the\ndimension of the data as $O\\left((n+d)\\log^2\n\\left(\\frac{nd}{\\delta}\\right)\\right)$ for error probability $\\delta$, rather\nthan linearly as in exact computation requiring $O(nd)$. We corroborate our\ntheoretical results with numerical simulations, showing that our algorithm\noutperforms both exact computation and state-of-the-art algorithms such as\nkGraph, NGT, and LSH on real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 23:28:30 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 00:26:39 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 23:10:15 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 21:10:05 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bagaria", "Vivek", ""], ["Baharav", "Tavor Z.", ""], ["Kamath", "Govinda M.", ""], ["Tse", "David N.", ""]]}, {"id": "1805.08356", "submitter": "Lydia Zakynthinou", "authors": "Huy L. Nguyen, Lydia Zakynthinou", "title": "Improved Algorithms for Collaborative PAC Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a recent model of collaborative PAC learning where $k$ players with\n$k$ different tasks collaborate to learn a single classifier that works for all\ntasks. Previous work showed that when there is a classifier that has very small\nerror on all tasks, there is a collaborative algorithm that finds a single\nclassifier for all tasks and has $O((\\ln (k))^2)$ times the worst-case sample\ncomplexity for learning a single task. In this work, we design new algorithms\nfor both the realizable and the non-realizable setting, having sample\ncomplexity only $O(\\ln (k))$ times the worst-case sample complexity for\nlearning a single task. The sample complexity upper bounds of our algorithms\nmatch previous lower bounds and in some range of parameters are even better\nthan previous algorithms that are allowed to output different classifiers for\ndifferent tasks.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 02:18:56 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 23:25:45 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Nguyen", "Huy L.", ""], ["Zakynthinou", "Lydia", ""]]}, {"id": "1805.08456", "submitter": "Stefan Mengel", "authors": "Michael Lampis and Stefan Mengel and Valia Mitsou", "title": "QBF as an Alternative to Courcelle's Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose reductions to quantified Boolean formulas (QBF) as a new approach\nto showing fixed-parameter linear algorithms for problems parameterized by\ntreewidth. We demonstrate the feasibility of this approach by giving new\nalgorithms for several well-known problems from artificial intelligence that\nare in general complete for the second level of the polynomial hierarchy. By\nreduction from QBF we show that all resulting algorithms are essentially\noptimal in their dependence on the treewidth. Most of the problems that we\nconsider were already known to be fixed-parameter linear by using Courcelle's\nTheorem or dynamic programming, but we argue that our approach has clear\nadvantages over these techniques: on the one hand, in contrast to Courcelle's\nTheorem, we get concrete and tight guarantees for the runtime dependence on the\ntreewidth. On the other hand, we avoid tedious dynamic programming and, after\nshowing some normalization results for CNF-formulas, our upper bounds often\nboil down to a few lines.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 08:35:41 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Lampis", "Michael", ""], ["Mengel", "Stefan", ""], ["Mitsou", "Valia", ""]]}, {"id": "1805.08539", "submitter": "Lior Kamma", "authors": "Casper Benjamin Freksen, Lior Kamma, Kasper Green Larsen", "title": "Fully Understanding the Hashing Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature hashing, also known as {\\em the hashing trick}, introduced by\nWeinberger et al. (2009), is one of the key techniques used in scaling-up\nmachine learning algorithms. Loosely speaking, feature hashing uses a random\nsparse projection matrix $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ (where $m \\ll n$)\nin order to reduce the dimension of the data from $n$ to $m$ while\napproximately preserving the Euclidean norm. Every column of $A$ contains\nexactly one non-zero entry, equals to either $-1$ or $1$.\n  Weinberger et al. showed tail bounds on $\\|Ax\\|_2^2$. Specifically they\nshowed that for every $\\varepsilon, \\delta$, if $\\|x\\|_{\\infty} / \\|x\\|_2$ is\nsufficiently small, and $m$ is sufficiently large, then $$\\Pr[ \\; |\n\\;\\|Ax\\|_2^2 - \\|x\\|_2^2\\; | < \\varepsilon \\|x\\|_2^2 \\;] \\ge 1 - \\delta \\;.$$\nThese bounds were later extended by Dasgupta \\etal (2010) and most recently\nrefined by Dahlgaard et al. (2017), however, the true nature of the performance\nof this key technique, and specifically the correct tradeoff between the\npivotal parameters $\\|x\\|_{\\infty} / \\|x\\|_2, m, \\varepsilon, \\delta$ remained\nan open question.\n  We settle this question by giving tight asymptotic bounds on the exact\ntradeoff between the central parameters, thus providing a complete\nunderstanding of the performance of feature hashing. We complement the\nasymptotic bound with empirical data, which shows that the constants \"hiding\"\nin the asymptotic notation are, in fact, very close to $1$, thus further\nillustrating the tightness of the presented bounds in practice.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:23:47 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Freksen", "Casper Benjamin", ""], ["Kamma", "Lior", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1805.08554", "submitter": "Holger Dell", "authors": "Amir Abboud, Karl Bringmann, Holger Dell, Jesper Nederlof", "title": "More Consequences of Falsifying SETH and the Orthogonal Vectors\n  Conjecture", "comments": "To appear in the proceedings of STOC'18", "journal-ref": null, "doi": "10.1145/3188745.3188938", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Strong Exponential Time Hypothesis and the OV-conjecture are two popular\nhardness assumptions used to prove a plethora of lower bounds, especially in\nthe realm of polynomial-time algorithms. The OV-conjecture in moderate\ndimension states there is no $\\epsilon>0$ for which an\n$O(N^{2-\\epsilon})\\mathrm{poly}(D)$ time algorithm can decide whether there is\na pair of orthogonal vectors in a given set of size $N$ that contains\n$D$-dimensional binary vectors.\n  We strengthen the evidence for these hardness assumptions. In particular, we\nshow that if the OV-conjecture fails, then two problems for which we are far\nfrom obtaining even tiny improvements over exhaustive search would have\nsurprisingly fast algorithms. If the OV conjecture is false, then there is a\nfixed $\\epsilon>0$ such that:\n  (1) For all $d$ and all large enough $k$, there is a randomized algorithm\nthat takes $O(n^{(1-\\epsilon)k})$ time to solve the Zero-Weight-$k$-Clique and\nMin-Weight-$k$-Clique problems on $d$-hypergraphs with $n$ vertices. As a\nconsequence, the OV-conjecture is implied by the Weighted Clique conjecture.\n  (2) For all $c$, the satisfiability of sparse TC1 circuits on $n$ inputs\n(that is, circuits with $cn$ wires, depth $c\\log n$, and negation, AND, OR, and\nthreshold gates) can be computed in time ${O((2-\\epsilon)^n)}$.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:53:08 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Abboud", "Amir", ""], ["Bringmann", "Karl", ""], ["Dell", "Holger", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1805.08571", "submitter": "Chris Schwiegelshohn", "authors": "Alexander Munteanu and Chris Schwiegelshohn and Christian Sohler and\n  David P. Woodruff", "title": "On Coresets for Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are one of the central methods to facilitate the analysis of large\ndata sets. We continue a recent line of research applying the theory of\ncoresets to logistic regression. First, we show a negative result, namely, that\nno strongly sublinear sized coresets exist for logistic regression. To deal\nwith intractable worst-case instances we introduce a complexity measure\n$\\mu(X)$, which quantifies the hardness of compressing a data set for logistic\nregression. $\\mu(X)$ has an intuitive statistical interpretation that may be of\nindependent interest. For data sets with bounded $\\mu(X)$-complexity, we show\nthat a novel sensitivity sampling scheme produces the first provably sublinear\n$(1\\pm\\varepsilon)$-coreset. We illustrate the performance of our method by\ncomparing to uniform sampling as well as to state of the art methods in the\narea. The experiments are conducted on real world benchmark data for logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 13:33:22 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 12:30:00 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 16:25:50 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Munteanu", "Alexander", ""], ["Schwiegelshohn", "Chris", ""], ["Sohler", "Christian", ""], ["Woodruff", "David P.", ""]]}, {"id": "1805.08602", "submitter": "Yakov Nekrich", "authors": "Timothy M. Chan, Yakov Nekrich, Saladi Rahul, Konstantinos Tsakalidis", "title": "Orthogonal Point Location and Rectangle Stabbing Queries in 3-d", "comments": "Full version of the ICALP'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a collection of new results on two fundamental\nproblems in geometric data structures: orthogonal point location and rectangle\nstabbing.\n  -We give the first linear-space data structure that supports 3-d point\nlocation queries on $n$ disjoint axis-aligned boxes with optimal $O\\left( \\log\nn\\right)$ query time in the (arithmetic) pointer machine model. This improves\nthe previous $O\\left( \\log^{3/2} n \\right)$ bound of Rahul [SODA 2015]. We\nsimilarly obtain the first linear-space data structure in the I/O model with\noptimal query cost, and also the first linear-space data structure in the word\nRAM model with sub-logarithmic query time.\n  -We give the first linear-space data structure that supports 3-d $4$-sided\nand $5$-sided rectangle stabbing queries in optimal $O(\\log_wn+k)$ time in the\nword RAM model. We similarly obtain the first optimal data structure for the\nclosely related problem of 2-d top-$k$ rectangle stabbing in the word RAM\nmodel, and also improved results for 3-d 6-sided rectangle stabbing.\n  For point location, our solution is simpler than previous methods, and is\nbased on an interesting variant of the van Emde Boas recursion, applied in a\nround-robin fashion over the dimensions, combined with bit-packing techniques.\nFor rectangle stabbing, our solution is a variant of Alstrup, Brodal, and\nRauhe's grid-based recursive technique (FOCS 2000), combined with a number of\nnew ideas.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 14:23:51 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Chan", "Timothy M.", ""], ["Nekrich", "Yakov", ""], ["Rahul", "Saladi", ""], ["Tsakalidis", "Konstantinos", ""]]}, {"id": "1805.08612", "submitter": "Vincent Jug\\'e", "authors": "Nicolas Auger, Vincent Jug\\'e, Cyril Nicaud, Carine Pivoteau", "title": "On the Worst-Case Complexity of TimSort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TimSort is an intriguing sorting algorithm designed in 2002 for Python, whose\nworst-case complexity was announced, but not proved until our recent preprint.\nIn fact, there are two slightly different versions of TimSort that are\ncurrently implemented in Python and in Java respectively. We propose a\npedagogical and insightful proof that the Python version runs in\n$\\mathcal{O}(n\\log n)$. The approach we use in the analysis also applies to the\nJava version, although not without very involved technical details. As a\nbyproduct of our study, we uncover a bug in the Java implementation that can\ncause the sorting method to fail during the execution. We also give a proof\nthat Python's TimSort running time is in $\\mathcal{O}(n + n\\log \\rho)$, where\n$\\rho$ is the number of runs (i.e. maximal monotonic sequences), which is quite\na natural parameter here and part of the explanation for the good behavior of\nTimSort on partially sorted inputs.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:27:38 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 10:02:31 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2019 21:03:01 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Auger", "Nicolas", ""], ["Jug\u00e9", "Vincent", ""], ["Nicaud", "Cyril", ""], ["Pivoteau", "Carine", ""]]}, {"id": "1805.08816", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Wojciech Bieniecki", "title": "copMEM: Finding maximal exact matches via sampling both genomes", "comments": "The source code of copMEM is freely available at\n  https://github.com/wbieniec/copmem. Contact: wbieniec@kis.p.lodz.pl,\n  wbieniec@kis.p.lodz.pl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-to-genome comparisons require designating anchor points, which are\ngiven by Maximum Exact Matches (MEMs) between their sequences. For large\ngenomes this is a challenging problem and the performance of existing\nsolutions, even in parallel regimes, is not quite satisfactory. We present a\nnew algorithm, copMEM, that allows to sparsely sample both input genomes, with\nsampling steps being coprime. Despite being a single-threaded implementation,\ncopMEM computes all MEMs of minimum length 100 between the human and mouse\ngenomes in less than 2 minutes, using less than 10 GB of RAM memory.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 18:58:54 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Grabowski", "Szymon", ""], ["Bieniecki", "Wojciech", ""]]}, {"id": "1805.08953", "submitter": "Nitesh Jha", "authors": "Sourav Chakraborty, Shamik Ghosh, Nitesh Jha, Sasanka Roy", "title": "Maximal and maximum transitive relation contained in a given binary\n  relation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a \\textit{maximal} transitive relation\ncontained in a given binary relation. Given a binary relation of size $m$\ndefined on a set of size $n$, we present a polynomial time algorithm that finds\na maximal transitive sub-relation in time $O(n^2 + nm)$.\n  We also study the problem of finding a \\textit{maximum} transitive relation\ncontained in a binary relation. This is the problem of computing a maximum\ntransitive subgraph in a given digraph. For the class of directed graphs with\nthe underlying graph being triangle-free, we present a $0.874$-approximation\nalgorithm. This is achieved via a simple connection to the problem of maximum\ndirected cut. Further, we give an upper bound for the size of any maximum\ntransitive relation to be $m/4 + cm^{4/5}$, where $c > 0$ and $m$ is the number\nof edges in the digraph.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 03:56:38 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Chakraborty", "Sourav", ""], ["Ghosh", "Shamik", ""], ["Jha", "Nitesh", ""], ["Roy", "Sasanka", ""]]}, {"id": "1805.09217", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Qin Zhang, Yuan Zhou", "title": "Tight Bounds for Collaborative PAC Learning via Multiplicative Weights", "comments": "Accepted to NIPS 2018. 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the collaborative PAC learning problem recently proposed in Blum et\nal.~\\cite{BHPQ17}, in which we have $k$ players and they want to learn a target\nfunction collaboratively, such that the learned function approximates the\ntarget function well on all players' distributions simultaneously. The quality\nof the collaborative learning algorithm is measured by the ratio between the\nsample complexity of the algorithm and that of the learning algorithm for a\nsingle distribution (called the overhead). We obtain a collaborative learning\nalgorithm with overhead $O(\\ln k)$, improving the one with overhead $O(\\ln^2\nk)$ in \\cite{BHPQ17}. We also show that an $\\Omega(\\ln k)$ overhead is\ninevitable when $k$ is polynomial bounded by the VC dimension of the hypothesis\nclass. Finally, our experimental study has demonstrated the superiority of our\nalgorithm compared with the one in Blum et al. on real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 15:18:01 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 19:58:29 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Chen", "Jiecao", ""], ["Zhang", "Qin", ""], ["Zhou", "Yuan", ""]]}, {"id": "1805.09261", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien and Christophe Guyeux", "title": "Online shortest paths with confidence intervals for routing in a time\n  varying random network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in the world's population and rising standards of living is\nleading to an ever-increasing number of vehicles on the roads, and with it\never-increasing difficulties in traffic management. This traffic management in\ntransport networks can be clearly optimized by using information and\ncommunication technologies referred as Intelligent Transport Systems (ITS).\nThis management problem is usually reformulated as finding the shortest path in\na time varying random graph. In this article, an online shortest path\ncomputation using stochastic gradient descent is proposed. This routing\nalgorithm for ITS traffic management is based on the online Frank-Wolfe\napproach. Our improvement enables to find a confidence interval for the\nshortest path, by using the stochastic gradient algorithm for approximate\nBayesian inference.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 08:32:32 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Guyeux", "Christophe", ""]]}, {"id": "1805.09423", "submitter": "Mart\\'in Farach-Colton", "authors": "Alex Conway, Martin Farach-Colton, Philip Shilane", "title": "Optimal Hashing in External Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Hash tables are a ubiquitous class of dictionary data structures. However,\nstandard hash table implementations do not translate well into the external\nmemory model, because they do not incorporate locality for insertions.\n  Iacono and Patracsu established an update/query tradeoff curve for external\nhash tables: a hash table that performs insertions in $O(\\lambda/B)$ amortized\nIOs requires $\\Omega(\\log_\\lambda N)$ expected IOs for queries, where $N$ is\nthe number of items that can be stored in the data structure, $B$ is the size\nof a memory transfer, $M$ is the size of memory, and $\\lambda$ is a tuning\nparameter.\n  They provide a hashing data structure that meets this curve for $\\lambda$\nthat is $\\Omega(\\log\\log M + \\log_M N)$. Their data structure, which we call an\n\\defn{IP hash table}, is complicated and, to the best of our knowledge, has not\nbeen implemented.\n  In this paper, we present a new and much simpler optimal external memory hash\ntable, the \\defn{Bundle of Arrays Hash Table} (BOA). BOAs are based on\nsize-tiered LSMs, a well-studied data structure, and are almost as easy to\nimplement. The BOA is optimal for a narrower range of $\\lambda$. However, the\nsimplicity of BOAs allows them to be readily modified to achieve the following\nresults:\n  \\begin{itemize}\n  \\item A new external memory data structure, the \\defn{Bundle of Trees Hash\nTable} (BOT), that matches the performance of the IP hash table, while\nretaining some of the simplicity of the BOAs.\n  \\item The \\defn{cache-oblivious Bundle of Trees Hash Table} (COBOT), the\nfirst cache-oblivious hash table. This data structure matches the optimality of\nBOTs and IP hash tables over the same range of $\\lambda$. \\end{itemize}\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 21:00:47 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Conway", "Alex", ""], ["Farach-Colton", "Martin", ""], ["Shilane", "Philip", ""]]}, {"id": "1805.09442", "submitter": "Peng Zhang", "authors": "Rasmus Kyng, Richard Peng, Robert Schwieterman, Peng Zhang", "title": "Incomplete Nested Dissection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an asymptotically faster algorithm for solving linear systems in\nwell-structured 3-dimensional truss stiffness matrices. These linear systems\narise from linear elasticity problems, and can be viewed as extensions of graph\nLaplacians into higher dimensions. Faster solvers for the 2-D variants of such\nsystems have been studied using generalizations of tools for solving graph\nLaplacians [Daitch-Spielman CSC'07, Shklarski-Toledo SIMAX'08].\n  Given a 3-dimensional truss over $n$ vertices which is formed from a union of\n$k$ convex structures (tetrahedral meshes) with bounded aspect ratios, whose\nindividual tetrahedrons are also in some sense well-conditioned, our algorithm\nsolves a linear system in the associated stiffness matrix up to accuracy\n$\\epsilon$ in time $O(k^{1/3} n^{5/3} \\log (1 / \\epsilon))$. This\nasymptotically improves the running time $O(n^2)$ by Nested Dissection for all\n$k \\ll n$.\n  We also give a result that improves on Nested Dissection even when we allow\nany aspect ratio for each of the $k$ convex structures (but we still require\nwell-conditioned individual tetrahedrons). In this regime, we improve on Nested\nDissection for $k \\ll n^{1/44}$.\n  The key idea of our algorithm is to combine nested dissection and support\ntheory. Both of these techniques for solving linear systems are well studied,\nbut usually separately. Our algorithm decomposes a 3-dimensional truss into\nseparate and balanced regions with small boundaries. We then bound the spectrum\nof each such region separately, and utilize such bounds to obtain improved\nalgorithms by preconditioning with partial states of separator-based Gaussian\nelimination.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 22:03:00 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Kyng", "Rasmus", ""], ["Peng", "Richard", ""], ["Schwieterman", "Robert", ""], ["Zhang", "Peng", ""]]}, {"id": "1805.09476", "submitter": "Rad Niazadeh", "authors": "Vaggos Chatziafratis, Rad Niazadeh, Moses Charikar", "title": "Hierarchical Clustering with Structural Constraints", "comments": "In Proc. 35th International Conference on Machine Learning (ICML\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a popular unsupervised data analysis method. For\nmany real-world applications, we would like to exploit prior information about\nthe data that imposes constraints on the clustering hierarchy, and is not\ncaptured by the set of features available to the algorithm. This gives rise to\nthe problem of \"hierarchical clustering with structural constraints\".\nStructural constraints pose major challenges for bottom-up approaches like\naverage/single linkage and even though they can be naturally incorporated into\ntop-down divisive algorithms, no formal guarantees exist on the quality of\ntheir output. In this paper, we provide provable approximation guarantees for\ntwo simple top-down algorithms, using a recently introduced optimization\nviewpoint of hierarchical clustering with pairwise similarity information\n[Dasgupta, 2016]. We show how to find good solutions even in the presence of\nconflicting prior information, by formulating a constraint-based regularization\nof the objective. We further explore a variation of this objective for\ndissimilarity information [Cohen-Addad et al., 2018] and improve upon current\ntechniques. Finally, we demonstrate our approach on a real dataset for the\ntaxonomy application.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 01:32:19 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 16:12:51 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Chatziafratis", "Vaggos", ""], ["Niazadeh", "Rad", ""], ["Charikar", "Moses", ""]]}, {"id": "1805.09480", "submitter": "Rad Niazadeh", "authors": "Rad Niazadeh, Tim Roughgarden, Joshua R. Wang", "title": "Optimal Algorithms for Continuous Non-monotone Submodular and\n  DR-Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the fundamental problems of maximizing a continuous\nnon-monotone submodular function over the hypercube, both with and without\ncoordinate-wise concavity. This family of optimization problems has several\napplications in machine learning, economics, and communication systems. Our\nmain result is the first $\\frac{1}{2}$-approximation algorithm for continuous\nsubmodular function maximization; this approximation factor of $\\frac{1}{2}$ is\nthe best possible for algorithms that only query the objective function at\npolynomially many points. For the special case of DR-submodular maximization,\ni.e. when the submodular functions is also coordinate wise concave along all\ncoordinates, we provide a different $\\frac{1}{2}$-approximation algorithm that\nruns in quasilinear time. Both of these results improve upon prior work [Bian\net al, 2017, Soma and Yoshida, 2017].\n  Our first algorithm uses novel ideas such as reducing the guaranteed\napproximation problem to analyzing a zero-sum game for each coordinate, and\nincorporates the geometry of this zero-sum game to fix the value at this\ncoordinate. Our second algorithm exploits coordinate-wise concavity to identify\na monotone equilibrium condition sufficient for getting the required\napproximation guarantee, and hunts for the equilibrium point using binary\nsearch. We further run experiments to verify the performance of our proposed\nalgorithms in related machine learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 02:08:14 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Niazadeh", "Rad", ""], ["Roughgarden", "Tim", ""], ["Wang", "Joshua R.", ""]]}, {"id": "1805.09495", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Erfan Sadeqi Azer, Qin Zhang", "title": "A Practical Algorithm for Distributed Clustering and Outlier Detection", "comments": "Accepted to NIPS 2018. 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic $k$-means/median clustering, which are fundamental\nproblems in unsupervised learning, in the setting where data are partitioned\nacross multiple sites, and where we are allowed to discard a small portion of\nthe data by labeling them as outliers. We propose a simple approach based on\nconstructing small summary for the original dataset. The proposed method is\ntime and communication efficient, has good approximation guarantees, and can\nidentify the global outliers effectively. To the best of our knowledge, this is\nthe first practical algorithm with theoretical guarantees for distributed\nclustering with outliers. Our experiments on both real and synthetic data have\ndemonstrated the clear superiority of our algorithm against all the baseline\nalgorithms in almost all metrics.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 03:00:55 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 00:56:10 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Chen", "Jiecao", ""], ["Azer", "Erfan Sadeqi", ""], ["Zhang", "Qin", ""]]}, {"id": "1805.09602", "submitter": "Anupam Gupta", "authors": "Anupam Gupta and Amit Kumar and Jason Li", "title": "Non-Preemptive Flow-Time Minimization via Rejections", "comments": "To appear in the ICALP 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online problem of minimizing weighted flow-time on unrelated\nmachines. Although much is known about this problem in the\nresource-augmentation setting, these results assume that jobs can be preempted.\nWe give the first constant-competitive algorithm for the non-preemptive setting\nin the rejection model. In this rejection model, we are allowed to reject an\n$\\varepsilon$-fraction of the total weight of jobs, and compare the resulting\nflow-time to that of the offline optimum which is required to schedule all\njobs. This is arguably the weakest assumption in which such a result is known\nfor weighted flow-time on unrelated machines. While our algorithms are simple,\nwe need a delicate dual-fitting argument to bound the flow-time while only a\nsmall fraction of elements are rejected.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 11:04:09 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Gupta", "Anupam", ""], ["Kumar", "Amit", ""], ["Li", "Jason", ""]]}, {"id": "1805.09675", "submitter": "Jeremy Kepner", "authors": "Siddharth Samsi, Vijay Gadepally, Michael Hurley, Michael Jones,\n  Edward Kao, Sanjeev Mohindra, Paul Monticciolo, Albert Reuther, Steven Smith,\n  William Song, Diane Staheli, Jeremy Kepner", "title": "GraphChallenge.org: Raising the Bar on Graph Analytic Performance", "comments": "7 pages, 6 figures; submitted to IEEE HPEC Graph Challenge. arXiv\n  admin note: text overlap with arXiv:1708.06866", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547527", "report-no": null, "categories": "cs.DC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of graph analytic systems has created a need for new ways to measure\nand compare the capabilities of graph processing systems. The MIT/Amazon/IEEE\nGraph Challenge has been developed to provide a well-defined community venue\nfor stimulating research and highlighting innovations in graph analysis\nsoftware, hardware, algorithms, and systems. GraphChallenge.org provides a wide\nrange of pre-parsed graph data sets, graph generators, mathematically defined\ngraph algorithms, example serial implementations in a variety of languages, and\nspecific metrics for measuring performance. Graph Challenge 2017 received 22\nsubmissions by 111 authors from 36 organizations. The submissions highlighted\ngraph analytic innovations in hardware, software, algorithms, systems, and\nvisualization. These submissions produced many comparable performance\nmeasurements that can be used for assessing the current state of the art of the\nfield. There were numerous submissions that implemented the triangle counting\nchallenge and resulted in over 350 distinct measurements. Analysis of these\nsubmissions show that their execution time is a strong function of the number\nof edges in the graph, $N_e$, and is typically proportional to $N_e^{4/3}$ for\nlarge values of $N_e$. Combining the model fits of the submissions presents a\npicture of the current state of the art of graph analysis, which is typically\n$10^8$ edges processed per second for graphs with $10^8$ edges. These results\nare $30$ times faster than serial implementations commonly used by many graph\nanalysts and underscore the importance of making these performance benefits\navailable to the broader community. Graph Challenge provides a clear picture of\ncurrent graph analysis systems and underscores the need for new innovations to\nachieve high performance on very large graphs.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:18:37 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Samsi", "Siddharth", ""], ["Gadepally", "Vijay", ""], ["Hurley", "Michael", ""], ["Jones", "Michael", ""], ["Kao", "Edward", ""], ["Mohindra", "Sanjeev", ""], ["Monticciolo", "Paul", ""], ["Reuther", "Albert", ""], ["Smith", "Steven", ""], ["Song", "William", ""], ["Staheli", "Diane", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1805.09697", "submitter": "Arnab Bhattacharyya", "authors": "Jayadev Acharya, Arnab Bhattacharyya, Constantinos Daskalakis,\n  Saravanan Kandasamy", "title": "Learning and Testing Causal Models with Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing and learning problems on causal Bayesian networks as\ndefined by Pearl (Pearl, 2009). Given a causal Bayesian network $\\mathcal{M}$\non a graph with $n$ discrete variables and bounded in-degree and bounded\n`confounded components', we show that $O(\\log n)$ interventions on an unknown\ncausal Bayesian network $\\mathcal{X}$ on the same graph, and\n$\\tilde{O}(n/\\epsilon^2)$ samples per intervention, suffice to efficiently\ndistinguish whether $\\mathcal{X}=\\mathcal{M}$ or whether there exists some\nintervention under which $\\mathcal{X}$ and $\\mathcal{M}$ are farther than\n$\\epsilon$ in total variation distance. We also obtain sample/time/intervention\nefficient algorithms for: (i) testing the identity of two unknown causal\nBayesian networks on the same graph; and (ii) learning a causal Bayesian\nnetwork on a given graph. Although our algorithms are non-adaptive, we show\nthat adaptivity does not help in general: $\\Omega(\\log n)$ interventions are\nnecessary for testing the identity of two unknown causal Bayesian networks on\nthe same graph, even adaptively. Our algorithms are enabled by a new\nsubadditivity inequality for the squared Hellinger distance between two causal\nBayesian networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:31:09 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Acharya", "Jayadev", ""], ["Bhattacharyya", "Arnab", ""], ["Daskalakis", "Constantinos", ""], ["Kandasamy", "Saravanan", ""]]}, {"id": "1805.09747", "submitter": "Rakesh Venkat", "authors": "Anand Louis, Rakesh Venkat", "title": "Semi-Random Graphs with Planted Sparse Vertex Cuts: Algorithms for Exact\n  and Approximate Recovery", "comments": "Full version of paper to appear in ICALP '18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing the vertex expansion of a graph is an NP-hard\nproblem. The current best worst-case approximation guarantees for computing the\nvertex expansion of a graph are a $O(\\sqrt{\\log n})$-approximation algorithm\ndue to Feige, Hajiaghayi and Lee [SIAM J. Comp., 2008], and $O(\\sqrt{OPT \\log\nd})$ bound in graphs having vertex degrees at most $d$, due to Louis,\nRaghavendra and Vempala [FOCS 2013].\n  We study a natural semi-random model of graphs with sparse vertex cuts. For\ncertain ranges of parameters, we give an algorithm to recover the planted\nsparse vertex cut exactly. For a larger range of parameters, we give a constant\nfactor bi-criteria approximation algorithm to compute the graph's balanced\nvertex expansion. Our algorithms are based on studying a semidefinite\nprogramming relaxation for the balanced vertex expansion of the graph.\n  In addition to being a family of instances that will help us to better\nunderstand the complexity of the computation of vertex expansion, our model can\nalso be used in the study of community detection where only a few nodes from\neach community interact with nodes from other communities. There has been a lot\nof work on studying random and semi-random graphs with planted sparse edge\ncuts. To the best of our knowledge, our model of semi-random graphs with\nplanted sparse vertex cuts has not been studied before.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 15:56:10 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Louis", "Anand", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1805.09887", "submitter": "Sofiat Olaosebikan", "authors": "Sofiat Olaosebikan and David Manlove", "title": "Super-stability in the Student-Project Allocation Problem with Ties", "comments": "28 pages (including Appendix), 6 figures, 2 tables. A preliminary\n  version of a part of this paper appeared in Proceedings of International\n  Conference on Combinatorial Optimisation and Applications (COCOA) 2018. This\n  paper has been accepted for publication in a special issue of Journal of\n  Combinatorial Optimisation featuring selected papers from COCOA 2018", "journal-ref": null, "doi": "10.1007/978-3-030-04651-4_24", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Student-Project Allocation problem with lecturer preferences over\nStudents (SPA-S) involves assigning students to projects based on student\npreferences over projects, lecturer preferences over students, and the maximum\nnumber of students that each project and lecturer can accommodate. This\nclassical model assumes that each project is offered by one lecturer and that\npreference lists are strictly ordered. Here, we study a generalisation of SPA-S\nwhere ties are allowed in the preference lists of students and lecturers, which\nwe refer to as the Student-Project Allocation problem with lecturer preferences\nover Students with Ties (SPA-ST). We investigate stable matchings under the\nmost robust definition of stability in this context, namely super-stability. We\ndescribe the first polynomial-time algorithm to find a super-stable matching or\nto report that no such matching exists, given an instance of SPA-ST. Our\nalgorithm runs in $O(L)$ time, where $L$ is the total length of all the\npreference lists. Finally, we present results obtained from an empirical\nevaluation of the linear-time algorithm based on randomly-generated SPA-ST\ninstances. Our main finding is that, whilst super-stable matchings can be\nelusive when ties are present in the students' and lecturers' preference lists,\nthe probability of such a matching existing is significantly higher if ties are\nrestricted to the lecturers' preference lists.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 20:19:06 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 16:23:38 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 10:15:50 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 15:30:28 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Olaosebikan", "Sofiat", ""], ["Manlove", "David", ""]]}, {"id": "1805.09924", "submitter": "Ritu Kundu", "authors": "Tomasz Kociumaka, Ritu Kundu, Manal Mohamed, and Solon P. Pissis", "title": "Longest Unbordered Factor in Quasilinear Time", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A border u of a word w is a proper factor of w occurring both as a prefix and\nas a suffix. The maximal unbordered factor of w is the longest factor of w\nwhich does not have a border. Here an O(n log n)-time with high probability (or\nO(n log n log^2 log n)-time deterministic) algorithm to compute the Longest\nUnbordered Factor Array of w for general alphabets is presented, where n is the\nlength of w. This array specifies the length of the maximal unbordered factor\nstarting at each position of w. This is a major improvement on the running time\nof the currently best worst-case algorithm working in O(n^{1.5} ) time for\ninteger alphabets [Gawrychowski et al., 2015].\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 22:14:27 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 07:39:26 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Kundu", "Ritu", ""], ["Mohamed", "Manal", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1805.09956", "submitter": "Rachit Nimavat", "authors": "Julia Chuzhoy, David H. K. Kim and Rachit Nimavat", "title": "Improved Approximation for Node-Disjoint Paths in Grids with Sources on\n  the Boundary", "comments": "To appear in the proceedings of ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical Node-Disjoint Paths (NDP) problem: given an undirected\n$n$-vertex graph G, together with a set {(s_1,t_1),...,(s_k,t_k)} of pairs of\nits vertices, called source-destination, or demand pairs, find a\nmaximum-cardinality set of mutually node-disjoint paths that connect the demand\npairs. The best current approximation for the problem is achieved by a simple\ngreedy $O(\\sqrt{n})$-approximation algorithm.\n  A special case of the problem called NDP-Grid, where the underlying graph is\na grid, has been studied extensively. The best current approximation algorithm\nfor NDP-Grid achieves an $\\tilde{O}(n^{1/4})$-approximation factor. On the\nnegative side, a recent result by the authors shows that NDP is hard to\napproximate to within factor $2^{\\Omega(\\sqrt{\\log n})}$, even if the\nunderlying graph is a sub-graph of a grid, and all source vertices lie on the\ngrid boundary. In a follow-up work, the authors further show that NDP-Grid is\nhard to approximate to within factor $\\Omega(2^{\\log^{1-\\epsilon}n})$ for any\nconstant $\\epsilon$ under standard complexity assumptions, and to within factor\n$n^{\\Omega(1/(\\log\\log n)^2)}$ under randomized ETH.\n  In this paper we study NDP-Grid, where all source vertices {s_1,...,s_k}\nappear on the grid boundary. Our main result is an efficient randomized\n$2^{O(\\sqrt{\\log n} \\cdot \\log\\log n)}$-approximation algorithm for this\nproblem. We generalize this result to instances where the source vertices lie\nwithin a prescribed distance from the grid boundary.\n  Much of the work on approximation algorithms for NDP relies on the\nmulticommodity flow relaxation of the problem, which is known to have an\n$\\Omega(\\sqrt n)$ integrality gap, even in grid graphs. Our work departs from\nthis paradigm, and uses a (completely different) linear program only to select\nthe pairs to be routed, while the routing itself is computed by other methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 03:01:00 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Kim", "David H. K.", ""], ["Nimavat", "Rachit", ""]]}, {"id": "1805.10042", "submitter": "Gabriele Fici", "authors": "Golnaz Badkobeh, Gabriele Fici, Simon J. Puglisi", "title": "Algorithms for Anti-Powers in Strings", "comments": null, "journal-ref": "Published in Informnation Processing Letters Volume 137, September\n  2018, Pages 57-60", "doi": "10.1016/j.ipl.2018.05.003", "report-no": null, "categories": "cs.DS cs.DM cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A string $S[1,n]$ is a power (or tandem repeat) of order $k$ and period $n/k$\nif it can decomposed into $k$ consecutive equal-length blocks of letters.\nPowers and periods are fundamental to string processing, and algorithms for\ntheir efficient computation have wide application and are heavily studied.\nRecently, Fici et al. (Proc. ICALP 2016) defined an {\\em anti-power} of order\n$k$ to be a string composed of $k$ pairwise-distinct blocks of the same length\n($n/k$, called {\\em anti-period}). Anti-powers are a natural converse to\npowers, and are objects of combinatorial interest in their own right. In this\npaper we initiate the algorithmic study of anti-powers. Given a string $S$, we\ndescribe an optimal algorithm for locating all substrings of $S$ that are\nanti-powers of a specified order. The optimality of the algorithm follows form\na combinatorial lemma that provides a lower bound on the number of distinct\nanti-powers of a given order: we prove that a string of length $n$ can contain\n$\\Theta(n^2/k)$ distinct anti-powers of order $k$.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:54:26 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Badkobeh", "Golnaz", ""], ["Fici", "Gabriele", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1805.10070", "submitter": "Bastien Cazaux", "authors": "Bastien Cazaux and Eric Rivals", "title": "Strong link between BWT and XBW via Aho-Corasick automaton and\n  applications to Run-Length Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boom of genomic sequencing makes compression of set of sequences\ninescapable. This underlies the need for multi-string indexing data structures\nthat helps compressing the data. The most prominent example of such data\nstructures is the Burrows-Wheeler Transform (BWT), a reversible permutation of\na text that improves its compressibility. A similar data structure, the\neXtended Burrows-Wheeler Transform (XBW), is able to index a tree labelled with\nalphabet symbols. A link between a multi-string BWT and the Aho-Corasick\nautomaton has already been found and led to a way to build a XBW from a\nmulti-string BWT. We exhibit a stronger link between a multi-string BWT and a\nXBW by using the order of the concatenation in the multi-string. This bijective\nlink has several applications: first, it allows to build one data structure\nfrom the other; second, it enables one to compute an ordering of the input\nstrings that optimises a Run-Length measure (i.e., the compressibility) of the\nBWT or of the XBW.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 10:24:12 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Cazaux", "Bastien", ""], ["Rivals", "Eric", ""]]}, {"id": "1805.10121", "submitter": "Edoardo di Napoli", "authors": "Jan Winkelmann (1) and Paul Springer (1) and Edoardo Di Napoli (1 and\n  2) ((1) AICES, RWTH Aachen University, (2) JSC, Forschungszentrum J\\\"ulich)", "title": "ChASE: Chebyshev Accelerated Subspace iteration Eigensolver for\n  sequences of Hermitian eigenvalue problems", "comments": "33 pages. Submitted to ACM TOMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving dense Hermitian eigenproblems arranged in a sequence with direct\nsolvers fails to take advantage of those spectral properties which are\npertinent to the entire sequence, and not just to the single problem. When such\nfeatures take the form of correlations between the eigenvectors of consecutive\nproblems, as is the case in many real-world applications, the potential benefit\nof exploiting them can be substantial. We present ChASE, a modern algorithm and\nlibrary based on subspace iteration with polynomial acceleration. Novel to\nChASE is the computation of the spectral estimates that enter in the filter and\nan optimization of the polynomial degree which further reduces the necessary\nFLOPs. ChASE is written in C++ using the modern software engineering concepts\nwhich favor a simple integration in application codes and a straightforward\nportability over heterogeneous platforms. When solving sequences of Hermitian\neigenproblems for a portion of their extremal spectrum, ChASE greatly benefits\nfrom the sequence's spectral properties and outperforms direct solvers in many\nscenarios. The library ships with two distinct parallelization schemes,\nsupports execution over distributed GPUs, and it is easily extensible to other\nparallel computing architectures.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:56:18 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Winkelmann", "Jan", "", "AICES, RWTH Aachen University"], ["Springer", "Paul", "", "AICES, RWTH Aachen University"], ["Di Napoli", "Edoardo", "", "1 and\n  2"]]}, {"id": "1805.10169", "submitter": "Anna Melnichenko", "authors": "Timo K\\\"otzing, J.A.Gregor Lagodzinski, Johannes Lengler, Anna\n  Melnichenko", "title": "Destructiveness of Lexicographic Parsimony Pressure and Alleviation by a\n  Concatenation Crossover in Genetic Programming", "comments": "to appear in PPSN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For theoretical analyses there are two specifics distinguishing GP from many\nother areas of evolutionary computation. First, the variable size\nrepresentations, in particular yielding a possible bloat (i.e. the growth of\nindividuals with redundant parts). Second, the role and realization of\ncrossover, which is particularly central in GP due to the tree-based\nrepresentation. Whereas some theoretical work on GP has studied the effects of\nbloat, crossover had a surprisingly little share in this work. We analyze a\nsimple crossover operator in combination with local search, where a preference\nfor small solutions minimizes bloat (lexicographic parsimony pressure); the\nresulting algorithm is denoted Concatenation Crossover GP. For this purpose\nthree variants of the well-studied MAJORITY test function with large plateaus\nare considered. We show that the Concatenation Crossover GP can efficiently\noptimize these test functions, while local search cannot be efficient for all\nthree variants independent of employing bloat control.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 14:18:25 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["K\u00f6tzing", "Timo", ""], ["Lagodzinski", "J. A. Gregor", ""], ["Lengler", "Johannes", ""], ["Melnichenko", "Anna", ""]]}, {"id": "1805.10262", "submitter": "Ankur Moitra", "authors": "Guy Bresler, Frederic Koehler, Ankur Moitra, Elchanan Mossel", "title": "Learning Restricted Boltzmann Machines via Influence Maximization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are a rich language for describing high-dimensional\ndistributions in terms of their dependence structure. While there are\nalgorithms with provable guarantees for learning undirected graphical models in\na variety of settings, there has been much less progress in the important\nscenario when there are latent variables. Here we study Restricted Boltzmann\nMachines (or RBMs), which are a popular model with wide-ranging applications in\ndimensionality reduction, collaborative filtering, topic modeling, feature\nextraction and deep learning.\n  The main message of our paper is a strong dichotomy in the feasibility of\nlearning RBMs, depending on the nature of the interactions between variables:\nferromagnetic models can be learned efficiently, while general models cannot.\nIn particular, we give a simple greedy algorithm based on influence\nmaximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn\na description of the distribution on the observed variables as a Markov Random\nField. Our analysis is based on tools from mathematical physics that were\ndeveloped to show the concavity of magnetization. Our algorithm extends\nstraighforwardly to general ferromagnetic Ising models with latent variables.\n  Conversely, we show that even for a contant number of latent variables with\nconstant degree, without ferromagneticity the problem is as hard as sparse\nparity with noise. This hardness result is based on a sharp and surprising\ncharacterization of the representational power of bounded degree RBMs: the\ndistribution on their observed variables can simulate any bounded order MRF.\nThis result is of independent interest since RBMs are the building blocks of\ndeep belief networks.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:32:19 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 19:31:28 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Bresler", "Guy", ""], ["Koehler", "Frederic", ""], ["Moitra", "Ankur", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1805.10406", "submitter": "Simon Du", "authors": "Simon S. Du, Yining Wang, Sivaraman Balakrishnan, Pradeep Ravikumar,\n  Aarti Singh", "title": "Robust Nonparametric Regression under Huber's $\\epsilon$-contamination\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-parametric regression problem under Huber's\n$\\epsilon$-contamination model, in which an $\\epsilon$ fraction of observations\nare subject to arbitrary adversarial noise. We first show that a simple local\nbinning median step can effectively remove the adversary noise and this median\nestimator is minimax optimal up to absolute constants over the H\\\"{o}lder\nfunction class with smoothness parameters smaller than or equal to 1.\nFurthermore, when the underlying function has higher smoothness, we show that\nusing local binning median as pre-preprocessing step to remove the adversarial\nnoise, then we can apply any non-parametric estimator on top of the medians. In\nparticular we show local median binning followed by kernel smoothing and local\npolynomial regression achieve minimaxity over H\\\"{o}lder and Sobolev classes\nwith arbitrary smoothness parameters. Our main proof technique is a decoupled\nanalysis of adversary noise and stochastic noise, which can be potentially\napplied to other robust estimation problems. We also provide numerical results\nto verify the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 00:39:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Balakrishnan", "Sivaraman", ""], ["Ravikumar", "Pradeep", ""], ["Singh", "Aarti", ""]]}, {"id": "1805.10708", "submitter": "Jason Li", "authors": "Jason Li", "title": "Distributed Treewidth Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of all the restricted graph families out there, the family of low treewidth\ngraphs has continuously proven to admit many algorithmic applications. For\nexample, many NP-hard algorithms can be solved in polynomial time on graphs of\nconstant treewidth. Other algorithmic techniques, such as Baker's technique,\npartition the graph into components of low treewidth. Therefore, computing the\ntreewidth of a graph remains an important problem in algorithm design. For\ngraphs of constant treewidth, linear-time algorithms are known in the classical\nsetting, and well as $\\text{polylog}(n)$-time parallel algorithms for computing\nan $O(1)$-approximation to treewidth. However, nothing is yet known in the\ndistributed setting.\n  In this paper, we give near-optimal algorithms for computing the treewidth on\na distributed network. We show that for graphs of constant treewidth, an\n$O(1)$-approximation to the treewidth can be computed in near-optimal $\\tilde\nO(D)$ time, where $D$ is the diameter of the network graph. In addition, we\nshow that many NP-hard problems that are tractable on constant treewidth graphs\ncan also be solved in $\\tilde O(D)$ time on a distributed network of constant\ntreewidth.\n  Our algorithms make use of the shortcuts framework of Ghaffari and Haeupler\n[SODA'16], which has proven to be a powerful tool in designing near-optimal\ndistributed algorithms for restricted graph networks, such as planar graphs,\nlow-treewidth graphs, and excluded minor graphs.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 23:01:25 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Li", "Jason", ""]]}, {"id": "1805.10754", "submitter": "Nils Kriege", "authors": "Nils M. Kriege, Andre Droschinsky, Petra Mutzel", "title": "A note on block-and-bridge preserving maximum common subgraph algorithms\n  for outerplanar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schietgat, Ramon and Bruynooghe proposed a polynomial-time algorithm for\ncomputing a maximum common subgraph under the block-and-bridge preserving\nsubgraph isomorphism (BBP-MCS) for outerplanar graphs. We show that the article\ncontains the following errors: (i) The running time of the presented approach\nis claimed to be $\\mathcal{O}(n^{2.5})$ for two graphs of order $n$. We show\nthat the algorithm of the authors allows no better bound than\n$\\mathcal{O}(n^4)$ when using state-of-the-art general purpose methods to solve\nthe matching instances arising as subproblems. This is even true for the\nspecial case, where both input graphs are trees. (ii) The article suggests that\nthe dissimilarity measure derived from BBP-MCS is a metric. We show that the\ntriangle inequality is not always satisfied and, hence, it is not a metric.\nTherefore, the dissimilarity measure should not be used in combination with\ntechniques that rely on or exploit the triangle inequality in any way. Where\npossible, we give hints on techniques that are suitable to improve the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 03:40:20 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 13:15:31 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Kriege", "Nils M.", ""], ["Droschinsky", "Andre", ""], ["Mutzel", "Petra", ""]]}, {"id": "1805.10885", "submitter": "Sumit Ganguly", "authors": "Sumit Ganguly and David P. Woodruff", "title": "High Probability Frequency Moment Sketches", "comments": "Extended Abstract to appear in Proceedings of ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of sketching the $p$-th frequency moment of a vector,\n$p>2$, with multiplicative error at most $1\\pm \\epsilon$ and \\emph{with high\nconfidence} $1-\\delta$. Despite the long sequence of work on this problem,\ntight bounds on this quantity are only known for constant $\\delta$. While one\ncan obtain an upper bound with error probability $\\delta$ by repeating a\nsketching algorithm with constant error probability $O(\\log(1/\\delta))$ times\nin parallel, and taking the median of the outputs, we show this is a suboptimal\nalgorithm! Namely, we show optimal upper and lower bounds of $\\Theta(n^{1-2/p}\n\\log(1/\\delta) + n^{1-2/p} \\log^{2/p} (1/\\delta) \\log n)$ on the sketching\ndimension, for any constant approximation. Our result should be contrasted with\nresults for estimating frequency moments for $1 \\leq p \\leq 2$, for which we\nshow the optimal algorithm for general $\\delta$ is obtained by repeating the\noptimal algorithm for constant error probability $O(\\log(1/\\delta))$ times and\ntaking the median output. We also obtain a matching lower bound for this\nproblem, up to constant factors.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 12:11:17 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ganguly", "Sumit", ""], ["Woodruff", "David P.", ""]]}, {"id": "1805.10902", "submitter": "Francesco Quinzan", "authors": "Tobias Friedrich and Andreas G\\\"obel and Francesco Quinzan and Markus\n  Wagner", "title": "Evolutionary Algorithms and Submodular Functions: Benefits of\n  Heavy-Tailed Mutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core feature of evolutionary algorithms is their mutation operator.\nRecently, much attention has been devoted to the study of mutation operators\nwith dynamic and non-uniform mutation rates. Following up on this line of work,\nwe propose a new mutation operator and analyze its performance on the (1+1)\nEvolutionary Algorithm (EA).\n  Our analyses show that this mutation operator competes with pre-existing\nones, when used by the (1+1) EA on classes of problems for which results on the\nother mutation operators are available. We show that the (1+1) EA using our\nmutation operator finds a (1/3)-approximation ratio on any non-negative\nsubmodular function in polynomial time. We also consider the problem of\nmaximizing a symmetric submodular function under a single matroid constraint\nand show that the (1+1) EA using our operator finds a (1/3)-approximation\nwithin polynomial time. This performance matches that of combinatorial local\nsearch algorithms specifically designed to solve these problems and outperforms\nthem with constant probability.\n  Finally, we evaluate the performance of the (1+1)EA using our operator\nexperimentally by considering two applications: (a) the maximum directed cut\nproblem on real-world graphs of different origins, and with up to 6.6 million\nvertices and 56 million edges and (b) the symmetric mutual information problem\nusing a four month period air pollution data set. In comparison with uniform\nmutation and a recently proposed dynamic scheme our operator comes out on top\non these instances.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 13:05:02 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 10:00:40 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Friedrich", "Tobias", ""], ["G\u00f6bel", "Andreas", ""], ["Quinzan", "Francesco", ""], ["Wagner", "Markus", ""]]}, {"id": "1805.10941", "submitter": "Daniel Lemire", "authors": "Daniel Lemire", "title": "Fast Random Integer Generation in an Interval", "comments": "to appear in ACM Transactions on Modeling and Computer Simulation", "journal-ref": "ACM Transactions on Modeling and Computer Simulation 29 (1), 2019", "doi": "10.1145/3230636", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In simulations, probabilistic algorithms and statistical tests, we often\ngenerate random integers in an interval (e.g., [0,s)). For example, random\nintegers in an interval are essential to the Fisher-Yates random shuffle.\nConsequently, popular languages like Java, Python, C++, Swift and Go include\nranged random integer generation functions as part of their runtime libraries.\n  Pseudo-random values are usually generated in words of a fixed number of bits\n(e.g., 32 bits, 64 bits) using algorithms such as a linear congruential\ngenerator. We need functions to convert such random words to random integers in\nan interval ([0,s)) without introducing statistical biases. The standard\nfunctions in programming languages such as Java involve integer divisions.\nUnfortunately, division instructions are relatively expensive. We review an\nunbiased function to generate ranged integers from a source of random words\nthat avoids integer divisions with high probability. To establish the practical\nusefulness of the approach, we show that this algorithm can multiply the speed\nof unbiased random shuffling on x64 processors. Our proposed approach has been\nadopted by the Go language for its implementation of the shuffle function.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 14:28:04 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 00:02:09 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 01:48:39 GMT"}, {"version": "v4", "created": "Fri, 28 Dec 2018 16:18:11 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Lemire", "Daniel", ""]]}, {"id": "1805.11170", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Strongly polynomial efficient approximation scheme for segmentation", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipl.2018.09.007", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning a sequence of length $n$ into $k$ coherent segments (Seg) is one\nof the classic optimization problems. As long as the optimization criterion is\nadditive, Seg can be solved exactly in $O(n^2k)$ time using a classic dynamic\nprogram. Due to the quadratic term, computing the exact segmentation may be too\nexpensive for long sequences, which has led to development of approximate\nsolutions. We consider an existing estimation scheme that computes $(1 +\n\\epsilon)$ approximation in polylogarithmic time. We augment this algorithm,\nmaking it strongly polynomial. We do this by first solving a slightly different\nsegmentation problem (MaxSeg), where the quality of the segmentation is the\nmaximum penalty of an individual segment. By using this solution to initialize\nthe estimation scheme, we are able to obtain a strongly polynomial algorithm.\nIn addition, we consider a cumulative version of Seg, where we are asked to\ndiscover the optimal segmentation for each prefix of the input sequence. We\npropose a strongly polynomial algorithm that yields $(1 + \\epsilon)$\napproximation in $O(nk^2 / \\epsilon)$ time. Finally, we consider a cumulative\nversion of MaxSeg, and show that we can solve the problem in $O(nk \\log k)$\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 20:55:47 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:30:31 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1805.11251", "submitter": "Shinsaku Sakaue", "authors": "Shinsaku Sakaue", "title": "On Maximization of Weakly Modular Functions: Guarantees of Multi-stage\n  Algorithms, Tractability, and Hardness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximization of {\\it non-submodular} functions appears in various scenarios,\nand many previous works studied it based on some measures that quantify the\ncloseness to being submodular. On the other hand, many practical non-submodular\nfunctions are actually close to being {\\it modular}, which has been utilized in\nfew studies. In this paper, we study cardinality-constrained maximization of\n{\\it weakly modular} functions, whose closeness to being modular is measured by\n{\\it submodularity} and {\\it supermodularity ratios}, and reveal what we can\nand cannot do by using the weak modularity. We first show that guarantees of\nmulti-stage algorithms can be proved with the weak modularity, which generalize\nand improve some existing results, and experiments confirm their effectiveness.\nWe then show that weakly modular maximization is {\\it fixed-parameter\ntractable} under certain conditions; as a byproduct, we provide a new\ntime--accuracy trade-off for $\\ell_0$-constrained minimization. We finally\nprove that, even if objective functions are weakly modular, no polynomial-time\nalgorithms can improve the existing approximation guarantees achieved by the\ngreedy algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 05:35:08 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 03:55:27 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 08:01:13 GMT"}, {"version": "v4", "created": "Tue, 21 May 2019 04:08:42 GMT"}, {"version": "v5", "created": "Thu, 3 Oct 2019 03:16:52 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Sakaue", "Shinsaku", ""]]}, {"id": "1805.11255", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Succinct data structure for dynamic trees with faster queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navarro and Sadakane [TALG 2014] gave a dynamic succinct data structure for\nstoring an ordinal tree. The structure supports tree queries in either $O(\\log\nn/\\log\\log n)$ or $O(\\log n)$ time, and insertion or deletion of a single node\nin $O(\\log n)$ time. In this paper we improve the result of Navarro and\nSadakane by reducing the time complexities of some queries (e.g.\\ degree and\nlevel\\_ancestor) from $O(\\log n)$ to $O(\\log n/\\log\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 05:53:08 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1805.11275", "submitter": "Benjamin Bergougnoux", "authors": "Benjamin Bergougnoux, Mamadou Moustapha Kant\\'e", "title": "More applications of the d-neighbor equivalence: acyclicity and\n  connectivity constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a framework to obtain efficient algorithms for\nseveral problems with a global constraint (acyclicity or connectivity) such as\nConnected Dominating Set, Node Weighted Steiner Tree, Maximum Induced Tree,\nLongest Induced Path, and Feedback Vertex Set. We design a meta-algorithm that\nsolves all these problems and whose running time is upper bounded by\n$2^{O(k)}\\cdot n^{O(1)}$, $2^{O(k \\log(k))}\\cdot n^{O(1)}$, $2^{O(k^2)}\\cdot\nn^{O(1)}$ and $n^{O(k)}$ where $k$ is respectively the clique-width,\n$\\mathbf{Q}$-rank-width, rank-width and maximum induced matching width of a\ngiven decomposition. Our approach simplifies and unifies the known algorithms\nfor each of the parameters and its running time matches asymptotically also the\nrunning times of the best known algorithms for basic NP-hard problems such as\nVertex Cover and Dominating Set. Our framework is based on the $d$-neighbor\nequivalence defined in [Bui-Xuan, Telle and Vatshelle, TCS 2013] and the\nrank-based approach introduced in [Bodlaender, Cygan, Kratsch and Nederlof,\nICALP 2013]. The results we obtain highlight the importance of the $d$-neighbor\nequivalence relation on the algorithmic applications of width measures.\n  We also prove that our framework could be useful for $W[1]$-hard problems\nparameterized by clique-width such as Max Cut and Maximum Minimal Cut. For\nthese latter problems, we obtain $n^{O(k)}$, $n^{O(k)}$ and $n^{2^{O(k)}}$ time\nalgorithms where $k$ is respectively the clique-width, the\n$\\mathbf{Q}$-rank-width and the rank-width of the input graph.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 07:16:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:40:44 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 20:30:58 GMT"}, {"version": "v4", "created": "Mon, 20 May 2019 08:31:39 GMT"}, {"version": "v5", "created": "Thu, 5 Dec 2019 15:06:53 GMT"}, {"version": "v6", "created": "Fri, 11 Sep 2020 15:05:12 GMT"}, {"version": "v7", "created": "Wed, 26 May 2021 08:45:58 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Kant\u00e9", "Mamadou Moustapha", ""]]}, {"id": "1805.11297", "submitter": "Christoph D\\\"urr", "authors": "Christoph D\\\"urr, {\\L}ukasz Je\\.z and Oscar C. V\\'asquez", "title": "Scheduling under dynamic speed-scaling for minimizing weighted\n  completion time and energy consumption", "comments": null, "journal-ref": "Discrete Applied Mathematics, Volume 196, 11 December 2015, Pages\n  20-27", "doi": "10.1016/j.dam.2014.08.001", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since a few years there is an increasing interest in minimizing the energy\nconsumption of computing systems. However in a shared computing system, users\nwant to optimize their experienced quality of service, at the price of a high\nenergy consumption. In this work, we address the problem of optimizing and\ndesigning mechanisms for a linear combination of weighted completion time and\nenergy consumption on a single machine with dynamic speed-scaling. We show that\nminimizing linear combination reduces to a unit speed scheduling problem under\na polynomial penalty function. In the mechanism design setting, we define a\ncost share mechanism and studied its properties, showing that it is truthful\nand the overcharging of total cost share is bounded by a constant.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 08:25:18 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["D\u00fcrr", "Christoph", ""], ["Je\u017c", "\u0141ukasz", ""], ["V\u00e1squez", "Oscar C.", ""]]}, {"id": "1805.11405", "submitter": "Andrej Risteski", "authors": "Frederic Koehler, Andrej Risteski", "title": "Representational Power of ReLU Networks and Polynomial Kernels: Beyond\n  Worst-Case Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a large amount of interest, both in the past and particularly\nrecently, into the power of different families of universal approximators, e.g.\nReLU networks, polynomials, rational functions. However, current research has\nfocused almost exclusively on understanding this problem in a worst-case\nsetting, e.g. bounding the error of the best infinity-norm approximation in a\nbox. In this setting a high-degree polynomial is required to even approximate a\nsingle ReLU.\n  However, in real applications with high dimensional data we expect it is only\nimportant to approximate the desired function well on certain relevant parts of\nits domain. With this motivation, we analyze the ability of neural networks and\npolynomial kernels of bounded degree to achieve good statistical performance on\na simple, natural inference problem with sparse latent structure. We give\nalmost-tight bounds on the performance of both neural networks and low degree\npolynomials for this problem. Our bounds for polynomials involve new techniques\nwhich may be of independent interest and show major qualitative differences\nwith what is known in the worst-case setting.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 13:05:56 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Koehler", "Frederic", ""], ["Risteski", "Andrej", ""]]}, {"id": "1805.11864", "submitter": "Torben Hagerup", "authors": "Torben Hagerup", "title": "Space-Efficient DFS and Applications: Simpler, Leaner, Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of space-efficient depth-first search (DFS) is reconsidered. A\nparticularly simple and fast algorithm is presented that, on a directed or\nundirected input graph $G=(V,E)$ with $n$ vertices and $m$ edges, carries out a\nDFS in $O(n+m)$ time with $n+\\sum_{v\\in V_{\\ge 3}}\\lceil{\\log_2(d_v-1)}\\rceil\n  +O(\\log n)\\le n+m+O(\\log n)$ bits of working memory, where $d_v$ is the\n(total) degree of $v$, for each $v\\in V$, and $V_{\\ge 3}=\\{v\\in V\\mid d_v\\ge\n3\\}$. A slightly more complicated variant of the algorithm works in the same\ntime with at most $n+({4/5})m+O(\\log n)$ bits. It is also shown that a DFS can\nbe carried out in a graph with $n$ vertices and $m$ edges in $O(n+m\\log^*\\! n)$\ntime with $O(n)$ bits or in $O(n+m)$ time with either $O(n\\log\\log(4+{m/n}))$\nbits or, for arbitrary integer $k\\ge 1$, $O(n\\log^{(k)}\\! n)$ bits. These\nresults among them subsume or improve most earlier results on space-efficient\nDFS. Some of the new time and space bounds are shown to extend to applications\nof DFS such as the computation of cut vertices, bridges, biconnected components\nand 2-edge-connected components in undirected graphs.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 08:56:47 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Hagerup", "Torben", ""]]}, {"id": "1805.12051", "submitter": "Saurabh Sawlani", "authors": "Timothy Chu, Yu Gao, Richard Peng, Sushant Sachdeva, Saurabh Sawlani,\n  Junxing Wang", "title": "Graph Sparsification, Spectral Sketches, and Faster Resistance\n  Computation, via Short Cycle Decompositions", "comments": "80 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for graph sparsification and sketching, based on a new\ntool, short cycle decomposition -- a decomposition of an unweighted graph into\nan edge-disjoint collection of short cycles, plus few extra edges. A simple\nobservation gives that every graph G on n vertices with m edges can be\ndecomposed in $O(mn)$ time into cycles of length at most $2\\log n$, and at most\n$2n$ extra edges. We give an $m^{1+o(1)}$ time algorithm for constructing a\nshort cycle decomposition, with cycles of length $n^{o(1)}$, and $n^{1+o(1)}$\nextra edges. These decompositions enable us to make progress on several open\nquestions:\n  * We give an algorithm to find $(1\\pm\\epsilon)$-approximations to effective\nresistances of all edges in time $m^{1+o(1)}\\epsilon^{-1.5}$, improving over\nthe previous best of $\\tilde{O}(\\min\\{m\\epsilon^{-2},n^2 \\epsilon^{-1}\\})$.\nThis gives an algorithm to approximate the determinant of a Laplacian up to\n$(1\\pm\\epsilon)$ in $m^{1 + o(1)} + n^{15/8+o(1)}\\epsilon^{-7/4}$ time.\n  * We show existence and efficient algorithms for constructing graphical\nspectral sketches -- a distribution over sparse graphs H such that for a fixed\nvector $x$, we have w.h.p. $x'L_Hx=(1\\pm\\epsilon)x'L_Gx$ and\n$x'L_H^+x=(1\\pm\\epsilon)x'L_G^+x$. This implies the existence of\nresistance-sparsifiers with about $n\\epsilon^{-1}$ edges that preserve the\neffective resistances between every pair of vertices up to $(1\\pm\\epsilon).$\n  * By combining short cycle decompositions with known tools in graph\nsparsification, we show the existence of nearly-linear sized degree-preserving\nspectral sparsifiers, as well as significantly sparser approximations of\ndirected graphs. The latter is critical to recent breakthroughs on faster\nalgorithms for solving linear systems in directed Laplacians.\n  Improved algorithms for constructing short cycle decompositions will lead to\nimprovements for each of the above results.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:14:18 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Chu", "Timothy", ""], ["Gao", "Yu", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""], ["Sawlani", "Saurabh", ""], ["Wang", "Junxing", ""]]}, {"id": "1805.12172", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan", "title": "Opinion Forming in Erdos-Renyi Random Graph and Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume for a graph $G=(V,E)$ and an initial configuration, where each node is\nblue or red, in each discrete-time round all nodes simultaneously update their\ncolor to the most frequent color in their neighborhood and a node keeps its\ncolor in case of a tie. We study the behavior of this basic process, which is\ncalled majority model, on the binomial random graph $\\mathcal{G}_{n,p}$ and\nregular expanders. First we consider the behavior of the majority model in\n$\\mathcal{G}_{n,p}$ with an initial random configuration, where each node is\nblue independently with probability $p_b$ and red otherwise. It is shown that\nin this setting the process goes through a phase transition at the connectivity\nthreshold, namely $\\frac{\\log n}{n}$. Furthermore, we discuss the majority\nmodel is a `good' and `fast' density classifier on regular expanders. More\nprecisely, we prove if the second-largest absolute eigenvalue of the adjacency\nmatrix of an $n$-node $\\Delta$-regular graph is sufficiently smaller than\n$\\Delta$ then the majority model by starting from $(\\frac{1}{2}-\\delta)n$ blue\nnodes (for an arbitrarily small constant $\\delta>0$) results in fully red\nconfiguration in sub-logarithmically many rounds. As a by-product of our\nresults, we show Ramanujan graphs are asymptotically optimally immune, that is\nfor an $n$-node $\\Delta$-regular Ramanujan graph if the initial number of blue\nnodes is $s\\leq \\beta n$, the number of blue nodes in the next round is at most\n$\\frac{cs}{\\Delta}$ for some constants $c,\\beta>0$. This settles an open\nproblem by Peleg.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 11:40:47 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 14:46:25 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zehmakan", "Ahad N.", ""]]}, {"id": "1805.12238", "submitter": "Eduar Castrillo Velilla", "authors": "Eduar Castrillo, Elizabeth Le\\'on, Jonatan G\\'omez", "title": "High-Quality Disjoint and Overlapping Community Structure in Large-Scale\n  Complex Networks", "comments": "8 pages, 5 figures, 3 tables, sent to peer-review to the\n  International Symposium on Foundations and Applications of Big Data Analytics\n  FAB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an improved version of an agglomerative\nhierarchical clustering algorithm that performs disjoint community detection in\nlarge-scale complex networks. The improved algorithm is achieved after\nreplacing the local structural similarity used in the original algorithm, with\nthe recently proposed Dynamic Structural Similarity. Additionally, the improved\nalgorithm is extended to detect fuzzy and crisp overlapping community\nstructure. The extended algorithm leverages the disjoint community structure\ngenerated by itself and the dynamic structural similarity measures, to compute\na proposed membership probability function that defines the fuzzy communities.\nMoreover, an experimental evaluation is performed on reference benchmark graphs\nin order to compare the proposed algorithms with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 21:44:27 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Castrillo", "Eduar", ""], ["Le\u00f3n", "Elizabeth", ""], ["G\u00f3mez", "Jonatan", ""]]}, {"id": "1805.12498", "submitter": "Nicol\\'as Quesada", "authors": "Andreas Bj\\\"orklund, Brajesh Gupt and Nicol\\'as Quesada", "title": "A faster hafnian formula for complex matrices and its benchmarking on a\n  supercomputer", "comments": "11 pages, 7 figures. The source code of the library is available at\n  https://github.com/XanaduAI/hafnian . Accepted for publication in Journal of\n  Experimental Algorithmics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new and simple algorithms for the calculation of the number of\nperfect matchings of complex weighted, undirected graphs with and without\nloops. Our compact formulas for the hafnian and loop hafnian of $n \\times n $\ncomplex matrices run in $O(n^3 2^{n/2})$ time, are embarrassingly\nparallelizable and, to the best of our knowledge, are the fastest exact\nalgorithms to compute these quantities. Despite our highly optimized algorithm,\nnumerical benchmarks on the Titan supercomputer with matrices up to size $56\n\\times 56$ indicate that one would require the 288000 CPUs of this machine for\nabout a month and a half to compute the hafnian of a $100 \\times 100$ matrix.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 14:44:06 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 15:23:38 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 16:17:04 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Gupt", "Brajesh", ""], ["Quesada", "Nicol\u00e1s", ""]]}, {"id": "1805.12537", "submitter": "Andrei Khrennikov Yu", "authors": "Ekaterina Yurova Axelsson and Andrei Khrennikov", "title": "Groups of automorphisms of p-adic integers and the problem of the\n  existence of fully homomorphic ciphers", "comments": "arXiv admin note: text overlap with arXiv:1603.07699", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study groups of automorphisms of algebraic systems over a\nset of $p$-adic integers with different sets of arithmetic and coordinate-wise\nlogical operations and congruence relations modulo $p^k,$ $k\\ge 1.$ The main\nresult of this paper is the description of groups of automorphisms of $p$-adic\nintegers with one or two arithmetic or coordinate-wise logical operations on\n$p$-adic integers. To describe groups of automorphisms, we use the apparatus of\nthe $p$-adic analysis and $p$-adic dynamical systems. The motive for the study\nof groups of automorphism of algebraic systems over $p$-adic integers is the\nquestion of the existence of a fully homomorphic encryption in a given family\nof ciphers. The relationship between these problems is based on the possibility\nof constructing a \"continuous\" $p$-adic model for some families of ciphers (in\nthis context, these ciphers can be considered as \"discrete\" systems). As a\nconsequence, we can apply the \"continuous\" methods of $p$-adic analysis to\nsolve the \"discrete\" problem of the existence of fully homomorphic ciphers.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 18:57:31 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Axelsson", "Ekaterina Yurova", ""], ["Khrennikov", "Andrei", ""]]}, {"id": "1805.12591", "submitter": "Lorenzo Orecchia", "authors": "Michael B. Cohen, Jelena Diakonikolas, Lorenzo Orecchia", "title": "On Acceleration with Noise-Corrupted Gradients", "comments": "Appeared in Proc. ICML'18; v2 corrects the statement of Corollary\n  3.9; v3 added references to concurrent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated algorithms have broad applications in large-scale optimization,\ndue to their generality and fast convergence. However, their stability in the\npractical setting of noise-corrupted gradient oracles is not well-understood.\nThis paper provides two main technical contributions: (i) a new accelerated\nmethod AGDP that generalizes Nesterov's AGD and improves on the recent method\nAXGD (Diakonikolas & Orecchia, 2018), and (ii) a theoretical study of\naccelerated algorithms under noisy and inexact gradient oracles, which is\nsupported by numerical experiments. This study leverages the simplicity of AGDP\nand its analysis to clarify the interaction between noise and acceleration and\nto suggest modifications to the algorithm that reduce the mean and variance of\nthe error incurred due to the gradient noise.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 17:56:28 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 21:23:51 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 17:42:41 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Cohen", "Michael B.", ""], ["Diakonikolas", "Jelena", ""], ["Orecchia", "Lorenzo", ""]]}]