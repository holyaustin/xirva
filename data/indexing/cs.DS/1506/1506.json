[{"id": "1506.00063", "submitter": "Xiaodong Wang", "authors": "Daxin Zhu, Lei Wang, Yingjie Wu, and Xiaodong Wang", "title": "An Efficient Dynamic Programming Algorithm for STR-IC-SEQ-EC-LCS Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a generalized longest common subsequence problem,\nin which a constraining sequence of length $s$ must be included as a substring\nand the other constraining sequence of length $t$ must be excluded as a\nsubsequence of two main sequences and the length of the result must be maximal.\nFor the two input sequences $X$ and $Y$ of lengths $n$ and $m$, and the given\ntwo constraining sequences of length $s$ and $t$, we present an $O(nmst)$ time\ndynamic programming algorithm for solving the new generalized longest common\nsubsequence problem. The time complexity can be reduced further to cubic time\nin a more detailed analysis. The correctness of the new algorithm is proved.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 03:14:38 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Zhu", "Daxin", ""], ["Wang", "Lei", ""], ["Wu", "Yingjie", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1506.00147", "submitter": "Maithra Raghu", "authors": "Jon Kleinberg, Maithra Raghu", "title": "Team Performance with Test Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Team performance is a ubiquitous area of inquiry in the social sciences, and\nit motivates the problem of team selection -- choosing the members of a team\nfor maximum performance. Influential work of Hong and Page has argued that\ntesting individuals in isolation and then assembling the highest-scoring ones\ninto a team is not an effective method for team selection. For a broad class of\nperformance measures, based on the expected maximum of random variables\nrepresenting individual candidates, we show that tests directly measuring\nindividual performance are indeed ineffective, but that a more subtle family of\ntests used in isolation can provide a constant-factor approximation for team\nperformance. These new tests measure the \"potential\" of individuals, in a\nprecise sense, rather than performance, to our knowledge they represent the\nfirst time that individual tests have been shown to produce near-optimal teams\nfor a non-trivial team performance measure. We also show families of\nsubdmodular and supermodular team performance functions for which no test\napplied to individuals can produce near-optimal teams, and discuss implications\nfor submodular maximization via hill-climbing.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 17:57:53 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 19:15:02 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kleinberg", "Jon", ""], ["Raghu", "Maithra", ""]]}, {"id": "1506.00227", "submitter": "YaJun Cui", "authors": "Yajun Cui, Yang Zhao, Kafei Xiao, Chenglong Zhang, Lei Wang", "title": "Parallel Spectral Clustering Algorithm Based on Hadoop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering and cloud computing is emerging branch of computer\nscience or related discipline. It overcome the shortcomings of some traditional\nclustering algorithm and guarantee the convergence to the optimal solution,\nthus have to the widespread attention. This article first introduced the\nparallel spectral clustering algorithm research background and significance,\nand then to Hadoop the cloud computing Framework has carried on the detailed\nintroduction, then has carried on the related to spectral clustering is\nintroduced, then introduces the spectral clustering arithmetic Method of\nparallel and relevant steps, finally made the related experiments, and the\nexperiment are summarized.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 13:39:41 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Cui", "Yajun", ""], ["Zhao", "Yang", ""], ["Xiao", "Kafei", ""], ["Zhang", "Chenglong", ""], ["Wang", "Lei", ""]]}, {"id": "1506.00242", "submitter": "Zhiwei Steven Wu", "authors": "Michael Kearns, Aaron Roth, Zhiwei Steven Wu, Grigory Yaroslavtsev", "title": "Privacy for the Protected (Only)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by tensions between data privacy for individual citizens, and\nsocietal priorities such as counterterrorism and the containment of infectious\ndisease, we introduce a computational model that distinguishes between parties\nfor whom privacy is explicitly protected, and those for whom it is not (the\ntargeted subpopulation). The goal is the development of algorithms that can\neffectively identify and take action upon members of the targeted subpopulation\nin a way that minimally compromises the privacy of the protected, while\nsimultaneously limiting the expense of distinguishing members of the two groups\nvia costly mechanisms such as surveillance, background checks, or medical\ntesting. Within this framework, we provide provably privacy-preserving\nalgorithms for targeted search in social networks. These algorithms are natural\nvariants of common graph search methods, and ensure privacy for the protected\nby the careful injection of noise in the prioritization of potential targets.\nWe validate the utility of our algorithms with extensive computational\nexperiments on two large-scale social network datasets.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 14:47:27 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Kearns", "Michael", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1506.00671", "submitter": "Ludwig Schmidt", "authors": "Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt", "title": "Sample-Optimal Density Estimation in Nearly-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new, fast algorithm for agnostically learning univariate\nprobability distributions whose densities are well approximated by piecewise\npolynomial functions. Let $f$ be the density function of an arbitrary\nunivariate distribution, and suppose that $f$ is $\\mathrm{OPT}$-close in\n$L_1$-distance to an unknown piecewise polynomial function with $t$ interval\npieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\\epsilon^2)$ samples\nfrom $f$, runs in time $\\tilde{O}(n \\cdot \\mathrm{poly}(d))$, and with\nprobability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis\n$h$ that is $4 \\cdot \\mathrm{OPT} +\\epsilon$ close to $f$.\n  Our general algorithm yields (nearly) sample-optimal and nearly-linear time\nestimators for a wide range of structured distribution families over both\ncontinuous and discrete domains in a unified way. For most of our applications,\nthese are the first sample-optimal and nearly-linear time estimators in the\nliterature. As a consequence, our work resolves the sample and computational\ncomplexities of a broad class of inference tasks via a single \"meta-algorithm\".\nMoreover, we experimentally demonstrate that our algorithm performs very well\nin practice.\n  Our algorithm consists of three \"levels\": (i) At the top level, we employ an\niterative greedy algorithm for finding a good partition of the real line into\nthe pieces of a piecewise polynomial. (ii) For each piece, we show that the\nsub-problem of finding a good polynomial fit on the current interval can be\nsolved efficiently with a separation oracle method. (iii) We reduce the task of\nfinding a separating hyperplane to a combinatorial problem and give an\nefficient algorithm for this problem. Combining these three procedures gives a\ndensity estimation algorithm with the claimed guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 20:44:22 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Acharya", "Jayadev", ""], ["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1506.00677", "submitter": "Katarzyna Paluch", "authors": "Pratik Ghosal and Adam Kunysz and Katarzyna Paluch", "title": "Characterisation of Strongly Stable Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of a strongly stable matching problem (SSMP) is an undirected\nbipartite graph $G=(A \\cup B, E)$, with an adjacency list of each vertex being\na linearly ordered list of ties, which are subsets of vertices equally good for\na given vertex. Ties are disjoint and may contain one vertex. A matching $M$ is\na set of vertex-disjoint edges. An edge $(x,y) \\in E \\setminus M$ is a {\\em\nblocking edge} for $M$ if $x$ is either unmatched or strictly prefers $y$ to\nits current partner in $M$, and $y$ is either unmatched or strictly prefers $x$\nto its current partner in $M$ or is indifferent between them. A matching is\n{\\em strongly stable} if there is no blocking edge with respect to it. We\npresent an algorithm for the generation of all strongly stable matchings, thus\nsolving an open problem already stated in the book by Gusfield and Irving\n\\cite{GI}. It has previously been shown that strongly stable matchings form a\ndistributive lattice and although the number of strongly stable matchings can\nbe exponential in the number of vertices, we show that there exists a partial\norder with $O(m)$ elements representing all strongly stable matchings, where\n$m$ denotes the number of edges in the graph. We give two algorithms that\nconstruct two such representations: one in $O(nm^2)$ time and the other in\n$O(nm)$ time, where $n$ denotes the number of vertices in the graph. Note that\nthe construction of the second representation has the same time complexity as\nthat of computing a single strongly stable matching.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 21:25:24 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Ghosal", "Pratik", ""], ["Kunysz", "Adam", ""], ["Paluch", "Katarzyna", ""]]}, {"id": "1506.00747", "submitter": "Chaoyang Jiang", "authors": "Chaoyang Jiang, Yeng Chai Soh, and Hua Li", "title": "Sensor placement by maximal projection on minimum eigenspace for linear\n  inverse problems", "comments": "15 pages, 7 figures, and 1 table. Accepted by IEEE Transactions on\n  Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2573767", "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two new greedy sensor placement algorithms, named minimum\nnonzero eigenvalue pursuit (MNEP) and maximal projection on minimum eigenspace\n(MPME), for linear inverse problems, with greater emphasis on the MPME\nalgorithm for performance comparison with existing approaches. We select the\nsensing locations one-by-one. In this way, the least number of required sensors\ncan be determined by checking whether the estimation accuracy is satisfied\nafter each sensing location is determined. The minimum eigenspace is defined as\nthe eigenspace associated with the minimum eigenvalue of the dual observation\nmatrix. For each sensing location, the projection of its observation vector\nonto the minimum eigenspace is shown to be monotonically decreasing w.r.t. the\nworst case error variance (WCEV) of the estimated parameters. We select the\nsensing location whose observation vector has the maximum projection onto the\nminimum eigenspace of the current dual observation matrix. The proposed MPME is\nshown to be one of the most computationally efficient algorithms. Our\nMonte-Carlo simulations showed that MPME outperforms the convex relaxation\nmethod [1], the SparSenSe method [2], and the FrameSense method [3] in terms of\nWCEV and the mean square error (MSE) of the estimated parameters, especially\nwhen the number of available sensor nodes is very limited.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 04:48:49 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 14:37:54 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 08:36:14 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Jiang", "Chaoyang", ""], ["Soh", "Yeng Chai", ""], ["Li", "Hua", ""]]}, {"id": "1506.00944", "submitter": "Fabio Protti", "authors": "Maise Dantas da Silva, F\\'abio Protti, Jayme Luiz Szwarcfiter", "title": "Parameterized mixed cluster editing via modular decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a natural generalization of the well-known\nproblems Cluster Editing and Bicluster Editing, whose parameterized versions\nhave been intensively investigated in the recent literature. The generalized\nproblem, called Mixed Cluster Editing or ${\\cal M}$-Cluster Editing, is\nformulated as follows. Let ${\\cal M}$ be a family of graphs. Given a graph $G$\nand a nonnegative integer $k$, transform $G$, through a sequence of at most $k$\nedge editions, into a target graph $G'$ with the following property: $G'$ is a\nvertex-disjoint union of graphs $G_1, G_2, \\ldots$ such that every $G_i$ is a\nmember of ${\\cal M}$. The graph $G'$ is called a mixed cluster graph or ${\\cal\nM}$-cluster graph. Let ${\\cal K}$ denote the family of complete graphs, ${\\cal\nKL}$ the family of complete $l$-partite graphs ($l \\geq 2$), and $\\L={\\cal K}\n\\cup {\\cal KL}$. In this work we focus on the case ${\\cal M} = {\\cal L}$. Using\nmodular decomposition techniques previously applied to Cluster/Bicluster\nEditing, we present a linear-time algorithm to construct a problem kernel for\nthe parameterized version of ${\\cal L}$-Cluster Editing. Keywords: bicluster\ngraphs, cluster graphs, edge edition problems, edge modification problems,\nfixed-parameter tractability, NP-complete problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 16:22:46 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["da Silva", "Maise Dantas", ""], ["Protti", "F\u00e1bio", ""], ["Szwarcfiter", "Jayme Luiz", ""]]}, {"id": "1506.01071", "submitter": "Aleksey Buzmakov", "authors": "Aleksey Buzmakov and Sergei O. Kuznetsov and Amedeo Napoli", "title": "Fast Generation of Best Interval Patterns for Nonmonotonic Constraints", "comments": "18 pages; 2 figures; 2 tables; 1 algorithm; PKDD 2015 Conference\n  Scientific Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pattern mining, the main challenge is the exponential explosion of the set\nof patterns. Typically, to solve this problem, a constraint for pattern\nselection is introduced. One of the first constraints proposed in pattern\nmining is support (frequency) of a pattern in a dataset. Frequency is an\nanti-monotonic function, i.e., given an infrequent pattern, all its\nsuperpatterns are not frequent. However, many other constraints for pattern\nselection are not (anti-)monotonic, which makes it difficult to generate\npatterns satisfying these constraints. In this paper we introduce the notion of\nprojection-antimonotonicity and $\\theta$-$\\Sigma\\o\\phi\\iota\\alpha$ algorithm\nthat allows efficient generation of the best patterns for some nonmonotonic\nconstraints. In this paper we consider stability and $\\Delta$-measure, which\nare nonmonotonic constraints, and apply them to interval tuple datasets. In the\nexperiments, we compute best interval tuple patterns w.r.t. these measures and\nshow the advantage of our approach over postfiltering approaches.\n  KEYWORDS: Pattern mining, nonmonotonic constraints, interval tuple data\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:32:14 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 15:31:19 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Buzmakov", "Aleksey", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""]]}, {"id": "1506.01082", "submitter": "Carlo Comin MSc", "authors": "Carlo Comin, Romeo Rizzi", "title": "An Improved Upper Bound on Maximal Clique Listing via Rectangular Fast\n  Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first output-sensitive algorithm for the Maximal Clique Listing problem\nwas given by Tsukiyama et.al. in 1977. As any algorithm falling within the\nReverse Search paradigm, it performs a DFS visit of a directed tree (the\nRS-tree) having the objects to be listed (i.e. maximal cliques) as its nodes.\nIn a recursive implementation, the RS-tree corresponds to the recursion tree of\nthe algorithm. The time delay is given by the cost of generating the next child\nof a node, and Tsukiyama showed it is $O(mn)$. In 2004, Makino and Uno\nsharpened the time delay to $O(n^{\\omega})$ by generating all the children of a\nnode in one single shot performed by computing a \\emph{square} fast matrix\nmultiplication. In this paper, we further improve the asymptotics for the\nexploration of the same RS-tree by grouping the offsprings' computation even\nfurther. Our idea is to rely on rectangular fast matrix multiplication in order\nto compute all children of $n^2$ nodes in one shot. According to the current\nupper bounds on fast matrix multiplication, with this the time delay improves\nfrom $O(n^{2.3728639})$ to $O(n^{2.093362})$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 22:48:34 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 19:48:10 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2015 23:05:59 GMT"}, {"version": "v4", "created": "Sun, 21 Jun 2015 12:35:25 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2015 08:19:05 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1506.01367", "submitter": "Ludwig Schmidt", "authors": "Jerry Li, Ludwig Schmidt", "title": "A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture\n  of k Gaussians, for any Constant k", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a Gaussian mixture model (GMM) is a fundamental problem in machine\nlearning, learning theory, and statistics. One notion of learning a GMM is\nproper learning: here, the goal is to find a mixture of $k$ Gaussians\n$\\mathcal{M}$ that is close to the density $f$ of the unknown distribution from\nwhich we draw samples. The distance between $\\mathcal{M}$ and $f$ is typically\nmeasured in the total variation or $L_1$-norm.\n  We give an algorithm for learning a mixture of $k$ univariate Gaussians that\nis nearly optimal for any fixed $k$. The sample complexity of our algorithm is\n$\\tilde{O}(\\frac{k}{\\epsilon^2})$ and the running time is $(k \\cdot\n\\log\\frac{1}{\\epsilon})^{O(k^4)} + \\tilde{O}(\\frac{k}{\\epsilon^2})$. It is\nwell-known that this sample complexity is optimal (up to logarithmic factors),\nand it was already achieved by prior work. However, the best known time\ncomplexity for proper learning a $k$-GMM was\n$\\tilde{O}(\\frac{1}{\\epsilon^{3k-1}})$. In particular, the dependence between\n$\\frac{1}{\\epsilon}$ and $k$ was exponential. We significantly improve this\ndependence by replacing the $\\frac{1}{\\epsilon}$ term with a $\\log\n\\frac{1}{\\epsilon}$ while only increasing the exponent moderately. Hence, for\nany fixed $k$, the $\\tilde{O} (\\frac{k}{\\epsilon^2})$ term dominates our\nrunning time, and thus our algorithm runs in time which is nearly-linear in the\nnumber of samples drawn. Achieving a running time of $\\textrm{poly}(k,\n\\frac{1}{\\epsilon})$ for proper learning of $k$-GMMs has recently been stated\nas an open problem by multiple researchers, and we make progress on this\nquestion.\n  Moreover, our approach offers an agnostic learning guarantee: our algorithm\nreturns a good GMM even if the distribution we are sampling from is not a\nmixture of Gaussians. To the best of our knowledge, our algorithm is the first\nagnostic proper learning algorithm for GMMs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 19:55:10 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1506.01406", "submitter": "Jose Rodrigues Jr", "authors": "Hugo Gualdron, Robson Cordeiro, Jose Rodrigues-Jr, Duen Chau, Minsuk\n  Kahng, U Kang", "title": "M-Flash: Fast Billion-scale Graph Computation Using a Bimodal Block\n  Processing Model", "comments": "Hugo Gualdron, Robson Cordeiro, Jose Rodrigues-Jr, Duen Chau, Minsuk\n  Kahng, U Kang (2016) M-Flash: Fast Billion-scale Graph Computation Using a\n  Bimodal Block Processing Model, In: ECML-PKDD16, pages 623-640, LNCS,\n  Springer", "journal-ref": null, "doi": "10.1007/978-3-319-46227-1_39", "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent graph computation approaches have demonstrated that a single PC can\nperform efficiently on billion-scale graphs. While these approaches achieve\nscalability by optimizing I/O operations, they do not fully exploit the\ncapabilities of modern hard drives and processors. To overcome their\nperformance, in this work, we introduce the Bimodal Block Processing (BBP), an\ninnovation that is able to boost the graph computation by minimizing the I/O\ncost even further. With this strategy, we achieved the following contributions:\n(1) M-Flash, the fastest graph computation framework to date; (2) a flexible\nand simple programming model to easily implement popular and essential graph\nalgorithms, including the first single-machine billion-scale eigensolver; and\n(3) extensive experiments on real graphs with up to 6.6 billion edges,\ndemonstrating M-Flash's consistent and significant speedup.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 20:56:30 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 17:38:52 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 16:04:40 GMT"}, {"version": "v4", "created": "Tue, 5 Jul 2016 19:01:36 GMT"}, {"version": "v5", "created": "Wed, 14 Sep 2016 20:26:33 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Gualdron", "Hugo", ""], ["Cordeiro", "Robson", ""], ["Rodrigues-Jr", "Jose", ""], ["Chau", "Duen", ""], ["Kahng", "Minsuk", ""], ["Kang", "U", ""]]}, {"id": "1506.01442", "submitter": "Sumit Ganguly", "authors": "Sumit Ganguly", "title": "Taylor Polynomial Estimator for Estimating Frequency Moments", "comments": "Supercedes arXiv:1104.4552. Extended Abstract of this paper to appear\n  in Proceedings of ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized algorithm for estimating the $p$th moment $F_p$ of\nthe frequency vector of a data stream in the general update (turnstile) model\nto within a multiplicative factor of $1 \\pm \\epsilon$, for $p > 2$, with high\nconstant confidence. For $0 < \\epsilon \\le 1$, the algorithm uses space $O(\nn^{1-2/p} \\epsilon^{-2} + n^{1-2/p} \\epsilon^{-4/p} \\log (n))$ words. This\nimproves over the current bound of $O(n^{1-2/p} \\epsilon^{-2-4/p} \\log (n))$\nwords by Andoni et. al. in \\cite{ako:arxiv10}. Our space upper bound matches\nthe lower bound of Li and Woodruff \\cite{liwood:random13} for $\\epsilon = (\\log\n(n))^{-\\Omega(1)}$ and the lower bound of Andoni et. al. \\cite{anpw:icalp13}\nfor $\\epsilon = \\Omega(1)$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 01:37:09 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Ganguly", "Sumit", ""]]}, {"id": "1506.01603", "submitter": "Sebastien Tixeuil", "authors": "Pierre Courtieu (CEDRIC), Lionel Rieg, S\\'ebastien Tixeuil (NPA,\n  LINCS, IUF, LIP6), Xavier Urbain (ENSIIE, LRI, CEDRIC)", "title": "A Certified Universal Gathering Algorithm for Oblivious Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DS cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for the problem of universal gathering mobile\noblivious robots (that is, starting from any initial configuration that is not\nbivalent, using any number of robots, the robots reach in a finite number of\nsteps the same position, not known beforehand) without relying on a common\nchirality. We give very strong guaranties on the correctness of our algorithm\nby proving formally that it is correct, using the COQ proof assistant. To our\nknowledge, this is the first certified positive (and constructive) result in\nthe context of oblivious mobile robots. It demonstrates both the effectiveness\nof the approach to obtain new algorithms that are truly generic, and its\nmanagability since the amount of developped code remains human readable.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 14:23:19 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Courtieu", "Pierre", "", "CEDRIC"], ["Rieg", "Lionel", "", "NPA,\n  LINCS, IUF, LIP6"], ["Tixeuil", "S\u00e9bastien", "", "NPA,\n  LINCS, IUF, LIP6"], ["Urbain", "Xavier", "", "ENSIIE, LRI, CEDRIC"]]}, {"id": "1506.01695", "submitter": "Murali Krishna Enduri", "authors": "Bireswar Das, Murali Krishna Enduri and I. Vinod Reddy", "title": "Polynomial-time Algorithm for Isomorphism of Graphs with Clique-width at\n  most Three", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clique-width is a measure of complexity of decomposing graphs into\ncertain tree-like structures. The class of graphs with bounded clique-width\ncontains bounded tree-width graphs. We give a polynomial time graph isomorphism\nalgorithm for graphs with clique-width at most three. Our work is independent\nof the work by Grohe et al. \\cite{grohe2015isomorphism} showing that the\nisomorphism problem for graphs of bounded clique-width is polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 19:33:13 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 13:57:50 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Das", "Bireswar", ""], ["Enduri", "Murali Krishna", ""], ["Reddy", "I. Vinod", ""]]}, {"id": "1506.01747", "submitter": "Gregory Vorsanger", "authors": "Vladimir Braverman, Rafail Ostrovsky, and Gregory Vorsanger", "title": "Weighted Sampling Without Replacement from Data Streams", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted sampling without replacement has proved to be a very important tool\nin designing new algorithms. Efraimidis and Spirakis (IPL 2006) presented an\nalgorithm for weighted sampling without replacement from data streams. Their\nalgorithm works under the assumption of precise computations over the interval\n[0,1]. Cohen and Kaplan (VLDB 2008) used similar methods for their bottom-k\nsketches.\n  Efraimidis and Spirakis ask as an open question whether using finite\nprecision arithmetic impacts the accuracy of their algorithm. In this paper we\nshow a method to avoid this problem by providing a precise reduction from\nk-sampling without replacement to k-sampling with replacement. We call the\nresulting method Cascade Sampling.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 23:19:04 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Braverman", "Vladimir", ""], ["Ostrovsky", "Rafail", ""], ["Vorsanger", "Gregory", ""]]}, {"id": "1506.01749", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Metric Dimension Parameterized by Max Leaf Number", "comments": "11 pages, 2 figures; to appear in J. Graph Algorithms & Applications", "journal-ref": "J. Graph Algorithms & Applications 19 (1): 313-323, 2015", "doi": "10.7155/jgaa.00360", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The metric dimension of a graph is the size of the smallest set of vertices\nwhose distances distinguish all pairs of vertices in the graph. We show that\nthis graph invariant may be calculated by an algorithm whose running time is\nlinear in the input graph size, added to a function of the largest possible\nnumber of leaves in a spanning tree of the graph.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 23:23:52 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1506.01799", "submitter": "Amir Abboud", "authors": "Amir Abboud, Virginia Vassilevska Williams, Joshua Wang", "title": "Approximation and Fixed Parameter Subquadratic Algorithms for Radius and\n  Diameter", "comments": "Submitted to FOCS 15 on April 2nd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The radius and diameter are fundamental graph parameters. They are defined as\nthe minimum and maximum of the eccentricities in a graph, respectively, where\nthe eccentricity of a vertex is the largest distance from the vertex to another\nnode. In directed graphs, there are several versions of these problems. For\ninstance, one may choose to define the eccentricity of a node in terms of the\nlargest distance into the node, out of the node, the sum of the two directions\n(i.e. roundtrip) and so on. All versions of diameter and radius can be solved\nvia solving all-pairs shortest paths (APSP), followed by a fast postprocessing\nstep. Solving APSP, however, on $n$-node graphs requires $\\Omega(n^2)$ time\neven in sparse graphs, as one needs to output $n^2$ distances.\n  Motivated by known and new negative results on the impossibility of computing\nthese measures exactly in general graphs in truly subquadratic time, under\nplausible assumptions, we search for \\emph{approximation} and \\emph{fixed\nparameter subquadratic} algorithms, and for reasons why they do not exist.\n  Our results include: - Truly subquadratic approximation algorithms for most\nof the versions of Diameter and Radius with \\emph{optimal} approximation\nguarantees (given truly subquadratic time), under plausible assumptions. In\nparticular, there is a $2$-approximation algorithm for directed Radius with\none-way distances that runs in $\\tilde{O}(m\\sqrt{n})$ time, while a\n$(2-\\delta)$-approximation algorithm in $O(n^{2-\\epsilon})$ time is unlikely. -\nOn graphs with treewidth $k$, we can solve the problems in\n$2^{O(k\\log{k})}n^{1+o(1)}$ time. We show that these algorithms are near\noptimal since even a $(3/2-\\delta)$-approximation algorithm that runs in time\n$2^{o(k)}n^{2-\\epsilon}$ would refute the plausible assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 07:31:38 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Abboud", "Amir", ""], ["Williams", "Virginia Vassilevska", ""], ["Wang", "Joshua", ""]]}, {"id": "1506.01972", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yang Yuan", "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "comments": "improved writing and included more experiments in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical algorithms are found until several years later to outlive the\nconfines in which they were conceived, and continue to be relevant in\nunforeseen settings. In this paper, we show that SVRG is one such method: being\noriginally designed for strongly convex objectives, it is also very robust in\nnon-strongly convex or sum-of-non-convex settings.\n  More precisely, we provide new analysis to improve the state-of-the-art\nrunning times in both settings by either applying SVRG or its novel variant.\nSince non-strongly convex objectives include important examples such as Lasso\nor logistic regression, and sum-of-non-convex objectives include famous\nexamples such as stochastic PCA and is even believed to be related to training\ndeep neural nets, our results also imply better performances in these\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 17:00:43 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 20:55:39 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:14:20 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Yuan", "Yang", ""]]}, {"id": "1506.02162", "submitter": "Shahin Jabbari", "authors": "Shahin Jabbari, Ryan Rogers, Aaron Roth, Zhiwei Steven Wu", "title": "Learning from Rational Behavior: Predicting Solutions to Unknown Linear\n  Programs", "comments": "The short version of this paper appears in the proceedings of NIPS-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the problem of predicting the solution to a linear\nprogram (LP) given only partial information about its objective and\nconstraints. This generalizes the problem of learning to predict the purchasing\nbehavior of a rational agent who has an unknown objective function, that has\nbeen studied under the name \"Learning from Revealed Preferences\". We give\nmistake bound learning algorithms in two settings: in the first, the objective\nof the LP is known to the learner but there is an arbitrary, fixed set of\nconstraints which are unknown. Each example is defined by an additional known\nconstraint and the goal of the learner is to predict the optimal solution of\nthe LP given the union of the known and unknown constraints. This models the\nproblem of predicting the behavior of a rational agent whose goals are known,\nbut whose resources are unknown. In the second setting, the objective of the LP\nis unknown, and changing in a controlled way. The constraints of the LP may\nalso change every day, but are known. An example is given by a set of\nconstraints and partial information about the objective, and the task of the\nlearner is again to predict the optimal solution of the partially known LP.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 15:10:25 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 20:07:43 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:41:01 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Jabbari", "Shahin", ""], ["Rogers", "Ryan", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1506.02227", "submitter": "Peter Richtarik", "authors": "Dominik Csiba and Peter Richt\\'arik", "title": "Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex\n  Losses", "comments": "13 pages, 3 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a new algorithm for regularized empirical risk\nminimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],\nwhich enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.\nMoreover, our method is able to better utilize the information in the data\ndefining the ERM problem. For convex loss functions, our complexity results\nmatch those of QUARTZ, which is a primal-dual method also allowing for\narbitrary mini-batching schemes. The advantage of a dual-free analysis comes\nfrom the fact that it guarantees convergence even for non-convex loss\nfunctions, as long as the average loss is convex. We illustrate through\nexperiments the utility of being able to design arbitrary mini-batching\nschemes.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 06:45:55 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Csiba", "Dominik", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1506.02243", "submitter": "Ioannis Papoutsakis", "authors": "Ioannis Papoutsakis", "title": "On approximating tree spanners that are breadth first search trees", "comments": null, "journal-ref": "Journal of Computer and System Sciences 82 (2016) 817-825", "doi": "10.1016/j.jcss.2016.02.008", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree $t$-spanner $T$ of a graph $G$ is a spanning tree of $G$ such that the\ndistance in $T$ between every pair of verices is at most $t$ times the distance\nin $G$ between them. There are efficient algorithms that find a tree $t\\cdot\nO(\\log n)$-spanner of a graph $G$, when $G$ admits a tree $t$-spanner. In this\npaper, the search space is narrowed to $v$-concentrated spanning trees, a\nsimple family that includes all the breadth first search trees starting from\nvertex $v$. In this case, it is not easy to find approximate tree spanners\nwithin factor almost $o(\\log n)$. Specifically, let $m$ and $t$ be integers,\nsuch that $m>0$ and $t\\geq 7$. If there is an efficient algorithm that receives\nas input a graph $G$ and a vertex $v$ and returns a $v$-concentrated tree\n$t\\cdot o((\\log n)^{m/(m+1)})$-spanner of $G$, when $G$ admits a\n$v$-concentrated tree $t$-spanner, then there is an algorithm that decides\n3-SAT in quasi-polynomial time.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 09:24:09 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2015 04:57:23 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Papoutsakis", "Ioannis", ""]]}, {"id": "1506.02424", "submitter": "Yue Wang", "authors": "Yue Wang and Zhongkai Zhao", "title": "Algorithms for finding transposons in gene sequences", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the process of evolution, some genes will change their relative\npositions in gene sequence. These \"jumping genes\" are called transposons.\nThrough some intuitive rules, we give a criterion to determine transposons\namong gene sequences of different individuals of the same species. Then we turn\nthis problem into graph theory and give algorithms for different situations\nwith acceptable time complexities. One of these algorithms has been reported\nbriefly as the \"iteration algorithm\" in Kang et al.'s paper (in this paper,\ntransposon is called \"core-gene-defined genome organizational framework\",\ncGOF). This paper provides the omitted details and discussions on general\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 09:58:19 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Yue", ""], ["Zhao", "Zhongkai", ""]]}, {"id": "1506.02574", "submitter": "Olivia Simpson", "authors": "Olivia Simpson and C. Seshadhri and Andrew McGregor", "title": "Catching the head, tail, and everything in between: a streaming\n  algorithm for the degree distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree distribution is one of the most fundamental graph properties of\ninterest for real-world graphs. It has been widely observed in numerous domains\nthat graphs typically have a tailed or scale-free degree distribution. While\nthe average degree is usually quite small, the variance is quite high and there\nare vertices with degrees at all scales. We focus on the problem of\napproximating the degree distribution of a large streaming graph, with small\nstorage. We design an algorithm headtail, whose main novelty is a new estimator\nof infrequent degrees using truncated geometric random variables. We give a\nmathematical analysis of headtail and show that it has excellent behavior in\npractice. We can process streams will millions of edges with storage less than\n1% and get extremely accurate approximations for all scales in the degree\ndistribution.\n  We also introduce a new notion of Relative Hausdorff distance between tailed\nhistograms. Existing notions of distances between distributions are not\nsuitable, since they ignore infrequent degrees in the tail. The Relative\nHausdorff distance measures deviations at all scales, and is a more suitable\ndistance for comparing degree distributions. By tracking this new measure, we\nare able to give strong empirical evidence of the convergence of headtail.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 16:27:25 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 01:13:29 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Simpson", "Olivia", ""], ["Seshadhri", "C.", ""], ["McGregor", "Andrew", ""]]}, {"id": "1506.02629", "submitter": "Vitaly Feldman", "authors": "Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer\n  Reingold, Aaron Roth", "title": "Generalization in Adaptive Data Analysis and Holdout Reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting is the bane of data analysts, even when data are plentiful.\nFormal approaches to understanding this problem focus on statistical inference\nand generalization of individual analysis procedures. Yet the practice of data\nanalysis is an inherently interactive and adaptive process: new analyses and\nhypotheses are proposed after seeing the results of previous ones, parameters\nare tuned on the basis of obtained results, and datasets are shared and reused.\nAn investigation of this gap has recently been initiated by the authors in\n(Dwork et al., 2014), where we focused on the problem of estimating\nexpectations of adaptively chosen functions.\n  In this paper, we give a simple and practical method for reusing a holdout\n(or testing) set to validate the accuracy of hypotheses produced by a learning\nalgorithm operating on a training set. Reusing a holdout set adaptively\nmultiple times can easily lead to overfitting to the holdout set itself. We\ngive an algorithm that enables the validation of a large number of adaptively\nchosen hypotheses, while provably avoiding overfitting. We illustrate the\nadvantages of our algorithm over the standard use of the holdout set via a\nsimple synthetic experiment.\n  We also formalize and address the general problem of data reuse in adaptive\ndata analysis. We show how the differential-privacy based approach given in\n(Dwork et al., 2014) is applicable much more broadly to adaptive data analysis.\nWe then show that a simple approach based on description length can also be\nused to give guarantees of statistical validity in adaptive settings. Finally,\nwe demonstrate that these incomparable approaches can be unified via the notion\nof approximate max-information that we introduce.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:34:29 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 19:04:32 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Dwork", "Cynthia", ""], ["Feldman", "Vitaly", ""], ["Hardt", "Moritz", ""], ["Pitassi", "Toniann", ""], ["Reingold", "Omer", ""], ["Roth", "Aaron", ""]]}, {"id": "1506.02717", "submitter": "Paul Kirchner", "authors": "Paul Kirchner and Pierre-Alain Fouque", "title": "An Improved BKW Algorithm for LWE with Applications to Cryptography and\n  Lattices", "comments": "CRYPTO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Learning With Errors problem and its binary\nvariant, where secrets and errors are binary or taken in a small interval. We\nintroduce a new variant of the Blum, Kalai and Wasserman algorithm, relying on\na quantization step that generalizes and fine-tunes modulus switching. In\ngeneral this new technique yields a significant gain in the constant in front\nof the exponent in the overall complexity. We illustrate this by solving p\nwithin half a day a LWE instance with dimension n = 128, modulus $q = n^2$,\nGaussian noise $\\alpha = 1/(\\sqrt{n/\\pi} \\log^2 n)$ and binary secret, using\n$2^{28}$ samples, while the previous best result based on BKW claims a time\ncomplexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We then\nintroduce variants of BDD, GapSVP and UniqueSVP, where the target point is\nrequired to lie in the fundamental parallelepiped, and show how the previous\nalgorithm is able to solve these variants in subexponential time. Moreover, we\nalso show how the previous algorithm can be used to solve the BinaryLWE problem\nwith n samples in subexponential time $2^{(\\ln 2/2+o(1))n/\\log \\log n}$. This\nanalysis does not require any heuristic assumption, contrary to other algebraic\napproaches; instead, it uses a variant of an idea by Lyubashevsky to generate\nmany samples from a small number of samples. This makes it possible to\nasymptotically and heuristically break the NTRU cryptosystem in subexponential\ntime (without contradicting its security assumption). We are also able to solve\nsubset sum problems in subexponential time for density $o(1)$, which is of\nindependent interest: for such density, the previous best algorithm requires\nexponential time. As a direct application, we can solve in subexponential time\nthe parameters of a cryptosystem based on this problem proposed at TCC 2010.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 22:22:25 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 19:53:27 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 22:16:33 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2015 23:24:59 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Kirchner", "Paul", ""], ["Fouque", "Pierre-Alain", ""]]}, {"id": "1506.02973", "submitter": "Adam Karczmarz", "authors": "Adam Karczmarz and Jakub {\\L}\\k{a}cki", "title": "Fast and simple connectivity in graph timelines", "comments": "21 pages, extended abstract to appear in WADS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of answering connectivity queries about a\n\\emph{graph timeline}. A graph timeline is a sequence of undirected graphs\n$G_1,\\ldots,G_t$ on a common set of vertices of size $n$ such that each graph\nis obtained from the previous one by an addition or a deletion of a single\nedge. We present data structures, which preprocess the timeline and can answer\nthe following queries:\n  - forall$(u,v,a,b)$ -- does the path $u\\to v$ exist in each of\n$G_a,\\ldots,G_b$?\n  - exists$(u,v,a,b)$ -- does the path $u\\to v$ exist in any of\n$G_a,\\ldots,G_b$?\n  - forall2$(u,v,a,b)$ -- do there exist two edge-disjoint paths connecting $u$\nand $v$ in each of $G_a,\\ldots,G_b$\n  We show data structures that can answer forall and forall2 queries in $O(\\log\nn)$ time after preprocessing in $O(m+t\\log n)$ time. Here by $m$ we denote the\nnumber of edges that remain unchanged in each graph of the timeline. For the\ncase of exists queries, we show how to extend an existing data structure to\nobtain a preprocessing/query trade-off of $\\langle O(m+\\min(nt, t^{2-\\alpha})),\nO(t^\\alpha)\\rangle$ and show a matching conditional lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 15:58:59 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Karczmarz", "Adam", ""], ["\u0141\u0105cki", "Jakub", ""]]}, {"id": "1506.03004", "submitter": "Xiaotian Wang", "authors": "Yingjie Guo, Linzhi Wu, Wei Yu, Bin Wu, Xiaotian Wang", "title": "The Improved Job Scheduling Algorithm of Hadoop Platform", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper discussed some job scheduling algorithms for Hadoop platform, and\nproposed a jobs scheduling optimization algorithm based on Bayes Classification\nviewing the shortcoming of those algorithms which are used. The proposed\nalgorithm can be summarized as follows. In the scheduling algorithm based on\nBayes Classification, the jobs in job queue will be classified into bad job and\ngood job by Bayes Classification, when JobTracker gets task request, it will\nselect a good job from job queue, and select tasks from good job to allocate\nJobTracker, then the execution result will feedback to the JobTracker.\nTherefore the scheduling algorithm based on Bayes Classification influence the\njob classification via learning the result of feedback with the JobTracker will\nselect the most appropriate job to execute on TaskTracker every time. We need\nto consider the feature usage of job resource and the influence of TaskTracker\nresource on task execution, the former of which we call it job feature, for\ninstance, the average usage rate of CPU and average usage rate of memory, the\nlatter node feature, such as the usage rate of CPU and the size of idle\nphysical memory, the two are called feature variables. Results show that it has\na significant improvement in execution efficiency and stability of job\nscheduling.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 17:00:52 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Guo", "Yingjie", ""], ["Wu", "Linzhi", ""], ["Yu", "Wei", ""], ["Wu", "Bin", ""], ["Wang", "Xiaotian", ""]]}, {"id": "1506.03137", "submitter": "Tselil Schramm", "authors": "Tselil Schramm and Benjamin Weitz", "title": "Symmetric Tensor Completion from Multilinear Entries and Learning\n  Product Mixtures over the Hypercube", "comments": "Removed adversarial matrix completion algorithm after discovering\n  that our matrix completion results can be derived from prior work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for completing an order-$m$ symmetric low-rank tensor\nfrom its multilinear entries in time roughly proportional to the number of\ntensor entries. We apply our tensor completion algorithm to the problem of\nlearning mixtures of product distributions over the hypercube, obtaining new\nalgorithmic results. If the centers of the product distribution are linearly\nindependent, then we recover distributions with as many as $\\Omega(n)$ centers\nin polynomial time and sample complexity. In the general case, we recover\ndistributions with as many as $\\tilde\\Omega(n)$ centers in quasi-polynomial\ntime, answering an open problem of Feldman et al. (SIAM J. Comp.) for the\nspecial case of distributions with incoherent bias vectors.\n  Our main algorithmic tool is the iterated application of a low-rank matrix\ncompletion algorithm for matrices with adversarially missing entries.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 23:53:42 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 22:54:20 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 21:32:41 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Schramm", "Tselil", ""], ["Weitz", "Benjamin", ""]]}, {"id": "1506.03163", "submitter": "Leonid Boytsov", "authors": "Bilegsaikhan Naidan, Leonid Boytsov, Eric Nyberg", "title": "Permutation Search Methods are Efficient, Yet Faster Search is Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey permutation-based methods for approximate k-nearest neighbor\nsearch. In these methods, every data point is represented by a ranked list of\npivots sorted by the distance to this point. Such ranked lists are called\npermutations. The underpinning assumption is that, for both metric and\nnon-metric spaces, the distance between permutations is a good proxy for the\ndistance between original points. Thus, it should be possible to efficiently\nretrieve most true nearest neighbors by examining only a tiny subset of data\npoints whose permutations are similar to the permutation of a query. We further\ntest this assumption by carrying out an extensive experimental evaluation where\npermutation methods are pitted against state-of-the art benchmarks (the\nmulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety\nof realistically large data set from the image and textual domain. The focus is\non the high-accuracy retrieval methods for generic spaces. Additionally, we\nassume that both data and indices are stored in main memory. We find\npermutation methods to be reasonably efficient and describe a setup where these\nmethods are most useful. To ease reproducibility, we make our software and data\nsets publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:50:29 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 10:21:06 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 20:35:03 GMT"}, {"version": "v4", "created": "Mon, 31 Oct 2016 18:50:48 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Naidan", "Bilegsaikhan", ""], ["Boytsov", "Leonid", ""], ["Nyberg", "Eric", ""]]}, {"id": "1506.03262", "submitter": "Travis Gagie", "authors": "Christina Boucher, Alexander Bowe, Travis Gagie, Giovanni Manzini and\n  Jouni Sir\\'en", "title": "Relative Select", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of storing coloured de Bruijn graphs, we show how,\nif we can already support fast select queries on one string, then we can store\na little extra information and support fairly fast select queries on a similar\nstring.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 11:38:11 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Boucher", "Christina", ""], ["Bowe", "Alexander", ""], ["Gagie", "Travis", ""], ["Manzini", "Giovanni", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1506.03282", "submitter": "Guillerme Duvilli\\'e", "authors": "Marin Bougeret, Guillerme Duvilli\\'e, Rodolphe Giroudeau, R\\'emi\n  Watrigant", "title": "Multidimensional Binary Vector Assignment problem: standard, structural\n  and above guarantee parameterizations", "comments": "16 pages, 6 figures", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  4, FCT '15, special issue FCT'15 (December 20, 2017) dmtcs:4150", "doi": "10.23638/DMTCS-19-4-3", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we focus on the parameterized complexity of the\nMultidimensional Binary Vector Assignment problem (called \\BVA). An input of\nthis problem is defined by $m$ disjoint sets $V^1, V^2, \\dots, V^m$, each\ncomposed of $n$ binary vectors of size $p$. An output is a set of $n$ disjoint\n$m$-tuples of vectors, where each $m$-tuple is obtained by picking one vector\nfrom each set $V^i$. To each $m$-tuple we associate a $p$ dimensional vector by\napplying the bit-wise AND operation on the $m$ vectors of the tuple. The\nobjective is to minimize the total number of zeros in these $n$ vectors. mBVA\ncan be seen as a variant of multidimensional matching where hyperedges are\nimplicitly locally encoded via labels attached to vertices, but was originally\nintroduced in the context of integrated circuit manufacturing.\n  We provide for this problem FPT algorithms and negative results ($ETH$-based\nresults, $W$[2]-hardness and a kernel lower bound) according to several\nparameters: the standard parameter $k$ i.e. the total number of zeros), as well\nas two parameters above some guaranteed values.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 12:58:35 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 10:42:48 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 14:44:58 GMT"}, {"version": "v4", "created": "Sat, 16 Dec 2017 01:02:49 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Bougeret", "Marin", ""], ["Duvilli\u00e9", "Guillerme", ""], ["Giroudeau", "Rodolphe", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1506.03473", "submitter": "Holger Petersen", "authors": "Holger Petersen", "title": "A Non-Oblivious Reduction of Counting Ones to Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm counting the number of ones in a binary word is presented\nrunning in time $O(\\log\\log b)$ where $b$ is the number of ones. The operations\navailable include bit-wise logical operations and multiplication.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 20:39:32 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Petersen", "Holger", ""]]}, {"id": "1506.03489", "submitter": "Rachel Cummings", "authors": "Rachel Cummings, Stratis Ioannidis, and Katrina Ligett", "title": "Truthful Linear Regression", "comments": "To appear in Proceedings of the 28th Annual Conference on Learning\n  Theory (COLT 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting a linear model to data held by individuals\nwho are concerned about their privacy. Incentivizing most players to truthfully\nreport their data to the analyst constrains our design to mechanisms that\nprovide a privacy guarantee to the participants; we use differential privacy to\nmodel individuals' privacy losses. This immediately poses a problem, as\ndifferentially private computation of a linear model necessarily produces a\nbiased estimation, and existing approaches to design mechanisms to elicit data\nfrom privacy-sensitive individuals do not generalize well to biased estimators.\nWe overcome this challenge through an appropriate design of the computation and\npayment scheme.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:39:44 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Cummings", "Rachel", ""], ["Ioannidis", "Stratis", ""], ["Ligett", "Katrina", ""]]}, {"id": "1506.03521", "submitter": "Samet Oymak", "authors": "Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi", "title": "Isometric sketching of any set via the Restricted Isometry Property", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that for the purposes of dimensionality reduction\ncertain class of structured random matrices behave similarly to random Gaussian\nmatrices. This class includes several matrices for which matrix-vector multiply\ncan be computed in log-linear time, providing efficient dimensionality\nreduction of general sets. In particular, we show that using such matrices any\nset from high dimensions can be embedded into lower dimensions with near\noptimal distortion. We obtain our results by connecting dimensionality\nreduction of any set to dimensionality reduction of sparse vectors via a\nchaining argument.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 00:49:51 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 20:09:41 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Oymak", "Samet", ""], ["Recht", "Benjamin", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1506.03528", "submitter": "Kevin A. Lai", "authors": "Mahdi Amani, Kevin A. Lai, and Robert E. Tarjan", "title": "Amortized Rotation Cost in AVL Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An AVL tree is the original type of balanced binary search tree. An insertion\nin an $n$-node AVL tree takes at most two rotations, but a deletion in an\n$n$-node AVL tree can take $\\Theta(\\log n)$. A natural question is whether\ndeletions can take many rotations not only in the worst case but in the\namortized case as well. A sequence of $n$ successive deletions in an $n$-node\ntree takes $O(n)$ rotations, but what happens when insertions are intermixed\nwith deletions? Heaupler, Sen, and Tarjan conjectured that alternating\ninsertions and deletions in an $n$-node AVL tree can cause each deletion to do\n$\\Omega(\\log n)$ rotations, but they provided no construction to justify their\nclaim. We provide such a construction: we show that, for infinitely many $n$,\nthere is a set $E$ of {\\it expensive} $n$-node AVL trees with the property\nthat, given any tree in $E$, deleting a certain leaf and then reinserting it\nproduces a tree in $E$, with the deletion having done $\\Theta(\\log n)$\nrotations. One can do an arbitrary number of such expensive deletion-insertion\npairs. The difficulty in obtaining such a construction is that in general the\ntree produced by an expensive deletion-insertion pair is not the original tree.\nIndeed, if the trees in $E$ have even height $k$, $2^{k/2}$ deletion-insertion\npairs are required to reproduce the original tree.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 01:50:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Amani", "Mahdi", ""], ["Lai", "Kevin A.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1506.03676", "submitter": "Tobias Christiani", "authors": "Tobias Christiani, Rasmus Pagh, Mikkel Thorup", "title": "From Independence to Expansion and Back Again", "comments": "An extended abstract of this paper was accepted to The 47th ACM\n  Symposium on Theory of Computing (STOC 2015). Copyright ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following fundamental problems: (1) Constructing\n$k$-independent hash functions with a space-time tradeoff close to Siegel's\nlower bound. (2) Constructing representations of unbalanced expander graphs\nhaving small size and allowing fast computation of the neighbor function. It is\nnot hard to show that these problems are intimately connected in the sense that\na good solution to one of them leads to a good solution to the other one. In\nthis paper we exploit this connection to present efficient, recursive\nconstructions of $k$-independent hash functions (and hence expanders with a\nsmall representation). While the previously most efficient construction\n(Thorup, FOCS 2013) needed time quasipolynomial in Siegel's lower bound, our\ntime bound is just a logarithmic factor from the lower bound.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 13:59:45 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Christiani", "Tobias", ""], ["Pagh", "Rasmus", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1506.03760", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Hossein Esfandiari, MohammadTaghi Hajiaghayi, Rohit\n  Khandekar, Guy Kortsarz, Saeed Seddighin", "title": "A Tight Algorithm for Strongly Connected Steiner Subgraph On Two\n  Terminals With Demands", "comments": "To appear in Algorithmica. An extended abstract appeared in IPEC '14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an edge-weighted directed graph $G=(V,E)$ on $n$ vertices and a set\n$T=\\{t_1, t_2, \\ldots, t_p\\}$ of $p$ terminals, the objective of the \\scss\n($p$-SCSS) problem is to find an edge set $H\\subseteq E$ of minimum weight such\nthat $G[H]$ contains an $t_{i}\\rightarrow t_j$ path for each $1\\leq i\\neq j\\leq\np$. In this paper, we investigate the computational complexity of a variant of\n$2$-SCSS where we have demands for the number of paths between each terminal\npair. Formally, the \\sharinggeneral problem is defined as follows: given an\nedge-weighted directed graph $G=(V,E)$ with weight function $\\omega:\nE\\rightarrow \\mathbb{R}^{\\geq 0}$, two terminal vertices $s, t$, and integers\n$k_1, k_2$ ; the objective is to find a set of $k_1$ paths $F_1, F_2, \\ldots,\nF_{k_1}$ from $s\\leadsto t$ and $k_2$ paths $B_1, B_2, \\ldots, B_{k_2}$ from\n$t\\leadsto s$ such that $\\sum_{e\\in E} \\omega(e)\\cdot \\phi(e)$ is minimized,\nwhere $\\phi(e)= \\max \\Big\\{|\\{i\\in [k_1] : e\\in F_i\\}|\\ ,\\ |\\{j\\in [k_2] : e\\in\nB_j\\}|\\Big\\}$. For each $k\\geq 1$, we show the following: The \\sharing problem\ncan be solved in $n^{O(k)}$ time. A matching lower bound for our algorithm: the\n\\sharing problem does not have an $f(k)\\cdot n^{o(k)}$ algorithm for any\ncomputable function $f$, unless the Exponential Time Hypothesis (ETH) fails.\n  Our algorithm for \\sharing relies on a structural result regarding an optimal\nsolution followed by using the idea of a \"token game\" similar to that of\nFeldman and Ruhl. We show with an example that the structural result does not\nhold for the \\sharinggeneral problem if $\\min\\{k_1, k_2\\}\\geq 2$. Therefore\n\\sharing is the most general problem one can attempt to solve with our\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:06:42 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 16:10:08 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Esfandiari", "Hossein", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Khandekar", "Rohit", ""], ["Kortsarz", "Guy", ""], ["Seddighin", "Saeed", ""]]}, {"id": "1506.03872", "submitter": "C. Seshadhri", "authors": "Grey Ballard and Ali Pinar and Tamara G. Kolda and C. Seshadhri", "title": "Diamond Sampling for Approximate Maximum All-pairs Dot-product (MAD)\n  Search", "comments": null, "journal-ref": "ICDM 2015: Proceedings of the 2015 IEEE International Conference\n  on Data Mining, pp. 11-20, November 2015", "doi": "10.1109/ICDM.2015.46", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of vectors, $A = \\{{a_1}, \\dots, {a_m}\\}$ and\n$B=\\{{b_1},\\dots,{b_n}\\}$, our problem is to find the top-$t$ dot products,\ni.e., the largest $|{a_i}\\cdot{b_j}|$ among all possible pairs. This is a\nfundamental mathematical problem that appears in numerous data applications\ninvolving similarity search, link prediction, and collaborative filtering. We\npropose a sampling-based approach that avoids direct computation of all $mn$\ndot products. We select diamonds (i.e., four-cycles) from the weighted\ntripartite representation of $A$ and $B$. The probability of selecting a\ndiamond corresponding to pair $(i,j)$ is proportional to $({a_i}\\cdot{b_j})^2$,\namplifying the focus on the largest-magnitude entries. Experimental results\nindicate that diamond sampling is orders of magnitude faster than direct\ncomputation and requires far fewer samples than any competing approach. We also\napply diamond sampling to the special case of maximum inner product search, and\nget significantly better results than the state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 23:45:36 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 20:51:30 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 00:39:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Ballard", "Grey", ""], ["Pinar", "Ali", ""], ["Kolda", "Tamara G.", ""], ["Seshadhri", "C.", ""]]}, {"id": "1506.03935", "submitter": "Abolfazl Ramezanpour", "authors": "A. Ramezanpour and S. Moghimi-Araghi", "title": "Statistical physics of loopy interactions: Independent-loop\n  approximation and beyond", "comments": "21 pages, 6 figures, 2 appendices, typos corrected", "journal-ref": "Phys. Rev. E 92, 032112 (2015)", "doi": "10.1103/PhysRevE.92.032112", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an interacting system of spin variables on a loopy interaction\ngraph, identified by a tree graph and a set of loopy interactions. We start\nfrom a high-temperature expansion for loopy interactions represented by a sum\nof nonnegative contributions from all the possible frustration-free loop\nconfigurations. We then compute the loop corrections using different\napproximations for the nonlocal loop interactions induced by the spin\ncorrelations in the tree graph. For distant loopy interactions, we can exploit\nthe exponential decay of correlations in the tree interaction graph to compute\nloop corrections within an independent-loop approximation. Higher orders of the\napproximation are obtained by considering the correlations between the nearby\nloopy interactions involving larger number of spin variables. In particular,\nthe sum over the loop configurations can be computed \"exactly\" by the belief\npropagation algorithm in the low orders of the approximation as long as the\nloopy interactions have a tree structure. These results might be useful in\ndeveloping more accurate and convergent message-passing algorithms exploiting\nthe structure of loopy interactions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 08:34:58 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 06:43:52 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Ramezanpour", "A.", ""], ["Moghimi-Araghi", "S.", ""]]}, {"id": "1506.04220", "submitter": "Drona Pratap Chandu", "authors": "Drona Pratap Chandu", "title": "Improved Greedy Algorithm for Set Covering Problem", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a greedy algorithm named as Big step greedy set cover\nalgorithm to compute approximate minimum set cover. The Big step greedy\nalgorithm, in each step selects p sets such that the union of selected p sets\ncontains greatest number of uncovered elements and adds the selected p sets to\npartial set cover. The process of adding p sets is repeated until all the\nelements are covered. When p=1 it behaves like the classical greedy algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 03:59:24 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Chandu", "Drona Pratap", ""]]}, {"id": "1506.04383", "submitter": "Christian Schulz", "authors": "Henning Meyerhenke, Martin N\\\"ollenburg, Christian Schulz", "title": "Drawing Large Graphs by Multilevel Maxent-Stress Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing large graphs appropriately is an important step for the visual\nanalysis of data from real-world networks. Here we present a novel multilevel\nalgorithm to compute a graph layout with respect to a recently proposed metric\nthat combines layout stress and entropy. As opposed to previous work, we do not\nsolve the linear systems of the maxent-stress metric with a typical numerical\nsolver. Instead we use a simple local iterative scheme within a multilevel\napproach. To accelerate local optimization, we approximate long-range forces\nand use shared-memory parallelism. Our experiments validate the high potential\nof our approach, which is particularly appealing for dynamic graphs. In\ncomparison to the previously best maxent-stress optimizer, which is sequential,\nour parallel implementation is on average 30 times faster already for static\ngraphs (and still faster if executed on one thread) while producing a\ncomparable solution quality.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 11:53:36 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 04:33:40 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Meyerhenke", "Henning", ""], ["N\u00f6llenburg", "Martin", ""], ["Schulz", "Christian", ""]]}, {"id": "1506.04417", "submitter": "Andrew McGregor", "authors": "Andrew McGregor and David Tench and Sofya Vorotnikova and Hoa T. Vu", "title": "Densest Subgraph in Dynamic Graph Streams", "comments": "To appear in MFCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of approximating the densest subgraph\nin the dynamic graph stream model. In this model of computation, the input\ngraph is defined by an arbitrary sequence of edge insertions and deletions and\nthe goal is to analyze properties of the resulting graph given memory that is\nsub-linear in the size of the stream. We present a single-pass algorithm that\nreturns a $(1+\\epsilon)$ approximation of the maximum density with high\nprobability; the algorithm uses $O(\\epsilon^{-2} n \\polylog n)$ space,\nprocesses each stream update in $\\polylog (n)$ time, and uses $\\poly(n)$\npost-processing time where $n$ is the number of nodes. The space used by our\nalgorithm matches the lower bound of Bahmani et al.~(PVLDB 2012) up to a\npoly-logarithmic factor for constant $\\epsilon$. The best existing results for\nthis problem were established recently by Bhattacharya et al.~(STOC 2015). They\npresented a $(2+\\epsilon)$ approximation algorithm using similar space and\nanother algorithm that both processed each update and maintained a\n$(4+\\epsilon)$ approximation of the current maximum density in $\\polylog (n)$\ntime per-update.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 16:32:06 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["McGregor", "Andrew", ""], ["Tench", "David", ""], ["Vorotnikova", "Sofya", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1506.04474", "submitter": "Toshiya Itoh", "authors": "Shun Hasegawa and Toshiya Itoh", "title": "Optimal Online Algorithms for the Multi-Objective Time Series Search\n  Problem", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiedemann, et al. [Proc. of WALCOM, LNCS 8973, 2015, pp.210-221] defined\nmulti-objective online problems (as an online version of multi-objective\noptimization problems) and the competitive analysis for multi-objective online\nproblems and showed that (1) with respect to the worst component competitive\nanalysis, the online algorithm RPP-HIGH is best possible for the\nmulti-objective time series search~problem; (2) with respect to the arithmetic\nmean component competitive analysis, the online algorithm RPP-MULT is best\npossible for the bi-objective time series search problem; (3) with respect to\nthe geometric mean component competitive analysis, the online algorithm\nRPP-MULT is best possible for the bi-objective time series search problem. In\nthis paper, we first point out that the definitions and frameworks of the\ncompetitive analysis due to Tiedemann, et al. do not necessarily capture the\nefficiency of online algorithms for multi-objective online problems and provide\nmodified definitions of the competitive analysis for multi-objective online\nproblems. Then under the modified framework, we present a simple online\nalgorithm Balanced Price Policy BPP_{k} for the multi-objective (k-objective)\ntime series search problem, and show that the algorithm BPP_{k} is best\npossible with respect to any measure of the competitive analysis (defined by a\nmonotone continuous function f). Under the modified framework, we derive exact\nvalues of the competitive ratio for the multi-objective time series search\nproblem with respect to the worst component competitive analysis, the\narithmetic mean component competitive analysis, and the geometric mean\ncomponent competitive analysis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 04:14:31 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 07:06:29 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 03:34:43 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2015 06:40:44 GMT"}, {"version": "v5", "created": "Tue, 8 Mar 2016 04:04:27 GMT"}, {"version": "v6", "created": "Fri, 29 Apr 2016 05:34:10 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Hasegawa", "Shun", ""], ["Itoh", "Toshiya", ""]]}, {"id": "1506.04486", "submitter": "Anas Al-Okaily", "authors": "Anas Al-Okaily", "title": "Error Tree: A Tree Structure for Hamming & Edit Distances & Wildcards\n  Matching", "comments": null, "journal-ref": null, "doi": "10.1089/cmb.2015.0132", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error Tree is a novel tree structure that is mainly oriented to solve the\napproximate pattern matching problems, Hamming and edit distances, as well as\nthe wildcards matching problem. The input is a text of length $n$ over a fixed\nalphabet of length $\\Sigma$, a pattern of length $m$, and $k$. The output is to\nfind all positions that have $\\leq$ $k$ Hamming distance, edit distance, or\nwildcards matching with $P$. The algorithm proposes for Hamming distance and\nwildcards matching a tree structure that needs $O(n\\frac{log_\\Sigma\n^{k}n}{k!})$ words and takes $O(\\frac {m^k}{k!} + occ$)($O(m + \\frac\n{log_\\Sigma ^kn}{k!} + occ$) in the average case) of query time for any\nonline/offline pattern, where $occ$ is the number of outputs. As well, a tree\nstructure of $O(2^{k}n\\frac{log_\\Sigma ^{k}n}{k!})$ words and $O(\\frac\n{m^k}{k!} + 3^{k}occ$)($O(m + \\frac {log_\\Sigma ^kn}{k!} + 3^{k}occ$) in the\naverage case) query time for edit distance for any online/offline pattern.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 06:08:39 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Al-Okaily", "Anas", ""]]}, {"id": "1506.04499", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Rajeev Raman", "title": "Tree Compression with Top Trees Revisited", "comments": "SEA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit tree compression with top trees (Bille et al, ICALP'13) and\npresent several improvements to the compressor and its analysis. By\nsignificantly reducing the amount of information stored and guiding the\ncompression step using a RePair-inspired heuristic, we obtain a fast compressor\nachieving good compression ratios, addressing an open problem posed by Bille et\nal. We show how, with relatively small overhead, the compressed file can be\nconverted into an in-memory representation that supports basic navigation\noperations in worst-case logarithmic time without decompression. We also show a\nmuch improved worst-case bound on the size of the output of top-tree\ncompression (answering an open question posed in a talk on this algorithm by\nWeimann in 2012).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 07:51:29 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Raman", "Rajeev", ""]]}, {"id": "1506.04505", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, MohammadTaghi Hajiaghayi, David P. Woodruff", "title": "Applications of Uniform Sampling: Densest Subgraph and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently [Bhattacharya et al., STOC 2015] provide the first non-trivial\nalgorithm for the densest subgraph problem in the streaming model with\nadditions and deletions to its edges, i.e., for dynamic graph streams. They\npresent a $(0.5-\\epsilon)$-approximation algorithm using $\\tilde{O}(n)$ space,\nwhere factors of $\\epsilon$ and $\\log(n)$ are suppressed in the $\\tilde{O}$\nnotation. However, the update time of this algorithm is large. To remedy this,\nthey also provide a $(0.25-\\epsilon)$-approximation algorithm using\n$\\tilde{O}(n)$ space with update time $\\tilde{O}(1)$.\n  In this paper we improve the algorithms by Bhattacharya et al. by providing a\n$(1-\\epsilon)$-approximation algorithm using $\\tilde{O}(n)$ space. Our\nalgorithm is conceptually simple - it samples $\\tilde{O}(n)$ edges uniformly at\nrandom, and finds the densest subgraph on the sampled graph. We also show how\nto perform this sampling with update time $\\tilde{O}(1)$. In addition to this,\nwe show that given oracle access to the edge set, we can implement our\nalgorithm in time $\\tilde{O}(n)$ on a graph in the standard RAM model. To the\nbest of our knowledge this is the fastest $(0.5-\\epsilon)$-approximation\nalgorithm for the densest subgraph problem in the RAM model given such oracle\naccess.\n  Further, we extend our results to a general class of graph optimization\nproblems that we call heavy subgraph problems. This class contains many\ninteresting problems such as densest subgraph, directed densest subgraph,\ndensest bipartite subgraph, $d$-cut and $d$-heavy connected component. Our\nresult, by characterizing heavy subgraph problems, partially addresses open\nproblem 13 at the IITK Workshop on Algorithms for Data Streams in 2006\nregarding the effects of subsampling in this context.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 08:08:55 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2015 20:18:05 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 05:43:49 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Woodruff", "David P.", ""]]}, {"id": "1506.04559", "submitter": "Fatima Vayani", "authors": "Maxime Crochemore and Costas S. Iliopoulos and Ritu Kundu and Manal\n  Mohamed and Fatima Vayani", "title": "Linear Algorithm for Conservative Degenerate Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A degenerate symbol x* over an alphabet A is a non-empty subset of A, and a\nsequence of such symbols is a degenerate string. A degenerate string is said to\nbe conservative if its number of non-solid symbols is upper-bounded by a fixed\npositive constant k. We consider here the matching problem of conservative\ndegenerate strings and present the first linear-time algorithm that can find,\nfor given degenerate strings P* and T* of total length n containing k non-solid\nsymbols in total, the occurrences of P* in T* in O(nk) time.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 11:45:53 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kundu", "Ritu", ""], ["Mohamed", "Manal", ""], ["Vayani", "Fatima", ""]]}, {"id": "1506.04642", "submitter": "Krasimir Yordzhev", "authors": "Krasimir Yordzhev", "title": "Semi-canonical binary matrices", "comments": "Sixth International Scientific Conference - FMNS2015, South-West\n  University, Blagoevgrad, Bulgaria, 113 - 124", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define the concepts of semi-canonical and canonical binary\nmatrix. Strictly mathematical, we prove the correctness of these definitions.\nWe describe and we implement an algorithm for finding all semi-canonical binary\nmatrices taking into account the number of 1 in each of them. This problem\nrelates to the combinatorial problem of finding all pairs of disjoint\nS-permutation matrices. In the described algorithm, the bit-wise operations are\nsubstantially used.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 15:54:56 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Yordzhev", "Krasimir", ""]]}, {"id": "1506.04838", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Zhenyu Liao and Lorenzo Orecchia", "title": "Spectral Sparsification and Regret Minimization Beyond Matrix\n  Multiplicative Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a novel construction of the linear-sized spectral\nsparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous\nconstructions required $\\Omega(n^4)$ running time [BSS14, Zou12], our\nsparsification routine can be implemented in almost-quadratic running time\n$O(n^{2+\\varepsilon})$.\n  The fundamental conceptual novelty of our work is the leveraging of a strong\nconnection between sparsification and a regret minimization problem over\ndensity matrices. This connection was known to provide an interpretation of the\nrandomized sparsifiers of Spielman and Srivastava [SS11] via the application of\nmatrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we\nexplain how matrix MWU naturally arises as an instance of the\nFollow-the-Regularized-Leader framework and generalize this approach to yield a\nlarger class of updates. This new class allows us to accelerate the\nconstruction of linear-sized spectral sparsifiers, and give novel insights on\nthe motivation behind Batson, Spielman and Srivastava [BSS14].\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 05:31:57 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Liao", "Zhenyu", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1506.04862", "submitter": "Arseny Shur", "authors": "Mikhail Rubinchik and Arseny M. Shur", "title": "EERTREE: An Efficient Data Structure for Processing Palindromes in\n  Strings", "comments": "21 pages, 2 figures. Accepted to IWOCA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new linear-size data structure which provides a fast access to\nall palindromic substrings of a string or a set of strings. This structure\ninherits some ideas from the construction of both the suffix trie and suffix\ntree. Using this structure, we present simple and efficient solutions for a\nnumber of problems involving palindromes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 07:26:33 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 18:39:04 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Rubinchik", "Mikhail", ""], ["Shur", "Arseny M.", ""]]}, {"id": "1506.04867", "submitter": "Siddharth Dawar", "authors": "Siddharth Dawar, Vikram Goyal, Debajyoti Bera", "title": "Efficient Reverse k Nearest Neighbor evaluation for hierarchical index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Reverse Nearest Neighbor\" query finds applications in decision support\nsystems, profile-based marketing, emergency services etc. In this paper, we\npoint out a few flaws in the branch and bound algorithms proposed earlier for\ncomputing monochromatic RkNN queries over data points stored in hierarchical\nindex. We give suitable counter examples to validate our claims and propose a\ncorrect algorithm for the corresponding problem. We show that our algorithm is\ncorrect by identifying necessary conditions behind correctness of algorithms\nfor this problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 08:15:39 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Dawar", "Siddharth", ""], ["Goyal", "Vikram", ""], ["Bera", "Debajyoti", ""]]}, {"id": "1506.04896", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski and Marcin Raniszewski and Sebastian Deorowicz", "title": "FM-index for dummies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FM-index is a celebrated compressed data structure for full-text pattern\nsearching. After the first wave of interest in its theoretical developments, we\ncan observe a surge of interest in practical FM-index variants in the last few\nyears. These enhancements are often related to a bit-vector representation,\naugmented with an efficient rank-handling data structure. In this work, we\npropose a new, cache-friendly, implementation of the rank primitive and\nadvocate for a very simple architecture of the FM-index, which trades\ncompression ratio for speed. Experimental results show that our variants are\n2--3 times faster than the fastest known ones, for the price of using typically\n1.5--5 times more space.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 09:53:29 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 18:51:02 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Grabowski", "Szymon", ""], ["Raniszewski", "Marcin", ""], ["Deorowicz", "Sebastian", ""]]}, {"id": "1506.04910", "submitter": "Ali Sezgin", "authors": "Ali Sezgin", "title": "Sequential Consistency and Concurrent Data Structures", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability, the de facto correctness condition for concurrent data\nstructure implementations, despite its intuitive appeal is known to lead to\npoor scalability. This disadvantage has led researchers to design scalable data\nstructures satisfying consistency conditions weaker than linearizability.\nDespite this recent trend, sequential consistency as a strictly weaker\nconsistency condition than linearizability has received no interest.\n  In this paper, we investigate the applicability of sequential consistency as\nan alternative correctness criterion for concurrent data structure\nimplementations. Our first finding formally justifies the reluctance in moving\ntowards sequentially consistent data structures: Implementations in which each\nthread modifies only its thread-local variables are sequentially consistent for\nvarious standard data structures such as pools, queues and stacks. We also show\nthat for almost all data structures, and the data structures we consider in\nthis paper, it is possible to have sequentially consistent behaviors in which a\ndesignated thread does not synchronize at all. As a potential remedy, we define\na hierarchy of quantitatively strengthened variants of sequential consistency\nsuch that the stronger the variant the more synchronization it enforces which\nat the limit is equal to that enforced by linearizability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 10:55:04 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Sezgin", "Ali", ""]]}, {"id": "1506.04917", "submitter": "Gabriele Fici", "authors": "Maxime Crochemore, Gabriele Fici, Robert Merca\\c{s} and Solon P.\n  Pissis", "title": "Linear-Time Sequence Comparison Using Minimal Absent Words &\n  Applications", "comments": "Accepted to LATIN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence comparison is a prerequisite to virtually all comparative genomic\nanalyses. It is often realized by sequence alignment techniques, which are\ncomputationally expensive. This has led to increased research into\nalignment-free techniques, which are based on measures referring to the\ncomposition of sequences in terms of their constituent patterns. These\nmeasures, such as $q$-gram distance, are usually computed in time linear with\nrespect to the length of the sequences. In this article, we focus on the\ncomplementary idea: how two sequences can be efficiently compared based on\ninformation that does not occur in the sequences. A word is an {\\em absent\nword} of some sequence if it does not occur in the sequence. An absent word is\n{\\em minimal} if all its proper factors occur in the sequence. Here we present\nthe first linear-time and linear-space algorithm to compare two sequences by\nconsidering {\\em all} their minimal absent words. In the process, we present\nresults of combinatorial interest, and also extend the proposed techniques to\ncompare circular sequences.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 11:04:02 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 08:48:14 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Crochemore", "Maxime", ""], ["Fici", "Gabriele", ""], ["Merca\u015f", "Robert", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1506.05093", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari", "title": "An Improved Distributed Algorithm for Maximal Independent Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximal Independent Set (MIS) problem is one of the basics in the study\nof locality in distributed graph algorithms. This paper presents an extremely\nsimple randomized algorithm providing a near-optimal local complexity for this\nproblem, which incidentally, when combined with some recent techniques, also\nleads to a near-optimal global complexity.\n  Classical algorithms of Luby [STOC'85] and Alon, Babai and Itai [JALG'86]\nprovide the global complexity guarantee that, with high probability, all nodes\nterminate after $O(\\log n)$ rounds. In contrast, our initial focus is on the\nlocal complexity, and our main contribution is to provide a very simple\nalgorithm guaranteeing that each particular node $v$ terminates after $O(\\log\n\\mathsf{deg}(v)+\\log 1/\\epsilon)$ rounds, with probability at least\n$1-\\epsilon$. The guarantee holds even if the randomness outside $2$-hops\nneighborhood of $v$ is determined adversarially. This degree-dependency is\noptimal, due to a lower bound of Kuhn, Moscibroda, and Wattenhofer [PODC'04].\n  Interestingly, this local complexity smoothly transitions to a global\ncomplexity: by adding techniques of Barenboim, Elkin, Pettie, and Schneider\n[FOCS'12, arXiv: 1202.1983v3], we get a randomized MIS algorithm with a high\nprobability global complexity of $O(\\log \\Delta) + 2^{O(\\sqrt{\\log \\log n})}$,\nwhere $\\Delta$ denotes the maximum degree. This improves over the $O(\\log^2\n\\Delta) + 2^{O(\\sqrt{\\log \\log n})}$ result of Barenboim et al., and gets close\nto the $\\Omega(\\min\\{\\log \\Delta, \\sqrt{\\log n}\\})$ lower bound of Kuhn et al.\n  Corollaries include improved algorithms for MIS in graphs of upper-bounded\narboricity, or lower-bounded girth, for Ruling Sets, for MIS in the Local\nComputation Algorithms (LCA) model, and a faster distributed algorithm for the\nLov\\'asz Local Lemma.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 19:38:40 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2015 23:11:11 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Ghaffari", "Mohsen", ""]]}, {"id": "1506.05185", "submitter": "Paolo Ribeca", "authors": "{\\L}ukasz Roguski, Paolo Ribeca", "title": "CARGO: Effective format-free compressed storage of genomic information", "comments": "13 (Main) + 31 (Supplementary) + 88 (Manual) pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent super-exponential growth in the amount of sequencing data\ngenerated worldwide has put techniques for compressed storage into the focus.\nMost available solutions, however, are strictly tied to specific bioinformatics\nformats, sometimes inheriting from them suboptimal design choices; this hinders\nflexible and effective data sharing. Here we present CARGO (Compressed\nARchiving for GenOmics), a high-level framework to automatically generate\nsoftware systems optimized for the compressed storage of arbitrary types of\nlarge genomic data collections. Straightforward applications of our approach to\nFASTQ and SAM archives require a few lines of code, produce solutions that\nmatch and sometimes outperform specialized format-tailored compressors, and\nscale well to multi-TB datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 02:11:42 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Roguski", "\u0141ukasz", ""], ["Ribeca", "Paolo", ""]]}, {"id": "1506.05203", "submitter": "Myoungji Han", "authors": "Myoungji Han, Munseong Kang, Sukhyeun Cho, Geonmo Gu, Jeong Seop Sim,\n  Kunsoo Park", "title": "Fast Multiple Order-Preserving Matching Algorithms", "comments": "15 pages, 8 figures, submitted to IWOCA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a text $T$ and a pattern $P$, the order-preserving matching problem is\nto find all substrings in $T$ which have the same relative orders as $P$.\nOrder-preserving matching has been an active research area since it was\nintroduced by Kubica et al. \\cite{kubica2013linear} and Kim et al.\n\\cite{kim2014order}. In this paper we present two algorithms for the multiple\norder-preserving matching problem, one of which runs in sublinear time on\naverage and the other in linear time on average. Both algorithms run much\nfaster than the previous algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 04:53:18 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Han", "Myoungji", ""], ["Kang", "Munseong", ""], ["Cho", "Sukhyeun", ""], ["Gu", "Geonmo", ""], ["Sim", "Jeong Seop", ""], ["Park", "Kunsoo", ""]]}, {"id": "1506.05393", "submitter": "Ze Wang", "authors": "Ze Wang", "title": "MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance\n  Fingerprinting", "comments": "7 figures", "journal-ref": "39th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC), 3256-3259, 2017", "doi": "10.1109/EMBC.2017.8037551", "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance fingerprinting (MRF) is a new technique for simultaneously\nquantifying multiple MR parameters using one temporally resolved MR scan. But\nits brute-force dictionary generating and searching (DGS) process causes a huge\ndisk space demand and computational burden, prohibiting it from a practical\nmultiple slice high-definition imaging. The purpose of this paper was to\nprovide a fast and space efficient DGS algorithm for MRF. Based on an empirical\nanalysis of properties of the distance function of the acquired MRF signal and\nthe pre-defined MRF dictionary entries, we proposed a parameter separable MRF\nDGS method, which breaks the multiplicative computation complexity into an\nadditive one and enabling a resolution scalable multi-resolution DGS process,\nwhich was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM was\nhundreds or thousands of times faster than the original brute-force DGS method.\nThe acceleration was even higher when considering the time difference for\ngenerating the dictionary. Using a high precision quantification, MRF can find\nthe right parameter values for a 64x64 imaging slice in 117 secs. Our data also\nshowed that spatial constraints can be used to further speed up MRF ZOOM.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 17:47:18 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Wang", "Ze", ""]]}, {"id": "1506.05620", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Christian Komusiewicz and Manuel Sorge", "title": "A parameterized approximation algorithm for the mixed and windy\n  Capacitated Arc Routing Problem: theory and experiments", "comments": "A preliminary version of this article appeared in the Proceedings of\n  the 15th Workshop on Algorithmic Approaches for Transportation Modeling,\n  Optimization, and Systems (ATMOS'15). This version describes several\n  algorithmic enhancements, contains an experimental evaluation of our\n  algorithm, and provides a new benchmark data set", "journal-ref": "Networks 70(3):262-278, 2017", "doi": "10.1002/net.21742", "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any polynomial-time $\\alpha(n)$-approximation algorithm for the\n$n$-vertex metric asymmetric Traveling Salesperson Problem yields a\npolynomial-time $O(\\alpha(C))$-approximation algorithm for the mixed and windy\nCapacitated Arc Routing Problem, where $C$ is the number of weakly connected\ncomponents in the subgraph induced by the positive-demand arcs---a small number\nin many applications. In conjunction with known results, we obtain\nconstant-factor approximations for $C\\in O(\\log n)$ and $O(\\log C/\\log\\log\nC)$-approximations in general. Experiments show that our algorithm, together\nwith several heuristic enhancements, outperforms many previous polynomial-time\nheuristics. Finally, since the solution quality achievable in polynomial time\nappears to mainly depend on $C$ and since $C=1$ in almost all benchmark\ninstances, we propose the Ob benchmark set, simulating cities that are divided\ninto several components by a river.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 10:47:28 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 03:50:46 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Komusiewicz", "Christian", ""], ["Sorge", "Manuel", ""]]}, {"id": "1506.05673", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius and Ignaz Rutter", "title": "A New Perspective on Clustered Planarity as a Combinatorial Embedding\n  Problem", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustered planarity problem (c-planarity) asks whether a hierarchically\nclustered graph admits a planar drawing such that the clusters can be nicely\nrepresented by regions. We introduce the cd-tree data structure and give a new\ncharacterization of c-planarity. It leads to efficient algorithms for\nc-planarity testing in the following cases. (i) Every cluster and every\nco-cluster (complement of a cluster) has at most two connected components. (ii)\nEvery cluster has at most five outgoing edges.\n  Moreover, the cd-tree reveals interesting connections between c-planarity and\nplanarity with constraints on the order of edges around vertices. On one hand,\nthis gives rise to a bunch of new open problems related to c-planarity, on the\nother hand it provides a new perspective on previous results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 13:35:28 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1506.05677", "submitter": "Attila Bern\\'ath", "authors": "Attila Bern\\'ath, Gyula Pap", "title": "Blocking optimal arborescences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of covering minimum cost common bases of two matroids is\nNP-complete, even if the two matroids coincide, and the costs are all equal to\n1. In this paper we show that the following special case is solvable in\npolynomial time: given a digraph $D=(V,A)$ with a designated root node $r\\in V$\nand arc-costs $c:A\\to \\mathbb{R}$, find a minimum cardinality subset $H$ of the\narc set $A$ such that $H$ intersects every minimum $c$-cost $r$-arborescence.\nBy an $r$-arborescence we mean a spanning arborescence of root $r$. The\nalgorithm we give solves a weighted version as well, in which a nonnegative\nweight function $w:A\\to \\mathbb{R}_+$ (unrelated to $c$) is also given, and we\nwant to find a subset $H$ of the arc set such that $H$ intersects every minimum\n$c$-cost $r$-arborescence, and $w(H)=\\sum_{a\\in H}w(a)$ is minimum. The running\ntime of the algorithm is $O(n^3T(n,m))$, where $n$ and $m$ denote the number of\nnodes and arcs of the input digraph, and $T(n,m)$ is the time needed for a\nminimum $s-t$ cut computation in this digraph. A polyhedral description is not\ngiven, and seems rather challenging.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 13:51:29 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Bern\u00e1th", "Attila", ""], ["Pap", "Gyula", ""]]}, {"id": "1506.05715", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius and Annette Karrer and Ignaz Rutter", "title": "Simultaneous Embedding: Edge Orderings, Relative Positions, Cutvertices", "comments": "64 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simultaneous embedding (with fixed edges) of two graphs $G^1$ and $G^2$\nwith common graph $G=G^1 \\cap G^2$ is a pair of planar drawings of $G^1$ and\n$G^2$ that coincide on $G$. It is an open question whether there is a\npolynomial-time algorithm that decides whether two graphs admit a simultaneous\nembedding (problem SEFE).\n  In this paper, we present two results. First, a set of three linear-time\npreprocessing algorithms that remove certain substructures from a given SEFE\ninstance, producing a set of equivalent SEFE instances without such\nsubstructures. The structures we can remove are (1) cutvertices of the union\ngraph $G^\\cup = G^1 \\cup G^2$, (2) most separating pairs of $G^\\cup$, and (3)\nconnected components of $G$ that are biconnected but not a cycle.\n  Second, we give an $O(n^3)$-time algorithm solving SEFE for instances with\nthe following restriction. Let $u$ be a pole of a P-node $\\mu$ in the SPQR-tree\nof a block of $G^1$ or $G^2$. Then at most three virtual edges of $\\mu$ may\ncontain common edges incident to $u$. All algorithms extend to the sunflower\ncase, i.e., to the case of more than three graphs pairwise intersecting in the\nsame common graph.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 15:20:22 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Karrer", "Annette", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1506.05721", "submitter": "Kevin Schewior", "authors": "Lin Chen, Nicole Megow, Kevin Schewior", "title": "An O(m^2 log m)-Competitive Algorithm for Online Machine Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online machine minimization problem in which jobs with hard\ndeadlines arrive online over time at their release dates. The task is to\ndetermine a feasible schedule on a minimum number of machines. Our main result\nis a general O(m^2 log m)-competitive algorithm for the preemptive online\nproblem, where m is the optimal number of machines used in an offline solution.\nThis is the first improvement on an O(log (p_max/p_min))-competitive algorithm\nby Phillips et al. (STOC 1997), which was to date the best known algorithm even\nwhen m=2. Our algorithm is O(1)-competitive for any m that is bounded by a\nconstant. To develop the algorithm, we investigate two complementary special\ncases of the problem, namely, laminar instances and agreeable instances, for\nwhich we provide an O(log m)-competitive and an O(1)-competitive algorithm,\nrespectively. Our O(1)-competitive algorithm for agreeable instances actually\nproduces a non-preemptive schedule, which is of its own interest as there\nexists a strong lower bound of n, the number of jobs, for the general\nnon-preemptive online machine minimization problem by Saha (FSTTCS 2013), which\neven holds for laminar instances.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 15:39:00 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Chen", "Lin", ""], ["Megow", "Nicole", ""], ["Schewior", "Kevin", ""]]}, {"id": "1506.05783", "submitter": "Matteo Pontecorvi", "authors": "Matteo Pontecorvi and Vijaya Ramachandran", "title": "A Faster Algorithm for Fully Dynamic Betweenness Centrality", "comments": "The current revision (v3) includes minor changes in the Introduction.\n  There is no change to the main result. A brief summary of this paper will\n  appear in Proc. ISAAC 2015, in a paper by the authors entitled \"Fully Dynamic\n  Betweenness Centrality''. arXiv admin note: text overlap with arXiv:1412.3852", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new fully dynamic algorithm for maintaining betweenness\ncentrality (BC) of vertices in a directed graph $G=(V,E)$ with positive edge\nweights. BC is a widely used parameter in the analysis of large complex\nnetworks. We achieve an amortized $O((\\nu^*)^2 \\log^2 n)$ time per update,\nwhere $n = |V| $ and $\\nu^*$ bounds the number of distinct edges that lie on\nshortest paths through any single vertex. This result improves on the amortized\nbound for fully dynamic BC in [Pontecorvi-Ramachandran2015] by a logarithmic\nfactor. Our algorithm uses new data structures and techniques that are\nextensions of the method in the fully dynamic algorithm in Thorup [Thorup2004]\nfor APSP in graphs with unique shortest paths. For graphs with $\\nu^* = O(n)$,\nour algorithm matches the fully dynamic APSP bound in [Thorup2004], which holds\nfor graphs with $\\nu^* = n-1$, since it assumes unique shortest paths.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 19:37:22 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 20:23:09 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 17:57:19 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Pontecorvi", "Matteo", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1506.05977", "submitter": "Andrea Marino", "authors": "Alessio Conte, Roberto Grossi, Andrea Marino, Romeo Rizzi", "title": "Enumerating Cyclic Orientations of a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acyclic and cyclic orientations of an undirected graph have been widely\nstudied for their importance: an orientation is acyclic if it assigns a\ndirection to each edge so as to obtain a directed acyclic graph (DAG) with the\nsame vertex set; it is cyclic otherwise. As far as we know, only the\nenumeration of acyclic orientations has been addressed in the literature. In\nthis paper, we pose the problem of efficiently enumerating all the\n\\emph{cyclic} orientations of an undirected connected graph with $n$ vertices\nand $m$ edges, observing that it cannot be solved using algorithmic techniques\npreviously employed for enumerating acyclic orientations.We show that the\nproblem is of independent interest from both combinatorial and algorithmic\npoints of view, and that each cyclic orientation can be listed with\n$\\tilde{O}(m)$ delay time. Space usage is $O(m)$ with an additional setup cost\nof $O(n^2)$ time before the enumeration begins, or $O(mn)$ with a setup cost of\n$\\tilde{O}(m)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 12:43:34 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Conte", "Alessio", ""], ["Grossi", "Roberto", ""], ["Marino", "Andrea", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1506.05981", "submitter": "Jo\\~ao Ferreira", "authors": "Roland Backhouse and Jo\\~ao F. Ferreira", "title": "On Euclid's Algorithm and Elementary Number Theory", "comments": null, "journal-ref": "Sci. Comput. Program. 76 (3) (2011) 160-180", "doi": "10.1016/j.scico.2010.05.006", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms can be used to prove and to discover new theorems. This paper\nshows how algorithmic skills in general, and the notion of invariance in\nparticular, can be used to derive many results from Euclid's algorithm. We\nillustrate how to use the algorithm as a verification interface (i.e., how to\nverify theorems) and as a construction interface (i.e., how to investigate and\nderive new theorems).\n  The theorems that we verify are well-known and most of them are included in\nstandard number theory books. The new results concern distributivity properties\nof the greatest common divisor and a new algorithm for efficiently enumerating\nthe positive rationals in two different ways. One way is known and is due to\nMoshe Newman. The second is new and corresponds to a deforestation of the\nStern-Brocot tree of rationals. We show that both enumerations stem from the\nsame simple algorithm. In this way, we construct a Stern-Brocot enumeration\nalgorithm with the same time and space complexity as Newman's algorithm. A\nshort review of the original papers by Stern and Brocot is also included.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 12:54:41 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Backhouse", "Roland", ""], ["Ferreira", "Jo\u00e3o F.", ""]]}, {"id": "1506.06107", "submitter": "Heather Smith", "authors": "Istv\\'an Mikl\\'os and Heather Smith", "title": "The computational complexity of calculating partition functions of\n  optimal medians with Hamming distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that calculating the partition function of optimal\nmedians of binary strings with Hamming distance is \\#P-complete for several\nweight functions. The case when the weight function is the factorial function\nhas application in bioinformatics. In that case, the partition function counts\nthe most parsimonious evolutionary scenarios on a star tree under several\nmodels in bioinformatics. The results are extended to binary trees and we show\nthat it is also \\#P-complete to calculate the most parsimonious evolutionary\nscenarios on an arbitrary binary tree under the substitution model of\nbiological sequences and under the Single Cut-or-Join model for genome\nrearrangements.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 18:47:43 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 20:21:07 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Mikl\u00f3s", "Istv\u00e1n", ""], ["Smith", "Heather", ""]]}, {"id": "1506.06157", "submitter": "Gregory Puleo", "authors": "Gregory J. Puleo", "title": "Complexity of a Disjoint Matching Problem on Bipartite Graphs", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following question: given an $(X,Y)$-bigraph $G$ and a set $S\n\\subset X$, does $G$ contain two disjoint matchings $M_1$ and $M_2$ such that\n$M_1$ saturates $X$ and $M_2$ saturates $S$? When $|S|\\geq |X|-1$, this\nquestion is solvable by finding an appropriate factor of the graph. In\ncontrast, we show that when $S$ is allowed to be an arbitrary subset of $X$,\nthe problem is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 21:04:31 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Puleo", "Gregory J.", ""]]}, {"id": "1506.06162", "submitter": "Adam Smith", "authors": "Christian Borgs and Jennifer T. Chayes and Adam Smith", "title": "Private Graphon Estimation for Sparse Graphs", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design algorithms for fitting a high-dimensional statistical model to a\nlarge, sparse network without revealing sensitive information of individual\nmembers. Given a sparse input graph $G$, our algorithms output a\nnode-differentially-private nonparametric block model approximation. By\nnode-differentially-private, we mean that our output hides the insertion or\nremoval of a vertex and all its adjacent edges. If $G$ is an instance of the\nnetwork obtained from a generative nonparametric model defined in terms of a\ngraphon $W$, our model guarantees consistency, in the sense that as the number\nof vertices tends to infinity, the output of our algorithm converges to $W$ in\nan appropriate version of the $L_2$ norm. In particular, this means we can\nestimate the sizes of all multi-way cuts in $G$.\n  Our results hold as long as $W$ is bounded, the average degree of $G$ grows\nat least like the log of the number of vertices, and the number of blocks goes\nto infinity at an appropriate rate. We give explicit error bounds in terms of\nthe parameters of the model; in several settings, our bounds improve on or\nmatch known nonprivate results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 21:29:52 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer T.", ""], ["Smith", "Adam", ""]]}, {"id": "1506.06163", "submitter": "Drona Pratap Chandu", "authors": "Drona Pratap Chandu", "title": "Big Step Greedy Algorithm for Maximum Coverage Problem", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a greedy heuristic named as Big step greedy heuristic and\ninvestigates the application of Big step greedy heuristic for maximum\nk-coverage problem. Greedy algorithms construct the solution in multiple steps,\nthe classical greedy algorithm for maximum k-coverage problem, in each step\nselects one set that contains the greatest number of uncovered elements. The\nBig step greedy heuristic, in each step selects p (1 <= p <= k) sets such that\nthe union of selected p sets contains the greatest number of uncovered elements\nby evaluating all possible p-combinations of given sets. When p=k Big step\ngreedy algorithm behaves like exact algorithm that computes optimal solution by\nevaluating all possible k-combinations of given sets. When p=1 it behaves like\nthe classical greedy algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 21:40:47 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 08:58:04 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Chandu", "Drona Pratap", ""]]}, {"id": "1506.06284", "submitter": "Roman Kolpakov", "authors": "Roman Kolpakov, Mikhail Posypkin", "title": "Upper bound on the number of steps for solving the subset sum problem by\n  the Branch-and-Bound method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of one of the particular cases of the\nknapsack problem: the subset sum problem. For solving this problem we consider\none of the basic variants of the Branch-and-Bound method in which any\nsub-problem is decomposed along the free variable with the maximal weight. By\nthe complexity of solving a problem by the Branch-and-Bound method we mean the\nnumber of steps required for solving the problem by this method. In the paper\nwe obtain upper bounds on the complexity of solving the subset sum problem by\nthe Branch-and-Bound method. These bounds can be easily computed from the input\ndata of the problem. So these bounds can be used for the the preliminary\nestimation of the computational resources required for solving the subset sum\nproblem by the Branch-and-Bound method.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 19:20:23 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kolpakov", "Roman", ""], ["Posypkin", "Mikhail", ""]]}, {"id": "1506.06302", "submitter": "Euiwoong Lee", "authors": "Venkatesan Guruswami and Euiwoong Lee", "title": "Inapproximability of $H$-Transversal/Packing", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G = (V_G, E_G)$ and a fixed \"pattern\" graph $H =\n(V_H, E_H)$ with $k$ vertices, we consider the $H$-Transversal and $H$-Packing\nproblems. The former asks to find the smallest $S \\subseteq V_G$ such that the\nsubgraph induced by $V_G \\setminus S$ does not have $H$ as a subgraph, and the\nlatter asks to find the maximum number of pairwise disjoint $k$-subsets $S_1,\n..., S_m \\subseteq V_G$ such that the subgraph induced by each $S_i$ has $H$ as\na subgraph.\n  We prove that if $H$ is 2-connected, $H$-Transversal and $H$-Packing are\nalmost as hard to approximate as general $k$-Hypergraph Vertex Cover and\n$k$-Set Packing, so it is NP-hard to approximate them within a factor of\n$\\Omega (k)$ and $\\widetilde \\Omega (k)$ respectively. We also show that there\nis a 1-connected $H$ where $H$-Transversal admits an $O(\\log k)$-approximation\nalgorithm, so that the connectivity requirement cannot be relaxed from 2 to 1.\nFor a special case of $H$-Transversal where $H$ is a (family of) cycles, we\nmention the implication of our result to the related Feedback Vertex Set\nproblem, and give a different hardness proof for directed graphs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 22:55:06 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Lee", "Euiwoong", ""]]}, {"id": "1506.06404", "submitter": "Steven Kelk", "authors": "Olivier Boes, Mareike Fischer, Steven Kelk", "title": "A linear bound on the number of states in optimal convex characters for\n  maximum parsimony distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two phylogenetic trees on the same set of taxa X, the maximum parsimony\ndistance d_MP is defined as the maximum, ranging over all characters c on X, of\nthe absolute difference in parsimony score induced by c on the two trees. In\nthis note we prove that for binary trees there exists a character achieving\nthis maximum that is convex on one of the trees (i.e. the parsimony score\ninduced on that tree is equal to the number of states in the character minus 1)\nand such that the number of states in the character is at most 7d_MP - 5. This\nis the first non-trivial bound on the number of states required by optimal\ncharacters, convex or otherwise. The result potentially has algorithmic\nsignificance because, unlike general characters, convex characters with a\nbounded number of states can be enumerated in polynomial time.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 19:36:21 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Boes", "Olivier", ""], ["Fischer", "Mareike", ""], ["Kelk", "Steven", ""]]}, {"id": "1506.06444", "submitter": "Vijay Bhattiprolu", "authors": "Vijay V. S. P. Bhattiprolu, Venkatesan Guruswami, Euiwoong Lee", "title": "Approximate Hypergraph Coloring under Low-discrepancy and Related\n  Promises", "comments": "Approx 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hypergraph is said to be $\\chi$-colorable if its vertices can be colored\nwith $\\chi$ colors so that no hyperedge is monochromatic. $2$-colorability is a\nfundamental property (called Property B) of hypergraphs and is extensively\nstudied in combinatorics. Algorithmically, however, given a $2$-colorable\n$k$-uniform hypergraph, it is NP-hard to find a $2$-coloring miscoloring fewer\nthan a fraction $2^{-k+1}$ of hyperedges (which is achieved by a random\n$2$-coloring), and the best algorithms to color the hypergraph properly require\n$\\approx n^{1-1/k}$ colors, approaching the trivial bound of $n$ as $k$\nincreases.\n  In this work, we study the complexity of approximate hypergraph coloring, for\nboth the maximization (finding a $2$-coloring with fewest miscolored edges) and\nminimization (finding a proper coloring using fewest number of colors)\nversions, when the input hypergraph is promised to have the following stronger\nproperties than $2$-colorability:\n  (A) Low-discrepancy: If the hypergraph has discrepancy $\\ell \\ll \\sqrt{k}$,\nwe give an algorithm to color the it with $\\approx n^{O(\\ell^2/k)}$ colors.\nHowever, for the maximization version, we prove NP-hardness of finding a\n$2$-coloring miscoloring a smaller than $2^{-O(k)}$ (resp. $k^{-O(k)}$)\nfraction of the hyperedges when $\\ell = O(\\log k)$ (resp. $\\ell=2$). Assuming\nthe UGC, we improve the latter hardness factor to $2^{-O(k)}$ for almost\ndiscrepancy-$1$ hypergraphs.\n  (B) Rainbow colorability: If the hypergraph has a $(k-\\ell)$-coloring such\nthat each hyperedge is polychromatic with all these colors, we give a\n$2$-coloring algorithm that miscolors at most $k^{-\\Omega(k)}$ of the\nhyperedges when $\\ell \\ll \\sqrt{k}$, and complement this with a matching UG\nhardness result showing that when $\\ell =\\sqrt{k}$, it is hard to even beat the\n$2^{-k+1}$ bound achieved by a random coloring.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 02:20:22 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Bhattiprolu", "Vijay V. S. P.", ""], ["Guruswami", "Venkatesan", ""], ["Lee", "Euiwoong", ""]]}, {"id": "1506.06564", "submitter": "Matthew Johnson", "authors": "Konrad K. Dabrowski, Francois Dross, Matthew Johnson, Daniel Paulusma", "title": "Filling the Complexity Gaps for Colouring Planar and Bounded Degree\n  Graphs", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A colouring of a graph $G=(V,E)$ is a function $c: V\\rightarrow\\{1,2,\\ldots\n\\}$ such that $c(u)\\neq c(v)$ for every $uv\\in E$. A $k$-regular list\nassignment of $G$ is a function $L$ with domain $V$ such that for every $u\\in\nV$, $L(u)$ is a subset of $\\{1, 2, \\dots\\}$ of size $k$. A colouring $c$ of $G$\nrespects a $k$-regular list assignment $L$ of $G$ if $c(u)\\in L(u)$ for every\n$u\\in V$. A graph $G$ is $k$-choosable if for every $k$-regular list assignment\n$L$ of $G$, there exists a colouring of $G$ that respects $L$. We may also ask\nif for a given $k$-regular list assignment $L$ of a given graph $G$, there\nexists a colouring of $G$ that respects $L$. This yields the $k$-Regular List\nColouring problem. For $k\\in \\{3,4\\}$ we determine a family of classes ${\\cal\nG}$ of planar graphs, such that either $k$-Regular List Colouring is\nNP-complete for instances $(G,L)$ with $G\\in {\\cal G}$, or every $G\\in {\\cal\nG}$ is $k$-choosable. By using known examples of non-$3$-choosable and\nnon-$4$-choosable graphs, this enables us to classify the complexity of\n$k$-Regular List Colouring restricted to planar graphs, planar bipartite\ngraphs, planar triangle-free graphs and to planar graphs with no $4$-cycles and\nno $5$-cycles. We also classify the complexity of $k$-Regular List Colouring\nand a number of related colouring problems for graphs with bounded maximum\ndegree.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 12:09:25 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 19:57:31 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2015 04:24:48 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 17:09:10 GMT"}, {"version": "v5", "created": "Thu, 7 Feb 2019 09:06:06 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Dross", "Francois", ""], ["Johnson", "Matthew", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1506.06671", "submitter": "Ethan R. Elenberg", "authors": "Ethan R. Elenberg, Karthikeyan Shanmugam, Michael Borokhovich,\n  Alexandros G. Dimakis", "title": "Beyond Triangles: A Distributed Framework for Estimating 3-profiles of\n  Large Graphs", "comments": "To appear in part at KDD'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating the $3$-profile of a large graph.\n$3$-profiles are generalizations of triangle counts that specify the number of\ntimes a small graph appears as an induced subgraph of a large graph. Our\nalgorithm uses the novel concept of $3$-profile sparsifiers: sparse graphs that\ncan be used to approximate the full $3$-profile counts for a given large graph.\nFurther, we study the problem of estimating local and ego $3$-profiles, two\ngraph quantities that characterize the local neighborhood of each vertex of a\ngraph.\n  Our algorithm is distributed and operates as a vertex program over the\nGraphLab PowerGraph framework. We introduce the concept of edge pivoting which\nallows us to collect $2$-hop information without maintaining an explicit\n$2$-hop neighborhood list at each vertex. This enables the computation of all\nthe local $3$-profiles in parallel with minimal communication.\n  We test out implementation in several experiments scaling up to $640$ cores\non Amazon EC2. We find that our algorithm can estimate the $3$-profile of a\ngraph in approximately the same time as triangle counting. For the harder\nproblem of ego $3$-profiles, we introduce an algorithm that can estimate\nprofiles of hundreds of thousands of vertices in parallel, in the timescale of\nminutes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 16:34:16 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Elenberg", "Ethan R.", ""], ["Shanmugam", "Karthikeyan", ""], ["Borokhovich", "Michael", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1506.06715", "submitter": "Morteza Zadimoghaddam", "authors": "Vahab Mirrokni and Morteza Zadimoghaddam", "title": "Randomized Composable Core-sets for Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective technique for solving optimization problems over massive data\nsets is to partition the data into smaller pieces, solve the problem on each\npiece and compute a representative solution from it, and finally obtain a\nsolution inside the union of the representative solutions for all pieces. This\ntechnique can be captured via the concept of {\\em composable core-sets}, and\nhas been recently applied to solve diversity maximization problems as well as\nseveral clustering problems. However, for coverage and submodular maximization\nproblems, impossibility bounds are known for this technique \\cite{IMMM14}. In\nthis paper, we focus on efficient construction of a randomized variant of\ncomposable core-sets where the above idea is applied on a {\\em random\nclustering} of the data. We employ this technique for the coverage, monotone\nand non-monotone submodular maximization problems. Our results significantly\nimprove upon the hardness results for non-randomized core-sets, and imply\nimproved results for submodular maximization in a distributed and streaming\nsettings.\n  In summary, we show that a simple greedy algorithm results in a\n$1/3$-approximate randomized composable core-set for submodular maximization\nunder a cardinality constraint. This is in contrast to a known $O({\\log k\\over\n\\sqrt{k}})$ impossibility result for (non-randomized) composable core-set. Our\nresult also extends to non-monotone submodular functions, and leads to the\nfirst 2-round MapReduce-based constant-factor approximation algorithm with\n$O(n)$ total communication complexity for either monotone or non-monotone\nfunctions. Finally, using an improved analysis technique and a new algorithm\n$\\mathsf{PseudoGreedy}$, we present an improved $0.545$-approximation algorithm\nfor monotone submodular maximization, which is in turn the first\nMapReduce-based algorithm beating factor $1/2$ in a constant number of rounds.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 18:50:41 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Mirrokni", "Vahab", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1506.06737", "submitter": "Laura Florescu", "authors": "Laura Florescu and Will Perkins", "title": "Spectral Thresholds in the Bipartite Stochastic Block Model", "comments": "updated version, will appear in COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a bipartite stochastic block model on vertex sets $V_1$ and\n$V_2$, with planted partitions in each, and ask at what densities efficient\nalgorithms can recover the partition of the smaller vertex set.\n  When $|V_2| \\gg |V_1|$, multiple thresholds emerge. We first locate a sharp\nthreshold for detection of the partition, in the sense of the results of\n\\cite{mossel2012stochastic,mossel2013proof} and \\cite{massoulie2014community}\nfor the stochastic block model. We then show that at a higher edge density, the\nsingular vectors of the rectangular biadjacency matrix exhibit a localization /\ndelocalization phase transition, giving recovery above the threshold and no\nrecovery below. Nevertheless, we propose a simple spectral algorithm, Diagonal\nDeletion SVD, which recovers the partition at a nearly optimal edge density.\n  The bipartite stochastic block model studied here was used by\n\\cite{feldman2014algorithm} to give a unified algorithm for recovering planted\npartitions and assignments in random hypergraphs and random $k$-SAT formulae\nrespectively. Our results give the best known bounds for the clause density at\nwhich solutions can be found efficiently in these models as well as showing a\nbarrier to further improvement via this reduction to the bipartite block model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 19:58:14 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 18:10:08 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Florescu", "Laura", ""], ["Perkins", "Will", ""]]}, {"id": "1506.06793", "submitter": "Ali Alatabbi", "authors": "Ali Alatabbi, A. S. Sohidull Islam, M. Sohel Rahman, Jamie Simpson and\n  W. F. Smyth", "title": "Enhanced Covers of Regular & Indeterminate Strings using Prefix Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\itbf{cover} of a string $x = x[1..n]$ is a proper substring $u$ of $x$\nsuch that $x$ can be constructed from possibly overlapping instances of $u$. A\nrecent paper \\cite{FIKPPST13} relaxes this definition --- an \\itbf{enhanced\ncover} $u$ of $x$ is a border of $x$ (that is, a proper prefix that is also a\nsuffix) that covers a {\\it maximum} number of positions in $x$ (not necessarily\nall) --- and proposes efficient algorithms for the computation of enhanced\ncovers. These algorithms depend on the prior computation of the \\itbf{border\narray} $\\beta[1..n]$, where $\\beta[i]$ is the length of the longest border of\n$x[1..i]$, $1 \\le i \\le n$. In this paper, we first show how to compute\nenhanced covers using instead the \\itbf{prefix table}: an array $\\pi[1..n]$\nsuch that $\\pi[i]$ is the length of the longest substring of $x$ beginning at\nposition $i$ that matches a prefix of $x$. Unlike the border array, the prefix\ntable is robust: its properties hold also for \\itbf{indeterminate strings} ---\nthat is, strings defined on {\\it subsets} of the alphabet $\\Sigma$ rather than\nindividual elements of $\\Sigma$. Thus, our algorithms, in addition to being\nfaster in practice and more space-efficient than those of \\cite{FIKPPST13},\nallow us to easily extend the computation of enhanced covers to indeterminate\nstrings. Both for regular and indeterminate strings, our algorithms execute in\nexpected linear time. Along the way we establish an important theoretical\nresult: that the expected maximum length of any border of any prefix of a\nregular string $x$ is approximately 1.64 for binary alphabets, less for larger\nones.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 21:13:36 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Alatabbi", "Ali", ""], ["Islam", "A. S. Sohidull", ""], ["Rahman", "M. Sohel", ""], ["Simpson", "Jamie", ""], ["Smyth", "W. F.", ""]]}, {"id": "1506.06983", "submitter": "Ali Alatabbi", "authors": "Ali Alatabbi, Jacqueline W. Daykin and M. Sohel Rahman", "title": "Linear Algorithms for Computing the Lyndon Border Array and the Lyndon\n  Suffix Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding repetitive structures and inherent\npatterns in a given string $\\s{s}$ of length $n$ over a finite totally ordered\nalphabet. A border $\\s{u}$ of a string $\\s{s}$ is both a prefix and a suffix of\n$\\s{s}$ such that $\\s{u} \\not= \\s{s}$. The computation of the border array of a\nstring $\\s{s}$, namely the borders of each prefix of $\\s{s}$, is strongly\nrelated to the string matching problem: given a string $\\s{w}$, find all of its\noccurrences in $\\s{s}$. A {\\itshape Lyndon word} is a primitive word (i.e., it\nis not a power of another word) which is minimal for the lexicographical order\nof its conjugacy class (i.e., the set of words obtained by cyclic rotations of\nthe letters). In this paper we combine these concepts to introduce the\n\\emph{Lyndon Border Array} $\\mathcal L \\beta$ of $\\s{s}$, whose $i$-th entry\n$\\mathcal L \\beta(\\s{s})[i]$ is the length of the longest border of $\\s{s}[1\n\\dd i]$ which is also a Lyndon word. We propose linear-time and linear-space\nalgorithms for computing $\\mathcal L \\beta (\\s{s})$. %in the case of both\nbinary and bounded alphabets. Further, we introduce the \\emph{Lyndon Suffix\nArray}, and by modifying the efficient suffix array technique of Ko and Aluru\n\\cite{KA03} outline a linear time and space algorithm for its construction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:29:26 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Alatabbi", "Ali", ""], ["Daykin", "Jacqueline W.", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1506.07076", "submitter": "Clifford Stein", "authors": "Aaron Bernstein and Cliff Stein", "title": "Fully Dynamic Matching in Bipartite Graphs", "comments": "Longer version of paper that appears in ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum cardinality matching in bipartite graphs is an important and\nwell-studied problem. The fully dynamic version, in which edges are inserted\nand deleted over time has also been the subject of much attention. Existing\nalgorithms for dynamic matching (in general graphs) seem to fall into two\ngroups: there are fast (mostly randomized) algorithms that do not achieve a\nbetter than 2-approximation, and there slow algorithms with $\\O(\\sqrt{m})$\nupdate time that achieve a better-than-2 approximation. Thus the obvious\nquestion is whether we can design an algorithm -- deterministic or randomized\n-- that achieves a tradeoff between these two: a $o(\\sqrt{m})$ approximation\nand a better-than-2 approximation simultaneously. We answer this question in\nthe affirmative for bipartite graphs.\n  Our main result is a fully dynamic algorithm that maintains a $3/2 + \\eps$\napproximation in worst-case update time $O(m^{1/4}\\eps^{/2.5})$. We also give\nstronger results for graphs whose arboricity is at most $\\al$, achieving a $(1+\n\\eps)$ approximation in worst-case time $O(\\al (\\al + \\log n))$ for constant\n$\\eps$. When the arboricity is constant, this bound is $O(\\log n)$ and when the\narboricity is polylogarithmic the update time is also polylogarithmic.\n  The most important technical developement is the use of an intermediate graph\nwe call an edge degree constrained subgraph (EDCS). This graph places\nconstraints on the sum of the degrees of the endpoints of each edge: upper\nbounds for matched edges and lower bounds for unmatched edges. The main\ntechnical content of our paper involves showing both how to maintain an EDCS\ndynamically and that and EDCS always contains a sufficiently large matching. We\nalso make use of graph orientations to help bound the amount of work done\nduring each update.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 16:21:50 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 02:30:26 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Bernstein", "Aaron", ""], ["Stein", "Cliff", ""]]}, {"id": "1506.07246", "submitter": "Martin Derka", "authors": "Therese Biedl, Martin Derka", "title": "$1$-String $B_1$-VPG Representations of Planar Partial $3$-Trees and\n  Some Subclasses", "comments": "To appear at the 27th Canadian Conference on Computational Geometry,\n  Kingston, Ontario, August 10--12, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planar partial $3$-trees are subgraphs of those planar graphs obtained by\nrepeatedly inserting a vertex of degree $3$ into a face. In this paper, we show\nthat planar partial $3$-trees have $1$-string $B_1$-VPG representations, i.e.,\nrepresentations where every vertex is represented by an orthogonal curve with\nat most one bend, every two curves intersect at most once, and intersections of\ncurves correspond to edges in the graph. We also that some subclasses of planar\npartial 3-trees have L-representations, i.e., a $B_1$-VPG representation where\nevery curve has the shape of an L.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 05:12:41 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Biedl", "Therese", ""], ["Derka", "Martin", ""]]}, {"id": "1506.07329", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "Polyhedral aspects of Submodularity, Convexity and Concavity", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seminal work by Edmonds and Lovasz shows the strong connection between\nsubmodularity and convexity. Submodular functions have tight modular lower\nbounds, and subdifferentials in a manner akin to convex functions. They also\nadmit poly-time algorithms for minimization and satisfy the Fenchel duality\ntheorem and the Discrete Seperation Theorem, both of which are fundamental\ncharacteristics of convex functions. Submodular functions also show signs\nsimilar to concavity. Submodular maximization, though NP hard, admits constant\nfactor approximation guarantees. Concave functions composed with modular\nfunctions are submodular, and they also satisfy diminishing returns property.\nThis manuscript provides a more complete picture on the relationship between\nsubmodularity with convexity and concavity, by extending many of the results\nconnecting submodularity with convexity to the concave aspects of\nsubmodularity. We first show the existence of superdifferentials, and\nefficiently computable tight modular upper bounds of a submodular function.\nWhile we show that it is hard to characterize this polyhedron, we obtain inner\nand outer bounds on the superdifferential along with certain specific and\nuseful supergradients. We then investigate forms of concave extensions of\nsubmodular functions and show interesting relationships to submodular\nmaximization. We next show connections between optimality conditions over the\nsuperdifferentials and submodular maximization, and show how forms of\napproximate optimality conditions translate into approximation factors for\nmaximization. We end this paper by studying versions of the discrete seperation\ntheorem and the Fenchel duality theorem when seen from the concave point of\nview. In every case, we relate our results to the existing results from the\nconvex point of view, thereby improving the analysis of the relationship\nbetween submodularity, convexity, and concavity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 11:45:28 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 18:43:23 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1506.07458", "submitter": "Cedric Chauve", "authors": "Julien Allali and Laetitia Bourgeade and Cedric Chauve", "title": "Chaining fragments in sequences: to sweep or not", "comments": "13 pages, 2 figures Longer version of an extended abstract to appear\n  in the proceedings of SPIRE (String Processing and Information REtrieval)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing an optimal chain of fragments is a classical problem in string\nalgorithms, with important applications in computational biology. There exist\ntwo efficient dynamic programming algorithms solving this problem, based on\ndifferent principles. In the present note, we show how it is possible to\ncombine the principles of two of these algorithms in order to design a hybrid\ndynamic programming algorithm that combines the advantages of both algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:37:07 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Allali", "Julien", ""], ["Bourgeade", "Laetitia", ""], ["Chauve", "Cedric", ""]]}, {"id": "1506.07490", "submitter": "Noah Stephens-Davidowitz", "authors": "Noah Stephens-Davidowitz", "title": "Discrete Gaussian Sampling Reduces to CVP and SVP", "comments": "SODA 2016", "journal-ref": "SODA 2016", "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete Gaussian $D_{L- t, s}$ is the distribution that assigns to each\nvector $x$ in a shifted lattice $L - t$ probability proportional to $e^{-\\pi\n\\|x\\|^2/s^2}$. It has long been an important tool in the study of lattices.\nMore recently, algorithms for discrete Gaussian sampling (DGS) have found many\napplications in computer science. In particular, polynomial-time algorithms for\nDGS with very high parameters $s$ have found many uses in cryptography and in\nreductions between lattice problems. And, in the past year, Aggarwal, Dadush,\nRegev, and Stephens-Davidowitz showed $2^{n+o(n)}$-time algorithms for DGS with\na much wider range of parameters and used them to obtain the current fastest\nknown algorithms for the two most important lattice problems, the Shortest\nVector Problem (SVP) and the Closest Vector Problem (CVP).\n  Motivated by its increasing importance, we investigate the complexity of DGS\nitself and its relationship to CVP and SVP. Our first result is a\npolynomial-time dimension-preserving reduction from DGS to CVP. There is a\nsimple reduction from CVP to DGS, so this shows that DGS is equivalent to CVP.\nOur second result, which we find to be more surprising, is a polynomial-time\ndimension-preserving reduction from centered DGS (the important special case\nwhen $ t = 0$) to SVP. In the other direction, there is a simple reduction from\n$\\gamma$-approximate SVP for any $\\gamma = \\Omega(\\sqrt{n/\\log n})$, and we\npresent some (relatively weak) evidence to suggest that this might be the best\nachievable approximation factor.\n  We also show that our CVP result extends to a much wider class of\ndistributions and even to other norms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 18:18:54 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 19:16:02 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2015 16:08:54 GMT"}, {"version": "v4", "created": "Tue, 19 Apr 2016 19:11:11 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1506.07512", "submitter": "Roy Frostig", "authors": "Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford", "title": "Un-regularizing: approximate proximal point and faster stochastic\n  algorithms for empirical risk minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a family of accelerated stochastic algorithms that minimize sums\nof convex functions. Our algorithms improve upon the fastest running time for\nempirical risk minimization (ERM), and in particular linear least-squares\nregression, across a wide range of problem settings. To achieve this, we\nestablish a framework based on the classical proximal point algorithm. Namely,\nwe provide several algorithms that reduce the minimization of a strongly convex\nfunction to approximate minimizations of regularizations of the function. Using\nthese results, we accelerate recent fast stochastic algorithms in a black-box\nfashion. Empirically, we demonstrate that the resulting algorithms exhibit\nnotions of stability that are advantageous in practice. Both in theory and in\npractice, the provided algorithms reap the computational benefits of adding a\nlarge strongly convex regularization term, without incurring a corresponding\nbias to the original problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:53:45 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Frostig", "Roy", ""], ["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1506.07568", "submitter": "Tal Wagner", "authors": "Michael Dinitz, Robert Krauthgamer, Tal Wagner", "title": "Towards Resistance Sparsifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study resistance sparsification of graphs, in which the goal is to find a\nsparse subgraph (with reweighted edges) that approximately preserves the\neffective resistances between every pair of nodes. We show that every dense\nregular expander admits a $(1+\\epsilon)$-resistance sparsifier of size $\\tilde\nO(n/\\epsilon)$, and conjecture this bound holds for all graphs on $n$ nodes. In\ncomparison, spectral sparsification is a strictly stronger notion and requires\n$\\Omega(n/\\epsilon^2)$ edges even on the complete graph.\n  Our approach leads to the following structural question on graphs: Does every\ndense regular expander contain a sparse regular expander as a subgraph? Our\nmain technical contribution, which may of independent interest, is a positive\nanswer to this question in a certain setting of parameters. Combining this with\na recent result of von Luxburg, Radl, and Hein~(JMLR, 2014) leads to the\naforementioned resistance sparsifiers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 21:25:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Dinitz", "Michael", ""], ["Krauthgamer", "Robert", ""], ["Wagner", "Tal", ""]]}, {"id": "1506.07675", "submitter": "Alexandru I. Tomescu", "authors": "Ademir Hujdurovi\\'c, Ur\\v{s}a Ka\\v{c}ar, Martin Milani\\v{c}, Bernard\n  Ries, Alexandru I. Tomescu", "title": "Complexity and algorithms for finding a perfect phylogeny from mixed\n  tumor samples", "comments": "This is the extended version of Hujdurovi\\'c et al, Finding a perfect\n  phylogeny from mixed tumor samples, WABI 2015, DOI:\n  10.1007/978-3-662-48221-6_6", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Hajirasouliha and Raphael (WABI 2014) proposed a model for\ndeconvoluting mixed tumor samples measured from a collection of high-throughput\nsequencing reads. This is related to understanding tumor evolution and critical\ncancer mutations. In short, their formulation asks to split each row of a\nbinary matrix so that the resulting matrix corresponds to a perfect phylogeny\nand has the minimum number of rows among all matrices with this property. In\nthis paper we disprove several claims about this problem, including an\nNP-hardness proof of it. However, we show that the problem is indeed NP-hard,\nby providing a different proof. We also prove NP-completeness of a variant of\nthis problem proposed in the same paper. On the positive side, we propose an\nefficient (though not necessarily optimal) heuristic algorithm based on\ncoloring co-comparability graphs, and a polynomial time algorithm for solving\nthe problem optimally on matrix instances in which no column is contained in\nboth columns of a pair of conflicting columns. Implementations of these\nalgorithms are freely available at\nhttps://github.com/alexandrutomescu/MixedPerfectPhylogeny\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 09:33:04 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 19:59:08 GMT"}, {"version": "v3", "created": "Sun, 3 Jul 2016 19:56:45 GMT"}, {"version": "v4", "created": "Thu, 7 Jul 2016 22:42:11 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Hujdurovi\u0107", "Ademir", ""], ["Ka\u010dar", "Ur\u0161a", ""], ["Milani\u010d", "Martin", ""], ["Ries", "Bernard", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "1506.07729", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen and Stefan Kratsch", "title": "A structural approach to kernels for ILPs: Treewidth and Total\n  Unimodularity", "comments": "Extended abstract in the Proceedings of the 23rd European Symposium\n  on Algorithms (ESA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization is a theoretical formalization of efficient preprocessing for\nNP-hard problems. Empirically, preprocessing is highly successful in practice,\nfor example in state-of-the-art ILP-solvers like CPLEX. Motivated by this,\nprevious work studied the existence of kernelizations for ILP related problems,\ne.g., for testing feasibility of Ax <= b. In contrast to the observed success\nof CPLEX, however, the results were largely negative. Intuitively, practical\ninstances have far more useful structure than the worst-case instances used to\nprove these lower bounds.\n  In the present paper, we study the effect that subsystems with (Gaifman graph\nof) bounded treewidth or totally unimodularity have on the kernelizability of\nthe ILP feasibility problem. We show that, on the positive side, if these\nsubsystems have a small number of variables on which they interact with the\nremaining instance, then we can efficiently replace them by smaller subsystems\nof size polynomial in the domain without changing feasibility. Thus, if large\nparts of an instance consist of such subsystems, then this yields a substantial\nsize reduction. We complement this by proving that relaxations to the\nconsidered structures, e.g., larger boundaries of the subsystems, allow\nworst-case lower bounds against kernelization. Thus, these relaxed structures\ncan be used to build instance families that cannot be efficiently reduced, by\nany approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 12:47:56 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1506.07773", "submitter": "Rogers Mathew", "authors": "Tushar Kalra, Rogers Mathew, Sudebkumar Prasant Pal, Vijay Pandey", "title": "Maximum weighted independent sets with a budget", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, a non-negative integer $k$, and a weight function that\nmaps each vertex in $G$ to a positive real number, the \\emph{Maximum Weighted\nBudgeted Independent Set (MWBIS) problem} is about finding a maximum weighted\nindependent set in $G$ of cardinality at most $k$. A special case of MWBIS,\nwhen the weight assigned to each vertex is equal to its degree in $G$, is\ncalled the \\emph{Maximum Independent Vertex Coverage (MIVC)} problem. In other\nwords, the MIVC problem is about finding an independent set of cardinality at\nmost $k$ with maximum coverage.\n  Since it is a generalization of the well-known Maximum Weighted Independent\nSet (MWIS) problem, MWBIS too does not have any constant factor polynomial time\napproximation algorithm assuming $P \\neq NP$. In this paper, we study MWBIS in\nthe context of bipartite graphs. We show that, unlike MWIS, the MIVC (and\nthereby the MWBIS) problem in bipartite graphs is NP-hard. Then, we show that\nthe MWBIS problem admits a $\\frac{1}{2}$-factor approximation algorithm in the\nclass of bipartite graphs, which matches the integrality gap of a natural LP\nrelaxation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 14:45:30 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2015 15:39:28 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Kalra", "Tushar", ""], ["Mathew", "Rogers", ""], ["Pal", "Sudebkumar Prasant", ""], ["Pandey", "Vijay", ""]]}, {"id": "1506.07776", "submitter": "David Williamson", "authors": "Kyle Genova and David P. Williamson", "title": "An Experimental Evaluation of the Best-of-Many Christofides' Algorithm\n  for the Traveling Salesman Problem", "comments": "An extended abstract of this paper will appear in ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers on approximation algorithms for the traveling salesman problem\n(TSP) have given a new variant on the well-known Christofides' algorithm for\nthe TSP, called the Best-of-Many Christofides' algorithm. The algorithm\ninvolves sampling a spanning tree from the solution the standard LP relaxation\nof the TSP, subject to the condition that each edge is sampled with probability\nat most its value in the LP relaxation. One then runs Christofides' algorithm\non the tree by computing a minimum-cost matching on the odd-degree vertices in\nthe tree, and shortcutting the resulting Eulerian graph to a tour. In this\npaper we perform an experimental evaluation of the Best-of-Many Christofides'\nalgorithm to see if there are empirical reasons to believe its performance is\nbetter than that of Christofides' algorithm. Furthermore, several different\nsampling schemes have been proposed; we implement several different schemes to\ndetermine which ones might be the most promising for obtaining improved\nperformance guarantees over that of Christofides' algorithm. In our\nexperiments, all of the implemented methods perform significantly better than\nthe Christofides' algorithm; an algorithm that samples from a maximum entropy\ndistribution over spanning trees seems to be particularly good, though there\nare others that perform almost as well.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 14:52:40 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Genova", "Kyle", ""], ["Williamson", "David P.", ""]]}, {"id": "1506.07810", "submitter": "Michael Elberfeld", "authors": "Michael Elberfeld and Pascal Schweitzer", "title": "Canonizing Graphs of Bounded Tree Width in Logspace", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph canonization is the problem of computing a unique representative, a\ncanon, from the isomorphism class of a given graph. This implies that two\ngraphs are isomorphic exactly if their canons are equal. We show that graphs of\nbounded tree width can be canonized by logarithmic-space (logspace) algorithms.\nThis implies that the isomorphism problem for graphs of bounded tree width can\nbe decided in logspace. In the light of isomorphism for trees being hard for\nthe complexity class logspace, this makes the ubiquitous class of graphs of\nbounded tree width one of the few classes of graphs for which the complexity of\nthe isomorphism problem has been exactly determined.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 16:45:48 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Elberfeld", "Michael", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1506.07895", "submitter": "Sebastien Tixeuil", "authors": "Jordan Adamek, Mikhail Nesterenko, S\\'ebastien Tixeuil (NPA, LINCS,\n  IUF, LIP6)", "title": "Stateless Geocasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two stateless algorithms that guarantee to deliver the message to\nevery device in a designated geographic area: flooding and planar geocasting.\nDue to the algorithms' statelessness, intermediate devices do not have to keep\nmessage data between message transmissions. We formally prove the algorithms\ncorrect, estimate their message complexity and evaluate their performance\nthrough simulation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 20:39:32 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Adamek", "Jordan", "", "NPA, LINCS,\n  IUF, LIP6"], ["Nesterenko", "Mikhail", "", "NPA, LINCS,\n  IUF, LIP6"], ["Tixeuil", "S\u00e9bastien", "", "NPA, LINCS,\n  IUF, LIP6"]]}, {"id": "1506.07898", "submitter": "Torsten M\\\"utze", "authors": "Torsten M\\\"utze, Jerri Nummenpalo", "title": "Efficient computation of middle levels Gray codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any integer $n\\geq 1$ a middle levels Gray code is a cyclic listing of\nall bitstrings of length $2n+1$ that have either $n$ or $n+1$ entries equal to\n1 such that any two consecutive bitstrings in the list differ in exactly one\nbit. The question whether such a Gray code exists for every $n\\geq 1$ has been\nthe subject of intensive research during the last 30 years, and has been\nanswered affirmatively only recently [T. M\\\"utze. Proof of the middle levels\nconjecture. Proc. London Math. Soc., 112(4):677--713, 2016]. In this work we\nprovide the first efficient algorithm to compute a middle levels Gray code. For\na given bitstring, our algorithm computes the next $\\ell$ bitstrings in the\nGray code in time $\\mathcal{O}(n\\ell(1+\\frac{n}{\\ell}))$, which is\n$\\mathcal{O}(n)$ on average per bitstring provided that $\\ell=\\Omega(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 20:50:17 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 12:24:37 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2016 18:17:19 GMT"}, {"version": "v4", "created": "Tue, 20 Jun 2017 06:52:24 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["M\u00fctze", "Torsten", ""], ["Nummenpalo", "Jerri", ""]]}, {"id": "1506.07905", "submitter": "Luk\\'a\\v{s} Folwarczn\\'y", "authors": "Luk\\'a\\v{s} Folwarczn\\'y and Ji\\v{r}\\'i Sgall", "title": "General Caching Is Hard: Even with Small Pages", "comments": "19 pages, 8 figures, an extended abstract appeared in the proceedings\n  of MAPSP 2015 (www.mapsp2015.com), a conference version has been submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching (also known as paging) is a classical problem concerning page\nreplacement policies in two-level memory systems. General caching is the\nvariant with pages of different sizes and fault costs. We give the first\nNP-hardness result for general caching with small pages: General caching is\n(strongly) NP-hard even when page sizes are limited to {1, 2, 3}. It holds\nalready in the fault model (each page has unit fault cost) as well as in the\nbit model (each page has the same fault cost as size). We also give a very\nshort proof of the strong NP-hardness of general caching with page sizes\nrestricted to {1, 2, 3} and arbitrary costs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 21:45:48 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Folwarczn\u00fd", "Luk\u00e1\u0161", ""], ["Sgall", "Ji\u0159\u00ed", ""]]}, {"id": "1506.08187", "submitter": "Yin Tat Lee", "authors": "S\\'ebastien Bubeck, Yin Tat Lee, Mohit Singh", "title": "A geometric alternative to Nesterov's accelerated gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for unconstrained optimization of a smooth and\nstrongly convex function, which attains the optimal rate of convergence of\nNesterov's accelerated gradient descent. The new algorithm has a simple\ngeometric interpretation, loosely inspired by the ellipsoid method. We provide\nsome numerical evidence that the new method can be superior to Nesterov's\naccelerated gradient descent.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 19:39:50 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin Tat", ""], ["Singh", "Mohit", ""]]}, {"id": "1506.08189", "submitter": "Gregory Puleo", "authors": "Gregory J. Puleo, Olgica Milenkovic", "title": "Correlation Clustering and Biclustering with Locally Bounded Errors", "comments": "20 pages, reorganized paper to emphasize the key properties of the\n  rounding algorithm and the broader class of possible objective functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalized version of the correlation clustering problem,\ndefined as follows. Given a complete graph $G$ whose edges are labeled with $+$\nor $-$, we wish to partition the graph into clusters while trying to avoid\nerrors: $+$ edges between clusters or $-$ edges within clusters. Classically,\none seeks to minimize the total number of such errors. We introduce a new\nframework that allows the objective to be a more general function of the number\nof errors at each vertex (for example, we may wish to minimize the number of\nerrors at the worst vertex) and provide a rounding algorithm which converts\n\"fractional clusterings\" into discrete clusterings while causing only a\nconstant-factor blowup in the number of errors at each vertex. This rounding\nalgorithm yields constant-factor approximation algorithms for the discrete\nproblem under a wide variety of objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 19:46:41 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 22:39:19 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 19:29:33 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1506.08204", "submitter": "Richard Peng", "authors": "Yin Tat Lee, Richard Peng, Daniel A. Spielman", "title": "Sparsified Cholesky Solvers for SDD linear systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Laplacian and symmetric diagonally dominant (SDD) matrices can\nbe well approximated by linear-sized sparse Cholesky factorizations. We show\nthat these matrices have constant-factor approximations of the form $L L^{T}$,\nwhere $L$ is a lower-triangular matrix with a number of nonzero entries linear\nin its dimension. Furthermore linear systems in $L$ and $L^{T}$ can be solved\nin $O (n)$ work and $O(\\log{n}\\log^2\\log{n})$ depth, where $n$ is the dimension\nof the matrix.\n  We present nearly linear time algorithms that construct solvers that are\nalmost this efficient. In doing so, we give the first nearly-linear work\nroutine for constructing spectral vertex sparsifiers---that is, spectral\napproximations of Schur complements of Laplacian matrices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 20:00:41 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 16:02:22 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Lee", "Yin Tat", ""], ["Peng", "Richard", ""], ["Spielman", "Daniel A.", ""]]}, {"id": "1506.08319", "submitter": "L\\'aszl\\'o Kozma", "authors": "Parinya Chalermsook, Mayank Goswami, Laszlo Kozma, Kurt Mehlhorn,\n  Thatchaphol Saranurak", "title": "Greedy Is an Almost Optimal Deque", "comments": "to be presented at WADS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the geometric binary search tree (BST) model of\nDemaine, Harmon, Iacono, Kane, and Patrascu (DHIKP) to accommodate for\ninsertions and deletions. Within this extended model, we study the online\nGreedy BST algorithm introduced by DHIKP. Greedy BST is known to be equivalent\nto a maximally greedy (but inherently offline) algorithm introduced\nindependently by Lucas in 1988 and Munro in 2000, conjectured to be dynamically\noptimal.\n  With the application of forbidden-submatrix theory, we prove a quasilinear\nupper bound on the performance of Greedy BST on deque sequences. It has been\nconjectured (Tarjan, 1985) that splay trees (Sleator and Tarjan, 1983) can\nserve such sequences in linear time. Currently neither splay trees, nor other\ngeneral-purpose BST algorithms are known to fulfill this requirement. As a\nspecial case, we show that Greedy BST can serve output-restricted deque\nsequences in linear time. A similar result is known for splay trees (Tarjan,\n1985; Elmasry, 2004).\n  As a further application of the insert-delete model, we give a simple proof\nthat, given a set U of permutations of [n], the access cost of any BST\nalgorithm is Omega(log |U| + n) on \"most\" of the permutations from U. In\nparticular, this implies that the access cost for a random permutation of [n]\nis Omega(n log n) with high probability.\n  Besides the splay tree noted before, Greedy BST has recently emerged as a\nplausible candidate for dynamic optimality. Compared to splay trees, much less\neffort has gone into analyzing Greedy BST. Our work is intended as a step\ntowards a full understanding of Greedy BST, and we remark that\nforbidden-submatrix arguments seem particularly well suited for carrying out\nthis program.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 19:02:54 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Goswami", "Mayank", ""], ["Kozma", "Laszlo", ""], ["Mehlhorn", "Kurt", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1506.08392", "submitter": "Michael Elkin", "authors": "Michael Elkin, Seth Pettie", "title": "A Linear-Size Logarithmic Stretch Path-Reporting Distance Oracle for\n  General Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2001 Thorup and Zwick devised a distance oracle, which given an $n$-vertex\nundirected graph and a parameter $k$, has size $O(k n^{1+1/k})$. Upon a query\n$(u,v)$ their oracle constructs a $(2k-1)$-approximate path $\\Pi$ between $u$\nand $v$. The query time of the Thorup-Zwick's oracle is $O(k)$, and it was\nsubsequently improved to $O(1)$ by Chechik. A major drawback of the oracle of\nThorup and Zwick is that its space is $\\Omega(n \\cdot \\log n)$. Mendel and Naor\ndevised an oracle with space $O(n^{1+1/k})$ and stretch $O(k)$, but their\noracle can only report distance estimates and not actual paths. In this paper\nwe devise a path-reporting distance oracle with size $O(n^{1+1/k})$, stretch\n$O(k)$ and query time $O(n^\\epsilon)$, for an arbitrarily small $\\epsilon > 0$.\nIn particular, our oracle can provide logarithmic stretch using linear size.\nAnother variant of our oracle has size $O(n \\log\\log n)$, polylogarithmic\nstretch, and query time $O(\\log\\log n)$.\n  For unweighted graphs we devise a distance oracle with multiplicative stretch\n$O(1)$, additive stretch $O(\\beta(k))$, for a function $\\beta(\\cdot)$, space\n$O(n^{1+1/k} \\cdot \\beta)$, and query time $O(n^\\epsilon)$, for an arbitrarily\nsmall constant $\\epsilon >0$. The tradeoff between multiplicative stretch and\nsize in these oracles is far below girth conjecture threshold (which is stretch\n$2k-1$ and size $O(n^{1+1/k})$). Breaking the girth conjecture tradeoff is\nachieved by exhibiting a tradeoff of different nature between additive stretch\n$\\beta(k)$ and size $O(n^{1+1/k})$. A similar type of tradeoff was exhibited by\na construction of $(1+\\epsilon,\\beta)$-spanners due to Elkin and Peleg.\nHowever, so far $(1+\\epsilon,\\beta)$-spanners had no counterpart in the\ndistance oracles' world.\n  An important novel tool that we develop on the way to these results is a\n{distance-preserving path-reporting oracle}.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 12:56:32 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Elkin", "Michael", ""], ["Pettie", "Seth", ""]]}, {"id": "1506.08477", "submitter": "O-joung Kwon", "authors": "Eun Jung Kim and O-joung Kwon", "title": "A polynomial kernel for Block Graph Deletion", "comments": "22 pages, 2 figures, An extended abstract appeared in IPEC2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Block Graph Deletion problem, we are given a graph $G$ on $n$ vertices\nand a positive integer $k$, and the objective is to check whether it is\npossible to delete at most $k$ vertices from $G$ to make it a block graph,\ni.e., a graph in which each block is a clique. In this paper, we obtain a\nkernel with $\\mathcal{O}(k^{6})$ vertices for the Block Graph Deletion problem.\nThis is a first step to investigate polynomial kernels for deletion problems\ninto non-trivial classes of graphs of bounded rank-width, but unbounded\ntree-width. Our result also implies that Chordal Vertex Deletion admits a\npolynomial-size kernel on diamond-free graphs. For the kernelization and its\nanalysis, we introduce the notion of `complete degree' of a vertex. We believe\nthat the underlying idea can be potentially applied to other problems. We also\nprove that the Block Graph Deletion problem can be solved in time $10^{k}\\cdot\nn^{\\mathcal{O}(1)}$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 00:01:09 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 21:37:24 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 21:06:50 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Kim", "Eun Jung", ""], ["Kwon", "O-joung", ""]]}, {"id": "1506.08503", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Braden Hancock, Peter Michaleas,\n  Elizabeth Michel, Mayank Varia", "title": "Parallel Vectorized Algebraic AES in MATLAB for Rapid Prototyping of\n  Encrypted Sensor Processing Algorithms and Database Analytics", "comments": "6 pages; accepted to IEEE High Performance Extreme Computing\n  Conference (HPEC) 2015", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322470", "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of networked sensor systems and networked databases has\nled to an increased interest in incorporating encryption directly into sensor\nalgorithms and database analytics. MATLAB is the dominant tool for rapid\nprototyping of sensor algorithms and has extensive database analytics\ncapabilities. The advent of high level and high performance Galois Field\nmathematical environments allows encryption algorithms to be expressed\nsuccinctly and efficiently. This work leverages the Galois Field primitives\nfound the MATLAB Communication Toolbox to implement a mode of the Advanced\nEncrypted Standard (AES) based on first principals mathematics. The resulting\nimplementation requires 100x less code than standard AES implementations and\ndelivers speed that is effective for many design purposes. The parallel version\nachieves speed comparable to native OpenSSL on a single node and is sufficient\nfor real-time prototyping of many sensor processing algorithms and database\nanalytics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 04:34:40 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Hancock", "Braden", ""], ["Michaleas", "Peter", ""], ["Michel", "Elizabeth", ""], ["Varia", "Mayank", ""]]}, {"id": "1506.08518", "submitter": "Gabriele Fici", "authors": "Gabriele Fici, Tomasz Kociumaka, Thierry Lecroq, Arnaud Lefebvre,\n  Elise Prieur-Gaston", "title": "Fast Computation of Abelian Runs", "comments": "To appear in Theoretical Computer Science", "journal-ref": "Theoretical Computer Science, 656 Part B: 256-264, 2016", "doi": "10.1016/j.tcs.2015.12.010", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a word $w$ and a Parikh vector $\\mathcal{P}$, an abelian run of period\n$\\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having\nabelian period $\\mathcal{P}$. Our main result is an online algorithm that,\ngiven a word $w$ of length $n$ over an alphabet of cardinality $\\sigma$ and a\nParikh vector $\\mathcal{P}$, returns all the abelian runs of period\n$\\mathcal{P}$ in $w$ in time $O(n)$ and space $O(\\sigma+p)$, where $p$ is the\nnorm of $\\mathcal{P}$, i.e., the sum of its components. We also present an\nonline algorithm that computes all the abelian runs with periods of norm $p$ in\n$w$ in time $O(np)$, for any given norm $p$. Finally, we give an $O(n^2)$-time\noffline randomized algorithm for computing all the abelian runs of $w$. Its\ndeterministic counterpart runs in $O(n^2\\log\\sigma)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 06:24:08 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 08:31:43 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Fici", "Gabriele", ""], ["Kociumaka", "Tomasz", ""], ["Lecroq", "Thierry", ""], ["Lefebvre", "Arnaud", ""], ["Prieur-Gaston", "Elise", ""]]}, {"id": "1506.08547", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "Commutativity in the Algorithmic Lovasz Local Lemma", "comments": "Accepted to SIAM Journal on Computing (SICOMP). Some proofs are\n  simplified compared to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recent formulation of the Algorithmic Lov\\'asz Local Lemma\n[10,2,3] for finding objects that avoid `bad features', or `flaws'. It extends\nthe Moser-Tardos resampling algorithm [17] to more general discrete spaces. At\neach step the method picks a flaw present in the current state and goes to a\nnew state according to some prespecified probability distribution (which\ndepends on the current state and the selected flaw). However, it is less\nflexible than the Moser-Tardos method since [10,2,3] require a specific flaw\nselection rule, whereas [17] allows an arbitrary rule (and thus can potentially\nbe implemented more efficiently).\n  We formulate a new \"commutativity\" condition, and prove that it is sufficient\nfor an arbitrary rule to work. It also enables an efficient parallelization\nunder an additional assumption. We then show that existing resampling oracles\nfor perfect matchings and permutations do satisfy this condition.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 08:53:55 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 03:43:04 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2015 10:50:44 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2015 14:30:35 GMT"}, {"version": "v5", "created": "Sat, 31 Oct 2015 19:04:42 GMT"}, {"version": "v6", "created": "Tue, 10 Nov 2015 09:20:31 GMT"}, {"version": "v7", "created": "Sat, 13 Aug 2016 14:14:59 GMT"}, {"version": "v8", "created": "Tue, 4 Sep 2018 10:04:15 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "1506.08620", "submitter": "Fabio Cannizzo", "authors": "Fabio Cannizzo", "title": "Fast and Vectorizable Alternative to Binary Search in O(1) Applicable to\n  a Wide Domain of Sorted Arrays of Floating Point Numbers", "comments": null, "journal-ref": "Journal of Parallel and Distributed Computing, Volume 113, March\n  2018, Pages 37-5", "doi": "10.1016/j.jpdc.2017.10.007", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an array $X$ of $N+1$ strictly ordered floating point numbers and a\nfloating point number $z$ in the interval $[X_0,X_N)$, a common problem is to\nfind the index $i$ of the interval $[X_{i},X_{i+1})$ containing $z$. This\nproblem arises for instance in the context of spline interpolation or the\ncomputation of empirical probability distribution from empirical data. Often it\nneeds to be solved for a large number of different values $z$ and the same\narray $X$, which makes it worth investing resources upfront in pre-processing\nthe array $X$ with the goal of speeding up subsequent search operations. In\nsome cases the values $z$ to be processed are known simultaneously in blocks of\nsize $M$, which offers the opportunity to solve the problem vectorially,\nexploiting the parallel capabilities of modern CPUs. The common solution is to\nsequentially invoke $M$ times the well known binary search algorithm, which has\ncomplexity $O(log_2 N)$ per individual search and, in its classic formulation,\nis not vectorizable, i.e. is not SIMD friendly. This paper describes technical\nimprovements to the binary search algorithm, which make it faster and\nvectorizable. Next it proposes a new vectorizable algorithm, based on an\nindexing technique, applicable to a wide family of $X$ partitions, which solves\nthe problem with complexity $O(1)$ per individual search at the cost of\nintroducing an initial overhead to compute the index and requiring extra memory\nfor its storage. Test results using streaming SIMD extensions compare the\nperformance of the algorithm versus various benchmarks and demonstrate its\neffectiveness. Depending on the test case, the algorithm can produce a\nthroughput up to two orders of magnitude larger than the classic binary search.\nApplicability limitations and cache-friendliness related aspects are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 13:44:21 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 15:39:53 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 06:10:03 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Cannizzo", "Fabio", ""]]}, {"id": "1506.08752", "submitter": "Sivakumar Rathinam", "authors": "Satyanarayana Manyam, Sivakumar Rathinam", "title": "On Tightly Bounding the Dubins Traveling Salesman's Optimum", "comments": "Presented at the International Symposium on Mathematical Programming,\n  2015.\n  https://informs.emeetingsonline.com/emeetings/formbuilder/clustersessiondtl.asp?csnno=22283&mmnno=264&ppnno=86444", "journal-ref": "ASME Journal of Dynamic Systems, Measurement, and Control, 2018", "doi": "10.1115/1.4039099", "report-no": null, "categories": "math.OC cs.DM cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dubins Traveling Salesman Problem (DTSP) has generated significant\ninterest over the last decade due to its occurrence in several civil and\nmilitary surveillance applications. Currently, there is no algorithm that can\nfind an optimal solution to the problem. In addition, relaxing the motion\nconstraints and solving the resulting Euclidean TSP (ETSP) provides the only\nlower bound available for the problem. However, in many problem instances, the\nlower bound computed by solving the ETSP is far below the cost of the feasible\nsolutions obtained by some well-known algorithms for the DTSP. This article\naddresses this fundamental issue and presents the first systematic procedure\nfor developing tight lower bounds for the DTSP.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:12:37 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 18:06:47 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 10:52:45 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Manyam", "Satyanarayana", ""], ["Rathinam", "Sivakumar", ""]]}, {"id": "1506.08834", "submitter": "Aram Harrow", "authors": "Aram W. Harrow, Anand Natarajan, Xiaodi Wu", "title": "An improved semidefinite programming hierarchy for testing entanglement", "comments": "22 pages. v2: published version, adds numerical results. Matlab code\n  available at https://github.com/isobovine/dpsplus/", "journal-ref": "Commun. Math. Phys. Vol. 352, No. 3, pp 881-904 (2017)", "doi": "10.1007/s00220-017-2859-0", "report-no": "MIT-CTP/4587", "categories": "quant-ph cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stronger version of the Doherty-Parrilo-Spedalieri (DPS)\nhierarchy of approximations for the set of separable states. Unlike DPS, our\nhierarchy converges exactly at a finite number of rounds for any fixed input\ndimension. This yields an algorithm for separability testing which is singly\nexponential in dimension and polylogarithmic in accuracy. Our analysis makes\nuse of tools from algebraic geometry, but our algorithm is elementary and\ndiffers from DPS only by one simple additional collection of constraints.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 20:01:18 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 18:17:42 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Harrow", "Aram W.", ""], ["Natarajan", "Anand", ""], ["Wu", "Xiaodi", ""]]}, {"id": "1506.08977", "submitter": "Maurice Roux", "authors": "Maurice Roux", "title": "A comparative study of divisive hierarchical clustering algorithms", "comments": "11 pages, 1 figure", "journal-ref": "J. of Classification 35 (2018): 345-366", "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general scheme for divisive hierarchical clustering algorithms is proposed.\nIt is made of three main steps : first a splitting procedure for the\nsubdivision of clusters into two subclusters, second a local evaluation of the\nbipartitions resulting from the tentative splits and, third, a formula for\ndetermining the nodes levels of the resulting dendrogram. A number of such\nalgorithms is given. These algorithms are compared using the Goodman-Kruskal\ncorrelation coefficient. As a global criterion it is an internal\ngoodness-of-fit measure based on the set order induced by the hierarchy\ncompared to the order associated to the given dissimilarities. Applied to a\nhundred of random data tables, these comparisons are in favor of two methods\nbased on unusual ratio-type formulas for the splitting procedures, namely the\nSilhouette criterion and Dunn's criterion. These two criteria take into account\nboth the within cluster and the between cluster mean dissimilarity. In general\nthe results of these two algorithms are better than the classical Agglomerative\nAverage Link method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 07:54:25 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 09:07:24 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Roux", "Maurice", ""]]}, {"id": "1506.09100", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Robert Ganian, Martin Kalany, Stefan Szeider, Jesper Larsson Tr\\\"aff", "title": "Polynomial-time Construction of Optimal Tree-structured Communication\n  Data Layout Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of constructing tree-structured descriptions of data\nlayouts that are optimal with respect to space or other criteria from given\nsequences of displacements, can be solved in polynomial time. The problem is\nrelevant for efficient compiler and library support for communication of\nnoncontiguous data, where tree-structured descriptions with low-degree nodes\nand small index arrays are beneficial for the communication soft- and hardware.\nAn important example is the Message-Passing Interface (MPI) which has a\nmechanism for describing arbitrary data layouts as trees using a set of\nincreasingly general constructors. Our algorithm shows that the so-called MPI\ndatatype reconstruction problem by trees with the full set of MPI constructors\ncan be solved optimally in polynomial time, refuting previous conjectures that\nthe problem is NP-hard. Our algorithm can handle further, natural constructors,\ncurrently not found in MPI.\n  Our algorithm is based on dynamic programming, and requires the solution of a\nseries of shortest path problems on an incrementally built, directed, acyclic\ngraph. The algorithm runs in $O(n^4)$ time steps and requires $O(n^2)$ space\nfor input displacement sequences of length $n$.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 14:11:34 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Ganian", "Robert", ""], ["Kalany", "Martin", ""], ["Szeider", "Stefan", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1506.09145", "submitter": "David Wood", "authors": "Michael J. Bannister, William E. Devanny, Vida Dujmovi\\'c, David\n  Eppstein, David R. Wood", "title": "Track Layouts, Layered Path Decompositions, and Leveled Planarity", "comments": "19 pages, 8 figures", "journal-ref": "Algorithmica 81 (4): 1561-1583, 2019", "doi": "10.1007/s00453-018-0487-5", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two types of graph layouts, track layouts and layered path\ndecompositions, and the relations between their associated parameters\ntrack-number and layered pathwidth. We use these two types of layouts to\ncharacterize leveled planar graphs, which are the graphs with planar leveled\ndrawings with no dummy vertices. It follows from the known NP-completeness of\nleveled planarity that track-number and layered pathwidth are also NP-complete,\neven for the smallest constant parameter values that make these parameters\nnontrivial. We prove that the graphs with bounded layered pathwidth include\nouterplanar graphs, Halin graphs, and squaregraphs, but that (despite having\nbounded track-number) series-parallel graphs do not have bounded layered\npathwidth. Finally, we investigate the parameterized complexity of these\nlayouts, showing that past methods used for book layouts do not work to\nparameterize the problem by treewidth or almost-tree number but that the\nproblem is (non-uniformly) fixed-parameter tractable for tree-depth.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 16:20:54 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 10:47:44 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Bannister", "Michael J.", ""], ["Devanny", "William E.", ""], ["Dujmovi\u0107", "Vida", ""], ["Eppstein", "David", ""], ["Wood", "David R.", ""]]}, {"id": "1506.09158", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico, Damiano Carra, and Pietro Michiardi", "title": "On Fair Size-Based Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By executing jobs serially rather than in parallel, size-based scheduling\npolicies can shorten time needed to complete jobs; however, major obstacles to\ntheir applicability are fairness guarantees and the fact that job sizes are\nrarely known exactly a-priori. Here, we introduce the Pri family of size-based\nscheduling policies; Pri simulates any reference scheduler and executes jobs in\nthe order of their simulated completion: we show that these schedulers give\nstrong fairness guarantees, since no job completes later in Pri than in the\nreference policy. In addition, we introduce PSBS, a practical implementation of\nsuch a scheduler: it works online (i.e., without needing knowledge of jobs\nsubmitted in the future), it has an efficient O(log n) implementation and it\nallows setting priorities to jobs. Most importantly, unlike earlier size-based\npolicies, the performance of PSBS degrades gracefully with errors, leading to\nperformances that are close to optimal in a variety of realistic use cases.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:02:17 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Dell'Amico", "Matteo", ""], ["Carra", "Damiano", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1506.09208", "submitter": "Xiaohu Wu", "authors": "Patrick Loiseau and Xiaohu Wu", "title": "Improved Competitive Analysis of Online Scheduling Deadline-Sensitive\n  Jobs", "comments": "further improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following scheduling problem. There is a single machine and\nthe jobs will arrive for completion online. Each job j is preemptive and, upon\nits arrival, its other characteristics are immediately revealed to the machine:\nthe deadline requirement, the workload and the value. The objective is to\nmaximize the aggregate value of jobs completed by their deadlines. Using the\nminimum of the ratios of deadline minus arrival time to workload over all jobs\nas the slackness s, a non-committed and a committed online scheduling algorithm\nis proposed in [Lucier et al., SPAA'13; Azar et al., EC'15], achieving\ncompetitive ratios of 2+f(s), where the big O notation\nf(s)=\\mathcal{O}(\\frac{1}{(\\sqrt[3]{s}-1)^{2}}), and (2+f(s*b))/b respectively,\nwhere b=\\omega*(1-\\omega), \\omega is in (0, 1), and s is no less than 1/b. In\nthis paper, without recourse to the dual fitting technique used in the above\nworks, we propose a simpler and more intuitive analytical framework for the two\nalgorithms, improving the competitive ratio of the first algorithm by 1 and\ntherefore improving the competitive ratio of the second algorithm by 1/b. As\nstated in [Lucier et al., SPAA'13; Azar et al. EC'15], it is justifiable in\nscenarios like the online batch processing for cloud computing that the\nslackness s is large, hence the big O notation in the above competitive ratios\ncan be ignored. Under the assumption, our analysis brings very significant\nimprovements to the competitive ratios of the two algorithms: from 2 to 1 and\nfrom 2/b to 1/b respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 19:37:55 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 04:37:19 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Loiseau", "Patrick", ""], ["Wu", "Xiaohu", ""]]}]