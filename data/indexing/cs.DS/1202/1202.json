[{"id": "1202.0082", "submitter": "Jianhang Gao", "authors": "Jianhang Gao, Qing Zhao, Wei Ren, Ananthram Swami, Ram Ramanathan,\n  Amotz Bar-Noy", "title": "Dynamic Shortest Path Algorithms for Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hypergraph is a set V of vertices and a set of non-empty subsets of V,\ncalled hyperedges. Unlike graphs, hypergraphs can capture higher-order\ninteractions in social and communication networks that go beyond a simple union\nof pairwise relationships. In this paper, we consider the shortest path problem\nin hypergraphs. We develop two algorithms for finding and maintaining the\nshortest hyperpaths in a dynamic network with both weight and topological\nchanges. These two algorithms are the first to address the fully dynamic\nshortest path problem in a general hypergraph. They complement each other by\npartitioning the application space based on the nature of the change dynamics\nand the type of the hypergraph.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 02:19:46 GMT"}], "update_date": "2012-02-02", "authors_parsed": [["Gao", "Jianhang", ""], ["Zhao", "Qing", ""], ["Ren", "Wei", ""], ["Swami", "Ananthram", ""], ["Ramanathan", "Ram", ""], ["Bar-Noy", "Amotz", ""]]}, {"id": "1202.0314", "submitter": "Sergio Cabello", "authors": "Sergio Cabello and Erin Wolf Chambers and Jeff Erickson", "title": "Multiple-Source Shortest Paths in Embedded Graphs", "comments": "31 pages, 3 figures. Accepted to SIAM Journal on Computing.\n  Preliminary version, without the third author's contributions, in Proceedings\n  of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, 89-97, 2007. In\n  this revision we provide a careful treatment of non-generic weights", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G be a directed graph with n vertices and non-negative weights in its\ndirected edges, embedded on a surface of genus g, and let f be an arbitrary\nface of G. We describe a randomized algorithm to preprocess the graph in O(gn\nlog n) time with high probability, so that the shortest-path distance from any\nvertex on the boundary of f to any other vertex in G can be retrieved in O(log\nn) time. Our result directly generalizes the O(n log n)-time algorithm of Klein\n[SODA 2005] for multiple-source shortest paths in planar graphs. Intuitively,\nour preprocessing algorithm maintains a shortest-path tree as its source point\nmoves continuously around the boundary of f. As an application of our\nalgorithm, we describe algorithms to compute a shortest non-contractible or\nnon-separating cycle in embedded, undirected graphs in O(g^2 n log n) time with\nhigh probability. Our high-probability time bounds hold in the worst-case for\ngeneric edge weights, or with an additional O(log n) factor for arbitrary edge\nweights.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 22:24:46 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 14:28:25 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Cabello", "Sergio", ""], ["Chambers", "Erin Wolf", ""], ["Erickson", "Jeff", ""]]}, {"id": "1202.0319", "submitter": "Luigi Laura", "authors": "Giorgio Ausiello, Donatella Firmani, and Luigi Laura", "title": "Real-Time Monitoring of Undirected Networks: Articulation Points,\n  Bridges, and Connected and Biconnected Components", "comments": null, "journal-ref": null, "doi": "10.1002/net.21450", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first algorithm in the streaming model to\ncharacterize completely the biconnectivity properties of undirected networks:\narticulation points, bridges, and connected and biconnected components. The\nmotivation of our work was the development of a real-time algorithm to monitor\nthe connectivity of the Autonomous Systems (AS) Network, but the solution\nprovided is general enough to be applied to any network.\n  The network structure is represented by a graph, and the algorithm is\nanalyzed in the datastream framework. Here, as in the \\emph{on-line} model, the\ninput graph is revealed one item (i.e., graph edge) after the other, in an\non-line fashion; but, if compared to traditional on-line computation, there are\nstricter requirements for both memory occupation and per item processing time.\nOur algorithm works by properly updating a forest over the graph nodes. All the\ngraph (bi)connectivity properties are stored in this forest. We prove the\ncorrectness of the algorithm, together with its space ($O(n\\,\\log n)$, with $n$\nbeing the number of nodes in the graph) and time bounds.\n  We also present the results of a brief experimental evaluation against\nreal-world graphs, including many samples of the AS network, ranging from\nmedium to massive size. These preliminary experimental results confirm the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 23:10:58 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ausiello", "Giorgio", ""], ["Firmani", "Donatella", ""], ["Laura", "Luigi", ""]]}, {"id": "1202.0364", "submitter": "Ton Kloks", "authors": "Ton Kloks", "title": "A note on probe cographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G be a graph and let N_1, ..., N_k be k independent sets in G. The graph\nG is a k-probe cograph if G can be embedded into a cograph by adding edges\nbetween vertices that are contained in the same independent set. We show that\nthere exists an O(k n^5) algorithm to check if a graph G is a k-probe cograph.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 05:24:25 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Kloks", "Ton", ""]]}, {"id": "1202.0569", "submitter": "Alexander Souza", "authors": "Alexander Souza", "title": "A Constructive Proof of the Cycle Double Cover Conjecture", "comments": "Due to a flaw in Lemma 9, the paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cycle double cover conjecture states that a graph is bridge-free if and\nonly if there is a family of edge-simple cycles such that each edge is\ncontained in exactly two of them. It was formulated independently by Szekeres\n(1973) and Seymour (1979). In this paper, we settle the conjecture in the\naffirmative. In particular, we give an algorithm, which inductively constructs\na cycle double cover in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 21:47:55 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2012 10:43:09 GMT"}], "update_date": "2012-02-08", "authors_parsed": [["Souza", "Alexander", ""]]}, {"id": "1202.0922", "submitter": "Aleksandrs Slivkins", "authors": "Ittai Abraham, Shiri Chechik, David Kempe, and Aleksandrs Slivkins", "title": "Low-distortion Inference of Latent Similarities from a Multiplex Social\n  Network", "comments": "51 pages. Compared to the previous version: many small changes to\n  improve presentation and clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of social network analysis is - implicitly or explicitly - predicated on\nthe assumption that individuals tend to be more similar to their friends than\nto strangers. Thus, an observed social network provides a noisy signal about\nthe latent underlying \"social space:\" the way in which individuals are similar\nor dissimilar. Many research questions frequently addressed via social network\nanalysis are in reality questions about this social space, raising the question\nof inverting the process: Given a social network, how accurately can we\nreconstruct the social structure of similarities and dissimilarities?\n  We begin to address this problem formally. Observed social networks are\nusually multiplex, in the sense that they reflect (dis)similarities in several\ndifferent \"categories,\" such as geographical proximity, kinship, or similarity\nof professions/hobbies. We assume that each such category is characterized by a\nlatent metric capturing (dis)similarities in this category. Each category gives\nrise to a separate social network: a random graph parameterized by this metric.\nFor a concrete model, we consider Kleinberg's small world model and some\nvariations thereof. The observed social network is the unlabeled union of these\ngraphs, i.e., the presence or absence of edges can be observed, but not their\norigins. Our main result is an algorithm which reconstructs each metric with\nprovably low distortion.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2012 21:38:00 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 03:49:42 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2012 20:54:06 GMT"}, {"version": "v4", "created": "Fri, 15 Aug 2014 01:47:50 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Abraham", "Ittai", ""], ["Chechik", "Shiri", ""], ["Kempe", "David", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1202.1041", "submitter": "Ton Kloks", "authors": "Ton Kloks", "title": "Packing interval graphs with vertex-disjoint triangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exists a polynomial algorithm to pack interval graphs with\nvertex-disjoint triangles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 03:36:23 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Kloks", "Ton", ""]]}, {"id": "1202.1090", "submitter": "Sandeep Sen", "authors": "Sandeep Sen and V. N. Muralidhara", "title": "The covert set-cover problem with application to Network Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a version of the set-cover problem where we do not know the sets\ninitially (and hence referred to as covert) but we can query an element to find\nout which sets contain this element as well as query a set to know the\nelements. We want to find a small set-cover using a minimal number of such\nqueries. We present a Monte Carlo randomized algorithm that approximates an\noptimal set-cover of size $OPT$ within $O(\\log N)$ factor with high probability\nusing $O(OPT \\cdot \\log^2 N)$ queries where $N$ is the input size.\n  We apply this technique to the network discovery problem that involves\ncertifying all the edges and non-edges of an unknown $n$-vertices graph based\non layered-graph queries from a minimal number of vertices. By reducing it to\nthe covert set-cover problem we present an $O(\\log^2 n)$-competitive Monte\nCarlo randomized algorithm for the covert version of network discovery problem.\nThe previously best known algorithm has a competitive ratio of $\\Omega\n(\\sqrt{n\\log n})$ and therefore our result achieves an exponential improvement.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 10:14:24 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Sen", "Sandeep", ""], ["Muralidhara", "V. N.", ""]]}, {"id": "1202.1111", "submitter": "Rasmus Pagh", "authors": "Po-Shen Loh and Rasmus Pagh", "title": "Thresholds for Extreme Orientability", "comments": "Corrected description of relationship to the work of LeLarge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-choice load balancing has been a topic of intense study since the\nseminal paper of Azar, Broder, Karlin, and Upfal. Questions in this area can be\nphrased in terms of orientations of a graph, or more generally a k-uniform\nrandom hypergraph. A (d,b)-orientation is an assignment of each edge to d of\nits vertices, such that no vertex has more than b edges assigned to it.\nConditions for the existence of such orientations have been completely\ndocumented except for the \"extreme\" case of (k-1,1)-orientations. We consider\nthis remaining case, and establish:\n  - The density threshold below which an orientation exists with high\nprobability, and above which it does not exist with high probability.\n  - An algorithm for finding an orientation that runs in linear time with high\nprobability, with explicit polynomial bounds on the failure probability.\n  Previously, the only known algorithms for constructing (k-1,1)-orientations\nworked for k<=3, and were only shown to have expected linear running time.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 12:09:12 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2012 09:27:43 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2012 08:03:15 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Loh", "Po-Shen", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1202.1342", "submitter": "Nicolas Broutin", "authors": "Nicolas Broutin, Ralph Neininger, Henning Sulzbach", "title": "A limit process for partial match queries in random quadtrees and $2$-d\n  trees", "comments": "Published in at http://dx.doi.org/10.1214/12-AAP912 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note: text\n  overlap with arXiv:1107.2231", "journal-ref": "Annals of Applied Probability 2013, Vol. 23, No. 6, 2560-2603", "doi": "10.1214/12-AAP912", "report-no": "IMS-AAP-AAP912", "categories": "math.PR cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering items matching a partially specified\npattern in multidimensional trees (quadtrees and $k$-d trees). We assume the\ntraditional model where the data consist of independent and uniform points in\nthe unit square. For this model, in a structure on $n$ points, it is known that\nthe number of nodes $C_n(\\xi )$ to visit in order to report the items matching\na random query $\\xi$, independent and uniformly distributed on $[0,1]$,\nsatisfies $\\mathbf {E}[{C_n(\\xi )}]\\sim\\kappa n^{\\beta}$, where $\\kappa$ and\n$\\beta$ are explicit constants. We develop an approach based on the analysis of\nthe cost $C_n(s)$ of any fixed query $s\\in[0,1]$, and give precise estimates\nfor the variance and limit distribution of the cost $C_n(x)$. Our results\npermit us to describe a limit process for the costs $C_n(x)$ as $x$ varies in\n$[0,1]$; one of the consequences is that $\\mathbf\n{E}[{\\max_{x\\in[0,1]}C_n(x)}]\\sim \\gamma n^{\\beta}$; this settles a question of\nDevroye [Pers. Comm., 2000].\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 03:36:00 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2012 03:36:55 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 08:53:33 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2012 08:06:30 GMT"}, {"version": "v5", "created": "Thu, 5 Dec 2013 12:02:18 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Broutin", "Nicolas", ""], ["Neininger", "Ralph", ""], ["Sulzbach", "Henning", ""]]}, {"id": "1202.1370", "submitter": "Ralph Neininger", "authors": "Ralph Neininger, Henning Sulzbach", "title": "On a functional contraction method", "comments": "Published at http://dx.doi.org/10.1214/14-AOP919 in the Annals of\n  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Probability 2015, Vol. 43, No. 4, 1777-1822", "doi": "10.1214/14-AOP919", "report-no": "IMS-AOP-AOP919", "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for proving functional limit laws are developed for sequences of\nstochastic processes which allow a recursive distributional decomposition\neither in time or space. Our approach is an extension of the so-called\ncontraction method to the space $\\mathcal{C}[0,1]$ of continuous functions\nendowed with uniform topology and the space $\\mathcal {D}[0,1]$ of\nc\\`{a}dl\\`{a}g functions with the Skorokhod topology. The contraction method\noriginated from the probabilistic analysis of algorithms and random trees where\ncharacteristics satisfy natural distributional recurrences. It is based on\nstochastic fixed-point equations, where probability metrics can be used to\nobtain contraction properties and allow the application of Banach's fixed-point\ntheorem. We develop the use of the Zolotarev metrics on the spaces\n$\\mathcal{C}[0,1]$ and $\\mathcal{D}[0,1]$ in this context. Applications are\ngiven, in particular, a short proof of Donsker's functional limit theorem is\nderived and recurrences arising in the probabilistic analysis of algorithms are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 08:41:19 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 16:51:46 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2015 11:36:42 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Neininger", "Ralph", ""], ["Sulzbach", "Henning", ""]]}, {"id": "1202.1639", "submitter": "Nino Antulov-Fantulin", "authors": "Nino Antulov-Fantulin, Alen Lancic, Hrvoje Stefancic, Mile Sikic", "title": "FastSIR Algorithm: A Fast Algorithm for simulation of epidemic spread in\n  large networks by using SIR compartment model", "comments": "8 figures", "journal-ref": null, "doi": "10.1016/j.ins.2013.03.036", "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The epidemic spreading on arbitrary complex networks is studied in SIR\n(Susceptible Infected Recovered) compartment model. We propose our\nimplementation of a Naive SIR algorithm for epidemic simulation spreading on\nnetworks that uses data structures efficiently to reduce running time. The\nNaive SIR algorithm models full epidemic dynamics and can be easily upgraded to\nparallel version. We also propose novel algorithm for epidemic simulation\nspreading on networks called the FastSIR algorithm that has better average case\nrunning time than the Naive SIR algorithm. The FastSIR algorithm uses novel\napproach to reduce average case running time by constant factor by using\nprobability distributions of the number of infected nodes. Moreover, the\nFastSIR algorithm does not follow epidemic dynamics in time, but still captures\nall infection transfers. Furthermore, we also propose an efficient recursive\nmethod for calculating probability distributions of the number of infected\nnodes. Average case running time of both algorithms has also been derived and\nexperimental analysis was made on five different empirical complex networks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 09:58:07 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Antulov-Fantulin", "Nino", ""], ["Lancic", "Alen", ""], ["Stefancic", "Hrvoje", ""], ["Sikic", "Mile", ""]]}, {"id": "1202.1801", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Asaf Cohen, Chen Avin, Muriel M\\'edard", "title": "Network Coded Gossip with Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyze gossip algorithms for networks with correlated data. In\nthese networks, either the data to be distributed, the data already available\nat the nodes, or both, are correlated. This model is applicable for a variety\nof modern networks, such as sensor, peer-to-peer and content distribution\nnetworks.\n  Although coding schemes for correlated data have been studied extensively,\nthe focus has been on characterizing the rate region in static memory-free\nnetworks. In a gossip-based scheme, however, nodes communicate among each other\nby continuously exchanging packets according to some underlying communication\nmodel. The main figure of merit in this setting is the stopping time -- the\ntime required until nodes can successfully decode. While Gossip schemes are\npractical, distributed and scalable, they have only been studied for\nuncorrelated data.\n  We wish to close this gap by providing techniques to analyze network coded\ngossip in (dynamic) networks with correlated data. We give a clean framework\nfor oblivious network models that applies to a multitude of network and\ncommunication scenarios, specify a general setting for distributed correlated\ndata, and give tight bounds on the stopping times of network coded protocols in\nthis wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 19:42:29 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Cohen", "Asaf", ""], ["Avin", "Chen", ""], ["M\u00e9dard", "Muriel", ""]]}, {"id": "1202.1842", "submitter": "Ning Ruan", "authors": "Ning Ruan, Ruoming Jin, Guan Wang, Kun Huang", "title": "Network Backbone Discovery Using Edge Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we investigate the problem of network backbone discovery. In\ncomplex systems, a \"backbone\" takes a central role in carrying out the system\nfunctionality and carries the bulk of system traffic. It also both simplifies\nand highlight underlying networking structure. Here, we propose an integrated\ngraph theoretical and information theoretical network backbone model. We\ndevelop an efficient mining algorithm based on Kullback-Leibler divergence\noptimization procedure and maximal weight connected subgraph discovery\nprocedure. A detailed experimental evaluation demonstrates both the\neffectiveness and efficiency of our approach. The case studies in the real\nworld domain further illustrates the usefulness of the discovered network\nbackbones.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 21:46:17 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2012 16:30:52 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Ruan", "Ning", ""], ["Jin", "Ruoming", ""], ["Wang", "Guan", ""], ["Huang", "Kun", ""]]}, {"id": "1202.2092", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Gopal Pandurangan, David Peleg, Rajmohan Rajaraman,\n  Zhifeng Sun", "title": "Discovery through Gossip", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomized gossip-based processes in dynamic networks that are\nmotivated by discovery processes in large-scale distributed networks like\npeer-to-peer or social networks.\n  A well-studied problem in peer-to-peer networks is the resource discovery\nproblem. There, the goal for nodes (hosts with IP addresses) is to discover the\nIP addresses of all other hosts. In social networks, nodes (people) discover\nnew nodes through exchanging contacts with their neighbors (friends). In both\ncases the discovery of new nodes changes the underlying network - new edges are\nadded to the network - and the process continues in the changed network.\nRigorously analyzing such dynamic (stochastic) processes with a continuously\nself-changing topology remains a challenging problem with obvious applications.\n  This paper studies and analyzes two natural gossip-based discovery processes.\nIn the push process, each node repeatedly chooses two random neighbors and puts\nthem in contact (i.e., \"pushes\" their mutual information to each other). In the\npull discovery process, each node repeatedly requests or \"pulls\" a random\ncontact from a random neighbor. Both processes are lightweight, local, and\nnaturally robust due to their randomization.\n  Our main result is an almost-tight analysis of the time taken for these two\nrandomized processes to converge. We show that in any undirected n-node graph\nboth processes take O(n log^2 n) rounds to connect every node to all other\nnodes with high probability, whereas Omega(n log n) is a lower bound. In the\ndirected case we give an O(n^2 log n) upper bound and an Omega(n^2) lower bound\nfor strongly connected directed graphs. A key technical challenge that we\novercome is the analysis of a randomized process that itself results in a\nconstantly changing network which leads to complicated dependencies in every\nround.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 19:48:46 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Pandurangan", "Gopal", ""], ["Peleg", "David", ""], ["Rajaraman", "Rajmohan", ""], ["Sun", "Zhifeng", ""]]}, {"id": "1202.2097", "submitter": "Joel Oren", "authors": "Allan Borodin and Mark Braverman and Brendan Lucier and Joel Oren", "title": "Truthful Mechanisms for Competing Submodular Processes", "comments": "The latest version contains a revised Section 4 and Appendix D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to word-of-mouth advertising, we consider a\ngame-theoretic scenario in which competing advertisers want to target initial\nadopters in a social network. Each advertiser wishes to maximize the resulting\ncascade of influence, modeled by a general network diffusion process. However,\ncompetition between products may adversely impact the rate of adoption for any\ngiven firm. The resulting framework gives rise to complex preferences that\ndepend on the specifics of the stochastic diffusion model and the network\ntopology.\n  We study this model from the perspective of a central mechanism, such as a\nsocial networking platform, that can optimize seed placement as a service for\nthe advertisers. We ask: given the reported demands of the competing firms, how\nshould a mechanism choose seeds to maximize overall efficiency? Beyond the\nalgorithmic problem, competition raises issues of strategic behaviour: rational\nagents should not be incentivized to underreport their budget demands.\n  We show that when there are two players, the social welfare can be\n$2$-approximated by a polynomial-time strategyproof mechanism. Our mechanism is\ndefined recursively, randomizing the order in which advertisers are allocated\nseeds according to a particular greedy method. For three or more players, we\ndemonstrate that under additional assumptions (satisfied by many existing\nmodels of influence spread) there exists a simpler strategyproof\n$\\frac{e}{e-1}$-approximation mechanism; notably, this second mechanism is not\nnecessarily strategyproof when there are only two players.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 20:01:34 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2012 22:15:25 GMT"}, {"version": "v3", "created": "Sun, 8 Dec 2013 02:56:55 GMT"}, {"version": "v4", "created": "Tue, 25 Mar 2014 16:37:55 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Borodin", "Allan", ""], ["Braverman", "Mark", ""], ["Lucier", "Brendan", ""], ["Oren", "Joel", ""]]}, {"id": "1202.2156", "submitter": "P\\'aid\\'i Creed", "authors": "P\\'aid\\'i Creed and Mary Cryan", "title": "The number of Euler tours of a random directed graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we obtain the expectation and variance of the number of Euler\ntours of a random Eulerian directed graph with fixed out-degree sequence. We\nuse this to obtain the asymptotic distribution of the number of Euler tours of\na random $d$-in/$d$-out graph and prove a concentration result. We are then\nable to show that a very simple approach for uniform sampling or approximately\ncounting Euler tours yields algorithms running in expected polynomial time for\nalmost every $d$-in/$d$-out graph. We make use of the BEST theorem of de\nBruijn, van Aardenne-Ehrenfest, Smith and Tutte, which shows that the number of\nEuler tours of an Eulerian directed graph with out-degree sequence $\\mathbf{d}$\nis the product of the number of arborescences and the term\n$\\frac{1}{n}[\\prod_{v \\in V}(d_v-1)!]$. Therefore most of our effort is towards\nestimating the moments of the number of arborescences of a random graph with\nfixed out-degree sequence.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 00:18:35 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Creed", "P\u00e1id\u00ed", ""], ["Cryan", "Mary", ""]]}, {"id": "1202.2465", "submitter": "Jierui Xie", "authors": "Jierui Xie and Boleslaw K. Szymanski", "title": "Towards Linear Time Overlapping Community Detection in Social Networks", "comments": "PAKDD 2012", "journal-ref": "Proc. 16th PAKDD Pacific-Asia Conference on Knowledge Discovery\n  and Data Mining, Kuala Lumpur, Malaysia, 2012, Lecture Notes AI vol. 7302,\n  Part II, Springer, Berlin, Germany, 2012, pp. 25-36", "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Membership diversity is a characteristic aspect of social networks in which a\nperson may belong to more than one social group. For this reason, discovering\noverlapping structures is necessary for realistic social analysis. In this\npaper, we present a fast algorithm1, called SLPA, for overlapping community\ndetection in large-scale networks. SLPA spreads labels according to dynamic\ninteraction rules. It can be applied to both unipartite and bipartite networks.\nIt is also able to uncover overlapping nested hierarchy. The time complexity of\nSLPA scales linearly with the number of edges in the network. Experiments in\nboth synthetic and real- world networks show that SLPA has an excellent\nperformance in identifying both node and community level overlapping\nstructures.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2012 20:07:45 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Xie", "Jierui", ""], ["Szymanski", "Boleslaw K.", ""]]}, {"id": "1202.2466", "submitter": "Amitabh Trehan", "authors": "Amitabh Trehan", "title": "Self-healing systems and virtual structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern networks are large, highly complex and dynamic. Add to that the\nmobility of the agents comprising many of these networks. It is difficult or\neven impossible for such systems to be managed centrally in an efficient\nmanner. It is imperative for such systems to attain a degree of\nself-management. Self-healing i.e. the capability of a system in a good state\nto recover to another good state in face of an attack, is desirable for such\nsystems. In this paper, we discuss the self-healing model for dynamic\nreconfigurable systems. In this model, an omniscient adversary inserts or\ndeletes nodes from a network and the algorithm responds by adding a limited\nnumber of edges in order to maintain invariants of the network. We look at some\nof the results in this model and argue for their applicability and further\nextensions of the results and the model. We also look at some of the techniques\nwe have used in our earlier work, in particular, we look at the idea of\nmaintaining virtual graphs mapped over the existing network and assert that\nthis may be a useful technique to use in many problem domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2012 20:11:39 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 14:55:51 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Trehan", "Amitabh", ""]]}, {"id": "1202.2595", "submitter": "James Allen Fill", "authors": "James Allen Fill, Svante Janson", "title": "The number of bit comparisons used by Quicksort: an average-case\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analyses of many algorithms and data structures (such as digital search\ntrees) for searching and sorting are based on the representation of the keys\ninvolved as bit strings and so count the number of bit comparisons. On the\nother hand, the standard analyses of many other algorithms (such as Quicksort)\nare performed in terms of the number of key comparisons. We introduce the\nprospect of a fair comparison between algorithms of the two types by providing\nan average-case analysis of the number of bit comparisons required by\nQuicksort. Counting bit comparisons rather than key comparisons introduces an\nextra logarithmic factor to the asymptotic average total. We also provide a new\nalgorithm, \"BitsQuick\", that reduces this factor to constant order by\neliminating needless bit comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 01:07:21 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Fill", "James Allen", ""], ["Janson", "Svante", ""]]}, {"id": "1202.2599", "submitter": "James Allen Fill", "authors": "James Allen Fill, Takehiko Nakama", "title": "Distributional convergence for the number of symbol comparisons used by\n  QuickSelect", "comments": "The first paragraph in the proof of Theorem 3.1 has been corrected in\n  this revision, and references have been updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the search algorithm QuickSelect compares keys during its execution in\norder to find a key of target rank, it must operate on the keys'\nrepresentations or internal structures, which were ignored by the previous\nstudies that quantified the execution cost for the algorithm in terms of the\nnumber of required key comparisons. In this paper, we analyze running costs for\nthe algorithm that take into account not only the number of key comparisons but\nalso the cost of each key comparison. We suppose that keys are represented as\nsequences of symbols generated by various probabilistic sources and that\nQuickSelect operates on individual symbols in order to find the target key. We\nidentify limiting distributions for the costs and derive integral and series\nexpressions for the expectations of the limiting distributions. These\nexpressions are used to recapture previously obtained results on the number of\nkey comparisons required by the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 01:14:39 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 02:02:26 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Fill", "James Allen", ""], ["Nakama", "Takehiko", ""]]}, {"id": "1202.2601", "submitter": "James Allen Fill", "authors": "James Allen Fill", "title": "Distributional convergence for the number of symbol comparisons used by\n  QuickSort", "comments": "Published in at http://dx.doi.org/10.1214/12-AAP866 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2013, Vol. 23, No. 3, 1129-1147", "doi": "10.1214/12-AAP866", "report-no": "IMS-AAP-AAP866", "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous studies of the sorting algorithm QuickSort have used the number\nof key comparisons as a measure of the cost of executing the algorithm. Here we\nsuppose that the n independent and identically distributed (i.i.d.) keys are\neach represented as a sequence of symbols from a probabilistic source and that\nQuickSort operates on individual symbols, and we measure the execution cost as\nthe number of symbol comparisons. Assuming only a mild \"tameness\" condition on\nthe source, we show that there is a limiting distribution for the number of\nsymbol comparisons after normalization: first centering by the mean and then\ndividing by n. Additionally, under a condition that grows more restrictive as p\nincreases, we have convergence of moments of orders p and smaller. In\nparticular, we have convergence in distribution and convergence of moments of\nevery order whenever the source is memoryless, that is, whenever each key is\ngenerated as an infinite string of i.i.d. symbols. This is somewhat surprising;\neven for the classical model that each key is an i.i.d. string of unbiased\n(\"fair\") bits, the mean exhibits periodic fluctuations of order n.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 01:22:21 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2012 22:39:24 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2013 14:46:53 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Fill", "James Allen", ""]]}, {"id": "1202.2624", "submitter": "Daniel Harvey", "authors": "Vida Dujmovi\\'c, Daniel J. Harvey, Gwena\\\"el Joret, Bruce Reed, David\n  R. Wood", "title": "A linear-time algorithm for finding a complete graph minor in a dense\n  graph", "comments": "6 pages, 0 figures; Clarification added in several places, no change\n  to arguments or results", "journal-ref": "SIAM Journal on Discrete Mathematics, 27/4:1770--1774, 2013", "doi": "10.1137/120866725", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let g(t) be the minimum number such that every graph G with average degree\nd(G) \\geq g(t) contains a K_{t}-minor. Such a function is known to exist, as\noriginally shown by Mader. Kostochka and Thomason independently proved that\ng(t) \\in \\Theta(t*sqrt{log t}). This article shows that for all fixed \\epsilon\n> 0 and fixed sufficiently large t \\geq t(\\epsilon), if d(G) \\geq\n(2+\\epsilon)g(t) then we can find this K_{t}-minor in linear time. This\nimproves a previous result by Reed and Wood who gave a linear-time algorithm\nwhen d(G) \\geq 2^{t-2}.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 04:46:01 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2013 00:15:06 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Dujmovi\u0107", "Vida", ""], ["Harvey", "Daniel J.", ""], ["Joret", "Gwena\u00ebl", ""], ["Reed", "Bruce", ""], ["Wood", "David R.", ""]]}, {"id": "1202.2771", "submitter": "Michael Brautbar", "authors": "Christian Borgs, Michael Brautbar, Jennifer Chayes and Shang-Hua Teng", "title": "Multi-Scale Matrix Sampling and Sublinear-Time PageRank Computation", "comments": "Accepted to Internet Mathematics journal for publication. An extended\n  abstract of this paper appeared in WAW 2012 under the title \"A Sublinear Time\n  Algorithm for PageRank Computations\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A fundamental problem arising in many applications in Web science and social\nnetwork analysis is, given an arbitrary approximation factor $c>1$, to output a\nset $S$ of nodes that with high probability contains all nodes of PageRank at\nleast $\\Delta$, and no node of PageRank smaller than $\\Delta/c$. We call this\nproblem {\\sc SignificantPageRanks}. We develop a nearly optimal, local\nalgorithm for the problem with runtime complexity $\\tilde{O}(n/\\Delta)$ on\nnetworks with $n$ nodes. We show that any algorithm for solving this problem\nmust have runtime of ${\\Omega}(n/\\Delta)$, rendering our algorithm optimal up\nto logarithmic factors.\n  Our algorithm comes with two main technical contributions. The first is a\nmulti-scale sampling scheme for a basic matrix problem that could be of\ninterest on its own. In the abstract matrix problem it is assumed that one can\naccess an unknown {\\em right-stochastic matrix} by querying its rows, where the\ncost of a query and the accuracy of the answers depend on a precision parameter\n$\\epsilon$. At a cost propositional to $1/\\epsilon$, the query will return a\nlist of $O(1/\\epsilon)$ entries and their indices that provide an\n$\\epsilon$-precision approximation of the row. Our task is to find a set that\ncontains all columns whose sum is at least $\\Delta$, and omits any column whose\nsum is less than $\\Delta/c$. Our multi-scale sampling scheme solves this\nproblem with cost $\\tilde{O}(n/\\Delta)$, while traditional sampling algorithms\nwould take time $\\Theta((n/\\Delta)^2)$.\n  Our second main technical contribution is a new local algorithm for\napproximating personalized PageRank, which is more robust than the earlier ones\ndeveloped in \\cite{JehW03,AndersenCL06} and is highly efficient particularly\nfor networks with large in-degrees or out-degrees. Together with our multiscale\nsampling scheme we are able to optimally solve the {\\sc SignificantPageRanks}\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 15:49:20 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2012 16:27:31 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 12:37:43 GMT"}, {"version": "v4", "created": "Sat, 25 May 2013 21:54:26 GMT"}, {"version": "v5", "created": "Tue, 28 May 2013 10:51:15 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Borgs", "Christian", ""], ["Brautbar", "Michael", ""], ["Chayes", "Jennifer", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1202.2792", "submitter": "Shahar Dobzinski", "authors": "Shahar Dobzinski and Jan Vondrak", "title": "On the Hardness of Welfare Maximization in Combinatorial Auctions with\n  Submodular Valuations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new type of monotone submodular functions: \\emph{multi-peak\nsubmodular functions}. Roughly speaking, given a family of sets $\\cF$, we\nconstruct a monotone submodular function $f$ with a high value $f(S)$ for every\nset $S \\in {\\cF}$ (a \"peak\"), and a low value on every set that does not\nintersect significantly any set in $\\cF$.\n  We use this construction to show that a better than\n$(1-\\frac{1}{2e})$-approximation ($\\simeq 0.816$) for welfare maximization in\ncombinatorial auctions with submodular valuations is (1) impossible in the\ncommunication model, (2) NP-hard in the computational model where valuations\nare given explicitly. Establishing a constant approximation hardness for this\nproblem in the communication model was a long-standing open question. The\nvaluations we construct for the hardness result in the computational model\ndepend only on a constant number of items, and hence the result holds even if\nthe players can answer arbitrary queries about their valuation, including\ndemand queries.\n  We also study two other related problems that received some attention\nrecently: max-min allocation (for which we also get hardness of $(1-\\frac 1\n{2e}+\\epsilon)$-approximation, in both models), and combinatorial public\nprojects (for which we prove hardness of $(3/4+\\epsilon)$-approximation in the\ncommunication model, and hardness of $(1 -\\frac 1 e+\\epsilon)$-approximation in\nthe computational model, using constant size valuations).\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 17:09:58 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dobzinski", "Shahar", ""], ["Vondrak", "Jan", ""]]}, {"id": "1202.2820", "submitter": "Christina Boucher", "authors": "Christina Boucher, Gad M. Landau, Avivit Levy, David Pritchard and\n  Oren Weimann", "title": "On Approximating String Selection Problems with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in bioinformatics are about finding strings that approximately\nrepresent a collection of given strings. We look at more general problems where\nsome input strings can be classified as outliers. The Close to Most Strings\nproblem is, given a set S of same-length strings, and a parameter d, find a\nstring x that maximizes the number of \"non-outliers\" within Hamming distance d\nof x. We prove this problem has no PTAS unless ZPP=NP, correcting a decade-old\nmistake. The Most Strings with Few Bad Columns problem is to find a\nmaximum-size subset of input strings so that the number of non-identical\npositions is at most k; we show it has no PTAS unless P=NP. We also observe\nClosest to k Strings has no EPTAS unless W[1]=FPT. In sum, outliers help model\nproblems associated with using biological data, but we show the problem of\nfinding an approximate solution is computationally difficult.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 19:09:26 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Boucher", "Christina", ""], ["Landau", "Gad M.", ""], ["Levy", "Avivit", ""], ["Pritchard", "David", ""], ["Weimann", "Oren", ""]]}, {"id": "1202.2840", "submitter": "Danupon Nanongkai", "authors": "Parinya Chalermsook, Khaled Elbassioni, Danupon Nanongkai, He Sun", "title": "Geometric Pricing: How Low Dimensionality Helps in Approximability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following toy problem. There are $m$ rectangles and $n$ points\non the plane. Each rectangle $R$ is a consumer with budget $B_R$, who is\ninterested in purchasing the cheapest item (point) inside R, given that she has\nenough budget. Our job is to price the items to maximize the revenue. This\nproblem can also be defined on higher dimensions. We call this problem the\ngeometric pricing problem.\n  In this paper, we study a new class of problems arising from a geometric\naspect of the pricing problem. It intuitively captures typical real-world\nassumptions that have been widely studied in marketing research, healthcare\neconomics, etc. It also helps classify other well-known pricing problems, such\nas the highway pricing problem and the graph vertex pricing problem on planar\nand bipartite graphs. Moreover, this problem turns out to have close\nconnections to other natural geometric problems such as the geometric versions\nof the unique coverage and maximum feasible subsystem problems.\n  We show that the low dimensionality arising in this pricing problem does lead\nto improved approximation ratios, by presenting sublinear-approximation\nalgorithms for two central versions of the problem: unit-demand uniform-budget\nmin-buying and single-minded pricing problems. Our algorithm is obtained by\ncombining algorithmic pricing and geometric techniques. These results suggest\nthat considering geometric aspect might be a promising research direction in\nobtaining improved approximation algorithms for such pricing problems. To the\nbest of our knowledge, this is one of very few problems in the intersection\nbetween geometry and algorithmic pricing areas. Thus its study may lead to new\nalgorithmic techniques that could benefit both areas.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 20:36:33 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2012 09:13:11 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Elbassioni", "Khaled", ""], ["Nanongkai", "Danupon", ""], ["Sun", "He", ""]]}, {"id": "1202.2928", "submitter": "Zhenming Liu", "authors": "Sharon Goldberg and Zhenming Liu", "title": "The Diffusion of Networking Technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.NI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant interest in the networking community on the impact\nof cascade effects on the diffusion of networking technology upgrades in the\nInternet. Thinking of the global Internet as a graph, where each node\nrepresents an economically-motivated Internet Service Provider (ISP), a key\nproblem is to determine the smallest set of nodes that can trigger a cascade\nthat causes every other node in the graph to adopt the protocol. We design the\nfirst approximation algorithm with a provable performance guarantee for this\nproblem, in a model that captures the following key issue: a node's decision to\nupgrade should be influenced by the decisions of the remote nodes it wishes to\ncommunicate with.\n  Given an internetwork G(V,E) and threshold function \\theta, we assume that\nnode $u$ activates (upgrades to the new technology) when it is adjacent to a\nconnected component of active nodes in G of size exceeding node $u$'s threshold\n\\theta(u). Our objective is to choose the smallest set of nodes that can cause\nthe rest of the graph to activate. Our main contribution is an approximation\nalgorithm based on linear programming, which we complement with computational\nhardness results and a near-optimum integrality gap. Our algorithm, which does\nnot rely on submodular optimization techniques, also highlights the substantial\nalgorithmic difference between our problem and similar questions studied in the\ncontext of social networks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 03:17:33 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2012 21:27:50 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2012 20:22:27 GMT"}, {"version": "v4", "created": "Mon, 26 Nov 2012 20:53:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Goldberg", "Sharon", ""], ["Liu", "Zhenming", ""]]}, {"id": "1202.3097", "submitter": "Friedrich Slivovsky", "authors": "Friedrich Slivovsky and Stefan Szeider", "title": "Computing Resolution-Path Dependencies in Linear Time", "comments": "14 pages, SAT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternation of existential and universal quantifiers in a quantified\nboolean formula (QBF) generates dependencies among variables that must be\nrespected when evaluating the formula. Dependency schemes provide a general\nframework for representing such dependencies. Since it is generally intractable\nto determine dependencies exactly, a set of potential dependencies is computed\ninstead, which may include false positives. Among the schemes proposed so far,\nresolution-path dependencies introduce the fewest spurious dependencies. In\nthis work, we describe an algorithm that detects resolution-path dependencies\nin linear time, resolving a problem posed by Van Gelder (CP 2011).\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 17:33:34 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2012 20:56:46 GMT"}, {"version": "v3", "created": "Sat, 5 May 2012 10:13:45 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Slivovsky", "Friedrich", ""], ["Szeider", "Stefan", ""]]}, {"id": "1202.3173", "submitter": "Olga Holtz", "authors": "Grey Ballard, James Demmel, Olga Holtz, Benjamin Lipshitz, Oded\n  Schwartz", "title": "Communication-Optimal Parallel Algorithm for Strassen's Matrix\n  Multiplication", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.NA math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel matrix multiplication is one of the most studied fundamental\nproblems in distributed and high performance computing. We obtain a new\nparallel algorithm that is based on Strassen's fast matrix multiplication and\nminimizes communication. The algorithm outperforms all known parallel matrix\nmultiplication algorithms, classical and Strassen-based, both asymptotically\nand in practice.\n  A critical bottleneck in parallelizing Strassen's algorithm is the\ncommunication between the processors. Ballard, Demmel, Holtz, and Schwartz\n(SPAA'11) prove lower bounds on these communication costs, using expansion\nproperties of the underlying computation graph. Our algorithm matches these\nlower bounds, and so is communication-optimal. It exhibits perfect strong\nscaling within the maximum possible range.\n  Benchmarking our implementation on a Cray XT4, we obtain speedups over\nclassical and Strassen-based algorithms ranging from 24% to 184% for a fixed\nmatrix dimension n=94080, where the number of nodes ranges from 49 to 7203.\n  Our parallelization approach generalizes to other fast matrix multiplication\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 23:12:23 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Holtz", "Olga", ""], ["Lipshitz", "Benjamin", ""], ["Schwartz", "Oded", ""]]}, {"id": "1202.3177", "submitter": "Olga Holtz", "authors": "Grey Ballard, James Demmel, Olga Holtz, Benjamin Lipshitz, Oded\n  Schwartz", "title": "Strong Scaling of Matrix Multiplication Algorithms and\n  Memory-Independent Communication Lower Bounds", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.NA math.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel algorithm has perfect strong scaling if its running time on P\nprocessors is linear in 1/P, including all communication costs.\nDistributed-memory parallel algorithms for matrix multiplication with perfect\nstrong scaling have only recently been found. One is based on classical matrix\nmultiplication (Solomonik and Demmel, 2011), and one is based on Strassen's\nfast matrix multiplication (Ballard, Demmel, Holtz, Lipshitz, and Schwartz,\n2012). Both algorithms scale perfectly, but only up to some number of\nprocessors where the inter-processor communication no longer scales.\n  We obtain a memory-independent communication cost lower bound on classical\nand Strassen-based distributed-memory matrix multiplication algorithms. These\nbounds imply that no classical or Strassen-based parallel matrix multiplication\nalgorithm can strongly scale perfectly beyond the ranges already attained by\nthe two parallel algorithms mentioned above. The memory-independent bounds and\nthe strong scaling bounds generalize to other algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 23:42:19 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Holtz", "Olga", ""], ["Lipshitz", "Benjamin", ""], ["Schwartz", "Oded", ""]]}, {"id": "1202.3205", "submitter": "Julian Shun", "authors": "Guy Blelloch, Jeremy Fineman, Julian Shun", "title": "Greedy Sequential Maximal Independent Set and Matching are Parallel on\n  Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy sequential algorithm for maximal independent set (MIS) loops over\nthe vertices in arbitrary order adding a vertex to the resulting set if and\nonly if no previous neighboring vertex has been added. In this loop, as in many\nsequential loops, each iterate will only depend directly on a subset of the\nprevious iterates (i.e. knowing that any one of a vertices neighbors is in the\nMIS or knowing that it has no previous neighbors is sufficient to decide its\nfate). This leads to a dependence structure among the iterates. If this\nstructure is shallow then running the iterates in parallel while respecting the\ndependencies can lead to an efficient parallel implementation mimicking the\nsequential algorithm.\n  In this paper, we show that for any graph, and for a random ordering of the\nvertices, the dependence depth of the sequential greedy MIS algorithm is\npolylogarithmic (O(log^2 n) with high probability). Our results extend previous\nresults that show polylogarithmic bounds only for random graphs. We show\nsimilar results for a greedy maximal matching (MM). For both problems we\ndescribe simple linear work parallel algorithms based on the approach. The\nalgorithms allow for a smooth tradeoff between more parallelism and reduced\nwork, but always return the same result as the sequential greedy algorithms. We\npresent experimental results that demonstrate efficiency and the tradeoff\nbetween work and parallelism.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 05:19:40 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Blelloch", "Guy", ""], ["Fineman", "Jeremy", ""], ["Shun", "Julian", ""]]}, {"id": "1202.3208", "submitter": "Travis Gagie", "authors": "Travis Gagie and Pawe{\\l} Gawrychowski", "title": "Linear-Space Substring Range Counting over Polylogarithmic Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bille and G{\\o}rtz (2011) recently introduced the problem of substring range\ncounting, for which we are asked to store compactly a string $S$ of $n$\ncharacters with integer labels in ([0, u]), such that later, given an interval\n([a, b]) and a pattern $P$ of length $m$, we can quickly count the occurrences\nof $P$ whose first characters' labels are in ([a, b]). They showed how to store\n$S$ in $\\Oh{n \\log n / \\log \\log n}$ space and answer queries in $\\Oh{m + \\log\n\\log u}$ time. We show that, if $S$ is over an alphabet of size (\\polylog (n)),\nthen we can achieve optimal linear space. Moreover, if (u = n \\polylog (n)),\nthen we can also reduce the time to $\\Oh{m}$. Our results give linear space and\ntime bounds for position-restricted substring counting and the counting\nversions of indexing substrings with intervals, indexing substrings with gaps\nand aligned pattern matching.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 05:29:25 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Gagie", "Travis", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "1202.3261", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov (INRIA Sophia Antipolis), Nelly Litvak (EEMCS),\n  Marina Sokol (INRIA Sophia Antipolis), Don Towsley", "title": "Quick Detection of Nodes with Large Degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to quickly find top $k$ lists of nodes with the largest degrees\nin large complex networks. If the adjacency list of the network is known (not\noften the case in complex networks), a deterministic algorithm to find a node\nwith the largest degree requires an average complexity of O(n), where $n$ is\nthe number of nodes in the network. Even this modest complexity can be very\nhigh for large complex networks. We propose to use the random walk based\nmethod. We show theoretically and by numerical experiments that for large\nnetworks the random walk method finds good quality top lists of nodes with high\nprobability and with computational savings of orders of magnitude. We also\npropose stopping criteria for the random walk method which requires very little\nknowledge about the structure of the network.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 10:22:19 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Avrachenkov", "Konstantin", "", "INRIA Sophia Antipolis"], ["Litvak", "Nelly", "", "EEMCS"], ["Sokol", "Marina", "", "INRIA Sophia Antipolis"], ["Towsley", "Don", ""]]}, {"id": "1202.3311", "submitter": "Hideo Bannai", "authors": "Keisuke Goto and Hideo Bannai and Shunsuke Inenaga and Masayuki Takeda", "title": "Speeding-up $q$-gram mining on grammar-based compressed texts", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-31265-6_18", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for calculating $q$-gram frequencies on\nstrings represented in compressed form, namely, as a straight line program\n(SLP). Given an SLP $\\mathcal{T}$ of size $n$ that represents string $T$, the\nalgorithm computes the occurrence frequencies of all $q$-grams in $T$, by\nreducing the problem to the weighted $q$-gram frequencies problem on a\ntrie-like structure of size $m = |T|-\\mathit{dup}(q,\\mathcal{T})$, where\n$\\mathit{dup}(q,\\mathcal{T})$ is a quantity that represents the amount of\nredundancy that the SLP captures with respect to $q$-grams. The reduced problem\ncan be solved in linear time. Since $m = O(qn)$, the running time of our\nalgorithm is $O(\\min\\{|T|-\\mathit{dup}(q,\\mathcal{T}),qn\\})$, improving our\nprevious $O(qn)$ algorithm when $q = \\Omega(|T|/n)$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 14:13:02 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Goto", "Keisuke", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1202.3367", "submitter": "Richard Peng", "authors": "Jonathan A. Kelner, Gary Miller, Richard Peng", "title": "Faster Approximate Multicommodity Flow Using Quadratically Coupled Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum multicommodity flow problem is a natural generalization of the\nmaximum flow problem to route multiple distinct flows. Obtaining a $1-\\epsilon$\napproximation to the multicommodity flow problem on graphs is a well-studied\nproblem. In this paper we present an adaptation of recent advances in\nsingle-commodity flow algorithms to this problem. As the underlying linear\nsystems in the electrical problems of multicommodity flow problems are no\nlonger Laplacians, our approach is tailored to generate specialized systems\nwhich can be preconditioned and solved efficiently using Laplacians. Given an\nundirected graph with m edges and k commodities, we give algorithms that find\n$1-\\epsilon$ approximate solutions to the maximum concurrent flow problem and\nthe maximum weighted multicommodity flow problem in time\n$\\tilde{O}(m^{4/3}\\poly(k,\\epsilon^{-1}))$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 17:01:35 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2012 13:49:52 GMT"}, {"version": "v3", "created": "Tue, 8 May 2012 03:59:37 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Kelner", "Jonathan A.", ""], ["Miller", "Gary", ""], ["Peng", "Richard", ""]]}, {"id": "1202.3470", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford, Markus Jalsenius, Ely Porat, Benjamin Sach", "title": "Pattern Matching in Multiple Streams", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of deterministic pattern matching in multiple\nstreams. In this model, one symbol arrives at a time and is associated with one\nof s streaming texts. The task at each time step is to report if there is a new\nmatch between a fixed pattern of length m and a newly updated stream. As is\nusual in the streaming context, the goal is to use as little space as possible\nwhile still reporting matches quickly. We give almost matching upper and lower\nspace bounds for three distinct pattern matching problems. For exact matching\nwe show that the problem can be solved in constant time per arriving symbol and\nO(m+s) words of space. For the k-mismatch and k-difference problems we give\nO(k) time solutions that require O(m+ks) words of space. In all three cases we\nalso give space lower bounds which show our methods are optimal up to a single\nlogarithmic factor. Finally we set out a number of open problems related to\nthis new model for pattern matching.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 23:11:48 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2012 13:54:14 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Porat", "Ely", ""], ["Sach", "Benjamin", ""]]}, {"id": "1202.3505", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail", "title": "Near-optimal Coresets For Least-Squares Regression", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2013.2272457", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study (constrained) least-squares regression as well as multiple response\nleast-squares regression and ask the question of whether a subset of the data,\na coreset, suffices to compute a good approximate solution to the regression.\nWe give deterministic, low order polynomial-time algorithms to construct such\ncoresets with approximation guarantees, together with lower bounds indicating\nthat there is not much room for improvement upon our results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 03:07:35 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2013 20:58:43 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Boutsidis", "Christos", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1202.3639", "submitter": "Karthekeyan Chandrasekaran", "authors": "Karthekeyan Chandrasekaran and Richard Karp", "title": "Finding a most biased coin with fewest flips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a most biased coin among a set of coins by\ntossing the coins adaptively. The goal is to minimize the number of tosses\nuntil we identify a coin i* whose posterior probability of being most biased is\nat least 1-delta for a given delta. Under a particular probabilistic model, we\ngive an optimal algorithm, i.e., an algorithm that minimizes the expected\nnumber of future tosses. The problem is closely related to finding the best arm\nin the multi-armed bandit problem using adaptive strategies. Our algorithm\nemploys an optimal adaptive strategy -- a strategy that performs the best\npossible action at each step after observing the outcomes of all previous coin\ntosses. Consequently, our algorithm is also optimal for any starting history of\noutcomes. To our knowledge, this is the first algorithm that employs an optimal\nadaptive strategy under a Bayesian setting for this problem. Our proof of\noptimality employs tools from the field of Markov games.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 16:40:56 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2012 14:53:35 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2013 17:09:32 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Karp", "Richard", ""]]}, {"id": "1202.3739", "submitter": "Akshat Kumar", "authors": "Akshat Kumar, Shlomo Zilberstein", "title": "Message-Passing Algorithms for Quadratic Programming Formulations of MAP\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-428-435", "categories": "cs.AI cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing maximum a posteriori (MAP) estimation in graphical models is an\nimportant inference problem with many applications. We present message-passing\nalgorithms for quadratic programming (QP) formulations of MAP estimation for\npairwise Markov random fields. In particular, we use the concave-convex\nprocedure (CCCP) to obtain a locally optimal algorithm for the non-convex QP\nformulation. A similar technique is used to derive a globally convergent\nalgorithm for the convex QP relaxation of MAP. We also show that a recently\ndeveloped expectation-maximization (EM) algorithm for the QP formulation of MAP\ncan be derived from the CCCP perspective. Experiments on synthetic and\nreal-world problems confirm that our new approach is competitive with\nmax-product and its variations. Compared with CPLEX, we achieve more than an\norder-of-magnitude speedup in solving optimally the convex QP relaxation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Kumar", "Akshat", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1202.4072", "submitter": "Leah Epstein", "authors": "Leah Epstein and Asaf Levin", "title": "An efficient polynomial time approximation scheme for load balancing on\n  uniformly related machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider basic problems of non-preemptive scheduling on uniformly related\nmachines. For a given schedule, defined by a partition of the jobs into m\nsubsets corresponding to the m machines, C_i denotes the completion time of\nmachine i. Our goal is to find a schedule which minimizes or maximizes\n\\sum_{i=1}^m C_i^p for a fixed value of p such that 0<p<\\infty. For p>1 the\nminimization problem is equivalent to the well-known problem of minimizing the\n\\ell_p norm of the vector of the completion times of the machines, and for\n0<p<1 the maximization problem is of interest. Our main result is an efficient\npolynomial time approximation scheme (EPTAS) for each one of these problems.\nOur schemes use a non-standard application of the so-called shifting technique.\nWe focus on the work (total size of jobs) assigned to each machine and\nintroduce intervals of forbidden work. These intervals are defined so that the\nresulting effect on the goal function is sufficiently small. This allows the\npartition of the problem into sub-problems (with subsets of machines and jobs)\nwhose solutions are combined into the final solution using dynamic programming.\nOur results are the first EPTAS's for this natural class of load balancing\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 11:48:17 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "1202.4076", "submitter": "Tatiana Starikovskaya", "authors": "Gregory Kucherov, Yakov Nekrich, and Tatiana Starikovskaya", "title": "Cross-Document Pattern Matching", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-31265-6_16", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new variant of the string matching problem called cross-document\nstring matching, which is the problem of indexing a collection of documents to\nsupport an efficient search for a pattern in a selected document, where the\npattern itself is a substring of another document. Several variants of this\nproblem are considered, and efficient linear-space solutions are proposed with\nquery time bounds that either do not depend at all on the pattern size or\ndepend on it in a very limited way (doubly logarithmic). As a side result, we\npropose an improved solution to the weighted level ancestor problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 13:31:36 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Kucherov", "Gregory", ""], ["Nekrich", "Yakov", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1202.4080", "submitter": "Leah Epstein", "authors": "Gyorgy Dosa and Leah Epstein", "title": "Generalized selfish bin packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard bin packing is the problem of partitioning a set of items with\npositive sizes no larger than 1 into a minimum number of subsets (called bins)\neach having a total size of at most 1. In bin packing games, an item has a\npositive weight, and given a valid packing or partition of the items, each item\nhas a cost or a payoff associated with it. We study a class of bin packing\ngames where the payoff of an item is the ratio between its weight and the total\nweight of items packed with it, that is, the cost sharing is based linearly on\nthe weights of items. We study several types of pure Nash equilibria: standard\nNash equilibria, strong equilibria, strictly Pareto optimal equilibria, and\nweakly Pareto optimal equilibria. We show that any game of this class admits\nall these types of equilibria. We study the (asymptotic) prices of anarchy and\nstability (PoA and PoS) of the problem with respect to these four types of\nequilibria, for the two cases of general weights and of unit weights. We show\nthat while the case of general weights is strongly related to the well-known\nFirst Fit algorithm, and all the four PoA values are equal to 1.7, this is not\ntrue for unit weights. In particular, we show that all of them are strictly\nbelow 1.7, the strong PoA is equal to approximately 1.691 (another well-known\nnumber in bin packing) while the strictly Pareto optimal PoA is much lower. We\nshow that all the PoS values are equal to 1, except for those of strong\nequilibria, which is equal to 1.7 for general weights, and to approximately\n1.611824 for unit weights. This last value is not known to be the (asymptotic)\napproximation ratio of any well-known algorithm for bin packing. Finally, we\nstudy convergence to equilibria.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 14:03:50 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dosa", "Gyorgy", ""], ["Epstein", "Leah", ""]]}, {"id": "1202.4134", "submitter": "Vahab Mirrokni", "authors": "Vahab Mirrokni, Nithum Thain and Adrian Vetta", "title": "On the Implications of Lookahead Search in Game Playing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lookahead search is perhaps the most natural and widely used game playing\nstrategy. Given the practical importance of the method, the aim of this paper\nis to provide a theoretical performance examination of lookahead search in a\nwide variety of applications.\n  To determine a strategy play using lookahead search}, each agent predicts\nmultiple levels of possible re-actions to her move (via the use of a search\ntree), and then chooses the play that optimizes her future payoff accounting\nfor these re-actions. There are several choices of optimization function the\nagents can choose, where the most appropriate choice of function will depend on\nthe specifics of the actual game - we illustrate this in our examples.\nFurthermore, the type of search tree chosen by computationally-constrained\nagent can vary. We focus on the case where agents can evaluate only a bounded\nnumber, $k$, of moves into the future. That is, we use depth $k$ search trees\nand call this approach {\\em k-lookahead search}.\n  We apply our method in five well-known settings: AdWord auctions; industrial\norganization (Cournot's model); congestion games; valid-utility games and\nbasic-utility games; cost-sharing network design games. We consider two\nquestions. First, what is the expected social quality of outcome when agents\napply lookahead search? Second, what interactive behaviours can be exhibited\nwhen players use lookahead search?\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2012 07:54:48 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Mirrokni", "Vahab", ""], ["Thain", "Nithum", ""], ["Vetta", "Adrian", ""]]}, {"id": "1202.4160", "submitter": "Frank Gurski", "authors": "Frank Gurski, Patrick Gwydion Poullie", "title": "Interval Routing Schemes for Circular-Arc Graphs", "comments": "17 pages, to appear in \"International Journal of Foundations of\n  Computer Science\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval routing is a space efficient method to realize a distributed routing\nfunction. In this paper we show that every circular-arc graph allows a shortest\npath strict 2-interval routing scheme, i.e., by introducing a global order on\nthe vertices and assigning at most two (strict) intervals in this order to the\nends of every edge allows to depict a routing function that implies exclusively\nshortest paths. Since circular-arc graphs do not allow shortest path 1-interval\nrouting schemes in general, the result implies that the class of circular-arc\ngraphs has strict compactness 2, which was a hitherto open question.\nAdditionally, we show that the constructed 2-interval routing scheme is a\n1-interval routing scheme with at most one additional interval assigned at each\nvertex and we an outline algorithm to calculate the routing scheme for\ncircular-arc graphs in O(n^2) time, where n is the number of vertices.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2012 15:33:15 GMT"}, {"version": "v2", "created": "Sun, 2 Nov 2014 15:15:18 GMT"}, {"version": "v3", "created": "Fri, 25 Mar 2016 12:10:05 GMT"}, {"version": "v4", "created": "Mon, 27 Jun 2016 08:28:03 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Gurski", "Frank", ""], ["Poullie", "Patrick Gwydion", ""]]}, {"id": "1202.4302", "submitter": "Thang Nguyen Kim", "authors": "Johanne Cohen and Christoph D\\\"urr and Nguyen Kim Thang", "title": "Smooth Inequalities and Equilibrium Inefficiency in Scheduling Games", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study coordination mechanisms for Scheduling Games (with unrelated\nmachines). In these games, each job represents a player, who needs to choose a\nmachine for its execution, and intends to complete earliest possible. Our goal\nis to design scheduling policies that always admit a pure Nash equilibrium and\nguarantee a small price of anarchy for the l_k-norm social cost --- the\nobjective balances overall quality of service and fairness. We consider\npolicies with different amount of knowledge about jobs: non-clairvoyant,\nstrongly-local and local. The analysis relies on the smooth argument together\nwith adequate inequalities, called smooth inequalities. With this unified\nframework, we are able to prove the following results.\n  First, we study the inefficiency in l_k-norm social costs of a strongly-local\npolicy SPT and a non-clairvoyant policy EQUI. We show that the price of anarchy\nof policy SPT is O(k). We also prove a lower bound of Omega(k/log k) for all\ndeterministic, non-preemptive, strongly-local and non-waiting policies\n(non-waiting policies produce schedules without idle times). These results\nensure that SPT is close to optimal with respect to the class of l_k-norm\nsocial costs. Moreover, we prove that the non-clairvoyant policy EQUI has price\nof anarchy O(2^k).\n  Second, we consider the makespan (l_infty-norm) social cost by making\nconnection within the l_k-norm functions. We revisit some local policies and\nprovide simpler, unified proofs from the framework's point of view. With the\nhighlight of the approach, we derive a local policy Balance. This policy\nguarantees a price of anarchy of O(log m), which makes it the currently best\nknown policy among the anonymous local policies that always admit a pure Nash\nequilibrium.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 12:25:32 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Cohen", "Johanne", ""], ["D\u00fcrr", "Christoph", ""], ["Thang", "Nguyen Kim", ""]]}, {"id": "1202.4326", "submitter": "Magnus M. Halldorsson", "authors": "Yuval Emek and Magnus M. Halldorsson and Adi Rosen", "title": "Space-Constrained Interval Selection", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study streaming algorithms for the interval selection problem: finding a\nmaximum cardinality subset of disjoint intervals on the line. A deterministic\n2-approximation streaming algorithm for this problem is developed, together\nwith an algorithm for the special case of proper intervals, achieving improved\napproximation ratio of 3/2. We complement these upper bounds by proving that\nthey are essentially best possible in the streaming setting: it is shown that\nan approximation ratio of $2 - \\epsilon$ (or $3 / 2 - \\epsilon$ for proper\nintervals) cannot be achieved unless the space is linear in the input size. In\npassing, we also answer an open question of Adler and Azar \\cite{AdlerAzar03}\nregarding the space complexity of constant-competitive randomized preemptive\nonline algorithms for the same problem.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 13:56:20 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 14:03:34 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Emek", "Yuval", ""], ["Halldorsson", "Magnus M.", ""], ["Rosen", "Adi", ""]]}, {"id": "1202.4331", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Strong Backdoors to Nested Satisfiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knuth (1990) introduced the class of nested formulas and showed that their\nsatisfiability can be decided in polynomial time. We show that, parameterized\nby the size of a smallest strong backdoor set to the target class of nested\nformulas, checking the satisfiability of any CNF formula is fixed-parameter\ntractable. Thus, for any k>0, the satisfiability problem can be solved in\npolynomial time for any formula F for which there exists a variable set B of\nsize at most k such that for every truth assignment t to B, the formula F[t] is\nnested; moreover, the degree of the polynomial is independent of k.\n  Our algorithm uses the grid-minor theorem of Robertson and Seymour (1986) to\neither find that the incidence graph of the formula has bounded treewidth - a\ncase that is solved using model checking for monadic second order logic - or to\nfind many vertex-disjoint obstructions in the incidence graph. For the latter\ncase, new combinatorial arguments are used to find a small backdoor set.\nCombining both cases leads to an approximation algorithm producing a strong\nbackdoor set whose size is upper bounded by a function of the optimum. Going\nthrough all assignments to this set of variables and using Knuth's algorithm,\nthe satisfiability of the input formula is decided.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 14:10:02 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 16:26:35 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1202.4419", "submitter": "Erik Jan van Leeuwen", "authors": "Petr A. Golovach and Daniel Paulusma and Erik Jan van Leeuwen", "title": "Induced Disjoint Paths in Claw-Free Graphs", "comments": "Minor revision. Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paths P1,...,Pk in a graph G=(V,E) are said to be mutually induced if for any\n1 <= i < j <= k, Pi and Pj have neither common vertices nor adjacent vertices\n(except perhaps their end-vertices). The Induced Disjoint Paths problem is to\ntest whether a graph G with k pairs of specified vertices (si,ti) contains k\nmutually induced paths Pi such that Pi connects si and ti for i=1,...,k. We\nshow that this problem is fixed-parameter tractable for claw-free graphs when\nparameterized by k. Several related problems, such as the k-in-a-Path problem,\nare proven to be fixed-parameter tractable for claw-free graphs as well. We\nshow that an improvement of these results in certain directions is unlikely,\nfor example by noting that the Induced Disjoint Paths problem cannot have a\npolynomial kernel for line graphs (a type of claw-free graphs), unless NP\n\\subseteq coNP/poly. Moreover, the problem becomes NP-complete, even when k=2,\nfor the more general class of K_1,4-free graphs. Finally, we show that the\nn^O(k)-time algorithm of Fiala et al. for testing whether a claw-free graph\ncontains some k-vertex graph H as a topological induced minor is essentially\noptimal by proving that this problem is W[1]-hard even if G and H are line\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 19:00:14 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 13:47:13 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Golovach", "Petr A.", ""], ["Paulusma", "Daniel", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1202.4473", "submitter": "Aleksandrs Slivkins", "authors": "Sebastien Bubeck and Aleksandrs Slivkins", "title": "The best of both worlds: stochastic and adversarial bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal),\nwhose regret is, essentially, optimal both for adversarial rewards and for\nstochastic rewards. Specifically, SAO combines the square-root worst-case\nregret of Exp3 (Auer et al., SIAM J. on Computing 2002) and the\n(poly)logarithmic regret of UCB1 (Auer et al., Machine Learning 2002) for\nstochastic rewards. Adversarial rewards and stochastic rewards are the two main\nsettings in the literature on (non-Bayesian) multi-armed bandits. Prior work on\nmulti-armed bandits treats them separately, and does not attempt to jointly\noptimize for both. Our result falls into a general theme of achieving good\nworst-case performance while also taking advantage of \"nice\" problem instances,\nan important issue in the design of algorithms with partially known inputs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 21:29:28 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Bubeck", "Sebastien", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1202.4504", "submitter": "Yuval Rabani", "authors": "Noa Avigdor-Elgrabli and Yuval Rabani", "title": "A Constant Factor Approximation Algorithm for Reordering Buffer\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the reordering buffer management problem (RBM) a sequence of $n$ colored\nitems enters a buffer with limited capacity $k$. When the buffer is full, one\nitem is removed to the output sequence, making room for the next input item.\nThis step is repeated until the input sequence is exhausted and the buffer is\nempty. The objective is to find a sequence of removals that minimizes the total\nnumber of color changes in the output sequence. The problem formalizes numerous\napplications in computer and production systems, and is known to be NP-hard.\n  We give the first constant factor approximation guarantee for RBM. Our\nalgorithm is based on an intricate \"rounding\" of the solution to an LP\nrelaxation for RBM, so it also establishes a constant upper bound on the\nintegrality gap of this relaxation. Our results improve upon the best previous\nbound of $O(\\sqrt{\\log k})$ of Adamaszek et al. (STOC 2011) that used different\nmethods and gave an online algorithm. Our constant factor approximation beats\nthe super-constant lower bounds on the competitive ratio given by Adamaszek et\nal. This is the first demonstration of an offline algorithm for RBM that is\nprovably better than any online algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 01:28:05 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Avigdor-Elgrabli", "Noa", ""], ["Rabani", "Yuval", ""]]}, {"id": "1202.4576", "submitter": "Maxwell Young", "authors": "Seth Gilbert, Maxwell Young", "title": "Making Evildoers Pay: Resource-Competitive Broadcast in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a time-slotted, single-hop, wireless sensor network (WSN) consisting\nof n correct devices and and t=f*n Byzantine devices where f>=0 is any\nconstant; that is, the Byzantine devices may outnumber the correct ones. There\nexists a trusted sender Alice who wishes to deliver a message m over a single\nchannel to the correct devices. There also exists a malicious user Carol who\ncontrols the t Byzantine devices and uses them to disrupt the communication\nchannel. For a constant k>=2, the correct and Byzantine devices each possess a\nmeager energy budget of O(n^{1/k}), Alice and Carol each possess a limited\nbudget of \\tilde{O}(n^{1/k}), and sending or listening in a slot incurs unit\ncost. This general setup captures the inherent challenges of guaranteeing\ncommunication despite scarce resources and attacks on the network. Given this\nAlice versus Carol scenario, we ask: Is communication of m feasible and, if so,\nat what cost?\n  We develop a protocol which, for an arbitrarily small constant \\epsilon>0,\nensures that at least (1-\\epsilon)n correct devices receive m with high\nprobability. Furthermore, if Carol's devices expend T energy jamming the\nchannel, then Alice and the correct devices each spend only\n\\tilde{O}(T^{1/(k+1)}). In other words, delaying the transmission of m forces a\njammer to rapidly deplete its energy supply and, consequently, cease attacks on\nthe network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 09:42:51 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2012 05:36:14 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2012 17:48:38 GMT"}, {"version": "v4", "created": "Tue, 15 May 2012 00:45:22 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Gilbert", "Seth", ""], ["Young", "Maxwell", ""]]}, {"id": "1202.4665", "submitter": "George Mertzios", "authors": "George B. Mertzios and Paul G. Spirakis", "title": "Algorithms and Almost Tight Results for 3-Colorability of Small Diameter\n  Graphs", "comments": "25 pages, 10 figures, 2 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the extensive studies of the 3-coloring problem with respect to\nseveral basic parameters, the complexity status of the 3-coloring problem on\ngraphs with small diameter, i.e. with diameter 2 or 3, has been a longstanding\nand challenging open question. For graphs with diameter 2 we provide the first\nsubexponential algorithm with complexity $2^{O(\\sqrt{n\\log n})}$, which is\nasymptotically the same as the currently best known time complexity for the\ngraph isomorphism (GI) problem. Moreover, we prove that the graph isomorphism\nproblem on 3-colorable graphs with diameter 2 is GI-complete. Furthermore we\npresent a subclass of graphs with diameter 2 that admits a polynomial algorithm\nfor 3-coloring. For graphs with diameter 3 we establish the complexity of\n3-coloring by proving that for every $\\varepsilon \\in [0,1)$, 3-coloring is\nNP-complete on triangle-free graphs of diameter 3 and radius 2 with $n$\nvertices and minimum degree $\\delta=\\Theta(n^{\\varepsilon})$. Moreover,\nassuming ETH, we provide three different amplifications of our hardness results\nto obtain for every $\\varepsilon \\in [0,1)$ subexponential lower bounds for the\ncomplexity of 3-coloring on triangle-free graphs with diameter 3 and minimum\ndegree $\\delta=\\Theta(n^{\\varepsilon})$. Finally, we provide a 3-coloring\nalgorithm with running time\n$2^{O(\\min\\{\\delta\\Delta,\\frac{n}{\\delta}\\log\\delta\\})}$ for graphs with\ndiameter 3, where $\\delta$ (resp. $\\Delta $) is the minimum (resp. maximum)\ndegree of the input graph. To the best of our knowledge, this algorithm is the\nfirst subexponential algorithm for graphs with $\\delta=\\omega(1)$ and for\ngraphs with $\\delta=O(1)$ and $\\Delta=o(n)$. Due to the above lower bounds of\nthe complexity of 3-coloring, the running time of this algorithm is\nasymptotically almost tight when the minimum degree if the input graph is\n$\\delta=\\Theta(n^{\\varepsilon})$, where $\\varepsilon \\in [1/2,1)$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 14:59:39 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 17:33:21 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2012 01:16:33 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Mertzios", "George B.", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1202.4741", "submitter": "Aaron Roth", "authors": "Katrina Ligett, Aaron Roth", "title": "Take it or Leave it: Running a Survey when Privacy Comes at a Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating a potentially sensitive\n(individually stigmatizing) statistic on a population. In our model,\nindividuals are concerned about their privacy, and experience some cost as a\nfunction of their privacy loss. Nevertheless, they would be willing to\nparticipate in the survey if they were compensated for their privacy cost.\nThese cost functions are not publicly known, however, nor do we make Bayesian\nassumptions about their form or distribution. Individuals are rational and will\nmisreport their costs for privacy if doing so is in their best interest. Ghosh\nand Roth recently showed in this setting, when costs for privacy loss may be\ncorrelated with private types, if individuals value differential privacy, no\nindividually rational direct revelation mechanism can compute any non-trivial\nestimate of the population statistic. In this paper, we circumvent this\nimpossibility result by proposing a modified notion of how individuals\nexperience cost as a function of their privacy loss, and by giving a mechanism\nwhich does not operate by direct revelation. Instead, our mechanism has the\nability to randomly approach individuals from a population and offer them a\ntake-it-or-leave-it offer. This is intended to model the abilities of a\nsurveyor who may stand on a street corner and approach passers-by.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 20:29:07 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2012 15:25:57 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Ligett", "Katrina", ""], ["Roth", "Aaron", ""]]}, {"id": "1202.4910", "submitter": "Justin Hsu", "authors": "Justin Hsu, Sanjeev Khanna, Aaron Roth", "title": "Distributed Private Heavy Hitters", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-31594-7_39", "report-no": null, "categories": "cs.DS cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give efficient algorithms and lower bounds for solving the\nheavy hitters problem while preserving differential privacy in the fully\ndistributed local model. In this model, there are n parties, each of which\npossesses a single element from a universe of size N. The heavy hitters problem\nis to find the identity of the most common element shared amongst the n\nparties. In the local model, there is no trusted database administrator, and so\nthe algorithm must interact with each of the $n$ parties separately, using a\ndifferentially private protocol. We give tight information-theoretic upper and\nlower bounds on the accuracy to which this problem can be solved in the local\nmodel (giving a separation between the local model and the more common\ncentralized model of privacy), as well as computationally efficient algorithms\neven in the case where the data universe N may be exponentially large.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 14:01:57 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 03:16:31 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 22:24:26 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Khanna", "Sanjeev", ""], ["Roth", "Aaron", ""]]}, {"id": "1202.4945", "submitter": "Sarah Miracle", "authors": "Sarah Miracle, Dana Randall, Amanda Pascoe Streib and Prasad Tetali", "title": "Algorithms for Sampling 3-Orientations of Planar Triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a planar triangulation, a 3-orientation is an orientation of the\ninternal edges so all internal vertices have out-degree three. Each\n3-orientation gives rise to a unique edge coloring known as a Schnyder wood\nthat has proven powerful for various computing and combinatorics applications.\nWe consider natural Markov chains for sampling uniformly from the set of\n3-orientations. First, we study a \"triangle-reversing\" chain on the space of\n3-orientations of a fixed triangulation that reverses the orientation of the\nedges around a triangle in each move. It was shown previously that this chain\nconnects the state space and we show that (i) when restricted to planar\ntriangulations of maximum degree six, the Markov chain is rapidly mixing, and\n(ii) there exists a triangulation with high degree on which this Markov chain\nmixes slowly. Next, we consider an \"edge-flipping\" chain on the larger state\nspace consisting of 3-orientations of all planar triangulations on a fixed\nnumber of vertices. It was also shown previously that this chain connects the\nstate space and we prove that the chain is always rapidly mixing. The\ntriangle-reversing and edge-flipping Markov chains both arise in the context of\nsampling other combinatorial structures, such as Eulerian orientations and\ntriangulations of planar point sets, so our results here may shed light on the\nmixing rate of these related chains as well.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 15:45:21 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Miracle", "Sarah", ""], ["Randall", "Dana", ""], ["Streib", "Amanda Pascoe", ""], ["Tetali", "Prasad", ""]]}, {"id": "1202.4961", "submitter": "Daniel Lemire", "authors": "Owen Kaser and Daniel Lemire", "title": "Strongly universal string hashing is fast", "comments": "Software is available at\n  http://code.google.com/p/variablelengthstringhashing/ and\n  https://github.com/lemire/StronglyUniversalStringHashing", "journal-ref": "Computer Journal (2014) 57 (11): 1624-1638", "doi": "10.1093/comjnl/bxt070", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present fast strongly universal string hashing families: they can process\ndata at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that\nthese families---though they require a large buffer of random numbers---are\noften faster than popular hash functions with weaker theoretical guarantees.\nMoreover, conventional wisdom is that hash functions with fewer multiplications\nare faster. Yet we find that they may fail to be faster due to operation\npipelining. We present experimental results on several processors including\nlow-powered processors. Our tests include hash functions designed for\nprocessors with the Carry-Less Multiplication (CLMUL) instruction set. We also\nprove, using accessible proofs, the strong universality of our families.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 16:34:24 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 22:17:05 GMT"}, {"version": "v3", "created": "Wed, 15 May 2013 01:16:04 GMT"}, {"version": "v4", "created": "Wed, 22 May 2013 17:36:50 GMT"}, {"version": "v5", "created": "Thu, 15 May 2014 14:43:50 GMT"}, {"version": "v6", "created": "Thu, 20 Sep 2018 19:33:55 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Kaser", "Owen", ""], ["Lemire", "Daniel", ""]]}, {"id": "1202.4970", "submitter": "Raghu Meka", "authors": "Raghu Meka", "title": "A polynomial time approximation scheme for computing the supremum of\n  Gaussian processes", "comments": "Published in at http://dx.doi.org/10.1214/13-AAP997 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 2, 465-476", "doi": "10.1214/13-AAP997", "report-no": "IMS-AAP-AAP997", "categories": "cs.DS math.FA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time approximation scheme (PTAS) for computing the\nsupremum of a Gaussian process. That is, given a finite set of vectors\n$V\\subseteq\\mathbb{R}^d$, we compute a $(1+\\varepsilon)$-factor approximation\nto $\\mathop {\\mathbb{E}}_{X\\leftarrow\\mathcal{N}^d}[\\sup_{v\\in V}|\\langle\nv,X\\rangle|]$ deterministically in time $\\operatorname\n{poly}(d)\\cdot|V|^{O_{\\varepsilon}(1)}$. Previously, only a constant factor\ndeterministic polynomial time approximation algorithm was known due to the work\nof Ding, Lee and Peres [Ann. of Math. (2) 175 (2012) 1409-1471]. This answers\nan open question of Lee (2010) and Ding [Ann. Probab. 42 (2014) 464-496]. The\nstudy of supremum of Gaussian processes is of considerable importance in\nprobability with applications in functional analysis, convex geometry, and in\nlight of the recent breakthrough work of Ding, Lee and Peres [Ann. of Math. (2)\n175 (2012) 1409-1471], to random walks on finite graphs. As such our result\ncould be of use elsewhere. In particular, combining with the work of Ding [Ann.\nProbab. 42 (2014) 464-496], our result yields a PTAS for computing the cover\ntime of bounded-degree graphs. Previously, such algorithms were known only for\ntrees. Along the way, we also give an explicit oblivious estimator for\nsemi-norms in Gaussian space with optimal query complexity. Our algorithm and\nits analysis are elementary in nature, using two classical comparison\ninequalities, Slepian's lemma and Kanter's lemma.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 16:52:16 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 13:57:15 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Meka", "Raghu", ""]]}, {"id": "1202.5003", "submitter": "Jens M. Schmidt", "authors": "Jens M. Schmidt", "title": "A Planarity Test via Construction Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal linear-time algorithms for testing the planarity of a graph are\nwell-known for over 35 years. However, these algorithms are quite involved and\nrecent publications still try to give simpler linear-time tests. We give a\nsimple reduction from planarity testing to the problem of computing a certain\nconstruction of a 3-connected graph. The approach is different from previous\nplanarity tests; as key concept, we maintain a planar embedding that is\n3-connected at each point in time. The algorithm runs in linear time and\ncomputes a planar embedding if the input graph is planar and a\nKuratowski-subdivision otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 18:46:56 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Schmidt", "Jens M.", ""]]}, {"id": "1202.5049", "submitter": "Konstantinos Georgiou", "authors": "Isaac Fung, Konstantinos Georgiou, Jochen Koenemann, Malcolm Sharpe", "title": "Efficient Algorithms for Solving Hypergraphic Steiner Tree Relaxations\n  in Quasi-Bipartite Instances", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Steiner tree problem in quasi-bipartite graphs, where no two\nSteiner vertices are connected by an edge. For this class of instances, we\npresent an efficient algorithm to exactly solve the so called directed\ncomponent relaxation (DCR), a specific form of hypergraphic LP relaxation that\nwas instrumental in the recent break-through result by Byrka et al. [BGRS10]\n(STOC 2010). Our algorithm hinges on an efficiently computable map from extreme\npoints of the bidirected cut relaxation to feasible solutions of (DCR). As a\nconsequence, together with [BGRS10] we immediately obtain an efficient\n73/60-approximation for quasi-bipartite Steiner tree instances. We also present\na particularly simple (BCR)-based random sampling algorithm that achieves a\nperformance guarantee slightly better than 77/60.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 21:19:30 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Fung", "Isaac", ""], ["Georgiou", "Konstantinos", ""], ["Koenemann", "Jochen", ""], ["Sharpe", "Malcolm", ""]]}, {"id": "1202.5074", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Solving Single-digit Sudoku Subproblems", "comments": "12 pages, 5 figures. To appear at the 6th International Conference on\n  Fun with Algorithms (FUN 2012). This revision simplifies the algorithm\n  description and adds more references to related prior work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that single-digit \"Nishio\" subproblems in nxn Sudoku puzzles may be\nsolved in time o(2^n), faster than previous solutions such as the pattern\noverlay method. We also show that single-digit deduction in Sudoku is NP-hard.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 01:30:56 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2012 21:52:05 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1202.5233", "submitter": "Tatiana Starikovskaya", "authors": "Tatiana Starikovskaya", "title": "Computing Lempel-Ziv Factorization Online", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-32589-2", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm which computes the Lempel-Ziv factorization of a word\n$W$ of length $n$ on an alphabet $\\Sigma$ of size $\\sigma$ online in the\nfollowing sense: it reads $W$ starting from the left, and, after reading each\n$r = O(\\log_{\\sigma} n)$ characters of $W$, updates the Lempel-Ziv\nfactorization. The algorithm requires $O(n \\log \\sigma)$ bits of space and O(n\n\\log^2 n) time. The basis of the algorithm is a sparse suffix tree combined\nwith wavelet trees.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 16:50:51 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2012 13:47:09 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2012 14:45:45 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2012 10:05:56 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Starikovskaya", "Tatiana", ""]]}, {"id": "1202.5471", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Xu Zhang, Xiaoqing Wang, Lotfi Senhadji, Huazhong Shu", "title": "L1-norm minimization for quaternion signals", "comments": "4 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The l1-norm minimization problem plays an important role in the compressed\nsensing (CS) theory. We present in this letter an algorithm for solving the\nproblem of l1-norm minimization for quaternion signals by converting it to\nsecond-order cone programming. An application example of the proposed algorithm\nis also given for practical guidelines of perfect recovery of quaternion\nsignals. The proposed algorithm may find its potential application when CS\ntheory meets the quaternion signal processing.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 15:01:37 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Wu", "Jiasong", ""], ["Zhang", "Xu", ""], ["Wang", "Xiaoqing", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1202.5619", "submitter": "Soroush Alamdari", "authors": "Soroush Alamdari, Elaheh Fata and Stephen L. Smith", "title": "Persistent Monitoring in Discrete Environments: Minimizing the Maximum\n  Weighted Latency Between Observations", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of planning a path for a robot to\nmonitor a known set of features of interest in an environment. We represent the\nenvironment as a graph with vertex weights and edge lengths. The vertices\nrepresent regions of interest, edge lengths give travel times between regions,\nand the vertex weights give the importance of each region. As the robot\nrepeatedly performs a closed walk on the graph, we define the weighted latency\nof a vertex to be the maximum time between visits to that vertex, weighted by\nthe importance (vertex weight) of that vertex. Our goal is to find a closed\nwalk that minimizes the maximum weighted latency of any vertex. We show that\nthere does not exist a polynomial time algorithm for the problem. We then\nprovide two approximation algorithms; an $O(\\log n)$-approximation algorithm\nand an $O(\\log \\rho_G)$-approximation algorithm, where $\\rho_G$ is the ratio\nbetween the maximum and minimum vertex weights. We provide simulation results\nwhich demonstrate that our algorithms can be applied to problems consisting of\nthousands of vertices, and a case study for patrolling a city for crime.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 06:50:39 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 23:29:45 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Alamdari", "Soroush", ""], ["Fata", "Elaheh", ""], ["Smith", "Stephen L.", ""]]}, {"id": "1202.5670", "submitter": "Pawel Gawrychowski", "authors": "Pawel Gawrychowski", "title": "(Really) Tight bounds for dispatching binary methods", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider binary dispatching problem originating from object oriented\nprogramming. We want to preprocess a hierarchy of classes and collection of\nmethods so that given a function call in the run-time we are able to retrieve\nthe most specialized implementation which can be invoked with the actual types\nof the arguments. For the binary dispatching, where the methods take exactly\ntwo arguments, logarithmic query time is possible, even if the structure is\nallowed to take linear space. Unfortunately, known solutions achieving such\ncomplexity require superlinear time for constructing the structure. Using a\ndifferent idea we are able to construct in (deterministic) linear time and\nspace a structure allowing dispatching binary methods in the same logarithmic\ntime. Then we show how to improve the query time to slightly sublogarithmic,\nwhich is easily seen to be optimal as a consequence of some already known lower\nbounds if we want to keep the size of the resulting structure close to linear.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 17:05:53 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Gawrychowski", "Pawel", ""]]}, {"id": "1202.5675", "submitter": "Tamar Zondiner", "authors": "Robert Krauthgamer and Tamar Zondiner", "title": "Preserving Terminal Distances using Minors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the following notion of compressing an undirected graph G with\nedge-lengths and terminal vertices $R\\subseteq V(G)$. A distance-preserving\nminor is a minor G' (of G) with possibly different edge-lengths, such that\n$R\\subseteq V(G')$ and the shortest-path distance between every pair of\nterminals is exactly the same in G and in G'. What is the smallest f*(k) such\nthat every graph G with k=|R| terminals admits a distance-preserving minor G'\nwith at most f*(k) vertices?\n  Simple analysis shows that $f*(k)\\leq O(k^4)$. Our main result proves that\n$f*(k)\\geq \\Omega(k^2)$, significantly improving over the trivial $f*(k)\\geq\nk$. Our lower bound holds even for planar graphs G, in contrast to graphs G of\nconstant treewidth, for which we prove that O(k) vertices suffice.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 17:33:37 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2012 20:46:43 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2012 15:49:59 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Krauthgamer", "Robert", ""], ["Zondiner", "Tamar", ""]]}, {"id": "1202.5715", "submitter": "Haitao Wang", "authors": "Danny Z. Chen and Haitao Wang", "title": "Computing L1 Shortest Paths among Polygonal Obstacles in the Plane", "comments": "48 pages; 19 figures; partial results appeared in ESA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point $s$ and a set of $h$ pairwise disjoint polygonal obstacles of\ntotally $n$ vertices in the plane, we present a new algorithm for building an\n$L_1$ shortest path map of size O(n) in $O(T)$ time and O(n) space such that\nfor any query point $t$, the length of the $L_1$ shortest obstacle-avoiding\npath from $s$ to $t$ can be reported in $O(\\log n)$ time and the actual\nshortest path can be found in additional time proportional to the number of\nedges of the path, where $T$ is the time for triangulating the free space. It\nis currently known that $T=O(n+h\\log^{1+\\epsilon}h)$ for an arbitrarily small\nconstant $\\epsilon>0$. If the triangulation can be done optimally (i.e.,\n$T=O(n+h\\log h)$), then our algorithm is optimal. Previously, the best\nalgorithm computes such an $L_1$ shortest path map in $O(n\\log n)$ time and\nO(n) space. Our techniques can be extended to obtain improved results for other\nrelated problems, e.g., computing the $L_1$ geodesic Voronoi diagram for a set\nof point sites in a polygonal domain, finding shortest paths with fixed\norientations, finding approximate Euclidean shortest paths, etc.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2012 01:53:37 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Chen", "Danny Z.", ""], ["Wang", "Haitao", ""]]}, {"id": "1202.5749", "submitter": "Marcin Pilipczuk", "authors": "Stefan Kratsch and Marcin Pilipczuk and Micha{\\l} Pilipczuk and Magnus\n  Wahlstr\\\"om", "title": "Fixed-parameter tractability of multicut in directed acyclic graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MULTICUT problem, given a graph G, a set of terminal pairs T={(s_i,t_i) |\n1 <= i <= r} and an integer p, asks whether one can find a cutset consisting of\nat most p non-terminal vertices that separates all the terminal pairs, i.e.,\nafter removing the cutset, t_i is not reachable from s_i for each 1 <= i <= r.\nThe fixed-parameter tractability of MULTICUT in undirected graphs,\nparameterized by the size of the cutset only, has been recently proven by Marx\nand Razgon (STOC'11) and, independently, by Bousquet et al. (STOC'11), after\nresisting attacks as a long-standing open problem. In this paper we prove that\nMULTICUT is fixed-parameter tractable on directed acyclic graphs, when\nparameterized both by the size of the cutset and the number of terminal pairs.\nWe complement this result by showing that this is implausible for\nparameterization by the size of the cutset only, as this version of the problem\nremains W[1]-hard.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2012 12:16:44 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Kratsch", "Stefan", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1202.5797", "submitter": "Rishi Saket", "authors": "Inge Li Goertz, Viswanath Nagarajan and Rishi Saket", "title": "Stochastic Vehicle Routing with Recourse", "comments": "20 Pages, 1 figure Revision corrects the statement and proof of\n  Theorem 1.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic Vehicle Routing Problem in the setting of stochastic\noptimization with recourse. StochVRP is a two-stage optimization problem, where\ndemand is satisfied using two routes: fixed and recourse. The fixed route is\ncomputed using only a demand distribution. Then after observing the demand\ninstantiations, a recourse route is computed -- but costs here become more\nexpensive by a factor lambda.\n  We present an O(log^2 n log(n lambda))-approximation algorithm for this\nstochastic routing problem, under arbitrary distributions. The main idea in\nthis result is relating StochVRP to a special case of submodular orienteering,\ncalled knapsack rank-function orienteering. We also give a better approximation\nratio for knapsack rank-function orienteering than what follows from prior\nwork. Finally, we provide a Unique Games Conjecture based omega(1) hardness of\napproximation for StochVRP, even on star-like metrics on which our algorithm\nachieves a logarithmic approximation.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2012 22:38:06 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 20:43:00 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Goertz", "Inge Li", ""], ["Nagarajan", "Viswanath", ""], ["Saket", "Rishi", ""]]}, {"id": "1202.5885", "submitter": "Marek Karpinski", "authors": "Marek Karpinski, Andrzej Rucinski, Edyta Szymanska", "title": "Approximate Counting of Matchings in Sparse Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a fully polynomial randomized approximation scheme\n(FPRAS) for the number of all matchings in hypergraphs belonging to a class of\nsparse, uniform hypergraphs. Our method is based on a generalization of the\ncanonical path method to the case of uniform hypergraphs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 10:51:09 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 17:29:35 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Karpinski", "Marek", ""], ["Rucinski", "Andrzej", ""], ["Szymanska", "Edyta", ""]]}, {"id": "1202.5888", "submitter": "Ashwin Ganesan", "authors": "Ashwin Ganesan", "title": "An efficient algorithm for the diameter of Cayley graphs generated by\n  transposition trees", "comments": "A journal version that includes parts of arXiv:1111.3114", "journal-ref": "IAENG International Journal of Applied Mathematics, vol. 42, no.\n  4, pp. 214-223, November 2012", "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem of practical and theoretical interest is to determine or estimate\nthe diameter of various families of Cayley networks. The previously known\nestimate for the diameter of Cayley graphs generated by transposition trees is\nan upper bound given in the oft-cited paper of Akers and Krishnamurthy (1989).\nIn this work, we first assess the performance of their upper bound. We show\nthat for every $n$, there exists a tree on $n$ vertices, such that the\ndifference between the upper bound and the true diameter value is at least\n$n-4$.\n  Evaluating their upper bound takes time $\\Omega(n!)$. In this paper, we\nprovide an algorithm that obtains an estimate of the diameter, but which\nrequires only time $O(n^2)$; furthermore, the value obtained by our algorithm\nis less than or equal to the previously known diameter upper bound. Such an\nimprovement to polynomial time, while still performing at least as well as the\nprevious bound, is possible because our algorithm works directly with the\ntransposition tree on $n$ vertices and does not require examining any of the\npermutations. We also provide a tree for which the value computed by our\nalgorithm is not necessarily unique, which is an important result because such\nexamples are quite rare. For all families of trees we have investigated so far,\neach of the possible values computed by our algorithm happens to also be an\nupper bound on the diameter.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 11:01:58 GMT"}, {"version": "v2", "created": "Fri, 25 May 2012 10:59:33 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2012 06:44:19 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Ganesan", "Ashwin", ""]]}, {"id": "1202.5985", "submitter": "Romain Giot", "authors": "Romain Giot (GREYC), Mohamad El-Abed (GREYC), Christophe Rosenberger\n  (GREYC)", "title": "Fast computation of the performance evaluation of biometric systems:\n  application to multibiometric", "comments": "Future Generation Computer Systems (2012)", "journal-ref": null, "doi": "10.1016/j.future.2012.02.003", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance evaluation of biometric systems is a crucial step when\ndesigning and evaluating such systems. The evaluation process uses the Equal\nError Rate (EER) metric proposed by the International Organization for\nStandardization (ISO/IEC). The EER metric is a powerful metric which allows\neasily comparing and evaluating biometric systems. However, the computation\ntime of the EER is, most of the time, very intensive. In this paper, we propose\na fast method which computes an approximated value of the EER. We illustrate\nthe benefit of the proposed method on two applications: the computing of non\nparametric confidence intervals and the use of genetic algorithms to compute\nthe parameters of fusion functions. Experimental results show the superiority\nof the proposed EER approximation method in term of computing time, and the\ninterest of its use to reduce the learning of parameters with genetic\nalgorithms. The proposed method opens new perspectives for the development of\nsecure multibiometrics systems by speeding up their computation time.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 16:05:10 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Giot", "Romain", "", "GREYC"], ["El-Abed", "Mohamad", "", "GREYC"], ["Rosenberger", "Christophe", "", "GREYC"]]}, {"id": "1202.6033", "submitter": "Michael Brautbar", "authors": "Christian Borgs and Michael Brautbar and Jennifer Chayes and Sanjeev\n  Khanna and Brendan Lucier", "title": "The Power of Local Information in Social Networks", "comments": "An extended abstract of this work appeared in the 8th Workshop on\n  Internet & Network Economics (WINE 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of \\textit{local information algorithms} for optimization\nproblems on social networks. We focus on sequential algorithms for which the\nnetwork topology is initially unknown and is revealed only within a local\nneighborhood of vertices that have been irrevocably added to the output set.\nThe distinguishing feature of this setting is that locality is necessitated by\nconstraints on the network information visible to the algorithm, rather than\nbeing desirable for reasons of efficiency or parallelizability. In this sense,\nchanges to the level of network visibility can have a significant impact on\nalgorithm design.\n  We study a range of problems under this model of algorithms with local\ninformation. We first consider the case in which the underlying graph is a\npreferential attachment network. We show that one can find the node of maximum\ndegree in the network in a polylogarithmic number of steps, using an\nopportunistic algorithm that repeatedly queries the visible node of maximum\ndegree. This addresses an open question of Bollob{\\'a}s and Riordan. In\ncontrast, local information algorithms require a linear number of queries to\nsolve the problem on arbitrary networks.\n  Motivated by problems faced by recruiters in online networks, we also\nconsider network coverage problems such as finding a minimum dominating set.\nFor this optimization problem we show that, if each node added to the output\nset reveals sufficient information about the set's neighborhood, then it is\npossible to design randomized algorithms for general networks that nearly match\nthe best approximations possible even with full access to the graph structure.\nWe show that this level of visibility is necessary.\n  We conclude that a network provider's decision of how much structure to make\nvisible to its users can have a significant effect on a user's ability to\ninteract strategically with the network.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 19:27:53 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2013 17:13:25 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2013 16:26:08 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Borgs", "Christian", ""], ["Brautbar", "Michael", ""], ["Chayes", "Jennifer", ""], ["Khanna", "Sanjeev", ""], ["Lucier", "Brendan", ""]]}, {"id": "1202.6101", "submitter": "Parikshit Ram", "authors": "Parikshit Ram and Alexander G. Gray", "title": "Maximum Inner-Product Search using Tree Data-structures", "comments": "Under submission in KDD 2012", "journal-ref": null, "doi": "10.1145/2339530.2339677", "report-no": null, "categories": "cs.CG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of {\\em efficiently} finding the best match for a query in a\ngiven set with respect to the Euclidean distance or the cosine similarity has\nbeen extensively studied in literature. However, a closely related problem of\nefficiently finding the best match with respect to the inner product has never\nbeen explored in the general setting to the best of our knowledge. In this\npaper we consider this general problem and contrast it with the existing\nbest-match algorithms. First, we propose a general branch-and-bound algorithm\nusing a tree data structure. Subsequently, we present a dual-tree algorithm for\nthe case where there are multiple queries. Finally we present a new data\nstructure for increasing the efficiency of the dual-tree algorithm. These\nbranch-and-bound algorithms involve novel bounds suited for the purpose of\nbest-matching with inner products. We evaluate our proposed algorithms on a\nvariety of data sets from various applications, and exhibit up to five orders\nof magnitude improvement in query time over the naive search technique.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 01:32:01 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ram", "Parikshit", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1202.6256", "submitter": "Florentina Pintea", "authors": "R. Gnanajeyaraman, G.Seenivasan", "title": "Diagonalization Matrix Method of Solving the First Problem of Hidden\n  Markov Model in Speech Recognition System", "comments": "10 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 17-26", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computationally efficient method of solving evaluation\nproblem of Hidden Markov Model (HMM) with a given set of discrete observation\nsymbols, number of states and probability distribution matrices. The\nobservation probability for a given HMM model is evaluated using an approach in\nwhich the probability evaluation is reduced to the problem of evaluating the\nproduct of matrices with different powers and formed out of state transition\nprobabilities and observation probabilities. Finding powers of a matrix is done\nby using the computationally efficient diagonalization method thereby reducing\nthe overall computational effort for evaluating the Evaluation problem of\nHMM.The proposed method is compared with the existing direct method. It is\nfound that evaluating matrix power by diagnolisation method is more suitable\nthan that of the direct, method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 15:28:50 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Gnanajeyaraman", "R.", ""], ["Seenivasan", "G.", ""]]}, {"id": "1202.6562", "submitter": "Deyu Meng", "authors": "Deyu Meng, Yee Leung, Qian Zhao, Zongben Xu", "title": "Dictionary learning under global sparsity constraint", "comments": "27 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed in this paper to learn overcomplete dictionary from\ntraining data samples. Differing from the current methods that enforce similar\nsparsity constraint on each of the input samples, the proposed method attempts\nto impose global sparsity constraint on the entire data set. This enables the\nproposed method to fittingly assign the atoms of the dictionary to represent\nvarious samples and optimally adapt to the complicated structures underlying\nthe entire data set. By virtue of the sparse coding and sparse PCA techniques,\na simple algorithm is designed for the implementation of the method. The\nefficiency and the convergence of the proposed algorithm are also theoretically\nanalyzed. Based on the experimental results implemented on a series of signal\nand image data sets, it is apparent that our method performs better than the\ncurrent dictionary learning methods in original dictionary recovering, input\ndata reconstructing, and salient data structure revealing.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 14:56:57 GMT"}, {"version": "v2", "created": "Sat, 11 May 2013 11:22:35 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Meng", "Deyu", ""], ["Leung", "Yee", ""], ["Zhao", "Qian", ""], ["Xu", "Zongben", ""]]}, {"id": "1202.6575", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "Simplified, stable parallel merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note makes an observation that significantly simplifies a number of\nprevious parallel, two-way merge algorithms based on binary search and\nsequential merge in parallel. First, it is shown that the additional merge step\nof distinguished elements as found in previous algorithms is not necessary,\nthus simplifying the implementation and reducing constant factors. Second, by\nfixating the requirements to the binary search, the merge algorithm becomes\nstable, provided that the sequential merge subroutine is stable. The stable,\nparallel merge algorithm can easily be used to implement a stable, parallel\nmerge sort.\n  For ordered sequences with $n$ and $m$ elements, $m\\leq n$, the simplified\nmerge algorithm runs in $O(n/p+\\log n)$ operations using $p$ processing\nelements. It can be implemented on an EREW PRAM, but since it requires only a\nsingle synchronization step, it is also a candidate for implementation on other\nparallel, shared-memory computers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 15:32:34 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2012 09:42:07 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2012 12:17:38 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1202.6598", "submitter": "Bin Fu", "authors": "Bin Fu, Wenfeng Li, and Zhiyong Peng", "title": "Sublinear Time Approximate Sum via Uniform Random Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the approximation for computing the sum $a_1+...+a_n$ with an\ninput of a list of nonnegative elements $a_1,..., a_n$. If all elements are in\nthe range $[0,1]$, there is a randomized algorithm that can compute an\n$(1+\\epsilon)$-approximation for the sum problem in time ${O({n(\\log\\log\nn)\\over\\sum_{i=1}^n a_i})}$, where $\\epsilon$ is a constant in $(0,1)$. Our\nrandomized algorithm is based on the uniform random sampling, which selects one\nelement with equal probability from the input list each time. We also prove a\nlower bound $\\Omega({n\\over \\sum_{i=1}^n a_i})$, which almost matches the upper\nbound, for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 16:52:17 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Fu", "Bin", ""], ["Li", "Wenfeng", ""], ["Peng", "Zhiyong", ""]]}, {"id": "1202.6642", "submitter": "Marek Cygan", "authors": "Marek Cygan", "title": "Deterministic parameterized connected vertex cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Connected Vertex Cover problem we are given an undirected graph G\ntogether with an integer k and we are to find a subset of vertices X of size at\nmost k, such that X contains at least one end-point of each edge and moreover X\ninduces a connected subgraph. For this problem we present a deterministic\nalgorithm running in O(2^k n^O(1)) time and polynomial space, improving over\npreviously best O(2.4882^k n^O(1)) deterministic algorithm and O(2^k n^O(1))\nrandomized algorithm. Furthermore, when usage of exponential space is allowed,\nwe present an O(2^k k(n+m)) time algorithm that solves a more general variant\nwith arbitrary real weights.\n  Finally, we show that in O(2k k(n + m)) time and O(2^k k) space one can count\nthe number of connected vertex covers of size at most k, which can not be\nimproved to O((2 - eps)^k nO(1)) for any eps > 0 under the Strong Exponential\nTime Hypothesis, as shown by Cygan et al. [CCC'12].\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 18:48:21 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Cygan", "Marek", ""]]}]