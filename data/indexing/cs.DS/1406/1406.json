[{"id": "1406.0140", "submitter": "Hamid Haghshenas", "authors": "MohammadAmin Fazli, Azin Ghazimatin, Jafar Habibi and Hamid Haghshenas", "title": "Team Selection For Prediction Tasks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random variable $O \\in \\mathbb{R}$ and a set of experts $E$, we\ndescribe a method for finding a subset of experts $S \\subseteq E$ whose\naggregated opinion best predicts the outcome of $O$. Therefore, the problem can\nbe regarded as a team formation for performing a prediction task. We show that\nin case of aggregating experts' opinions by simple averaging, finding the best\nteam (the team with the lowest total error during past $k$ turns) can be\nmodeled with an integer quadratic programming and we prove its NP-hardness\nwhereas its relaxation is solvable in polynomial time. Finally, we do an\nexperimental comparison between different rounding and greedy heuristics and\nshow that our suggested tabu search works effectively.\n  Keywords: Team Selection, Information Aggregation, Opinion Pooling, Quadratic\nProgramming, NP-Hard\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 07:44:54 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 05:39:43 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Fazli", "MohammadAmin", ""], ["Ghazimatin", "Azin", ""], ["Habibi", "Jafar", ""], ["Haghshenas", "Hamid", ""]]}, {"id": "1406.0263", "submitter": "Hideo Bannai", "authors": "Hideo Bannai and Tomohiro I and Shunsuke Inenaga and Yuto Nakashima\n  and Masayuki Takeda and Kazuya Tsuruta", "title": "The \"Runs\" Theorem", "comments": "simple proof with some more bounds", "journal-ref": "SIAM J. Comput., 46(5), 1501-1514, 2017", "doi": "10.1137/15M1011032", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new characterization of maximal repetitions (or runs) in strings\nbased on Lyndon words. The characterization leads to a proof of what was known\nas the \"runs\" conjecture (Kolpakov \\& Kucherov (FOCS '99)), which states that\nthe maximum number of runs $\\rho(n)$ in a string of length $n$ is less than\n$n$. The proof is remarkably simple, considering the numerous endeavors to\ntackle this problem in the last 15 years, and significantly improves our\nunderstanding of how runs can occur in strings. In addition, we obtain an upper\nbound of $3n$ for the maximum sum of exponents $\\sigma(n)$ of runs in a string\nof length $n$, improving on the best known bound of $4.1n$ by Crochemore et al.\n(JDA 2012), as well as other improved bounds on related problems. The\ncharacterization also gives rise to a new, conceptually simple linear-time\nalgorithm for computing all the runs in a string. A notable characteristic of\nour algorithm is that, unlike all existing linear-time algorithms, it does not\nutilize the Lempel-Ziv factorization of the string. We also establish a\nrelationship between runs and nodes of the Lyndon tree, which gives a simple\noptimal solution to the 2-Period Query problem that was recently solved by\nKociumaka et al. (SODA 2015).\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 06:51:41 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 09:37:27 GMT"}, {"version": "v3", "created": "Wed, 23 Jul 2014 11:37:10 GMT"}, {"version": "v4", "created": "Sun, 27 Jul 2014 08:11:30 GMT"}, {"version": "v5", "created": "Fri, 9 Jan 2015 02:59:00 GMT"}, {"version": "v6", "created": "Mon, 12 Jan 2015 18:08:55 GMT"}, {"version": "v7", "created": "Wed, 3 Jun 2015 04:08:34 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Bannai", "Hideo", ""], ["I", "Tomohiro", ""], ["Inenaga", "Shunsuke", ""], ["Nakashima", "Yuto", ""], ["Takeda", "Masayuki", ""], ["Tsuruta", "Kazuya", ""]]}, {"id": "1406.0426", "submitter": "Heng Li", "authors": "Heng Li", "title": "Fast construction of FM-index for long sequence reads", "comments": "2 pages", "journal-ref": null, "doi": "10.1093/bioinformatics/btu541", "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary: We present a new method to incrementally construct the FM-index for\nboth short and long sequence reads, up to the size of a genome. It is the first\nalgorithm that can build the index while implicitly sorting the sequences in\nthe reverse (complement) lexicographical order without a separate sorting step.\nThe implementation is among the fastest for indexing short reads and the only\none that practically works for reads of averaged kilobases in length.\n  Availability and implementation: https://github.com/lh3/ropebwt2\n  Contact: hengli@broadinstitute.org\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 16:06:04 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Li", "Heng", ""]]}, {"id": "1406.0492", "submitter": "Jannik Silvanus", "authors": "Stefan Hougardy and Jannik Silvanus and Jens Vygen", "title": "Dijkstra meets Steiner: a fast exact goal-oriented Steiner tree\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new exact algorithm for the Steiner tree problem in\nedge-weighted graphs. Our algorithm improves the classical dynamic programming\napproach by Dreyfus and Wagner. We achieve a significantly better practical\nperformance via pruning and future costs, a generalization of a well-known\nconcept to speed up shortest path computations. Our algorithm matches the best\nknown worst-case run time and has a fast, often superior, practical\nperformance: on some large instances originating from VLSI design, previous\nbest run times are improved upon by orders of magnitudes. We are also able to\nsolve larger instances of the $d$-dimensional rectilinear Steiner tree problem\nfor $d \\in \\{3, 4, 5\\}$, whose Hanan grids contain up to several millions of\nedges.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 19:46:15 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 12:13:27 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 10:33:03 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Hougardy", "Stefan", ""], ["Silvanus", "Jannik", ""], ["Vygen", "Jens", ""]]}, {"id": "1406.0576", "submitter": "Inbal Talgam-Cohen", "authors": "Shahar Dobzinski, Michal Feldman, Inbal Talgam-Cohen and Omri\n  Weinstein", "title": "Welfare and Revenue Guarantees for Competitive Bundling Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study equilibria of markets with $m$ heterogeneous indivisible goods and\n$n$ consumers with combinatorial preferences. It is well known that a\ncompetitive equilibrium is not guaranteed to exist when valuations are not\ngross substitutes. Given the widespread use of bundling in real-life markets,\nwe study its role as a stabilizing and coordinating device by considering the\nnotion of \\emph{competitive bundling equilibrium}: a competitive equilibrium\nover the market induced by partitioning the goods for sale into fixed bundles.\nCompared to other equilibrium concepts involving bundles, this notion has the\nadvantage of simulatneous succinctness ($O(m)$ prices) and market clearance.\n  Our first set of results concern welfare guarantees. We show that in markets\nwhere consumers care only about the number of goods they receive (known as\nmulti-unit or homogeneous markets), even in the presence of complementarities,\nthere always exists a competitive bundling equilibrium that guarantees a\nlogarithmic fraction of the optimal welfare, and this guarantee is tight. We\nalso establish non-trivial welfare guarantees for general markets, two-consumer\nmarkets, and markets where the consumer valuations are additive up to a fixed\nbudget (budget-additive).\n  Our second set of results concern revenue guarantees. Motivated by the fact\nthat the revenue extracted in a standard competitive equilibrium may be zero\n(even with simple unit-demand consumers), we show that for natural subclasses\nof gross substitutes valuations, there always exists a competitive bundling\nequilibrium that extracts a logarithmic fraction of the optimal welfare, and\nthis guarantee is tight. The notion of competitive bundling equilibrium can\nthus be useful even in markets which possess a standard competitive\nequilibrium.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 05:04:44 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Dobzinski", "Shahar", ""], ["Feldman", "Michal", ""], ["Talgam-Cohen", "Inbal", ""], ["Weinstein", "Omri", ""]]}, {"id": "1406.1022", "submitter": "Gustavo Sacomoto", "authors": "Gustavo Sacomoto, Blerina Sinaimeri, Camille Marchet, Vincent Miele,\n  Marie-France Sagot and Vincent Lacroix", "title": "Navigating in a sea of repeats in RNA-seq without drowning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge in de novo assembly of NGS data is certainly to deal with\nrepeats that are longer than the reads. This is particularly true for RNA- seq\ndata, since coverage information cannot be used to flag repeated sequences, of\nwhich transposable elements are one of the main examples. Most transcriptome\nassemblers are based on de Bruijn graphs and have no clear and explicit model\nfor repeats in RNA-seq data, relying instead on heuristics to deal with them.\nThe results of this work are twofold. First, we introduce a formal model for\nrepre- senting high copy number repeats in RNA-seq data and exploit its\nproperties for inferring a combinatorial characteristic of repeat-associated\nsubgraphs. We show that the problem of identifying in a de Bruijn graph a\nsubgraph with this charac- teristic is NP-complete. In a second step, we show\nthat in the specific case of a local assembly of alternative splicing (AS)\nevents, we can implicitly avoid such subgraphs. In particular, we designed and\nimplemented an algorithm to efficiently identify AS events that are not\nincluded in repeated regions. Finally, we validate our results using synthetic\ndata. We also give an indication of the usefulness of our method on real data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 12:18:16 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Sacomoto", "Gustavo", ""], ["Sinaimeri", "Blerina", ""], ["Marchet", "Camille", ""], ["Miele", "Vincent", ""], ["Sagot", "Marie-France", ""], ["Lacroix", "Vincent", ""]]}, {"id": "1406.1077", "submitter": "Miguel Lerma", "authors": "Miguel A. Lerma", "title": "How inefficient can a sort algorithm be?", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find large lower bounds for a certain family of algorithms, and prove that\nsuch bounds are limited only by natural computability arguments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 19:13:17 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Lerma", "Miguel A.", ""]]}, {"id": "1406.1158", "submitter": "Lukas Mach", "authors": "Ivan Bliznets, Marek Cygan, Pawel Komosa, Lukas Mach", "title": "Kernelization lower bound for Permutation Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A permutation $\\pi$ contains a permutation $\\sigma$ as a pattern if it\ncontains a subsequence of length $|\\sigma|$ whose elements are in the same\nrelative order as in the permutation $\\sigma$. This notion plays a major role\nin enumerative combinatorics. We prove that the problem does not have a\npolynomial kernel (under the widely believed complexity assumption $\\mbox{NP}\n\\not\\subseteq \\mbox{co-NP}/\\mbox{poly}$) by introducing a new polynomial\nreduction from the clique problem to permutation pattern matching.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 19:32:04 GMT"}, {"version": "v2", "created": "Sun, 11 Jan 2015 23:58:02 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Bliznets", "Ivan", ""], ["Cygan", "Marek", ""], ["Komosa", "Pawel", ""], ["Mach", "Lukas", ""]]}, {"id": "1406.1244", "submitter": "Stephan Holzer", "authors": "Alexandra Hochuli, Stephan Holzer, Roger Wattenhofer", "title": "Distributed Approximation of Minimum Routing Cost Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the NP-hard problem of approximating a Minimum Routing Cost Spanning\nTree in the message passing model with limited bandwidth (CONGEST model). In\nthis problem one tries to find a spanning tree of a graph $G$ over $n$ nodes\nthat minimizes the sum of distances between all pairs of nodes. In the\nconsidered model every node can transmit a different (but short) message to\neach of its neighbors in each synchronous round. We provide a randomized\n$(2+\\epsilon)$-approximation with runtime $O(D+\\frac{\\log n}{\\epsilon})$ for\nunweighted graphs. Here, $D$ is the diameter of $G$. This improves over both,\nthe (expected) approximation factor $O(\\log n)$ and the runtime $O(D\\log^2 n)$\nof the best previously known algorithm.\n  Due to stating our results in a very general way, we also derive an (optimal)\nruntime of $O(D)$ when considering $O(\\log n)$-approximations as done by the\nbest previously known algorithm. In addition we derive a deterministic\n$2$-approximation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:08:07 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Hochuli", "Alexandra", ""], ["Holzer", "Stephan", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1406.1368", "submitter": "Jan Kyn\\v{c}l", "authors": "Sergio Cabello, Josef Cibulka, Jan Kyn\\v{c}l, Maria Saumell, Pavel\n  Valtr", "title": "Peeling potatoes near-optimally in near-linear time", "comments": "30 pages, 7 figures; minor revision. Preliminary version was\n  presented at SoCG 2014", "journal-ref": "SIAM Journal on Computing 46(5) (2017), 1574-1602", "doi": "10.1137/16M1079695", "report-no": null, "categories": "cs.CG cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following geometric optimization problem: find a convex\npolygon of maximum area contained in a given simple polygon $P$ with $n$\nvertices. We give a randomized near-linear-time $(1-\\varepsilon)$-approximation\nalgorithm for this problem: in $O(n( \\log^2 n + (1/\\varepsilon^3) \\log n +\n1/\\varepsilon^4))$ time we find a convex polygon contained in $P$ that, with\nprobability at least $2/3$, has area at least $(1-\\varepsilon)$ times the area\nof an optimal solution. We also obtain similar results for the variant of\ncomputing a convex polygon inside $P$ with maximum perimeter.\n  To achieve these results we provide new results in geometric probability. The\nfirst result is a bound relating the probability that two points chosen\nuniformly at random inside $P$ are mutually visible and the area of the largest\nconvex body inside $P$. The second result is a bound on the expected value of\nthe difference between the perimeter of any planar convex body $K$ and the\nperimeter of the convex hull of a uniform random sample inside $K$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 12:54:07 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 20:03:48 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 18:18:09 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Cabello", "Sergio", ""], ["Cibulka", "Josef", ""], ["Kyn\u010dl", "Jan", ""], ["Saumell", "Maria", ""], ["Valtr", "Pavel", ""]]}, {"id": "1406.1579", "submitter": "Chinmay Hegde", "authors": "Chinmay Hegde, Piotr Indyk, Ludwig Schmidt", "title": "Approximation Algorithms for Model-Based Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive Sensing (CS) stipulates that a sparse signal can be recovered\nfrom a small number of linear measurements, and that this recovery can be\nperformed efficiently in polynomial time. The framework of model-based\ncompressive sensing (model-CS) leverages additional structure in the signal and\nprescribes new recovery schemes that can reduce the number of measurements even\nfurther. However, model-CS requires an algorithm that solves the\nmodel-projection problem: given a query signal, produce the signal in the model\nthat is also closest to the query signal. Often, this optimization can be\ncomputationally very expensive. Moreover, an approximation algorithm is not\nsufficient for this optimization task. As a result, the model-projection\nproblem poses a fundamental obstacle for extending model-CS to many interesting\nmodels.\n  In this paper, we introduce a new framework that we call\napproximation-tolerant model-based compressive sensing. This framework includes\na range of algorithms for sparse recovery that require only approximate\nsolutions for the model-projection problem. In essence, our work removes the\naforementioned obstacle to model-based compressive sensing, thereby extending\nthe applicability of model-CS to a much wider class of models. We instantiate\nthis new framework for the Constrained Earth Mover Distance (CEMD) model, which\nis particularly useful for signal ensembles where the positions of the nonzero\ncoefficients do not change significantly as a function of spatial (or temporal)\nlocation. We develop novel approximation algorithms for both the maximization\nand the minimization versions of the model-projection problem via graph\noptimization techniques. Leveraging these algorithms into our framework results\nin a nearly sample-optimal sparse recovery scheme for the CEMD model.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 04:37:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 14:54:54 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 22:00:35 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Hegde", "Chinmay", ""], ["Indyk", "Piotr", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1406.1677", "submitter": "Ankit Chadha Mr.", "authors": "Ankit R. Chadha, Rishikesh Misal, Tanaya Mokashi", "title": "Modified Binary Search Algorithm", "comments": null, "journal-ref": "International Journal of Applied Information Systems 7(2):37-40,\n  April 2014", "doi": "10.5120/ijais14-451131", "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a modification to the traditional binary search algorithm\nin which it checks the presence of the input element with the middle element of\nthe given set of elements at each iteration. Modified binary search algorithm\noptimizes the worst case of the binary search algorithm by comparing the input\nelement with the first & last element of the data set along with the middle\nelement and also checks the input number belongs to the range of numbers\npresent in the given data set at each iteration there by reducing the time\ntaken by the worst cases of binary search algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 13:12:19 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Chadha", "Ankit R.", ""], ["Misal", "Rishikesh", ""], ["Mokashi", "Tanaya", ""]]}, {"id": "1406.1717", "submitter": "Jukka Suomela", "authors": "Jukka Suomela", "title": "Median Filtering is Equivalent to Sorting", "comments": "1 + 24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows that the following problems are equivalent, both in theory\nand in practice:\n  - median filtering: given an $n$-element vector, compute the sliding window\nmedian with window size $k$,\n  - piecewise sorting: given an $n$-element vector, divide it in $n/k$ blocks\nof length $k$ and sort each block.\n  By prior work, median filtering is known to be at least as hard as piecewise\nsorting: with a single median filter operation we can sort $\\Theta(n/k)$ blocks\nof length $\\Theta(k)$. The present work shows that median filtering is also as\neasy as piecewise sorting: we can do median filtering with one piecewise\nsorting operation and linear-time postprocessing. In particular, median\nfiltering can directly benefit from the vast literature on sorting\nalgorithms---for example, adaptive sorting algorithms imply adaptive median\nfiltering algorithms.\n  The reduction is very efficient in practice---for random inputs the\nperformance of the new sorting-based algorithm is on a par with the fastest\nheap-based algorithms, and for benign data distributions it typically\noutperforms prior algorithms.\n  The key technical idea is that we can represent the sliding window with a\npair of sorted doubly-linked lists: we delete items from one list and add items\nto the other list. Deletions are easy; additions can be done efficiently if we\nreverse the time twice: First we construct the full list and delete the items\nin the reverse order. Then we undo each deletion with Knuth's dancing links\ntechnique.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 15:54:03 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Suomela", "Jukka", ""]]}, {"id": "1406.1796", "submitter": "Paul Tarau", "authors": "Paul Tarau", "title": "A Generic Numbering System based on Catalan Families of Combinatorial\n  Objects", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe arithmetic algorithms on a canonical number representation based\non the Catalan family of combinatorial objects specified as a Haskell type\nclass.\n  Our algorithms work on a {\\em generic} representation that we illustrate on\ninstances members of the Catalan family, like ordered binary and multiway\ntrees. We validate the correctness of our algorithms by defining an instance of\nthe same type class based the usual bitstring-based natural numbers.\n  While their average and worst case complexity is within constant factors of\ntheir traditional counterparts, our algorithms provide super-exponential gains\nfor numbers corresponding to Catalan objects of low representation size.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 19:22:44 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 21:15:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Tarau", "Paul", ""]]}, {"id": "1406.2008", "submitter": "{\\L}ukasz Kuszner", "authors": "Dariusz Dereniowski, Ralf Klasing, Adrian Kosowski and {\\L}ukasz\n  Kuszner", "title": "Rendezvous of Heterogeneous Mobile Agents in Edge-weighted Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of the deterministic rendezvous problem for a pair of\nheterogeneous agents operating in an undirected graph, which differ in the time\nthey require to traverse particular edges of the graph. Each agent knows the\ncomplete topology of the graph and the initial positions of both agents. The\nagent also knows its own traversal times for all of the edges of the graph, but\nis unaware of the corresponding traversal times for the other agent. The goal\nof the agents is to meet on an edge or a node of the graph. In this scenario,\nwe study the time required by the agents to meet, compared to the meeting time\n$T_{OPT}$ in the offline scenario in which the agents have complete knowledge\nabout each others speed characteristics. When no additional assumptions are\nmade, we show that rendezvous in our model can be achieved after time $O(n\nT_{OPT})$ in a $n$-node graph, and that such time is essentially in some cases\nthe best possible. However, we prove that the rendezvous time can be reduced to\n$\\Theta (T_{OPT})$ when the agents are allowed to exchange $\\Theta(n)$ bits of\ninformation at the start of the rendezvous process. We then show that under\nsome natural assumption about the traversal times of edges, the hardness of the\nheterogeneous rendezvous problem can be substantially decreased, both in terms\nof time required for rendezvous without communication, and the communication\ncomplexity of achieving rendezvous in time $\\Theta (T_{OPT})$.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 18:12:51 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["Klasing", "Ralf", ""], ["Kosowski", "Adrian", ""], ["Kuszner", "\u0141ukasz", ""]]}, {"id": "1406.2107", "submitter": "Boaz Benmoshe", "authors": "Boaz Ben-Moshe and Michael Elkin and Lee-Ad Gottlieb and Eran Omri", "title": "Optimizing Budget Allocation in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical facility location problem we consider a graph $G$ with fixed\nweights on the edges of $G$. The goal is then to find an optimal positioning\nfor a set of facilities on the graph with respect to some objective function.\nWe introduce a new framework for facility location problems, where the weights\non the graph edges are not fixed, but rather should be assigned. The goal is to\nfind a valid assignment for which the resulting weighted graph optimizes the\nfacility location objective function. We present algorithms for finding the\noptimal {\\em budget allocation} for the center point problem and for the median\npoint problem on trees. Our algorithms run in linear time, both for the case\nwhere a candidate vertex is given as part of the input, and for the case where\nfinding a vertex that optimizes the solution is part of the problem. We also\npresent a hardness result for the general graph case of the center point\nproblem, followed by an $O(\\log^2(n))$ approximation algorithm on graphs - with\ngeneral metric spaces.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 08:35:32 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Ben-Moshe", "Boaz", ""], ["Elkin", "Michael", ""], ["Gottlieb", "Lee-Ad", ""], ["Omri", "Eran", ""]]}, {"id": "1406.2154", "submitter": "Pawel Gawrychowski", "authors": "Pawel Gawrychowski and Damian Rusak", "title": "Euclidean TSP with few inner points in linear space", "comments": "under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points in the Euclidean plane, such that just $k$ points\nare strictly inside the convex hull of the whole set, we want to find the\nshortest tour visiting every point. The fastest known algorithm for the version\nwhen $k$ is significantly smaller than $n$, i.e., when there are just few inner\npoints, works in $O(k^{11\\sqrt{k}} k^{1.5} n^{3})$ time [Knauer and Spillner,\nWG 2006], but also requires space of order $k^{c\\sqrt{k}}n^{2}$. The best\nlinear space algorithm takes $O(k! k n)$ time [Deineko, Hoffmann, Okamoto,\nWoeginer, Oper. Res. Lett. 34(1), 106-110]. We construct a linear space\n$O(nk^2+k^{O(\\sqrt{k})})$ time algorithm. The new insight is extending the\nknown divide-and-conquer method based on planar separators with a\nmatching-based argument to shrink the instance in every recursive call. This\nargument also shows that the problem admits a quadratic bikernel.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 12:18:55 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Rusak", "Damian", ""]]}, {"id": "1406.2262", "submitter": "Ankit Chadha Mr.", "authors": "Ankit Chadha, Rishikesh Misal, Tanaya Mokashi and Aman Chadha", "title": "ARC Sort: Enhanced and Time Efficient Sorting Algorithm", "comments": null, "journal-ref": "International Journal of Applied Information Systems 7(2), 1-7,\n  March 2014", "doi": "10.5120/ijais14-451130", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses about a sorting algorithm which uses the concept of\nbuckets where each bucket represents a certain number of digits. A two\ndimensional data structure is used where one dimension represents buckets i. e;\nnumber of digits and each bucket's corresponding dimensions represents the\ninput numbers that belong to that bucket. Each bucket is then individually\nsorted. Since every preceding bucket elements will always be smaller than the\nsucceeding buckets no comparison between them is required. By doing this we can\nsignificantly reduced the time complexity of any sorting algorithm used to sort\nthe given set of inputs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 12:17:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Chadha", "Ankit", ""], ["Misal", "Rishikesh", ""], ["Mokashi", "Tanaya", ""], ["Chadha", "Aman", ""]]}, {"id": "1406.2294", "submitter": "John Lamping", "authors": "John Lamping, Eric Veach", "title": "A Fast, Minimal Memory, Consistent Hash Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present jump consistent hash, a fast, minimal memory, consistent hash\nalgorithm that can be expressed in about 5 lines of code. In comparison to the\nalgorithm of Karger et al., jump consistent hash requires no storage, is\nfaster, and does a better job of evenly dividing the key space among the\nbuckets and of evenly dividing the workload when the number of buckets changes.\nIts main limitation is that the buckets must be numbered sequentially, which\nmakes it more suitable for data storage applications than for distributed web\ncaching.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 19:30:12 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Lamping", "John", ""], ["Veach", "Eric", ""]]}, {"id": "1406.2296", "submitter": "Siddharth Barman", "authors": "Siddharth Barman", "title": "Approximating Nash Equilibria and Dense Subgraphs via an Approximate\n  Version of Carath\\'{e}odory's Theorem", "comments": "28 pages; added references and extensions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithmic applications of an approximate version of\nCarath\\'{e}odory's theorem. The theorem states that given a set of vectors $X$\nin $\\mathbb{R}^d$, for every vector in the convex hull of $X$ there exists an\n$\\varepsilon$-close (under the $p$-norm distance, for $2\\leq p < \\infty$)\nvector that can be expressed as a convex combination of at most $b$ vectors of\n$X$, where the bound $b$ depends on $\\varepsilon$ and the norm $p$ and is\nindependent of the dimension $d$. This theorem can be derived by instantiating\nMaurey's lemma, early references to which can be found in the work of Pisier\n(1981) and Carl (1985). However, in this paper we present a self-contained\nproof of this result.\n  Using this theorem we establish that in a bimatrix game with $ n \\times n$\npayoff matrices $A, B$, if the number of non-zero entries in any column of\n$A+B$ is at most $s$ then an $\\varepsilon$-Nash equilibrium of the game can be\ncomputed in time $n^{O\\left(\\frac{\\log s }{\\varepsilon^2}\\right)}$. This, in\nparticular, gives us a polynomial-time approximation scheme for Nash\nequilibrium in games with fixed column sparsity $s$. Moreover, for arbitrary\nbimatrix games---since $s$ can be at most $n$---the running time of our\nalgorithm matches the best-known upper bound, which was obtained by Lipton,\nMarkakis, and Mehta (2003).\n  The approximate Carath\\'{e}odory's theorem also leads to an additive\napproximation algorithm for the normalized densest $k$-subgraph problem. Given\na graph with $n$ vertices and maximum degree $d$, the developed algorithm\ndetermines a subgraph with exactly $k$ vertices with normalized density within\n$\\varepsilon$ (in the additive sense) of the optimal in time $n^{O\\left(\n\\frac{\\log d}{\\varepsilon^2}\\right)}$. Additionally, we show that a similar\napproximation result can be achieved for the problem of finding a $k \\times\nk$-bipartite subgraph of maximum normalized density.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 19:39:50 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 06:27:10 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 23:17:02 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Barman", "Siddharth", ""]]}, {"id": "1406.2348", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Marcin Raniszewski", "title": "Sampling the suffix array with minimizers", "comments": "One new SamSAMi variant; extended experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling (evenly) the suffixes from the suffix array is an old idea trading\nthe pattern search time for reduced index space. A few years ago Claude et al.\nshowed an alphabet sampling scheme allowing for more efficient pattern searches\ncompared to the sparse suffix array, for long enough patterns. A drawback of\ntheir approach is the requirement that sought patterns need to contain at least\none character from the chosen subalphabet. In this work we propose an\nalternative suffix sampling approach with only a minimum pattern length as a\nrequirement, which seems more convenient in practice. Experiments show that our\nalgorithm achieves competitive time-space tradeoffs on most standard benchmark\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 20:36:01 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 19:39:41 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Grabowski", "Szymon", ""], ["Raniszewski", "Marcin", ""]]}, {"id": "1406.2587", "submitter": "Felix Reidl", "authors": "Erik D. Demaine and Felix Reidl and Peter Rossmanith and Fernando\n  Sanchez Villaamil and Somnath Sikdar and Blair D. Sullivan", "title": "Structural Sparsity of Complex Networks: Bounded Expansion in Random\n  Models and Real-World Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research establishes that many real-world networks exhibit bounded\nexpansion, a strong notion of structural sparsity, and demonstrates that it can\nbe leveraged to design efficient algorithms for network analysis. We analyze\nseveral common network models regarding their structural sparsity. We show\nthat, with high probability, (1) graphs sampled with a prescribed s parse\ndegree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block\nmodels with small probabilities; result in graphs of bounded expansion.\n  In contrast, we show that the Kleinberg and the Barabasi-Albert model have\nunbounded expansion. We support our findings with empirical measurements on a\ncorpus of real-world networks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:21:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 13:13:00 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 11:43:46 GMT"}, {"version": "v4", "created": "Thu, 12 Feb 2015 17:01:52 GMT"}, {"version": "v5", "created": "Thu, 5 Nov 2015 12:16:48 GMT"}, {"version": "v6", "created": "Fri, 12 Oct 2018 15:03:07 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Demaine", "Erik D.", ""], ["Reidl", "Felix", ""], ["Rossmanith", "Peter", ""], ["Villaamil", "Fernando Sanchez", ""], ["Sikdar", "Somnath", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1406.2648", "submitter": "Huan Wang", "authors": "Huan Wang, Christos Boutsidis, Edo Liberty, Daniel Hsu", "title": "Fast Matrix Multiplication with Sketching", "comments": "Theorem 1 may be problematic, and more careful thought is required.\n  The authors are discussing a solution on it. Currently it is better to\n  withdraw the draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximate algorithm for matrix multiplication based on matrix\nsketching techniques. First one of the matrix is chosen and sparsified using\nthe online matrix sketching algorithm, and then the matrix product is\ncalculated using the sparsified matrix. We prove when the sample number grows\nlarge compared to the sample dimensions the proposed algorithm achieves similar\naccuracy bound with a smaller computational cost compared to the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 18:45:57 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 18:17:50 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Wang", "Huan", ""], ["Boutsidis", "Christos", ""], ["Liberty", "Edo", ""], ["Hsu", "Daniel", ""]]}, {"id": "1406.2741", "submitter": "Aidan Roy", "authors": "Jun Cai and William G. Macready and Aidan Roy", "title": "A practical heuristic for finding graph minors", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a heuristic algorithm for finding a graph $H$ as a minor of a\ngraph $G$ that is practical for sparse $G$ and $H$ with hundreds of vertices.\nWe also explain the practical importance of finding graph minors in mapping\nquadratic pseudo-boolean optimization problems onto an adiabatic quantum\nannealer.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 23:07:30 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Cai", "Jun", ""], ["Macready", "William G.", ""], ["Roy", "Aidan", ""]]}, {"id": "1406.2795", "submitter": "Adrian Kosowski", "authors": "Shantanu Das (LIF), Dariusz Dereniowski, Adrian Kosowski (INRIA\n  Rocquencourt, LIAFA), Przemyslaw Uznanski (LIF)", "title": "Rendezvous of Distance-aware Mobile Agents in Unknown Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of rendezvous of two mobile agents starting at distinct\nlocations in an unknown graph. The agents have distinct labels and walk in\nsynchronous steps. However the graph is unlabelled and the agents have no means\nof marking the nodes of the graph and cannot communicate with or see each other\nuntil they meet at a node. When the graph is very large we want the time to\nrendezvous to be independent of the graph size and to depend only on the\ninitial distance between the agents and some local parameters such as the\ndegree of the vertices, and the size of the agent's label. It is well known\nthat even for simple graphs of degree $\\Delta$, the rendezvous time can be\nexponential in $\\Delta$ in the worst case. In this paper, we introduce a new\nversion of the rendezvous problem where the agents are equipped with a device\nthat measures its distance to the other agent after every step. We show that\nthese \\emph{distance-aware} agents are able to rendezvous in any unknown graph,\nin time polynomial in all the local parameters such the degree of the nodes,\nthe initial distance $D$ and the size of the smaller of the two agent labels $l\n= \\min(l_1, l_2)$. Our algorithm has a time complexity of\n$O(\\Delta(D+\\log{l}))$ and we show an almost matching lower bound of\n$\\Omega(\\Delta(D+\\log{l}/\\log{\\Delta}))$ on the time complexity of any\nrendezvous algorithm in our scenario. Further, this lower bound extends\nexisting lower bounds for the general rendezvous problem without distance\nawareness.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 07:01:42 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Das", "Shantanu", "", "LIF"], ["Dereniowski", "Dariusz", "", "INRIA\n  Rocquencourt, LIAFA"], ["Kosowski", "Adrian", "", "INRIA\n  Rocquencourt, LIAFA"], ["Uznanski", "Przemyslaw", "", "LIF"]]}, {"id": "1406.2951", "submitter": "Thomas Pensyl", "authors": "Jaros{\\l}aw Byrka, Thomas Pensyl, Bartosz Rybicki, Aravind Srinivasan,\n  Khoa Trinh", "title": "An Improved Approximation for $k$-median, and Positive Correlation in\n  Budgeted Optimization", "comments": null, "journal-ref": "Proceedings of ACM-Siam Symposium on Discrete Algorithms (SODA),\n  pages 737-756, 2015", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependent rounding is a useful technique for optimization problems with hard\nbudget constraints. This framework naturally leads to \\emph{negative\ncorrelation} properties. However, what if an application naturally calls for\ndependent rounding on the one hand, and desires \\emph{positive} correlation on\nthe other? More generally, we develop algorithms that guarantee the known\nproperties of dependent rounding, but also have nearly best-possible behavior -\nnear-independence, which generalizes positive correlation - on \"small\" subsets\nof the variables. The recent breakthrough of Li & Svensson for the classical\n$k$-median problem has to handle positive correlation in certain\ndependent-rounding settings, and does so implicitly. We improve upon\nLi-Svensson's approximation ratio for $k$-median from $2.732 + \\epsilon$ to\n$2.675 + \\epsilon$ by developing an algorithm that improves upon various\naspects of their work. Our dependent-rounding approach helps us improve the\ndependence of the runtime on the parameter $\\epsilon$ from Li-Svensson's\n$N^{O(1/\\epsilon^2)}$ to $N^{O((1/\\epsilon) \\log(1/\\epsilon))}$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 16:14:27 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 15:52:47 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 15:54:16 GMT"}, {"version": "v4", "created": "Sat, 23 Apr 2016 23:47:51 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Pensyl", "Thomas", ""], ["Rybicki", "Bartosz", ""], ["Srinivasan", "Aravind", ""], ["Trinh", "Khoa", ""]]}, {"id": "1406.3058", "submitter": "Zuzana Patakova", "authors": "Jiri Matousek, Zuzana Patakova", "title": "Multilevel polynomial partitions and simplified range searching", "comments": "19 pages; The proof that the Groebner basis can be effectively\n  computed is stated in more detail", "journal-ref": "Disc. Comput. Geom. 54(1):22-41, 2015", "doi": null, "report-no": null, "categories": "cs.DS math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polynomial partitioning method of Guth and Katz [arXiv:1011.4105] has\nnumerous applications in discrete and computational geometry. It partitions a\ngiven $n$-point set $P\\subset\\mathbb{R}^d$ using the zero set $Z(f)$ of a\nsuitable $d$-variate polynomial $f$. Applications of this result are often\ncomplicated by the problem, what should be done with the points of $P$ lying\nwithin $Z(f)$? A natural approach is to partition these points with another\npolynomial and continue further in a similar manner. So far it has been pursued\nwith limited success---several authors managed to construct and apply a second\npartitioning polynomial, but further progress has been prevented by technical\nobstacles. We provide a polynomial partitioning method with up to $d$\npolynomials in dimension $d$, which allows for a complete decomposition of the\ngiven point set. We apply it to obtain a new algorithm for the semialgebraic\nrange searching problem. Our algorithm has running time bounds similar to a\nrecent algorithm by Agarwal, Sharir, and the first author [SIAM~J.~Comput.\n42(2013) 2039--2062], but it is simpler both conceptually and technically.\nWhile this paper has been in preparation, Basu and Sombra, as well as Fox,\nPach, Sheffer, Suk, and Zahl, obtained results concerning polynomial partitions\nwhich overlap with ours to some extent.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 20:38:43 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 13:03:08 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Matousek", "Jiri", ""], ["Patakova", "Zuzana", ""]]}, {"id": "1406.3092", "submitter": "Xiaodong Wang", "authors": "Yingjie Wu, Daxin Zhu, Lei Wang and Xiaodong Wang", "title": "A note on the largest number of red nodes in red-black trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in the number of red nodes in red-black\ntrees. We first present an $O(n^2\\log n)$ time dynamic programming solution for\ncomputing $r(n)$, the largest number of red internal nodes in a red-black tree\non $n$ keys. Then the algorithm is improved to some $O(\\log n)$ time recursive\nand nonrecursive algorithms. Based on these improved algorithms we finally find\na closed-form solution of $r(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 00:56:02 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Wu", "Yingjie", ""], ["Zhu", "Daxin", ""], ["Wang", "Lei", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1406.3124", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Guarantees and Limits of Preprocessing in Constraint Satisfaction and\n  Reasoning", "comments": "arXiv admin note: substantial text overlap with arXiv:1104.2541,\n  arXiv:1104.5566", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first theoretical analysis of the power of polynomial-time\npreprocessing for important combinatorial problems from various areas in AI. We\nconsider problems from Constraint Satisfaction, Global Constraints,\nSatisfiability, Nonmonotonic and Bayesian Reasoning under structural\nrestrictions. All these problems involve two tasks: (i) identifying the\nstructure in the input as required by the restriction, and (ii) using the\nidentified structure to solve the reasoning task efficiently. We show that for\nmost of the considered problems, task (i) admits a polynomial-time\npreprocessing to a problem kernel whose size is polynomial in a structural\nproblem parameter of the input, in contrast to task (ii) which does not admit\nsuch a reduction to a problem kernel of polynomial size, subject to a\ncomplexity theoretic assumption. As a notable exception we show that the\nconsistency problem for the AtMost-NValue constraint admits a polynomial kernel\nconsisting of a quadratic number of variables and domain values. Our results\nprovide a firm worst-case guarantees and theoretical boundaries for the\nperformance of polynomial-time preprocessing algorithms for the considered\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 05:44:06 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1406.3170", "submitter": "Simon Gog", "authors": "Simon Gog and Matthias Petri", "title": "Compact Indexes for Flexible Top-k Retrieval", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer a self-index based retrieval system capable of rank-safe\nevaluation of top-k queries. The framework generalizes the GREEDY approach of\nCulpepper et al. (ESA 2010) to handle multi-term queries, including over\nphrases. We propose two techniques which significantly reduce the ranking time\nfor a wide range of popular Information Retrieval (IR) relevance measures, such\nas TFxIDF and BM25. First, we reorder elements in the document array according\nto document weight. Second, we introduce the repetition array, which\ngeneralizes Sadakane's (JDA 2007) document frequency structure to document\nsubsets. Combining document and repetition array, we achieve attractive\nfunctionality-space trade-offs. We provide an extensive evaluation of our\nsystem on terabyte-sized IR collections.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:47:09 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Gog", "Simon", ""], ["Petri", "Matthias", ""]]}, {"id": "1406.3279", "submitter": "Tobias Lieber", "authors": "Riko Jacob, Tobias Lieber, Nodari Sitchinava", "title": "On the Complexity of List Ranking in the Parallel External Memory Model", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-44465-8_33", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of list ranking in the parallel external memory (PEM)\nmodel. We observe an interesting dual nature for the hardness of the problem\ndue to limited information exchange among the processors about the structure of\nthe list, on the one hand, and its close relationship to the problem of\npermuting data, which is known to be hard for the external memory models, on\nthe other hand.\n  By carefully defining the power of the computational model, we prove a\npermuting lower bound in the PEM model. Furthermore, we present a stronger\n\\Omega(log^2 N) lower bound for a special variant of the problem and for a\nspecific range of the model parameters, which takes us a step closer toward\nproving a non-trivial lower bound for the list ranking problem in the\nbulk-synchronous parallel (BSP) and MapReduce models. Finally, we also present\nan algorithm that is tight for a larger range of parameters of the model than\nin prior work.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 16:04:17 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Jacob", "Riko", ""], ["Lieber", "Tobias", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1406.3280", "submitter": "Alban Ponse", "authors": "Jan A. Bergstra and Alban Ponse", "title": "Three Datatype Defining Rewrite Systems for Datatypes of Integers each\n  extending a Datatype of Naturals", "comments": "33 pages; 19 tables. All DDRSes in S.2 are proven ground-complete\n  (gc). In S.3, the DDRS for Z_{ut} contains 16 equations and is proven gc; the\n  DDRS for Z_{bt} has one more equation ([bt22]) and is proven gc; the DDRSes\n  for N_{dt} (Table 14) and Z_{dt} (Table 16) are proven gc in [13]. In\n  Appendix C, corrected versions of the DDRSes for N_{u'} and Z_{u'} are proven\n  gc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer arithmetic is specified according to three views: unary, binary, and\ndecimal notation. The binary and decimal view have as their characteristic that\neach normal form resembles common number notation, that is, either a digit, or\na string of digits without leading zero, or the negated versions of the latter.\nThe unary view comprises a specification of integer arithmetic based on 0,\nsuccessor function $S$, and predecessor function, with negative normal forms\n$-S^i(0)$. Integer arithmetic in binary and decimal notation is based on\n(postfix) digit append functions. For each view we define a ground-confluent\nand terminating datatype defining rewrite system (DDRS), and in each case the\nresulting datatype is a canonical term algebra that extends a corresponding\ncanonical term algebra for natural numbers.\n  Then, for each view, we consider an alternative DDRS based on tree\nconstructors that yield comparable normal forms, which for that binary and\ndecimal view admits expressions that are algorithmically more involved. These\nDDRSes are incorporated because they are closer to existing literature. For\nthese DDRSes we also provide ground-completeness results.\n  Finally, we define a DDRS for the ring of Integers (comprising fifteen\nrewrite rules) and prove its ground-completeness.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 16:21:18 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 12:21:03 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2016 16:45:56 GMT"}, {"version": "v4", "created": "Mon, 18 Jul 2016 11:45:16 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Bergstra", "Jan A.", ""], ["Ponse", "Alban", ""]]}, {"id": "1406.3295", "submitter": "Cesar Caiafa", "authors": "Cesar F. Caiafa and Andrzej Cichocki", "title": "Stable, Robust and Super Fast Reconstruction of Tensors Using Multi-Way\n  Projections", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2385040", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of multidimensional Compressed Sensing (CS), we introduce an\nanalytical reconstruction formula that allows one to recover an $N$th-order\n$(I_1\\times I_2\\times \\cdots \\times I_N)$ data tensor $\\underline{\\mathbf{X}}$\nfrom a reduced set of multi-way compressive measurements by exploiting its low\nmultilinear-rank structure. Moreover, we show that, an interesting property of\nmulti-way measurements allows us to build the reconstruction based on\ncompressive linear measurements taken only in two selected modes, independently\nof the tensor order $N$. In addition, it is proved that, in the matrix case and\nin a particular case with $3$rd-order tensors where the same 2D sensor operator\nis applied to all mode-3 slices, the proposed reconstruction\n$\\underline{\\mathbf{X}}_\\tau$ is stable in the sense that the approximation\nerror is comparable to the one provided by the best low-multilinear-rank\napproximation, where $\\tau$ is a threshold parameter that controls the\napproximation error. Through the analysis of the upper bound of the\napproximation error we show that, in the 2D case, an optimal value for the\nthreshold parameter $\\tau=\\tau_0 > 0$ exists, which is confirmed by our\nsimulation results. On the other hand, our experiments on 3D datasets show that\nvery good reconstructions are obtained using $\\tau=0$, which means that this\nparameter does not need to be tuned. Our extensive simulation results\ndemonstrate the stability and robustness of the method when it is applied to\nreal-world 2D and 3D signals. A comparison with state-of-the-arts sparsity\nbased CS methods specialized for multidimensional signals is also included. A\nvery attractive characteristic of the proposed method is that it provides a\ndirect computation, i.e. it is non-iterative in contrast to all existing\nsparsity based CS algorithms, thus providing super fast computations, even for\nlarge datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 18:35:07 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 17:05:36 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Caiafa", "Cesar F.", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1406.3405", "submitter": "Sanguthevar Rajasekaran", "authors": "Sanguthevar Rajasekaran and Marius Nicolae", "title": "An error correcting parser for context free grammars that takes less\n  than cubic time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of parsing has been studied extensively for various formal\ngrammars. Given an input string and a grammar, the parsing problem is to check\nif the input string belongs to the language generated by the grammar. A closely\nrelated problem of great importance is one where the input are a string ${\\cal\nI}$ and a grammar $G$ and the task is to produce a string ${\\cal I}'$ that\nbelongs to the language generated by $G$ and the `distance' between ${\\cal I}$\nand ${\\cal I}'$ is the smallest (from among all the strings in the language).\nSpecifically, if ${\\cal I}$ is in the language generated by $G$, then the\noutput should be ${\\cal I}$. Any parser that solves this version of the problem\nis called an {\\em error correcting parser}. In 1972 Aho and Peterson presented\na cubic time error correcting parser for context free grammars. Since then this\nasymptotic time bound has not been improved under the (standard) assumption\nthat the grammar size is a constant. In this paper we present an error\ncorrecting parser for context free grammars that runs in $O(T(n))$ time, where\n$n$ is the length of the input string and $T(n)$ is the time needed to compute\nthe tropical product of two $n\\times n$ matrices.\n  In this paper we also present an $\\frac{n}{M}$-approximation algorithm for\nthe {\\em language edit distance problem} that has a run time of $O(Mn^\\omega)$,\nwhere $O(n^\\omega)$ is the time taken to multiply two $n\\times n$ matrices. To\nthe best of our knowledge, no approximation algorithms have been proposed for\nerror correcting parsing for general context free grammars.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 02:05:37 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Rajasekaran", "Sanguthevar", ""], ["Nicolae", "Marius", ""]]}, {"id": "1406.3414", "submitter": "Huiwen Yu", "authors": "Martin Furer, Huiwen Yu", "title": "Space Saving by Dynamic Algebraization", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming is widely used for exact computations based on tree\ndecompositions of graphs. However, the space complexity is usually exponential\nin the treewidth. We study the problem of designing efficient dynamic\nprogramming algorithm based on tree decompositions in polynomial space. We show\nhow to construct a tree decomposition and extend the algebraic techniques of\nLokshtanov and Nederlof such that the dynamic programming algorithm runs in\ntime $O^*(2^h)$, where $h$ is the maximum number of vertices in the union of\nbags on the root to leaf paths on a given tree decomposition, which is a\nparameter closely related to the tree-depth of a graph. We apply our algorithm\nto the problem of counting perfect matchings on grids and show that it\noutperforms other polynomial-space solutions. We also apply the algorithm to\nother set covering and partitioning problems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 04:14:52 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Furer", "Martin", ""], ["Yu", "Huiwen", ""]]}, {"id": "1406.3514", "submitter": "Roland Mark\\'o", "authors": "Marek Karpinski and Roland Mark\\'o", "title": "Limits of CSP Problems and Efficient Parameter Testing", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework on the limits of constraint satisfaction\nproblems (CSPs) and efficient parameter testing which depends only on array\nexchangeability and the method of cut decomposition without recourse to the\nweakly regular partitions. In particular, we formulate and prove a\nrepresentation theorem for compact colored $r$-uniform directed hypergraph\n($r$-graph) limits, and apply this to $r$CSP limits. We investigate the sample\ncomplexity of testable $r$-graph parameters, we discuss the generalized ground\nstate energies and demonstrate that they are efficiently testable.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 12:01:30 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 14:05:58 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Karpinski", "Marek", ""], ["Mark\u00f3", "Roland", ""]]}, {"id": "1406.3655", "submitter": "Aline Saettler", "authors": "Aline Saettler, Eduardo Laber, Ferdinando Cicalese", "title": "Trading off Worst and Expected Cost in Decision Tree Problems and a\n  Value Dependent Model", "comments": "arXiv admin note: substantial text overlap with arXiv:1309.2796", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of evaluating a discrete function by adaptively querying\nthe values of its variables until the values read uniquely determine the value\nof the function. Reading the value of a variable is done at the expense of some\ncost, and the goal is to design a strategy (decision tree) for evaluating the\nfunction incurring as little cost as possible in the worst case or in\nexpectation (according to a prior distribution on the possible variables\nassignments). Except for particular cases of the problem, in general, only the\nminimization of one of these two measures is addressed in the literature.\nHowever, there are instances of the problem for which the minimization of one\nmeasure leads to a strategy with a high cost with respect to the other measure\n(even exponentially bigger than the optimal). We provide a new construction\nwhich can guarantee a trade-off between the two criteria. More precisely, given\na decision tree guaranteeing expected cost $E$ and a decision tree guaranteeing\nworst cost $W$ our method can guarantee for any chosen trade-off value $\\rho$\nto produce a decision tree whose worst cost is $(1 + \\rho)W$ and whose expected\ncost is $(1 + \\frac{1}{\\rho})E.$ These bounds are improved for the relevant\ncase of uniform testing costs. Motivated by applications, we also study a\nvariant of the problem where the cost of reading a variable depends on the\nvariable's value. We provide an $O(\\log n)$ approximation algorithm for the\nminimization of the worst cost measure, which is best possible under the\nassumption $P \\neq NP$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 21:49:12 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Saettler", "Aline", ""], ["Laber", "Eduardo", ""], ["Cicalese", "Ferdinando", ""]]}, {"id": "1406.3671", "submitter": "Jelena Marasevic", "authors": "Jelena Marasevic, Cliff Stein, Gil Zussman", "title": "Max-min Fair Rate Allocation and Routing in Energy Harvesting Networks:\n  Algorithmic Analysis", "comments": "Full version of the paper published at ACM MobiHoc'14", "journal-ref": null, "doi": "10.1007/s00453-016-0171-6", "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers max-min fair rate allocation and routing in energy\nharvesting networks where fairness is required among both the nodes and the\ntime slots. Unlike most previous work on fairness, we focus on multihop\ntopologies and consider different routing methods. We assume a predictable\nenergy profile and focus on the design of efficient and optimal algorithms that\ncan serve as benchmarks for distributed and approximate algorithms. We first\ndevelop an algorithm that obtains a max-min fair rate assignment for any given\n(time-variable or time-invariable) unsplittable routing or a routing tree. For\ntime-invariable unsplittable routing, we also develop an algorithm that finds\nroutes that maximize the minimum rate assigned to any node in any slot. For\nfractional routing, we study the joint routing and rate assignment problem. We\ndevelop an algorithm for the time-invariable case with constant rates. We show\nthat the time-variable case is at least as hard as the 2-commodity feasible\nflow problem and design an FPTAS to combat the high running time. Finally, we\nshow that finding an unsplittable routing or a routing tree that provides\nlexicographically maximum rate assignment (i.e., that is the best in the\nmax-min fairness terms) is NP-hard, even for a time horizon of a single slot.\nOur analysis provides insights into the problem structure and can be applied to\nother related fairness problems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 00:42:23 GMT"}, {"version": "v2", "created": "Sun, 9 Nov 2014 19:57:28 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Marasevic", "Jelena", ""], ["Stein", "Cliff", ""], ["Zussman", "Gil", ""]]}, {"id": "1406.3812", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Pinar Heggernes, Pim van 't Hof, and Christophe Paul", "title": "Hadwiger number of graphs with small chordality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hadwiger number of a graph G is the largest integer h such that G has the\ncomplete graph K_h as a minor. We show that the problem of determining the\nHadwiger number of a graph is NP-hard on co-bipartite graphs, but can be solved\nin polynomial time on cographs and on bipartite permutation graphs. We also\nconsider a natural generalization of this problem that asks for the largest\ninteger h such that G has a minor with h vertices and diameter at most $s$. We\nshow that this problem can be solved in polynomial time on AT-free graphs when\ns>=2, but is NP-hard on chordal graphs for every fixed s>=2.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 13:08:56 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Hof", "Pim van 't", ""], ["Paul", "Christophe", ""]]}, {"id": "1406.4056", "submitter": "Radu Curticapean", "authors": "Radu Curticapean", "title": "Counting perfect matchings in graphs that exclude a single-crossing\n  minor", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $H$ is single-crossing if it can be drawn in the plane with at most\none crossing. For any single-crossing graph $H$, we give an $O(n^4)$ time\nalgorithm for counting perfect matchings in graphs excluding $H$ as a minor.\nThe runtime can be lowered to $O(n^{1.5})$ when $G$ excludes $K_5$ or $K_{3,3}$\nas a minor. This is the first generalization of an algorithm for counting\nperfect matchings in $K_{3,3}$-free graphs (Little 1974, Vazirani 1989). Our\nalgorithm uses black-boxes for counting perfect matchings in planar graphs and\nfor computing certain graph decompositions. Together with an independent recent\nresult (Straub et al. 2014) for graphs excluding $K_5$, it is one of the first\nnontrivial algorithms to not inherently rely on Pfaffian orientations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 16:06:46 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Curticapean", "Radu", ""]]}, {"id": "1406.4173", "submitter": "D\\'ora Erd\\H{o}s", "authors": "Dora Erdos, Vatche Ishakian, Azer Bestavros, Evimaria Terzi", "title": "A Divide-and-Conquer Algorithm for Betweenness Centrality", "comments": "Shorter version of this paper appeared in Siam Data Mining 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of efficiently computing the betweenness centrality of nodes has\nbeen researched extensively. To date, the best known exact and centralized\nalgorithm for this task is an algorithm proposed in 2001 by Brandes. The\ncontribution of our paper is Brandes++, an algorithm for exact efficient\ncomputation of betweenness centrality. The crux of our algorithm is that we\ncreate a sketch of the graph, that we call the skeleton, by replacing subgraphs\nwith simpler graph structures. Depending on the underlying graph structure,\nusing this skeleton and by keeping appropriate summaries Brandes++ we can\nachieve significantly low running times in our computations. Extensive\nexperimental evaluation on real life datasets demonstrate the efficacy of our\nalgorithm for different types of graphs. We release our code for benefit of the\nresearch community.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 21:18:51 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 19:58:34 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Erdos", "Dora", ""], ["Ishakian", "Vatche", ""], ["Bestavros", "Azer", ""], ["Terzi", "Evimaria", ""]]}, {"id": "1406.4433", "submitter": "Paul Withers", "authors": "Colin McDiarmid, Alex Scott and Paul Withers", "title": "Uniform multicommodity flow in the hypercube with random edge capacities", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two results for multicommodity flows in the $d$-dimensional hypercube\n${Q}^d$ with independent random edge capacities distributed like $C$ where\n$\\Pr[C>0]>1/2$. Firstly, with high probability as $d \\rightarrow \\infty$, the\nnetwork can support simultaneous multicommodity flows of volume close to $E[C]$\nbetween all antipodal vertex pairs. Secondly, with high probability, the\nnetwork can support simultaneous multicommodity flows of volume close to\n$2^{1-d} E[C]$ between all vertex pairs. Both results are best possible.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 17:19:25 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2015 10:40:37 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 13:35:37 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["McDiarmid", "Colin", ""], ["Scott", "Alex", ""], ["Withers", "Paul", ""]]}, {"id": "1406.4454", "submitter": "Shanfei Li", "authors": "Shanfei Li", "title": "An Improved Approximation Algorithm for the Hard Uniform Capacitated\n  k-median Problem", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-median problem, given a set of locations, the goal is to select a\nsubset of at most $k$ centers so as to minimize the total cost of connecting\neach location to its nearest center. We study the uniform hard capacitated\nversion of the $k$-median problem, in which each selected center can only serve\na limited number of locations.\n  Inspired by the algorithm of Charikar, Guha, Tardos and Shmoys, we give a\n$(6+10\\alpha)$-approximation algorithm for this problem with increasing the\ncapacities by a factor of $2+\\frac{2}{\\alpha}, \\alpha\\geq 4$, which improves\nthe previous best $(32 l^2+28 l+7)$-approximation algorithm proposed by Byrka,\nFleszar, Rybicki and Spoerhase violating the capacities by factor\n$2+\\frac{3}{l-1}, l\\in \\{2,3,4,\\dots\\}$.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 18:00:48 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Li", "Shanfei", ""]]}, {"id": "1406.4712", "submitter": "Virendra Sule", "authors": "Virendra Sule", "title": "An algorithm for Boolean satisfiability based on generalized orthonormal\n  expansion", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm for deciding consistency of systems of\nBoolean equations in several variables with co-efficients in the two element\nBoolean algebra $B_{0}=\\{0,1\\}$ and find all satisfying assignments. The\nalgorithm is based on the application of a well known generalized Boole-Shannon\northonormal (ON) expansion of Boolean functions. A necessary and sufficient\nconsistency condition for a special class of functions was developed in\n\\cite{sule} using such an expansion. Paper \\cite{sule} develops a condition for\nconsistency of the equation $f(X)=0$ for the special classes of Boolean\nfunctions 1) $f$ in $B(\\Phi(X))$ for an ON set $\\Phi$ of Boolean functions in\n$X$ over a general Boolean algebra $B$ and 2) $f$ in $B(X_{2})(\\Phi(X_{1}))$.\nThe present paper addresses the problem of obtaining the consistency conditions\nfor arbitrary Boolean functions in $B_{0}(X)$. Next, the consistency for a\nsingle equation is shown equivalent to another system of Boolean equations\nwhich involves the ON functions and characterizes all solutions. This result is\nthen extended for Boolean systems in several variables over the algebra\n$B_{0}=\\{0,1\\}$ which does not convert the system into a single equation. This\ncondition leads to the algorithm for computing all solutions of the Boolean\nsystem without using analogous resolution and determine satisfiability. For\nspecial systems defined by CNF formulas this algorithm results into an\nextension of the DPLL algorithm in which the \\emph{splitting rule} is\ngeneralized to several variables in terms of ON terms in the sense that\nsplitting of CNF set in a single variable $x$ is equivalent to ON terms $x,x'$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 13:42:52 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 15:31:24 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 16:57:39 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Sule", "Virendra", ""]]}, {"id": "1406.4718", "submitter": "Jannis Bulian", "authors": "Jannis Bulian and Anuj Dawar", "title": "Graph Isomorphism Parameterized by Elimination Distance to Bounded\n  Degree", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly studied means of parameterizing graph problems is the deletion\ndistance from triviality (Guo et al. 2004), which counts vertices that need to\nbe deleted from a graph to place it in some class for which efficient\nalgorithms are known. In the context of graph isomorphism, we define triviality\nto mean a graph with maximum degree bounded by a constant, as such graph\nclasses admit polynomial-time isomorphism tests. We generalise deletion\ndistance to a measure we call elimination distance to triviality, based on\nelimination trees or tree-depth decompositions. We establish that graph\ncanonisation, and thus graph isomorphism, is FPT when parameterized by\nelimination distance to bounded degree, extending results of Bouland et al.\n(2012).\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 14:08:39 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 14:55:59 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Bulian", "Jannis", ""], ["Dawar", "Anuj", ""]]}, {"id": "1406.4784", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Densification of One Permutation Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing work on densification of one permutation hashing reduces the\nquery processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing\n(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,\nwhere $d$ is the number of nonzeros of the data vector, $K$ is the number of\nhashes in each hash table, and $L$ is the number of hash tables. While that is\na substantial improvement, our analysis reveals that the existing densification\nscheme is sub-optimal. In particular, there is no enough randomness in that\nprocedure, which affects its accuracy on very sparse datasets.\n  In this paper, we provide a new densification procedure which is provably\nbetter than the existing scheme. This improvement is more significant for very\nsparse datasets which are common over the web. The improved technique has the\nsame cost of $O(d + KL)$ for query processing, thereby making it strictly\npreferable over the existing procedure. Experimental evaluations on public\ndatasets, in the task of hashing based near neighbor search, support our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 16:16:22 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1406.4799", "submitter": "Martin Gro{\\ss}", "authors": "Martin Gro{\\ss} and Martin Skutella", "title": "A tight bound on the speed-up through storage for quickest\n  multi-commodity flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-commodity flows over time exhibit the non-intuitive property that\nletting flow wait can allow us to send flow faster overall. Fleischer and\nSkutella (IPCO~2002) show that the speed-up through storage is at most a factor\nof~$2$, and that there are instances where the speed-up is as large as a factor\nof~$4/3$. We close this gap by presenting a family of instances for which the\nspeed-up factor through storage converges to~$2$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 17:12:14 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Gro\u00df", "Martin", ""], ["Skutella", "Martin", ""]]}, {"id": "1406.4828", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "Analyzing Traffic Problem Model With Graph Theory Algorithms", "comments": "7 pages, 5 figures, Science and Information Conference (SAI), 2015", "journal-ref": null, "doi": "10.1109/SAI.2015.7237137", "report-no": null, "categories": "cs.DS cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper will contribute to a practical problem, Urban Traffic. We will\ninvestigate those features, try to simplify the complexity and formulize this\ndynamic system. These contents mainly contain how to analyze a decision problem\nwith combinatorial method and graph theory algorithms; how to optimize our\nstrategy to gain a feasible solution through employing other principles of\nComputer Science.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 09:20:35 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 14:42:35 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1406.5301", "submitter": "Borko Boskovic", "authors": "Borko Bo\\v{s}kovi\\'c, Franc Brglez, Janez Brest", "title": "Low-Autocorrelation Binary Sequences: On Improved Merit Factors and\n  Runtime Predictions to Achieve Them", "comments": null, "journal-ref": "Applied Soft Computing, Volume 56, July 2017, Pages 262-285", "doi": "10.1016/j.asoc.2017.02.024", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for binary sequences with a high figure of merit, known as the low\nautocorrelation binary sequence ($labs$}) problem, represents a formidable\ncomputational challenge. To mitigate the computational constraints of the\nproblem, we consider solvers that accept odd values of sequence length $L$ and\nreturn solutions for skew-symmetric binary sequences only -- with the\nconsequence that not all best solutions under this constraint will be optimal\nfor each $L$. In order to improve both, the search for best merit factor $and$\nthe asymptotic runtime performance, we instrumented three stochastic solvers,\nthe first two are state-of-the-art solvers that rely on variants of memetic and\ntabu search ($lssMAts$ and $lssRRts$), the third solver ($lssOrel$) organizes\nthe search as a sequence of independent contiguous self-avoiding walk segments.\nBy adapting a rigorous statistical methodology to performance testing of all\nthree combinatorial solvers, experiments show that the solver with the best\nasymptotic average-case performance, $lssOrel\\_8 = 0.000032*1.1504^L$, has the\nbest chance of finding solutions that improve, as $L$ increases, figures of\nmerit reported to date. The same methodology can be applied to engineering new\n$labs$ solvers that may return merit factors even closer to the conjectured\nasymptotic value of 12.3248.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 08:00:15 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 06:57:01 GMT"}, {"version": "v3", "created": "Mon, 14 Jul 2014 13:54:15 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2016 12:17:46 GMT"}, {"version": "v5", "created": "Thu, 24 Mar 2016 12:44:48 GMT"}, {"version": "v6", "created": "Sat, 6 May 2017 08:36:16 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bo\u0161kovi\u0107", "Borko", ""], ["Brglez", "Franc", ""], ["Brest", "Janez", ""]]}, {"id": "1406.5433", "submitter": "Xavier Allamigeon", "authors": "Xavier Allamigeon, Pascal Benchimol, and St\\'ephane Gaubert", "title": "The tropical shadow-vertex algorithm solves mean payoff games in\n  polynomial time on average", "comments": "17 pages, 7 figures, appears in 41st International Colloquium, ICALP\n  2014, Copenhagen, Denmark, July 8-11, 2014, Proceedings, Part I", "journal-ref": null, "doi": "10.1007/978-3-662-43948-7_8", "report-no": null, "categories": "cs.GT cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm which solves mean payoff games in polynomial time\non average, assuming the distribution of the games satisfies a flip invariance\nproperty on the set of actions associated with every state. The algorithm is a\ntropical analogue of the shadow-vertex simplex algorithm, which solves mean\npayoff games via linear feasibility problems over the tropical semiring\n$(\\mathbb{R} \\cup \\{-\\infty\\}, \\max, +)$. The key ingredient in our approach is\nthat the shadow-vertex pivoting rule can be transferred to tropical polyhedra,\nand that its computation reduces to optimal assignment problems through\nPl\\\"ucker relations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 15:39:44 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 15:31:42 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Allamigeon", "Xavier", ""], ["Benchimol", "Pascal", ""], ["Gaubert", "St\u00e9phane", ""]]}, {"id": "1406.5480", "submitter": "Carl Barton", "authors": "Carl Barton, Costas S. Iliopoulos, and Solon P. Pissis", "title": "Average-Case Optimal Approximate Circular String Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate string matching is the problem of finding all factors of a text t\nof length n that are at a distance at most k from a pattern x of length m.\nApproximate circular string matching is the problem of finding all factors of t\nthat are at a distance at most k from x or from any of its rotations. In this\narticle, we present a new algorithm for approximate circular string matching\nunder the edit distance model with optimal average-case search time O(n(k + log\nm)/m). Optimal average-case search time can also be achieved by the algorithms\nfor multiple approximate string matching (Fredriksson and Navarro, 2004) using\nx and its rotations as the set of multiple patterns. Here we reduce the\npreprocessing time and space requirements compared to that approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 18:37:14 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 18:04:19 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Barton", "Carl", ""], ["Iliopoulos", "Costas S.", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1406.5481", "submitter": "Matthieu Roy", "authors": "Matthieu Roy (LAAS), Stefan Schmid (LAAS), Gilles Tr\\'edan (LAAS)", "title": "Modeling and Measuring Graph Similarity: The Case for Centrality\n  Distance", "comments": "FOMC 2014, 10th ACM International Workshop on Foundations of Mobile\n  Computing, Philadelphia : United States (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of the topological structure of complex networks has fascinated\nresearchers for several decades, and today we have a fairly good understanding\nof the types and reoccurring characteristics of many different complex\nnetworks. However, surprisingly little is known today about models to compare\ncomplex graphs, and quantitatively measure their similarity. This paper\nproposes a natural similarity measure for complex networks: centrality\ndistance, the difference between two graphs with respect to a given node\ncentrality. Centrality distances allow to take into account the specific roles\nof the different nodes in the network, and have many interesting applications.\nAs a case study, we consider the closeness centrality in more detail, and show\nthat closeness centrality distance can be used to effectively distinguish\nbetween randomly generated and actual evolutionary paths of two dynamic social\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 18:37:20 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Roy", "Matthieu", "", "LAAS"], ["Schmid", "Stefan", "", "LAAS"], ["Tr\u00e9dan", "Gilles", "", "LAAS"]]}, {"id": "1406.5597", "submitter": "Anando Chatterjee", "authors": "A. G. Chatterjee, M. K. Verma, and M. Chaudhuri", "title": "Transpose-free Fast Fourier Transform for Turbulence Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.DS physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-spectral method is one of the most accurate techniques for simulating\nturbulent flows. Fast Fourier transform (FFT) is an integral part of this\nmethod. In this paper, we present a new procedure to compute FFT in which we\nsave operations during interprocess communications by avoiding transpose of the\narray. As a result, our transpose-free FFT is 15\\% to 20\\% faster than FFTW.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 11:19:59 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Chatterjee", "A. G.", ""], ["Verma", "M. K.", ""], ["Chaudhuri", "M.", ""]]}, {"id": "1406.5665", "submitter": "Aravindan Vijayaraghavan", "authors": "Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan", "title": "Constant Factor Approximation for Balanced Cut in the PIE model", "comments": "Full version of the paper at the 46th ACM Symposium on the Theory of\n  Computing (STOC 2014). 32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new semi-random semi-adversarial model for Balanced\nCut, a planted model with permutation-invariant random edges (PIE). Our model\nis much more general than planted models considered previously. Consider a set\nof vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$\nbe an arbitrary graph on $V$ with no edges between $L$ and $R$. Let\n$E_{random}$ be a set of edges sampled from an arbitrary permutation-invariant\ndistribution (a distribution that is invariant under permutation of vertices in\n$L$ and in $R$). Then we say that $G + E_{random}$ is a graph with\npermutation-invariant random edges.\n  We present an approximation algorithm for the Balanced Cut problem that finds\na balanced cut of cost $O(|E_{random}|) + n \\text{polylog}(n)$ in this model.\nIn the regime when $|E_{random}| = \\Omega(n \\text{polylog}(n))$, this is a\nconstant factor approximation with respect to the cost of the planted cut.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 03:00:32 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1406.5667", "submitter": "Aravindan Vijayaraghavan", "authors": "Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan", "title": "Correlation Clustering with Noisy Partial Information", "comments": "To appear at Conference on Learning Theory (COLT) 2015. Substantial\n  changes from previous version, including a new section on recovery of the\n  ground truth clustering. 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study a semi-random model for the Correlation\nClustering problem on arbitrary graphs G. We give two approximation algorithms\nfor Correlation Clustering instances from this model. The first algorithm finds\na solution of value $(1+ \\delta) optcost + O_{\\delta}(n\\log^3 n)$ with high\nprobability, where $optcost$ is the value of the optimal solution (for every\n$\\delta > 0$). The second algorithm finds the ground truth clustering with an\narbitrarily small classification error $\\eta$ (under some additional\nassumptions on the instance).\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 03:07:55 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:33:12 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1406.5687", "submitter": "Shaikh Arifuzzaman", "authors": "Shaikh Arifuzzaman, Maleq Khan, and Madhav Marathe", "title": "Parallel Algorithms for Counting Triangles in Networks with Large\n  Degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the number of triangles in a network is an important problem in the\nanalysis of complex networks. The number of triangles also has important\napplications in data mining. Existing distributed memory parallel algorithms\nfor counting triangles are either Map-Reduce based or message passing interface\n(MPI) based and work with overlapping partitions of the given network. These\nalgorithms are designed for very sparse networks and do not work well when the\ndegrees of the nodes are relatively larger. For networks with larger degrees,\nMap-Reduce based algorithm generates prohibitively large intermediate data, and\nin MPI based algorithms with overlapping partitions, each partition can grow as\nlarge as the original network, wiping out the benefit of partitioning the\nnetwork.\n  In this paper, we present two efficient MPI-based parallel algorithms for\ncounting triangles in massive networks with large degrees. The first algorithm\nis a space-efficient algorithm for networks that do not fit in the main memory\nof a single compute node. This algorithm divides the network into\nnon-overlapping partitions. The second algorithm is for the case where the main\nmemory of each node is large enough to contain the entire network. We observe\nthat for such a case, computation load can be balanced dynamically and present\na dynamic load balancing scheme which improves the performance significantly.\nBoth of our algorithms scale well to large networks and to a large number of\nprocessors.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 09:11:22 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Arifuzzaman", "Shaikh", ""], ["Khan", "Maleq", ""], ["Marathe", "Madhav", ""]]}, {"id": "1406.5826", "submitter": "Demetres Christofides", "authors": "Demetres Christofides", "title": "The asymptotic complexity of matrix reduction over finite fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an invertible n \\times n matrix over some field. The Gauss-Jordan\nelimination reduces this matrix to the identity matrix using at most n^2 row\noperations and in general that many operations might be needed.\n  In [1] the authors considered matrices in GL(n;q), the set of n \\times n\ninvertible matrices in the finite field of q elements, and provided an\nalgorithm using only row operations which performs asymptotically better than\nthe Gauss-Jordan elimination. More specifically their `striped elimination\nalgorithm' has asymptotic complexity \\frac{n^2}{\\log_q{n}}. Furthermore they\nproved that up to a constant factor this algorithm is best possible as almost\nall matrices in GL(n;g) need asymptotically at least \\frac{n^2}{2\\log_q{n}}\noperations.\n  In this short note we show that the `striped elimination algorithm' is\nasymptotically optimal by proving that almost all matrices in GL(n;q) need\nasymptotically at least frac{n^2}{\\log_q{n}} operations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 08:03:23 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Christofides", "Demetres", ""]]}, {"id": "1406.5943", "submitter": "David Harris", "authors": "David G. Harris and Aravind Srinivasan", "title": "The Moser-Tardos Framework with Partial Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resampling algorithm of Moser \\& Tardos is a powerful approach to develop\nconstructive versions of the Lov\\'{a}sz Local Lemma (LLL). We generalize this\nto partial resampling: when a bad event holds, we resample an\nappropriately-random subset of the variables that define this event, rather\nthan the entire set as in Moser & Tardos. This is particularly useful when the\nbad events are determined by sums of random variables. This leads to several\nimproved algorithmic applications in scheduling, graph transversals, packet\nrouting etc. For instance, we settle a conjecture of Szab\\'{o} & Tardos (2006)\non graph transversals asymptotically, and obtain improved approximation ratios\nfor a packet routing problem of Leighton, Maggs, & Rao (1994).\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 15:26:49 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 20:10:24 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 14:58:26 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2018 19:15:24 GMT"}, {"version": "v5", "created": "Fri, 28 Jun 2019 02:01:53 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Harris", "David G.", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1406.6047", "submitter": "Gustavo Sacomoto", "authors": "Gustavo Sacomoto", "title": "Efficient Algorithms for de novo Assembly of Alternative Splicing Events\n  from RNA-seq Data", "comments": "PhD thesis, 139 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we address the problem of identifying and quantifying\nvariants (alternative splicing and genomic polymorphism) in RNA-seq data when\nno reference genome is available, without assembling the full transcripts.\nBased on the fundamental idea that each variant corresponds to a recognizable\npattern, a bubble, in a de Bruijn graph constructed from the RNA-seq reads, we\npropose a general model for all variants in such graphs. We then introduce an\nexact method, called KisSplice, to extract alternative splicing events.\nFinally, we show that it enables to identify more correct events than general\npurpose transcriptome assemblers.\n  In order to deal with ever-increasing volumes of NGS data, we put an extra\neffort to make KisSplice as scalable as possible. First, to improve its running\ntime, we propose a new polynomial delay algorithm to enumerate bubbles. We show\nthat it is several orders of magnitude faster than previous approaches. Then,\nto reduce its memory consumption, we propose a new compact way to build and\nrepresent a de Bruijn graph. We show that our approach uses 30% to 40% less\nmemory than the state of the art, with an insignificant impact on the\nconstruction time.\n  Additionally, we apply the same techniques developed to list bubbles in two\nclassical problems: cycle enumeration and the K-shortest paths problem. We give\nthe first optimal algorithm to list cycles in undirected graphs, improving over\nJohnson's algorithm. This is the first improvement to this problem in almost 40\nyears. We then consider a different parameterization of the classical\nK-shortest (simple) paths problem: instead of bounding the number of st-paths,\nwe bound the weight of the st-paths. We present new algorithms with the same\ntime complexities but using exponentially less memory than previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 11:17:23 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Sacomoto", "Gustavo", ""]]}, {"id": "1406.6084", "submitter": "Zhenming Liu", "authors": "Henry Lam and Zhenming Liu", "title": "From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-stochastic online learning approach to price financial\noptions by modeling the market dynamic as a repeated game between the nature\n(adversary) and the investor. We demonstrate that such framework yields\nanalogous structure as the Black-Scholes model, the widely popular option\npricing model in stochastic finance, for both European and American options\nwith convex payoffs. In the case of non-convex options, we construct\napproximate pricing algorithms, and demonstrate that their efficiency can be\nanalyzed through the introduction of an artificial probability measure, in\nparallel to the so-called risk-neutral measure in the finance literature, even\nthough our framework is completely adversarial. Continuous-time convergence\nresults and extensions to incorporate price jumps are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 20:40:14 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Lam", "Henry", ""], ["Liu", "Zhenming", ""]]}, {"id": "1406.6169", "submitter": "Parter Merav", "authors": "Merav Parter and David Peleg", "title": "Fault Tolerant Approximate BFS Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of designing a {\\em fault-tolerant}\n$(\\alpha, \\beta)$ approximate BFS structure (or {\\em FT-ABFS structure} for\nshort), namely, a subgraph $H$ of the network $G$ such that subsequent to the\nfailure of some subset $F$ of edges or vertices, the surviving part of $H$\nstill contains an \\emph{approximate} BFS spanning tree for (the surviving part\nof) $G$, satisfying $dist(s,v,H\\setminus F) \\leq \\alpha \\cdot\ndist(s,v,G\\setminus F)+\\beta$ for every $v \\in V$. We first consider {\\em\nmultiplicative} $(\\alpha,0)$ FT-ABFS structures resilient to a failure of a\nsingle edge and present an algorithm that given an $n$-vertex unweighted\nundirected graph $G$ and a source $s$ constructs a $(3,0)$ FT-ABFS structure\nrooted at $s$ with at most $4n$ edges (improving by an $O(\\log n)$ factor on\nthe near-tight result of \\cite{BS10} for the special case of edge failures).\nAssuming at most $f$ edge failures, for constant integer $f>1$, we prove that\nthere exists a (poly-time constructible) $(3(f+1), (f+1) \\log n)$ FT-ABFS\nstructure with $O(f n)$ edges.\n  We then consider {\\em additive} $(1,\\beta)$ FT-ABFS structures. In contrast\nto the linear size of $(\\alpha,0)$ FT-ABFS structures, we show that for every\n$\\beta \\in [1, O(\\log n)]$ there exists an $n$-vertex graph $G$ with a source\n$s$ for which any $(1,\\beta)$ FT-ABFS structure rooted at $s$ has\n$\\Omega(n^{1+\\epsilon(\\beta)})$ edges, for some function $\\epsilon(\\beta) \\in\n(0,1)$. In particular, $(1,3)$ FT-ABFS structures admit a lower bound of\n$\\Omega(n^{5/4})$ edges. Our lower bounds are complemented by an upper bound,\nshowing that there exists a poly-time algorithm that for every $n$-vertex\nunweighted undirected graph $G$ and source $s$ constructs a $(1,4)$ FT-ABFS\nstructure rooted at $s$ with at most $O(n^{4/3})$ edges.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 08:46:12 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Parter", "Merav", ""], ["Peleg", "David", ""]]}, {"id": "1406.6341", "submitter": "Solon Pissis", "authors": "Carl Barton and Alice Heliou and Laurent Mouchard and Solon P. Pissis", "title": "Linear-time Computation of Minimal Absent Words Using Suffix Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An absent word of a word y of length n is a word that does not occur in y. It\nis a minimal absent word if all its proper factors occur in y. Minimal absent\nwords have been computed in genomes of organisms from all domains of life;\ntheir computation provides a fast alternative for measuring approximation in\nsequence comparison. There exists an O(n)-time and O(n)-space algorithm for\ncomputing all minimal absent words on a fixed-sized alphabet based on the\nconstruction of suffix automata (Crochemore et al., 1998). No implementation of\nthis algorithm is publicly available. There also exists an O(n^2)-time and\nO(n)-space algorithm for the same problem based on the construction of suffix\narrays (Pinho et al., 2009). An implementation of this algorithm was also\nprovided by the authors and is currently the fastest available. In this\narticle, we bridge this unpleasant gap by presenting an O(n)-time and\nO(n)-space algorithm for computing all minimal absent words based on the\nconstruction of suffix arrays. Experimental results using real and synthetic\ndata show that the respective implementation outperforms the one by Pinho et\nal.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 19:24:53 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 10:52:31 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Barton", "Carl", ""], ["Heliou", "Alice", ""], ["Mouchard", "Laurent", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1406.6355", "submitter": "Yichen Huang", "authors": "Yichen Huang", "title": "A polynomial-time algorithm for the ground state of one-dimensional\n  gapped Hamiltonians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A (deterministic) polynomial-time algorithm is proposed for approximating the\nground state of (general) one-dimensional gapped Hamiltonians. Let\n$\\epsilon,n,\\eta$ be the energy gap, the system size, and the desired\nprecision, respectively. Neglecting $\\epsilon$-dependent subpolynomial (in $n$)\nand constant factors, the running time of the algorithm is $n^{O(1)}$ for\n$\\eta=n^{-O(1)}$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 19:55:33 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 06:43:18 GMT"}, {"version": "v3", "created": "Mon, 9 Feb 2015 09:46:35 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2015 18:49:29 GMT"}, {"version": "v5", "created": "Mon, 28 Sep 2015 03:13:45 GMT"}, {"version": "v6", "created": "Mon, 26 Oct 2015 19:44:53 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Huang", "Yichen", ""]]}, {"id": "1406.6474", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, Stefanie Jegelka, Michael I. Jordan", "title": "On the Convergence Rate of Decomposable Submodular Function Minimization", "comments": "17 pages, 3 figures", "journal-ref": "Neural Information Processing Systems 27, 2014", "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions describe a variety of discrete problems in machine\nlearning, signal processing, and computer vision. However, minimizing\nsubmodular functions poses a number of algorithmic challenges. Recent work\nintroduced an easy-to-use, parallelizable algorithm for minimizing submodular\nfunctions that decompose as the sum of \"simple\" submodular functions.\nEmpirically, this algorithm performs extremely well, but no theoretical\nanalysis was given. In this paper, we show that the algorithm converges\nlinearly, and we provide upper and lower bounds on the rate of convergence. Our\nproof relies on the geometry of submodular polyhedra and draws on results from\nspectral graph theory.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 06:52:33 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 18:12:03 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 07:19:00 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Nishihara", "Robert", ""], ["Jegelka", "Stefanie", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1406.6533", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini and Giordano Da Lozzo and Giuseppe Di Battista and\n  Fabrizio Frati and Vincenzo Roselli", "title": "On the Complexity of Clustered-Level Planarity and T-Level Planarity", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study two problems related to the drawing of level graphs,\nthat is, T-LEVEL PLANARITY and CLUSTERED-LEVEL PLANARITY. We show that both\nproblems are NP-complete in the general case and that they become\npolynomial-time solvable when restricted to proper instances.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 12:04:08 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1406.6567", "submitter": "Marcin Kami\\'nski", "authors": "Takehiro Ito, Marcin Kami\\'nski, Hirotaka Ono", "title": "Fixed-Parameter Tractability of Token Jumping on Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given two independent sets $I_0$ and $I_r$ of a graph\nsuch that $|I_0| = |I_r|$, and imagine that a token is placed on each vertex in\n$I_0$. The token jumping problem is to determine whether there exists a\nsequence of independent sets which transforms $I_0$ into $I_r$ so that each\nindependent set in the sequence results from the previous one by moving exactly\none token to another vertex. This problem is known to be PSPACE-complete even\nfor planar graphs of maximum degree three, and W[1]-hard for general graphs\nwhen parameterized by the number of tokens. In this paper, we present a\nfixed-parameter algorithm for the token jumping problem on planar graphs, where\nthe parameter is only the number of tokens. Furthermore, the algorithm can be\nmodified so that it finds a shortest sequence for a yes-instance. The same\nscheme of the algorithms can be applied to a wider class of graphs,\n$K_{3,t}$-free graphs for any fixed integer $t \\ge 3$, and it yields\nfixed-parameter algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:49:56 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 12:49:24 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Ito", "Takehiro", ""], ["Kami\u0144ski", "Marcin", ""], ["Ono", "Hirotaka", ""]]}, {"id": "1406.6576", "submitter": "Takehiro Ito", "authors": "Erik D. Demaine, Martin L. Demaine, Eli Fox-Epstein, Duc A. Hoang,\n  Takehiro Ito, Hirotaka Ono, Yota Otachi, Ryuhei Uehara, Takeshi Yamada", "title": "Linear-Time Algorithm for Sliding Tokens on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph\nsuch that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in\n$I_b$. Then, the sliding token problem is to determine whether there exists a\nsequence of independent sets which transforms $I_b$ into $I_r$ so that each\nindependent set in the sequence results from the previous one by sliding\nexactly one token along an edge in the graph. This problem is known to be\nPSPACE-complete even for planar graphs, and also for bounded treewidth graphs.\nIn this paper, we thus study the problem restricted to trees, and give the\nfollowing three results: (1) the decision problem is solvable in linear time;\n(2) for a yes-instance, we can find in quadratic time an actual sequence of\nindependent sets between $I_b$ and $I_r$ whose length (i.e., the number of\ntoken-slides) is quadratic; and (3) there exists an infinite family of\ninstances on paths for which any sequence requires quadratic length.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 14:06:40 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 23:56:36 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Fox-Epstein", "Eli", ""], ["Hoang", "Duc A.", ""], ["Ito", "Takehiro", ""], ["Ono", "Hirotaka", ""], ["Otachi", "Yota", ""], ["Uehara", "Ryuhei", ""], ["Yamada", "Takeshi", ""]]}, {"id": "1406.6773", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Approximately Optimal Mechanism Design: Motivation, Examples, and\n  Lessons Learned", "comments": "Based on a talk given by the author at the 15th ACM Conference on\n  Economics and Computation (EC), June 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal mechanism design enjoys a beautiful and well-developed theory, and\nalso a number of killer applications. Rules of thumb produced by the field\ninfluence everything from how governments sell wireless spectrum licenses to\nhow the major search engines auction off online advertising. There are,\nhowever, some basic problems for which the traditional optimal mechanism design\napproach is ill-suited --- either because it makes overly strong assumptions,\nor because it advocates overly complex designs. The thesis of this paper is\nthat approximately optimal mechanisms allow us to reason about fundamental\nquestions that seem out of reach of the traditional theory.\n  This survey has three main parts. The first part describes the approximately\noptimal mechanism design paradigm --- how it works, and what we aim to learn by\napplying it. The second and third parts of the survey cover two case studies,\nwhere we instantiate the general design paradigm to investigate two basic\nquestions. In the first example, we consider revenue maximization in a\nsingle-item auction with heterogeneous bidders. Our goal is to understand if\ncomplexity --- in the sense of detailed distributional knowledge --- is an\nessential feature of good auctions for this problem, or alternatively if there\nare simpler auctions that are near-optimal. The second example considers\nwelfare maximization with multiple items. Our goal here is similar in spirit:\nwhen is complexity --- in the form of high-dimensional bid spaces --- an\nessential feature of every auction that guarantees reasonable welfare? Are\nthere interesting cases where low-dimensional bid spaces suffice?\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 05:38:25 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "1406.6889", "submitter": "Pierre-\\'Etienne Meunier", "authors": "Pierre-\\'Etienne Meunier", "title": "Noncooperative algorithms in self-assembly", "comments": "A few bug fixes and typo corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the first non-trivial positive algorithmic results (i.e. programs\nwhose output is larger than their size), in a model of self-assembly that has\nso far resisted many attempts of formal analysis or programming: the planar\nnon-cooperative variant of Winfree's abstract Tile Assembly Model.\n  This model has been the center of several open problems and conjectures in\nthe last fifteen years, and the first fully general results on its\ncomputational power were only proven recently (SODA 2014). These results, as\nwell as ours, exemplify the intricate connections between computation and\ngeometry that can occur in self-assembly.\n  In this model, tiles can stick to an existing assembly as soon as one of\ntheir sides matches the existing assembly. This feature contrasts with the\ngeneral cooperative model, where it can be required that tiles match on\n\\emph{several} of their sides in order to bind.\n  In order to describe our algorithms, we also introduce a generalization of\nregular expressions called Baggins expressions. Finally, we compare this model\nto other automata-theoretic models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 13:52:19 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 17:22:50 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Meunier", "Pierre-\u00c9tienne", ""]]}, {"id": "1406.7107", "submitter": "Martijn van Brink", "authors": "Martijn van Brink, Ruben van der Zwaan", "title": "A branch and price procedure for the container premarshalling problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the loading phase of a vessel, only the containers that are on top of\ntheir stack are directly accessible. If the container that needs to be loaded\nnext is not the top container, extra moves have to be performed, resulting in\nan increased loading time. One way to resolve this issue is via a procedure\ncalled premarshalling. The goal of premarshalling is to reshuffle the\ncontainers into a desired lay-out prior to the arrival of the vessel, in the\nminimum number of moves possible. This paper presents an exact algorithm based\non branch and bound, that is evaluated on a large set of instances. The\ncomplexity of the premarshalling problem is also considered, and this paper\nshows that the problem at hand is NP-hard, even in the natural case of stacks\nwith fixed height.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 08:28:25 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["van Brink", "Martijn", ""], ["van der Zwaan", "Ruben", ""]]}, {"id": "1406.7279", "submitter": "Rakesh Venkat", "authors": "Amit Deshpande, Rakesh Venkat", "title": "Guruswami-Sinop Rounding without Higher Level Lasserre", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guruswami and Sinop give a $O(1/\\delta)$ approximation guarantee for the\nnon-uniform Sparsest Cut problem by solving $O(r)$-level Lasserre semidefinite\nconstraints, provided that the generalized eigenvalues of the Laplacians of the\ncost and demand graphs satisfy a certain spectral condition, namely,\n$\\lambda_{r+1} \\geq \\Phi^{*}/(1-\\delta)$. Their key idea is a rounding\ntechnique that first maps a vector-valued solution to $[0, 1]$ using\nappropriately scaled projections onto Lasserre vectors. In this paper, we show\nthat similar projections and analysis can be obtained using only $\\ell_{2}^{2}$\ntriangle inequality constraints. This results in a $O(r/\\delta^{2})$\napproximation guarantee for the non-uniform Sparsest Cut problem by adding only\n$\\ell_{2}^{2}$ triangle inequality constraints to the usual semidefinite\nprogram, provided that the same spectral condition, $\\lambda_{r+1} \\geq\n\\Phi^{*}/(1-\\delta)$, holds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 19:31:33 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Deshpande", "Amit", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1406.7363", "submitter": "Mikhail Berlinkov", "authors": "Mikhail V. Berlinkov", "title": "On the Synchronization Rate for e-machines", "comments": "A result about computing prediction rate constant has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.FL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known, that an $\\epsilon$-machine is either exactly or asymptotically\nsynchronizing. In the exact case, the observer can infer the current machine\nstate after observing $L$ generated symbols with probability $1-a^L$ where $0\n\\leq a<1$ is a so-called synchronization rate constant. In the asymptotic case,\nthe probability of the correct prediction the current machine state after\nobserving $L$ generated symbols tends to $1$ exponentially fast as $1-b^L$ for\n$0<b<1$ and the infimum of such $b$ is a so-called prediction rate constant.\n  Hence the synchronization and prediction rate constants serve as natural\nmeasures of synchronization for $\\epsilon$-machines. In the present work we\nshow how to approximate these constants in polynomial time in terms of the\nnumber of machine states.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 06:55:37 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 04:05:27 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Berlinkov", "Mikhail V.", ""]]}, {"id": "1406.7570", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis", "title": "Streaming Graph Partitioning in the Planted Partition Model", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer increase in the size of graph data has created a lot of interest\ninto developing efficient distributed graph processing frameworks. Popular\nexisting frameworks such as Graphlab and Pregel rely on balanced graph\npartitioning in order to minimize communication and achieve work balance.\n  In this work we contribute to the recent research line of streaming graph\npartitioning \\cite{stantonstreaming,stanton,fennel} which computes an\napproximately balanced $k$-partitioning of the vertex set of a graph using a\nsingle pass over the graph stream using degree-based criteria. This graph\npartitioning framework is well tailored to processing large-scale and dynamic\ngraphs. In this work we introduce the use of higher length walks for streaming\ngraph partitioning and show that their use incurs a minor computational cost\nwhich can significantly improve the quality of the graph partition. We perform\nan average case analysis of our algorithm using the planted partition model\n\\cite{condon2001algorithms,mcsherry2001spectral}. We complement the recent\nresults of Stanton \\cite{stantonstreaming} by showing that our proposed method\nrecovers the true partition with high probability even when the gap of the\nmodel tends to zero as the size of the graph grows. Furthermore, among the wide\nnumber of choices for the length of the walks we show that the proposed length\nis optimal. Finally, we conduct experiments which verify the value of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 00:08:42 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 16:13:35 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1406.7716", "submitter": "Patrick Nicholson", "authors": "Pawel Gawrychowski and Moshe Lewenstein and Patrick K. Nicholson", "title": "Weighted ancestors in suffix trees", "comments": "27 pages, LNCS format. A condensed version will appear in ESA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical, ubiquitous, predecessor problem is to construct a data\nstructure for a set of integers that supports fast predecessor queries. Its\ngeneralization to weighted trees, a.k.a. the weighted ancestor problem, has\nbeen extensively explored and successfully reduced to the predecessor problem.\nIt is known that any solution for both problems with an input set from a\npolynomially bounded universe that preprocesses a weighted tree in O(n\npolylog(n)) space requires \\Omega(loglogn) query time. Perhaps the most\nimportant and frequent application of the weighted ancestors problem is for\nsuffix trees. It has been a long-standing open question whether the weighted\nancestors problem has better bounds for suffix trees. We answer this question\npositively: we show that a suffix tree built for a text w[1..n] can be\npreprocessed using O(n) extra space, so that queries can be answered in O(1)\ntime. Thus we improve the running times of several applications. Our\nimprovement is based on a number of data structure tools and a\nperiodicity-based insight into the combinatorial structure of a suffix tree.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 13:10:00 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Lewenstein", "Moshe", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1406.7841", "submitter": "Neil Olver", "authors": "Neil Olver", "title": "A note on hierarchical hubbing for a generalization of the VPN problem", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust network design refers to a class of optimization problems that occur\nwhen designing networks to efficiently handle variable demands. The notion of\n\"hierarchical hubbing\" was introduced (in the narrow context of a specific\nrobust network design question), by Olver and Shepherd [2010]. Hierarchical\nhubbing allows for routings with a multiplicity of \"hubs\" which are connected\nto the terminals and to each other in a treelike fashion. Recently, Fr\\'echette\net al. [2013] explored this notion much more generally, focusing on its\napplicability to an extension of the well-studied hose model that allows for\nupper bounds on individual point-to-point demands. In this paper, we consider\nhierarchical hubbing in the context of a previously studied (and extremely\nnatural) generalization of the hose model, and prove that the optimal\nhierarchical hubbing solution can be found efficiently. This result is relevant\nto a recently proposed generalization of the \"VPN Conjecture\".\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 18:32:28 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Olver", "Neil", ""]]}]