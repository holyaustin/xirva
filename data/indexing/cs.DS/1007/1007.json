[{"id": "1007.0089", "submitter": "Nayantara Bhatnagar", "authors": "Nayantara Bhatnagar, Andrej Bogdanov and Elchanan Mossel", "title": "The Computational Complexity of Estimating Convergence Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in the implementation of Markov Chain Monte Carlo\nalgorithms is to determine the convergence time, or the number of iterations\nbefore the chain is close to stationarity. For many Markov chains used in\npractice this time is not known. Even in cases where the convergence time is\nknown to be polynomial, the theoretical bounds are often too crude to be\npractical. Thus, practitioners like to carry out some form of statistical\nanalysis in order to assess convergence. This has led to the development of a\nnumber of methods known as convergence diagnostics which attempt to diagnose\nwhether the Markov chain is far from stationarity. We study the problem of\ntesting convergence in the following settings and prove that the problem is\nhard in a computational sense: Given a Markov chain that mixes rapidly, it is\nhard for Statistical Zero Knowledge (SZK-hard) to distinguish whether starting\nfrom a given state, the chain is close to stationarity by time t or far from\nstationarity at time ct for a constant c. We show the problem is in AM\nintersect coAM. Second, given a Markov chain that mixes rapidly it is coNP-hard\nto distinguish whether it is close to stationarity by time t or far from\nstationarity at time ct for a constant c. The problem is in coAM. Finally, it\nis PSPACE-complete to distinguish whether the Markov chain is close to\nstationarity by time t or far from being mixed at time ct for c at least 1.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 07:34:58 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Bhatnagar", "Nayantara", ""], ["Bogdanov", "Andrej", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1007.0217", "submitter": "Neal E. Young", "authors": "Neal E. Young", "title": "A Bound on the Sum of Weighted Pairwise Distances of Points Constrained\n  to Balls", "comments": "Cornell ORIE Tech Report", "journal-ref": null, "doi": null, "report-no": "1103", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of choosing Euclidean points to maximize the sum of\ntheir weighted pairwise distances, when each point is constrained to a ball\ncentered at the origin. We derive a dual minimization problem and show strong\nduality holds (i.e., the resulting upper bound is tight) when some locally\noptimal configuration of points is affinely independent. We sketch a polynomial\ntime algorithm for finding a near-optimal set of points.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 17:09:00 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Young", "Neal E.", ""]]}, {"id": "1007.0372", "submitter": "Magnus Wahlstr\\\"om", "authors": "Benjamin Doerr, Marvin K\\\"unnemann, and Magnus Wahlstr\\\"om", "title": "Randomized Rounding for Routing and Covering Problems: Experiments and\n  Improvements", "comments": "Longer version of SEA 2010 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following previous theoretical work by Srinivasan (FOCS 2001) and the first\nauthor (STACS 2006) and a first experimental evaluation on random instances\n(ALENEX 2009), we investigate how the recently developed different approaches\nto generate randomized roundings satisfying disjoint cardinality constraints\nbehave when used in two classical algorithmic problems, namely low-congestion\nrouting in networks and max-coverage problems in hypergraphs.\n  We generally find that all randomized rounding algorithms work well, much\nbetter than what is guaranteed by existing theoretical work. The derandomized\nversions produce again significantly better rounding errors, with running times\nstill negligible compared to the one for solving the corresponding LP. It thus\nseems worth preferring them over the randomized variants.\n  The data created in these experiments lets us propose and investigate the\nfollowing new ideas. For the low-congestion routing problems, we suggest to\nsolve a second LP, which yields the same congestion, but aims at producing a\nsolution that is easier to round. Experiments show that this reduces the\nrounding errors considerably, both in combination with randomized and\nderandomized rounding.\n  For the max-coverage instances, we generally observe that the greedy\nheuristics also performs very good. We develop a strengthened method of\nderandomized rounding, and a simple greedy/rounding hybrid approach using\ngreedy and LP-based rounding elements, and observe that both these improvements\nyield again better solutions than both earlier approaches on their own.\n  For unit disk max-domination, we also develop a PTAS. Contrary to all other\nalgorithms investigated, it performs not much better in experiments than in\ntheory; thus, unless extremely good solutions are to be obtained with huge\ncomputational resources, greedy, LP-based rounding or hybrid approaches are\npreferable.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2010 14:22:26 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Doerr", "Benjamin", ""], ["K\u00fcnnemann", "Marvin", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1007.0489", "submitter": "Victor Chepoi", "authors": "Victor Chepoi, Feodor Dragan, Ilan Newman, Yuri Rabinovich, Yann Vaxes", "title": "Constant approximation algorithms for embedding graph metrics into trees\n  and outerplanar graphs", "comments": "27 pages, 4 figires, extended abstract to appear in the proceedings\n  of APPROX-RANDOM 2010", "journal-ref": "Discrete & Computational Geometry 47 (2012), 187-214", "doi": null, "report-no": null, "categories": "math.MG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple factor 6 algorithm for approximating the\noptimal multiplicative distortion of embedding a graph metric into a tree\nmetric (thus improving and simplifying the factor 100 and 27 algorithms of\nB\\v{a}doiu, Indyk, and Sidiropoulos (2007) and B\\v{a}doiu, Demaine, Hajiaghayi,\nSidiropoulos, and Zadimoghaddam (2008)). We also present a constant factor\nalgorithm for approximating the optimal distortion of embedding a graph metric\ninto an outerplanar metric. For this, we introduce a general notion of metric\nrelaxed minor and show that if G contains an alpha-metric relaxed H-minor, then\nthe distortion of any embedding of G into any metric induced by a H-minor free\ngraph is at meast alpha. Then, for H=K_{2,3}, we present an algorithm which\neither finds an alpha-relaxed minor, or produces an O(alpha)-embedding into an\nouterplanar metric.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2010 10:20:24 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chepoi", "Victor", ""], ["Dragan", "Feodor", ""], ["Newman", "Ilan", ""], ["Rabinovich", "Yuri", ""], ["Vaxes", "Yann", ""]]}, {"id": "1007.0501", "submitter": "Glenn Langford", "authors": "Glenn Langford", "title": "An Improved Neighbourhood for the Traveling Tournament Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Traveling Tournament Problem (TTP) is a challenging combinatorial\noptimization problem that has attracted the interest of researchers around the\nworld. This paper proposes an improved search neighbourhood for the TTP that\nhas been tested in a simulated annealing context. The neighbourhood encompasses\nboth feasible and infeasible schedules, and can be generated efficiently. For\nthe largest TTP challenge problems with up to 40 teams, solutions found using\nthis neighbourhood are the best currently known, and for smaller problems with\n10 teams, three solutions found were subsequently proven optimal.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2010 16:17:35 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Langford", "Glenn", ""]]}, {"id": "1007.0515", "submitter": "Pranav Dandekar", "authors": "Pranav Dandekar, Ashish Goel, Ramesh Govindan, Ian Post", "title": "Liquidity in Credit Networks: A Little Trust Goes a Long Way", "comments": "Version that appeared in ACM EC '11", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit networks represent a way of modeling trust between entities in a\nnetwork. Nodes in the network print their own currency and trust each other for\na certain amount of each other's currency. This allows the network to serve as\na decentralized payment infrastructure---arbitrary payments can be routed\nthrough the network by passing IOUs between trusting nodes in their respective\ncurrencies---and obviates the need for a common currency. Nodes can repeatedly\ntransact with each other and pay for the transaction using trusted currency. A\nnatural question to ask in this setting is: how long can the network sustain\nliquidity, i.e., how long can the network support the routing of payments\nbefore credit dries up? We answer this question in terms of the long term\nfailure probability of transactions for various network topologies and credit\nvalues.\n  We prove that the transaction failure probability is independent of the path\nalong which transactions are routed. We show that under symmetric transaction\nrates, the transaction failure probability in a number of well-known graph\nfamilies goes to zero as the size, density or credit capacity of the network\nincreases. We also show via simulations that even networks of small size and\ncredit capacity can route transactions with high probability if they are\nwell-connected. Further, we characterize a centralized currency system as a\nspecial type of a star network (one where edges to the root have infinite\ncredit capacity, and transactions occur only between leaf nodes) and compute\nthe steady-state transaction failure probability in a centralized system. We\nshow that liquidity in star networks, complete graphs and Erd\\\"{o}s-R\\'{e}nyi\nnetworks is comparable to that in equivalent centralized currency systems; thus\nwe do not lose much liquidity in return for their robustness and decentralized\nproperties.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2010 22:39:45 GMT"}, {"version": "v2", "created": "Sun, 13 Feb 2011 01:21:21 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2012 20:34:06 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Dandekar", "Pranav", ""], ["Goel", "Ashish", ""], ["Govindan", "Ramesh", ""], ["Post", "Ian", ""]]}, {"id": "1007.1161", "submitter": "Thore Husfeldt", "authors": "Andreas Bj\\\"orklund, Thore Husfeldt, Petteri Kaski, Mikko Koivisto", "title": "Narrow sieves for parameterized paths and packings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present randomized algorithms for some well-studied, hard combinatorial\nproblems: the k-path problem, the p-packing of q-sets problem, and the\nq-dimensional p-matching problem. Our algorithms solve these problems with high\nprobability in time exponential only in the parameter (k, p, q) and using\npolynomial space; the constant bases of the exponentials are significantly\nsmaller than in previous works. For example, for the k-path problem the\nimprovement is from 2 to 1.66. We also show how to detect if a d-regular graph\nadmits an edge coloring with $d$ colors in time within a polynomial factor of\nO(2^{(d-1)n/2}).\n  Our techniques build upon and generalize some recently published ideas by I.\nKoutis (ICALP 2009), R. Williams (IPL 2009), and A. Bj\\\"orklund (STACS 2010,\nFOCS 2010).\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 15:08:09 GMT"}], "update_date": "2010-07-08", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Husfeldt", "Thore", ""], ["Kaski", "Petteri", ""], ["Koivisto", "Mikko", ""]]}, {"id": "1007.1166", "submitter": "Dominik Scheder", "authors": "Konstantin Kutzkov and Dominik Scheder", "title": "Using CSP To Improve Deterministic 3-SAT", "comments": "corrected typos, extended the introduction and added a notation\n  section to make paper self-contained", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how one can use certain deterministic algorithms for higher-value\nconstraint satisfaction problems (CSPs) to speed up deterministic local search\nfor 3-SAT. This way, we improve the deterministic worst-case running time for\n3-SAT to O(1.439^n).\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 15:25:27 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2010 16:49:30 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Kutzkov", "Konstantin", ""], ["Scheder", "Dominik", ""]]}, {"id": "1007.1253", "submitter": "Eric Price", "authors": "Eric Price", "title": "Efficient Sketches for the Set Query Problem", "comments": "16 pages, 2 figures. Appearing in SODA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We develop an algorithm for estimating the values of a vector x in R^n over a\nsupport S of size k from a randomized sparse binary linear sketch Ax of size\nO(k). Given Ax and S, we can recover x' with ||x' - x_S||_2 <= eps ||x -\nx_S||_2 with probability at least 1 - k^{-\\Omega(1)}. The recovery takes O(k)\ntime.\n  While interesting in its own right, this primitive also has a number of\napplications. For example, we can:\n  1. Improve the linear k-sparse recovery of heavy hitters in Zipfian\ndistributions with O(k log n) space from a (1+eps) approximation to a (1 +\no(1)) approximation, giving the first such approximation in O(k log n) space\nwhen k <= O(n^{1-eps}).\n  2. Recover block-sparse vectors with O(k) space and a (1+eps) approximation.\nPrevious algorithms required either omega(k) space or omega(1) approximation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 22:03:06 GMT"}, {"version": "v2", "created": "Fri, 19 Nov 2010 07:43:35 GMT"}], "update_date": "2010-11-22", "authors_parsed": [["Price", "Eric", ""]]}, {"id": "1007.1259", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich, Michael Mitzenmacher", "title": "Privacy-Preserving Access of Outsourced Data via Oblivious RAM\n  Simulation", "comments": "A more complete version of a paper appearing in the 38th\n  International Colloquium on Automata, Languages and Programming (ICALP) 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a client, Alice, has outsourced her data to an external storage\nprovider, Bob, because he has capacity for her massive data set, of size n,\nwhereas her private storage is much smaller--say, of size O(n^{1/r}), for some\nconstant r > 1. Alice trusts Bob to maintain her data, but she would like to\nkeep its contents private. She can encrypt her data, of course, but she also\nwishes to keep her access patterns hidden from Bob as well. We describe schemes\nfor the oblivious RAM simulation problem with a small logarithmic or\npolylogarithmic amortized increase in access times, with a very high\nprobability of success, while keeping the external storage to be of size O(n).\nTo achieve this, our algorithmic contributions include a parallel MapReduce\ncuckoo-hashing algorithm and an external-memory dataoblivious sorting\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 23:21:31 GMT"}, {"version": "v2", "created": "Mon, 2 May 2011 23:41:14 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1007.1260", "submitter": "Bin Fu", "authors": "Richard Beigel and Bin Fu", "title": "A Dense Hierarchy of Sublinear Time Approximation Schemes for Bin\n  Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bin packing problem is to find the minimum number of bins of size one to\npack a list of items with sizes $a_1,..., a_n$ in $(0,1]$. Using uniform\nsampling, which selects a random element from the input list each time, we\ndevelop a randomized $O({n(\\log n)(\\log\\log n)\\over \\sum_{i=1}^n a_i}+({1\\over\n\\epsilon})^{O({1\\over\\epsilon})})$ time $(1+\\epsilon)$-approximation scheme for\nthe bin packing problem. We show that every randomized algorithm with uniform\nrandom sampling needs $\\Omega({n\\over \\sum_{i=1}^n a_i})$ time to give an\n$(1+\\epsilon)$-approximation. For each function $s(n): N\\rightarrow N$, define\n$\\sum(s(n))$ to be the set of all bin packing problems with the sum of item\nsizes equal to $s(n)$. For a constant $b\\in (0,1)$, every problem in\n$\\sum(n^{b})$ has an $O(n^{1-b}(\\log n)(\\log\\log n)+({1\\over\n\\epsilon})^{O({1\\over\\epsilon})})$ time $(1+\\epsilon)$-approximation for an\narbitrary constant $\\epsilon$. On the other hand, there is no $o(n^{1-b})$ time\n$(1+\\epsilon)$-approximation scheme for the bin packing problems in\n$\\sum(n^{b})$ for some constant $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 23:22:53 GMT"}, {"version": "v2", "created": "Thu, 18 Nov 2010 18:26:16 GMT"}, {"version": "v3", "created": "Thu, 24 Feb 2011 18:20:17 GMT"}], "update_date": "2011-02-25", "authors_parsed": [["Beigel", "Richard", ""], ["Fu", "Bin", ""]]}, {"id": "1007.1271", "submitter": "Chinmay Karande", "authors": "Gagan Aggarwal, Gagan Goel, Chinmay Karande and Aranyak Mehta", "title": "Online Vertex-Weighted Bipartite Matching and Single-bid Budgeted\n  Allocations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following vertex-weighted online bipartite matching problem:\n$G(U, V, E)$ is a bipartite graph. The vertices in $U$ have weights and are\nknown ahead of time, while the vertices in $V$ arrive online in an arbitrary\norder and have to be matched upon arrival. The goal is to maximize the sum of\nweights of the matched vertices in $U$. When all the weights are equal, this\nreduces to the classic \\emph{online bipartite matching} problem for which Karp,\nVazirani and Vazirani gave an optimal $\\left(1-\\frac{1}{e}\\right)$-competitive\nalgorithm in their seminal work~\\cite{KVV90}. Our main result is an optimal\n$\\left(1-\\frac{1}{e}\\right)$-competitive randomized algorithm for general\nvertex weights. We use \\emph{random perturbations} of weights by appropriately\nchosen multiplicative factors. Our solution constitutes the first known\ngeneralization of the algorithm in~\\cite{KVV90} in this model and provides new\ninsights into the role of randomization in online allocation problems. It also\neffectively solves the problem of \\emph{online budgeted allocations}\n\\cite{MSVV05} in the case when an agent makes the same bid for any desired\nitem, even if the bid is comparable to his budget - complementing the results\nof \\cite{MSVV05, BJN07} which apply when the bids are much smaller than the\nbudgets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 01:04:12 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Aggarwal", "Gagan", ""], ["Goel", "Gagan", ""], ["Karande", "Chinmay", ""], ["Mehta", "Aranyak", ""]]}, {"id": "1007.1345", "submitter": "Chetan Rao S", "authors": "Chetan S Rao, Jeffrey John Geevarghese and Karthik Rajan", "title": "Improved approximation bounds for Vector Bin Packing", "comments": "15 pages, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an improved approximation scheme for the Vector Bin\nPacking problem (VBP), based on the combination of (near-)optimal solution of\nthe Linear Programming (LP) relaxation and a greedy (modified first-fit)\nheuristic. The Vector Bin Packing problem of higher dimension (d \\geq 2) is not\nknown to have asymptotic polynomial-time approximation schemes (unless P = NP).\n  Our algorithm improves over the previously-known guarantee of (ln d + 1 +\nepsilon) by Bansal et al. [1] for higher dimensions (d > 2). We provide a\n{\\theta}(1) approximation scheme for certain set of inputs for any dimension d.\nMore precisely, we provide a 2-OPT algorithm, a result which is irrespective of\nthe number of dimensions d.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 10:56:15 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Rao", "Chetan S", ""], ["Geevarghese", "Jeffrey John", ""], ["Rajan", "Karthik", ""]]}, {"id": "1007.1361", "submitter": "Yakov Nekrich", "authors": "Marek Karpinski and Yakov Nekrich", "title": "Top-K Color Queries for Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a new efficient (in fact optimal) data structure\nfor the {\\em top-$K$ color problem}. Each element of an array $A$ is assigned a\ncolor $c$ with priority $p(c)$. For a query range $[a,b]$ and a value $K$, we\nhave to report $K$ colors with the highest priorities among all colors that\noccur in $A[a..b]$, sorted in reverse order by their priorities. We show that\nsuch queries can be answered in $O(K)$ time using an $O(N\\log \\sigma)$ bits\ndata structure, where $N$ is the number of elements in the array and $\\sigma$\nis the number of colors. Thus our data structure is asymptotically optimal with\nrespect to the worst-case query time and space. As an immediate application of\nour results, we obtain optimal time solutions for several document retrieval\nproblems. The method of the paper could be also of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 12:51:03 GMT"}, {"version": "v2", "created": "Mon, 18 Oct 2010 10:58:22 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Karpinski", "Marek", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1007.1484", "submitter": "David Eppstein", "authors": "Erin Chambers and David Eppstein", "title": "Flows in One-Crossing-Minor-Free Graphs", "comments": "16 pages, 4 figures", "journal-ref": "J. Graph Algorithms & Applications 17(3): 201-220, 2013", "doi": "10.7155/jgaa.00291", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum flow problem in directed H-minor-free graphs where H can\nbe drawn in the plane with one crossing. If a structural decomposition of the\ngraph as a clique-sum of planar graphs and graphs of constant complexity is\ngiven, we show that a maximum flow can be computed in O(n log n) time. In\nparticular, maximum flows in directed K_{3,3}-minor-free graphs and directed\nK_5-minor-free graphs can be computed in O(n log n) time without additional\nassumptions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 23:24:59 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Chambers", "Erin", ""], ["Eppstein", "David", ""]]}, {"id": "1007.1528", "submitter": "Yingyu Zhang", "authors": "Ying-Yu Zhang and Song-Feng Lu", "title": "Quantum search by partial adiabatic evolution", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevA.82.034304", "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum search algorithm based on the partial adiabatic\nevolution\\cite{Tulsi2009} is provided. We calculate its time complexity by\nstudying the Hamiltonian in a two-dimensional Hilbert space. It is found that\nthe algorithm improves the time complexity, which is $O(\\sqrt{N/M})$, of the\nlocal adiabatic search algorithm\\cite{Roland2002}, to $O(\\sqrt{N}/M)$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 08:09:59 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Zhang", "Ying-Yu", ""], ["Lu", "Song-Feng", ""]]}, {"id": "1007.1535", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski", "title": "An Optimal Lower Bound for Buffer Management in Multi-Queue Switches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online packet buffering problem (also known as the unweighted FIFO\nvariant of buffer management), we focus on a single network packet switching\ndevice with several input ports and one output port. This device forwards\nunit-size, unit-value packets from input ports to the output port. Buffers\nattached to input ports may accumulate incoming packets for later transmission;\nif they cannot accommodate all incoming packets, their excess is lost. A packet\nbuffering algorithm has to choose from which buffers to transmit packets in\norder to minimize the number of lost packets and thus maximize the throughput.\n  We present a tight lower bound of e/(e-1) ~ 1.582 on the competitive ratio of\nthe throughput maximization, which holds even for fractional or randomized\nalgorithms. This improves the previously best known lower bound of 1.4659 and\nmatches the performance of the algorithm Random Schedule. Our result\ncontradicts the claimed performance of the algorithm Random Permutation; we\npoint out a flaw in its original analysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 09:05:38 GMT"}, {"version": "v2", "created": "Wed, 13 Oct 2010 20:49:13 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2012 09:36:08 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Bienkowski", "Marcin", ""]]}, {"id": "1007.1593", "submitter": "Yakov Nekrich", "authors": "Yakov Nekrich", "title": "A Fast Algorithm for Three-Dimensional Layers of Maxima Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the three-dimensional layers-of-maxima problem can be solved in\n$o(n\\log n)$ time in the word RAM model. Our algorithm runs in $O(n(\\log \\log\nn)^3)$ deterministic time or $O(n(\\log\\log n)^2)$ expected time and uses O(n)\nspace. We also describe an algorithm that uses optimal O(n) space and solves\nthe three-dimensional layers-of-maxima problem in $O(n\\log n)$ time in the\npointer machine model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 13:45:05 GMT"}, {"version": "v2", "created": "Tue, 3 May 2011 13:08:33 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Nekrich", "Yakov", ""]]}, {"id": "1007.1604", "submitter": "Alberto Pettarin", "authors": "Alberto Pettarin, Andrea Pietracaprina, Geppino Pucci, Eli Upfal", "title": "Infectious Random Walks", "comments": "21 pages, 3 figures --- The results presented in this paper have been\n  extended in: Pettarin et al., Tight Bounds on Information Dissemination in\n  Sparse Mobile Networks, http://arxiv.org/abs/1101.4609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamics of information (or virus) dissemination by $m$ mobile\nagents performing independent random walks on an $n$-node grid. We formulate\nour results in terms of two scenarios: broadcasting and gossiping. In the\nbroadcasting scenario, the mobile agents are initially placed uniformly at\nrandom among the grid nodes. At time 0, one agent is informed of a rumor and\nstarts a random walk. When an informed agent meets an uninformed agent, the\nlatter becomes informed and starts a new random walk. We study the broadcasting\ntime of the system, that is, the time it takes for all agents to know the\nrumor. In the gossiping scenario, each agent is given a distinct rumor at time\n0 and all agents start random walks. When two agents meet, they share all\nrumors they are aware of. We study the gossiping time of the system, that is,\nthe time it takes for all agents to know all rumors. We prove that both the\nbroadcasting and the gossiping times are $\\tilde\\Theta(n/\\sqrt{m})$ w.h.p.,\nthus achieving a tight characterization up to logarithmic factors. Previous\nresults for the grid provided bounds which were weaker and only concerned\naverage times. In the context of virus infection, a corollary of our results is\nthat static and dynamically moving agents are infected at about the same speed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 15:05:30 GMT"}, {"version": "v2", "created": "Tue, 25 Jan 2011 13:50:23 GMT"}], "update_date": "2011-01-26", "authors_parsed": [["Pettarin", "Alberto", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""]]}, {"id": "1007.1611", "submitter": "Thomas Kesselheim", "authors": "Thomas Kesselheim", "title": "A Constant-Factor Approximation for Wireless Capacity Maximization with\n  Power Control in the SINR Model", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern wireless networks, devices are able to set the power for each\ntransmission carried out. Experimental but also theoretical results indicate\nthat such power control can improve the network capacity significantly. We\nstudy this problem in the physical interference model using SINR constraints.\n  In the SINR capacity maximization problem, we are given n pairs of senders\nand receivers, located in a metric space (usually a so-called fading metric).\nThe algorithm shall select a subset of these pairs and choose a power level for\neach of them with the objective of maximizing the number of simultaneous\ncommunications. This is, the selected pairs have to satisfy the SINR\nconstraints with respect to the chosen powers.\n  We present the first algorithm achieving a constant-factor approximation in\nfading metrics. The best previous results depend on further network parameters\nsuch as the ratio of the maximum and the minimum distance between a sender and\nits receiver. Expressed only in terms of n, they are (trivial) Omega(n)\napproximations.\n  Our algorithm still achieves an O(log n) approximation if we only assume to\nhave a general metric space rather than a fading metric. Furthermore, by using\nstandard techniques the algorithm can also be used in single-hop and multi-hop\nscheduling scenarios. Here, we also get polylog(n) approximations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 15:37:28 GMT"}, {"version": "v2", "created": "Thu, 26 Aug 2010 14:16:14 GMT"}], "update_date": "2010-08-27", "authors_parsed": [["Kesselheim", "Thomas", ""]]}, {"id": "1007.1632", "submitter": "Shayan Oveis Gharan", "authors": "Shayan Oveis Gharan and Jan Vondr\\'ak", "title": "Submodular Maximization by Simulated Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a nonnegative (possibly non-monotone)\nsubmodular set function with or without constraints. Feige et al. [FOCS'07]\nshowed a 2/5-approximation for the unconstrained problem and also proved that\nno approximation better than 1/2 is possible in the value oracle model.\nConstant-factor approximation was also given for submodular maximization\nsubject to a matroid independence constraint (a factor of 0.309 Vondrak\n[FOCS'09]) and for submodular maximization subject to a matroid base\nconstraint, provided that the fractional base packing number is at least 2 (a\n1/4-approximation, Vondrak [FOCS'09]).\n  In this paper, we propose a new algorithm for submodular maximization which\nis based on the idea of {\\em simulated annealing}. We prove that this algorithm\nachieves improved approximation for two problems: a 0.41-approximation for\nunconstrained submodular maximization, and a 0.325-approximation for submodular\nmaximization subject to a matroid independence constraint.\n  On the hardness side, we show that in the value oracle model it is impossible\nto achieve a 0.478-approximation for submodular maximization subject to a\nmatroid independence constraint, or a 0.394-approximation subject to a matroid\nbase constraint in matroids with two disjoint bases. Even for the special case\nof cardinality constraint, we prove it is impossible to achieve a\n0.491-approximation. (Previously it was conceivable that a 1/2-approximation\nexists for these problems.) It is still an open question whether a\n1/2-approximation is possible for unconstrained submodular maximization.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 17:31:43 GMT"}], "update_date": "2010-07-12", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Vondr\u00e1k", "Jan", ""]]}, {"id": "1007.1660", "submitter": "Benjamin Barsdell", "authors": "Benjamin R. Barsdell, David G. Barnes, Christopher J. Fluke", "title": "Analysing Astronomy Algorithms for GPUs and Beyond", "comments": "10 pages, 3 figures, accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1111/j.1365-2966.2010.17257.x", "report-no": null, "categories": "astro-ph.IM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomy depends on ever increasing computing power. Processor clock-rates\nhave plateaued, and increased performance is now appearing in the form of\nadditional processor cores on a single chip. This poses significant challenges\nto the astronomy software community. Graphics Processing Units (GPUs), now\ncapable of general-purpose computation, exemplify both the difficult\nlearning-curve and the significant speedups exhibited by massively-parallel\nhardware architectures. We present a generalised approach to tackling this\nparadigm shift, based on the analysis of algorithms. We describe a small\ncollection of foundation algorithms relevant to astronomy and explain how they\nmay be used to ease the transition to massively-parallel computing\narchitectures. We demonstrate the effectiveness of our approach by applying it\nto four well-known astronomy problems: Hogbom CLEAN, inverse ray-shooting for\ngravitational lensing, pulsar dedispersion and volume rendering. Algorithms\nwith well-defined memory access patterns and high arithmetic intensity stand to\nreceive the greatest performance boost from massively-parallel architectures,\nwhile those that involve a significant amount of decision-making may struggle\nto take advantage of the available processing power.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 20:00:21 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Barsdell", "Benjamin R.", ""], ["Barnes", "David G.", ""], ["Fluke", "Christopher J.", ""]]}, {"id": "1007.1673", "submitter": "Shayan Oveis Gharan", "authors": "Vahideh H. Manshadi, Shayan Oveis Gharan and Amin Saberi", "title": "Online Stochastic Matching: Online Actions Based on Offline Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online stochastic matching problem proposed by Feldman et al.\n[FMMM09] as a model of display ad allocation. We are given a bipartite graph;\none side of the graph corresponds to a fixed set of bins and the other side\nrepresents the set of possible ball types. At each time step, a ball is sampled\nindependently from the given distribution and it needs to be matched upon its\narrival to an empty bin. The goal is to maximize the number of allocations.\n  We present an online algorithm for this problem with a competitive ratio of\n0.702. Before our result, algorithms with a competitive ratio better than\n$1-1/e$ were known under the assumption that the expected number of arriving\nballs of each type is integral. A key idea of the algorithm is to collect\nstatistics about the decisions of the optimum offline solution using Monte\nCarlo sampling and use those statistics to guide the decisions of the online\nalgorithm. We also show that our algorithm achieves a competitive ratio of\n0.705 when the rates are integral.\n  On the hardness side, we prove that no online algorithm can have a\ncompetitive ratio better than 0.823 under the known distribution model (and\nhenceforth under the permutation model). This improves upon the 5/6 hardness\nresult proved by Goel and Mehta \\cite{GM08} for the permutation model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 20:40:42 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2011 16:56:58 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Manshadi", "Vahideh H.", ""], ["Gharan", "Shayan Oveis", ""], ["Saberi", "Amin", ""]]}, {"id": "1007.1726", "submitter": "Stoicho Dimitrov Stoichev", "authors": "Stoicho D. Stoichev", "title": "Vsep-New Heuristic and Exact Algorithms for Graph Automorphism Group\n  Computation", "comments": "47 pages; 1. Entirely revised 2. Algorithms analysis removed 3. New\n  algorithm versions added, one version removed 4. Changed algorithm COMP -\n  cases CS2/CS4 are solved in a new way", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One exact and two heuristic algorithms for determining the generators, orbits\nand order of the graph automorphism group are presented. A basic tool of these\nalgorithms is the well-known individualization and refinement procedure. A\nsearch tree is used in the algorithms - each node of the tree is a partition.\nAll nonequivalent discreet partitions derivative of the selected vertices are\nstored in a coded form. A new strategy is used in the exact algorithm: if\nduring its execution some of the searched or intermediate variables obtain a\nwrong value then the algorithm continues from a new start point losing some of\nthe results determined so far. The algorithms has been tested on one of the\nknown benchmark graphs and shows lower running times for some graph families.\nThe heuristic versions of the algorithms are based on determining some number\nof discreet partitions derivative of each vertex in the selected cell of the\ninitial partition and comparing them for an automorphism - their search trees\nare reduced. The heuristic algorithms are almost exact and are many times\nfaster than the exact one. The experimental tests exhibit that the worst-cases\nrunning time of the exact algorithm is exponential but it is polynomial for the\nheuristic algorithms. Several cell selectors are used. Some of them are new. We\nalso use a chooser of cell selector for choosing the optimal cell selector for\nthe manipulated graph. The proposed heuristic algorithms use two main heuristic\nprocedures that generate two different forests of search trees.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2010 15:11:36 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2010 05:48:20 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 07:40:04 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 07:07:54 GMT"}, {"version": "v5", "created": "Tue, 26 Jul 2016 17:26:40 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Stoichev", "Stoicho D.", ""]]}, {"id": "1007.1733", "submitter": "Serge Gaspers", "authors": "Serge Gaspers, Mathieu Liedloff, Maya Stein, Karol Suchan", "title": "Complexity of Splits Reconstruction for Low-Degree Trees", "comments": null, "journal-ref": "Proc. of WG 2011, Springer LNCS 6986, pp. 167-178", "doi": "10.1007/978-3-642-25870-1_16", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex-weighted tree T, the split of an edge xy in T is min{s_x(xy),\ns_y(xy)} where s_u(uv) is the sum of all weights of vertices that are closer to\nu than to v in T. Given a set of weighted vertices V and a multiset of splits\nS, we consider the problem of constructing a tree on V whose splits correspond\nto S. The problem is known to be NP-complete, even when all vertices have unit\nweight and the maximum vertex degree of T is required to be no more than 4. We\nshow that the problem is strongly NP-complete when T is required to be a path,\nthe problem is NP-complete when all vertices have unit weight and the maximum\ndegree of T is required to be no more than 3, and it remains NP-complete when\nall vertices have unit weight and T is required to be a caterpillar with\nunbounded hair length and maximum degree at most 3. We also design polynomial\ntime algorithms for the variant where T is required to be a path and the number\nof distinct vertex weights is constant, and the variant where all vertices have\nunit weight and T has a constant number of leaves. The latter algorithm is not\nonly polynomial when the number of leaves, k, is a constant, but also\nfixed-parameter tractable when parameterized by k. Finally, we shortly discuss\nthe problem when the vertex weights are not given but can be freely chosen by\nan algorithm.\n  The considered problem is related to building libraries of chemical compounds\nused for drug design and discovery. In these inverse problems, the goal is to\ngenerate chemical compounds having desired structural properties, as there is a\nstrong correlation between structural properties, such as the Wiener index,\nwhich is closely connected to the considered problem, and biological activity.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2010 17:03:16 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2010 14:54:10 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 16:39:10 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Gaspers", "Serge", ""], ["Liedloff", "Mathieu", ""], ["Stein", "Maya", ""], ["Suchan", "Karol", ""]]}, {"id": "1007.1800", "submitter": "Piotr Faliszewski", "authors": "Piotr Faliszewski, Edith Hemaspaandra, Lane A. Hemaspaandra", "title": "Multimode Control Attacks on Elections", "comments": "41 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": "URCS TR-2010-960", "categories": "cs.GT cs.CC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1992, Bartholdi, Tovey, and Trick opened the study of control attacks on\nelections---attempts to improve the election outcome by such actions as\nadding/deleting candidates or voters. That work has led to many results on how\nalgorithms can be used to find attacks on elections and how\ncomplexity-theoretic hardness results can be used as shields against attacks.\nHowever, all the work in this line has assumed that the attacker employs just a\nsingle type of attack. In this paper, we model and study the case in which the\nattacker launches a multipronged (i.e., multimode) attack. We do so to more\nrealistically capture the richness of real-life settings. For example, an\nattacker might simultaneously try to suppress some voters, attract new voters\ninto the election, and introduce a spoiler candidate. Our model provides a\nunified framework for such varied attacks, and by constructing polynomial-time\nmultiprong attack algorithms we prove that for various election systems even\nsuch concerted, flexible attacks can be perfectly planned in deterministic\npolynomial time.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2010 20:57:47 GMT"}], "update_date": "2010-07-13", "authors_parsed": [["Faliszewski", "Piotr", ""], ["Hemaspaandra", "Edith", ""], ["Hemaspaandra", "Lane A.", ""]]}, {"id": "1007.1946", "submitter": "Yossi Kanizo", "authors": "Yossi Kanizo, David Hay, Isaac Keslassy", "title": "Maximum Bipartite Matching Size And Application to Cuckoo Hashing", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo hashing with a stash is a robust multiple choice hashing scheme with\nhigh memory utilization that can be used in many network device applications.\nUnfortunately, for memory loads beyond 0.5, little is known on its performance.\n  In this paper, we analyze its average performance over such loads. We tackle\nthis problem by recasting the problem as an analysis of the expected maximum\nmatching size of a given random bipartite graph. We provide exact results for\nany finite system, and also deduce asymptotic results as the memory size\nincreases. We further consider other variants of this problem, and finally\nevaluate the performance of our models on Internet backbone traces. More\ngenerally, our results give a tight lower bound on the size of the stash needed\nfor any multiple-choice hashing scheme.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2010 17:23:57 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2011 12:14:34 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Kanizo", "Yossi", ""], ["Hay", "David", ""], ["Keslassy", "Isaac", ""]]}, {"id": "1007.2021", "submitter": "Serge Gaspers", "authors": "Michael R. Fellows, Serge Gaspers, Frances A. Rosamond", "title": "Parameterizing by the Number of Numbers", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_13", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usefulness of parameterized algorithmics has often depended on what\nNiedermeier has called, \"the art of problem parameterization\". In this paper we\nintroduce and explore a novel but general form of parameterization: the number\nof numbers. Several classic numerical problems, such as Subset Sum, Partition,\n3-Partition, Numerical 3-Dimensional Matching, and Numerical Matching with\nTarget Sums, have multisets of integers as input. We initiate the study of\nparameterizing these problems by the number of distinct integers in the input.\nWe rely on an FPT result for ILPF to show that all the above-mentioned problems\nare fixed-parameter tractable when parameterized in this way. In various\napplied settings, problem inputs often consist in part of multisets of integers\nor multisets of weighted objects (such as edges in a graph, or jobs to be\nscheduled). Such number-of-numbers parameterized problems often reduce to\nsubproblems about transition systems of various kinds, parameterized by the\nsize of the system description. We consider several core problems of this kind\nrelevant to number-of-numbers parameterization. Our main hardness result\nconsiders the problem: given a non-deterministic Mealy machine M (a finite\nstate automaton outputting a letter on each transition), an input word x, and a\ncensus requirement c for the output word specifying how many times each letter\nof the output alphabet should be written, decide whether there exists a\ncomputation of M reading x that outputs a word y that meets the requirement c.\nWe show that this problem is hard for W[1]. If the question is whether there\nexists an input word x such that a computation of M on x outputs a word that\nmeets c, the problem becomes fixed-parameter tractable.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2010 02:20:45 GMT"}, {"version": "v2", "created": "Sun, 31 Oct 2010 00:32:21 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2011 16:05:14 GMT"}, {"version": "v4", "created": "Thu, 6 Oct 2011 00:46:31 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Fellows", "Michael R.", ""], ["Gaspers", "Serge", ""], ["Rosamond", "Frances A.", ""]]}, {"id": "1007.2140", "submitter": "Jos\\'e Soto", "authors": "Michel X. Goemans and Jos\\'e A. Soto", "title": "Symmetric Submodular Function Minimization Under Hereditary Family\n  Constraints", "comments": "13 pages, Submitted to SODA 2011", "journal-ref": "SIAM J. Discrete Math., 27(2), 1123--1145. 2013", "doi": "10.1137/120891502", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm to find non-empty minimizers of a symmetric\nsubmodular function over any family of sets closed under inclusion. This for\nexample includes families defined by a cardinality constraint, a knapsack\nconstraint, a matroid independence constraint, or any combination of such\nconstraints. Our algorithm make $O(n^3)$ oracle calls to the submodular\nfunction where $n$ is the cardinality of the ground set. In contrast, the\nproblem of minimizing a general submodular function under a cardinality\nconstraint is known to be inapproximable within $o(\\sqrt{n/\\log n})$ (Svitkina\nand Fleischer [2008]).\n  The algorithm is similar to an algorithm of Nagamochi and Ibaraki [1998] to\nfind all nontrivial inclusionwise minimal minimizers of a symmetric submodular\nfunction over a set of cardinality $n$ using $O(n^3)$ oracle calls. Their\nprocedure in turn is based on Queyranne's algorithm [1998] to minimize a\nsymmetric submodular\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2010 16:46:51 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Goemans", "Michel X.", ""], ["Soto", "Jos\u00e9 A.", ""]]}, {"id": "1007.2152", "submitter": "Jos\\'e Soto", "authors": "Jos\\'e A. Soto", "title": "Matroid Secretary Problem in the Random Assignment Model", "comments": "16 pages. Submitted to SODA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Matroid Secretary Problem, introduced by Babaioff et al. [SODA 2007],\nthe elements of a given matroid are presented to an online algorithm in random\norder. When an element is revealed, the algorithm learns its weight and decides\nwhether or not to select it under the restriction that the selected elements\nform an independent set in the matroid. The objective is to maximize the total\nweight of the chosen elements. In the most studied version of this problem, the\nalgorithm has no information about the weights beforehand. We refer to this as\nthe zero information model. In this paper we study a different model, also\nproposed by Babaioff et al., in which the relative order of the weights is\nrandom in the matroid. To be precise, in the random assignment model, an\nadversary selects a collection of weights that are randomly assigned to the\nelements of the matroid. Later, the elements are revealed to the algorithm in a\nrandom order independent of the assignment.\n  Our main result is the first constant competitive algorithm for the matroid\nsecretary problem in the random assignment model. This solves an open question\nof Babaioff et al. Our algorithm achieves a competitive ratio of $2e^2/(e-1)$.\nIt exploits the notion of principal partition of a matroid, its decomposition\ninto uniformly dense minors, and a $2e$-competitive algorithm for uniformly\ndense matroids we also develop. As additional results, we present simple\nconstant competitive algorithms in the zero information model for various\nclasses of matroids including cographic, low density and the case when every\nelement is in a small cocircuit. In the same model, we also give a\n$ke$-competitive algorithm for $k$-column sparse linear matroids, and a new\n$O(\\log r)$-competitive algorithm for general matroids of rank $r$ which only\nuses the relative order of the weights seen and not their numerical value, as\npreviously needed.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2010 17:09:33 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Soto", "Jos\u00e9 A.", ""]]}, {"id": "1007.2216", "submitter": "Virginia Vassilevska Williams", "authors": "Virginia Vassilevska Williams", "title": "Faster Replacement Paths", "comments": "the current version contains an improved result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The replacement paths problem for directed graphs is to find for given nodes\ns and t and every edge e on the shortest path between them, the shortest path\nbetween s and t which avoids e. For unweighted directed graphs on n vertices,\nthe best known algorithm runtime was \\tilde{O}(n^{2.5}) by Roditty and Zwick.\nFor graphs with integer weights in {-M,...,M}, Weimann and Yuster recently\nshowed that one can use fast matrix multiplication and solve the problem in\nO(Mn^{2.584}) time, a runtime which would be O(Mn^{2.33}) if the exponent\n\\omega of matrix multiplication is 2.\n  We improve both of these algorithms. Our new algorithm also relies on fast\nmatrix multiplication and runs in O(M n^{\\omega} polylog(n)) time if \\omega>2\nand O(n^{2+\\eps}) for any \\eps>0 if \\omega=2. Our result shows that, at least\nfor small integer weights, the replacement paths problem in directed graphs may\nbe easier than the related all pairs shortest paths problem in directed graphs,\nas the current best runtime for the latter is \\Omega(n^{2.5}) time even if\n\\omega=2.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2010 22:06:16 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Williams", "Virginia Vassilevska", ""]]}, {"id": "1007.2365", "submitter": "Georgios Zervas", "authors": "John Byers, Brent Heeringa, Michael Mitzenmacher, and Georgios Zervas", "title": "Heapable Sequences and Subsequences", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let us call a sequence of numbers heapable if they can be sequentially\ninserted to form a binary tree with the heap property, where each insertion\nsubsequent to the first occurs at a leaf of the tree, i.e. below a previously\nplaced number. In this paper we consider a variety of problems related to\nheapable sequences and subsequences that do not appear to have been studied\npreviously. Our motivation for introducing these concepts is two-fold. First,\nsuch problems correspond to natural extensions of the well-known secretary\nproblem for hiring an organization with a hierarchical structure. Second, from\na purely combinatorial perspective, our problems are interesting variations on\nsimilar longest increasing subsequence problems, a problem paradigm that has\nled to many deep mathematical connections.\n  We provide several basic results. We obtain an efficient algorithm for\ndetermining the heapability of a sequence, and also prove that the question of\nwhether a sequence can be arranged in a complete binary heap is NP-hard.\nRegarding subsequences we show that, with high probability, the longest\nheapable subsequence of a random permutation of n numbers has length (1 - o(1))\nn, and a subsequence of length (1 - o(1)) n can in fact be found online with\nhigh probability. We similarly show that for a random permutation a subsequence\nthat yields a complete heap of size \\alpha n for a constant \\alpha can be found\nwith high probability. Our work highlights the interesting structure underlying\nthis class of subsequence problems, and we leave many further interesting\nvariations open for future work.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 16:02:37 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Byers", "John", ""], ["Heeringa", "Brent", ""], ["Mitzenmacher", "Michael", ""], ["Zervas", "Georgios", ""]]}, {"id": "1007.2484", "submitter": "Olivier Bernardi", "authors": "Olivier Bernardi (MIT), Eric Fusy (LIX)", "title": "Schnyder decompositions for regular plane graphs and application to\n  drawing", "comments": null, "journal-ref": "Algorithmica 62 (2012) pp 1159-1197", "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schnyder woods are decompositions of simple triangulations into three\nedge-disjoint spanning trees crossing each other in a specific way. In this\narticle, we define a generalization of Schnyder woods to $d$-angulations (plane\ngraphs with faces of degree $d$) for all $d\\geq 3$. A \\emph{Schnyder\ndecomposition} is a set of $d$ spanning forests crossing each other in a\nspecific way, and such that each internal edge is part of exactly $d-2$ of the\nspanning forests. We show that a Schnyder decomposition exists if and only if\nthe girth of the $d$-angulation is $d$. As in the case of Schnyder woods\n($d=3$), there are alternative formulations in terms of orientations\n(\"fractional\" orientations when $d\\geq 5$) and in terms of corner-labellings.\nMoreover, the set of Schnyder decompositions on a fixed $d$-angulation of girth\n$d$ is a distributive lattice. We also show that the structures dual to\nSchnyder decompositions (on $d$-regular plane graphs of mincut $d$ rooted at a\nvertex $v^*$) are decompositions into $d$ spanning trees rooted at $v^*$ such\nthat each edge not incident to $v^*$ is used in opposite directions by two\ntrees. Additionally, for even values of $d$, we show that a subclass of\nSchnyder decompositions, which are called even, enjoy additional properties\nthat yield a reduced formulation; in the case d=4, these correspond to\nwell-studied structures on simple quadrangulations (2-orientations and\npartitions into 2 spanning trees). In the case d=4, the dual of even Schnyder\ndecompositions yields (planar) orthogonal and straight-line drawing algorithms.\nFor a 4-regular plane graph $G$ of mincut 4 with $n$ vertices plus a marked\nvertex $v$, the vertices of $G\\backslash v$ are placed on a $(n-1) \\times\n(n-1)$ grid according to a permutation pattern, and in the orthogonal drawing\neach of the $2n-2$ edges of $G\\backslash v$ has exactly one bend. Embedding\nalso the marked vertex $v$ is doable at the cost of two additional rows and\ncolumns and 8 additional bends for the 4 edges incident to $v$. We propose a\nfurther compaction step for the drawing algorithm and show that the obtained\ngrid-size is strongly concentrated around $25n/32\\times 25n/32$ for a uniformly\nrandom instance with $n$ vertices.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2010 06:04:25 GMT"}, {"version": "v2", "created": "Sun, 30 Jan 2011 17:41:56 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Bernardi", "Olivier", "", "MIT"], ["Fusy", "Eric", "", "LIX"]]}, {"id": "1007.2503", "submitter": "Iftah Gamzu", "authors": "Yossi Azar, Iftah Gamzu", "title": "Ranking with Submodular Valuations", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of ranking with submodular valuations. An instance of\nthis problem consists of a ground set $[m]$, and a collection of $n$ monotone\nsubmodular set functions $f^1, \\ldots, f^n$, where each $f^i: 2^{[m]} \\to R_+$.\nAn additional ingredient of the input is a weight vector $w \\in R_+^n$. The\nobjective is to find a linear ordering of the ground set elements that\nminimizes the weighted cover time of the functions. The cover time of a\nfunction is the minimal number of elements in the prefix of the linear ordering\nthat form a set whose corresponding function value is greater than a unit\nthreshold value.\n  Our main contribution is an $O(\\ln(1 / \\epsilon))$-approximation algorithm\nfor the problem, where $\\epsilon$ is the smallest non-zero marginal value that\nany function may gain from some element. Our algorithm orders the elements\nusing an adaptive residual updates scheme, which may be of independent\ninterest. We also prove that the problem is $\\Omega(\\ln(1 / \\epsilon))$-hard to\napproximate, unless P = NP. This implies that the outcome of our algorithm is\noptimal up to constant factors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2010 08:42:04 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Azar", "Yossi", ""], ["Gamzu", "Iftah", ""]]}, {"id": "1007.2618", "submitter": "Bin Fu", "authors": "Bin Fu and Yunhui Fu", "title": "Sublinear Time Motif Discovery from Multiple Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural probabilistic model for motif discovery has been used to\nexperimentally test the quality of motif discovery programs. In this model,\nthere are $k$ background sequences, and each character in a background sequence\nis a random character from an alphabet $\\Sigma$. A motif $G=g_1g_2...g_m$ is a\nstring of $m$ characters. Each background sequence is implanted a\nprobabilistically generated approximate copy of $G$. For a probabilistically\ngenerated approximate copy $b_1b_2...b_m$ of $G$, every character $b_i$ is\nprobabilistically generated such that the probability for $b_i\\neq g_i$ is at\nmost $\\alpha$. We develop three algorithms that under the probabilistic model\ncan find the implanted motif with high probability via a tradeoff between\ncomputational time and the probability of mutation. The methods developed in\nthis paper have been used in the software implementation. We observed some\nencouraging results that show improved performance for motif detection compared\nwith other softwares.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2010 17:09:54 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2012 04:11:02 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Fu", "Bin", ""], ["Fu", "Yunhui", ""]]}, {"id": "1007.2671", "submitter": "Bin Fu", "authors": "Artem Chebotko and Bin Fu", "title": "XML Reconstruction View Selection in XML Databases: Complexity Analysis\n  and Approximation Scheme", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-17461-2_8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query evaluation in an XML database requires reconstructing XML subtrees\nrooted at nodes found by an XML query. Since XML subtree reconstruction can be\nexpensive, one approach to improve query response time is to use reconstruction\nviews - materialized XML subtrees of an XML document, whose nodes are\nfrequently accessed by XML queries. For this approach to be efficient, the\nprincipal requirement is a framework for view selection. In this work, we are\nthe first to formalize and study the problem of XML reconstruction view\nselection. The input is a tree $T$, in which every node $i$ has a size $c_i$\nand profit $p_i$, and the size limitation $C$. The target is to find a subset\nof subtrees rooted at nodes $i_1,\\cdots, i_k$ respectively such that\n$c_{i_1}+\\cdots +c_{i_k}\\le C$, and $p_{i_1}+\\cdots +p_{i_k}$ is maximal.\nFurthermore, there is no overlap between any two subtrees selected in the\nsolution. We prove that this problem is NP-hard and present a fully\npolynomial-time approximation scheme (FPTAS) as a solution.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2010 22:34:59 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Chebotko", "Artem", ""], ["Fu", "Bin", ""]]}, {"id": "1007.3036", "submitter": "Marek Adamczyk", "authors": "Marek Adamczyk", "title": "Greedy algorithm for stochastic matching is a 2-approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in online dating and kidney exchange, the\nstochastic matching problem was introduced by Chen, Immorlica, Karlin, Mahdian\nand Rudra (2009). They have proven a 4-approximation of a simple greedy\nstrategy, but conjectured that it is in fact a 2-approximation. In this paper\nwe confirm this hypothesis.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2010 20:23:19 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 19:31:05 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Adamczyk", "Marek", ""]]}, {"id": "1007.3157", "submitter": "Gregory  Karagiorgos", "authors": "John Alexandris, Gregory Karagiorgos 'and' Ioannis Stavrakakis", "title": "Enhanced Random Walk with Choice: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random walk with choice is a well known variation to the random walk that\nfirst selects a subset of $d$ neighbours nodes and then decides to move to the\nnode which maximizes the value of a certain metric; this metric captures the\nnumber of (past) visits of the walk to the node. In this paper we propose an\nenhancement to the random walk with choice by considering a new metric that\ncaptures not only the actual visits to a given node, but also the intensity of\nthe visits to the neighbourhood of the node. We compare the random walk with\nchoice with its enhanced counterpart. Simulation results show a significant\nimprovement in cover time, maximum node load and load balancing, mainly in\nrandom geometric graphs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 14:35:50 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Alexandris", "John", ""], ["Stavrakakis", "Gregory Karagiorgos 'and' Ioannis", ""]]}, {"id": "1007.3292", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida", "title": "Lower Bounds on Query Complexity for Testing Bounded-Degree CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider lower bounds on the query complexity for testing\nCSPs in the bounded-degree model.\n  First, for any ``symmetric'' predicate $P:{0,1}^{k} \\to {0,1}$ except \\equ\nwhere $k\\geq 3$, we show that every (randomized) algorithm that distinguishes\nsatisfiable instances of CSP(P) from instances $(|P^{-1}(0)|/2^k-\\epsilon)$-far\nfrom satisfiability requires $\\Omega(n^{1/2+\\delta})$ queries where $n$ is the\nnumber of variables and $\\delta>0$ is a constant that depends on $P$ and\n$\\epsilon$. This breaks a natural lower bound $\\Omega(n^{1/2})$, which is\nobtained by the birthday paradox. We also show that every one-sided error\ntester requires $\\Omega(n)$ queries for such $P$. These results are hereditary\nin the sense that the same results hold for any predicate $Q$ such that\n$P^{-1}(1) \\subseteq Q^{-1}(1)$. For EQU, we give a one-sided error tester\nwhose query complexity is $\\tilde{O}(n^{1/2})$. Also, for 2-XOR (or,\nequivalently E2LIN2), we show an $\\Omega(n^{1/2+\\delta})$ lower bound for\ndistinguishing instances between $\\epsilon$-close to and $(1/2-\\epsilon)$-far\nfrom satisfiability.\n  Next, for the general k-CSP over the binary domain, we show that every\nalgorithm that distinguishes satisfiable instances from instances\n$(1-2k/2^k-\\epsilon)$-far from satisfiability requires $\\Omega(n)$ queries. The\nmatching NP-hardness is not known, even assuming the Unique Games Conjecture or\nthe $d$-to-$1$ Conjecture. As a corollary, for Maximum Independent Set on\ngraphs with $n$ vertices and a degree bound $d$, we show that every\napproximation algorithm within a factor $d/\\poly\\log d$ and an additive error\nof $\\epsilon n$ requires $\\Omega(n)$ queries. Previously, only super-constant\nlower bounds were known.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 21:44:55 GMT"}], "update_date": "2010-07-21", "authors_parsed": [["Yoshida", "Yuichi", ""]]}, {"id": "1007.3296", "submitter": "Nirman Kumar", "authors": "Sariel Har-Peled, Nirman Kumar", "title": "Approximate Nearest Neighbor Search for Low Dimensional Queries", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Approximate Nearest Neighbor problem for metric spaces where the\nquery points are constrained to lie on a subspace of low doubling dimension,\nwhile the data is high-dimensional. We show that this problem can be solved\nefficiently despite the high dimensionality of the data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 22:09:52 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2012 11:24:40 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2012 05:27:05 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Har-Peled", "Sariel", ""], ["Kumar", "Nirman", ""]]}, {"id": "1007.3415", "submitter": "Yakov Nekrich", "authors": "Yakov Nekrich", "title": "Searching in Dynamic Catalogs on a Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the following modification of the iterative search\nproblem. We are given a tree $T$, so that a dynamic catalog $C(v)$ is\nassociated with every tree node $v$. For any $x$ and for any node-to-root path\n$\\pi$ in $T$, we must find the predecessor of $x$ in $\\cup_{v\\in \\pi} C(v)$. We\npresent a linear space dynamic data structure that supports such queries in\n$O(t(n)+|\\pi|)$ time, where $t(n)$ is the time needed to search in one catalog\nand $|\\pi|$ denotes the number of nodes on path $\\pi$. We also consider the\nreporting variant of this problem, in which for any $x_1$, $x_2$ and for any\npath $\\pi'$ all elements of $\\cup_{v\\in \\pi'} (C(v)\\cap [x_1,x_2])$ must be\nreported; here $\\pi'$ denotes a path between an arbitrary node $v_0$ and its\nancestor $v_1$. We show that such queries can be answered in $O(t(n)+|\\pi'|+\nk)$ time, where $k$ is the number of elements in the answer. To illustrate\napplications of our technique, we describe the first dynamic data structures\nfor the stabbing-max problem, the horizontal point location problem, and the\northogonal line-segment intersection problem with optimal $O(\\log n/\\log \\log\nn)$ query time and poly-logarithmic update time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2010 13:28:50 GMT"}], "update_date": "2010-07-21", "authors_parsed": [["Nekrich", "Yakov", ""]]}, {"id": "1007.3604", "submitter": "Iftah Gamzu", "authors": "Yossi Azar and Iftah Gamzu", "title": "Efficient Submodular Function Maximization under Linear Packing\n  Constraints", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone submodular set function subject\nto linear packing constraints. An instance of this problem consists of a matrix\n$A \\in [0,1]^{m \\times n}$, a vector $b \\in [1,\\infty)^m$, and a monotone\nsubmodular set function $f: 2^{[n]} \\rightarrow \\bbR_+$. The objective is to\nfind a set $S$ that maximizes $f(S)$ subject to $A x_{S} \\leq b$, where $x_S$\nstands for the characteristic vector of the set $S$. A well-studied special\ncase of this problem is when $f$ is linear. This special case captures the\nclass of packing integer programs.\n  Our main contribution is an efficient combinatorial algorithm that achieves\nan approximation ratio of $\\Omega(1 / m^{1/W})$, where $W = \\min\\{b_i / A_{ij}\n: A_{ij} > 0\\}$ is the width of the packing constraints. This result matches\nthe best known performance guarantee for the linear case. One immediate\ncorollary of this result is that the algorithm under consideration achieves\nconstant factor approximation when the number of constraints is constant or\nwhen the width of the constraints is sufficiently large. This motivates us to\nstudy the large width setting, trying to determine its exact approximability.\nWe develop an algorithm that has an approximation ratio of $(1 - \\epsilon)(1 -\n1/e)$ when $W = \\Omega(\\ln m / \\epsilon^2)$. This result essentially matches\nthe theoretical lower bound of $1 - 1/e$. We also study the special setting in\nwhich the matrix $A$ is binary and $k$-column sparse. A $k$-column sparse\nmatrix has at most $k$ non-zero entries in each of its column. We design a fast\ncombinatorial algorithm that achieves an approximation ratio of $\\Omega(1 /\n(Wk^{1/W}))$, that is, its performance guarantee only depends on the sparsity\nand width parameters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 10:23:47 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2012 11:30:37 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Azar", "Yossi", ""], ["Gamzu", "Iftah", ""]]}, {"id": "1007.3611", "submitter": "Jaroslaw Byrka", "authors": "Jaroslaw Byrka and MohammadReza Ghodsi and Aravind Srinivasan", "title": "LP-rounding algorithms for facility-location problems", "comments": "Added funding information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study LP-rounding approximation algorithms for metric uncapacitated\nfacility-location problems. We first give a new analysis for the algorithm of\nChudak and Shmoys, which differs from the analysis of Byrka and Aardal in that\nnow we do not need any bound based on the solution to the dual LP program.\nBesides obtaining the optimal bifactor approximation as do Byrka and Aardal, we\ncan now also show that the algorithm with scaling parameter equaling 1.58 is,\nin fact, an 1.58-approximation algorithm. More importantly, we suggest an\napproach based on additional randomization and analyses such as ours, which\ncould achieve or approach the conjectured optimal 1.46...--approximation for\nthis basic problem.\n  Next, using essentially the same techniques, we obtain improved approximation\nalgorithms in the 2-stage stochastic variant of the problem, where we must open\na subset of facilities having only stochastic information about the future\ndemand from the clients. For this problem we obtain a 2.2975-approximation\nalgorithm in the standard setting, and a 2.4957-approximation in the more\nrestricted, per-scenario setting.\n  We then study robust fault-tolerant facility location, introduced by Chechik\nand Peleg: solutions here are designed to provide low connection cost in case\nof failure of up to $k$ facilities. Chechik and Peleg gave a 6.5-approximation\nalgorithm for $k=1$ and a ($7.5k + 1.5$)-approximation algorithm for general\n$k$. We improve this to an LP-rounding $(k+5+4/k)$-approximation algorithm. We\nalso observe that in case of oblivious failures the expected approximation\nratio can be reduced to $k + 1.5$, and that the integrality gap of the natural\nLP-relaxation of the problem is at least $k + 1$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 10:48:52 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2012 13:50:59 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Byrka", "Jaroslaw", ""], ["Ghodsi", "MohammadReza", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1007.3747", "submitter": "Benjmain Moseley", "authors": "Benjamin Moseley", "title": "Scheduling to Minimize Energy and Flow Time in Broadcast Scheduling", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we initiate the study of minimizing power consumption in the\nbroadcast scheduling model. In this setting there is a wireless transmitter.\nOver time requests arrive at the transmitter for pages of information. Multiple\nrequests may be for the same page. When a page is transmitted, all requests for\nthat page receive the transmission simulteneously. The speed the transmitter\nsends data at can be dynamically scaled to conserve energy. We consider the\nproblem of minimizing flow time plus energy, the most popular scheduling metric\nconsidered in the standard scheduling model when the scheduler is energy aware.\nWe will assume that the power consumed is modeled by an arbitrary convex\nfunction. For this problem there is a $\\Omega(n)$ lower bound. Due to the lower\nbound, we consider the resource augmentation model of Gupta \\etal\n\\cite{GuptaKP10}. Using resource augmentation, we give a scalable algorithm.\nOur result also gives a scalable non-clairvoyant algorithm for minimizing\nweighted flow time plus energy in the standard scheduling model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 20:06:13 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Moseley", "Benjamin", ""]]}, {"id": "1007.4011", "submitter": "Anthony Perez", "authors": "Sylvain Guillemot and Christophe Paul and Anthony Perez", "title": "On the (non-)existence of polynomial kernels for Pl-free edge\n  modification problems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_15", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G = (V,E) and an integer k, an edge modification problem for a\ngraph property P consists in deciding whether there exists a set of edges F of\nsize at most k such that the graph H = (V,E \\vartriangle F) satisfies the\nproperty P. In the P edge-completion problem, the set F of edges is constrained\nto be disjoint from E; in the P edge-deletion problem, F is a subset of E; no\nconstraint is imposed on F in the P edge-edition problem. A number of\noptimization problems can be expressed in terms of graph modification problems\nwhich have been extensively studied in the context of parameterized complexity.\nWhen parameterized by the size k of the edge set F, it has been proved that if\nP is an hereditary property characterized by a finite set of forbidden induced\nsubgraphs, then the three P edge-modification problems are FPT. It was then\nnatural to ask whether these problems also admit a polynomial size kernel.\nUsing recent lower bound techniques, Kratsch and Wahlstr\u007fom answered this\nquestion negatively. However, the problem remains open on many natural graph\nclasses characterized by forbidden induced subgraphs. Kratsch and Wahlstr\u007fom\nasked whether the result holds when the forbidden subgraphs are paths or cycles\nand pointed out that the problem is already open in the case of P4-free graphs\n(i.e. cographs). This paper provides positive and negative results in that line\nof research. We prove that parameterized cograph edge modification problems\nhave cubic vertex kernels whereas polynomial kernels are unlikely to exist for\nthe Pl-free and Cl-free edge-deletion problems for large enough l.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 21:14:40 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Guillemot", "Sylvain", ""], ["Paul", "Christophe", ""], ["Perez", "Anthony", ""]]}, {"id": "1007.4191", "submitter": "Jelani Nelson", "authors": "Daniel M. Kane, Jelani Nelson, Ely Porat, David P. Woodruff", "title": "Fast Moment Estimation in Data Streams in Optimal Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a space-optimal algorithm with update time\nO(log^2(1/eps)loglog(1/eps)) for (1+eps)-approximating the pth frequency\nmoment, 0 < p < 2, of a length-n vector updated in a data stream. This provides\na nearly exponential improvement in the update time complexity over the\nprevious space-optimal algorithm of [Kane-Nelson-Woodruff, SODA 2010], which\nhad update time Omega(1/eps^2).\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 19:31:06 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Kane", "Daniel M.", ""], ["Nelson", "Jelani", ""], ["Porat", "Ely", ""], ["Woodruff", "David P.", ""]]}, {"id": "1007.4230", "submitter": "C. Seshadhri", "authors": "Artur Czumaj and Oded Goldreich and Dana Ron and C. Seshadhri and Asaf\n  Shapira and Christian Sohler", "title": "Finding Cycles and Trees in Sublinear Time", "comments": "Keywords: Sublinear-Time Algorithms, Property Testing, Bounded-Degree\n  Graphs, One-Sided vs Two-Sided Error Probability Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sublinear-time (randomized) algorithms for finding simple cycles\nof length at least $k\\geq 3$ and tree-minors in bounded-degree graphs. The\ncomplexity of these algorithms is related to the distance of the graph from\nbeing $C_k$-minor-free (resp., free from having the corresponding tree-minor).\nIn particular, if the graph is far (i.e., $\\Omega(1)$-far) {from} being\ncycle-free, i.e. if one has to delete a constant fraction of edges to make it\ncycle-free, then the algorithm finds a cycle of polylogarithmic length in time\n$\\tildeO(\\sqrt{N})$, where $N$ denotes the number of vertices. This time\ncomplexity is optimal up to polylogarithmic factors.\n  The foregoing results are the outcome of our study of the complexity of {\\em\none-sided error} property testing algorithms in the bounded-degree graphs\nmodel. For example, we show that cycle-freeness of $N$-vertex graphs can be\ntested with one-sided error within time complexity\n$\\tildeO(\\poly(1/\\e)\\cdot\\sqrt{N})$. This matches the known $\\Omega(\\sqrt{N})$\nquery lower bound, and contrasts with the fact that any minor-free property\nadmits a {\\em two-sided error} tester of query complexity that only depends on\nthe proximity parameter $\\e$. For any constant $k\\geq3$, we extend this result\nto testing whether the input graph has a simple cycle of length at least $k$.\nOn the other hand, for any fixed tree $T$, we show that $T$-minor-freeness has\na one-sided error tester of query complexity that only depends on the proximity\nparameter $\\e$.\n  Our algorithm for finding cycles in bounded-degree graphs extends to general\ngraphs, where distances are measured with respect to the actual number of\nedges. Such an extension is not possible with respect to finding tree-minors in\n$o(\\sqrt{N})$ complexity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 23:34:05 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2010 00:56:59 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2012 17:37:12 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Czumaj", "Artur", ""], ["Goldreich", "Oded", ""], ["Ron", "Dana", ""], ["Seshadhri", "C.", ""], ["Shapira", "Asaf", ""], ["Sohler", "Christian", ""]]}, {"id": "1007.4338", "submitter": "Ho-Tsang Ng", "authors": "H. T. Ng and Franco Nori", "title": "A proposal for factorization using Kerr nonlinearities between three\n  harmonic oscillators", "comments": "21 pages, 5 figures; title changed, major revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative method to factorize an integer by using three\nharmonic oscillators. These oscillators are coupled together via specific Kerr\nnonlinear interactions. This method can be applied even if two harmonic\noscillators are prepared in mixed states. As simple examples, we show how to\nfactorize N=15 and 35 using this approach. The effect of dissipation of the\nharmonic oscillators on the performance of this method is studied. We also\nstudy the realization of nonlinear interactions between the coupled\noscillators. However, the probability of finding the factors of a number is\ninversely proportional to its input size. The probability becomes low when this\nnumber is large. We discuss the limitations of this approach.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2010 18:01:02 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 09:06:26 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Ng", "H. T.", ""], ["Nori", "Franco", ""]]}, {"id": "1007.4389", "submitter": "Stefan Schmid", "authors": "Andrea Richa, Christian Scheideler, Stefan Schmid, Jin Zhang", "title": "AntiJam: Efficient Medium Access despite Adaptive and Reactive Jamming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intentional interference constitutes a major threat for communication\nnetworks operating over a shared medium where availability is imperative.\nJamming attacks are often simple and cheap to implement. In particular, today's\njammers can perform physical carrier sensing in order to disrupt communication\nmore efficiently, specially in a network of simple wireless devices such as\nsensor nodes, which usually operate over a single frequency (or a limited\nfrequency band) and which cannot benefit from the use of spread spectrum or\nother more advanced technologies. This article proposes the medium access (MAC)\nprotocol \\textsc{AntiJam} that is provably robust against a powerful reactive\nadversary who can jam a $(1-\\epsilon)$-portion of the time steps, where\n$\\epsilon$ is an arbitrary constant. The adversary uses carrier sensing to make\ninformed decisions on when it is most harmful to disrupt communications;\nmoreover, we allow the adversary to be adaptive and to have complete knowledge\nof the entire protocol history. Our MAC protocol is able to make efficient use\nof the non-jammed time periods and achieves an asymptotically optimal,\n$\\Theta{(1)}$-competitive throughput in this harsh scenario. In addition,\n\\textsc{AntiJam} features a low convergence time and has good fairness\nproperties. Our simulation results validate our theoretical results and also\nshow that our algorithm manages to guarantee constant throughput where the\n802.11 MAC protocol basically fails to deliver any packets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2010 08:07:38 GMT"}, {"version": "v2", "created": "Fri, 30 Jul 2010 06:12:49 GMT"}, {"version": "v3", "created": "Wed, 9 Feb 2011 17:37:52 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2011 08:05:04 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Richa", "Andrea", ""], ["Scheideler", "Christian", ""], ["Schmid", "Stefan", ""], ["Zhang", "Jin", ""]]}, {"id": "1007.4400", "submitter": "Edoardo Di Napoli", "authors": "Edoardo Di Napoli and Paolo Bientinesi", "title": "Matrix Structure Exploitation in Generalized Eigenproblems Arising in\n  Density Functional Theory", "comments": "To appear in the proceedings of 8th International Conference on\n  Numerical Analysis and Applied Mathematics (ICNAAM 2010)", "journal-ref": null, "doi": "10.1063/1.3498648", "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, the authors report a new computational approach in the\ncontext of Density Functional Theory (DFT). It is shown how it is possible to\nspeed up the self-consistent cycle (iteration) characterizing one of the most\nwell-known DFT implementations: FLAPW. Generating the Hamiltonian and overlap\nmatrices and solving the associated generalized eigenproblems $Ax = \\lambda Bx$\nconstitute the two most time-consuming fractions of each iteration. Two\npromising directions, implementing the new methodology, are presented that will\nultimately improve the performance of the generalized eigensolver and save\ncomputational time.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2010 09:05:49 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Di Napoli", "Edoardo", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1007.4636", "submitter": "Frank Neumann", "authors": "Greg Durrett, Frank Neumann, Una-May O'Reilly", "title": "Computational Complexity Analysis of Simple Genetic Programming On Two\n  Problems Modeling Isolated Program Semantics", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the computational complexity of evolutionary algorithms for binary\nsearch spaces has significantly increased their theoretical understanding. With\nthis paper, we start the computational complexity analysis of genetic\nprogramming. We set up several simplified genetic programming algorithms and\nanalyze them on two separable model problems, ORDER and MAJORITY, each of which\ncaptures an important facet of typical genetic programming problems. Both\nanalyses give first rigorous insights on aspects of genetic programming design,\nhighlighting in particular the impact of accepting or rejecting neutral moves\nand the importance of a local mutation operator.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2010 08:18:52 GMT"}, {"version": "v2", "created": "Mon, 15 Nov 2010 08:52:23 GMT"}], "update_date": "2010-11-16", "authors_parsed": [["Durrett", "Greg", ""], ["Neumann", "Frank", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1007.5032", "submitter": "Thomas Kesselheim", "authors": "Martin Hoefer, Thomas Kesselheim, Berthold V\\\"ocking", "title": "Approximation Algorithms for Secondary Spectrum Auctions", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study combinatorial auctions for the secondary spectrum market. In this\nmarket, short-term licenses shall be given to wireless nodes for communication\nin their local neighborhood. In contrast to the primary market, channels can be\nassigned to multiple bidders, provided that the corresponding devices are well\nseparated such that the interference is sufficiently low. Interference\nconflicts are described in terms of a conflict graph in which the nodes\nrepresent the bidders and the edges represent conflicts such that the feasible\nallocations for a channel correspond to the independent sets in the conflict\ngraph.\n  In this paper, we suggest a novel LP formulation for combinatorial auctions\nwith conflict graph using a non-standard graph parameter, the so-called\ninductive independence number. Taking into account this parameter enables us to\nbypass the well-known lower bound of \\Omega(n^{1-\\epsilon}) on the\napproximability of independent set in general graphs with n nodes (bidders). We\nachieve significantly better approximation results by showing that interference\nconstraints for wireless networks yield conflict graphs with bounded inductive\nindependence number.\n  Our framework covers various established models of wireless communication,\ne.g., the protocol or the physical model. For the protocol model, we achieve an\nO(\\sqrt{k})-approximation, where k is the number of available channels. For the\nmore realistic physical model, we achieve an O(\\sqrt{k} \\log^2 n) approximation\nbased on edge-weighted conflict graphs. Combining our approach with the the\nLP-based framework of Lavi and Swamy, we obtain incentive compatible mechanisms\nfor general bidders with arbitrary valuations on bundles of channels specified\nin terms of demand oracles.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2010 16:59:53 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2010 10:51:16 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2011 14:16:40 GMT"}], "update_date": "2011-04-29", "authors_parsed": [["Hoefer", "Martin", ""], ["Kesselheim", "Thomas", ""], ["V\u00f6cking", "Berthold", ""]]}, {"id": "1007.5110", "submitter": "Manish Patil", "authors": "Manish Patil, Rahul Shah, Sharma V. Thankachan", "title": "Fully Dynamic Data Structure for Top-k Queries on Uncertain Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-$k$ queries allow end-users to focus on the most important (top-$k$)\nanswers amongst those which satisfy the query. In traditional databases, a user\ndefined score function assigns a score value to each tuple and a top-$k$ query\nreturns $k$ tuples with the highest score. In uncertain database, top-$k$\nanswer depends not only on the scores but also on the membership probabilities\nof tuples. Several top-$k$ definitions covering different aspects of\nscore-probability interplay have been proposed in recent\npast~\\cite{R10,R4,R2,R8}. Most of the existing work in this research field is\nfocused on developing efficient algorithms for answering top-$k$ queries on\nstatic uncertain data. Any change (insertion, deletion of a tuple or change in\nmembership probability, score of a tuple) in underlying data forces\nre-computation of query answers. Such re-computations are not practical\nconsidering the dynamic nature of data in many applications. In this paper, we\npropose a fully dynamic data structure that uses ranking function\n$PRF^e(\\alpha)$ proposed by Li et al.~\\cite{R8} under the generally adopted\nmodel of $x$-relations~\\cite{R11}. $PRF^e$ can effectively approximate various\nother top-$k$ definitions on uncertain data based on the value of parameter\n$\\alpha$. An $x$-relation consists of a number of $x$-tuples, where $x$-tuple\nis a set of mutually exclusive tuples (up to a constant number) called\nalternatives. Each $x$-tuple in a relation randomly instantiates into one tuple\nfrom its alternatives. For an uncertain relation with $N$ tuples, our structure\ncan answer top-$k$ queries in $O(k\\log N)$ time, handles an update in $O(\\log\nN)$ time and takes $O(N)$ space. Finally, we evaluate practical efficiency of\nour structure on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2010 05:20:38 GMT"}], "update_date": "2010-07-30", "authors_parsed": [["Patil", "Manish", ""], ["Shah", "Rahul", ""], ["Thankachan", "Sharma V.", ""]]}, {"id": "1007.5318", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "Intrinsic Dimensionality", "comments": "4 pages, 4 figures, latex; diagram (c) has been corrected", "journal-ref": "The SIGSPATIAL Special, vol. 2, No. 2 (2010), 8-11", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This entry for the SIGSPATIAL Special July 2010 issue on Similarity Searching\nin Metric Spaces discusses the notion of intrinsic dimensionality of data in\nthe context of similarity search.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2010 20:07:49 GMT"}, {"version": "v2", "created": "Mon, 2 Aug 2010 00:15:36 GMT"}, {"version": "v3", "created": "Mon, 23 Aug 2010 13:55:22 GMT"}], "update_date": "2010-11-08", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1007.5406", "submitter": "Markus Lohrey", "authors": "Markus Lohrey, Sebastian Maneth and Roy Mennicke", "title": "Tree structure compression with RePair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new linear time compression algorithm, called\n\"Re-pair for Trees\", which compresses ranked ordered trees using linear\nstraight-line context-free tree grammars. Such grammars generalize\nstraight-line context-free string grammars and allow basic tree operations,\nlike traversal along edges, to be executed without prior decompression. Our\nalgorithm can be considered as a generalization of the \"Re-pair\" algorithm\ndeveloped by N. Jesper Larsson and Alistair Moffat in 2000. The latter\nalgorithm is a dictionary-based compression algorithm for strings. We also\nintroduce a succinct coding which is specialized in further compressing the\ngrammars generated by our algorithm. This is accomplished without loosing the\nability do directly execute queries on this compressed representation of the\ninput tree. Finally, we compare the grammars and output files generated by a\nprototype of the Re-pair for Trees algorithm with those of similar compression\nalgorithms. The obtained results show that that our algorithm outperforms its\ncompetitors in terms of compression ratio, runtime and memory usage.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 10:14:21 GMT"}], "update_date": "2010-08-02", "authors_parsed": [["Lohrey", "Markus", ""], ["Maneth", "Sebastian", ""], ["Mennicke", "Roy", ""]]}, {"id": "1007.5450", "submitter": "Daniel Lokshtanov", "authors": "Daniel Lokshtanov and D\\'aniel Marx and Saket Saurabh", "title": "Known Algorithms on Graphs of Bounded Treewidth are Probably Optimal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a number of lower bounds on the running time of algorithms solving\nproblems on graphs of bounded treewidth. We prove the results under the Strong\nExponential Time Hypothesis of Impagliazzo and Paturi. In particular, assuming\nthat SAT cannot be solved in (2-\\epsilon)^{n}m^{O(1)} time, we show that for\nany e > 0; {\\sc Independent Set} cannot be solved in (2-e)^{tw(G)}|V(G)|^{O(1)}\ntime, {\\sc Dominating Set} cannot be solved in (3-e)^{tw(G)}|V(G)|^{O(1)} time,\n{\\sc Max Cut} cannot be solved in (2-e)^{tw(G)}|V(G)|^{O(1)} time, {\\sc Odd\nCycle Transversal} cannot be solved in (3-e)^{tw(G)}|V(G)|^{O(1)} time, For any\n$q \\geq 3$, $q$-{\\sc Coloring} cannot be solved in (q-e)^{tw(G)}|V(G)|^{O(1)}\ntime, {\\sc Partition Into Triangles} cannot be solved in\n(2-e)^{tw(G)}|V(G)|^{O(1)} time. Our lower bounds match the running times for\nthe best known algorithms for the problems, up to the e in the base.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 13:49:18 GMT"}], "update_date": "2010-08-02", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Marx", "D\u00e1niel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1007.5475", "submitter": "Christian Reitwie{\\ss}ner", "authors": "Christian Gla{\\ss}er, Christian Reitwie{\\ss}ner, Maximilian Witek", "title": "Balanced Combinations of Solutions in Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every list of integers x_1, ..., x_m there is some j such that x_1 + ...\n+ x_j - x_{j+1} - ... - x_m \\approx 0. So the list can be nearly balanced and\nfor this we only need one alternation between addition and subtraction. But\nwhat if the x_i are k-dimensional integer vectors? Using results from\ntopological degree theory we show that balancing is still possible, now with k\nalternations.\n  This result is useful in multi-objective optimization, as it allows a\npolynomial-time computable balance of two alternatives with conflicting costs.\nThe application to two multi-objective optimization problems yields the\nfollowing results:\n  - A randomized 1/2-approximation for multi-objective maximum asymmetric\ntraveling salesman, which improves and simplifies the best known approximation\nfor this problem.\n  - A deterministic 1/2-approximation for multi-objective maximum weighted\nsatisfiability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 15:25:17 GMT"}], "update_date": "2010-08-02", "authors_parsed": [["Gla\u00dfer", "Christian", ""], ["Reitwie\u00dfner", "Christian", ""], ["Witek", "Maximilian", ""]]}]