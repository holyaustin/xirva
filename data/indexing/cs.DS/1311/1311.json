[{"id": "1311.0224", "submitter": "Sang-il Oum", "authors": "Sang-il Oum and Sigve Hortemo S{\\ae}ther and Martin Vatshelle", "title": "Faster Algorithms For Vertex Partitioning Problems Parameterized by\n  Clique-width", "comments": "13 pages, 5 figures", "journal-ref": "Theoret. Comput. Sci. 535(May 2014), pp. 16-24", "doi": "10.1016/j.tcs.2014.03.024", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many NP-hard problems, such as Dominating Set, are FPT parameterized by\nclique-width. For graphs of clique-width $k$ given with a $k$-expression,\nDominating Set can be solved in $4^k n^{O(1)}$ time. However, no FPT algorithm\nis known for computing an optimal $k$-expression. For a graph of clique-width\n$k$, if we rely on known algorithms to compute a $(2^{3k}-1)$-expression via\nrank-width and then solving Dominating Set using the $(2^{3k}-1)$-expression,\nthe above algorithm will only give a runtime of $4^{2^{3k}} n^{O(1)}$. There\nhave been results which overcome this exponential jump; the best known\nalgorithm can solve Dominating Set in time $2^{O(k^2)} n^{O(1)}$ by avoiding\nconstructing a $k$-expression [Bui-Xuan, Telle, and Vatshelle. Fast dynamic\nprogramming for locally checkable vertex subset and vertex partitioning\nproblems. Theoret. Comput. Sci., 2013. doi:10.1016/j.tcs.2013.01.009]. We\nimprove this to $2^{O(k\\log k)}n^{O(1)}$. Indeed, we show that for a graph of\nclique-width $k$, a large class of domination and partitioning problems\n(LC-VSP), including Dominating Set, can be solved in $2^{O(k\\log{k})}\nn^{O(1)}$. Our main tool is a variant of rank-width using the rank of a $0$-$1$\nmatrix over the rational field instead of the binary field.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 17:03:09 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 23:09:18 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Oum", "Sang-il", ""], ["S\u00e6ther", "Sigve Hortemo", ""], ["Vatshelle", "Martin", ""]]}, {"id": "1311.0358", "submitter": "Hsueh-I Lu", "authors": "Hsien-Chih Chang and Hsueh-I Lu", "title": "A Faster Algorithm to Recognize Even-Hole-Free Graphs", "comments": "18 pages, 7 figures, to appear in Journal of Combinatorial Theory,\n  Series B. A preliminary version of this paper appeared in SODA 2012", "journal-ref": null, "doi": "10.1016/j.jctb.2015.02.001", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of determining whether an $n$-node graph $G$ has an even\nhole, i.e., an induced simple cycle consisting of an even number of nodes.\nConforti, Cornu\\'ejols, Kapoor, and Vu\\v{s}kovi\\'c gave the first\npolynomial-time algorithm for the problem, which runs in $O(n^{40})$ time.\nLater, Chudnovsky, Kawarabayashi, and Seymour reduced the running time to\n$O(n^{31})$. The best previously known algorithm for the problem, due to da\nSilva and Vu\\v{s}kovi\\'c, runs in $O(n^{19})$ time. In this paper, we solve the\nproblem in $O(n^{11})$ time. Moreover, if $G$ has even holes, our algorithm\nalso outputs an even hole of $G$ in $O(n^{11})$ time.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 09:03:10 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 09:11:44 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Chang", "Hsien-Chih", ""], ["Lu", "Hsueh-I", ""]]}, {"id": "1311.0362", "submitter": "Sameh Samra Mr", "authors": "Sameh Samra, Ahmed El-Mahdy, Yasutaka Wada", "title": "A Linear-Time and Space Algorithm for Optimal Traffic Signal Durations\n  at an Intersection", "comments": "New Dynamic programming Traffic Control Algorithm 5 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TITS.2014.2336657", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an optimal solution of signal traffic control durations is a\ncomputationally intensive task. It is typically O(T3) in time, and O(T2) in\nspace, where T is the length of the control interval in discrete time steps. In\nthis paper, we propose a linear time and space algorithm for the same problem.\nThe algorithm provides for an efficient dynamic programming formulation of the\nstate space, the prunes non-optimal states, early on. The paper proves the\ncorrectness of the algorithm and provides an initial experimental validation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 09:55:07 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Samra", "Sameh", ""], ["El-Mahdy", "Ahmed", ""], ["Wada", "Yasutaka", ""]]}, {"id": "1311.0366", "submitter": "Ishay Haviv", "authors": "Ishay Haviv and Oded Regev", "title": "On the Lattice Isomorphism Problem", "comments": "23 pages, SODA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Lattice Isomorphism Problem (LIP), in which given two lattices\nL_1 and L_2 the goal is to decide whether there exists an orthogonal linear\ntransformation mapping L_1 to L_2. Our main result is an algorithm for this\nproblem running in time n^{O(n)} times a polynomial in the input size, where n\nis the rank of the input lattices. A crucial component is a new generalized\nisolation lemma, which can isolate n linearly independent vectors in a given\nsubset of Z^n and might be useful elsewhere. We also prove that LIP lies in the\ncomplexity class SZK.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 10:38:44 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Haviv", "Ishay", ""], ["Regev", "Oded", ""]]}, {"id": "1311.0484", "submitter": "Meirav Zehavi", "authors": "Meirav Zehavi", "title": "Deterministic Parameterized Algorithms for Matching and Packing Problems", "comments": "Consideration for replacement: A better bound on the running time of\n  WSP-Alg (see Section 5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three deterministic parameterized algorithms for well-studied\npacking and matching problems, namely, Weighted q-Dimensional p-Matching\n((q,p)-WDM) and Weighted q-Set p-Packing ((q,p)-WSP). More specifically, we\npresent an O*(2.85043^{(q-1)p}) time deterministic algorithm for (q,p)-WDM, an\nO*(8.04143^p) time deterministic algorithm for the unweighted version of\n(3,p)-WDM, and an O*((0.56201\\cdot 2.85043^q)^p) time deterministic algorithm\nfor (q,p)-WSP. Our algorithms significantly improve the previously best known\nO* running times in solving (q,p)-WDM and (q,p)-WSP, and the previously best\nknown deterministic O* running times in solving the unweighted versions of\nthese problems. Moreover, we present kernels of size O(e^qq(p-1)^q) for\n(q,p)-WDM and (q,p)-WSP, improving the previously best known kernels of size\nO(q!q(p-1)^q) for these problems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 15:56:34 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2013 14:42:32 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Zehavi", "Meirav", ""]]}, {"id": "1311.0603", "submitter": "Pawel Rzazewski", "authors": "Konstanty Junosza-Szaniawski, Pawe{\\l} Rz\\k{a}\\.zewski", "title": "An Exact Algorithm for the Generalized List $T$-Coloring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized list $T$-coloring is a common generalization of many graph\ncoloring models, including classical coloring, $L(p,q)$-labeling, channel\nassignment and $T$-coloring. Every vertex from the input graph has a list of\npermitted labels. Moreover, every edge has a set of forbidden differences. We\nask for such a labeling of vertices of the input graph with natural numbers, in\nwhich every vertex gets a label from its list of permitted labels and the\ndifference of labels of the endpoints of each edge does not belong to the set\nof forbidden differences of this edge. In this paper we present an exact\nalgorithm solving this problem, running in time $\\mathcal{O}^*((\\tau+2)^n)$,\nwhere $\\tau$ is the maximum forbidden difference over all edges of the input\ngraph and $n$ is the number of its vertices. Moreover, we show how to improve\nthis bound if the input graph has some special structure, e.g. a bounded\nmaximum degree, no big induced stars or a perfect matching.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 08:37:36 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 22:21:14 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Junosza-Szaniawski", "Konstanty", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1311.0713", "submitter": "Guy Kortsarz", "authors": "Rajiv Gandhi and G. Kortsarz", "title": "Edge covering with budget constrains", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two related problems: finding a set of k vertices and minimum number\nof edges (kmin) and finding a graph with at least m' edges and minimum number\nof vertices (mvms).\n  Goldschmidt and Hochbaum \\cite{GH97} show that the mvms problem is NP-hard\nand they give a 3-approximation algorithm for the problem. We improve\n\\cite{GH97} by giving a ratio of 2. A 2(1+\\epsilon)-approximation for the\nproblem follows from the work of Carnes and Shmoys \\cite{CS08}. We improve the\napproximation ratio to 2. algorithm for the problem. We show that the natural\nLP for \\kmin has an integrality gap of 2-o(1). We improve the NP-completeness\nof \\cite{GH97} by proving the pronlem are APX-hard unless a well-known instance\nof the dense k-subgraph admits a constant ratio. The best approximation\nguarantee known for this instance of dense k-subgraph is O(n^{2/9})\n\\cite{BCCFV}. We show that for any constant \\rho>1, an approximation guarantee\nof \\rho for the \\kmin problem implies a \\rho(1+o(1)) approximation for \\mwms.\nFinally, we define we give an exact algorithm for the density version of kmin.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 14:36:54 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Gandhi", "Rajiv", ""], ["Kortsarz", "G.", ""]]}, {"id": "1311.0750", "submitter": "Jens M. Schmidt", "authors": "Jens M. Schmidt", "title": "Mondshein Sequences (a.k.a. (2,1)-Orders)", "comments": "to appear in SIAM Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical orderings [STOC'88, FOCS'92] have been used as a key tool in graph\ndrawing, graph encoding and visibility representations for the last decades. We\nstudy a far-reaching generalization of canonical orderings to non-planar graphs\nthat was published by Lee Mondshein in a PhD-thesis at M.I.T. as early as 1971.\n  Mondshein proposed to order the vertices of a graph in a sequence such that,\nfor any i, the vertices from 1 to i induce essentially a 2-connected graph\nwhile the remaining vertices from i+1 to n induce a connected graph.\nMondshein's sequence generalizes canonical orderings and became later and\nindependently known under the name non-separating ear decomposition.\nSurprisingly, this fundamental link between canonical orderings and\nnon-separating ear decomposition has not been established before. Currently,\nthe fastest known algorithm for computing a Mondshein sequence achieves a\nrunning time of O(nm); the main open problem in Mondshein's and follow-up work\nis to improve this running time to subquadratic time.\n  After putting Mondshein's work into context, we present an algorithm that\ncomputes a Mondshein sequence in optimal time and space O(m). This improves the\nprevious best running time by a factor of n. We illustrate the impact of this\nresult by deducing linear-time algorithms for five other problems, for four out\nof which the previous best running times have been quadratic. In particular, we\nshow how to - compute three independent spanning trees of a 3-connected graph\nin time O(m), - improve the preprocessing time from O(n^2) to O(m) for a data\nstructure reporting 3 internally disjoint paths between any given vertex pair,\n- derive a very simple O(n)-time planarity test once a Mondshein sequence has\nbeen computed, - compute a nested family of contractible subgraphs of\n3-connected graphs in time O(m), - compute a 3-partition in time O(m).\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 16:16:33 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 10:36:06 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 12:49:58 GMT"}, {"version": "v4", "created": "Fri, 19 Aug 2016 17:19:23 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Schmidt", "Jens M.", ""]]}, {"id": "1311.0776", "submitter": "Sewoong Oh", "authors": "Peter Kairouz, Sewoong Oh and Pramod Viswanath", "title": "The Composition Theorem for Differential Privacy", "comments": "32 pages 4 figures, Added a new section on private multi-party\n  computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential querying of differentially private mechanisms degrades the overall\nprivacy level. In this paper, we answer the fundamental question of\ncharacterizing the level of overall privacy degradation as a function of the\nnumber of queries and the privacy levels maintained by each privatization\nmechanism. Our solution is complete: we prove an upper bound on the overall\nprivacy level and construct a sequence of privatization mechanisms that\nachieves this bound. The key innovation is the introduction of an operational\ninterpretation of differential privacy (involving hypothesis testing) and the\nuse of new data processing inequalities. Our result improves over the\nstate-of-the-art, and has immediate applications in several problems studied in\nthe literature including differentially private multi-party computation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 17:22:00 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 04:35:43 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2015 16:10:03 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2015 22:06:01 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Kairouz", "Peter", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1311.0798", "submitter": "Stephane Rovedakis", "authors": "L\\'elia Blin (LIP6), Shlomi Dolev, Maria Gradinariu Potop-Butucaru\n  (LIP6), Stephane Rovedakis (CEDRIC)", "title": "Fast Self-Stabilizing Minimum Spanning Tree Construction Using Compact\n  Nearest Common Ancestor Labeling Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel self-stabilizing algorithm for minimum spanning tree (MST)\nconstruction. The space complexity of our solution is $O(\\log^2n)$ bits and it\nconverges in $O(n^2)$ rounds. Thus, this algorithm improves the convergence\ntime of previously known self-stabilizing asynchronous MST algorithms by a\nmultiplicative factor $\\Theta(n)$, to the price of increasing the best known\nspace complexity by a factor $O(\\log n)$. The main ingredient used in our\nalgorithm is the design, for the first time in self-stabilizing settings, of a\nlabeling scheme for computing the nearest common ancestor with only\n$O(\\log^2n)$ bits.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 18:17:47 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Blin", "L\u00e9lia", "", "LIP6"], ["Dolev", "Shlomi", "", "LIP6"], ["Potop-Butucaru", "Maria Gradinariu", "", "LIP6"], ["Rovedakis", "Stephane", "", "CEDRIC"]]}, {"id": "1311.1249", "submitter": "Simon Gog", "authors": "Simon Gog and Timo Beller and Alistair Moffat and Matthias Petri", "title": "From Theory to Practice: Plug and Play with Succinct Data Structures", "comments": "10 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering efficient implementations of compact and succinct structures is a\ntime-consuming and challenging task, since there is no standard library of\neasy-to- use, highly optimized, and composable components. One consequence is\nthat measuring the practical impact of new theoretical proposals is a difficult\ntask, since older base- line implementations may not rely on the same basic\ncomponents, and reimplementing from scratch can be very time-consuming. In this\npaper we present a framework for experimentation with succinct data structures,\nproviding a large set of configurable components, together with tests,\nbenchmarks, and tools to analyze resource requirements. We demonstrate the\nfunctionality of the framework by recomposing succinct solutions for document\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 23:37:14 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Gog", "Simon", ""], ["Beller", "Timo", ""], ["Moffat", "Alistair", ""], ["Petri", "Matthias", ""]]}, {"id": "1311.1298", "submitter": "Alexandru Popa Dr.", "authors": "Anna Adamaszek and Alexandru Popa", "title": "Algorithmic and Hardness Results for the Colorful Components Problems", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the colorful components framework, motivated by\napplications emerging from comparative genomics. The general goal is to remove\na collection of edges from an undirected vertex-colored graph $G$ such that in\nthe resulting graph $G'$ all the connected components are colorful (i.e., any\ntwo vertices of the same color belong to different connected components). We\nwant $G'$ to optimize an objective function, the selection of this function\nbeing specific to each problem in the framework.\n  We analyze three objective functions, and thus, three different problems,\nwhich are believed to be relevant for the biological applications: minimizing\nthe number of singleton vertices, maximizing the number of edges in the\ntransitive closure, and minimizing the number of connected components.\n  Our main result is a polynomial time algorithm for the first problem. This\nresult disproves the conjecture of Zheng et al. that the problem is $ NP$-hard\n(assuming $P \\neq NP$). Then, we show that the second problem is $ APX$-hard,\nthus proving and strengthening the conjecture of Zheng et al. that the problem\nis $ NP$-hard. Finally, we show that the third problem does not admit\npolynomial time approximation within a factor of $|V|^{1/14 - \\epsilon}$ for\nany $\\epsilon > 0$, assuming $P \\neq NP$ (or within a factor of $|V|^{1/2 -\n\\epsilon}$, assuming $ZPP \\neq NP$).\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 06:57:25 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Adamaszek", "Anna", ""], ["Popa", "Alexandru", ""]]}, {"id": "1311.1338", "submitter": "Mario K\\\"oppen", "authors": "Mario Koeppen and Kei Ohnishi", "title": "Significance Relations for the Benchmarking of Meta-Heuristic Algorithms", "comments": "6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental analysis of meta-heuristic algorithm performance is usually\nbased on comparing average performance metric values over a set of algorithm\ninstances. When algorithms getting tight in performance gains, the additional\nconsideration of significance of a metric improvement comes into play. However,\nfrom this moment the comparison changes from an absolute to a relative mode.\nHere the implications of this paradigm shift are investigated. Significance\nrelations are formally established. Based on this, a trade-off between\nincreasing cycle-freeness of the relation and small maximum sets can be\nidentified, allowing for the selection of a proper significance level and\nresulting ranking of a set of algorithms. The procedure is exemplified on the\nCEC'05 benchmark of real parameter single objective optimization problems. The\nsignificance relation here is based on awarding ranking points for relative\nperformance gains, similar to the Borda count voting method or the Wilcoxon\nsigned rank test. In the particular CEC'05 case, five ranks for algorithm\nperformance can be clearly identified.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 10:20:56 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Koeppen", "Mario", ""], ["Ohnishi", "Kei", ""]]}, {"id": "1311.1626", "submitter": "Hasan Jamil", "authors": "Belma Yelbay, S. Ilker Birbil, Kerem Bulbul, and Hasan M. Jamil", "title": "Trade-offs Computing Minimum Hub Cover toward Optimized Graph Query\n  Processing", "comments": "12 pages, 6 figures and 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  As techniques for graph query processing mature, the need for optimization is\nincreasingly becoming an imperative. Indices are one of the key ingredients\ntoward efficient query processing strategies via cost-based optimization. Due\nto the apparent absence of a common representation model, it is difficult to\nmake a focused effort toward developing access structures, metrics to evaluate\nquery costs, and choose alternatives. In this context, recent interests in\ncovering-based graph matching appears to be a promising direction of research.\nIn this paper, our goal is to formally introduce a new graph representation\nmodel, called Minimum Hub Cover, and demonstrate that this representation\noffers interesting strategic advantages, facilitates construction of candidate\ngraphs from graph fragments, and helps leverage indices in novel ways for query\noptimization. However, similar to other covering problems, minimum hub cover is\nNP-hard, and thus is a natural candidate for optimization. We claim that\ncomputing the minimum hub cover leads to substantial cost reduction for graph\nquery processing. We present a computational characterization of minimum hub\ncover based on integer programming to substantiate our claim and investigate\nits computational cost on various graph types.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 10:17:24 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 01:48:52 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Yelbay", "Belma", ""], ["Birbil", "S. Ilker", ""], ["Bulbul", "Kerem", ""], ["Jamil", "Hasan M.", ""]]}, {"id": "1311.1714", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "KaHIP v3.00 -- Karlsruhe High Quality Partitioning -- User Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper severs as a user guide to the graph partitioning framework KaHIP\n(Karlsruhe High Quality Partitioning). We give a rough overview of the\ntechniques used within the framework and describe the user interface as well as\nthe file formats used. Moreover, we provide a short description of the current\nlibrary functions provided within the framework. Since version 3.00 we support\nmultilevel partitioning, memetic algorithms, distributed and shared-memory\nparallel algorithms, node separator and ordering algorithms, edge partitioning\nalgorithms as well as ILP solvers.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 15:38:09 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 12:26:23 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 10:46:48 GMT"}, {"version": "v4", "created": "Mon, 19 May 2014 09:46:47 GMT"}, {"version": "v5", "created": "Fri, 21 Nov 2014 12:52:51 GMT"}, {"version": "v6", "created": "Wed, 1 Mar 2017 12:16:21 GMT"}, {"version": "v7", "created": "Fri, 4 Jan 2019 11:18:22 GMT"}, {"version": "v8", "created": "Mon, 3 Aug 2020 10:27:46 GMT"}, {"version": "v9", "created": "Wed, 5 Aug 2020 08:01:08 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1311.1762", "submitter": "Tsvi Kopelowitz", "authors": "Richard Cole, Tsvi Kopelowitz, Moshe Lewenstein", "title": "Suffix Trays and Suffix Trists: Structures for Faster Text Indexing", "comments": "Results from this paper have appeared as an extended abstract in\n  ICALP 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix trees and suffix arrays are two of the most widely used data\nstructures for text indexing. Each uses linear space and can be constructed in\nlinear time for polynomially sized alphabets. However, when it comes to\nanswering queries with worst-case deterministic time bounds, the prior does so\nin $O(m\\log|\\Sigma|)$ time, where $m$ is the query size, $|\\Sigma|$ is the\nalphabet size, and the latter does so in $O(m+\\log n)$ time, where $n$ is the\ntext size. If one wants to output all appearances of the query, an additive\ncost of $O(occ)$ time is sufficient, where $occ$ is the size of the output.\n  We propose a novel way of combining the two into, what we call, a {\\em suffix\ntray}. The space and construction time remain linear and the query time\nimproves to $O(m+\\log|\\Sigma|)$ for integer alphabets from a linear range, i.e.\n$\\Sigma \\subset \\{1,\\cdots, cn\\}$, for an arbitrary constant $c$. The\nconstruction and query are deterministic. Here also an additive $O(occ)$ time\nis sufficient if one desires to output all appearances of the query.\n  We also consider the online version of indexing, where the text arrives\nonline, one character at a time, and indexing queries are answered in tandem.\nIn this variant we create a cross between a suffix tree and a suffix list (a\ndynamic variant of suffix array) to be called a {\\em suffix trist}; it supports\nqueries in $O(m+\\log|\\Sigma|)$ time. The suffix trist also uses linear space.\nFurthermore, if there exists an online construction for a linear-space suffix\ntree such that the cost of adding a character is worst-case deterministic\n$f(n,|\\Sigma|)$ ($n$ is the size of the current text), then one can further\nupdate the suffix trist in $O(f(n,|\\Sigma|)+\\log |\\Sigma|)$ time. The best\ncurrently known worst-case deterministic bound for $f(n,|\\Sigma|)$ is $O(\\log\nn)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:44:02 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Cole", "Richard", ""], ["Kopelowitz", "Tsvi", ""], ["Lewenstein", "Moshe", ""]]}, {"id": "1311.1851", "submitter": "Guoming Wang", "authors": "Guoming Wang", "title": "Efficient Quantum Algorithms for Analyzing Large Sparse Electrical\n  Networks", "comments": "40 pages, 2 figures. Final version", "journal-ref": "Quantum Information & Computation 17 (11 & 12): 987-1026, 2017", "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large sparse electrical networks is a fundamental task in physics,\nelectrical engineering and computer science. We propose two classes of quantum\nalgorithms for this task. The first class is based on solving linear systems,\nand the second class is based on using quantum walks. These algorithms compute\nvarious electrical quantities, including voltages, currents, dissipated powers\nand effective resistances, in time $\\operatorname{poly}(d, c,\n\\operatorname{log}(N), 1/\\lambda, 1/\\epsilon)$, where $N$ is the number of\nvertices in the network, $d$ is the maximum unweighted degree of the vertices,\n$c$ is the ratio of largest to smallest edge resistance, $\\lambda$ is the\nspectral gap of the normalized Laplacian of the network, and $\\epsilon$ is the\naccuracy. Furthermore, we show that the polynomial dependence on $1/\\lambda$ is\nnecessary. This implies that our algorithms are optimal up to polynomial\nfactors and cannot be significantly improved.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 00:11:28 GMT"}, {"version": "v10", "created": "Mon, 24 Jul 2017 20:13:41 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 19:13:54 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2013 10:32:59 GMT"}, {"version": "v4", "created": "Mon, 10 Mar 2014 15:11:18 GMT"}, {"version": "v5", "created": "Tue, 1 Apr 2014 21:49:06 GMT"}, {"version": "v6", "created": "Mon, 23 Jan 2017 23:21:15 GMT"}, {"version": "v7", "created": "Wed, 1 Feb 2017 21:30:08 GMT"}, {"version": "v8", "created": "Fri, 3 Feb 2017 03:54:08 GMT"}, {"version": "v9", "created": "Thu, 13 Jul 2017 22:22:00 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wang", "Guoming", ""]]}, {"id": "1311.1859", "submitter": "Nathan Lindzey", "authors": "Benson Joeris, Nathan Lindzey, Ross M. McConnell, Nissa Osheim", "title": "Simple DFS on the Complement of a Graph and on Partially Complemented\n  Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complementation operation on a vertex of a digraph changes all outgoing\narcs into non-arcs, and outgoing non-arcs into arcs. A partially complemented\ndigraph $\\widetilde{G}$ is a digraph obtained from a sequence of vertex\ncomplement operations on $G$. Dahlhaus et al. showed that, given an\nadjacency-list representation of $\\widetilde{G}$, depth-first search (DFS) on\n$G$ can be performed in $O(n + \\widetilde{m})$ time, where $n$ is the number of\nvertices and $\\widetilde{m}$ is the number of edges in $\\widetilde{G}$. To\nachieve this bound, their algorithm makes use of a somewhat complicated\nstack-like data structure to simulate the recursion stack, instead of\nimplementing it directly as a recursive algorithm. We give a recursive\n$O(n+\\widetilde{m})$ algorithm that uses no complicated data-structures.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 01:08:10 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Joeris", "Benson", ""], ["Lindzey", "Nathan", ""], ["McConnell", "Ross M.", ""], ["Osheim", "Nissa", ""]]}, {"id": "1311.2037", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher and Rasmus Pagh", "title": "Simple Multi-Party Set Reconciliation", "comments": "22 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As users migrate information to cloud storage, many distributed cloud-based\nservices use multiple loosely consistent replicas of user information to avoid\nthe high overhead of more tightly coupled synchronization. Periodically, the\ninformation must be synchronized, or reconciled. One can place this problem in\nthe theoretical framework of {\\em set reconciliation}: two parties $A_1$ and\n$A_2$ each hold a set of keys, named $S_1$ and $S_2$ respectively, and the goal\nis for both parties to obtain $S_1 \\cup S_2$. Typically, set reconciliation is\ninteresting algorithmically when sets are large but the set difference\n$|S_1-S_2|+|S_2-S_1|$ is small. In this setting the focus is on accomplishing\nreconciliation efficiently in terms of communication; ideally, the\ncommunication should depend on the size of the set difference, and not on the\nsize of the sets.\n  In this paper, we extend recent approaches using Invertible Bloom Lookup\nTables (IBLTs) for set reconciliation to the multi-party setting. In this\nsetting there are three or more parties $A_1,A_2,\\ldots,A_n$ holding sets of\nkeys $S_1,S_2,\\ldots,S_n$ respectively, and the goal is for all parties to\nobtain $\\cup_i S_i$. This could of course be done by pairwise reconciliations,\nbut we seek more effective methods.\n  Our methodology uses network coding techniques in conjunction with IBLTs,\nallowing efficiency in network utilization along with efficiency obtained by\npassing messages of size $O(|\\cup_i S_i - \\cap_i S_i|)$. Further, our approach\ncan function even if the number of parties is not exactly known in advance, and\nin many cases can be used to determine which parties contain keys not in the\njoint union. By connecting reconciliation with network coding, we can allow for\nsubstantially more efficient reconciliation methods that apply to a number of\nnatural distributed computing problems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 19:06:27 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 12:37:26 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1311.2106", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack\n  Constraints", "comments": "23 pages. A short version of this appeared in Advances of NIPS-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two new optimization problems -- minimizing a submodular\nfunction subject to a submodular lower bound constraint (submodular cover) and\nmaximizing a submodular function subject to a submodular upper bound constraint\n(submodular knapsack). We are motivated by a number of real-world applications\nin machine learning including sensor placement and data subset selection, which\nrequire maximizing a certain submodular function (like coverage or diversity)\nwhile simultaneously minimizing another (like cooperative cost). These problems\nare often posed as minimizing the difference between submodular functions [14,\n35] which is in the worst case inapproximable. We show, however, that by\nphrasing these problems as constrained optimization, which is more natural for\nmany applications, we achieve a number of bounded approximation guarantees. We\nalso show that both these problems are closely related and an approximation\nalgorithm solving one can be used to obtain an approximation guarantee for the\nother. We provide hardness results for both problems thus showing that our\napproximation factors are tight up to log-factors. Finally, we empirically\ndemonstrate the performance and good scalability properties of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 23:28:02 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1311.2110", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Stefanie Jegelka and Jeff Bilmes", "title": "Curvature and Optimal Algorithms for Learning and Minimizing Submodular\n  Functions", "comments": "21 pages. A shorter version appeared in Advances of NIPS-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate three related and important problems connected to machine\nlearning: approximating a submodular function everywhere, learning a submodular\nfunction (in a PAC-like setting [53]), and constrained minimization of\nsubmodular functions. We show that the complexity of all three problems depends\non the 'curvature' of the submodular function, and provide lower and upper\nbounds that refine and improve previous results [3, 16, 18, 52]. Our proof\ntechniques are fairly generic. We either use a black-box transformation of the\nfunction (for approximation and learning), or a transformation of algorithms to\nuse an appropriate surrogate function (for minimization). Curiously, curvature\nhas been known to influence approximations for submodular maximization [7, 55],\nbut its effect on minimization, approximation and learning has hitherto been\nopen. We complete this picture, and also support our theoretical claims by\nempirical results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 23:42:34 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Iyer", "Rishabh", ""], ["Jegelka", "Stefanie", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1311.2147", "submitter": "Meghana Nasre Ms.", "authors": "Meghana Nasre, Matteo Pontecorvi, Vijaya Ramachandran", "title": "Betweenness Centrality -- Incremental and Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the incremental computation of the betweenness centrality of all\nvertices in a large complex network modeled as a graph G = (V, E), directed or\nundirected, with positive real edge-weights. The current widely used algorithm\nto compute the betweenness centrality of all vertices in G is the Brandes\nalgorithm that runs in O(mn + n^2 log n) time, where n = |V| and m = |E|.\n  We present an incremental algorithm that updates the betweenness centrality\nscore of all vertices in G when a new edge is added to G, or the weight of an\nexisting edge is reduced. Our incremental algorithm runs in O(m' n + n^2) time,\nwhere m' is the size of a certain subset of E*, the set of edges in G that lie\non a shortest path. We achieve the same bound for the more general incremental\nupdate of a vertex v, where the edge update can be performed on any subset of\nedges incident to v.\n  Our incremental algorithm is the first algorithm that is asymptotically\nfaster on sparse graphs than recomputing with the Brandes algorithm. Our\nalgorithm is also likely to be much faster than the Brandes algorithm on dense\ngraphs since m*, the size of E*, is often close to linear in n.\n  Our incremental algorithm is very simple and the only data structures it uses\nare arrays, lists, and stack. We give an efficient cache-oblivious\nimplementation that incurs O(scan(n^2) + n sort(m')) cache misses, where scan\nand sort are well-known measures for efficient caching. We also give a static\nalgorithm for computing betweenness centrality of all vertices that runs in\ntime O(m* n + n^2 log n), which is faster than the Brandes algorithm on any\ngraph with n log n = o(m) and m* = o(m).\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 07:59:22 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 10:28:47 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2013 06:13:08 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Nasre", "Meghana", ""], ["Pontecorvi", "Matteo", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1311.2309", "submitter": "Manish Purohit", "authors": "Samir Khuller, Manish Purohit, Kanthi Sarpatwar", "title": "Analyzing the Optimal Neighborhood: Algorithms for Budgeted and Partial\n  Connected Dominating Set Problems", "comments": "15 pages, Conference version to appear in ACM-SIAM SODA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study partial and budgeted versions of the well studied connected\ndominating set problem. In the partial connected dominating set problem, we are\ngiven an undirected graph G = (V,E) and an integer n', and the goal is to find\na minimum subset of vertices that induces a connected subgraph of G and\ndominates at least n' vertices. We obtain the first polynomial time algorithm\nwith an O(\\ln \\Delta) approximation factor for this problem, thereby\nsignificantly extending the results of Guha and Khuller (Algorithmica, Vol.\n20(4), Pages 374-387, 1998) for the connected dominating set problem. We note\nthat none of the methods developed earlier can be applied directly to solve\nthis problem. In the budgeted connected dominating set problem, there is a\nbudget on the number of vertices we can select, and the goal is to dominate as\nmany vertices as possible. We obtain a (1/13)(1 - 1/e) approximation algorithm\nfor this problem. Finally, we show that our techniques extend to a more general\nsetting where the profit function associated with a subset of vertices is a\nmonotone \"special\" submodular function. This generalization captures the\nconnected dominating set problem with capacities and/or weighted profits as\nspecial cases. This implies a O(\\ln q) approximation (where q denotes the\nquota) and an O(1) approximation algorithms for the partial and budgeted\nversions of these problems. While the algorithms are simple, the results make a\nsurprising use of the greedy set cover framework in defining a useful profit\nfunction.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 21:51:40 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Khuller", "Samir", ""], ["Purohit", "Manish", ""], ["Sarpatwar", "Kanthi", ""]]}, {"id": "1311.2456", "submitter": "Alexander Golovnev", "authors": "Alexander Golovnev, Alexander S. Kulikov, Ivan Mihajlin", "title": "Families with infants: a general approach to solve hard partition\n  problems", "comments": "18 pages, a revised version of this paper is available at\n  http://arxiv.org/abs/1410.2209", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general approach for solving partition problems where the goal\nis to represent a given set as a union (either disjoint or not) of subsets\nsatisfying certain properties. Many NP-hard problems can be naturally stated as\nsuch partition problems. We show that if one can find a large enough system of\nso-called families with infants for a given problem, then this problem can be\nsolved faster than by a straightforward algorithm. We use this approach to\nimprove known bounds for several NP-hard problems as well as to simplify the\nproofs of several known results.\n  For the chromatic number problem we present an algorithm with\n$O^*((2-\\varepsilon(d))^n)$ time and exponential space for graphs of average\ndegree $d$. This improves the algorithm by Bj\\\"{o}rklund et al. [Theory Comput.\nSyst. 2010] that works for graphs of bounded maximum (as opposed to average)\ndegree and closes an open problem stated by Cygan and Pilipczuk [ICALP 2013].\n  For the traveling salesman problem we give an algorithm working in\n$O^*((2-\\varepsilon(d))^n)$ time and polynomial space for graphs of average\ndegree $d$. The previously known results of this kind is a polyspace algorithm\nby Bj\\\"{o}rklund et al. [ICALP 2008] for graphs of bounded maximum degree and\nan exponential space algorithm for bounded average degree by Cygan and\nPilipczuk [ICALP 2013].\n  For counting perfect matching in graphs of average degree~$d$ we present an\nalgorithm with running time $O^*((2-\\varepsilon(d))^{n/2})$ and polynomial\nspace. Recent algorithms of this kind due to Cygan, Pilipczuk [ICALP 2013] and\nIzumi, Wadayama [FOCS 2012] (for bipartite graphs only) use exponential space.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 14:56:50 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 01:38:24 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Golovnev", "Alexander", ""], ["Kulikov", "Alexander S.", ""], ["Mihajlin", "Ivan", ""]]}, {"id": "1311.2466", "submitter": "Michael Lampis", "authors": "Michael Lampis", "title": "Parameterized Approximation Schemes using Graph Widths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining the techniques of approximation algorithms and parameterized\ncomplexity has long been considered a promising research area, but relatively\nfew results are currently known. In this paper we study the parameterized\napproximability of a number of problems which are known to be hard to solve\nexactly when parameterized by treewidth or clique-width. Our main contribution\nis to present a natural randomized rounding technique that extends well-known\nideas and can be used for both of these widths. Applying this very generic\ntechnique we obtain approximation schemes for a number of problems, evading\nboth polynomial-time inapproximability and parameterized intractability bounds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 15:38:31 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 10:27:18 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Lampis", "Michael", ""]]}, {"id": "1311.2495", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Eric Price", "title": "The Noisy Power Method: A Meta Algorithm with Applications", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new robust convergence analysis of the well-known power method\nfor computing the dominant singular vectors of a matrix that we call the noisy\npower method. Our result characterizes the convergence behavior of the\nalgorithm when a significant amount noise is introduced after each\nmatrix-vector multiplication. The noisy power method can be seen as a\nmeta-algorithm that has recently found a number of important applications in a\nbroad range of machine learning problems including alternating minimization for\nmatrix completion, streaming principal component analysis (PCA), and\nprivacy-preserving spectral analysis. Our general analysis subsumes several\nexisting ad-hoc convergence bounds and resolves a number of open problems in\nmultiple applications including streaming PCA and privacy-preserving singular\nvector computation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 16:47:25 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 19:17:32 GMT"}, {"version": "v3", "created": "Mon, 8 Dec 2014 21:53:05 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 23:43:37 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Hardt", "Moritz", ""], ["Price", "Eric", ""]]}, {"id": "1311.2513", "submitter": "Timon Hertli", "authors": "Timon Hertli", "title": "Breaking the PPSZ Barrier for Unique 3-SAT", "comments": "13 pages; major revision with simplified algorithm but slightly worse\n  constants", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PPSZ algorithm by Paturi, Pudl\\'ak, Saks, and Zane (FOCS 1998) is the\nfastest known algorithm for (Promise) Unique k-SAT. We give an improved\nalgorithm with exponentially faster bounds for Unique 3-SAT.\n  For uniquely satisfiable 3-CNF formulas, we do the following case\ndistinction: We call a clause critical if exactly one literal is satisfied by\nthe unique satisfying assignment. If a formula has many critical clauses, we\nobserve that PPSZ by itself is already faster. If there are only few clauses\nallover, we use an algorithm by Wahlstr\\\"om (ESA 2005) that is faster than PPSZ\nin this case. Otherwise we have a formula with few critical and many\nnon-critical clauses. Non-critical clauses have at least two literals\nsatisfied; we show how to exploit this to improve PPSZ.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 17:48:45 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 13:41:04 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Hertli", "Timon", ""]]}, {"id": "1311.2542", "submitter": "Jelani Nelson", "authors": "Jean Bourgain, Sjoerd Dirksen, Jelani Nelson", "title": "Toward a unified theory of sparse dimensionality reduction in Euclidean\n  space", "comments": null, "journal-ref": "Geometric and Functional Analysis 25 (2015), no. 4, 1009-1088", "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IT math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\Phi\\in\\mathbb{R}^{m\\times n}$ be a sparse Johnson-Lindenstrauss\ntransform [KN14] with $s$ non-zeroes per column. For a subset $T$ of the unit\nsphere, $\\varepsilon\\in(0,1/2)$ given, we study settings for $m,s$ required to\nensure $$ \\mathop{\\mathbb{E}}_\\Phi \\sup_{x\\in T} \\left|\\|\\Phi x\\|_2^2 - 1\n\\right| < \\varepsilon , $$ i.e. so that $\\Phi$ preserves the norm of every\n$x\\in T$ simultaneously and multiplicatively up to $1+\\varepsilon$. We\nintroduce a new complexity parameter, which depends on the geometry of $T$, and\nshow that it suffices to choose $s$ and $m$ such that this parameter is small.\nOur result is a sparse analog of Gordon's theorem, which was concerned with a\ndense $\\Phi$ having i.i.d. Gaussian entries. We qualitatively unify several\nresults related to the Johnson-Lindenstrauss lemma, subspace embeddings, and\nFourier-based restricted isometries. Our work also implies new results in using\nthe sparse Johnson-Lindenstrauss transform in numerical linear algebra,\nclassical and model-based compressed sensing, manifold learning, and\nconstrained least squares problems such as the Lasso.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 19:30:30 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 19:17:52 GMT"}, {"version": "v3", "created": "Fri, 19 Dec 2014 14:45:27 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2015 21:08:59 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Bourgain", "Jean", ""], ["Dirksen", "Sjoerd", ""], ["Nelson", "Jelani", ""]]}, {"id": "1311.2557", "submitter": "Barna Saha", "authors": "Barna Saha", "title": "Efficiently Computing Edit Distance to Dyck Language", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $\\sigma$ over alphabet $\\Sigma$ and a grammar $G$ defined over\nthe same alphabet, how many minimum number of repairs: insertions, deletions\nand substitutions are required to map $\\sigma$ into a valid member of $G$ ? We\ninvestigate this basic question in this paper for $Dyck(s)$. $Dyck(s)$ is a\nfundamental context free grammar representing the language of well-balanced\nparentheses with s different types of parentheses and has played a pivotal role\nin the development of theory of context free languages. Computing edit distance\nto $Dyck(s)$ significantly generalizes string edit distance problem and has\nnumerous applications ranging from repairing semi-structured documents such as\nXML to memory checking, automated compiler optimization, natural language\nprocessing etc.\n  In this paper we give the first near-linear time algorithm for edit distance\ncomputation to $Dyck(s)$ that achieves a nontrivial approximation factor of\n$O(\\frac{1}{\\epsilon}\\log{OPT}(\\log{n})^{\\frac{1}{\\epsilon}})$ in\n$O(n^{1+\\epsilon}\\log{n})$ time. In fact, given there exists an algorithm for\ncomputing string edit distance on input of size $n$ in $\\alpha(n)$ time with\n$\\beta(n)$-approximation factor, we can devise an algorithm for edit distance\nproblem to $Dyck(s)$ running in $\\tilde{O}(n^{1+\\epsilon}+\\alpha(n))$ and\nachieving an approximation factor of $O(\\frac{1}{\\epsilon}\\beta(n)\\log{OPT})$.\n  We show that the framework for efficiently approximating edit distance to\n$Dyck(s)$ can be applied to many other languages. We illustrate this by\nconsidering various memory checking languages which comprise of valid\ntranscripts of stacks, queues, priority queues, double-ended queues etc.\nTherefore, any language that can be recognized by these data structures, can\nalso be repaired efficiently by our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 20:05:08 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 07:56:08 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Saha", "Barna", ""]]}, {"id": "1311.2563", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Daniel Lokshtanov and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk and Saket Saurabh", "title": "Minimum Bisection is fixed parameter tractable", "comments": "A full version of an extended abstract to appear in the proceedings\n  of STOC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic Minimum Bisection problem we are given as input a graph $G$\nand an integer $k$. The task is to determine whether there is a partition of\n$V(G)$ into two parts $A$ and $B$ such that $||A|-|B|| \\leq 1$ and there are at\nmost $k$ edges with one endpoint in $A$ and the other in $B$. In this paper we\ngive an algorithm for Minimum Bisection with running time $O(2^{O(k^{3})}n^3\n\\log^3 n)$. This is the first fixed parameter tractable algorithm for Minimum\nBisection. At the core of our algorithm lies a new decomposition theorem that\nstates that every graph $G$ can be decomposed by small separators into parts\nwhere each part is \"highly connected\" in the following sense: any cut of\nbounded size can separate only a limited number of vertices from each part of\nthe decomposition. Our techniques generalize to the weighted setting, where we\nseek for a bisection of minimum weight among solutions that contain at most $k$\nedges.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 20:21:32 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 09:11:40 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Cygan", "Marek", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""]]}, {"id": "1311.2578", "submitter": "Thomas Kesselheim", "authors": "Thomas Kesselheim, Klaus Radke, Andreas T\\\"onnis, Berthold V\\\"ocking", "title": "Primal Beats Dual on Online Packing LPs in the Random-Order Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study packing LPs in an online model where the columns are presented to\nthe algorithm in random order. This natural problem was investigated in various\nrecent studies motivated, e.g., by online ad allocations and yield management\nwhere rows correspond to resources and columns to requests specifying demands\nfor resources. Our main contribution is a $1-O(\\sqrt{(\\log{d})/B})$-competitive\nonline algorithm, where $d$ denotes the column sparsity, i.e., the maximum\nnumber of resources that occur in a single column, and $B$ denotes the capacity\nratio $B$, i.e., the ratio between the capacity of a resource and the maximum\ndemand for this resource. In other words, we achieve a $(1 -\n\\epsilon)$-approximation if the capacity ratio satisfies $B=\\Omega((\\log\nd)/\\epsilon^2)$, which is known to be best-possible for any (randomized) online\nalgorithms.\n  Our result improves exponentially on previous work with respect to the\ncapacity ratio. In contrast to existing results on packing LP problems, our\nalgorithm does not use dual prices to guide the allocation of resources.\nInstead, it simply solves, for each request, a scaled version of the partially\nknown primal program and randomly rounds the obtained fractional solution to\nobtain an integral allocation for this request. We show that this simple\nalgorithmic technique is not restricted to packing LPs with large capacity\nratio: We prove an upper bound on the competitive ratio of\n$\\Omega(d^{-1/(B-1)})$, for any $B \\ge 2$. In addition, we show that our\napproach can be combined with VCG payments and obtain an incentive compatible\n$(1-\\epsilon)$-competitive mechanism for packing LPs with $B=\\Omega((\\log\nm)/\\epsilon^2)$, where $m$ is the number of constraints. Finally, we apply our\ntechnique to the generalized assignment problem for which we obtain the first\nonline algorithm with competitive ratio $O(1)$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 20:58:56 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Kesselheim", "Thomas", ""], ["Radke", "Klaus", ""], ["T\u00f6nnis", "Andreas", ""], ["V\u00f6cking", "Berthold", ""]]}, {"id": "1311.2625", "submitter": "Aaron Roth", "authors": "Ryan Rogers and Aaron Roth", "title": "Asymptotically Truthful Equilibrium Selection in Large Congestion Games", "comments": "The conference version of this paper appeared in EC 2014. This\n  manuscript has been merged and subsumed by the preprint \"Robust Mediators in\n  Large Games\": http://arxiv.org/abs/1512.02698", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying games in the complete information model makes them analytically\ntractable. However, large $n$ player interactions are more realistically\nmodeled as games of incomplete information, where players may know little to\nnothing about the types of other players. Unfortunately, games in incomplete\ninformation settings lose many of the nice properties of complete information\ngames: the quality of equilibria can become worse, the equilibria lose their\nex-post properties, and coordinating on an equilibrium becomes even more\ndifficult. Because of these problems, we would like to study games of\nincomplete information, but still implement equilibria of the complete\ninformation game induced by the (unknown) realized player types.\n  This problem was recently studied by Kearns et al. and solved in large games\nby means of introducing a weak mediator: their mediator took as input reported\ntypes of players, and output suggested actions which formed a correlated\nequilibrium of the underlying game. Players had the option to play\nindependently of the mediator, or ignore its suggestions, but crucially, if\nthey decided to opt-in to the mediator, they did not have the power to lie\nabout their type. In this paper, we rectify this deficiency in the setting of\nlarge congestion games. We give, in a sense, the weakest possible mediator: it\ncannot enforce participation, verify types, or enforce its suggestions.\nMoreover, our mediator implements a Nash equilibrium of the complete\ninformation game. We show that it is an (asymptotic) ex-post equilibrium of the\nincomplete information game for all players to use the mediator honestly, and\nthat when they do so, they end up playing an approximate Nash equilibrium of\nthe induced complete information game. In particular, truthful use of the\nmediator is a Bayes-Nash equilibrium in any Bayesian game for any prior.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 21:50:44 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 16:26:13 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Rogers", "Ryan", ""], ["Roth", "Aaron", ""]]}, {"id": "1311.2828", "submitter": "Justin Hsu", "authors": "Justin Hsu, Zhiyi Huang, Aaron Roth, Tim Roughgarden, Zhiwei Steven Wu", "title": "Private Matchings and Allocations", "comments": "Journal version published in SIAM Journal on Computation; an extended\n  abstract appeared in STOC 2014", "journal-ref": "SIAM Journal on Computing 45(6) 1953--1984 (2016)", "doi": "10.1137/15100271X", "report-no": null, "categories": "cs.GT cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a private variant of the classical allocation problem: given k\ngoods and n agents with individual, private valuation functions over bundles of\ngoods, how can we partition the goods amongst the agents to maximize social\nwelfare? An important special case is when each agent desires at most one good,\nand specifies her (private) value for each good: in this case, the problem is\nexactly the maximum-weight matching problem in a bipartite graph.\n  Private matching and allocation problems have not been considered in the\ndifferential privacy literature, and for good reason: they are plainly\nimpossible to solve under differential privacy. Informally, the allocation must\nmatch agents to their preferred goods in order to maximize social welfare, but\nthis preference is exactly what agents wish to hide. Therefore, we consider the\nproblem under the relaxed constraint of joint differential privacy: for any\nagent i, no coalition of agents excluding i should be able to learn about the\nvaluation function of agent i. In this setting, the full allocation is no\nlonger published---instead, each agent is told what good to get. We first show\nthat with a small number of identical copies of each good, it is possible to\nefficiently and accurately solve the maximum weight matching problem while\nguaranteeing joint differential privacy. We then consider the more general\nallocation problem, when bidder valuations satisfy the gross substitutes\ncondition. Finally, we prove that the allocation problem cannot be solved to\nnon-trivial accuracy under joint differential privacy without requiring\nmultiple copies of each type of good.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 16:22:05 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 18:03:36 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 17:38:57 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Huang", "Zhiyi", ""], ["Roth", "Aaron", ""], ["Roughgarden", "Tim", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1311.2839", "submitter": "He Sun", "authors": "Zeyu Guo and He Sun", "title": "Gossip vs. Markov Chains, and Randomness-Efficient Rumor Spreading", "comments": "41 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1304.1359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study gossip algorithms for the rumor spreading problem which asks one\nnode to deliver a rumor to all nodes in an unknown network. We present the\nfirst protocol for any expander graph $G$ with $n$ nodes such that, the\nprotocol informs every node in $O(\\log n)$ rounds with high probability, and\nuses $\\tilde{O}(\\log n)$ random bits in total. The runtime of our protocol is\ntight, and the randomness requirement of $\\tilde{O}(\\log n)$ random bits almost\nmatches the lower bound of $\\Omega(\\log n)$ random bits for dense graphs. We\nfurther show that, for many graph families, polylogarithmic number of random\nbits in total suffice to spread the rumor in $O(\\mathrm{poly}\\log n)$ rounds.\nThese results together give us an almost complete understanding of the\nrandomness requirement of this fundamental gossip process.\n  Our analysis relies on unexpectedly tight connections among gossip processes,\nMarkov chains, and branching programs. First, we establish a connection between\nrumor spreading processes and Markov chains, which is used to approximate the\nrumor spreading time by the mixing time of Markov chains. Second, we show a\nreduction from rumor spreading processes to branching programs, and this\nreduction provides a general framework to derandomize gossip processes. In\naddition to designing rumor spreading protocols, these novel techniques may\nhave applications in studying parallel and multiple random walks, and\nrandomness complexity of distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 17:09:25 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Guo", "Zeyu", ""], ["Sun", "He", ""]]}, {"id": "1311.2879", "submitter": "Abhishek Awasthi M.Sc.", "authors": "Abhishek Awasthi, J\\\"org L\\\"assig and Oliver Kramer", "title": "Common Due-Date Problem: Exact Polynomial Algorithms for a Given Job\n  Sequence", "comments": "15th International Symposium on Symbolic and Numeric Algorithms for\n  Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of scheduling jobs on single and parallel\nmachines where all the jobs possess different processing times but a common due\ndate. There is a penalty involved with each job if it is processed earlier or\nlater than the due date. The objective of the problem is to find the assignment\nof jobs to machines, the processing sequence of jobs and the time at which they\nare processed, which minimizes the total penalty incurred due to tardiness or\nearliness of the jobs. This work presents exact polynomial algorithms for\noptimizing a given job sequence or single and parallel machines with the\nrun-time complexities of $O(n \\log n)$ and $O(mn^2 \\log n)$ respectively, where\n$n$ is the number of jobs and $m$ the number of machines. The algorithms take a\nsequence consisting of all the jobs $(J_i, i=1,2,\\dots,n)$ as input and\ndistribute the jobs to machines (for $m>1$) along with their best completion\ntimes so as to get the least possible total penalty for this sequence. We prove\nthe optimality for the single machine case and the runtime complexities of\nboth. Henceforth, we present the results for the benchmark instances and\ncompare with previous work for single and parallel machine cases, up to $200$\njobs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2013 14:33:44 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Awasthi", "Abhishek", ""], ["L\u00e4ssig", "J\u00f6rg", ""], ["Kramer", "Oliver", ""]]}, {"id": "1311.2880", "submitter": "Abhishek Awasthi M.Sc.", "authors": "Abhishek Awasthi, Oliver Kramer and J\\\"org L\\\"assig", "title": "Aircraft Landing Problem: Efficient Algorithm for a Given Landing\n  Sequence", "comments": "16th IEEE International Conference on Computational Science and\n  Engineering (CSE 2013)", "journal-ref": null, "doi": "10.1109/CSE.2013.14", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a special case of the static aircraft landing\nproblem (ALP) with the objective to optimize landing sequences and landing\ntimes for a set of air planes. The problem is to land the planes on one or\nmultiple runways within a time window as close as possible to the preferable\ntarget landing time, maintaining a safety distance constraint. The objective of\nthis well-known NP-hard optimization problem is to minimize the sum of the\ntotal penalty incurred by all the aircraft for arriving earlier or later than\ntheir preferred landing times. For a problem variant that optimizes a given\nfeasible landing sequence for the single runway case, we present an exact\npolynomial algorithm and prove the run-time complexity to lie in $O(N^3)$,\nwhere $N$ is the number of aircraft. The proposed algorithm returns the optimal\nsolution for the ALP for a given feasible landing sequence on a single runway\nfor a common practical case of the ALP described in the paper. Furthermore, we\npropose a strategy for the ALP with multiple runways and present our results\nfor all the benchmark instances with single and multiple runways, while\ncomparing them to previous results in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2013 14:37:47 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Awasthi", "Abhishek", ""], ["Kramer", "Oliver", ""], ["L\u00e4ssig", "J\u00f6rg", ""]]}, {"id": "1311.2891", "submitter": "Joseph Anderson", "authors": "Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James\n  Voss", "title": "The More, the Merrier: the Blessing of Dimensionality for Learning Large\n  Gaussian Mixtures", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that very large mixtures of Gaussians are efficiently\nlearnable in high dimension. More precisely, we prove that a mixture with known\nidentical covariance matrices whose number of components is a polynomial of any\nfixed degree in the dimension n is polynomially learnable as long as a certain\nnon-degeneracy condition on the means is satisfied. It turns out that this\ncondition is generic in the sense of smoothed complexity, as soon as the\ndimensionality of the space is high enough. Moreover, we prove that no such\ncondition can possibly exist in low dimension and the problem of learning the\nparameters is generically hard. In contrast, much of the existing work on\nGaussian Mixtures relies on low-dimensional projections and thus hits an\nartificial barrier. Our main result on mixture recovery relies on a new\n\"Poissonization\"-based technique, which transforms a mixture of Gaussians to a\nlinear map of a product distribution. The problem of learning this map can be\nefficiently solved using some recent results on tensor decompositions and\nIndependent Component Analysis (ICA), thus giving an algorithm for recovering\nthe mixture. In addition, we combine our low-dimensional hardness results for\nGaussian mixtures with Poissonization to show how to embed difficult instances\nof low-dimensional Gaussian mixtures into the ICA setting, thus establishing\nexponential information-theoretic lower bounds for underdetermined ICA in low\ndimension. To the best of our knowledge, this is the first such result in the\nliterature. In addition to contributing to the problem of Gaussian mixture\nlearning, we believe that this work is among the first steps toward better\nunderstanding the rare phenomenon of the \"blessing of dimensionality\" in the\ncomputational aspects of statistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 19:21:03 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 20:32:45 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 03:34:38 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Anderson", "Joseph", ""], ["Belkin", "Mikhail", ""], ["Goyal", "Navin", ""], ["Rademacher", "Luis", ""], ["Voss", "James", ""]]}, {"id": "1311.2959", "submitter": "David Monniaux", "authors": "Thomas Braibant (Gallium), Jacques-Henri Jourdan (Gallium), David\n  Monniaux", "title": "Implementing and reasoning about hash-consed data structures in Coq", "comments": null, "journal-ref": "Journal of Automated Reasoning, Springer Verlag (Germany), 2014,\n  53 (3), pp.271-304", "doi": "10.1007/s10817-014-9306-0", "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on four different approaches to implementing hash-consing in Coq\nprograms. The use cases include execution inside Coq, or execution of the\nextracted OCaml code. We explore the different trade-offs between faithful use\nof pristine extracted code, and code that is fine-tuned to make use of OCaml\nprogramming constructs not available in Coq. We discuss the possible\nconsequences in terms of performances and guarantees. We use the running\nexample of binary decision diagrams and then demonstrate the generality of our\nsolutions by applying them to other examples of hash-consed data structures.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 19:49:43 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 16:28:37 GMT"}, {"version": "v3", "created": "Mon, 15 Dec 2014 14:57:17 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2015 17:39:37 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Braibant", "Thomas", "", "Gallium"], ["Jourdan", "Jacques-Henri", "", "Gallium"], ["Monniaux", "David", ""]]}, {"id": "1311.3048", "submitter": "Ofer Neiman", "authors": "Ittai Abraham and Cyril Gavoille and Anupam Gupta and Ofer Neiman and\n  Kunal Talwar", "title": "Cops, Robbers, and Threatening Skeletons: Padded Decomposition for\n  Minor-Free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any graph excluding $K_r$ as a minor has can be partitioned\ninto clusters of diameter at most $\\Delta$ while removing at most $O(r/\\Delta)$\nfraction of the edges. This improves over the results of Fakcharoenphol and\nTalwar, who building on the work of Klein, Plotkin and Rao gave a partitioning\nthat required to remove $O(r^2/\\Delta)$ fraction of the edges.\n  Our result is obtained by a new approach to relate the topological properties\n(excluding a minor) of a graph to its geometric properties (the induced\nshortest path metric). Specifically, we show that techniques used by Andreae in\nhis investigation of the cops-and-robbers game on excluded-minor graphs can be\nused to construct padded decompositions of the metrics induced by such graphs.\nIn particular, we get probabilistic partitions with padding parameter $O(r)$\nand strong-diameter partitions with padding parameter $O(r^2)$ for $K_r$-free\ngraphs, padding $O(k)$ for graphs with treewidth $k$, and padding $O(\\log g)$\nfor graphs with genus $g$.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 08:21:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 14:46:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Abraham", "Ittai", ""], ["Gavoille", "Cyril", ""], ["Gupta", "Anupam", ""], ["Neiman", "Ofer", ""], ["Talwar", "Kunal", ""]]}, {"id": "1311.3054", "submitter": "Amir Abboud", "authors": "Amir Abboud, Kevin Lewi, Ryan Williams", "title": "Losing Weight by Gaining Edges", "comments": "Title of an earlier version of this paper: On the Parameterized\n  Complexity of k-SUM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new way to encode weighted sums into unweighted pairwise\nconstraints, obtaining the following results.\n  - Define the k-SUM problem to be: given n integers in [-n^2k, n^2k] are there\nk which sum to zero? (It is well known that the same problem over arbitrary\nintegers is equivalent to the above definition, by linear-time randomized\nreductions.) We prove that this definition of k-SUM remains W[1]-hard, and is\nin fact W[1]-complete: k-SUM can be reduced to f(k) * n^o(1) instances of\nk-Clique.\n  - The maximum node-weighted k-Clique and node-weighted k-dominating set\nproblems can be reduced to n^o(1) instances of the unweighted k-Clique and\nk-dominating set problems, respectively. This implies a strong equivalence\nbetween the time complexities of the node weighted problems and the unweighted\nproblems: any polynomial improvement on one would imply an improvement for the\nother.\n  - A triangle of weight 0 in a node weighted graph with m edges can be\ndeterministically found in m^1.41 time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 09:26:57 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 02:58:29 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Abboud", "Amir", ""], ["Lewi", "Kevin", ""], ["Williams", "Ryan", ""]]}, {"id": "1311.3121", "submitter": "Mikkel Thorup", "authors": "Mikkel Thorup", "title": "Simple Tabulation, Fast Expanders, Double Tabulation, and High\n  Independence", "comments": null, "journal-ref": "This paper was published in the Proceedings of the 54nd IEEE\n  Symposium on Foundations of Computer Science (FOCS'13), pages 90-99, 2013.\n  Copyright IEEE", "doi": "10.1109/FOCS.2013.18", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple tabulation dates back to Zobrist in 1970. Keys are viewed as c\ncharacters from some alphabet A. We initialize c tables h_0, ..., h_{c-1}\nmapping characters to random hash values. A key x=(x_0, ..., x_{c-1}) is hashed\nto h_0[x_0] xor...xor h_{c-1}[x_{c-1}]. The scheme is extremely fast when the\ncharacter hash tables h_i are in cache. Simple tabulation hashing is not\n4-independent, but we show that if we apply it twice, then we get high\nindependence. First we hash to intermediate keys that are 6 times longer than\nthe original keys, and then we hash the intermediate keys to the final hash\nvalues.\n  The intermediate keys have d=6c characters from A. We can view the hash\nfunction as a degree d bipartite graph with keys on one side, each with edges\nto d output characters. We show that this graph has nice expansion properties,\nand from that we get that with another level of simple tabulation on the\nintermediate keys, the composition is a highly independent hash function. The\nindependence we get is |A|^{Omega(1/c)}.\n  Our space is O(c|A|) and the hash function is evaluated in O(c) time. Siegel\n[FOCS'89, SICOMP'04] proved that with this space, if the hash function is\nevaluated in o(c) time, then the independence can only be o(c), so our\nevaluation time is best possible for Omega(c) independence---our independence\nis much higher if c=|A|^{o(1)}.\n  Siegel used O(c)^c evaluation time to get the same independence with similar\nspace. Siegel's main focus was c=O(1), but we are exponentially faster when\nc=omega(1).\n  Applying our scheme recursively, we can increase our independence to\n|A|^{Omega(1)} with o(c^{log c}) evaluation time. Compared with Siegel's scheme\nthis is both faster and higher independence.\n  Our scheme is easy to implement, and it does provide realistic\nimplementations of 100-independent hashing for, say, 32 and 64-bit keys.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 13:20:09 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 20:45:34 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Thorup", "Mikkel", ""]]}, {"id": "1311.3144", "submitter": "Christian Schulz", "authors": "Aydin Buluc, Henning Meyerhenke, Ilya Safro, Peter Sanders, Christian\n  Schulz", "title": "Recent Advances in Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey recent trends in practical algorithms for balanced graph\npartitioning together with applications and future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 14:33:09 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 15:40:29 GMT"}, {"version": "v3", "created": "Tue, 3 Feb 2015 18:37:10 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Buluc", "Aydin", ""], ["Meyerhenke", "Henning", ""], ["Safro", "Ilya", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1311.3286", "submitter": "Richard Peng", "authors": "Richard Peng, Daniel A. Spielman", "title": "An Efficient Parallel Solver for SDD Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first parallel algorithm for solving systems of linear\nequations in symmetric, diagonally dominant (SDD) matrices that runs in\npolylogarithmic time and nearly-linear work. The heart of our algorithm is a\nconstruction of a sparse approximate inverse chain for the input matrix: a\nsequence of sparse matrices whose product approximates its inverse. Whereas\nother fast algorithms for solving systems of equations in SDD matrices exploit\nlow-stretch spanning trees, our algorithm only requires spectral graph\nsparsifiers.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 20:41:01 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Peng", "Richard", ""], ["Spielman", "Daniel A.", ""]]}, {"id": "1311.3623", "submitter": "Viswanath Nagarajan", "authors": "Nikhil Bansal and Viswanath Nagarajan", "title": "On the Adaptivity Gap of Stochastic Orienteering", "comments": "Full version of IPCO 2014 paper, 27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the stochastic orienteering problem consists of a budget $B$ and\nmetric $(V,d)$ where each vertex $v$ has a job with deterministic reward and\nrandom processing time (drawn from a known distribution). The processing times\nare independent across vertices. The goal is to obtain a non-anticipatory\npolicy to run jobs at different vertices, that maximizes expected reward,\nsubject to the total distance traveled plus processing times being at most $B$.\nAn adaptive policy is one that can choose the next vertex to visit based on\nobserved random instantiations. Whereas, a non-adaptive policy is just given by\na fixed ordering of vertices. The adaptivity gap is the worst-case ratio of the\nexpected rewards of the optimal adaptive and non-adaptive policies.\n  We prove an $\\Omega(\\log\\log B)^{1/2}$ lower bound on the adaptivity gap of\nstochastic orienteering. This provides a negative answer to the $O(1)$\nadaptivity gap conjectured earlier, and comes close to the $O(\\log\\log B)$\nupper bound. This result holds even on a line metric.\n  We also show an $O(\\log\\log B)$ upper bound on the adaptivity gap for the\ncorrelated stochastic orienteering problem, where the reward of each job is\nrandom and possibly correlated to its processing time. Using this, we obtain an\nimproved quasi-polynomial time approximation algorithm for correlated\nstochastic orienteering.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 19:38:00 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 15:51:12 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Bansal", "Nikhil", ""], ["Nagarajan", "Viswanath", ""]]}, {"id": "1311.3640", "submitter": "Jeremy Karp", "authors": "Jeremy Karp, R. Ravi", "title": "A 9/7-Approximation Algorithm for Graphic TSP in Cubic Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove new results for approximating Graphic TSP. Specifically, we provide\na polynomial-time \\frac{9}{7}-approximation algorithm for cubic bipartite\ngraphs and a (\\frac{9}{7}+\\frac{1}{21(k-2)})-approximation algorithm for\nk-regular bipartite graphs, both of which are improved approximation factors\ncompared to previous results. Our approach involves finding a cycle cover with\nrelatively few cycles, which we are able to do by leveraging the fact that all\ncycles in bipartite graphs are of even length along with our knowledge of the\nstructure of cubic graphs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 20:03:09 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 20:25:49 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Karp", "Jeremy", ""], ["Ravi", "R.", ""]]}, {"id": "1311.3651", "submitter": "Aravindan Vijayaraghavan", "authors": "Aditya Bhaskara, Moses Charikar, Ankur Moitra and Aravindan\n  Vijayaraghavan", "title": "Smoothed Analysis of Tensor Decompositions", "comments": "32 pages (including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor decompositions are a powerful tool for learning generative\nmodels, and uniqueness results give them a significant advantage over matrix\ndecomposition methods. However, tensors pose significant algorithmic challenges\nand tensors analogs of much of the matrix algebra toolkit are unlikely to exist\nbecause of hardness results. Efficient decomposition in the overcomplete case\n(where rank exceeds dimension) is particularly challenging. We introduce a\nsmoothed analysis model for studying these questions and develop an efficient\nalgorithm for tensor decomposition in the highly overcomplete case (rank\npolynomial in the dimension). In this setting, we show that our algorithm is\nrobust to inverse polynomial error -- a crucial property for applications in\nlearning since we are only allowed a polynomial number of samples. While\nalgorithms are known for exact tensor decomposition in some overcomplete\nsettings, our main contribution is in analyzing their stability in the\nframework of smoothed analysis.\n  Our main technical contribution is to show that tensor products of perturbed\nvectors are linearly independent in a robust sense (i.e. the associated matrix\nhas singular values that are at least an inverse polynomial). This key result\npaves the way for applying tensor methods to learning problems in the smoothed\nsetting. In particular, we use it to obtain results for learning multi-view\nmodels and mixtures of axis-aligned Gaussians where there are many more\n\"components\" than dimensions. The assumption here is that the model is not\nadversarially chosen, formalized by a perturbation of model parameters. We\nbelieve this an appealing way to analyze realistic instances of learning\nproblems, since this framework allows us to overcome many of the usual\nlimitations of using tensor methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 20:49:55 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2013 15:28:47 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 21:09:31 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2014 06:19:39 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Charikar", "Moses", ""], ["Moitra", "Ankur", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1311.3728", "submitter": "Jingcheng Liu", "authors": "Jingcheng Liu and Pinyan Lu", "title": "FPTAS for Counting Monotone CNF", "comments": "24 pages, 2 figures. version 1=>2: minor edits, highlighted the\n  picture of set cover/packing, and an implication of our previous result in 3D\n  matching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A monotone CNF formula is a Boolean formula in conjunctive normal form where\neach variable appears positively. We design a deterministic fully\npolynomial-time approximation scheme (FPTAS) for counting the number of\nsatisfying assignments for a given monotone CNF formula when each variable\nappears in at most $5$ clauses. Equivalently, this is also an FPTAS for\ncounting set covers where each set contains at most $5$ elements. If we allow\nvariables to appear in a maximum of $6$ clauses (or sets to contain $6$\nelements), it is NP-hard to approximate it. Thus, this gives a complete\nunderstanding of the approximability of counting for monotone CNF formulas. It\nis also an important step towards a complete characterization of the\napproximability for all bounded degree Boolean #CSP problems. In addition, we\nstudy the hypergraph matching problem, which arises naturally towards a\ncomplete classification of bounded degree Boolean #CSP problems, and show an\nFPTAS for counting 3D matchings of hypergraphs with maximum degree $4$.\n  Our main technique is correlation decay, a powerful tool to design\ndeterministic FPTAS for counting problems defined by local constraints among a\nnumber of variables. All previous uses of this design technique fall into two\ncategories: each constraint involves at most two variables, such as independent\nset, coloring, and spin systems in general; or each variable appears in at most\ntwo constraints, such as matching, edge cover, and holant problem in general.\nThe CNF problems studied here have more complicated structures than these\nproblems and require new design and proof techniques. As it turns out, the\ntechnique we developed for the CNF problem also works for the hypergraph\nmatching problem. We believe that it may also find applications in other CSP or\nmore general counting problems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 05:06:27 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 11:45:10 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Liu", "Jingcheng", ""], ["Lu", "Pinyan", ""]]}, {"id": "1311.3731", "submitter": "Victor Pan", "authors": "Ioannis Z. Emiris, Victor Y. Pan and Elias P. Tsigaridas", "title": "Chapter 10: Algebraic Algorithms", "comments": "41.1 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our Chapter in the upcoming Volume I: Computer Science and Software\nEngineering of Computing Handbook (Third edition), Allen Tucker, Teo Gonzales\nand Jorge L. Diaz-Herrera, editors, covers Algebraic Algorithms, both symbolic\nand numerical, for matrix computations and root-finding for polynomials and\nsystems of polynomials equations. We cover part of these large subjects and\ninclude basic bibliography for further study. To meet space limitation we cite\nbooks, surveys, and comprehensive articles with pointers to further references,\nrather than including all the original technical papers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 05:50:38 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Emiris", "Ioannis Z.", ""], ["Pan", "Victor Y.", ""], ["Tsigaridas", "Elias P.", ""]]}, {"id": "1311.3785", "submitter": "Vijay  Menon", "authors": "Vijay Menon", "title": "Deterministic Primality Testing - understanding the AKS algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prime numbers play a very vital role in modern cryptography and especially\nthe difficulties involved in factoring numbers composed of product of two large\nprime numbers have been put to use in many modern cryptographic designs. Thus,\nthe problem of distinguishing prime numbers from the rest is vital and\ntherefore there is a need to have efficient primality testing algorithms.\nAlthough there had been many probabilistic algorithms for primality testing,\nthere wasn't a deterministic polynomial time algorithm until 2002 when Agrawal,\nKayal and Saxena came with an algorithm, popularly known as the AKS algorithm,\nwhich could test whether a given number is prime or composite in polynomial\ntime. This project is an attempt at understanding the ingenious idea behind\nthis algorithm and the underlying principles of mathematics that is required to\nstudy it. In fact, through out this project, one of the major objectives has\nbeen to make it as much self contained as possible. Finally, the project\nprovides an implementation of the algorithm using Software for Algebra and\nGeometry Experimentation (SAGE) and arrives at conclusions on how practical or\notherwise it is.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 09:41:54 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Menon", "Vijay", ""]]}, {"id": "1311.4021", "submitter": "Matthias Mnich", "authors": "Matthias Mnich and Andreas Wiese", "title": "Scheduling Meets Fixed-Parameter Tractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-parameter tractability analysis and scheduling are two core domains of\ncombinatorial optimization which led to deep understanding of many important\nalgorithmic questions. However, even though fixed-parameter algorithms are\nappealing for many reasons, no such algorithms are known for many fundamental\nscheduling problems.\n  In this paper we present the first fixed-parameter algorithms for classical\nscheduling problems such as makespan minimization, scheduling with\njob-dependent cost functions-one important example being weighted flow time-and\nscheduling with rejection. To this end, we identify crucial parameters that\ndetermine the problems' complexity. In particular, we manage to cope with the\nproblem complexity stemming from numeric input values, such as job processing\ntimes, which is usually a core bottleneck in the design of fixed-parameter\nalgorithms. We complement our algorithms with W[1]-hardness results showing\nthat for smaller sets of parameters the respective problems do not allow\nFPT-algorithms. In particular, our positive and negative results for scheduling\nwith rejection explore a research direction proposed by D\\'aniel Marx.\n  We hope that our contribution yields a new and fresh perspective on\nscheduling and fixed-parameter algorithms and will lead to further fruitful\ninterdisciplinary research connecting these two areas.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 05:50:21 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Mnich", "Matthias", ""], ["Wiese", "Andreas", ""]]}, {"id": "1311.4055", "submitter": "Micha{\\l} Pilipczuk", "authors": "Ivan Bliznets, Fedor V. Fomin, Micha{\\l} Pilipczuk, Yngve Villanger", "title": "Largest chordal and interval subgraphs faster than 2^n", "comments": "The preliminary version of this work appeared in the proceedings of\n  ESA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that in an n-vertex graph, induced chordal and interval subgraphs\nwith the maximum number of vertices can be found in time $O(2^{\\lambda n})$ for\nsome $\\lambda<1$. These are the first algorithms breaking the trivial $2^n\nn^{O(1)}$ bound of the brute-force search for these problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 13:00:19 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Bliznets", "Ivan", ""], ["Fomin", "Fedor V.", ""], ["Pilipczuk", "Micha\u0142", ""], ["Villanger", "Yngve", ""]]}, {"id": "1311.4066", "submitter": "Susan Margulies", "authors": "S. Margulies and J. Morton", "title": "Polynomial-time Solvable #CSP Problems via Algebraic Models and Pfaffian\n  Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Pfaffian circuit is a tensor contraction network where the edges are\nlabeled with changes of bases in such a way that a very specific set of\ncombinatorial properties are satisfied. By modeling the permissible changes of\nbases as systems of polynomial equations, and then solving via computation, we\nare able to identify classes of 0/1 planar #CSP problems solvable in\npolynomial-time via the Pfaffian circuit evaluation theorem (a variant of L.\nValiant's Holant Theorem). We present two different models of 0/1 variables,\none that is possible under a homogeneous change of basis, and one that is\npossible under a heterogeneous change of basis only. We enumerate a series of\n1,2,3, and 4-arity gates/cogates that represent constraints, and define a class\nof constraints that is possible under the assumption of a ``bridge\" between two\nparticular changes of bases. We discuss the issue of planarity of Pfaffian\ncircuits, and demonstrate possible directions in algebraic computation for\ndesigning a Pfaffian tensor contraction network fragment that can simulate a\nswap gate/cogate. We conclude by developing the notion of a decomposable\ngate/cogate, and discuss the computational benefits of this definition.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 14:20:00 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Margulies", "S.", ""], ["Morton", "J.", ""]]}, {"id": "1311.4394", "submitter": "Srinivasa Rao Satti", "authors": "Pooya Davoodi and Gonzalo Navarro and Rajeev Raman and S. Srinivasa\n  Rao", "title": "Encoding Range Minimum Queries", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of encoding range minimum queries (RMQs): given an\narray A[1..n] of distinct totally ordered values, to pre-process A and create a\ndata structure that can answer the query RMQ(i,j), which returns the index\ncontaining the smallest element in A[i..j], without access to the array A at\nquery time. We give a data structure whose space usage is 2n + o(n) bits, which\nis asymptotically optimal for worst-case data, and answers RMQs in O(1)\nworst-case time. This matches the previous result of Fischer and Heun [SICOMP,\n2011], but is obtained in a more natural way. Furthermore, our result can\nencode the RMQs of a random array A in 1.919n + o(n) bits in expectation, which\nis not known to hold for Fischer and Heun's result. We then generalize our\nresult to the encoding range top-2 query (RT2Q) problem, which is like the\nencoding RMQ problem except that the query RT2Q(i,j) returns the indices of\nboth the smallest and second-smallest elements of A[i..j]. We introduce a data\nstructure using 3.272n+o(n) bits that answers RT2Qs in constant time, and also\ngive lower bounds on the effective entropy} of RT2Q.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 14:29:58 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Davoodi", "Pooya", ""], ["Navarro", "Gonzalo", ""], ["Raman", "Rajeev", ""], ["Rao", "S. Srinivasa", ""]]}, {"id": "1311.4552", "submitter": "Szymon Grabowski", "authors": "Sebastian Deorowicz, Szymon Grabowski", "title": "Efficient algorithms for the longest common subsequence in $k$-length\n  substrings", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the longest common subsequence in $k$-length substrings (LCS$k$) is a\nrecently proposed problem motivated by computational biology. This is a\ngeneralization of the well-known LCS problem in which matching symbols from two\nsequences $A$ and $B$ are replaced with matching non-overlapping substrings of\nlength $k$ from $A$ and $B$. We propose several algorithms for LCS$k$, being\nnon-trivial incarnations of the major concepts known from LCS research (dynamic\nprogramming, sparse dynamic programming, tabulation). Our algorithms make use\nof a linear-time and linear-space preprocessing finding the occurrences of all\nthe substrings of length $k$ from one sequence in the other sequence.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:02:38 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Deorowicz", "Sebastian", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1311.4563", "submitter": "Chun Ye", "authors": "Daniel Bienstock, Jay Sethuraman, Chun Ye", "title": "Approximation Algorithms for the Incremental Knapsack Problem via\n  Disjunctive Programming", "comments": "Key words: Approximation Algorithms, Integer Programming, Disjunctive\n  Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the incremental knapsack problem ($\\IK$), we are given a knapsack whose\ncapacity grows weakly as a function of time. There is a time horizon of $T$\nperiods and the capacity of the knapsack is $B_t$ in period $t$ for $t = 1,\n\\ldots, T$. We are also given a set $S$ of $N$ items to be placed in the\nknapsack. Item $i$ has a value of $v_i$ and a weight of $w_i$ that is\nindependent of the time period. At any time period $t$, the sum of the weights\nof the items in the knapsack cannot exceed the knapsack capacity $B_t$.\nMoreover, once an item is placed in the knapsack, it cannot be removed from the\nknapsack at a later time period. We seek to maximize the sum of (discounted)\nknapsack values over time subject to the capacity constraints. We first give a\nconstant factor approximation algorithm for $\\IK$, under mild restrictions on\nthe growth rate of $B_t$ (the constant factor depends on the growth rate). We\nthen give a PTAS for $\\IIK$, the special case of $\\IK$ with no discounting,\nwhen $T = O(\\sqrt{\\log N})$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:28:02 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Bienstock", "Daniel", ""], ["Sethuraman", "Jay", ""], ["Ye", "Chun", ""]]}, {"id": "1311.4609", "submitter": "Kyle Treleaven", "authors": "Kyle Treleaven, Josh Bialkowski, Emilio Frazzoli", "title": "An O(M log M) Algorithm for Bipartite Matching with Roadmap Distances", "comments": "14 pages, 1 figure, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is presented which produces the minimum cost bipartite matching\nbetween two sets of M points each, where the cost of matching two points is\nproportional to the minimum distance by which a particle could reach one point\nfrom the other while constrained to travel on a connected set of curves, or\nroads. Given any such roadmap, the algorithm obtains O(M log M) total runtime\nin terms of M, which is the best possible bound in the sense that any algorithm\nfor minimal matching has runtime Omega(M log M). The algorithm is strongly\npolynomial and is based on a capacity-scaling approach to the [minimum] convex\ncost flow problem. The result generalizes the known Theta(M log M) complexity\nof computing optimal matchings between two sets of points on (i) a line\nsegment, and (ii) a circle.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 02:22:50 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Treleaven", "Kyle", ""], ["Bialkowski", "Josh", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1311.4721", "submitter": "Sigal Oren", "authors": "Shahar Dobzinski, Noam Nisan and Sigal Oren", "title": "Economic Efficiency Requires Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the necessity of interaction between individuals for obtaining\napproximately efficient allocations. The role of interaction in markets has\nreceived significant attention in economic thinking, e.g. in Hayek's 1945\nclassic paper.\n  We consider this problem in the framework of simultaneous communication\ncomplexity. We analyze the amount of simultaneous communication required for\nachieving an approximately efficient allocation. In particular, we consider two\nsettings: combinatorial auctions with unit demand bidders (bipartite matching)\nand combinatorial auctions with subadditive bidders. For both settings we first\nshow that non-interactive systems have enormous communication costs relative to\ninteractive ones. On the other hand, we show that limited interaction enables\nus to find approximately efficient allocations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 12:46:14 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 10:57:56 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Dobzinski", "Shahar", ""], ["Nisan", "Noam", ""], ["Oren", "Sigal", ""]]}, {"id": "1311.4728", "submitter": "Justin Ward", "authors": "Maxim Sviridenko and Jan Vondr\\'ak and Justin Ward", "title": "Optimal approximation for submodular and supermodular optimization with\n  bounded curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design new approximation algorithms for the problems of optimizing\nsubmodular and supermodular functions subject to a single matroid constraint.\nSpecifically, we consider the case in which we wish to maximize a nondecreasing\nsubmodular function or minimize a nonincreasing supermodular function in the\nsetting of bounded total curvature $c$. In the case of submodular maximization\nwith curvature $c$, we obtain a $(1-c/e)$-approximation --- the first\nimprovement over the greedy $(1-e^{-c})/c$-approximation of Conforti and\nCornuejols from 1984, which holds for a cardinality constraint, as well as\nrecent approaches that hold for an arbitrary matroid constraint.\n  Our approach is based on modifications of the continuous greedy algorithm and\nnon-oblivious local search, and allows us to approximately maximize the sum of\na nonnegative, nondecreasing submodular function and a (possibly negative)\nlinear function. We show how to reduce both submodular maximization and\nsupermodular minimization to this general problem when the objective function\nhas bounded total curvature. We prove that the approximation results we obtain\nare the best possible in the value oracle model, even in the case of a\ncardinality constraint.\n  We define an extension of the notion of curvature to general monotone set\nfunctions and show $(1-c)$-approximation for maximization and\n$1/(1-c)$-approximation for minimization cases. Finally, we give two concrete\napplications of our results in the settings of maximum entropy sampling, and\nthe column-subset selection problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 13:04:46 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 16:31:25 GMT"}, {"version": "v3", "created": "Fri, 12 Dec 2014 16:12:20 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Sviridenko", "Maxim", ""], ["Vondr\u00e1k", "Jan", ""], ["Ward", "Justin", ""]]}, {"id": "1311.4759", "submitter": "Shanfei Li", "authors": "Karen Aardal, Pieter van den Berg, Dion Gijswijt, Shanfei Li", "title": "Approximation Algorithms for Hard Capacitated $k$-facility Location\n  Problems", "comments": "We add new results obtained with Karen Aardal and Pieter van den Berg\n  to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the capacitated $k$-facility location problem, in which we are given\na set of clients with demands, a set of facilities with capacities and a\nconstant number $k$. It costs $f_i$ to open facility $i$, and $c_{ij}$ for\nfacility $i$ to serve one unit of demand from client $j$. The objective is to\nopen at most $k$ facilities serving all the demands and satisfying the capacity\nconstraints while minimizing the sum of service and opening costs.\n  In this paper, we give the first fully polynomial time approximation scheme\n(FPTAS) for the single-sink (single-client) capacitated $k$-facility location\nproblem. Then, we show that the capacitated $k$-facility location problem with\nuniform capacities is solvable in polynomial time if the number of clients is\nfixed by reducing it to a collection of transportation problems. Third, we\nanalyze the structure of extreme point solutions, and examine the efficiency of\nthis structure in designing approximation algorithms for capacitated\n$k$-facility location problems. Finally, we extend our results to obtain an\nimproved approximation algorithm for the capacitated facility location problem\nwith uniform opening cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 14:54:58 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 09:57:13 GMT"}, {"version": "v3", "created": "Thu, 8 May 2014 17:07:43 GMT"}, {"version": "v4", "created": "Fri, 12 Sep 2014 15:31:07 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Aardal", "Karen", ""], ["Berg", "Pieter van den", ""], ["Gijswijt", "Dion", ""], ["Li", "Shanfei", ""]]}, {"id": "1311.4768", "submitter": "Petr Golovach", "authors": "Petr A. Golovach", "title": "Editing to a Graph of Given Degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Editing to a Graph of Given Degrees problem that asks for a\ngraph G, non-negative integers d,k and a function \\delta:V(G)->{1,...,d},\nwhether it is possible to obtain a graph G' from G such that the degree of v is\n\\delta(v) for any vertex v by at most k vertex or edge deletions or edge\nadditions. We construct an FPT-algorithm for Editing to a Graph of Given\nDegrees parameterized by d+k. We complement this result by showing that the\nproblem has no polynomial kernel unless NP\\subseteq coNP/poly.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 15:06:44 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 10:31:18 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Golovach", "Petr A.", ""]]}, {"id": "1311.4821", "submitter": "Will Perkins", "authors": "Vitaly Feldman, Will Perkins, Santosh Vempala", "title": "On the Complexity of Random Satisfiability Problems with Planted\n  Solutions", "comments": "Extended abstract appeared in STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying a planted assignment given a random $k$-SAT\nformula consistent with the assignment exhibits a large algorithmic gap: while\nthe planted solution becomes unique and can be identified given a formula with\n$O(n\\log n)$ clauses, there are distributions over clauses for which the best\nknown efficient algorithms require $n^{k/2}$ clauses. We propose and study a\nunified model for planted $k$-SAT, which captures well-known special cases. An\ninstance is described by a planted assignment $\\sigma$ and a distribution on\nclauses with $k$ literals. We define its distribution complexity as the largest\n$r$ for which the distribution is not $r$-wise independent ($1 \\le r \\le k$ for\nany distribution with a planted assignment).\n  Our main result is an unconditional lower bound, tight up to logarithmic\nfactors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012],\nmatching known upper bounds, which, as we show, can be implemented using a\nstatistical algorithm. Since known approaches for problems over distributions\nhave statistical analogues (spectral, MCMC, gradient-based, convex optimization\netc.), this lower bound provides a rigorous explanation of the observed\nalgorithmic gap. The proof introduces a new general technique for the analysis\nof statistical query algorithms. It also points to a geometric paring\nphenomenon in the space of all planted assignments.\n  We describe consequences of our lower bounds to Feige's refutation hypothesis\n[Feige 2002] and to lower bounds on general convex programs that solve planted\n$k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in\nparticular, provide concrete evidence for the security of Goldreich's one-way\nfunction and the associated pseudorandom generator when used with a\nsufficiently hard predicate [Goldreich 2000].\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 18:13:32 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 17:17:43 GMT"}, {"version": "v3", "created": "Thu, 17 Jul 2014 13:20:31 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 07:29:18 GMT"}, {"version": "v5", "created": "Thu, 14 May 2015 04:57:56 GMT"}, {"version": "v6", "created": "Fri, 15 Jan 2016 18:58:50 GMT"}, {"version": "v7", "created": "Sun, 15 Oct 2017 16:37:42 GMT"}, {"version": "v8", "created": "Tue, 6 Mar 2018 18:24:11 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Feldman", "Vitaly", ""], ["Perkins", "Will", ""], ["Vempala", "Santosh", ""]]}, {"id": "1311.4922", "submitter": "Yipeng Liu Dr.", "authors": "Yurrit Avonds, Yipeng Liu, Sabine Van Huffel", "title": "Simultaneous Greedy Analysis Pursuit for Compressive Sensing of\n  Multi-Channel ECG Signals", "comments": "8 pages, 2 figures, Internal Report 13-82, ESAT-STADIUS, University\n  of Leuven, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses compressive sensing for multi-channel ECG. Compared to\nthe traditional sparse signal recovery approach which decomposes the signal\ninto the product of a dictionary and a sparse vector, the recently developed\ncosparse approach exploits sparsity of the product of an analysis matrix and\nthe original signal. We apply the cosparse Greedy Analysis Pursuit (GAP)\nalgorithm for compressive sensing of ECG signals. Moreover, to reduce\nprocessing time, classical signal-channel GAP is generalized to the\nmulti-channel GAP algorithm, which simultaneously reconstructs multiple signals\nwith similar support. Numerical experiments show that the proposed method\noutperforms the classical sparse multi-channel greedy algorithms in terms of\naccuracy and the single-channel cosparse approach in terms of processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 00:02:00 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Avonds", "Yurrit", ""], ["Liu", "Yipeng", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1311.5022", "submitter": "Shaona Ghosh", "authors": "Shaona Ghosh, Adam Prugel-Bennett", "title": "Extended Formulations for Online Linear Bandit Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line linear optimization on combinatorial action sets (d-dimensional\nactions) with bandit feedback, is known to have complexity in the order of the\ndimension of the problem. The exponential weighted strategy achieves the best\nknown regret bound that is of the order of $d^{2}\\sqrt{n}$ (where $d$ is the\ndimension of the problem, $n$ is the time horizon). However, such strategies\nare provably suboptimal or computationally inefficient. The complexity is\nattributed to the combinatorial structure of the action set and the dearth of\nefficient exploration strategies of the set. Mirror descent with entropic\nregularization function comes close to solving this problem by enforcing a\nmeticulous projection of weights with an inherent boundary condition. Entropic\nregularization in mirror descent is the only known way of achieving a\nlogarithmic dependence on the dimension. Here, we argue otherwise and recover\nthe original intuition of exponential weighting by borrowing a technique from\ndiscrete optimization and approximation algorithms called `extended\nformulation'. Such formulations appeal to the underlying geometry of the set\nwith a guaranteed logarithmic dependence on the dimension underpinned by an\ninformation theoretic entropic analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 11:39:26 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 12:25:29 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 16:43:29 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Ghosh", "Shaona", ""], ["Prugel-Bennett", "Adam", ""]]}, {"id": "1311.5081", "submitter": "Tong-Wook Shinn", "authors": "Tong-Wook Shinn, Tadao Takaoka", "title": "Combining the Shortest Paths and the Bottleneck Paths Problems", "comments": "Will be presented at ACSC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine the well known Shortest Paths (SP) problem and the Bottleneck\nPaths (BP) problem to introduce a new problem called the Shortest Paths for All\nFlows (SP-AF) problem that has relevance in real life applications. We first\nsolve the Single Source Shortest Paths for All Flows (SSSP-AF) problem on\ndirected graphs with unit edge costs in $O(mn)$ worst case time bound. We then\npresent two algorithms to solve SSSP-AF on directed graphs with integer edge\ncosts bounded by $c$ in $O(m^2 + nc)$ and $O(m^2 + mn\\log{(\\frac{c}{m})})$ time\nbounds. Finally we extend our algorithms for the SSSP-AF problem to solve the\nAll Pairs Shortest Paths for All Flows (APSP-AF) problem in $O(m^{2}n + nc)$\nand $O(m^{2}n + mn^{2}\\log{(\\frac{c}{mn})})$ time bounds. All algorithms\npresented in this paper are practical for implementation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 14:57:37 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Shinn", "Tong-Wook", ""], ["Takaoka", "Tadao", ""]]}, {"id": "1311.5090", "submitter": "Madhur Tulsiani", "authors": "Arnab Bhattacharyya and Pooya Hatami and Madhur Tulsiani", "title": "Algorithmic regularity for polynomials and applications", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analogy with the regularity lemma of Szemer\\'edi, regularity lemmas for\npolynomials shown by Green and Tao (Contrib. Discrete Math. 2009) and by\nKaufman and Lovett (FOCS 2008) modify a given collection of polynomials \\calF =\n{P_1,...,P_m} to a new collection \\calF' so that the polynomials in \\calF' are\n\"pseudorandom\". These lemmas have various applications, such as (special cases)\nof Reed-Muller testing and worst-case to average-case reductions for\npolynomials. However, the transformation from \\calF to \\calF' is not\nalgorithmic for either regularity lemma. We define new notions of regularity\nfor polynomials, which are analogous to the above, but which allow for an\nefficient algorithm to compute the pseudorandom collection \\calF'. In\nparticular, when the field is of high characteristic, in polynomial time, we\ncan refine \\calF into \\calF' where every nonzero linear combination of\npolynomials in \\calF' has desirably small Gowers norm.\n  Using the algorithmic regularity lemmas, we show that if a polynomial P of\ndegree d is within (normalized) Hamming distance 1-1/|F| -\\eps of some unknown\npolynomial of degree k over a prime field F (for k < d < |F|), then there is an\nefficient algorithm for finding a degree-k polynomial Q, which is within\ndistance 1-1/|F| -\\eta of P, for some \\eta depending on \\eps. This can be\nthought of as decoding the Reed-Muller code of order k beyond the list decoding\nradius (finding one close codeword), when the received word P itself is a\npolynomial of degree d (with k < d < |F|).\n  We also obtain an algorithmic version of the worst-case to average-case\nreductions by Kaufman and Lovett. They show that if a polynomial of degree d\ncan be weakly approximated by a polynomial of lower degree, then it can be\ncomputed exactly using a collection of polynomials of degree at most d-1. We\ngive an efficient (randomized) algorithm to find this collection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 05:07:16 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Hatami", "Pooya", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1311.5193", "submitter": "Ugo Vaccaro", "authors": "Luisa Gargano, Pavol Hell, Joseph G. Peters, Ugo Vaccaro", "title": "Influence Diffusion in Social Networks under Time Window Constraints", "comments": "An extended abstract of a preliminary version of this paper appeared\n  in: Proceedings of 20th International Colloquium on Structural Information\n  and Communication Complexity (Sirocco 2013), Lectures Notes in Computer\n  Science vol. 8179, T. Moscibroda and A.A. Rescigno (Eds.), pp. 141-152, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a combinatorial model of the spread of influence in networks that\ngeneralizes existing schemata recently proposed in the literature. In our\nmodel, agents change behaviors/opinions on the basis of information collected\nfrom their neighbors in a time interval of bounded size whereas agents are\nassumed to have unbounded memory in previously studied scenarios. In our\nmathematical framework, one is given a network $G=(V,E)$, an integer value\n$t(v)$ for each node $v\\in V$, and a time window size $\\lambda$. The goal is to\ndetermine a small set of nodes (target set) that influences the whole graph.\nThe spread of influence proceeds in rounds as follows: initially all nodes in\nthe target set are influenced; subsequently, in each round, any uninfluenced\nnode $v$ becomes influenced if the number of its neighbors that have been\ninfluenced in the previous $\\lambda$ rounds is greater than or equal to $t(v)$.\nWe prove that the problem of finding a minimum cardinality target set that\ninfluences the whole network $G$ is hard to approximate within a\npolylogarithmic factor. On the positive side, we design exact polynomial time\nalgorithms for paths, rings, trees, and complete graphs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 19:54:24 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Gargano", "Luisa", ""], ["Hell", "Pavol", ""], ["Peters", "Joseph G.", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1311.5317", "submitter": "Mohsen Ghaffari", "authors": "Keren Censor-Hillel, Mohsen Ghaffari, Fabian Kuhn", "title": "Distributed Connectivity Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present time-efficient distributed algorithms for decomposing graphs with\nlarge edge or vertex connectivity into multiple spanning or dominating trees,\nrespectively. As their primary applications, these decompositions allow us to\nachieve information flow with size close to the connectivity by parallelizing\nit along the trees. More specifically, our distributed decomposition algorithms\nare as follows:\n  (I) A decomposition of each undirected graph with vertex-connectivity $k$\ninto (fractionally) vertex-disjoint weighted dominating trees with total weight\n$\\Omega(\\frac{k}{\\log n})$, in $\\widetilde{O}(D+\\sqrt{n})$ rounds.\n  (II) A decomposition of each undirected graph with edge-connectivity\n$\\lambda$ into (fractionally) edge-disjoint weighted spanning trees with total\nweight $\\lceil\\frac{\\lambda-1}{2}\\rceil(1-\\varepsilon)$, in\n$\\widetilde{O}(D+\\sqrt{n\\lambda})$ rounds.\n  We also show round complexity lower bounds of\n$\\tilde{\\Omega}(D+\\sqrt{\\frac{n}{k}})$ and\n$\\tilde{\\Omega}(D+\\sqrt{\\frac{n}{\\lambda}})$ for the above two decompositions,\nusing techniques of [Das Sarma et al., STOC'11]. Moreover, our\nvertex-connectivity decomposition extends to centralized algorithms and\nimproves the time complexity of [Censor-Hillel et al., SODA'14] from $O(n^3)$\nto near-optimal $\\tilde{O}(m)$.\n  As corollaries, we also get distributed oblivious routing broadcast with\n$O(1)$-competitive edge-congestion and $O(\\log n)$-competitive\nvertex-congestion. Furthermore, the vertex connectivity decomposition leads to\nnear-time-optimal $O(\\log n)$-approximation of vertex connectivity: centralized\n$\\widetilde{O}(m)$ and distributed $\\tilde{O}(D+\\sqrt{n})$. The former moves\ntoward the 1974 conjecture of Aho, Hopcroft, and Ullman postulating an $O(m)$\ncentralized exact algorithm while the latter is the first distributed vertex\nconnectivity approximation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 06:24:20 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1311.5481", "submitter": "Georgios Stamoulis", "authors": "Monaldo Mastrolilli and Georgios Stamoulis", "title": "Bi-Criteria and Approximation Algorithms for Restricted Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study approximation algorithms for the \\textit{Bounded Color\nMatching} problem (a.k.a. Restricted Matching problem) which is defined as\nfollows: given a graph in which each edge $e$ has a color $c_e$ and a profit\n$p_e \\in \\mathbb{Q}^+$, we want to compute a maximum (cardinality or profit)\nmatching in which no more than $w_j \\in \\mathbb{Z}^+$ edges of color $c_j$ are\npresent. This kind of problems, beside the theoretical interest on its own\nright, emerges in multi-fiber optical networking systems, where we interpret\neach unique wavelength that can travel through the fiber as a color class and\nwe would like to establish communication between pairs of systems. We study\napproximation and bi-criteria algorithms for this problem which are based on\nlinear programming techniques and, in particular, on polyhedral\ncharacterizations of the natural linear formulation of the problem. In our\nsetting, we allow violations of the bounds $w_j$ and we model our problem as a\nbi-criteria problem: we have two objectives to optimize namely (a) to maximize\nthe profit (maximum matching) while (b) minimizing the violation of the color\nbounds. We prove how we can \"beat\" the integrality gap of the natural linear\nprogramming formulation of the problem by allowing only a slight violation of\nthe color bounds. In particular, our main result is \\textit{constant}\napproximation bounds for both criteria of the corresponding bi-criteria\noptimization problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 17:07:08 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Mastrolilli", "Monaldo", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "1311.5930", "submitter": "Ilya Safro", "authors": "William Hager, James Hungerford, Ilya Safro", "title": "A Continuous Refinement Strategy for the Multilevel Computation of\n  Vertex Separators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vertex Separator Problem (VSP) on a graph is the problem of finding the\nsmallest collection of vertices whose removal separates the graph into two\ndisjoint subsets of roughly equal size. Recently, Hager and Hungerford [1]\ndeveloped a continuous bilinear programming formulation of the VSP. In this\npaper, we reinforce the bilinear programming approach with a multilevel scheme\nfor learning the structure of the graph.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 23:45:24 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Hager", "William", ""], ["Hungerford", "James", ""], ["Safro", "Ilya", ""]]}, {"id": "1311.5935", "submitter": "Yann Disser", "authors": "Yann Disser, Martin Skutella", "title": "The Simplex Algorithm is NP-mighty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to classify the power of algorithms by the complexity of the\nproblems that they can be used to solve. Instead of restricting to the problem\na particular algorithm was designed to solve explicitly, however, we include\nproblems that, with polynomial overhead, can be solved 'implicitly' during the\nalgorithm's execution. For example, we allow to solve a decision problem by\nsuitably transforming the input, executing the algorithm, and observing whether\na specific bit in its internal configuration ever switches during the\nexecution. We show that the Simplex Method, the Network Simplex Method (both\nwith Dantzig's original pivot rule), and the Successive Shortest Path Algorithm\nare NP-mighty, that is, each of these algorithms can be used to solve any\nproblem in NP. This result casts a more favorable light on these algorithms'\nexponential worst-case running times. Furthermore, as a consequence of our\napproach, we obtain several novel hardness results. For example, for a given\ninput to the Simplex Algorithm, deciding whether a given variable ever enters\nthe basis during the algorithm's execution and determining the number of\niterations needed are both NP-hard problems. Finally, we close a long-standing\nopen problem in the area of network flows over time by showing that earliest\narrival flows are NP-hard to obtain.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 00:09:17 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 17:44:27 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Disser", "Yann", ""], ["Skutella", "Martin", ""]]}, {"id": "1311.5989", "submitter": "Yipeng Liu Dr.", "authors": "Yurrit Avonds and Yipeng Liu and Sabine Van Huffel", "title": "Robust Cosparse Greedy Signal Reconstruction for Compressive Sensing\n  with Multiplicative and Additive Noise", "comments": "This paper has been withdrawn by the author due to errors (missed\n  \\gamma in the 2nd term on the right) in equation 10, equation 11, and\n  equation 12, which leads to further error in Algorithm 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy algorithms are popular in compressive sensing for their high\ncomputational efficiency. But the performance of current greedy algorithms can\nbe degenerated seriously by noise (both multiplicative noise and additive\nnoise). A robust version of greedy cosparse greedy algorithm (greedy analysis\npursuit) is presented in this paper. Comparing with previous methods, The\nproposed robust greedy analysis pursuit algorithm is based on an optimization\nmodel which allows both multiplicative noise and additive noise in the data\nfitting constraint. Besides, a new stopping criterion that is derived. The new\nalgorithm is applied to compressive sensing of ECG signals. Numerical\nexperiments based on real-life ECG signals demonstrate the performance\nimprovement of the proposed greedy algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 11:29:06 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 10:41:29 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Avonds", "Yurrit", ""], ["Liu", "Yipeng", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1311.6024", "submitter": "Chaitanya Swamy", "authors": "Zachary Friggstad and Chaitanya Swamy", "title": "Approximation Algorithms for Regret-Bounded Vehicle Routing and\n  Applications to Distance-Constrained Vehicle Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider vehicle-routing problems (VRPs) that incorporate the notion of\n{\\em regret} of a client, which is a measure of the waiting time of a client\nrelative to its shortest-path distance from the depot. Formally, we consider\nboth the additive and multiplicative versions of, what we call, the {\\em\nregret-bounded vehicle routing problem} (RVRP). In these problems, we are given\nan undirected complete graph $G=(\\{r\\}\\cup V,E)$ on $n$ nodes with a\ndistinguished root (depot) node $r$, edge costs $\\{c_{uv}\\}$ that form a\nmetric, and a regret bound $R$. Given a path $P$ rooted at $r$ and a node $v\\in\nP$, let $c_P(v)$ be the distance from $r$ to $v$ along $P$. The goal is to find\nthe fewest number of paths rooted at $r$ that cover all the nodes so that for\nevery node $v$ covered by (say) path $P$: (i) its additive regret\n$c_P(v)-c_{rv}$, with respect to $P$ is at most $R$ in {\\em additive-RVRP}; or\n(ii) its multiplicative regret, $c_P(c)/c_{rv}$, with respect to $P$ is at most\n$R$ in {\\em multiplicative-RVRP}.\n  Our main result is the {\\em first} constant-factor approximation algorithm\nfor additive-RVRP by devising rounding techniques for a natural {\\em\nconfiguration-style LP}. This is a substantial improvement over the\nprevious-best $O(\\log n)$-approximation. Additive-RVRP turns out be a rather\ncentral vehicle-routing problem, whose study reveals insights into a variety of\nother regret-related problems as well as the classical {\\em\ndistance-constrained VRP} ({DVRP}). We obtain approximation ratios of\n$O\\bigl(\\log(\\frac{R}{R-1})\\bigr)$ for multiplicative-RVRP, and\n$O\\bigl(\\min\\bigl\\{\\mathit{OPT},\\frac{\\log D}{\\log\\log D}\\bigr\\}\\bigr)$ for\nDVRP with distance bound $D$ via reductions to additive-RVRP; the latter\nimproves upon the previous-best approximation for DVRP.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 17:33:35 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Friggstad", "Zachary", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1311.6093", "submitter": "Pushkar Mishra", "authors": "Pushkar Mishra", "title": "A New Algorithm for Updating and Querying Sub-arrays of Multidimensional\n  Arrays", "comments": "14 Pages, 3 Figures, 1 Table", "journal-ref": null, "doi": "10.13140/RG.2.1.2394.2485", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a $d$-dimensional array $A$, an update operation adds a given constant\n$C$ to each element within a continuous sub-array of $A$. A query operation\ncomputes the sum of all the elements within a continuous sub-array of $A$. The\none-dimensional update and query handling problem has been studied intensively\nand is usually solved using segment trees with lazy propagation technique. In\nthis paper, we present a new algorithm incorporating Binary Indexed Trees and\nInclusion-Exclusion Principle to accomplish the same task. We extend the\nalgorithm to update and query sub-matrices of matrices (two-dimensional array).\nFinally, we propose a general form of the algorithm for $d$-dimensions which\nachieves $\\mathcal{O}(4^d*\\log^{d}n)$ time complexity for both updates and\nqueries. This is an improvement over the previously known algorithms which\nutilize hierarchical data structures like quadtrees and octrees and have a\nworst-case time complexity of $\\Omega(n^{d-1})$ per update/query.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 08:18:04 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 15:28:05 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2013 17:27:11 GMT"}, {"version": "v4", "created": "Thu, 23 Jan 2014 12:19:36 GMT"}, {"version": "v5", "created": "Sun, 1 Nov 2015 10:34:52 GMT"}, {"version": "v6", "created": "Wed, 3 Aug 2016 22:19:11 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Mishra", "Pushkar", ""]]}, {"id": "1311.6126", "submitter": "Valmir Barbosa", "authors": "Leonardo I. L. Oliveira, Valmir C. Barbosa, F\\'abio Protti", "title": "An energy function and its application to the periodic behavior of\n  k-reversible processes", "comments": null, "journal-ref": "Discrete Applied Mathematics 245 (2018), 77-93", "doi": "10.1016/j.dam.2017.07.005", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the graph dynamical systems known as k-reversible processes. In\nsuch processes, each vertex in the graph has one of two possible states at each\ndiscrete time step. Each vertex changes its state between the current time and\nthe next if and only if it currently has at least k neighbors in a state\ndifferent than its own. For such processes, we present a monotonic function\nsimilar to the decreasing energy functions used to study threshold networks.\nUsing this new function, we show an alternative proof for the maximum period\nlength in a k-reversible process and provide better upper bounds on the\ntransient length in both the general case and the case of trees.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 14:12:41 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Oliveira", "Leonardo I. L.", ""], ["Barbosa", "Valmir C.", ""], ["Protti", "F\u00e1bio", ""]]}, {"id": "1311.6204", "submitter": "Aleksandar Nikolov", "authors": "Aleksandar Nikolov, Kunal Talwar", "title": "Approximating Hereditary Discrepancy via Small Width Ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Discrepancy of a hypergraph is the minimum attainable value, over\ntwo-colorings of its vertices, of the maximum absolute imbalance of any\nhyperedge. The Hereditary Discrepancy of a hypergraph, defined as the maximum\ndiscrepancy of a restriction of the hypergraph to a subset of its vertices, is\na measure of its complexity. Lovasz, Spencer and Vesztergombi (1986) related\nthe natural extension of this quantity to matrices to rounding algorithms for\nlinear programs, and gave a determinant based lower bound on the hereditary\ndiscrepancy. Matousek (2011) showed that this bound is tight up to a\npolylogarithmic factor, leaving open the question of actually computing this\nbound. Recent work by Nikolov, Talwar and Zhang (2013) showed a polynomial time\n$\\tilde{O}(\\log^3 n)$-approximation to hereditary discrepancy, as a by-product\nof their work in differential privacy. In this paper, we give a direct simple\n$O(\\log^{3/2} n)$-approximation algorithm for this problem. We show that up to\nthis approximation factor, the hereditary discrepancy of a matrix $A$ is\ncharacterized by the optimal value of simple geometric convex program that\nseeks to minimize the largest $\\ell_{\\infty}$ norm of any point in a ellipsoid\ncontaining the columns of $A$. This characterization promises to be a useful\ntool in discrepancy theory.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 03:44:06 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 05:20:59 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Nikolov", "Aleksandar", ""], ["Talwar", "Kunal", ""]]}, {"id": "1311.6209", "submitter": "Peter Robinson", "authors": "Hartmut Klauck, Danupon Nanongkai, Gopal Pandurangan, Peter Robinson", "title": "Distributed Computation of Large-scale Graph Problems", "comments": "In Proceedings of SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing need for fast distributed processing of\nlarge-scale graphs such as the Web graph and various social networks, we study\na message-passing distributed computing model for graph processing and present\nlower bounds and algorithms for several graph problems. This work is inspired\nby recent large-scale graph processing systems (e.g., Pregel and Giraph) which\nare designed based on the message-passing model of distributed computing.\n  Our model consists of a point-to-point communication network of $k$ machines\ninterconnected by bandwidth-restricted links. Communicating data between the\nmachines is the costly operation (as opposed to local computation). The network\nis used to process an arbitrary $n$-node input graph (typically $n \\gg k > 1$)\nthat is randomly partitioned among the $k$ machines (a common implementation in\nmany real world systems). Our goal is to study fundamental complexity bounds\nfor solving graph problems in this model.\n  We present techniques for obtaining lower bounds on the distributed time\ncomplexity. Our lower bounds develop and use new bounds in random-partition\ncommunication complexity. We first show a lower bound of $\\Omega(n/k)$ rounds\nfor computing a spanning tree (ST) of the input graph. This result also implies\nthe same bound for other fundamental problems such as computing a minimum\nspanning tree (MST). We also show an $\\Omega(n/k^2)$ lower bound for\nconnectivity, ST verification and other related problems.\n  We give algorithms for various fundamental graph problems in our model. We\nshow that problems such as PageRank, MST, connectivity, and graph covering can\nbe solved in $\\tilde{O}(n/k)$ time, whereas for shortest paths, we present\nalgorithms that run in $\\tilde{O}(n/\\sqrt{k})$ time (for $(1+\\epsilon)$-factor\napprox.) and in $\\tilde{O}(n/k)$ time (for $O(\\log n)$-factor approx.)\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 05:14:48 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 16:44:54 GMT"}, {"version": "v3", "created": "Sat, 11 Oct 2014 18:13:20 GMT"}, {"version": "v4", "created": "Thu, 6 Nov 2014 05:23:40 GMT"}, {"version": "v5", "created": "Mon, 2 Feb 2015 08:28:47 GMT"}, {"version": "v6", "created": "Wed, 16 Sep 2015 17:18:40 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Klauck", "Hartmut", ""], ["Nanongkai", "Danupon", ""], ["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""]]}, {"id": "1311.6235", "submitter": "Tomasz Kociumaka", "authors": "Tomasz Kociumaka, Jakub Radoszewski, Wojciech Rytter, Tomasz Wale\\'n", "title": "Internal Pattern Matching Queries in a Text and Applications", "comments": "31 pages, 9 figures; accepted to SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several types of internal queries: questions about subwords of a\ntext. As the main tool we develop an optimal data structure for the problem\ncalled here internal pattern matching. This data structure provides\nconstant-time answers to queries about occurrences of one subword $x$ in\nanother subword $y$ of a given text, assuming that $|y|=\\mathcal{O}(|x|)$,\nwhich allows for a constant-space representation of all occurrences. This\nproblem can be viewed as a natural extension of the well-studied pattern\nmatching problem. The data structure has linear size and admits a linear-time\nconstruction algorithm.\n  Using the solution to the internal pattern matching problem, we obtain very\nefficient data structures answering queries about: primitivity of subwords,\nperiods of subwords, general substring compression, and cyclic equivalence of\ntwo subwords. All these results improve upon the best previously known\ncounterparts. The linear construction time of our data structure also allows to\nimprove the algorithm for finding $\\delta$-subrepetitions in a text (a more\ngeneral version of maximal repetitions, also called runs). For any fixed\n$\\delta$ we obtain the first linear-time algorithm, which matches the linear\ntime complexity of the algorithm computing runs. Our data structure has already\nbeen used as a part of the efficient solutions for subword suffix rank &\nselection, as well as substring compression using Burrows-Wheeler transform\ncomposed with run-length encoding.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 08:49:39 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 09:56:29 GMT"}, {"version": "v3", "created": "Tue, 26 Aug 2014 16:11:45 GMT"}, {"version": "v4", "created": "Mon, 13 Oct 2014 16:33:19 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1311.6543", "submitter": "Haibin Zhang", "authors": "Haibin Zhang (1), Yan Wang (1), Xiuzhen Zhang (2), Ee-Peng Lim (3),\n  ((1) Macquarie University, Sydney, Australia, (2) RMIT University, Melbourne,\n  Australia, (3) Singapore Management University, Singapore)", "title": "ReputationPro: The Efficient Approaches to Contextual Transaction Trust\n  Computation in E-Commerce Environments", "comments": "This paper has been withdrawn by the author due to the loss of some\n  figures", "journal-ref": "ACM Trans. Web 9, 1, Article 2 (January 2015)", "doi": "10.1145/2697390", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce environments, the trustworthiness of a seller is utterly\nimportant to potential buyers, especially when the seller is unknown to them.\nMost existing trust evaluation models compute a single value to reflect the\ngeneral trust level of a seller without taking any transaction context\ninformation into account. In this paper, we first present a trust vector\nconsisting of three values for Contextual Transaction Trust (CTT). In the\ncomputation of three CTT values, the identified three important context\ndimensions, including product category, transaction amount and transaction\ntime, are taken into account. In particular, with different parameters\nregarding context dimensions that are specified by a buyer, different sets of\nCTT values can be calculated. As a result, all these values can outline the\nreputation profile of a seller that indicates the dynamic trust levels of a\nseller in different product categories, price ranges, time periods, and any\nnecessary combination of them. We term this new model as ReputationPro.\nHowever, in ReputationPro, the computation of reputation profile requires novel\nalgorithms for the precomputation of aggregates over large-scale ratings and\ntransaction data of three context dimensions as well as new data structures for\nappropriately indexing aggregation results to promptly answer buyers' CTT\nrequests. To solve these challenging problems, we then propose a new index\nscheme CMK-tree. After that, we further extend CMK-tree and propose a\nCMK-treeRS approach to reducing the storage space allocated to each seller.\nFinally, the experimental results illustrate that the CMK-tree is superior in\nefficiency for computing CTT values to all three existing approaches in the\nliterature. In addition, though with reduced storage space, the CMK-treeRS\napproach can further improve the performance in answering buyers' CTT queries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 02:32:24 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 04:36:20 GMT"}, {"version": "v3", "created": "Mon, 5 May 2014 23:27:25 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Zhang", "Haibin", ""], ["Wang", "Yan", ""], ["Zhang", "Xiuzhen", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "1311.6615", "submitter": "Bartosz Rybicki", "authors": "Bartosz Rybicki and Jaroslaw Byrka", "title": "Improved approximation algorithm for Fault-Tolerant Facility Placement", "comments": "We modify one figure; fix a small problem with Lemma 5(iv); add one\n  refference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Fault-Tolerant Facility Placement problem ($FTFP$), which is\na generalization of the classical Uncapacitated Facility Location problem\n($UFL$). In the $FTFP$ problem we have a set of clients $C$ and a set of\nfacilities $F$. Each facility $i \\in F$ can be opened many times. For each\nopening of facility $i$ we pay $f_i \\geq 0$. Our goal is to connect each client\n$j \\in C$ with $r_j \\geq 1$ open facilities in a way that minimizes the total\ncost of open facilities and established connections.\n  In a series of recent papers $FTFP$ was essentially reduced to $FTFL$ and\nthen to $UFL$ showing it could be approximated with ratio $1.575$. In this\npaper we show that $FTFP$ can actually be approximated even better. We consider\napproximation ratio as a function of $r = min_{j \\in C} r_j$ (minimum\nrequirement of a client). With increasing $r$ the approximation ratio of our\nalgorithm $\\lambda_r$ converges to one. Furthermore, for $r > 1$ the value of\n$\\lambda_r$ is less than 1.463 (hardness of approximation of $UFL$). We also\nshow a lower bound of 1.278 for the approximability of the Fault-Tolerant\nFacility Location problem ($FTFL$) for arbitrary $r$. Already for $r > 3$ we\nobtain that $FTFP$ can be approximated with ratio 1.275, showing that under\nstandard complexity theoretic assumptions $FTFP$ is strictly better\napproximable than $FTFL$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 10:49:36 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 15:46:58 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Rybicki", "Bartosz", ""], ["Byrka", "Jaroslaw", ""]]}, {"id": "1311.6883", "submitter": "Chaitanya Swamy", "authors": "Hadi Minooei and Chaitanya Swamy", "title": "Near-Optimal and Robust Mechanism Design for Covering Problems with\n  Correlated Players", "comments": "Major changes compared to the previous version. Please consult this\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing incentive-compatible, ex-post\nindividually rational (IR) mechanisms for covering problems in the Bayesian\nsetting, where players' types are drawn from an underlying distribution and may\nbe correlated, and the goal is to minimize the expected total payment made by\nthe mechanism. We formulate a notion of incentive compatibility (IC) that we\ncall {\\em support-based IC} that is substantially more robust than Bayesian IC,\nand develop black-box reductions from support-based-IC mechanism design to\nalgorithm design. For single-dimensional settings, this black-box reduction\napplies even when we only have an LP-relative {\\em approximation algorithm} for\nthe algorithmic problem. Thus, we obtain near-optimal mechanisms for various\ncovering settings including single-dimensional covering problems, multi-item\nprocurement auctions, and multidimensional facility location.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 07:20:32 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 04:57:27 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Minooei", "Hadi", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1311.7011", "submitter": "Yasset Perez-Riverol PhD", "authors": "Yasset Perez-Riverol and Roberto Vera Alvarez", "title": "A UML-based Approach to Design Parallel and Distributed Applications", "comments": "5 Figures, Work presented in two conferences and related with the\n  design of a Parallel Program for Conformational Search in small molecules\n  (PMID: 23030613)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Parallel and distributed application design is a major area of interest in\nthe domain of high performance scientific and industrial computing. Over the\nyears, various approaches have been proposed to aid parallel program developers\nto modeling their applications. In this paper it will be used some concepts\nfrom agile development methodologies and Unified Modeling Language (UML) to\nmodeling parallel and distributed applications. The UML-based approach of this\npaper describes through different artifacts and graphs the main flows of events\nin the development of parallel and high performance applications. Here, we\npresented three work flows to describe and to model our parallel program,\nDomain Model, Design and Modeling and Test. All these phases of the development\nsoftware allow to programmers convert the requirements of the problem in a good\nand efficient solution.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 15:41:22 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Perez-Riverol", "Yasset", ""], ["Alvarez", "Roberto Vera", ""]]}, {"id": "1311.7178", "submitter": "Anindya De", "authors": "Anindya De, Rocco Servedio", "title": "Efficient deterministic approximate counting for low-degree polynomial\n  threshold functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic algorithm for approximately counting satisfying\nassignments of a degree-$d$ polynomial threshold function (PTF). Given a\ndegree-$d$ input polynomial $p(x_1,\\dots,x_n)$ over $R^n$ and a parameter\n$\\epsilon> 0$, our algorithm approximates $\\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq\n0]$ to within an additive $\\pm \\epsilon$ in time $O_{d,\\epsilon}(1)\\cdot\n\\mathop{poly}(n^d)$. (Any sort of efficient multiplicative approximation is\nimpossible even for randomized algorithms assuming $NP\\not=RP$.) Note that the\nrunning time of our algorithm (as a function of $n^d$, the number of\ncoefficients of a degree-$d$ PTF) is a \\emph{fixed} polynomial. The fastest\nprevious algorithm for this problem (due to Kane), based on constructions of\nunconditional pseudorandom generators for degree-$d$ PTFs, runs in time\n$n^{O_{d,c}(1) \\cdot \\epsilon^{-c}}$ for all $c > 0$.\n  The key novel contributions of this work are: A new multivariate central\nlimit theorem, proved using tools from Malliavin calculus and Stein's Method.\nThis new CLT shows that any collection of Gaussian polynomials with small\neigenvalues must have a joint distribution which is very close to a\nmultidimensional Gaussian distribution. A new decomposition of low-degree\nmultilinear polynomials over Gaussian inputs. Roughly speaking we show that (up\nto some small error) any such polynomial can be decomposed into a bounded\nnumber of multilinear polynomials all of which have extremely small\neigenvalues. We use these new ingredients to give a deterministic algorithm for\na Gaussian-space version of the approximate counting problem, and then employ\nstandard techniques for working with low-degree PTFs (invariance principles and\nregularity lemmas) to reduce the original approximate counting problem over the\nBoolean hypercube to the Gaussian version.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 00:00:59 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["De", "Anindya", ""], ["Servedio", "Rocco", ""]]}, {"id": "1311.7201", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Shiladitya Munshi, Ayan Chakraborty, Debajyoti Mukhopadhyay", "title": "Theories of Hypergraph-Graph (HG(2)) Data Structure", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current paper introduces a Hypergraph Graph model of data storage which can\nbe represented as a hybrid data structure based on Hypergraph and Graph. The\npro-posed data structure is claimed to realize complex combinatorial\nstructures. The formal definition of the data structure is presented along with\nthe proper justification from real world scenarios. The paper reports some\nelementary concepts of Hypergraph and presents theoretical aspects of the\nproposed data structure including the concepts of Path, Cycle etc. The detailed\nanalysis of weighted HG 2 is presented along with discussions on Cost involved\nwith HG 2 paths.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 04:12:17 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Munshi", "Shiladitya", ""], ["Chakraborty", "Ayan", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1311.7202", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Shiladitya Munshi, Ayan Chakraborty, Debajyoti Mukhopadhyay", "title": "Integrating RDF into Hypergraph-Graph (HG(2)) Data Structure", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current paper discusses the methodologies involved in integrating Resource\nDescription Framework into a HyperGraph Graph HG 2 data structure in order to\npreserve the semantics of the information contained in RDF document for dealing\nfuture cross platform information portability issues. The entire semantic web\nis mostly dominated by few information frameworks like RDF, Topic Map, OWL etc.\nHence semantic web currently faces the problem of non existence of common\ninformation meta-model which can integrate them all for ex-panded semantic\nsearch. On the background of development of Hyper Graph Graph HG 2 data\nstructure, an RDF document if integrated to it, maintains the original\nsemantics and exposes some critical semantic and object mapping lift as well\nwhich could further be exploited for semantic search and information\ntransitional problems. The focus of the paper is to present the mapping\nconstructs between RDF elements and HyperGraph Graph HG 2 elements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 04:12:48 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Munshi", "Shiladitya", ""], ["Chakraborty", "Ayan", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1311.7357", "submitter": "Shahin Kamali", "authors": "Joan Boyar, Shahin Kamali, Kim S. Larsen, Alejandro L\\'opez-Ortiz", "title": "On the List Update Problem with Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online list update problem under the advice model of\ncomputation. Under this model, an online algorithm receives partial information\nabout the unknown parts of the input in the form of some bits of advice\ngenerated by a benevolent offline oracle. We show that advice of linear size is\nrequired and sufficient for a deterministic algorithm to achieve an optimal\nsolution or even a competitive ratio better than $15/14$. On the other hand, we\nshow that surprisingly two bits of advice are sufficient to break the lower\nbound of $2$ on the competitive ratio of deterministic online algorithms and\nachieve a deterministic algorithm with a competitive ratio of $5/3$. In this\nupper-bound argument, the bits of advice determine the algorithm with smaller\ncost among three classical online algorithms, TIMESTAMP and two members of the\nMTF2 family of algorithms. We also show that MTF2 algorithms are\n$2.5$-competitive.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 16:25:17 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 22:48:31 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 22:13:05 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Boyar", "Joan", ""], ["Kamali", "Shahin", ""], ["Larsen", "Kim S.", ""], ["L\u00f3pez-Ortiz", "Alejandro", ""]]}, {"id": "1311.7369", "submitter": "Christian Lavault", "authors": "Christian Lavault (LIPN), Sidi Mohamed Sedjelmaci (LIPN)", "title": "Worst-Case Analysis of Weber's Algorithm", "comments": "11 pages", "journal-ref": "Information Processing Letters 72, 3-4 (1999) 125-130", "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Ken Weber introduced an algorithm for finding the $(a,b)$-pairs\nsatisfying $au+bv\\equiv 0\\pmod{k}$, with $0<|a|,|b|<\\sqrt{k}$, where $(u,k)$\nand $(v,k)$ are coprime. It is based on Sorenson's and Jebelean's \"$k$-ary\nreduction\" algorithms. We provide a formula for $N(k)$, the maximal number of\niterations in the loop of Weber's GCD algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 16:52:43 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Lavault", "Christian", "", "LIPN"], ["Sedjelmaci", "Sidi Mohamed", "", "LIPN"]]}, {"id": "1311.7589", "submitter": "Marc Renault", "authors": "Marc P. Renault, Adi Ros\\'en and Rob van Stee", "title": "Online Algorithms with Advice for Bin Packing and Scheduling Problems", "comments": "20 pages", "journal-ref": null, "doi": "10.1016/j.tcs.2015.07.050", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of online computation with advice, and study the bin\npacking problem and a number of scheduling problems. We show that it is\npossible, for any of these problems, to arbitrarily approach a competitive\nratio of $1$ with only a constant number of bits of advice per request. For the\nbin packing problem, we give an online algorithm with advice that is\n$(1+\\varepsilon)$-competitive and uses $O\\left(\\frac{1}{\\varepsilon}\\log\n\\frac{1}{\\varepsilon} \\right)$ bits of advice per request. For scheduling on\n$m$ identical machines, with the objective function of any of makespan, machine\ncovering and the minimization of the $\\ell_p$ norm, $p >1$, we give similar\nresults. We give online algorithms with advice which are\n$(1+\\varepsilon)$-competitive ($(1/(1-\\varepsilon))$-competitive for machine\ncovering) and also use $O\\left(\\frac{1}{\\varepsilon}\\log \\frac{1}{\\varepsilon}\n\\right)$ bits of advice per request. We complement our results by giving a\nlower bound showing that for any online algorithm with advice to be optimal,\nfor any of the above scheduling problems, a non-constant number (namely, at\nleast $\\left(1 - \\frac{2m}{n}\\right)\\log m$, where $n$ is the number of jobs\nand $m$ is the number of machines) of bits of advice per request is needed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 15:02:30 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2015 13:16:39 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Renault", "Marc P.", ""], ["Ros\u00e9n", "Adi", ""], ["van Stee", "Rob", ""]]}, {"id": "1311.7631", "submitter": "Leslie Ann Goldberg", "authors": "Josep Diaz and Leslie Ann Goldberg and David Richerby and Maria Serna", "title": "Absorption Time of the Moran Process", "comments": "minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Moran process models the spread of mutations in populations on graphs. We\ninvestigate the absorption time of the process, which is the time taken for a\nmutation introduced at a randomly chosen vertex to either spread to the whole\npopulation, or to become extinct. It is known that the expected absorption time\nfor an advantageous mutation is O(n^4) on an n-vertex undirected graph, which\nallows the behaviour of the process on undirected graphs to be analysed using\nthe Markov chain Monte Carlo method. We show that this does not extend to\ndirected graphs by exhibiting an infinite family of directed graphs for which\nthe expected absorption time is exponential in the number of vertices. However,\nfor regular directed graphs, we show that the expected absorption time is\nOmega(n log n) and O(n^2). We exhibit families of graphs matching these bounds\nand give improved bounds for other families of graphs, based on isoperimetric\nnumber. Our results are obtained via stochastic dominations which we\ndemonstrate by establishing a coupling in a related continuous-time model. The\ncoupling also implies several natural domination results regarding the fixation\nprobability of the original (discrete-time) process, resolving a conjecture of\nShakarian, Roos and Johnson.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 16:53:32 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 17:53:35 GMT"}, {"version": "v3", "created": "Fri, 12 Sep 2014 10:51:58 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Diaz", "Josep", ""], ["Goldberg", "Leslie Ann", ""], ["Richerby", "David", ""], ["Serna", "Maria", ""]]}]