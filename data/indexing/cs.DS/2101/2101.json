[{"id": "2101.00061", "submitter": "Simina Br\\^anzei", "authors": "Simina Br\\^anzei and Jiawei Li", "title": "The Query Complexity of Local Search and Brouwer in Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the query complexity of local search and Brouwer fixed-point\ncomputation when there are $k$ rounds of interaction with the oracle that\nanswers queries. Thus in each round, any number of simultaneous queries can be\nissued, the oracle answers are received, and then the queries for the next\nround are submitted. The algorithm must stop by the end of the $k$-th round.\nThis model captures distributed settings, where each query is time consuming\nand can be executed by a separate processor to speed up computation time.\n  We present several new algorithms and lower bounds, which characterize the\ntrade-off between the number of rounds of adaptivity and the total number of\nqueries on local search and Brouwer fixed-point. We mainly focus on studying\nthese problems on the $d$-dimensional grid $[n]^d$, where $d$ is a constant.\n  For local search, if the number of rounds $k$ is a constant, then we obtain a\nquery complexity of $\\Theta\\bigl(n^{\\frac{d^{k+1} - d^k}{d^k - 1}}\\bigl)$ for\nboth deterministic and randomized algorithms. On the other hand, when the\nnumber of rounds is polynomial, i.e. of the form $n = k^{\\alpha}$, where\n$\\alpha$ is a constant with $0 < \\alpha < d/2$, we obtain an upper bound of\n$O\\left(n^{(d-1) - \\frac{d-2}{d}\\alpha}\\right)$ and a lower bound of\n$\\widetilde{\\Omega}\\bigl(\\max(n^{(d-1)-\\alpha}, n^{\\frac{d}{2}})\\bigr)$ for\nrandomized algorithms. For Brouwer fixed-point, we have query complexity of\n$\\Theta\\bigl(n^{\\frac{d^{k+1} - d^k}{d^k - 1}}\\bigl)$ for both deterministic\nand randomized algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:27:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Br\u00e2nzei", "Simina", ""], ["Li", "Jiawei", ""]]}, {"id": "2101.00101", "submitter": "Leonid A. Levin", "authors": "Leonid A. Levin", "title": "Climbing LP Algorithms", "comments": "2 pages. Significant additions", "journal-ref": "STOC 2021", "doi": "10.1145/3406325.3457137", "report-no": null, "categories": "cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  NP (search) problems allow easy correctness tests for solutions. Climbing\nalgorithms allow also easy assessment of how close to yielding the correct\nanswer is the configuration at any stage of their run. This offers a great\nflexibility, as how sensible is any deviation from the standard procedures can\nbe instantly assessed.\n  An example is the Dual Matrix Algorithm (DMA) for linear programming,\nvariations of which were considered by A.Y. Levin in 1965 and by Yamnitsky and\nmyself in 1982. It has little sensitivity to numerical errors and to the number\nof inequalities. It offers substantial flexibility and, thus, potential for\nfurther developments.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 22:45:44 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 18:34:52 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Levin", "Leonid A.", ""]]}, {"id": "2101.00172", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Chunk List: Concurrent Data Structures", "comments": "20 pages, 3 figures A full implementation can be found at\n  https://github.com/danielathome19/Chunk-List Update: Revised format to align\n  closer to IEEE standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chunking data is obviously no new concept; however, I had never found any\ndata structures that used chunking as the basis of their implementation. I\nfigured that by using chunking alongside concurrency, I could create an\nextremely fast run-time in regards to particular methods as searching and/or\nsorting. By using chunking and concurrency to my advantage, I came up with the\nchunk list - a dynamic list-based data structure that would separate large\namounts of data into specifically sized chunks, each of which should be able to\nbe searched at the exact same time by searching each chunk on a separate\nthread. As a result of implementing this concept into its own class, I was able\nto create something that almost consistently gives around 20x-300x faster\nresults than a regular ArrayList. However, should speed be a particular issue\neven after implementation, users can modify the size of the chunks and\nbenchmark the speed of using smaller or larger chunks, depending on the amount\nof data being stored.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:45:56 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:32:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00211", "submitter": "Ryan Mann", "authors": "Ryan L. Mann", "title": "Simulating Quantum Computations with Tutte Polynomials", "comments": "13 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a classical heuristic algorithm for exactly computing quantum\nprobability amplitudes. Our algorithm is based on mapping output probability\namplitudes of quantum circuits to evaluations of the Tutte polynomial of\ngraphic matroids. The algorithm evaluates the Tutte polynomial recursively\nusing the deletion-contraction property while attempting to exploit structural\nproperties of the matroid. We consider several variations of our algorithm and\npresent experimental results comparing their performance on two classes of\nrandom quantum circuits. Further, we obtain an explicit form for Clifford\ncircuit amplitudes in terms of matroid invariants and an alternative efficient\nclassical algorithm for computing the output probability amplitudes of Clifford\ncircuits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 11:11:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mann", "Ryan L.", ""]]}, {"id": "2101.00314", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "SetSketch: Filling the Gap between MinHash and HyperLogLog", "comments": "extended version of major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and HyperLogLog are sketching algorithms that have become\nindispensable for set summaries in big data applications. While HyperLogLog\nallows counting different elements with very little space, MinHash is suitable\nfor the fast comparison of sets as it allows estimating the Jaccard similarity\nand other joint quantities. This work presents a new data structure called\nSetSketch that is able to continuously fill the gap between both use cases. Its\ncommutative and idempotent insert operation and its mergeable state make it\nsuitable for distributed environments. Fast, robust, and easy-to-implement\nestimators for cardinality and joint quantities, as well as the ability to use\nSetSketch for similarity search, enable versatile applications. The presented\njoint estimator can also be applied to other data structures such as MinHash,\nHyperLogLog, or HyperMinHash, where it even performs better than the\ncorresponding state-of-the-art estimators in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 20:14:33 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 07:23:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "2101.00326", "submitter": "Joseph S. B. Mitchell", "authors": "Joseph S. B. Mitchell", "title": "Approximating Maximum Independent Set for Rectangles in the Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a polynomial-time constant-factor approximation algorithm for maximum\nindependent set for (axis-aligned) rectangles in the plane. Using a\npolynomial-time algorithm, the best approximation factor previously known is\n$O(\\log\\log n)$. The results are based on a new form of recursive partitioning\nin the plane, in which faces that are constant-complexity and orthogonally\nconvex are recursively partitioned into a constant number of such faces.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 22:30:26 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:37:11 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 02:58:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mitchell", "Joseph S. B.", ""]]}, {"id": "2101.00579", "submitter": "Tom Demeulemeester", "authors": "Tom Demeulemeester, Dries Goossens, Ben Hermans, Roel Leus", "title": "Risk aversion in one-sided matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by real-world applications such as the assignment of pupils to\nschools or the allocation of social housing, the one-sided matching problem\nstudies how a set of agents can be assigned to a set of objects when the agents\nhave preferences over the objects, but not vice versa. For fairness reasons,\nmost mechanisms use randomness, and therefore result in a probabilistic\nassignment. We study the problem of decomposing these probabilistic assignments\ninto a weighted sum of ex-post (Pareto-)efficient matchings, while maximizing\nthe worst-case number of assigned agents. This decomposition preserves all the\nassignments' desirable properties, most notably strategy-proofness. For a\nspecific class of probabilistic assignments, including the assignment by the\nProbabilistic Serial mechanism, we propose a polynomial-time algorithm for this\nproblem that obtains a decomposition in which all matchings assign at least the\nexpected number of assigned agents by the probabilistic assignment, rounded\ndown, thus achieving the theoretically best possible guarantee. For general\nprobabilistic assignments, the problem becomes NP-hard. For the Random Serial\nDictatorship (RSD) mechanism, we show that the worst-case number of assigned\nagents by RSD is at least half of the optimal, and that this bound is\nasymptotically tight. Lastly, we propose a column generation framework for the\nintroduced problem, which we evaluate both on randomly generated data, and on\nreal-world school choice data from the Belgian cities Antwerp and Ghent.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:22:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Demeulemeester", "Tom", ""], ["Goossens", "Dries", ""], ["Hermans", "Ben", ""], ["Leus", "Roel", ""]]}, {"id": "2101.00694", "submitter": "Hauke Brinkop", "authors": "Hauke Brinkop, Klaus Jansen, Tim Wei{\\ss}enfels", "title": "An optimal FPT algorithm parametrized by treewidth for\n  Weighted-Max-Bisection given a tree decomposition as advice assuming SETH and\n  the hardness of MinConv", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted maximal bisection problem is, given an edge weighted graph, to\nfind a bipartition of the vertex set into two sets such that their cardinality\ndiffers by at most one and the sum of the weight of the edges between vertices\nthat are not in the same set is maximized. This problem is known to be NP-hard,\neven when a tree decomposition of width $t$ and $\\mathcal O(n)$ nodes is given\nas an advice as part of the input, where $n$ is the number of vertices of the\ninput graph. But, given such an advice, the problem is decidable in FPT time in\n$n$ parametrized by $t$. In particular Jansen et al. presented an algorithm\nwith running time $\\mathcal O(2^tn^3)$. Hanaka, Kobayashi, and Sone enhanced\nthe analysis of the complexity to $\\mathcal O(2^t(nt)^2)$. By slightly\nmodifying the approach, we improve the running time to $\\mathcal O(2^tn^2)$ in\nthe RAM model, which is asymptotically optimal in $n$ under the hardness of\nMinConv.\n  We proof that this is also asymptotically optimal in its dependence on $t$\nassuming SETH by showing for a slightly easier problem (maximal cut) that there\nis no $\\mathcal O(2^{\\epsilon t} \\operatorname{poly} n)$ algorithm for any\n$\\varepsilon < 1$ under SETH. This was already claimed by Hanaka, Kobayashi,\nand Sone but without a correct proof.\n  We also present a hardness result (no $\\mathcal O(2^t n^{2-\\varepsilon})$\nalgorithm for any $\\varepsilon > 0$) for a broad family of subclasses of the\nweighted maximal bisection problem that are characterized only by the\ndependence of $t$ from $n$, more precisely, all instances with $t = f(n)$ for\nan arbitrary but fixed $f(n) \\in \\operatorname o(\\log n)$. This holds even when\nonly considering planar graphs.\n  Moreover we present a detailed description of the implementation details and\nassumptions that are necessary to achieve the optimal running time.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:00:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Brinkop", "Hauke", ""], ["Jansen", "Klaus", ""], ["Wei\u00dfenfels", "Tim", ""]]}, {"id": "2101.00711", "submitter": "Amirbehshad Shahrasbi", "authors": "Bernhard Haeupler and Amirbehshad Shahrasbi", "title": "Synchronization Strings and Codes for Insertions and Deletions -- a\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Already in the 1960s, Levenshtein and others studied error-correcting codes\nthat protect against synchronization errors, such as symbol insertions and\ndeletions. However, despite significant efforts, progress on designing such\ncodes has been lagging until recently, particularly compared to the detailed\nunderstanding of error-correcting codes for symbol substitution or erasure\nerrors. This paper surveys the recent progress in designing efficient\nerror-correcting codes over finite alphabets that can correct a constant\nfraction of worst-case insertions and deletions.\n  Most state-of-the-art results for such codes rely on synchronization strings,\nsimple yet powerful pseudo-random objects that have proven to be very effective\nsolutions for coping with synchronization errors in various settings. This\nsurvey also includes an overview of what is known about synchronization strings\nand discusses communication settings related to error-correcting codes in which\nsynchronization strings have been applied.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 22:11:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Shahrasbi", "Amirbehshad", ""]]}, {"id": "2101.00718", "submitter": "Simone Faro", "authors": "Domenico Cantone, Simone Faro and Arianna Pavone", "title": "Text Searching Allowing for Non-Overlapping Adjacent Unbalanced\n  Translocations", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.00421", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the \\emph{approximate string matching problem}\nwhen the allowed edit operations are \\emph{non-overlapping unbalanced\ntranslocations of adjacent factors}. Such kind of edit operations take place\nwhen two adjacent sub-strings of the text swap, resulting in a modified string.\nThe two involved substrings are allowed to be of different lengths.\n  Such large-scale modifications on strings have various applications. They are\namong the most frequent chromosomal alterations, accounted for 30\\% of all\nlosses of heterozygosity, a major genetic event causing inactivation of cancer\nsuppressor genes. In addition, among other applications, they are frequent\nmodifications accounted in musical or in natural language information\nretrieval. However, despite of their central role in so many fields of text\nprocessing, little attention has been devoted to the problem of matching\nstrings allowing for this kind of edit operation.\n  In this paper we present three algorithms for solving the problem, all of\nthem with a $\\bigO(nm^3)$ worst-case and a $\\bigO(m^2)$-space complexity, where\n$m$ and $n$ are the length of the pattern and of the text, respectively. % In\nparticular, our first algorithm is based on the dynamic-programming approach.\nOur second solution improves the previous one by making use of the Directed\nAcyclic Word Graph of the pattern. Finally our third algorithm is based on an\nalignment procedure. We also show that under the assumptions of equiprobability\nand independence of characters, our second algorithm has a\n$\\bigO(n\\log^2_{\\sigma} m)$ average time complexity, for an alphabet of size\n$\\sigma \\geq 4$.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 22:21:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cantone", "Domenico", ""], ["Faro", "Simone", ""], ["Pavone", "Arianna", ""]]}, {"id": "2101.01100", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Enric Boix-Adsera", "title": "Wasserstein barycenters are NP-hard to compute", "comments": "18 pages (9 pages main text)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing Wasserstein barycenters (a.k.a. Optimal Transport\nbarycenters) has attracted considerable recent attention due to many\napplications in data science. While there exist polynomial-time algorithms in\nany fixed dimension, all known runtimes suffer exponentially in the dimension.\nIt is an open question whether this exponential dependence is improvable to a\npolynomial dependence. This paper proves that unless P=NP, the answer is no.\nThis uncovers a \"curse of dimensionality\" for Wasserstein barycenter\ncomputation which does not occur for Optimal Transport computation. Moreover,\nour hardness results for computing Wasserstein barycenters extend to\napproximate computation, to seemingly simple cases of the problem, and to\naveraging probability distributions in other Optimal Transport metrics.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:16:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Boix-Adsera", "Enric", ""]]}, {"id": "2101.01108", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Binary Dynamic Time Warping in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic time warping distance (DTW) is a widely used distance measure between\ntime series $x, y \\in \\Sigma^n$. It was shown by Abboud, Backurs, and Williams\nthat in the \\emph{binary case}, where $|\\Sigma| = 2$, DTW can be computed in\ntime $O(n^{1.87})$. We improve this running time $O(n)$.\n  Moreover, if $x$ and $y$ are run-length encoded, then there is an algorithm\nrunning in time $\\tilde{O}(k + \\ell)$, where $k$ and $\\ell$ are the number of\nruns in $x$ and $y$, respectively. This improves on the previous best bound of\n$O(k\\ell)$ due to Dupont and Marteau.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:32:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "2101.01137", "submitter": "Haim Avron", "authors": "Paz Fink Shustin, Haim Avron", "title": "Gauss-Legendre Features for Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes provide a powerful probabilistic kernel learning\nframework, which allows learning high quality nonparametric regression models\nvia methods such as Gaussian process regression. Nevertheless, the learning\nphase of Gaussian process regression requires massive computations which are\nnot realistic for large datasets. In this paper, we present a Gauss-Legendre\nquadrature based approach for scaling up Gaussian process regression via a low\nrank approximation of the kernel matrix. We utilize the structure of the low\nrank approximation to achieve effective hyperparameter learning, training and\nprediction. Our method is very much inspired by the well-known random Fourier\nfeatures approach, which also builds low-rank approximations via numerical\nintegration. However, our method is capable of generating high quality\napproximation to the kernel using an amount of features which is\npoly-logarithmic in the number of training points, while similar guarantees\nwill require an amount that is at the very least linear in the number of\ntraining points when random Fourier features. Furthermore, the structure of the\nlow-rank approximation that our method builds is subtly different from the one\ngenerated by random Fourier features, and this enables much more efficient\nhyperparameter learning. The utility of our method for learning with\nlow-dimensional datasets is demonstrated using numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:09:25 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 13:30:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Shustin", "Paz Fink", ""], ["Avron", "Haim", ""]]}, {"id": "2101.01146", "submitter": "Arnold Filtser", "authors": "Arnold Filtser and Hung Le", "title": "Clan Embeddings into Trees, and Low Treewidth Graphs", "comments": "To appear in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In low distortion metric embeddings, the goal is to embed a host \"hard\"\nmetric space into a \"simpler\" target space while approximately preserving\npairwise distances. A highly desirable target space is that of a tree metric.\nUnfortunately, such embedding will result in a huge distortion. A celebrated\nbypass to this problem is stochastic embedding with logarithmic expected\ndistortion. Another bypass is Ramsey-type embedding, where the distortion\nguarantee applies only to a subset of the points. However, both these solutions\nfail to provide an embedding into a single tree with a worst-case distortion\nguarantee on all pairs. In this paper, we propose a novel third bypass called\n\\emph{clan embedding}. Here each point $x$ is mapped to a subset of points\n$f(x)$, called a \\emph{clan}, with a special \\emph{chief} point $\\chi(x)\\in\nf(x)$. The clan embedding has multiplicative distortion $t$ if for every pair\n$(x,y)$ some copy $y'\\in f(y)$ in the clan of $y$ is close to the chief of $x$:\n$\\min_{y'\\in f(y)}d(y',\\chi(x))\\le t\\cdot d(x,y)$. Our first result is a clan\nembedding into a tree with multiplicative distortion $O(\\frac{\\log\nn}{\\epsilon})$ such that each point has $1+\\epsilon$ copies (in expectation).\nIn addition, we provide a \"spanning\" version of this theorem for graphs and use\nit to devise the first compact routing scheme with constant size routing\ntables.\n  We then focus on minor-free graphs of diameter prameterized by $D$, which\nwere known to be stochastically embeddable into bounded treewidth graphs with\nexpected additive distortion $\\epsilon D$. We devise Ramsey-type embedding and\nclan embedding analogs of the stochastic embedding. We use these embeddings to\nconstruct the first (bicriteria quasi-polynomial time) approximation scheme for\nthe metric $\\rho$-dominating set and metric $\\rho$-independent set problems in\nminor-free graphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:24:52 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:57:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Filtser", "Arnold", ""], ["Le", "Hung", ""]]}, {"id": "2101.01509", "submitter": "Stefan Tiegel", "authors": "David Steurer, Stefan Tiegel", "title": "SoS Degree Reduction with Applications to Clustering and Robust Moment\n  Estimation", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework to significantly reduce the degree of\nsum-of-squares proofs by introducing new variables. To illustrate the power of\nthis framework, we use it to speed up previous algorithms based on\nsum-of-squares for two important estimation problems, clustering and robust\nmoment estimation. The resulting algorithms offer the same statistical\nguarantees as the previous best algorithms but have significantly faster\nrunning times. Roughly speaking, given a sample of $n$ points in dimension $d$,\nour algorithms can exploit order-$\\ell$ moments in time $d^{O(\\ell)}\\cdot\nn^{O(1)}$, whereas a naive implementation requires time $(d\\cdot n)^{O(\\ell)}$.\nSince for the aforementioned applications, the typical sample size is\n$d^{\\Theta(\\ell)}$, our framework improves running times from $d^{O(\\ell^2)}$\nto $d^{O(\\ell)}$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:49:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Steurer", "David", ""], ["Tiegel", "Stefan", ""]]}, {"id": "2101.01576", "submitter": "Lucas Murtinho", "authors": "Eduardo Laber, Lucas Murtinho", "title": "On the price of explainability for some clustering problems", "comments": "23 pages, 2 figures; general revision of text + added section on\n  practical algorithm (with experimental results) for the $k$-means problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The price of explainability for a clustering task can be defined as the\nunavoidable loss,in terms of the objective function, if we force the final\npartition to be explainable.\n  Here, we study this price for the following clustering problems: $k$-means,\n$k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds\nfor a natural model where explainability is achieved via decision trees. For\nthe $k$-means and $k$-medians problems our upper bounds improve those obtained\nby [Moshkovitz et. al, ICML 20] for low dimensions.\n  Another contribution is a simple and efficient algorithm for building\nexplainable clusterings for the $k$-means problem. We provide empirical\nevidence that its performance is better than the current state of the art for\ndecision-tree based explainable clustering.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:08:25 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 14:39:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Laber", "Eduardo", ""], ["Murtinho", "Lucas", ""]]}, {"id": "2101.01631", "submitter": "Pierre Perrault", "authors": "Pierre Perrault, Jennifer Healey, Zheng Wen, Michal Valko", "title": "On the Approximation Relationship between Optimizing Ratio of Submodular\n  (RS) and Difference of Submodular (DS) Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that from an algorithm guaranteeing an approximation factor\nfor the ratio of submodular (RS) optimization problem, we can build another\nalgorithm having a different kind of approximation guarantee -- weaker than the\nclassical one -- for the difference of submodular (DS) optimization problem,\nand vice versa. We also illustrate the link between these two problems by\nanalyzing a \\textsc{Greedy} algorithm which approximately maximizes objective\nfunctions of the form $\\Psi(f,g)$, where $f,g$ are two non-negative, monotone,\nsubmodular functions and $\\Psi$ is a {quasiconvex} 2-variables function, which\nis non decreasing with respect to the first variable. For the choice\n$\\Psi(f,g)\\triangleq f/g$, we recover RS, and for the choice\n$\\Psi(f,g)\\triangleq f-g$, we recover DS. To the best of our knowledge, this\ngreedy approach is new for DS optimization. For RS optimization, it reduces to\nthe standard \\textsc{GreedRatio} algorithm that has already been analyzed\npreviously. However, our analysis is novel for this case.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:29:21 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Perrault", "Pierre", ""], ["Healey", "Jennifer", ""], ["Wen", "Zheng", ""], ["Valko", "Michal", ""]]}, {"id": "2101.01719", "submitter": "Jim Apple", "authors": "Jim Apple", "title": "Split block Bloom filters", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note describes a Bloom filter variant that takes advantage of\nmodern SIMD instructions to increase speed by 30%-450%. This filter, the split\nblock Bloom filter, is used by Apache Impala, Apache Kudu, Apache Parquet, and\nApache Arrow.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:35:40 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 15:21:51 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 03:08:06 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Apple", "Jim", ""]]}, {"id": "2101.01739", "submitter": "Aaron Roth", "authors": "Varun Gupta, Christopher Jung, Georgy Noarov, Mallesh M. Pai, Aaron\n  Roth", "title": "Online Multivalid Learning: Means, Moments, and Prediction Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general, efficient technique for providing contextual\npredictions that are \"multivalid\" in various senses, against an online sequence\nof adversarially chosen examples $(x,y)$. This means that the resulting\nestimates correctly predict various statistics of the labels $y$ not just\nmarginally -- as averaged over the sequence of examples -- but also\nconditionally on $x \\in G$ for any $G$ belonging to an arbitrary intersecting\ncollection of groups $\\mathcal{G}$.\n  We provide three instantiations of this framework. The first is mean\nprediction, which corresponds to an online algorithm satisfying the notion of\nmulticalibration from Hebert-Johnson et al. The second is variance and higher\nmoment prediction, which corresponds to an online algorithm satisfying the\nnotion of mean-conditioned moment multicalibration from Jung et al. Finally, we\ndefine a new notion of prediction interval multivalidity, and give an algorithm\nfor finding prediction intervals which satisfy it. Because our algorithms\nhandle adversarially chosen examples, they can equally well be used to predict\nstatistics of the residuals of arbitrary point prediction methods, giving rise\nto very general techniques for quantifying the uncertainty of predictions of\nblack box algorithms, even in an online adversarial setting. When instantiated\nfor prediction intervals, this solves a similar problem as conformal\nprediction, but in an adversarial environment and with multivalidity guarantees\nstronger than simple marginal coverage guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:08:11 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Gupta", "Varun", ""], ["Jung", "Christopher", ""], ["Noarov", "Georgy", ""], ["Pai", "Mallesh M.", ""], ["Roth", "Aaron", ""]]}, {"id": "2101.01945", "submitter": "Markus Schmid", "authors": "Katrin Casel and Markus L. Schmid", "title": "Fine-Grained Complexity of Regular Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regular path query (RPQ) is a regular expression q that returns all node\npairs (u, v) from a graph database that are connected by an arbitrary path\nlabelled with a word from L(q). The obvious algorithmic approach to\nRPQ-evaluation (called PG-approach), i.e., constructing the product graph\nbetween an NFA for q and the graph database, is appealing due to its simplicity\nand also leads to efficient algorithms. However, it is unclear whether the\nPG-approach is optimal. We address this question by thoroughly investigating\nwhich upper complexity bounds can be achieved by the PG-approach, and we\ncomplement these with conditional lower bounds (in the sense of the\nfine-grained complexity framework). A special focus is put on enumeration and\ndelay bounds, as well as the data complexity perspective. A main insight is\nthat we can achieve optimal (or near optimal) algorithms with the PG-approach,\nbut the delay for enumeration is rather high (linear in the database). We\nexplore three successful approaches towards enumeration with sub-linear delay:\nsuper-linear preprocessing, approximations of the solution sets, and restricted\nclasses of RPQs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 10:07:16 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Casel", "Katrin", ""], ["Schmid", "Markus L.", ""]]}, {"id": "2101.02003", "submitter": "Joshua Lau", "authors": "Joshua Lau and Angus Ritossa", "title": "Algorithms and Hardness for Multidimensional Range Updates and Queries", "comments": "38 pages, 3 figures, 1 table. Full version of paper to appear in ITCS\n  2021. Abstract abridged for arXiv limits", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2021.37", "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional orthogonal range problems allow queries over a static set of\npoints, each with some value. Dynamic variants allow points to be added or\nremoved, one at a time. To support more powerful updates, we introduce the Grid\nRange class of data structure problems over integer arrays in one or more\ndimensions. These problems allow range updates (such as filling all cells in a\nrange with a constant) and queries (such as finding the sum or maximum of\nvalues in a range). In this work, we consider these operations along with\nupdates that replace each cell in a range with the minimum, maximum, or sum of\nits existing value, and a constant. In one dimension, it is known that segment\ntrees can be leveraged to facilitate any $n$ of these operations in\n$\\tilde{O}(n)$ time overall. Other than a few specific cases, until now, higher\ndimensional variants have been largely unexplored.\n  We show that no truly subquadratic time algorithm can support certain pairs\nof these updates simultaneously without falsifying several popular conjectures.\nOn the positive side, we show that truly subquadratic algorithms can be\nobtained for variants induced by other subsets. We provide two approaches to\ndesigning such algorithms that can be generalised to online and higher\ndimensional settings. First, we give almost-tight $\\tilde{O}(n^{3/2})$ time\nalgorithms for single-update variants where the update operation distributes\nover the query operation. Second, for other variants, we provide a general\nframework for reducing to instances with a special geometry. Using this, we\nshow that $O(m^{3/2-\\epsilon})$ time algorithms for counting paths and walks of\nlength 2 and 3 between vertex pairs in sparse graphs imply truly subquadratic\ndata structures for certain variants; to this end, we give an\n$\\tilde{O}(m^{(4\\omega-1)/(2\\omega+1)}) = O(m^{1.478})$ time algorithm for\ncounting simple 3-paths between vertex pairs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 13:18:02 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Lau", "Joshua", ""], ["Ritossa", "Angus", ""]]}, {"id": "2101.02086", "submitter": "Laurent Viennot", "authors": "Filippo Brunelli (Inria, IRIF (UMR\\_8243), UP), Pierluigi Crescenzi\n  (GSSI), Laurent Viennot (Inria, IRIF (UMR\\_8243), UP)", "title": "On Computing Pareto Optimal Paths in Weighted Time-Dependent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted point-availability time-dependent network is a list of temporal\nedges, where each temporal edge has an appearing time value, a travel time\nvalue, and a cost value. In this paper we consider the single source Pareto\nproblem in weighted point-availability time-dependent networks, which consists\nof computing, for any destination d, all Pareto optimal pairs (t, c), where t\nand c are the arrival time and the cost of a path from s to d, respectively (a\npair (t, c) is Pareto optimal if there is no path with arrival time smaller\nthan t and cost no worse than c or arrival time no greater than t and better\ncost). We design and analyse a general algorithm for solving this problem,\nwhose time complexity is O(M log P), where M is the number of temporal edges\nand P is the maximum number of Pareto optimal pairs for each node of the\nnetwork. This complexity significantly improves the time complexity of the\npreviously known solution. Our algorithm can be used to solve several different\nminimum cost path problems in weighted point-availability time-dependent\nnetworks with a vast variety of cost definitions, and it can be easily modified\nin order to deal with the single destination Pareto problem. All our results\napply to directed networks, but they can be easily adapted to undirected\nnetworks with no edges with zero travel time.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:16:11 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Brunelli", "Filippo", "", "Inria, IRIF"], ["Crescenzi", "Pierluigi", "", "GSSI"], ["Viennot", "Laurent", "", "Inria, IRIF"]]}, {"id": "2101.02153", "submitter": "Benedek Rozemberczki", "authors": "Benedek Rozemberczki and Rik Sarkar", "title": "The Shapley Value of Classifiers in Ensemble Games", "comments": "Source code is available here:\n  https://github.com/benedekrozemberczki/shapley", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.GT cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What is the value of an individual model in an ensemble of binary\nclassifiers? We answer this question by introducing a class of transferable\nutility cooperative games called \\textit{ensemble games}. In machine learning\nensembles, pre-trained models cooperate to make classification decisions. To\nquantify the importance of models in these ensemble games, we define\n\\textit{Troupe} -- an efficient algorithm which allocates payoffs based on\napproximate Shapley values of the classifiers. We argue that the Shapley value\nof models in these games is an effective decision metric for choosing a high\nperforming subset of models from the ensemble. Our analytical findings prove\nthat our Shapley value estimation scheme is precise and scalable; its\nperformance increases with size of the dataset and ensemble. Empirical results\non real world graph classification tasks demonstrate that our algorithm\nproduces high quality estimates of the Shapley value. We find that Shapley\nvalues can be utilized for ensemble pruning, and that adversarial models\nreceive a low valuation. Complex classifiers are frequently found to be\nresponsible for both correct and incorrect classification decisions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:40:23 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:38:54 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rozemberczki", "Benedek", ""], ["Sarkar", "Rik", ""]]}, {"id": "2101.02311", "submitter": "Adam Karczmarz", "authors": "Adam Karczmarz, Piotr Sankowski", "title": "A Deterministic Parallel APSP Algorithm and its Applications", "comments": "A SODA'21 paper. Slightly extended preliminaries. Abstract shortened\n  to meet arXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we show a deterministic parallel all-pairs shortest paths\nalgorithm for real-weighted directed graphs. The algorithm has\n$\\tilde{O}(nm+(n/d)^3)$ work and $\\tilde{O}(d)$ depth for any depth parameter\n$d\\in [1,n]$. To the best of our knowledge, such a trade-off has only been\npreviously described for the real-weighted single-source shortest paths problem\nusing randomization [Bringmann et al., ICALP'17]. Moreover, our result improves\nupon the parallelism of the state-of-the-art randomized parallel algorithm for\ncomputing transitive closure, which has $\\tilde{O}(nm+n^3/d^2)$ work and\n$\\tilde{O}(d)$ depth [Ullman and Yannakakis, SIAM J. Comput. '91]. Our APSP\nalgorithm turns out to be a powerful tool for designing efficient planar graph\nalgorithms in both parallel and sequential regimes.\n  One notable ingredient of our parallel APSP algorithm is a simple\ndeterministic $\\tilde{O}(nm)$-work $\\tilde{O}(d)$-depth procedure for computing\n$\\tilde{O}(n/d)$-size hitting sets of shortest $d$-hop paths between all pairs\nof vertices of a real-weighted digraph. Such hitting sets have also been called\n$d$-hub sets. Hub sets have previously proved especially useful in designing\nparallel or dynamic shortest paths algorithms and are typically obtained via\nrandom sampling. Our procedure implies, for example, an $\\tilde{O}(nm)$-time\ndeterministic algorithm for finding a shortest negative cycle of a\nreal-weighted digraph. Such a near-optimal bound for this problem has been so\nfar only achieved using a randomized algorithm [Orlin et al., Discret. Appl.\nMath. '18].\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:24:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Karczmarz", "Adam", ""], ["Sankowski", "Piotr", ""]]}, {"id": "2101.02312", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet", "title": "4 vs 7 sparse undirected unweighted Diameter is SETH-hard at time\n  $n^{4/3}$", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show, assuming the Strong Exponential Time Hypothesis, that for every\n$\\varepsilon > 0$, approximating undirected unweighted Diameter on $n$-vertex\n$n^{1+o(1)}$-edge graphs within ratio $7/4 - \\varepsilon$ requires $m^{4/3 -\no(1)}$ time. This is the first result that conditionally rules out a\nnear-linear time $5/3$-approximation for undirected Diameter.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:25:10 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Bonnet", "\u00c9douard", ""]]}, {"id": "2101.02574", "submitter": "Adam Karczmarz", "authors": "Giuseppe F. Italiano, Adam Karczmarz, Nikos Parotsidis", "title": "Planar Reachability Under Single Vertex or Edge Failures", "comments": "Full version of a SODA'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present an efficient reachability oracle under single-edge\nor single-vertex failures for planar directed graphs. Specifically, we show\nthat a planar digraph $G$ can be preprocessed in $O(n\\log^2{n}/\\log\\log{n})$\ntime, producing an $O(n\\log{n})$-space data structure that can answer in\n$O(\\log{n})$ time whether $u$ can reach $v$ in $G$ if the vertex $x$ (the\nedge~$f$) is removed from $G$, for any query vertices $u,v$ and failed vertex\n$x$ (failed edge $f$). To the best of our knowledge, this is the first data\nstructure for planar directed graphs with nearly optimal preprocessing time\nthat answers all-pairs queries under any kind of failures in polylogarithmic\ntime.\n  We also consider 2-reachability problems, where we are given a planar digraph\n$G$ and we wish to determine if there are two vertex-disjoint (edge-disjoint)\npaths from $u$ to $v$, for query vertices $u,v$. In this setting we provide a\nnearly optimal 2-reachability oracle, which is the existential variant of the\nreachability oracle under single failures, with the following bounds. We can\nconstruct in $O(n\\log^{O(1)}{n})$ time an $O(n\\log^{3+o(1)}{n})$-space data\nstructure that can check in $O(\\log^{2+o(1)}{n})$ time for any query vertices\n$u,v$ whether $v$ is 2-reachable from $u$, or otherwise find some separating\nvertex (edge) $x$ lying on all paths from $u$ to $v$ in $G$.\n  To obtain our results, we follow the general recursive approach of Thorup for\nreachability in planar graphs [J.~ACM~'04] and we present new data structures\nwhich generalize dominator trees and previous data structures for\nstrong-connectivity under failures [Georgiadis et al., SODA~'17]. Our new data\nstructures work also for general digraphs and may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:52:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Italiano", "Giuseppe F.", ""], ["Karczmarz", "Adam", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "2101.02751", "submitter": "Alex Gavryushkin", "authors": "Lena Collienne, Kieran Elmes, Mareike Fischer, David Bryant, Alex\n  Gavryushkin", "title": "The Geometry of the space of Discrete Coalescent Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computational inference of dated evolutionary histories relies upon various\nhypotheses about RNA, DNA, and protein sequence mutation rates. Using mutation\nrates to infer these dated histories is referred to as molecular clock\nassumption. Coalescent theory is a popular class of evolutionary models that\nimplements the molecular clock hypothesis to facilitate computational inference\nof dated phylogenies. Cancer and virus evolution are two areas where these\nmethods are particularly important.\n  Methodologically, phylogenetic inference methods require a tree space over\nwhich the inference is performed, and geometry of this space plays an important\nrole in statistical and computational aspects of tree inference algorithms. It\nhas recently been shown that molecular clock, and hence coalescent, trees\npossess a unique geometry, different from that of classical phylogenetic tree\nspaces which do not model mutation rates.\n  Here we introduce and study a space of discrete coalescent trees, that is, we\nassume that time is discrete, which is inevitable in many computational\nformalisations. We establish several geometrical properties of the space and\nshow how these properties impact various algorithms used in phylogenetic\nanalyses. Our tree space is a discretisation of a known time tree space, called\nt-space, and hence our results can be used to approximate solutions to various\nopen problems in t-space. Our tree space is also a generalisation of another\nknown trees space, called the ranked nearest neighbour interchange space, hence\nour advances in this paper imply new and generalise existing results about\nranked trees.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 20:23:14 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Collienne", "Lena", ""], ["Elmes", "Kieran", ""], ["Fischer", "Mareike", ""], ["Bryant", "David", ""], ["Gavryushkin", "Alex", ""]]}, {"id": "2101.02854", "submitter": "Sai Sandeep", "authors": "Sai Sandeep", "title": "Almost Optimal Inapproximability of Multidimensional Packing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional packing problems generalize the classical packing problems\nsuch as Bin Packing, Multiprocessor Scheduling by allowing the jobs to be\n$d$-dimensional vectors. While the approximability of the scalar problems is\nwell understood, there has been a significant gap between the approximation\nalgorithms and the hardness results for the multidimensional variants. In this\npaper, we close this gap by giving almost tight hardness results for these\nproblems.\n  1. We show that Vector Bin Packing has no polynomial time $\\Omega( \\log d)$\nfactor asymptotic approximation algorithm when $d$ is a large constant,\nassuming $\\textsf{P}\\neq \\textsf{NP}$. This matches the $\\ln d + O(1)$ factor\napproximation algorithms (Chekuri, Khanna SICOMP 2004, Bansal, Caprara,\nSviridenko SICOMP 2009, Bansal, Eli\\'{a}s, Khan SODA 2016) upto constants.\n  2. We show that Vector Scheduling has no polynomial time algorithm with an\napproximation ratio of $\\Omega\\left( (\\log d)^{1-\\epsilon}\\right)$ when $d$ is\npart of the input, assuming $\\textsf{NP}\\nsubseteq \\textsf{ZPTIME}\\left(\nn^{(\\log n)^{O(1)}}\\right)$. This almost matches the $O\\left( \\frac{\\log\nd}{\\log \\log d}\\right)$ factor algorithms(Harris, Srinivasan JACM 2019, Im,\nKell, Kulkarni, Panigrahi SICOMP 2019). We also show that the problem is\nNP-hard to approximate within $(\\log \\log d)^{\\omega(1)}$.\n  3. We show that Vector Bin Covering is NP-hard to approximate within\n$\\Omega\\left( \\frac{\\log d}{\\log \\log d}\\right)$ when $d$ is part of the input,\nalmost matching the $O(\\log d)$ factor algorithm (Alon et al., Algorithmica\n1998).\n  Previously, no hardness results that grow with $d$ were known for Vector\nScheduling and Vector Bin Covering when $d$ is part of the input and for Vector\nBin Packing when $d$ is a fixed constant.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 05:31:16 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 17:05:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sandeep", "Sai", ""]]}, {"id": "2101.03165", "submitter": "Kaustubh Joshi", "authors": "Kaustubh Joshi", "title": "Cantor Mapping Technique", "comments": "3 pages, 2 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A new technique specific to String ordering utilizing a method called \"Cantor\nMapping\" is explained in this paper and used to perform string comparative sort\nin loglinear time while utilizing linear extra space.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:50:28 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Joshi", "Kaustubh", ""]]}, {"id": "2101.03196", "submitter": "Bei Wang", "authors": "Mingzhe Li, Sourabh Palande, Lin Yan, Bei Wang", "title": "Sketching Merge Trees for Scientific Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merge trees are a type of topological descriptors that record the\nconnectivity among the sublevel sets of scalar fields. They are among the most\nwidely used topological tools in visualization. In this paper, we are\ninterested in sketching a set of merge trees. That is, given a large set T of\nmerge trees, we would like to find a much smaller basis set S such that each\ntree in T can be approximately reconstructed from a linear combination of merge\ntrees in S. A set of high-dimensional vectors can be sketched via matrix\nsketching techniques such as principal component analysis and column subset\nselection. However, up until now, topological descriptors such as merge trees\nhave not been known to be sketchable. We develop a framework for sketching a\nset of merge trees that combines the Gromov-Wasserstein probabilistic matching\nwith techniques from matrix sketching. We demonstrate the applications of our\nframework in sketching merge trees that arise from time-varying scientific\nsimulations. Specifically, our framework obtains a much smaller representation\nof a large set of merge trees for downstream analysis and visualization. It is\nshown to be useful in identifying good representatives and outliers with\nrespect to a chosen basis. Finally, our work shows a promising direction of\nutilizing randomized linear algebra within scientific visualization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:38:46 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 23:39:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Mingzhe", ""], ["Palande", "Sourabh", ""], ["Yan", "Lin", ""], ["Wang", "Bei", ""]]}, {"id": "2101.03258", "submitter": "John Golden", "authors": "John Golden, Andreas B\\\"artschi, Daniel O'Malley, Stephan Eidenbenz", "title": "QAOA-based Fair Sampling on NISQ Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the status of fair sampling on Noisy Intermediate Scale Quantum\n(NISQ) devices, in particular the IBM Q family of backends. Using the recently\nintroduced Grover Mixer-QAOA algorithm for discrete optimization, we generate\nfair sampling circuits to solve six problems of varying difficulty, each with\nseveral optimal solutions, which we then run on ten different backends\navailable on the IBM Q system. For a given circuit evaluated on a specific set\nof qubits, we evaluate: how frequently the qubits return an optimal solution to\nthe problem, the fairness with which the qubits sample from all optimal\nsolutions, and the reported hardware error rate of the qubits. To quantify\nfairness, we define a novel metric based on Pearson's $\\chi^2$ test. We find\nthat fairness is relatively high for circuits with small and large error rates,\nbut drops for circuits with medium error rates. This indicates that structured\nerrors dominate in this regime, while unstructured errors, which are random and\nthus inherently fair, dominate in noisier qubits and longer circuits. Our\nresults provide a simple, intuitive means of quantifying fairness in quantum\ncircuits, and show that reducing structured errors is necessary to improve fair\nsampling on NISQ hardware.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:48:53 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Golden", "John", ""], ["B\u00e4rtschi", "Andreas", ""], ["O'Malley", "Daniel", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2101.03519", "submitter": "Xiao Mao", "authors": "Xiao Mao", "title": "Shortest non-separating st-path on chordal graphs", "comments": "Fixed one misleading typo as well as some other minor issues in\n  section 6.1.1 Some further polishment for submission to ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many NP-Hard problems on general graphs, such as maximum independence set,\nmaximal cliques and graph coloring can be solved efficiently on chordal graphs.\nIn this paper, we explore the problem of non-separating st-paths defined on\nedges: for a connected undirected graph and two vertices, a non-separating path\nis a path between the two vertices such that if we remove all the edges on the\npath, the graph remains connected. We show that on general graphs, checking the\nexistence of non-separating st-paths is NP-Hard, but the same problem can be\nsolved in linear time on chordal graphs. In the case that such path exists, we\nintroduce an algorithm that finds the shortest non-separating st-path on a\nconnected chordal graph of $n$ vertices and $m$ edges with positive edge\nlengths that runs in $O(n\\log{n} + m)$ time.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 10:52:49 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 13:01:04 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 13:57:31 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mao", "Xiao", ""]]}, {"id": "2101.03574", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Beyond Helly graphs: the diameter problem on absolute retracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Characterizing the graph classes such that, on $n$-vertex $m$-edge graphs in\nthe class, we can compute the diameter faster than in ${\\cal O}(nm)$ time is an\nimportant research problem both in theory and in practice. We here make a new\nstep in this direction, for some metrically defined graph classes.\nSpecifically, a subgraph $H$ of a graph $G$ is called a retract of $G$ if it is\nthe image of some idempotent endomorphism of $G$. Two necessary conditions for\n$H$ being a retract of $G$ is to have $H$ is an isometric and isochromatic\nsubgraph of $G$. We say that $H$ is an absolute retract of some graph class\n${\\cal C}$ if it is a retract of any $G \\in {\\cal C}$ of which it is an\nisochromatic and isometric subgraph. In this paper, we study the complexity of\ncomputing the diameter within the absolute retracts of various hereditary graph\nclasses. First, we show how to compute the diameter within absolute retracts of\nbipartite graphs in randomized $\\tilde{\\cal O}(m\\sqrt{n})$ time. For the\nspecial case of chordal bipartite graphs, it can be improved to linear time,\nand the algorithm even computes all the eccentricities. Then, we generalize\nthese results to the absolute retracts of $k$-chromatic graphs, for every fixed\n$k \\geq 3$. Finally, we study the diameter problem within the absolute retracts\nof planar graphs and split graphs, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 16:02:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "2101.03652", "submitter": "Hao Wu", "authors": "Hao Wu, Junhao Gan, Zhewei Wei, Rui Zhang", "title": "Unifying the Global and Local Approaches: An Efficient Power Iteration\n  with Forward Push", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized PageRank (PPR) is a critical measure of the importance of a node\nt to a source node s in a graph. The Single-Source PPR (SSPPR) query computes\nthe PPR's of all the nodes with respect to s on a directed graph $G$ with $n$\nnodes and $m$ edges, and it is an essential operation widely used in graph\napplications. In this paper, we propose novel algorithms for solving two\nvariants of SSPPR: (i) high-precision queries and (ii) approximate queries. For\nhigh-precision queries, Power Iteration (PowItr) and Forward Push (FwdPush) are\ntwo fundamental approaches. Given an absolute error threshold $\\lambda$, the\nonly known bound of FwdPush is $O(\\frac{m}{\\lambda})$, much worse than the $O(m\n\\log \\frac{1}{\\lambda})$-bound of PowItr. Whether FwdPush can achieve the same\nrunning time bound as PowItr does still remains an open question in the\nresearch community. We give a positive answer to this question by showing that\nthe running time of a common implementation of FwdPush is actually bounded by\n$O(m \\cdot \\log \\frac{1}{\\lambda})$.Based on this finding, we propose a new\nalgorithm, called Power Iteration with Forward Push (PowerPush), which\nincorporates the strengths of both PowItr and FwdPush. For approximate queries\n(with a relative error $\\epsilon$), we propose a new algorithm, called\nSpeedPPR, with overall expected time bounded by $O(n \\cdot \\log n \\cdot \\log\n\\frac{1}{\\epsilon})$ on scale-free graphs. This bound greatly improves the\n$O(\\frac{n \\cdot \\log n}{\\epsilon})$ bound of a state-of-the-art algorithm\nFORA.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 01:05:48 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 00:24:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wu", "Hao", ""], ["Gan", "Junhao", ""], ["Wei", "Zhewei", ""], ["Zhang", "Rui", ""]]}, {"id": "2101.03712", "submitter": "Shaleen Deep", "authors": "Shaleen Deep, Xiao Hu, Paraschos Koutris", "title": "Enumeration Algorithms for Conjunctive Queries with Projection", "comments": "To appear in proceedings of ICDT 2021. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the enumeration of query results for an important subset of\nCQs with projections, namely star and path queries. The task is to design data\nstructures and algorithms that allow for efficient enumeration with delay\nguarantees after a preprocessing phase. Our main contribution is a series of\nresults based on the idea of interleaving precomputed output with further join\nprocessing to maintain delay guarantees, which maybe of independent interest.\nIn particular, for star queries, we design combinatorial algorithms that\nprovide instance-specific delay guarantees in linear preprocessing time. These\nalgorithms improve upon the currently best known results. Further, we show how\nexisting results can be improved upon by using fast matrix multiplication. We\nalso present new results involving tradeoff between preprocessing time and\ndelay guarantees for enumeration of path queries that contain projections. CQs\nwith projection where the join attribute is projected away is equivalent to\nboolean matrix multiplication. Our results can therefore also be interpreted as\nsparse, output-sensitive matrix multiplication with delay guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:49:49 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 06:28:39 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Deep", "Shaleen", ""], ["Hu", "Xiao", ""], ["Koutris", "Paraschos", ""]]}, {"id": "2101.03800", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Christian Komusiewicz, Dieter Kratsch, and Van Bang\n  Le", "title": "Refined Notions of Parameterized Enumeration Kernels with Applications\n  to Matching Cut Enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An enumeration kernel as defined by Creignou et al. [Theory Comput. Syst.\n2017] for a parameterized enumeration problem consists of an algorithm that\ntransforms each instance into one whose size is bounded by the parameter plus a\nsolution-lifting algorithm that efficiently enumerates all solutions from the\nset of the solutions of the kernel. We propose to consider two new versions of\nenumeration kernels by asking that the solutions of the original instance can\nbe enumerated in polynomial time or with polynomial delay from the kernel\nsolutions. Using the NP-hard Matching Cut problem parameterized by structural\nparameters such as the vertex cover number or the cyclomatic number of the\ninput graph, we show that the new enumeration kernels present a useful notion\nof data reduction for enumeration problems which allows to compactly represent\nthe set of feasible solutions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:25:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Golovach", "Petr A.", ""], ["Komusiewicz", "Christian", ""], ["Kratsch", "Dieter", ""], ["Le", "Van Bang", ""]]}, {"id": "2101.03978", "submitter": "Karol Pokorski", "authors": "Bart{\\l}omiej Dudek, Pawe{\\l} Gawrychowski, Karol Pokorski", "title": "Strictly In-Place Algorithms for Permuting and Inverting Permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the problem of permuting an array of length $n$ according to a\ngiven permutation in place, that is, using only a small number of bits of extra\nstorage. Fich, Munro and Poblete [FOCS 1990, SICOMP 1995] obtained an elegant\n$\\mathcal{O}(n\\log n)$-time algorithm using only $\\mathcal{O}(\\log^{2}n)$ bits\nof extra space for this basic problem by designing a procedure that scans the\npermutation and outputs exactly one element from each of its cycles. However,\nin the strict sense in place should be understood as using only an\nasymptotically optimal $\\mathcal{O}(\\log n)$ bits of extra space, or storing a\nconstant number of indices. The problem of permuting in this version is, in\nfact, a well-known interview question, with the expected solution being a\nquadratic-time algorithm. Surprisingly, no faster algorithm seems to be known\nin the literature.\n  Our first contribution is a strictly in-place generalisation of the method of\nFich et al. that works in $\\mathcal{O}_{\\varepsilon}(n^{1+\\varepsilon})$ time,\nfor any $\\varepsilon > 0$. Then, we build on this generalisation to obtain a\nstrictly in-place algorithm for inverting a given permutation on $n$ elements\nworking in the same complexity. This is a significant improvement on a recent\nresult of Gu\\'spiel [arXiv 2019], who designed an $\\mathcal{O}(n^{1.5})$-time\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:36:35 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Pokorski", "Karol", ""]]}, {"id": "2101.04176", "submitter": "Michela Meister", "authors": "Michela Meister and Sloan Nietert", "title": "Learning with Comparison Feedback: Online Estimation of Sample\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online version of the noisy binary search problem where feedback\nis generated by a non-stochastic adversary rather than perturbed by random\nnoise. We reframe this as maintaining an accurate estimate for the median of an\nadversarial sequence of integers, $x_1, x_2, \\dots$, in a model where each\nnumber $x_t$ can only be accessed through a single threshold query of the form\n${1(x_t \\leq q_t)}$. In this online comparison feedback model, we explore\nestimation of general sample statistics, providing robust algorithms for\nmedian, CDF, and mean estimation with nearly matching lower bounds. We conclude\nwith several high-dimensional generalizations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:28:32 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Meister", "Michela", ""], ["Nietert", "Sloan", ""]]}, {"id": "2101.04339", "submitter": "Jay Tenenbaum", "authors": "Haim Kaplan, Jay Tenenbaum", "title": "Locality Sensitive Hashing for Efficient Similar Polygon Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality Sensitive Hashing (LSH) is an effective method of indexing a set of\nitems to support efficient nearest neighbors queries in high-dimensional\nspaces. The basic idea of LSH is that similar items should produce hash\ncollisions with higher probability than dissimilar items.\n  We study LSH for (not necessarily convex) polygons, and use it to give\nefficient data structures for similar shape retrieval. Arkin et al. represent\npolygons by their \"turning function\" - a function which follows the angle\nbetween the polygon's tangent and the $ x $-axis while traversing the perimeter\nof the polygon. They define the distance between polygons to be variations of\nthe $ L_p $ (for $p=1,2$) distance between their turning functions. This metric\nis invariant under translation, rotation and scaling (and the selection of the\ninitial point on the perimeter) and therefore models well the intuitive notion\nof shape resemblance.\n  We develop and analyze LSH near neighbor data structures for several\nvariations of the $ L_p $ distance for functions (for $p=1,2$). By applying our\nschemes to the turning functions of a collection of polygons we obtain\nefficient near neighbor LSH-based structures for polygons. To tune our\nstructures to turning functions of polygons, we prove some new properties of\nthese turning functions that may be of independent interest.\n  As part of our analysis, we address the following problem which is of\nindependent interest. Find the vertical translation of a function $ f $ that is\nclosest in $ L_1 $ distance to a function $ g $. We prove tight bounds on the\napproximation guarantee obtained by the translation which is equal to the\ndifference between the averages of $ g $ and $ f $.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:00:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 12:21:08 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 11:09:14 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kaplan", "Haim", ""], ["Tenenbaum", "Jay", ""]]}, {"id": "2101.04425", "submitter": "Girija Limaye", "authors": "Girija Limaye and Meghana Nasre", "title": "Stable Matchings with Flexible Quotas", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of assigning agents to programs in the presence of\ntwo-sided preferences, commonly known as the Hospital Residents problem. In the\nstandard setting each program has a rigid upper quota which cannot be violated.\nMotivated by applications where quotas may be flexible, we propose and study\nthe problem of computing stable matchings under flexible quotas -- denoted as\nthe SMFQ setting.\n  In the SMFQ setting we have a cost associated with every program and these\ncosts control the quotas. Our goal is to compute a stable matching that matches\nall agents and is optimal with respect to the cost criteria. We study two\noptimization problems with respect to the costs and show that there is a sharp\ncontrast in the complexity status of the two optimization criteria. We also\nestablish the connection of our model with the stable extension problem in an\napparently different two-round setting of the stable matching problem. We show\nthat our results in the SMFQ setting generalize the stable extension problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:54:18 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 03:20:16 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Limaye", "Girija", ""], ["Nasre", "Meghana", ""]]}, {"id": "2101.04500", "submitter": "Cyril Cayron", "authors": "Cyril Cayron", "title": "Lattice reduction by cubification", "comments": "17 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lattice reduction is a NP-hard problem well known in computer science and\ncryptography. The Lenstra-Lenstra-Lovasz (LLL) algorithm based on the\ncalculation of orthogonal Gram-Schmidt (GS) bases is efficient and gives a good\nsolution in polynomial time. Here, we present a new approach called\ncubification that does not require the calculation of the GS bases. It relies\non complementary directional and hyperplanar reductions. The deviation from\ncubicity at each step of the reduction process is evaluated by a parameter\ncalled lattice rhombicity, which is simply the sum of the absolute values of\nthe metric tensor. Cubification seems to equal LLL; it even outperforms it in\nthe reduction of columnar matrices. We wrote a Python program that is ten times\nfaster than a reference Python LLL code. This work may open new perspectives\nfor lattice reduction and may have implications and applications beyond\ncrystallography.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:22:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cayron", "Cyril", ""]]}, {"id": "2101.04633", "submitter": "Geevarghese Philip", "authors": "Fedor V. Fomin and Petr A. Golovach and Fahad Panolan and Geevarghese\n  Philip and Saket Saurabh", "title": "Diverse Collections in Matroids and Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We investigate the parameterized complexity of finding diverse sets of\nsolutions to three fundamental combinatorial problems, two from the theory of\nmatroids and the third from graph theory. The input to the Weighted Diverse\nBases problem consists of a matroid $M$, a weight function\n$\\omega:E(M)\\to\\mathbb{N}$, and integers $k\\geq 1, d\\geq 0$. The task is to\ndecide if there is a collection of $k$ bases $B_{1}, \\dotsc, B_{k}$ of $M$ such\nthat the weight of the symmetric difference of any pair of these bases is at\nleast $d$. This is a diverse variant of the classical matroid base packing\nproblem. The input to the Weighted Diverse Common Independent Sets problem\nconsists of two matroids $M_{1},M_{2}$ defined on the same ground set $E$, a\nweight function $\\omega:E\\to\\mathbb{N}$, and integers $k\\geq 1, d\\geq 0$. The\ntask is to decide if there is a collection of $k$ common independent sets\n$I_{1}, \\dotsc, I_{k}$ of $M_{1}$ and $M_{2}$ such that the weight of the\nsymmetric difference of any pair of these sets is at least $d$. This is\nmotivated by the classical weighted matroid intersection problem. The input to\nthe Diverse Perfect Matchings problem consists of a graph $G$ and integers\n$k\\geq 1, d\\geq 0$. The task is to decide if $G$ contains $k$ perfect matchings\n$M_{1},\\dotsc,M_{k}$ such that the symmetric difference of any two of these\nmatchings is at least $d$.\n  We show that Weighted Diverse Bases and Weighted Diverse Common Independent\nSets are both NP-hard, and derive fixed-parameter tractable (FPT) algorithms\nfor all three problems with $(k,d)$ as the parameter.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:41:40 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Panolan", "Fahad", ""], ["Philip", "Geevarghese", ""], ["Saurabh", "Saket", ""]]}, {"id": "2101.04818", "submitter": "Marina Knittel", "authors": "MohammadTaghi Hajiaghayi and Marina Knittel", "title": "Improved Hierarchical Clustering on Massive Datasets with Broad\n  Guarantees", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a stronger extension of one of today's most\ninfluential unsupervised learning methods: clustering. The goal of this method\nis to create a hierarchy of clusters, thus constructing cluster evolutionary\nhistory and simultaneously finding clusterings at all resolutions. We propose\nfour traits of interest for hierarchical clustering algorithms: (1) empirical\nperformance, (2) theoretical guarantees, (3) cluster balance, and (4)\nscalability. While a number of algorithms are designed to achieve one to two of\nthese traits at a time, there exist none that achieve all four.\n  Inspired by Bateni et al.'s scalable and empirically successful Affinity\nClustering [NeurIPs 2017], we introduce Affinity Clustering's successor,\nMatching Affinity Clustering. Like its predecessor, Matching Affinity\nClustering maintains strong empirical performance and uses Massively Parallel\nCommunication as its distributed model. Designed to maintain provably balanced\nclusters, we show that our algorithm achieves good, constant factor\napproximations for Moseley and Wang's revenue and Cohen-Addad et al.'s value.\nWe show Affinity Clustering cannot approximate either function. Along the way,\nwe also introduce an efficient $k$-sized maximum matching algorithm in the MPC\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:34:03 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hajiaghayi", "MohammadTaghi", ""], ["Knittel", "Marina", ""]]}, {"id": "2101.04914", "submitter": "Martin P. Seybold", "authors": "Joachim Gudmundsson, Martin P. Seybold", "title": "A Tail Estimate with Exponential Decay for the Randomized Incremental\n  Construction of Search Structures", "comments": "New high probability bound in Section 4.1; Further applications in\n  Section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Randomized Incremental Construction (RIC) of search DAGs for point\nlocation in planar subdivisions, nearest-neighbor search in 2D points, and\nextreme point search in 3D convex hulls, are well known to take ${\\cal O}(n\n\\log n)$ expected time for structures of ${\\cal O}(n)$ expected size. Moreover,\nsearching takes w.h.p. ${\\cal O}(\\log n)$ comparisons in the first and w.h.p.\n${\\cal O}(\\log^2 n)$ comparisons in the latter two DAGs. However, the expected\ndepth of the DAGs and high probability bounds for their size are unknown.\n  Using a novel analysis technique, we show that the three DAGs have w.h.p. i)\na size of ${\\cal O}(n)$, ii) a depth of ${\\cal O}(\\log n)$, and iii) a\nconstruction time of ${\\cal O}(n \\log n)$. One application of these new and\nimproved results are \\emph{remarkably simple} Las Vegas verifiers to obtain\nsearch DAGs with optimal worst-case bounds. This positively answers the\nconjectured logarithmic search cost in the DAG of Delaunay triangulations\n[Guibas et al.; ICALP 1990] and a conjecture on the depth of the DAG of\nTrapezoidal subdivisions [Hemmer et al.; ESA 2012]. It also shows that\nhistory-based RIC circumvents a lower bound on runtime tail estimates of\nconflict-graph RICs [Sen; STACS 2019].\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:33:50 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 06:40:41 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 03:27:21 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gudmundsson", "Joachim", ""], ["Seybold", "Martin P.", ""]]}, {"id": "2101.05032", "submitter": "Murilo Santos de Lima", "authors": "Thomas Erlebach, Michael Hoffmann, Murilo S. de Lima (School of\n  Informatics, University of Leicester, United Kingdom)", "title": "Round-Competitive Algorithms for Uncertainty Problems with Parallel\n  Queries", "comments": "An extended abstract is to appear in the proceedings of the 38th\n  International Symposium on Theoretical Aspects of Computer Science (STACS\n  2021); [v2] minor fixes and typesetting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of computing with uncertainty considers problems where some\ninformation about the input elements is uncertain, but can be obtained using\nqueries. For example, instead of the weight of an element, we may be given an\ninterval that is guaranteed to contain the weight, and a query can be performed\nto reveal the weight. While previous work has considered models where queries\nare asked either sequentially (adaptive model) or all at once (non-adaptive\nmodel), and the goal is to minimize the number of queries that are needed to\nsolve the given problem, we propose and study a new model where $k$ queries can\nbe made in parallel in each round, and the goal is to minimize the number of\nquery rounds. We use competitive analysis and present upper and lower bounds on\nthe number of query rounds required by any algorithm in comparison with the\noptimal number of query rounds. Given a set of uncertain elements and a family\nof $m$ subsets of that set, we present an algorithm for determining the value\nof the minimum of each of the subsets that requires at most $(2+\\varepsilon)\n\\cdot \\mathrm{opt}_k+\\mathrm{O}\\left(\\frac{1}{\\varepsilon} \\cdot \\lg m\\right)$\nrounds for every $0<\\varepsilon<1$, where $\\mathrm{opt}_k$ is the optimal\nnumber of rounds, as well as nearly matching lower bounds. For the problem of\ndetermining the $i$-th smallest value and identifying all elements with that\nvalue in a set of uncertain elements, we give a $2$-round-competitive\nalgorithm. We also show that the problem of sorting a family of sets of\nuncertain elements admits a $2$-round-competitive algorithm and this is the\nbest possible.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:38:05 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 01:46:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Erlebach", "Thomas", "", "School of\n  Informatics, University of Leicester, United Kingdom"], ["Hoffmann", "Michael", "", "School of\n  Informatics, University of Leicester, United Kingdom"], ["de Lima", "Murilo S.", "", "School of\n  Informatics, University of Leicester, United Kingdom"]]}, {"id": "2101.05033", "submitter": "Alexander Noe", "authors": "Monika Henzinger, Alexander Noe, Christian Schulz", "title": "Practical Fully Dynamic Minimum Cut Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practically efficient algorithm for maintaining a global minimum\ncut in large dynamic graphs under both edge insertions and deletions. While\nthere has been theoretical work on this problem, our algorithm is the first\nimplementation of a fully-dynamic algorithm. The algorithm uses the theoretical\nfoundation and combines it with efficient and finely-tuned implementations to\ngive an algorithm that can maintain the global minimum cut of a graph with\nrapid update times. We show that our algorithm gives up to multiple orders of\nmagnitude speedup compared to static approaches both on edge insertions and\ndeletions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:41:41 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "2101.05223", "submitter": "Michael Luby", "authors": "Michael Luby and Thomas Richardson", "title": "Distributed storage algorithms with optimal tradeoffs", "comments": "22 pages, readability will be improved in revised versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.IR math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the primary objectives of a distributed storage system is to reliably\nstore large amounts of source data for long durations using a large number $N$\nof unreliable storage nodes, each with $c$ bits of storage capacity. Storage\nnodes fail randomly over time and are replaced with nodes of equal capacity\ninitialized to zeroes, and thus bits are erased at some rate $e$. To maintain\nrecoverability of the source data, a repairer continually reads data over a\nnetwork from nodes at an average rate $r$, and generates and writes data to\nnodes based on the read data.\n  The distributed storage source capacity is the maximum amount of source that\ncan be reliably stored for long periods of time. Previous research shows that\nasymptotically the distributed storage source capacity is at most\n$\\left(1-\\frac{e}{2 \\cdot r}\\right) \\cdot N \\cdot c$ as $N$ and $r$ grow.\n  In this work we introduce and analyze algorithms such that asymptotically the\ndistributed storage source data capacity is at least the above equation. Thus,\nthe above equation expresses a fundamental trade-off between network traffic\nand storage overhead to reliably store source data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:36:27 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Luby", "Michael", ""], ["Richardson", "Thomas", ""]]}, {"id": "2101.05329", "submitter": "Petra Wolf", "authors": "Sven Fiergolla and Petra Wolf", "title": "Improving Run Length Encoding by Preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Run Length Encoding (RLE) compression method is a long standing simple\nlossless compression scheme which is easy to implement and achieves a good\ncompression on input data which contains repeating consecutive symbols. In its\npure form RLE is not applicable on natural text or other input data with short\nsequences of identical symbols. We present a combination of preprocessing steps\nthat turn arbitrary input data in a byte-wise encoding into a bit-string which\nis highly suitable for RLE compression. The main idea is to first read all most\nsignificant bits of the input byte-string, followed by the second most\nsignificant bit, and so on. We combine this approach by a dynamic byte\nremapping as well as a Burrows-Wheeler-Scott transform on a byte level.\nFinally, we apply a Huffman Encoding on the output of the bit-wise RLE encoding\nto allow for more dynamic lengths of code words encoding runs of the RLE. With\nour technique we can achieve a lossless average compression which is better\nthan the standard RLE compression by a factor of 8 on average.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 20:11:47 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 21:17:27 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Fiergolla", "Sven", ""], ["Wolf", "Petra", ""]]}, {"id": "2101.05549", "submitter": "Aida Mousavifar", "authors": "Grzegorz Gluch, Michael Kapralov, Silvio Lattanzi, Aida Mousavifar,\n  Christian Sohler", "title": "Spectral Clustering Oracles in Sublinear Time", "comments": "Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms\n  (SODA). Society for Industrial and Applied Mathematics, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a graph $G$ that can be partitioned into $k$ disjoint expanders with\nouter conductance upper bounded by $\\epsilon\\ll 1$, can we efficiently\nconstruct a small space data structure that allows quickly classifying vertices\nof $G$ according to the expander (cluster) they belong to? Formally, we would\nlike an efficient local computation algorithm that misclassifies at most an\n$O(\\epsilon)$ fraction of vertices in every expander. We refer to such a data\nstructure as a \\textit{spectral clustering oracle}. Our main result is a\nspectral clustering oracle with query time $O^*(n^{1/2+O(\\epsilon)})$ and\npreprocessing time $2^{O(\\frac{1}{\\epsilon} k^4 \\log^2(k))}\nn^{1/2+O(\\epsilon)}$ that provides misclassification error $O(\\epsilon \\log k)$\nper cluster for any $\\epsilon \\ll 1/\\log k$. More generally, query time can be\nreduced at the expense of increasing the preprocessing time appropriately (as\nlong as the product is about $n^{1+O(\\epsilon)}$) -- this in particular gives a\nnearly linear time spectral clustering primitive. The main technical\ncontribution is a sublinear time oracle that provides dot product access to the\nspectral embedding of $G$ by estimating distributions of short random walks\nfrom vertices in $G$. The distributions themselves provide a poor approximation\nto the spectral embedding, but we show that an appropriate linear\ntransformation can be used to achieve high precision dot product access. We\nthen show that dot product access to the spectral embedding is sufficient to\ndesign a clustering oracle. At a high level our approach amounts to hyperplane\npartitioning in the spectral embedding of $G$, but crucially operates on a\nnested sequence of carefully defined subspaces in the spectral embedding to\nachieve per cluster recovery guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 11:09:26 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Gluch", "Grzegorz", ""], ["Kapralov", "Michael", ""], ["Lattanzi", "Silvio", ""], ["Mousavifar", "Aida", ""], ["Sohler", "Christian", ""]]}, {"id": "2101.05657", "submitter": "Ankur Moitra", "authors": "Linus Hamilton, Ankur Moitra", "title": "No-go Theorem for Acceleration in the Hyperbolic Plane", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been significant effort to adapt the key tools and\nideas in convex optimization to the Riemannian setting. One key challenge has\nremained: Is there a Nesterov-like accelerated gradient method for geodesically\nconvex functions on a Riemannian manifold? Recent work has given partial\nanswers and the hope was that this ought to be possible.\n  Here we dash these hopes. We prove that in a noisy setting, there is no\nanalogue of accelerated gradient descent for geodesically convex functions on\nthe hyperbolic plane. Our results apply even when the noise is exponentially\nsmall. The key intuition behind our proof is short and simple: In negatively\ncurved spaces, the volume of a ball grows so fast that information about the\npast gradients is not useful in the future.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:11:24 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 03:01:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hamilton", "Linus", ""], ["Moitra", "Ankur", ""]]}, {"id": "2101.05719", "submitter": "Yang P. Liu", "authors": "Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak,\n  Aaron Sidford, Zhao Song, Di Wang", "title": "Minimum Cost Flows, MDPs, and $\\ell_1$-Regression in Nearly Linear Time\n  for Dense Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide new randomized algorithms with improved runtimes for\nsolving linear programs with two-sided constraints. In the special case of the\nminimum cost flow problem on $n$-vertex $m$-edge graphs with integer\npolynomially-bounded costs and capacities we obtain a randomized method which\nsolves the problem in $\\tilde{O}(m+n^{1.5})$ time. This improves upon the\nprevious best runtime of $\\tilde{O}(m\\sqrt{n})$ (Lee-Sidford 2014) and, in the\nspecial case of unit-capacity maximum flow, improves upon the previous best\nruntimes of $m^{4/3+o(1)}$ (Liu-Sidford 2020, Kathuria 2020) and\n$\\tilde{O}(m\\sqrt{n})$ (Lee-Sidford 2014) for sufficiently dense graphs.\n  For $\\ell_1$-regression in a matrix with $n$-columns and $m$-rows we obtain a\nrandomized method which computes an $\\epsilon$-approximate solution in\n$\\tilde{O}(mn+n^{2.5})$ time. This yields a randomized method which computes an\n$\\epsilon$-optimal policy of a discounted Markov Decision Process with $S$\nstates and $A$ actions per state in time $\\tilde{O}(S^2A+S^{2.5})$. These\nmethods improve upon the previous best runtimes of methods which depend\npolylogarithmically on problem parameters, which were $\\tilde{O}(mn^{1.5})$\n(Lee-Sidford 2015) and $\\tilde{O}(S^{2.5}A)$ (Lee-Sidford 2014,\nSidford-Wang-Wu-Ye 2018).\n  To obtain this result we introduce two new algorithmic tools of independent\ninterest. First, we design a new general interior point method for solving\nlinear programs with two sided constraints which combines techniques from\n(Lee-Song-Zhang 2019, Brand et al. 2020) to obtain a robust stochastic method\nwith iteration count nearly the square root of the smaller dimension. Second,\nto implement this method we provide dynamic data structures for efficiently\nmaintaining approximations to variants of Lewis-weights, a fundamental\nimportance measure for matrices which generalize leverage scores and effective\nresistances.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:50:13 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Brand", "Jan van den", ""], ["Lee", "Yin Tat", ""], ["Liu", "Yang P.", ""], ["Saranurak", "Thatchaphol", ""], ["Sidford", "Aaron", ""], ["Song", "Zhao", ""], ["Wang", "Di", ""]]}, {"id": "2101.05792", "submitter": "Batuhan Arasli", "authors": "Batuhan Arasli and Sennur Ulukus", "title": "Group Testing with a Graph Infection Spread Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CY cs.DS cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel infection spread model based on a random connection graph\nwhich represents connections between $n$ individuals. Infection spreads via\nconnections between individuals and this results in a probabilistic cluster\nformation structure as well as a non-i.i.d. (correlated) infection status for\nindividuals. We propose a class of two-step sampled group testing algorithms\nwhere we exploit the known probabilistic infection spread model. We investigate\nthe metrics associated with two-step sampled group testing algorithms. To\ndemonstrate our results, for analytically tractable exponentially split cluster\nformation trees, we calculate the required number of tests and the expected\nnumber of false classifications in terms of the system parameters, and identify\nthe trade-off between them. For such exponentially split cluster formation\ntrees, for zero-error construction, we prove that the required number of tests\nis $O(\\log_2n)$. Thus, for such cluster formation trees, our algorithm\noutperforms any zero-error non-adaptive group test, binary splitting algorithm,\nand Hwang's generalized binary splitting algorithm. Our results imply that, by\nexploiting probabilistic information on the connections of individuals, group\ntesting can be used to reduce the number of required tests significantly even\nwhen infection rate is high, contrasting the prevalent belief that group\ntesting is useful only when infection rate is low.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:51:32 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Arasli", "Batuhan", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2101.05921", "submitter": "Nathan Klein", "authors": "Anna R. Karlin, Nathan Klein, Shayan Oveis Gharan, and Xinzhi Zhang", "title": "An Improved Approximation Algorithm for the Minimum $k$-Edge Connected\n  Multi-Subgraph Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a randomized $1+\\sqrt{\\frac{8\\ln k}{k}}$-approximation algorithm for\nthe minimum $k$-edge connected spanning multi-subgraph problem, $k$-ECSM.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 00:54:22 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Karlin", "Anna R.", ""], ["Klein", "Nathan", ""], ["Gharan", "Shayan Oveis", ""], ["Zhang", "Xinzhi", ""]]}, {"id": "2101.06192", "submitter": "Eugenio Angriman", "authors": "Alexander van der Grinten, Eugenio Angriman, Maria Predari, Henning\n  Meyerhenke", "title": "New Approximation Algorithms for Forest Closeness Centrality -- for\n  Individual Vertices and Vertex Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of massive graph data sets requires fast mining algorithms.\nCentrality measures to identify important vertices belong to the most popular\nanalysis methods in graph mining. A measure that is gaining attention is forest\ncloseness centrality; it is closely related to electrical measures using\ncurrent flow but can also handle disconnected graphs. Recently, [Jin et al.,\nICDM'19] proposed an algorithm to approximate this measure probabilistically.\nTheir algorithm processes small inputs quickly, but does not scale well beyond\nhundreds of thousands of vertices.\n  In this paper, we first propose a different approximation algorithm; it is up\nto two orders of magnitude faster and more accurate in practice. Our method\nexploits the strong connection between uniform spanning trees and forest\ndistances by adapting and extending recent approximation algorithms for related\nsingle-vertex problems. This results in a nearly-linear time algorithm with an\nabsolute probabilistic error guarantee. In addition, we are the first to\nconsider the problem of finding an optimal group of vertices w.r.t. forest\ncloseness. We prove that this latter problem is NP-hard; to approximate it, we\nadapt a greedy algorithm by [Li et al., WWW'19], which is based on (partial)\nmatrix inversion. Moreover, our experiments show that on disconnected graphs,\ngroup forest closeness outperforms existing centrality measures in the context\nof semi-supervised vertex classification.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:11:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["van der Grinten", "Alexander", ""], ["Angriman", "Eugenio", ""], ["Predari", "Maria", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "2101.06201", "submitter": "Artur Je\\.z", "authors": "Robert Ferens, Artur Je\\.z", "title": "Solving one variable word equations in the free group in cubic time", "comments": "52 pages, accepted to STACS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.DM cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A word equation with one variable in a free group is given as $U = V$, where\nboth $U$ and $V$ are words over the alphabet of generators of the free group\nand $X, X^{-1}$, for a fixed variable $X$. An element of the free group is a\nsolution when substituting it for $X$ yields a true equality (interpreted in\nthe free group) of left- and right-hand sides. It is known that the set of all\nsolutions of a given word equation with one variable is a finite union of sets\nof the form $\\{\\alpha w^i \\beta \\: : \\: i \\in \\mathbb Z \\}$, where $\\alpha, w,\n\\beta$ are reduced words over the alphabet of generators, and a polynomial-time\nalgorithm (of a high degree) computing this set is known. We provide a cubic\ntime algorithm for this problem, which also shows that the set of solutions\nconsists of at most a quadratic number of the above-mentioned sets. The\nalgorithm uses only simple tools of word combinatorics and group theory and is\nsimple to state. Its analysis is involved and focuses on the combinatorics of\noccurrences of powers of a word within a larger word.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:33:03 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ferens", "Robert", ""], ["Je\u017c", "Artur", ""]]}, {"id": "2101.06645", "submitter": "Dariusz Dereniowski", "authors": "Piotr Borowiecki, Dariusz Dereniowski, Dorota Osula", "title": "The Complexity of Bicriteria Tree-Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree-depth problem can be seen as finding an elimination tree of minimum\nheight for a given input graph $G$. We introduce a bicriteria generalization in\nwhich additionally the width of the elimination tree needs to be bounded by\nsome input integer $b$. We are interested in the case when $G$ is the line\ngraph of a tree, proving that the problem is NP-hard and obtaining a\npolynomial-time additive $2b$-approximation algorithm. This particular class of\ngraphs received significant attention in the past, mainly due to a number of\npotential applications, e.g. in parallel assembly of modular products, or\nparallel query processing in relational databases, as well as purely\ncombinatorial applications, including searching in tree-like partial orders\n(which in turn generalizes binary search on sorted data).\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:18:26 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 17:22:14 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 05:17:18 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Borowiecki", "Piotr", ""], ["Dereniowski", "Dariusz", ""], ["Osula", "Dorota", ""]]}, {"id": "2101.06758", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Catiuscia Melle, Italo Epicoco, Marco Pulimeno", "title": "Data stream fusion for accurate quantile tracking and analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UDDSKETCH is a recent algorithm for accurate tracking of quantiles in data\nstreams, derived from the DDSKETCH algorithm. UDDSKETCH provides accuracy\nguarantees covering the full range of quantiles independently of the input\ndistribution and greatly improves the accuracy with regard to DDSKETCH. In this\npaper we show how to compress and fuse data streams (or datasets) by using\nUDDSKETCH data summaries that are fused into a new summary related to the union\nof the streams (or datasets) processed by the input summaries whilst preserving\nboth the error and size guarantees provided by UDDSKETCH. This property of\nsketches, known as mergeability, enables parallel and distributed processing.\nWe prove that UDDSKETCH is fully mergeable and introduce a parallel version of\nUDDSKETCH suitable for message-passing based architectures. We formally prove\nits correctness and compare it to a parallel version of DDSKETCH, showing\nthrough extensive experimental results that our parallel algorithm almost\nalways outperforms the parallel DDSKETCH algorithm with regard to the overall\naccuracy in determining the quantiles.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:30:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Cafaro", "Massimo", ""], ["Melle", "Catiuscia", ""], ["Epicoco", "Italo", ""], ["Pulimeno", "Marco", ""]]}, {"id": "2101.06998", "submitter": "Roopam Saxena", "authors": "N R Aravind and Roopam Saxena", "title": "An FPT algorithm for Matching Cut and d-cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a positive integer $d$, the $d$-CUT problem is to decide if an\nundirected graph $G=(V,E)$ has a non trivial bipartition $(A,B)$ of $V$ such\nthat every vertex in $A$ (resp. $B$) has at most $d$ neighbors in $B$ (resp.\n$A$). When $d=1$, this is the MATCHING CUT problem. Gomes and Sau, in IPEC\n2019, gave the first fixed parameter tractable algorithm for $d$-CUT, when\nparameterized by maximum number of the crossing edges in the cut (i.e. the size\nof edge cut). However, their paper doesn't provide an explicit bound on the\nrunning time, as it indirectly relies on a MSOL formulation and Courcelle's\nTheorem. Motivated by this, we design and present an FPT algorithm for the\nMATCHING CUT (and more generally for $d$-CUT) for general graphs with running\ntime $2^{O(k\\log k)}n^{O(1)}$ where $k$ is the maximum size of the edge cut.\nThis is the first FPT algorithm for the MATCHING CUT (and $d$-CUT) with an\nexplicit dependence on this parameter. We also observe a lower bound of\n$2^{\\Omega(k)}n^{O(1)}$ with same parameter for MATCHING CUT assuming ETH.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:57:16 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:54:35 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Aravind", "N R", ""], ["Saxena", "Roopam", ""]]}, {"id": "2101.07026", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai, Nikos Tziritas, Toyotaro Suzumura, Wentong Cai,\n  Georgios Theodoropoulos", "title": "Time-Efficient and High-Quality Graph Partitioning for Graph Dynamic\n  Scaling", "comments": "21 pages, 15 figures. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic scaling of distributed computations plays an important role in\nthe utilization of elastic computational resources, such as the cloud. It\nenables the provisioning and de-provisioning of resources to match dynamic\nresource availability and demands. In the case of distributed graph processing,\nchanging the number of the graph partitions while maintaining high partitioning\nquality imposes serious computational overheads as typically a time-consuming\ngraph partitioning algorithm needs to execute each time repartitioning is\nrequired. In this paper, we propose a dynamic scaling method that can\nefficiently change the number of graph partitions while keeping its quality\nhigh. Our idea is based on two techniques: preprocessing and very fast edge\npartitioning, called graph edge ordering and chunk-based edge partitioning,\nrespectively. The former converts the graph data into an ordered edge list in\nsuch a way that edges with high locality are closer to each other. The latter\nimmediately divides the ordered edge list into an arbitrary number of\nhigh-quality partitions. The evaluation with the real-world billion-scale\ngraphs demonstrates that our proposed approach significantly reduces the\nrepartitioning time, while the partitioning quality it achieves is on par with\nthat of the best existing static method.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:06:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Tziritas", "Nikos", ""], ["Suzumura", "Toyotaro", ""], ["Cai", "Wentong", ""], ["Theodoropoulos", "Georgios", ""]]}, {"id": "2101.07149", "submitter": "Maximilian Probst Gutenberg", "authors": "Aaron Bernstein, Maximilian Probst Gutenberg, Thatchaphol Saranurak", "title": "Deterministic Decremental SSSP and Approximate Min-Cost Flow in\n  Almost-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the decremental single-source shortest paths problem, the goal is to\nmaintain distances from a fixed source $s$ to every vertex $v$ in an $m$-edge\ngraph undergoing edge deletions. In this paper, we conclude a long line of\nresearch on this problem by showing a near-optimal deterministic data structure\nthat maintains $(1+\\epsilon)$-approximate distance estimates and runs in\n$m^{1+o(1)}$ total update time.\n  Our result, in particular, removes the oblivious adversary assumption\nrequired by the previous breakthrough result by Henzinger et al. [FOCS'14],\nwhich leads to our second result: the first almost-linear time algorithm for\n$(1-\\epsilon)$-approximate min-cost flow in undirected graphs where capacities\nand costs can be taken over edges and vertices. Previously, algorithms for max\nflow with vertex capacities, or min-cost flow with any capacities required\nsuper-linear time. Our result essentially completes the picture for approximate\nflow in undirected graphs.\n  The key technique of the first result is a novel framework that allows us to\ntreat low-diameter graphs like expanders. This allows us to harness expander\nproperties while bypassing shortcomings of expander decomposition, which almost\nall previous expander-based algorithms needed to deal with. For the second\nresult, we break the notorious flow-decomposition barrier from the\nmultiplicative-weight-update framework using randomization.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:29:31 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 17:06:54 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bernstein", "Aaron", ""], ["Gutenberg", "Maximilian Probst", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2101.07152", "submitter": "Ilie Sarpe", "authors": "Ilie Sarpe, Fabio Vandin", "title": "PRESTO: Simple and Scalable Sampling Techniques for the Rigorous\n  Approximation of Temporal Motif Counts", "comments": "19 pages, 5 figures, to appear in SDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and counting of small graph patterns, called network\nmotifs, is a fundamental primitive in the analysis of networks, with\napplication in various domains, from social networks to neuroscience. Several\ntechniques have been designed to count the occurrences of motifs in static\nnetworks, with recent work focusing on the computational challenges provided by\nlarge networks. Modern networked datasets contain rich information, such as the\ntime at which the events modeled by the networks edges happened, which can\nprovide useful insights into the process modeled by the network. The analysis\nof motifs in temporal networks, called temporal motifs, is becoming an\nimportant component in the analysis of modern networked datasets. Several\nmethods have been recently designed to count the number of instances of\ntemporal motifs in temporal networks, which is even more challenging than its\ncounterpart for static networks. Such methods are either exact, and not\napplicable to large networks, or approximate, but provide only weak guarantees\non the estimates they produce and do not scale to very large networks. In this\nwork we present an efficient and scalable algorithm to obtain rigorous\napproximations of the count of temporal motifs. Our algorithm is based on a\nsimple but effective sampling approach, which renders our algorithm practical\nfor very large datasets. Our extensive experimental evaluation shows that our\nalgorithm provides estimates of temporal motif counts which are more accurate\nthan the state-of-the-art sampling algorithms, with significantly lower running\ntime than exact approaches, enabling the study of temporal motifs, of size\nlarger than the ones considered in previous works, on billion edges networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:35:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sarpe", "Ilie", ""], ["Vandin", "Fabio", ""]]}, {"id": "2101.07155", "submitter": "Megha Khosla", "authors": "Megha Khosla and Avishek Anand", "title": "Revisiting the Auction Algorithm for Weighted Bipartite Perfect\n  Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the classical weighted perfect matchings problem for bipartite\ngraphs or sometimes referred to as the assignment problem, i.e., given a\nweighted bipartite graph $G = (U\\cup V,E)$ with weights $w : E \\rightarrow\n\\mathcal{R}$ we are interested to find the maximum matching in $G$ with the\nminimum/maximum weight. In this work we present a new and arguably simpler\nanalysis of one of the earliest techniques developed for solving the assignment\nproblem, namely the auction algorithm. Using our analysis technique we present\ntighter and improved bounds on the runtime complexity for finding an\napproximate minumum weight perfect matching in $k$-left regular sparse\nbipartite graphs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:41:34 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Khosla", "Megha", ""], ["Anand", "Avishek", ""]]}, {"id": "2101.07157", "submitter": "Grigorios Loukides", "authors": "Leqian Zheng and Hau Chan and Grigorios Loukides and Minming Li", "title": "Maximizing approximately k-submodular functions", "comments": "To be published in SIAM International Conference on Data Mining (SDM)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the problem of maximizing approximately $k$-submodular functions\nsubject to size constraints. In this problem, one seeks to select $k$-disjoint\nsubsets of a ground set with bounded total size or individual sizes, and\nmaximum utility, given by a function that is \"close\" to being $k$-submodular.\nThe problem finds applications in tasks such as sensor placement, where one\nwishes to install $k$ types of sensors whose measurements are noisy, and\ninfluence maximization, where one seeks to advertise $k$ topics to users of a\nsocial network whose level of influence is uncertain. To deal with the problem,\nwe first provide two natural definitions for approximately $k$-submodular\nfunctions and establish a hierarchical relationship between them. Next, we show\nthat simple greedy algorithms offer approximation guarantees for different\ntypes of size constraints. Last, we demonstrate experimentally that the greedy\nalgorithms are effective in sensor placement and influence maximization\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:48:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zheng", "Leqian", ""], ["Chan", "Hau", ""], ["Loukides", "Grigorios", ""], ["Li", "Minming", ""]]}, {"id": "2101.07233", "submitter": "Yang P. Liu", "authors": "Yu Gao, Yang P. Liu, Richard Peng", "title": "Fully Dynamic Electrical Flows: Sparse Maxflow Faster Than Goldberg-Rao", "comments": "78 pages, v2. Fixes an issue relating to handling of adaptivity and\n  randomness -- we thank Aaron Sidford for discussions during which this error\n  was pointed out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for computing exact maximum flows on graphs with $m$\nedges and integer capacities in the range $[1, U]$ in\n$\\widetilde{O}(m^{\\frac{3}{2} - \\frac{1}{328}} \\log U)$ time. For sparse graphs\nwith polynomially bounded integer capacities, this is the first improvement\nover the $\\widetilde{O}(m^{1.5} \\log U)$ time bound from [Goldberg-Rao JACM\n`98].\n  Our algorithm revolves around dynamically maintaining the augmenting\nelectrical flows at the core of the interior point method based algorithm from\n[M\\k{a}dry JACM `16]. This entails designing data structures that, in limited\nsettings, return edges with large electric energy in a graph undergoing\nresistance updates.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:38:34 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 03:19:18 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gao", "Yu", ""], ["Liu", "Yang P.", ""], ["Peng", "Richard", ""]]}, {"id": "2101.07360", "submitter": "Saeed Seddighin", "authors": "Michael Mitzenmacher and Saeed Seddighin", "title": "Dynamic Longest Increasing Subsequence and the Erd\\\"{o}s-Szekeres\n  Partitioning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we provide new approximation algorithms for dynamic variations\nof the longest increasing subsequence (\\textsf{LIS}) problem, and the\ncomplementary distance to monotonicity (\\textsf{DTM}) problem. In this setting,\noperations of the following form arrive sequentially: (i) add an element, (ii)\nremove an element, or (iii) substitute an element for another. At every point\nin time, the algorithm has an approximation to the longest increasing\nsubsequence (or distance to monotonicity). We present a\n$(1+\\epsilon)$-approximation algorithm for \\textsf{DTM} with polylogarithmic\nworst-case update time and a constant factor approximation algorithm for\n\\textsf{LIS} with worst-case update time $\\tilde O(n^\\epsilon)$ for any\nconstant $\\epsilon > 0$.% $n$ in the runtime denotes the size of the array at\nthe time the operation arrives.\n  Our dynamic algorithm for \\textsf{LIS} leads to an almost optimal algorithm\nfor the Erd\\\"{o}s-Szekeres partitioning problem. Erd\\\"{o}s-Szekeres\npartitioning problem was introduced by Erd\\\"{o}s and Szekeres in 1935 and was\nknown to be solvable in time $O(n^{1.5}\\log n)$. Subsequent work improve the\nruntime to $O(n^{1.5})$ only in 1998. Our dynamic \\textsf{LIS} algorithm leads\nto a solution for Erd\\\"{o}s-Szekeres partitioning problem with runtime $\\tilde\nO_{\\epsilon}(n^{1+\\epsilon})$ for any constant $\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:55:32 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Seddighin", "Saeed", ""]]}, {"id": "2101.07428", "submitter": "Arnold Filtser", "authors": "Arnold Filtser, Hung Le", "title": "Reliable Spanners: Locality-Sensitive Orderings Strike Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chan, Har-Peled, and Jones [2020] recently developed locality-sensitive\nordering (LSO), a new tool that allows one to reduce problems in the Euclidean\nspace $\\mathbb{R}^d$ to the $1$-dimensional line. They used LSO's to solve a\nhost of problems. Later, Buchin, Har-Peled, and Ol{\\'{a}}h [2019,2020] used the\nLSO of Chan {\\em et al. } to construct very sparse \\emph{reliable spanners} for\nthe Euclidean space. A highly desirable feature of a reliable spanner is its\nability to withstand a massive failure: the network remains functioning even if\n90\\% of the nodes fail. In a follow-up work, Har-Peled, Mendel, and Ol{\\'{a}}h\n[2021] constructed reliable spanners for general and topologically structured\nmetrics. Their construction used a different approach, and is based on sparse\ncovers.\n  In this paper, we develop the theory of LSO's to non-Euclidean metrics by\nintroducing new types of LSO's suitable for general and topologically\nstructured metrics. We then construct such LSO's, as well as constructing\nconsiderably improved LSO's for doubling metrics. Afterwards, we use our new\nLSO's to construct reliable spanners with improved stretch and sparsity\nparameters. Most prominently, we construct $\\tilde{O}(n)$-size reliable\nspanners for trees and planar graphs with the optimal stretch of $2$. Along the\nway to the construction of LSO's and reliable spanners, we introduce and\nconstruct ultrametric covers, and construct $2$-hop reliable spanners for the\nline.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:52:36 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 20:44:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Filtser", "Arnold", ""], ["Le", "Hung", ""]]}, {"id": "2101.07546", "submitter": "Charlie Dickens", "authors": "Graham Cormode, Charlie Dickens, David P. Woodruff", "title": "Subspace exploration: Bounds on Projected Frequency Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an $n \\times d$ dimensional dataset $A$, a projection query specifies a\nsubset $C \\subseteq [d]$ of columns which yields a new $n \\times |C|$ array. We\nstudy the space complexity of computing data analysis functions over such\nsubspaces, including heavy hitters and norms, when the subspaces are revealed\nonly after observing the data. We show that this important class of problems is\ntypically hard: for many problems, we show $2^{\\Omega(d)}$ lower bounds.\nHowever, we present upper bounds which demonstrate space dependency better than\n$2^d$. That is, for $c,c' \\in (0,1)$ and a parameter $N=2^d$ an\n$N^c$-approximation can be obtained in space $\\min(N^{c'},n)$, showing that it\nis possible to improve on the na\\\"{i}ve approach of keeping information for all\n$2^d$ subsets of $d$ columns. Our results are based on careful constructions of\ninstances using coding theory and novel combinatorial reductions that exhibit\nsuch space-approximation tradeoffs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:18:22 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Cormode", "Graham", ""], ["Dickens", "Charlie", ""], ["Woodruff", "David P.", ""]]}, {"id": "2101.07550", "submitter": "Louis Dublois", "authors": "Louis Dublois, Michael Lampis, Vangelis Th. Paschos", "title": "Upper Dominating Set: Tight Algorithms for Pathwidth and Sub-Exponential\n  Approximation", "comments": "This paper has been accepted to CIAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An upper dominating set is a minimal dominating set in a graph. In the\n\\textsc{Upper Dominating Set} problem, the goal is to find an upper dominating\nset of maximum size. We study the complexity of parameterized algorithms for\n\\textsc{Upper Dominating Set}, as well as its sub-exponential approximation.\nFirst, we prove that, under ETH, \\textsc{$k$-Upper Dominating Set} cannot be\nsolved in time $O(n^{o(k)})$ (improving on $O(n^{o(\\sqrt{k})})$), and in the\nsame time we show under the same complexity assumption that for any constant\nratio $r$ and any $\\varepsilon > 0$, there is no $r$-approximation algorithm\nrunning in time $O(n^{k^{1-\\varepsilon}})$. Then, we settle the problem's\ncomplexity parameterized by pathwidth by giving an algorithm running in time\n$O^*(6^{pw})$ (improving the current best $O^*(7^{pw})$), and a lower bound\nshowing that our algorithm is the best we can get under the SETH. Furthermore,\nwe obtain a simple sub-exponential approximation algorithm for this problem: an\nalgorithm that produces an $r$-approximation in time $n^{O(n/r)}$, for any\ndesired approximation ratio $r < n$. We finally show that this\ntime-approximation trade-off is tight, up to an arbitrarily small constant in\nthe second exponent: under the randomized ETH, and for any ratio $r > 1$ and\n$\\varepsilon > 0$, no algorithm can output an $r$-approximation in time\n$n^{(n/r)^{1-\\varepsilon}}$. Hence, we completely characterize the\napproximability of the problem in sub-exponential time.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:31:08 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Dublois", "Louis", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "2101.07554", "submitter": "Tillmann Miltzow", "authors": "Fabian Klute, Meghana M. Reddy, Tillmann Miltzow", "title": "Local Complexity of Polygons", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many problems in Discrete and Computational Geometry deal with simple\npolygons or polygonal regions. Many algorithms and data-structures perform\nconsiderably faster, if the underlying polygonal region has low local\ncomplexity. One obstacle to make this intuition rigorous, is the lack of a\nformal definition of local complexity. Here, we give two possible definitions\nand show how they are related in a combinatorial sense. We say that a polygon\n$P$ has point visibility width $w=pvw$, if there is no point $q\\in P$ that sees\nmore than $w$ reflex vertices. We say that a polygon $P$ has chord visibility\nwidth $w=cvw $, if there is no chord $c=\\textrm{seg}(a,b)\\subset P$ that sees\nmore than w reflex vertices. We show that \\[ cvw \\leq pvw ^{O( pvw )},\\] for\nany simple polygon. Furthermore, we show that there exists a simple polygon\nwith \\[ cvw \\geq 2^{\\Omega( pvw )}.\\]\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:38:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Klute", "Fabian", ""], ["Reddy", "Meghana M.", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "2101.07590", "submitter": "Orr Fischer", "authors": "Keren Censor-Hillel, Orr Fischer, Tzlil Gonen, Fran\\c{c}ois Le Gall,\n  Dean Leitersdorf, Rotem Oshman", "title": "Fast Distributed Algorithms for Girth, Cycles and Small Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we give fast distributed graph algorithms for detecting and\nlisting small subgraphs, and for computing or approximating the girth. Our\nalgorithms improve upon the state of the art by polynomial factors, and for\ngirth, we obtain an constant-time algorithm for additive +1 approximation in\nthe Congested Clique, and the first parametrized algorithm for exact\ncomputation in CONGEST.\n  In the Congested Clique, we develop a technique for learning small\nneighborhoods, and apply it to obtain an $O(1)$-round algorithm that computes\nthe girth with only an additive +1 error. Next, we introduce a new technique\n(the partition tree technique) allowing for efficiently and deterministically\nlisting all copies of any subgraph, improving upon the state-of the-art for\nnon-dense graphs. We give two applications of this technique: First we show\nthat for constant $k$, $C_{2k}$-detection can be solved in $O(1)$ rounds in the\nCongested Clique, improving on prior work which used matrix multiplication and\nhad polynomial round complexity. Second, we show that in triangle-free graphs,\nthe girth can be exactly computed in time polynomially faster than the best\nknown bounds for general graphs.\n  In CONGEST, we describe a new approach for finding cycles, and apply it in\ntwo ways: first we show a fast parametrized algorithm for girth with round\ncomplexity $\\tilde{O}(\\min(g\\cdot n^{1-1/\\Theta(g)},n))$ for any girth $g$; and\nsecond, we show how to find small even-length cycles $C_{2k}$ for $k = 3,4,5$\nin $O(n^{1-1/k})$ rounds, which is a polynomial improvement upon the previous\nrunning times.\n  Finally, using our improved $C_6$-freeness algorithm and the barrier on\nproving lower bounds on triangle-freeness of Eden et al., we show that\nimproving the current $\\tilde\\Omega(\\sqrt{n})$ lower bound for $C_6$-freeness\nof Korhonen et al. by any polynomial factor would imply strong circuit\ncomplexity lower bounds.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:29:24 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Fischer", "Orr", ""], ["Gonen", "Tzlil", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Leitersdorf", "Dean", ""], ["Oshman", "Rotem", ""]]}, {"id": "2101.07726", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Ashwin Sah, Mehtaab Sawhney", "title": "Anticoncentration versus the number of subset sums", "comments": "10 pages; revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\vec{w} = (w_1,\\dots, w_n) \\in \\mathbb{R}^{n}$. We show that for any\n$n^{-2}\\le\\epsilon\\le 1$, if \\[\\#\\{\\vec{\\xi} \\in \\{0,1\\}^{n}: \\langle\n\\vec{\\xi}, \\vec{w} \\rangle = \\tau\\} \\ge 2^{-\\epsilon n}\\cdot 2^{n}\\] for some\n$\\tau \\in \\mathbb{R}$, then \\[\\#\\{\\langle \\vec{\\xi}, \\vec{w} \\rangle :\n\\vec{\\xi} \\in \\{0,1\\}^{n}\\} \\le 2^{O(\\sqrt{\\epsilon}n)}.\\] This exponentially\nimproves the $\\epsilon$ dependence in a recent result of Nederlof, Pawlewicz,\nSwennenhuis, and W\\k{e}grzycki and leads to a similar improvement in the\nparameterized (by the number of bins) runtime of bin packing.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:59:03 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 21:48:47 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Jain", "Vishesh", ""], ["Sah", "Ashwin", ""], ["Sawhney", "Mehtaab", ""]]}, {"id": "2101.07813", "submitter": "Gian Giacomo Guerreschi", "authors": "Gian Giacomo Guerreschi", "title": "Solving Quadratic Unconstrained Binary Optimization with\n  divide-and-conquer and quantum algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic Unconstrained Binary Optimization (QUBO) is a broad class of\noptimization problems with many practical applications. To solve its hard\ninstances in an exact way, known classical algorithms require exponential time\nand several approximate methods have been devised to reduce such cost. With the\ngrowing maturity of quantum computing, quantum algorithms have been proposed to\nspeed up the solution by using either quantum annealers or universal quantum\ncomputers. Here we apply the divide-and-conquer approach to reduce the original\nproblem to a collection of smaller problems whose solutions can be assembled to\nform a single Polynomial Binary Unconstrained Optimization instance with fewer\nvariables. This technique can be applied to any QUBO instance and leads to\neither an all-classical or a hybrid quantum-classical approach. When quantum\nheuristics like the Quantum Approximate Optimization Algorithm (QAOA) are used,\nour proposal leads to a double advantage: a substantial reduction of quantum\nresources, specifically an average of ~42% fewer qubits to solve MaxCut on\nrandom 3-regular graphs, together with an improvement in the quality of the\napproximate solutions reached.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:00:40 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Guerreschi", "Gian Giacomo", ""]]}, {"id": "2101.07856", "submitter": "Daniel Paulusma", "authors": "Barnaby Martin and Daniel Paulusma and Siani Smith", "title": "Colouring Graphs of Bounded Diameter in the Absence of Small Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $k\\geq 1$, a $k$-colouring $c$ of $G$ is a mapping from $V(G)$ to\n$\\{1,2,\\ldots,k\\}$ such that $c(u)\\neq c(v)$ for any two non-adjacent vertices\n$u$ and $v$. The $k$-Colouring problem is to decide if a graph $G$ has a\n$k$-colouring. For a family of graphs ${\\cal H}$, a graph $G$ is ${\\cal\nH}$-free if $G$ does not contain any graph from ${\\cal H}$ as an induced\nsubgraph. Let $C_s$ be the $s$-vertex cycle. In previous work (MFCS 2019) we\nexamined the effect of bounding the diameter on the complexity of $3$-Colouring\nfor $(C_3,\\ldots,C_s)$-free graphs and $H$-free graphs where $H$ is some\npolyad. Here, we prove for certain small values of $s$ that $3$-Colouring is\npolynomial-time solvable for $C_s$-free graphs of diameter $2$ and\n$(C_4,C_s)$-free graphs of diameter $2$. In fact, our results hold for the more\ngeneral problem List $3$-Colouring. We complement these results with some\nhardness result for diameter $4$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:42:11 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Martin", "Barnaby", ""], ["Paulusma", "Daniel", ""], ["Smith", "Siani", ""]]}, {"id": "2101.07881", "submitter": "Carola Doerr", "authors": "Carola Doerr and Lu\\'is Paquete", "title": "Star Discrepancy Subset Selection: Problem Formulation and Efficient\n  Approaches for Low Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in instance selection, we introduce the \\emph{star\ndiscrepancy subset selection problem}, which consists of finding a subset of\n\\(m\\) out of \\(n\\) points that minimizes the star discrepancy. We introduce two\nmixed integer linear formulations (MILP) and a combinatorial branch-and-bound\n(BB) algorithm for this problem and we evaluate our approaches against random\nsubset selection and a greedy construction on different use-cases in dimension\ntwo and three. Our results show that one of the MILPs and BB are efficient in\ndimension two for large and small $m/n$ ratio, respectively, and for not too\nlarge $n$. However, the performance of both approaches decays strongly for\nlarger dimensions and set sizes.\n  As a side effect of our empirical comparisons we obtain point sets of\ndiscrepancy values that are much smaller than those of common low-discrepancy\nsequences, random point sets, and of Latin Hypercube Sampling. This suggests\nthat subset selection could be an interesting approach for generating point\nsets of small discrepancy value.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 22:24:41 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Doerr", "Carola", ""], ["Paquete", "Lu\u00eds", ""]]}, {"id": "2101.07981", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Cody Freitag, Ziteng Sun,\n  Himanshu Tyagi", "title": "Inference under Information Constraints III: Local Privacy Constraints", "comments": "To appear in the Special Issue on Privacy and Security of Information\n  Systems of the IEEE Journal on Selected Areas in Information Theory (JSAIT),\n  2021. Journal version of the AISTATS'19 paper \"Test without Trust: Optimal\n  Locally Private Distribution Testing\" (arXiv:1808.02174), which it extends\n  and supersedes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study goodness-of-fit and independence testing of discrete distributions\nin a setting where samples are distributed across multiple users. The users\nwish to preserve the privacy of their data while enabling a central server to\nperform the tests. Under the notion of local differential privacy, we propose\nsimple, sample-optimal, and communication-efficient protocols for these two\nquestions in the noninteractive setting, where in addition users may or may not\nshare a common random seed. In particular, we show that the availability of\nshared (public) randomness greatly reduces the sample complexity. Underlying\nour public-coin protocols are privacy-preserving mappings which, when applied\nto the samples, minimally contract the distance between their respective\nprobability distributions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:07:49 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Freitag", "Cody", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2101.08208", "submitter": "Zhao Song", "authors": "Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao", "title": "Solving Tall Dense SDPs in the Current Matrix Multiplication Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces a new interior point method algorithm that solves\nsemidefinite programming (SDP) with variable size $n \\times n$ and $m$\nconstraints in the (current) matrix multiplication time $m^{\\omega}$ when $m\n\\geq \\Omega(n^2)$. Our algorithm is optimal because even finding a feasible\nmatrix that satisfies all the constraints requires solving an linear system in\n$m^{\\omega}$ time. Our work improves the state-of-the-art SDP solver [Jiang,\nKathuria, Lee, Padmanabhan and Song, FOCS 2020], and it is the first result\nthat SDP can be solved in the optimal running time.\n  Our algorithm is based on two novel techniques:\n  $\\bullet$ Maintaining the inverse of a Kronecker product using lazy updates.\n  $\\bullet$ A general amortization scheme for positive semidefinite matrices.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:41:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Huang", "Baihe", ""], ["Jiang", "Shunhua", ""], ["Song", "Zhao", ""], ["Tao", "Runzhou", ""]]}, {"id": "2101.08561", "submitter": "Meike Neuwohner", "authors": "Stefan Hougardy, Meike Neuwohner, Ulrike Schorr", "title": "A Fast Optimal Double Row Legalization Algorithm", "comments": "8 pages, 3 figures, to be published in ISPD'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In Placement Legalization, it is often assumed that (almost) all standard\ncells possess the same height and can therefore be aligned in cell rows, which\ncan then be treated independently. However, this is no longer true for recent\ntechnologies, where a substantial number of cells of double- or even arbitrary\nmultiple-row height is to be expected. Due to interdependencies between the\ncell placements within several rows, the legalization task becomes considerably\nharder. In this paper, we show how to optimize quadratic cell movement for\npairs of adjacent rows comprising cells of single- as well as double-row height\nwith a fixed left-to-right ordering in time $\\mathcal{O}(n\\cdot\\log(n))$,\nwhereby $n$ denotes the number of cells involved. Opposed to prior works, we\nthereby do not artificially bound the maximum cell movement and can guarantee\nto find an optimum solution. Experimental results show an average percental\ndecrease of over $26\\%$ in the total quadratic movement when compared to a\nlegalization approach that fixes cells of more than single-row height after\nGlobal Placement.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:39:16 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hougardy", "Stefan", ""], ["Neuwohner", "Meike", ""], ["Schorr", "Ulrike", ""]]}, {"id": "2101.08637", "submitter": "Gordon Hoi", "authors": "Gordon Hoi and Frank Stephan", "title": "Improved Algorithms for the General Exact Satisfiability Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Exact Satisfiability problem asks if we can find a satisfying assignment\nto each clause such that exactly one literal in each clause is assigned $1$,\nwhile the rest are all assigned $0$. We can generalise this problem further by\ndefining that a $C^j$ clause is solved iff exactly $j$ of the literals in the\nclause are $1$ and all others are $0$. We now introduce the family of\nGeneralised Exact Satisfiability problems called G$i$XSAT as the problem to\ncheck whether a given instance consisting of $C^j$ clauses with $j \\in\n\\{0,1,\\ldots,i\\}$ for each clause has a satisfying assignment. In this paper,\nwe present faster exact polynomial space algorithms, using a nonstandard\nmeasure, to solve G$i$XSAT, for $i\\in \\{2,3,4\\}$, in $O(1.3674^n)$ time,\n$O(1.5687^n)$ time and $O(1.6545^n)$ time, respectively, using polynomial\nspace, where $n$ is the number of variables. This improves the current state of\nthe art for polynomial space algorithms from $O(1.4203^n)$ time for G$2$XSAT by\nZhou, Jiang and Yin and from $O(1.6202^n)$ time for G$3$XSAT by Dahll\\\"of and\nfrom $O(1.6844^n)$ time for G$4$XSAT which was by Dahll\\\"of as well. In\naddition, we present faster exact algorithms solving G$2$XSAT, G$3$XSAT and\nG$4$XSAT in $O(1.3188^n)$ time, $O(1.3407^n)$ time and $O(1.3536^n)$ time\nrespectively at the expense of using exponential space.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:33:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hoi", "Gordon", ""], ["Stephan", "Frank", ""]]}, {"id": "2101.08688", "submitter": "Yingdong Lu", "authors": "Soumyadip Ghosh, Yingdong Lu, Tomasz Nowicki", "title": "HMC, an example of Functional Analysis applied to Algorithms in Data\n  Mining. The convergence in $L^p$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.DS cs.LG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof of convergence of the Hamiltonian Monte Carlo algorithm in\nterms of Functional Analysis. We represent the algorithm as an operator on the\ndensity functions, and prove the convergence of iterations of this operator in\n$L^p$, for $1<p<\\infty$, and strong convergence for $2\\le p<\\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:59:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ghosh", "Soumyadip", ""], ["Lu", "Yingdong", ""], ["Nowicki", "Tomasz", ""]]}, {"id": "2101.08735", "submitter": "Jonas Schmidt", "authors": "Jonas Schmidt, Thomas Schwentick, Till Tantau, Nils Vortmeier, Thomas\n  Zeume", "title": "Work-sensitive Dynamic Complexity of Formal Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Which amount of parallel resources is needed for updating a query result\nafter changing an input? In this work we study the amount of work required for\ndynamically answering membership and range queries for formal languages in\nparallel constant time with polynomially many processors. As a prerequisite, we\npropose a framework for specifying dynamic, parallel, constant-time programs\nthat require small amounts of work. This framework is based on the dynamic\ndescriptive complexity framework by Patnaik and Immerman.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:25:47 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Schmidt", "Jonas", ""], ["Schwentick", "Thomas", ""], ["Tantau", "Till", ""], ["Vortmeier", "Nils", ""], ["Zeume", "Thomas", ""]]}, {"id": "2101.09005", "submitter": "Nicholas Coxon", "authors": "Nicholas Coxon (GRACE)", "title": "An in-place truncated Fourier transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that simple modifications to van der Hoeven's forward and inverse\ntruncated Fourier transforms allow the algorithms to be performed in-place, and\nwith only a linear overhead in complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:51:06 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Coxon", "Nicholas", "", "GRACE"]]}, {"id": "2101.09054", "submitter": "Yuval Dagan", "authors": "Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, Eylon\n  Yogev", "title": "Adversarial Laws of Large Numbers and Optimal Regret in Online\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laws of large numbers guarantee that given a large enough sample from some\npopulation, the measure of any fixed sub-population is well-estimated by its\nfrequency in the sample. We study laws of large numbers in sampling processes\nthat can affect the environment they are acting upon and interact with it.\nSpecifically, we consider the sequential sampling model proposed by Ben-Eliezer\nand Yogev (2020), and characterize the classes which admit a uniform law of\nlarge numbers in this model: these are exactly the classes that are\n\\emph{online learnable}. Our characterization may be interpreted as an online\nanalogue to the equivalence between learnability and uniform convergence in\nstatistical (PAC) learning.\n  The sample-complexity bounds we obtain are tight for many parameter regimes,\nand as an application, we determine the optimal regret bounds in online\nlearning, stated in terms of \\emph{Littlestone's dimension}, thus resolving the\nmain open question from Ben-David, P\\'al, and Shalev-Shwartz (2009), which was\nalso posed by Rakhlin, Sridharan, and Tewari (2015).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:15:19 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Alon", "Noga", ""], ["Ben-Eliezer", "Omri", ""], ["Dagan", "Yuval", ""], ["Moran", "Shay", ""], ["Naor", "Moni", ""], ["Yogev", "Eylon", ""]]}, {"id": "2101.09414", "submitter": "Yota Otachi", "authors": "Tatsuya Gima, Tesshu Hanaka, Masashi Kiyomi, Yasuaki Kobayashi, Yota\n  Otachi", "title": "Exploring the Gap Between Treedepth and Vertex Cover Through Vertex\n  Integrity", "comments": "30 pages, 5 figures, CIAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For intractable problems on graphs of bounded treewidth, two graph parameters\ntreedepth and vertex cover number have been used to obtain fine-grained\ncomplexity results. Although the studies in this direction are successful, we\nstill need a systematic way for further investigations because the graphs of\nbounded vertex cover number form a rather small subclass of the graphs of\nbounded treedepth. To fill this gap, we use vertex integrity, which is placed\nbetween the two parameters mentioned above. For several graph problems, we\ngeneralize fixed-parameter tractability results parameterized by vertex cover\nnumber to the ones parameterized by vertex integrity. We also show some finer\ncomplexity contrasts by showing hardness with respect to vertex integrity or\ntreedepth.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 04:32:07 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gima", "Tatsuya", ""], ["Hanaka", "Tesshu", ""], ["Kiyomi", "Masashi", ""], ["Kobayashi", "Yasuaki", ""], ["Otachi", "Yota", ""]]}, {"id": "2101.09918", "submitter": "Elham Havvaei", "authors": "David Eppstein, Siddharth Gupta, Elham Havvaei", "title": "Parameterized Complexity of Finding Subgraphs with Hereditary Properties\n  on Hereditary Graph Classes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity of finding subgraphs with\nhereditary properties on graphs belonging to a hereditary graph class. Given a\ngraph $G$, a non-trivial hereditary property $\\Pi$ and an integer parameter\n$k$, the general problem $P(G,\\Pi,k)$ asks whether there exists $k$ vertices of\n$G$ that induce a subgraph satisfying property $\\Pi$. This problem,\n$P(G,\\Pi,k)$ has been proved to be NP-complete by Lewis and Yannakakis. The\nparameterized complexity of this problem is shown to be W[1]-complete by Khot\nand Raman, if $\\Pi$ includes all trivial graphs but not all complete graphs and\nvice versa; and is fixed-parameter tractable (FPT), otherwise. As the problem\nis W[1]-complete on general graphs when $\\Pi$ includes all trivial graphs but\nnot all complete graphs and vice versa, it is natural to further investigate\nthe problem on restricted graph classes.\n  Motivated by this line of research, we study the problem on graphs which also\nbelong to a hereditary graph class and establish a framework which settles the\nparameterized complexity of the problem for various hereditary graph classes.\nIn particular, we show that:\n  $P(G,\\Pi,k)$ is solvable in polynomial time when the graph $G$ is\nco-bipartite and $\\Pi$ is the property of being planar, bipartite or\ntriangle-free (or vice-versa).\n  $P(G,\\Pi,k)$ is FPT when the graph $G$ is planar, bipartite or triangle-free\nand $\\Pi$ is the property of being planar, bipartite or triangle-free, or graph\n$G$ is co-bipartite and $\\Pi$ is the property of being co-bipartite.\n  $P(G,\\Pi,k)$ is W[1]-complete when the graph $G$ is $C_4$-free,\n$K_{1,4}$-free or a unit disk graph and $\\Pi$ is the property of being either\nplanar or bipartite.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 07:04:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Eppstein", "David", ""], ["Gupta", "Siddharth", ""], ["Havvaei", "Elham", ""]]}, {"id": "2101.10040", "submitter": "Cyrille W. Combettes", "authors": "Cyrille W. Combettes and Sebastian Pokutta", "title": "Complexity of Linear Minimization and Projection on Some Sets", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frank-Wolfe algorithm is a method for constrained optimization that\nrelies on linear minimizations, as opposed to projections. Therefore, a\nmotivation put forward in a large body of work on the Frank-Wolfe algorithm is\nthe computational advantage of solving linear minimizations instead of\nprojections. However, the discussions supporting this advantage are often too\nsuccinct or incomplete. In this paper, we review the complexity bounds for both\ntasks on several sets commonly used in optimization. Projection methods onto\nthe $\\ell_p$-ball, $p\\in\\left]1,2\\right[\\cup\\left]2,+\\infty\\right[$, and the\nBirkhoff polytope are also proposed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:14:34 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 17:08:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Combettes", "Cyrille W.", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2101.10473", "submitter": "Kunihiro Wasa", "authors": "Kazuhiro Kurita, Kunihiro Wasa", "title": "Constant Amortized Time Enumeration of Eulerian trails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider enumeration problems for edge-distinct and\nvertex-distinct Eulerian trails. Here, two Eulerian trails are\n\\emph{edge-distinct} if the edge sequences are not identical, and they are\n\\emph{vertex-distinct} if the vertex sequences are not identical. As the main\nresult, we propose optimal enumeration algorithms for both problems, that is,\nthese algorithm runs in $\\mathcal{O}(N)$ total time, where $N$ is the number of\nsolutions. Our algorithms are based on the reverse search technique introduced\nby [Avis and Fukuda, DAM 1996], and the push out amortization technique\nintroduced by [Uno, WADS 2015].\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 23:36:19 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kurita", "Kazuhiro", ""], ["Wasa", "Kunihiro", ""]]}, {"id": "2101.10639", "submitter": "Danny Vainstein", "authors": "Danny Vainstein, Vaggos Chatziafratis, Gui Citovsky, Anand\n  Rajagopalan, Mohammad Mahdian and Yossi Azar", "title": "Hierarchical Clustering via Sketches and Hierarchical Correlation\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Hierarchical Clustering (HC) has been considered through the lens\nof optimization. In particular, two maximization objectives have been defined.\nMoseley and Wang defined the \\emph{Revenue} objective to handle similarity\ninformation given by a weighted graph on the data points (w.l.o.g., $[0,1]$\nweights), while Cohen-Addad et al. defined the \\emph{Dissimilarity} objective\nto handle dissimilarity information. In this paper, we prove structural lemmas\nfor both objectives allowing us to convert any HC tree to a tree with constant\nnumber of internal nodes while incurring an arbitrarily small loss in each\nobjective. Although the best-known approximations are 0.585 and 0.667\nrespectively, using our lemmas we obtain approximations arbitrarily close to 1,\nif not all weights are small (i.e., there exist constants $\\epsilon, \\delta$\nsuch that the fraction of weights smaller than $\\delta$, is at most $1 -\n\\epsilon$); such instances encompass many metric-based similarity instances,\nthereby improving upon prior work. Finally, we introduce Hierarchical\nCorrelation Clustering (HCC) to handle instances that contain similarity and\ndissimilarity information simultaneously. For HCC, we provide an approximation\nof 0.4767 and for complementary similarity/dissimilarity weights (analogous to\n$+/-$ correlation clustering), we again present nearly-optimal approximations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:09:51 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Vainstein", "Danny", ""], ["Chatziafratis", "Vaggos", ""], ["Citovsky", "Gui", ""], ["Rajagopalan", "Anand", ""], ["Mahdian", "Mohammad", ""], ["Azar", "Yossi", ""]]}, {"id": "2101.10695", "submitter": "Joseph Lehec", "authors": "Joseph Lehec", "title": "The Langevin Monte Carlo algorithm in the non-smooth log-concave case", "comments": "v2: Fixes a few typos and improves a bit Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove non asymptotic polynomial bounds on the convergence of the Langevin\nMonte Carlo algorithm in the case where the potential is a convex function\nwhich is globally Lipschitz on its domain, typically the maximum of a finite\nnumber of affine functions on an arbitrary convex set. In particular the\npotential is not assumed to be gradient Lipschitz, in contrast with most (if\nnot all) existing works on the topic.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:40:03 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:28:20 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Lehec", "Joseph", ""]]}, {"id": "2101.10742", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis", "title": "A Tight Lower Bound for Edge-Disjoint Paths on Planar DAGs", "comments": "A preliminary version of the paper to appear in CIAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  (see paper for full abstract)\n  We show that the Edge-Disjoint Paths problem is W[1]-hard parameterized by\nthe number $k$ of terminal pairs, even when the input graph is a planar\ndirected acyclic graph (DAG). This answers a question of Slivkins (ESA '03,\nSIDMA '10). Moreover, under the Exponential Time Hypothesis (ETH), we show that\nthere is no $f(k)\\cdot n^{o(k)}$ algorithm for Edge-Disjoint Paths on planar\nDAGs, where $k$ is the number of terminal pairs, $n$ is the number of vertices\nand $f$ is any computable function. Our hardness holds even if both the maximum\nin-degree and maximum out-degree of the graph are at most $2$.\n  Our result shows that the $n^{O(k)}$ algorithm of Fortune et al. (TCS '80)\nfor Edge-Disjoint Paths on DAGs is asymptotically tight, even if we add an\nextra restriction of planarity. As a special case of our result, we obtain that\nEdge-Disjoint Paths on planar directed graphs is W[1]-hard parameterized by the\nnumber $k$ of terminal pairs. This answers a question of Cygan et al. (FOCS\n'13) and Schrijver (pp. 417-444, Building Bridges II, '19), and completes the\nlandscape of the parameterized complexity status of edge and vertex versions of\nthe Disjoint Paths problem on planar directed and planar undirected graphs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:25:58 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chitnis", "Rajesh", ""]]}, {"id": "2101.10836", "submitter": "Uri Stemmer", "authors": "Haim Kaplan, Yishay Mansour, Kobbi Nissim, Uri Stemmer", "title": "Separating Adaptive Streaming from Oblivious Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a streaming problem for which every adversarially-robust streaming\nalgorithm must use polynomial space, while there exists a classical (oblivious)\nstreaming algorithm that uses only polylogarithmic space. This is the first\nseparation between oblivious streaming and adversarially-robust streaming, and\nresolves one of the central open questions in adversarial robust streaming.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:05:35 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 17:56:48 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "2101.10882", "submitter": "Sidhanth Mohanty", "authors": "Siqi Liu, Sidhanth Mohanty, Prasad Raghavendra", "title": "On statistical inference when fixed points of belief propagation are\n  unstable", "comments": "Title changed. More detailed description of results and technical\n  overview in the intro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical inference problems correspond to recovering the values of a\nset of hidden variables from sparse observations on them. For instance, in a\nplanted constraint satisfaction problem such as planted 3-SAT, the clauses are\nsparse observations from which the hidden assignment is to be recovered. In the\nproblem of community detection in a stochastic block model, the community\nlabels are hidden variables that are to be recovered from the edges of the\ngraph.\n  Inspired by ideas from statistical physics, the presence of a stable fixed\npoint for belief propogation has been widely conjectured to characterize the\ncomputational tractability of these problems. For community detection in\nstochastic block models, many of these predictions have been rigorously\nconfirmed.\n  In this work, we consider a general model of statistical inference problems\nthat includes both community detection in stochastic block models, and all\nplanted constraint satisfaction problems as special cases. We carry out the\ncavity method calculations from statistical physics to compute the regime of\nparameters where detection and recovery should be algorithmically tractable. At\nprecisely the predicted tractable regime, we give:\n  (i) a general polynomial-time algorithm for the problem of detection:\ndistinguishing an input with a planted signal from one without;\n  (ii) a general polynomial-time algorithm for the problem of recovery:\noutputting a vector that correlates with the hidden assignment significantly\nbetter than a random guess would.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:51:44 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 18:53:55 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Siqi", ""], ["Mohanty", "Sidhanth", ""], ["Raghavendra", "Prasad", ""]]}, {"id": "2101.10890", "submitter": "Markus Schmid", "authors": "Markus L. Schmid and Nicole Schweikardt", "title": "Spanner Evaluation over SLP-Compressed Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating regular spanners over compressed\ndocuments, i.e., we wish to solve evaluation tasks directly on the compressed\ndata, without decompression. As compressed forms of the documents we use\nstraight-line programs (SLPs) -- a lossless compression scheme for textual data\nwidely used in different areas of theoretical computer science and particularly\nwell-suited for algorithmics on compressed data. In terms of data complexity,\nour results are as follows. For a regular spanner M and an SLP S that\nrepresents a document D, we can solve the tasks of model checking and of\nchecking non-emptiness in time O(size(S)). Computing the set M(D) of all\nspan-tuples extracted from D can be done in time O(size(S) size(M(D))), and\nenumeration of M(D) can be done with linear preprocessing O(size(S)) and a\ndelay of O(depth(S)), where depth(S) is the depth of S's derivation tree. Note\nthat size(S) can be exponentially smaller than the document's size |D|; and,\ndue to known balancing results for SLPs, we can always assume that depth(S) =\nO(log(|D|)) independent of D's compressibility. Hence, our enumeration\nalgorithm has a delay logarithmic in the size of the non-compressed data and a\npreprocessing time that is at best (i.e., in the case of highly compressible\ndocuments) also logarithmic, but at worst still linear. Therefore, in a\nbig-data perspective, our enumeration algorithm for SLP-compressed documents\nmay nevertheless beat the known linear preprocessing and constant delay\nalgorithms for non-compressed documents.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:37:35 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Schmid", "Markus L.", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "2101.10898", "submitter": "Ying Cao", "authors": "Ying Cao, Bo Sun, and Danny H.K. Tsang", "title": "Online Network Utility Maximization: Algorithm, Competitive Analysis,\n  and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider an online version of the well-studied network utility\nmaximization problem, where users arrive one by one and an operator makes\nirrevocable decisions for each user without knowing the details of future\narrivals. We propose a threshold-based algorithm and analyze its worst-case\nperformance. We prove that the competitive ratio of the proposed algorithm is\nlinearly increasing in the number of links in a network and show this\ncompetitive analysis is tight. Extensive trace-driven simulations are conducted\nto demonstrate the performance of our proposed algorithm. In addition, since\nworst-case scenarios rarely occur in practice, we devise an adaptive\nimplementation of our algorithm to improve its average-case performance and\nvalidate its effectiveness via simulations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:06:51 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cao", "Ying", ""], ["Sun", "Bo", ""], ["Tsang", "Danny H. K.", ""]]}, {"id": "2101.10905", "submitter": "Francesco Silvestri", "authors": "Martin Aum\\\"uller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh,\n  Francesco Silvestri", "title": "Sampling a Near Neighbor in High Dimensions -- Who is the Fairest of\n  Them All?", "comments": "arXiv admin note: text overlap with arXiv:1906.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. Given a set of points $S$ and a radius parameter\n$r>0$, the $r$-near neighbor ($r$-NN) problem asks for a data structure that,\ngiven any query point $q$, returns a point $p$ within distance at most $r$ from\n$q$. In this paper, we study the $r$-NN problem in the light of individual\nfairness and providing equal opportunities: all points that are within distance\n$r$ from the query should have the same probability to be returned. In the\nlow-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS\n2014). Locality sensitive hashing (LSH), the theoretically strongest approach\nto similarity search in high dimensions, does not provide such a fairness\nguarantee. In this work, we show that LSH based algorithms can be made fair,\nwithout a significant loss in efficiency. We propose several efficient data\nstructures for the exact and approximate variants of the fair NN problem. Our\napproach works more generally for sampling uniformly from a sub-collection of\nsets of a given collection and can be used in a few other applications. We also\ndevelop a data structure for fair similarity search under inner product that\nrequires nearly-linear space and exploits locality sensitive filters. The paper\nconcludes with an experimental evaluation that highlights the inherent\nunfairness of NN data structures and shows the performance of our algorithms on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:13:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Har-Peled", "Sariel", ""], ["Mahabadi", "Sepideh", ""], ["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2101.11023", "submitter": "Taro Sakurai", "authors": "Taro Sakurai (Chiba University)", "title": "On formal concepts of random formal contexts", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In formal concept analysis, it is well-known that the number of formal\nconcepts can be exponential in the worst case. To analyze the average case, we\nintroduce a probabilistic model for random formal contexts and prove that the\naverage number of formal concepts has a superpolynomial asymptotic lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:00:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Sakurai", "Taro", "", "Chiba University"]]}, {"id": "2101.11041", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Crist\\'obal Guzm\\'an", "title": "Complementary Composite Minimization, Small Gradients in General Norms,\n  and Applications to Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Composite minimization is a powerful framework in large-scale convex\noptimization, based on decoupling of the objective function into terms with\nstructurally different properties and allowing for more flexible algorithmic\ndesign. In this work, we introduce a new algorithmic framework for\ncomplementary composite minimization, where the objective function decouples\ninto a (weakly) smooth and a uniformly convex term. This particular form of\ndecoupling is pervasive in statistics and machine learning, due to its link to\nregularization.\n  The main contributions of our work are summarized as follows. First, we\nintroduce the problem of complementary composite minimization in general normed\nspaces; second, we provide a unified accelerated algorithmic framework to\naddress broad classes of complementary composite minimization problems; and\nthird, we prove that the algorithms resulting from our framework are\nnear-optimal in most of the standard optimization settings. Additionally, we\nshow that our algorithmic framework can be used to address the problem of\nmaking the gradients small in general normed spaces. As a concrete example, we\nobtain a nearly-optimal method for the standard $\\ell_1$ setup (small gradients\nin the $\\ell_\\infty$ norm), essentially matching the bound of Nesterov (2012)\nthat was previously known only for the Euclidean setup. Finally, we show that\nour composite methods are broadly applicable to a number of regression\nproblems, leading to complexity bounds that are either new or match the best\nexisting ones.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:21:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""]]}, {"id": "2101.11070", "submitter": "Zhaoheng Li", "authors": "Zhaoheng Li, Xinyu Pi, Mingyuan Wu, Hanghang Tong", "title": "REFORM: Fast and Adaptive Solution for Subteam Replacement", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the novel problem of Subteam Replacement: given a\nteam of people embedded in a social network to complete a certain task, and a\nsubset of members - subteam - in this team which have become unavailable, find\nanother set of people who can perform the subteam's role in the larger team.\nThe ability to simultaneously replace multiple team members is highly\nappreciated in settings such as corporate management where team structure is\nhighly volatile and large-scale changes are commonplace. We conjecture that a\ngood candidate subteam should have high skill and structural similarity with\nthe replaced subteam while sharing a similar connection with the larger team as\na whole. Based on this conjecture, we propose a novel graph kernel which\nevaluates the goodness of candidate subteams in this holistic way freely\nadjustable to the need of the situation. To tackle the significant\ncomputational difficulties, we combine our kernel with a fast approximate\nalgorithm which (a) employs effective pruning strategies, (b) exploits the\nsimilarity between candidate team structures to reduce kernel computations, and\n(c) features a solid theoretical bound obtained from mathematical properties of\nthe problem. We extensively test our solution on both synthetic and real\ndatasets to demonstrate its consistency and efficiency. Our proposed graph\nkernel results in more suitable replacements being proposed compared to graph\nkernels used in previous work, and our algorithm consistently outperforms\nalternative choices by finding near-optimal solutions while scaling linearly\nwith the size of the replaced subteam.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:29:47 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Zhaoheng", ""], ["Pi", "Xinyu", ""], ["Wu", "Mingyuan", ""], ["Tong", "Hanghang", ""]]}, {"id": "2101.11300", "submitter": "Dor Mesica", "authors": "Julian Enoch, Kyle Fox, Dor Mesica and Shay Mozes", "title": "Faster Algorithm for Maximum Flow in Directed Planar Graphs with Vertex\n  Capacities", "comments": "This version contains an improvement for the case there $k=o(\\log^2\n  n)$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(k^3 n \\log n \\min(k,\\log^2 n) \\log^2(nC))$-time algorithm for\ncomputing maximum integer flows in planar graphs with integer arc {\\em and\nvertex} capacities bounded by $C$, and $k$ sources and sinks. This improves by\na factor of $\\max(k^2,k\\log^2 n)$ over the fastest algorithm previously known\nfor this problem [Wang, SODA 2019].\n  The speedup is obtained by two independent ideas. First we replace an\niterative procedure of Wang that uses $O(k)$ invocations of an $O(k^3 n \\log^3\nn)$-time algorithm for maximum flow algorithm in a planar graph with $k$ apices\n[Borradaile et al., FOCS 2012, SICOMP 2017], by an alternative procedure that\nonly makes one invocation of the algorithm of Borradaile et al. Second, we show\ntwo alternatives for computing flows in the $k$-apex graphs that arise in our\nmodification of Wang's procedure faster than the algorithm of Borradaile et al.\nIn doing so, we introduce and analyze a sequential implementation of the\nparallel highest-distance push-relabel algorithm of Goldberg and Tarjan~[JACM\n1988].\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:20:22 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:00:56 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 11:45:10 GMT"}, {"version": "v4", "created": "Thu, 22 Jul 2021 08:47:15 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Enoch", "Julian", ""], ["Fox", "Kyle", ""], ["Mesica", "Dor", ""], ["Mozes", "Shay", ""]]}, {"id": "2101.11350", "submitter": "Fabien Le Floc'h", "authors": "Fabien Le Floc'h", "title": "Entropy of Mersenne-Twisters", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Mersenne-Twister is one of the most popular generators of uniform\npseudo-random numbers. It is used in many numerical libraries and software. In\nthis paper, we look at the Komolgorov entropy of the original Mersenne-Twister,\nas well as of more modern variations such as the 64-bit Mersenne-Twisters, the\nWell generators, and the Melg generators.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:29:03 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Floc'h", "Fabien Le", ""]]}, {"id": "2101.11408", "submitter": "Daniel Lemire", "authors": "Daniel Lemire", "title": "Number Parsing at a Gigabyte per Second", "comments": "Software at https://github.com/fastfloat/fast_float and\n  https://github.com/lemire/simple_fastfloat_benchmark/", "journal-ref": "Software: Practice and Experience 51 (8), 2021", "doi": "10.1002/spe.2984", "report-no": null, "categories": "cs.DS cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With disks and networks providing gigabytes per second, parsing decimal\nnumbers from strings becomes a bottleneck. We consider the problem of parsing\ndecimal numbers to the nearest binary floating-point value. The general problem\nrequires variable-precision arithmetic. However, we need at most 17 digits to\nrepresent 64-bit standard floating-point numbers (IEEE 754). Thus we can\nrepresent the decimal significand with a single 64-bit word. By combining the\nsignificand and precomputed tables, we can compute the nearest floating-point\nnumber using as few as one or two 64-bit multiplications. Our implementation\ncan be several times faster than conventional functions present in standard C\nlibraries on modern 64-bit systems (Intel, AMD, ARM and POWER9). Our work is\navailable as open source software used by major systems such as Apache Arrow\nand Yandex ClickHouse. The Go standard library has adopted a version of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:31:27 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:57:29 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 17:17:14 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 00:52:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lemire", "Daniel", ""]]}, {"id": "2101.11421", "submitter": "Shin-Cheng Mu", "authors": "Shin-Cheng Mu, Tsung-Ju Chiang", "title": "Deriving monadic quicksort (Declarative Pearl)", "comments": null, "journal-ref": "In Nakano K., Sagonas K. (eds) Functional and Logic Programming\n  (FLOPS 2020). LNCS 12073. pp 124-138. 2020", "doi": "10.1007/978-3-030-59025-3_8", "report-no": null, "categories": "cs.PL cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To demonstrate derivation of monadic programs, we present a specification of\nsorting using the non-determinism monad, and derive pure quicksort on lists and\nstate-monadic quicksort on arrays. In the derivation one may switch between\npoint-free and pointwise styles, and deploy techniques familiar to functional\nprogrammers such as pattern matching and induction on structures or on sizes.\nDerivation of stateful programs resembles reasoning backwards from the\npostcondition.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:15:46 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mu", "Shin-Cheng", ""], ["Chiang", "Tsung-Ju", ""]]}, {"id": "2101.11514", "submitter": "Manish Kumar", "authors": "Yefim Dinitz, Shlomi Dolev, Manish Kumar", "title": "Polynomial Time $k$-Shortest Multi-Criteria Prioritized and\n  All-Criteria-Disjoint Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortest secure path (routing) problem in communication networks has to\ndeal with multiple attack layers e.g., man-in-the-middle, eavesdropping, packet\ninjection, packet insertion, etc. Consider different probabilities for each\nsuch attack over an edge, probabilities that can differ across edges.\nFurthermore, usage of a single shortest path (for routing) implies possible\ntraffic bottleneck, which should be avoided if possible, which we term pathneck\nsecurity avoidance. Finding all Pareto-optimal solutions for the multi-criteria\nsingle-source single-destination shortest secure path problem with non-negative\nedge lengths might yield a solution with an exponential number of paths. In the\nfirst part of this paper, we study specific settings of the multi-criteria\nshortest secure path problem, which are based on prioritized multi-criteria and\non $k$-shortest secure paths. In the second part, we show a polynomial-time\nalgorithm that, given an undirected graph $G$ and a pair of vertices $(s,t)$,\nfinds prioritized multi-criteria $2$-disjoint (vertex/edge) shortest secure\npaths between $s$ and $t$. In the third part of the paper, we introduce the\n$k$-disjoint all-criteria-shortest secure paths problem, which is solved in\ntime $O(\\min(k|E|, |E|^{3/2}))$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:19:33 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 13:17:43 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dinitz", "Yefim", ""], ["Dolev", "Shlomi", ""], ["Kumar", "Manish", ""]]}, {"id": "2101.11559", "submitter": "Hamida Seba", "authors": "Abd Errahmane Kiouche, Julien Baste, Mohammed Haddad, Hamida Seba", "title": "A Neighborhood-preserving Graph Summarization", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce in this paper a new summarization method for large graphs. Our\nsummarization approach retains only a user-specified proportion of the\nneighbors of each node in the graph. Our main aim is to simplify large graphs\nso that they can be analyzed and processed effectively while preserving as many\nof the node neighborhood properties as possible. Since many graph algorithms\nare based on the neighborhood information available for each node, the idea is\nto produce a smaller graph which can be used to allow these algorithms to\nhandle large graphs and run faster while providing good approximations.\nMoreover, our compression allows users to control the size of the compressed\ngraph by adjusting the amount of information loss that can be tolerated. The\nexperiments conducted on various real and synthetic graphs show that our\ncompression reduces considerably the size of the graphs. Moreover, we conducted\nseveral experiments on the obtained summaries using various graph algorithms\nand applications, such as node embedding, graph classification and shortest\npath approximations. The obtained results show interesting trade-offs between\nthe algorithms runtime speed-up and the precision loss.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:32:32 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kiouche", "Abd Errahmane", ""], ["Baste", "Julien", ""], ["Haddad", "Mohammed", ""], ["Seba", "Hamida", ""]]}, {"id": "2101.11783", "submitter": "Cheng Mao", "authors": "Cheng Mao, Mark Rudelson, and Konstantin Tikhomirov", "title": "Random Graph Matching with Improved Noise Robustness", "comments": "34 pages. Accepted for presentation at Conference on Learning Theory\n  (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching, also known as network alignment, refers to finding a\nbijection between the vertex sets of two given graphs so as to maximally align\ntheir edges. This fundamental computational problem arises frequently in\nmultiple fields such as computer vision and biology. Recently, there has been a\nplethora of work studying efficient algorithms for graph matching under\nprobabilistic models. In this work, we propose a new algorithm for graph\nmatching: Our algorithm associates each vertex with a signature vector using a\nmultistage procedure and then matches a pair of vertices from the two graphs if\ntheir signature vectors are close to each other. We show that, for two\nErd\\H{o}s--R\\'enyi graphs with edge correlation $1-\\alpha$, our algorithm\nrecovers the underlying matching exactly with high probability when $\\alpha \\le\n1 / (\\log \\log n)^C$, where $n$ is the number of vertices in each graph and $C$\ndenotes a positive universal constant. This improves the condition $\\alpha \\le\n1 / (\\log n)^C$ achieved in previous work.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:39:27 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 21:11:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mao", "Cheng", ""], ["Rudelson", "Mark", ""], ["Tikhomirov", "Konstantin", ""]]}, {"id": "2101.12101", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Puqian Wang", "title": "Potential Function-based Framework for Making the Gradients Small in\n  Convex and Min-Max Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making the gradients small is a fundamental optimization problem that has\neluded unifying and simple convergence arguments in first-order optimization,\nso far primarily reserved for other convergence criteria, such as reducing the\noptimality gap. We introduce a novel potential function-based framework to\nstudy the convergence of standard methods for making the gradients small in\nsmooth convex optimization and convex-concave min-max optimization. Our\nframework is intuitive and it provides a lens for viewing algorithms that make\nthe gradients small as being driven by a trade-off between reducing either the\ngradient norm or a certain notion of an optimality gap. On the lower bounds\nside, we discuss tightness of the obtained convergence results for the convex\nsetup and provide a new lower bound for minimizing norm of cocoercive operators\nthat allows us to argue about optimality of methods in the min-max setup.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:41:00 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Wang", "Puqian", ""]]}, {"id": "2101.12158", "submitter": "Nikolaos Tziavelis", "authors": "Nikolaos Tziavelis, Wolfgang Gatterbauer, Mirek Riedewald", "title": "Beyond Equi-joins: Ranking, Enumeration and Factorization", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study theta-joins in general and join predicates with conjunctions and\ndisjunctions of inequalities in particular, focusing on ranked enumeration\nwhere the answers are returned incrementally in an order dictated by a given\nranking function. Our approach achieves strong time and space complexity\nproperties: with $n$ denoting the number of tuples in the database, we\nguarantee for acyclic full join queries with inequality conditions that for\nevery value of $k$, the $k$ top-ranked answers are returned in $O(n\n\\operatorname{polylog} n + k \\log k)$ time. This is within a polylogarithmic\nfactor of the best known complexity for equi-joins and even of\n$\\mathcal{O}(n+k)$, the time it takes to look at the input and return $k$\nanswers in any order. Our guarantees extend to join queries with selections and\nmany types of projections, such as the so-called free-connex queries.\nRemarkably, they hold even when the entire output is of size $n^\\ell$ for a\njoin of $\\ell$ relations. The key ingredient is a novel $\\mathcal{O}(n\n\\operatorname{polylog} n)$-size factorized representation of the query output,\nwhich is constructed on-the-fly for a given query and database. In addition to\nproviding the first non-trivial theoretical guarantees beyond equi-joins, we\nshow in an experimental study that our ranked-enumeration approach is also\nmemory-efficient and fast in practice, beating the running time of\nstate-of-the-art database systems by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:12:26 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 19:37:00 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 10:00:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tziavelis", "Nikolaos", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2101.12160", "submitter": "Daan Rutten", "authors": "Daan Rutten, Debankur Mukherjee", "title": "A New Approach to Capacity Scaling Augmented With Unreliable Machine\n  Learning Predictions", "comments": "47 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NI cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers suffer from immense power consumption. The erratic\nbehavior of internet traffic forces data centers to maintain excess capacity in\nthe form of idle servers in case the workload suddenly increases. As an idle\nserver still consumes a significant fraction of the peak energy, data center\noperators have heavily invested in capacity scaling solutions. In simple terms,\nthese aim to deactivate servers if the demand is low and to activate them again\nwhen the workload increases. To do so, an algorithm needs to strike a delicate\nbalance between power consumption, flow-time, and switching costs. Over the\nlast decade, the research community has developed competitive online algorithms\nwith worst-case guarantees. In the presence of historic data patterns,\nprescription from Machine Learning (ML) predictions typically outperform such\ncompetitive algorithms. This, however, comes at the cost of sacrificing the\nrobustness of performance, since unpredictable surges in the workload are not\nuncommon. The current work builds on the emerging paradigm of augmenting\nunreliable ML predictions with online algorithms to develop novel robust\nalgorithms that enjoy the benefits of both worlds.\n  We analyze a continuous-time model for capacity scaling, where the goal is to\nminimize the weighted sum of flow-time, switching cost, and power consumption\nin an online fashion. We propose a novel algorithm, called Adaptive Balanced\nCapacity Scaling (ABCS), that has access to black-box ML predictions, but is\ncompletely oblivious to the accuracy of these predictions. In particular, if\nthe predictions turn out to be accurate in hindsight, we prove that ABCS is\n$(1+\\varepsilon)$-competitive. Moreover, even when the predictions are\ninaccurate, ABCS guarantees a bounded competitive ratio. The performance of the\nABCS algorithm on a real-world dataset positively support the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:14:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rutten", "Daan", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "2101.12334", "submitter": "Aida Sheshbolouki", "authors": "Aida Sheshbolouki and M. Tamer \\\"Ozsu", "title": "sGrapp: Butterfly Approximation in Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of butterfly (i.e. (2,2)-bicliques) counting\nin bipartite streaming graphs. Similar to triangles in unipartite graphs,\nenumerating butterflies is crucial in understanding the structure of bipartite\ngraphs. This benefits many applications where studying the cohesion in a graph\nshaped data is of particular interest. Examples include investigating the\nstructure of computational graphs or input graphs to the algorithms, as well as\ndynamic phenomena and analytic tasks over complex real graphs. Butterfly\ncounting is computationally expensive, and known techniques do not scale to\nlarge graphs; the problem is even harder in streaming graphs. In this paper,\nfollowing a data-driven methodology, we first conduct an empirical analysis to\nuncover temporal organizing principles of butterflies in real streaming graphs\nand then we introduce an approximate adaptive window-based algorithm, sGrapp,\nfor counting butterflies as well as its optimized version sGrapp-x. sGrapp is\ndesigned to operate efficiently and effectively over any graph stream with any\ntemporal behavior. Experimental studies of sGrapp and sGrapp-x show superior\nperformance in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:58:12 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 21:39:53 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 16:50:55 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Sheshbolouki", "Aida", ""], ["\u00d6zsu", "M. Tamer", ""]]}, {"id": "2101.12341", "submitter": "Travis Gagie", "authors": "Travis Gagie", "title": "$r$-indexing Wheeler graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a Wheeler graph and $r$ be the number of runs in a Burrows-Wheeler\nTransform of $G$, and suppose $G$ can be decomposed into $\\upsilon$\nedge-disjoint directed paths whose internal vertices each have in- and\nout-degree exactly 1. We show how to store $G$ in $O (r + \\upsilon)$ space such\nthat later, given a pattern $P$, in $O (|P| \\log \\log |G|)$ time we can count\nthe vertices of $G$ reachable by directed paths labelled $P$, and then report\nthose vertices in $O (\\log \\log |G|)$ time per vertex.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:12:29 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Gagie", "Travis", ""]]}]