[{"id": "1505.00001", "submitter": "Asbj{\\o}rn Br{\\ae}ndeland", "authors": "Asbj{\\o}rn Br{\\ae}ndeland", "title": "Rule based lexicographical permutation sequences", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a permutation sequence built by means of sub permutations the transition\nbetween successive permutations are subject to a set of n(n - 1)/2 rules that\ngroup into n - 1 matrices with a high degree of regularity. By means of these\nrules the sequence can be produced in O(3n!) time and O(n^3) space.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 22:04:00 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Br\u00e6ndeland", "Asbj\u00f8rn", ""]]}, {"id": "1505.00062", "submitter": "Ben Appleton", "authors": "Ben Appleton, Michael O'Reilly", "title": "Multi-probe consistent hashing", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a consistent hashing algorithm which performs multiple lookups\nper key in a hash table of nodes. It requires no additional storage beyond the\nhash table, and achieves a peak-to-average load ratio of 1 + epsilon with just\n1 + 1/epsilon lookups per key.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 00:29:44 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Appleton", "Ben", ""], ["O'Reilly", "Michael", ""]]}, {"id": "1505.00081", "submitter": "Huang Lingxiao", "authors": "Lingxiao Huang, Jian Li, Qicai Shi", "title": "Approximation Algorithms for the Connected Sensor Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum connected sensor cover problem (MIN-CSC) and the\nbudgeted connected sensor cover (Budgeted-CSC) problem, both motivated by\nimportant applications (e.g., reduce the communication cost among sensors) in\nwireless sensor networks. In both problems, we are given a set of sensors and a\nset of target points in the Euclidean plane. In MIN-CSC, our goal is to find a\nset of sensors of minimum cardinality, such that all target points are covered,\nand all sensors can communicate with each other (i.e., the communication graph\nis connected). We obtain a constant factor approximation algorithm, assuming\nthat the ratio between the sensor radius and communication radius is bounded.\nIn Budgeted-CSC problem, our goal is to choose a set of $B$ sensors, such that\nthe number of targets covered by the chosen sensors is maximized and the\ncommunication graph is connected. We also obtain a constant approximation under\nthe same assumption.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 03:35:34 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 02:59:43 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Huang", "Lingxiao", ""], ["Li", "Jian", ""], ["Shi", "Qicai", ""]]}, {"id": "1505.00113", "submitter": "Ashley Montanaro", "authors": "Ashley Montanaro", "title": "The quantum complexity of approximating the frequency moments", "comments": "22 pages; v3: essentially published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$'th frequency moment of a sequence of integers is defined as $F_k =\n\\sum_j n_j^k$, where $n_j$ is the number of times that $j$ occurs in the\nsequence. Here we study the quantum complexity of approximately computing the\nfrequency moments in two settings. In the query complexity setting, we wish to\nminimise the number of queries to the input used to approximate $F_k$ up to\nrelative error $\\epsilon$. We give quantum algorithms which outperform the best\npossible classical algorithms up to quadratically. In the multiple-pass\nstreaming setting, we see the elements of the input one at a time, and seek to\nminimise the amount of storage space, or passes over the data, used to\napproximate $F_k$. We describe quantum algorithms for $F_0$, $F_2$ and\n$F_\\infty$ in this model which substantially outperform the best possible\nclassical algorithms in certain parameter regimes.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 08:15:20 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 19:46:20 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 14:40:12 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Montanaro", "Ashley", ""]]}, {"id": "1505.00147", "submitter": "Jesper Sindahl Nielsen", "authors": "Gerth St{\\o}lting Brodal, Jesper Sindahl Nielsen, and Jakob Truelsen", "title": "Strictly Implicit Priority Queues: On the Number of Moves and Worst-Case\n  Time", "comments": "To appear at WADS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary heap of Williams (1964) is a simple priority queue characterized\nby only storing an array containing the elements and the number of elements $n$\n- here denoted a strictly implicit priority queue. We introduce two new\nstrictly implicit priority queues. The first structure supports amortized\n$O(1)$ time Insert and $O(\\log n)$ time ExtractMin operations, where both\noperations require amortized $O(1)$ element moves. No previous implicit heap\nwith $O(1)$ time Insert supports both operations with $O(1)$ moves. The second\nstructure supports worst-case $O(1)$ time Insert and $O(\\log n)$ time (and\nmoves) ExtractMin operations. Previous results were either amortized or needed\n$O(\\log n)$ bits of additional state information between operations.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:40:07 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Brodal", "Gerth St\u00f8lting", ""], ["Nielsen", "Jesper Sindahl", ""], ["Truelsen", "Jakob", ""]]}, {"id": "1505.00164", "submitter": "Benjamin Niedermann", "authors": "Jan-Henrik Haunert, Benjamin Niedermann", "title": "An Algorithmic Framework for Labeling Network Maps", "comments": "Full version of COCOON 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing network maps automatically comprises two challenging steps, namely\nlaying out the map and placing non-overlapping labels. In this paper we tackle\nthe problem of labeling an already existing network map considering the\napplication of metro maps. We present a flexible and versatile labeling model.\nDespite its simplicity, we prove that it is NP-complete to label a single line\nof the network. For a restricted variant of that model, we then introduce an\nefficient algorithm that optimally labels a single line with respect to a given\nweighting function. Based on that algorithm, we present a general and\nsophisticated workflow for multiple metro lines, which is experimentally\nevaluated on real-world metro maps.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 11:54:40 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 12:28:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Haunert", "Jan-Henrik", ""], ["Niedermann", "Benjamin", ""]]}, {"id": "1505.00184", "submitter": "J\\'er\\'emy Barbay", "authors": "Peyman Afshani, J\\'er\\'emy Barbay, Timothy Chan", "title": "Instance Optimal Geometric Algorithms", "comments": "28 pages in fullpage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the existence of an algorithm $A$ for computing 2-d or 3-d convex\nhulls that is optimal for every point set in the following sense: for every\nsequence $\\sigma$ of $n$ points and for every algorithm $A'$ in a certain class\n$\\mathcal{A}$, the running time of $A$ on input $\\sigma$ is at most a constant\nfactor times the maximum running time of $A'$ on the worst possible permutation\nof $\\sigma$ for $A'$. We establish a stronger property: for every sequence\n$\\sigma$ of points and every algorithm $A'$, the running time of $A$ on\n$\\sigma$ is at most a constant factor times the average running time of $A'$\nover all permutations of $\\sigma$. We call algorithms satisfying these\nproperties instance-optimal in the order-oblivious and random-order setting.\nSuch instance-optimal algorithms simultaneously subsume output-sensitive\nalgorithms and distribution-dependent average-case algorithms, and all\nalgorithms that do not take advantage of the order of the input or that assume\nthe input is given in a random order. The class $\\mathcal{A}$ under\nconsideration consists of all algorithms in a decision tree model where the\ntests involve only multilinear functions with a constant number of arguments.\nTo establish an instance-specific lower bound, we deviate from traditional\nBen-Or-style proofs and adopt a new adversary argument. For 2-d convex hulls,\nwe prove that a version of the well known algorithm by Kirkpatrick and Seidel\n(1986) or Chan, Snoeyink, and Yap (1995) already attains this lower bound. For\n3-d convex hulls, we propose a new algorithm. We further obtain\ninstance-optimal results for a few other standard problems in computational\ngeometry. Our framework also reveals connection to distribution-sensitive data\nstructures and yields new results as a byproduct, for example, on on-line\northogonal range searching in 2-d and on-line halfspace range reporting in 2-d\nand 3-d.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 14:19:12 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Afshani", "Peyman", ""], ["Barbay", "J\u00e9r\u00e9my", ""], ["Chan", "Timothy", ""]]}, {"id": "1505.00212", "submitter": "Robert Piro", "authors": "Boris Motik, Yavor Nenov, Robert Piro, Ian Horrocks", "title": "Combining Rewriting and Incremental Materialisation Maintenance for\n  Datalog Programs with Equality", "comments": "All proofs contained in the appendix. 7 pages + 4 pages appendix. 7\n  algorithms and one table with evaluation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Materialisation precomputes all consequences of a set of facts and a datalog\nprogram so that queries can be evaluated directly (i.e., independently from the\nprogram). Rewriting optimises materialisation for datalog programs with\nequality by replacing all equal constants with a single representative; and\nincremental maintenance algorithms can efficiently update a materialisation for\nsmall changes in the input facts. Both techniques are critical to practical\napplicability of datalog systems; however, we are unaware of an approach that\ncombines rewriting and incremental maintenance. In this paper we present the\nfirst such combination, and we show empirically that it can speed up updates by\nseveral orders of magnitude compared to using either rewriting or incremental\nmaintenance in isolation.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 16:21:17 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Motik", "Boris", ""], ["Nenov", "Yavor", ""], ["Piro", "Robert", ""], ["Horrocks", "Ian", ""]]}, {"id": "1505.00244", "submitter": "Aleksandar Nikolov", "authors": "Aleksandar Nikolov", "title": "An Improved Private Mechanism for Small Databases", "comments": "To appear in ICALP 2015, Track A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of answering a workload of linear queries $\\mathcal{Q}$,\non a database of size at most $n = o(|\\mathcal{Q}|)$ drawn from a universe\n$\\mathcal{U}$ under the constraint of (approximate) differential privacy.\nNikolov, Talwar, and Zhang~\\cite{NTZ} proposed an efficient mechanism that, for\nany given $\\mathcal{Q}$ and $n$, answers the queries with average error that is\nat most a factor polynomial in $\\log |\\mathcal{Q}|$ and $\\log |\\mathcal{U}|$\nworse than the best possible. Here we improve on this guarantee and give a\nmechanism whose competitiveness ratio is at most polynomial in $\\log n$ and\n$\\log |\\mathcal{U}|$, and has no dependence on $|\\mathcal{Q}|$. Our mechanism\nis based on the projection mechanism of Nikolov, Talwar, and Zhang, but in\nplace of an ad-hoc noise distribution, we use a distribution which is in a\nsense optimal for the projection mechanism, and analyze it using convex duality\nand the restricted invertibility principle.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 18:55:16 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Nikolov", "Aleksandar", ""]]}, {"id": "1505.00290", "submitter": "Anup Rao", "authors": "Rasmus Kyng, Anup Rao, Sushant Sachdeva and Daniel A. Spielman", "title": "Algorithms for Lipschitz Learning on Graphs", "comments": "Code used in this work is available at\n  https://github.com/danspielman/YINSlex 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop fast algorithms for solving regression problems on graphs where\none is given the value of a function at some vertices, and must find its\nsmoothest possible extension to all vertices. The extension we compute is the\nabsolutely minimal Lipschitz extension, and is the limit for large $p$ of\n$p$-Laplacian regularization. We present an algorithm that computes a minimal\nLipschitz extension in expected linear time, and an algorithm that computes an\nabsolutely minimal Lipschitz extension in expected time $\\widetilde{O} (m n)$.\nThe latter algorithm has variants that seem to run much faster in practice.\nThese extensions are particularly amenable to regularization: we can perform\n$l_{0}$-regularization on the given values in polynomial time and\n$l_{1}$-regularization on the initial function values and on graph edge weights\nin time $\\widetilde{O} (m^{3/2})$.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 22:30:45 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 15:16:01 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Kyng", "Rasmus", ""], ["Rao", "Anup", ""], ["Sachdeva", "Sushant", ""], ["Spielman", "Daniel A.", ""]]}, {"id": "1505.00357", "submitter": "Neal E. Young", "authors": "Marek Chrobak, Mordecai Golin, J. Ian Munro and Neal E. Young", "title": "Optimal Search Trees with 2-Way Comparisons", "comments": "ERRATUM: The proof of Theorem 3 of the ISAAC'15 paper (v4 here) is\n  incorrect. Version v5 here contains: a full erratum, proofs of the other\n  results, and pointers to journal versions expanding those results", "journal-ref": "Optimal Search Trees with 2-Way Comparisons. In: Elbassioni K.,\n  Makino K. (eds) Algorithms and Computation. ISAAC 2015. Lecture Notes in\n  Computer Science, vol 9472 (2105). Springer, Berlin, Heidelberg", "doi": "10.1007/978-3-662-48971-0_7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1971, Knuth gave an $O(n^2)$-time algorithm for the classic problem of\nfinding an optimal binary search tree. Knuth's algorithm works only for search\ntrees based on 3-way comparisons, while most modern computers support only\n2-way comparisons (e.g., $<, \\le, =, \\ge$, and $>$). Until this paper, the\nproblem of finding an optimal search tree using 2-way comparisons remained open\n-- poly-time algorithms were known only for restricted variants. We solve the\ngeneral case, giving (i) an $O(n^4)$-time algorithm and (ii) an $O(n \\log\nn)$-time additive-3 approximation algorithm. Also, for finding optimal binary\nsplit trees, we (iii) obtain a linear speedup and (iv) prove some previous work\nincorrect.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 17:12:20 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 15:22:14 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 18:47:15 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2015 15:32:58 GMT"}, {"version": "v5", "created": "Tue, 9 Mar 2021 15:58:38 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chrobak", "Marek", ""], ["Golin", "Mordecai", ""], ["Munro", "J. Ian", ""], ["Young", "Neal E.", ""]]}, {"id": "1505.00449", "submitter": "Jin-Kao Hao", "authors": "Yan Jin, Jean-Philippe Hamiez, Jin-Kao Hao", "title": "Algorithms for the minimum sum coloring problem: a review", "comments": null, "journal-ref": null, "doi": "10.1007/s10462-016-9485-7", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Sum Coloring Problem (MSCP) is a variant of the well-known vertex\ncoloring problem which has a number of AI related applications. Due to its\ntheoretical and practical relevance, MSCP attracts increasing attention. The\nonly existing review on the problem dates back to 2004 and mainly covers the\nhistory of MSCP and theoretical developments on specific graphs. In recent\nyears, the field has witnessed significant progresses on approximation\nalgorithms and practical solution algorithms. The purpose of this review is to\nprovide a comprehensive inspection of the most recent and representative MSCP\nalgorithms. To be informative, we identify the general framework followed by\npractical solution algorithms and the key ingredients that make them\nsuccessful. By classifying the main search strategies and putting forward the\ncritical elements of the reviewed methods, we wish to encourage future\ndevelopment of more powerful methods and motivate new applications.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 16:48:38 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 19:33:42 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Jin", "Yan", ""], ["Hamiez", "Jean-Philippe", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1505.00558", "submitter": "Ammar Muqaddas", "authors": "Ammar Muqaddas", "title": "Triple State QuickSort, A replacement for the C/C++ library qsort", "comments": "31 pages, 49 Figures. Minor fix in page 15 and a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An industrial grade Quicksort function along with its new algorithm is\npresented. Compared to 4 other well known implementations of Quicksort, the new\nalgorithm reduces both the number of comparisons and swaps in most cases while\nstaying close to the best of the 4 in worst cases. We trade space for\nperformance, at the price of n/2 temporary extra spaces in the worst case. Run\ntime tests reveal an overall improvement of at least 15.8% compared to the\noverall best of the other 4 functions. Furthermore, our function scores a 32.7%\nrun time improvement against Yaroslavskiy's new Dual Pivot Quicksort. Our\nfunction is pointer based, which is meant as a replacement for the C/C++\nlibrary qsort(). But we also provide an array based function of the same\nalgorithm for easy porting to different programming languages.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 08:45:38 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 11:01:43 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Muqaddas", "Ammar", ""]]}, {"id": "1505.00599", "submitter": "Emmanuel Godard", "authors": "J\\'er\\'emie Chalopin, Emmanuel Godard and Antoine Naudin", "title": "Anonymous Graph Exploration with Binoculars", "comments": "Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the exploration of networks by a mobile agent. It is long\nknown that, without global information about the graph, it is not possible to\nmake the agent halts after the exploration except if the graph is a tree. We\ntherefore endow the agent with binoculars, a sensing device that can show the\nlocal structure of the environment at a constant distance of the agent current\nlocation. We show that, with binoculars, it is possible to explore and halt in\na large class of non-tree networks. We give a complete characterization of the\nclass of networks that can be explored using binoculars using standard notions\nof discrete topology. Our characterization is constructive, we present an\nExploration algorithm that is universal; this algorithm explores any network\nexplorable with binoculars, and never halts in non-explorable networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 11:46:59 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Chalopin", "J\u00e9r\u00e9mie", ""], ["Godard", "Emmanuel", ""], ["Naudin", "Antoine", ""]]}, {"id": "1505.00612", "submitter": "P{\\aa}l Gr{\\o}n{\\aa}s Drange", "authors": "P{\\aa}l Gr{\\o}n{\\aa}s Drange and Markus Sortland Dregi and Daniel\n  Lokshtanov and Blair D. Sullivan", "title": "On the Threshold of Intractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of the graph modification problems\nThreshold Editing and Chain Editing, adding and deleting as few edges as\npossible to transform the input into a threshold (or chain) graph. In this\narticle, we show that both problems are NP-complete, resolving a conjecture by\nNatanzon, Shamir, and Sharan (Discrete Applied Mathematics, 113(1):109--128,\n2001). On the positive side, we show the problem admits a quadratic vertex\nkernel. Furthermore, we give a subexponential time parameterized algorithm\nsolving Threshold Editing in $2^{O(\\surd k \\log k)} + \\text{poly}(n)$ time,\nmaking it one of relatively few natural problems in this complexity class on\ngeneral graphs. These results are of broader interest to the field of social\nnetwork analysis, where recent work of Brandes (ISAAC, 2014) posits that the\nminimum edit distance to a threshold graph gives a good measure of consistency\nfor node centralities. Finally, we show that all our positive results extend to\nthe related problem of Chain Editing, as well as the completion and deletion\nvariants of both problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 12:46:20 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Drange", "P\u00e5l Gr\u00f8n\u00e5s", ""], ["Dregi", "Markus Sortland", ""], ["Lokshtanov", "Daniel", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1505.00619", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Abhishek Bhowmick", "title": "Using higher-order Fourier analysis over general fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order Fourier analysis, developed over prime fields, has been recently\nused in different areas of computer science, including list decoding,\nalgorithmic decomposition and testing. We extend the tools of higher-order\nFourier analysis to analyze functions over general fields. Using these new\ntools, we revisit the results in the above areas.\n  * For any fixed finite field $\\mathbb{K}$, we show that the list decoding\nradius of the generalized Reed Muller code over $\\mathbb{K}$ equals the minimum\ndistance of the code. Previously, this had been proved over prime fields [BL14]\nand for the case when $|\\mathbb{K}|-1$ divides the order of the code [GKZ08].\n  * For any fixed finite field $\\mathbb{K}$, we give a polynomial time\nalgorithm to decide whether a given polynomial $P: \\mathbb{K}^n \\to \\mathbb{K}$\ncan be decomposed as a particular composition of lesser degree polynomials.\nThis had been previously established over prime fields [Bha14, BHT15].\n  * For any fixed finite field $\\mathbb{K}$, we prove that all locally\ncharacterized affine-invariant properties of functions $f: \\mathbb{K}^n \\to\n\\mathbb{K}$ are testable with one-sided error. The same result was known when\n$\\mathbb{K}$ is prime [BFHHL13] and when the property is linear [KS08].\nMoreover, we show that for any fixed finite field $\\mathbb{F}$, an\naffine-invariant property of functions $f: \\mathbb{K}^n \\to \\mathbb{F}$, where\n$\\mathbb{K}$ is a growing field extension over $\\mathbb{F}$, is testable if it\nis locally characterized by constraints of bounded weight.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 13:05:20 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Bhowmick", "Abhishek", ""]]}, {"id": "1505.00662", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Optimal Learning via the Fourier Transform for Sums of Independent\n  Integer Random Variables", "comments": "Main differences from v1: Changed title and restructured\n  introduction. Added new sample optimal algorithm. Generalized sample lower\n  bound for any value of k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the structure and learnability of sums of independent integer random\nvariables (SIIRVs). For $k \\in \\mathbb{Z}_{+}$, a $k$-SIIRV of order $n \\in\n\\mathbb{Z}_{+}$ is the probability distribution of the sum of $n$ independent\nrandom variables each supported on $\\{0, 1, \\dots, k-1\\}$. We denote by ${\\cal\nS}_{n,k}$ the set of all $k$-SIIRVs of order $n$.\n  In this paper, we tightly characterize the sample and computational\ncomplexity of learning $k$-SIIRVs. More precisely, we design a computationally\nefficient algorithm that uses $\\widetilde{O}(k/\\epsilon^2)$ samples, and learns\nan arbitrary $k$-SIIRV within error $\\epsilon,$ in total variation distance.\nMoreover, we show that the {\\em optimal} sample complexity of this learning\nproblem is $\\Theta((k/\\epsilon^2)\\sqrt{\\log(1/\\epsilon)}).$ Our algorithm\nproceeds by learning the Fourier transform of the target $k$-SIIRV in its\neffective support. Its correctness relies on the {\\em approximate sparsity} of\nthe Fourier transform of $k$-SIIRVs -- a structural property that we establish,\nroughly stating that the Fourier transform of $k$-SIIRVs has small magnitude\noutside a small set.\n  Along the way we prove several new structural results about $k$-SIIRVs. As\none of our main structural contributions, we give an efficient algorithm to\nconstruct a sparse {\\em proper} $\\epsilon$-cover for ${\\cal S}_{n,k},$ in total\nvariation distance. We also obtain a novel geometric characterization of the\nspace of $k$-SIIRVs. Our characterization allows us to prove a tight lower\nbound on the size of $\\epsilon$-covers for ${\\cal S}_{n,k}$, and is the key\ningredient in our tight sample complexity lower bound.\n  Our approach of exploiting the sparsity of the Fourier transform in\ndistribution learning is general, and has recently found additional\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:48:01 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:03:28 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1505.00692", "submitter": "Parter Merav", "authors": "Merav Parter", "title": "Dual Failure Resilient BFS Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study {\\em breadth-first search (BFS)} spanning trees, and address the\nproblem of designing a sparse {\\em fault-tolerant} BFS structure, or {\\em\nFT-BFS } for short, resilient to the failure of up to two edges in the given\nundirected unweighted graph $G$, i.e., a sparse subgraph $H$ of $G$ such that\nsubsequent to the failure of up to two edges, the surviving part $H'$ of $H$\nstill contains a BFS spanning tree for (the surviving part of) $G$. FT-BFS\nstructures, as well as the related notion of replacement paths, have been\nstudied so far for the restricted case of a single failure. It has been noted\nwidely that when concerning shortest-paths in a variety of contexts, there is a\nsharp qualitative difference between a single failure and two or more failures.\n  Our main results are as follows. We present an algorithm that for every\n$n$-vertex unweighted undirected graph $G$ and source node $s$ constructs a\n(two edge failure) FT-BFS structure rooted at $s$ with $O(n^{5/3})$ edges. To\nprovide a useful theory of shortest paths avoiding 2 edges failures, we take a\nprincipled approach to classifying the arrangement these paths. We believe that\nthe structural analysis provided in this paper may decrease the barrier for\nunderstanding the general case of $f\\geq 2$ faults and pave the way to the\nfuture design of $f$-fault resilient structures for $f \\geq 2$. We also provide\na matching lower bound, which in fact holds for the general case of $f \\geq 1$\nand multiple sources $S \\subseteq V$. It shows that for every $f\\geq 1$, and\ninteger $1 \\leq \\sigma \\leq n$, there exist $n$-vertex graphs with a source set\n$S \\subseteq V$ of cardinality $\\sigma$ for which any FT-BFS structure rooted\nat each $s \\in S$, resilient to up to $f$-edge faults has\n$\\Omega(\\sigma^{1/(f+1)} \\cdot n^{2-1/(f+1)})$ edges.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 16:10:47 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "1505.00693", "submitter": "Sebastian Schlag", "authors": "Vitali Henne, Henning Meyerhenke, Peter Sanders, Sebastian Schlag,\n  Christian Schulz", "title": "n-Level Hypergraph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multilevel algorithm for hypergraph partitioning that contracts\nthe vertices one at a time and thus allows very high quality. This includes a\nrating function that avoids nonuniform vertex weights, an efficient\n\"semi-dynamic\" hypergraph data structure, a very fast coarsening algorithm, and\ntwo new local search algorithms. One is a $k$-way hypergraph adaptation of\nFiduccia-Mattheyses local search and gives high quality at reasonable cost. The\nother is an adaptation of size-constrained label propagation to hypergraphs.\nComparisons with hMetis and PaToH indicate that the new algorithm yields better\nquality over several benchmark sets and has a running time that is comparable\nto hMetis. Using label propagation local search is several times faster than\nhMetis and gives better quality than PaToH for a VLSI benchmark set.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 16:13:11 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Henne", "Vitali", ""], ["Meyerhenke", "Henning", ""], ["Sanders", "Peter", ""], ["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""]]}, {"id": "1505.00709", "submitter": "Meirav Zehavi", "authors": "Meirav Zehavi", "title": "Parameterized Approximation Algorithms for Packing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, many parameterized algorithms were developed for packing\nproblems. Our goal is to obtain tradeoffs that improve the running times of\nthese algorithms at the cost of computing approximate solutions. Consider a\npacking problem for which there is no known algorithm with approximation ratio\n$\\alpha$, and a parameter $k$. If the value of an optimal solution is at least\n$k$, we seek a solution of value at least $\\alpha k$; otherwise, we seek an\narbitrary solution. Clearly, if the best known parameterized algorithm that\nfinds a solution of value $t$ runs in time $O^*(f(t))$ for some function $f$,\nwe are interested in running times better than $O^*(f(\\alpha k))$. We present\ntradeoffs between running times and approximation ratios for the $P_2$-Packing,\n$3$-Set $k$-Packing and $3$-Dimensional $k$-Matching problems. Our tradeoffs\nare based on combinations of several known results, as well as a computation of\n\"approximate lopsided universal sets.\"\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 16:57:06 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Zehavi", "Meirav", ""]]}, {"id": "1505.00752", "submitter": "Asbj{\\o}rn Br{\\ae}ndeland", "authors": "Asbj{\\o}rn Br{\\ae}ndeland", "title": "A family of greedy algorithms for finding maximum independent sets", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy algorithm A iterates over a set of uniformly sized independent\nsets of a given graph G and checks for each set S which non-neighbor of S, if\nany, is best suited to be added to S, until no more suitable non-neighbors are\nfound for any of the sets. The algorithms receives as arguments the graph, the\nheuristic used to evaluate the independent set candidates, and the initial\ncardinality of the independent sets, and returns the final set of independent\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 18:56:55 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Br\u00e6ndeland", "Asbj\u00f8rn", ""]]}, {"id": "1505.00828", "submitter": "Carlo Comin MSc", "authors": "Carlo Comin, Romeo Rizzi", "title": "Dynamic Consistency of Conditional Simple Temporal Networks via Mean\n  Payoff Games: a Singly-Exponential Time DC-Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Simple Temporal Network (CSTN) is a constraint-based\ngraph-formalism for conditional temporal planning. It offers a more flexible\nformalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack,\nfrom which it was derived mainly as a sound formalization. Three notions of\nconsistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic\nconsistency is the most interesting notion, but it is also the most challenging\nand it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack\ngave a doubly-exponential time algorithm for deciding whether a CSTN is\ndynamically-consistent and to produce, in the positive case, a dynamic\nexecution strategy of exponential size. In the present work we offer a proof\nthat deciding whether a CSTN is dynamically-consistent is coNP-hard and provide\nthe first singly-exponential time algorithm for this problem, also producing a\ndynamic execution strategy whenever the input CSTN is dynamically-consistent.\nThe algorithm is based on a novel connection with Mean Payoff Games, a family\nof two-player combinatorial games on graphs well known for having applications\nin model-checking and formal verification. The presentation of such connection\nis mediated by the Hyper Temporal Network model, a tractable generalization of\nSimple Temporal Networks whose consistency checking is equivalent to\ndetermining Mean Payoff Games. In order to analyze the algorithm we introduce a\nrefined notion of dynamic-consistency, named \\epsilon-dynamic-consistency, and\npresent a sharp lower bounding analysis on the critical value of the reaction\ntime \\hat{\\varepsilon} where the CSTN transits from being, to not being,\ndynamically-consistent. The proof technique introduced in this analysis of\n\\hat{\\varepsilon} is applicable more in general when dealing with linear\ndifference constraints which include strict inequalities.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 22:14:28 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 09:48:07 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2015 08:16:45 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2015 15:42:23 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1505.00875", "submitter": "Kevin Deweese", "authors": "Erik G. Boman, Kevin Deweese, John R. Gilbert", "title": "Evaluating the Potential of a Dual Randomized Kaczmarz Solver for\n  Laplacian Linear Systems", "comments": "increased font size in figures for readability, added weak scaling\n  figures, improved citations to application areas, changed terminology\n  slightly from network graphs to irregular graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for solving Laplacian linear systems proposed by Kelner et al.\ninvolves the random sampling and update of fundamental cycles in a graph.\nKelner et al. proved asymptotic bounds on the complexity of this method but did\nnot report experimental results. We seek to both evaluate the performance of\nthis approach and to explore improvements to it in practice. We compare the\nperformance of this method to other Laplacian solvers on a variety of real\nworld graphs. We consider different ways to improve the performance of this\nmethod by exploring different ways of choosing the set of cycles and the\nsequence of updates, with the goal of providing more flexibility and potential\nparallelism. We propose a parallel model of the Kelner et al. method, for\nevaluating potential parallelism in terms of the span of edges updated at each\niteration. We provide experimental results comparing the potential parallelism\nof the fundamental cycle basis and our extended cycle set. Our preliminary\nexperiments show that choosing a non-fundamental set of cycles can save\nsignificant work compared to a fundamental cycle basis.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 03:59:47 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 19:47:05 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 04:05:02 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Boman", "Erik G.", ""], ["Deweese", "Kevin", ""], ["Gilbert", "John R.", ""]]}, {"id": "1505.00895", "submitter": "Indranil Chakrabarty", "authors": "Indranil Chakrabarty, Shahzor Khan and Vanshdeep Singh", "title": "Dynamic Grover Search: Applications in Recommendation systems and\n  Optimization problems", "comments": "8 pages, 7 figures, Accepted in Quantum Information Processing", "journal-ref": "Quantum Inf Process, 16, 153 (2017)", "doi": "10.1007/s11128-017-1600-4", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, we have seen that Grover search algorithm (Proceedings,\n28th annual ACM symposium on the theory of computing, pp. 212-219, 1996) by\nusing quantum parallelism has revolutionized the field of solving huge class of\nNP problems in comparisons to classical systems. In this work, we explore the\nidea of extending Grover search algorithm to approximate algorithms. Here we\ntry to analyze the applicability of Grover search to process an unstructured\ndatabase with a dynamic selection function in contrast to the static selection\nfunction used in the original work (Grover in Proceedings, 28th annual ACM\nsymposium on the theory of computing, pp. 212-219, 1996). We show that this\nalteration facilitates us to extend the application of Grover search to the\nfield of randomized search algorithms. Further, we use the dynamic Grover\nsearch algorithm to define the goals for a recommendation system based on which\nwe propose a recommendation algorithm which uses binomial similarity\ndistribution space giving us a quadratic speedup over traditional classical\nunstructured recommendation systems. Finally, we see how dynamic Grover search\ncan be used to tackle a wide range of optimization problems where we improve\ncomplexity over existing optimization algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 07:04:21 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 07:09:09 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Chakrabarty", "Indranil", ""], ["Khan", "Shahzor", ""], ["Singh", "Vanshdeep", ""]]}, {"id": "1505.01005", "submitter": "Levent Tun\\c{c}el", "authors": "Peruvemba Sundaram Ravi and Levent Tuncel", "title": "Approximation Ratio of LD Algorithm for Multi-Processor Scheduling and\n  the Coffman-Sethi Conjecture", "comments": "This paper, building on the intermediate results in arXiv:1312.3345\n  (by the authors and Huang) proves that the Coffman-Sethi conjecture holds. As\n  a result, this paper and arXIv:1312.3345 (cited in the current paper) have\n  many definitions and mathematical statements in common, some of them in\n  free-style text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coffman and Sethi proposed a heuristic algorithm, called LD, for\nmulti-processor scheduling, to minimize makespan over flowtime-optimal\nschedules. LD algorithm is a natural extension of a very well-known list\nscheduling algorithm, Longest Processing Time (LPT) list scheduling, to our\nbicriteria scheduling problem. Moreover, in 1976, Coffman and Sethi conjectured\nthat LD algorithm has precisely the following worst-case performance bound:\n$\\frac{5}{4} - \\frac{3}{4(4m-1)}$, where m is the number of machines. In this\npaper, utilizing some recent work by the authors and Huang, from 2013, which\nexposed some very strong combinatorial properties of various presumed minimal\ncounterexamples to the conjecture, we provide a proof of this conjecture. The\nproblem and the LD algorithm have connections to other fundamental problems\n(such as the assembly line-balancing problem) and to other algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 13:40:02 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 23:28:33 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ravi", "Peruvemba Sundaram", ""], ["Tuncel", "Levent", ""]]}, {"id": "1505.01116", "submitter": "Dinesh Kumar", "authors": "Dinesh Kumar, Pankaj Srivastava", "title": "Approaching unstructured search from function bilateral symmetry\n  detection - A quantum algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": "MTP42013IS-07", "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of symmetry is vital to problem solving. Most of the problems of\ncomputer vision and computer graphics and machine intelligence in general, can\nbe reduced to symmetry detection problem. Unstructured search problem can also\nbe looked upon from symmetry detection point of view. Unstructured search can\nbe thought as searching a binary string satisfying some search condition in an\nunsorted list of binary strings. In this paper unstructured search problem is\nreduced to function bilateral symmetry detection problem with polynomial\noverhead in terms of the size of the input.\n  Keywords: Unstructured Search, Quantum algorithm, Function bilateral symmetry\ndetection, Decision Problem, Quantum Black Box, Solving NP complete problems.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 10:45:07 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 04:12:52 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 16:26:03 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Kumar", "Dinesh", ""], ["Srivastava", "Pankaj", ""]]}, {"id": "1505.01140", "submitter": "Asbj{\\o}rn Br{\\ae}ndeland", "authors": "Asbj{\\o}rn Br{\\ae}ndeland", "title": "Depth-first search in split-by-edges trees", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A layerwise search in a split-by-edges tree (as defined by Br{\\ae}ndeland,\n2015) of agiven graph produces a maximum independent set in exponential time. A\ndepth-first search produces an independent set, which may or may not be a\nmaximum, in linear time, but the worst case success rate is maybe not high\nenough to make it really interesting. What may make depth-first searching in\nsplit-by-edges trees interesting, though, is the pronounced oscillation of its\nsuccess rate along the graph size axis.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 18:47:58 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Br\u00e6ndeland", "Asbj\u00f8rn", ""]]}, {"id": "1505.01187", "submitter": "Changshuai Wei", "authors": "Changshuai Wei, and Qing Lu", "title": "GWGGI: software for genome-wide gene-gene interaction analysis", "comments": null, "journal-ref": "BMC Genetics 2014, 15:101", "doi": "10.1186/s12863-014-0101-z", "report-no": null, "categories": "q-bio.QM cs.DS q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: While the importance of gene-gene interactions in human diseases\nhas been well recognized, identifying them has been a great challenge,\nespecially through association studies with millions of genetic markers and\nthousands of individuals. Computationally efficient and powerful tools are in\ngreat need for the identification of new gene-gene interactions in\nhigh-dimensional association studies. Result: We develop C++ software for\ngenome-wide gene-gene interaction analyses (GWGGI). GWGGI utilizes tree-based\nalgorithms to search a large number of genetic markers for a disease-associated\njoint association with the consideration of high-order interactions, and then\nuses non-parametric statistics to test the joint association. The package\nincludes two functions, likelihood ratio Mann-whitney (LRMW) and Tree\nAssembling Mann-whitney (TAMW).We optimize the data storage and computational\nefficiency of the software, making it feasible to run the genome-wide analysis\non a personal computer. The use of GWGGI was demonstrated by using two real\ndata-sets with nearly 500 k genetic markers. Conclusion: Through the empirical\nstudy, we demonstrated that the genome-wide gene-gene interaction analysis\nusing GWGGI could be accomplished within a reasonable time on a personal\ncomputer (i.e., ~3.5 hours for LRMW and ~10 hours for TAMW). We also showed\nthat LRMW was suitable to detect interaction among a small number of genetic\nvariants with moderate-to-strong marginal effect, while TAMW was useful to\ndetect interaction among a larger number of low-marginal-effect genetic\nvariants.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 21:11:22 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Wei", "Changshuai", ""], ["Lu", "Qing", ""]]}, {"id": "1505.01210", "submitter": "Lars Bonnichsen", "authors": "Lars F. Bonnichsen, Christian W. Probst, Sven Karlsson", "title": "Implementation of BT-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document presents the full implementation details of BT-trees, a highly\nefficient ordered map, and an evaluation which compares BT-trees with unordered\nmaps. BT- trees are often much faster than other ordered maps, and have\ncomparable performance to unordered map implementations. However, in benchmarks\nwhich favor unordered maps, BT-trees are not faster than the fastest unordered\nmap implementations we know of.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:40:19 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Bonnichsen", "Lars F.", ""], ["Probst", "Christian W.", ""], ["Karlsson", "Sven", ""]]}, {"id": "1505.01439", "submitter": "Marek Adamczyk", "authors": "Marek Adamczyk, Fabrizio Grandoni and Joydeep Mukherjee", "title": "Improved Approximation Algorithms for Stochastic Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the Stochastic Matching problem, which is motivated\nby applications in kidney exchange and online dating. We are given an\nundirected graph in which every edge is assigned a probability of existence and\na positive profit, and each node is assigned a positive integer called timeout.\nWe know whether an edge exists or not only after probing it. On this random\ngraph we are executing a process, which one-by-one probes the edges and\ngradually constructs a matching. The process is constrained in two ways: once\nan edge is taken it cannot be removed from the matching, and the timeout of\nnode $v$ upper-bounds the number of edges incident to $v$ that can be probed.\nThe goal is to maximize the expected profit of the constructed matching.\n  For this problem Bansal et al. (Algorithmica 2012) provided a\n$3$-approximation algorithm for bipartite graphs, and a $4$-approximation for\ngeneral graphs. In this work we improve the approximation factors to $2.845$\nand $3.709$, respectively.\n  We also consider an online version of the bipartite case, where one side of\nthe partition arrives node by node, and each time a node $b$ arrives we have to\ndecide which edges incident to $b$ we want to probe, and in which order. Here\nwe present a $4.07$-approximation, improving on the $7.92$-approximation of\nBansal et al.\n  The main technical ingredient in our result is a novel way of probing edges\naccording to a random but non-uniform permutation. Patching this method with an\nalgorithm that works best for large probability edges (plus some additional\nideas) leads to our improved approximation factors.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 17:29:44 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Adamczyk", "Marek", ""], ["Grandoni", "Fabrizio", ""], ["Mukherjee", "Joydeep", ""]]}, {"id": "1505.01446", "submitter": "Thomas Pajor", "authors": "Daniel Delling, Julian Dibbelt, Thomas Pajor, Renato F. Werneck", "title": "Public Transit Labeling", "comments": "An extended abstract of this paper has been accepted at the 14th\n  International Symposium on Experimental Algorithms (SEA'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the journey planning problem in public transit networks. Developing\nefficient preprocessing-based speedup techniques for this problem has been\nchallenging: current approaches either require massive preprocessing effort or\nprovide limited speedups. Leveraging recent advances in Hub Labeling, the\nfastest algorithm for road networks, we revisit the well-known time-expanded\nmodel for public transit. Exploiting domain-specific properties, we provide\nsimple and efficient algorithms for the earliest arrival, profile, and\nmulticriteria problems, with queries that are orders of magnitude faster than\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 17:51:01 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Delling", "Daniel", ""], ["Dibbelt", "Julian", ""], ["Pajor", "Thomas", ""], ["Werneck", "Renato F.", ""]]}, {"id": "1505.01460", "submitter": "Christian Konrad", "authors": "Christian Konrad", "title": "Maximum Matching in Turnstile Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the unweighted bipartite maximum matching problem in the one-pass\nturnstile streaming model where the input stream consists of edge insertions\nand deletions. In the insertion-only model, a one-pass $2$-approximation\nstreaming algorithm can be easily obtained with space $O(n \\log n)$, where $n$\ndenotes the number of vertices of the input graph. We show that no such result\nis possible if edge deletions are allowed, even if space $O(n^{3/2-\\delta})$ is\ngranted, for every $\\delta > 0$. Specifically, for every $0 \\le \\epsilon \\le\n1$, we show that in the one-pass turnstile streaming model, in order to compute\na $O(n^{\\epsilon})$-approximation, space $\\Omega(n^{3/2 - 4\\epsilon})$ is\nrequired for constant error randomized algorithms, and, up to logarithmic\nfactors, space $O( n^{2-2\\epsilon} )$ is sufficient. Our lower bound result is\nproved in the simultaneous message model of communication and may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 19:02:34 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Konrad", "Christian", ""]]}, {"id": "1505.01467", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna, Yang Li, Grigory Yaroslavtsev", "title": "Tight Bounds for Linear Sketches of Approximate Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve the space complexity of linear sketches for approximating the\nmaximum matching problem in dynamic graph streams where the stream may include\nboth edge insertion and deletion. Specifically, we show that for any $\\epsilon\n> 0$, there exists a one-pass streaming algorithm, which only maintains a\nlinear sketch of size $\\tilde{O}(n^{2-3\\epsilon})$ bits and recovers an\n$n^\\epsilon$-approximate maximum matching in dynamic graph streams, where $n$\nis the number of vertices in the graph. In contrast to the extensively studied\ninsertion-only model, to the best of our knowledge, no non-trivial single-pass\nstreaming algorithms were previously known for approximating the maximum\nmatching problem on general dynamic graph streams.\n  Furthermore, we show that our upper bound is essentially tight. Namely, any\nlinear sketch for approximating the maximum matching to within a factor of\n$O(n^\\epsilon)$ has to be of size $n^{2-3\\epsilon -o(1)}$ bits. We establish\nthis lower bound by analyzing the corresponding simultaneous number-in-hand\ncommunication model, with a combinatorial construction based on\nRuzsa-Szemer\\'{e}di graphs.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 19:16:17 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1505.01523", "submitter": "Mikkel Thorup", "authors": "Mikkel Thorup", "title": "Fast and Powerful Hashing using Tabulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms are often enjoyed for their simplicity, but the hash\nfunctions employed to yield the desired probabilistic guarantees are often too\ncomplicated to be practical. Here we survey recent results on how simple\nhashing schemes based on tabulation provide unexpectedly strong guarantees.\n  Simple tabulation hashing dates back to Zobrist [1970]. Keys are viewed as\nconsisting of $c$ characters and we have precomputed character tables\n$h_1,...,h_c$ mapping characters to random hash values. A key $x=(x_1,...,x_c)$\nis hashed to $h_1[x_1] \\oplus h_2[x_2].....\\oplus h_c[x_c]$. This schemes is\nvery fast with character tables in cache. While simple tabulation is not even\n4-independent, it does provide many of the guarantees that are normally\nobtained via higher independence, e.g., linear probing and Cuckoo hashing.\n  Next we consider twisted tabulation where one input character is \"twisted\" in\na simple way. The resulting hash function has powerful distributional\nproperties: Chernoff-Hoeffding type tail bounds and a very small bias for\nmin-wise hashing. This also yields an extremely fast pseudo-random number\ngenerator that is provably good for many classic randomized algorithms and\ndata-structures.\n  Finally, we consider double tabulation where we compose two simple tabulation\nfunctions, applying one to the output of the other, and show that this yields\nvery high independence in the classic framework of Carter and Wegman [1977]. In\nfact, w.h.p., for a given set of size proportional to that of the space\nconsumed, double tabulation gives fully-random hashing. We also mention some\nmore elaborate tabulation schemes getting near-optimal independence for given\ntime and space.\n  While these tabulation schemes are all easy to implement and use, their\nanalysis is not.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 21:47:25 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 10:38:35 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 21:26:42 GMT"}, {"version": "v4", "created": "Wed, 28 Dec 2016 17:12:34 GMT"}, {"version": "v5", "created": "Tue, 3 Jan 2017 16:51:14 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Thorup", "Mikkel", ""]]}, {"id": "1505.01731", "submitter": "Andrew McGregor", "authors": "Rajesh Chitnis and Graham Cormode and Hossein Esfandiari and\n  MohammadTaghi Hajiaghayi and Andrew McGregor and Morteza Monemizadeh and\n  Sofya Vorotnikova", "title": "Kernelization via Sampling with Applications to Dynamic Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a simple but powerful subgraph sampling primitive\nthat is applicable in a variety of computational models including dynamic graph\nstreams (where the input graph is defined by a sequence of edge/hyperedge\ninsertions and deletions) and distributed systems such as MapReduce. In the\ncase of dynamic graph streams, we use this primitive to prove the following\nresults:\n  -- Matching: First, there exists an $\\tilde{O}(k^2)$ space algorithm that\nreturns an exact maximum matching on the assumption the cardinality is at most\n$k$. The best previous algorithm used $\\tilde{O}(kn)$ space where $n$ is the\nnumber of vertices in the graph and we prove our result is optimal up to\nlogarithmic factors. Our algorithm has $\\tilde{O}(1)$ update time. Second,\nthere exists an $\\tilde{O}(n^2/\\alpha^3)$ space algorithm that returns an\n$\\alpha$-approximation for matchings of arbitrary size. (Assadi et al. (2015)\nshowed that this was optimal and independently and concurrently established the\nsame upper bound.) We generalize both results for weighted matching. Third,\nthere exists an $\\tilde{O}(n^{4/5})$ space algorithm that returns a constant\napproximation in graphs with bounded arboricity.\n  -- Vertex Cover and Hitting Set: There exists an $\\tilde{O}(k^d)$ space\nalgorithm that solves the minimum hitting set problem where $d$ is the\ncardinality of the input sets and $k$ is an upper bound on the size of the\nminimum hitting set. We prove this is optimal up to logarithmic factors. Our\nalgorithm has $\\tilde{O}(1)$ update time. The case $d=2$ corresponds to minimum\nvertex cover.\n  Finally, we consider a larger family of parameterized problems (including\n$b$-matching, disjoint paths, vertex coloring among others) for which our\nsubgraph sampling primitive yields fast, small-space dynamic graph stream\nalgorithms. We then show lower bounds for natural problems outside this family.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 14:59:05 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Cormode", "Graham", ""], ["Esfandiari", "Hossein", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["McGregor", "Andrew", ""], ["Monemizadeh", "Morteza", ""], ["Vorotnikova", "Sofya", ""]]}, {"id": "1505.01742", "submitter": "Sergio Consoli", "authors": "J. A. Moreno Perez and S. Consoli", "title": "On the Minimum Labelling Spanning bi-Connected Subgraph problem", "comments": "MIC 2015: The XI Metaheuristics International Conference, 3 pages,\n  Agadir, June 7-10, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the minimum labelling spanning bi-connected subgraph problem\n(MLSBP) replacing connectivity by bi-connectivity in the well known minimum\nlabelling spanning tree problem (MLSTP). A graph is bi-connected if, for every\ntwo vertices, there are, at least, two vertex-disjoint paths joining them. The\nproblem consists in finding the spanning bi-connected subgraph or block with\nminimum set of labels. We adapt the exact method of the MLSTP to solve the\nMLSTB and the basic greedy constructive heuristic, the maximum vertex covering\nalgorithm (MVCA). This proce- dure is a basic component in the application of\nmetaheuristics to solve the problem.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 15:24:53 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Perez", "J. A. Moreno", ""], ["Consoli", "S.", ""]]}, {"id": "1505.01927", "submitter": "C. Seshadhri", "authors": "C. Seshadhri", "title": "A simpler sublinear algorithm for approximating the triangle count", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent result of Eden, Levi, and Ron (ECCC 2015) provides a sublinear time\nalgorithm to estimate the number of triangles in a graph. Given an undirected\ngraph $G$, one can query the degree of a vertex, the existence of an edge\nbetween vertices, and the $i$th neighbor of a vertex. Suppose the graph has $n$\nvertices, $m$ edges, and $t$ triangles. In this model, Eden et al provided a\n$O(\\poly(\\eps^{-1}\\log n)(n/t^{1/3} + m^{3/2}/t))$ time algorithm to get a\n$(1+\\eps)$-multiplicative approximation for $t$, the triangle count. This paper\nprovides a simpler algorithm with the same running time (up to differences in\nthe $\\poly(\\eps^{-1}\\log n)$ factor) that has a substantially simpler analysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 04:53:10 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Seshadhri", "C.", ""]]}, {"id": "1505.01962", "submitter": "Peter Schneider-Kamp", "authors": "Michael Codish and Lu\\'is Cruz-Filipe and Markus Nebel and Peter\n  Schneider-Kamp", "title": "Applying Sorting Networks to Synthesize Optimized Sorting Libraries", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": "10.1007/978-3-319-27436-2_8", "report-no": null, "categories": "cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows an application of the theory of sorting networks to\nfacilitate the synthesis of optimized general purpose sorting libraries.\nStandard sorting libraries are often based on combinations of the classic\nQuicksort algorithm with insertion sort applied as the base case for small\nfixed numbers of inputs. Unrolling the code for the base case by ignoring loop\nconditions eliminates branching and results in code which is equivalent to a\nsorting network. This enables the application of further program\ntransformations based on sorting network optimizations, and eventually the\nsynthesis of code from sorting networks. We show that if considering the number\nof comparisons and swaps then theory predicts no real advantage of this\napproach. However, significant speed-ups are obtained when taking advantage of\ninstruction level parallelism and non-branching conditional assignment\ninstructions, both of which are common in modern CPU architectures. We provide\nempirical evidence that using code synthesized from efficient sorting networks\nas the base case for Quicksort libraries results in significant real-world\nspeed-ups.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 09:13:25 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 14:33:46 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Codish", "Michael", ""], ["Cruz-Filipe", "Lu\u00eds", ""], ["Nebel", "Markus", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1505.02019", "submitter": "Marc Bury", "authors": "Marc Bury, Chris Schwiegelshohn", "title": "Sublinear Estimation of Weighted Matchings in Dynamic Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for estimating the weight of a maximum\nweighted matching by augmenting any estimation routine for the size of an\nunweighted matching. The algorithm is implementable in any streaming model\nincluding dynamic graph streams. We also give the first constant estimation for\nthe maximum matching size in a dynamic graph stream for planar graphs (or any\ngraph with bounded arboricity) using $\\tilde{O}(n^{4/5})$ space which also\nextends to weighted matching. Using previous results by Kapralov, Khanna, and\nSudan (2014) we obtain a $\\mathrm{polylog}(n)$ approximation for general graphs\nusing $\\mathrm{polylog}(n)$ space in random order streams, respectively. In\naddition, we give a space lower bound of $\\Omega(n^{1-\\varepsilon})$ for any\nrandomized algorithm estimating the size of a maximum matching up to a\n$1+O(\\varepsilon)$ factor for adversarial streams.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 12:49:10 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 18:26:47 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Bury", "Marc", ""], ["Schwiegelshohn", "Chris", ""]]}, {"id": "1505.02155", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen, Wen-Hung Huang, Cong Liu", "title": "Evaluate and Compare Two Utilization-Based Schedulability-Test\n  Frameworks for Real-Time Systems", "comments": "arXiv admin note: text overlap with arXiv:1501.07084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report summarizes two general frameworks, namely k2Q and k2U, that have\nbeen recently developed by us. The purpose of this report is to provide\ndetailed evaluations and comparisons of these two frameworks. These two\nframeworks share some similar characteristics, but they are useful for\ndifferent application cases. These two frameworks together provide\ncomprehensive means for the users to automatically convert the pseudo\npolynomial-time tests (or even exponential-time tests) into polynomial-time\ntests with closed mathematical forms. With the quadratic and hyperbolic forms,\nk2Q and k2U frameworks can be used to provide many quantitive features to be\nmeasured and evaluated, like the total utilization bounds, speed-up factors,\netc., not only for uniprocessor scheduling but also for multiprocessor\nscheduling. These frameworks can be viewed as \"blackbox\" interfaces for\nproviding polynomial-time schedulability tests and response time analysis for\nreal-time applications. We have already presented their advantages for being\napplied in some models in the previous papers. However, it was not possible to\npresent a more comprehensive comparison between these two frameworks. We hope\nthis report can help the readers and users clearly understand the difference of\nthese two frameworks, their unique characteristics, and their advantages. We\ndemonstrate their differences and properties by using the traditional sporadic\nrealtime task models in uniprocessor scheduling and multiprocessor global\nscheduling.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 19:52:46 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 21:39:47 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 07:13:59 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Huang", "Wen-Hung", ""], ["Liu", "Cong", ""]]}, {"id": "1505.02250", "submitter": "Mert Pilanci", "authors": "Mert Pilanci, Martin J. Wainwright", "title": "Newton Sketch: A Linear-time Optimization Algorithm with\n  Linear-Quadratic Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized second-order method for optimization known as the\nNewton Sketch: it is based on performing an approximate Newton step using a\nrandomly projected or sub-sampled Hessian. For self-concordant functions, we\nprove that the algorithm has super-linear convergence with exponentially high\nprobability, with convergence and complexity guarantees that are independent of\ncondition numbers and related problem-dependent quantities. Given a suitable\ninitialization, similar guarantees also hold for strongly convex and smooth\nobjectives without self-concordance. When implemented using randomized\nprojections based on a sub-sampled Hadamard basis, the algorithm typically has\nsubstantially lower complexity than Newton's method. We also describe\nextensions of our methods to programs involving convex constraints that are\nequipped with self-concordant barriers. We discuss and illustrate applications\nto linear programs, quadratic programs with convex constraints, logistic\nregression and other generalized linear models, as well as semidefinite\nprograms.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:36:26 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1505.02371", "submitter": "Oliver Kullmann", "authors": "Oliver Kullmann and Joao Marques-Silva", "title": "Computing maximal autarkies with few and simple oracle queries", "comments": "18 pages; second version with editorial changes, to appear in LNCS\n  for Theory and Applications of Satisfiability Testing - SAT 2015", "journal-ref": "LNCS 9340, pages 138-155, 2015", "doi": "10.1007/978-3-319-24318-4_11", "report-no": null, "categories": "cs.LO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the algorithmic task of computing a maximal autarky for a\nclause-set F, i.e., a partial assignment which satisfies every clause of F it\ntouches, and where this property is destroyed by adding any non-empty set of\nfurther assignments. We employ SAT solvers as oracles, using various\ncapabilities. Using the standard SAT oracle, log_2(n(F)) oracle calls suffice,\nwhere n(F) is the number of variables, but the drawback is that (translated)\ncardinality constraints are employed, which makes this approach less efficient\nin practice. Using an extended SAT oracle, motivated by the capabilities of\nmodern SAT solvers, we show how to compute maximal autarkies with 2 n(F)^{1/2}\nsimpler oracle calls. This novel algorithm combines the previous two main\napproaches, based on the autarky-resolution duality and on SAT translations.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 11:36:54 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2015 13:09:47 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Kullmann", "Oliver", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "1505.02445", "submitter": "Tomaso Aste", "authors": "Guido Previde Massara, T. Di Matteo, Tomaso Aste", "title": "Network Filtering for Big Data: Triangulated Maximally Filtered Graph", "comments": "16 pages, 7 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network-filtering method, the Triangulated Maximally Filtered\nGraph (TMFG), that provides an approximate solution to the Weighted Maximal\nPlanar Graph problem. The underlying idea of TMFG consists in building a\ntriangulation that maximizes a score function associated with the amount of\ninformation retained by the network. TMFG uses as weights any arbitrary\nsimilarity measure to arrange data into a meaningful network structure that can\nbe used for clustering, community detection and modeling. The method is fast,\nadaptable and scalable to very large datasets, it allows online updating and\nlearning as new data can be inserted and deleted with combinations of local and\nnon-local moves. TMFG permits readjustments of the network in consequence of\nchanges in the strength of the similarity measure. The method is based on local\ntopological moves and can therefore take advantage of parallel and GPUs\ncomputing. We discuss how this network-filtering method can be used intuitively\nand efficiently for big data studies and its significance from an\ninformation-theoretic perspective.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:47:38 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 16:02:37 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Massara", "Guido Previde", ""], ["Di Matteo", "T.", ""], ["Aste", "Tomaso", ""]]}, {"id": "1505.02558", "submitter": "Victor Zamaraev", "authors": "Alain Hertz, Vadim Lozin, Bernard Ries, Victor Zamaraev, Dominique de\n  Werra", "title": "Dominating induced matchings in graphs containing no long claw", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An induced matching $M$ in a graph $G$ is dominating if every edge not in $M$\nshares exactly one vertex with an edge in $M$. The dominating induced matching\nproblem (also known as efficient edge domination) asks whether a graph $G$\ncontains a dominating induced matching. This problem is generally NP-complete,\nbut polynomial-time solvable for graphs with some special properties. In\nparticular, it is solvable in polynomial time for claw-free graphs. In the\npresent paper, we study this problem for graphs containing no long claw, i.e.\nno induced subgraph obtained from the claw by subdividing each of its edges\nexactly once. To solve the problem in this class, we reduce it to the following\nquestion: given a graph $G$ and a subset of its vertices, does $G$ contain a\nmatching saturating all vertices of the subset? We show that this question can\nbe answered in polynomial time, thus providing a polynomial-time algorithm to\nsolve the dominating induced matching problem for graphs containing no long\nclaw.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 10:39:24 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Hertz", "Alain", ""], ["Lozin", "Vadim", ""], ["Ries", "Bernard", ""], ["Zamaraev", "Victor", ""], ["de Werra", "Dominique", ""]]}, {"id": "1505.02681", "submitter": "Chih-Ya Shen", "authors": "Chih-Ya Shen, De-Nian Yang, Liang-Hao Huang, Wang-Chien Lee, and\n  Ming-Syan Chen", "title": "Socio-Spatial Group Queries for Impromptu Activity Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development and integration of social networking services and smartphones\nhave made it easy for individuals to organize impromptu social activities\nanywhere and anytime. Main challenges arising in organizing impromptu\nactivities are mostly due to the requirements of making timely invitations in\naccordance with the potential activity locations, corresponding to the\nlocations of and the relationship among the candidate attendees. Various\ncombinations of candidate attendees and activity locations create a large\nsolution space. Thus, in this paper, we propose Multiple Rally-Point Social\nSpatial Group Query (MRGQ), to select an appropriate activity location for a\ngroup of nearby attendees with tight social relationships. Although MRGQ is\nNP-hard, the number of attendees in practice is usually small enough such that\nan optimal solution can be found efficiently. Therefore, we first propose an\nInteger Linear Programming optimization model for MRGQ. We then design an\nefficient algorithm, called MAGS, which employs effective search space\nexploration and pruning strategies to reduce the running time for finding the\noptimal solution. We also propose to further optimize efficiency by indexing\nthe potential activity locations. A user study demonstrates the strength of\nusing MAGS over manual coordination in terms of both solution quality and\nefficiency. Experimental results on real datasets show that our algorithms can\nprocess MRGQ efficiently and significantly outperform other baseline\nalgorithms, including one based on the commercial parallel optimizer IBM CPLEX.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 15:58:31 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 10:35:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Shen", "Chih-Ya", ""], ["Yang", "De-Nian", ""], ["Huang", "Liang-Hao", ""], ["Lee", "Wang-Chien", ""], ["Chen", "Ming-Syan", ""]]}, {"id": "1505.02708", "submitter": "Tamara Mchedlidze David", "authors": "Tamara Mchedlidze", "title": "Simultaneous straight-line drawing of a planar graph and its rectangular\n  dual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to represent on the plane both a planar graph and its dual is\nto follow the definition of the dual, thus, to place vertices inside their\ncorresponding primal faces, and to draw the dual edges so that they only cross\ntheir corresponding primal edges. The problem of constructing such drawings has\na long tradition when the drawings of both primal and dual are required to be\nstraight-line. We consider the same problem for a planar graph and its\nrectangular dual. We show that the rectangular dual can be resized to host a\nplanar straight-line drawing of its primal.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 17:35:30 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Mchedlidze", "Tamara", ""]]}, {"id": "1505.02710", "submitter": "Lior Pachter", "authors": "Nicolas Bray, Harold Pimentel, P\\'all Melsted and Lior Pachter", "title": "Near-optimal RNA-Seq quantification", "comments": "- Added some results (paralog analysis, allele specific expression\n  analysis, alignment comparison, accuracy analysis with TPMs) - Switched\n  bootstrap analysis to human sample from SEQC-MAQCIII - Provided link to a\n  snakefile that allows for reproducibility of all results and figures in the\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to RNA-Seq quantification that is near optimal in\nspeed and accuracy. Software implementing the approach, called kallisto, can be\nused to analyze 30 million unaligned paired-end RNA-Seq reads in less than 5\nminutes on a standard laptop computer while providing results as accurate as\nthose of the best existing tools. This removes a major computational bottleneck\nin RNA-Seq analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 17:42:04 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 17:12:58 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Bray", "Nicolas", ""], ["Pimentel", "Harold", ""], ["Melsted", "P\u00e1ll", ""], ["Pachter", "Lior", ""]]}, {"id": "1505.02820", "submitter": "Carlos Ochoa", "authors": "Jeremy Barbay, Carlos Ochoa and Pablo Perez-Lantero", "title": "Refining the Analysis of Divide and Conquer: How and When", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide-and-conquer is a central paradigm for the design of algorithms,\nthrough which some fundamental computational problems, such as sorting arrays\nand computing convex hulls, are solved in optimal time within\n$\\Theta(n\\log{n})$ in the worst case over instances of size $n$. A finer\nanalysis of those problems yields complexities within $O(n(1 + \\mathcal{H}(n_1,\n\\dots, n_k))) \\subseteq O(n(1{+}\\log{k})) \\subseteq O(n\\log{n})$ in the worst\ncase over all instances of size $n$ composed of $k$ \"easy\" fragments of\nrespective sizes $n_1, \\dots, n_k$ summing to $n$, where the entropy function\n$\\mathcal{H}(n_1, \\dots, n_k) = \\sum_{i=1}^k{\\frac{n_i}{n}}\\log{\\frac{n}{n_i}}$\nmeasures the \"difficulty\" of the instance. We consider whether such refined\nanalysis can be applied to other algorithms based on divide-and-conquer, such\nas polynomial multiplication, input-order adaptive computation of convex hulls\nin 2D and 3D, and computation of Delaunay triangulations.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 22:12:23 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 04:04:21 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2015 13:48:21 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Barbay", "Jeremy", ""], ["Ochoa", "Carlos", ""], ["Perez-Lantero", "Pablo", ""]]}, {"id": "1505.02829", "submitter": "Elis\\^angela Silva Dias", "authors": "Elis\\^angela Silva Dias and Diane Castonguay", "title": "Polynomial enumeration of chordless cycles on cyclically orientable\n  graphs", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a finite undirected simple graph, a chordless cycle is an induced subgraph\nwhich is a cycle. A graph is called cyclically orientable if it admits an\norientation in which every chordless cycle is cyclically oriented. We propose\nan algorithm to enumerate all chordless cycles of such a graph. Compared to\nother similar algorithms, the proposed algorithm have the advantage of finding\neach chordless cycle only once in time complexity $\\mathcal{O}(n^2)$ in the\ninput size, where $n$ is the number of vertices.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 22:58:00 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Dias", "Elis\u00e2ngela Silva", ""], ["Castonguay", "Diane", ""]]}, {"id": "1505.02855", "submitter": "Javiel Rojas-Ledesma", "authors": "J\\'er\\'emy Barbay, Pablo P\\'erez-Lantero, Javiel Rojas-Ledesma", "title": "Adaptive Computation of the Klee's Measure in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The KLEE'S MESURE of $n$ axis-parallel boxes in $\\mathbb{R}^d$ is the volume\nof their union. It can be computed in time within $O(n^{d/2})$ in the worst\ncase. We describe three techniques to boost its computation: one based on some\ntype of \"degeneracy'' of the input, and two ones on the inherent \"easiness'' of\nthe structure of the input. The first technique benefits from instances where\nthe MAXIMA of the input is of small size $h$, and yields a solution running in\ntime within $O(n\\log^{2d-2}{h}+ h^{d/2}) \\subseteq O(n^{d/2}$). The second\ntechnique takes advantage of instances where no $d$-dimensional axis-aligned\nhyperplane intersects more than $k$ boxes in some dimension, and yields a\nsolution running in time within $O(n \\log n + n k^{(d-2)/2}) \\subseteq\nO(n^{d/2})$. The third technique takes advantage of instances where the\n\\emph{intersection graph} of the input has small treewidth $\\omega$. It yields\nan algorithm running in time within $O(n^4\\omega \\log \\omega + n (\\omega \\log\n\\omega)^{d/2})$ in general, and in time within $O(n \\log n + n \\omega ^{d/2})$\nif an optimal tree decomposition of the intersection graph is given. We show\nhow to combine these techniques in an algorithm which takes advantage of all\nthree configurations.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 01:55:25 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 18:16:39 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["P\u00e9rez-Lantero", "Pablo", ""], ["Rojas-Ledesma", "Javiel", ""]]}, {"id": "1505.02867", "submitter": "Charles Mathy", "authors": "Charles Mathy, Nate Derbinsky, Jos\\'e Bento, Jonathan Rosenthal and\n  Jonathan Yedidia", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised\n  Learning", "comments": "7 pages, 4 figs, 1 page supp. info", "journal-ref": "Proc. of the 29th AAAI Conference on Artificial Intelligence\n  (AAAI), 2864-2870. Austin, TX, USA. (2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new instance-based learning algorithm called the Boundary\nForest (BF) algorithm, that can be used for supervised and unsupervised\nlearning. The algorithm builds a forest of trees whose nodes store previously\nseen examples. It can be shown data points one at a time and updates itself\nincrementally, hence it is naturally online. Few instance-based algorithms have\nthis property while being simultaneously fast, which the BF is. This is crucial\nfor applications where one needs to respond to input data in real time. The\nnumber of children of each node is not set beforehand but obtained from the\ntraining procedure, which makes the algorithm very flexible with regards to\nwhat data manifolds it can learn. We test its generalization performance and\nspeed on a range of benchmark datasets and detail in which settings it\noutperforms the state of the art. Empirically we find that training time scales\nas O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N\nthe amount of data,\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 03:45:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Mathy", "Charles", ""], ["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Rosenthal", "Jonathan", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1505.02984", "submitter": "Anmer Daskin", "authors": "Anmer Daskin", "title": "Quantum Eigenvalue Estimation for Irreducible Non-negative Matrices", "comments": "A few typos are corrected", "journal-ref": null, "doi": "10.1142/S0219749916500052", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum phase estimation algorithm has been successfully adapted as a sub\nframe of many other algorithms applied to a wide variety of applications in\ndifferent fields. However, the requirement of a good approximate eigenvector\ngiven as an input to the algorithm hinders the application of the algorithm to\nthe problems where we do not have any prior knowledge about the eigenvector.\n  In this paper, we show that the principal eigenvalue of an irreducible\nnon-negative operator can be determined by using an equal superposition initial\nstate in the phase estimation algorithm. This removes the necessity of the\nexistence of an initial good approximate eigenvector. Moreover, we show that\nthe success probability of the algorithm is related to the closeness of the\noperator to a stochastic matrix. Therefore, we draw an estimate for the success\nprobability by using the variance of the column sums of the operator. This\nprovides a priori information which can be used to know the success probability\nof the algorithm beforehand for the non-negative matrices and apply the\nalgorithm only in cases when the estimated probability reasonably high.\nFinally, we discuss the possible applications and show the results for random\nsymmetric matrices and 3-local Hamiltonians with non-negative off-diagonal\nelements.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 12:45:48 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 09:45:19 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 06:57:08 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2015 10:19:25 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2015 10:37:11 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Daskin", "Anmer", ""]]}, {"id": "1505.02993", "submitter": "Tyson Williams", "authors": "Jin-Yi Cai, Zhiguo Fu, Heng Guo, Tyson Williams", "title": "A Holant Dichotomy: Is the FKT Algorithm Universal?", "comments": "128 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a complexity dichotomy for complex-weighted Holant problems with an\narbitrary set of symmetric constraint functions on Boolean variables. This\ndichotomy is specifically to answer the question: Is the FKT algorithm under a\nholographic transformation a \\emph{universal} strategy to obtain\npolynomial-time algorithms for problems over planar graphs that are intractable\nin general? This dichotomy is a culmination of previous ones, including those\nfor Spin Systems, Holant, and #CSP. A recurring theme has been that a\nholographic reduction to FKT is a universal strategy. Surprisingly, for planar\nHolant, we discover new planar tractable problems that are not expressible by a\nholographic reduction to FKT.\n  In previous work, an important tool was a dichotomy for #CSP^d, which denotes\n#CSP where every variable appears a multiple of d times. However its proof\nviolates planarity. We prove a dichotomy for planar #CSP^2. We apply this\nplanar #CSP^2 dichotomy in the proof of the planar Holant dichotomy.\n  As a special case of our new planar tractable problems, counting perfect\nmatchings (#PM) over k-uniform hypergraphs is polynomial-time computable when\nthe incidence graph is planar and k >= 5. The same problem is #P-hard when k=3\nor k=4, which is also a consequence of our dichotomy. When k=2, it becomes #PM\nover planar graphs and is tractable again. More generally, over hypergraphs\nwith specified hyperedge sizes and the same planarity assumption, #PM is\npolynomial-time computable if the greatest common divisor of all hyperedge\nsizes is at least 5.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 13:08:25 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Cai", "Jin-Yi", ""], ["Fu", "Zhiguo", ""], ["Guo", "Heng", ""], ["Williams", "Tyson", ""]]}, {"id": "1505.03190", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski", "title": "Efficient data hashing with structured binary embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here new mechanisms for hashing data via binary embeddings.\nContrary to most of the techniques presented before, the embedding matrix of\nour mechanism is highly structured. That enables us to perform hashing more\nefficiently and use less memory. What is crucial and nonintuitive is the fact\nthat imposing structured mechanism does not affect the quality of the produced\nhash. To the best of our knowledge, we are the first to give strong theoretical\nguarantees of the proposed binary hashing method by proving the efficiency of\nthe mechanism for several classes of structured projection matrices. As a\ncorollary, we obtain binary hashing mechanisms with strong concentration\nresults for circulant and Topelitz matrices. Our approach is however much more\ngeneral.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 23:29:42 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Choromanski", "Krzysztof", ""]]}, {"id": "1505.03334", "submitter": "Frederic Magniez", "authors": "Nathana\\\"el Fran\\c{c}ois and Fr\\'ed\\'eric Magniez and Michel de\n  Rougemont and Olivier Serre", "title": "Streaming Property Testing of Visibly Pushdown Languages", "comments": "23 pages. Major modifications in the presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of language recognition, we demonstrate the superiority of\nstreaming property testers against streaming algorithms and property testers,\nwhen they are not combined. Initiated by Feigenbaum et al., a streaming\nproperty tester is a streaming algorithm recognizing a language under the\nproperty testing approximation: it must distinguish inputs of the language from\nthose that are $\\varepsilon$-far from it, while using the smallest possible\nmemory (rather than limiting its number of input queries).\n  Our main result is a streaming $\\varepsilon$-property tester for visibly\npushdown languages (VPL) with one-sided error using memory space\n$\\mathrm{poly}((\\log n) / \\varepsilon)$.\n  This constructions relies on a (non-streaming) property tester for weighted\nregular languages based on a previous tester by Alon et al. We provide a simple\napplication of this tester for streaming testing special cases of instances of\nVPL that are already hard for both streaming algorithms and property testers.\n  Our main algorithm is a combination of an original simulation of visibly\npushdown automata using a stack with small height but possible items of linear\nsize. In a second step, those items are replaced by small sketches. Those\nsketches relies on a notion of suffix-sampling we introduce. This sampling is\nthe key idea connecting our streaming tester algorithm to property testers.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 11:21:39 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 12:49:08 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 09:15:36 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2015 10:22:10 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Fran\u00e7ois", "Nathana\u00ebl", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""], ["de Rougemont", "Michel", ""], ["Serre", "Olivier", ""]]}, {"id": "1505.03424", "submitter": "Aravindan Vijayaraghavan", "authors": "Boaz Barak, Ankur Moitra, Ryan O'Donnell, Prasad Raghavendra, Oded\n  Regev, David Steurer, Luca Trevisan, Aravindan Vijayaraghavan, David Witmer\n  and John Wright", "title": "Beating the random assignment on constraint satisfaction problems of\n  bounded degree", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for any odd $k$ and any instance of the Max-kXOR constraint\nsatisfaction problem, there is an efficient algorithm that finds an assignment\nsatisfying at least a $\\frac{1}{2} + \\Omega(1/\\sqrt{D})$ fraction of\nconstraints, where $D$ is a bound on the number of constraints that each\nvariable occurs in. This improves both qualitatively and quantitatively on the\nrecent work of Farhi, Goldstone, and Gutmann (2014), which gave a\n\\emph{quantum} algorithm to find an assignment satisfying a $\\frac{1}{2} +\n\\Omega(D^{-3/4})$ fraction of the equations.\n  For arbitrary constraint satisfaction problems, we give a similar result for\n\"triangle-free\" instances; i.e., an efficient algorithm that finds an\nassignment satisfying at least a $\\mu + \\Omega(1/\\sqrt{D})$ fraction of\nconstraints, where $\\mu$ is the fraction that would be satisfied by a uniformly\nrandom assignment.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 15:27:16 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 14:18:26 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Barak", "Boaz", ""], ["Moitra", "Ankur", ""], ["O'Donnell", "Ryan", ""], ["Raghavendra", "Prasad", ""], ["Regev", "Oded", ""], ["Steurer", "David", ""], ["Trevisan", "Luca", ""], ["Vijayaraghavan", "Aravindan", ""], ["Witmer", "David", ""], ["Wright", "John", ""]]}, {"id": "1505.03532", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Kesheng Wu, Alex Sim, Michael Churchill, Jong Y. Choi,\n  Andreas Stathopoulos, Cs Chang and Scott Klasky", "title": "Towards Real-Time Detection and Tracking of Spatio-Temporal Features:\n  Blob-Filaments in Fusion Plasma", "comments": "14 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DS physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm and implementation of real-time identification and tracking\nof blob-filaments in fusion reactor data is presented. Similar spatio-temporal\nfeatures are important in many other applications, for example, ignition\nkernels in combustion and tumor cells in a medical image. This work presents an\napproach for extracting these features by dividing the overall task into three\nsteps: local identification of feature cells, grouping feature cells into\nextended feature, and tracking movement of feature through overlapping in\nspace. Through our extensive work in parallelization, we demonstrate that this\napproach can effectively make use of a large number of compute nodes to detect\nand track blob-filaments in real time in fusion plasma. On a set of 30GB fusion\nsimulation data, we observed linear speedup on 1024 processes and completed\nblob detection in less than three milliseconds using Edison, a Cray XC30 system\nat NERSC.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 20:01:14 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 13:22:05 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 17:19:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Wu", "Lingfei", ""], ["Wu", "Kesheng", ""], ["Sim", "Alex", ""], ["Churchill", "Michael", ""], ["Choi", "Jong Y.", ""], ["Stathopoulos", "Andreas", ""], ["Chang", "Cs", ""], ["Klasky", "Scott", ""]]}, {"id": "1505.03537", "submitter": "Hai-Jun Zhou", "authors": "Yusupjan Habibulla, Jin-Hua Zhao, Hai-Jun Zhou", "title": "The Directed Dominating Set Problem: Generalized Leaf Removal and Belief\n  Propagation", "comments": "11 pages, 3 figures in EPS format", "journal-ref": "Lecture Notes in Computer Science 9130, 78--88 (2015)", "doi": "10.1007/978-3-319-19647-3_8", "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimum dominating set for a digraph (directed graph) is a smallest set of\nvertices such that each vertex either belongs to this set or has at least one\nparent vertex in this set. We solve this hard combinatorial optimization\nproblem approximately by a local algorithm of generalized leaf removal and by a\nmessage-passing algorithm of belief propagation. These algorithms can construct\nnear-optimal dominating sets or even exact minimum dominating sets for random\ndigraphs and also for real-world digraph instances. We further develop a core\npercolation theory and a replica-symmetric spin glass theory for this problem.\nOur algorithmic and theoretical results may facilitate applications of\ndominating sets to various network problems involving directed interactions.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 20:02:17 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Habibulla", "Yusupjan", ""], ["Zhao", "Jin-Hua", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1505.03737", "submitter": "Martin Grohe", "authors": "Martin Grohe and Pascal Schweitzer", "title": "Isomorphism Testing for Graphs of Bounded Rank Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm that, for every fixed k, decides isomorphism of graphs\nof rank width at most k in polynomial time. As the clique width of a graph is\nbounded in terms of its rank width, we also obtain a polynomial time\nisomorphism test for graph classes of bounded clique width.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 14:40:15 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Grohe", "Martin", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1505.03840", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Yutong Chen and Amit Singer", "title": "Non-unique games over compact groups and orientation estimation in\n  cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{G}$ be a compact group and let $f_{ij} \\in L^2(\\mathcal{G})$.\nWe define the Non-Unique Games (NUG) problem as finding $g_1,\\dots,g_n \\in\n\\mathcal{G}$ to minimize $\\sum_{i,j=1}^n f_{ij} \\left( g_i g_j^{-1}\\right)$. We\ndevise a relaxation of the NUG problem to a semidefinite program (SDP) by\ntaking the Fourier transform of $f_{ij}$ over $\\mathcal{G}$, which can then be\nsolved efficiently. The NUG framework can be seen as a generalization of the\nlittle Grothendieck problem over the orthogonal group and the Unique Games\nproblem and includes many practically relevant problems, such as the maximum\nlikelihood estimator} to registering bandlimited functions over the unit sphere\nin $d$-dimensions and orientation estimation in cryo-Electron Microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 19:11:27 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Chen", "Yutong", ""], ["Singer", "Amit", ""]]}, {"id": "1505.03883", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen, Wen-Hung Huang, Cong Liu", "title": "k2Q: A Quadratic-Form Response Time and Schedulability Analysis\n  Framework for Utilization-Based Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.02155;\n  text overlap with arXiv:1501.07084, a complete version of RTSS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a general response-time analysis and\nschedulability-test framework, called k2Q (k to Q). It provides automatic\nconstructions of closed-form quadratic bounds or utilization bounds for a wide\nrange of applications in real-time systems under fixed-priority scheduling. The\nkey of the framework is a $k$-point schedulability test or a $k$-point response\ntime analysis that is based on the utilizations and the execution times of\n$k-1$ higher-priority tasks. The natural condition of k2Q is a quadratic form\nfor testing the schedulability or analyzing the response time. The response\ntime analysis and the schedulability analysis provided by the framework can be\nviewed as a \"blackbox\" interface that can result in sufficient\nutilization-based analysis. Since the framework is independent from the task\nand platform models, it can be applied to a wide range of applications.\n  We show the generality of k2Q by applying it to several different task\nmodels. k2Q produces better uniprocessor and/or multiprocessor schedulability\ntests not only for the traditional sporadic task model, but also more\nexpressive task models such as the generalized multi-frame task model and the\nacyclic task model. Another interesting contribution is that in the past,\nexponential-time schedulability tests were typically not recommended and most\nof time ignored due to high complexity. We have successfully shown that\nexponential-time schedulability tests may lead to good polynomial-time tests\n(almost automatically) by using the k2Q framework.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 20:44:04 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 21:36:53 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 07:25:17 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Huang", "Wen-Hung", ""], ["Liu", "Cong", ""]]}, {"id": "1505.03924", "submitter": "Colin White", "authors": "Maria-Florina Balcan, Nika Haghtalab, Colin White", "title": "$k$-center Clustering under Perturbation Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-center problem is a canonical and long-studied facility location and\nclustering problem with many applications in both its symmetric and asymmetric\nforms. Both versions of the problem have tight approximation factors on worst\ncase instances. Therefore to improve on these ratios, one must go beyond the\nworst case.\n  In this work, we take this approach and provide strong positive results both\nfor the asymmetric and symmetric $k$-center problems under a natural input\nstability (promise) condition called $\\alpha$-perturbation resilience [Bilu and\nLinia 2012], which states that the optimal solution does not change under any\nalpha-factor perturbation to the input distances. We provide algorithms that\ngive strong guarantees simultaneously for stable and non-stable instances: our\nalgorithms always inherit the worst-case guarantees of clustering approximation\nalgorithms, and output the optimal solution if the input is $2$-perturbation\nresilient. Furthermore, we prove our result is tight by showing symmetric\n$k$-center under $(2-\\epsilon)$-perturbation resilience is hard unless $NP=RP$.\n  The impact of our results are multifaceted. This is the first tight result\nfor any problem under perturbation resilience. Furthermore, our results\nillustrate a surprising relationship between symmetric and asymmetric\n$k$-center instances under perturbation resilience. Unlike approximation ratio,\nfor which symmetric $k$-center is easily solved to a factor of 2 but asymmetric\n$k$-center cannot be approximated to any constant factor, both symmetric and\nasymmetric $k$-center can be solved optimally under resilience to\n2-perturbations. Finally, our guarantees in the setting where only part of the\ndata satisfies perturbation resilience makes these algorithms more applicable\nto real-life instances.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 23:59:14 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 15:42:23 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 22:49:26 GMT"}, {"version": "v4", "created": "Fri, 28 Dec 2018 22:02:13 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Haghtalab", "Nika", ""], ["White", "Colin", ""]]}, {"id": "1505.04019", "submitter": "Fatima Vayani", "authors": "Ljiljana Brankovic and Costas S. Iliopoulos and Ritu Kundu and Manal\n  Mohamed and Solon P. Pissis and Fatima Vayani", "title": "Linear-Time Superbubble Identification Algorithm for Genome Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA sequencing is the process of determining the exact order of the\nnucleotide bases of an individual's genome in order to catalogue sequence\nvariation and understand its biological implications. Whole-genome sequencing\ntechniques produce masses of data in the form of short sequences known as\nreads. Assembling these reads into a whole genome constitutes a major\nalgorithmic challenge. Most assembly algorithms utilize de Bruijn graphs\nconstructed from reads for this purpose. A critical step of these algorithms is\nto detect typical motif structures in the graph caused by sequencing errors and\ngenome repeats, and filter them out; one such complex subgraph class is a\nso-called superbubble. In this paper, we propose an O(n+m)-time algorithm to\ndetect all superbubbles in a directed acyclic graph with n nodes and m\n(directed) edges, improving the best-known O(m log m)-time algorithm by Sung et\nal.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 10:52:50 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 12:06:38 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 09:38:20 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2015 17:31:31 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2015 11:35:36 GMT"}, {"version": "v6", "created": "Thu, 17 Sep 2015 18:09:38 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Brankovic", "Ljiljana", ""], ["Iliopoulos", "Costas S.", ""], ["Kundu", "Ritu", ""], ["Mohamed", "Manal", ""], ["Pissis", "Solon P.", ""], ["Vayani", "Fatima", ""]]}, {"id": "1505.04198", "submitter": "Matthias Poloczek", "authors": "Bert Besser and Matthias Poloczek", "title": "Greedy Matching: Guarantees and Limitations", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Tinhofer proposed the MinGreedy algorithm for maximum cardinality\nmatching in 1984, several experimental studies found the randomized algorithm\nto perform excellently for various classes of random graphs and benchmark\ninstances. In contrast, only few analytical results are known. We show that\nMinGreedy cannot improve on the trivial approximation ratio 1/2 whp., even for\nbipartite graphs. Our hard inputs seem to require a small number of high-degree\nnodes.\n  This motivates an investigation of greedy algorithms on graphs with maximum\ndegree D: We show that MinGreedy achieves a (D-1)/(2D-3)-approximation for\ngraphs with D=3 and for D-regular graphs, and a guarantee of (D-1/2)/(2D-2) for\ngraphs with maximum degree D. Interestingly, our bounds even hold for the\ndeterministic MinGreedy that breaks all ties arbitrarily.\n  Moreover, we investigate the limitations of the greedy paradigm, using the\nmodel of priority algorithms introduced by Borodin, Nielsen, and Rackoff. We\nstudy deterministic priority algorithms and prove a\n(D-1)/(2D-3)-inapproximability result for graphs with maximum degree D; thus,\nthese greedy algorithms do not achieve a 1/2+eps-approximation and in\nparticular the 2/3-approximation obtained by the deterministic MinGreedy for\nD=3 is optimal in this class. For k-uniform hypergraphs we show a tight\n1/k-inapproximability bound. We also study fully randomized priority algorithms\nand give a 5/6-inapproximability bound. Thus, they cannot compete with matching\nalgorithms of other paradigms.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 20:26:21 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Besser", "Bert", ""], ["Poloczek", "Matthias", ""]]}, {"id": "1505.04229", "submitter": "Setareh Borjian", "authors": "Setareh Borjian, Virgile Galle, Vahideh H. Manshadi, Cynthia Barnhart,\n  Patrick Jaillet", "title": "Container Relocation Problem: Approximation, Asymptotic, and Incomplete\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Container Relocation Problem (CRP) is concerned with finding a sequence\nof moves of containers that minimizes the number of relocations needed to\nretrieve all containers respecting a given order of retrieval. While the\nproblem is known to be NP-hard, certain algorithms such as the A* search and\nheuristics perform reasonably well on many instances of the problem. In this\npaper, we first focus on the A* search algorithm, and analyze lower and upper\nbounds that are easy to compute and can be used to prune nodes. Our analysis\nsheds light on which bounds result in fast computation within a given\napproximation gap. We present extensive simulation results that improve upon\nour theoretical analysis, and further show that our method finds the optimum\nsolution on most instances of medium-size bays. On \"hard\" instances, our method\nfinds an approximate solution with a small gap and within a time frame that is\nfast for practical applications. We also study the average-case asymptotic\nbehavior of the CRP where the number of columns grows. We calculate the\nexpected number of relocations in the limit, and show that the optimum number\nof relocations converges to a simple and intuitive lower-bound. We further\nstudy the CRP with incomplete information by relaxing the assumption that the\norder of retrieval of all containers are initially known. This assumption is\nparticularly unrealistic in ports without an appointment system. We assume that\nthe retrieval order of a subset of containers is known initially and the\nretrieval order of the remaining containers is observed later at a given\nspecific time. Before this time, we assume a probabilistic distribution on the\nretrieval order of unknown containers. We combine the A* algorithm with\nsampling technique to solve this two-stage stochastic optimization problem. We\nshow that our algorithm is fast and the error due to sampling and pruning is\nreasonably small.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 01:27:11 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 01:56:08 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Borjian", "Setareh", ""], ["Galle", "Virgile", ""], ["Manshadi", "Vahideh H.", ""], ["Barnhart", "Cynthia", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1505.04308", "submitter": "Avery Miller", "authors": "Christian Glacet, Avery Miller, Andrzej Pelc", "title": "Time vs. Information Tradeoffs for Leader Election in Anonymous Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leader election task calls for all nodes of a network to agree on a\nsingle node. If the nodes of the network are anonymous, the task of leader\nelection is formulated as follows: every node $v$ of the network must output a\nsimple path, coded as a sequence of port numbers, such that all these paths end\nat a common node, the leader. In this paper, we study deterministic leader\nelection in anonymous trees.\n  Our aim is to establish tradeoffs between the allocated time $\\tau$ and the\namount of information that has to be given $\\textit{a priori}$ to the nodes to\nenable leader election in time $\\tau$ in all trees for which leader election in\nthis time is at all possible. Following the framework of $\\textit{algorithms\nwith advice}$, this information (a single binary string) is provided to all\nnodes at the start by an oracle knowing the entire tree. The length of this\nstring is called the $\\textit{size of advice}$. For an allocated time $\\tau$,\nwe give upper and lower bounds on the minimum size of advice sufficient to\nperform leader election in time $\\tau$.\n  We consider $n$-node trees of diameter $diam \\leq D$. While leader election\nin time $diam$ can be performed without any advice, for time $diam-1$ we give\ntight upper and lower bounds of $\\Theta (\\log D)$. For time $diam-2$ we give\ntight upper and lower bounds of $\\Theta (\\log D)$ for even values of $diam$,\nand tight upper and lower bounds of $\\Theta (\\log n)$ for odd values of $diam$.\nFor the time interval $[\\beta \\cdot diam, diam-3]$ for constant $\\beta >1/2$,\nwe prove an upper bound of $O(\\frac{n\\log n}{D})$ and a lower bound of\n$\\Omega(\\frac{n}{D})$, the latter being valid whenever $diam$ is odd or when\nthe time is at most $diam-4$. Finally, for time $\\alpha \\cdot diam$ for any\nconstant $\\alpha <1/2$ (except for the case of very small diameters), we give\ntight upper and lower bounds of $\\Theta (n)$.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 18:41:40 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 03:45:23 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Glacet", "Christian", ""], ["Miller", "Avery", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1505.04383", "submitter": "David Witmer", "authors": "Sarah R. Allen, Ryan O'Donnell, David Witmer", "title": "How to refute a random CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a $k$-ary predicate over a finite alphabet. Consider a random\nCSP$(P)$ instance $I$ over $n$ variables with $m$ constraints. When $m \\gg n$\nthe instance $I$ will be unsatisfiable with high probability, and we want to\nfind a refutation - i.e., a certificate of unsatisfiability. When $P$ is the\n$3$-ary OR predicate, this is the well studied problem of refuting random\n$3$-SAT formulas, and an efficient algorithm is known only when $m \\gg\nn^{3/2}$. Understanding the density required for refutation of other predicates\nis important in cryptography, proof complexity, and learning theory.\nPreviously, it was known that for a $k$-ary predicate, having $m \\gg n^{\\lceil\nk/2 \\rceil}$ constraints suffices for refutation. We give a criterion for\npredicates that often yields efficient refutation algorithms at much lower\ndensities. Specifically, if $P$ fails to support a $t$-wise uniform\ndistribution, then there is an efficient algorithm that refutes random CSP$(P)$\ninstances $I$ whp when $m \\gg n^{t/2}$. Indeed, our algorithm will \"somewhat\nstrongly\" refute $I$, certifying $\\mathrm{Opt}(I) \\leq 1-\\Omega_k(1)$, if $t =\nk$ then we get the strongest possible refutation, certifying $\\mathrm{Opt}(I)\n\\leq \\mathrm{E}[P] + o(1)$. This last result is new even in the context of\nrandom $k$-SAT. Regarding the optimality of our $m \\gg n^{t/2}$ requirement,\nprior work on SDP hierarchies has given some evidence that efficient refutation\nof random CSP$(P)$ may be impossible when $m \\ll n^{t/2}$. Thus there is an\nindication our algorithm's dependence on $m$ is optimal for every $P$, at least\nin the context of SDP hierarchies. Along these lines, we show that our\nrefutation algorithm can be carried out by the $O(1)$-round SOS SDP hierarchy.\nFinally, as an application of our result, we falsify assumptions used to show\nhardness-of-learning results in recent work of Daniely, Linial, and\nShalev-Shwartz.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 11:35:12 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 21:44:21 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 06:18:55 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Allen", "Sarah R.", ""], ["O'Donnell", "Ryan", ""], ["Witmer", "David", ""]]}, {"id": "1505.04693", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen", "title": "Partitioned Multiprocessor Fixed-Priority Scheduling of Sporadic\n  Real-Time Tasks", "comments": "Extended version of ECRTS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioned multiprocessor scheduling has been widely accepted in academia\nand industry to statically assign and partition real-time tasks onto identical\nmultiprocessor systems. This paper studies fixed-priority partitioned\nmultiprocessor scheduling for sporadic real-time systems, in which\ndeadline-monotonic scheduling is applied on each processor. Prior to this\npaper, the best known results are by Fisher, Baruah, and Baker with speedup\nfactors $4-\\frac{2}{M}$ and $3-\\frac{1}{M}$ for arbitrary-deadline and\nconstrained-deadline sporadic real-time task systems, respectively, where $M$\nis the number of processors. We show that a greedy mapping strategy has a\nspeedup factor $3-\\frac{1}{M}$ when considering task systems with arbitrary\ndeadlines. Such a factor holds for polynomial-time schedulability tests and\nexponential-time (exact) schedulability tests. Moreover, we also improve the\nspeedup factor to $2.84306$ when considering constrained-deadline task systems.\nWe also provide tight examples when the fitting strategy in the mapping stage\nis arbitrary and $M$ is sufficiently large. For both constrained- and\narbitrary-deadline task systems, the analytical result surprisingly shows that\nusing exact tests does not gain theoretical benefits (with respect to speedup\nfactors) for an arbitrary fitting strategy.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 15:54:58 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 21:40:33 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 18:32:38 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Chen", "Jian-Jia", ""]]}, {"id": "1505.04778", "submitter": "Soledad Villar", "authors": "Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar", "title": "On the tightness of an SDP relaxation of k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recently, Awasthi et al. introduced an SDP relaxation of the $k$-means\nproblem in $\\mathbb R^m$. In this work, we consider a random model for the data\npoints in which $k$ balls of unit radius are deterministically distributed\nthroughout $\\mathbb R^m$, and then in each ball, $n$ points are drawn according\nto a common rotationally invariant probability distribution. For any fixed ball\nconfiguration and probability distribution, we prove that the SDP relaxation of\nthe $k$-means problem exactly recovers these planted clusters with probability\n$1-e^{-\\Omega(n)}$ provided the distance between any two of the ball centers is\n$>2+\\epsilon$, where $\\epsilon$ is an explicit function of the configuration of\nthe ball centers, and can be arbitrarily small when $m$ is large.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:50:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Iguchi", "Takayuki", ""], ["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""], ["Villar", "Soledad", ""]]}, {"id": "1505.04901", "submitter": "Marc Goerigk", "authors": "Marc Goerigk and Anita Sch\\\"obel", "title": "Algorithm Engineering in Robust Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust optimization is a young and emerging field of research having received\na considerable increase of interest over the last decade. In this paper, we\nargue that the the algorithm engineering methodology fits very well to the\nfield of robust optimization and yields a rewarding new perspective on both the\ncurrent state of research and open research directions.\n  To this end we go through the algorithm engineering cycle of design and\nanalysis of concepts, development and implementation of algorithms, and\ntheoretical and experimental evaluation. We show that many ideas of algorithm\nengineering have already been applied in publications on robust optimization.\nMost work on robust optimization is devoted to analysis of the concepts and the\ndevelopment of algorithms, some papers deal with the evaluation of a particular\nconcept in case studies, and work on comparison of concepts just starts. What\nis still a drawback in many papers on robustness is the missing link to include\nthe results of the experiments again in the design.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 08:09:13 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 14:03:23 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 11:15:02 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Goerigk", "Marc", ""], ["Sch\u00f6bel", "Anita", ""]]}, {"id": "1505.04911", "submitter": "Antoine Limasset", "authors": "Antoine Limasset, Bastien Cazaux, Eric Rivals and Pierre Peterlongo", "title": "Read Mapping on de Bruijn graph", "comments": "BMC Bioinformatics", "journal-ref": null, "doi": "10.1186/s12859-016-1103-9", "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Next Generation Sequencing (NGS) has dramatically enhanced our\nability to sequence genomes, but not to assemble them. In practice, many\npublished genome sequences remain in the state of a large set of contigs. Each\ncontig describes the sequence found along some path of the assembly graph,\nhowever, the set of contigs does not record all the sequence information\ncontained in that graph. Although many subsequent analyses can be performed\nwith the set of contigs, one may ask whether mapping reads on the contigs is as\ninformative as mapping them on the paths of the assembly graph. Currently, one\nlacks practical tools to perform mapping on such graphs. Results Here, we\npropose a formal definition of mapping on a de Bruijn graph, analyse the\nproblem complexity which turns out to be NP-complete, and provide a practical\nsolution.We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty\nis a procedure to map reads on branching paths of the graph, for which we\ndesigned a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping\nTool). For the sake of efficiency, BGREAT rewrites a read sequence as a\nsuccession of unitigs sequences. GGMAP can map millions of reads per CPU hour\non a de Bruijn graph built from a large set of human genomic reads.\nSurprisingly, results show that up to 22% more reads can be mapped on the graph\nbut not on the contig set. Conclusions Although mapping reads on a de Bruijn\ngraph is complex task, our proposal offers a practical solution combining\nefficiency with an improved mapping capacity compared to assembly-based mapping\neven for complex eukaryotic data. Availability: github.com/Malfoy/BGREAT\nKeywords: Read mapping; De bruijn graphs; NGS; NP-completeness\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 09:01:43 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2015 16:50:42 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 09:59:14 GMT"}, {"version": "v4", "created": "Tue, 31 May 2016 08:09:35 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Limasset", "Antoine", ""], ["Cazaux", "Bastien", ""], ["Rivals", "Eric", ""], ["Peterlongo", "Pierre", ""]]}, {"id": "1505.05025", "submitter": "Sebastien Tixeuil", "authors": "Quentin Bramas (LINCS, UPMC, LIP6, NPA), Dianne Foreback, Mikhail\n  Nesterenko, S\\'ebastien Tixeuil (IUF, LINCS, UPMC, LIP6, NPA)", "title": "Packet Efficient Implementation of the Omega Failure Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume that a message may be delivered by packets through multiple hops\nand investigate the feasibility and efficiency of an implementation of the\nOmega Failure Detector under such an assumption.To motivate the study, we prove\nthat the existence and sustainability of a leader is exponentially more\nprobable in a multi-hop Omega implementation than in a single-hop one.An\nimplementation is: \\emph{message efficient} if all but finitely many messages\nare sent by a single process; \\emph{packet efficient} if the number of packets\nused to transmit a message in all but finitely many messages is linear w.r.t\nthe number of processes, packets of different messages may potentially use\ndifferent channels, thus the number of used channels is not limited;\n\\emph{super packet efficient} if the number of channels used by packets to\ntransmit all but finitely many messages is linear.We present the following\nresults for deterministic algorithms. If reliability and timeliness of one\nmessage does not correlate with another, i.e., there are no channel reliability\nproperties, then a packet efficient implementation of Omega is impossible. If\neventuallytimely and fair-lossy channels are considered, we establish necessary\nand sufficient conditions for the existence of a message and packet efficient\nimplementation of Omega. We also prove that the eventuality of timeliness of\nchannels makes a super packet efficientimplementation of Omega impossible. On\nthe constructive side, we present and prove correct a deterministic packet\nefficient implementation of Omega that matches the necessary conditions we\nestablished.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:46:18 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 23:03:04 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 14:39:49 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Bramas", "Quentin", "", "LINCS, UPMC, LIP6, NPA"], ["Foreback", "Dianne", "", "IUF, LINCS, UPMC, LIP6, NPA"], ["Nesterenko", "Mikhail", "", "IUF, LINCS, UPMC, LIP6, NPA"], ["Tixeuil", "S\u00e9bastien", "", "IUF, LINCS, UPMC, LIP6, NPA"]]}, {"id": "1505.05599", "submitter": "Greg Bodwin", "authors": "Greg Bodwin and Virginia Vassilevska Williams", "title": "Better Distance Preservers and Additive Spanners", "comments": "SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make improvements to the upper bounds on several popular types of distance\npreserving graph sketches. These sketches are all various restrictions of the\n{\\em additive pairwise spanner} problem, in which one is given an undirected\nunweighted graph $G$, a set of node pairs $P$, and an error allowance $+\\beta$,\nand one must construct a sparse subgraph $H$ satisfying $\\delta_H(u, v) \\le\n\\delta_G(u, v) + \\beta$ for all $(u, v) \\in P$.\n  The first part of our paper concerns {\\em pairwise distance preservers},\nwhich make the restriction $\\beta=0$ (i.e. distances must be preserved {\\em\nexactly}). Our main result here is an upper bound of $|H| = O(n^{2/3}|P|^{2/3}\n+ n|P|^{1/3})$ when $G$ is undirected and unweighted. This improves on existing\nbounds whenever $|P| = \\omega(n^{3/4})$, and it is the first such improvement\nin the last ten years.\n  We then devise a new application of distance preservers to graph clustering\nalgorithms, and we apply this algorithm to {\\em subset spanners}, which require\n$P = S \\times S$ for some node subset $S$, and {\\em (standard) spanners}, which\nrequire $P = V \\times V$. For both of these objects, our construction\ngeneralizes the best known bounds when the error allowance is constant, and we\nobtain the strongest polynomial error/sparsity tradeoff that has yet been\nreported (in fact, for subset spanners, ours is the {\\em first} nontrivial\nconstruction that enjoys improved sparsity from a polynomial error allowance).\n  We leave open a conjecture that $O(n^{2/3}|P|^{2/3} + n)$ pairwise distance\npreservers are possible for undirected unweighted graphs. Resolving this\nconjecture in the affirmative would improve and simplify our upper bounds for\nall the graph sketches mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 04:19:28 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 21:26:39 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 16:29:35 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Bodwin", "Greg", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1505.05610", "submitter": "Zhang WenKai", "authors": "Wenkai Zhang and Jing Li", "title": "Extended fast search clustering algorithm: widely density clusters, no\n  density peaks", "comments": "18 pages, 10 figures, DBDM 2015", "journal-ref": null, "doi": "10.5121/csit.2015.50701", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CFSFDP (clustering by fast search and find of density peaks) is recently\ndeveloped density-based clustering algorithm. Compared to DBSCAN, it needs less\nparameters and is computationally cheap for its non-iteration. Alex. at al have\ndemonstrated its power by many applications. However, CFSFDP performs not well\nwhen there are more than one density peak for one cluster, what we name as \"no\ndensity peaks\". In this paper, inspired by the idea of a hierarchical\nclustering algorithm CHAMELEON, we propose an extension of CFSFDP,E_CFSFDP, to\nadapt more applications. In particular, we take use of original CFSFDP to\ngenerating initial clusters first, then merge the sub clusters in the second\nphase. We have conducted the algorithm to several data sets, of which, there\nare \"no density peaks\". Experiment results show that our approach outperforms\nthe original one due to it breaks through the strict claim of data sets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:02:40 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zhang", "Wenkai", ""], ["Li", "Jing", ""]]}, {"id": "1505.05630", "submitter": "Greg Bodwin", "authors": "Greg Bodwin and Virginia Vassilevska Williams", "title": "Very Sparse Additive Spanners and Emulators", "comments": "ITCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two popular ways to sketch the shortest path distances of an input\ngraph. The first is distance preservers, which are sparse subgraphs that agree\nwith the distances of the original graph on a given set of demand pairs. Prior\nwork on distance preservers has exploited only a simple structural property of\nshortest paths, called consistency, stating that one can break shortest path\nties such that no two paths intersect, split apart, and then intersect again\nlater. We prove that consistency alone is not enough to understand distance\npreservers, by showing both a lower bound on the power of consistency and a new\ngeneral upper bound that polynomially surpasses it. Specifically, our new upper\nbound is that any $p$ demand pairs in an $n$-node undirected unweighted graph\nhave a distance preserver on $O(n^{2/3}p^{2/3} + np^{1/3})$ edges. We leave a\nconjecture that the right bound is $O(n^{2/3}p^{2/3} + n)$ or better.\n  The second part of this paper leverages these distance preservers in a new\nconstruction of additive spanners, which are subgraphs that preserve all\npairwise distances up to an additive error function. We give improved error\nbounds for spanners with relatively few edges; for example, we prove that all\ngraphs have spanners on $O(n)$ edges with $+O(n^{3/7 + \\varepsilon})$ error.\nOur construction can be viewed as an extension of the popular path-buying\nframework to clusters of larger radii.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 07:35:51 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 16:23:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bodwin", "Greg", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1505.05637", "submitter": "Elchanan Mossel", "authors": "Noga Alon and Elchanan Mossel and Robin Pemantle", "title": "Distributed Corruption Detection in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed corruption detection in networks. In\nthis model, each vertex of a directed graph is either truthful or corrupt. Each\nvertex reports the type (truthful or corrupt) of each of its outneighbors. If\nit is truthful, it reports the truth, whereas if it is corrupt, it reports\nadversarially. This model, first considered by Preparata, Metze, and Chien in\n1967, motivated by the desire to identify the faulty components of a digital\nsystem by having the other components checking them, became known as the PMC\nmodel. The main known results for this model characterize networks in which\n\\emph{all} corrupt (that is, faulty) vertices can be identified, when there is\na known upper bound on their number.\n  We are interested in networks in which the identity of a \\emph{large\nfraction} of the vertices can be identified.\n  It is known that in the PMC model, in order to identify all corrupt vertices\nwhen their number is $t$, all indegrees have to be at least $t$. In contrast,\nwe show that in $d$ regular-graphs with strong expansion properties, a\n$1-O(1/d)$ fraction of the corrupt vertices, and a $1-O(1/d)$ fraction of the\ntruthful vertices can be identified, whenever there is a majority of truthful\nvertices. We also observe that if the graph is very far from being a good\nexpander, namely, if the deletion of a small set of vertices splits the graph\ninto small components, then no corruption detection is possible even if most of\nthe vertices are truthful. Finally, we discuss the algorithmic aspects and the\ncomputational hardness of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 08:01:38 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 09:07:02 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 16:13:39 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Alon", "Noga", ""], ["Mossel", "Elchanan", ""], ["Pemantle", "Robin", ""]]}, {"id": "1505.05697", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim, Michael Elkin, and Cyril Gavoille", "title": "A Fast Network-Decomposition Algorithm and its Applications to\n  Constant-Time Distributed Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partition $(C_1,C_2,...,C_q)$ of $G = (V,E)$ into clusters of strong\n(respectively, weak) diameter $d$, such that the supergraph obtained by\ncontracting each $C_i$ is $\\ell$-colorable is called a strong (resp., weak)\n$(d, \\ell)$-network-decomposition. Network-decompositions were introduced in a\nseminal paper by Awerbuch, Goldberg, Luby and Plotkin in 1989. Awerbuch et al.\nshowed that strong $(exp\\{O(\\sqrt{ \\log n \\log \\log n})\\}, exp\\{O(\\sqrt{ \\log n\n\\log \\log n})\\})$-network-decompositions can be computed in distributed\ndeterministic time $exp\\{O(\\sqrt{ \\log n \\log \\log n})\\}$.\n  The result of Awerbuch et al. was improved by Panconesi and Srinivasan in\n1992: in the latter result $d = \\ell = exp\\{O(\\sqrt{\\log n})\\}$, and the\nrunning time is $exp\\{O(\\sqrt{\\log n})\\}$ as well. Much more recently Barenboim\n(2012) devised a distributed randomized constant-time algorithm for computing\nstrong network decompositions with $d = O(1)$. However, the parameter $\\ell$ in\nhis result is $O(n^{1/2 + \\epsilon})$.\n  In this paper we drastically improve the result of Barenboim and devise a\ndistributed randomized constant-time algorithm for computing strong $(O(1),\nO(n^{\\epsilon}))$-network-decompositions. As a corollary we derive a\nconstant-time randomized $O(n^{\\epsilon})$-approximation algorithm for the\ndistributed minimum coloring problem, improving the previously best-known\n$O(n^{1/2 + \\epsilon})$ approximation guarantee. We also derive other improved\ndistributed algorithms for a variety of problems.\n  Most notably, for the extremely well-studied distributed minimum dominating\nset problem currently there is no known deterministic polylogarithmic-time\nalgorithm. We devise a {deterministic} polylogarithmic-time approximation\nalgorithm for this problem, addressing an open problem of Lenzen and\nWattenhofer (2010).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 12:22:07 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 19:17:35 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Barenboim", "Leonid", ""], ["Elkin", "Michael", ""], ["Gavoille", "Cyril", ""]]}, {"id": "1505.05740", "submitter": "S\\'ebastien Adam", "authors": "Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre H\\'eroux, and\n  S\\'ebastien Adam", "title": "Graph edit distance : a new binary linear programming formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph edit distance (GED) is a powerful and flexible graph matching paradigm\nthat can be used to address different tasks in structural pattern recognition,\nmachine learning, and data mining. In this paper, some new binary linear\nprogramming formulations for computing the exact GED between two graphs are\nproposed. A major strength of the formulations lies in their genericity since\nthe GED can be computed between directed or undirected fully attributed graphs\n(i.e. with attributes on both vertices and edges). Moreover, a relaxation of\nthe domain constraints in the formulations provides efficient lower bound\napproximations of the GED. A complete experimental study comparing the proposed\nformulations with 4 state-of-the-art algorithms for exact and approximate graph\nedit distances is provided. By considering both the quality of the proposed\nsolution and the efficiency of the algorithms as performance criteria, the\nresults show that none of the compared methods dominates the others in the\nPareto sense. As a consequence, faced to a given real-world problem, a\ntrade-off between quality and efficiency has to be chosen w.r.t. the\napplication constraints. In this context, this paper provides a guide that can\nbe used to choose the appropriate method.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 13:57:40 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Lerouge", "Julien", ""], ["Abu-Aisheh", "Zeina", ""], ["Raveaux", "Romain", ""], ["H\u00e9roux", "Pierre", ""], ["Adam", "S\u00e9bastien", ""]]}, {"id": "1505.05825", "submitter": "Thore Husfeldt", "authors": "Thore Husfeldt", "title": "Graph colouring algorithms", "comments": "Thore Husfeldt, Graph colouring algorithms. Chapter 13 of Topics in\n  Chromatic Graph Theory, L. W. Beineke and Robin J. Wilson (eds.),\n  Encyclopedia of Mathematics and its Applications 156, Cambridge University\n  Press, ISBN 978-1-107-03350-4, 2015, pp. 277-303", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter presents an introduction to graph colouring algorithms. The\nfocus is on vertex-colouring algorithms that work for general classes of graphs\nwith worst-case performance guarantees in a sequential model of computation.\nThe presentation aims to demonstrate the breadth of available techniques and is\norganized by algorithmic paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 18:24:03 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Husfeldt", "Thore", ""]]}, {"id": "1505.05962", "submitter": "Sriniwas Pandey", "authors": "Pankaj Kumar Yadav, Sriniwas Pandey, Sraban Kumar Mohanty", "title": "Nearest Neighbor based Clustering Algorithm for Large Data Sets", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an unsupervised learning technique in which data or objects are\ngrouped into sets based on some similarity measure. Most of the clustering\nalgorithms assume that the main memory is infinite and can accommodate the set\nof patterns. In reality many applications give rise to a large set of patterns\nwhich does not fit in the main memory. When the data set is too large, much of\nthe data is stored in the secondary memory. Input/Outputs (I/O) from the disk\nare the major bottleneck in designing efficient clustering algorithms for large\ndata sets. Different designing techniques have been used to design clustering\nalgorithms for large data sets. External memory algorithms are one class of\nalgorithms which can be used for large data sets. These algorithms exploit the\nhierarchical memory structure of the computers by incorporating locality of\nreference directly in the algorithm. This paper makes some contribution towards\ndesigning clustering algorithms in the external memory model (Proposed by\nAggarwal and Vitter 1988) to make the algorithms scalable. In this paper, it is\nshown that the Shared near neighbors algorithm is not very I/O efficient since\nthe computational complexity is same as the I/O complexity. The algorithm is\ndesigned in the external memory model and I/O complexity is reduced. The\ncomputational complexity remains same. We substantiate the theoretical analysis\nby showing the performance of the algorithms with their traditional counterpart\nby implementing in STXXL library.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 06:29:38 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Yadav", "Pankaj Kumar", ""], ["Pandey", "Sriniwas", ""], ["Mohanty", "Sraban Kumar", ""]]}, {"id": "1505.05983", "submitter": "Yann Ponty", "authors": "Cedric Chauve, Julien Courtiel, Yann Ponty (LIX, AMIB)", "title": "Counting, generating and sampling tree alignments", "comments": "ALCOB - 3rd International Conference on Algorithms for Computational\n  Biology - 2016, Jun 2016, Trujillo, Spain. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise ordered tree alignment are combinatorial objects that appear in RNA\nsecondary structure comparison. However, the usual representation of tree\nalignments as supertrees is ambiguous, i.e. two distinct supertrees may induce\nidentical sets of matches between identical pairs of trees. This ambiguity is\nuninformative, and detrimental to any probabilistic analysis.In this work, we\nconsider tree alignments up to equivalence. Our first result is a precise\nasymptotic enumeration of tree alignments, obtained from a context-free grammar\nby mean of basic analytic combinatorics. Our second result focuses on\nalignments between two given ordered trees $S$ and $T$. By refining our grammar\nto align specific trees, we obtain a decomposition scheme for the space of\nalignments, and use it to design an efficient dynamic programming algorithm for\nsampling alignments under the Gibbs-Boltzmann probability distribution. This\ngeneralizes existing tree alignment algorithms, and opens the door for a\nprobabilistic analysis of the space of suboptimal RNA secondary structures\nalignments.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 08:25:05 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 13:03:28 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 19:44:22 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Chauve", "Cedric", "", "LIX, AMIB"], ["Courtiel", "Julien", "", "LIX, AMIB"], ["Ponty", "Yann", "", "LIX, AMIB"]]}, {"id": "1505.06032", "submitter": "Dragan Matic", "authors": "Dragan Matic, Jozef Kratica, Vladimir Filipovic", "title": "Variable Neighborhood Search for solving Bandwidth Coloring Problem", "comments": null, "journal-ref": "Computer Science and Information Systems 2017 Volume 14, Issue 2,\n  Pages: 309-327", "doi": "10.2298/CSIS160320012M", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a variable neighborhood search (VNS) algorithm for\nsolving bandwidth coloring problem (BCP) and bandwidth multicoloring problem\n(BMCP). BCP and BMCP are generalizations of the well known vertex coloring\nproblem and they are of a great interest from both theoretical and practical\npoints of view. Presented VNS combines a shaking procedure which perturbs the\ncolors for an increasing number of vertices and a specific variable\nneighborhood descent (VND) procedure, based on the specially designed\narrangement of the vertices which are the subject of re-coloring. By this\napproach, local search is split in a series of disjoint procedures, enabling\nbetter choice of the vertices which are addressed to re-color. The experiments\nshow that proposed method is highly competitive with the state-of-the-art\nalgorithms and improves 2 out of 33 previous best known solutions for BMCP.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 11:24:03 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Matic", "Dragan", ""], ["Kratica", "Jozef", ""], ["Filipovic", "Vladimir", ""]]}, {"id": "1505.06072", "submitter": "Pedro Felzenszwalb", "authors": "Pedro F. Felzenszwalb, Benar F. Svaiter", "title": "Diffusion Methods for Classification with Pairwise Relationships", "comments": "To appear in the Quarterly of Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define two algorithms for propagating information in classification\nproblems with pairwise relationships. The algorithms are based on contraction\nmaps and are related to non-linear diffusion and random walks on graphs. The\napproach is also related to message passing algorithms, including belief\npropagation and mean field methods. The algorithms we describe are guaranteed\nto converge on graphs with arbitrary topology. Moreover they always converge to\na unique fixed point, independent of initialization. We prove that the fixed\npoints of the algorithms under consideration define lower-bounds on the energy\nfunction and the max-marginals of a Markov random field. The theoretical\nresults also illustrate a relationship between message passing algorithms and\nvalue iteration for an infinite horizon Markov decision process. We illustrate\nthe practical application of the algorithms under study with numerical\nexperiments in image restoration, stereo depth estimation and binary\nclassification on a grid.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 13:36:58 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 14:37:37 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 14:10:08 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 18:10:26 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Felzenszwalb", "Pedro F.", ""], ["Svaiter", "Benar F.", ""]]}, {"id": "1505.06284", "submitter": "Ahmed Younes Dr.", "authors": "Ahmed Younes", "title": "A Bounded-error Quantum Polynomial Time Algorithm for Two Graph\n  Bisection Problems", "comments": "17 Pages, 5 figures", "journal-ref": null, "doi": "10.1007/s11128-015-1069-y", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to propose a bounded-error quantum polynomial time\n(BQP) algorithm for the max-bisection and the min-bisection problems. The\nmax-bisection and the min-bisection problems are fundamental NP-hard problems.\nGiven a graph with even number of vertices, the aim of the max-bisection\nproblem is to divide the vertices into two subsets of the same size to maximize\nthe number of edges between the two subsets, while the aim of the min-bisection\nproblem is to minimize the number of edges between the two subsets. The\nproposed algorithm runs in $O(m^2)$ for a graph with $m$ edges and in the worst\ncase runs in $O(n^4)$ for a dense graph with $n$ vertices. The proposed\nalgorithm targets a general graph by representing both problems as Boolean\nconstraint satisfaction problems where the set of satisfied constraints are\nsimultaneously maximized/minimized using a novel iterative partial negation and\npartial measurement technique. The algorithm is shown to achieve an arbitrary\nhigh probability of success of $1-\\epsilon$ for small $\\epsilon>0$ using a\npolynomial space resources.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 07:54:39 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Younes", "Ahmed", ""]]}, {"id": "1505.06386", "submitter": "Michele Trevisiol", "authors": "Michele Trevisiol, Luca Maria Aiello, Paolo Boldi, Roi Blanco", "title": "Local Ranking Problem on the BrowseGraph", "comments": null, "journal-ref": null, "doi": "10.1145/2766462.2767704", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"Local Ranking Problem\" (LRP) is related to the computation of a\ncentrality-like rank on a local graph, where the scores of the nodes could\nsignificantly differ from the ones computed on the global graph. Previous work\nhas studied LRP on the hyperlink graph but never on the BrowseGraph, namely a\ngraph where nodes are webpages and edges are browsing transitions. Recently,\nthis graph has received more and more attention in many different tasks such as\nranking, prediction and recommendation. However, a web-server has only the\nbrowsing traffic performed on its pages (local BrowseGraph) and, as a\nconsequence, the local computation can lead to estimation errors, which hinders\nthe increasing number of applications in the state of the art. Also, although\nthe divergence between the local and global ranks has been measured, the\npossibility of estimating such divergence using only local knowledge has been\nmainly overlooked. These aspects are of great interest for online service\nproviders who want to: (i) gauge their ability to correctly assess the\nimportance of their resources only based on their local knowledge, and (ii)\ntake into account real user browsing fluxes that better capture the actual user\ninterest than the static hyperlink network. We study the LRP problem on a\nBrowseGraph from a large news provider, considering as subgraphs the\naggregations of browsing traces of users coming from different domains. We show\nthat the distance between rankings can be accurately predicted based only on\nstructural information of the local graph, being able to achieve an average\nrank correlation as high as 0.8.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 22:43:00 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Trevisiol", "Michele", ""], ["Aiello", "Luca Maria", ""], ["Boldi", "Paolo", ""], ["Blanco", "Roi", ""]]}, {"id": "1505.06425", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow, Galina Cariowa and Rafa{\\l} {\\L}entek", "title": "An algorithm for multipication of Kaluza numbers", "comments": "22 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the derivation of a new algorithm for multiplying of two\nKaluza numbers. Performing this operation directly requires 1024 real\nmultiplications and 992 real additions. The proposed algorithm can compute the\nsame result with only 512 real multiplications and 576 real additions. The\nderivation of our algorithm is based on utilizing the fact that multiplication\nof two Kaluza numbers can be expressed as a matrixvector product. The matrix\nmultiplicand that participates in the product calculating has unique structural\nproperties. Namely exploitation of these specific properties leads to\nsignificant reducing of the complexity of Kaluza numbers multiplication.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 11:01:54 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""], ["\u0141entek", "Rafa\u0142", ""]]}, {"id": "1505.06529", "submitter": "Xiaodong Wang", "authors": "Daxin Zhu, Lei Wang, Yingjie Wu, and Xiaodong Wang", "title": "An efficient dynamic programming algorithm for the generalized LCS\n  problem with multiple substring inclusive constraints", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.1872", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we consider a generalized longest common subsequence problem\nwith multiple substring inclusive constraints. For the two input sequences $X$\nand $Y$ of lengths $n$ and $m$, and a set of $d$ constraints\n$P=\\{P_1,\\cdots,P_d\\}$ of total length $r$, the problem is to find a common\nsubsequence $Z$ of $X$ and $Y$ including each of constraint string in $P$ as a\nsubstring and the length of $Z$ is maximized. A new dynamic programming\nsolution to this problem is presented in this paper. The correctness of the new\nalgorithm is proved. The time complexity of our algorithm is $O(d2^dnmr)$. In\nthe case of the number of constraint strings is fixed, our new algorithm for\nthe generalized longest common subsequence problem with multiple substring\ninclusive constraints requires $O(nmr)$ time and space.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 03:13:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Zhu", "Daxin", ""], ["Wang", "Lei", ""], ["Wu", "Yingjie", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1505.06550", "submitter": "Yang Li", "authors": "Yang Li and XifengYan", "title": "MSPKmerCounter: A Fast and Memory Efficient Approach for K-mer Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in next-generation genome sequencing (NGS) is to assemble\nmassive overlapping short reads that are randomly sampled from DNA fragments.\nTo complete assembling, one needs to finish a fundamental task in many leading\nassembly algorithms: counting the number of occurrences of k-mers (length-k\nsubstrings in sequences). The counting results are critical for many components\nin assembly (e.g. variants detection and read error correction). For large\ngenomes, the k-mer counting task can easily consume a huge amount of memory,\nmaking it impossible for large-scale parallel assembly on commodity servers.\n  In this paper, we develop MSPKmerCounter, a disk-based approach, to\nefficiently perform k-mer counting for large genomes using a small amount of\nmemory. Our approach is based on a novel technique called Minimum Substring\nPartitioning (MSP). MSP breaks short reads into multiple disjoint partitions\nsuch that each partition can be loaded into memory and processed individually.\nBy leveraging the overlaps among the k-mers derived from the same short read,\nMSP can achieve astonishing compression ratio so that the I/O cost can be\nsignificantly reduced. For the task of k-mer counting, MSPKmerCounter offers a\nvery fast and memory-efficient solution. Experiment results on large real-life\nshort reads data sets demonstrate that MSPKmerCounter can achieve better\noverall performance than state-of-the-art k-mer counting approaches.\n  MSPKmerCounter is available at http://www.cs.ucsb.edu/~yangli/MSPKmerCounter\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 07:21:56 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Li", "Yang", ""], ["XifengYan", "", ""]]}, {"id": "1505.06661", "submitter": "Syama Sundar Rangapuram", "authors": "Syama Sundar Rangapuram, Thomas B\\\"uhler and Matthias Hein", "title": "Towards Realistic Team Formation in Social Networks based on Densest\n  Subgraphs", "comments": "Technical report of paper accepted at WWW 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a task $\\mathcal{T}$, a set of experts $V$ with multiple skills and a\nsocial network $G(V, W)$ reflecting the compatibility among the experts, team\nformation is the problem of identifying a team $C \\subseteq V$ that is both\ncompetent in performing the task $\\mathcal{T}$ and compatible in working\ntogether. Existing methods for this problem make too restrictive assumptions\nand thus cannot model practical scenarios. The goal of this paper is to\nconsider the team formation problem in a realistic setting and present a novel\nformulation based on densest subgraphs. Our formulation allows modeling of many\nnatural requirements such as (i) inclusion of a designated team leader and/or a\ngroup of given experts, (ii) restriction of the size or more generally cost of\nthe team (iii) enforcing locality of the team, e.g., in a geographical sense or\nsocial sense, etc.\n  The proposed formulation leads to a generalized version of the classical\ndensest subgraph problem with cardinality constraints (DSP), which is an NP\nhard problem and has many applications in social network analysis. In this\npaper, we present a new method for (approximately) solving the generalized DSP\n(GDSP). Our method, FORTE, is based on solving an equivalent continuous\nrelaxation of GDSP. The solution found by our method has a quality guarantee\nand always satisfies the constraints of GDSP. Experiments show that the\nproposed formulation (GDSP) is useful in modeling a broader range of team\nformation problems and that our method produces more coherent and compact teams\nof high quality. We also show, with the help of an LP relaxation of GDSP, that\nour method gives close to optimal solutions to GDSP.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 15:17:46 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rangapuram", "Syama Sundar", ""], ["B\u00fchler", "Thomas", ""], ["Hein", "Matthias", ""]]}, {"id": "1505.06811", "submitter": "Huacheng Yu", "authors": "Huacheng Yu", "title": "An Improved Combinatorial Algorithm for Boolean Matrix Multiplication", "comments": "A preliminary version of this paper appeared in ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new combinatorial algorithm for triangle finding and Boolean\nmatrix multiplication that runs in $\\hat{O}(n^3/\\log^4 n)$ time, where the\n$\\hat{O}$ notation suppresses poly(loglog) factors. This improves the previous\nbest combinatorial algorithm by Chan that runs in $\\hat{O}(n^3/\\log^3 n)$ time.\nOur algorithm generalizes the divide-and-conquer strategy of Chan's algorithm.\nMoreover, we propose a general framework for detecting triangles in graphs and\ncomputing Boolean matrix multiplication. Roughly speaking, if we can find the\n\"easy parts\" of a given instance efficiently, we can solve the whole problem\nfaster than $n^3$.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 05:51:26 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Yu", "Huacheng", ""]]}, {"id": "1505.06893", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Robust recoverable and two-stage selection problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the following selection problem is discussed. A set of $n$\nitems is given and we wish to choose a subset of exactly $p$ items of the\nminimum total cost. This problem is a special case of 0-1 knapsack in which all\nthe item weights are equal to~1. Its deterministic version has a trivial\n$O(n)$-time algorithm, which consists in choosing $p$ items of the smallest\ncosts. In this paper it is assumed that the item costs are uncertain. Two\nrobust models, namely two-stage and recoverable ones, under discrete and\ninterval uncertainty representations, are discussed. Several positive and\nnegative complexity results for both of them are provided.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 10:48:41 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 14:42:49 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 09:50:43 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1505.06897", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA)", "title": "Times series averaging from a probabilistic interpretation of\n  time-elastic kernel", "comments": null, "journal-ref": null, "doi": "10.2478/amcs-2019-0028", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the light of regularized dynamic time warping kernels, this paper\nreconsider the concept of time elastic centroid (TEC) for a set of time series.\nFrom this perspective, we show first how TEC can easily be addressed as a\npreimage problem. Unfortunately this preimage problem is ill-posed, may suffer\nfrom over-fitting especially for long time series and getting a sub-optimal\nsolution involves heavy computational costs. We then derive two new algorithms\nbased on a probabilistic interpretation of kernel alignment matrices that\nexpresses in terms of probabilistic distributions over sets of alignment paths.\nThe first algorithm is an iterative agglomerative heuristics inspired from the\nstate of the art DTW barycenter averaging (DBA) algorithm proposed specifically\nfor the Dynamic Time Warping measure. The second proposed algorithm achieves a\nclassical averaging of the aligned samples but also implements an averaging of\nthe time of occurrences of the aligned samples. It exploits a straightforward\nprogressive agglomerative heuristics. An experimentation that compares for 45\ntime series datasets classification error rates obtained by first near\nneighbors classifiers exploiting a single medoid or centroid estimate to\nrepresent each categories show that: i) centroids based approaches\nsignificantly outperform medoids based approaches, ii) on the considered\nexperience, the two proposed algorithms outperform the state of the art DBA\nalgorithm, and iii) the second proposed algorithm that implements an averaging\njointly in the sample space and along the time axes emerges as the most\nsignificantly robust time elastic averaging heuristic with an interesting noise\nreduction capability. Index Terms-Time series averaging Time elastic kernel\nDynamic Time Warping Time series clustering and classification.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 11:02:36 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 07:17:13 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 12:00:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"]]}, {"id": "1505.06955", "submitter": "Wei Wei", "authors": "Wei Wei, Yunjia Zhang, Ting Wang, Baifeng Li, Baolong Niu, Zhiming\n  Zheng", "title": "Research on Solution Space of Bipartite Graph Vertex-Cover by Maximum\n  Matchings", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": "10.1088/1742-5468/2015/11/P11027", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some rigorous results and statistics of the solution space of Vertex-Covers\non bipartite graphs are given in this paper. Based on the $K\\ddot{o}nig$'s\ntheorem, an exact solution space expression algorithm is proposed and\nstatistical analysis of the nodes' states is provided. The statistical results\nfit well with the algorithmic results until the emergence of the unfrozen core,\nwhich makes the fluctuation of statistical quantities and causes the replica\nsymmetric breaking in the solutions. Besides, the entropy of bipartite\nVertex-Cover solutions is calculated with the clustering entropy using a cycle\nsimplification technique for the unfrozen core. Furthermore, as generalization\nof bipartite graphs, bipartite core graph is proposed, the solution space of\nwhich can also be easily determined; and based on these results, how to\ngenerate a $K\\ddot{o}nig-Egerv\\acute{a}ry$ subgraph is studied by a growth\nprocess of adding edges. The investigation of solution space of bipartite graph\nVertex-Cover provides intensive understanding and some insights on the solution\nspace complexity, and will produce benefit for finding maximal\n$K\\ddot{o}nig-Egerv\\acute{a}ry$ subgraphs, solving general graph Vertex-Cover\nand recognizing the intrinsic hardness of NP-complete problems.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 02:06:31 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Wei", "Wei", ""], ["Zhang", "Yunjia", ""], ["Wang", "Ting", ""], ["Li", "Baifeng", ""], ["Niu", "Baolong", ""], ["Zheng", "Zhiming", ""]]}, {"id": "1505.07203", "submitter": "Jean Cousty", "authors": "Jean Cousty (LIGM), Laurent Najman (LIGM), Yukiko Kenmochi (LIGM),\n  Silvio Guimar\\~A{\\pounds}es (VIPLAB, LIGM)", "title": "New characterizations of minimum spanning trees and of saliency maps\n  based on quasi-flat zones", "comments": null, "journal-ref": "12th International Symposium on Mathematical Morphology (ISMM),\n  May 2015, Reykjavik, Iceland. Lecture Notes in Computer Science (LNCS), 9082,\n  pp.205-216, Mathematical Morphology and Its Applications to Signal and Image\n  Processing", "doi": "10.1007/978-3-319-18720-4_18", "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three representations of hierarchies of partitions: dendrograms\n(direct representations), saliency maps, and minimum spanning trees. We provide\na new bijection between saliency maps and hierarchies based on quasi-flat zones\nas used in image processing and characterize saliency maps and minimum spanning\ntrees as solutions to constrained minimization problems where the constraint is\nquasi-flat zones preservation. In practice, these results form a toolkit for\nnew hierarchical methods where one can choose the most convenient\nrepresentation. They also invite us to process non-image data with\nmorphological hierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 06:36:10 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Cousty", "Jean", "", "LIGM"], ["Najman", "Laurent", "", "LIGM"], ["Kenmochi", "Yukiko", "", "LIGM"], ["Guimar\u00c3\u00a3es", "Silvio", "", "VIPLAB, LIGM"]]}, {"id": "1505.07321", "submitter": "Ralph Neininger", "authors": "Kevin Leckey, Ralph Neininger, Wojciech Szpankowski", "title": "A Limit Theorem for Radix Sort and Tries with Markovian Input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tries are among the most versatile and widely used data structures on words.\nIn particular, they are used in fundamental sorting algorithms such as radix\nsort which we study in this paper. While the performance of radix sort and\ntries under a realistic probabilistic model for the generation of words is of\nsignificant importance, its analysis, even for simplest memoryless sources, has\nproved difficult. In this paper we consider a more realistic model where words\nare generated by a Markov source. By a novel use of the contraction method\ncombined with moment transfer techniques we prove a central limit theorem for\nthe complexity of radix sort and for the external path length in a trie. This\nis the first application of the contraction method to the analysis of\nalgorithms and data structures with Markovian inputs; it relies on the use of\nsystems of stochastic recurrences combined with a product version of the\nZolotarev metric.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 13:58:55 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Leckey", "Kevin", ""], ["Neininger", "Ralph", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "1505.07391", "submitter": "Daniel Roche", "authors": "Daniel S. Roche, Adam J. Aviv, Seung Geol Choi", "title": "A Practical Oblivious Map Data Structure with Secure Deletion and\n  History Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a new oblivious RAM that supports variable-sized storage blocks\n(vORAM), which is the first ORAM to allow varying block sizes without trivial\npadding. We also present a new history-independent data structure (a HIRB tree)\nthat can be stored within a vORAM. Together, this construction provides an\nefficient and practical oblivious data structure (ODS) for a key/value map, and\ngoes further to provide an additional privacy guarantee as compared to prior\nODS maps: even upon client compromise, deleted data and the history of old\noperations remain hidden to the attacker.\n  We implement and measure the performance of our system using Amazon Web\nServices, and the single-operation time for a realistic database (up to\n$2^{18}$ entries) is less than 1 second. This represents a 100x speed-up\ncompared to the current best oblivious map data structure (which provides\nneither secure deletion nor history independence) by Wang et al. (CCS 14).\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:08:34 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 17:00:18 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Roche", "Daniel S.", ""], ["Aviv", "Adam J.", ""], ["Choi", "Seung Geol", ""]]}, {"id": "1505.07862", "submitter": "Sandor P. Fekete", "authors": "Erik D. Demaine, S\\'andor P. Fekete, Christian Scheffer, Arne Schmidt", "title": "New Geometric Algorithms for Fully Connected Staged Self-Assembly", "comments": "21 pages, 14 figures; full version of conference paper in DNA21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider staged self-assembly systems, in which square-shaped tiles can be\nadded to bins in several stages. Within these bins, the tiles may connect to\neach other, depending on the glue types of their edges. Previous work by\nDemaine et al. showed that a relatively small number of tile types suffices to\nproduce arbitrary shapes in this model. However, these constructions were only\nbased on a spanning tree of the geometric shape, so they did not produce full\nconnectivity of the underlying grid graph in the case of shapes with holes;\ndesigning fully connected assemblies with a polylogarithmic number of stages\nwas left as a major open problem. We resolve this challenge by presenting new\nsystems for staged assembly that produce fully connected polyominoes in O(log^2\nn) stages, for various scale factors and temperature {\\tau} = 2 as well as\n{\\tau} = 1. Our constructions work even for shapes with holes and uses only a\nconstant number of glues and tiles. Moreover, the underlying approach is more\ngeometric in nature, implying that it promised to be more feasible for shapes\nwith compact geometric description.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 21:22:49 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 19:52:04 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Demaine", "Erik D.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Scheffer", "Christian", ""], ["Schmidt", "Arne", ""]]}, {"id": "1505.08003", "submitter": "Ulrich Breunig", "authors": "Ulrich Breunig, Verena Schmid, Richard F. Hartl, Thibaut Vidal", "title": "A large neighbourhood based heuristic for two-echelon routing problems", "comments": null, "journal-ref": "Computers & Operations Research 2016; 76: 208-225", "doi": "10.1016/j.cor.2016.06.014", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address two optimisation problems arising in the context of\ncity logistics and two-level transportation systems. The two-echelon vehicle\nrouting problem and the two-echelon location routing problem seek to produce\nvehicle itineraries to deliver goods to customers, with transits through\nintermediate facilities. To efficiently solve these problems, we propose a\nhybrid metaheuristic which combines enumerative local searches with\ndestroy-and-repair principles, as well as some tailored operators to optimise\nthe selections of intermediate facilities. We conduct extensive computational\nexperiments to investigate the contribution of these operators to the search\nperformance, and measure the performance of the method on both problem classes.\nThe proposed algorithm finds the current best known solutions, or better ones,\nfor 95% of the two-echelon vehicle routing problem benchmark instances.\nOverall, for both problems, it achieves high-quality solutions within short\ncomputing times. Finally, for future reference, we resolve inconsistencies\nbetween different versions of benchmark instances, document their differences,\nand provide them all online in a unified format.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 11:53:20 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 11:59:58 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Breunig", "Ulrich", ""], ["Schmid", "Verena", ""], ["Hartl", "Richard F.", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1505.08126", "submitter": "Lior Eldar", "authors": "Michael Ben-Or, Lior Eldar", "title": "A Quasi-Random Approach to Matrix Spectral Analysis", "comments": "Replacing previous version: parallel algorithm runs in total\n  complexity $n^{\\omega+1}$ and not $n^{\\omega}$. However, the depth of the\n  implementing circuit is $\\log^2(n)$: hence comparable to fastest\n  eigen-decomposition algorithms known", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the quantum computing algorithms for Linear Algebra problems\n[HHL,TaShma] we study how the simulation on a classical computer of this type\nof \"Phase Estimation algorithms\" performs when we apply it to solve the\nEigen-Problem of Hermitian matrices. The result is a completely new, efficient\nand stable, parallel algorithm to compute an approximate spectral decomposition\nof any Hermitian matrix. The algorithm can be implemented by Boolean circuits\nin $O(\\log^2 n)$ parallel time with a total cost of $O(n^{\\omega+1})$ Boolean\noperations. This Boolean complexity matches the best known rigorous $O(\\log^2\nn)$ parallel time algorithms, but unlike those algorithms our algorithm is\n(logarithmically) stable, so further improvements may lead to practical\nimplementations.\n  All previous efficient and rigorous approaches to solve the Eigen-Problem use\nrandomization to avoid bad condition as we do too. Our algorithm makes further\nuse of randomization in a completely new way, taking random powers of a unitary\nmatrix to randomize the phases of its eigenvalues. Proving that a tiny Gaussian\nperturbation and a random polynomial power are sufficient to ensure almost\npairwise independence of the phases $(\\mod (2\\pi))$ is the main technical\ncontribution of this work. This randomization enables us, given a Hermitian\nmatrix with well separated eigenvalues, to sample a random eigenvalue and\nproduce an approximate eigenvector in $O(\\log^2 n)$ parallel time and\n$O(n^\\omega)$ Boolean complexity. We conjecture that further improvements of\nour method can provide a stable solution to the full approximate spectral\ndecomposition problem with complexity similar to the complexity (up to a\nlogarithmic factor) of sampling a single eigenvector.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 18:02:03 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 00:59:01 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 12:56:27 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Ben-Or", "Michael", ""], ["Eldar", "Lior", ""]]}]