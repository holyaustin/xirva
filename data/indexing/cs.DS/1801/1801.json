[{"id": "1801.00202", "submitter": "Darren Strash", "authors": "Pablo San Segundo and Jorge Artieda and Darren Strash", "title": "Efficiently Enumerating all Maximal Cliques with Bit-Parallelism", "comments": "15 pages, 1 figure", "journal-ref": "Computers & Operations Research 92 (2018) 37-46", "doi": "10.1016/j.cor.2017.12.006", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal clique enumeration (MCE) problem has numerous applications in\nbiology, chemistry, sociology, and graph modeling. Though this problem is well\nstudied, most current research focuses on finding solutions in large sparse\ngraphs or very dense graphs, while sacrificing efficiency on the most difficult\nmedium-density benchmark instances that are representative of data sets often\nencountered in practice. We show that techniques that have been successfully\napplied to the maximum clique problem give significant speed gains over the\nstate-of-the-art MCE algorithms on these instances. Specifically, we show that\na simple greedy pivot selection based on a fixed maximum-degree first ordering\nof vertices, when combined with bit-parallelism, performs consistently better\nthan the theoretical worst-case optimal pivoting of the state-of-the-art\nalgorithms of Tomita et al. [Theoretical Computer Science, 2006] and Naud\\'e\n[Theoretical Computer Science, 2016].\n  Experiments show that our algorithm is faster than the worst-case optimal\nalgorithm of Tomita et al. on 60 out of 74 standard structured and random\nbenchmark instances: we solve 48 instances 1.2 to 2.2 times faster, and solve\nthe remaining 12 instances 3.6 to 47.6 times faster. We also see consistent\nspeed improvements over the algorithm of Naud\\'e: solving 61 instances 1.2 to\n2.4 times faster. To the best of our knowledge, we are the first to achieve\nsuch speed-ups compared to these state-of-the-art algorithms on these standard\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 22:29:04 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Segundo", "Pablo San", ""], ["Artieda", "Jorge", ""], ["Strash", "Darren", ""]]}, {"id": "1801.00313", "submitter": "Mateusz Lewandowski", "authors": "Jaros{\\l}aw Byrka, Mateusz Lewandowski, Joachim Spoerhase", "title": "Approximating Node-Weighted k-MST on Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a minimum weight connected subgraph spanning\nat least $k$ vertices on planar, node-weighted graphs. We give a\n$(4+\\eps)$-approximation algorithm for this problem. We achieve this by\nutilizing the recent LMP primal-dual $3$-approximation for the node-weighted\nprize-collecting Steiner tree problem by Byrka et al (SWAT'16) and adopting an\napproach by Chudak et al. (Math.\\ Prog.\\ '04) regarding Lagrangian relaxation\nfor the edge-weighted variant. In particular, we improve the procedure of\npicking additional vertices (tree merging procedure) given by Sadeghian (2013)\nby taking a constant number of recursive steps and utilizing the limited\nguessing procedure of Arora and Karakostas (Math.\\ Prog.\\ '06). More generally,\nour approach readily gives a $(\\nicefrac{4}{3}\\cdot r+\\eps)$-approximation on\nany graph class where the algorithm of Byrka et al.\\ for the prize-collecting\nversion gives an $r$-approximation. We argue that this can be interpreted as a\ngeneralization of an analogous result by K\\\"onemann et al. (Algorithmica~'11)\nfor partial cover problems. Together with a lower bound construction by Mestre\n(STACS'08) for partial cover this implies that our bound is essentially best\npossible among algorithms that utilize an LMP algorithm for the Lagrangian\nrelaxation as a black box. In addition to that, we argue by a more involved\nlower bound construction that even using the LMP algorithm by Byrka et al.\\ in\na \\emph{non-black-box} fashion could not beat the factor $\\nicefrac{4}{3}\\cdot\nr$ when the tree merging step relies only on the solutions output by the LMP\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 16:48:56 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 13:43:44 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Lewandowski", "Mateusz", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1801.00316", "submitter": "Rami Daknama", "authors": "Rami Daknama", "title": "Pull and Push&Pull in Random Evolving Graphs", "comments": "13 pages; corrected mistake in the proof for Push&Pull, results\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Push, the Pull and the Push&Pull algorithms are well-studied rumor\nspreading protocols. In all three, in the beginning one node of a graph is\ninformed. In the Push setting, every round every informed node chooses a\nneighbor uniformly at random and, if it is not already informed anyway, informs\nit. In the Pull setting, each round each uninformed node chooses a neighbor\nuniformly at random and asks it for the rumor; if the asked neighbor is\ninformed, now also the asking node is informed. Push&Pull is a combination of\nPush and Pull: In each round, each node picks a neighbor uniformly at random.\nIf at least one of both knows the rumor, after this round, both know the rumor.\nClementi et al. have considered Push in settings where the underlying graph\nchanges each round. In one setting they investigated, in each round the\nunderlying graph is a newly sampled Erd\\H{o}s-R\\'enyi random graph $G(n,p)$.\nThey show that if $p\\geq 1/n$ then with probability $1-o(1)$ (as $n\\rightarrow\n\\infty$) the number of rounds needed until all nodes are informed is\n$\\mathcal{O}(\\ln(n))$. Doerr and Kostrygin introduced a general framework to\nanalyze rumor spreading algorithms; using this framework, for $a>0$ and $p=a/n$\nthey improved the previous results in the described setting: The expected\nnumber of rounds needed by Push was determined to be\n$\\log_{2-e^{-a}}(n)+1/(1-e^{-a})\\ln(n)+\\mathcal{O}(1)$; also large deviation\nbounds were obtained. Using their framework, we investigate Pull and Push&Pull\nin that setting: We prove that the expected number of rounds needed by Pull to\ninform all nodes is $\\log_{2-e^{-a}}(n)+1/a \\ln(n)+\\mathcal{O}(1)$. Let $\\gamma\n:= 2(1-e^{-a})-(1-e^{-a})^2/a$; we prove that the expected number of rounds\nneeded by Push&Pull is $\\log_{1+\\gamma}(n)+1/a\\ln(n)+\\mathcal{O}(1)$; as a\nbyproduct, we obtain large deviation bounds, too.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 17:03:54 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 18:39:46 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Daknama", "Rami", ""]]}, {"id": "1801.00365", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Karol Golab and Dariusz R. Kowalski", "title": "Broadcasting Spanning Forests on a Multiple-Access Channel", "comments": null, "journal-ref": "Theory of Computing Systems, 36(6) : 711 - 733, 2003", "doi": "10.1007/s00224-003-1149-8", "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a spanning forest of a graph in a\ndistributed-processing environment is studied. If an input graph is weighted,\nthen the goal is to find a minimum-weight spanning forest. The processors\ncommunicate by broadcasting. The output consists of the edges that make a\nspanning forest and have been broadcast on the network. Input edges are\ndistributed among the processors, with each edge held by one processor.\n  The underlying broadcast network is implemented as a multiple-access channel.\nIf exactly one processor attempts to perform a broadcast, then the broadcast is\nsuccessful. A message broadcast successfully is delivered to all the processors\nin one step. If more than one processors broadcast simultaneously, then the\nmessages interfere with each other and no processor can receive any of them.\n  Optimality of algorithmic solutions is investigated, by way of comparing\ndeterministic with randomized algorithms, and adaptive with oblivious ones.\nLower bounds are proved that either justify the optimality of specific\nalgorithms or show that the optimal performance depends on a class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 22:20:46 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["Golab", "Karol", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1801.00527", "submitter": "Matthew Gelber", "authors": "Matthew K. Gelber, Greg Hurst, Rohit Bhargava", "title": "Freeform Assembly Planning", "comments": "Submitted to IEEE Transactions on Automation Science and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CC cs.DS cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D printing enables the fabrication of complex architectures by automating\nlong sequences of additive steps. The increasing sophistication of printers,\nmaterials, and generative design promises to make geometric complexity a\nnon-issue in manufacturing; however, this complexity can only be realized if a\ndesign can be translated into a physically executable sequence of printing\noperations. We investigate this planning problem for freeform direct-write\nassembly, in which filaments of material are deposited through a nozzle\ntranslating along a 3D path to create sparse, frame-like structures. We\nenumerate the process constraints for different variants of the freeform\nassembly process and show that, in the case where material stiffens via a glass\ntransition, determining whether a feasible sequence exists is NP-complete.\nNonetheless, for topologies typically encountered in real-world applications,\nfinding a feasible or even optimal sequence is a tractable problem. We develop\na sequencing algorithm that maximizes the fidelity of the printed part and\nminimizes the probability of print failure by modeling the assembly as a\nlinear, elastic frame. We implement the algorithm and validate our approach\nexperimentally, printing objects composed of thousands of sugar alcohol\nfilaments with diameters of 100-200 microns. The assembly planner allows the\nfreeform process to be applied to arbitrarily complex parts, from tissue\nengineering and microfluidics at the micrometer scale, to vascularized\nfunctional materials and soft robots at the millimeter scale, to structural\ncomponents at the meter scale, thus opening a variety of assembly\npossibilities.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 23:41:50 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 05:53:04 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 21:26:37 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Gelber", "Matthew K.", ""], ["Hurst", "Greg", ""], ["Bhargava", "Rohit", ""]]}, {"id": "1801.00716", "submitter": "Till Tantau", "authors": "Max Bannach, Till Tantau", "title": "Computing Hitting Set Kernels By AC^0-Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypergraph $H = (V,E)$, what is the smallest subset $X \\subseteq V$\nsuch that $e \\cap X \\neq \\emptyset$ holds for all $e \\in E$? This problem,\nknown as the hitting set problem, is a basic problem in parameterized\ncomplexity theory. There are well-known kernelization algorithms for it, which\nget a hypergraph $H$ and a number $k$ as input and output a hypergraph $H'$\nsuch that (1) $H$ has a hitting set of size $k$ if, and only if, $H'$ has such\na hitting set and (2) the size of $H'$ depends only on $k$ and on the maximum\ncardinality $d$ of edges in $H$. The algorithms run in polynomial time, but are\nhighly sequential. Recently, it has been shown that one of them can be\nparallelized to a certain degree: one can compute hitting set kernels in\nparallel time $O(d)$ -- but it was conjectured that this is the best parallel\nalgorithm possible. We refute this conjecture and show how hitting set kernels\ncan be computed in constant parallel time. For our proof, we introduce a new,\ngeneralized notion of hypergraph sunflowers and show how iterated applications\nof the color coding technique can sometimes be collapsed into a single\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 16:42:26 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Bannach", "Max", ""], ["Tantau", "Till", ""]]}, {"id": "1801.00760", "submitter": "Alan Frieze", "authors": "Colin Cooper, Alan Frieze, Tony Johansson", "title": "The cover time of a biased random walk on a random cubic graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a random walk that prefers tou se unvisited edges in the context of\nrandom cubic graphs. We establish asymptotically correct estimates for the\nvertex and edge cover times, these being $\\approx n\\log n$ and $\\approx\n\\frac32n\\log n$ respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 18:29:32 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 13:48:19 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Cooper", "Colin", ""], ["Frieze", "Alan", ""], ["Johansson", "Tony", ""]]}, {"id": "1801.00776", "submitter": "Yijie Han", "authors": "Yijie Han", "title": "Sorting Real Numbers in $O(n\\sqrt{\\log n})$ Time and Linear Space", "comments": "Fixed some issues in the early versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O(n\\sqrt{\\log n})$ time and linear space algorithm for sorting\nreal numbers. This breaks the long time illusion that real numbers have to be\nsorted by comparison sorting and take $\\Omega (n\\log n)$ time to be sorted.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 21:23:58 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 14:36:04 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 12:48:45 GMT"}, {"version": "v4", "created": "Sat, 1 Dec 2018 23:06:06 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Han", "Yijie", ""]]}, {"id": "1801.00777", "submitter": "Sergey Tarasov", "authors": "Alexander Shananin and Sergey Tarasov", "title": "Effective algorithms for homogeneous utility functions", "comments": "8 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Under the assumption of (positive) homogeneity (PH in the sequel) of the\ncorresponding utility functions, we construct polynomial time algorithms for\nthe weak separability, the collective consumption behavior and some related\nproblems. These problems are known to be at least $NP$-hard if the homogeneity\nassumption is dropped.\n  Keywords: the utility function, the economic indices theory, the collective\naxiom of revealed preference, the weak separability property, the class of the\ndifferential form of the demand.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 21:53:03 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Shananin", "Alexander", ""], ["Tarasov", "Sergey", ""]]}, {"id": "1801.01059", "submitter": "Pawel Gawrychowski", "authors": "Bart{\\l}omiej Dudek and Pawe{\\l} Gawrychowski", "title": "Slowing Down Top Trees for Better Worst-Case Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the top tree compression scheme introduced by Bille et al. [ICALP\n2013] and construct an infinite family of trees on $n$ nodes labeled from an\nalphabet of size $\\sigma$, for which the size of the top DAG is\n$\\Theta(\\frac{n}{\\log_\\sigma n}\\log\\log_\\sigma n)$. Our construction matches a\npreviously known upper bound and exhibits a weakness of this scheme, as the\ninformation-theoretic lower bound is $\\Omega(\\frac{n}{\\log_\\sigma n})$. This\nsettles an open problem stated by Lohrey et al. [arXiv 2017], who designed a\nmore involved version achieving the lower bound. We show that this can be also\nguaranteed by a very minor modification of the original scheme: informally, one\nonly needs to ensure that different parts of the tree are not compressed too\nquickly. Arguably, our version is more uniform, and in particular, the\ncompression procedure is oblivious to the value of $\\sigma$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 16:00:26 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "1801.01096", "submitter": "Tomasz Kociumaka", "authors": "Tomasz Kociumaka and Jakub Radoszewski and Wojciech Rytter and Tomasz\n  Wale\\'n", "title": "On Periodicity Lemma for Partial Words", "comments": "Full version of a paper accepted to LATA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the function $L(h,p,q)$, called here the threshold function,\nrelated to periodicity of partial words (words with holes). The value\n$L(h,p,q)$ is defined as the minimum length threshold which guarantees that a\nnatural extension of the periodicity lemma is valid for partial words with $h$\nholes and (strong) periods $p,q$. We show how to evaluate the threshold\nfunction in $O(\\log p + \\log q)$ time, which is an improvement upon the best\npreviously known $O(p+q)$-time algorithm. In a series of papers, the formulae\nfor the threshold function, in terms of $p$ and $q$, were provided for each\nfixed $h \\le 7$. We demystify the generic structure of such formulae, and for\neach value $h$ we express the threshold function in terms of a piecewise-linear\nfunction with $O(h)$ pieces.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 18:01:29 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1801.01105", "submitter": "Sven J\\\"ager", "authors": "Sven J\\\"ager and Martin Skutella", "title": "Generalizing the Kawaguchi-Kyan bound to stochastic parallel machine\n  scheduling", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2018.43", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the sum of weighted completion times on $m$ identical parallel\nmachines is one of the most important and classical scheduling problems. For\nthe stochastic variant where processing times of jobs are random variables,\nM\\\"ohring, Schulz, and Uetz (1999) presented the first and still best known\napproximation result achieving, for arbitrarily many machines, performance\nratio $1+\\frac12(1+\\Delta)$, where $\\Delta$ is an upper bound on the squared\ncoefficient of variation of the processing times. We prove performance ratio\n$1+\\frac12(\\sqrt{2}-1)(1+\\Delta)$ for the same underlying algorithm---the\nWeighted Shortest Expected Processing Time (WSEPT) rule. For the special case\nof deterministic scheduling (i.e., $\\Delta=0$), our bound matches the tight\nperformance ratio $\\frac12(1+\\sqrt{2})$ of this algorithm (WSPT rule), derived\nby Kawaguchi and Kyan in a 1986 landmark paper. We present several further\nimprovements for WSEPT's performance ratio, one of them relying on a carefully\nrefined analysis of WSPT yielding, for every fixed number of machines $m$,\nWSPT's exact performance ratio of order $\\frac12(1+\\sqrt{2})-O(1/m^2)$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 18:39:29 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 13:47:57 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["J\u00e4ger", "Sven", ""], ["Skutella", "Martin", ""]]}, {"id": "1801.01404", "submitter": "Jakub Radoszewski", "authors": "Garance Gourdel, Tomasz Kociumaka, Jakub Radoszewski, Wojciech Rytter,\n  Arseny Shur, and Tomasz Wale\\'n", "title": "String Periods in the Order-Preserving Model", "comments": "Full version of a paper accepted to STACS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The order-preserving model (op-model, in short) was introduced quite recently\nbut has already attracted significant attention because of its applications in\ndata analysis. We introduce several types of periods in this setting\n(op-periods). Then we give algorithms to compute these periods in time $O(n)$,\n$O(n\\log\\log n)$, $O(n \\log^2 \\log n/\\log \\log \\log n)$, $O(n\\log n)$ depending\non the type of periodicity. In the most general variant the number of different\nperiods can be as big as $\\Omega(n^2)$, and a compact representation is needed.\nOur algorithms require novel combinatorial insight into the properties of such\nperiods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 15:31:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Gourdel", "Garance", ""], ["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Shur", "Arseny", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1801.01689", "submitter": "Christian Scheffer", "authors": "Erik D. Demaine and S\\'andor P. Fekete and Phillip Keldenich and Henk\n  Meijer and Christian Scheffer", "title": "Coordinated Motion Planning: Reconfiguring a Swarm of Labeled Robots\n  with Bounded Stretch", "comments": "32 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a number of breakthroughs for coordinated motion planning, in\nwhich the objective is to reconfigure a swarm of labeled convex objects by a\ncombination of parallel, continuous, collision-free translations into a given\ntarget arrangement. Problems of this type can be traced back to the classic\nwork of Schwartz and Sharir (1983), who gave a method for deciding the\nexistence of a coordinated motion for a set of disks between obstacles; their\napproach is polynomial in the complexity of the obstacles, but exponential in\nthe number of disks. Other previous work has largely focused on {\\em\nsequential} schedules, in which one robot moves at a time.\n  We provide constant-factor approximation algorithms for minimizing the\nexecution time of a coordinated, {\\em parallel} motion plan for a swarm of\nrobots in the absence of obstacles, provided some amount of separability.\n  Our algorithm achieves {\\em constant stretch factor}: If all robots are at\nmost $d$ units from their respective starting positions, the total duration of\nthe overall schedule is $O(d)$. Extensions include unlabeled robots and\ndifferent classes of robots. We also prove that finding a plan with minimal\nexecution time is NP-hard, even for a grid arrangement without any stationary\nobstacles. On the other hand, we show that for densely packed disks that cannot\nbe well separated, a stretch factor $\\Omega(N^{1/4})$ may be required. On the\npositive side, we establish a stretch factor of $O(N^{1/2})$ even in this case.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:15:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Demaine", "Erik D.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Keldenich", "Phillip", ""], ["Meijer", "Henk", ""], ["Scheffer", "Christian", ""]]}, {"id": "1801.01903", "submitter": "Anisur Molla Rahaman", "authors": "Anisur Rahaman Molla and Gopal Pandurangan", "title": "Local Mixing Time: Distributed Computation and Applications", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixing time of a graph is an important metric, which is not only useful\nin analyzing connectivity and expansion properties of the network, but also\nserves as a key parameter in designing efficient algorithms. We introduce a new\nnotion of mixing of a random walk on a (undirected) graph, called local mixing.\nInformally, the local mixing with respect to a given node $s$, is the mixing of\na random walk probability distribution restricted to a large enough subset of\nnodes --- say, a subset of size at least $n/\\beta$ for a given parameter\n$\\beta$ --- containing $s$. The time to mix over such a subset by a random walk\nstarting from a source node $s$ is called the local mixing time with respect to\n$s$. The local mixing time captures the local connectivity and expansion\nproperties around a given source node and is a useful parameter that determines\nthe running time of algorithms for partial information spreading, gossip etc.\n  Our first contribution is formally defining the notion of local mixing time\nin an undirected graph. We then present an efficient distributed algorithm\nwhich computes a constant factor approximation to the local mixing time with\nrespect to a source node $s$ in $\\tilde{O}(\\tau_s)$ rounds, where $\\tau_s$ is\nthe local mixing time w.r.t $s$ in an $n$-node regular graph. This bound holds\nwhen $\\tau_s$ is significantly smaller than the conductance of the local mixing\nset (i.e., the set where the walk mixes locally); this is typically the\ninteresting case where the local mixing time is significantly smaller than the\nmixing time (with respect to $s$). We also present a distributed algorithm that\ncomputes the exact local mixing time in $\\tilde{O}(\\tau_s \\mathcal{D})$ rounds,\nwhere $\\mathcal{D} =\\min\\{\\tau_s, D\\}$ and $D$ is the diameter of the graph. We\nfurther show that local mixing time tightly characterizes the complexity of\npartial information spreading.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 19:24:34 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1801.02239", "submitter": "Radu Stefan Mincu", "authors": "Radu Stefan Mincu, Alexandru Popa", "title": "Heuristic algorithms for the min-max edge 2-coloring problem", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in International Computing and Combinatorics Conference\n  (COCOON'18). The final authenticated version is available online at:\n  http://www.doi.org/10.1007/978-3-319-94776-1_55", "journal-ref": null, "doi": "10.1007/978-3-319-94776-1_55", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-channel Wireless Mesh Networks (WMN), each node is able to use\nmultiple non-overlapping frequency channels. Raniwala et al. (MC2R 2004,\nINFOCOM 2005) propose and study several such architectures in which a computer\ncan have multiple network interface cards. These architectures are modeled as a\ngraph problem named \\emph{maximum edge $q$-coloring} and studied in several\npapers by Feng et. al (TAMC 2007), Adamaszek and Popa (ISAAC 2010, JDA 2016).\nLater on Larjomaa and Popa (IWOCA 2014, JGAA 2015) define and study an\nalternative variant, named the \\emph{min-max edge $q$-coloring}.\n  The above mentioned graph problems, namely the maximum edge $q$-coloring and\nthe min-max edge $q$-coloring are studied mainly from the theoretical\nperspective. In this paper, we study the min-max edge 2-coloring problem from a\npractical perspective. More precisely, we introduce, implement and test four\nheuristic approximation algorithms for the min-max edge $2$-coloring problem.\nThese algorithms are based on a \\emph{Breadth First Search} (BFS)-based\nheuristic and on \\emph{local search} methods like basic \\emph{hill climbing},\n\\emph{simulated annealing} and \\emph{tabu search} techniques, respectively.\nAlthough several algorithms for particular graph classes were proposed by\nLarjomaa and Popa (e.g., trees, planar graphs, cliques, bi-cliques,\nhypergraphs), we design the first algorithms for general graphs.\n  We study and compare the running data for all algorithms on Unit Disk Graphs,\nas well as some graphs from the DIMACS vertex coloring benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 20:01:33 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 12:29:08 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Mincu", "Radu Stefan", ""], ["Popa", "Alexandru", ""]]}, {"id": "1801.02358", "submitter": "Priyanka Mukhopadhyay Ms", "authors": "Divesh Aggarwal, Priyanka Mukhopadhyay", "title": "Improved algorithms for the Shortest Vector Problem and the Closest\n  Vector Problem in the infinity norm", "comments": "Changed the title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blomer and Naewe[BN09] modified the randomized sieving algorithm of Ajtai,\nKumar and Sivakumar[AKS01] to solve the shortest vector problem (SVP). The\nalgorithm starts with $N = 2^{O(n)}$ randomly chosen vectors in the lattice and\nemploys a sieving procedure to iteratively obtain shorter vectors in the\nlattice. The running time of the sieving procedure is quadratic in $N$.\n  We study this problem for the special but important case of the $\\ell_\\infty$\nnorm. We give a new sieving procedure that runs in time linear in $N$, thereby\nsignificantly improving the running time of the algorithm for SVP in the\n$\\ell_\\infty$ norm. As in [AKS02,BN09], we also extend this algorithm to obtain\nsignificantly faster algorithms for approximate versions of the shortest vector\nproblem and the closest vector problem (CVP) in the $\\ell_\\infty$ norm.\n  We also show that the heuristic sieving algorithms of Nguyen and Vidick[NV08]\nand Wang et al.[WLTB11] can also be analyzed in the $\\ell_{\\infty}$ norm. The\nmain technical contribution in this part is to calculate the expected volume of\nintersection of a unit ball centred at origin and another ball of a different\nradius centred at a uniformly random point on the boundary of the unit ball.\nThis might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 09:43:43 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 17:04:41 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Mukhopadhyay", "Priyanka", ""]]}, {"id": "1801.02363", "submitter": "Michele Amoretti", "authors": "Davide Ferrari and Michele Amoretti", "title": "Efficient and Effective Quantum Compiling for Entanglement-based Machine\n  Learning on IBM Q Devices", "comments": "14 pages, 13 figures", "journal-ref": "International Journal of Quantum Information 16 (08), 1840006,\n  2018", "doi": "10.1142/S0219749918400063", "report-no": null, "categories": "quant-ph cs.DS cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum compiling means fast, device-aware implementation of quantum\nalgorithms (i.e., quantum circuits, in the quantum circuit model of\ncomputation). In this paper, we present a strategy for compiling IBM Q -aware,\nlow-depth quantum circuits that generate Greenberger-Horne-Zeilinger (GHZ)\nentangled states. The resulting compiler can replace the QISKit compiler for\nthe specific purpose of obtaining improved GHZ circuits. It is well known that\nGHZ states have several practical applications, including quantum machine\nlearning. We illustrate our experience in implementing and querying a uniform\nquantum example oracle based on the GHZ circuit, for solving the classically\nhard problem of learning parity with noise.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 10:06:53 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 09:23:09 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 15:36:18 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Ferrari", "Davide", ""], ["Amoretti", "Michele", ""]]}, {"id": "1801.02693", "submitter": "Jiehua Chen", "authors": "Jiehua Chen and Rolf Niedermeier and Piotr Skowron", "title": "Stable Marriage with Multi-Modal Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generalized version of the famous Stable Marriage problem, now\nbased on multi-modal preference lists. The central twist herein is to allow\neach agent to rank its potentially matching counterparts based on more than one\n\"evaluation mode\" (e.g., more than one criterion); thus, each agent is equipped\nwith multiple preference lists, each ranking the counterparts in a possibly\ndifferent way. We introduce and study three natural concepts of stability,\ninvestigate their mutual relations and focus on computational complexity\naspects with respect to computing stable matchings in these new scenarios.\nMostly encountering computational hardness (NP-hardness), we can also spot few\nislands of tractability and make a surprising connection to the \\textsc{Graph\nIsomorphism} problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 21:21:05 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Chen", "Jiehua", ""], ["Niedermeier", "Rolf", ""], ["Skowron", "Piotr", ""]]}, {"id": "1801.02790", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty and Sanjeev Khanna", "title": "Better and Simpler Error Analysis of the Sinkhorn-Knopp Algorithm for\n  Matrix Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a non-negative $n \\times m$ real matrix $A$, the {\\em matrix scaling}\nproblem is to determine if it is possible to scale the rows and columns so that\neach row and each column sums to a specified target value for it. This problem\narises in many algorithmic applications, perhaps most notably as a\npreconditioning step in solving a linear system of equations. One of the most\nnatural and by now classical approach to matrix scaling is the Sinkhorn-Knopp\nalgorithm (also known as the RAS method) where one alternately scales either\nall rows or all columns to meet the target values. In addition to being\nextremely simple and natural, another appeal of this procedure is that it\neasily lends itself to parallelization. A central question is to understand the\nrate of convergence of the Sinkhorn-Knopp algorithm.\n  In this paper, we present an elementary convergence analysis for the\nSinkhorn-Knopp algorithm that improves upon the previous best bound. In a\nnutshell, our approach is to show a simple bound on the number of iterations\nneeded so that the KL-divergence between the current row-sums and the target\nrow-sums drops below a specified threshold $\\delta$, and then connect the\nKL-divergence with $\\ell_1$ and $\\ell_2$ distances. For $\\ell_1$, we can use\nPinsker's inequality. For $\\ell_2$, we develop a strengthening of Pinsker's\ninequality, called (KL vs $\\ell_1/\\ell_2$) in the paper, which lower bounds the\nKL-divergence by a combination of $\\ell_1$ and $\\ell_2$ distance. This\ninequality may be of independent interest.\n  The idea of studying Sinkhorn-Knopp convergence via KL-divergence is not new\nand has indeed been previously explored. Our contribution is an elementary,\nself-contained presentation of this approach and an interesting new inequality\nthat yields a significantly stronger convergence guarantee for the extensively\nstudied $\\ell_2$-error.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 04:24:01 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 16:15:31 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1801.02793", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi and Sanjeev Khanna", "title": "Tight Bounds on the Round Complexity of the Distributed Maximum Coverage\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum $k$-set coverage problem in the following distributed\nsetting. A collection of sets $S_1,\\ldots,S_m$ over a universe $[n]$ is\npartitioned across $p$ machines and the goal is to find $k$ sets whose union\ncovers the most number of elements. The computation proceeds in synchronous\nrounds. In each round, all machines simultaneously send a message to a central\ncoordinator who then communicates back to all machines a summary to guide the\ncomputation for the next round. At the end, the coordinator outputs the answer.\nThe main measures of efficiency in this setting are the approximation ratio of\nthe returned solution, the communication cost of each machine, and the number\nof rounds of computation.\n  Our main result is an asymptotically tight bound on the tradeoff between\nthese measures for the distributed maximum coverage problem. We first show that\nany $r$-round protocol for this problem either incurs a communication cost of $\nk \\cdot m^{\\Omega(1/r)}$ or only achieves an approximation factor of\n$k^{\\Omega(1/r)}$. This implies that any protocol that simultaneously achieves\ngood approximation ratio ($O(1)$ approximation) and good communication cost\n($\\widetilde{O}(n)$ communication per machine), essentially requires\nlogarithmic (in $k$) number of rounds. We complement our lower bound result by\nshowing that there exist an $r$-round protocol that achieves an\n$\\frac{e}{e-1}$-approximation (essentially best possible) with a communication\ncost of $k \\cdot m^{O(1/r)}$ as well as an $r$-round protocol that achieves a\n$k^{O(1/r)}$-approximation with only $\\widetilde{O}(n)$ communication per each\nmachine (essentially best possible).\n  We further use our results in this distributed setting to obtain new bounds\nfor the maximum coverage problem in two other main models of computation for\nmassive datasets, namely, the dynamic streaming model and the MapReduce model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 04:35:28 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 01:44:36 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1801.02816", "submitter": "C. Seshadhri", "authors": "Deeparnab Chakrabarty and C. Seshadhri", "title": "Adaptive Boolean Monotonicity Testing in Total Influence Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing monotonicity of a Boolean function $f:\\{0,1\\}^n \\to\n\\{0,1\\}$ has received much attention recently. Denoting the proximity parameter\nby $\\varepsilon$, the best tester is the non-adaptive\n$\\widetilde{O}(\\sqrt{n}/\\varepsilon^2)$ tester of Khot-Minzer-Safra (FOCS\n2015). Let $I(f)$ denote the total influence of $f$. We give an adaptive tester\nwhose running time is $I(f)poly(\\varepsilon^{-1}\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 06:43:29 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Seshadhri", "C.", ""]]}, {"id": "1801.02982", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "How To Make the Gradients Small Stochastically: Even Faster Convex and\n  Nonconvex SGD", "comments": "V2 added two applications to nonconvex stochastic optimization, and\n  V3 corrects a citation. arXiv admin note: text overlap with arXiv:1708.08694", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) gives an optimal convergence rate when\nminimizing convex stochastic objectives $f(x)$. However, in terms of making the\ngradients small, the original SGD does not give an optimal rate, even when\n$f(x)$ is convex.\n  If $f(x)$ is convex, to find a point with gradient norm $\\varepsilon$, we\ndesign an algorithm SGD3 with a near-optimal rate\n$\\tilde{O}(\\varepsilon^{-2})$, improving the best known rate\n$O(\\varepsilon^{-8/3})$ of [18].\n  If $f(x)$ is nonconvex, to find its $\\varepsilon$-approximate local minimum,\nwe design an algorithm SGD5 with rate $\\tilde{O}(\\varepsilon^{-3.5})$, where\npreviously SGD variants only achieve $\\tilde{O}(\\varepsilon^{-4})$ [6, 15, 33].\nThis is no slower than the best known stochastic version of Newton's method in\nall parameter regimes [30].\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 10:26:50 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 08:27:48 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 22:02:16 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1801.03190", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis, Shreyas Sekar, Johnson Lam, Liu Yang", "title": "Risk-Averse Matchings over Uncertain Graph Databases", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of applications such as querying sensor networks, and\nanalyzing protein-protein interaction (PPI) networks, rely on mining uncertain\ngraph and hypergraph databases. In this work we study the following problem:\ngiven an uncertain, weighted (hyper)graph, how can we efficiently find a\n(hyper)matching with high expected reward, and low risk?\n  This problem naturally arises in the context of several important\napplications, such as online dating, kidney exchanges, and team formation. We\nintroduce a novel formulation for finding matchings with maximum expected\nreward and bounded risk under a general model of uncertain weighted\n(hyper)graphs that we introduce in this work. Our model generalizes\nprobabilistic models used in prior work, and captures both continuous and\ndiscrete probability distributions, thus allowing to handle privacy related\napplications that inject appropriately distributed noise to (hyper)edge\nweights. Given that our optimization problem is NP-hard, we turn our attention\nto designing efficient approximation algorithms. For the case of uncertain\nweighted graphs, we provide a $\\frac{1}{3}$-approximation algorithm, and a\n$\\frac{1}{5}$-approximation algorithm with near optimal run time. For the case\nof uncertain weighted hypergraphs, we provide a\n$\\Omega(\\frac{1}{k})$-approximation algorithm, where $k$ is the rank of the\nhypergraph (i.e., any hyperedge includes at most $k$ nodes), that runs in\nalmost (modulo log factors) linear time.\n  We complement our theoretical results by testing our approximation algorithms\non a wide variety of synthetic experiments, where we observe in a controlled\nsetting interesting findings on the trade-off between reward, and risk. We also\nprovide an application of our formulation for providing recommendations of\nteams that are likely to collaborate, and have high impact.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 23:41:28 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""], ["Sekar", "Shreyas", ""], ["Lam", "Johnson", ""], ["Yang", "Liu", ""]]}, {"id": "1801.03253", "submitter": "Arijit Ghosh", "authors": "Arijit Ghosh, Sudeshna Kolay, and Gopinath Mishra", "title": "FPT algorithms for embedding into low complexity graphic metrics", "comments": "41 pages; corrected a minor mistake in Section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Metric Embedding problem takes as input two metric spaces $(X,D_X)$ and\n$(Y,D_Y)$, and a positive integer $d$. The objective is to determine whether\nthere is an embedding $F:X \\rightarrow Y$ such that $d_{F} \\leq d$, where\n$d_{F}$ denotes the distortion of the map $F$. Such an embedding is called a\ndistortion $d$ embedding. The bijective Metric Embedding problem is a special\ncase of the Metric Embedding problem where $|X| = |Y|$. In parameterized\ncomplexity, the Metric Embedding problem, in full generality, is known to be\nW-hard and therefore, not expected to have an FPT algorithm. In this paper, we\nconsider the Gen-Graph Metric Embedding problem, where the two metric spaces\nare graph metrics. We explore the extent of tractability of the problem in the\nparameterized complexity setting. We determine whether an unweighted graph\nmetric $(G,D_G)$ can be embedded, or bijectively embedded, into another\nunweighted graph metric $(H,D_H)$, where the graph $H$ has low structural\ncomplexity. For example, $H$ is a cycle, or $H$ has bounded treewidth or\nbounded connected treewidth. The parameters for the algorithms are chosen from\nthe upper bound $d$ on distortion, bound $\\Delta$ on the maximum degree of $H$,\ntreewidth $\\alpha$ of $H$, and the connected treewidth $\\alpha_{c}$ of $H$.\n  Our general approach to these problems can be summarized as trying to\nunderstand the behavior of the shortest paths in $G$ under a low distortion\nembedding into $H$, and the structural relation the mapping of these paths has\nto shortest paths in $H$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 07:19:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 15:33:32 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 10:34:41 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ghosh", "Arijit", ""], ["Kolay", "Sudeshna", ""], ["Mishra", "Gopinath", ""]]}, {"id": "1801.03347", "submitter": "J.  Miguel Diaz-Ba\\~nez", "authors": "Luis-Evaristo Caraballo, Jos\\'e-Miguel D\\'iaz-B\\'a\\~nez, Nadine Kroher", "title": "A Polynomial Algorithm for Balanced Clustering via Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of clustering is to discover natural groups in datasets and to\nidentify geometrical structures which might reside there, without assuming any\nprior knowledge on the characteristics of the data. The problem can be seen as\ndetecting the inherent separations between groups of a given point set in a\nmetric space governed by a similarity function. The pairwise similarities\nbetween all data objects form a weighted graph adjacency matrix which contains\nall necessary information for the clustering process, which can consequently be\nformulated as a graph partitioning problem. In this context, we propose a new\ncluster quality measure which uses the maximum spanning tree and allows us to\ncompute the optimal clustering under the min-max principle in polynomial time.\nOur algorithm can be applied when a load-balanced clustering is required.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 12:36:48 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 10:35:17 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Caraballo", "Luis-Evaristo", ""], ["D\u00edaz-B\u00e1\u00f1ez", "Jos\u00e9-Miguel", ""], ["Kroher", "Nadine", ""]]}, {"id": "1801.03462", "submitter": "Christoph D\\\"urr", "authors": "Spyros Angelopoulos, Christoph D\\\"urr and Shendan Jin", "title": "Online Maximum Matching with Recourse", "comments": null, "journal-ref": null, "doi": "10.1007/s10878-020-00641-w", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online maximum matching problem in a model in which the edges\nare associated with a known recourse parameter $k$. An online algorithm for\nthis problem has to maintain a valid matching while edges of the underlying\ngraph are presented one after the other. At any moment the algorithm can decide\nto include an edge into the matching or to exclude it, under the restriction\nthat at most $k$ such actions per edge take place, where $k$ is typically a\nsmall constant. This problem was introduced and studied in the context of\ngeneral online packing problems with recourse by Avitabile et al. [Information\nProcessing Letters, 2013], whereas the special case $k=2$ was studied by Boyar\net al. [WADS 2017].\n  In the first part of this paper we consider the edge arrival model, in which\nan arriving edge never disappears from the graph. Here, we first show an\nimproved analysis on the performance of the algorithm AMP of Avitabile et al.,\nby exploiting the structure of the matching problem. In addition, we show that\nthe greedy algorithm has competitive ratio $3/2$ for every even $k$ and ratio\n$2$ for every odd $k$. Moreover, we present and analyze an improvement of the\ngreedy algorithm which we call $L$-Greedy, and we show that for small values of\n$k$ it outperforms the algorithm AMP. In terms of lower bounds, we show that no\ndeterministic algorithm better than $1+1/(k-1)$ exists, improving upon the\nknown lower bound of $1+1/k$.\n  The second part of the paper is devoted to the edge arrival/departure model,\nwhich is the fully dynamic variant of online matching with recourse. The\nanalysis of $L$-Greedy and AMP carry through in this model; moreover we show a\nlower bound of $(k^2-3k+6) / (k^2-4k+7)$ for all even $k \\ge 4$. For\n$k\\in\\{2,3\\}$, the competitive ratio is $3/2$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 17:22:23 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 15:48:44 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 15:09:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["D\u00fcrr", "Christoph", ""], ["Jin", "Shendan", ""]]}, {"id": "1801.03879", "submitter": "Michael Lampis", "authors": "R\\'emy Belmonte, Michael Lampis, Valia Mitsou", "title": "Parameterized (Approximate) Defective Coloring", "comments": "Accepted to STACS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Defective Coloring we are given a graph $G = (V, E)$ and two integers\n$\\chi_d, \\Delta^*$ and are asked if we can partition $V$ into $\\chi_d$ color\nclasses, so that each class induces a graph of maximum degree $\\Delta^*$. We\ninvestigate the complexity of this generalization of Coloring with respect to\nseveral well-studied graph parameters, and show that the problem is W-hard\nparameterized by treewidth, pathwidth, tree-depth, or feedback vertex set, if\n$\\chi_d = 2$. As expected, this hardness can be extended to larger values of\n$\\chi_d$ for most of these parameters, with one surprising exception: we show\nthat the problem is FPT parameterized by feedback vertex set for any $\\chi_d\n\\ge 2$, and hence 2-coloring is the only hard case for this parameter. In\naddition to the above, we give an ETH-based lower bound for treewidth and\npathwidth, showing that no algorithm can solve the problem in $n^{o(pw)}$,\nessentially matching the complexity of an algorithm obtained with standard\ntechniques.\n  We complement these results by considering the problem's approximability and\nshow that, with respect to $\\Delta^*$, the problem admits an algorithm which\nfor any $\\epsilon > 0$ runs in time $(tw/\\epsilon)^{O(tw)}$ and returns a\nsolution with exactly the desired number of colors that approximates the\noptimal $\\Delta^*$ within $(1 + \\epsilon)$. We also give a $(tw)^{O(tw)}$\nalgorithm which achieves the desired $\\Delta^*$ exactly while 2-approximating\nthe minimum value of $\\chi_d$. We show that this is close to optimal, by\nestablishing that no FPT algorithm can (under standard assumptions) achieve a\nbetter than $3/2$-approximation to $\\chi_d$, even when an extra constant\nadditive error is also allowed.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 17:13:11 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Lampis", "Michael", ""], ["Mitsou", "Valia", ""]]}, {"id": "1801.04191", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Computing permanents of complex diagonally dominant matrices and tensors", "comments": "13 pages, minor improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that for any $\\lambda > 1$, fixed in advance, the permanent of an $n\n\\times n$ complex matrix, where the absolute value of each diagonal entry is at\nleast $\\lambda$ times bigger than the sum of the absolute values of all other\nentries in the same row, can be approximated within any relative error $0 <\n\\epsilon < 1$ in quasi-polynomial $n^{O(\\ln n - \\ln \\epsilon)}$ time. We extend\nthis result to multidimensional permanents of tensors and discuss its\napplication to weighted counting of perfect matchings in hypergraphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 15:00:48 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 19:04:00 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "1801.04307", "submitter": "Shaogang Wang", "authors": "Shaogang Wang, Vishal M. Patel and Athina Petropulu", "title": "Robust Sparse Fourier Transform Based on The Fourier Projection-Slice\n  Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art automotive radars employ multidimensional discrete\nFourier transforms (DFT) in order to estimate various target parameters. The\nDFT is implemented using the fast Fourier transform (FFT), at sample and\ncomputational complexity of $O(N)$ and $O(N \\log N)$, respectively, where $N$\nis the number of samples in the signal space. We have recently proposed a\nsparse Fourier transform based on the Fourier projection-slice theorem\n(FPS-SFT), which applies to multidimensional signals that are sparse in the\nfrequency domain. FPS-SFT achieves sample complexity of $O(K)$ and\ncomputational complexity of $O(K \\log K)$ for a multidimensional, $K$-sparse\nsignal. While FPS-SFT considers the ideal scenario, i.e., exactly sparse data\nthat contains on-grid frequencies, in this paper, by extending FPS-SFT into a\nrobust version (RFPS-SFT), we emphasize on addressing noisy signals that\ncontain off-grid frequencies; such signals arise from radar applications. This\nis achieved by employing a windowing technique and a voting-based frequency\ndecoding procedure; the former reduces the frequency leakage of the off-grid\nfrequencies below the noise level to preserve the sparsity of the signal, while\nthe latter significantly lowers the frequency localization error stemming from\nthe noise. The performance of the proposed method is demonstrated both\ntheoretically and numerically.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 00:22:16 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Wang", "Shaogang", ""], ["Patel", "Vishal M.", ""], ["Petropulu", "Athina", ""]]}, {"id": "1801.04414", "submitter": "Ruosong Wang", "authors": "Ruosong Wang and David P. Woodruff", "title": "Tight Bounds for $\\ell_p$ Oblivious Subspace Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $\\ell_p$ oblivious subspace embedding is a distribution over $r \\times n$\nmatrices $\\Pi$ such that for any fixed $n \\times d$ matrix $A$,\n$$\\Pr_{\\Pi}[\\textrm{for all }x, \\ \\|Ax\\|_p \\leq \\|\\Pi Ax\\|_p \\leq \\kappa\n\\|Ax\\|_p] \\geq 9/10,$$ where $r$ is the dimension of the embedding, $\\kappa$ is\nthe distortion of the embedding, and for an $n$-dimensional vector $y$,\n$\\|y\\|_p$ is the $\\ell_p$-norm. Another important property is the sparsity of\n$\\Pi$, that is, the maximum number of non-zero entries per column, as this\ndetermines the running time of computing $\\Pi \\cdot A$. While for $p = 2$ there\nare nearly optimal tradeoffs in terms of the dimension, distortion, and\nsparisty, for the important case of $1 \\leq p < 2$, much less was known. In\nthis paper we obtain nearly optimal tradeoffs for $\\ell_p$ oblivious subspace\nembeddings for every $1 \\leq p < 2$.\n  We show for every $1 \\leq p < 2$, any oblivious subspace embedding with\ndimension $r$ has distortion $\\kappa = \\Omega\n\\left(\\frac{1}{\\left(\\frac{1}{d}\\right)^{1 / p} \\cdot \\log^{2 / p}r +\n\\left(\\frac{r}{n}\\right)^{1 / p - 1 / 2}}\\right).$ When $r = \\mathrm{poly}(d)$\nin applications, this gives a $\\kappa = \\Omega(d^{1/p}\\log^{-2/p} d)$ lower\nbound, and shows the oblivious subspace embedding of Sohler and Woodruff (STOC,\n2011) for $p = 1$ and the oblivious subspace embedding of Meng and Mahoney\n(STOC, 2013) for $1 < p < 2$ are optimal up to $\\mathrm{poly}(\\log(d))$\nfactors. We also give sparse oblivious subspace embeddings for every $1 \\leq p\n< 2$ which are optimal in dimension and distortion, up to $\\mathrm{poly}(\\log\nd)$ factors.\n  Oblivious subspace embeddings are crucial for distributed and streaming\nenvironments, as well as entrywise $\\ell_p$ low rank approximation. Our results\ngive improved algorithms for these applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 10:40:05 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 23:43:26 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Wang", "Ruosong", ""], ["Woodruff", "David P.", ""]]}, {"id": "1801.04425", "submitter": "Solon Pissis", "authors": "Lorraine A.K. Ayad and Panagiotis Charalampopoulos and Costas S.\n  Iliopoulos and Solon P. Pissis", "title": "Longest Common Prefixes with $k$-Errors and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although real-world text datasets, such as DNA sequences, are far from being\nuniformly random, average-case string searching algorithms perform\nsignificantly better than worst-case ones in most applications of interest. In\nthis paper, we study the problem of computing the longest prefix of each suffix\nof a given string of length $n$ over a constant-sized alphabet that occurs\nelsewhere in the string with $k$-errors. This problem has already been studied\nunder the Hamming distance model. Our first result is an improvement upon the\nstate-of-the-art average-case time complexity for non-constant $k$ and using\nonly linear space under the Hamming distance model. Notably, we show that our\ntechnique can be extended to the edit distance model with the same time and\nspace complexities. Specifically, our algorithms run in $\\mathcal{O}(n \\log^k n\n\\log \\log n)$ time on average using $\\mathcal{O}(n)$ space. We show that our\ntechnique is applicable to several algorithmic problems in computational\nbiology and elsewhere.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 11:55:45 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Ayad", "Lorraine A. K.", ""], ["Charalampopoulos", "Panagiotis", ""], ["Iliopoulos", "Costas S.", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1801.04497", "submitter": "Devanathan Thiruvenkatachari", "authors": "Amey Bhangale, Subhash Khot, Swastik Kopparty, Sushant Sachdeva,\n  Devanathan Thiruvenkatachari", "title": "Near-optimal approximation algorithm for simultaneous Max-Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the simultaneous Max-Cut problem, we are given $k$ weighted graphs on the\nsame set of $n$ vertices, and the goal is to find a cut of the vertex set so\nthat the minimum, over the $k$ graphs, of the cut value is as large as\npossible. Previous work [BKS15] gave a polynomial time algorithm which achieved\nan approximation factor of $1/2 - o(1)$ for this problem (and an approximation\nfactor of $1/2 + \\epsilon_k$ in the unweighted case, where $\\epsilon_k\n\\rightarrow 0$ as $k \\rightarrow \\infty$).\n  In this work, we give a polynomial time approximation algorithm for\nsimultaneous Max-Cut with an approximation factor of $0.8780$ (for all constant\n$k$). The natural SDP formulation for simultaneous Max-Cut was shown to have an\nintegrality gap of $1/2+\\epsilon_k$ in [BKS15]. In achieving the better\napproximation guarantee, we use a stronger Sum-of-Squares hierarchy SDP\nrelaxation and a rounding algorithm based on Raghavendra-Tan [RT12], in\naddition to techniques from [BKS15].\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 01:50:32 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Bhangale", "Amey", ""], ["Khot", "Subhash", ""], ["Kopparty", "Swastik", ""], ["Sachdeva", "Sushant", ""], ["Thiruvenkatachari", "Devanathan", ""]]}, {"id": "1801.04498", "submitter": "Louxin Zhang", "authors": "Andreas D. M. Gunawan, Bingxin Lu, Louxin Zhang", "title": "Fast Methods for Solving the Cluster Containment Problem for\n  Phylogenetic Networks", "comments": "8 figure, 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic and comparative genomic studies indicate that extant genomes are more\nproperly considered to be a fusion product of random mutations over generations\nand genomic material transfers between individuals of different lineages. This\nhas motivated researchers to adopt phylogenetic networks and other general\nmodels to study genome evolution. One important problem arising from\nreconstruction and verification of phylogenetic networks is the cluster\ncontainment problem, namely determining whether or not a cluster of taxa is\ndisplayed in a phylogenetic network. In this work, a new upper bound for this\nNP-complete problem is established through an efficient reduction to the SAT\nproblem. Two efficient (albeit exponential time) methods are also implemented.\nIt is developed on the basis of generalization of the so-called\nreticulation-visible property of phylogenetic networks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 02:11:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Gunawan", "Andreas D. M.", ""], ["Lu", "Bingxin", ""], ["Zhang", "Louxin", ""]]}, {"id": "1801.04641", "submitter": "Sam Buss", "authors": "Sam Buss, Alexander Knop", "title": "Strategies for Stable Merge Sorting", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new stable natural merge sort algorithms, called $2$-merge sort\nand $\\alpha$-merge sort. We prove upper and lower bounds for several merge sort\nalgorithms, including Timsort, Shivers' sort, $\\alpha$-stack sorts, and our new\n$2$-merge and $\\alpha$-merge sorts. The upper and lower bounds have the forms\n$c \\cdot n \\log m$ and $c \\cdot n \\log n$ for inputs of length~$n$ comprising\n$m$~monotone runs. For Timsort, we prove a lower bound of $(1.5 - o(1)) n \\log\nn$. For $2$-merge sort, we prove optimal upper and lower bounds of\napproximately $(1.089 \\pm o(1))n \\log m$. We prove similar asymptotically\nmatching upper and lower bounds for $\\alpha$-merge sort, when $\\varphi < \\alpha\n< 2$, where $\\varphi$ is the golden ratio.\n  Our bounds are in terms of merge cost; this upper bounds the number of\ncomparisons and accurately models runtime. The merge strategies can be used for\nany stable merge sort, not just natural merge sorts. The new $2$-merge and\n$\\alpha$-merge sorts have better worst-case merge cost upper bounds and are\nslightly simpler to implement than the widely-used Timsort; they also perform\nbetter in experiments. We report also experimental comparisons with algorithms\ndeveloped by Munro-Wild and Jug\\'e subsequently to the results of the present\npaper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 02:16:39 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 18:41:43 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 04:48:39 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2019 19:47:26 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Buss", "Sam", ""], ["Knop", "Alexander", ""]]}, {"id": "1801.04696", "submitter": "Thomas Ridremont", "authors": "C\\'edric Bentz (CEDRIC), Marie-Christine Costa (OC), Pierre-Louis\n  Poirion (CEDRIC), Thomas Ridremont", "title": "Robust capacitated trees and networks with uniform demands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the design of robust (or resilient) capacitated rooted\nSteiner networks in case of terminals with uniform demands. Formally, we are\ngiven a graph, capacity and cost functions on the edges, a root, a subset of\nnodes called terminals, and a bound k on the number of edge failures. We first\nstudy the problem where k = 1 and the network that we want to design must be a\ntree covering the root and the terminals: we give complexity results and\npropose models to optimize both the cost of the tree and the number of\nterminals disconnected from the root in the worst case of an edge failure,\nwhile respecting the capacity constraints on the edges. Second, we consider the\nproblem of computing a minimum-cost survivable network, i.e., a network that\ncovers the root and terminals even after the removal of any k edges, while\nstill respecting the capacity constraints on the edges. We also consider the\npossibility of protecting a given number of edges. We propose three different\nformulations: a cut-set based formulation, a flow based one, and a bilevel one\n(with an attacker and a defender). We propose algorithms to solve each\nformulation and compare their efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 08:22:13 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Bentz", "C\u00e9dric", "", "CEDRIC"], ["Costa", "Marie-Christine", "", "OC"], ["Poirion", "Pierre-Louis", "", "CEDRIC"], ["Ridremont", "Thomas", ""]]}, {"id": "1801.04702", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, George B. Mertzios, Felix Reidl", "title": "Searching for Maximum Out-Degree Vertices in Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex $x$ in a tournament $T$ is called a king if for every vertex $y$ of\n$T$ there is a directed path from $x$ to $y$ of length at most 2. It is not\nhard to show that every vertex of maximum out-degree in a tournament is a king.\nHowever, tournaments may have kings which are not vertices of maximum\nout-degree. A binary inquiry asks for the orientation of the edge between a\npair of vertices and receives the answer. The cost of finding a king in an\nunknown tournament is the number of binary inquiries required to detect a king.\nFor the cost of finding a king in a tournament, in the worst case, Shen, Sheng\nand Wu (SIAM J. Comput., 2003) proved a lower and upper bounds of\n$\\Omega(n^{4/3})$ and $O(n^{3/2})$, respectively. In contrast to their result,\nwe prove that the cost of finding a vertex of maximum out-degree is ${n \\choose\n2} -O(n)$ in the worst case.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 09:01:20 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Gutin", "Gregory", ""], ["Mertzios", "George B.", ""], ["Reidl", "Felix", ""]]}, {"id": "1801.04783", "submitter": "Nina Holden", "authors": "Nina Holden, Robin Pemantle, Yuval Peres, and Alex Zhai", "title": "Subpolynomial trace reconstruction for random strings and arbitrary\n  deletion probability", "comments": "Analysis of running time added and proof simplified. Alex Zhai added\n  as author. 37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insertion-deletion channel takes as input a bit string ${\\bf\nx}\\in\\{0,1\\}^{n}$, and outputs a string where bits have been deleted and\ninserted independently at random. The trace reconstruction problem is to\nrecover $\\bf x$ from many independent outputs (called \"traces\") of the\ninsertion-deletion channel applied to $\\bf x$. We show that if $\\bf x$ is\nchosen uniformly at random, then $\\exp(O(\\log^{1/3} n))$ traces suffice to\nreconstruct $\\bf x$ with high probability. For the deletion channel with\ndeletion probability $q < 1/2$ the earlier upper bound was $\\exp(O(\\log^{1/2}\nn))$. The case of $q\\geq 1/2$ or the case where insertions are allowed has not\nbeen previously analyzed, and therefore the earlier upper bound was as for\nworst-case strings, i.e., $\\exp(O( n^{1/3}))$. We also show that our\nreconstruction algorithm runs in $n^{1+o(1)}$ time.\n  A key ingredient in our proof is a delicate two-step alignment procedure\nwhere we estimate the location in each trace corresponding to a given bit of\n$\\bf x$. The alignment is done by viewing the strings as random walks and\ncomparing the increments in the walk associated with the input string and the\ntrace, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 13:03:19 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 13:07:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Holden", "Nina", ""], ["Pemantle", "Robin", ""], ["Peres", "Yuval", ""], ["Zhai", "Alex", ""]]}, {"id": "1801.04801", "submitter": "Rosario Scatamacchia", "authors": "Federico Della Croce, Ulrich Pferschy, Rosario Scatamacchia", "title": "Approximating the Incremental Knapsack Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the 0-1 Incremental Knapsack Problem (IKP) where the capacity\ngrows over time periods and if an item is placed in the knapsack in a certain\nperiod, it cannot be removed afterwards. The contribution of a packed item in\neach time period depends on its profit as well as on a time factor which\nreflects the importance of the period in the objective function. The problem\ncalls for maximizing the weighted sum of the profits over the whole time\nhorizon. In this work, we provide approximation results for IKP and its\nrestricted variants. In some results, we rely on Linear Programming (LP) to\nderive approximation bounds and show how the proposed LP-based analysis can be\nseen as a valid alternative to more formal proof systems. We first manage to\nprove the tightness of some approximation ratios of a general purpose algorithm\ncurrently available in the literature and originally applied to a\ntime-invariant version of the problem. We also devise a Polynomial Time\nApproximation Scheme (PTAS) when the input value indicating the number of\nperiods is considered as a constant. Then, we add the mild and natural\nassumption that each item can be packed in the first time period. For this\nvariant, we discuss different approximation algorithms suited for any number of\ntime periods and for the special case with two periods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 13:53:27 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Della Croce", "Federico", ""], ["Pferschy", "Ulrich", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "1801.04991", "submitter": "Stephan Held", "authors": "Stephan Held, Jochen K\\\"onemann, Jens Vygen", "title": "Vehicle Routing with Subtours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When delivering items to a set of destinations, one can save time and cost by\npassing a subset to a sub-contractor at any point en route. We consider a model\nwhere a set of items are initially loaded in one vehicle and should be\ndistributed before a given deadline {\\Delta}. In addition to travel time and\ntime for deliveries, we assume that there is a fixed delay for handing over an\nitem from one vehicle to another.\n  We will show that it is easy to decide whether an instance is feasible, i.e.,\nwhether it is possible to deliver all items before the deadline {\\Delta}. We\nthen consider computing a feasible tour of minimum cost, where we incur a cost\nper unit distance traveled by the vehicles, and a setup cost for every used\nvehicle. Our problem arises in practical applications and generalizes classical\nproblems such as shallow-light trees and the bounded-latency problem.\n  Our main result is a polynomial-time algorithm that, for any given {\\epsilon}\n> 0 and any feasible instance, computes a solution that delivers all items\nbefore time (1+ {\\epsilon}){\\Delta} and has cost O(1 + 1 / {\\epsilon}) OPT,\nwhere OPT is the minimum cost of any feasible solution.\n  We show that our result is best possible in the sense that any improvement\nwould lead to progress on 25-year-old questions on shallow-light trees.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 20:33:06 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Held", "Stephan", ""], ["K\u00f6nemann", "Jochen", ""], ["Vygen", "Jens", ""]]}, {"id": "1801.05055", "submitter": "Edward Raff", "authors": "Edward Raff and Charles Nicholas", "title": "Toward Metric Indexes for Incremental Insertion and Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the use of metric index structures, which accelerate\nnearest neighbor queries, in the scenario where we need to interleave\ninsertions and queries during deployment. This use-case is inspired by a\nreal-life need in malware analysis triage, and is surprisingly understudied.\nExisting literature tends to either focus on only final query efficiency, often\ndoes not support incremental insertion, or does not support arbitrary distance\nmetrics. We modify and improve three algorithms to support our scenario of\nincremental insertion and querying with arbitrary metrics, and evaluate them on\nmultiple datasets and distance metrics while varying the value of $k$ for the\ndesired number of nearest neighbors. In doing so we determine that our improved\nVantage-Point tree of Minimum-Variance performs best for this scenario.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:25:16 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Raff", "Edward", ""], ["Nicholas", "Charles", ""]]}, {"id": "1801.05127", "submitter": "David Wajc", "authors": "Bernhard Haeupler, D. Ellis Hershkowitz, David Wajc", "title": "Round- and Message-Optimal Distributed Graph Algorithms", "comments": "To appear in PODC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed graph algorithms that separately optimize for either the number\nof rounds used or the total number of messages sent have been studied\nextensively. However, algorithms simultaneously efficient with respect to both\nmeasures have been elusive. For example, only very recently was it shown that\nfor Minimum Spanning Tree (MST), an optimal message and round complexity is\nachievable (up to polylog terms) by a single algorithm in the CONGEST model of\ncommunication.\n  In this paper we provide algorithms that are simultaneously round- and\nmessage-optimal for a number of well-studied distributed optimization problems.\nOur main result is such a distributed algorithm for the fundamental primitive\nof computing simple functions over each part of a graph partition. From this\nalgorithm we derive round- and message-optimal algorithms for multiple\nproblems, including MST, Approximate Min-Cut and Approximate Single Source\nShortest Paths, among others. On general graphs all of our algorithms achieve\nworst-case optimal $\\tilde{O}(D+\\sqrt n)$ round complexity and $\\tilde{O}(m)$\nmessage complexity. Furthermore, our algorithms require an optimal\n$\\tilde{O}(D)$ rounds and $\\tilde{O}(n)$ messages on planar, genus-bounded,\ntreewidth-bounded and pathwidth-bounded graphs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 06:22:32 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 20:43:48 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 02:53:45 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Hershkowitz", "D. Ellis", ""], ["Wajc", "David", ""]]}, {"id": "1801.05360", "submitter": "Shuai Ma", "authors": "Xuelian Lin, Jiahao Jiang, Shuai Ma, Yimeng Zuo, Chunming Hu", "title": "One-Pass Trajectory Simplification Using the Synchronous Euclidean\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various mobile devices have been used to collect, store and transmit\ntremendous trajectory data, and it is known that raw trajectory data seriously\nwastes the storage, network band and computing resource. To attack this issue,\none-pass line simplification (LS) algorithms have are been developed, by\ncompressing data points in a trajectory to a set of continuous line segments.\nHowever, these algorithms adopt the perpendicular Euclidean distance, and none\nof them uses the synchronous Euclidean distance (SED), and cannot support\nspatio-temporal queries. To do this, we develop two one-pass error bounded\ntrajectory simplification algorithms (CISED-S and CISED-W) using SED, based on\na novel spatio-temporal cone intersection technique. Using four real-life\ntrajectory datasets, we experimentally show that our approaches are both\nefficient and effective. In terms of running time, algorithms CISED-S and\nCISED-W are on average 3 times faster than SQUISH-E (the most efficient\nexisting LS algorithm using SED). In terms of compression ratios, algorithms\nCISED-S and CISED-W are comparable with and 19.6% better than DPSED (the most\neffective existing LS algorithm using SED) on average, respectively, and are\n21.1% and 42.4% better than SQUISH-E on average, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:36:09 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Lin", "Xuelian", ""], ["Jiang", "Jiahao", ""], ["Ma", "Shuai", ""], ["Zuo", "Yimeng", ""], ["Hu", "Chunming", ""]]}, {"id": "1801.05489", "submitter": "Rosario Scatamacchia", "authors": "Federico Della Croce, Rosario Scatamacchia", "title": "Longest Processing Time rule for identical parallel machines revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Pm || Cmax scheduling problem where the goal is to schedule n\njobs on m identical parallel machines to minimize makespan. We revisit the\nfamous Longest Processing Time (LPT) rule proposed by Graham in 1969. LPT\nrequires to sort jobs in non-ascending order of processing times and then to\nassign one job at a time to the machine whose load is smallest so far. We\nprovide new insights on LPT and discuss the approximation ratio of a\nmodification of LPT that improves Graham's bound from 4/3 - 1/(3m) to 4/3 -\n1/(3(m-1)) for m >= 3 and from 7/6 to 9/8 for m = 2. We use Linear Programming\n(LP) to analyze the approximation ratio of our approach. This performance\nanalysis can be seen as a valid alternative to formal proofs based on\nanalytical derivation. Also, we derive from the proposed approach an O(n log n)\nheuristic. The heuristic splits the sorted jobset in tuples of m consecutive\njobs (1,...,m; m+1,...,2m; etc.) and sorts the tuples in non-increasing order\nof the difference (slack) between largest job and smallest job in the tuple.\nThen, List Scheduling is applied to the set of sorted tuples. This approach\nstrongly outperforms LPT on benchmark literature instances.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 21:39:50 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Della Croce", "Federico", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "1801.05768", "submitter": "Zhen Chen", "authors": "Zhen Chen, Zhiying Wang and Syed Jafar", "title": "The Asymptotic Capacity of Private Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The private search problem is introduced, where a dataset comprised of $L$\ni.i.d. records is replicated across $N$ non-colluding servers, each record\ntakes values uniformly from an alphabet of size $K$, and a user wishes to\nsearch for all records that match a privately chosen value, without revealing\nany information about the chosen value to any individual server. The capacity\nof private search is the maximum number of bits of desired information that can\nbe retrieved per bit of download. The asymptotic (large $K$) capacity of\nprivate search is shown to be $1-1/N$, even as the scope of private search is\nfurther generalized to allow approximate (OR) search over a number of\nrealizations that grows with $K$. The results are based on the asymptotic\nbehavior of a new converse bound for private information retrieval with\narbitrarily dependent messages.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 05:29:38 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 18:17:36 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Chen", "Zhen", ""], ["Wang", "Zhiying", ""], ["Jafar", "Syed", ""]]}, {"id": "1801.05832", "submitter": "Renato J Cintra", "authors": "D. F. G. Coelho, R. J. Cintra, V. S. Dimitrov", "title": "Efficient Computation of the 8-point DCT via Summation by Parts", "comments": "Fixed Fig. 1 with the block diagram of the proposed architecture.\n  Manuscript contains 13 pages, 4 figures, 2 tables", "journal-ref": "J Sign Process Syst (2017)", "doi": "10.1007/s11265-017-1270-6", "report-no": null, "categories": "cs.DS cs.NA math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new fast algorithm for the 8-point discrete cosine\ntransform (DCT) based on the summation-by-parts formula. The proposed method\nconverts the DCT matrix into an alternative transformation matrix that can be\ndecomposed into sparse matrices of low multiplicative complexity. The method is\ncapable of scaled and exact DCT computation and its associated fast algorithm\nachieves the theoretical minimal multiplicative complexity for the 8-point DCT.\nDepending on the nature of the input signal simplifications can be introduced\nand the overall complexity of the proposed algorithm can be further reduced.\nSeveral types of input signal are analyzed: arbitrary, null mean, accumulated,\nand null mean/accumulated signal. The proposed tool has potential application\nin harmonic detection, image enhancement, and feature extraction, where input\nsignal DC level is discarded and/or the signal is required to be integrated.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 19:21:15 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 21:43:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Coelho", "D. F. G.", ""], ["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1801.05855", "submitter": "Fei Jiang", "authors": "Fei Jiang, Lifang He, Yi Zheng, Enqiang Zhu, Jin Xu, Philip S. Yu", "title": "On Spectral Graph Embedding: A Non-Backtracking Perspective and Graph\n  Approximation", "comments": "SDM 2018 (Full version including all proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding has been proven to be efficient and effective in facilitating\ngraph analysis. In this paper, we present a novel spectral framework called\nNOn-Backtracking Embedding (NOBE), which offers a new perspective that\norganizes graph data at a deep level by tracking the flow traversing on the\nedges with backtracking prohibited. Further, by analyzing the non-backtracking\nprocess, a technique called graph approximation is devised, which provides a\nchannel to transform the spectral decomposition on an edge-to-edge matrix to\nthat on a node-to-node matrix. Theoretical guarantees are provided by bounding\nthe difference between the corresponding eigenvalues of the original graph and\nits graph approximation. Extensive experiments conducted on various real-world\nnetworks demonstrate the efficacy of our methods on both macroscopic and\nmicroscopic levels, including clustering and structural hole spanner detection.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 19:11:54 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Jiang", "Fei", ""], ["He", "Lifang", ""], ["Zheng", "Yi", ""], ["Zhu", "Enqiang", ""], ["Xu", "Jin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1801.05857", "submitter": "EPTCS", "authors": "Nathan Cassee (Eindhoven University of Technology, Eindhoven, The\n  Netherlands), Thomas Neele (Eindhoven University of Technology, Eindhoven,\n  The Netherlands), Anton Wijs (Eindhoven University of Technology, Eindhoven,\n  The Netherlands)", "title": "On the Scalability of the GPUexplore Explicit-State Model Checker", "comments": "In Proceedings GaM 2017, arXiv:1712.08345", "journal-ref": "EPTCS 263, 2017, pp. 38-52", "doi": "10.4204/EPTCS.263.4", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of graphics processors (GPUs) is a promising approach to speed up\nmodel checking to such an extent that it becomes feasible to instantly verify\nsoftware systems during development. GPUexplore is an explicit-state model\nchecker that runs all its computations on the GPU. Over the years it has been\nextended with various techniques, and the possibilities to further improve its\nperformance have been continuously investigated. In this paper, we discuss how\nthe hash table of the tool works, which is at the heart of its functionality.\nWe propose an alteration of the hash table that in isolated experiments seems\npromising, and analyse its effect when integrated in the tool. Furthermore, we\ninvestigate the current scalability of GPUexplore, by experimenting both with\ninput models of varying sizes and running the tool on one of the latest GPUs of\nNVIDIA.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 05:16:04 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Cassee", "Nathan", "", "Eindhoven University of Technology, Eindhoven, The\n  Netherlands"], ["Neele", "Thomas", "", "Eindhoven University of Technology, Eindhoven,\n  The Netherlands"], ["Wijs", "Anton", "", "Eindhoven University of Technology, Eindhoven,\n  The Netherlands"]]}, {"id": "1801.06216", "submitter": "St\\'ephane Bessy", "authors": "Joergen Bang-Jensen and St\\'ephane Bessy", "title": "Degree-constrained 2-partitions of graphs", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $(\\delta\\geq k_1,\\delta\\geq k_2)$-partition of a graph $G$ is a\nvertex-partition $(V_1,V_2)$ of $G$ satisfying that $\\delta(G[V_i])\\geq k_i$\nfor $i=1,2$. We determine, for all positive integers $k_1,k_2$, the complexity\nof deciding whether a given graph has a $(\\delta\\geq k_1,\\delta\\geq\nk_2)$-partition.\n  We also address the problem of finding a function $g(k_1,k_2)$ such that the\n$(\\delta\\geq k_1,\\delta\\geq k_2)$-partition problem is ${\\cal\n  NP}$-complete for the class of graphs of minimum degree less than\n$g(k_1,k_2)$ and polynomial for all graphs with minimum degree at least\n$g(k_1,k_2)$. We prove that $g(1,k)=k$ for $k\\ge 3$, that $g(2,2)=3$ and that\n$g(2,3)$, if it exists, has value 4 or 5.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 19:57:39 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Bang-Jensen", "Joergen", ""], ["Bessy", "St\u00e9phane", ""]]}, {"id": "1801.06232", "submitter": "Helmut Katzgraber", "authors": "Chao Fang, Zheng Zhu, Helmut G. Katzgraber", "title": "NAE-SAT-based probabilistic membership filters", "comments": "13 pages, 4 figures, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic membership filters are a type of data structure designed to\nquickly verify whether an element of a large data set belongs to a subset of\nthe data. While false negatives are not possible, false positives are.\nTherefore, the main goal of any good probabilistic membership filter is to have\na small false-positive rate while being memory efficient and fast to query.\nAlthough Bloom filters are fast to construct, their memory efficiency is\nbounded by a strict theoretical upper bound. Weaver et al. introduced random\nsatisfiability-based filters that significantly improved the efficiency of the\nprobabilistic filters, however, at the cost of solving a complex random\nsatisfiability (SAT) formula when constructing the filter. Here we present an\nimproved SAT filter approach with a focus on reducing the filter building\ntimes, as well as query times. Our approach is based on using not-all-equal\n(NAE) SAT formulas to build the filters, solving these via a mapping to random\nSAT using traditionally-fast random SAT solvers, as well as bit packing and the\nreduction of the number of hash functions. Paired with fast hardware, NAE-SAT\nfilters could result in enterprise-size applications.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 20:47:59 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Fang", "Chao", ""], ["Zhu", "Zheng", ""], ["Katzgraber", "Helmut G.", ""]]}, {"id": "1801.06237", "submitter": "Goran \\v{Z}u\\v{z}i\\'c", "authors": "Bernhard Haeupler, Jason Li, Goran Zuzic", "title": "Minor Excluded Network Families Admit Fast Distributed Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed network optimization algorithms, such as minimum spanning tree,\nminimum cut, and shortest path, are an active research area in distributed\ncomputing. This paper presents a fast distributed algorithm for such problems\nin the CONGEST model, on networks that exclude a fixed minor.\n  On general graphs, many optimization problems, including the ones mentioned\nabove, require $\\tilde\\Omega(\\sqrt n)$ rounds of communication in the CONGEST\nmodel, even if the network graph has a much smaller diameter. Naturally, the\nnext step in algorithm design is to design efficient algorithms which bypass\nthis lower bound on a restricted class of graphs. Currently, the only known\nmethod of doing so uses the low-congestion shortcut framework of Ghaffari and\nHaeupler [SODA'16]. Building off of their work, this paper proves that excluded\nminor graphs admit high-quality shortcuts, leading to an $\\tilde O(D^2)$ round\nalgorithm for the aforementioned problems, where $D$ is the diameter of the\nnetwork graph. To work with excluded minor graph families, we utilize the Graph\nStructure Theorem of Robertson and Seymour. To the best of our knowledge, this\nis the first time the Graph Structure Theorem has been used for an algorithmic\nresult in the distributed setting.\n  Even though the proof is involved, merely showing the existence of good\nshortcuts is sufficient to obtain simple, efficient distributed algorithms. In\nparticular, the shortcut framework can efficiently construct near-optimal\nshortcuts and then use them to solve the optimization problems. This, combined\nwith the very general family of excluded minor graphs, which includes most\nother important graph classes, makes this result of significant interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 20:58:04 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Li", "Jason", ""], ["Zuzic", "Goran", ""]]}, {"id": "1801.06460", "submitter": "Marten Maack", "authors": "Klaus Jansen, Kim-Manuel Klein, Marten Maack, Malin Rau", "title": "Empowering the Configuration-IP $-$ New PTAS Results for Scheduling with\n  Setups Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer linear programs of configurations, or configuration IPs, are a\nclassical tool in the design of algorithms for scheduling and packing problems,\nwhere a set of items has to be placed in multiple target locations. Herein a\nconfiguration describes a possible placement on one of the target locations,\nand the IP is used to chose suitable configurations covering the items. We give\nan augmented IP formulation, which we call the module configuration IP. It can\nbe described within the framework of n-fold integer programming and therefore\nbe solved efficiently. As an application, we consider scheduling problems with\nsetup times, in which a set of jobs has to be scheduled on a set of identical\nmachines, with the objective of minimizing the makespan. For instance, we\ninvestigate the case that jobs can be split and scheduled on multiple machines.\nHowever, before a part of a job can be processed an uninterrupted setup\ndepending on the job has to be paid. For both of the variants that jobs can be\nexecuted in parallel or not, we obtain an efficient polynomial time\napproximation scheme (EPTAS) of running time $f(1/\\varepsilon)\\times\n\\mathrm{poly}(|I|)$ with a single exponential term in $f$ for the first and a\ndouble exponential one for the second case. Previously, only constant factor\napproximations of $5/3$ and $4/3 + \\varepsilon$ respectively were known.\nFurthermore, we present an EPTAS for a problem where classes of\n(non-splittable) jobs are given, and a setup has to be paid for each class of\njobs being executed on one machine.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 15:27:46 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:45:43 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""], ["Maack", "Marten", ""], ["Rau", "Malin", ""]]}, {"id": "1801.06733", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Probabilistic Tools for the Analysis of Randomized Optimization\n  Heuristics", "comments": "91 pages", "journal-ref": "In Benjamin Doerr and Frank Neumann, editors, Theory of\n  Evolutionary Computation: Recent Developments in Discrete Optimization, pages\n  1-87. Springer, 2020", "doi": "10.1007/978-3-030-29414-4_1", "report-no": null, "categories": "cs.DS cs.DM cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter collects several probabilistic tools that proved to be useful in\nthe analysis of randomized search heuristics. This includes classic material\nlike Markov, Chebyshev and Chernoff inequalities, but also lesser known topics\nlike stochastic domination and coupling or Chernoff bounds for geometrically\ndistributed random variables and for negatively correlated random variables.\nMost of the results presented here have appeared previously, some, however,\nonly in recent conference publications. While the focus is on collecting tools\nfor the analysis of randomized search heuristics, many of these may be useful\nas well in the analysis of classic randomized algorithms or discrete random\nstructures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 21:47:13 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 21:29:21 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 21:42:36 GMT"}, {"version": "v4", "created": "Wed, 18 Mar 2020 17:20:34 GMT"}, {"version": "v5", "created": "Sun, 18 Apr 2021 20:09:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1801.06846", "submitter": "Lu\\'is M. S. Russo", "authors": "Lu\\'is M. S. Russo and Andreia Sofia Teixeira and Alexandre P\n  Francisco", "title": "Linking and Cutting Spanning Trees", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Prototype implementation at\n  https://github.com/LuisRusso-INESC-ID/LinkCutSpanningTrees", "journal-ref": "Algorithms 2018, 11(4), 53", "doi": "10.3390/a11040053", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of uniformly generating a spanning tree, of a\nconnected undirected graph. This process is useful to compute statistics,\nnamely for phylogenetic trees. We describe a Markov chain for producing these\ntrees. For cycle graphs we prove that this approach significantly outperforms\nexisting algorithms. For general graphs we obtain no analytical bounds, but\nexperimental results show that the chain still converges quickly. This yields\nan efficient algorithm, also due to the use of proper fast data structures. To\nbound the mixing time of the chain we describe a coupling, which we analyse for\ncycle graphs and simulate for other graphs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 16:30:50 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 18:38:37 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 15:20:29 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Russo", "Lu\u00eds M. S.", ""], ["Teixeira", "Andreia Sofia", ""], ["Francisco", "Alexandre P", ""]]}, {"id": "1801.06900", "submitter": "Liang Ding", "authors": "Liang Ding, Di Chang, Russell Malmberg, Aaron Martinez, David\n  Robinson, Matthew Wicker, Hongfei Yan, and Liming Cai", "title": "Efficient Learning of Optimal Markov Network Topology with k-Tree\n  Modeling", "comments": "18 pages main text, 2 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal work of Chow and Liu (1968) shows that approximation of a finite\nprobabilistic system by Markov trees can achieve the minimum information loss\nwith the topology of a maximum spanning tree. Our current paper generalizes the\nresult to Markov networks of tree width $\\leq k$, for every fixed $k\\geq 2$. In\nparticular, we prove that approximation of a finite probabilistic system with\nsuch Markov networks has the minimum information loss when the network topology\nis achieved with a maximum spanning $k$-tree. While constructing a maximum\nspanning $k$-tree is intractable for even $k=2$, we show that polynomial\nalgorithms can be ensured by a sufficient condition accommodated by many\nmeaningful applications. In particular, we prove an efficient algorithm for\nlearning the optimal topology of higher order correlations among random\nvariables that belong to an underlying linear structure.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 22:16:48 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Ding", "Liang", ""], ["Chang", "Di", ""], ["Malmberg", "Russell", ""], ["Martinez", "Aaron", ""], ["Robinson", "David", ""], ["Wicker", "Matthew", ""], ["Yan", "Hongfei", ""], ["Cai", "Liming", ""]]}, {"id": "1801.07013", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION)", "title": "Sequence Covering Similarity for Symbolic Sequence Comparison", "comments": "arXiv admin note: text overlap with arXiv:1712.02084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the sequence covering similarity, that we formally\ndefine for evaluating the similarity between a symbolic sequence (string) and a\nset of symbolic sequences (strings). From this covering similarity we derive a\npair-wise distance to compare two symbolic sequences. We show that this\ncovering distance is a semimetric. Few examples are given to show how this\nstring metric in $O(n \\cdot log n)$ compares with the Levenshtein's distance\nthat is in $O(n^2)$. A final example presents its application to plagiarism\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 09:53:25 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 07:24:32 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 15:53:27 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION"]]}, {"id": "1801.07029", "submitter": "Ma\\\"el Guiraud", "authors": "Dominique Barth, Ma\\\"el Guiraud, Yann Strozecki", "title": "Deterministic Scheduling of Periodic Messages for Low Latency in Cloud\n  RAN", "comments": "40 pages, 23 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend in mobile networks is to centralize in distant data-centers\nthe processing units which were attached to antennas until now. The main\nchallenge is to guarantee that the latency of the periodic messages sent from\nthe antennas to their processing units and back, fulfills protocol time\nconstraints. We show that traditional statistical multiplexing does not allow\nsuch a low latency, due to collisions and buffering at nodes. Hence, we propose\nin this article to use a deterministic scheme for sending periodic messages\nwithout collisions in the network thus saving the latency incurred by\nbuffering.\n  We give several algorithms to compute such schemes for a common topology\nwhere one link is shared by all antennas. We show that there is always a\nsolution when the routes are short or the load is small. When the parameters\nare unconstrained, and some buffering is allowed in the processing units, we\npropose an algorithm (PMLS) adapted from a classical scheduling method. The\nexperimental results show that even under full load, most of the time PMLS\nfinds a deterministic sending scheme with no latency.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 10:24:48 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 10:00:59 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 11:50:51 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 15:12:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Barth", "Dominique", ""], ["Guiraud", "Ma\u00ebl", ""], ["Strozecki", "Yann", ""]]}, {"id": "1801.07150", "submitter": "Muhammad Abulaish", "authors": "Muhammad Abulaish, Jahiruddin", "title": "A Novel Weighted Distance Measure for Multi-Attributed Graph", "comments": null, "journal-ref": "Muhammad Abulaish and Jahiruddin, A Novel Weighted Distance\n  Measure for Multi-Attributed Graph, In Proceedings of the 10th Annual ACM\n  India Conference (COMPUTE), Bhopal, India, Nov. 16-18, 2017, pp. 1-9", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to exponential growth of complex data, graph structure has become\nincreasingly important to model various entities and their interactions, with\nmany interesting applications including, bioinformatics, social network\nanalysis, etc. Depending on the complexity of the data, the underlying graph\nmodel can be a simple directed/undirected and/or weighted/un-weighted graph to\na complex graph (aka multi-attributed graph) where vertices and edges are\nlabelled with multi-dimensional vectors. In this paper, we present a novel\nweighted distance measure based on weighted Euclidean norm which is defined as\na function of both vertex and edge attributes, and it can be used for various\ngraph analysis tasks including classification and cluster analysis. The\nproposed distance measure has flexibility to increase/decrease the weightage of\nedge labels while calculating the distance between vertex-pairs. We have also\nproposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV\nfiles containing the list of vertex vectors and edge vectors, and calculates\nthe distance between each vertex-pair using the proposed weighted distance\nmeasure. Finally, we have proposed a multi-attributed similarity graph\ngeneration algorithm, MAGSim, which reads the output of MAGDist algorithm and\ngenerates a similarity graph that can be analysed using classification and\nclustering algorithms. The significance and accuracy of the proposed distance\nmeasure and algorithms is evaluated on Iris and Twitter data sets, and it is\nfound that the similarity graph generated by our proposed method yields better\nclustering results than the existing similarity graph generation methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:46:40 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Abulaish", "Muhammad", ""], ["Jahiruddin", "", ""]]}, {"id": "1801.07301", "submitter": "Hayim Shaul", "authors": "Hayim Shaul, Dan Feldman, Daniela Rus", "title": "Secure $k$-ish Nearest Neighbors Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, classifiers are used to predict a class of a given query\nbased on an existing (classified) database. Given a database S of n\nd-dimensional points and a d-dimensional query q, the k-nearest neighbors (kNN)\nclassifier assigns q with the majority class of its k nearest neighbors in S.\n  In the secure version of kNN, S and q are owned by two different parties that\ndo not want to share their data. Unfortunately, all known solutions for secure\nkNN either require a large communication complexity between the parties, or are\nvery inefficient to run.\n  In this work we present a classifier based on kNN, that can be implemented\nefficiently with homomorphic encryption (HE). The efficiency of our classifier\ncomes from a relaxation we make on kNN, where we allow it to consider kappa\nnearest neighbors for kappa ~ k with some probability. We therefore call our\nclassifier k-ish Nearest Neighbors (k-ish NN).\n  The success probability of our solution depends on the distribution of the\ndistances from q to S and increase as its statistical distance to Gaussian\ndecrease.\n  To implement our classifier we introduce the concept of double-blinded\ncoin-toss. In a doubly-blinded coin-toss the success probability as well as the\noutput of the toss are encrypted. We use this coin-toss to efficiently\napproximate the average and variance of the distances from q to S. We believe\nthese two techniques may be of independent interest.\n  When implemented with HE, the k-ish NN has a circuit depth that is\nindependent of n, therefore making it scalable. We also implemented our\nclassifier in an open source library based on HELib and tested it on a breast\ntumor database. The accuracy of our classifier (F_1 score) were 98\\% and\nclassification took less than 3 hours compared to (estimated) weeks in current\nHE implementations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:11:09 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 08:30:01 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Shaul", "Hayim", ""], ["Feldman", "Dan", ""], ["Rus", "Daniela", ""]]}, {"id": "1801.07342", "submitter": "Mark Jerrum", "authors": "Heng Guo, Mark Jerrum", "title": "Perfect simulation of the Hard Disks Model by Partial Rejection Sampling", "comments": "Minor revisions. This version is accepted for publication in Annales\n  de l'Institut Henri Poincar\\'e D", "journal-ref": "Ann. Inst. Henri Poincar\\'e D 8 (2021), no. 2, 159-177", "doi": "10.4171/AIHPD/99", "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a perfect simulation of the hard disks model via the partial\nrejection sampling method. Provided the density of disks is not too high, the\nmethod produces exact samples in $O(\\log n)$ rounds, and total time $O(n)$,\nwhere $n$ is the expected number of disks. The method extends easily to the\nhard spheres model in $d>2$ dimensions. In order to apply the partial rejection\nmethod to this continuous setting, we provide an alternative perspective of its\ncorrectness and run-time analysis that is valid for general state spaces.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 22:17:41 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 21:05:11 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 13:49:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Guo", "Heng", ""], ["Jerrum", "Mark", ""]]}, {"id": "1801.07355", "submitter": "Eric Balkanski", "authors": "Eric Balkanski, Nicole Immorlica, Yaron Singer", "title": "The Importance of Communities for Learning to Influence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the canonical problem of influence maximization in social\nnetworks. Since the seminal work of Kempe, Kleinberg, and Tardos, there have\nbeen two largely disjoint efforts on this problem. The first studies the\nproblem associated with learning the parameters of the generative influence\nmodel. The second focuses on the algorithmic challenge of identifying a set of\ninfluencers, assuming the parameters of the generative model are known. Recent\nresults on learning and optimization imply that in general, if the generative\nmodel is not known but rather learned from training data, no algorithm can\nyield a constant factor approximation guarantee using polynomially-many\nsamples, drawn from any distribution.\n  In this paper, we design a simple heuristic that overcomes this negative\nresult in practice by leveraging the strong community structure of social\nnetworks. Although in general the approximation guarantee of our algorithm is\nnecessarily unbounded, we show that this algorithm performs well\nexperimentally. To justify its performance, we prove our algorithm obtains a\nconstant factor approximation guarantee on graphs generated through the\nstochastic block model, traditionally used to model networks with community\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 00:14:09 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Balkanski", "Eric", ""], ["Immorlica", "Nicole", ""], ["Singer", "Yaron", ""]]}, {"id": "1801.07386", "submitter": "Cameron Musco", "authors": "Jeremy G. Hoskins and Cameron Musco and Christopher Musco and\n  Charalampos E. Tsourakakis", "title": "Learning Networks from Random Walk-Based Node Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital presence in the world of online social media entails significant\nprivacy risks. In this work we consider a privacy threat to a social network in\nwhich an attacker has access to a subset of random walk-based node\nsimilarities, such as effective resistances (i.e., commute times) or\npersonalized PageRank scores. Using these similarities, the attacker's goal is\nto infer as much information as possible about the underlying network,\nincluding any remaining unknown pairwise node similarities and edges.\n  For the effective resistance metric, we show that with just a small subset of\nmeasurements, the attacker can learn a large fraction of edges in a social\nnetwork, even when the measurements are noisy. We also show that it is possible\nto learn a graph which accurately matches the underlying network on all other\neffective resistances. This second observation is interesting from a data\nmining perspective, since it can be expensive to accurately compute all\neffective resistances. As an alternative, our graphs learned from just a subset\nof approximate effective resistances can be used as surrogates in a wide range\nof applications that use effective resistances to probe graph structure,\nincluding for graph clustering, node centrality evaluation, and anomaly\ndetection.\n  We obtain our results by formalizing the graph learning objective\nmathematically, using two optimization problems. One formulation is convex and\ncan be solved provably in polynomial time. The other is not, but we solve it\nefficiently with projected gradient and coordinate descent. We demonstrate the\neffectiveness of these methods on a number of social networks obtained from\nFacebook. We also discuss how our methods can be generalized to other random\nwalk-based similarities, such as personalized PageRank. Our code is available\nat https://github.com/cnmusco/graph-similarity-learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 03:22:57 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Hoskins", "Jeremy G.", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1801.07399", "submitter": "Pan Xu", "authors": "Pan Xu, Cuong Nguyen and Srikanta Tirthapura", "title": "Onion Curve: A Space Filling Curve with Near-Optimal Clustering", "comments": "The short version is published in ICDE 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space filling curves (SFCs) are widely used in the design of indexes for\nspatial and temporal data. Clustering is a key metric for an SFC, that measures\nhow well the curve preserves locality in moving from higher dimensions to a\nsingle dimension. We present the {\\em onion curve}, an SFC whose clustering\nperformance is provably close to optimal for the cube and near-cube shaped\nquery sets, irrespective of the side length of the query. We show that in\ncontrast, the clustering performance of the widely used Hilbert curve can be\nfar from optimal, even for cube-shaped queries. Since the clustering\nperformance of an SFC is critical to the efficiency of multi-dimensional\nindexes based on the SFC, the onion curve can deliver improved performance for\ndata structures involving multi-dimensional data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 05:35:00 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 17:44:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Xu", "Pan", ""], ["Nguyen", "Cuong", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1801.07449", "submitter": "Matev\\v{z} Jekovec", "authors": "Andrej Brodnik, Matev\\v{z} Jekovec", "title": "Sliding Suffix Tree using LCA", "comments": "A simplified version with improved maintenance and without using the\n  LCA is available at https://doi.org/10.3390/a11080118", "journal-ref": null, "doi": null, "report-no": "LUSY-2018/01", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sliding window $W$ over a stream of characters from some\nalphabet of constant size. The user wants to perform deterministic substring\nmatching on the current sliding window content and obtain positions of the\nmatches. We present an indexed version of the sliding window using the suffix\ntree, the link tree and the lowest common ancestor. The data structure of size\n$\\Theta(|W|)$ has optimal time queries $\\Theta(m+occ)$ and amortized constant\ntime updates, where $m$ is the length of the query string and $occ$ is the\nnumber of its occurrences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 09:25:19 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 21:04:45 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 11:56:57 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Brodnik", "Andrej", ""], ["Jekovec", "Matev\u017e", ""]]}, {"id": "1801.07456", "submitter": "Sebastian B\\\"ocker", "authors": "Kai D\\\"uhrkop, Marie Anne Lataretu, W. Timothy J. White, Sebastian\n  B\\\"ocker", "title": "Heuristic algorithms for the Maximum Colorful Subtree problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In metabolomics, small molecules are structurally elucidated using tandem\nmass spectrometry (MS/MS); this resulted in the computational Maximum Colorful\nSubtree problem, which is NP-hard. Unfortunately, data from a single metabolite\nrequires us to solve hundreds or thousands of instances of this problem; and in\na single Liquid Chromatography MS/MS run, hundreds or thousands of metabolites\nare measured.\n  Here, we comprehensively evaluate the performance of several heuristic\nalgorithms for the problem against an exact algorithm. We put particular\nemphasis on whether a heuristic is able to rank candidates such that the\ncorrect solution is ranked highly. We propose this \"intermediate\" evaluation\nbecause evaluating the approximating quality of heuristics is misleading: Even\na slightly suboptimal solution can be structurally very different from the true\nsolution. On the other hand, we cannot structurally evaluate against the ground\ntruth, as this is unknown. We find that one particular heuristic consistently\nranks the correct solution in a top position, allowing us to speed up\ncomputations about 100-fold. We also find that scores of the best heuristic\nsolutions are very close to the optimal score; in contrast, the structure of\nthe solutions can deviate significantly from the optimal structures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 09:52:58 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 15:35:00 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 08:41:11 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["D\u00fchrkop", "Kai", ""], ["Lataretu", "Marie Anne", ""], ["White", "W. Timothy J.", ""], ["B\u00f6cker", "Sebastian", ""]]}, {"id": "1801.07541", "submitter": "Arindam Khan", "authors": "Waldo G\\'alvez and Fabrizio Grandoni and Salvatore Ingala and Arindam\n  Khan", "title": "Improved Pseudo-Polynomial-Time Approximation for Strip Packing", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the strip packing problem, a classical packing problem which\ngeneralizes both bin packing and makespan minimization. Here we are given a set\nof axis-parallel rectangles in the two-dimensional plane and the goal is to\npack them in a vertical strip of a fixed width such that the height of the\nobtained packing is minimized. The packing must be non-overlapping and the\nrectangles cannot be rotated. A reduction from the partition problem shows that\nno approximation better than 3/2 is possible for strip packing in polynomial\ntime (assuming P$\\neq$NP). Nadiradze and Wiese [SODA16] overcame this barrier\nby presenting a $(\\frac{7}{5}+\\epsilon)$-approximation algorithm in\npseudo-polynomial-time (PPT). As the problem is strongly NP-hard, it does not\nadmit an exact PPT algorithm. In this paper, we make further progress on the\nPPT approximability of strip packing, by presenting a\n$(\\frac43+\\epsilon)$-approximation algorithm. Our result is based on a\nnon-trivial repacking of some rectangles in the \\emph{empty space} left by the\nconstruction by Nadiradze and Wiese, and in some sense pushes their approach to\nits limit. Our PPT algorithm can be adapted to the case where we are allowed to\nrotate the rectangles by $90^\\circ$, achieving the same approximation factor\nand breaking the polynomial-time approximation barrier of 3/2 for the case with\nrotations as well.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 14:03:48 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["G\u00e1lvez", "Waldo", ""], ["Grandoni", "Fabrizio", ""], ["Ingala", "Salvatore", ""], ["Khan", "Arindam", ""]]}, {"id": "1801.07544", "submitter": "Paul Weng", "authors": "Viet Hung Nguyen and Paul Weng", "title": "An Efficient Primal-Dual Algorithm for Fair Combinatorial Optimization\n  Problems", "comments": "accepted at COCOA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general class of combinatorial optimization problems including\namong others allocation, multiple knapsack, matching or travelling salesman\nproblems. The standard version of those problems is the maximum weight\noptimization problem where a sum of values is optimized. However, the sum is\nnot a good aggregation function when the fairness of the distribution of those\nvalues (corresponding for example to different agents' utilities or criteria)\nis important. In this paper, using the generalized Gini index (GGI), a\nwell-known inequality measure, instead of the sum to model fairness, we\nformulate a new general problem, that we call fair combinatorial optimization.\nAlthough GGI is a non-linear aggregating function, a $0,1$-linear program (IP)\ncan be formulated for finding a GGI-optimal solution by exploiting a\nlinearization of GGI proposed by Ogryczak and Sliwinski. However, the time\nspent by commercial solvers (e.g., CPLEX, Gurobi...) for solving (IP) increases\nvery quickly with instances' size and can reach hours even for relatively\nsmall-sized ones. As a faster alternative, we propose a heuristic for solving\n(IP) based on a primal-dual approach using Lagrangian decomposition. %We\nexperimentally evaluate our methods against the exact solution of (IP) by CPLEX\non several fair optimization problems related to matching to demonstrate the\nefficiency of our proposition. We demonstrate the efficiency of our method by\nevaluating it against the exact solution of (IP) by CPLEX on several fair\noptimization problems related to matching. The numerical results show that our\nmethod outputs in a very short time efficient solutions giving lower bounds\nthat CPLEX may take several orders of magnitude longer to obtain. Moreover, for\ninstances for which we know the optimal value, these solutions are\nquasi-optimal with optimality gap less than 0.3%.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 14:11:19 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Nguyen", "Viet Hung", ""], ["Weng", "Paul", ""]]}, {"id": "1801.07553", "submitter": "Marieke van der Wegen", "authors": "Ragnar Groot Koerkamp and Marieke van der Wegen", "title": "Stable gonality is computable", "comments": "15 pages; v2 minor changes; v3 minor changes", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 21 no.\n  1, ICGT 2018 (June 13, 2019) dmtcs:5557", "doi": "10.23638/DMTCS-21-1-10", "report-no": null, "categories": "cs.DM cs.DS math.CO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable gonality is a multigraph parameter that measures the complexity of a\ngraph. It is defined using maps to trees. Those maps, in some sense, divide the\nedges equally over the edges of the tree; stable gonality asks for the map with\nthe minimum number of edges mapped to each edge of the tree. This parameter is\nrelated to treewidth, but unlike treewidth, it distinguishes multigraphs from\ntheir underlying simple graphs. Stable gonality is relevant for problems in\nnumber theory. In this paper, we show that deciding whether the stable gonality\nof a given graph is at most a given integer $k$ belongs to the class NP, and we\ngive an algorithm that computes the stable gonality of a graph in\n$O((1.33n)^nm^m \\text{poly}(n,m))$ time.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 14:21:55 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 16:15:34 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 10:03:46 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 12:38:10 GMT"}], "update_date": "2021-01-04", "authors_parsed": [["Koerkamp", "Ragnar Groot", ""], ["van der Wegen", "Marieke", ""]]}, {"id": "1801.07656", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Anissa Lamani", "title": "Byzantine Gathering in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of Byzantine gathering in a network modeled as a graph.\nDespite the presence of Byzantine agents, all the other (good) agents, starting\nfrom possibly different nodes and applying the same deterministic algorithm,\nhave to meet at the same node in finite time and stop moving. An adversary\nchooses the initial nodes of the agents and assigns a different label to each\nof them. The agents move in synchronous rounds and communicate with each other\nonly when located at the same node. Within the team, f of the agents are\nByzantine. A Byzantine agent acts in an unpredictable way: in particular it may\nforge the label of another agent or create a completely new one. Besides its\nlabel, which corresponds to a local knowledge, an agent is assigned some global\nknowledge GK that is common to all agents. In literature, the Byzantine\ngathering problem has been analyzed in arbitrary n-node graphs by considering\nthe scenario when GK=(n,f) and the scenario when GK=f. In the first (resp.\nsecond) scenario, it has been shown that the minimum number of good agents\nguaranteeing deterministic gathering of all of them is f+1 (resp. f+2). For\nboth these scenarios, all the existing deterministic algorithms, whether or not\nthey are optimal in terms of required number of good agents, have a time\ncomplexity that is exponential in n and L, where L is the largest label\nbelonging to a good agent.\n  In this paper, we seek to design a deterministic solution for Byzantine\ngathering that makes a concession on the proportion of Byzantine agents within\nthe team, but that offers a significantly lower complexity. We also seek to use\na global knowledge whose the length of the binary representation is small.\nAssuming that the agents are in a strong team i.e., a team in which the number\nof good agents is at least some prescribed value that is quadratic in f, we\ngive positive and negative results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:53:19 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Lamani", "Anissa", ""]]}, {"id": "1801.08098", "submitter": "Patrick Mackey", "authors": "Patrick Mackey, Katherine Porterfield, Erin Fitzhenry, Sutanay\n  Choudhury, George Chin Jr", "title": "A Chronological Edge-Driven Approach to Temporal Subgraph Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world networks are considered temporal networks, in which the\nchronological ordering of the edges has importance to the meaning of the data.\nPerforming temporal subgraph matching on such graphs requires the edges in the\nsubgraphs to match the order of the temporal graph motif we are searching for.\nPrevious methods for solving this rely on the use of static subgraph matching\nto find potential matches first, before filtering them based on edge order to\nfind the true temporal matches. We present a new algorithm for temporal\nsubgraph isomorphism that performs the subgraph matching directly on the\nchronologically sorted edges. By restricting our search to only the subgraphs\nwith chronologically correct edges, we can improve the performance of the\nalgorithm significantly. We present experimental timing results to show\nsignificant performance improvements on publicly available datasets for a\nnumber of different temporal query graph motifs with four or more nodes. We\nalso demonstrate a practical example of how temporal subgraph isomorphism can\nproduce more meaningful results than traditional static subgraph searches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:48:55 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Mackey", "Patrick", ""], ["Porterfield", "Katherine", ""], ["Fitzhenry", "Erin", ""], ["Choudhury", "Sutanay", ""], ["Chin", "George", "Jr"]]}, {"id": "1801.08293", "submitter": "Clara Stegehuis", "authors": "Ellen Cardinaels, Johan S.H. van Leeuwaarden, Clara Stegehuis", "title": "Finding induced subgraphs in scale-free inhomogeneous random graphs", "comments": "14 pages", "journal-ref": null, "doi": "10.1007/978-3-319-92871-5_1", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a copy of a specific induced subgraph on\ninhomogeneous random graphs with infinite variance power-law degrees. We\nprovide a fast algorithm that finds a copy of any connected graph $H$ on a\nfixed number of $k$ vertices as an induced subgraph in a random graph with $n$\nvertices. By exploiting the scale-free graph structure, the algorithm runs in\n$O(n k)$ time for small values of $k$. As a corollary, this shows that the\ninduced subgraph isomorphism problem can be solved in time $O(nk)$ for the\ninhomogeneous random graph. We test our algorithm on several real-world data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 07:21:20 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 13:05:58 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 09:23:59 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Cardinaels", "Ellen", ""], ["van Leeuwaarden", "Johan S. H.", ""], ["Stegehuis", "Clara", ""]]}, {"id": "1801.08565", "submitter": "Ahmad Biniaz", "authors": "Therese Biedl, Ahmad Biniaz, Robert Cummings, Anna Lubiw, Florin\n  Manea, Dirk Nowotka, and Jeffrey Shallit", "title": "Rollercoasters and Caterpillars", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rollercoaster is a sequence of real numbers for which every maximal\ncontiguous subsequence, that is increasing or decreasing, has length at least\nthree. By translating this sequence to a set of points in the plane, a\nrollercoaster can be defined as a polygonal path for which every maximal\nsub-path, with positive- or negative-slope edges, has at least three points.\nGiven a sequence of distinct real numbers, the rollercoaster problem asks for a\nmaximum-length subsequence that is a rollercoaster. It was conjectured that\nevery sequence of $n$ distinct real numbers contains a rollercoaster of length\nat least $\\lceil n/2\\rceil$ for $n>7$, while the best known lower bound is\n$\\Omega(n/\\log n)$. In this paper we prove this conjecture. Our proof is\nconstructive and implies a linear-time algorithm for computing a rollercoaster\nof this length. Extending the $O(n\\log n)$-time algorithm for computing a\nlongest increasing subsequence, we show how to compute a maximum-length\nrollercoaster within the same time bound. A maximum-length rollercoaster in a\npermutation of $\\{1,\\dots,n\\}$ can be computed in $O(n \\log \\log n)$ time.\n  The search for rollercoasters was motivated by orthogeodesic point-set\nembedding of caterpillars. A caterpillar is a tree such that deleting the\nleaves gives a path, called the spine. A top-view caterpillar is one of degree\n4 such that the two leaves adjacent to each vertex lie on opposite sides of the\nspine. As an application of our result on rollercoasters, we are able to find a\nplanar drawing of every $n$-node top-view caterpillar on every set of\n$\\frac{25}{3}n$ points in the plane, such that each edge is an orthogonal path\nwith one bend. This improves the previous best known upper bound on the number\nof required points, which is $O(n \\log n)$. We also show that such a drawing\ncan be obtained in linear time, provided that the points are given in sorted\norder.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:14:37 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Biedl", "Therese", ""], ["Biniaz", "Ahmad", ""], ["Cummings", "Robert", ""], ["Lubiw", "Anna", ""], ["Manea", "Florin", ""], ["Nowotka", "Dirk", ""], ["Shallit", "Jeffrey", ""]]}, {"id": "1801.08589", "submitter": "Renato J Cintra", "authors": "J. Adikari, V. S. Dimitrov, R. J. Cintra", "title": "A New Algorithm for Double Scalar Multiplication over Koblitz Curves", "comments": "5 pages, 2 figures, 1 table", "journal-ref": "Circuits and Systems (ISCAS), 2011 IEEE International Symposium on", "doi": "10.1109/ISCAS.2011.5937664", "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Koblitz curves are a special set of elliptic curves and have improved\nperformance in computing scalar multiplication in elliptic curve cryptography\ndue to the Frobenius endomorphism. Double-base number system approach for\nFrobenius expansion has improved the performance in single scalar\nmultiplication. In this paper, we present a new algorithm to generate a sparse\nand joint $\\tau$-adic representation for a pair of scalars and its application\nin double scalar multiplication. The new algorithm is inspired from double-base\nnumber system. We achieve 12% improvement in speed against state-of-the-art\n$\\tau$-adic joint sparse form.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 20:52:25 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Adikari", "J.", ""], ["Dimitrov", "V. S.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1801.08652", "submitter": "Georg Hahn", "authors": "Georg Hahn, Hristo N. Djidjev", "title": "Reducing Binary Quadratic Forms for More Scalable Quantum Annealing", "comments": null, "journal-ref": "IEEE Intl Conference on Rebooting Computing 2017", "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the development of commercial quantum annealers such as\nthe D-Wave 2X allow solving NP-hard optimization problems that can be expressed\nas quadratic unconstrained binary programs. However, the relatively small\nnumber of available qubits (around 1000 for the D-Wave 2X quantum annealer)\nposes a severe limitation to the range of problems that can be solved. This\npaper explores the suitability of preprocessing methods for reducing the sizes\nof the input programs and thereby the number of qubits required for their\nsolution on quantum computers. Such methods allow us to determine the value of\ncertain variables that hold in either any optimal solution (called strong\npersistencies) or in at least one optimal solution (weak persistencies). We\ninvestigate preprocessing methods for two important NP-hard graph problems, the\ncomputation of a maximum clique and a maximum cut in a graph. We show that the\nidentification of strong and weak persistencies for those two optimization\nproblems is very instance-specific, but can lead to substantial reductions in\nthe number of variables.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 02:08:09 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 17:03:45 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Hahn", "Georg", ""], ["Djidjev", "Hristo N.", ""]]}, {"id": "1801.08709", "submitter": "Aleksandrs Belovs", "authors": "Aleksandrs Belovs", "title": "Adaptive Lower Bound for Testing Monotonicity on the Line", "comments": "10 pages, added the proof of a matching upper bound for the lower\n  bound in case of small epsilon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the property testing model, the task is to distinguish objects possessing\nsome property from the objects that are far from it. One of such properties is\nmonotonicity, when the objects are functions from one poset to another. This is\nan active area of research. In this paper we study query complexity of\n$\\epsilon$-testing monotonicity of a function $f\\colon [n]\\to[r]$. All our\nlower bounds are for adaptive two-sided testers.\n  * We prove a nearly tight lower bound for this problem in terms of $r$. The\nbound is $\\Omega(\\frac{\\log r}{\\log \\log r})$ when $\\epsilon = 1/2$. No\nprevious satisfactory lower bound in terms of $r$ was known.\n  * We completely characterise query complexity of this problem in terms of $n$\nfor smaller values of $\\epsilon$. The complexity is $\\Theta(\\epsilon^{-1} \\log\n(\\epsilon n))$. Apart from giving the lower bound, this improves on the best\nknown upper bound.\n  Finally, we give an alternative proof of the $\\Omega(\\epsilon^{-1}d\\log n -\n\\epsilon^{-1}\\log\\epsilon^{-1})$ lower bound for testing monotonicity on the\nhypergrid $[n]^d$ due to Chakrabarty and Seshadhri (RANDOM'13).\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 08:24:16 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 21:26:56 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Belovs", "Aleksandrs", ""]]}, {"id": "1801.08850", "submitter": "Igor Malinovi\\'c", "authors": "Yuri Faenza, Igor Malinovi\\'c, Monaldo Mastrolilli, Ola Svensson", "title": "On bounded pitch inequalities for the min-knapsack polytope", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the min-knapsack problem one aims at choosing a set of objects with\nminimum total cost and total profit above a given threshold. In this paper, we\nstudy a class of valid inequalities for min-knapsack known as bounded pitch\ninequalities, which generalize the well-known unweighted cover inequalities.\nWhile separating over pitch-1 inequalities is NP-hard, we show that approximate\nseparation over the set of pitch-1 and pitch-2 inequalities can be done in\npolynomial time. We also investigate integrality gaps of linear relaxations for\nmin-knapsack when these inequalities are added. Among other results, we show\nthat, for any fixed $t$, the $t$-th CG closure of the natural linear relaxation\nhas the unbounded integrality gap.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 15:38:21 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Faenza", "Yuri", ""], ["Malinovi\u0107", "Igor", ""], ["Mastrolilli", "Monaldo", ""], ["Svensson", "Ola", ""]]}, {"id": "1801.09089", "submitter": "Guangwei Wu", "authors": "Guangwei Wu, Jianer Chen, Jianxin Wang", "title": "On Scheduling Two-Stage Jobs on Multiple Two-Stage Flowshops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the current research in data centers and cloud computing, we\nstudy the problem of scheduling a set of two-stage jobs on multiple two-stage\nflowshops. A new formulation for configurations of such scheduling is proposed,\nwhich leads directly to improvements to the complexity of scheduling algorithms\nfor the problem. Motivated by the observation that the costs of the two stages\ncan be significantly different, we present deeper study on the structures of\nthe problem that leads to a new approach to designing scheduling algorithms for\nthe problem. With more thorough analysis, we show that the new approach gives\nvery significant improved scheduling algorithms for the problem when the costs\nof the two stages are different significantly. Improved approximation\nalgorithms for the problem are also presented.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 13:35:08 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Wu", "Guangwei", ""], ["Chen", "Jianer", ""], ["Wang", "Jianxin", ""]]}, {"id": "1801.09159", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Przemys{\\l}aw Uzna\\'nski", "title": "Faster Approximate(d) Text-to-Pattern L1 Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding \\emph{distance} between \\emph{pattern} of length $m$\nand \\emph{text} of length $n$ is a typical way of generalizing pattern matching\nto incorporate dissimilarity score. For both Hamming and $L_1$ distances only a\nsuper linear upper bound $\\widetilde{O}(n\\sqrt{m})$ are known, which prompts\nthe question of relaxing the problem: either by asking for $(1 \\pm\n\\varepsilon)$ approximate distance (every distance is reported up to a\nmultiplicative factor), or $k$-approximated distance (distances exceeding $k$\nare reported as $\\infty$). We focus on $L_1$ distance, for which we show new\nalgorithms achieving complexities respectively $\\widetilde{O}(\\varepsilon^{-1}\nn)$ and $\\widetilde{O}((m+k\\sqrt{m}) \\cdot n/m)$. This is a significant\nimprovement upon previous algorithms with runtime\n$\\widetilde{O}(\\varepsilon^{-2} n)$ of Lipsky and Porat [Algorithmica 2011] and\n$\\widetilde{O}(n\\sqrt{k})$ of Amir, Lipsky, Porat and Umanski [CPM 2005].\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 00:43:49 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 22:08:09 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1801.09431", "submitter": "Eliezer Albacea", "authors": "Eliezer A. Albacea", "title": "Generalized Leapfrogging Samplesort: A Class of $O(n \\log^2 n)$\n  Worst-Case Complexity and $O(n \\log n)$ Average-Case Complexity Sorting\n  Algorithms", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original Leapfrogging Samplesort operates on a sorted sample of size $s$\nand an unsorted part of size $s+1$. We generalize this to a sorted sample of\nsize $s$ and an unsorted part of size $(2^k-1)(s+1)$, where $k = O(1)$. We\npresent a practical implementation of this class of algorithms and we show that\nthe worst-case complexity is $O(n \\log^2 n)$ and the average-case complexity is\n$O(n \\log n)$.\n  Keywords: Samplesort, Quicksort, Leapfrogging Samplesort, sorting, analysis\nof algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 10:22:20 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Albacea", "Eliezer A.", ""]]}, {"id": "1801.09488", "submitter": "Magnus Wahlstr\\\"om", "authors": "Victor Lagerkvist and Magnus Wahlstr\\\"om", "title": "Which NP-Hard SAT and CSP Problems Admit Exponentially Improved\n  Algorithms?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of SAT($\\Gamma$) problems for potentially infinite\nlanguages $\\Gamma$ closed under variable negation (sign-symmetric languages).\nVia an algebraic connection, this reduces to the study of restricted partial\npolymorphisms of $\\Gamma$ we refer to as \\emph{pSDI-operations} (for partial,\nself-dual and idempotent). First, we study the language classes themselves. We\nclassify the structure of the least restrictive pSDI-operations, corresponding\nto the most powerful languages $\\Gamma$, and find that these operations can be\ndivided into \\emph{levels}, corresponding to a rough notion of difficulty; and\nthat within each level there is a strongest operation (the partial $k$-NU\noperation, preserving $(k-1)$-SAT) and a weakest operation (the $k$-universal\noperation $u_k$, preserving problems definable via bounded-degree polynomials).\nWe show that every sign-symmetric $\\Gamma$ not preserved by $u_k$ implements\nall $k$-clauses; thus if $\\Gamma$ is not preserved by $u_k$ for any $k$, then\nSAT($\\Gamma$) is trivially SETH-hard and cannot be solved faster than\n$O^*(2^n)$ unless SETH fails.\n  Second, we study upper and lower bounds for SAT($\\Gamma$) for such languages.\nWe show that several classes in the hierarchy correspond to problems which can\nbe solved faster than $2^n$ using previously known algorithmic strategies such\nas Subset Sum-style meet-in-the-middle and fast matrix multiplication.\nFurthermore, if the sunflower conjecture holds for sunflowers with k sets, then\nthe partial k-NU language has an improved algorithm via local search.\nComplementing this, we show that for every class there is a concrete lower\nbound $c$ such that SAT($\\Gamma$) cannot be solved faster than $O^*(c^n)$ for\nall problems in the class unless SETH fails. This gives the first known case of\na SAT-problem which simultaneously has non-trivial upper and lower bounds under\nSETH.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 13:06:44 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lagerkvist", "Victor", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1801.09584", "submitter": "Du\\v{s}an Knop", "authors": "Du\\v{s}an Knop and Martin Kouteck\\'y and Matthias Mnich", "title": "A Unifying Framework for Manipulation Problems", "comments": "15 pages, accepted to AAMAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation models for electoral systems are a core research theme in social\nchoice theory; they include bribery (unweighted, weighted, swap, shift, ...),\ncontrol (by adding or deleting voters or candidates), lobbying in referenda and\nothers.\n  We develop a unifying framework for manipulation models with few types of\npeople, one of the most commonly studied scenarios. A critical insight of our\nframework is to separate the descriptive complexity of the voting rule R from\nthe number of types of people. This allows us to finally settle the\ncomputational complexity of R-Swap Bribery, one of the most fundamental\nmanipulation problems. In particular, we prove that R-Swap Bribery is\nfixed-parameter tractable when R is Dodgson's rule and Young's rule, when\nparameterized by the number of candidates. This way, we resolve a long-standing\nopen question from 2007 which was explicitly asked by Faliszewski et al. [JAIR\n40, 2011].\n  Our algorithms reveal that the true hardness of bribery problems often stems\nfrom the complexity of the voting rules. On one hand, we give a fixed-parameter\nalgorithm parameterized by number of types of people for complex voting rules.\nThus, we reveal that R-Swap Bribery with Dodgson's rule is much harder than\nwith Condorcet's rule, which can be expressed by a conjunction of linear\ninequalities, while Dodson's rule requires quantifier alternation and a bounded\nnumber of disjunctions of linear systems. On the other hand, we give an\nalgorithm for quantifier-free voting rules which is parameterized only by the\nnumber of conjunctions of the voting rule and runs in time polynomial in the\nnumber of types of people. This way, our framework explains why Shift Bribery\nis polynomial-time solvable for the plurality voting rule, making explicit that\nthe rule is simple in that it can be expressed with a single linear inequality,\nand that the number of voter types is polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 15:45:05 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""], ["Mnich", "Matthias", ""]]}, {"id": "1801.09639", "submitter": "Hui Li", "authors": "Hui Li and Sizhe Peng and Jian Li and Jingjing Li and Jiangtao Cui and\n  Jianfeng Ma", "title": "ONCE and ONCE+: Counting the Frequency of Time-constrained Serial\n  Episodes in a Streaming Sequence", "comments": "14 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a representative sequential pattern mining problem, counting the frequency\nof serial episodes from a streaming sequence has drawn continuous attention in\nacademia due to its wide application in practice, e.g., telecommunication\nalarms, stock market, transaction logs, bioinformatics, etc. Although a number\nof serial episodes mining algorithms have been developed recently, most of them\nare neither stream-oriented, as they require multi-pass of dataset, nor\ntime-aware, as they fail to take into account the time constraint of serial\nepisodes. In this paper, we propose two novel one-pass algorithms, ONCE and\nONCE+, each of which can respectively compute two popular frequencies of given\nepisodes satisfying predefined time-constraint as signals in a stream arrives\none-after-another. ONCE is only used for non-overlapped frequency where the\noccurrences of a serial episode in sequence are not intersected. ONCE+ is\ndesigned for the distinct frequency where the occurrences of a serial episode\ndo not share any event. Theoretical study proves that our algorithm can\ncorrectly mine the frequency of target time constraint serial episodes in a\ngiven stream. Experimental study over both real-world and synthetic datasets\ndemonstrates that the proposed algorithm can work, with little time and space,\nin signal-intensive streams where millions of signals arrive within a single\nsecond. Moreover, the algorithm has been applied in a real stream processing\nsystem, where the efficacy and efficiency of this work is tested in practical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:26:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Li", "Hui", ""], ["Peng", "Sizhe", ""], ["Li", "Jian", ""], ["Li", "Jingjing", ""], ["Cui", "Jiangtao", ""], ["Ma", "Jianfeng", ""]]}, {"id": "1801.09720", "submitter": "Ammar Daskin", "authors": "Ammar Daskin, Sabre Kais", "title": "A Generalized Circuit for the Hamiltonian Dynamics Through the Truncated\n  Series", "comments": "MATLAB source code for the circuits can be downloaded from\n  https://github.com/adaskin/circuitforTaylorseries", "journal-ref": "Quantum Information Processing, 17:328, 2018", "doi": "10.1007/s11128-018-2099-z", "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for the Hamiltonian simulation in the\ncontext of eigenvalue estimation problems which improves earlier results\ndealing with Hamiltonian simulation through the truncated Taylor series. In\nparticular, we present a fixed-quantum circuit design for the simulation of the\nHamiltonian dynamics, $H(t)$, through the truncated Taylor series method\ndescribed by Berry et al. \\cite{berry2015simulating}. The circuit is general\nand can be used to simulate any given matrix in the phase estimation algorithm\nby only changing the angle values of the quantum gates implementing the time\nvariable $t$ in the series. The circuit complexity depends on the number of\nsummation terms composing the Hamiltonian and requires $O(Ln)$ number of\nquantum gates for the simulation of a molecular Hamiltonian. Here, $n$ is the\nnumber of states of a spin orbital, and $L$ is the number of terms in the\nmolecular Hamiltonian and generally bounded by $O(n^4)$. We also discuss how to\nuse the circuit in adaptive processes and eigenvalue related problems along\nwith a slight modified version of the iterative phase estimation algorithm. In\naddition, a simple divide and conquer method is presented for mapping a matrix\nwhich are not given as sums of unitary matrices into the circuit. The\ncomplexity of the circuit is directly related to the structure of the matrix\nand can be bounded by $O(poly(n))$ for a matrix with $poly(n)-$sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 19:28:10 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 17:57:46 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 07:54:14 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Daskin", "Ammar", ""], ["Kais", "Sabre", ""]]}, {"id": "1801.09798", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer and Eldar Fischer", "title": "Earthmover Resilience and Testing in Ordered Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in property testing is to characterize those\nproperties that are testable with a constant number of queries. For unordered\nstructures such as graphs and hypergraphs this task has been mostly settled.\nHowever, for ordered structures such as strings, images, and ordered graphs,\nthe characterization problem seems very difficult in general.\n  In this paper, we identify a wide class of properties of ordered structures -\nthe earthmover resilient (ER) properties - and show that the \"good behavior\" of\nsuch properties allows us to obtain general testability results that are\nsimilar to (and more general than) those of unordered graphs. A property P is\nER if, roughly speaking, slight changes in the order of the elements in an\nobject satisfying P cannot make this object far from P. The class of ER\nproperties includes, e.g., all unordered graph properties, many natural visual\nproperties of images, such as convexity, and all hereditary properties of\nordered graphs and images.\n  A special case of our results implies, building on a recent result of Alon\nand the authors, that the distance of a given image or ordered graph from any\nhereditary property can be estimated (with good probability) up to a constant\nadditive error, using a constant number of queries.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 23:36:46 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Fischer", "Eldar", ""]]}, {"id": "1801.09973", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis, Vana Kalogeraki, Dimitrios Gunopulos", "title": "Social Event Scheduling", "comments": "This paper appears in 34th IEEE International Conference on Data\n  Engineering (ICDE 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for social event organizers (e.g., event planning and\nmarketing companies, venues) is attracting the maximum number of participants,\nsince it has great impact on the success of the event, and, consequently, the\nexpected gains (e.g., revenue, artist/brand publicity). In this paper, we\nintroduce the Social Event Scheduling (SES) problem, which schedules a set of\nsocial events considering user preferences and behavior, events' spatiotemporal\nconflicts, and competing vents, in order to maximize the overall number of\nattendees. We show that SES is strongly NP-hard, even in highly restricted\ninstances. To cope with the hardness of the SES problem we design a greedy\napproximation algorithm. Finally, we evaluate our method experimentally using a\ndataset from the Meetup event-based social network.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 13:16:51 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 23:09:54 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Bikakis", "Nikos", ""], ["Kalogeraki", "Vana", ""], ["Gunopulos", "Dimitrios", ""]]}, {"id": "1801.10139", "submitter": "Pablo Rotondo", "authors": "Pablo Rotondo, Brigitte Vallee and Alfredo Viola", "title": "Analysis of the Continued Logarithm Algorithm", "comments": "Accepted in LATIN2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Continued Logarithm Algorithm - CL for short- introduced by Gosper in\n1978 computes the gcd of two integers; it seems very efficient, as it only\nperforms shifts and subtractions. Shallit has studied its worst-case complexity\nin 2016 and showed it to be linear. We here perform the average-case analysis\nof the algorithm: we study its main parameters (number of iterations, total\nnumber of shifts) and obtain precise asymptotics for their mean values. Our\n'dynamical' analysis involves the dynamical system underlying the algorithm,\nthat produces continued fraction expansions whose quotients are powers of 2.\nEven though this CL system has already been studied by Chan (around 2005), the\npresence of powers of 2 in the quotients ingrains into the central parameters a\ndyadic flavour that cannot be grasped solely by studying the CL system. We thus\nintroduce a dyadic component and deal with a two-component system. With this\nnew mixed system at hand, we then provide a complete average-case analysis of\nthe CL algorithm, with explicit constants.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 18:46:42 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 08:28:27 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Rotondo", "Pablo", ""], ["Vallee", "Brigitte", ""], ["Viola", "Alfredo", ""]]}, {"id": "1801.10264", "submitter": "Natalie Durgin", "authors": "Natalie Durgin, Rachel Grotheer, Chenxi Huang, Shuang Li, Anna Ma,\n  Deanna Needell, Jing Qin", "title": "Compressed Anomaly Detection with Multiple Mixed Observations", "comments": "27 pages, 9 figures. Incorporates reviewer feedback, additional\n  experiments, and additional figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS eess.SP math.IT math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a collection of independent random variables that are identically\ndistributed, except for a small subset which follows a different, anomalous\ndistribution. We study the problem of detecting which random variables in the\ncollection are governed by the anomalous distribution. Recent work proposes to\nsolve this problem by conducting hypothesis tests based on mixed observations\n(e.g. linear combinations) of the random variables. Recognizing the connection\nbetween taking mixed observations and compressed sensing, we view the problem\nas recovering the \"support\" (index set) of the anomalous random variables from\nmultiple measurement vectors (MMVs). Many algorithms have been developed for\nrecovering jointly sparse signals and their support from MMVs. We establish the\ntheoretical and empirical effectiveness of these algorithms at detecting\nanomalies. We also extend the LASSO algorithm to an MMV version for our\npurpose. Further, we perform experiments on synthetic data, consisting of\nsamples from the random variables, to explore the trade-off between the number\nof mixed observations per sample and the number of samples required to detect\nanomalies.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 01:18:33 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 22:26:23 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Durgin", "Natalie", ""], ["Grotheer", "Rachel", ""], ["Huang", "Chenxi", ""], ["Li", "Shuang", ""], ["Ma", "Anna", ""], ["Needell", "Deanna", ""], ["Qin", "Jing", ""]]}, {"id": "1801.10339", "submitter": "Tewabe Chekole", "authors": "Tewabe Chekole", "title": "A Continuous - Time Quantum Walk for Attributed Graphs Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse facets Of the Theory of Quantum Walks on Graph are reviewed Till now\n.In specific, Quantum network routing, Quantum Walk Search Algorithm, Element\ndistinctness associated to the eigenvalues of Graphs and the use of these\nrelation /connection in the study of Quantum walks is furthermore described.\nDifferent Researchers had contribution and put their benchmark idea Pertaining\nwith this research concept. I furthermore try to investigate recent Application\nof Quantum walks, In specific the problem pertained with Graph matching i.e\nMatching nodes(vertices) of the Graphs. In this research paper,I consider how\nContinuous-time quantum walk (CTQW) can be directed to Graph-matching problems.\nThe matching problem is abstracted using weighted(attributed) Graphs that\nconnects vertices's of one Graph to other and Try to compute the distance b/n\nthose Graphs Node's Beside that finding the matched nodes and the Cost related\nto Matching. eventually measuring the distance between two Graphs which might\nhave different size then by using k-nearest neighbor (k-NN) method try to\nclassifying those graph based on closest training examples in the feature\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 08:10:22 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Chekole", "Tewabe", ""]]}, {"id": "1801.10401", "submitter": "Marcelo Garlet Millani", "authors": "Marcelo Garlet Millani, Hendrik Molter, Rolf Niedermeier and Manuel\n  Sorge", "title": "Efficient Algorithms for Measuring the Funnel-likeness of DAGs", "comments": "Submitted to ISCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Funnels are a new natural subclass of DAGs. Intuitively, a DAG is a funnel if\nevery source-sink path can be uniquely identified by one of its arcs. Funnels\nare an analog to trees for directed graphs that is more restrictive than DAGs\nbut more expressive than in-/out-trees. Computational problems such as finding\nvertex-disjoint paths or tracking the origin of memes remain NP-hard on DAGs\nwhile on funnels they become solvable in polynomial time. Our main focus is the\nalgorithmic complexity of finding out how funnel-like a given DAG is. To this\nend, we study the NP-hard problem of computing the arc-deletion distance to a\nfunnel of a given DAG. We develop efficient exact and approximation algorithms\nfor the problem and test them on synthetic random graphs and real-world graphs.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 11:11:57 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Millani", "Marcelo Garlet", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Sorge", "Manuel", ""]]}, {"id": "1801.10416", "submitter": "Mattia D'Emidio Ph.D.", "authors": "Mattia D'Emidio, Luca Forlizzi, Daniele Frigioni, Stefano Leucci,\n  Guido Proietti", "title": "Hardness, Approximability, and Fixed-Parameter Tractability of the\n  Clustered Shortest-Path Tree Problem", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an $n$-vertex non-negatively real-weighted graph $G$, whose vertices\nare partitioned into a set of $k$ clusters, a \\emph{clustered network design\nproblem} on $G$ consists of solving a given network design optimization problem\non $G$, subject to some additional constraint on its clusters.\n  In particular, we focus on the classic problem of designing a\n\\emph{single-source shortest-path tree}, and we analyze its computational\nhardness when in a feasible solution each cluster is required to form a\nsubtree. We first study the \\emph{unweighted} case, and prove that the problem\nis \\np-hard. However, on the positive side, we show the existence of an\napproximation algorithm whose quality essentially depends on few parameters,\nbut which remarkably is an $O(1)$-approximation when the largest out of all the\n\\emph{diameters} of the clusters is either $O(1)$ or $\\Theta(n)$. Furthermore,\nwe also show that the problem is \\emph{fixed-parameter tractable} with respect\nto $k$ or to the number of vertices that belong to clusters of size at least 2.\nThen, we focus on the \\emph{weighted} case, and show that the problem can be\napproximated within a tight factor of $O(n)$, and that it is fixed-parameter\ntractable as well. Finally, we analyze the unweighted \\emph{single-pair\nshortest path problem}, and we show it is hard to approximate within a (tight)\nfactor of $n^{1-\\epsilon}$, for any $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 12:02:04 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["D'Emidio", "Mattia", ""], ["Forlizzi", "Luca", ""], ["Frigioni", "Daniele", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1801.10465", "submitter": "Jingjin Yu", "authors": "Jingjin Yu", "title": "Constant Factor Time Optimal Multi-Robot Routing on High-Dimensional\n  Grids in Mostly Sub-Quadratic Time", "comments": "26 pages, 30 figures, a short version appearing in the 2018 Robotics:\n  Science and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G = (V, E)$ be an $m_1 \\times \\ldots \\times m_k$ grid. Assuming that\neach $v \\in V$ is occupied by a robot and a robot may move to a neighboring\nvertex in a step via synchronized rotations along cycles of $G$, we first\nestablish that the arbitrary reconfiguration of labeled robots on $G$ can be\nperformed in $O(k\\sum_i m_i)$ makespan and requires $O(|V|^2)$ running time in\nthe worst case and $o(|V|^2)$ when $G$ is non-degenerate (in the current\ncontext, a grid is degenerate if it is nearly one dimensional). The resulting\nalgorithm, iSAG, provides average case $O(1)$-approximate (i.e.,\nconstant-factor) time optimality guarantee. When all dimensions are of similar\nsize $O(|V|^{\\frac{1}{k}})$, the running time of iSAG approaches a linear\n$O(|V|)$. Define $d_g(p)$ as the largest distance between individual initial\nand goal configurations over all robots for a given problem instance $p$,\nbuilding on iSAG, we develop the PartitionAndFlow (PAF) algorithm that computes\n$O(d_g(p))$ makespan solutions for arbitrary fixed $k \\ge 2$, using mostly\n$o(|V|^2)$ running time. PAF provides worst case $O(1)$-approximation regarding\nsolution time optimality. We note that the worst case running time for the\nproblem is $\\Omega(|V|^2)$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 14:40:29 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 15:15:42 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yu", "Jingjin", ""]]}, {"id": "1801.10476", "submitter": "Bruno Escoffier", "authors": "Eric Angel, Evripidis Bampis, Bruno Escoffier, Michael Lampis", "title": "Parameterized Power Vertex Cover", "comments": "Short version presented at the conference WG 2016, Graph-Theoretic\n  Concepts in Computer Science, LNCS 9941", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 20 no.\n  2, Discrete Algorithms (October 8, 2018) dmtcs:4873", "doi": "10.23638/DMTCS-20-2-10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a recently introduced generalization of the Vertex Cover (VC)\nproblem, called Power Vertex Cover (PVC). In this problem, each edge of the\ninput graph is supplied with a positive integer demand. A solution is an\nassignment of (power) values to the vertices, so that for each edge one of its\nendpoints has value as high as the demand, and the total sum of power values\nassigned is minimized. We investigate how this generalization affects the\nparameterized complexity of Vertex Cover. On the positive side, when\nparameterized by the value of the optimal P, we give an O*(1.274^P)-time\nbranching algorithm (O* is used to hide factors polynomial in the input size),\nand also an O*(1.325^P)-time algorithm for the more general asymmetric case of\nthe problem, where the demand of each edge may differ for its two endpoints.\nWhen the parameter is the number of vertices k that receive positive value, we\ngive O*(1.619^k) and O*(k^k)-time algorithms for the symmetric and asymmetric\ncases respectively, as well as a simple quadratic kernel for the asymmetric\ncase. We also show that PVC becomes significantly harder than classical VC when\nparameterized by the graph's treewidth t. More specifically, we prove that\nunless the ETH is false, there is no n^o(t)-time algorithm for PVC. We give a\nmethod to overcome this hardness by designing an FPT approximation scheme which\ngives a (1+epsilon)-approximation to the optimal solution in time FPT in\nparameters t and 1/epsilon.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 15:09:10 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 07:32:09 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 07:49:19 GMT"}, {"version": "v4", "created": "Thu, 4 Oct 2018 08:39:20 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Angel", "Eric", ""], ["Bampis", "Evripidis", ""], ["Escoffier", "Bruno", ""], ["Lampis", "Michael", ""]]}]