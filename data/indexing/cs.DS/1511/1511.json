[{"id": "1511.00021", "submitter": "Fred Glover", "authors": "Fred Glover, Vladimir Shylo, Oleg Shylo", "title": "Narrow Gauge and Analytical Branching Strategies for Mixed Integer\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art branch and bound algorithms for mixed integer programming\nmake use of special methods for making branching decisions. Strategies that\nhave gained prominence include modern variants of so-called strong branching\n(Applegate, et al.,1995) and reliability branching (Achterberg, Koch and\nMartin, 2005; Hendel, 2015), which select variables for branching by solving\nassociated linear programs and exploit pseudo-costs (Benichou et al., 1971). We\nsuggest new branching criteria and propose alternative branching approaches\ncalled narrow gauge and analytical branching. The perspective underlying our\napproaches is to focus on prioritization of child nodes to examine fewer\ncandidate variables at the current node of the B&B tree, balanced with\nprocedures to extrapolate the implications of choosing these candidates by\ngenerating a small-depth look-ahead tree. Our procedures can also be used in\nrules to select among open tree nodes (those whose child nodes have not yet\nbeen generated). We incorporate pre- and post-winnowing procedures to\nprogressively isolate preferred branching candidates, and employ derivative\n(created) variables whose branches are able to explore the solution space more\ndeeply.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 20:12:27 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 16:17:56 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Glover", "Fred", ""], ["Shylo", "Vladimir", ""], ["Shylo", "Oleg", ""]]}, {"id": "1511.00243", "submitter": "Ryuhei Uehara", "authors": "Takeshi Yamada and Ryuhei Uehara", "title": "Shortest Reconfiguration of Sliding Tokens on a Caterpillar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given two independent sets I_b and I_r of a graph such\nthat |I_b|=|I_r|, and imagine that a token is placed on each vertex in |I_b|.\nThen, the sliding token problem is to determine whether there exists a sequence\nof independent sets which transforms I_b into I_r so that each independent set\nin the sequence results from the previous one by sliding exactly one token\nalong an edge in the graph. The sliding token problem is one of the\nreconfiguration problems that attract the attention from the viewpoint of\ntheoretical computer science. The reconfiguration problems tend to be\nPSPACE-complete in general, and some polynomial time algorithms are shown in\nrestricted cases. Recently, the problems that aim at finding a shortest\nreconfiguration sequence are investigated. For the 3SAT problem, a trichotomy\nfor the complexity of finding the shortest sequence has been shown, that is, it\nis in P, NP-complete, or PSPACE-complete in certain conditions. In general,\neven if it is polynomial time solvable to decide whether two instances are\nreconfigured with each other, it can be NP-complete to find a shortest sequence\nbetween them. Namely, finding a shortest sequence between two independent sets\ncan be more difficult than the decision problem of reconfigurability between\nthem. In this paper, we show that the problem for finding a shortest sequence\nbetween two independent sets is polynomial time solvable for some graph classes\nwhich are subclasses of the class of interval graphs. More precisely, we can\nfind a shortest sequence between two independent sets on a graph G in\npolynomial time if either G is a proper interval graph, a trivially perfect\ngraph, or a caterpillar. As far as the authors know, this is the first\npolynomial time algorithm for the shortest sliding token problem for a graph\nclass that requires detours.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 13:12:49 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Yamada", "Takeshi", ""], ["Uehara", "Ryuhei", ""]]}, {"id": "1511.00310", "submitter": "Daniel Lokshtanov", "authors": "Daniel Lokshtanov", "title": "Parameterized Integer Quadratic Programming: Variables and Coefficients", "comments": "Added algorithm for undounded IQP's", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Integer Quadratic Programming problem input is an n*n integer matrix\nQ, an m*n integer matrix A and an m-dimensional integer vector b. The task is\nto find a vector x in Z^n, minimizing x^TQx, subject to Ax <= b. We give a\nfixed parameter tractable algorithm for Integer Quadratic Programming\nparameterized by n+a. Here a is the largest absolute value of an entry of Q and\nA. As an application of our main result we show that Optimal Linear Arrangement\nis fixed parameter tractable parameterized by the size of the smallest vertex\ncover of the input graph. This resolves an open problem from the recent\nmonograph by Downey and Fellows.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 21:47:44 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 11:39:57 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Lokshtanov", "Daniel", ""]]}, {"id": "1511.00493", "submitter": "Heng Guo", "authors": "Heng Guo and Pinyan Lu", "title": "Uniqueness, Spatial Mixing, and Approximation for Ferromagnetic 2-Spin\n  Systems", "comments": "to appear in ACM TOCT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give fully polynomial-time approximation schemes (FPTAS) for the partition\nfunction of ferromagnetic 2-spin systems in certain parameter regimes. The\nthreshold we obtain is almost tight up to an integrality gap. Our technique is\nbased on the correlation decay framework. The main technical contribution is a\nnew potential function, with which we establish a new kind of spatial mixing.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 13:34:52 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 07:55:37 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 07:15:00 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Guo", "Heng", ""], ["Lu", "Pinyan", ""]]}, {"id": "1511.00628", "submitter": "Mohamad Dolatshah", "authors": "Mohamad Dolatshah, Ali Hadian and Behrouz Minaei-Bidgoli", "title": "Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor\n  search in metric spaces", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging location-based systems and data analysis frameworks requires\nefficient management of spatial data for approximate and exact search. Exact\nsimilarity search can be done using space partitioning data structures, such as\nKd-tree, R*-tree, and Ball-tree. In this paper, we focus on Ball-tree, an\nefficient search tree that is specific for spatial queries which use euclidean\ndistance. Each node of a Ball-tree defines a ball, i.e. a hypersphere that\ncontains a subset of the points to be searched.\n  In this paper, we propose Ball*-tree, an improved Ball-tree that is more\nefficient for spatial queries. Ball*-tree enjoys a modified space partitioning\nalgorithm that considers the distribution of the data points in order to find\nan efficient splitting hyperplane. Also, we propose a new algorithm for KNN\nqueries with restricted range using Ball*-tree, which performs better than both\nKNN and range search for such queries. Results show that Ball*-tree performs\n39%-57% faster than the original Ball-tree algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 18:54:49 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Dolatshah", "Mohamad", ""], ["Hadian", "Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1511.00648", "submitter": "Yuan Zhou", "authors": "Xue Chen, Yuan Zhou", "title": "Parameterized Algorithms for Constraint Satisfaction Problems Above\n  Average with Global Cardinality Constraints", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a constraint satisfaction problem (CSP) on $n$ variables, $x_1, x_2,\n\\dots, x_n \\in \\{\\pm 1\\}$, and $m$ constraints, a global cardinality constraint\nhas the form of $\\sum_{i = 1}^{n} x_i = (1-2p)n$, where $p \\in (\\Omega(1), 1 -\n\\Omega(1))$ and $pn$ is an integer. Let $AVG$ be the expected number of\nconstraints satisfied by randomly choosing an assignment to $x_1, x_2, \\dots,\nx_n$, complying with the global cardinality constraint. The CSP above average\nwith the global cardinality constraint problem asks whether there is an\nassignment (complying with the cardinality constraint) that satisfies more than\n$(AVG+t)$ constraints, where $t$ is an input parameter.\n  In this paper, we present an algorithm that finds a valid assignment\nsatisfying more than $(AVG+t)$ constraints (if there exists one) in time\n$(2^{O(t^2)} + n^{O(d)})$. Therefore, the CSP above average with the global\ncardinality constraint problem is fixed-parameter tractable.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 19:37:03 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 23:01:30 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Chen", "Xue", ""], ["Zhou", "Yuan", ""]]}, {"id": "1511.00661", "submitter": "Stephen Chestnut", "authors": "Vladimir Braverman, Stephen R. Chestnut, Nikita Ivkin, David P.\n  Woodruff", "title": "Beating CountSketch for Heavy Hitters in Insertion Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a stream $p_1, \\ldots, p_m$ of items from a universe $\\mathcal{U}$,\nwhich, without loss of generality we identify with the set of integers $\\{1, 2,\n\\ldots, n\\}$, we consider the problem of returning all $\\ell_2$-heavy hitters,\ni.e., those items $j$ for which $f_j \\geq \\epsilon \\sqrt{F_2}$, where $f_j$ is\nthe number of occurrences of item $j$ in the stream, and $F_2 = \\sum_{i \\in\n[n]} f_i^2$. Such a guarantee is considerably stronger than the\n$\\ell_1$-guarantee, which finds those $j$ for which $f_j \\geq \\epsilon m$. In\n2002, Charikar, Chen, and Farach-Colton suggested the {\\sf CountSketch} data\nstructure, which finds all such $j$ using $\\Theta(\\log^2 n)$ bits of space (for\nconstant $\\epsilon > 0$). The only known lower bound is $\\Omega(\\log n)$ bits\nof space, which comes from the need to specify the identities of the items\nfound. In this paper we show it is possible to achieve $O(\\log n \\log \\log n)$\nbits of space for this problem. Our techniques, based on Gaussian processes,\nlead to a number of other new results for data streams, including\n  (1) The first algorithm for estimating $F_2$ simultaneously at all points in\na stream using only $O(\\log n\\log\\log n)$ bits of space, improving a natural\nunion bound and the algorithm of Huang, Tai, and Yi (2014).\n  (2) A way to estimate the $\\ell_{\\infty}$ norm of a stream up to additive\nerror $\\epsilon \\sqrt{F_2}$ with $O(\\log n\\log\\log n)$ bits of space, resolving\nOpen Question 3 from the IITK 2006 list for insertion only streams.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 20:03:39 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Braverman", "Vladimir", ""], ["Chestnut", "Stephen R.", ""], ["Ivkin", "Nikita", ""], ["Woodruff", "David P.", ""]]}, {"id": "1511.00700", "submitter": "Greg Bodwin", "authors": "Amir Abboud and Greg Bodwin", "title": "The 4/3 Additive Spanner Exponent is Tight", "comments": "Updated for journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spanner is a sparse subgraph that approximately preserves the pairwise\ndistances of the original graph. It is well known that there is a smooth\ntradeoff between the sparsity of a spanner and the quality of its\napproximation, so long as distance error is measured multiplicatively. A\ncentral open question in the field is to prove or disprove whether such a\ntradeoff exists also in the regime of \\emph{additive} error. That is, is it\ntrue that for all $\\varepsilon>0$, there is a constant $k_{\\varepsilon}$ such\nthat every graph has a spanner on $O(n^{1+\\varepsilon})$ edges that preserves\nits pairwise distances up to $+k_{\\varepsilon}$? Previous lower bounds are\nconsistent with a positive resolution to this question, while previous upper\nbounds exhibit the beginning of a tradeoff curve: all graphs have $+2$ spanners\non $O(n^{3/2})$ edges, $+4$ spanners on $\\tilde{O}(n^{7/5})$ edges, and $+6$\nspanners on $O(n^{4/3})$ edges. However, progress has mysteriously halted at\nthe $n^{4/3}$ bound, and despite significant effort from the community, the\nquestion has remained open for all $0 < \\varepsilon < 1/3$.\n  Our main result is a surprising negative resolution of the open question,\neven in a highly generalized setting. We show a new information theoretic\nincompressibility bound: there is no function that compresses graphs into\n$O(n^{4/3 - \\varepsilon})$ bits so that distance information can be recovered\nwithin $+n^{o(1)}$ error. As a special case of our theorem, we get a tight\nlower bound on the sparsity of additive spanners: the $+6$ spanner on\n$O(n^{4/3})$ edges cannot be improved in the exponent, even if any\nsubpolynomial amount of additive error is allowed. Our theorem implies new\nlower bounds for related objects as well; for example, the twenty-year-old $+4$\nemulator on $O(n^{4/3})$ edges also cannot be improved in the exponent unless\nthe error allowance is polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 21:01:43 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 06:09:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Abboud", "Amir", ""], ["Bodwin", "Greg", ""]]}, {"id": "1511.00715", "submitter": "Junichiro Fukuyama", "authors": "Piotr Berman and Junichiro Fukuyama", "title": "Distributed Selection in $O ( \\log n )$ Time with $O ( n \\log \\log n )$\n  Messages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the selection problem on a completely connected network of $n$\nprocessors with no shared memory. Each processor initially holds a given\nnumeric item of $b$ bits allowed to send a message of $\\max ( b, \\lg n )$ bits\nto another processor at a time. On such a communication network ${\\cal G}$, we\nshow that the $k$th smallest of the $n$ inputs can be detected in $O ( \\log n\n)$ time with $O ( n \\log \\log n )$ messages. The possibility of such a parallel\nalgorithm for this distributed $k$-selection problem has been unknown despite\nthe intensive investigation on many variations of the selection problem carried\nout since 1970s. The main trick of our algorithm is to simulate the comparisons\nand swaps performed by the AKS sorting network, the $n$-input sorting network\nof logarithmic depth discovered by Ajtai, Koml{\\'o}s and Szemer{\\'e}di in 1983.\nWe also show the universal time lower bound $\\lg n$ for many basic data\naggregation problems on ${\\cal G}$, confirming the asymptotic time optimality\nof our parallel algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 21:30:12 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 16:30:03 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 20:16:26 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 22:11:17 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Berman", "Piotr", ""], ["Fukuyama", "Junichiro", ""]]}, {"id": "1511.00838", "submitter": "Alan Roytman", "authors": "Vladimir Braverman, Alan Roytman, Gregory Vorsanger", "title": "Approximating Subadditive Hadamard Functions on Implicit Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in the streaming model is to maintain small-space\napproximations of entrywise functions performed on a matrix that is generated\nby the outer product of two vectors given as a stream. In other works, streams\ntypically define matrices in a standard way via a sequence of updates, as in\nthe work of Woodruff (2014) and others. We describe the matrix formed by the\nouter product, and other matrices that do not fall into this category, as\nimplicit matrices. As such, we consider the general problem of computing over\nsuch implicit matrices with Hadamard functions, which are functions applied\nentrywise on a matrix. In this paper, we apply this generalization to provide\nnew techniques for identifying independence between two vectors in the\nstreaming model. The previous state of the art algorithm of Braverman and\nOstrovsky (2010) gave a $(1 \\pm \\epsilon)$-approximation for the $L_1$ distance\nbetween the product and joint distributions, using space $O(\\log^{1024}(nm)\n\\epsilon^{-1024})$, where $m$ is the length of the stream and $n$ denotes the\nsize of the universe from which stream elements are drawn. Our general\ntechniques include the $L_1$ distance as a special case, and we give an\nimproved space bound of $O(\\log^{12}(n) \\log^{2}({nm \\over\n\\epsilon})\\epsilon^{-7})$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 09:54:24 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Braverman", "Vladimir", ""], ["Roytman", "Alan", ""], ["Vorsanger", "Gregory", ""]]}, {"id": "1511.00873", "submitter": "Martin Derka", "authors": "Therese Biedl, Martin Derka", "title": "The (3,1)-ordering for 4-connected planar triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical orderings of planar graphs have frequently been used in graph\ndrawing and other graph algorithms. In this paper we introduce the notion of an\n$(r,s)$-canonical order, which unifies many of the existing variants of\ncanonical orderings. We then show that $(3,1)$-canonical ordering for\n4-connected triangulations always exist; to our knowledge this variant of\ncanonical ordering was not previously known. We use it to give much simpler\nproofs of two previously known graph drawing results for 4-connected planar\ntriangulations, namely, rectangular duals and rectangle-of-influence drawings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 12:17:51 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Biedl", "Therese", ""], ["Derka", "Martin", ""]]}, {"id": "1511.00876", "submitter": "Rob Van Stee", "authors": "Sandy Heydrich, Rob van Stee", "title": "Beating the Harmonic lower bound for online bin packing", "comments": "Added reference and clarified some sentences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online bin packing problem, items of sizes in (0,1] arrive online to\nbe packed into bins of size 1. The goal is to minimize the number of used bins.\nIn this paper, we present an online bin packing algorithm with asymptotic\ncompetitive ratio of 1.5813. This is the first improvement in fifteen years and\nreduces the gap to the lower bound by 15%. Within the well-known SuperHarmonic\nframework, no competitive ratio below 1.58333 can be achieved.\n  We make two crucial changes to that framework. First, some of our algorithm's\ndecisions depend on exact sizes of items, instead of only their types. In\nparticular, for each item with size in (1/3,1/2], we use its exact size to\ndetermine if it can be packed together with an item of size greater than 1/2.\nSecond, we add constraints to the linear programs considered by Seiden, in\norder to better lower bound the optimal solution. These extra constraints are\nbased on marks that we give to items based on how they are packed by our\nalgorithm. We show that for each input, a single weighting function can be\nconstructed to upper bound the competitive ratio on it.\n  We use this idea to simplify the analysis of SuperHarmonic, and show that the\nalgorithm Harmonic++ is in fact 1.58880-competitive (Seiden proved 1.58889),\nand that 1.5884 can be achieved within the SuperHarmonic framework. Finally, we\ngive a lower bound of 1.5762 for our new framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 12:24:08 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 09:10:42 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 12:57:52 GMT"}, {"version": "v4", "created": "Fri, 7 Jul 2017 12:32:23 GMT"}, {"version": "v5", "created": "Fri, 4 Aug 2017 14:19:26 GMT"}, {"version": "v6", "created": "Thu, 11 Jan 2018 08:22:45 GMT"}, {"version": "v7", "created": "Thu, 28 Jun 2018 08:20:06 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Heydrich", "Sandy", ""], ["van Stee", "Rob", ""]]}, {"id": "1511.00898", "submitter": "Jouni Sir\\'en", "authors": "Jouni Sir\\'en", "title": "Burrows-Wheeler transform for terabases", "comments": "This is the full version of the paper that was accepted to DCC 2016.\n  The implementation is available at https://github.com/jltsiren/bwt-merge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to avoid the reference bias introduced by mapping reads to a\nreference genome, bioinformaticians are investigating reference-free methods\nfor analyzing sequenced genomes. With large projects sequencing thousands of\nindividuals, this raises the need for tools capable of handling terabases of\nsequence data. A key method is the Burrows-Wheeler transform (BWT), which is\nwidely used for compressing and indexing reads. We propose a practical\nalgorithm for building the BWT of a large read collection by merging the BWTs\nof subcollections. With our 2.4 Tbp datasets, the algorithm can merge 600\nGbp/day on a single system, using 30 gigabytes of memory overhead on top of the\nrun-length encoded BWTs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 13:14:37 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 15:35:19 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Sir\u00e9n", "Jouni", ""]]}, {"id": "1511.01038", "submitter": "Yan Gu", "authors": "Guy E. Blelloch and Jeremy T. Fineman and Phillip B. Gibbons and Yan\n  Gu and Julian Shun", "title": "Efficient Algorithms with Asymmetric Read and Write Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several emerging technologies for computer memory (main memory), the cost\nof reading is significantly cheaper than the cost of writing. Such asymmetry in\nmemory costs poses a fundamentally different model from the RAM for algorithm\ndesign. In this paper we study lower and upper bounds for various problems\nunder such asymmetric read and write costs. We consider both the case in which\nall but $O(1)$ memory has asymmetric cost, and the case of a small cache of\nsymmetric memory. We model both cases using the $(M,\\omega)$-ARAM, in which\nthere is a small (symmetric) memory of size $M$ and a large unbounded\n(asymmetric) memory, both random access, and where reading from the large\nmemory has unit cost, but writing has cost $\\omega\\gg 1$.\n  For FFT and sorting networks we show a lower bound cost of $\\Omega(\\omega\nn\\log_{\\omega M} n)$, which indicates that it is not possible to achieve\nasymptotic improvements with cheaper reads when $\\omega$ is bounded by a\npolynomial in $M$. Also, there is an asymptotic gap (of $\\min(\\omega,\\log\nn)/\\log(\\omega M)$) between the cost of sorting networks and comparison sorting\nin the model. This contrasts with the RAM, and most other models. We also show\na lower bound for computations on an $n\\times n$ diamond DAG of $\\Omega(\\omega\nn^2/M)$ cost, which indicates no asymptotic improvement is achievable with fast\nreads. However, we show that for the edit distance problem (and related\nproblems), which would seem to be a diamond DAG, there exists an algorithm with\nonly $O(\\omega n^2/(M\\min(\\omega^{1/3},M^{1/2})))$ cost. To achieve this we\nmake use of a \"path sketch\" technique that is forbidden in a strict DAG\ncomputation. Finally, we show several interesting upper bounds for shortest\npath problems, minimum spanning trees, and other problems. A common theme in\nmany of the upper bounds is to have redundant computation to tradeoff between\nreads and writes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 19:18:55 GMT"}, {"version": "v2", "created": "Sun, 3 Jul 2016 02:28:22 GMT"}, {"version": "v3", "created": "Sun, 28 Aug 2016 17:41:31 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Fineman", "Jeremy T.", ""], ["Gibbons", "Phillip B.", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "1511.01061", "submitter": "Ramon Ferrer i Cancho", "authors": "Juan Luis Esteban and Ramon Ferrer-i-Cancho", "title": "A correction on Shiloach's algorithm for minimum linear arrangement of\n  trees", "comments": "A new introductory paragraph has been added; error solutions and\n  notation improvements are discussed with more depth", "journal-ref": "SIAM Journal of Computing 46(3), 1146-1151 (2017)", "doi": "10.1137/15M1046289", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than 30 years ago, Shiloach published an algorithm to solve the minimum\nlinear arrangement problem for undirected trees. Here we fix a small error in\nthe original version of the algorithm and discuss its effect on subsequent\nliterature. We also improve some aspects of the notation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 18:34:31 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 10:18:53 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Esteban", "Juan Luis", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1511.01080", "submitter": "Michel Rueher", "authors": "H\\'el\\`ene Collavizza, Claude Michel, Michel Rueher", "title": "Searching input values hitting suspicious Intervals in programs with\n  floating-point operations", "comments": null, "journal-ref": "28th International Conference on Software and Systems\n  (ICTSS-2016)., Oct 2016, Graz, Austria. 2016, LNCS", "doi": null, "report-no": null, "categories": "cs.PL cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs with floating-point computations are often derived from mathematical\nmodels or designed with the semantics of the real numbers in mind. However, for\na given input, the computed path with floating-point numbers may differ from\nthe path corresponding to the same computation with real numbers. A common\npractice when validating such programs consists in estimating the accuracy of\nfloating-point computations with respect to the same sequence of operations in\nan ide-alized semantics of real numbers. However, state-of-the-art tools\ncompute an over-approximation of the error introduced by floating-point\noperations. As a consequence, totally inappropriate behaviors of a program may\nbe dreaded but the developer does not know whether these behaviors will\nactually occur, or not. In this paper, we introduce a new constraint-based\napproach that searches for test cases in the part of the over-approximation\nwhere errors due to floating-point arithmetic would lead to inappropriate\nbehaviors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 20:46:32 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 08:22:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Collavizza", "H\u00e9l\u00e8ne", ""], ["Michel", "Claude", ""], ["Rueher", "Michel", ""]]}, {"id": "1511.01111", "submitter": "Lin Yang", "authors": "Jaroslaw Blasiok, Vladimir Braverman, Stephen R. Chestnut, Robert\n  Krauthgamer, Lin F. Yang", "title": "Streaming Symmetric Norms via Measure Concentration", "comments": "published in STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the streaming space complexity of every symmetric norm $l$ (a\nnorm on $\\mathbb{R}^n$ invariant under sign-flips and coordinate-permutations),\nby relating this space complexity to the measure-concentration characteristics\nof $l$. Specifically, we provide nearly matching upper and lower bounds on the\nspace complexity of calculating a $(1\\pm\\epsilon)$-approximation to the norm of\nthe stream, for every $0<\\epsilon\\leq 1/2$. (The bounds match up to\n$poly(\\epsilon^{-1} \\log n)$ factors.) We further extend those bounds to any\nlarge approximation ratio $D\\geq 1.1$, showing that the decrease in space\ncomplexity is proportional to $D^2$, and that this factor the best possible.\nAll of the bounds depend on the median of $l(x)$ when $x$ is drawn uniformly\nfrom the $l_2$ unit sphere. The same median governs many phenomena in\nhigh-dimensional spaces, such as large-deviation bounds and the critical\ndimension in Dvoretzky's Theorem.\n  The family of symmetric norms contains several well-studied norms, such as\nall $l_p$~norms, and indeed we provide a new explanation for the disparity in\nspace complexity between $p\\le 2$ and $p>2$. In addition, we apply our general\nresults to easily derive bounds for several norms that were not studied before\nin the streaming model, including the top-$k$ norm and the $k$-support norm,\nwhich was recently employed for machine learning tasks.\n  Overall, these results make progress on two outstanding problems in the area\nof sublinear algorithms (Problems 5 and 30 in~\\url{http://sublinear.info}).\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 21:09:55 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 04:59:13 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 04:40:07 GMT"}, {"version": "v4", "created": "Mon, 26 Jun 2017 05:23:35 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Blasiok", "Jaroslaw", ""], ["Braverman", "Vladimir", ""], ["Chestnut", "Stephen R.", ""], ["Krauthgamer", "Robert", ""], ["Yang", "Lin F.", ""]]}, {"id": "1511.01137", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Matthias Mnich, Virginia Vassilevska Williams, L\\'aszl\\'o A. V\\'egh", "title": "A 7/3-Approximation for Feedback Vertex Sets in Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum-weight feedback vertex set problem in tournaments:\ngiven a tournament with non-negative vertex weights, remove a minimum-weight\nset of vertices that intersects all cycles. This problem is $\\mathsf{NP}$-hard\nto solve exactly, and Unique Games-hard to approximate by a factor better than\n2. We present the first $7/3$ approximation algorithm for this problem,\nimproving on the previously best known ratio $5/2$ given by Cai et al. [FOCS\n1998, SICOMP 2001].\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 22:09:46 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Mnich", "Matthias", ""], ["Williams", "Virginia Vassilevska", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1511.01138", "submitter": "Sebastian Wild", "authors": "Sebastian Wild", "title": "Why Is Dual-Pivot Quicksort Fast?", "comments": "extended abstract for Theorietage 2015\n  (https://www.uni-trier.de/index.php?id=55089) (v2 fixes a small bug in the\n  pseudocode)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I discuss the new dual-pivot Quicksort that is nowadays used to sort arrays\nof primitive types in Java. I sketch theoretical analyses of this algorithm\nthat offer a possible, and in my opinion plausible, explanation why (a)\ndual-pivot Quicksort is faster than the previously used (classic) Quicksort and\n(b) why this improvement was not already found much earlier.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 22:13:47 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 10:20:01 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Wild", "Sebastian", ""]]}, {"id": "1511.01175", "submitter": "Pu Gao", "authors": "Pu Gao and Nicholas Wormald", "title": "Uniform generation of random regular graphs", "comments": "This is a slightly corrected version of the published paper, in which\n  the part of the argument using Lemma 11 had been inadvertently omitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for uniform generation of combinatorial objects,\nand apply it to derive a uniform sampler REG for d-regular graphs. REG can be\nimplemented such that each graph is generated in expected time O(nd^3),\nprovided that d=o(n^{1/2}). Our result significantly improves the previously\nbest uniform sampler, which works efficiently only when d=O(n^{1/3}), with\nessentially the same running time for the same d. We also give a linear-time\napproximate sampler REG*, which generates a random d-regular graph whose\ndistribution differs from the uniform by o(1) in total variation distance, when\nd=o(n^{1/2}).\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 01:02:55 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 20:45:39 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Gao", "Pu", ""], ["Wormald", "Nicholas", ""]]}, {"id": "1511.01287", "submitter": "Adrian Kosowski", "authors": "Pierre Fraigniaud (GANG), Marc Heinrich (GANG), Adrian Kosowski (GANG)", "title": "Local Conflict Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally finding a solution to symmetry-breaking tasks such as\nvertex-coloring, edge-coloring, maximal matching, maximal independent set,\netc., is a long-standing challenge in distributed network computing. More\nrecently, it has also become a challenge in the framework of centralized local\ncomputation. We introduce conflict coloring as a general symmetry-breaking task\nthat includes all the aforementioned tasks as specific instantiations ---\nconflict coloring includes all locally checkable labeling tasks from\n[Naor\\&Stockmeyer, STOC 1993]. Conflict coloring is characterized by two\nparameters $l$ and $d$, where the former measures the amount of freedom given\nto the nodes for selecting their colors, and the latter measures the number of\nconstraints which colors of adjacent nodes are subject to.We show that, in the\nstandard LOCAL model for distributed network computing, if $l/d \\textgreater{}\n\\Delta$, then conflict coloring can be solved in $\\tilde\nO(\\sqrt{\\Delta})+\\log^*n$ rounds in $n$-node graphs with maximum degree\n$\\Delta$, where $\\tilde O$ ignores the polylog factors in $\\Delta$. The\ndependency in~$n$ is optimal, as a consequence of the $\\Omega(\\log^*n)$ lower\nbound by [Linial, SIAM J. Comp. 1992] for $(\\Delta+1)$-coloring. An important\nspecial case of our result is a significant improvement over the best known\nalgorithm for distributed $(\\Delta+1)$-coloring due to [Barenboim, PODC 2015],\nwhich required $\\tilde O(\\Delta^{3/4})+\\log^*n$ rounds. Improvements for other\nvariants of coloring, including $(\\Delta+1)$-list-coloring,\n$(2\\Delta-1)$-edge-coloring, $T$-coloring, etc., also follow from our general\nresult on conflict coloring. Likewise, in the framework of centralized local\ncomputation algorithms (LCAs), our general result yields an LCA which requires\na smaller number of probes than the previously best known algorithm for\nvertex-coloring, and works for a wide range of coloring problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:56:55 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 12:38:21 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Fraigniaud", "Pierre", "", "GANG"], ["Heinrich", "Marc", "", "GANG"], ["Kosowski", "Adrian", "", "GANG"]]}, {"id": "1511.01379", "submitter": "Micha{\\l} Pilipczuk", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Micha{\\l} Pilipczuk, Saket Saurabh,\n  Marcin Wrochna", "title": "Fully polynomial-time parameterized computations for graphs and matrices\n  of low treewidth", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of several fundamental polynomial-time solvable\nproblems on graphs and on matrices, when the given instance has low treewidth;\nin the case of matrices, we consider the treewidth of the graph formed by\nnon-zero entries. In each of the considered cases, the best known algorithms\nworking on general graphs run in polynomial time, however the exponent of the\npolynomial is large. Therefore, our main goal is to construct algorithms with\nrunning time of the form $\\textrm{poly}(k)\\cdot n$ or $\\textrm{poly}(k)\\cdot\nn\\log n$, where $k$ is the width of the tree decomposition given on the input.\nSuch procedures would outperform the best known algorithms for the considered\nproblems already for moderate values of the treewidth, like $O(n^{1/c})$ for\nsome small constant $c$.\n  Our results include:\n  -- an algorithm for computing the determinant and the rank of an $n\\times n$\nmatrix using $O(k^3\\cdot n)$ time and arithmetic operations;\n  -- an algorithm for solving a system of linear equations using $O(k^3\\cdot\nn)$ time and arithmetic operations;\n  -- an $O(k^3\\cdot n\\log n)$-time randomized algorithm for finding the\ncardinality of a maximum matching in a graph;\n  -- an $O(k^4\\cdot n\\log^2 n)$-time randomized algorithm for constructing a\nmaximum matching in a graph;\n  -- an $O(k^2\\cdot n\\log n)$-time algorithm for finding a maximum vertex flow\nin a directed graph.\n  Moreover, we give an approximation algorithm for treewidth with time\ncomplexity suited to the running times as above. Namely, the algorithm, when\ngiven a graph $G$ and integer $k$, runs in time $O(k^7\\cdot n\\log n)$ and\neither correctly reports that the treewidth of $G$ is larger than $k$, or\nconstructs a tree decomposition of $G$ of width $O(k^2)$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 16:06:09 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1511.01473", "submitter": "Alexander Wein", "authors": "Ankur Moitra and William Perry and Alexander S. Wein", "title": "How Robust are Reconstruction Thresholds for Community Detection?", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is one of the oldest and most ubiquitous models\nfor studying clustering and community detection. In an exciting sequence of\ndevelopments, motivated by deep but non-rigorous ideas from statistical\nphysics, Decelle et al. conjectured a sharp threshold for when community\ndetection is possible in the sparse regime. Mossel, Neeman and Sly and\nMassoulie proved the conjecture and gave matching algorithms and lower bounds.\n  Here we revisit the stochastic block model from the perspective of semirandom\nmodels where we allow an adversary to make `helpful' changes that strengthen\nties within each community and break ties between them. We show a surprising\nresult that these `helpful' changes can shift the information-theoretic\nthreshold, making the community detection problem strictly harder. We\ncomplement this by showing that an algorithm based on semidefinite programming\n(which was known to get close to the threshold) continues to work in the\nsemirandom model (even for partial recovery). This suggests that algorithms\nbased on semidefinite programming are robust in ways that any algorithm meeting\nthe information-theoretic threshold cannot be.\n  These results point to an interesting new direction: Can we find robust,\nsemirandom analogues to some of the classical, average-case thresholds in\nstatistics? We also explore this question in the broadcast tree model, and we\nshow that the viewpoint of semirandom models can help explain why some\nalgorithms are preferred to others in practice, in spite of the gaps in their\nstatistical performance on random models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 20:50:21 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 21:28:21 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Moitra", "Ankur", ""], ["Perry", "William", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1511.01661", "submitter": "Gregory Gutin", "authors": "Gregory Gutin and Anders Yeo", "title": "Note on Perfect Forests in Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spanning subgraph $F$ of a graph $G$ is called {\\em perfect} if $F$ is a\nforest, the degree $d_F(x)$ of each vertex $x$ in $F$ is odd, and each tree of\n$F$ is an induced subgraph of $G$. Alex Scott (Graphs \\& Combin., 2001) proved\nthat every connected graph $G$ contains a perfect forest if and only if $G$ has\nan even number of vertices. We consider four generalizations to directed graphs\nof the concept of a perfect forest. While the problem of existence of the most\nstraightforward one is NP-hard, for the three others this problem is\npolynomial-time solvable. Moreover, every digraph with only one strong\ncomponent contains a directed forest of each of these three generalization\ntypes. One of our results extends Scott's theorem to digraphs in a non-trivial\nway.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 09:11:14 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Gutin", "Gregory", ""], ["Yeo", "Anders", ""]]}, {"id": "1511.01699", "submitter": "Chen Dan", "authors": "Chen Dan, Kristoffer Arnsfelt Hansen, He Jiang, Liwei Wang, Yuchen\n  Zhou", "title": "Low Rank Approximation of Binary Matrices: Column Subset Selection and\n  Generalizations", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank matrix approximation is an important tool in machine learning. Given\na data matrix, low rank approximation helps to find factors, patterns and\nprovides concise representations for the data. Research on low rank\napproximation usually focus on real matrices. However, in many applications\ndata are binary (categorical) rather than continuous. This leads to the problem\nof low rank approximation of binary matrix. Here we are given a $d \\times n$\nbinary matrix $A$ and a small integer $k$. The goal is to find two binary\nmatrices $U$ and $V$ of sizes $d \\times k$ and $k \\times n$ respectively, so\nthat the Frobenius norm of $A - U V$ is minimized. There are two models of this\nproblem, depending on the definition of the dot product of binary vectors: The\n$\\mathrm{GF}(2)$ model and the Boolean semiring model. Unlike low rank\napproximation of real matrix which can be efficiently solved by Singular Value\nDecomposition, approximation of binary matrix is $NP$-hard even for $k=1$.\n  In this paper, we consider the problem of Column Subset Selection (CSS), in\nwhich one low rank matrix must be formed by $k$ columns of the data matrix. We\ncharacterize the approximation ratio of CSS for binary matrices. For $GF(2)$\nmodel, we show the approximation ratio of CSS is bounded by\n$\\frac{k}{2}+1+\\frac{k}{2(2^k-1)}$ and this bound is asymptotically tight. For\nBoolean model, it turns out that CSS is no longer sufficient to obtain a bound.\nWe then develop a Generalized CSS (GCSS) procedure in which the columns of one\nlow rank matrix are generated from Boolean formulas operating bitwise on\ncolumns of the data matrix. We show the approximation ratio of GCSS is bounded\nby $2^{k-1}+1$, and the exponential dependency on $k$ is inherent.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 11:26:52 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 14:47:54 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Dan", "Chen", ""], ["Hansen", "Kristoffer Arnsfelt", ""], ["Jiang", "He", ""], ["Wang", "Liwei", ""], ["Zhou", "Yuchen", ""]]}, {"id": "1511.01770", "submitter": "Both Emerite Neou", "authors": "Both Emerite Neou, Romeo Rizzi, St\\'ephane Vialette", "title": "Pattern matching in $(213,231)$-avoiding permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given permutations $\\sigma \\in S_k$ and $\\pi \\in S_n$ with $k<n$, the\n\\emph{pattern matching} problem is to decide whether $\\pi$ matches $\\sigma$ as\nan order-isomorphic subsequence. We give a linear-time algorithm in case both\n$\\pi$ and $\\sigma$ avoid the two size-$3$ permutations $213$ and $231$. For the\nspecial case where only $\\sigma$ avoids $213$ and $231$, we present a\n$O(max(kn^2,n^2\\log(\\log(n)))$ time algorithm. We extend our research to\nbivincular patterns that avoid $213$ and $231$ and present a $O(kn^4)$ time\nalgorithm. Finally we look at the related problem of the longest subsequence\nwhich avoids $213$ and $231$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 15:08:59 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Neou", "Both Emerite", ""], ["Rizzi", "Romeo", ""], ["Vialette", "St\u00e9phane", ""]]}, {"id": "1511.01818", "submitter": "Pierre Le Bodic", "authors": "Pierre Le Bodic and George L. Nemhauser", "title": "An Abstract Model for Branching and its Application to Mixed Integer\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of branching variables is a key component of branch-and-bound\nalgorithms for solving Mixed-Integer Programming (MIP) problems since the\nquality of the selection procedure is likely to have a significant effect on\nthe size of the enumeration tree. State-of-the-art procedures base the\nselection of variables on their \"LP gains\", which is the dual bound improvement\nobtained after branching on a variable. There are various ways of selecting\nvariables depending on their LP gains. However, all methods are evaluated\nempirically. In this paper we present a theoretical model for the selection of\nbranching variables. It is based upon an abstraction of MIPs to a simpler\nsetting in which it is possible to analytically evaluate the dual bound\nimprovement of choosing a given variable. We then discuss how the analytical\nresults can be used to choose branching variables for MIPs, and we give\nexperimental results that demonstrate the effectiveness of the method on MIPLIB\n2010 \"tree\" instances where we achieve a 5% geometric average time and node\nimprovement over the default rule of SCIP, a state-of-the-art MIP solver.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 17:19:17 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 00:09:57 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 00:28:20 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bodic", "Pierre Le", ""], ["Nemhauser", "George L.", ""]]}, {"id": "1511.01994", "submitter": "Julian Yarkony", "authors": "Julian Yarkony", "title": "Next Generation Multicuts for Semi-Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multicut segmentation. We introduce modified versions\nof the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work\nto natural image segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 07:18:56 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Yarkony", "Julian", ""]]}, {"id": "1511.02074", "submitter": "Marcin Bienkowski", "authors": "Chen Avin and Marcin Bienkowski and Andreas Loukas and Maciej Pacut\n  and Stefan Schmid", "title": "Dynamic Balanced Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper initiates the study of the classic balanced graph partitioning\nproblem from an online perspective: Given an arbitrary sequence of pairwise\ncommunication requests between $n$ nodes, with patterns that may change over\ntime, the objective is to service these requests efficiently by partitioning\nthe nodes into $\\ell$ clusters, each of size $k$, such that frequently\ncommunicating nodes are located in the same cluster. The partitioning can be\nupdated dynamically by migrating nodes between clusters. The goal is to devise\nonline algorithms which jointly minimize the amount of inter-cluster\ncommunication and migration cost.\n  The problem features interesting connections to other well-known online\nproblems. For example, scenarios with $\\ell=2$ generalize online paging, and\nscenarios with $k=2$ constitute a novel online variant of maximum matching. We\npresent several lower bounds and algorithms for settings both with and without\ncluster-size augmentation. In particular, we prove that any deterministic\nonline algorithm has a competitive ratio of at least $k$, even with significant\naugmentation. Our main algorithmic contributions are an $O(k\n\\log{k})$-competitive deterministic algorithm for the general setting with\nconstant augmentation, and a constant competitive algorithm for the maximum\nmatching variant.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 13:34:01 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2015 17:23:01 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 17:14:11 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 09:30:03 GMT"}, {"version": "v5", "created": "Tue, 1 Oct 2019 21:50:56 GMT"}, {"version": "v6", "created": "Wed, 13 May 2020 21:54:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Avin", "Chen", ""], ["Bienkowski", "Marcin", ""], ["Loukas", "Andreas", ""], ["Pacut", "Maciej", ""], ["Schmid", "Stefan", ""]]}, {"id": "1511.02141", "submitter": "Markus Lohrey", "authors": "Markus Lohrey, Sebastian Maneth, and Carl Philipp Reh", "title": "Traversing Grammar-Compressed Trees with Constant Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grammar-compressed ranked tree is represented with a linear space overhead\nso that a single traversal step, i.e., the move to the parent or the i-th\nchild, can be carried out in constant time. Moreover, we extend our data\nstructure such that equality of subtrees can be checked in constant time.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 16:29:47 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 05:11:53 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lohrey", "Markus", ""], ["Maneth", "Sebastian", ""], ["Reh", "Carl Philipp", ""]]}, {"id": "1511.02163", "submitter": "Jennifer Gillenwater", "authors": "Jennifer Gillenwater, Rishabh Iyer, Bethany Lusch, Rahul Kidambi, Jeff\n  Bilmes", "title": "Submodular Hamming Metrics", "comments": "15 pages, 1 figure, a short version of this will appear in the NIPS\n  2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is a largely unexplored class of functions (positive\npolymatroids) that can define proper discrete metrics over pairs of binary\nvectors and that are fairly tractable to optimize over. By exploiting\nsubmodularity, we are able to give hardness results and approximation\nalgorithms for optimizing over such metrics. Additionally, we demonstrate\nempirically the effectiveness of these metrics and associated algorithms on\nboth a metric minimization task (a form of clustering) and also a metric\nmaximization task (generating diverse k-best lists).\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 17:13:32 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Gillenwater", "Jennifer", ""], ["Iyer", "Rishabh", ""], ["Lusch", "Bethany", ""], ["Kidambi", "Rahul", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1511.02235", "submitter": "Shelby Kimmel", "authors": "Stacey Jeffery and Shelby Kimmel", "title": "NAND-Trees, Average Choice Complexity, and Effective Resistance", "comments": "This article is superseded by arXiv:1704.00765", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the quantum query complexity of evaluating NAND-tree instances\nwith average choice complexity at most $W$ is $O(W)$, where average choice\ncomplexity is a measure of the difficulty of winning the associated two-player\ngame. This generalizes a superpolynomial speedup over classical query\ncomplexity due to Zhan et al. [Zhan et al., ITCS 2012, 249-265]. We further\nshow that the player with a winning strategy for the two-player game associated\nwith the NAND-tree can win the game with an expected\n$\\widetilde{O}(N^{1/4}\\sqrt{{\\cal C}(x)})$ quantum queries against a random\nopponent, where ${\\cal C }(x)$ is the average choice complexity of the\ninstance. This gives an improvement over the query complexity of the naive\nstrategy, which costs $\\widetilde{O}(\\sqrt{N})$ queries.\n  The results rely on a connection between NAND-tree evaluation and\n$st$-connectivity problems on certain graphs, and span programs for\n$st$-connectivity problems. Our results follow from relating average choice\ncomplexity to the effective resistance of these graphs, which itself\ncorresponds to the span program witness size.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 21:01:07 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 13:55:19 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Jeffery", "Stacey", ""], ["Kimmel", "Shelby", ""]]}, {"id": "1511.02296", "submitter": "Zhiyi Huang", "authors": "Nikhil R. Devanur, Zhiyi Huang, Christos-Alexandros Psomas", "title": "The Sample Complexity of Auctions with Side Information", "comments": "A version of this paper appeared in STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the Bayesian optimal auction design problem has been\nconsidered either when the bidder values are i.i.d, or when each bidder is\nindividually identifiable via her value distribution. The latter is a\nreasonable approach when the bidders can be classified into a few categories,\nbut there are many instances where the classification of bidders is a\ncontinuum. For example, the classification of the bidders may be based on their\nannual income, their propensity to buy an item based on past behavior, or in\nthe case of ad auctions, the click through rate of their ads. We introduce an\nalternate model that captures this aspect, where bidders are a priori\nidentical, but can be distinguished based (only) on some side information the\nauctioneer obtains at the time of the auction. We extend the sample complexity\napproach of Dhangwatnotai et al. and Cole and Roughgarden to this model and\nobtain almost matching upper and lower bounds. As an aside, we obtain a revenue\nmonotonicity lemma which may be of independent interest. We also show how to\nuse Empirical Risk Minimization techniques to improve the sample complexity\nbound of Cole and Roughgarden for the non-identical but independent value\ndistribution case.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 03:25:30 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 01:01:06 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 21:55:22 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 14:18:51 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Devanur", "Nikhil R.", ""], ["Huang", "Zhiyi", ""], ["Psomas", "Christos-Alexandros", ""]]}, {"id": "1511.02393", "submitter": "Bojian Xu", "authors": "Bojian Xu", "title": "On Stabbing Queries for Generalized Longest Repeat", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longest repeat query on a string, motivated by its applications in many\nsubfields including computational biology, asks for the longest repetitive\nsubstring(s) covering a particular string position (point query). In this\npaper, we extend the longest repeat query from point query to \\emph{interval\nquery}, allowing the search for longest repeat(s) covering any position\ninterval, and thus significantly improve the usability of the solution. Our\nmethod for interval query takes a different approach using the insight from a\nrecent work on \\emph{shortest unique substrings} [1], as the prior work's\napproach for point query becomes infeasible in the setting of interval query.\nUsing the critical insight from [1], we propose an indexing structure, which\ncan be constructed in the optimal $O(n)$ time and space for a string of size\n$n$, such that any future interval query can be answered in $O(1)$ time.\nFurther, our solution can find \\emph{all} longest repeats covering any given\ninterval using optimal $O(occ)$ time, where $occ$ is the number of longest\nrepeats covering that given interval, whereas the prior $O(n)$-time and space\nwork can find only one candidate for each point query. Experiments with\nreal-world biological data show that our proposal is competitive with prior\nworks, both time and space wise, while providing with the new functionality of\ninterval queries as opposed to point queries provided by prior works.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 20:11:33 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Xu", "Bojian", ""]]}, {"id": "1511.02460", "submitter": "Ken-ichi Kawarabayashi", "authors": "Ken-ichi Kawarabayashi", "title": "Graph Isomorphism for Bounded Genus Graphs In Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every integer $g$, isomorphism of graphs of Euler genus at most $g$ can\nbe decided in linear time.\n  This improves previously known algorithms whose time complexity is $n^{O(g)}$\n(shown in early 1980's), and in fact, this is the first fixed-parameter\ntractable algorithm for the graph isomorphism problem for bounded genus graphs\nin terms of the Euler genus $g$. Our result also generalizes the seminal result\nof Hopcroft and Wong in 1974, which says that the graph isomorphism problem can\nbe decided in linear time for planar graphs.\n  Our proof is quite lengthly and complicated, but if we are satisfied with an\n$O(n^3)$ time algorithm for the same problem, the proof is shorter and easier.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 09:22:59 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1511.02476", "submitter": "Florent Krzakala", "authors": "Lenka Zdeborov\\'a, and Florent Krzakala", "title": "Statistical physics of inference: Thresholds and algorithms", "comments": "86 pages, 16 Figures. Review article based on HDR thesis of the first\n  author and lecture notes of the second", "journal-ref": "Advances in Physics Volume 65, 2016 - Issue 5", "doi": "10.1080/00018732.2016.1211393", "report-no": null, "categories": "cond-mat.stat-mech cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many questions of fundamental interest in todays science can be formulated as\ninference problems: Some partial, or noisy, observations are performed over a\nset of variables and the goal is to recover, or infer, the values of the\nvariables based on the indirect information contained in the measurements. For\nsuch problems, the central scientific questions are: Under what conditions is\nthe information contained in the measurements sufficient for a satisfactory\ninference to be possible? What are the most efficient algorithms for this task?\nA growing body of work has shown that often we can understand and locate these\nfundamental barriers by thinking of them as phase transitions in the sense of\nstatistical physics. Moreover, it turned out that we can use the gained\nphysical insight to develop new promising algorithms. Connection between\ninference and statistical physics is currently witnessing an impressive\nrenaissance and we review here the current state-of-the-art, with a pedagogical\nfocus on the Ising model which formulated as an inference problem we call the\nplanted spin glass. In terms of applications we review two classes of problems:\n(i) inference of clusters on graphs and networks, with community detection as a\nspecial case and (ii) estimating a signal from its noisy linear measurements,\nwith compressed sensing as a case of sparse estimation. Our goal is to provide\na pedagogical review for researchers in physics and other fields interested in\nthis fascinating topic.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 12:36:23 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 21:37:44 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 01:27:30 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 12:40:25 GMT"}, {"version": "v5", "created": "Mon, 22 Jan 2018 19:41:41 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Zdeborov\u00e1", "Lenka", ""], ["Krzakala", "Florent", ""]]}, {"id": "1511.02484", "submitter": "Stephen Chestnut", "authors": "Stephen R. Chestnut and Rico Zenklusen", "title": "Interdicting Structured Combinatorial Optimization Problems with\n  {0,1}-Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interdiction problems ask about the worst-case impact of a limited change to\nan underlying optimization problem. They are a natural way to measure the\nrobustness of a system, or to identify its weakest spots. Interdiction problems\nhave been studied for a wide variety of classical combinatorial optimization\nproblems, including maximum $s$-$t$ flows, shortest $s$-$t$ paths, maximum\nweight matchings, minimum spanning trees, maximum stable sets, and graph\nconnectivity. Most interdiction problems are NP-hard, and furthermore, even\ndesigning efficient approximation algorithms that allow for estimating the\norder of magnitude of a worst-case impact, has turned out to be very difficult.\nNot very surprisingly, the few known approximation algorithms are heavily\ntailored for specific problems.\n  Inspired by an approach of Burch et al. (2003), we suggest a general method\nto obtain pseudoapproximations for many interdiction problems. More precisely,\nfor any $\\alpha>0$, our algorithm will return either a\n$(1+\\alpha)$-approximation, or a solution that may overrun the interdiction\nbudget by a factor of at most $1+\\alpha^{-1}$ but is also at least as good as\nthe optimal solution that respects the budget. Furthermore, our approach can\nhandle submodular interdiction costs when the underlying problem is to find a\nmaximum weight independent set in a matroid, as for example the maximum weight\nforest problem. The approach can sometimes be refined by exploiting additional\nstructural properties of the underlying optimization problem to obtain stronger\nresults. We demonstrate this by presenting a PTAS for interdicting $b$-stable\nsets in bipartite graphs.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 13:32:42 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Chestnut", "Stephen R.", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1511.02486", "submitter": "Stephen Chestnut", "authors": "Stephen R. Chestnut and Rico Zenklusen", "title": "Hardness and Approximation for Network Flow Interdiction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Network Flow Interdiction problem an adversary attacks a network in\norder to minimize the maximum s-t-flow. Very little is known about the\napproximatibility of this problem despite decades of interest in it. We present\nthe first approximation hardness, showing that Network Flow Interdiction and\nseveral of its variants cannot be much easier to approximate than Densest\nk-Subgraph. In particular, any $n^{o(1)}$-approximation algorithm for Network\nFlow Interdiction would imply an $n^{o(1)}$-approximation algorithm for Densest\nk-Subgraph. We complement this hardness results with the first approximation\nalgorithm for Network Flow Interdiction, which has approximation ratio 2(n-1).\nWe also show that Network Flow Interdiction is essentially the same as the\nBudgeted Minimum s-t-Cut problem, and transferring our results gives the first\napproximation hardness and algorithm for that problem, as well.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 14:02:04 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Chestnut", "Stephen R.", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1511.02513", "submitter": "Thomas Steinke", "authors": "Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer,\n  Jonathan Ullman", "title": "Algorithmic Stability for Adaptive Data Analysis", "comments": "This work unifies and subsumes the two arXiv manuscripts\n  arXiv:1503.04843 and arXiv:1504.05800", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptivity is an important feature of data analysis---the choice of questions\nto ask about a dataset often depends on previous interactions with the same\ndataset. However, statistical validity is typically studied in a nonadaptive\nmodel, where all questions are specified before the dataset is drawn. Recent\nwork by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiated\nthe formal study of this problem, and gave the first upper and lower bounds on\nthe achievable generalization error for adaptive data analysis.\n  Specifically, suppose there is an unknown distribution $\\mathbf{P}$ and a set\nof $n$ independent samples $\\mathbf{x}$ is drawn from $\\mathbf{P}$. We seek an\nalgorithm that, given $\\mathbf{x}$ as input, accurately answers a sequence of\nadaptively chosen queries about the unknown distribution $\\mathbf{P}$. How many\nsamples $n$ must we draw from the distribution, as a function of the type of\nqueries, the number of queries, and the desired level of accuracy?\n  In this work we make two new contributions:\n  (i) We give upper bounds on the number of samples $n$ that are needed to\nanswer statistical queries. The bounds improve and simplify the work of Dwork\net al. (STOC, 2015), and have been applied in subsequent work by those authors\n(Science, 2015, NIPS, 2015).\n  (ii) We prove the first upper bounds on the number of samples required to\nanswer more general families of queries. These include arbitrary\nlow-sensitivity queries and an important class of optimization queries.\n  As in Dwork et al., our algorithms are based on a connection with algorithmic\nstability in the form of differential privacy. We extend their work by giving a\nquantitatively optimal, more general, and simpler proof of their main theorem\nthat stability implies low generalization error. We also study weaker stability\nguarantees such as bounded KL divergence and total variation distance.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 18:26:50 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bassily", "Raef", ""], ["Nissim", "Kobbi", ""], ["Smith", "Adam", ""], ["Steinke", "Thomas", ""], ["Stemmer", "Uri", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1511.02525", "submitter": "Jukka Suomela", "authors": "Alon Efrat, S\\'andor P. Fekete, Joseph S. B. Mitchell, Valentin\n  Polishchuk, Jukka Suomela", "title": "Improved Approximation Algorithms for Relay Placement", "comments": "1+29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the relay placement problem the input is a set of sensors and a number $r\n\\ge 1$, the communication range of a relay. In the one-tier version of the\nproblem the objective is to place a minimum number of relays so that between\nevery pair of sensors there is a path through sensors and/or relays such that\nthe consecutive vertices of the path are within distance $r$ if both vertices\nare relays and within distance 1 otherwise. The two-tier version adds the\nrestrictions that the path must go through relays, and not through sensors. We\npresent a 3.11-approximation algorithm for the one-tier version and a PTAS for\nthe two-tier version. We also show that the one-tier version admits no PTAS,\nassuming P $\\ne$ NP.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 20:58:12 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Efrat", "Alon", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Mitchell", "Joseph S. B.", ""], ["Polishchuk", "Valentin", ""], ["Suomela", "Jukka", ""]]}, {"id": "1511.02591", "submitter": "R. Krithika", "authors": "R. Krithika and N. S. Narayanaswamy", "title": "Faster Randomized Branching Algorithms for $r$-SAT", "comments": "This paper has been withdrawn due to a gap in the algorithm analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of determining if an $r$-CNF boolean formula $F$ over $n$\nvariables is satisifiable reduces to the problem of determining if $F$ has a\nsatisfying assignment with a Hamming distance of at most $d$ from a fixed\nassignment $\\alpha$. This problem is also a very important subproblem in\nSchoning's local search algorithm for $r$-SAT. While Schoning described a\nrandomized algorithm solves this subproblem in $O((r-1)^d)$ time, Dantsin et\nal. presented a deterministic branching algorithm with $O^*(r^d)$ running time.\nIn this paper we present a simple randomized branching algorithm that runs in\ntime $O^*({(\\frac{r+1}{2})}^d)$. As a consequence we get a randomized algorithm\nfor $r$-SAT that runs in $O^*({(\\frac{2(r+1)}{r+3})}^n)$ time. This algorithm\nmatches the running time of Schoning's algorithm for 3-SAT and is an\nimprovement over Schoning's algorithm for all $r \\geq 4$.\n  For $r$-uniform hitting set parameterized by solution size $k$, we describe a\nrandomized FPT algorithm with a running time of $O^*({(\\frac{r+1}{2})}^k)$. For\nthe above LP guarantee parameterization of vertex cover, we have a randomized\nFPT algorithm to find a vertex cover of size $k$ in a running time of\n$O^*(2.25^{k-vc^*})$, where $vc^*$ is the LP optimum of the natural LP\nrelaxation of vertex cover. In both the cases, these randomized algorithms have\na better running time than the current best deterministic algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 07:45:16 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 13:14:16 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 05:21:41 GMT"}, {"version": "v4", "created": "Sun, 6 Mar 2016 05:37:52 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Krithika", "R.", ""], ["Narayanaswamy", "N. S.", ""]]}, {"id": "1511.02595", "submitter": "Cong Xie", "authors": "Cong Xie, Wu-Jun Li and Zhihua Zhang", "title": "A New Relaxation Approach to Normalized Hypergraph Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized graph cut (NGC) has become a popular research topic due to its\nwide applications in a large variety of areas like machine learning and very\nlarge scale integration (VLSI) circuit design. Most of traditional NGC methods\nare based on pairwise relationships (similarities). However, in real-world\napplications relationships among the vertices (objects) may be more complex\nthan pairwise, which are typically represented as hyperedges in hypergraphs.\nThus, normalized hypergraph cut (NHC) has attracted more and more attention.\nExisting NHC methods cannot achieve satisfactory performance in real\napplications. In this paper, we propose a novel relaxation approach, which is\ncalled relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as an\noptimization problem on the Stiefel manifold. To solve this problem, we resort\nto the Cayley transformation to devise a feasible learning algorithm.\nExperimental results on a set of large hypergraph benchmarks for clustering and\npartitioning in VLSI domain show that RNHC can outperform the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 08:30:03 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Xie", "Cong", ""], ["Li", "Wu-Jun", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1511.02599", "submitter": "Erel Segal-Halevi", "authors": "Erel Segal-Halevi and Avinatan Hassidim and Yonatan Aumann", "title": "Waste Makes Haste: Bounded Time Protocols for Envy-Free Cake Cutting\n  with Free Disposal", "comments": "The first version was presented at AAMAS 2015:\n  http://dl.acm.org/citation.cfm?id=2773268 . The current version is\n  substantially revised and extended", "journal-ref": "Published in ACM Transactions on Algorithms (TALG), Volume 13,\n  Issue 1, December 2016", "doi": "10.1145/2988232", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of envy-free division of a heterogeneous good\n(\"cake\") among several agents. It is known that, when the allotted pieces must\nbe connected, the problem cannot be solved by a finite algorithm for 3 or more\nagents. The impossibility result, however, assumes that the entire cake must be\nallocated. In this paper we replace the entire-allocation requirement with a\nweaker \\emph{partial-proportionality} requirement: the piece given to each\nagent must be worth for it at least a certain positive fraction of the entire\ncake value. We prove that this version of the problem is solvable in bounded\ntime even when the pieces must be connected. We present simple, bounded-time\nenvy-free cake-cutting algorithms for: (1) giving each of $n$ agents a\nconnected piece with a positive value; (2) giving each of 3 agents a connected\npiece worth at least 1/3; (3) giving each of 4 agents a connected piece worth\nat least 1/7; (4) giving each of 4 agents a disconnected piece worth at least\n1/4; (5) giving each of $n$ agents a disconnected piece worth at least\n$(1-\\epsilon)/n$ for any positive $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 09:00:47 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 17:38:37 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 10:51:55 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Segal-Halevi", "Erel", ""], ["Hassidim", "Avinatan", ""], ["Aumann", "Yonatan", ""]]}, {"id": "1511.02612", "submitter": "Tomasz Kociumaka", "authors": "Pawe{\\l} Gawrychowski, Adam Karczmarz, Tomasz Kociumaka, Jakub\n  {\\L}\\k{a}cki, Piotr Sankowski", "title": "Optimal Dynamic Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the fundamental problem of maintaining a dynamic\ncollection of strings under the following operations: concat - concatenates two\nstrings, split - splits a string into two at a given position, compare - finds\nthe lexicographical order (less, equal, greater) between two strings, LCP -\ncalculates the longest common prefix of two strings. We present an efficient\ndata structure for this problem, where an update requires only $O(\\log n)$\nworst-case time with high probability, with $n$ being the total length of all\nstrings in the collection, and a query takes constant worst-case time. On the\nlower bound side, we prove that even if the only possible query is checking\nequality of two strings, either updates or queries take amortized $\\Omega(\\log\nn)$ time; hence our implementation is optimal.\n  Such operations can be used as a basic building block to solve other string\nproblems. We provide two examples. First, we can augment our data structure to\nprovide pattern matching queries that may locate occurrences of a specified\npattern $p$ in the strings in our collection in optimal $O(|p|)$ time, at the\nexpense of increasing update time to $O(\\log^2 n)$. Second, we show how to\nmaintain a history of an edited text, processing updates in $O(\\log t \\log \\log\nt)$ time, where $t$ is the number of edits, and how to support pattern matching\nqueries against the whole history in $O(|p| \\log t \\log \\log t)$ time.\n  Finally, we note that our data structure can be applied to test dynamic tree\nisomorphism and to compare strings generated by dynamic straight-line grammars.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 09:38:47 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 07:57:20 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Karczmarz", "Adam", ""], ["Kociumaka", "Tomasz", ""], ["\u0141\u0105cki", "Jakub", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1511.02650", "submitter": "Jan-Thierry Wegener", "authors": "Sahar Bsaybes, Alain Quilliot, Annegret K. Wagler, Jan-Thierry Wegener", "title": "Two Flow-Based Approaches for the Static Relocation Problem in\n  Carsharing Systems", "comments": "12 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a carsharing system, a fleet of cars is distributed at stations in an\nurban area, customers can take and return cars at any time and station. For\noperating such a system in a satisfactory way, the stations have to keep a good\nratio between the numbers of free places and cars in each station. This leads\nto the problem of relocating cars between stations, which can be modeled within\nthe framework of a metric task system. In this paper, we focus on the Static\nRelocation Problem, where the system has to be set into a certain state,\noutgoing from the current state. We present two approaches to solve this\nproblem, a fast heuristic approach and an exact integer programming based\nmethod using flows in time-expanded networks, and provide some computational\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 11:57:16 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bsaybes", "Sahar", ""], ["Quilliot", "Alain", ""], ["Wagler", "Annegret K.", ""], ["Wegener", "Jan-Thierry", ""]]}, {"id": "1511.02667", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen, Francesco Tudisco, Antoine Gautier, Matthias Hein", "title": "An Efficient Multilinear Optimization Framework for Hypergraph Matching", "comments": "accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (PAMI) 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2574706", "report-no": null, "categories": "cs.CV cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph matching has recently become a popular approach for solving\ncorrespondence problems in computer vision as it allows to integrate\nhigher-order geometric information. Hypergraph matching can be formulated as a\nthird-order optimization problem subject to the assignment constraints which\nturns out to be NP-hard. In recent work, we have proposed an algorithm for\nhypergraph matching which first lifts the third-order problem to a fourth-order\nproblem and then solves the fourth-order problem via optimization of the\ncorresponding multilinear form. This leads to a tensor block coordinate ascent\nscheme which has the guarantee of providing monotonic ascent in the original\nmatching score function and leads to state-of-the-art performance both in terms\nof achieved matching score and accuracy. In this paper we show that the lifting\nstep to a fourth-order problem can be avoided yielding a third-order scheme\nwith the same guarantees and performance but being two times faster. Moreover,\nwe introduce a homotopy type method which further improves the performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 13:15:10 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 19:06:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nguyen", "Quynh", ""], ["Tudisco", "Francesco", ""], ["Gautier", "Antoine", ""], ["Hein", "Matthias", ""]]}, {"id": "1511.02751", "submitter": "Jan-Thierry Wegener", "authors": "Sahar Bsaybes, Sven O. Krumke, Alain Quilliot, Annegret K. Wagler,\n  Jan-Thierry Wegener", "title": "ReOpt: an Algorithm with a Quality Guaranty for Solving the Static\n  Relocation Problem", "comments": "39 pages, 8 figures, 2 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a carsharing system, a fleet of cars is distributed at stations in an\nurban area, customers can take and return cars at any time and station. For\noperating such a system in a satisfactory way, the stations have to keep a good\nratio between the numbers of free places and cars in each station. This leads\nto the problem of relocating cars between stations, which can be modeled within\nthe framework of a metric task system. In this paper, we focus on the Static\nRelocation Problem, where the system has to be set into a certain state,\noutgoing from the current state. We present a combinatorial approach and\nprovide approximation factors for several different situations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 16:39:51 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bsaybes", "Sahar", ""], ["Krumke", "Sven O.", ""], ["Quilliot", "Alain", ""], ["Wagler", "Annegret K.", ""], ["Wegener", "Jan-Thierry", ""]]}, {"id": "1511.02786", "submitter": "Alireza Rezaei", "authors": "Shayan Oveis Gharan, Alireza Rezaei", "title": "Approximation Algorithms for Finding Maximum Induced Expanders", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of approximating the largest induced expander in a\ngiven graph $G$. Given a $\\Delta$-regular graph $G$ with $n$ vertices, the goal\nis to find the set with the largest induced expansion of size at least $\\delta\n\\cdot n$. We design a bi-criteria approximation algorithm for this problem; if\nthe optimum has induced spectral expansion $\\lambda$ our algorithm returns a\n$\\frac{\\lambda}{\\log^2\\delta \\exp(\\Delta/\\lambda)}$-(spectral) expander of size\nat least $\\delta n$ (up to constants).\n  Our proof introduces and employs a novel semidefinite programming relaxation\nfor the largest induced expander problem. We expect to see further applications\nof our SDP relaxation in graph partitioning problems. In particular, because of\nthe close connection to the small set expansion problem, one may be able to\nobtain new insights into the unique games problem.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 17:56:29 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1511.02801", "submitter": "Du\\v{s}an Knop", "authors": "Du\\v{s}an Knop and Pavel Dvo\\v{r}\\'ak", "title": "Parameterized complexity of length-bounded cuts and multi-cuts", "comments": "20 pages, 7 figures, TAMC 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Minimal Length-Bounded L-But problem can be computed in\nlinear time with respect to L and the tree-width of the input graph as\nparameters. In this problem the task is to find a set of edges of a graph such\nthat after removal of this set, the shortest path between two prescribed\nvertices is at least L long. We derive an FPT algorithm for a more general\nmulti-commodity length bounded cut problem when parameterized by the number of\nterminals also.\n  For the former problem we show a W[1]-hardness result when the\nparameterization is done by the path-width only (instead of the tree-width) and\nthat this problem does not admit polynomial kernel when parameterized by\ntree-width and L. We also derive an FPT algorithm for the Minimal\nLength-Bounded Cut problem when parameterized by the tree-depth. Thus showing\nan interesting paradigm for this problem and parameters tree-depth and\npath-width.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:53:32 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Dvo\u0159\u00e1k", "Pavel", ""]]}, {"id": "1511.02913", "submitter": "Nikos Parotsidis", "authors": "Loukas Georgiadis, Giuseppe F. Italiano, Nikos Parotsidis", "title": "Strong Connectivity in Directed Graphs under Failures, with Application", "comments": "An extended abstract of this work appeared in the SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate some basic connectivity problems in directed\ngraphs (digraphs). Let $G$ be a digraph with $m$ edges and $n$ vertices, and\nlet $G\\setminus e$ be the digraph obtained after deleting edge $e$ from $G$. As\na first result, we show how to compute in $O(m+n)$ worst-case time: $(i)$ The\ntotal number of strongly connected components in $G\\setminus e$, for all edges\n$e$ in $G$. $(ii)$ The size of the largest and of the smallest strongly\nconnected components in $G\\setminus e$, for all edges $e$ in $G$.\n  Let $G$ be strongly connected. We say that edge $e$ separates two vertices\n$x$ and $y$, if $x$ and $y$ are no longer strongly connected in $G\\setminus e$.\nAs a second set of results, we show how to build in $O(m+n)$ time $O(n)$-space\ndata structures that can answer in optimal time the following basic\nconnectivity queries on digraphs: $(i)$ Report in $O(n)$ worst-case time all\nthe strongly connected components of $G\\setminus e$, for a query edge $e$.\n$(ii)$ Test whether an edge separates two query vertices in $O(1)$ worst-case\ntime. $(iii)$ Report all edges that separate two query vertices in optimal\nworst-case time, i.e., in time $O(k)$, where $k$ is the number of separating\nedges. (For $k=0$, the time is $O(1)$).\n  All of the above results extend to vertex failures. All our bounds are tight\nand are obtained with a common algorithmic framework, based on a novel compact\nrepresentation of the decompositions induced by the $1$-connectivity (i.e.,\n$1$-edge and $1$-vertex) cuts in digraphs, which might be of independent\ninterest. With the help of our data structures we can design efficient\nalgorithms for several other connectivity problems on digraphs and we can also\nobtain in linear time a strongly connected spanning subgraph of $G$ with $O(n)$\nedges that maintains the $1$-connectivity cuts of $G$ and the decompositions\ninduced by those cuts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:20:08 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 19:38:27 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1511.03137", "submitter": "Sebastian Schlag", "authors": "Sebastian Schlag, Vitali Henne, Tobias Heuer, Henning Meyerhenke,\n  Peter Sanders, Christian Schulz", "title": "k-way Hypergraph Partitioning via n-Level Recursive Bisection", "comments": "arXiv admin note: text overlap with arXiv:1505.00693", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multilevel algorithm for hypergraph partitioning that contracts\nthe vertices one at a time. Using several caching and lazy-evaluation\ntechniques during coarsening and refinement, we reduce the running time by up\nto two-orders of magnitude compared to a naive $n$-level algorithm that would\nbe adequate for ordinary graph partitioning. The overall performance is even\nbetter than the widely used hMetis hypergraph partitioner that uses a classical\nmultilevel algorithm with few levels. Aided by a portfolio-based approach to\ninitial partitioning and adaptive budgeting of imbalance within recursive\nbipartitioning, we achieve very high quality. We assembled a large benchmark\nset with 310 hypergraphs stemming from application areas such VLSI, SAT\nsolving, social networks, and scientific computing. We achieve significantly\nsmaller cuts than hMetis and PaToH, while being faster than hMetis.\nConsiderably larger improvements are observed for some instance classes like\nsocial networks, for bipartitioning, and for partitions with an allowed\nimbalance of 10%. The algorithm presented in this work forms the basis of our\nhypergraph partitioning framework KaHyPar (Karlsruhe Hypergraph Partitioning).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 15:29:19 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Schlag", "Sebastian", ""], ["Henne", "Vitali", ""], ["Heuer", "Tobias", ""], ["Meyerhenke", "Henning", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1511.03148", "submitter": "Lu\\'is M. S. Russo", "authors": "Lu\\'is M. S. Russo", "title": "A Study on Splay Trees", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Theor. Comput. Sci. 776: 1-18 (2019)", "doi": "10.1016/j.tcs.2018.12.020", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamic optimality conjecture, which predicts that splay trees\nare a form of universally efficient binary search tree, for any access\nsequence. We reduce this claim to a regular access bound, which seems plausible\nand might be easier to prove. This approach may be useful to establish dynamic\noptimality.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 15:38:45 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 18:33:21 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 13:37:03 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Russo", "Lu\u00eds M. S.", ""]]}, {"id": "1511.03186", "submitter": "Anand Louis", "authors": "Anand Louis, Santosh S. Vempala", "title": "Accelerated Newton Iteration: Roots of Black Box Polynomials and Matrix\n  Eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing the largest root of a real rooted\npolynomial $p(x)$ to within error $\\varepsilon $ given only black box access to\nit, i.e., for any $x \\in {\\mathbb R}$, the algorithm can query an oracle for\nthe value of $p(x)$, but the algorithm is not allowed access to the\ncoefficients of $p(x)$. A folklore result for this problem is that the largest\nroot of a polynomial can be computed in $O(n \\log (1/\\varepsilon ))$ polynomial\nqueries using the Newton iteration. We give a simple algorithm that queries the\noracle at only $O(\\log n \\log(1/\\varepsilon ))$ points, where $n$ is the degree\nof the polynomial. Our algorithm is based on a novel approach for accelerating\nthe Newton method by using higher derivatives.\n  As a special case, we consider the problem of computing the top eigenvalue of\na symmetric matrix in ${\\mathbb Q}^{n \\times n}$ to within error $\\varepsilon $\nin time polynomial in the input description, i.e., the number of bits to\ndescribe the matrix and $\\log(1/\\varepsilon )$. Well-known methods such as the\npower iteration and Lanczos iteration incur running time polynomial in\n$1/\\varepsilon $, while Gaussian elimination takes $\\Omega(n^4)$ bit\noperations. As a corollary of our main result, we obtain a\n$\\tilde{O}(n^{\\omega} \\log^2 ( ||A||_F/\\varepsilon ))$ bit complexity algorithm\nto compute the top eigenvalue of the matrix $A$ or to check if it is\napproximately PSD ($A \\succeq -\\varepsilon I$).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 17:07:35 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 23:24:16 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Louis", "Anand", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1511.03229", "submitter": "Aravindan Vijayaraghavan", "authors": "Konstantin Makarychev, Yury Makarychev and Aravindan Vijayaraghavan", "title": "Learning Communities in the Presence of Errors", "comments": "34 pages. Appearing in the Conference on Learning Theory (COLT)'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning communities in the presence of modeling\nerrors and give robust recovery algorithms for the Stochastic Block Model\n(SBM). This model, which is also known as the Planted Partition Model, is\nwidely used for community detection and graph partitioning in various fields,\nincluding machine learning, statistics, and social sciences. Many algorithms\nexist for learning communities in the Stochastic Block Model, but they do not\nwork well in the presence of errors.\n  In this paper, we initiate the study of robust algorithms for partial\nrecovery in SBM with modeling errors or noise. We consider graphs generated\naccording to the Stochastic Block Model and then modified by an adversary. We\nallow two types of adversarial errors, Feige---Kilian or monotone errors, and\nedge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question\nabout whether an almost exact recovery is possible when the adversary is\nallowed to add $o(n)$ edges. Our work answers this question affirmatively even\nin the case of $k>2$ communities.\n  We then show that our algorithms work not only when the instances come from\nSBM, but also work when the instances come from any distribution of graphs that\nis $\\epsilon m$ close to SBM in the Kullback---Leibler divergence. This result\nalso works in the presence of adversarial errors. Finally, we present almost\ntight lower bounds for two communities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 19:03:47 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 18:51:52 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 04:02:00 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1511.03333", "submitter": "Jinyu Xie", "authors": "Xi Chen and Jinyu Xie", "title": "Tight Bounds for the Distribution-Free Testing of Monotone Conjunctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve both upper and lower bounds for the distribution-free testing of\nmonotone conjunctions.\n  Given oracle access to an unknown Boolean function $f:\\{0,1\\}^n \\rightarrow\n\\{0,1\\}$ and sampling oracle access to an unknown distribution $\\mathcal{D}$\nover $\\{0,1\\}^n$, we present an $\\tilde{O}(n^{1/3}/\\epsilon^5)$-query algorithm\nthat tests whether $f$ is a monotone conjunction versus $\\epsilon$-far from any\nmonotone conjunction with respect to $\\mathcal{D}$. This improves the previous\nbest upper bound of $\\tilde{O}(n^{1/2}/\\epsilon)$ by Dolev and Ron when\n$1/\\epsilon$ is small compared to $n$.\n  For some constant $\\epsilon_0>0$, we also prove a lower bound of\n$\\tilde{\\Omega}(n^{1/3})$ for the query complexity, improving the previous best\nlower bound of $\\tilde{\\Omega}(n^{1/5})$ by Glasner and Servedio.\n  Our upper and lower bounds are tight, up to a poly-logarithmic factor, when\nthe distance parameter $\\epsilon$ is a constant. Furthermore, the same upper\nand lower bounds can be extended to the distribution-free testing of general\nconjunctions, and the lower bound can be extended to that of decision lists and\nlinear threshold functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 23:09:47 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Chen", "Xi", ""], ["Xie", "Jinyu", ""]]}, {"id": "1511.03403", "submitter": "Shmuel Onn", "authors": "Shmuel Onn", "title": "Huge tables and multicommodity flows are fixed parameter tractable via\n  unimodular integer Caratheodory", "comments": null, "journal-ref": "Journal of Computer and System Sciences, 83: 207-214 (2017)", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-way table problem is to decide if there exists an l x m x n table\nsatisfying given line sums, and find a table if there is one. It is NP-complete\nalready for l=3 and every bounded integer program can be isomorphically\nrepresented in polynomial time for some m and n as some 3 x m x n table\nproblem. Recently, the problem was shown to be fixed-parameter tractable with\nparameters l,m. Here we extend this and show that the huge version of the\nproblem, where the variable side n is a huge number encoded in binary, is also\nfixed-parameter tractable with parameters l,m. We also conclude that the huge\nmulticommodity flow problem with m suppliers and a huge number n of consumers\nis fixed-parameter tractable parameterized by the numbers of commodities and\nconsumer types.\n  One of our tools is a theorem about unimodular monoids which is of interest\non its own right. The monoid problem is to decide if a given integer vector is\na finite nonnegative integer combination of a given set of integer vectors, and\nfind such a decomposition if one exists. We consider sets given implicitly by\nan inequality system. For such sets, it was recently shown that in fixed\ndimension the problem is solvable in polynomial time with degree which is\nexponential in the dimension. Here we show that when the inequality system\nwhich defines the set is defined by a totally unimodular matrix, the monoid\nproblem can be solved in polynomial time even in variable dimension.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 07:10:10 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 10:45:51 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2016 13:53:30 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Onn", "Shmuel", ""]]}, {"id": "1511.03407", "submitter": "Aymeric Grodet", "authors": "Aymeric Grodet and Takuya Tsuchiya", "title": "Reorganizing topologies of Steiner trees to accelerate their elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique to reorganize topologies of Steiner trees by\nexchanging neighbors of adjacent Steiner points. We explain how to use the\nsystematic way of building trees, and therefore topologies, to find the correct\ntopology after nodes have been exchanged. Topology reorganizations can be\ninserted into the enumeration scheme commonly used by exact algorithms for the\nEuclidean Steiner tree problem in $d$-space, providing a method of improvement\ndifferent than the usual approaches. As an example, we show how topology\nreorganizations can be used to dynamically change the exploration of the usual\nbranch-and-bound tree when two Steiner points collide during the optimization\nprocess. We also turn our attention to the erroneous use of a pre-optimization\nlower bound in the original algorithm and give an example to confirm its usage\nis incorrect. In order to provide numerical results on correct solutions, we\nuse planar equilateral points to quickly compute this lower bound, even in\ndimensions higher than two. Finally, we describe planar twin trees, identical\ntrees yielded by different topologies, whose generalization to higher\ndimensions could open a new way of building Steiner trees.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 07:47:23 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 07:35:50 GMT"}, {"version": "v3", "created": "Thu, 29 Jun 2017 10:56:29 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2018 11:08:07 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Grodet", "Aymeric", ""], ["Tsuchiya", "Takuya", ""]]}, {"id": "1511.03592", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "The Fourier Transform of Poisson Multinomial Distributions and its\n  Algorithmic Applications", "comments": "68 pages, full version of STOC 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable of\nthe form $X = \\sum_{i=1}^n X_i$, where the $X_i$'s are independent random\nvectors supported on the set of standard basis vectors in $\\mathbb{R}^k.$ In\nthis paper, we obtain a refined structural understanding of PMDs by analyzing\ntheir Fourier transform. As our core structural result, we prove that the\nFourier transform of PMDs is {\\em approximately sparse}, i.e., roughly\nspeaking, its $L_1$-norm is small outside a small set. By building on this\nresult, we obtain the following applications:\n  {\\bf Learning Theory.} We design the first computationally efficient learning\nalgorithm for PMDs with respect to the total variation distance. Our algorithm\nlearns an arbitrary $(n, k)$-PMD within variation distance $\\epsilon$ using a\nnear-optimal sample size of $\\widetilde{O}_k(1/\\epsilon^2),$ and runs in time\n$\\widetilde{O}_k(1/\\epsilon^2) \\cdot \\log n.$ Previously, no algorithm with a\n$\\mathrm{poly}(1/\\epsilon)$ runtime was known, even for $k=3.$\n  {\\bf Game Theory.} We give the first efficient polynomial-time approximation\nscheme (EPTAS) for computing Nash equilibria in anonymous games. For normalized\nanonymous games with $n$ players and $k$ strategies, our algorithm computes a\nwell-supported $\\epsilon$-Nash equilibrium in time $n^{O(k^3)} \\cdot\n(k/\\epsilon)^{O(k^3\\log(k/\\epsilon)/\\log\\log(k/\\epsilon))^{k-1}}.$ The best\nprevious algorithm for this problem had running time $n^{(f(k)/\\epsilon)^k},$\nwhere $f(k) = \\Omega(k^{k^2})$, for any $k>2.$\n  {\\bf Statistics.} We prove a multivariate central limit theorem (CLT) that\nrelates an arbitrary PMD to a discretized multivariate Gaussian with the same\nmean and covariance, in total variation distance. Our new CLT strengthens the\nCLT of Valiant and Valiant by completely removing the dependence on $n$ in the\nerror bound.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 18:00:37 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 19:42:04 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1511.03641", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis, Anindya De, Gautam Kamath, Christos Tzamos", "title": "A Size-Free CLT for Poisson Multinomials and its Applications", "comments": "To appear in STOC 2016", "journal-ref": null, "doi": "10.1145/2897518.2897519", "report-no": null, "categories": "cs.DS cs.GT cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We show\nthat any $(n,k)$-PMD is ${\\rm poly}\\left({k\\over \\sigma}\\right)$-close in total\nvariation distance to the (appropriately discretized) multi-dimensional\nGaussian with the same first two moments, removing the dependence on $n$ from\nthe Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT is\nobtained by bootstrapping the Valiant-Valiant CLT itself through the structural\ncharacterization of PMDs shown in recent work by Daskalakis, Kamath, and\nTzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTAS\nfor approximate Nash equilibria in anonymous games, significantly improving the\nstate of the art, and matching qualitatively the running time dependence on $n$\nand $1/\\varepsilon$ of the best known algorithm for two-strategy anonymous\ngames. Our new CLT also enables the construction of covers for the set of\n$(n,k)$-PMDs, which are proper and whose size is shown to be essentially\noptimal. Our cover construction combines our CLT with the Shapley-Folkman\ntheorem and recent sparsification results for Laplacian matrices by Batson,\nSpielman, and Srivastava. Our cover size lower bound is based on an algebraic\ngeometric construction. Finally, leveraging the structural properties of the\nFourier spectrum of PMDs we show that these distributions can be learned from\n$O_k(1/\\varepsilon^2)$ samples in ${\\rm poly}_k(1/\\varepsilon)$-time, removing\nthe quasi-polynomial dependence of the running time on $1/\\varepsilon$ from the\nalgorithm of Daskalakis, Kamath, and Tzamos.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 20:27:33 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 05:02:43 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["De", "Anindya", ""], ["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}, {"id": "1511.03774", "submitter": "Lijie Chen", "authors": "Lijie Chen, Jian Li", "title": "On the Optimal Sample Complexity for Best Arm Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the best arm identification (BEST-1-ARM) problem, which is defined\nas follows. We are given $n$ stochastic bandit arms. The $i$th arm has a reward\ndistribution $D_i$ with an unknown mean $\\mu_{i}$. Upon each play of the $i$th\narm, we can get a reward, sampled i.i.d. from $D_i$. We would like to identify\nthe arm with the largest mean with probability at least $1-\\delta$, using as\nfew samples as possible. We provide a nontrivial algorithm for BEST-1-ARM,\nwhich improves upon several prior upper bounds on the same problem. We also\nstudy an important special case where there are only two arms, which we call\nthe sign problem. We provide a new lower bound of sign, simplifying and\nsignificantly extending a classical result by Farrell in 1964, with a\ncompletely new proof. Using the new lower bound for sign, we obtain the first\nlower bound for BEST-1-ARM that goes beyond the classic Mannor-Tsitsiklis lower\nbound, by an interesting reduction from Sign to BEST-1-ARM. We propose an\ninteresting conjecture concerning the optimal sample complexity of BEST-1-ARM\nfrom the perspective of instance-wise optimality.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 04:49:46 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 05:47:39 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 18:05:29 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chen", "Lijie", ""], ["Li", "Jian", ""]]}, {"id": "1511.03803", "submitter": "Weijie Su", "authors": "Cynthia Dwork and Weijie Su and Li Zhang", "title": "Private False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first differentially private algorithms for controlling the\nfalse discovery rate (FDR) in multiple hypothesis testing, with essentially no\nloss in power under certain conditions. Our general approach is to adapt a\nwell-known variant of the Benjamini-Hochberg procedure (BHq), making each step\ndifferentially private. This destroys the classical proof of FDR control. To\nprove FDR control of our method, (a) we develop a new proof of the original\n(non-private) BHq algorithm and its robust variants -- a proof requiring only\nthe assumption that the true null test statistics are independent, allowing for\narbitrary correlations between the true nulls and false nulls. This assumption\nis fairly weak compared to those previously shown in the vast literature on\nthis topic, and explains in part the empirical robustness of BHq. Then (b) we\nrelate the FDR control properties of the differentially private version to the\ncontrol properties of the non-private version. \\end{enumerate} We also present\na low-distortion \"one-shot\" differentially private primitive for \"top $k$\"\nproblems, e.g., \"Which are the $k$ most popular hobbies?\" (which we apply to:\n\"Which hypotheses have the $k$ most significant $p$-values?\"), and use it to\nget a faster privacy-preserving instantiation of our general approach at little\ncost in accuracy. The proof of privacy for the one-shot top~$k$ algorithm\nintroduces a new technique of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 07:31:55 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Dwork", "Cynthia", ""], ["Su", "Weijie", ""], ["Zhang", "Li", ""]]}, {"id": "1511.03927", "submitter": "Emanuele Natale", "authors": "Luca Becchetti, Andrea Clementi, Emanuele Natale, Francesco Pasquale,\n  Luca Trevisan", "title": "Find Your Place: Simple Distributed Algorithms for Community Detection", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an underlying graph, we consider the following \\emph{dynamics}:\nInitially, each node locally chooses a value in $\\{-1,1\\}$, uniformly at random\nand independently of other nodes. Then, in each consecutive round, every node\nupdates its local value to the average of the values held by its neighbors, at\nthe same time applying an elementary, local clustering rule that only depends\non the current and the previous values held by the node.\n  We prove that the process resulting from this dynamics produces a clustering\nthat exactly or approximately (depending on the graph) reflects the underlying\ncut in logarithmic time, under various graph models that exhibit a sparse\nbalanced cut, including the stochastic block model. We also prove that a\nnatural extension of this dynamics performs community detection on a\nregularized version of the stochastic block model with multiple communities.\n  Rather surprisingly, our results provide rigorous evidence for the ability of\nan extremely simple and natural dynamics to address a computational problem\nthat is non-trivial even in a centralized setting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 15:30:00 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 10:04:27 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 07:29:04 GMT"}, {"version": "v4", "created": "Sat, 23 Jul 2016 22:26:26 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Becchetti", "Luca", ""], ["Clementi", "Andrea", ""], ["Natale", "Emanuele", ""], ["Pasquale", "Francesco", ""], ["Trevisan", "Luca", ""]]}, {"id": "1511.04001", "submitter": "Burkhard Morgenstern", "authors": "Lars Hahn, Chris-Andr\\'e Leimeister, Rachid Ounit, Stefano Lonardi,\n  Burkhard Morgenstern", "title": "RasBhari: optimizing spaced seeds for database searching, read mapping\n  and alignment-free sequence comparison", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005107", "report-no": null, "categories": "q-bio.GN cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for sequence analysis rely on word matching or word\nstatistics. Often, these approaches can be improved if binary patterns\nrepresenting match and don't-care positions are used as a filter, such that\nonly those positions of words are considered that correspond to the match\npositions of the patterns. The performance of these approaches, however,\ndepends on the underlying patterns. Herein, we show that the overlap complexity\nof a pattern set that was introduced by Ilie and Ilie is closely related to the\nvariance of the number of matches between two evolutionarily related sequences\nwith respect to this pattern set. We propose a modified hill-climbing algorithm\nto optimize pattern sets for database searching, read mapping and\nalignment-free sequence comparison of nucleic-acid sequences; our\nimplementation of this algorithm is called rasbhari. Depending on the\napplication at hand, rasbhari can either minimize the overlap complexity of\npattern sets, maximize their sensitivity in database searching or minimize the\nvariance of the number of pattern-based matches in alignment-free sequence\ncomparison. We show that, for database searching, rasbhari generates pattern\nsets with slightly higher sensitivity than existing approaches. In our Spaced\nWords approach to alignment-free sequence comparison, pattern sets calculated\nwith rasbhari led to more accurate estimates of phylogenetic distances than the\nrandomly generated pattern sets that we previously used. Finally, we used\nrasbhari to generate patterns for short read classification with CLARK-S. Here\ntoo, the sensitivity of the results could be improved, compared to the default\npatterns of the program. We integrated rasbhari into Spaced Words; the source\ncode of rasbhari is freely available at http://rasbhari.gobics.de/\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 18:48:08 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 13:06:46 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Hahn", "Lars", ""], ["Leimeister", "Chris-Andr\u00e9", ""], ["Ounit", "Rachid", ""], ["Lonardi", "Stefano", ""], ["Morgenstern", "Burkhard", ""]]}, {"id": "1511.04066", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Properly Learning Poisson Binomial Distributions in Almost Polynomial\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for properly learning Poisson binomial distributions. A\nPoisson binomial distribution (PBD) of order $n$ is the discrete probability\ndistribution of the sum of $n$ mutually independent Bernoulli random variables.\nGiven $\\widetilde{O}(1/\\epsilon^2)$ samples from an unknown PBD $\\mathbf{p}$,\nour algorithm runs in time $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$, and\noutputs a hypothesis PBD that is $\\epsilon$-close to $\\mathbf{p}$ in total\nvariation distance. The previously best known running time for properly\nlearning PBDs was $(1/\\epsilon)^{O(\\log(1/\\epsilon))}$.\n  As one of our main contributions, we provide a novel structural\ncharacterization of PBDs. We prove that, for all $\\epsilon >0,$ there exists an\nexplicit collection $\\cal{M}$ of $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$\nvectors of multiplicities, such that for any PBD $\\mathbf{p}$ there exists a\nPBD $\\mathbf{q}$ with $O(\\log(1/\\epsilon))$ distinct parameters whose\nmultiplicities are given by some element of ${\\cal M}$, such that $\\mathbf{q}$\nis $\\epsilon$-close to $\\mathbf{p}$. Our proof combines tools from Fourier\nanalysis and algebraic geometry.\n  Our approach to the proper learning problem is as follows: Starting with an\naccurate non-proper hypothesis, we fit a PBD to this hypothesis. More\nspecifically, we essentially start with the hypothesis computed by the\ncomputationally efficient non-proper learning algorithm in our recent\nwork~\\cite{DKS15}. Our aforementioned structural characterization allows us to\nreduce the corresponding fitting problem to a collection of\n$(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$ systems of low-degree polynomial\ninequalities. We show that each such system can be solved in time\n$(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$, which yields the overall running\ntime of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 20:47:37 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1511.04190", "submitter": "Palash Dey", "authors": "Palash Dey, Neeldhara Misra, and Y. Narahari", "title": "On Choosing Committees Based on Approval Votes in the Presence of\n  Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CY cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of committee selection problem for\nseveral approval-based voting rules in the presence of outliers. Our first\nresult shows that outlier consideration makes committee selection problem\nintractable for approval, net approval, and minisum approval voting rules. We\nthen study parameterized complexity of this problem with five natural\nparameters, namely the target score, the size of the committee (and its dual\nparameter, the number of candidates outside the committee), the number of\noutliers (and its dual parameter, the number of non-outliers). For net approval\nand minisum approval voting rules, we provide a dichotomous result, resolving\nthe parameterized complexity of this problem for all subsets of five natural\nparameters considered (by showing either FPT or W[1]-hardness for all subsets\nof parameters). For the approval voting rule, we resolve the parameterized\ncomplexity of this problem for all subsets of parameters except one.\n  We also study approximation algorithms for this problem. We show that there\ndoes not exist any alpha(.) factor approximation algorithm for approval and net\napproval voting rules, for any computable function alpha(.), unless P=NP. For\nthe minisum voting rule, we provide a pseudopolynomial (1+eps) factor\napproximation algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 07:00:12 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""], ["Narahari", "Y.", ""]]}, {"id": "1511.04303", "submitter": "Fabian Fuchs", "authors": "Fabian Fuchs", "title": "Experimental Evaluation of Distributed Node Coloring Algorithms for\n  Wireless Networks", "comments": "Full version of paper accepted to ALENEX'16; 19 pages plus 12 pages\n  appendix,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate distributed node coloring algorithms for wireless\nnetworks using the network simulator Sinalgo [by DCG@ETHZ]. All considered\nalgorithms operate in the realistic signal-to-interference-and-noise-ratio\n(SINR) model of interference. We evaluate two recent coloring algorithms,\nRand4DColor and ColorReduction (in the following ColorRed), proposed by Fuchs\nand Prutkin in [SIROCCO'15], the MW-Coloring algorithm introduced by Moscibroda\nand Wattenhofer [DC'08] and transferred to the SINR model by Derbel and Talbi\n[ICDCS'10], and a variant of the coloring algorithm of Yu et al. [TCS'14]. We\nadditionally consider several practical improvements to the algorithms and\nevaluate their performance in both static and dynamic scenarios. Our\nexperiments show that Rand4DColor is very fast, computing a valid\n(4Degree)-coloring in less than one third of the time slots required for local\nbroadcasting, where Degree is the maximum node degree in the network. Regarding\nother O(Degree)-coloring algorithms Rand4DColor is at least 4 to 5 times\nfaster. Additionally, the algorithm is robust even in networks with mobile\nnodes and an additional listening phase at the start of the algorithm makes\nRand4DColor robust against the late wake-up of large parts of the network.\nRegarding (Degree+1)-coloring algorithms, we observe that ColorRed it is\nsignificantly faster than the considered variant of the Yu et al. coloring\nalgorithm, which is the only other (Degree+1)-coloring algorithm for the SINR\nmodel. Further improvement can be made with an error-correcting variant that\nincreases the runtime by allowing some uncertainty in the communication and\nafterwards correcting the introduced conflicts.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 15:01:54 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Fuchs", "Fabian", ""]]}, {"id": "1511.04387", "submitter": "Daniel Karapetyan Dr", "authors": "Shahriar Asta, Daniel Karapetyan, Ahmed Kheiri, Ender \\\"Ozcan, Andrew\n  J. Parkes", "title": "Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode\n  Resource-constrained Multi-project Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-mode resource and precedence-constrained project scheduling is a\nwell-known challenging real-world optimisation problem. An important variant of\nthe problem requires scheduling of activities for multiple projects considering\navailability of local and global resources while respecting a range of\nconstraints. A critical aspect of the benchmarks addressed in this paper is\nthat the primary objective is to minimise the sum of the project completion\ntimes, with the usual makespan minimisation as a secondary objective. We\nobserve that this leads to an expected different overall structure of good\nsolutions and discuss the effects this has on the algorithm design. This paper\npresents a carefully designed hybrid of Monte-Carlo tree search, novel\nneighbourhood moves, memetic algorithms, and hyper-heuristic methods. The\nimplementation is also engineered to increase the speed with which iterations\nare performed, and to exploit the computing power of multicore machines.\nEmpirical evaluation shows that the resulting information-sharing\nmulti-component algorithm significantly outperforms other solvers on a set of\n\"hidden\" instances, i.e. instances not available at the algorithm design phase.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:17:32 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 16:43:16 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Asta", "Shahriar", ""], ["Karapetyan", "Daniel", ""], ["Kheiri", "Ahmed", ""], ["\u00d6zcan", "Ender", ""], ["Parkes", "Andrew J.", ""]]}, {"id": "1511.04433", "submitter": "Richard Barnes", "authors": "Richard Barnes, Clarence Lehman, David Mulla", "title": "An Efficient Assignment of Drainage Direction Over Flat Surfaces in\n  Raster Digital Elevation Models", "comments": "13 pages, 4 figures, 8 subalgorithms", "journal-ref": "Computers & Geosciences. Vol 62, Jan 2014, pp 12--135", "doi": "10.1016/j.cageo.2013.01.009", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In processing raster digital elevation models (DEMs) it is often necessary to\nassign drainage directions over flats---that is, over regions with no local\nelevation gradient. This paper presents an approach to drainage direction\nassignment which is not restricted by a flat's shape, number of outlets, or\nsurrounding topography. Flow is modeled by superimposing a gradient away from\nhigher terrain with a gradient towards lower terrain resulting in a drainage\nfield exhibiting flow convergence, an improvement over methods which produce\nregions of parallel flow. This approach builds on previous work by Garbrecht\nand Martz (1997), but presents several important improvements. The improved\nalgorithm guarantees that flats are only resolved if they have outlets. The\nalgorithm does not require iterative application; a single pass is sufficient\nto resolve all flats. The algorithm presents a clear strategy for identifying\nflats and their boundaries. The algorithm is not susceptible to loss of\nfloating-point precision. Furthermore, the algorithm is efficient, operating in\nO( N ) time whereas the older algorithm operates in O( N^(3/2) ) time. In\ntesting, the improved algorithm ran 6.5 times faster than the old for a 100 x\n100 cell flat and 69 times faster for a 700 x 700 cell flat. In tests on actual\nDEMs, the improved algorithm finished its processing 38--110 times sooner while\nrunning on a single processor than a parallel implementation of the old\nalgorithm did while running on 16 processors. The improved algorithm is an\noptimal, accurate, easy-to-implement drop-in replacement for the original.\nPseudocode is provided in the paper and working source code is provided in the\nSupplemental Materials.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 20:49:05 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Barnes", "Richard", ""], ["Lehman", "Clarence", ""], ["Mulla", "David", ""]]}, {"id": "1511.04463", "submitter": "Richard Barnes", "authors": "Richard Barnes, Clarence Lehman, David Mulla", "title": "Priority-Flood: An Optimal Depression-Filling and Watershed-Labeling\n  Algorithm for Digital Elevation Models", "comments": "17 pages, 4 figures, 5 algorithms", "journal-ref": "Computers & Geosciences. Vol 62, Jan 2014, pp 117--127", "doi": "10.1016/j.cageo.2013.04.024", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depressions (or pits) are low areas within a digital elevation model that are\nsurrounded by higher terrain, with no outlet to lower areas. Filling them so\nthey are level, as fluid would fill them if the terrain were impermeable, is\noften necessary in preprocessing DEMs. The depression-filling algorithm\npresented here---called Priority-Flood---unifies and improves on the work of a\nnumber of previous authors who have published similar algorithms. The algorithm\noperates by flooding DEMs inwards from their edges using a priority queue to\ndetermine the next cell to be flooded. The resultant DEM has no depressions or\ndigital dams: every cell is guaranteed to drain. The algorithm is optimal for\nboth integer and floating-point data, working in O(n) and O(n lg n) time,\nrespectively. It is shown that by using a plain queue to fill depressions once\nthey have been found, an O(m lg m) time-complexity can be achieved, where m\ndoes not exceed the number of cells n. This is the lowest time complexity of\nany known floating-point depression-filling algorithm. In testing, this\nimproved variation of the algorithm performed up to 37% faster than the\noriginal. Additionally, a parallel version of an older, but widely-used\ndepression-filling algorithm required six parallel processors to achieve a\nrun-time on par with what the newer algorithm's improved variation took on a\nsingle processor. The Priority-Flood Algorithm is simple to understand and\nimplement: the included pseudocode is only 20 lines and the included C++\nreference implementation is under a hundred lines. The algorithm can work on\nirregular meshes as well as 4-, 6-, 8-, and n-connected grids. It can also be\nadapted to label watersheds and determine flow directions through either\nincremental elevation changes or depression carving. In the case of incremental\nelevation changes, the algorithm includes safety checks not present in prior\nworks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 21:18:21 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Barnes", "Richard", ""], ["Lehman", "Clarence", ""], ["Mulla", "David", ""]]}, {"id": "1511.04466", "submitter": "Jasper C.H. Lee", "authors": "Jasper C.H. Lee and Paul Valiant", "title": "Optimizing Star-Convex Functions", "comments": "30 pages (including appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a polynomial time algorithm for optimizing the class of\nstar-convex functions, under no restrictions except boundedness on a region\nabout the origin, and Lebesgue measurability. The algorithm's performance is\npolynomial in the requested number of digits of accuracy, contrasting with the\nprevious best known algorithm of Nesterov and Polyak that has exponential\ndependence, and that further requires Lipschitz second differentiability of the\nfunction, but has milder dependence on the dimension of the domain. Star-convex\nfunctions constitute a rich class of functions generalizing convex functions to\nnew parameter regimes, and which confound standard variants of gradient\ndescent; more generally, we construct a family of star-convex functions where\ngradient-based algorithms provably give no information about the location of\nthe global optimum.\n  We introduce a new randomized algorithm for finding cutting planes based only\non function evaluations, where, counterintuitively, the algorithm must look\noutside the feasible region to discover the structure of the star-convex\nfunction that lets it compute the next cut of the feasible region. We emphasize\nthat the class of star-convex functions we consider is as unrestricted as\npossible: the class of Lebesgue measurable star-convex functions has\ntheoretical appeal, introducing to the domain of polynomial-time algorithms a\nhuge class with many interesting pathologies. We view our results as a step\nforward in understanding the scope of optimization techniques beyond the garden\nof convex optimization and local gradient-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 21:36:13 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 22:15:13 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Lee", "Jasper C. H.", ""], ["Valiant", "Paul", ""]]}, {"id": "1511.04467", "submitter": "Umberto Esposito", "authors": "Umberto Esposito and Eleni Vasilaki", "title": "Detection of multiple and overlapping bidirectional communities within\n  large, directed and weighted networks of neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent explosion of publicly available biological data, the analysis\nof networks has gained significant interest. In particular, recent promising\nresults in Neuroscience show that the way neurons and areas of the brain are\nconnected to each other plays a fundamental role in cognitive functions and\nbehaviour. Revealing pattern and structures within such an intricate volume of\nconnections is a hard problem that has its roots in Graph and Network Theory.\nSince many real world situations can be modelled through networks, structures\ndetection algorithms find application in almost every field of Science. These\nare NP-complete problems; therefore the generally used approach is through\nheuristic algorithms. Here, we formulate the problem of finding structures in\nnetworks of neurons in terms of a community detection problem. We introduce a\ndefinition of community and we construct a statistics-based heuristic algorithm\nfor directed and weighted networks aiming at identifying overlapping\nbidirectional communities in large networks. We carry out a systematic analysis\nof the algorithm's performance, showing excellent results over a wide range of\nparameters (successful detection percentages almost $100\\%$ all the time).\nAlso, we show results on the computational time needed and we suggest future\ndirections on how to improve computational performance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 21:36:43 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Esposito", "Umberto", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1511.04478", "submitter": "Julien Langou", "authors": "Massimiliano Fasi and Julien Langou and Yves Robert and Bora Ucar", "title": "A Backward/Forward Recovery Approach for the Preconditioned Conjugate\n  Gradient Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent papers have introduced a periodic verification mechanism to\ndetect silent errors in iterative solvers. Chen [PPoPP'13, pp. 167--176] has\nshown how to combine such a verification mechanism (a stability test checking\nthe orthogonality of two vectors and recomputing the residual) with\ncheckpointing: the idea is to verify every $d$ iterations, and to checkpoint\nevery $c \\times d$ iterations. When a silent error is detected by the\nverification mechanism, one can rollback to and re-execute from the last\ncheckpoint. In this paper, we also propose to combine checkpointing and\nverification, but we use algorithm-based fault tolerance (ABFT) rather than\nstability tests. ABFT can be used for error detection, but also for error\ndetection and correction, allowing a forward recovery (and no rollback nor\nre-execution) when a single error is detected. We introduce an abstract\nperformance model to compute the performance of all schemes, and we instantiate\nit using the preconditioned conjugate gradient algorithm. Finally, we validate\nour new approach through a set of simulations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 23:06:35 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Fasi", "Massimiliano", ""], ["Langou", "Julien", ""], ["Robert", "Yves", ""], ["Ucar", "Bora", ""]]}, {"id": "1511.04479", "submitter": "Martin F\\\"urer", "authors": "Martin F\\\"urer", "title": "Multi-Clique-Width", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-clique-width is obtained by a simple modification in the definition of\nclique-width. It has the advantage of providing a natural extension of\ntree-width. Unlike clique-width, it does not explode exponentially compared to\ntree-width. Efficient algorithms based on multi-clique-width are still possible\nfor interesting tasks like computing the independent set polynomial or testing\n$c$-colorability. In particular, $c$-colorability can be tested in time linear\nin $n$ and singly exponential in $c$ and the width $k$ of a given\nmulti-$k$-expression. For these tasks, the running time as a function of the\nmulti-clique-width is the same as the running time of the fastest known\nalgorithm as a function of the clique-width. This results in an exponential\nspeed-up for some graphs, if the corresponding graph generating expressions are\ngiven. The reason is that the multi-clique-width is never bigger, but is\nexponentially smaller than the clique-width for many graphs. This gap shows up\nwhen the tree-width is basically equal to the multi-clique width as well as\nwhen the tree-width is not bounded by any function of the clique-width.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 23:25:44 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["F\u00fcrer", "Martin", ""]]}, {"id": "1511.04631", "submitter": "Adam Sealfon", "authors": "Adam Sealfon", "title": "Shortest Paths and Distances with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for differentially private analysis of weighted graphs\nin which the graph topology $(V,E)$ is assumed to be public and the private\ninformation consists only of the edge weights $w:E\\to\\mathbb{R}^+$. This can\nexpress hiding congestion patterns in a known system of roads. Differential\nprivacy requires that the output of an algorithm provides little advantage,\nmeasured by privacy parameters $\\epsilon$ and $\\delta$, for distinguishing\nbetween neighboring inputs, which are thought of as inputs that differ on the\ncontribution of one individual. In our model, two weight functions $w,w'$ are\nconsidered to be neighboring if they have $\\ell_1$ distance at most one.\n  We study the problems of privately releasing a short path between a pair of\nvertices and of privately releasing approximate distances between all pairs of\nvertices. We are concerned with the approximation error, the difference between\nthe length of the released path or released distance and the length of the\nshortest path or actual distance.\n  For privately releasing a short path between a pair of vertices, we prove a\nlower bound of $\\Omega(|V|)$ on the additive approximation error for fixed\n$\\epsilon,\\delta$. We provide a differentially private algorithm that matches\nthis error bound up to a logarithmic factor and releases paths between all\npairs of vertices. The approximation error of our algorithm can be bounded by\nthe number of edges on the shortest path, so we achieve better accuracy than\nthe worst-case bound for vertex pairs that are connected by a low-weight path\nwith $o(|V|)$ vertices.\n  For privately releasing all-pairs distances, we show that for trees we can\nrelease all distances with approximation error $O(\\log^{2.5}|V|)$ for fixed\nprivacy parameters. For arbitrary bounded-weight graphs with edge weights in\n$[0,M]$ we can release all distances with approximation error\n$\\tilde{O}(\\sqrt{|V|M})$.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 22:43:31 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 05:43:42 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Sealfon", "Adam", ""]]}, {"id": "1511.04731", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang", "title": "Hardness of RNA Folding Problem with Four Symbols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An RNA sequence is a string composed of four types of nucleotides, $A, C, G$,\nand $U$. The goal of the RNA folding problem is to find a maximum cardinality\nset of crossing-free pairs of the form $\\{A,U\\}$ or $\\{C,G\\}$ in a given RNA\nsequence. The problem is central in bioinformatics and has received much\nattention over the years. Abboud, Backurs, and Williams (FOCS 2015)\ndemonstrated a conditional lower bound for a generalized version of the RNA\nfolding problem based on a conjectured hardness of the $k$-clique problem.\nTheir lower bound requires the RNA sequence to have at least 36 types of\nsymbols, making the result not applicable to the RNA folding problem in real\nlife (i.e., alphabet size 4). In this paper, we present an improved lower bound\nthat works for the alphabet size 4 case.\n  We also investigate the Dyck edit distance problem, which is a string problem\nclosely related to RNA folding. We demonstrate a reduction from RNA folding to\nDyck edit distance with alphabet size 10. This leads to a much simpler proof of\nthe conditional lower bound for Dyck edit distance problem given by Abboud,\nBackurs, and Williams (FOCS 2015), and lowers the alphabet size requirement.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 17:09:26 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 23:28:29 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 14:47:42 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Chang", "Yi-Jun", ""]]}, {"id": "1511.04750", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, George Papastefanatos, Melina Skourla, Timos Sellis", "title": "A Hierarchical Aggregation Framework for Efficient Multilevel Visual\n  Exploration and Analysis", "comments": "Semantic Web Journal 2016 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exploration and visualization systems are of great importance in the Big\nData era, in which the volume and heterogeneity of available information make\nit difficult for humans to manually explore and analyse data. Most traditional\nsystems operate in an offline way, limited to accessing preprocessed (static)\nsets of data. They also restrict themselves to dealing with small dataset\nsizes, which can be easily handled with conventional techniques. However, the\nBig Data era has realized the availability of a great amount and variety of big\ndatasets that are dynamic in nature; most of them offer API or query endpoints\nfor online access, or the data is received in a stream fashion. Therefore,\nmodern systems must address the challenge of on-the-fly scalable visualizations\nover large dynamic sets of data, offering efficient exploration techniques, as\nwell as mechanisms for information abstraction and summarization. In this work,\nwe present a generic model for personalized multilevel exploration and analysis\nover large dynamic sets of numeric and temporal data. Our model is built on top\nof a lightweight tree-based structure which can be efficiently constructed\non-the-fly for a given set of data. This tree structure aggregates input\nobjects into a hierarchical multiscale model. Considering different exploration\nscenarios over large datasets, the proposed model enables efficient multilevel\nexploration, offering incremental construction and prefetching via user\ninteraction, and dynamic adaptation of the hierarchies based on user\npreferences. A thorough theoretical analysis is presented, illustrating the\nefficiency of the proposed model. The proposed model is realized in a web-based\nprototype tool, called SynopsViz that offers multilevel visual exploration and\nanalysis over Linked Data datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 18:23:27 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2015 12:51:23 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2016 18:08:18 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 14:33:45 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bikakis", "Nikos", ""], ["Papastefanatos", "George", ""], ["Skourla", "Melina", ""], ["Sellis", "Timos", ""]]}, {"id": "1511.04762", "submitter": "Ananya Christman", "authors": "Hamza Alsarhan, Davin Chia, Ananya Christman, Shannia Fu, Yanfeng Jin", "title": "Bin Packing with Multiple Colors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Colored Bin Packing problem a set of items with varying weights and\ncolors must be packed into bins of uniform weight limit such that no two items\nof the same color may be packed adjacently within a bin. We solve this problem\nfor the case where there are two or more colors when the items have zero weight\nand when the items have unit weight. Our algorithms are optimal and run in\nlinear time. Since our algorithms apply for two or more colors, they\ndemonstrate that the problem does not get harder as the number of colors\nincreases. We also provide closed-form expressions for the optimal number of\nbins.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 20:28:24 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Alsarhan", "Hamza", ""], ["Chia", "Davin", ""], ["Christman", "Ananya", ""], ["Fu", "Shannia", ""], ["Jin", "Yanfeng", ""]]}, {"id": "1511.04952", "submitter": "Dimitrios Thilikos", "authors": "Isolde Adler, Stavros G. Kolliopoulos, Dimitrios M. Thilikos", "title": "Planar Disjoint-Paths Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  introduce {\\sc Planar Disjoint Paths Completion}, a completion counterpart of\nthe Disjoint Paths problem, and study its parameterized complexity. The problem\ncan be stated as follows: given a, not necessarily connected, plane graph $G,$\n$k$ pairs of terminals, and a face $F$ of $G,$ find a minimum-size set of\nedges, if one exists, to be added inside $F$ so that the embedding remains\nplanar and the pairs become connected by $k$ disjoint paths in the augmented\nnetwork. Our results are twofold: first, we give an upper bound on the number\nof necessary additional edges when a solution exists. This bound is a function\nof $k$, independent of the size of $G.$ Second, we show that the problem is\nfixed-parameter tractable, in particular, it can be solved in time $f(k)\\cdot\nn^{2}.$\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 13:36:39 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 13:00:00 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Adler", "Isolde", ""], ["Kolliopoulos", "Stavros G.", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1511.05010", "submitter": "Carlos Baquero", "authors": "Marek Zawirski, Carlos Baquero, Annette Bieniusa, Nuno Pregui\\c{c}a,\n  Marc Shapiro", "title": "Eventually Consistent Register Revisited", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to converge in the presence of concurrent updates, modern eventually\nconsistent replication systems rely on causality information and operation\nsemantics. It is relatively easy to use semantics of high-level operations on\nreplicated data structures, such as sets, lists, etc. However, it is difficult\nto exploit semantics of operations on registers, which store opaque data. In\nexisting register designs, concurrent writes are resolved either by the\napplication, or by arbitrating them according to their timestamps. The former\nis complex and may require user intervention, whereas the latter causes\narbitrary updates to be lost. In this work, we identify a register construction\nthat generalizes existing ones by combining runtime causality ordering, to\nidentify concurrent writes, with static data semantics, to resolve them. We\npropose a simple conflict resolution template based on an\napplication-predefined order on the domain of values. It eliminates or reduces\nthe number of conflicts that need to be resolved by the user or by an explicit\napplication logic. We illustrate some variants of our approach with use cases,\nand how it generalizes existing designs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 15:59:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Zawirski", "Marek", ""], ["Baquero", "Carlos", ""], ["Bieniusa", "Annette", ""], ["Pregui\u00e7a", "Nuno", ""], ["Shapiro", "Marc", ""]]}, {"id": "1511.05053", "submitter": "Aleksandrs Belovs", "authors": "Aleksandrs Belovs, Eric Blais", "title": "A Polynomial Lower Bound for Testing Monotonicity", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every algorithm for testing $n$-variate Boolean functions for\nmonotonicity must have query complexity $\\tilde{\\Omega}(n^{1/4})$. All previous\nlower bounds for this problem were designed for non-adaptive algorithms and, as\na result, the best previous lower bound for general (possibly adaptive)\nmonotonicity testers was only $\\Omega(\\log n)$. Combined with the query\ncomplexity of the non-adaptive monotonicity tester of Khot, Minzer, and Safra\n(FOCS 2015), our lower bound shows that adaptivity can result in at most a\nquadratic reduction in the query complexity for testing monotonicity.\n  By contrast, we show that there is an exponential gap between the query\ncomplexity of adaptive and non-adaptive algorithms for testing regular linear\nthreshold functions (LTFs) for monotonicity. Chen, De, Servedio, and Tan (STOC\n2015) recently showed that non-adaptive algorithms require almost\n$\\Omega(n^{1/2})$ queries for this task. We introduce a new adaptive\nmonotonicity testing algorithm which has query complexity $O(\\log n)$ when the\ninput is a regular LTF.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:38:59 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Belovs", "Aleksandrs", ""], ["Blais", "Eric", ""]]}, {"id": "1511.05449", "submitter": "Christian Komusiewicz", "authors": "Christian Komusiewicz", "title": "Tight Running Time Lower Bounds for Vertex Deletion Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph class $\\Pi$, the $\\Pi$-Vertex Deletion problem has as input an\nundirected graph $G=(V,E)$ and an integer $k$ and asks whether there is a set\nof at most $k$ vertices that can be deleted from $G$ such that the resulting\ngraph is a member of $\\Pi$. By a classic result of Lewis and Yannakakis [J.\nComput. Syst. Sci. '80], $\\Pi$-Vertex Deletion is NP-hard for all hereditary\nproperties $\\Pi$. We adapt the original NP-hardness construction to show that\nunder the Exponential Time Hypothesis (ETH) tight complexity results can be\nobtained. We show that $\\Pi$-Vertex Deletion does not admit a $2^{o(n)}$-time\nalgorithm where $n$ is the number of vertices in $G$. We also obtain a\ndichotomy for running time bounds that include the number $m$ of edges in the\ninput graph: On the one hand, if $\\Pi$ contains all independent sets, then\nthere is no $2^{o(n+m)}$-time algorithm for $\\Pi$-Vertex Deletion. On the other\nhand, if there is a fixed independent set that is not contained in $\\Pi$ and\ncontainment in $\\Pi$ can determined in $2^{O(n)}$ time or $2^{o(m)}$ time, then\n$\\Pi$-Vertex Deletion can be solved in $2^{O(\\sqrt{m})}+O(n)$ or\n$2^{o({m})}+O(n)$ time, respectively. We also consider restrictions on the\ndomain of the input graph $G$. For example, we obtain that $\\Pi$-Vertex\nDeletion cannot be solved in $2^{o(\\sqrt{n})}$ time if $G$ is planar and $\\Pi$\nis hereditary and contains and excludes infinitely many planar graphs. Finally,\nwe provide similar results for the problem variant where the deleted vertex set\nhas to induce a connected graph.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:59:57 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 09:05:23 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Komusiewicz", "Christian", ""]]}, {"id": "1511.05514", "submitter": "Corinna Gottschalk", "authors": "Corinna Gottschalk and Jens Vygen", "title": "Better $s$-$t$-Tours by Gao Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $s$-$t$-path TSP: given a finite metric space with two\nelements $s$ and $t$, we look for a path from $s$ to $t$ that contains all the\nelements and has minimum total distance. We improve the approximation ratio for\nthis problem from 1.599 to 1.566. Like previous algorithms, we solve the\nnatural LP relaxation and represent an optimum solution $x^*$ as a convex\ncombination of spanning trees. Gao showed that there exists a spanning tree in\nthe support of $x^*$ that has only one edge in each narrow cut (i.e., each cut\n$C$ with $x^*(C)<2$). Our main theorem says that the spanning trees in the\nconvex combination can be chosen such that many of them are such \"Gao trees''.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 19:19:37 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Gottschalk", "Corinna", ""], ["Vygen", "Jens", ""]]}, {"id": "1511.05546", "submitter": "Holger Dell", "authors": "Holger Dell, Eun Jung Kim, Michael Lampis, Valia Mitsou, Tobias\n  M\\\"omke", "title": "Complexity and Approximability of Parameterized MAX-CSPs", "comments": "Appeared in IPEC 2015", "journal-ref": "Algorithmica 79(1): 230-250 (2017)", "doi": "10.1007/s00453-017-0310-8 10.4230/LIPIcs.IPEC.2015.294", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the optimization version of constraint satisfaction problems\n(Max-CSPs) in the framework of parameterized complexity; the goal is to compute\nthe maximum fraction of constraints that can be satisfied simultaneously. In\nstandard CSPs, we want to decide whether this fraction equals one. The\nparameters we investigate are structural measures, such as the treewidth or the\nclique-width of the variable-constraint incidence graph of the CSP instance.\n  We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY,\nand with various parameters k, and we attempt to fully classify them into the\nfollowing three cases: 1. The exact optimum can be computed in FPT time. 2. It\nis W[1]-hard to compute the exact optimum, but there is a randomized FPT\napproximation scheme (FPTAS), which computes a $(1-\\epsilon)$-approximation in\ntime $f(k,\\epsilon)\\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].\n  For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 20:51:47 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 10:42:11 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dell", "Holger", ""], ["Kim", "Eun Jung", ""], ["Lampis", "Michael", ""], ["Mitsou", "Valia", ""], ["M\u00f6mke", "Tobias", ""]]}, {"id": "1511.05646", "submitter": "Alon Eden", "authors": "Vincent Cohen-Addad, Alon Eden, Michal Feldman, Amos Fiat", "title": "The Invisible Hand of Dynamic Market Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walrasian prices, if they exist, have the property that one can assign every\nbuyer some bundle in her demand set, such that the resulting assignment will\nmaximize social welfare. Unfortunately, this assumes carefully breaking ties\namongst different bundles in the buyer demand set. Presumably, the shopkeeper\ncleverly convinces the buyer to break ties in a manner consistent with\nmaximizing social welfare. Lacking such a shopkeeper, if buyers arrive\nsequentially and simply choose some arbitrary bundle in their demand set, the\nsocial welfare may be arbitrarily bad. In the context of matching markets, we\nshow how to compute dynamic prices, based upon the current inventory, that\nguarantee that social welfare is maximized. Such prices are set without knowing\nthe identity of the next buyer to arrive. We also show that this is impossible\nin general (e.g., for coverage valuations), but consider other scenarios where\nthis can be done. We further extend our results to Bayesian and bounded\nrationality models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 02:50:09 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 11:55:23 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Eden", "Alon", ""], ["Feldman", "Michal", ""], ["Fiat", "Amos", ""]]}, {"id": "1511.05680", "submitter": "Wuxuan Jiang", "authors": "Wuxuan Jiang, Cong Xie, Zhihua Zhang", "title": "Wishart Mechanism for Differentially Private Principal Components\n  Analysis", "comments": "A full version with technical proofs. Accepted to AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new input perturbation mechanism for publishing a covariance\nmatrix to achieve $(\\epsilon,0)$-differential privacy. Our mechanism uses a\nWishart distribution to generate matrix noise. In particular, We apply this\nmechanism to principal component analysis. Our mechanism is able to keep the\npositive semi-definiteness of the published covariance matrix. Thus, our\napproach gives rise to a general publishing framework for input perturbation of\na symmetric positive semidefinite matrix. Moreover, compared with the classic\nLaplace mechanism, our method has better utility guarantee. To the best of our\nknowledge, Wishart mechanism is the best input perturbation approach for\n$(\\epsilon,0)$-differentially private PCA. We also compare our work with\nprevious exponential mechanism algorithms in the literature and provide near\noptimal bound while having more flexibility and less computational\nintractability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 07:34:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 06:41:29 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Jiang", "Wuxuan", ""], ["Xie", "Cong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1511.05690", "submitter": "Oliver Serang", "authors": "Oliver Serang", "title": "Fast Computation on Semirings Isomorphic to $(\\times, \\max)$ on\n  $\\mathbb{R}_+$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important problems across multiple disciplines involve computations on the\nsemiring $(\\times, \\max)$ (or its equivalents, the negated version $(\\times,\n\\min)$), the log-transformed version $(+, \\max)$, or the negated\nlog-transformed version $(+, \\min)$): max-convolution, all-pairs shortest paths\nin a weighted graph, and finding the largest $k$ values in $x_i+y_j$ for two\nlists $x$ and $y$. However, fast algorithms such as those enabling FFT\nconvolution, sub-cubic matrix multiplication, \\emph{etc.}, require inverse\noperations, and thus cannot be computed on semirings. This manuscript\ngeneralizes recent advances on max-convolution: in this approach a small family\nof $p$-norm rings are used to efficiently approximate results on a nonnegative\nsemiring. The general approach can be used to easily compute sub-cubic\nestimates of the all-pairs shortest paths in a graph with nonnegative edge\nweights and sub-quadratic estimates of the top $k$ values in $x_i+y_j$ when $x$\nand $y$ are nonnegative. These methods are fast in practice and can benefit\nfrom coarse-grained parallelization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 08:17:37 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 12:59:00 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Serang", "Oliver", ""]]}, {"id": "1511.05886", "submitter": "Jesper W. Mikkelsen", "authors": "Jesper W. Mikkelsen", "title": "Randomization can be as helpful as a glimpse of the future in online\n  computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple but surprisingly useful direct product theorems for proving\nlower bounds on online algorithms with a limited amount of advice about the\nfuture. As a consequence, we are able to translate decades of research on\nrandomized online algorithms to the advice complexity model. Doing so improves\nsignificantly on the previous best advice complexity lower bounds for many\nonline problems, or provides the first known lower bounds. For example, if $n$\nis the number of requests, we show that:\n  (1) A paging algorithm needs $\\Omega(n)$ bits of advice to achieve a\ncompetitive ratio better than $H_k=\\Omega(\\log k)$, where $k$ is the cache\nsize. Previously, it was only known that $\\Omega(n)$ bits of advice were\nnecessary to achieve a constant competitive ratio smaller than $5/4$.\n  (2) Every $O(n^{1-\\varepsilon})$-competitive vertex coloring algorithm must\nuse $\\Omega(n\\log n)$ bits of advice. Previously, it was only known that\n$\\Omega(n\\log n)$ bits of advice were necessary to be optimal.\n  For certain online problems, including the MTS, $k$-server, paging, list\nupdate, and dynamic binary search tree problem, our results imply that\nrandomization and sublinear advice are equally powerful (if the underlying\nmetric space or node set is finite). This means that several long-standing open\nquestions regarding randomized online algorithms can be equivalently stated as\nquestions regarding online algorithms with sublinear advice. For example, we\nshow that there exists a deterministic $O(\\log k)$-competitive $k$-server\nalgorithm with advice complexity $o(n)$ if and only if there exists a\nrandomized $O(\\log k)$-competitive $k$-server algorithm without advice.\n  Technically, our main direct product theorem is obtained by extending an\ninformation theoretical lower bound technique due to Emek, Fraigniaud, Korman,\nand Ros\\'en [ICALP'09].\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 17:20:34 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 10:38:05 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Mikkelsen", "Jesper W.", ""]]}, {"id": "1511.05987", "submitter": "Jurek Czyzowicz", "authors": "Jerzy Czyzowicz, Krzysztof Diks, Jean Moussi, Wojciech Rytter", "title": "Algorithms for Communication Problems for Mobile Agents Exchanging\n  Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider communication problems in the setting of mobile agents deployed\nin an edge-weighted network. The assumption of the paper is that each agent has\nsome energy that it can transfer to any other agent when they meet (together\nwith the information it holds).\n  The paper deals with three communication problems: data delivery,convergecast\nand broadcast. These problems are posed for a centralized scheduler which has\nfull knowledge of the instance.\n  It is already known that, without energy exchange, all three problems are\nNP-complete even if the network is a line. Surprisingly, if we allow the agents\nto exchange energy, we show that all three problems are polynomially solvable\non trees and have linear time algorithms on the line. On the other hand for\ngeneral undirected and directed graphs we show that these problems, even if\nenergy exchange is allowed, are still NP-complete.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 21:32:21 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Czyzowicz", "Jerzy", ""], ["Diks", "Krzysztof", ""], ["Moussi", "Jean", ""], ["Rytter", "Wojciech", ""]]}, {"id": "1511.06000", "submitter": "Frans Schalekamp", "authors": "Frans Schalekamp, Anke van Zuylen and Suzanne van der Ster", "title": "A Duality Based 2-Approximation Algorithm for Maximum Agreement Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a 2-approximation algorithm for the Maximum Agreement Forest problem\non two rooted binary trees. This NP-hard problem has been studied extensively\nin the past two decades, since it can be used to compute the Subtree\nPrune-and-Regraft (SPR) distance between two phylogenetic trees. Our result\nimproves on the very recent 2.5-approximation algorithm due to Shi, Feng, You\nand Wang (2015). Our algorithm is the first approximation algorithm for this\nproblem that uses LP duality in its analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 22:10:28 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 20:00:02 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Schalekamp", "Frans", ""], ["van Zuylen", "Anke", ""], ["van der Ster", "Suzanne", ""]]}, {"id": "1511.06022", "submitter": "Amir Abboud", "authors": "Amir Abboud and Thomas Dueholm Hansen and Virginia Vassilevska\n  Williams and Ryan Williams", "title": "Simulating Branching Programs with Edit Distance and Friends or: A\n  Polylog Shaved is a Lower Bound Made", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent and active line of work achieves tight lower bounds for fundamental\nproblems under the Strong Exponential Time Hypothesis (SETH). A celebrated\nresult of Backurs and Indyk (STOC'15) proves that the Edit Distance of two\nsequences of length n cannot be computed in strongly subquadratic time under\nSETH. The result was extended by follow-up works to simpler looking problems\nlike finding the Longest Common Subsequence (LCS).\n  SETH is a very strong assumption, asserting that even linear size CNF\nformulas cannot be analyzed for satisfiability with an exponential speedup over\nexhaustive search. We consider much safer assumptions, e.g. that such a speedup\nis impossible for SAT on much more expressive representations, like NC\ncircuits. Intuitively, this seems much more plausible: NC circuits can\nimplement complex cryptographic primitives, while CNFs cannot even\napproximately compute an XOR of bits.\n  Our main result is a surprising reduction from SAT on Branching Programs to\nfundamental problems in P like Edit Distance, LCS, and many others. Truly\nsubquadratic algorithms for these problems therefore have consequences that we\nconsider to be far more remarkable than merely faster CNF SAT algorithms. For\nexample, SAT on arbitrary o(n)-depth bounded fan-in circuits (and therefore\nalso NC-Circuit-SAT) can be solved in (2-eps)^n time.\n  A very interesting feature of our work is that we can prove major\nconsequences even from mildly subquadratic algorithms for Edit Distance or LCS.\nFor example, we show that if we can shave an arbitrarily large polylog factor\nfrom n^2 for Edit Distance then NEXP does not have non-uniform NC^1 circuits. A\nmore fine-grained examination shows that even shaving a $\\log^c{n}$ factor, for\na specific constant $c \\approx 10^3$, already implies new circuit lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 23:28:37 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Abboud", "Amir", ""], ["Hansen", "Thomas Dueholm", ""], ["Williams", "Virginia Vassilevska", ""], ["Williams", "Ryan", ""]]}, {"id": "1511.06037", "submitter": "Alexander Iriza", "authors": "Alexander Iriza", "title": "Enumeration and Random Generation of Unlabeled Classes of Graphs: A\n  Practical Study of Cycle Pointing and the Dissymmetry Theorem", "comments": "59 pages, 43 figures. Master's thesis, supervised by J\\'er\\'emie\n  Lumbroso and Robert Sedgewick. Full code available at\n  https://github.com/alexiriza/unlabeled-graph-samplers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work studies the enumeration and random generation of unlabeled\ncombinatorial classes of unrooted graphs. While the technique of vertex\npointing provides a straightforward procedure for analyzing a labeled class of\nunrooted graphs by first studying its rooted counterpart, the existence of\nnontrivial symmetries in the unlabeled case causes this technique to break\ndown. Instead, techniques such as the dissymmetry theorem (of Otter) and cycle\npointing (of Bodirsky et al.) have emerged in the unlabeled case, with the\nformer providing an enumeration of the class and the latter providing both an\nenumeration and an unbiased sampler. In this work, we extend the power of the\ndissymmetry theorem by showing that it in fact provides a Boltzmann sampler for\nthe class in question. We then present an exposition of the cycle pointing\ntechnique, with a focus on the enumeration and random generation of the\nunderlying unpointed class. Finally, we apply cycle pointing to enumerate and\nimplement samplers for the classes of distance-hereditary graphs and three-leaf\npower graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 01:13:23 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Iriza", "Alexander", ""]]}, {"id": "1511.06099", "submitter": "Jiecao Chen", "authors": "Alexandr Andoni, Jiecao Chen, Robert Krauthgamer, Bo Qin, David P.\n  Woodruff, Qin Zhang", "title": "On Sketching Quadratic Forms", "comments": "46 pages; merging of arXiv:1403.7058 and arXiv:1412.8225", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We undertake a systematic study of sketching a quadratic form: given an $n\n\\times n$ matrix $A$, create a succinct sketch $\\textbf{sk}(A)$ which can\nproduce (without further access to $A$) a multiplicative\n$(1+\\epsilon)$-approximation to $x^T A x$ for any desired query $x \\in\n\\mathbb{R}^n$. While a general matrix does not admit non-trivial sketches,\npositive semi-definite (PSD) matrices admit sketches of size\n$\\Theta(\\epsilon^{-2} n)$, via the Johnson-Lindenstrauss lemma, achieving the\n\"for each\" guarantee, namely, for each query $x$, with a constant probability\nthe sketch succeeds. (For the stronger \"for all\" guarantee, where the sketch\nsucceeds for all $x$'s simultaneously, again there are no non-trivial\nsketches.)\n  We design significantly better sketches for the important subclass of graph\nLaplacian matrices, which we also extend to symmetric diagonally dominant\nmatrices. A sequence of work culminating in that of Batson, Spielman, and\nSrivastava (SIAM Review, 2014), shows that by choosing and reweighting\n$O(\\epsilon^{-2} n)$ edges in a graph, one achieves the \"for all\" guarantee.\nOur main results advance this front.\n  $\\bullet$ For the \"for all\" guarantee, we prove that Batson et al.'s bound is\noptimal even when we restrict to \"cut queries\" $x\\in \\{0,1\\}^n$.\n  In contrast, previous lower bounds showed the bound only for {\\em\nspectral-sparsifiers}.\n  $\\bullet$ For the \"for each\" guarantee, we design a sketch of size $\\tilde\nO(\\epsilon^{-1} n)$ bits for \"cut queries\" $x\\in \\{0,1\\}^n$. We prove a\nnearly-matching lower bound of $\\Omega(\\epsilon^{-1} n)$ bits. For general\nqueries $x \\in \\mathbb{R}^n$, we construct sketches of size\n$\\tilde{O}(\\epsilon^{-1.6} n)$ bits.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 09:24:39 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Andoni", "Alexandr", ""], ["Chen", "Jiecao", ""], ["Krauthgamer", "Robert", ""], ["Qin", "Bo", ""], ["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1511.06253", "submitter": "Fragkiskos Koufogiannis", "authors": "Fragkiskos Koufogiannis and George Pappas", "title": "Diffusing Private Data over Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of social and technological networks has enabled rapid sharing\nof data and information. This has resulted in significant privacy concerns\nwhere private information can be either leaked or inferred from public data.\nThe problem is significantly harder for social networks where we may reveal\nmore information to our friends than to strangers. Nonetheless, our private\ninformation can still leak to strangers as our friends are their friends and so\non. In order to address this important challenge, in this paper, we present a\nprivacy-preserving mechanism that enables private data to be diffused over a\nnetwork. In particular, whenever a user wants to access another users' data,\nthe proposed mechanism returns a differentially private response that ensures\nthat the amount of private data leaked depends on the distance between the two\nusers in the network. While allowing global statistics to be inferred by users\nacting as analysts, our mechanism guarantees that no individual user, or a\ngroup of users, can harm the privacy guarantees of any other user. We\nillustrate our mechanism with two examples: one on synthetic data where the\nusers share their GPS coordinates; and one on a Facebook ego-network where a\nuser shares her infection status.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:55:03 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Koufogiannis", "Fragkiskos", ""], ["Pappas", "George", ""]]}, {"id": "1511.06441", "submitter": "Ivan Matic", "authors": "Ivan Matic", "title": "A parallel algorithm for the constrained shortest path problem on\n  lattice graphs", "comments": "In: Adamatzky, A (Ed.) Shortest path solvers. From software to\n  wetware. Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edges of a graph are assigned weights and passage times which are assumed\nto be positive integers. We present a parallel algorithm for finding the\nshortest path whose total weight is smaller than a pre-determined value. In\neach step the processing elements are not analyzing the entire graph. Instead\nthey are focusing on a subset of vertices called {\\em active vertices}. The set\nof active vertices at time $t$ is related to the boundary of the ball $B_t$ of\nradius $t$ in the first passage percolation metric. Although it is believed\nthat the number of active vertices is an order of magnitude smaller than the\nsize of the graph, we prove that this need not be the case with an example of a\ngraph for which the active vertices form a large fractal. We analyze an OpenCL\nimplementation of the algorithm on GPU for cubes in $\\mathbb Z^d$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:06:12 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 17:36:15 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Matic", "Ivan", ""]]}, {"id": "1511.06468", "submitter": "Di Wang", "authors": "Di Wang, Michael Mahoney, Nishanth Mohan, Satish Rao", "title": "Faster Parallel Solver for Positive Linear Programs via\n  Dynamically-Bucketed Selective Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide improved parallel approximation algorithms for the important class\nof packing and covering linear programs. In particular, we present new parallel\n$\\epsilon$-approximate packing and covering solvers which run in\n$\\tilde{O}(1/\\epsilon^2)$ expected time, i.e., in expectation they take\n$\\tilde{O}(1/\\epsilon^2)$ iterations and they do $\\tilde{O}(N/\\epsilon^2)$\ntotal work, where $N$ is the size of the constraint matrix and $\\epsilon$ is\nthe error parameter, and where the $\\tilde{O}$ hides logarithmic factors. To\nachieve our improvement, we introduce an algorithmic technique of broader\ninterest: dynamically-bucketed selective coordinate descent (DB-SCD). At each\nstep of the iterative optimization algorithm, the DB-SCD method dynamically\nbuckets the coordinates of the gradient into those of roughly equal magnitude,\nand it updates all the coordinates in one of the buckets. This\ndynamically-bucketed updating permits us to take steps along several\ncoordinates with similar-sized gradients, thereby permitting more appropriate\nstep sizes at each step of the algorithm. In particular, this technique allows\nus to use in a straightforward manner the recent analysis from the breakthrough\nresults of Allen-Zhu and Orecchia [2] to achieve our still-further improved\nbounds. More generally, this method addresses \"interference\" among coordinates,\nby which we mean the impact of the update of one coordinate on the gradients of\nother coordinates. Such interference is a core issue in parallelizing\noptimization routines that rely on smoothness properties. Since our DB-SCD\nmethod reduces interference via updating a selective subset of variables at\neach iteration, we expect it may also have more general applicability in\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 01:10:13 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Wang", "Di", ""], ["Mahoney", "Michael", ""], ["Mohan", "Nishanth", ""], ["Rao", "Satish", ""]]}, {"id": "1511.06480", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Aditya Bhaskara, Sanjiv Kumar, Yunchao Gong, Shih-Fu\n  Chang", "title": "On Binary Embedding using Circulant Matrices", "comments": "This is an extended version of a paper by the first, third, fourth\n  and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary embeddings provide efficient and powerful ways to perform operations\non large scale data. However binary embedding typically requires long codes in\norder to preserve the discriminative power of the input space. Thus binary\ncoding methods traditionally suffer from high computation and storage costs in\nsuch a scenario. To address this problem, we propose Circulant Binary Embedding\n(CBE) which generates binary codes by projecting the data with a circulant\nmatrix. The circulant structure allows us to use Fast Fourier Transform\nalgorithms to speed up the computation. For obtaining $k$-bit binary codes from\n$d$-dimensional data, this improves the time complexity from $O(dk)$ to\n$O(d\\log{d})$, and the space complexity from $O(dk)$ to $O(d)$.\n  We study two settings, which differ in the way we choose the parameters of\nthe circulant matrix. In the first, the parameters are chosen randomly and in\nthe second, the parameters are learned using the data. For randomized CBE, we\ngive a theoretical analysis comparing it with binary embedding using an\nunstructured random projection matrix. The challenge here is to show that the\ndependencies in the entries of the circulant matrix do not lead to a loss in\nperformance. In the second setting, we design a novel time-frequency\nalternating optimization to learn data-dependent circulant projections, which\nalternatively minimizes the objective in original and Fourier domains. In both\nthe settings, we show by extensive experiments that the CBE approach gives much\nbetter performance than the state-of-the-art approaches if we fix a running\ntime, and provides much faster computation with negligible performance\ndegradation if we fix the number of bits in the embedding.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 03:05:15 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 02:36:10 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Yu", "Felix X.", ""], ["Bhaskara", "Aditya", ""], ["Kumar", "Sanjiv", ""], ["Gong", "Yunchao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1511.06558", "submitter": "Preetum Nakkiran", "authors": "Pasin Manurangsi, Preetum Nakkiran, Luca Trevisan", "title": "Near-Optimal UGC-hardness of Approximating Max k-CSP_R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove an almost-optimal hardness for Max $k$-CSP$_R$ based\non Khot's Unique Games Conjecture (UGC). In Max $k$-CSP$_R$, we are given a set\nof predicates each of which depends on exactly $k$ variables. Each variable can\ntake any value from $1, 2, \\dots, R$. The goal is to find an assignment to\nvariables that maximizes the number of satisfied predicates.\n  Assuming the Unique Games Conjecture, we show that it is NP-hard to\napproximate Max $k$-CSP$_R$ to within factor $2^{O(k \\log k)}(\\log\nR)^{k/2}/R^{k - 1}$ for any $k, R$. To the best of our knowledge, this result\nimproves on all the known hardness of approximation results when $3 \\leq k =\no(\\log R/\\log \\log R)$. In this case, the previous best hardness result was\nNP-hardness of approximating within a factor $O(k/R^{k-2})$ by Chan. When $k =\n2$, our result matches the best known UGC-hardness result of Khot, Kindler,\nMossel and O'Donnell.\n  In addition, by extending an algorithm for Max 2-CSP$_R$ by Kindler, Kolla\nand Trevisan, we provide an $\\Omega(\\log R/R^{k - 1})$-approximation algorithm\nfor Max $k$-CSP$_R$. This algorithm implies that our inapproximability result\nis tight up to a factor of $2^{O(k \\log k)}(\\log R)^{k/2 - 1}$. In comparison,\nwhen $3 \\leq k$ is a constant, the previously known gap was $O(R)$, which is\nsignificantly larger than our gap of $O(\\text{polylog } R)$.\n  Finally, we show that we can replace the Unique Games Conjecture assumption\nwith Khot's $d$-to-1 Conjecture and still get asymptotically the same hardness\nof approximation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 11:13:10 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Manurangsi", "Pasin", ""], ["Nakkiran", "Preetum", ""], ["Trevisan", "Luca", ""]]}, {"id": "1511.06559", "submitter": "Bundit Laekhanukit", "authors": "Bundit Laekhanukit", "title": "Approximating Directed Steiner Problems via Tree Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-edge connected directed Steiner tree (k-DST) problem, we are given a\ndirected graph G on n vertices with edge-costs, a root vertex r, a set of h\nterminals T and an integer k. The goal is to find a min-cost subgraph H of G\nthat connects r to each terminal t by k edge-disjoint r,t-paths. This problem\nincludes as special cases the well-known directed Steiner tree (DST) problem\n(the case k = 1) and the group Steiner tree (GST) problem. Despite having been\nstudied and mentioned many times in literature, e.g., by Feldman et al.\n[SODA'09, JCSS'12], by Cheriyan et al. [SODA'12, TALG'14] and by Laekhanukit\n[SODA'14], there was no known non-trivial approximation algorithm for k-DST for\nk >= 2 even in the special case that an input graph is directed acyclic and has\na constant number of layers. If an input graph is not acyclic, the complexity\nstatus of k-DST is not known even for a very strict special case that k= 2 and\n|T| = 2.\n  In this paper, we make a progress toward developing a non-trivial\napproximation algorithm for k-DST. We present an O(D k^{D-1} log\nn)-approximation algorithm for k-DST on directed acyclic graphs (DAGs) with D\nlayers, which can be extended to a special case of k-DST on \"general graphs\"\nwhen an instance has a D-shallow optimal solution, i.e., there exist k\nedge-disjoint r,t-paths, each of length at most D, for every terminal t. For\nthe case k= 1 (DST), our algorithm yields an approximation ratio of O(D log h),\nthus implying an O(log^3 h)-approximation algorithm for DST that runs in\nquasi-polynomial-time (due to the height-reduction of Zelikovsky\n[Algorithmica'97]). Consequently, as our algorithm works for general graphs, we\nobtain an O(D k^{D-1} log n)-approximation algorithm for a D-shallow instance\nof the k-edge-connected directed Steiner subgraph problem, where we wish to\nconnect every pair of terminals by k-edge-disjoint paths.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 11:15:53 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 12:04:10 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 16:14:26 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Laekhanukit", "Bundit", ""]]}, {"id": "1511.06620", "submitter": "Natarajan Meghanathan", "authors": "Natarajan Meghanathan", "title": "Use of Eigenvector Centrality to Detect Graph Isomorphism", "comments": "9 pages, 4 figures; Proceedings of the Fourth International\n  Conference on Advanced Information Technologies and Applications (ICAITA),\n  pp. 1-9, Dubai, UAE, November 6-7, 2015", "journal-ref": null, "doi": "10.5121/csit.2015.51501", "report-no": null, "categories": "cs.SI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Isomorphism is one of the classical problems of graph theory for which\nno deterministic polynomial-time algorithm is currently known, but has been\nneither proven to be NP-complete. Several heuristic algorithms have been\nproposed to determine whether or not two graphs are isomorphic (i.e.,\nstructurally the same). In this research, we propose to use the sequence\n(either the non-decreasing or nonincreasing order) of eigenvector centrality\n(EVC) values of the vertices of two graphs as a precursor step to decide\nwhether or not to further conduct tests for graph isomorphism. The eigenvector\ncentrality of a vertex in a graph is a measure of the degree of the vertex as\nwell as the degrees of its neighbors. We hypothesize that if the non-increasing\n(or non-decreasing) order of listings of the EVC values of the vertices of two\ntest graphs are not the same, then the two graphs are not isomorphic. If two\ntest graphs have an identical non-increasing order of the EVC sequence, then\nthey are declared to be potentially isomorphic and confirmed through additional\nheuristics. We test our hypothesis on random graphs (generated according to the\nErdos-Renyi model) and we observe the hypothesis to be indeed true: graph pairs\nthat have the same sequence of non-increasing order of EVC values have been\nconfirmed to be isomorphic using the well-known Nauty software.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 14:47:33 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Meghanathan", "Natarajan", ""]]}, {"id": "1511.06773", "submitter": "Sebastian Krinninger", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, Thatchaphol\n  Saranurak", "title": "Unifying and Strengthening Hardness for Dynamic Problems via the Online\n  Matrix-Vector Multiplication Conjecture", "comments": "A preliminary version of this paper was presented at the 47th ACM\n  Symposium on Theory of Computing (STOC 2015)", "journal-ref": null, "doi": "10.1145/2746539.2746609", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following Online Boolean Matrix-Vector Multiplication problem:\nWe are given an $n\\times n$ matrix $M$ and will receive $n$ column-vectors of\nsize $n$, denoted by $v_1,\\ldots,v_n$, one by one. After seeing each vector\n$v_i$, we have to output the product $Mv_i$ before we can see the next vector.\nA naive algorithm can solve this problem using $O(n^3)$ time in total, and its\nrunning time can be slightly improved to $O(n^3/\\log^2 n)$ [Williams SODA'07].\nWe show that a conjecture that there is no truly subcubic ($O(n^{3-\\epsilon})$)\ntime algorithm for this problem can be used to exhibit the underlying\npolynomial time hardness shared by many dynamic problems. For a number of\nproblems, such as subgraph connectivity, Pagh's problem, $d$-failure\nconnectivity, decremental single-source shortest paths, and decremental\ntransitive closure, this conjecture implies tight hardness results. Thus,\nproving or disproving this conjecture will be very interesting as it will\neither imply several tight unconditional lower bounds or break through a common\nbarrier that blocks progress with these problems. This conjecture might also be\nconsidered as strong evidence against any further improvement for these\nproblems since refuting it will imply a major breakthrough for combinatorial\nBoolean matrix multiplication and other long-standing problems if the term\n\"combinatorial algorithms\" is interpreted as \"non-Strassen-like algorithms\"\n[Ballard et al. SPAA'11]. The conjecture also leads to hardness results for\nproblems that were previously based on diverse problems and conjectures, such\nas 3SUM, combinatorial Boolean matrix multiplication, triangle detection, and\nmultiphase, thus providing a uniform way to prove polynomial hardness results\nfor dynamic algorithms; some of the new proofs are also simpler or even become\ntrivial. The conjecture also leads to stronger and new, non-trivial, hardness\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 21:08:18 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1511.06905", "submitter": "Anjeneya Swami Kare Mr.", "authors": "Anjeneya Swami Kare", "title": "A Simple Algorithm For Replacement Paths Problem", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G=(V,E)(|V|=n and |E|=m) be an undirected graph with positive edge\nweights. Let P_{G}(s, t) be a shortest s-t path in G. Let l be the number of\nedges in P_{G}(s, t). The \\emph{Edge Replacement Path} problem is to compute a\nshortest s-t path in G\\{e}, for every edge e in P_{G}(s, t). The \\emph{Node\nReplacement Path} problem is to compute a shortest s-t path in G\\{v}, for every\nvertex v in P_{G}(s, t). In this paper we present an O(T_{SPT}(G)+m+l^2) time\nand O(m+l^2) space algorithm for both the problems. Where, T_{SPT}(G) is the\nasymptotic time to compute a single source shortest path tree in G. The\nproposed algorithm is simple and easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 17:32:42 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Kare", "Anjeneya Swami", ""]]}, {"id": "1511.07020", "submitter": "Damian Straszak", "authors": "Damian Straszak, Nisheeth K. Vishnoi", "title": "On a Natural Dynamics for Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.DS math.OC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study dynamics inspired by Physarum polycephalum (a slime\nmold) for solving linear programs [NTY00, IJNT11, JZ12]. These dynamics are\narrived at by a local and mechanistic interpretation of the inner workings of\nthe slime mold and a global optimization perspective has been lacking even in\nthe simplest of instances. Our first result is an interpretation of the\ndynamics as an optimization process. We show that Physarum dynamics can be seen\nas a steepest-descent type algorithm on a certain Riemannian manifold.\nMoreover, we prove that the trajectories of Physarum are in fact paths of\noptimizers to a parametrized family of convex programs, in which the objective\nis a linear cost function regularized by an entropy barrier. Subsequently, we\nrigorously establish several important properties of solution curves of\nPhysarum. We prove global existence of such solutions and show that they have\nlimits, being optimal solutions of the underlying LP. Finally, we show that the\ndiscretization of the Physarum dynamics is efficient for a class of linear\nprograms, which include unimodular constraint matrices. Thus, together, our\nresults shed some light on how nature might be solving instances of perhaps the\nmost complex problem in P: linear programming.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 14:54:46 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1511.07038", "submitter": "Jakub Tarnawski", "authors": "Ola Svensson and Jakub Tarnawski and L\\'aszl\\'o A. V\\'egh", "title": "Constant Factor Approximation for ATSP with Two Edge Weights", "comments": null, "journal-ref": "Proc. of Integer Programming and Combinatorial Optimization: 18th\n  International Conference, IPCO 2016, pages 226-237", "doi": "10.1007/978-3-319-33461-5_19", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a constant factor approximation algorithm for the Asymmetric\nTraveling Salesman Problem on shortest path metrics of directed graphs with two\ndifferent edge weights. For the case of unit edge weights, the first constant\nfactor approximation was given recently by Svensson. This was accomplished by\nintroducing an easier problem called Local-Connectivity ATSP and showing that a\ngood solution to this problem can be used to obtain a constant factor\napproximation for ATSP. In this paper, we solve Local-Connectivity ATSP for two\ndifferent edge weights. The solution is based on a flow decomposition theorem\nfor solutions of the Held-Karp relaxation, which may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 17:42:34 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 13:58:47 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Svensson", "Ola", ""], ["Tarnawski", "Jakub", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1511.07070", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk", "title": "Which Regular Expression Patterns are Hard to Match?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions constitute a fundamental notion in formal language theory\nand are frequently used in computer science to define search patterns. A\nclassic algorithm for these problems constructs and simulates a\nnon-deterministic finite automaton corresponding to the expression, resulting\nin an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is\nthe length of the text). This running time can be improved slightly (by a\npolylogarithmic factor), but no significantly faster solutions are known. At\nthe same time, much faster algorithms exist for various special cases of\nregular expressions, including dictionary matching, wildcard matching, subset\nmatching, word break problem etc.\n  In this paper, we show that the complexity of regular expression matching can\nbe characterized based on its {\\em depth} (when interpreted as a formula). Our\nresults hold for expressions involving concatenation, OR, Kleene star and\nKleene plus. For regular expressions of depth two (involving any combination of\nthe above operators), we show the following dichotomy: matching and membership\ntesting can be solved in near-linear time, except for \"concatenations of\nstars\", which cannot be solved in strongly sub-quadratic time assuming the\nStrong Exponential Time Hypothesis (SETH). For regular expressions of depth\nthree the picture is more complex. Nevertheless, we show that all problems can\neither be solved in strongly sub-quadratic time, or cannot be solved in\nstrongly sub-quadratic time assuming SETH.\n  An intriguing special case of membership testing involves regular expressions\nof the form \"a star of an OR of concatenations\", e.g., $[a|ab|bc]^*$. This\ncorresponds to the so-called {\\em word break} problem, for which a dynamic\nprogramming algorithm with a runtime of (roughly) $O(n\\sqrt{m})$ is known. We\nshow that the latter bound is not tight and improve the runtime to\n$O(nm^{0.44\\ldots})$.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 21:10:49 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 22:37:30 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""]]}, {"id": "1511.07077", "submitter": "Friedrich Eisenbrand", "authors": "Alfonso Cevallos and Friedrich Eisenbrand and Rico Zenklusen", "title": "Max-sum diversity via convex programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity maximization is an important concept in information retrieval,\ncomputational geometry and operations research. Usually, it is a variant of the\nfollowing problem: Given a ground set, constraints, and a function $f(\\cdot)$\nthat measures diversity of a subset, the task is to select a feasible subset\n$S$ such that $f(S)$ is maximized. The \\emph{sum-dispersion} function $f(S) =\n\\sum_{x,y \\in S} d(x,y)$, which is the sum of the pairwise distances in $S$, is\nin this context a prominent diversification measure. The corresponding\ndiversity maximization is the \\emph{max-sum} or \\emph{sum-sum diversification}.\nMany recent results deal with the design of constant-factor approximation\nalgorithms of diversification problems involving sum-dispersion function under\na matroid constraint. In this paper, we present a PTAS for the max-sum\ndiversification problem under a matroid constraint for distances\n$d(\\cdot,\\cdot)$ of \\emph{negative type}. Distances of negative type are, for\nexample, metric distances stemming from the $\\ell_2$ and $\\ell_1$ norm, as well\nas the cosine or spherical, or Jaccard distance which are popular similarity\nmetrics in web and image search.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 22:25:47 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Cevallos", "Alfonso", ""], ["Eisenbrand", "Friedrich", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1511.07147", "submitter": "Rishi Gupta", "authors": "Rishi Gupta and Tim Roughgarden", "title": "A PAC Approach to Application-Specific Algorithm Selection", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best algorithm for a computational problem generally depends on the\n\"relevant inputs,\" a concept that depends on the application domain and often\ndefies formal articulation. While there is a large literature on empirical\napproaches to selecting the best algorithm for a given application domain,\nthere has been surprisingly little theoretical analysis of the problem.\n  This paper adapts concepts from statistical and online learning theory to\nreason about application-specific algorithm selection. Our models capture\nseveral state-of-the-art empirical and theoretical approaches to the problem,\nranging from self-improving algorithms to empirical performance models, and our\nresults identify conditions under which these approaches are guaranteed to\nperform well. We present one framework that models algorithm selection as a\nstatistical learning problem, and our work here shows that dimension notions\nfrom statistical learning theory, historically used to measure the complexity\nof classes of binary- and real-valued functions, are relevant in a much broader\nalgorithmic context. We also study the online version of the algorithm\nselection problem, and give possibility and impossibility results for the\nexistence of no-regret learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 09:30:19 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 21:06:20 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Gupta", "Rishi", ""], ["Roughgarden", "Tim", ""]]}, {"id": "1511.07180", "submitter": "Florin Manea", "authors": "Marius Dumitran, Pawe{\\l} Gawrychowski and Florin Manea", "title": "Longest Gapped Repeats and Palindromes", "comments": "This is an extension of the conference papers \"Longest\n  $\\alpha$-Gapped Repeat and Palindrome\", presented by the second and third\n  authors at FCT 2015, and \"Longest Gapped Repeats and Palindromes\", presented\n  by the first and third authors at MFCS 2015", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  4, FCT '15, special issue FCT'15 (October 13, 2017) dmtcs:3988", "doi": "10.23638/DMTCS-19-4-4", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A gapped repeat (respectively, palindrome) occurring in a word $w$ is a\nfactor $uvu$ (respectively, $u^Rvu$) of $w$. In such a repeat (palindrome) $u$\nis called the arm of the repeat (respectively, palindrome), while $v$ is called\nthe gap. We show how to compute efficiently, for every position $i$ of the word\n$w$, the longest gapped repeat and palindrome occurring at that position,\nprovided that the length of the gap is subject to various types of\nrestrictions. That is, that for each position $i$ we compute the longest prefix\n$u$ of $w[i..n]$ such that $uv$ (respectively, $u^Rv$) is a suffix of\n$w[1..i-1]$ (defining thus a gapped repeat $uvu$ -- respectively, palindrome\n$u^Rvu$), and the length of $v$ is subject to the aforementioned restrictions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 11:19:00 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:50:14 GMT"}, {"version": "v3", "created": "Thu, 29 Jun 2017 15:02:47 GMT"}, {"version": "v4", "created": "Wed, 11 Oct 2017 09:40:01 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Dumitran", "Marius", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Manea", "Florin", ""]]}, {"id": "1511.07263", "submitter": "Cameron Musco", "authors": "Michael B. Cohen, Cameron Musco, Christopher Musco", "title": "Input Sparsity Time Low-Rank Approximation via Ridge Leverage Score\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for finding a near optimal low-rank approximation\nof a matrix $A$ in $O(nnz(A))$ time. Our method is based on a recursive\nsampling scheme for computing a representative subset of $A$'s columns, which\nis then used to find a low-rank approximation.\n  This approach differs substantially from prior $O(nnz(A))$ time algorithms,\nwhich are all based on fast Johnson-Lindenstrauss random projections. It\nmatches the guarantees of these methods while offering a number of advantages.\n  Not only are sampling algorithms faster for sparse and structured data, but\nthey can also be applied in settings where random projections cannot. For\nexample, we give new single-pass streaming algorithms for the column subset\nselection and projection-cost preserving sample problems. Our method has also\nbeen used to give the fastest algorithms for provably approximating kernel\nmatrices [MM16].\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 15:10:05 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 22:48:49 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Cohen", "Michael B.", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1511.07412", "submitter": "Ger Yang", "authors": "Ger Yang, Evdokia Nikolova", "title": "Approximation Algorithms for Route Planning with Nonlinear Objectives", "comments": "9 pages, 2 figures, main part of this paper is to be appear in\n  AAAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimal route planning when the objective function is a general\nnonlinear and non-monotonic function. Such an objective models user behavior\nmore accurately, for example, when a user is risk-averse, or the utility\nfunction needs to capture a penalty for early arrival. It is known that as\nnonlinearity arises, the problem becomes NP-hard and little is known about\ncomputing optimal solutions when in addition there is no monotonicity\nguarantee. We show that an approximately optimal non-simple path can be\nefficiently computed under some natural constraints. In particular, we provide\na fully polynomial approximation scheme under hop constraints. Our\napproximation algorithm can extend to run in pseudo-polynomial time under a\nmore general linear constraint that sometimes is useful. As a by-product, we\nshow that our algorithm can be applied to the problem of finding a path that is\nmost likely to be on time for a given deadline.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:48:29 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Yang", "Ger", ""], ["Nikolova", "Evdokia", ""]]}, {"id": "1511.07494", "submitter": "Bartosz Rybicki", "authors": "Jaros{\\l}aw Byrka, Bartosz Rybicki, Sumedha Uniyal", "title": "An approximation algorithm for Uniform Capacitated k-Median problem with\n  1 + {\\epsilon} capacity violation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Capacitated k-Median problem, for which all the known constant\nfactor approximation algorithms violate either the number of facilities or the\ncapacities. While the standard LP-relaxation can only be used for algorithms\nviolating one of the two by a factor of at least two, Shi Li [SODA'15, SODA'16]\ngave algorithms violating the number of facilities by a factor of 1+{\\epsilon}\nexploring properties of extended relaxations.\n  In this paper we develop a constant factor approximation algorithm for\nUniform Capacitated k-Median violating only the capacities by a factor of\n1+{\\epsilon}. The algorithm is based on a configuration LP. Unlike in the\nalgorithms violating the number of facilities, we cannot simply open extra few\nfacilities at selected locations. Instead, our algorithm decides about the\nfacility openings in a carefully designed dependent rounding process.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 22:25:40 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Rybicki", "Bartosz", ""], ["Uniyal", "Sumedha", ""]]}, {"id": "1511.07527", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Tradeoffs for nearest neighbors on the sphere", "comments": "16 pages, 1 table, 2 figures. Mostly subsumed by arXiv:1608.03580\n  [cs.DS] (along with arXiv:1605.02701 [cs.DS])", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tradeoffs between the query and update complexities for the\n(approximate) nearest neighbor problem on the sphere, extending the recent\nspherical filters to sparse regimes and generalizing the scheme and analysis to\naccount for different tradeoffs. In a nutshell, for the sparse regime the\ntradeoff between the query complexity $n^{\\rho_q}$ and update complexity\n$n^{\\rho_u}$ for data sets of size $n$ is given by the following equation in\nterms of the approximation factor $c$ and the exponents $\\rho_q$ and $\\rho_u$:\n$$c^2\\sqrt{\\rho_q}+(c^2-1)\\sqrt{\\rho_u}=\\sqrt{2c^2-1}.$$\n  For small $c=1+\\epsilon$, minimizing the time for updates leads to a linear\nspace complexity at the cost of a query time complexity $n^{1-4\\epsilon^2}$.\nBalancing the query and update costs leads to optimal complexities\n$n^{1/(2c^2-1)}$, matching bounds from [Andoni-Razenshteyn, 2015] and [Dubiner,\nIEEE-TIT'10] and matching the asymptotic complexities of [Andoni-Razenshteyn,\nSTOC'15] and [Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt, NIPS'15]. A\nsubpolynomial query time complexity $n^{o(1)}$ can be achieved at the cost of a\nspace complexity of the order $n^{1/(4\\epsilon^2)}$, matching the bound\n$n^{\\Omega(1/\\epsilon^2)}$ of [Andoni-Indyk-Patrascu, FOCS'06] and\n[Panigrahy-Talwar-Wieder, FOCS'10] and improving upon results of\n[Indyk-Motwani, STOC'98] and [Kushilevitz-Ostrovsky-Rabani, STOC'98].\n  For large $c$, minimizing the update complexity results in a query complexity\nof $n^{2/c^2+O(1/c^4)}$, improving upon the related exponent for large $c$ of\n[Kapralov, PODS'15] by a factor $2$, and matching the bound $n^{\\Omega(1/c^2)}$\nof [Panigrahy-Talwar-Wieder, FOCS'08]. Balancing the costs leads to optimal\ncomplexities $n^{1/(2c^2-1)}$, while a minimum query time complexity can be\nachieved with update complexity $n^{2/c^2+O(1/c^4)}$, improving upon the\nprevious best exponents of Kapralov by a factor $2$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 01:07:08 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 00:21:29 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1511.07529", "submitter": "Christopher Whidden", "authors": "Chris Whidden and Frederick A. Matsen IV", "title": "Calculating the Unrooted Subtree Prune-and-Regraft Distance", "comments": "21 double-column pages, 11 figures. Revised in response to peer\n  review. The sections introducing socket forests and on chain reduction were\n  spun off into a conference-length paper arXiv:1611.02351 to reduce the length\n  and complexity of the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subtree prune-and-regraft (SPR) distance metric is a fundamental way of\ncomparing evolutionary trees. It has wide-ranging applications, such as to\nstudy lateral genetic transfer, viral recombination, and Markov chain Monte\nCarlo phylogenetic inference. Although the rooted version of SPR distance can\nbe computed relatively efficiently between rooted trees using\nfixed-parameter-tractable maximum agreement forest (MAF) algorithms, no MAF\nformulation is known for the unrooted case. Correspondingly, previous\nalgorithms are unable to compute unrooted SPR distances larger than 7.\n  In this paper, we substantially advance understanding of and computational\nalgorithms for the unrooted SPR distance. First we identify four properties of\noptimal SPR paths, each of which suggests that no MAF formulation exists in the\nunrooted case. Then we introduce the replug distance, a new lower bound on the\nunrooted SPR distance that is amenable to MAF methods, and give an efficient\nfixed-parameter algorithm for calculating it. Finally, we develop a\n\"progressive A*\" search algorithm using multiple heuristics, including the TBR\nand replug distances, to exactly compute the unrooted SPR distance. Our\nalgorithm is nearly two orders of magnitude faster than previous methods on\nsmall trees, and allows computation of unrooted SPR distances as large as 14 on\ntrees with 50 leaves.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 01:15:51 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 22:34:00 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 21:13:59 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Whidden", "Chris", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1511.07559", "submitter": "Chi-Kin Chau", "authors": "Chi-Kin Chau and Guanglin Zhang and Minghua Chen", "title": "Cost Minimizing Online Algorithms for Energy Storage Management with\n  Worst-case Guarantee", "comments": "To appear in IEEE Transactions on Smart Grid", "journal-ref": "IEEE Transactions on Smart Grid, Vol. 7, No. 6, pp2691-2702 (Nov\n  2016)", "doi": "10.1109/TSG.2016.2514412", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fluctuations of electricity prices in demand response schemes and\nintermittency of renewable energy supplies necessitate the adoption of energy\nstorage in microgrids. However, it is challenging to design effective real-time\nenergy storage management strategies that can deliver assured optimality,\nwithout being hampered by the uncertainty of volatile electricity prices and\nrenewable energy supplies. This paper presents a simple effective online\nalgorithm for the charging and discharging decisions of energy storage that\nminimizes the electricity cost in the presence of electricity price\nfluctuations and renewable energy supplies, without relying on the future\ninformation of prices, demands or renewable energy supplies. The proposed\nalgorithm is supported by a near-best worst-case guarantee (i.e., competitive\nratio), as compared to the offline optimal decisions based on full future\ninformation. Furthermore, the algorithm can be adapted to take advantage of\nlimited future information, if available. By simulations on real-world data, it\nis observed that the proposed algorithms can achieve satisfactory outcome in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 04:08:54 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 09:22:50 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Chau", "Chi-Kin", ""], ["Zhang", "Guanglin", ""], ["Chen", "Minghua", ""]]}, {"id": "1511.07605", "submitter": "Nisheeth Vishnoi", "authors": "Christos H. Papadimitriou and Nisheeth K. Vishnoi", "title": "On the Computational Complexity of Limit Cycles in Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Poincare-Bendixson theorem for two-dimensional continuous\ndynamical systems in compact domains from the point of view of computation,\nseeking algorithms for finding the limit cycle promised by this classical\nresult. We start by considering a discrete analogue of this theorem and show\nthat both finding a point on a limit cycle, and determining if a given point is\non one, are PSPACE-complete.\n  For the continuous version, we show that both problems are uncomputable in\nthe real complexity sense; i.e., their complexity is arbitrarily high.\nSubsequently, we introduce a notion of an \"approximate cycle\" and prove an\n\"approximate\" Poincar\\'e-Bendixson theorem guaranteeing that some orbits come\nvery close to forming a cycle in the absence of approximate fixpoints;\nsurprisingly, it holds for all dimensions. The corresponding computational\nproblem defined in terms of arithmetic circuits is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:31:03 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Papadimitriou", "Christos H.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1511.07642", "submitter": "Pradeesha Ashok", "authors": "Pradeesha Ashok, Sudeshna Kolay, Saket Saurabh", "title": "Multivariate Complexity Analysis of Geometric {\\sc Red Blue Set Cover}", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity of GENERALIZED RED BLUE SET COVER\n(Gen-RBSC), a generalization of the classic SET COVER problem and the more\nrecently studied RED BLUE SET COVER problem. Given a universe $U$ containing\n$b$ blue elements and $r$ red elements, positive integers $k_\\ell$ and $k_r$,\nand a family $\\F$ of $\\ell$ sets over $U$, the \\srbsc\\ problem is to decide\nwhether there is a subfamily $\\F'\\subseteq \\F$ of size at most $k_\\ell$ that\ncovers all blue elements, but at most $k_r$ of the red elements. This\ngeneralizes SET COVER and thus in full generality it is intractable in the\nparameterized setting. In this paper, we study a geometric version of this\nproblem, called Gen-RBSC-lines, where the elements are points in the plane and\nsets are defined by lines. We study this problem for an array of parameters,\nnamely, $k_\\ell, k_r, r, b$, and $\\ell$, and all possible combinations of them.\nFor all these cases, we either prove that the problem is W-hard or show that\nthe problem is fixed parameter tractable (FPT). In particular, on the\nalgorithmic side, our study shows that a combination of $k_\\ell$ and $k_r$\ngives rise to a nontrivial algorithm for Gen-RBSC-lines. On the hardness side,\nwe show that the problem is para-NP-hard when parameterized by $k_r$, and\nW[1]-hard when parameterized by $k_\\ell$. Finally, for the combination of\nparameters for which Gen-RBSC-lines admits FPT algorithms, we ask for the\nexistence of polynomial kernels. We are able to provide a complete\nkernelization dichotomy by either showing that the problem admits a polynomial\nkernel or that it does not contain a polynomial kernel unless $\\CoNP \\subseteq\n\\NP/\\mbox{poly}$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 10:53:03 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 12:35:17 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Ashok", "Pradeesha", ""], ["Kolay", "Sudeshna", ""], ["Saurabh", "Saket", ""]]}, {"id": "1511.07741", "submitter": "Loukas Georgiadis", "authors": "Loukas Georgiadis, Robert E. Tarjan", "title": "A Note on Fault Tolerant Reachability for Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we describe an application of low-high orders in fault-tolerant\nnetwork design. Baswana et al. [DISC 2015] study the following reachability\nproblem. We are given a flow graph $G = (V, A)$ with start vertex $s$, and a\nspanning tree $T =(V, A_T)$ rooted at $s$. We call a set of arcs $A'$ valid if\nthe subgraph $G' = (V, A_T \\cup A')$ of $G$ has the same dominators as $G$. The\ngoal is to find a valid set of minimum size. Baswana et al. gave an $O(m\n\\log{n})$-time algorithm to compute a minimum-size valid set in $O(m \\log{n})$\ntime, where $n = |V|$ and $m = |A|$. Here we provide a simple $O(m)$-time\nalgorithm that uses the dominator tree $D$ of $G$ and a low-high order of it.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 14:55:45 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1511.07826", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal, Aravind Srinivasan and Ola Svensson", "title": "Lift-and-Round to Improve Weighted Completion Time on Unrelated Machines", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling jobs on unrelated machines so as to\nminimize the sum of weighted completion times. Our main result is a\n$(3/2-c)$-approximation algorithm for some fixed $c>0$, improving upon the\nlong-standing bound of 3/2 (independently due to Skutella, Journal of the ACM,\n2001, and Sethuraman & Squillante, SODA, 1999). To do this, we first introduce\na new lift-and-project based SDP relaxation for the problem. This is necessary\nas the previous convex programming relaxations have an integrality gap of\n$3/2$. Second, we give a new general bipartite-rounding procedure that produces\nan assignment with certain strong negative correlation properties.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 18:08:10 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 18:52:14 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Bansal", "Nikhil", ""], ["Srinivasan", "Aravind", ""], ["Svensson", "Ola", ""]]}, {"id": "1511.07983", "submitter": "Teng Li", "authors": "Teng Li, Vikram K. Narayana, Tarek El-Ghazawi", "title": "Reordering GPU Kernel Launches to Enable Efficient Concurrent Execution", "comments": "2 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary GPUs allow concurrent execution of small computational kernels\nin order to prevent idling of GPU resources. Despite the potential concurrency\nbetween independent kernels, the order in which kernels are issued to the GPU\nwill significantly influence the application performance. A technique for\nderiving suitable kernel launch orders is therefore presented, with the aim of\nreducing the total execution time. Experimental results indicate that the\nproposed method yields solutions that are well above the 90 percentile mark in\nthe design space of all possible permutations of the kernel launch sequences.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 08:01:18 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Li", "Teng", ""], ["Narayana", "Vikram K.", ""], ["El-Ghazawi", "Tarek", ""]]}, {"id": "1511.08152", "submitter": "Xiangkun Shen", "authors": "Jon Lee, Viswanath Nagarajan, and Xiangkun Shen", "title": "Max-Cut under Graph Constraints", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-33461-5_5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of the graph-constrained max-cut (GCMC) problem consists of (i)\nan undirected graph G and (ii) edge-weights on a complete undirected graph on\nthe same vertex set. The objective is to find a subset of vertices satisfying\nsome graph-based constraint in G that maximizes the total weight of edges in\nthe cut. The types of graph constraints we can handle include independent set,\nvertex cover, dominating set and connectivity. Our main results are for the\ncase when G is a graph with bounded treewidth, where we obtain a\n0.5-approximation algorithm. Our algorithm uses an LP relaxation based on the\nSherali-Adams hierarchy. It can handle any graph constraint for which there is\na (certain type of) dynamic program that exactly optimizes linear objectives.\nUsing known decomposition results, these imply essentially the same\napproximation ratio for GCMC under constraints such as independent set,\ndominating set and connectivity on a planar graph G (more generally for\nbounded-genus or excluded-minor graphs).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 19:01:05 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Lee", "Jon", ""], ["Nagarajan", "Viswanath", ""], ["Shen", "Xiangkun", ""]]}, {"id": "1511.08270", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Ameet Gadekar and Suprovat Ghoshal and Rishi\n  Saket", "title": "On the hardness of learning sparse parities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the hardness of computing sparse solutions to systems\nof linear equations over F_2. Consider the k-EvenSet problem: given a\nhomogeneous system of linear equations over F_2 on n variables, decide if there\nexists a nonzero solution of Hamming weight at most k (i.e. a k-sparse\nsolution). While there is a simple O(n^{k/2})-time algorithm for it,\nestablishing fixed parameter intractability for k-EvenSet has been a notorious\nopen problem. Towards this goal, we show that unless k-Clique can be solved in\nn^{o(k)} time, k-EvenSet has no poly(n)2^{o(sqrt{k})} time algorithm and no\npolynomial time algorithm when k = (log n)^{2+eta} for any eta > 0.\n  Our work also shows that the non-homogeneous generalization of the problem --\nwhich we call k-VectorSum -- is W[1]-hard on instances where the number of\nequations is O(k log n), improving on previous reductions which produced\nOmega(n) equations. We also show that for any constant eps > 0, given a system\nof O(exp(O(k))log n) linear equations, it is W[1]-hard to decide if there is a\nk-sparse linear form satisfying all the equations or if every function on at\nmost k-variables (k-junta) satisfies at most (1/2 + eps)-fraction of the\nequations. In the setting of computational learning, this shows hardness of\napproximate non-proper learning of k-parities. In a similar vein, we use the\nhardness of k-EvenSet to show that that for any constant d, unless k-Clique can\nbe solved in n^{o(k)} time there is no poly(m, n)2^{o(sqrt{k}) time algorithm\nto decide whether a given set of m points in F_2^n satisfies: (i) there exists\na non-trivial k-sparse homogeneous linear form evaluating to 0 on all the\npoints, or (ii) any non-trivial degree d polynomial P supported on at most k\nvariables evaluates to zero on approx. Pr_{F_2^n}[P(z) = 0] fraction of the\npoints i.e., P is fooled by the set of points.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:08:36 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gadekar", "Ameet", ""], ["Ghoshal", "Suprovat", ""], ["Saket", "Rishi", ""]]}, {"id": "1511.08283", "submitter": "Marcin Pilipczuk", "authors": "Marcin Pilipczuk", "title": "A tight lower bound for Vertex Planarization on graphs of bounded\n  treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Vertex Planarization problem one asks to delete the minimum possible\nnumber of vertices from an input graph to obtain a planar graph. The\nparameterized complexity of this problem, parameterized by the solution size\n(the number of deleted vertices) has recently attracted significant attention.\nThe state-of-the-art algorithm of Jansen, Lokshtanov, and Saurabh [SODA 2014]\nruns in time $2^{O(k \\log k)} \\cdot n$ on $n$-vertex graph with a solution of\nsize $k$. It remains open if one can obtain a single-exponential dependency on\n$k$ in the running time bound.\n  One of the core technical contributions of the work of Jansen, Lokshtanov,\nand Saurabh is an algorithm that solves a weighted variant of Vertex\nPlanarization in time $2^{O(w \\log w)} \\cdot n$ on graphs of treewidth $w$. In\nthis short note we prove that the running time of this routine is tight under\nthe Exponential Time Hypothesis, even in unweighted graphs and when\nparameterizing by treedepth. Consequently, it is unlikely that a potential\nsingle-exponential algorithm for Vertex Planarization parameterized by the\nsolution size can be obtained by merely improving upon the aforementioned\nbounded treewidth subroutine.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 03:44:37 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Pilipczuk", "Marcin", ""]]}, {"id": "1511.08303", "submitter": "Spyros Kontogiannis", "authors": "Spyros Kontogiannis and George Michalopoulos and Georgia Papastavrou\n  and Andreas Paraskevopoulos and Dorothea Wagner and Christos Zaroliagis", "title": "Engineering Oracles for Time-Dependent Road Networks", "comments": "In ALENEX 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement and experimentally evaluate landmark-based oracles for min-cost\npaths in large-scale time-dependent road networks. We exploit parallelism and\nlossless compression, combined with a novel travel-time approximation\ntechnique, to severely reduce preprocessing space and time. We significantly\nimprove the FLAT oracle, improving the previous query time by $30\\%$ and\ndoubling the Dijkstra-rank speedup. We also implement and experimentally\nevaluate a novel oracle (HORN), based on a landmark hierarchy, achieving even\nbetter performance wrt to FLAT.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 07:26:30 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kontogiannis", "Spyros", ""], ["Michalopoulos", "George", ""], ["Papastavrou", "Georgia", ""], ["Paraskevopoulos", "Andreas", ""], ["Wagner", "Dorothea", ""], ["Zaroliagis", "Christos", ""]]}, {"id": "1511.08431", "submitter": "Gabriele Fici", "authors": "Gabriele Fici, Tomasz Kociumaka, Jakub Radoszewski, Wojciech Rytter,\n  Tomasz Wale\\'n", "title": "On the Greedy Algorithm for the Shortest Common Superstring Problem with\n  Reversals", "comments": "Published in Information Processing Letters", "journal-ref": null, "doi": "10.1016/j.ipl.2015.11.015", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variation of the classical Shortest Common Superstring (SCS)\nproblem in which a shortest superstring of a finite set of strings $S$ is\nsought containing as a factor every string of $S$ or its reversal. We call this\nproblem Shortest Common Superstring with Reversals (SCS-R). This problem has\nbeen introduced by Jiang et al., who designed a greedy-like algorithm with\nlength approximation ratio $4$. In this paper, we show that a natural\nadaptation of the classical greedy algorithm for SCS has (optimal) compression\nratio $\\frac12$, i.e., the sum of the overlaps in the output string is at least\nhalf the sum of the overlaps in an optimal solution. We also provide a\nlinear-time implementation of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:55:50 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 14:40:40 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Fici", "Gabriele", ""], ["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1511.08447", "submitter": "Brijesh Dongol", "authors": "Brijesh Dongol, Robert M. Hierons", "title": "Decidability and Complexity for Quiescent Consistency and its Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quiescent consistency is a notion of correctness for a concurrent object that\ngives meaning to the object's behaviours in quiescent states, i.e., states in\nwhich none of the object's operations are being executed. Correctness of an\nimplementation object is defined in terms of a corresponding abstract\nspecification. This gives rise to two important verification questions:\nmembership (checking whether a behaviour of the implementation is allowed by\nthe specification) and correctness (checking whether all behaviours of the\nimplementation are allowed by the specification). In this paper, we show that\nthe membership problem for quiescent consistency is NP-complete and that the\ncorrectness problem is decidable, but coNP-hard and in EXPSPACE. For both\nproblems, we consider restricted versions of quiescent consistency by assuming\nan upper limit on the number of events between two quiescent points. Here, we\nshow that the membership problem is in PTIME, whereas correctness is in PSPACE.\n  Quiescent consistency does not guarantee sequential consistency, i.e., it\nallows operation calls by the same process to be reordered when mapping to an\nabstract specification. Therefore, we also consider quiescent sequential\nconsistency, which strengthens quiescent consistency with an additional\nsequential consistency condition. We show that the unrestricted versions of\nmembership and correctness are NP-complete and undecidable, respectively. When\nby placing a limit on the number of events between two quiescent points,\nmembership is in PTIME, while correctness is in PSPACE. Finally, we consider a\nversion of quiescent sequential consistency that places an upper limit on the\nnumber of processes for every run of the implementation, and show that the\nmembership problem for quiescent sequential consistency with this restriction\nis in PTIME.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 16:46:30 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Dongol", "Brijesh", ""], ["Hierons", "Robert M.", ""]]}, {"id": "1511.08552", "submitter": "Mark Bun", "authors": "Mark Bun and Kobbi Nissim and Uri Stemmer", "title": "Simultaneous Private Learning of Multiple Concepts", "comments": "29 pages. To appear in ITCS '16", "journal-ref": null, "doi": "10.1145/2840728.2840747", "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the direct-sum problem in the context of differentially\nprivate PAC learning: What is the sample complexity of solving $k$ learning\ntasks simultaneously under differential privacy, and how does this cost compare\nto that of solving $k$ learning tasks without privacy? In our setting, an\nindividual example consists of a domain element $x$ labeled by $k$ unknown\nconcepts $(c_1,\\ldots,c_k)$. The goal of a multi-learner is to output $k$\nhypotheses $(h_1,\\ldots,h_k)$ that generalize the input examples.\n  Without concern for privacy, the sample complexity needed to simultaneously\nlearn $k$ concepts is essentially the same as needed for learning a single\nconcept. Under differential privacy, the basic strategy of learning each\nhypothesis independently yields sample complexity that grows polynomially with\n$k$. For some concept classes, we give multi-learners that require fewer\nsamples than the basic strategy. Unfortunately, however, we also give lower\nbounds showing that even for very simple concept classes, the sample cost of\nprivate multi-learning must grow polynomially in $k$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 03:57:22 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Bun", "Mark", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "1511.08598", "submitter": "Vladim\\'ir \\v{C}un\\'at", "authors": "Vladim\\'ir \\v{C}un\\'at", "title": "Predecessor problem on smooth distributions", "comments": "6 pages, to be submitted to DMTCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We follow a research thread studying the predecessor problem on \"smooth\"\ndistribution families. We propose a conceptually simpler solution utilizing\nwell-known results from much better studied variant of the problem that assumes\nnothing about the input. As a side effect, we are able to extend the range of\nhandled input distributions for the most studied case needing expected\n$\\mathcal O(\\log \\log n)$ time, and we provide better insight into why the\nrelated methods are faster on smooth inputs.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 09:54:00 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 21:05:55 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["\u010cun\u00e1t", "Vladim\u00edr", ""]]}, {"id": "1511.08644", "submitter": "Adam Kurpisz", "authors": "Adam Kurpisz, Samuli Lepp\\\"anen, Monaldo Mastrolilli", "title": "A Lasserre Lower Bound for the Min-Sum Single Machine Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Min-sum single machine scheduling problem (denoted 1||sum f_j)\ngeneralizes a large number of sequencing problems. The first constant\napproximation guarantees have been obtained only recently and are based on\nnatural time-indexed LP relaxations strengthened with the so called\nKnapsack-Cover inequalities (see Bansal and Pruhs, Cheung and Shmoys and the\nrecent 4+\\epsilon-approximation by Mestre and Verschae). These relaxations have\nan integrality gap of 2, since the Min-knapsack problem is a special case. No\nAPX-hardness result is known and it is still conceivable that there exists a\nPTAS. Interestingly, the Lasserre hierarchy relaxation, when the objective\nfunction is incorporated as a constraint, reduces the integrality gap for the\nMin-knapsack problem to 1+\\epsilon.\n  In this paper we study the complexity of the Min-sum single machine\nscheduling problem under algorithms from the Lasserre hierarchy. We prove the\nfirst lower bound for this model by showing that the integrality gap is\nunbounded at level \\Omega(\\sqrt{n}) even for a variant of the problem that is\nsolvable in O(n log n) time by the Moore-Hodgson algorithm, namely Min-number\nof tardy jobs. We consider a natural formulation that incorporates the\nobjective function as a constraint and prove the result by partially\ndiagonalizing the matrix associated with the relaxation and exploiting this\ncharacterization.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 12:22:38 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kurpisz", "Adam", ""], ["Lepp\u00e4nen", "Samuli", ""], ["Mastrolilli", "Monaldo", ""]]}, {"id": "1511.08647", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Lior Kamma, Robert Krauthgamer", "title": "Tight Bounds for Gomory-Hu-like Cut Counting", "comments": "This version contains additional references to previous work (which\n  have some overlap with our results), see Bibliographic Update 1.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By a classical result of Gomory and Hu (1961), in every edge-weighted graph\n$G=(V,E,w)$, the minimum $st$-cut values, when ranging over all $s,t\\in V$,\ntake at most $|V|-1$ distinct values. That is, these $\\binom{|V|}{2}$ instances\nexhibit redundancy factor $\\Omega(|V|)$. They further showed how to construct\nfrom $G$ a tree $(V,E',w')$ that stores all minimum $st$-cut values. Motivated\nby this result, we obtain tight bounds for the redundancy factor of several\ngeneralizations of the minimum $st$-cut problem.\n  1. Group-Cut: Consider the minimum $(A,B)$-cut, ranging over all subsets\n$A,B\\subseteq V$ of given sizes $|A|=\\alpha$ and $|B|=\\beta$. The redundancy\nfactor is $\\Omega_{\\alpha,\\beta}(|V|)$.\n  2. Multiway-Cut: Consider the minimum cut separating every two vertices of\n$S\\subseteq V$, ranging over all subsets of a given size $|S|=k$. The\nredundancy factor is $\\Omega_{k}(|V|)$.\n  3. Multicut: Consider the minimum cut separating every demand-pair in\n$D\\subseteq V\\times V$, ranging over collections of $|D|=k$ demand pairs. The\nredundancy factor is $\\Omega_{k}(|V|^k)$. This result is a bit surprising, as\nthe redundancy factor is much larger than in the first two problems.\n  A natural application of these bounds is to construct small data structures\nthat stores all relevant cut values, like the Gomory-Hu tree. We initiate this\ndirection by giving some upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 12:40:24 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 12:03:41 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 10:46:36 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Kamma", "Lior", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1511.08990", "submitter": "Artem Barger", "authors": "Artem Barger and Dan Feldman", "title": "k-Means for Streaming and Distributed Big Sparse Data", "comments": "16 pages, 44 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first streaming algorithm for computing a provable\napproximation to the $k$-means of sparse Big data. Here, sparse Big Data is a\nset of $n$ vectors in $\\mathbb{R}^d$, where each vector has $O(1)$ non-zeroes\nentries, and $d\\geq n$. E.g., adjacency matrix of a graph, web-links, social\nnetwork, document-terms, or image-features matrices.\n  Our streaming algorithm stores at most $\\log n\\cdot k^{O(1)}$ input points in\nmemory. If the stream is distributed among $M$ machines, the running time\nreduces by a factor of $M$, while communicating a total of $M\\cdot k^{O(1)}$\n(sparse) input points between the machines.\n  % Our main technical result is a deterministic algorithm for computing a\nsparse $(k,\\epsilon)$-coreset, which is a weighted subset of $k^{O(1)}$ input\npoints that approximates the sum of squared distances from the $n$ input points\nto every $k$ centers, up to $(1\\pm\\epsilon)$ factor, for any given constant\n$\\epsilon>0$. This is the first such coreset of size independent of both $d$\nand $n$.\n  Existing algorithms use coresets of size at least polynomial in $d$, or\nproject the input points on a subspace which diminishes their sparsity, thus\nrequire memory and communication $\\Omega(d)=\\Omega(n)$ even for $k=2$.\n  Experimental results real public datasets shows that our algorithm boost the\nperformance of such given heuristics even in the off-line setting. Open code is\nprovided for reproducibility.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 10:06:11 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 17:01:46 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Barger", "Artem", ""], ["Feldman", "Dan", ""]]}, {"id": "1511.09123", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "A Short Survey on Data Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapidly increasing data, clustering algorithms are important tools for\ndata analytics in modern research. They have been successfully applied to a\nwide range of domains; for instance, bioinformatics, speech recognition, and\nfinancial analysis. Formally speaking, given a set of data instances, a\nclustering algorithm is expected to divide the set of data instances into the\nsubsets which maximize the intra-subset similarity and inter-subset\ndissimilarity, where a similarity measure is defined beforehand. In this work,\nthe state-of-the-arts clustering algorithms are reviewed from design concept to\nmethodology; Different clustering paradigms are discussed. Advanced clustering\nalgorithms are also discussed. After that, the existing clustering evaluation\nmetrics are reviewed. A summary with future insights is provided at the end.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 08:02:37 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1511.09156", "submitter": "Takuro Fukunaga", "authors": "Takuro Fukunaga", "title": "Constant-approximation algorithms for highly connected multi-dominating\n  sets in unit disk graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph on a node set $V$ and positive integers $k$ and\n$m$, a $k$-connected $m$-dominating set ($(k,m)$-CDS) is defined as a subset\n$S$ of $V$ such that each node in $V \\setminus S$ has at least $m$ neighbors in\n$S$, and a $k$-connected subgraph is induced by $S$. The weighted $(k,m)$-CDS\nproblem is to find a minimum weight $(k,m)$-CDS in a given node-weighted graph.\nThe problem is called the unweighted $(k,m)$-CDS problem if the objective is to\nminimize the cardinality of a $(k,m)$-CDS. These problems have been actively\nstudied for unit disk graphs, motivated by the application of constructing a\nvirtual backbone in a wireless ad hoc network. However, constant-approximation\nalgorithms are known only for $k \\leq 3$ in the unweighted $(k,m)$-CDS problem,\nand for $(k,m)=(1,1)$ in the weighted $(k,m)$-CDS problem. In this paper, we\nconsider the case in which $m \\geq k$, and we present a simple $O(5^k\nk!)$-approximation algorithm for the unweighted $(k,m)$-CDS problem, and a\nprimal-dual $O(k^2 \\log k)$-approximation algorithm for the weighted\n$(k,m)$-CDS problem. Both algorithms achieve constant approximation factors\nwhen $k$ is a fixed constant.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 04:59:24 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 01:20:37 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Fukunaga", "Takuro", ""]]}, {"id": "1511.09196", "submitter": "Hamid Kameli", "authors": "Hamid Kameli", "title": "Non-adaptive Group Testing on Graphs", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 20 no.\n  1, Combinatorics (March 26, 2018) dmtcs:4351", "doi": "10.23638/DMTCS-20-1-9", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grebinski and Kucherov (1998) and Alon et al. (2004-2005) study the problem\nof learning a hidden graph for some especial cases, such as hamiltonian cycle,\ncliques, stars, and matchings. This problem is motivated by problems in\nchemical reactions, molecular biology and genome sequencing.\n  In this paper, we present a generalization of this problem. Precisely, we\nconsider a graph G and a subgraph H of G and we assume that G contains exactly\none defective subgraph isomorphic to H. The goal is to find the defective\nsubgraph by testing whether an induced subgraph contains an edge of the\ndefective subgraph, with the minimum number of tests. We present an upper bound\nfor the number of tests to find the defective subgraph by using the symmetric\nand high probability variation of Lov\\'asz Local Lemma.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 08:26:10 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 10:11:58 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 15:14:24 GMT"}, {"version": "v4", "created": "Sat, 30 Dec 2017 12:07:46 GMT"}, {"version": "v5", "created": "Sun, 4 Mar 2018 09:39:48 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Kameli", "Hamid", ""]]}, {"id": "1511.09208", "submitter": "Thomas Kesselheim", "authors": "Paul D\\\"utting, Thomas Kesselheim, \\'Eva Tardos", "title": "Algorithms as Mechanisms: The Price of Anarchy of Relax-and-Round", "comments": "Extended abstract appeared in Proc. of 16th ACM Conference on\n  Economics and Computation (EC'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms that are originally designed without explicitly considering\nincentive properties are later combined with simple pricing rules and used as\nmechanisms. The resulting mechanisms are often natural and simple to\nunderstand. But how good are these algorithms as mechanisms? Truthful reporting\nof valuations is typically not a dominant strategy (certainly not with a\npay-your-bid, first-price rule, but it is likely not a good strategy even with\na critical value, or second-price style rule either). Our goal is to show that\na wide class of approximation algorithms yields this way mechanisms with low\nPrice of Anarchy.\n  The seminal result of Lucier and Borodin [SODA 2010] shows that combining a\ngreedy algorithm that is an $\\alpha$-approximation algorithm with a\npay-your-bid payment rule yields a mechanism whose Price of Anarchy is\n$O(\\alpha)$. In this paper we significantly extend the class of algorithms for\nwhich such a result is available by showing that this close connection between\napproximation ratio on the one hand and Price of Anarchy on the other also\nholds for the design principle of relaxation and rounding provided that the\nrelaxation is smooth and the rounding is oblivious.\n  We demonstrate the far-reaching consequences of our result by showing its\nimplications for sparse packing integer programs, such as multi-unit auctions\nand generalized matching, for the maximum traveling salesman problem, for\ncombinatorial auctions, and for single source unsplittable flow problems. In\nall these problems our approach leads to novel simple, near-optimal mechanisms\nwhose Price of Anarchy either matches or beats the performance guarantees of\nknown mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 09:08:30 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Kesselheim", "Thomas", ""], ["Tardos", "\u00c9va", ""]]}, {"id": "1511.09229", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui", "title": "Efficient Deterministic Single Round Document Exchange for Edit Distance", "comments": "12 pages, under submission. This version has some minor corrections,\n  clarifications and a simplification of the message size bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we have two parties that possess each a binary string. Suppose\nthat the length of the first string (document) is $n$ and that the two strings\n(documents) have edit distance (minimal number of deletes, inserts and\nsubstitutions needed to transform one string into the other) at most $k$. The\nproblem we want to solve is to devise an efficient protocol in which the first\nparty sends a single message that allows the second party to guess the first\nparty's string. In this paper we show an efficient deterministic protocol for\nthis problem. The protocol runs in time $O(n\\cdot \\mathtt{polylog}(n))$ and has\nmessage size $O(k^2+k\\log^2n)$ bits. To the best of our knowledge, ours is the\nfirst efficient deterministic protocol for this problem, if efficiency is\nmeasured in both the message size and the running time. As an immediate\napplication of our new protocol, we show a new error correcting code that is\nefficient even for large numbers of (adversarial) edit errors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 10:26:49 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 13:51:04 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Belazzougui", "Djamal", ""]]}, {"id": "1511.09259", "submitter": "Alantha Newman", "authors": "Alantha Newman, Heiko R\\\"oglin, Johanna Seif", "title": "The Alternating Stock Size Problem and the Gasoline Puzzle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set S of integers whose sum is zero, consider the problem of finding\na permutation of these integers such that: (i) all prefix sums of the ordering\nare nonnegative, and (ii) the maximum value of a prefix sum is minimized.\nKellerer et al. referred to this problem as the \"Stock Size Problem\" and showed\nthat it can be approximated to within 3/2. They also showed that an\napproximation ratio of 2 can be achieved via several simple algorithms.\n  We consider a related problem, which we call the \"Alternating Stock Size\nProblem\", where the number of positive and negative integers in the input set S\nare equal. The problem is the same as above, but we are additionally required\nto alternate the positive and negative numbers in the output ordering. This\nproblem also has several simple 2-approximations. We show that it can be\napproximated to within 1.79.\n  Then we show that this problem is closely related to an optimization version\nof the gasoline puzzle due to Lov\\'asz, in which we want to minimize the size\nof the gas tank necessary to go around the track. We present a 2-approximation\nfor this problem, using a natural linear programming relaxation whose feasible\nsolutions are doubly stochastic matrices. Our novel rounding algorithm is based\non a transformation that yields another doubly stochastic matrix with special\nproperties, from which we can extract a suitable permutation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 11:55:25 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 10:40:48 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 04:02:14 GMT"}, {"version": "v4", "created": "Thu, 18 Jan 2018 19:45:38 GMT"}, {"version": "v5", "created": "Sun, 8 Apr 2018 11:54:02 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Newman", "Alantha", ""], ["R\u00f6glin", "Heiko", ""], ["Seif", "Johanna", ""]]}, {"id": "1511.09293", "submitter": "Christos Kalaitzis", "authors": "Christos Kalaitzis", "title": "An Improved Approximation Guarantee for the Maximum Budgeted Allocation\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Maximum Budgeted Allocation problem, which is the problem of\nassigning indivisible items to players with budget constraints. In its most\ngeneral form, an instance of the MBA problem might include many different\nprices for the same item among different players, and different budget\nconstraints for every player. So far, the best approximation algorithms we know\nfor the MBA problem achieve a $3/4$-approximation ratio, and employ a natural\nLP relaxation, called the Assignment-LP. In this paper, we give an algorithm\nfor MBA, and prove that it achieves a $3/4+c$-approximation ratio, for some\nconstant $c>0$. This algorithm works by rounding solutions to an LP called the\nConfiguration-LP, therefore also showing that the Configuration-LP is strictly\nstronger than the Assignment-LP (for which we know that the integrality gap is\n$3/4$) for the MBA problem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 13:11:47 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Kalaitzis", "Christos", ""]]}, {"id": "1511.09360", "submitter": "Faisal Abu-Khzam", "authors": "Faisal N. Abu-Khzam", "title": "On the Complexity of Multi-Parameterized Cluster Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cluster Editing problem seeks a transformation of a given undirected\ngraph into a disjoint union of cliques via a minimum number of edge additions\nor deletions. A multi-parameterized version of the problem is studied,\nfeaturing a number of input parameters that bound the amount of both\nedge-additions and deletions per single vertex, as well as the size of a\nclique-cluster. We show that the problem remains NP-hard even when only one\nedge can be deleted and at most two edges can be added per vertex. However, the\nnew formulation allows us to solve Cluster Editing (exactly) in polynomial time\nwhen the number of edge-edit operations per vertex is smaller than half the\nminimum cluster size. In other words, Correlation Clustering can be solved\nefficiently when the number of false positives/negatives per single data\nelement is expected to be small compared to the minimum cluster size. As a\nbyproduct, we obtain a kernelization algorithm that delivers linear-size\nkernels when the two edge-edit bounds are small constants.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 15:56:47 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""]]}, {"id": "1511.09433", "submitter": "Joel Tropp", "authors": "Samet Oymak and Joel A. Tropp", "title": "Universality laws for randomized dimension reduction, with applications", "comments": "v2 and v3 with technical corrections. Code for reproducing figures\n  available at http://users.cms.caltech.edu/~jtropp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is the process of embedding high-dimensional data into a\nlower dimensional space to facilitate its analysis. In the Euclidean setting,\none fundamental technique for dimension reduction is to apply a random linear\nmap to the data. This dimension reduction procedure succeeds when it preserves\ncertain geometric features of the set.\n  The question is how large the embedding dimension must be to ensure that\nrandomized dimension reduction succeeds with high probability.\n  This paper studies a natural family of randomized dimension reduction maps\nand a large class of data sets. It proves that there is a phase transition in\nthe success probability of the dimension reduction map as the embedding\ndimension increases. For a given data set, the location of the phase transition\nis the same for all maps in this family. Furthermore, each map has the same\nstability properties, as quantified through the restricted minimum singular\nvalue. These results can be viewed as new universality laws in high-dimensional\nstochastic geometry.\n  Universality laws for randomized dimension reduction have many applications\nin applied mathematics, signal processing, and statistics. They yield design\nprinciples for numerical linear algebra algorithms, for compressed sensing\nmeasurement ensembles, and for random linear codes. Furthermore, these results\nhave implications for the performance of statistical estimation methods under a\nlarge class of random experimental designs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 19:14:23 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 18:22:19 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 00:45:33 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Oymak", "Samet", ""], ["Tropp", "Joel A.", ""]]}]