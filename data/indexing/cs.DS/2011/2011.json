[{"id": "2011.00001", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Distance problems within Helly graphs and $k$-Helly graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ball hypergraph of a graph $G$ is the family of balls of all possible\ncenters and radii in $G$. It has Helly number at most $k$ if every subfamily of\n$k$-wise intersecting balls has a nonempty common intersection. A graph is\n$k$-Helly (or Helly, if $k=2$) if its ball hypergraph has Helly number at most\n$k$. We prove that a central vertex and all the medians in an $n$-vertex\n$m$-edge Helly graph can be computed w.h.p. in $\\tilde{\\cal O}(m\\sqrt{n})$\ntime. Both results extend to a broader setting where we define a non-negative\ncost function over the vertex-set. For any fixed $k$, we also present an\n$\\tilde{\\cal O}(m\\sqrt{kn})$-time randomized algorithm for radius computation\nwithin $k$-Helly graphs. If we relax the definition of Helly number (for what\nis sometimes called an \"almost Helly-type\" property in the literature), then\nour approach leads to an approximation algorithm for computing the radius with\nan additive one-sided error of at most some constant.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:41:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "2011.00029", "submitter": "Florent Foucaud", "authors": "Florent Foucaud and Shih-Shun Kao and Ralf Klasing and Mirka Miller\n  and Joe Ryan", "title": "Monitoring the edges of a graph using distances", "comments": "19 pages; 5 figures. A preliminary version appeared in the\n  proceedings of CALDAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new graph-theoretic concept in the area of network monitoring.\nA set $M$ of vertices of a graph $G$ is a \\emph{distance-edge-monitoring set}\nif for every edge $e$ of $G$, there is a vertex $x$ of $M$ and a vertex $y$ of\n$G$ such that $e$ belongs to all shortest paths between $x$ and $y$. We denote\nby $dem(G)$ the smallest size of such a set in $G$. The vertices of $M$\nrepresent distance probes in a network modeled by $G$; when the edge $e$ fails,\nthe distance from $x$ to $y$ increases, and thus we are able to detect the\nfailure. It turns out that not only we can detect it, but we can even correctly\nlocate the failing edge.\n  In this paper, we initiate the study of this new concept. We show that for a\nnontrivial connected graph $G$ of order $n$, $1\\leq dem(G)\\leq n-1$ with\n$dem(G)=1$ if and only if $G$ is a tree, and $dem(G)=n-1$ if and only if it is\na complete graph. We compute the exact value of $dem$ for grids, hypercubes,\nand complete bipartite graphs.\n  Then, we relate $dem$ to other standard graph parameters. We show that\n$demG)$ is lower-bounded by the arboricity of the graph, and upper-bounded by\nits vertex cover number. It is also upper-bounded by twice its feedback edge\nset number. Moreover, we characterize connected graphs $G$ with $dem(G)=2$.\n  Then, we show that determining $dem(G)$ for an input graph $G$ is an\nNP-complete problem, even for apex graphs. There exists a polynomial-time\nlogarithmic-factor approximation algorithm, however it is NP-hard to compute an\nasymptotically better approximation, even for bipartite graphs of small\ndiameter and for bipartite subcubic graphs. For such instances, the problem is\nalso unlikey to be fixed parameter tractable when parameterized by the solution\nsize.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 18:20:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Foucaud", "Florent", ""], ["Kao", "Shih-Shun", ""], ["Klasing", "Ralf", ""], ["Miller", "Mirka", ""], ["Ryan", "Joe", ""]]}, {"id": "2011.00083", "submitter": "Ziteng Sun", "authors": "Jayadev Acharya, Peter Kairouz, Yuhan Liu, Ziteng Sun", "title": "Estimating Sparse Discrete Distributions Under Local Privacy and\n  Communication Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DS cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating sparse discrete distributions under\nlocal differential privacy (LDP) and communication constraints. We characterize\nthe sample complexity for sparse estimation under LDP constraints up to a\nconstant factor and the sample complexity under communication constraints up to\na logarithmic factor. Our upper bounds under LDP are based on the Hadamard\nResponse, a private coin scheme that requires only one bit of communication per\nuser. Under communication constraints, we propose public coin schemes based on\nrandom hashing functions. Our tight lower bounds are based on the recently\nproposed method of chi squared contractions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 20:06:35 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 19:48:16 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 04:06:00 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Acharya", "Jayadev", ""], ["Kairouz", "Peter", ""], ["Liu", "Yuhan", ""], ["Sun", "Ziteng", ""]]}, {"id": "2011.00130", "submitter": "Yenhung Chen", "authors": "Yen Hung Chen", "title": "Approximability results for the $p$-centdian and the converse centdian\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G=(V,E,l)$ with a nonnegative edge length function\n$l$, and two integers $p$, $\\lambda$, $0<p<|V|$, $0\\le\\lambda\\le 1$, let\n$V^{\\prime}$ be a subset of $V$ with $|V^{\\prime}|=p$. For each vertex $v \\in\nV$, we let $d(v,V^{\\prime})$ denote the shortest distance from $v$ to\n$V^{\\prime}$. An {\\it eccentricity} $\\pounds_C(V^{\\prime})$ of $V^{\\prime}$\ndenotes the maximum distance of $d(v,V^{\\prime})$ for all $v \\in V$. A\n{median-distance} $\\pounds_M(V^{\\prime})$ of $V^{\\prime}$ denotes the total\ndistance of $d(v,V^{\\prime})$ for all $v \\in V$. The $p$-centdian problem is to\nfind a vertex set $V^{\\prime}$ of $V$ with $|V^{\\prime}|=p$, such that $\\lambda\n\\pounds_C(V^{\\prime})+(1-\\lambda) \\pounds_M(V^{\\prime})$ is minimized. The\nvertex set $V^{\\prime}$ is called as the {\\it centdian set} and $\\lambda\n\\pounds_C(V^{\\prime})+(1-\\lambda) \\pounds_M(V^{\\prime})$ is called as the\n{centdian-distance}. If we converse the two criteria, that is given the bound\n$U$ of the {centdian-distance} and the objective function is to minimize the\ncardinality of the {centdian set}, this problem is called as the converse\ncentdian problem. In this paper, we prove the $p$-centdian problem is\nNP-Complete even when the {centdian-distance} is\n$\\pounds_C(V^{\\prime})+\\pounds_M(V^{\\prime})$. Then we design the first\nnon-trivial brute force exact algorithms for the $p$-centdian problem and the\nconverse centdian problem, respectively. Finally, we design a\n$(1+\\epsilon)$-approximation (respectively,\n$(1+1/\\epsilon)(ln|V|+1)$-approximation) algorithm for the $p$-centdian problem\n(respectively, converse centdian problem) satisfying the cardinality of\n{centdian set} is less than or equal to $(1+1/\\epsilon)(ln|V|+1)p$\n(respectively, $(1+\\epsilon)U$), in which $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 22:31:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Yen Hung", ""]]}, {"id": "2011.00131", "submitter": "Yenhung Chen", "authors": "Yen Hung Chen", "title": "The clustered selected-internal Steiner tree problem", "comments": "I withdrawed this submitted (but not published) manuscript from\n  Discrete Mathematics & Theoretical Computer Science (Journal) and\n  \"Theoretical Computer Science\"(Journal), and then transferred to submit this\n  Manuscript to \"International Journal of Foundations of Computer Science\", so\n  i need to replace the manuscript by the form of International Journal of\n  Foundations of Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a complete graph $G=(V,E)$, with nonnegative edge costs, two subsets $R\n\\subset V$ and $R^{\\prime} \\subset R$, a partition\n$\\mathcal{R}=\\{R_1,R_2,\\ldots,R_k\\}$ of $R$, $R_i \\cap R_j=\\phi$, $i \\neq j$\nand $\\mathcal{R}^{\\prime}=\\{R^{\\prime}_1,R^{\\prime}_2,\\ldots,R^{\\prime}_k\\}$ of\n$R^{\\prime}$, $R^{\\prime}_i \\subset R_i$, a clustered Steiner tree is a tree\n$T$ of $G$ that spans all vertices in $R$ such that $T$ can be cut into $k$\nsubtrees $T_i$ by removing $k-1$ edges and each subtree $T_i$ spanning all\nvertices in $R_i$, $1 \\leq i \\leq k$. The cost of a clustered Steiner tree is\ndefined to be the sum of the costs of all its edges. A clustered\nselected-internal Steiner tree of $G$ is a clustered Steiner tree for $R$ if\nall vertices in $R^{\\prime}_i$ are internal vertices of $T_i$, $1 \\leq i \\leq\nk$. The clustered selected-internal Steiner tree problem is concerned with the\ndetermination of a clustered selected-internal Steiner tree $T$ for $R$ and\n$R^{\\prime}$ in $G$ with minimum cost. In this paper, we present the first\nknown approximation algorithm with performance ratio $(\\rho+4)$ for the\nclustered selected-internal Steiner tree problem, where $\\rho$ is the\nbest-known performance ratio for the Steiner tree problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 22:37:37 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 01:45:21 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 12:53:00 GMT"}, {"version": "v4", "created": "Sun, 4 Apr 2021 14:11:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chen", "Yen Hung", ""]]}, {"id": "2011.00164", "submitter": "Fanhua Shang", "authors": "Tao Xu, Fanhua Shang, Yuanyuan Liu, Hongying Liu, Longjie Shen, Maoguo\n  Gong", "title": "Differentially Private ADMM Algorithms for Machine Learning", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study efficient differentially private alternating\ndirection methods of multipliers (ADMM) via gradient perturbation for many\nmachine learning problems. For smooth convex loss functions with (non)-smooth\nregularization, we propose the first differentially private ADMM (DP-ADMM)\nalgorithm with performance guarantee of $(\\epsilon,\\delta)$-differential\nprivacy ($(\\epsilon,\\delta)$-DP). From the viewpoint of theoretical analysis,\nwe use the Gaussian mechanism and the conversion relationship between R\\'enyi\nDifferential Privacy (RDP) and DP to perform a comprehensive privacy analysis\nfor our algorithm. Then we establish a new criterion to prove the convergence\nof the proposed algorithms including DP-ADMM. We also give the utility analysis\nof our DP-ADMM. Moreover, we propose an accelerated DP-ADMM (DP-AccADMM) with\nthe Nesterov's acceleration technique. Finally, we conduct numerical\nexperiments on many real-world datasets to show the privacy-utility tradeoff of\nthe two proposed algorithms, and all the comparative analysis shows that\nDP-AccADMM converges faster and has a better utility than DP-ADMM, when the\nprivacy budget $\\epsilon$ is larger than a threshold.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 01:37:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Xu", "Tao", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Liu", "Hongying", ""], ["Shen", "Longjie", ""], ["Gong", "Maoguo", ""]]}, {"id": "2011.00172", "submitter": "Xuandi Ren", "authors": "Pinyan Lu, Xuandi Ren, Enze Sun, Yubo Zhang", "title": "Generalized Sorting with Predictions", "comments": "To appear in SOSA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized sorting problem, also known as sorting with forbidden\ncomparisons, was first introduced by Huang et al. together with a randomized\nalgorithm which requires $\\tilde O(n^{3/2})$ probes. We study this problem with\nadditional predictions for all pairs of allowed comparisons as input. We\npropose a randomized algorithm which uses $O(n \\log n+w)$ probes with high\nprobability and a deterministic algorithm which uses $O(nw)$ probes, where $w$\nis the number of mistakes made by prediction.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 02:39:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Lu", "Pinyan", ""], ["Ren", "Xuandi", ""], ["Sun", "Enze", ""], ["Zhang", "Yubo", ""]]}, {"id": "2011.00364", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan", "title": "Efficient Methods for Structured Nonconvex-Nonconcave Min-Max\n  Optimization", "comments": "in Proc. AISTATS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of min-max optimization in adversarial training of deep neural\nnetwork classifiers and training of generative adversarial networks has\nmotivated the study of nonconvex-nonconcave optimization objectives, which\nfrequently arise in these applications. Unfortunately, recent results have\nestablished that even approximate first-order stationary points of such\nobjectives are intractable, even under smoothness conditions, motivating the\nstudy of min-max objectives with additional structure. We introduce a new class\nof structured nonconvex-nonconcave min-max optimization problems, proposing a\ngeneralization of the extragradient algorithm which provably converges to a\nstationary point. The algorithm applies not only to Euclidean spaces, but also\nto general $\\ell_p$-normed finite-dimensional real vector spaces. We also\ndiscuss its stability under stochastic oracles and provide bounds on its sample\ncomplexity. Our iteration complexity and sample complexity bounds either match\nor improve the best known bounds for the same or less general\nnonconvex-nonconcave settings, such as those that satisfy variational coherence\nor in which a weak solution to the associated variational inequality problem is\nassumed to exist.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 21:35:42 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 22:09:24 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Daskalakis", "Constantinos", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.00503", "submitter": "Peyman Afshani", "authors": "Peyman Afshani", "title": "A Lower Bound for Dynamic Fractional Cascading", "comments": "Minor edits, and fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the limits of one of the fundamental ideas in data structures:\nfractional cascading. This is an important data structure technique to speed up\nrepeated searches for the same key in multiple lists and it has numerous\napplications. Specifically, the input is a \"catalog\" graph, $G$, of constant\ndegree together with a list of values assigned to every vertex of $G$. The goal\nis to preprocess the input such that given a connected subgraph $H$ of $G$ and\na single query value $q$, one can find the predecessor of $q$ in every list\nthat belongs to $\\scat$. The classical result by Chazelle and Guibas shows that\nin a pointer machine, this can be done in the optimal time of $\\O(\\log n +\n|\\scat|)$ where $n$ is the total number of values. However, if insertion and\ndeletion of values are allowed, then the query time slows down to $\\O(\\log n +\n|\\scat| \\log\\log n)$. If only insertions (or deletions) are allowed, then once\nagain, an optimal query time can be obtained but by using amortization at\nupdate time.\n  We prove a lower bound of $\\Omega( \\log n \\sqrt{\\log\\log n})$ on the\nworst-case query time of dynamic fractional cascading, when queries are paths\nof length $O(\\log n)$. The lower bound applies both to fully dynamic data\nstructures with amortized polylogarithmic update time and incremental data\nstructures with polylogarithmic worst-case update time. As a side, this also\nroves that amortization is crucial for obtaining an optimal incremental data\nstructure.\n  This is the first non-trivial pointer machine lower bound for a dynamic data\nstructure that breaks the $\\Omega(\\log n)$ barrier. In order to obtain this\nresult, we develop a number of new ideas and techniques that hopefully can be\nuseful to obtain additional dynamic lower bounds in the pointer machine model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 13:55:53 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 19:46:51 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Afshani", "Peyman", ""]]}, {"id": "2011.00511", "submitter": "David Schaller", "authors": "David Schaller, Manuela Gei{\\ss}, Marc Hellmuth, Peter F. Stadler", "title": "Best Match Graphs with Binary Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Best match graphs (BMG) are a key intermediate in graph-based orthology\ndetection and contain a large amount of information on the gene tree. We\nprovide a near-cubic algorithm to determine whether a BMG is\nbinary-explainable, i.e., whether it can be explained by a fully resolved gene\ntree and, if so, to construct such a tree. Moreover, we show that all such\nbinary trees are refinements of the unique binary-resolvable tree (BRT), which\nin general is a substantial refinement of the also unique least resolved tree\nof a BMG. Finally, we show that the problem of editing an arbitrary\nvertex-colored graph to a binary-explainable BMG is NP-complete and provide an\ninteger linear program formulation for this task.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:22:30 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 12:43:50 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Schaller", "David", ""], ["Gei\u00df", "Manuela", ""], ["Hellmuth", "Marc", ""], ["Stadler", "Peter F.", ""]]}, {"id": "2011.00542", "submitter": "Gamal Sallam", "authors": "Gamal Sallam, Zizhan Zheng, Jie Wu, and Bo Ji", "title": "Robust Sequence Submodular Maximization", "comments": "Proceedings of NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodularity is an important property of set functions and has been\nextensively studied in the literature. It models set functions that exhibit a\ndiminishing returns property, where the marginal value of adding an element to\na set decreases as the set expands. This notion has been generalized to\nconsidering sequence functions, where the order of adding elements plays a\ncrucial role and determines the function value; the generalized notion is\ncalled sequence (or string) submodularity. In this paper, we study a new\nproblem of robust sequence submodular maximization with cardinality\nconstraints. The robustness is against the removal of a subset of elements in\nthe selected sequence (e.g., due to malfunctions or adversarial attacks).\nCompared to robust submodular maximization for set function, new challenges\narise when sequence functions are concerned. Specifically, there are multiple\ndefinitions of submodularity for sequence functions, which exhibit subtle yet\ncritical differences. Another challenge comes from two directions of\nmonotonicity: forward monotonicity and backward monotonicity, both of which are\nimportant to proving performance guarantees. To address these unique\nchallenges, we design two robust greedy algorithms: while one algorithm\nachieves a constant approximation ratio but is robust only against the removal\nof a subset of contiguous elements, the other is robust against the removal of\nan arbitrary subset of the selected elements but requires a stronger assumption\nand achieves an approximation ratio that depends on the number of the removed\nelements. Finally, we generalize the analyses to considering sequence functions\nunder weaker assumptions based on approximate versions of sequence\nsubmodularity and backward monotonicity\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:55:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sallam", "Gamal", ""], ["Zheng", "Zizhan", ""], ["Wu", "Jie", ""], ["Ji", "Bo", ""]]}, {"id": "2011.00817", "submitter": "Shichuan Deng", "authors": "Shichuan Deng", "title": "Generalized Load Balancing and Clustering Problems with Norm\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fundamental combinatorial optimization problems, a feasible solution\ninduces some real cost vectors as an intermediate result, and the optimization\nobjective is a certain function of the vectors. For example, in the problem of\nmakespan minimization on unrelated parallel machines, a feasible job assignment\ninduces a vector containing the sizes of assigned jobs for each machine, and\nthe goal is to minimize the $L_\\infty$ norm of $L_1$ norms of the vectors.\nAnother example is fault-tolerant $k$-center, where each client is connected to\nmultiple open facilities, thus having a vector of distances to these\nfacilities, and the goal is to minimize the $L_\\infty$ norm of $L_\\infty$ norms\nof these vectors. In this paper, we study the maximum of norm problem. Given an\narbitrary symmetric monotone norm $f$, the objective is defined as the maximum\n($L_\\infty$ norm) of $f$-norm values of the induced cost vectors. This\nversatile formulation captures a wide variety of problems, including makespan\nminimization, fault-tolerant $k$-center and many others. We give concrete\nresults for load balancing on unrelated parallel machines and clustering\nproblems, including constant-factor approximation algorithms when $f$ belongs\nwith a certain rich family of norms, and $O(\\log n)$-approximations when $f$ is\ngeneral and satisfies some mild assumptions. We also consider the\naforementioned problems in a generalized fairness setting. As a concrete\nexample, the insight is to prevent a scheduling algorithm from assigning too\nmany jobs consistently on any machine in a job-recurring scenario, and causing\nthe machine's controller to fail. Our algorithm needs to stochastically output\na feasible solution minimizing the objective function, and satisfy the given\nmarginal fairness constraints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 08:35:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 11:23:14 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 16:07:43 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 08:58:36 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Deng", "Shichuan", ""]]}, {"id": "2011.00822", "submitter": "Laurent Decreusefond", "authors": "Laurent Decreusefond (INFRES, LTCI, DIG), Guillaume Moroz", "title": "Optimal transport between determinantal point processes and application\n  to fast simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze several optimal transportation problems between de-terminantal\npoint processes. We show how to estimate some of the distances between\ndistributions of DPP they induce. We then apply these results to evaluate the\naccuracy of a new and fast DPP simulation algorithm. We can now simulate in a\nreasonable amount of time more than ten thousands points.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 08:42:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Decreusefond", "Laurent", "", "INFRES, LTCI, DIG"], ["Moroz", "Guillaume", ""]]}, {"id": "2011.00868", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty, Debarati Das and Robert Krauthgamer", "title": "Approximating the Median under the Ulam Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximation algorithms for variants of the \\emph{median string}\nproblem, which asks for a string that minimizes the sum of edit distances from\na given set of $m$ strings of length $n$. Only the straightforward\n$2$-approximation is known for this NP-hard problem. This problem is motivated\ne.g.~by computational biology, and belongs to the class of median problems\n(over different metric spaces), which are fundamental tasks in data analysis.\n  Our main result is for the Ulam metric, where all strings are permutations\nover $[n]$ and each edit operation moves a symbol (deletion plus insertion). We\ndevise for this problem an algorithms that breaks the $2$-approximation\nbarrier, i.e., computes a $(2-\\delta)$-approximate median permutation for some\nconstant $\\delta>0$ in time $\\tilde{O}(nm^2+n^3)$. We further use these\ntechniques to achieve a $(2-\\delta)$ approximation for the median string\nproblem in the special case where the median is restricted to length $n$ and\nthe optimal objective is large $\\Omega(mn)$.\n  We also design an approximation algorithm for the following probabilistic\nmodel of the Ulam median: the input consists of $m$ perturbations of an\n(unknown) permutation $x$, each generated by moving every symbol to a random\nposition with probability (a parameter) $\\epsilon>0$. Our algorithm computes\nwith high probability a $(1+o(1/\\epsilon))$-approximate median permutation in\ntime $O(mn^2+n^3)$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:20:46 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Das", "Debarati", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "2011.00968", "submitter": "Yushi Uno", "authors": "Joep Hamersma, Marc van Kreveld, Yushi Uno, Tom C. van der Zanden", "title": "Gourds: a sliding-block puzzle with turning", "comments": "15 pages + 3 pages appendix, including 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new kind of sliding-block puzzle, called Gourds, where the\nobjective is to rearrange 1 x 2 pieces on a hexagonal grid board of 2n + 1\ncells with n pieces, using sliding, turning and pivoting moves. This puzzle has\na single empty cell on a board and forms a natural extension of the 15-puzzle\nto include rotational moves. We analyze the puzzle and completely characterize\nthe cases when the puzzle can always be solved. We also study the complexity of\ndetermining whether a given set of colored pieces can be placed on a colored\nhexagonal grid board with matching colors. We show this problem is NP-complete\nfor arbitrarily many colors, but solvable in randomized polynomial time if the\nnumber of colors is a fixed constant.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:41:22 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hamersma", "Joep", ""], ["van Kreveld", "Marc", ""], ["Uno", "Yushi", ""], ["van der Zanden", "Tom C.", ""]]}, {"id": "2011.00977", "submitter": "Pan Peng", "authors": "Monika Henzinger, Pan Peng", "title": "Constant-Time Dynamic Weight Approximation for Minimum Spanning Forest", "comments": "Partial results have been reported in arXiv:1907.04745", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two fully dynamic algorithms that maintain a\n$(1+\\varepsilon)$-approximation of the weight $M$ of the minimum spanning\nforest of an $n$-node graph $G$ with edges weights in $[1,W]$, for any\n$\\varepsilon>0$.\n  (1) Our deterministic algorithm takes $O({W^2 \\log W}/{\\varepsilon^3})$\nworst-case update time, which is $O(1)$ if both $W$ and $\\varepsilon$ are\nconstants. Note that there is a lower bound by Patrascu and Demaine (SIAM J.\nComput. 2006) shows that it takes $\\Omega(\\log n)$ time per operation to\nmaintain the exact weight of the MSF that holds even in the unweighted case,\ni.e. for $W=1$. We further show that any deterministic data structure that\ndynamically maintains the $(1+\\varepsilon)$-approximate weight of the MSF\nrequires super constant time per operation, if $W\\geq (\\log n)^{\\omega_n(1)}$.\n  (2) Our randomized (Monte-Carlo style) algorithm works with high probability\nand runs in worst-case\n$O(\\frac{1}{\\varepsilon^4}\\log^3(\\frac{1}{\\varepsilon}))$ update time if $W$ is\nnot too large, more specifically, if $W= O({(m^*)^{1/6}}/{\\log n})$, where\n$m^*$ is the minimum number of edges in the graph throughout all the updates.\nIt works even against an adaptive adversary. We complement this result by\nshowing that for any constant $\\varepsilon,\\alpha>0$ and $W=n^{\\alpha}$, any\n(randomized) data structure that dynamically maintains the weight of the MSF of\na graph $G$ with edge weights in $[1,W]$ and $W = \\Omega(\\varepsilon m^*)$\nwithin a multiplicative factor of $(1+\\varepsilon)$ takes $\\Omega(\\log n)$ time\nper operation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:53:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Henzinger", "Monika", ""], ["Peng", "Pan", ""]]}, {"id": "2011.00981", "submitter": "Huang Lingxiao", "authors": "Lingxiao Huang, K. Sudhir, Nisheeth K. Vishnoi", "title": "Coresets for Regressions with Panel Data", "comments": "This is a Full version of a paper to appear in NeurIPS 2020. The code\n  can be found in\n  https://github.com/huanglx12/Coresets-for-regressions-with-panel-data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DS econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the problem of coresets for regression problems to\npanel data settings. We first define coresets for several variants of\nregression problems with panel data and then present efficient algorithms to\nconstruct coresets of size that depend polynomially on 1/$\\varepsilon$ (where\n$\\varepsilon$ is the error parameter) and the number of regression parameters -\nindependent of the number of individuals in the panel data or the time units\neach individual is observed for. Our approach is based on the Feldman-Langberg\nframework in which a key step is to upper bound the \"total sensitivity\" that is\nroughly the sum of maximum influences of all individual-time pairs taken over\nall possible choices of regression parameters. Empirically, we assess our\napproach with synthetic and real-world datasets; the coreset sizes constructed\nusing our approach are much smaller than the full dataset and coresets indeed\naccelerate the running time of computing the regression objective.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:58:31 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 02:52:41 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Huang", "Lingxiao", ""], ["Sudhir", "K.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2011.01017", "submitter": "Stefan Neumann", "authors": "Monika Henzinger, Stefan Neumann, Harald R\\\"acke, Stefan Schmid", "title": "Tight Bounds for Online Graph Partitioning", "comments": "Full version of a paper that will appear at SODA'21. Abstract\n  shortened to obey arxiv's abstract requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following online optimization problem. We are given a graph\n$G$ and each vertex of the graph is assigned to one of $\\ell$ servers, where\nservers have capacity $k$ and we assume that the graph has $\\ell \\cdot k$\nvertices. Initially, $G$ does not contain any edges and then the edges of $G$\nare revealed one-by-one. The goal is to design an online algorithm\n$\\operatorname{ONL}$, which always places the connected components induced by\nthe revealed edges on the same server and never exceeds the server capacities\nby more than $\\varepsilon k$ for constant $\\varepsilon>0$. Whenever\n$\\operatorname{ONL}$ learns about a new edge, the algorithm is allowed to move\nvertices from one server to another. Its objective is to minimize the number of\nvertex moves. More specifically, $\\operatorname{ONL}$ should minimize the\ncompetitive ratio: the total cost $\\operatorname{ONL}$ incurs compared to an\noptimal offline algorithm $\\operatorname{OPT}$.\n  Our main contribution is a polynomial-time randomized algorithm, that is\nasymptotically optimal: we derive an upper bound of $O(\\log \\ell + \\log k)$ on\nits competitive ratio and show that no randomized online algorithm can achieve\na competitive ratio of less than $\\Omega(\\log \\ell + \\log k)$. We also settle\nthe open problem of the achievable competitive ratio by deterministic online\nalgorithms, by deriving a competitive ratio of $\\Theta(\\ell \\lg k)$; to this\nend, we present an improved lower bound as well as a deterministic\npolynomial-time online algorithm.\n  Our algorithms rely on a novel technique which combines efficient integer\nprogramming with a combinatorial approach for maintaining ILP solutions. We\nbelieve this technique is of independent interest and will find further\napplications in the future.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:01:21 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Henzinger", "Monika", ""], ["Neumann", "Stefan", ""], ["R\u00e4cke", "Harald", ""], ["Schmid", "Stefan", ""]]}, {"id": "2011.01024", "submitter": "Chen Chen", "authors": "Chen Chen, Wenshao Zhong, Xingbo Wu", "title": "Efficient Data Management with Flexible File Address Space", "comments": "14 pages incl. references; 13 figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management applications store their data using structured files in which\ndata are usually sorted to serve indexing and queries. In order to insert or\nremove a record in a sorted file, the positions of existing data need to be\nshifted. To this end, the existing data after the insertion or removal point\nmust be rewritten to admit the change in place, which can be unaffordable for\napplications that make frequent updates. As a result, applications often employ\nextra layers of indirections to admit changes out-of-place. However, it causes\nincreased access costs and excessive complexity.\n  This paper presents a novel file abstraction, FlexFile, that provides a\nflexible file address space where in-place updates of arbitrary-sized data,\nsuch as insertions and removals, can be performed efficiently. With FlexFile,\napplications can manage their data in a linear file address space with minimal\ncomplexity. Extensive evaluation results show that a simple key-value store\nbuilt on top of this abstraction can achieve high performance for both reads\nand writes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:48:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:07:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Chen", ""], ["Zhong", "Wenshao", ""], ["Wu", "Xingbo", ""]]}, {"id": "2011.01366", "submitter": "Martin Grohe", "authors": "Martin Grohe and Daniel Neuen", "title": "Recent Advances on the Graph Isomorphism Problem", "comments": "arXiv admin note: text overlap with arXiv:2002.06997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of recent advances on the graph isomorphism problem. Our\nmain focus will be on Babai's quasi-polynomial time isomorphism test and\nsubsequent developments that led to the design of isomorphism algorithms with a\nquasi-polynomial parameterized running time of the from\n$n^{\\text{polylog}(k)}$, where $k$ is a graph parameter such as the maximum\ndegree. A second focus will be the combinatorial Weisfeiler-Leman algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:47:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 15:15:41 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Grohe", "Martin", ""], ["Neuen", "Daniel", ""]]}, {"id": "2011.01435", "submitter": "Marco Molinaro", "authors": "Marco Molinaro", "title": "Robust Algorithms for Online Convex Problems via Primal-Dual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primal-dual methods in online optimization give several of the state-of-the\nart results in both of the most common models: adversarial and\nstochastic/random order. Here we try to provide a more unified analysis of\nprimal-dual algorithms to better understand the mechanisms behind this\nimportant method. With this we are able of recover and extend in one goal\nseveral results of the literature.\n  In particular, we obtain robust online algorithm for fairly general online\nconvex problems: we consider the MIXED model where in some of the time steps\nthe data is stochastic and in the others the data is adversarial. Both the\nquantity and location of the adversarial time steps are unknown to the\nalgorithm. The guarantees of our algorithms interpolate between the (close to)\nbest guarantees for each of the pure models. In particular, the presence of\nadversarial times does not degrade the guarantee relative to the stochastic\npart of the instance.\n  Concretely, we first consider Online Convex Programming: at each time a\nfeasible set $V_t$ is revealed, and the algorithm needs to select $v_t \\in V_t$\nto minimize the total cost $\\psi(\\sum_t v_t)$, for a convex function $\\psi$.\nOur robust primal-dual algorithm for this problem on the MIXED model recovers\nand extends, for example, a result of Gupta et al. and recent work on\n$\\ell_p$-norm load balancing by the author. We also consider the problem of\nWelfare Maximization with Convex Production Costs: at each time a customer\npresents a value $c_t$ and resource consumption vector $a_t$, and the goal is\nto fractionally select customers to maximize the profit $\\sum_t c_t x_t -\n\\psi(\\sum_t a_t x_t)$. Our robust primal-dual algorithm on the MIXED model\nrecovers and extends the result of Azar et al.\n  Given the ubiquity of primal-dual algorithms we hope the ideas presented here\nwill be useful in obtaining other robust algorithm in the MIXED or related\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:50:11 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Molinaro", "Marco", ""]]}, {"id": "2011.01441", "submitter": "Yuan Tang", "authors": "Yuan Tang, Weiguo Gao", "title": "Balanced Partitioning of Several Cache-Oblivious Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frigo et al. proposed an ideal cache model and a recursive technique to\ndesign sequential cache-efficient algorithms in a cache-oblivious fashion.\nBallard et al. pointed out that it is a fundamental open problem to extend the\ntechnique to an arbitrary architecture. Ballard et al. raised another open\nquestion on how to parallelize Strassen's algorithm exactly and efficiently on\nan arbitrary number of processors.\n  We propose a novel way of partitioning a cache-oblivious algorithm to achieve\nperfect strong scaling on an arbitrary number, even a prime number, of\nprocessors within a certain range in a shared-memory setting. Our approach is\nProcessor-Aware but Cache-Oblivious (PACO). We demonstrate our approach on\nseveral important cache-oblivious algorithms, including LCS, 1D, GAP, classic\nrectangular matrix multiplication on a semiring, and Strassen's algorithm. We\ndiscuss how to extend our approach to a distributed-memory architecture, or\neven a heterogeneous computing system. Hence, our work may provide a new\nperspective on the fundamental open problem of extending the recursive\ncache-oblivious technique to an arbitrary architecture. We provide an almost\nexact solution to the open problem on parallelizing Strassen. Our approach may\nprovide a new perspective on extending the recursive cache-oblivious technique\nto an arbitrary architecture. All our algorithms demonstrate better scalability\nor better overall parallel cache complexities than the best known algorithms.\nPreliminary experiments justify our theoretical prediction that the PACO\nalgorithms can outperform significantly state-of-the-art Processor-Oblivious\n(PO) and Processor-Aware (PA) counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:13:29 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Tang", "Yuan", ""], ["Gao", "Weiguo", ""]]}, {"id": "2011.01489", "submitter": "Samer Nofal", "authors": "Samer Nofal, Amani Abu Jabal, Abdullah Alfarrarjeh, Ismail Hababeh", "title": "On Computing Stable Extensions of Abstract Argumentation Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \\textit{abstract argumentation framework} ({\\sc af} for short) is a\ndirected graph $(A,R)$ where $A$ is a set of \\textit{abstract arguments} and\n$R\\subseteq A \\times A$ is the \\textit{attack} relation. Let $H=(A,R)$ be an\n{\\sc af}, $S \\subseteq A$ be a set of arguments and $S^+ = \\{y \\mid \\exists\nx\\in S \\text{ with }(x,y)\\in R\\}$. Then, $S$ is a \\textit{stable extension} in\n$H$ if and only if $S^+ = A\\setminus S$. In this paper, we present a thorough,\nformal validation of a known backtracking algorithm for listing all stable\nextensions in a given {\\sc af}.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 05:38:52 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 11:31:40 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 11:51:24 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 06:47:25 GMT"}, {"version": "v5", "created": "Tue, 15 Jun 2021 03:56:29 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Nofal", "Samer", ""], ["Jabal", "Amani Abu", ""], ["Alfarrarjeh", "Abdullah", ""], ["Hababeh", "Ismail", ""]]}, {"id": "2011.01559", "submitter": "Tomer Ezra", "authors": "Tomer Ezra, Michal Feldman, Nick Gravin and Zhihao Gavin Tang", "title": "Secretary Matching with General Arrivals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide online algorithms for secretary matching in general weighted\ngraphs, under the well-studied models of vertex and edge arrivals. In both\nmodels, edges are associated with arbitrary weights that are unknown from the\noutset, and are revealed online. Under vertex arrival, vertices arrive online\nin a uniformly random order; upon the arrival of a vertex $v$, the weights of\nedges from $v$ to all previously arriving vertices are revealed, and the\nalgorithm decides which of these edges, if any, to include in the matching.\nUnder edge arrival, edges arrive online in a uniformly random order; upon the\narrival of an edge $e$, its weight is revealed, and the algorithm decides\nwhether to include it in the matching or not. We provide a $5/12$-competitive\nalgorithm for vertex arrival, and show it is tight. For edge arrival, we\nprovide a $1/4$-competitive algorithm. Both results improve upon state of the\nart bounds for the corresponding settings. Interestingly, for vertex arrival,\nsecretary matching in general graphs outperforms secretary matching in\nbipartite graphs with 1-sided arrival, where $1/e$ is the best possible\nguarantee.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 08:32:05 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Ezra", "Tomer", ""], ["Feldman", "Michal", ""], ["Gravin", "Nick", ""], ["Tang", "Zhihao Gavin", ""]]}, {"id": "2011.01564", "submitter": "Marek Chalupa", "authors": "Marek Chalupa, David Kla\\v{s}ka, Jan Strej\\v{c}ek, Luk\\'a\\v{s}\n  Tomovi\\v{c}", "title": "Fast Computation of Strong Control Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new algorithms for computing non-termination sensitive control\ndependence (NTSCD) and decisive order dependence (DOD). These relations on\ncontrol flow graph vertices have many applications including program slicing\nand compiler optimizations. Our algorithms are asymptotically faster than the\ncurrent algorithms. We also show that the original algorithms for computing\nNTSCD and DOD may produce incorrect results. We implemented the new as well as\nfixed versions of the original algorithms for the computation of NTSCD and DOD\nand we experimentally compare their performance and outcome. Our algorithms\ndramatically outperform the original ones.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 08:41:04 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Chalupa", "Marek", ""], ["Kla\u0161ka", "David", ""], ["Strej\u010dek", "Jan", ""], ["Tomovi\u010d", "Luk\u00e1\u0161", ""]]}, {"id": "2011.01584", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan", "title": "Estimating decision tree learnability with polylogarithmic sample\n  complexity", "comments": "25 pages, to appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that top-down decision tree learning heuristics are amenable to\nhighly efficient learnability estimation: for monotone target functions, the\nerror of the decision tree hypothesis constructed by these heuristics can be\nestimated with polylogarithmically many labeled examples, exponentially smaller\nthan the number necessary to run these heuristics, and indeed, exponentially\nsmaller than information-theoretic minimum required to learn a good decision\ntree. This adds to a small but growing list of fundamental learning algorithms\nthat have been shown to be amenable to learnability estimation.\n  En route to this result, we design and analyze sample-efficient minibatch\nversions of top-down decision tree learning heuristics and show that they\nachieve the same provable guarantees as the full-batch versions. We further\ngive \"active local\" versions of these heuristics: given a test point $x^\\star$,\nwe show how the label $T(x^\\star)$ of the decision tree hypothesis $T$ can be\ncomputed with polylogarithmically many labeled examples, exponentially smaller\nthan the number necessary to learn $T$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 09:26:27 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Blanc", "Guy", ""], ["Gupta", "Neha", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2011.01649", "submitter": "Giorgio Camerani", "authors": "Giorgio Camerani", "title": "The Long, the Short and the Random", "comments": "18 pages, 10 figures. Software implementation (in Java) available for\n  download", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We furnish solid evidence, both theoretical and empirical, towards the\nexistence of a deterministic algorithm for random sparse $\\#\\Omega(\\log n)$-SAT\ninstances, which computes the exact counting of satisfying assignments in\nsub-exponential time. The algorithm uses a nice combinatorial property that\nevery CNF formula has, which relates its number of unsatisfying assignments to\nthe space of its monotone sub-formulae.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:00:07 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 22:39:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Camerani", "Giorgio", ""]]}, {"id": "2011.01726", "submitter": "Markus Anders", "authors": "Markus Anders and Pascal Schweitzer", "title": "Search Problems in Trees with Symmetries: near optimal traversal\n  strategies for individualization-refinement algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a search problem on trees that closely captures the backtracking\nbehavior of all current practical graph isomorphism algorithms. Given two trees\nwith colored leaves, the goal is to find two leaves of matching color, one in\neach of the trees. The trees are subject to an invariance property which\npromises that for every pair of leaves of equal color there must be a symmetry\n(or an isomorphism) that maps one leaf to the other.\n  We describe a randomized algorithm with errors for which the number of\nvisited leaves is quasilinear in the square root of the size of the smaller of\nthe two trees. For inputs of bounded degree, we develop a Las Vegas algorithm\nwith a similar running time.\n  We prove that these results are optimal up to logarithmic factors. We show a\nlower bound for randomized algorithms on inputs of bounded degree that is the\nsquare root of the tree sizes. For inputs of unbounded degree, we show a linear\nlower bound for Las Vegas algorithms. For deterministic algorithms we can prove\na linear bound even for inputs of bounded degree. This shows why randomized\nalgorithms outperform deterministic ones.\n  Our results explain why the randomized \"breadth-first with intermixed\nexperimental path\" search strategy of the isomorphism tool Traces (Piperno\n2008) is often superior to the depth-first search strategy of other tools such\nas nauty (McKay 1977) or bliss (Junttila, Kaski 2007). However, our algorithm\nalso provides a new traversal strategy, which is theoretically near optimal\nwith better worst case behavior than traversal strategies that have previously\nbeen used.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:21:39 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Anders", "Markus", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "2011.01777", "submitter": "Shay Sapir", "authors": "Vladimir Braverman, Robert Krauthgamer, Aditya Krishnan and Shay Sapir", "title": "Near-Optimal Entrywise Sampling of Numerically Sparse Matrices", "comments": "20 pages. To appear in COLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world data sets are sparse or almost sparse. One method to measure\nthis for a matrix $A\\in \\mathbb{R}^{n\\times n}$ is the \\emph{numerical\nsparsity}, denoted $\\mathsf{ns}(A)$, defined as the minimum $k\\geq 1$ such that\n$\\|a\\|_1/\\|a\\|_2 \\leq \\sqrt{k}$ for every row and every column $a$ of $A$. This\nmeasure of $a$ is smooth and is clearly only smaller than the number of\nnon-zeros in the row/column $a$. The seminal work of Achlioptas and McSherry\n[2007] has put forward the question of approximating an input matrix $A$ by\nentrywise sampling. More precisely, the goal is to quickly compute a sparse\nmatrix $\\tilde{A}$ satisfying $\\|A - \\tilde{A}\\|_2 \\leq \\epsilon \\|A\\|_2$\n(i.e., additive spectral approximation) given an error parameter $\\epsilon>0$.\nThe known schemes sample and rescale a small fraction of entries from $A$. We\npropose a scheme that sparsifies an almost-sparse matrix $A$ -- it produces a\nmatrix $\\tilde{A}$ with $O(\\epsilon^{-2}\\mathsf{ns}(A) \\cdot n\\ln n)$ non-zero\nentries with high probability. We also prove that this upper bound on\n$\\mathsf{nnz}(\\tilde{A})$ is \\emph{tight} up to logarithmic factors. Moreover,\nour upper bound improves when the spectrum of $A$ decays quickly (roughly\nreplacing $n$ with the stable rank of $A$). Our scheme can be implemented in\ntime $O(\\mathsf{nnz}(A))$ when $\\|A\\|_2$ is given. Previously, a similar upper\nbound was obtained by Achlioptas et. al [2013] but only for a restricted class\nof inputs that does not even include symmetric or covariance matrices. Finally,\nwe demonstrate two applications of these sampling techniques, to faster\napproximate matrix multiplication, and to ridge regression by using sparse\npreconditioners.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:18:48 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 13:37:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Braverman", "Vladimir", ""], ["Krauthgamer", "Robert", ""], ["Krishnan", "Aditya", ""], ["Sapir", "Shay", ""]]}, {"id": "2011.01851", "submitter": "Nisheeth Vishnoi", "authors": "Jonathan Leake and Nisheeth K. Vishnoi", "title": "On the Computability of Continuous Maximum Entropy Distributions:\n  Adjoint Orbits of Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math-ph math.MP math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point $A$ in the convex hull of a given adjoint orbit\n$\\mathcal{O}(F)$ of a compact Lie group $G$, we give a polynomial time\nalgorithm to compute the probability density supported on $\\mathcal{O}(F)$\nwhose expectation is $A$ and that minimizes the Kullback-Leibler divergence to\nthe $G$-invariant measure on $\\mathcal{O}(F)$. This significantly extends the\nrecent work of the authors (STOC 2020) who presented such a result for the\nmanifold of rank $k$-projections which is a specific adjoint orbit of the\nunitary group $\\mathrm{U}(n)$. Our result relies on the ellipsoid method-based\nframework proposed in prior work; however, to apply it to the general setting\nof compact Lie groups, we need tools from Lie theory. For instance, properties\nof the adjoint representation are used to find the defining equalities of the\nminimal affine space containing the convex hull of $\\mathcal{O}(F)$, and to\nestablish a bound on the optimal dual solution. Also, the Harish-Chandra\nintegral formula is used to obtain an evaluation oracle for the dual objective\nfunction. While the Harish-Chandra integral formula allows us to write certain\nintegrals over the adjoint orbit of a Lie group as a sum of a small number of\ndeterminants, it is only defined for elements of a chosen Cartan subalgebra of\nthe Lie algebra $\\mathfrak{g}$ of $G.$ We show how it can be applied to our\nsetting with the help of Kostant's convexity theorem. Further, the convex hull\nof an adjoint orbit is a type of orbitope, and the orbitopes studied in this\npaper are known to be spectrahedral. Thus our main result can be viewed as\nextending the maximum entropy framework to a class of spectrahedra.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:14:27 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Leake", "Jonathan", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2011.01898", "submitter": "Zdenek Hanzalek", "authors": "Claire Hanen, Zdenek Hanzalek", "title": "Periodic Scheduling and Packing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by periodic data transmission in autonomous cars. We\nconsidered periodic tasks (with different periods) on one or several machines.\nAfter reviewing the literature on the subject, we managed to generalize a\nresult of Lukasiewicz et al. (i.e., the equivalence of periodic scheduling with\nthe power of two periods and special 2D bin packing) to harmonic periods.\nFurthermore, we use quite old results by Coffman, Garey, and Johnson to get an\napproximation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:18:56 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Hanen", "Claire", ""], ["Hanzalek", "Zdenek", ""]]}, {"id": "2011.01973", "submitter": "Neharika Jali", "authors": "Neharika Jali, Nikhil Karamchandani, and Sharayu Moharir", "title": "Greedy $k$-Center from Noisy Distance Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the canonical $k$-center problem over a set of vertices\nin a metric space, where the underlying distances are apriori unknown. Instead,\nwe can query an oracle which provides noisy/incomplete estimates of the\ndistance between any pair of vertices. We consider two oracle models: Dimension\nSampling where each query to the oracle returns the distance between a pair of\npoints in one dimension; and Noisy Distance Sampling where the oracle returns\nthe true distance corrupted by noise. We propose active algorithms, based on\nideas such as UCB and Thompson sampling developed in the closely related\nMulti-Armed Bandit problem, which adaptively decide which queries to send to\nthe oracle and are able to solve the $k$-center problem within an approximation\nratio of two with high probability. We analytically characterize\ninstance-dependent query complexity of our algorithms and also demonstrate\nsignificant improvements over naive implementations via numerical evaluations\non two real-world datasets (Tiny ImageNet and UT Zappos50K).\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:37:02 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 15:19:50 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Jali", "Neharika", ""], ["Karamchandani", "Nikhil", ""], ["Moharir", "Sharayu", ""]]}, {"id": "2011.02017", "submitter": "Danny Vainstein", "authors": "Yossi Azar, Runtian Ren, Danny Vainstein", "title": "The Min-Cost Matching with Concave Delays Problem", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online min-cost perfect matching with concave\ndelays. We begin with the single location variant. Specifically, requests\narrive in an online fashion at a single location. The algorithm must then\nchoose between matching a pair of requests or delaying them to be matched later\non. The cost is defined by a concave function on the delay. Given linear or\neven convex delay functions, matching any two available requests is trivially\noptimal. However, this does not extend to concave delays. We solve this by\nproviding an $O(1)$-competitive algorithm that is defined through a series of\ndelay counters.\n  Thereafter we consider the problem given an underlying $n$-points metric. The\ncost of a matching is then defined as the connection cost (as defined by the\nmetric) plus the delay cost. Given linear delays, this problem was introduced\nby Emek et al. and dubbed the Min-cost perfect matching with linear delays\n(MPMD) problem. Liu et al. considered convex delays and subsequently asked\nwhether there exists a solution with small competitive ratio given concave\ndelays. We show this to be true by extending our single location algorithm and\nproving $O(\\log n)$ competitiveness. Finally, we turn our focus to the\nbichromatic case, wherein requests have polarities and only opposite polarities\nmay be matched. We show how to alter our former algorithms to again achieve\n$O(1)$ and $O(\\log n)$ competitiveness for the single location and for the\nmetric case.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 21:42:50 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Azar", "Yossi", ""], ["Ren", "Runtian", ""], ["Vainstein", "Danny", ""]]}, {"id": "2011.02046", "submitter": "Helen Xu", "authors": "Shahin Kamali, Helen Xu", "title": "Beyond Worst-case Analysis of Multicore Caching Strategies", "comments": "22 pages, short version to appear in APOCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every processor with multiple cores sharing a cache needs to implement a\ncache-replacement algorithm. Previous work demonstrated that the competitive\nratio of a large class of online algorithms, including Least-Recently-Used\n(LRU), grows with the length of the input. Furthermore, even offline algorithms\nlike Furthest-In-Future, the optimal algorithm in single-core caching, cannot\ncompete in the multicore setting. These negative results motivate a more\nin-depth comparison of multicore caching algorithms via alternative analysis\nmeasures. Specifically, the power of the adversary to adapt to online\nalgorithms suggests the need for a direct comparison of online algorithms to\neach other.\n  In this paper, we introduce cyclic analysis, a generalization of bijective\nanalysis introduced by Angelopoulos and Schweitzer [JACM'13]. Cyclic analysis\ncaptures the advantages of bijective analysis while offering flexibility that\nmakes it more useful for comparing algorithms for a variety online problems. In\nparticular, we take the first steps beyond worst-case analysis for analysis of\nmulticore caching algorithms. We use cyclic analysis to establish relationships\nbetween multicore caching algorithms, including the advantage of LRU over all\nother multicore caching algorithms in the presence of locality of reference.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 22:41:20 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kamali", "Shahin", ""], ["Xu", "Helen", ""]]}, {"id": "2011.02075", "submitter": "Kuikui Liu", "authors": "Zongchen Chen, Kuikui Liu, Eric Vigoda", "title": "Optimal Mixing of Glauber Dynamics: Entropy Factorization via\n  High-Dimensional Expansion", "comments": "v2: fixed typos, simplified proof of Lemma 5.6, added references;\n  accepted to STOC 2021 conference; comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an optimal mixing time bound on the single-site update Markov chain\nknown as the Glauber dynamics or Gibbs sampling in a variety of settings. Our\nwork presents an improved version of the spectral independence approach of\nAnari et al. (2020) and shows $O(n\\log{n})$ mixing time on any $n$-vertex graph\nof bounded degree when the maximum eigenvalue of an associated influence matrix\nis bounded. As an application of our results, for the hard-core model on\nindependent sets weighted by a fugacity $\\lambda$, we establish $O(n\\log{n})$\nmixing time for the Glauber dynamics on any $n$-vertex graph of constant\nmaximum degree $\\Delta$ when $\\lambda<\\lambda_c(\\Delta)$ where\n$\\lambda_c(\\Delta)$ is the critical point for the uniqueness/non-uniqueness\nphase transition on the $\\Delta$-regular tree. More generally, for any\nantiferromagnetic 2-spin system we prove $O(n\\log{n})$ mixing time of the\nGlauber dynamics on any bounded degree graph in the corresponding tree\nuniqueness region. Our results apply more broadly; for example, we also obtain\n$O(n\\log{n})$ mixing for $q$-colorings of triangle-free graphs of maximum\ndegree $\\Delta$ when the number of colors satisfies $q > \\alpha \\Delta$ where\n$\\alpha \\approx 1.763$, and $O(m\\log{n})$ mixing for generating random\nmatchings of any graph with bounded degree and $m$ edges.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:14:40 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 04:54:45 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 21:25:57 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chen", "Zongchen", ""], ["Liu", "Kuikui", ""], ["Vigoda", "Eric", ""]]}, {"id": "2011.02325", "submitter": "Till Fluschnik", "authors": "Till Fluschnik", "title": "A Multistage View on 2-Satisfiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $q$-SAT in the multistage model, focusing on the linear-time\nsolvable 2-SAT. Herein, given a sequence of $q$-CNF fomulas and a non-negative\ninteger $d$, the question is whether there is a sequence of satisfying truth\nassignments such that for every two consecutive truth assignments, the number\nof variables whose values changed is at most $d$. We prove that Multistage\n2-SAT is NP-hard even in quite restricted cases. Moreover, we present\nparameterized algorithms (including kernelization) for Multistage 2-SAT and\nprove them to be asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 14:50:53 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Fluschnik", "Till", ""]]}, {"id": "2011.02375", "submitter": "Paolo Giulio Franciosa", "authors": "Giorgio Ausiello, Paolo G. Franciosa, Isabella Lari, Andrea Ribichini", "title": "Max-flow vitality in undirected unweighted planar graphs", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a fast algorithm for determining the set of relevant edges in a\nplanar undirected unweighted graph with respect to the maximum flow. This is a\nspecial case of the \\emph{max flow vitality} problem, that has been efficiently\nsolved for general undirected graphs and $st$-planar graphs. The\n\\emph{vitality} of an edge of a graph with respect to the maximum flow between\ntwo fixed vertices $s$ and $t$ is defined as the reduction of the maximum flow\ncaused by the removal of that edge. In this paper we show that the set of edges\nhaving vitality greater than zero in a planar undirected unweighted graph with\n$n$ vertices, can be found in $O(n \\log n)$ worst-case time and $O(n)$ space.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:07:50 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Ausiello", "Giorgio", ""], ["Franciosa", "Paolo G.", ""], ["Lari", "Isabella", ""], ["Ribichini", "Andrea", ""]]}, {"id": "2011.02431", "submitter": "Fabrizio Frati", "authors": "Patrizio Angelini, Giordano Da Lozzo, Giuseppe Di Battista, Fabrizio\n  Frati, and Maurizio Patrignani", "title": "2-Level Quasi-Planarity or How Caterpillars Climb (SPQR-)Trees", "comments": "Extended version of a paper to appear at SODA '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a bipartite graph $G=(V_b,V_r,E)$, the $2$-Level Quasi-Planarity\nproblem asks for the existence of a drawing of $G$ in the plane such that the\nvertices in $V_b$ and in $V_r$ lie along two parallel lines $\\ell_b$ and\n$\\ell_r$, respectively, each edge in $E$ is drawn in the unbounded strip of the\nplane delimited by $\\ell_b$ and $\\ell_r$, and no three edges in $E$ pairwise\ncross.\n  We prove that the $2$-Level Quasi-Planarity problem is NP-complete. This\nanswers an open question of Dujmovi\\'c, P\\'{o}r, and Wood. Furthermore, we show\nthat the problem becomes linear-time solvable if the ordering of the vertices\nin $V_b$ along $\\ell_b$ is prescribed. Our contributions provide the first\nresults on the computational complexity of recognizing quasi-planar graphs,\nwhich is a long-standing open question.\n  Our linear-time algorithm exploits several ingredients, including a\ncombinatorial characterization of the positive instances of the problem in\nterms of the existence of a planar embedding with a caterpillar-like structure,\nand an SPQR-tree-based algorithm for testing the existence of such a planar\nembedding. Our algorithm builds upon a classification of the types of\nembeddings with respect to the structure of the portion of the caterpillar they\ncontain and performs a computation of the realizable embedding types based on a\nsuccinct description of their features by means of constant-size gadgets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:29:32 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "2011.02446", "submitter": "Siad Daboul", "authors": "Siad Daboul, Stephan Held and Jens Vygen", "title": "Approximating the discrete time-cost tradeoff problem with bounded depth", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the deadline version of the discrete time-cost tradeoff problem\nfor the special case of bounded depth. Such instances occur for example in VLSI\ndesign. The depth of an instance is the number of jobs in a longest chain and\nis denoted by $d$. We prove new upper and lower bounds on the approximability.\nFirst we observe that the problem can be regarded as a special case of finding\na minimum-weight vertex cover in a $d$-partite hypergraph. Next, we study the\nnatural LP relaxation, which can be solved in polynomial time for fixed $d$ and\n-- for time-cost tradeoff instances -- up to an arbitrarily small error in\ngeneral. Improving on prior work of Lov\\'asz and of Aharoni, Holzman and\nKrivelevich, we describe a deterministic algorithm with approximation ratio\nslightly less than $\\frac{d}{2}$ for minimum-weight vertex cover in $d$-partite\nhypergraphs for fixed $d$ and given $d$-partition. This is tight and yields\nalso a $\\frac{d}{2}$-approximation algorithm for general time-cost tradeoff\ninstances. We also study the inapproximability and show that no better\napproximation ratio than $\\frac{d+2}{4}$ is possible, assuming the Unique Games\nConjecture and $\\text{P}\\neq\\text{NP}$. This strengthens a result of Svensson,\nwho showed that under the same assumptions no constant-factor approximation\nalgorithm exists for general time-cost tradeoff instances (of unbounded depth).\nPreviously, only APX-hardness was known for bounded depth.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:58:25 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 15:15:05 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Daboul", "Siad", ""], ["Held", "Stephan", ""], ["Vygen", "Jens", ""]]}, {"id": "2011.02466", "submitter": "Zhao Song", "authors": "Josh Alman, Timothy Chu, Aaron Schild, Zhao Song", "title": "Algorithms and Hardness for Linear Algebra on Geometric Graphs", "comments": "FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For a function $\\mathsf{K} : \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\to\n\\mathbb{R}_{\\geq 0}$, and a set $P = \\{ x_1, \\ldots, x_n\\} \\subset\n\\mathbb{R}^d$ of $n$ points, the $\\mathsf{K}$ graph $G_P$ of $P$ is the\ncomplete graph on $n$ nodes where the weight between nodes $i$ and $j$ is given\nby $\\mathsf{K}(x_i, x_j)$. In this paper, we initiate the study of when\nefficient spectral graph theory is possible on these graphs. We investigate\nwhether or not it is possible to solve the following problems in $n^{1+o(1)}$\ntime for a $\\mathsf{K}$-graph $G_P$ when $d < n^{o(1)}$:\n  $\\bullet$ Multiply a given vector by the adjacency matrix or Laplacian matrix\nof $G_P$\n  $\\bullet$ Find a spectral sparsifier of $G_P$\n  $\\bullet$ Solve a Laplacian system in $G_P$'s Laplacian matrix\n  For each of these problems, we consider all functions of the form\n$\\mathsf{K}(u,v) = f(\\|u-v\\|_2^2)$ for a function $f:\\mathbb{R} \\rightarrow\n\\mathbb{R}$. We provide algorithms and comparable hardness results for many\nsuch $\\mathsf{K}$, including the Gaussian kernel, Neural tangent kernels, and\nmore. For example, in dimension $d = \\Omega(\\log n)$, we show that there is a\nparameter associated with the function $f$ for which low parameter values imply\n$n^{1+o(1)}$ time algorithms for all three of these problems and high parameter\nvalues imply the nonexistence of subquadratic time algorithms assuming Strong\nExponential Time Hypothesis ($\\mathsf{SETH}$), given natural assumptions on\n$f$.\n  As part of our results, we also show that the exponential dependence on the\ndimension $d$ in the celebrated fast multipole method of Greengard and Rokhlin\ncannot be improved, assuming $\\mathsf{SETH}$, for a broad class of functions\n$f$. To the best of our knowledge, this is the first formal limitation proven\nabout fast multipole methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 18:35:02 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Alman", "Josh", ""], ["Chu", "Timothy", ""], ["Schild", "Aaron", ""], ["Song", "Zhao", ""]]}, {"id": "2011.02601", "submitter": "Valentin Buchhold", "authors": "Valentin Buchhold, Peter Sanders, Dorothea Wagner", "title": "Fast, Exact and Scalable Dynamic Ridesharing", "comments": "Previous version augmented in several ways", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of servicing a set of ride requests by dispatching a set\nof shared vehicles, which is faced by ridesharing companies such as Uber and\nLyft. Solving this problem at a large scale might be crucial in the future for\neffectively using large fleets of autonomous vehicles. Since finding a solution\nfor the entire set of requests that minimizes the total driving time is\nNP-complete, most practical approaches process the requests one by one. Each\nrequest is inserted into any vehicle's route such that the increase in driving\ntime is minimized. Although this variant is solvable in polynomial time, it\nstill takes considerable time in current implementations, even when inexact\nfiltering heuristics are used. In this work, we present a novel algorithm for\nfinding best insertions, based on (customizable) contraction hierarchies with\nlocal buckets. Our algorithm finds provably exact solutions, is still 30 times\nfaster than a state-of-the-art algorithm currently used in industry and\nacademia, and scales much better. When used within iterative transport\nsimulations, our algorithm decreases the simulation time for largescale\nscenarios with many requests from days to hours.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:20:51 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 22:00:39 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Buchhold", "Valentin", ""], ["Sanders", "Peter", ""], ["Wagner", "Dorothea", ""]]}, {"id": "2011.02615", "submitter": "Neal E. Young", "authors": "Claire Mathieu, Rajmohan Rajaraman, Neal E. Young, Arman Yousefi", "title": "Competitive Data-Structure Dynamization", "comments": "SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-structure dynamization is a general approach for making static data\nstructures dynamic. It is used extensively in geometric settings and in the\nguise of so-called merge (or compaction) policies in big-data databases such as\nGoogle Bigtable and LevelDB (our focus). Previous theoretical work is based on\nworst-case analyses for uniform inputs -- insertions of one item at a time and\nconstant read rate. In practice, merge policies must not only handle batch\ninsertions and varying read/write ratios, they can take advantage of such\nnon-uniformity to reduce cost on a per-input basis.\n  To model this, we initiate the study of data-structure dynamization through\nthe lens of competitive analysis, via two new online set-cover problems. For\neach, the input is a sequence of disjoint sets of weighted items. The sets are\nrevealed one at a time. The algorithm must respond to each with a set cover\nthat covers all items revealed so far. It obtains the cover incrementally from\nthe previous cover by adding one or more sets and optionally removing existing\nsets. For each new set the algorithm incurs build cost equal to the weight of\nthe items in the set. In the first problem the objective is to minimize total\nbuild cost plus total query cost, where the algorithm incurs a query cost at\neach time $t$ equal to the current cover size. In the second problem, the\nobjective is to minimize the build cost while keeping the query cost from\nexceeding $k$ (a given parameter) at any time. We give deterministic online\nalgorithms for both variants, with competitive ratios of $\\Theta(\\log^* n)$ and\n$k$, respectively. The latter ratio is optimal for the second variant.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 02:09:04 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 02:26:52 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Mathieu", "Claire", ""], ["Rajaraman", "Rajmohan", ""], ["Young", "Neal E.", ""], ["Yousefi", "Arman", ""]]}, {"id": "2011.02743", "submitter": "Gorka Kobeaga", "authors": "Gorka Kobeaga, Mar\\'ia Merino, Jose A. Lozano", "title": "A revisited branch-and-cut algorithm for large-scale orienteering\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The orienteering problem is a route optimization problem which consists in\nfinding a simple cycle that maximizes the total collected profit subject to a\nmaximum distance limitation. In the last few decades, the occurrence of this\nproblem in real-life applications has boosted the development of many heuristic\nalgorithms to solve it. However, during the same period, not much research has\nbeen devoted to the field of exact algorithms for the orienteering problem. The\naim of this work is to develop an exact method which is able to obtain\noptimality certification in a wider set of instances than with previous\nmethods, or to improve the lower and upper bounds in its disability.\n  We propose a revisited version of the branch-and-cut algorithm for the\norienteering problem which includes new contributions in the separation\nalgorithms of inequalities stemming from the cycle problem, in the separation\nloop, in the variables pricing, and in the calculation of the lower and upper\nbounds of the problem. Our proposal is compared to three state-of-the-art\nalgorithms on 258 benchmark instances with up to 7397 nodes. The computational\nexperiments show the relevance of the designed components where 18 new optima,\n76 new best-known solutions and 85 new upper-bound values were obtained.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 10:30:29 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 17:07:46 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kobeaga", "Gorka", ""], ["Merino", "Mar\u00eda", ""], ["Lozano", "Jose A.", ""]]}, {"id": "2011.02761", "submitter": "Kirankumar Shiragur", "authors": "Nima Anari, Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "Instance Based Approximations to Profile Maximum Likelihood", "comments": "Accepted at Thirty-fourth Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a new efficient algorithm for approximately\ncomputing the profile maximum likelihood (PML) distribution, a prominent\nquantity in symmetric property estimation. We provide an algorithm which\nmatches the previous best known efficient algorithms for computing approximate\nPML distributions and improves when the number of distinct observed frequencies\nin the given instance is small. We achieve this result by exploiting new\nsparsity structure in approximate PML distributions and providing a new matrix\nrounding algorithm, of independent interest. Leveraging this result, we obtain\nthe first provable computationally efficient implementation of PseudoPML, a\ngeneral framework for estimating a broad class of symmetric properties.\nAdditionally, we obtain efficient PML-based estimators for distributions with\nsmall profile entropy, a natural instance-based complexity measure. Further, we\nprovide a simpler and more practical PseudoPML implementation that matches the\nbest-known theoretical guarantees of such an estimator and evaluate this method\nempirically.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 11:17:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Anari", "Nima", ""], ["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "2011.03004", "submitter": "Samer Nofal", "authors": "Samer Nofal", "title": "A Smart Backtracking Algorithm for Computing Set Partitions with Parts\n  of Certain Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\alpha=\\{a_1,a_2,a_3,...,a_n\\}$ be a set of elements, $\\delta < n$ be a\nnon-negative integer, and $\\Gamma: \\alpha \\to \\{0, 1, 2, ..., n\\}$ be a total\nmapping. Then, we call $\\Gamma$ a \\emph{partition} of $\\alpha$ if and only if\nfor all $x \\in \\alpha$, $\\Gamma(x) \\neq 0$. Further, we call $\\Gamma$ a\n$\\delta$-\\emph{partition} of $\\alpha$ if and only if $\\Gamma$ is a partition of\n$\\alpha$ and for all $i \\in \\{1, 2, 3, ..., n\\}$, $|\\{x: \\Gamma(x)=i\\}| >\n\\delta$. We give a non-trivial algorithm that computes all $\\delta$-partitions\nof $\\alpha$ in $\\Omega(n)$ time. On the opposite, a naive generate-and-test\nalgorithm would compute all $\\delta$-partitions of $\\alpha$ in $\\Omega(nB_n)$\ntime where $B_n$ is the Bell number.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 17:57:43 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 16:17:54 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Nofal", "Samer", ""]]}, {"id": "2011.03194", "submitter": "Manuel Torres", "authors": "Chandra Chekuri, Kent Quanrud and Manuel R. Torres", "title": "Fast Approximation Algorithms for Bounded Degree and Crossing Spanning\n  Tree Problems", "comments": "Updated to reflect overlap in results with arXiv:1811.07464", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop fast approximation algorithms for the minimum-cost version of the\nBounded-Degree MST problem (BD-MST) and its generalization the Crossing\nSpanning Tree problem (Crossing-ST). We solve the underlying LP to within a\n$(1+\\epsilon)$ approximation factor in near-linear time via the multiplicative\nweight update (MWU) technique. This yields, in particular, a near-linear time\nalgorithm that outputs an estimate $B$ such that $B \\le B^* \\le \\lceil\n(1+\\epsilon)B \\rceil +1$ where $B^*$ is the minimum-degree of a spanning tree\nof a given graph. To round the fractional solution, in our main technical\ncontribution, we describe a fast near-linear time implementation of\nswap-rounding in the spanning tree polytope of a graph. The fractional solution\ncan also be used to sparsify the input graph that can in turn be used to speed\nup existing combinatorial algorithms. Together, these ideas lead to\nsignificantly faster approximation algorithms than known before for the two\nproblems of interest. In addition, a fast algorithm for swap rounding in the\ngraphic matroid is a generic tool that has other applications, including to TSP\nand submodular function maximization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 05:14:27 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 21:54:10 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""], ["Torres", "Manuel R.", ""]]}, {"id": "2011.03212", "submitter": "Tiancheng Qin", "authors": "Tiancheng Qin, S. Rasoul Etesami", "title": "Optimal Online Algorithms for File-Bundle Caching and Generalization to\n  Distributed Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of the standard cache problem called file-bundle\ncaching, where different queries (tasks), each containing $l\\ge 1$ files,\nsequentially arrive. An online algorithm that does not know the sequence of\nqueries ahead of time must adaptively decide on what files to keep in the cache\nto incur the minimum number of cache misses. Here a cache miss refers to the\ncase where at least one file in a query is missing among the cache files. In\nthe special case where $l=1$, this problem reduces to the standard cache\nproblem. We first analyze the performance of the classic least recently used\n(LRU) algorithm in this setting and show that LRU is a near-optimal online\ndeterministic algorithm for file-bundle caching with regard to competitive\nratio. We then extend our results to a generalized $(h,k)$-paging problem in\nthis file-bundle setting, where the performance of the online algorithm with a\ncache size $k$ is compared to an optimal offline benchmark of a smaller cache\nsize $h<k$. In this latter case, we provide a randomized $O(l \\ln\n\\frac{k}{k-h})$-competitive algorithm for our generalized $(h,k)$-paging\nproblem, which can be viewed as an extension of the classic marking algorithm.\nWe complete this result by providing a matching lower bound for the competitive\nratio, indicating that the performance of this modified marking algorithm is\nwithin a factor of two of any randomized online algorithm. Finally, we look at\nthe distributed version of the file-bundle caching problem where there are\n$m\\ge 1$ identical caches in the system. In this case we show that for $m=l+1$\ncaches, there is a deterministic distributed caching algorithm which is\n$(l^2+l)$-competitive and a randomized distributed caching algorithm which is\n$O(l\\ln(2l+1))$-competitive when $l\\ge 2$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 07:15:39 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Qin", "Tiancheng", ""], ["Etesami", "S. Rasoul", ""]]}, {"id": "2011.03291", "submitter": "Abhyuday Pandey", "authors": "Surender Baswana and Abhyuday Pandey", "title": "Fault-Tolerant All-Pairs Mincuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected unweighted graph on $n$ vertices and $m$\nedges. We address the problem of fault-tolerant data structure for all-pairs\nmincuts in $G$ defined as follows.\n  Build a compact data structure that, on receiving a pair of vertices $s,t\\in\nV$ and any edge $(x,y)$ as query, can efficiently report the value of the\nmincut between $s$ and $t$ upon failure of the edge $(x,y)$.\n  To the best of our knowledge, there exists no data structure for this problem\nwhich takes $o(mn)$ space and a non-trivial query time. We present two compact\ndata structures for this problem.\n  - Our first data structure guarantees ${\\cal O}(1)$ query time. The space\noccupied by this data structure is ${\\cal O}(n^2)$ which matches the worst-case\nsize of a graph on $n$ vertices.\n  - Our second data structure takes ${\\cal O}(m)$ space which matches the size\nof the graph. The query time is ${\\cal O}(\\min(m,n c_{s,t}))$ where $c_{s,t}$\nis the value of the mincut between $s$ and $t$ in $G$. The query time\nguaranteed by our data structure is faster by a factor of $\\Omega(\\sqrt{n})$\ncompared to the best known algorithm to compute a $(s,t)$-mincut.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 11:33:12 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Baswana", "Surender", ""], ["Pandey", "Abhyuday", ""]]}, {"id": "2011.03433", "submitter": "Marc Roth", "authors": "Marc Roth, Johannes Schmitt and Philip Wellnitz", "title": "Detecting and Counting Small Subgraphs, and Evaluating a Parameterized\n  Tutte Polynomial: Lower Bounds via Toroidal Grids and Cayley Graph Expanders", "comments": "59 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "MPIM-Bonn-2020", "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph property $\\Phi$, we consider the problem\n$\\mathtt{EdgeSub}(\\Phi)$, where the input is a pair of a graph $G$ and a\npositive integer $k$, and the task is to decide whether $G$ contains a $k$-edge\nsubgraph that satisfies $\\Phi$. Specifically, we study the parameterized\ncomplexity of $\\mathtt{EdgeSub}(\\Phi)$ and of its counting problem\n$\\#\\mathtt{EdgeSub}(\\Phi)$ with respect to both approximate and exact counting.\nWe obtain a complete picture for minor-closed properties $\\Phi$: the decision\nproblem $\\mathtt{EdgeSub}(\\Phi)$ always admits an FPT algorithm and the\ncounting problem $\\#\\mathtt{EdgeSub}(\\Phi)$ always admits an FPTRAS. For exact\ncounting, we present an exhaustive and explicit criterion on the property\n$\\Phi$ which, if satisfied, yields fixed-parameter tractability and otherwise\n$\\#\\mathsf{W[1]}$-hardness. Additionally, most of our hardness results come\nwith an almost tight conditional lower bound under the so-called Exponential\nTime Hypothesis, ruling out algorithms for $\\#\\mathtt{EdgeSub}(\\Phi)$ that run\nin time $f(k)\\cdot|G|^{o(k/\\log k)}$ for any computable function $f$.\n  As a main technical result, we gain a complete understanding of the\ncoefficients of toroidal grids and selected Cayley graph expanders in the\nhomomorphism basis of $\\#\\mathtt{EdgeSub}(\\Phi)$. This allows us to establish\nhardness of exact counting using the Complexity Monotonicity framework due to\nCurticapean, Dell and Marx (STOC'17). Our methods can also be applied to a\nparameterized variant of the Tutte Polynomial $T^k_G$ of a graph $G$, to which\nmany known combinatorial interpretations of values of the (classical) Tutte\nPolynomial can be extended. As an example, $T^k_G(2,1)$ corresponds to the\nnumber of $k$-forests in the graph $G$. Our techniques allow us to completely\nunderstand the parametrized complexity of computing the evaluation of $T^k_G$\nat every pair of rational coordinates $(x,y)$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:29:08 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 15:12:31 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Roth", "Marc", ""], ["Schmitt", "Johannes", ""], ["Wellnitz", "Philip", ""]]}, {"id": "2011.03434", "submitter": "Telikepalli Kavitha", "authors": "Telikepalli Kavitha", "title": "Maximum Matchings and Popularity", "comments": "17 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a bipartite graph where every node has a strict ranking of its\nneighbors. For every node, its preferences over neighbors extend naturally to\npreferences over matchings. Matching $N$ is more popular than matching $M$ if\nthe number of nodes that prefer $N$ to $M$ is more than the number that prefer\n$M$ to $N$. A maximum matching $M$ in $G$ is a \"popular max-matching\" if there\nis no maximum matching in $G$ that is more popular than $M$. Such matchings are\nrelevant in applications where the set of admissible solutions is the set of\nmaximum matchings and we wish to find a best maximum matching as per node\npreferences. It is known that a popular max-matching always exists in $G$. Here\nwe show a compact extended formulation for the popular max-matching polytope.\nSo when there are edge costs, a min-cost popular max-matching in $G$ can be\ncomputed in polynomial time. This is in contrast to the min-cost popular\nmatching problem which is known to be NP-hard. We also consider\nPareto-optimality, which is a relaxation of popularity, and show that computing\na min-cost Pareto-optimal matching/max-matching is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:30:39 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Kavitha", "Telikepalli", ""]]}, {"id": "2011.03454", "submitter": "Weihang Wang", "authors": "Karthekeyan Chandrasekaran, Weihang Wang", "title": "Fixed Parameter Approximation Scheme for Min-max $k$-cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the graph $k$-partitioning problem under the min-max objective,\ntermed as Minmax $k$-cut. The input here is a graph $G=(V,E)$ with non-negative\nedge weights $w:E\\rightarrow \\mathbb{R}_+$ and an integer $k\\geq 2$ and the\ngoal is to partition the vertices into $k$ non-empty parts $V_1, \\ldots, V_k$\nso as to minimize $\\max_{i=1}^k w(\\delta(V_i))$. Although minimizing the sum\nobjective $\\sum_{i=1}^k w(\\delta(V_i))$, termed as Minsum $k$-cut, has been\nstudied extensively in the literature, very little is known about minimizing\nthe max objective. We initiate the study of Minmax $k$-cut by showing that it\nis NP-hard and W[1]-hard when parameterized by $k$, and design a parameterized\napproximation scheme when parameterized by $k$. The main ingredient of our\nparameterized approximation scheme is an exact algorithm for Minmax $k$-cut\nthat runs in time $(\\lambda k)^{O(k^2)}n^{O(1)}$, where $\\lambda$ is value of\nthe optimum and $n$ is the number of vertices. Our algorithmic technique builds\non the technique of Lokshtanov, Saurabh, and Surianarayanan (FOCS, 2020) who\nshowed a similar result for Minsum $k$-cut. Our algorithmic techniques are more\ngeneral and can be used to obtain parameterized approximation schemes for\nminimizing $\\ell_p$-norm measures of $k$-partitioning for every $p\\geq 1$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:09:21 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Wang", "Weihang", ""]]}, {"id": "2011.03495", "submitter": "Kevin Tian", "authors": "Sepehr Assadi, Arun Jambulapati, Yujia Jin, Aaron Sidford, Kevin Tian", "title": "Semi-Streaming Bipartite Matching in Fewer Passes and Optimal Space", "comments": "54 pages, v3 adds applications to transshipment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide $\\widetilde{O}(\\epsilon^{-1})$-pass semi-streaming algorithms for\ncomputing $(1-\\epsilon)$-approximate maximum cardinality matchings in bipartite\ngraphs. Our most efficient methods are deterministic and use optimal, $O(n)$,\nspace, improving upon the space complexity of the previous state-of-the-art\n$\\widetilde{O}(\\epsilon^{-1})$-pass algorithm of Ahn and Gupta. To obtain our\nresults we provide semi-streaming adaptations of more general continuous\noptimization tools. Further, we leverage these techniques to obtain\nimprovements for streaming variants of approximate linear programming, optimal\ntransport, exact matching, transshipment, and shortest path problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:06:31 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 21:40:02 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 17:57:10 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Assadi", "Sepehr", ""], ["Jambulapati", "Arun", ""], ["Jin", "Yujia", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "2011.03603", "submitter": "Kiril Solovey", "authors": "Kiril Solovey, Saptarshi Bandyopadhyay, Federico Rossi, Michael T.\n  Wolf, and Marco Pavone", "title": "Fast Near-Optimal Heterogeneous Task Allocation via Flow Decomposition", "comments": "Extended version of a conference paper that appeared in the\n  International Conference on Robotics and Automation (ICRA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot systems are uniquely well-suited to performing complex tasks such\nas patrolling and tracking, information gathering, and pick-up and delivery\nproblems, offering significantly higher performance than single-robot systems.\nA fundamental building block in most multi-robot systems is task allocation:\nassigning robots to tasks (e.g., patrolling an area, or servicing a\ntransportation request) as they appear based on the robots' states to maximize\nreward. In many practical situations, the allocation must account for\nheterogeneous capabilities (e.g., availability of appropriate sensors or\nactuators) to ensure the feasibility of execution, and to promote a higher\nreward, over a long time horizon. To this end, we present the FlowDec algorithm\nfor efficient heterogeneous task-allocation achieving an approximation factor\nof at least 1/2 of the optimal reward. Our approach decomposes the\nheterogeneous problem into several homogeneous subproblems that can be solved\nefficiently using min-cost flow. Through simulation experiments, we show that\nour algorithm is faster by several orders of magnitude than a MILP approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:29:55 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 18:56:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Solovey", "Kiril", ""], ["Bandyopadhyay", "Saptarshi", ""], ["Rossi", "Federico", ""], ["Wolf", "Michael T.", ""], ["Pavone", "Marco", ""]]}, {"id": "2011.03607", "submitter": "Charlie Dickens", "authors": "Charlie Dickens", "title": "Ridge Regression with Frequent Directions: Statistical and Optimization\n  Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its impressive theory \\& practical performance, Frequent Directions\n(\\acrshort{fd}) has not been widely adopted for large-scale regression tasks.\nPrior work has shown randomized sketches (i) perform worse in estimating the\ncovariance matrix of the data than \\acrshort{fd}; (ii) incur high error when\nestimating the bias and/or variance on sketched ridge regression. We give the\nfirst constant factor relative error bounds on the bias \\& variance for\nsketched ridge regression using \\acrshort{fd}. We complement these statistical\nresults by showing that \\acrshort{fd} can be used in the optimization setting\nthrough an iterative scheme which yields high-accuracy solutions. This improves\non randomized approaches which need to compromise the need for a new sketch\nevery iteration with speed of convergence. In both settings, we also show using\n\\emph{Robust Frequent Directions} further enhances performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:40:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Dickens", "Charlie", ""]]}, {"id": "2011.03619", "submitter": "Danil Sagunov", "authors": "Fedor V. Fomin, Petr A. Golovach, Danil Sagunov, Kirill Simonov", "title": "Algorithmic Extensions of Dirac's Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1952, Dirac proved the following theorem about long cycles in graphs with\nlarge minimum vertex degrees: Every $n$-vertex $2$-connected graph $G$ with\nminimum vertex degree $\\delta\\geq 2$ contains a cycle with at least\n$\\min\\{2\\delta,n\\}$ vertices. In particular, if $\\delta\\geq n/2$, then $G$ is\nHamiltonian. The proof of Dirac's theorem is constructive, and it yields an\nalgorithm computing the corresponding cycle in polynomial time. The\ncombinatorial bound of Dirac's theorem is tight in the following sense. There\nare 2-connected graphs that do not contain cycles of length more than\n$2\\delta+1$. Also, there are non-Hamiltonian graphs with all vertices but one\nof degree at least $n/2$. This prompts naturally to the following algorithmic\nquestions. For $k\\geq 1$,\n  (A) How difficult is to decide whether a 2-connected graph contains a cycle\nof length at least $\\min\\{2\\delta+k,n\\}$?\n  (B) How difficult is to decide whether a graph $G$ is Hamiltonian, when at\nleast $n - k$ vertices of $G$ are of degrees at least $n/2-k$?\n  The first question was asked by Fomin, Golovach, Lokshtanov, Panolan,\nSaurabh, and Zehavi. The second question is due to Jansen, Kozma, and Nederlof.\nEven for a very special case of $k=1$, the existence of a polynomial-time\nalgorithm deciding whether $G$ contains a cycle of length at least\n$\\min\\{2\\delta+1,n\\}$ was open. We resolve both questions by proving the\nfollowing algorithmic generalization of Dirac's theorem: If all but $k$\nvertices of a $2$-connected graph $G$ are of degree at least $\\delta$, then\ndeciding whether $G$ has a cycle of length at least $\\min\\{2\\delta +k, n\\}$ can\nbe done in time $2^{\\mathcal{O}(k)}\\cdot n^{\\mathcal{O}(1)}$.\n  The proof of the algorithmic generalization of Dirac's theorem builds on new\ngraph-theoretical results that are interesting on their own.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:32:33 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 10:35:54 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 17:14:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Sagunov", "Danil", ""], ["Simonov", "Kirill", ""]]}, {"id": "2011.03622", "submitter": "Allen Liu", "authors": "Allen Liu, Ankur Moitra", "title": "Settling the Robust Learnability of Mixtures of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work represents a natural coalescence of two important lines of work:\nlearning mixtures of Gaussians and algorithmic robust statistics. In particular\nwe give the first provably robust algorithm for learning mixtures of any\nconstant number of Gaussians. We require only mild assumptions on the mixing\nweights (bounded fractionality) and that the total variation distance between\ncomponents is bounded away from zero. At the heart of our algorithm is a new\nmethod for proving dimension-independent polynomial identifiability through\napplying a carefully chosen sequence of differential operations to certain\ngenerating functions that not only encode the parameters we would like to learn\nbut also the system of polynomial equations we would like to solve. We show how\nthe symbolic identities we derive can be directly used to analyze a natural\nsum-of-squares relaxation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:36:00 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:49:01 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 20:46:03 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2011.03636", "submitter": "Victor Parque", "authors": "Victor Parque, Tomoyuki Miyashita", "title": "An Efficient Scheme for the Generation of Ordered Trees in Constant\n  Amortized Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trees are useful entities allowing to model data structures and hierarchical\nrelationships in networked decision systems ubiquitously. An ordered tree is a\nrooted tree where the order of the subtrees (children) of a node is\nsignificant. In combinatorial optimization, generating ordered trees is\nrelevant to evaluate candidate combinatorial objects. In this paper, we present\nan algebraic scheme to generate ordered trees with $n$ vertices with utmost\nefficiency; whereby our approach uses $\\mathcal{O}(n)$ space and\n$\\mathcal{O}(1)$ time in average per tree. Our computational studies have shown\nthe feasibility and efficiency to generate ordered trees in constant time in\naverage, in about one tenth of a millisecond per ordered tree. Due to the 1-1\nbijective nature to other combinatorial classes, our approach is favorable to\nstudy the generation of binary trees with $n$ external nodes, trees with $n$\nnodes, legal sequences of $n$ pairs of parentheses, triangulated $n$-gons,\ngambler's sequences and lattice paths. We believe our scheme may find its use\nin devising algorithms for planning and combinatorial optimization involving\nCatalan numbers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:41:26 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Parque", "Victor", ""], ["Miyashita", "Tomoyuki", ""]]}, {"id": "2011.03639", "submitter": "Hunter Lang", "authors": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "title": "Graph cuts always find a global optimum for Potts models (with a catch)", "comments": "Published at ICML 2021. 18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the $\\alpha$-expansion algorithm for MAP inference always\nreturns a globally optimal assignment for Markov Random Fields with Potts\npairwise potentials, with a catch: the returned assignment is only guaranteed\nto be optimal for an instance within a small perturbation of the original\nproblem instance. In other words, all local minima with respect to expansion\nmoves are global minima to slightly perturbed versions of the problem. On\n\"real-world\" instances, MAP assignments of small perturbations of the problem\nshould be very similar to the MAP assignment(s) of the original problem\ninstance. We design an algorithm that can certify whether this is the case in\npractice. On several MAP inference problem instances from computer vision, this\nalgorithm certifies that MAP solutions to all of these perturbations are very\nclose to solutions of the original instance. These results taken together give\na cohesive explanation for the good performance of \"graph cuts\" algorithms in\npractice. Every local expansion minimum is a global minimum in a small\nperturbation of the problem, and all of these global minima are close to the\noriginal solution.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 00:01:06 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 01:33:06 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lang", "Hunter", ""], ["Sontag", "David", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2011.03693", "submitter": "Dmitriy Kunisky", "authors": "Dmitriy Kunisky", "title": "Hypothesis testing with low-degree polynomials in the Morris class of\n  exponential families", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of low-degree polynomial algorithms is a powerful, newly-popular\nmethod for predicting computational thresholds in hypothesis testing problems.\nOne limitation of current techniques for this analysis is their restriction to\nBernoulli and Gaussian distributions. We expand this range of possibilities by\nperforming the low-degree analysis of hypothesis testing for the Morris class\nof exponential families, giving a unified treatment of Gaussian, Poisson,\ngamma, binomial, negative binomial, and generalized hyperbolic secant\ndistributions. We then give several algorithmic applications.\n  1. In models where a random signal is observed through an exponential family,\nthe success or failure of low-degree polynomials is governed by the $z$-score\noverlap, the inner product of $z$-score vectors with respect to the null\ndistribution of two independent copies of the signal.\n  2. In the same models, testing with low-degree polynomials exhibits channel\nmonotonicity: the above distributions admit a total ordering by computational\ncost of hypothesis testing, according to a scalar parameter describing how the\nvariance depends on the mean in an exponential family.\n  3. In a spiked matrix model with a particular non-Gaussian noise\ndistribution, the low-degree prediction is incorrect unless polynomials with\narbitrarily large degree in individual matrix entries are permitted. This shows\nthat polynomials summing over self-avoiding walks and variants thereof, as\nproposed recently by Ding, Hopkins, and Steurer (2020) for spiked matrix models\nwith heavy-tailed noise, are suboptimal for this model. Thus low-degree\npolynomials appear to offer a tradeoff between robustness and strong\nperformance fine-tuned to specific models, and may struggle with problems\nrequiring an algorithm to first examine the input and then use some\nintermediate computation to choose from one of several inference subroutines.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 04:48:00 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kunisky", "Dmitriy", ""]]}, {"id": "2011.03700", "submitter": "Akbar Rafiey", "authors": "Andrei A. Bulatov, Akbar Rafiey", "title": "On the Complexity of CSP-based Ideal Membership Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LO math.AC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the Ideal Membership Problem (IMP for short), in\nwhich we are given real polynomials $f_0,f_1,\\dots, f_k$ and the question is to\ndecide whether $f_0$ belongs to the ideal generated by $f_1,\\dots,f_k$. In the\nmore stringent version the task is also to find a proof of this fact. The IMP\nunderlies many proof systems based on polynomials such as Nullstellensatz,\nPolynomial Calculus, and Sum-of-Squares. In the majority of such applications\nthe IMP involves so called combinatorial ideals that arise from a variety of\ndiscrete combinatorial problems. This restriction makes the IMP significantly\neasier and in some cases allows for an efficient algorithm to solve it.\n  The first part of this paper follows the work of Mastrolilli [SODA'19] who\ninitiated a systematic study of IMPs arising from Constraint Satisfaction\nProblems (CSP) of the form $CSP(\\Gamma)$, that is, CSPs in which the type of\nconstraints is limited to relations from a set $\\Gamma$. We show that many CSP\ntechniques can be translated to IMPs thus allowing us to significantly improve\nthe methods of studying the complexity of the IMP. We also develop universal\nalgebraic techniques for the IMP that have been so useful in the study of the\nCSP. This allows us to prove a general necessary condition for the tractability\nof the IMP, and three sufficient ones. The sufficient conditions include IMPs\narising from systems of linear equations over $GF(p)$, $p$ prime, and also some\nconditions defined through special kinds of polymorphisms.\n  Our work has several consequences and applications in terms of bit complexity\nof sum-of-squares (SOS) proofs and their automatizability, and studying\n(construction of) theta bodies of combinatorial problems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 05:22:42 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 15:50:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bulatov", "Andrei A.", ""], ["Rafiey", "Akbar", ""]]}, {"id": "2011.03704", "submitter": "Matthew Ferland", "authors": "Kyle Burke, Matthew Ferland, Shang-Hua Teng", "title": "Quantum Combinatorial Games: Structures and Computational Complexity", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a standardized framework was proposed for introducing\nquantum-inspired moves in mathematical games with perfect information and no\nchance. The beauty of quantum games-succinct in representation, rich in\nstructures, explosive in complexity, dazzling for visualization, and\nsophisticated for strategic reasoning-has drawn us to play concrete games full\nof subtleties and to characterize abstract properties pertinent to complexity\nconsequence. Going beyond individual games, we explore the tractability of\nquantum combinatorial games as whole, and address fundamental questions\nincluding:\n  Quantum Leap in Complexity: Are there polynomial-time solvable games whose\nquantum extensions are intractable?\n  Quantum Collapses in Complexity: Are there PSPACE-complete games whose\nquantum extensions fall to the lower levels of the polynomial-time hierarchy?\n  Quantumness Matters: How do outcome classes and strategies change under\nquantum moves? Under what conditions doesn't quantumness matter?\n  PSPACE Barrier for Quantum Leap: Can quantum moves launch PSPACE games into\nouter polynomial space\n  We show that quantum moves not only enrich the game structure, but also\nimpact their computational complexity. In settling some of these basic\nquestions, we characterize both the powers and limitations of quantum moves as\nwell as the superposition of game configurations that they create. Our\nconstructive proofs-both on the leap of complexity in concrete Quantum Nim and\nQuantum Undirected Geography and on the continuous collapses, in the quantum\nsetting, of complexity in abstract PSPACE-complete games to each level of the\npolynomial-time hierarchy-illustrate the striking computational landscape over\nquantum games and highlight surprising turns with unexpected quantum impact.\nOur studies also enable us to identify several elegant open questions\nfundamental to quantum combinatorial game theory (QCGT).\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 06:09:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Burke", "Kyle", ""], ["Ferland", "Matthew", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "2011.03778", "submitter": "Karol W\\k{e}grzycki", "authors": "S\\'andor Kisfaludi-Bak, Jesper Nederlof, Karol W\\k{e}grzycki", "title": "A Gap-ETH-Tight Approximation Scheme for Euclidean TSP", "comments": "43 pages, faster algorithms for Euclidean and Rectilinear Steiner\n  Tree", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic task of finding the shortest tour of $n$ points in\n$d$-dimensional Euclidean space, for any fixed constant $d \\geq 2$. We\ndetermine the optimal dependence on $\\varepsilon$ in the running time of an\nalgorithm that computes a $(1+\\varepsilon)$-approximate tour, under a plausible\nassumption. Specifically, we give an algorithm that runs in\n$2^{O(1/\\varepsilon^{d-1})} n\\log n$ time. This improves the previously\nsmallest dependence on $\\varepsilon$ in the running time\n$(1/\\varepsilon)^{O(1/\\varepsilon^{d-1})}n \\log n$ of the algorithm by Rao and\nSmith (STOC 1998). We also show that a\n$2^{o(1/\\varepsilon^{d-1})}\\mathrm{poly}(n)$ algorithm would violate the\nGap-Exponential Time Hypothesis (Gap-ETH).\n  Our new algorithm builds upon the celebrated quadtree-based methods initially\nproposed by Arora (J. ACM 1998), but it adds a new idea that we call\nsparsity-sensitive patching. On a high level this lets the granularity with\nwhich we simplify the tour depend on how sparse it is locally. We demonstrate\nthat our technique extends to other problems, by showing that for Steiner Tree\nand Rectilinear Steiner Tree it yields the same running time. We complement our\nresults with a matching Gap-ETH lower bound for Rectilinear Steiner Tree.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 13:50:43 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:59:33 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kisfaludi-Bak", "S\u00e1ndor", ""], ["Nederlof", "Jesper", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "2011.03819", "submitter": "Ce Jin", "authors": "Ce Jin, Nikhil Vyas, Ryan Williams", "title": "Fast Low-Space Algorithms for Subset Sum", "comments": "To appear in SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the canonical Subset Sum problem: given a list of positive\nintegers $a_1,\\ldots,a_n$ and a target integer $t$ with $t > a_i$ for all $i$,\ndetermine if there is an $S \\subseteq [n]$ such that $\\sum_{i \\in S} a_i = t$.\nThe well-known pseudopolynomial-time dynamic programming algorithm [Bellman,\n1957] solves Subset Sum in $O(nt)$ time, while requiring $\\Omega(t)$ space.\n  In this paper we present algorithms for Subset Sum with $\\tilde O(nt)$\nrunning time and much lower space requirements than Bellman's algorithm, as\nwell as that of prior work. We show that Subset Sum can be solved in $\\tilde\nO(nt)$ time and $O(\\log(nt))$ space with access to $O(\\log n \\log \\log n+\\log\nt)$ random bits. This significantly improves upon the $\\tilde O(n\nt^{1+\\varepsilon})$-time, $\\tilde O(n\\log t)$-space algorithm of Bringmann\n(SODA 2017). We also give an $\\tilde O(n^{1+\\varepsilon}t)$-time,\n$O(\\log(nt))$-space randomized algorithm, improving upon previous\n$(nt)^{O(1)}$-time $O(\\log(nt))$-space algorithms by Elberfeld, Jakoby, and\nTantau (FOCS 2010), and Kane (2010). In addition, we also give a $\\mathrm{poly}\n\\log(nt)$-space, $\\tilde O(n^2 t)$-time deterministic algorithm.\n  We also study time-space trade-offs for Subset Sum. For parameter $1\\le k\\le\n\\min\\{n,t\\}$, we present a randomized algorithm running in $\\tilde O((n+t)\\cdot\nk)$ time and $O((t/k) \\mathrm{polylog} (nt))$ space.\n  As an application of our results, we give an\n$\\tilde{O}(\\min\\{n^2/\\varepsilon, n/\\varepsilon^2\\})$-time and\n$\\mathrm{polylog}(nt)$-space algorithm for \"weak\" $\\varepsilon$-approximations\nof Subset Sum.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 17:46:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Jin", "Ce", ""], ["Vyas", "Nikhil", ""], ["Williams", "Ryan", ""]]}, {"id": "2011.03865", "submitter": "Rad Niazadeh", "authors": "Rad Niazadeh, Renato Paes Leme, Jon Schneider", "title": "Combinatorial Bernoulli Factories: Matchings, Flows and Other Polytopes", "comments": "41 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bernoulli factory is an algorithmic procedure for exact sampling of certain\nrandom variables having only Bernoulli access to their parameters. Bernoulli\naccess to a parameter $p \\in [0,1]$ means the algorithm does not know $p$, but\nhas sample access to independent draws of a Bernoulli random variable with mean\nequal to $p$. In this paper, we study the problem of Bernoulli factories for\npolytopes: given Bernoulli access to a vector $x\\in \\mathcal{P}$ for a given\npolytope $\\mathcal{P}\\subset [0,1]^n$, output a randomized vertex such that the\nexpected value of the $i$-th coordinate is \\emph{exactly} equal to $x_i$. For\nexample, for the special case of the perfect matching polytope, one is given\nBernoulli access to the entries of a doubly stochastic matrix $[x_{ij}]$ and\nasked to sample a matching such that the probability of each edge $(i,j)$ be\npresent in the matching is exactly equal to $x_{ij}$.\n  We show that a polytope $\\mathcal{P}$ admits a Bernoulli factory if and and\nonly if $\\mathcal{P}$ is the intersection of $[0,1]^n$ with an affine subspace.\nOur construction is based on an algebraic formulation of the problem, involving\nidentifying a family of Bernstein polynomials (one per vertex) that satisfy a\ncertain algebraic identity on $\\mathcal{P}$. The main technical tool behind our\nconstruction is a connection between these polynomials and the geometry of\nzonotope tilings. We apply these results to construct an explicit factory for\nthe perfect matching polytope. The resulting factory is deeply connected to the\ncombinatorial enumeration of arborescences and may be of independent interest.\nFor the $k$-uniform matroid polytope, we recover a sampling procedure known in\nstatistics as Sampford sampling.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 23:09:16 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Niazadeh", "Rad", ""], ["Leme", "Renato Paes", ""], ["Schneider", "Jon", ""]]}, {"id": "2011.03892", "submitter": "Nicole Wein", "authors": "Mina Dalirrooyfard and Nicole Wein", "title": "Tight Conditional Lower Bounds for Approximating Diameter in Directed\n  Graphs", "comments": "Updated to cite concurrent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the most fundamental graph parameters is the Diameter, the largest\ndistance between any pair of vertices. Computing the Diameter of a graph with\n$m$ edges requires $m^{2-o(1)}$ time under the Strong Exponential Time\nHypothesis (SETH), which can be prohibitive for very large graphs, so efficient\napproximation algorithms for Diameter are desired.\n  There is a folklore algorithm that gives a $2$-approximation for Diameter in\n$\\tilde{O}(m)$ time. Additionally, a line of work concludes with a\n$3/2$-approximation algorithm for Diameter in weighted directed graphs that\nruns in $\\tilde{O}(m^{3/2})$ time. The $3/2$-approximation algorithm is known\nto be tight under SETH: Roditty and Vassilevska W. proved that under SETH any\n$3/2-\\epsilon$ approximation algorithm for Diameter in undirected unweighted\ngraphs requires $m^{2-o(1)}$ time, and then Backurs, Roditty, Segal,\nVassilevska W., and Wein and the follow-up work of Li proved that under SETH\nany $5/3-\\epsilon$ approximation algorithm for Diameter in undirected\nunweighted graphs requires $m^{3/2-o(1)}$ time.\n  Whether or not the folklore 2-approximation algorithm is tight, however, is\nunknown, and has been explicitly posed as an open problem in numerous papers.\nTowards this question, Bonnet recently proved that under SETH, any\n$7/4-\\epsilon$ approximation requires $m^{4/3-o(1)}$, only for directed\nweighted graphs.\n  We completely resolve this question for directed graphs by proving that the\nfolklore 2-approximation algorithm is conditionally optimal. In doing so, we\nobtain a series of conditional lower bounds that together with prior work, give\na complete time-accuracy trade-off that is tight with all known algorithms for\ndirected graphs. Specifically, we prove that under SETH for any $\\delta>0$, a\n$(\\frac{2k-1}{k}-\\delta)$-approximation algorithm for Diameter on directed\nunweighted graphs requires $m^{\\frac{k}{k-1}-o(1)}$ time.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 02:46:21 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 18:46:15 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Dalirrooyfard", "Mina", ""], ["Wein", "Nicole", ""]]}, {"id": "2011.03915", "submitter": "Kun He", "authors": "Weiming Feng, Kun He, Yitong Yin", "title": "Sampling Constraint Satisfaction Solutions in the Local Lemma Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a Markov chain based algorithm for sampling almost uniform solutions\nof constraint satisfaction problems (CSPs). Assuming a canonical setting for\nthe Lov\\'asz local lemma, where each constraint is violated by a small number\nof forbidden local configurations, our sampling algorithm is accurate in a\nlocal lemma regime, and the running time is a fixed polynomial whose dependency\non $n$ is close to linear, where $n$ is the number of variables. Our main\napproach is a new technique called state compression, which generalizes the\n\"mark/unmark\" paradigm of Moitra (Moitra, JACM, 2019), and can give fast\nlocal-lemma-based sampling algorithms. As concrete applications of our\ntechnique, we give the current best almost-uniform samplers for hypergraph\ncolorings and for CNF solutions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 07:33:52 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 04:53:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Feng", "Weiming", ""], ["He", "Kun", ""], ["Yin", "Yitong", ""]]}, {"id": "2011.04010", "submitter": "Anand Natrajan", "authors": "Anand Natrajan, Mallige Anand", "title": "Scout Algorithm For Fast Substring Matching", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exact substring matching is a common task in many software applications.\nDespite the existence of several algorithms for finding whether or not a\npattern string is present in a target string, the most common implementation is\na na\\\"ive, brute force approach. Alternative approaches either do not provide\nenough of a benefit for the added complexity, or are impractical for modern\ncharacter sets, e.g., Unicode. We present a new algorithm, Scout, that is\nstraightforward, quick and appropriate for all applications. We also compare\nthe performance characteristics of the Scout algorithm with several others.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:09:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Natrajan", "Anand", ""], ["Anand", "Mallige", ""]]}, {"id": "2011.04022", "submitter": "Ren\\'e van Bevern", "authors": "Vsevolod A. Afanasev and Ren\\'e van Bevern and Oxana Yu. Tsidulko", "title": "The Hierarchical Chinese Postman Problem: the slightest disorder makes\n  it hard, yet disconnectedness is manageable", "comments": "Fixed Figure 4 and an argument in the proof of Lemma 3.7(iii)", "journal-ref": "Operations Research Letters, 49(2): 270-277, 2021", "doi": "10.1016/j.orl.2021.01.017", "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hierarchical Chinese Postman Problem is finding a shortest traversal of\nall edges of a graph respecting precedence constraints given by a partial order\non classes of edges. We show that the special case with connected classes is\nNP-hard even on orders decomposable into a chain and an incomparable class. For\nthe case with linearly ordered (possibly disconnected) classes, we get\n5/3-approximations and fixed-parameter algorithms by transferring results from\nthe Rural Postman Problem.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:56:26 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 05:37:50 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Afanasev", "Vsevolod A.", ""], ["van Bevern", "Ren\u00e9", ""], ["Tsidulko", "Oxana Yu.", ""]]}, {"id": "2011.04047", "submitter": "Lorenzo Balzotti", "authors": "Lorenzo Balzotti and Paolo G. Franciosa", "title": "Computing Lengths of Shortest Non-Crossing Paths in Planar Graphs", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a plane undirected graph $G$ with non-negative edge weights and a set\nof $k$ terminal pairs on the external face, it is shown in Takahashi et al.,\n(Algorithmica, 16, 1996, pp. 339-357) that the lengths of $k$ non-crossing\nshortest paths joining the $k$ terminal pairs (if they exist) can be computed\nin $O(n \\log n)$ worst-case time, where $n$ is the number of vertices of $G$.\nThis technique only applies when the union $U$ of the computed shortest paths\nis a forest. We show that given a plane undirected weighted graph $U$ and a set\nof $k$ terminal pairs on the external face, it is always possible to compute\nthe lengths of $k$ non-crossing shortest paths joining the $k$ terminal pairs\nin linear worst-case time, provided that the graph $U$ is the union of $k$\nshortest paths, possibly containing cycles. Moreover, each shortest path $\\pi$\ncan be listed in $O(\\ell+\\ell\\log\\lceil{\\frac{k}{\\ell}}\\rceil)$, where $\\ell$\nis the number of edges in $\\pi$. As a consequence, the problem of computing\nmulti-terminal distances in a plane undirected weighted graph can always be\nsolved in $O(n \\log k)$ worst-case time in the general case.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:36:28 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 20:52:51 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Balzotti", "Lorenzo", ""], ["Franciosa", "Paolo G.", ""]]}, {"id": "2011.04125", "submitter": "Nadiia Chepurko", "authors": "Nadiia Chepurko, Kenneth L. Clarkson, Lior Horesh, David P. Woodruff", "title": "Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra", "comments": "The analysis of the sparse gaussian contained a bug. The fix along\n  with applications appears in arXiv:2107.08090 , the quantum data structure\n  results remain unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create classical (non-quantum) dynamic data structures supporting queries\nfor recommender systems and least-squares regression that are comparable to\ntheir quantum analogues. De-quantizing such algorithms has received a flurry of\nattention in recent years; we obtain sharper bounds for these problems. More\nsignificantly, we achieve these improvements by arguing that the previous\nquantum-inspired algorithms for these problems are doing leverage or\nridge-leverage score sampling in disguise; these are powerful and standard\ntechniques in randomized numerical linear algebra. With this recognition, we\nare able to employ the large body of work in numerical linear algebra to obtain\nalgorithms for these problems that are simpler or faster (or both) than\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 01:13:07 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 03:18:48 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 23:03:54 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 19:40:46 GMT"}, {"version": "v5", "created": "Tue, 20 Jul 2021 15:40:20 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chepurko", "Nadiia", ""], ["Clarkson", "Kenneth L.", ""], ["Horesh", "Lior", ""], ["Woodruff", "David P.", ""]]}, {"id": "2011.04144", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Sutanu Gayen, Eric Price, N. V. Vinodchandran", "title": "Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide finite sample guarantees for the classical Chow-Liu algorithm\n(IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model\nof a distribution. For a distribution $P$ on $\\Sigma^n$ and a tree $T$ on $n$\nnodes, we say $T$ is an $\\varepsilon$-approximate tree for $P$ if there is a\n$T$-structured distribution $Q$ such that $D(P\\;||\\;Q)$ is at most\n$\\varepsilon$ more than the best possible tree-structured distribution for $P$.\nWe show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with\nthe plug-in estimator for mutual information with $\\widetilde{O}(|\\Sigma|^3\nn\\varepsilon^{-1})$ i.i.d.~samples outputs an $\\varepsilon$-approximate tree\nfor $P$ with constant probability. In contrast, for a general $P$ (which may\nnot be tree-structured), $\\Omega(n^2\\varepsilon^{-2})$ samples are necessary to\nfind an $\\varepsilon$-approximate tree. Our upper bound is based on a new\nconditional independence tester that addresses an open problem posed by\nCanonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three\nrandom variables $X,Y,Z$ each over $\\Sigma$, testing if $I(X; Y \\mid Z)$ is $0$\nor $\\geq \\varepsilon$ is possible with $\\widetilde{O}(|\\Sigma|^3/\\varepsilon)$\nsamples. Finally, we show that for a specific tree $T$, with $\\widetilde{O}\n(|\\Sigma|^2n\\varepsilon^{-1})$ samples from a distribution $P$ over $\\Sigma^n$,\none can efficiently learn the closest $T$-structured distribution in KL\ndivergence by applying the add-1 estimator at each node.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 02:08:56 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 06:37:12 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gayen", "Sutanu", ""], ["Price", "Eric", ""], ["Vinodchandran", "N. V.", ""]]}, {"id": "2011.04219", "submitter": "Anay Mehrotra", "authors": "Anay Mehrotra and L. Elisa Celis", "title": "Mitigating Bias in Set Selection with Noisy Protected Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection algorithms are ubiquitous in AI-driven applications,\nincluding, online recruiting portals and image search engines, so it is\nimperative that these tools are not discriminatory on the basis of protected\nattributes such as gender or race. Currently, fair subset selection algorithms\nassume that the protected attributes are known as part of the dataset. However,\nprotected attributes may be noisy due to errors during data collection or if\nthey are imputed (as is often the case in real-world settings). While a wide\nbody of work addresses the effect of noise on the performance of machine\nlearning algorithms, its effect on fairness remains largely unexamined. We find\nthat in the presence of noisy protected attributes, in attempting to increase\nfairness without considering noise, one can, in fact, decrease the fairness of\nthe result!\n  Towards addressing this, we consider an existing noise model in which there\nis probabilistic information about the protected attributes (e.g., [58, 34, 20,\n46]), and ask is fair selection possible under noisy conditions? We formulate a\n``denoised'' selection problem which functions for a large class of fairness\nmetrics; given the desired fairness goal, the solution to the denoised problem\nviolates the goal by at most a small multiplicative amount with high\nprobability. Although this denoised problem turns out to be NP-hard, we give a\nlinear-programming based approximation algorithm for it. We evaluate this\napproach on both synthetic and real-world datasets. Our empirical results show\nthat this approach can produce subsets which significantly improve the fairness\nmetrics despite the presence of noisy protected attributes, and, compared to\nprior noise-oblivious approaches, has better Pareto-tradeoffs between utility\nand fairness.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:45:15 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 17:56:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mehrotra", "Anay", ""], ["Celis", "L. Elisa", ""]]}, {"id": "2011.04221", "submitter": "Dishant Goyal", "authors": "Anup Bhattacharya, Dishant Goyal, Ragesh Jaiswal", "title": "Hardness of Approximation of Euclidean $k$-Median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euclidean $k$-median problem is defined in the following manner: given a\nset $\\mathcal{X}$ of $n$ points in $\\mathbb{R}^{d}$, and an integer $k$, find a\nset $C \\subset \\mathbb{R}^{d}$ of $k$ points (called centers) such that the\ncost function $\\Phi(C,\\mathcal{X}) \\equiv \\sum_{x \\in \\mathcal{X}} \\min_{c \\in\nC} \\|x-c\\|_{2}$ is minimized. The Euclidean $k$-means problem is defined\nsimilarly by replacing the distance with squared distance in the cost function.\nVarious hardness of approximation results are known for the Euclidean $k$-means\nproblem. However, no hardness of approximation results were known for the\nEuclidean $k$-median problem. In this work, assuming the unique games\nconjecture (UGC), we provide the first hardness of approximation result for the\nEuclidean $k$-median problem.\n  Furthermore, we study the hardness of approximation for the Euclidean\n$k$-means/$k$-median problems in the bi-criteria setting where an algorithm is\nallowed to choose more than $k$ centers. That is, bi-criteria approximation\nalgorithms are allowed to output $\\beta k$ centers (for constant $\\beta>1$) and\nthe approximation ratio is computed with respect to the optimal\n$k$-means/$k$-median cost. In this setting, we show the first hardness of\napproximation result for the Euclidean $k$-median problem for any $\\beta <\n1.015$, assuming UGC. We also show a similar bi-criteria hardness of\napproximation result for the Euclidean $k$-means problem with a stronger bound\nof $\\beta < 1.28$, again assuming UGC.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:50:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Goyal", "Dishant", ""], ["Jaiswal", "Ragesh", ""]]}, {"id": "2011.04273", "submitter": "Ilan Doron-Arad", "authors": "Ilan Doron-Arad, Ariel Kulik and Hadas Shachnai", "title": "An APTAS for Bin Packing with Clique-graph Conflicts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following variant of the classic {\\em bin packing} problem.\nGiven a set of items of various sizes, partitioned into groups, find a packing\nof the items in a minimum number of identical (unit-size) bins, such that no\ntwo items of the same group are assigned to the same bin. This problem, known\nas {\\em bin packing with clique-graph conflicts}, has natural applications in\nstoring file replicas, security in cloud computing and signal distribution.\n  Our main result is an {\\em asymptotic polynomial time approximation scheme\n(APTAS)} for the problem, improving upon the best known ratio of $2$. %In\nparticular, for any instance $I$ and a fixed $\\eps \\in (0,1)$, the items are\npacked in at most $(1+\\eps)OPT(I) +1$ bins, where $OPT(I)$ is the minimum\nnumber of bins required for packing the instance. As a key tool, we apply a\nnovel {\\em Shift \\& Swap} technique which generalizes the classic linear\nshifting technique to scenarios allowing conflicts between items. The major\nchallenge of packing {\\em small} items using only a small number of extra bins\nis tackled through an intricate combination of enumeration and a greedy-based\napproach that utilizes the rounded solution of a {\\em linear program}.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:46:56 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 00:35:40 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 15:05:10 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2021 18:05:42 GMT"}, {"version": "v5", "created": "Thu, 7 Jan 2021 08:59:56 GMT"}, {"version": "v6", "created": "Sat, 5 Jun 2021 07:58:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Doron-Arad", "Ilan", ""], ["Kulik", "Ariel", ""], ["Shachnai", "Hadas", ""]]}, {"id": "2011.04289", "submitter": "Shichuan Deng", "authors": "Shichuan Deng, Qianfan Zhang", "title": "Ordered $k$-Median with Fault-Tolerance and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fault-tolerant ordered $k$-median and robust ordered $k$-median,\nboth as novel generalizations of the celebrated ordered $k$-median problem. In\nordered $k$-median, given a metric space $(X,d)$, we need to open a set of $k$\nfacilities $S\\subseteq X$, such that if $\\vec{c}:=\\{d(j,S):j\\in X\\}$ is the\nservice cost vector of connection distances, the objective\n$w^T\\vec{c}^{\\downarrow}$ is minimized. Here $w\\in\\mathbb{R}^{|X|}$ is a\npredefined non-increasing non-negative vector, and $\\vec{c}^{\\downarrow}$ is\nthe non-increasingly sorted version of $\\vec{c}$, therefore multiplying larger\ncosts with larger weights. In the fault-tolerant variant of ordered $k$-median,\nevery $j\\in X$ needs to be assigned $r_j\\geq 1$ distinct facilities in $S$. The\nservice cost of $j$ is now the sum of distances to its assigned facilities, and\nthe objective is still $w^T\\vec{c}^{\\downarrow}$. We also study the robust\nvariant of ordered $k$-median, where a parameter $m$ is given and we need to\nconnect the nearest open facility to $m$ points in $X$. In this case, the\nservice cost vector $\\vec{c}\\in\\mathbb{R}^m$ only contains the connection\ndistances for points that are connected, and the ordered objective stays\nunchanged. We give constant-factor polynomial time approximation algorithms for\nboth problems, based on several novel techniques in LP rounding. We also\nconsider ordered knapsack median and ordered matroid median, and use the same\nmethodology to obtain constant-factor approximations as well. We note that none\nof the four problems had constant approximations prior to our study.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:56:48 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 08:37:36 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 05:39:18 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 16:42:01 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Deng", "Shichuan", ""], ["Zhang", "Qianfan", ""]]}, {"id": "2011.04324", "submitter": "Shaofeng Jiang", "authors": "Artur Czumaj, Shaofeng H.-C. Jiang, Robert Krauthgamer, Pavel Vesel\\'y", "title": "Streaming Algorithms for Geometric Steiner Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural generalization of the Steiner tree problem, the Steiner\nforest problem, in the Euclidean plane: the input is a multiset $X \\subseteq\n\\mathbb{R}^2$, partitioned into $k$ color classes $C_1, C_2, \\ldots, C_k\n\\subseteq X$. The goal is to find a minimum-cost Euclidean graph $G$ such that\nevery color class $C_i$ is connected in $G$. We study this Steiner forest\nproblem in the streaming setting, where the stream consists of insertions and\ndeletions of points to $X$. Each input point $x\\in X$ arrives with its color\n$\\mathsf{color}(x) \\in [k]$, and as usual for dynamic geometric streams, the\ninput points are restricted to the discrete grid $\\{0, \\ldots, \\Delta\\}^2$.\n  We design a single-pass streaming algorithm that uses $\\mathrm{poly}(k \\cdot\n\\log\\Delta)$ space and time, and estimates the cost of an optimal Steiner\nforest solution within ratio arbitrarily close to the famous Euclidean Steiner\nratio $\\alpha_2$ (currently $1.1547 \\le \\alpha_2 \\le 1.214$). Our approach\nrelies on a novel combination of streaming techniques, like sampling and linear\nsketching, with the classical dynamic-programming framework for geometric\noptimization problems, which usually requires large memory and has so far not\nbeen applied in the streaming setting.\n  We complement our streaming algorithm for the Steiner forest problem with\nsimple arguments showing that any finite approximation requires $\\Omega(k)$\nbits of space. In addition, our approximation ratio is currently the best even\nfor streaming Steiner tree, i.e., $k=1$.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:46:33 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Czumaj", "Artur", ""], ["Jiang", "Shaofeng H. -C.", ""], ["Krauthgamer", "Robert", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "2011.04483", "submitter": "Steve Hanneke", "authors": "Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, Amir\n  Yehudayoff", "title": "A Theory of Universal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How quickly can a given class of concepts be learned from examples? It is\ncommon to measure the performance of a supervised machine learning algorithm by\nplotting its \"learning curve\", that is, the decay of the error rate as a\nfunction of the number of training examples. However, the classical theoretical\nframework for understanding learnability, the PAC model of Vapnik-Chervonenkis\nand Valiant, does not explain the behavior of learning curves: the\ndistribution-free PAC model of learning can only bound the upper envelope of\nthe learning curves over all possible data distributions. This does not match\nthe practice of machine learning, where the data source is typically fixed in\nany given scenario, while the learner may choose the number of training\nexamples on the basis of factors such as computational resources and desired\naccuracy.\n  In this paper, we study an alternative learning model that better captures\nsuch practical aspects of machine learning, but still gives rise to a complete\ntheory of the learnable in the spirit of the PAC model. More precisely, we\nconsider the problem of universal learning, which aims to understand the\nperformance of learning algorithms on every data distribution, but without\nrequiring uniformity over the distribution. The main result of this paper is a\nremarkable trichotomy: there are only three possible rates of universal\nlearning. More precisely, we show that the learning curves of any given concept\nclass decay either at an exponential, linear, or arbitrarily slow rates.\nMoreover, each of these cases is completely characterized by appropriate\ncombinatorial parameters, and we exhibit optimal learning algorithms that\nachieve the best possible rate in each case.\n  For concreteness, we consider in this paper only the realizable case, though\nanalogous results are expected to extend to more general learning scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:10:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bousquet", "Olivier", ""], ["Hanneke", "Steve", ""], ["Moran", "Shay", ""], ["van Handel", "Ramon", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "2011.04511", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari and Fabian Kuhn", "title": "Deterministic Distributed Vertex Coloring: Simpler, Faster, and without\n  Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple deterministic distributed algorithm that computes a\n$(\\Delta+1)$-vertex coloring in $O(\\log^2 \\Delta \\cdot \\log n)$ rounds, in any\ngraph with at most $n$ nodes and maximum degree at most $\\Delta$. The algorithm\ncan be implemented with $O(\\log n)$-bit messages. It can also be extended to\nthe more general $(degree+1)$-list coloring problem.\n  Obtaining a polylogarithmic-time deterministic algorithm for\n$(\\Delta+1)$-vertex coloring had remained a central open question in the area\nof distributed graph algorithms since the 1980s, until a recent network\ndecomposition algorithm of Rozho\\v{n} and Ghaffari [STOC '20]. The current\nstate of the art is based on an improved variant of their decomposition, which\nleads to an $O(\\log^5 n)$-round algorithm for $(\\Delta+1)$-vertex coloring.\n  Our coloring algorithm is completely different and considerably simpler and\nfaster. It solves the coloring problem in a direct way, without using network\ndecomposition, by gradually rounding a certain fractional color assignment\nuntil reaching an integral color assignment. Moreover, via the approach of\nChang, Li, and Pettie [STOC '18], this improved deterministic algorithm also\nleads to an improvement in the complexity of randomized algorithms for\n$(\\Delta+1)$-coloring, now reaching the bound of $O(\\log^3\\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:42:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "2011.04528", "submitter": "Ziena Elijazyfer Zeif", "authors": "Katrin Casel, Tobias Friedrich, Davis Issac, Aikaterini Niklanovits,\n  Ziena Zeif", "title": "Balanced Crown Decomposition for Connectivity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the balanced crown decomposition that captures the structure\nimposed on graphs by their connected induced subgraphs of a given size. Such\nsubgraphs are a popular modeling tool in various application areas, where the\nnon-local nature of the connectivity condition usually results in very\nchallenging algorithmic tasks. The balanced crown decomposition is a\ncombination of a crown decomposition and a balanced partition which makes it\napplicable to graph editing as well as graph packing and partitioning problems.\nWe illustrate this by deriving improved kernelization and approximation\nalgorithms for a variety of such problems. In particular, through this\nstructure, we obtain the first constant-factor approximation for the Balanced\nConnected Partition (BCP) problem, where the task is to partition a\nvertex-weighted graph into $k$ connected components of approximately equal\nweight. We derive a 3-approximation for the two most commonly used objectives\nof maximizing the weight of the lightest component or minimizing the weight of\nthe heaviest component.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:09:50 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 02:16:24 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 11:25:13 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Casel", "Katrin", ""], ["Friedrich", "Tobias", ""], ["Issac", "Davis", ""], ["Niklanovits", "Aikaterini", ""], ["Zeif", "Ziena", ""]]}, {"id": "2011.04564", "submitter": "Praneeth Kacham", "authors": "Praneeth Kacham and David P. Woodruff", "title": "Reduced-Rank Regression with Operator Norm Error", "comments": "38 pages. To appear at COLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A common data analysis task is the reduced-rank regression problem:\n$$\\min_{\\textrm{rank-}k \\ X} \\|AX-B\\|,$$ where $A \\in \\mathbb{R}^{n \\times c}$\nand $B \\in \\mathbb{R}^{n \\times d}$ are given large matrices and $\\|\\cdot\\|$ is\nsome norm. Here the unknown matrix $X \\in \\mathbb{R}^{c \\times d}$ is\nconstrained to be of rank $k$ as it results in a significant parameter\nreduction of the solution when $c$ and $d$ are large. In the case of Frobenius\nnorm error, there is a standard closed form solution to this problem and a fast\nalgorithm to find a $(1+\\varepsilon)$-approximate solution. However, for the\nimportant case of operator norm error, no closed form solution is known and the\nfastest known algorithms take singular value decomposition time.\n  We give the first randomized algorithms for this problem running in time\n$$(nnz{(A)} + nnz{(B)} + c^2) \\cdot k/\\varepsilon^{1.5} + (n+d)k^2/\\epsilon +\nc^{\\omega},$$ up to a polylogarithmic factor involving condition numbers,\nmatrix dimensions, and dependence on $1/\\varepsilon$. Here $nnz{(M)}$ denotes\nthe number of non-zero entries of a matrix $M$, and $\\omega$ is the exponent of\nmatrix multiplication. As both (1) spectral low rank approximation ($A = B$)\nand (2) linear system solving ($n = c$ and $d = 1$) are special cases, our time\ncannot be improved by more than a $1/\\varepsilon$ factor (up to polylogarithmic\nfactors) without a major breakthrough in linear algebra. Interestingly, known\ntechniques for low rank approximation, such as alternating minimization or\nsketch-and-solve, provably fail for this problem. Instead, our algorithm uses\nan existential characterization of a solution, together with Krylov methods,\nlow degree polynomial approximation, and sketching-based preconditioning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:09:57 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 23:55:23 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kacham", "Praneeth", ""], ["Woodruff", "David P.", ""]]}, {"id": "2011.04593", "submitter": "Andr\\'e Schidler", "authors": "Johannes K. Fichte, Markus Hecher, Andre Schidler", "title": "Solving the Steiner Tree Problem with few Terminals", "comments": "Authors' version of a paper which is to appear in the proceedings of\n  ICTAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Steiner tree problem is a well-known problem in network design, routing,\nand VLSI design. Given a graph, edge costs, and a set of dedicated vertices\n(terminals), the Steiner tree problem asks to output a sub-graph that connects\nall terminals at minimum cost. A state-of-the-art algorithm to solve the\nSteiner tree problem by means of dynamic programming is the Dijkstra-Steiner\nalgorithm. The algorithm builds a Steiner tree of the entire instance by\nsystematically searching for smaller instances, based on subsets of the\nterminals, and combining Steiner trees for these smaller instances. The search\nheavily relies on a guiding heuristic function in order to prune the search\nspace. However, to ensure correctness, this algorithm allows only for limited\nheuristic functions, namely, those that satisfy a so-called consistency\ncondition. In this paper, we enhance the Dijkstra-Steiner algorithm and\nestablish a revisited algorithm, called DS*. The DS* algorithm allows for\narbitrary lower bounds as heuristics relaxing the previous condition on the\nheuristic function. Notably, we can now use linear programming based lower\nbounds. Further, we capture new requirements for a heuristic function in a\ncondition, which we call admissibility. We show that admissibility is indeed\nweaker than consistency and establish correctness of the DS* algorithm when\nusing an admissible heuristic function. We implement DS* and combine it with\nmodern preprocessing, resulting in an open-source solver (DS* Solve). Finally,\nwe compare its performance on standard benchmarks and observe a competitive\nbehavior.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:46:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Schidler", "Andre", ""]]}, {"id": "2011.04915", "submitter": "David Gamarnik", "authors": "David Gamarnik", "title": "Correlation Decay and the Absence of Zeros Property of Partition\n  Functions", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Absence of (complex) zeros property is at the heart of the interpolation\nmethod developed by Barvinok \\cite{barvinok2017combinatorics} for designing\ndeterministic approximation algorithms for various graph counting and computing\npartition functions problems. Earlier methods for solving the same problem\ninclude the one based on the correlation decay property. Remarkably, the\nclasses of graphs for which the two methods apply sometimes coincide or nearly\ncoincide. In this paper we show that this is more than just a coincidence. We\nestablish that if the interpolation method is valid for a family of graphs\nsatisfying the self-reducibility property, then this family exhibits a form of\ncorrelation decay property which is asymptotic Strong Spatial Mixing (SSM) at\ndistances $\\omega(\\log n)$, where $n$ is the number of nodes of the graph. This\napplies in particular to amenable graphs, such as graphs which are finite\nsubsets of lattices.\n  Our proof is based on a certain graph polynomial representation of the\nassociated partition function. This representation is at the heart of the\ndesign of the polynomial time algorithms underlying the interpolation method\nitself. We conjecture that our result holds for all, and not just amenable\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 05:35:15 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 05:47:20 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Gamarnik", "David", ""]]}, {"id": "2011.04928", "submitter": "Jan Konecny", "authors": "Radek Janostik, Jan Konecny, Petr Kraj\\v{c}a", "title": "LinCbO: fast algorithm for computation of the Duquenne-Guigues basis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate a novel algorithm for computation of the\nDuquenne-Guigues basis which combines Close-by-One and LinClosure algorithms.\nThis combination enables us to reuse attribute counters used in LinClosure and\nspeed up the computation. Our experimental evaluation shows that it is the most\nefficient algorithm for computation of the Duquenne-Guigues basis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:18:09 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 07:08:47 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Janostik", "Radek", ""], ["Konecny", "Jan", ""], ["Kraj\u010da", "Petr", ""]]}, {"id": "2011.05066", "submitter": "Yuval Efron", "authors": "Bertie Ancona, Keren Censor-Hillel, Mina Dalirrooyfard, Yuval Efron,\n  Virginia Vassilevska Williams", "title": "Distributed Distance Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diameter, radius and eccentricities are fundamental graph parameters, which\nare extensively studied in various computational settings. Typically, computing\napproximate answers can be much more efficient compared with computing exact\nsolutions. In this paper, we give a near complete characterization of the\ntrade-offs between approximation ratios and round complexity of distributed\nalgorithms for approximating these parameters, with a focus on the weighted and\ndirected variants.\n  Furthermore, we study \\emph{bi-chromatic} variants of these parameters\ndefined on a graph whose vertices are colored either red or blue, and one\nfocuses only on distances for pairs of vertices that are colored differently.\nMotivated by applications in computational geometry, bi-chromatic diameter,\nradius and eccentricities have been recently studied in the sequential setting\n[Backurs et al. STOC'18, Dalirrooyfard et al. ICALP'19]. We provide the first\ndistributed upper and lower bounds for such problems.\n  Our technical contributions include introducing the notion of\n\\emph{approximate pseudo-center}, which extends the \\emph{pseudo-centers} of\n[Choudhary and Gold SODA'20], and presenting an efficient distributed algorithm\nfor computing approximate pseudo-centers. On the lower bound side, our\nconstructions introduce the usage of new functions into the framework of\nreductions from 2-party communication complexity to distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 12:09:00 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 23:44:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ancona", "Bertie", ""], ["Censor-Hillel", "Keren", ""], ["Dalirrooyfard", "Mina", ""], ["Efron", "Yuval", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "2011.05080", "submitter": "Steinar Laenen", "authors": "Steinar Laenen and He Sun", "title": "Higher-Order Spectral Clustering of Directed Graphs", "comments": "24 pages. To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering is an important topic in algorithms, and has a number of\napplications in machine learning, computer vision, statistics, and several\nother research disciplines. Traditional objectives of graph clustering are to\nfind clusters with low conductance. Not only are these objectives just\napplicable for undirected graphs, they are also incapable to take the\nrelationships between clusters into account, which could be crucial for many\napplications. To overcome these downsides, we study directed graphs (digraphs)\nwhose clusters exhibit further \"structural\" information amongst each other.\nBased on the Hermitian matrix representation of digraphs, we present a\nnearly-linear time algorithm for digraph clustering, and further show that our\nproposed algorithm can be implemented in sublinear time under reasonable\nassumptions. The significance of our theoretical work is demonstrated by\nextensive experimental results on the UN Comtrade Dataset: the output\nclustering of our algorithm exhibits not only how the clusters (sets of\ncountries) relate to each other with respect to their import and export\nrecords, but also how these clusters evolve over time, in accordance with known\nfacts in international trade.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 13:06:37 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Laenen", "Steinar", ""], ["Sun", "He", ""]]}, {"id": "2011.05085", "submitter": "Troy Lee", "authors": "Troy Lee and Tongyang Li and Miklos Santha and Shengyu Zhang", "title": "On the cut dimension of a graph", "comments": "40 pages, 4 figures. Updated with a counterexample to a conjecture\n  made in the first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Let $G = (V,w)$ be a weighted undirected graph with $m$ edges. The cut\ndimension of $G$ is the dimension of the span of the characteristic vectors of\nthe minimum cuts of $G$, viewed as vectors in $\\{0,1\\}^m$. For every $n \\ge 2$\nwe show that the cut dimension of an $n$-vertex graph is at most $2n-3$, and\nconstruct graphs realizing this bound.\n  The cut dimension was recently defined by Graur et al.\\ \\cite{GPRW20}, who\nshow that the maximum cut dimension of an $n$-vertex graph is a lower bound on\nthe number of cut queries needed by a deterministic algorithm to solve the\nminimum cut problem on $n$-vertex graphs. For every $n\\ge 2$, Graur et al.\\\nexhibit a graph on $n$ vertices with cut dimension at least $3n/2 -2$, giving\nthe first lower bound larger than $n$ on the deterministic cut query complexity\nof computing mincut. We observe that the cut dimension is even a lower bound on\nthe number of \\emph{linear} queries needed by a deterministic algorithm to\nsolve mincut, where a linear query can ask any vector $x \\in\n\\mathbb{R}^{\\binom{n}{2}}$ and receives the answer $w^T x$. Our results thus\nshow a lower bound of $2n-3$ on the number of linear queries needed by a\ndeterministic algorithm to solve minimum cut on $n$-vertex graphs, and imply\nthat one cannot show a lower bound larger than this via the cut dimension.\n  We further introduce a generalization of the cut dimension which we call the\n$\\ell_1$-approximate cut dimension. The $\\ell_1$-approximate cut dimension is\nalso a lower bound on the number of linear queries needed by a deterministic\nalgorithm to compute minimum cut. It is always at least as large as the cut\ndimension, and we construct an infinite family of graphs on $n=3k+1$ vertices\nwith $\\ell_1$-approximate cut dimension $2n-2$, showing that it can be strictly\nlarger than the cut dimension.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 13:18:32 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 01:51:17 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lee", "Troy", ""], ["Li", "Tongyang", ""], ["Santha", "Miklos", ""], ["Zhang", "Shengyu", ""]]}, {"id": "2011.05181", "submitter": "Lukas N\\\"olke", "authors": "Franziska Eberle, Ruben Hoeksma, Nicole Megow, Lukas N\\\"olke, Kevin\n  Schewior, Bertrand Simon", "title": "Speed-Robust Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speed-robust scheduling problem is a two-stage problem where given $m$\nmachines, jobs must be grouped into at most $m$ bags while the processing\nspeeds of the given $m$ machines are unknown. After the speeds are revealed,\nthe grouped jobs must be assigned to the machines without being separated. To\nevaluate the performance of algorithms, we determine upper bounds on the\nworst-case ratio of the algorithm's makespan and the optimal makespan given\nfull information. We refer to this ratio as the robustness factor. We give an\nalgorithm with a robustness factor $2-1/m$ for the most general setting and\nimprove this to $1.8$ for equal-size jobs. For the special case of\ninfinitesimal jobs, we give an algorithm with an optimal robustness factor\nequal to $e/(e-1) \\approx 1.58$. The particular machine environment in which\nall machines have either speed $0$ or $1$ was studied before by Stein and Zhong\n(SODA 2019). For this setting, we provide an algorithm for scheduling\ninfinitesimal jobs with an optimal robustness factor of $(1+\\sqrt{2})/2 \\approx\n1.207$. It lays the foundation for an algorithm matching the lower bound of\n$4/3$ for equal-size jobs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:32:43 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Eberle", "Franziska", ""], ["Hoeksma", "Ruben", ""], ["Megow", "Nicole", ""], ["N\u00f6lke", "Lukas", ""], ["Schewior", "Kevin", ""], ["Simon", "Bertrand", ""]]}, {"id": "2011.05234", "submitter": "Oren Becker", "authors": "Oren Becker, Alexander Lubotzky, Jonathan Mosheiff", "title": "Testability of relations between permutations", "comments": "42 pages; improved presentation and paper organization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of property testing problems concerning relations\nbetween permutations. In such problems, the input is a tuple\n$(\\sigma_1,\\dotsc,\\sigma_d)$ of permutations on $\\{1,\\dotsc,n\\}$, and one\nwishes to determine whether this tuple satisfies a certain system of relations\n$E$, or is far from every tuple that satisfies $E$. If this computational\nproblem can be solved by querying only a small number of entries of the given\npermutations, we say that $E$ is testable. For example, when $d=2$ and $E$\nconsists of the single relation $\\mathsf{XY=YX}$, this corresponds to testing\nwhether $\\sigma_1\\sigma_2=\\sigma_2\\sigma_1$, where $\\sigma_1\\sigma_2$ and\n$\\sigma_2\\sigma_1$ denote composition of permutations.\n  We define a collection of graphs, naturally associated with the system $E$,\nthat encodes all the information relevant to the testability of $E$. We then\nprove two theorems that provide criteria for testability and non-testability in\nterms of expansion properties of these graphs. By virtue of a deep connection\nwith group theory, both theorems are applicable to wide classes of systems of\nrelations.\n  In addition, we formulate the well-studied group-theoretic notion of\nstability in permutations as a special case of the testability notion above,\ninterpret all previous works on stability as testability results, survey\nprevious results on stability from a computational perspective, and describe\nmany directions for future research on stability and testability.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:47:07 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 02:49:48 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 14:28:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Becker", "Oren", ""], ["Lubotzky", "Alexander", ""], ["Mosheiff", "Jonathan", ""]]}, {"id": "2011.05235", "submitter": "Vera Traub", "authors": "Jannis Blauth, Vera Traub, Jens Vygen", "title": "Improving the Approximation Ratio for Capacitated Vehicle Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a new approximation algorithm for capacitated vehicle routing. Our\nalgorithm yields a better approximation ratio for general capacitated vehicle\nrouting as well as for the unit-demand case and the splittable variant. Our\nresults hold in arbitrary metric spaces. This is the first improvement upon the\nclassical tour partitioning algorithm by Haimovich and Rinnooy Kan and\nAltinkemer and Gavish.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:47:08 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Blauth", "Jannis", ""], ["Traub", "Vera", ""], ["Vygen", "Jens", ""]]}, {"id": "2011.05258", "submitter": "Ilias Zadik", "authors": "Fotis Iliopoulos and Ilias Zadik", "title": "Group testing and local search: is there a computational-statistical\n  gap?", "comments": "Accepted for publication in COLT 2021. Various minor mistakes are\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the fundamental limits of approximate recovery in the\ncontext of group testing. One of the most well-known, theoretically optimal,\nand easy to implement testing procedures is the non-adaptive Bernoulli group\ntesting problem, where all tests are conducted in parallel, and each item is\nchosen to be part of any certain test independently with some fixed\nprobability. In this setting, there is an observed gap between the number of\ntests above which recovery is information theoretically (IT) possible, and the\nnumber of tests required by the currently best known efficient algorithms to\nsucceed. Often times such gaps are explained by a phase transition in the\nlandscape of the solution space of the problem (an Overlap Gap Property phase\ntransition).\n  In this paper we seek to understand whether such a phenomenon takes place for\nBernoulli group testing as well. Our main contributions are the following: (1)\nWe provide first moment evidence that, perhaps surprisingly, such a phase\ntransition does not take place throughout the regime for which recovery is IT\npossible. This fact suggests that the model is in fact amenable to local search\nalgorithms ; (2) we prove the complete absence of \"bad\" local minima for a part\nof the \"hard\" regime, a fact which implies an improvement over known\ntheoretical results on the performance of efficient algorithms for approximate\nrecovery without false-negatives, and finally (3) we present extensive\nsimulations that strongly suggest that a very simple local algorithm known as\nGlauber Dynamics does indeed succeed, and can be used to efficiently implement\nthe well-known (theoretically optimal) Smallest Satisfying Set (SSS) estimator.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:25:48 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 03:15:16 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Iliopoulos", "Fotis", ""], ["Zadik", "Ilias", ""]]}, {"id": "2011.05353", "submitter": "Francesco Bonchi", "authors": "Ioanna Tsalouchidou and Francesco Bonchi and Ricardo Baeza-Yates", "title": "Adaptive Community Search in Dynamic Networks", "comments": "IEEE BigData 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community search is a well-studied problem which, given a static graph and a\nquery set of vertices, requires to find a cohesive (or dense) subgraph\ncontaining the query vertices. In this paper we study the problem of community\nsearch in temporal dynamic networks. We adapt to the temporal setting the\nnotion of \\emph{network inefficiency} which is based on the pairwise\nshortest-path distance among all the vertices in a solution. For this purpose\nwe define the notion of \\emph{shortest-fastest-path distance}: a linear\ncombination of the temporal and spatial dimensions governed by a user-defined\nparameter. We thus define the \\textsc{Minimum Temporal-Inefficiency Subgraph}\nproblem and show that it is \\NPhard. We develop an algorithm which exploits a\ncareful transformation of the temporal network to a static directed and\nweighted graph, and some recent approximation algorithm for finding the minimum\nDirected Steiner Tree. We finally generalize our framework to the streaming\nsetting in which new snapshots of the temporal graph keep arriving continuously\nand our goal is to produce a community search solution for the temporal graph\ncorresponding to a sliding time window.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 19:04:15 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Tsalouchidou", "Ioanna", ""], ["Bonchi", "Francesco", ""], ["Baeza-Yates", "Ricardo", ""]]}, {"id": "2011.05365", "submitter": "Guanghao Ye", "authors": "Sally Dong, Yin Tat Lee and Guanghao Ye", "title": "A Nearly-Linear Time Algorithm for Linear Programs with Small Treewidth:\n  A Multiscale Representation of Robust Central Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arising from structural graph theory, treewidth has become a focus of study\nin fixed-parameter tractable algorithms in various communities including\ncombinatorics, integer-linear programming, and numerical analysis. Many NP-hard\nproblems are known to be solvable in $\\widetilde{O}(n \\cdot\n2^{O(\\mathrm{tw})})$ time, where $\\mathrm{tw}$ is the treewidth of the input\ngraph. Analogously, many problems in P should be solvable in $\\widetilde{O}(n\n\\cdot \\mathrm{tw}^{O(1)})$ time; however, due to the lack of appropriate tools,\nonly a few such results are currently known. [Fom+18] conjectured this to hold\nas broadly as all linear programs; in our paper, we show this is true:\n  Given a linear program of the form $\\min_{Ax=b,\\ell \\leq x\\leq u} c^{\\top}\nx$, and a width-$\\tau$ tree decomposition of a graph $G_A$ related to $A$, we\nshow how to solve it in time $$\\widetilde{O}(n \\cdot \\tau^2 \\log\n(1/\\varepsilon)),$$ where $n$ is the number of variables and $\\varepsilon$ is\nthe relative accuracy. Combined with recent techniques in vertex-capacitated\nflow [BGS21], this leads to an algorithm with $\\widetilde{O}(n \\cdot\n\\mathrm{tw}^2 \\log (1/\\varepsilon))$ run-time. Besides being the first of its\nkind, our algorithm has run-time nearly matching the fastest run-time for\nsolving the sub-problem $Ax=b$ (under the assumption that no fast matrix\nmultiplication is used).\n  We obtain these results by combining recent techniques in interior-point\nmethods (IPMs), sketching, and a novel representation of the solution under a\nmultiscale basis similar to the wavelet basis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 19:35:02 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 08:45:31 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Dong", "Sally", ""], ["Lee", "Yin Tat", ""], ["Ye", "Guanghao", ""]]}, {"id": "2011.05417", "submitter": "Jonathan Leake", "authors": "Jonathan Leake and Colin S. McSwiggen and Nisheeth K. Vishnoi", "title": "Sampling Matrices from Harish-Chandra-Itzykson-Zuber Densities with\n  Applications to Quantum Inference and Differential Privacy", "comments": "Full version of a paper appearing in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC math.PR math.RT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two $n \\times n$ Hermitian matrices $Y$ and $\\Lambda$, the\nHarish-Chandra-Itzykson-Zuber (HCIZ) distribution on the unitary group\n$\\text{U}(n)$ is $e^{\\text{tr}(U\\Lambda U^*Y)}d\\mu(U)$, where $\\mu$ is the Haar\nmeasure on $\\text{U}(n)$. The density $e^{\\text{tr}(U\\Lambda U^*Y)}$ is known\nas the HCIZ density. Random unitary matrices distributed according to the HCIZ\ndensity are important in various settings in physics and random matrix theory.\nHowever, the basic question of efficient sampling from the HCIZ distribution\nhas remained open. We present two efficient algorithms to sample matrices from\ndistributions that are close to the HCIZ distribution. The first algorithm\noutputs samples that are $\\xi$-close in total variation distance and requires\npolynomially many arithmetic operations in $\\log 1/\\xi$ and the number of bits\nneeded to encode $Y$ and $\\Lambda$. The second algorithm comes with a stronger\nguarantee that the samples are $\\xi$-close in infinity divergence, but the\nnumber of arithmetic operations depends polynomially on $1/\\xi$, the number of\nbits needed to encode $Y$ and $\\Lambda$, and the differences of the largest and\nthe smallest eigenvalues of $Y$ and $\\Lambda$.\n  HCIZ densities can also be viewed as exponential densities on\n$\\text{U}(n)$-orbits, and these densities have been studied in statistics,\nmachine learning, and theoretical computer science. Thus our results have the\nfollowing applications: 1) an efficient algorithm to sample from complex\nversions of matrix Langevin distributions studied in statistics, 2) an\nefficient algorithm to sample from continuous max-entropy distributions on\nunitary orbits, which implies an efficient algorithm to sample a pure quantum\nstate from the entropy-maximizing ensemble representing a given density matrix,\nand 3) an efficient algorithm for differentially private rank-$k$\napproximation, with improved utility bounds for $k>1$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 21:55:16 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:47:47 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Leake", "Jonathan", ""], ["McSwiggen", "Colin S.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2011.05425", "submitter": "Sami Boulebnane", "authors": "Sami Boulebnane", "title": "Improving the Quantum Approximate Optimization Algorithm with\n  postselection", "comments": "51 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combinatorial optimization is among the main applications envisioned for\nnear-term and fault-tolerant quantum computers. In this work, we consider a\nwell-studied quantum algorithm for combinatorial optimization: the Quantum\nApproximate Optimization Algorithm (QAOA) applied to the MaxCut problem on\n3-regular graphs. We explore the idea of improving the solutions returned by\nthe simplest version of the algorithm (depth-1 QAOA) using a form of\npostselection that can be efficiently simulated by state preparation. We derive\ntheoretical upper and lower bounds showing that a constant (though small)\nincrease of the fraction of satisfied edges is indeed achievable. Numerical\nexperiments on large problem instances (beyond classical simulatability)\ncomplement and support our bounds. We also consider a distinct technique: local\nupdates, which can be applied not only to QAOA but any optimization algorithm.\nIn the case of QAOA, the resulting improvement can be sharply quantified\ntheoretically for large problem instances and in absence of postselection.\nCombining postselection and local updates, the theory is no longer tractable\nbut numerical evidence suggests that improvements from both methods can be\ncombined.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:17:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Boulebnane", "Sami", ""]]}, {"id": "2011.05467", "submitter": "Fernando Granha Jeronimo", "authors": "Vedat Levi Alev and Fernando Granha Jeronimo and Dylan Quintana and\n  Shashank Srivastava and Madhur Tulsiani", "title": "List Decoding of Direct Sum Codes", "comments": "Full version of paper from SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider families of codes obtained by \"lifting\" a base code $\\mathcal{C}$\nthrough operations such as $k$-XOR applied to \"local views\" of codewords of\n$\\mathcal{C}$, according to a suitable $k$-uniform hypergraph. The $k$-XOR\noperation yields the direct sum encoding used in works of [Ta-Shma, STOC 2017]\nand [Dinur and Kaufman, FOCS 2017].\n  We give a general framework for list decoding such lifted codes, as long as\nthe base code admits a unique decoding algorithm, and the hypergraph used for\nlifting satisfies certain expansion properties. We show that these properties\nare satisfied by the collection of length $k$ walks on an expander graph, and\nby hypergraphs corresponding to high-dimensional expanders. Instantiating our\nframework, we obtain list decoding algorithms for direct sum liftings on the\nabove hypergraph families. Using known connections between direct sum and\ndirect product, we also recover the recent results of Dinur et al. [SODA 2019]\non list decoding for direct product liftings.\n  Our framework relies on relaxations given by the Sum-of-Squares (SOS) SDP\nhierarchy for solving various constraint satisfaction problems (CSPs). We view\nthe problem of recovering the closest codeword to a given word, as finding the\noptimal solution of a CSP. Constraints in the instance correspond to edges of\nthe lifting hypergraph, and the solutions are restricted to lie in the base\ncode $\\mathcal{C}$. We show that recent algorithms for (approximately) solving\nCSPs on certain expanding hypergraphs also yield a decoding algorithm for such\nlifted codes.\n  We extend the framework to list decoding, by requiring the SOS solution to\nminimize a convex proxy for negative entropy. We show that this ensures a\ncovering property for the SOS solution, and the \"condition and round\" approach\nused in several SOS algorithms can then be used to recover the required list of\ncodewords.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 00:03:30 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Alev", "Vedat Levi", ""], ["Jeronimo", "Fernando Granha", ""], ["Quintana", "Dylan", ""], ["Srivastava", "Shashank", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "2011.05500", "submitter": "Fernando Granha Jeronimo", "authors": "Fernando Granha Jeronimo and Dylan Quintana and Shashank Srivastava\n  and Madhur Tulsiani", "title": "Unique Decoding of Explicit $\\epsilon$-balanced Codes Near the\n  Gilbert-Varshamov Bound", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Gilbert-Varshamov bound (non-constructively) establishes the existence of\nbinary codes of distance $1/2 -\\epsilon$ and rate $\\Omega(\\epsilon^2)$ (where\nan upper bound of $O(\\epsilon^2\\log(1/\\epsilon))$ is known). Ta-Shma [STOC\n2017] gave an explicit construction of $\\epsilon$-balanced binary codes, where\nany two distinct codewords are at a distance between $1/2 -\\epsilon/2$ and\n$1/2+\\epsilon/2$, achieving a near optimal rate of\n$\\Omega(\\epsilon^{2+\\beta})$, where $\\beta \\to 0$ as $\\epsilon \\to 0$.\n  We develop unique and list decoding algorithms for (essentially) the family\nof codes constructed by Ta-Shma. We prove the following results for\n$\\epsilon$-balanced codes with block length $N$ and rate\n$\\Omega(\\epsilon^{2+\\beta})$ in this family:\n  - For all $\\epsilon, \\beta > 0$ there are explicit codes which can be\nuniquely decoded up to an error of half the minimum distance in time\n$N^{O_{\\epsilon, \\beta}(1)}$.\n  - For any fixed constant $\\beta$ independent of $\\epsilon$, there is an\nexplicit construction of codes which can be uniquely decoded up to an error of\nhalf the minimum distance in time $(\\log(1/\\epsilon))^{O(1)} \\cdot\nN^{O_\\beta(1)}$.\n  - For any $\\epsilon > 0$, there are explicit $\\epsilon$-balanced codes with\nrate $\\Omega(\\epsilon^{2+\\beta})$ which can be list decoded up to error $1/2 -\n\\epsilon'$ in time $N^{O_{\\epsilon,\\epsilon',\\beta}(1)}$, where $\\epsilon',\n\\beta \\to 0$ as $\\epsilon \\to 0$.\n  The starting point of our algorithms is the list decoding framework from Alev\net al. [SODA 2020], which uses the Sum-of-Squares SDP hierarchy. The rates\nobtained there were quasipolynomial in $\\epsilon$. Here, we show how to\novercome the far from optimal rates of this framework obtaining unique decoding\nalgorithms for explicit binary codes of near optimal rate. These codes are\nbased on simple modifications of Ta-Shma's construction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:28:55 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Jeronimo", "Fernando Granha", ""], ["Quintana", "Dylan", ""], ["Srivastava", "Shashank", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "2011.05502", "submitter": "Michiel Smid", "authors": "Lu\\'is Fernando Schultz Xavier da Silveira, Michiel Smid", "title": "An Instance-Based Algorithm for Deciding the Bias of a Coin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Let $q \\in (0,1)$ and $\\delta \\in (0,1)$ be real numbers, and let $C$ be a\ncoin that comes up heads with an unknown probability $p$, such that $p \\neq q$.\nWe present an algorithm that, on input $C$, $q$, and $\\delta$, decides, with\nprobability at least $1-\\delta$, whether $p<q$ or $p>q$. The expected number of\ncoin flips made by this algorithm is $O \\left( \\frac{\\log\\log(1/\\varepsilon) +\n\\log(1/\\delta)}{\\varepsilon^2} \\right)$, where $\\varepsilon = |p-q|$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:38:36 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["da Silveira", "Lu\u00eds Fernando Schultz Xavier", ""], ["Smid", "Michiel", ""]]}, {"id": "2011.05555", "submitter": "Tselil Schramm", "authors": "Pasin Manurangsi, Aviad Rubinstein, Tselil Schramm", "title": "The Strongish Planted Clique Hypothesis and Its Consequences", "comments": "Appears in ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a new hardness assumption, the Strongish Planted Clique\nHypothesis (SPCH), which postulates that any algorithm for planted clique must\nrun in time $n^{\\Omega(\\log{n})}$ (so that the state-of-the-art running time of\n$n^{O(\\log n)}$ is optimal up to a constant in the exponent).\n  We provide two sets of applications of the new hypothesis. First, we show\nthat SPCH implies (nearly) tight inapproximability results for the following\nwell-studied problems in terms of the parameter $k$: Densest $k$-Subgraph,\nSmallest $k$-Edge Subgraph, Densest $k$-Subhypergraph, Steiner $k$-Forest, and\nDirected Steiner Network with $k$ terminal pairs. For example, we show, under\nSPCH, that no polynomial time algorithm achieves $o(k)$-approximation for\nDensest $k$-Subgraph. This inapproximability ratio improves upon the previous\nbest $k^{o(1)}$ factor from (Chalermsook et al., FOCS 2017). Furthermore, our\nlower bounds hold even against fixed-parameter tractable algorithms with\nparameter $k$.\n  Our second application focuses on the complexity of graph pattern detection.\nFor both induced and non-induced graph pattern detection, we prove hardness\nresults under SPCH, which improves the running time lower bounds obtained by\n(Dalirrooyfard et al., STOC 2019) under the Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:34:00 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Manurangsi", "Pasin", ""], ["Rubinstein", "Aviad", ""], ["Schramm", "Tselil", ""]]}, {"id": "2011.05610", "submitter": "Dominik K\\\"oppl", "authors": "Christina Boucher and Travis Gagie and Tomohiro I and Dominik K\\\"oppl\n  and Ben Langmead and Giovanni Manzini and Gonzalo Navarro and Alejandro\n  Pacheco and Massimiliano Rossi", "title": "PHONI: Streamed Matching Statistics with Multi-Genome References", "comments": "Our code is available at https://github.com/koeppl/phoni", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the matching statistics of patterns with respect to a text is a\nfundamental task in bioinformatics, but a formidable one when the text is a\nhighly compressed genomic database. Bannai et al. gave an efficient solution\nfor this case, which Rossi et al. recently implemented, but it uses two passes\nover the patterns and buffers a pointer for each character during the first\npass. In this paper, we simplify their solution and make it streaming, at the\ncost of slowing it down slightly. This means that, first, we can compute the\nmatching statistics of several long patterns (such as whole human chromosomes)\nin parallel while still using a reasonable amount of RAM; second, we can\ncompute matching statistics online with low latency and thus quickly recognize\nwhen a pattern becomes incompressible relative to the database.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:50:10 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:56:52 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Boucher", "Christina", ""], ["Gagie", "Travis", ""], ["I", "Tomohiro", ""], ["K\u00f6ppl", "Dominik", ""], ["Langmead", "Ben", ""], ["Manzini", "Giovanni", ""], ["Navarro", "Gonzalo", ""], ["Pacheco", "Alejandro", ""], ["Rossi", "Massimiliano", ""]]}, {"id": "2011.05676", "submitter": "Lars Rohwedder", "authors": "Lars Rohwedder and Andreas Wiese", "title": "A $(2+\\varepsilon)$-approximation algorithm for preemptive weighted flow\n  time on a single machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weighted flow time is a fundamental and very well-studied objective function\nin scheduling. In this paper, we study the setting of a single machine with\npreemptions. The input consists of a set of jobs, characterized by their\nprocessing times, release times, and weights and we want to compute a (possibly\npreemptive) schedule for them. The objective is to minimize the sum of the\nweighted flow times of the jobs, where the flow time of a job is the time\nbetween its release date and its completion time.\n  It had been a long-standing open problem to find a polynomial time\n$O(1)$-approximation algorithm for this setting. In a recent break-through\nresult, Batra, Garg, and Kumar (FOCS 2018) found such an algorithm if the input\ndata are polynomially bounded integers, and Feige, Kulkarni, and Li (SODA 2019)\npresented a black-box reduction to this setting. The resulting approximation\nratio is a (not explicitly stated) constant which is at least $10.000$. In this\npaper we improve this ratio to $2+\\varepsilon$. The algorithm by Batra, Garg,\nand Kumar (FOCS 2018) reduces the problem to Demand MultiCut on trees and\nsolves the resulting instances via LP-rounding and a dynamic program. Instead,\nwe first reduce the problem to a (different) geometric problem while losing\nonly a factor $1+\\epsilon$, and then solve its resulting instances up to a\nfactor of $2+\\epsilon$ by a dynamic program. In particular, our reduction\nensures certain structural properties, thanks to which we do not need\nLP-rounding methods.\n  We believe that our result makes substantial progress towards finding a PTAS\nfor weighted flow time on a single machine.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 10:10:25 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Rohwedder", "Lars", ""], ["Wiese", "Andreas", ""]]}, {"id": "2011.05921", "submitter": "Pascal Su", "authors": "Anders Martinsson and Pascal Su", "title": "Mastermind with a Linear Number of Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the 60's Mastermind has been studied for the combinatorial and\ninformation theoretical interest the game has to offer. Many results have been\ndiscovered starting with Erd\\H{o}s and R\\'enyi determining the optimal number\nof queries needed for two colors. For $k$ colors and $n$ positions, Chv\\'atal\nfound asymptotically optimal bounds when $k \\le n^{1-\\epsilon}$. Following a\nsequence of gradual improvements for $k \\geq n$ colors, the central open\nquestion is to resolve the gap between $\\Omega(n)$ and $\\mathcal{O}(n\\log \\log\nn)$ for $k=n$.\n  In this paper, we resolve this gap by presenting the first algorithm for\nsolving $k=n$ Mastermind with a linear number of queries. As a consequence, we\nare able to determine the query complexity of Mastermind for any parameters $k$\nand $n$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:25:21 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Martinsson", "Anders", ""], ["Su", "Pascal", ""]]}, {"id": "2011.05957", "submitter": "Lior Gishboliner", "authors": "Lior Gishboliner, Yevgeny Levanzov, Asaf Shapira, Raphael Yuster", "title": "Counting Homomorphic Cycles in Degenerate Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since computing most variants of the subgraph counting problem in general\ngraphs is conjectured to be hard, it is natural to try and design fast\nalgorithms for restricted families of graphs. One such family that has been\nextensively studied is that of graphs of bounded degeneracy (e.g., planar\ngraphs). This line of work, which started in the early 80's, culminated in a\nrecent work of Gishboliner et al., which highlighted the importance of the task\nof counting homomorphic copies of cycles in graphs of bounded degeneracy.\n  Our main result in this paper is a surprisingly tight relation between the\nabove task and the well-studied problem of detecting (standard) copies of\ndirected cycles in general directed graphs. More precisely, we prove the\nfollowing:\n  1. One can compute the number of homomorphic copies of $C_{2k}$ and\n$C_{2k+1}$ in $n$-vertex graphs of bounded degeneracy in time $O(n^{d_{k}})$,\nwhere $d_k$ is the exponent of the fastest known algorithm for detecting\ndirected copies of $C_k$ in general $m$-edge digraphs.\n  2. Conversely, one can transform any $n^{b_{k}}$ algorithm for computing the\nnumber of homomorphic copies of $C_{2k}$ or of $C_{2k+1}$ in $n$-vertex graphs\nof bounded degeneracy, into an $\\tilde{O}(m^{b_{k}})$ time algorithm for\ndetecting directed copies of $C_k$ in general $m$-edge digraphs.\n  We emphasize that our first result does not use a black-box reduction (as\nopposed to the second result which does). Instead, we design an algorithm for\ncomputing the number of $C_{2k}$-homomorphisms (or $C_{2k+1}$-homomorphisms) in\ndegenerate graphs and show that one part of its analysis can be reduced to the\nanalysis of the fastest known algorithm for detecting directed $k$-cycles in\ngeneral digraphs, which was carried out in a recent breakthrough of\nDalirrooyfard, Vuong and Vassilevska Williams.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:23:45 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:20:58 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 09:09:14 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gishboliner", "Lior", ""], ["Levanzov", "Yevgeny", ""], ["Shapira", "Asaf", ""], ["Yuster", "Raphael", ""]]}, {"id": "2011.06108", "submitter": "Ellis Hershkowitz", "authors": "D Ellis Hershkowitz, Gregory Kehne, R. Ravi", "title": "An Optimal Rounding for Half-Integral Weighted Minimum Strongly\n  Connected Spanning Subgraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the weighted minimum strongly connected spanning subgraph (WMSCSS) problem\nwe must purchase a minimum-cost strongly connected spanning subgraph of a\ndigraph. We show that half-integral linear program (LP) solutions for WMSCSS\ncan be efficiently rounded to integral solutions at a multiplicative $1.5$\ncost. This rounding matches a known $1.5$ integrality gap lower bound for a\nhalf-integral instance. More generally, we show that LP solutions whose\nnon-zero entries are at least a value $f > 0$ can be rounded at a\nmultiplicative cost of $2 - f$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 22:53:18 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Hershkowitz", "D Ellis", ""], ["Kehne", "Gregory", ""], ["Ravi", "R.", ""]]}, {"id": "2011.06112", "submitter": "Ellis Hershkowitz", "authors": "Bernhard Haeupler and D Ellis Hershkowitz and Goran Zuzic", "title": "Tree Embeddings for Hop-Constrained Network Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network design problems aim to compute low-cost structures such as routes,\ntrees and subgraphs. Often, it is natural and desirable to require that these\nstructures have small hop length or hop diameter. Unfortunately, optimization\nproblems with hop constraints are much harder and less well understood than\ntheir hop-unconstrained counterparts. A significant algorithmic barrier in this\nsetting is the fact that hop-constrained distances in graphs are very far from\nbeing a metric.\n  We show that, nonetheless, hop-constrained distances can be approximated by\ndistributions over \"partial tree metrics.\" We build this result into a powerful\nand versatile algorithmic tool which, similarly to classic probabilistic tree\nembeddings, reduces hop-constrained problems in general graphs to\nhop-unconstrained problems on trees. We then use this tool to give the first\npoly-logarithmic bicriteria approximations for the hop-constrained variants of\nmany classic network design problems. These include Steiner forest, group\nSteiner tree, group Steiner forest, buy-at-bulk network design as well as\nonline and oblivious versions of many of these problems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:00:22 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Zuzic", "Goran", ""]]}, {"id": "2011.06135", "submitter": "Min Jae Song", "authors": "Young Kun Ko and Min Jae Song", "title": "Hardness of Approximate Nearest Neighbor Search under L-infinity", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show conditional hardness of Approximate Nearest Neighbor Search (ANN)\nunder the $\\ell_\\infty$ norm with two simple reductions. Our first reduction\nshows that hardness of a special case of the Shortest Vector Problem (SVP),\nwhich captures many provably hard instances of SVP, implies a lower bound for\nANN with polynomial preprocessing time under the same norm. Combined with a\nrecent quantitative hardness result on SVP under $\\ell_\\infty$ (Bennett et al.,\nFOCS 2017), our reduction implies that finding a $(1+\\varepsilon)$-approximate\nnearest neighbor under $\\ell_\\infty$ with polynomial preprocessing requires\nnear-linear query time, unless the Strong Exponential Time Hypothesis (SETH) is\nfalse. This complements the results of Rubinstein (STOC 2018), who showed\nhardness of ANN under $\\ell_1$, $\\ell_2$, and edit distance.\n  Further improving the approximation factor for hardness, we show that,\nassuming SETH, near-linear query time is required for any approximation factor\nless than $3$ under $\\ell_\\infty$. This shows a conditional separation between\nANN under the $\\ell_1/ \\ell_2$ norm and the $\\ell_\\infty$ norm since there are\nsublinear time algorithms achieving better than $3$-approximation for the\n$\\ell_1$ and $\\ell_2$ norm. Lastly, we show that the approximation factor of\n$3$ is a barrier for any naive gadget reduction from the Orthogonal Vectors\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 00:45:16 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ko", "Young Kun", ""], ["Song", "Min Jae", ""]]}, {"id": "2011.06173", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "V\\'it Jel\\'inek, Tereza Klimo\\v{s}ov\\'a, Tom\\'a\\v{s} Masa\\v{r}\\'ik,\n  Jana Novotn\\'a, Aneta Pokorn\\'a", "title": "On 3-Coloring of $(2P_4,C_5)$-Free Graphs", "comments": "18 pages, 13 figures. Accepted to International Workshop on\n  Graph-Theoretic Concepts in Computer Science (WG) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3-coloring of hereditary graph classes has been a deeply-researched\nproblem in the last decade. A hereditary graph class is characterized by a\n(possibly infinite) list of minimal forbidden induced subgraphs\n$H_1,H_2,\\ldots$; the graphs in the class are called $(H_1,H_2,\\ldots)$-free.\nThe complexity of 3-coloring is far from being understood, even for classes\ndefined by a few small forbidden induced subgraphs. For $H$-free graphs, the\ncomplexity is settled for any $H$ on up to seven vertices. There are only two\nunsolved cases on eight vertices, namely $2P_4$ and $P_8$. For $P_8$-free\ngraphs, some partial results are known, but to the best of our knowledge,\n$2P_4$-free graphs have not been explored yet. In this paper, we show that the\n3-coloring problem is polynomial-time solvable on $(2P_4,C_5)$-free graphs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 02:50:00 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 19:56:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Jel\u00ednek", "V\u00edt", ""], ["Klimo\u0161ov\u00e1", "Tereza", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Novotn\u00e1", "Jana", ""], ["Pokorn\u00e1", "Aneta", ""]]}, {"id": "2011.06202", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Christos Tzamos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Ilias\n  Zadik", "title": "Optimal Private Median Estimation under Minimal Distributional\n  Assumptions", "comments": "49 pages, NeurIPS 2020, Spotlight talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the fundamental task of estimating the median of an underlying\ndistribution from a finite number of samples, under pure differential privacy\nconstraints. We focus on distributions satisfying the minimal assumption that\nthey have a positive density at a small neighborhood around the median. In\nparticular, the distribution is allowed to output unbounded values and is not\nrequired to have finite moments. We compute the exact, up-to-constant terms,\nstatistical rate of estimation for the median by providing nearly-tight upper\nand lower bounds. Furthermore, we design a polynomial-time differentially\nprivate algorithm which provably achieves the optimal performance. At a\ntechnical level, our results leverage a Lipschitz Extension Lemma which allows\nus to design and analyze differentially private algorithms solely on\nappropriately defined \"typical\" instances of the samples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 04:54:30 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tzamos", "Christos", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Zadik", "Ilias", ""]]}, {"id": "2011.06204", "submitter": "Peng Zhang", "authors": "Peng Zhang", "title": "Approximating the Weighted Minimum Label $s$-$t$ Cut Problem", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the weighted (minimum) {\\sf Label $s$-$t$ Cut} problem, we are given a\n(directed or undirected) graph $G=(V,E)$, a label set $L = \\{\\ell_1, \\ell_2,\n\\dots, \\ell_q \\}$ with positive label weights $\\{w_\\ell\\}$, a source $s \\in V$\nand a sink $t \\in V$. Each edge edge $e$ of $G$ has a label $\\ell(e)$ from $L$.\nDifferent edges may have the same label. The problem asks to find a minimum\nweight label subset $L'$ such that the removal of all edges with labels in $L'$\ndisconnects $s$ and $t$.\n  The unweighted {\\sf Label $s$-$t$ Cut} problem (i.e., every label has a unit\nweight) can be approximated within $O(n^{2/3})$, where $n$ is the number of\nvertices of graph $G$. However, it is unknown for a long time how to\napproximate the weighted {\\sf Label $s$-$t$ Cut} problem within $o(n)$. In this\npaper, we provide an approximation algorithm for the weighted {\\sf Label\n$s$-$t$ Cut} problem with ratio $O(n^{2/3})$. The key point of the algorithm is\na mechanism to interpret label weight on an edge as both its length and\ncapacity.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:08:12 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhang", "Peng", ""]]}, {"id": "2011.06250", "submitter": "Konstantina Mellou", "authors": "Niv Buchbinder, Yaron Fairstein, Konstantina Mellou, Ishai Menache,\n  Joseph (Seffi) Naor", "title": "Online Virtual Machine Allocation with Predictions", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud computing industry has grown rapidly over the last decade, and with\nthis growth there is a significant increase in demand for compute resources.\nDemand is manifested in the form of Virtual Machine (VM) requests, which need\nto be assigned to physical machines in a way that minimizes resource\nfragmentation and efficiently utilizes the available machines. This problem can\nbe modeled as a dynamic version of the bin packing problem with the objective\nof minimizing the total usage time of the bins (physical machines). Earlier\nworks on dynamic bin packing assumed that no knowledge is available to the\nscheduler and later works studied models in which lifetime/duration of each\n\"item\" (VM in our context) is available to the scheduler. This extra\ninformation was shown to improve exponentially the achievable competitive\nratio.\n  Motivated by advances in Machine Learning that provide good estimates of\nworkload characteristics, this paper studies the effect of having extra\ninformation regarding future (total) demand. In the cloud context, since demand\nis an aggregate over many VM requests, it can be predicted with high accuracy\n(e.g., using historical data). We show that the competitive factor can be\ndramatically improved by using this additional information; in some cases, we\nachieve constant competitiveness, or even a competitive factor that approaches\n1. Along the way, we design new offline algorithms with improved approximation\nratios for the dynamic bin-packing problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 08:09:02 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Buchbinder", "Niv", "", "Seffi"], ["Fairstein", "Yaron", "", "Seffi"], ["Mellou", "Konstantina", "", "Seffi"], ["Menache", "Ishai", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""]]}, {"id": "2011.06268", "submitter": "Justin Ward", "authors": "Chien-Chung Huang, Justin Ward", "title": "FPT-Algorithms for the l-Matchoid Problem with Linear and Submodular\n  Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a fixed-parameter deterministic algorithm for computing a maximum\nweight feasible set under a $\\ell$-matchoid of rank $k$, parameterized by\n$\\ell$ and $k$. Unlike previous work that presumes linear representativity of\nmatroids, we consider the general oracle model. Our result, combined with the\nlower bounds of Lovasz, and Jensen and Korte, demonstrates a separation between\nthe $\\ell$-matchoid and the matroid $\\ell$-parity problems in the setting of\nfixed-parameter tractability.\n  Our algorithms are obtained by means of kernelization: we construct a small\nrepresentative set which contains an optimal solution. Such a set gives us much\nflexibility in adapting to other settings, allowing us to optimize not only a\nlinear function, but also several important submodular functions. It also helps\nto transform our algorithms into streaming algorithms.\n  In the streaming setting, we show that we can find a feasible solution of\nvalue $z$ and the number of elements to be stored in memory depends only on $z$\nand $\\ell$ but totally independent of $n$. This shows that it is possible to\ncircumvent the recent space lower bound of Feldman et al., by parameterizing\nthe solution value. This result, combined with existing lower bounds, also\nprovides a new separation between the space and time complexity of maximizing\nan arbitrary submodular function and a coverage function in the value oracle\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:10:04 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Ward", "Justin", ""]]}, {"id": "2011.06475", "submitter": "Alessandro Luongo", "authors": "Alessandro Luongo, Changpeng Shao", "title": "Quantum algorithms for spectral sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose and analyze new quantum algorithms for estimating the most common\nspectral sums of symmetric positive definite (SPD) matrices. For a function $f$\nand a matrix $A \\in \\mathbb{R}^{n\\times n}$, the spectral sum is defined as\n$S_f(A) :=\\text{Tr}[f(A)] = \\sum_j f(\\lambda_j)$, where $\\lambda_j$ are the\neigenvalues. Examples of spectral sums are the von Neumann entropy, the trace\nof inverse, the log-determinant, and the Schatten-$p$ norm, where the latter\ndoes not require the matrix to be SPD. The fastest classical randomized\nalgorithms estimate these quantities have a runtime that depends at least\nlinearly on the number of nonzero components of the matrix. Assuming quantum\naccess to the matrix, our algorithms are sub-linear in the matrix size, and\ndepend at most quadratically on other quantities, like the condition number and\nthe approximation error, and thus can compete with most of the randomized and\ndistributed classical algorithms proposed in recent literature. These\nalgorithms can be used as subroutines for solving many practical problems, for\nwhich the estimation of a spectral sum often represents a computational\nbottleneck.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:29:45 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Luongo", "Alessandro", ""], ["Shao", "Changpeng", ""]]}, {"id": "2011.06481", "submitter": "Jakab Tardos", "authors": "Michael Kapralov, Gilbert Maystre, Jakab Tardos", "title": "Communication Efficient Coresets for Maximum Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the problem of constructing randomized composable\ncoresets for bipartite matching. In this problem the input graph is randomly\npartitioned across $k$ players, each of which sends a single message to a\ncoordinator, who then must output a good approximation to the maximum matching\nin the input graph. Assadi and Khanna gave the first such coreset, achieving a\n$1/9$-approximation by having every player send a maximum matching, i.e. at\nmost $n/2$ words per player. The approximation factor was improved to $1/3$ by\nBernstein et al.\n  In this paper, we show that the matching skeleton construction of Goel,\nKapralov and Khanna, which is a carefully chosen (fractional) matching, is a\nrandomized composable coreset that achieves a $1/2-o(1)$ approximation using at\nmost $n-1$ words of communication per player. We also show an upper bound of\n$2/3+o(1)$ on the approximation ratio achieved by this coreset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:33:12 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kapralov", "Michael", ""], ["Maystre", "Gilbert", ""], ["Tardos", "Jakab", ""]]}, {"id": "2011.06516", "submitter": "Andr\\'es Cristi", "authors": "Jos\\'e Correa, Andr\\'es Cristi, Boris Epstein, Jos\\'e Soto", "title": "Sample-driven optimal stopping: From the secretary problem to the i.i.d.\n  prophet inequality", "comments": "38 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Two fundamental models in online decision making are that of competitive\nanalysis and that of optimal stopping. In the former the input is produced by\nan adversary, while in the latter the algorithm has full distributional\nknowledge of the input. In recent years, there has been a lot of interest in\nbridging these two models by considering data-driven or sample-based versions\nof optimal stopping problems. In this paper, we study such a version of the\nclassic single selection optimal stopping problem, as introduced by Kaplan et\nal. [2020]. In this problem a collection of arbitrary non-negative numbers is\nshuffled in uniform random order. A decision maker gets to observe a fraction\n$p\\in [0,1)$ of the numbers and the remaining are revealed sequentially. Upon\nseeing a number, she must decide whether to take that number and stop the\nsequence, or to drop it and continue with the next number. Her goal is to\nmaximize the expected value with which she stops.\n  On one end of the spectrum, when $p=0$, the problem is essentially equivalent\nto the secretary problem and thus the optimal algorithm guarantees a reward\nwithin a factor $1/e$ of the expected maximum value. We develop an approach,\nbased on the continuous limit of a factor revealing LP, that allows us to\nobtain the best possible rank-based (ordinal) algorithm for any value of $p$.\nNotably, we prove that as $p$ approaches 1, our guarantee approaches 0.745,\nmatching the best possible guarantee for the i.i.d. prophet inequality. This\nimplies that there is no loss by considering this more general combinatorial\nversion without full distributional knowledge. Furthermore, we prove that this\nconvergence is very fast. Along the way we show that the optimal rank-based\nalgorithm takes the form of a sequence of thresholds $t_1,t_2,\\ldots$ such that\nat time $t_i$ the algorithm starts accepting values which are among the top $i$\nvalues seen so far.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:31:48 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Correa", "Jos\u00e9", ""], ["Cristi", "Andr\u00e9s", ""], ["Epstein", "Boris", ""], ["Soto", "Jos\u00e9", ""]]}, {"id": "2011.06530", "submitter": "Jakab Tardos", "authors": "Michael Kapralov, Robert Krauthgamer, Jakab Tardos, Yuichi Yoshida", "title": "Towards Tight Bounds for Spectral Sparsification of Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cut and spectral sparsification of graphs have numerous applications,\nincluding e.g. speeding up algorithms for cuts and Laplacian solvers. These\npowerful notions have recently been extended to hypergraphs, which are much\nricher and may offer new applications. However, the current bounds on the size\nof hypergraph sparsifiers are not as tight as the corresponding bounds for\ngraphs.\n  Our first result is a polynomial-time algorithm that, given a hypergraph on\n$n$ vertices with maximum hyperedge size $r$, outputs an $\\epsilon$-spectral\nsparsifier with $O^*(nr)$ hyperedges, where $O^*$ suppresses $(\\epsilon^{-1}\n\\log n)^{O(1)}$ factors. This size bound improves the two previous bounds:\n$O^*(n^3)$ [Soma and Yoshida, SODA'19] and $O^*(nr^3)$ [Bansal, Svensson and\nTrevisan, FOCS'19]. Our main technical tool is a new method for proving\nconcentration of the nonlinear analogue of the quadratic form of the Laplacians\nfor hypergraph expanders.\n  We complement this with lower bounds on the bit complexity of any compression\nscheme that $(1+\\epsilon)$-approximates all the cuts in a given hypergraph, and\nhence also on the bit complexity of every $\\epsilon$-cut/spectral sparsifier.\nThese lower bounds are based on Ruzsa-Szemer\\'edi graphs, and a particular\ninstantiation yields an $\\Omega(nr)$ lower bound on the bit complexity even for\nfixed constant $\\epsilon$. This is tight up to polylogarithmic factors in $n$,\ndue to recent hypergraph cut sparsifiers of [Chen, Khanna and Nagda, FOCS'20].\n  Finally, for directed hypergraphs, we present an algorithm that computes an\n$\\epsilon$-spectral sparsifier with $O^*(n^2r^3)$ hyperarcs, where $r$ is the\nmaximum size of a hyperarc. For small $r$, this improves over $O^*(n^3)$ known\nfrom [Soma and Yoshida, SODA'19], and is getting close to the trivial lower\nbound of $\\Omega(n^2)$ hyperarcs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:42:48 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 14:17:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kapralov", "Michael", ""], ["Krauthgamer", "Robert", ""], ["Tardos", "Jakab", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2011.06545", "submitter": "Zihan Tan", "authors": "Julia Chuzhoy, Sepideh Mahabadi, Zihan Tan", "title": "Towards Better Approximation of Graph Crossing Number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Crossing Number is a fundamental problem with various applications. In\nthis problem, the goal is to draw an input graph $G$ in the plane so as to\nminimize the number of crossings between the images of its edges. Despite\nextensive work, non-trivial approximation algorithms are only known for\nbounded-degree graphs. Even for this special case, the best current algorithm\nachieves a $\\tilde O(\\sqrt n)$-approximation, while the best current negative\nresult is APX-hardness. All current approximation algorithms for the problem\nbuild on the same paradigm: compute a set $E'$ of edges (called a\n\\emph{planarizing set}) such that $G\\setminus E'$ is planar; compute a planar\ndrawing of $G\\setminus E'$; then add the drawings of the edges of $E'$ to the\nresulting drawing. Unfortunately, there are examples of graphs, in which any\nimplementation of this method must incur $\\Omega (\\text{OPT}^2)$ crossings,\nwhere $\\text{OPT}$ is the value of the optimal solution. This barrier seems to\ndoom the only known approach to designing approximation algorithms for the\nproblem, and to prevent it from yielding a better than $O(\\sqrt\nn)$-approximation.\n  In this paper we propose a new paradigm that allows us to overcome this\nbarrier. We show an algorithm that, given a bounded-degree graph $G$ and a\nplanarizing set $E'$ of its edges, computes another set $E''$ with $E'\\subseteq\nE''$, such that $|E''|$ is relatively small, and there exists a near-optimal\ndrawing of $G$ in which only edges of $E''$ participate in crossings. This\nallows us to reduce the Crossing Number problem to \\emph{Crossing Number with\nRotation System} -- a variant in which the ordering of the edges incident to\nevery vertex is fixed as part of input. We show a randomized algorithm for this\nnew problem, that allows us to obtain an $O(n^{1/2-\\epsilon})$-approximation\nfor Crossing Number on bounded-degree graphs, for some constant $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:04:55 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 01:50:37 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Mahabadi", "Sepideh", ""], ["Tan", "Zihan", ""]]}, {"id": "2011.06572", "submitter": "Kevin Tian", "authors": "Michael B. Cohen, Aaron Sidford, Kevin Tian", "title": "Relative Lipschitzness in Extragradient Methods and a Direct Recipe for\n  Acceleration", "comments": "32 pages. This is the full version of a paper appearing in ITCS 2021.\n  v2 addresses reviewer comments and adds citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that standard extragradient methods (i.e. mirror prox and dual\nextrapolation) recover optimal accelerated rates for first-order minimization\nof smooth convex functions. To obtain this result we provide a fine-grained\ncharacterization of the convergence rates of extragradient methods for solving\nmonotone variational inequalities in terms of a natural condition we call\nrelative Lipschitzness. We further generalize this framework to handle local\nand randomized notions of relative Lipschitzness and thereby recover rates for\nbox-constrained $\\ell_\\infty$ regression based on area convexity and complexity\nbounds achieved by accelerated (randomized) coordinate descent for smooth\nconvex function minimization.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:43:40 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 03:01:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Cohen", "Michael B.", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "2011.06585", "submitter": "Gleb Novikov", "authors": "Tommaso d'Orsi, Pravesh K. Kothari, Gleb Novikov, David Steurer", "title": "Sparse PCA: Algorithms, Adversarial Perturbations and Certificates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficient algorithms for Sparse PCA in standard statistical models\n(spiked covariance in its Wishart form). Our goal is to achieve optimal\nrecovery guarantees while being resilient to small perturbations. Despite a\nlong history of prior works, including explicit studies of perturbation\nresilience, the best known algorithmic guarantees for Sparse PCA are fragile\nand break down under small adversarial perturbations.\n  We observe a basic connection between perturbation resilience and\n\\emph{certifying algorithms} that are based on certificates of upper bounds on\nsparse eigenvalues of random matrices. In contrast to other techniques, such\ncertifying algorithms, including the brute-force maximum likelihood estimator,\nare automatically robust against small adversarial perturbation.\n  We use this connection to obtain the first polynomial-time algorithms for\nthis problem that are resilient against additive adversarial perturbations by\nobtaining new efficient certificates for upper bounds on sparse eigenvalues of\nrandom matrices. Our algorithms are based either on basic semidefinite\nprogramming or on its low-degree sum-of-squares strengthening depending on the\nparameter regimes. Their guarantees either match or approach the best known\nguarantees of \\emph{fragile} algorithms in terms of sparsity of the unknown\nvector, number of samples and the ambient dimension.\n  To complement our algorithmic results, we prove rigorous lower bounds\nmatching the gap between fragile and robust polynomial-time algorithms in a\nnatural computational model based on low-degree polynomials (closely related to\nthe pseudo-calibration technique for sum-of-squares lower bounds) that is known\nto capture the best known guarantees for related statistical estimation\nproblems. The combination of these results provides formal evidence of an\ninherent price to pay to achieve robustness.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:58:51 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["d'Orsi", "Tommaso", ""], ["Kothari", "Pravesh K.", ""], ["Novikov", "Gleb", ""], ["Steurer", "David", ""]]}, {"id": "2011.06726", "submitter": "Sergei Vassilvitskii", "authors": "Paul D\\\"utting and Silvio Lattanzi and Renato Paes Leme and Sergei\n  Vassilvitskii", "title": "Secretaries with Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The secretary problem is probably the purest model of decision making under\nuncertainty. In this paper we ask which advice can we give the algorithm to\nimprove its success probability?\n  We propose a general model that unifies a broad range of problems: from the\nclassic secretary problem with no advice, to the variant where the quality of a\nsecretary is drawn from a known distribution and the algorithm learns each\ncandidate's quality on arrival, to more modern versions of advice in the form\nof samples, to an ML-inspired model where a classifier gives us noisy signal\nabout whether or not the current secretary is the best on the market.\n  Our main technique is a factor revealing LP that captures all of the problems\nabove. We use this LP formulation to gain structural insight into the optimal\npolicy. Using tools from linear programming, we present a tight analysis of\noptimal algorithms for secretaries with samples, optimal algorithms when\nsecretaries' qualities are drawn from a known distribution, and a new noisy\nbinary advice model.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 02:24:02 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Lattanzi", "Silvio", ""], ["Leme", "Renato Paes", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "2011.06767", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Prithwish Basu and Don Towsley", "title": "Matching through Embedding in Dense Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding optimal matchings in dense graphs is of general interest and of\nparticular importance in social, transportation and biological networks. While\ndeveloping optimal solutions for various matching problems is important, the\nrunning times of the fastest available optimal matching algorithms are too\ncostly. However, when the vertices of the graphs are point-sets in $R^d$ and\nedge weights correspond to the euclidean distances, the available optimal\nmatching algorithms are substantially faster. In this paper, we propose a novel\nnetwork embedding based heuristic algorithm to solve various matching problems\nin dense graphs. In particular, using existing network embedding techniques, we\nfirst find a low dimensional representation of the graph vertices in $R^d$ and\nthen run faster available matching algorithms on the embedded vertices. To the\nbest of our knowledge, this is the first work that applies network embedding to\nsolve various matching problems. Experimental results validate the efficacy of\nour proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 05:26:54 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Basu", "Prithwish", ""], ["Towsley", "Don", ""]]}, {"id": "2011.06888", "submitter": "Hendrik Fichtenberger", "authors": "Hendrik Fichtenberger, Silvio Lattanzi, Ashkan Norouzi-Fard, Ola\n  Svensson", "title": "Consistent k-Clustering for General Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a stream of points in a metric space, is it possible to maintain a\nconstant approximate clustering by changing the cluster centers only a small\nnumber of times during the entire execution of the algorithm? This question\nreceived attention in recent years in the machine learning literature and,\nbefore our work, the best known algorithm performs $\\widetilde{O}(k^2)$ center\nswaps (the $\\widetilde{O}(\\cdot)$ notation hides polylogarithmic factors in the\nnumber of points $n$ and the aspect ratio $\\Delta$ of the input instance). This\nis a quadratic increase compared to the offline case -- the whole stream is\nknown in advance and one is interested in keeping a constant approximation at\nany point in time -- for which $\\widetilde{O}(k)$ swaps are known to be\nsufficient and simple examples show that $\\Omega(k \\log(n \\Delta))$ swaps are\nnecessary. We close this gap by developing an algorithm that, perhaps\nsurprisingly, matches the guarantees in the offline setting. Specifically, we\nshow how to maintain a constant-factor approximation for the $k$-median problem\nby performing an optimal (up to polylogarithimic factors) number\n$\\widetilde{O}(k)$ of center swaps. To obtain our result we leverage new\nstructural properties of $k$-median clustering that may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 12:57:50 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fichtenberger", "Hendrik", ""], ["Lattanzi", "Silvio", ""], ["Norouzi-Fard", "Ashkan", ""], ["Svensson", "Ola", ""]]}, {"id": "2011.06939", "submitter": "Lars Rohwedder", "authors": "Etienne Bamas, Paritosh Garg, Lars Rohwedder", "title": "The Submodular Santa Claus Problem in the Restricted Assignment Case", "comments": "This paper supersedes the work in arXiv:2007.09116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The submodular Santa Claus problem was introduced in a seminal work by\nGoemans, Harvey, Iwata, and Mirrokni (SODA'09) as an application of their\nstructural result. In the mentioned problem $n$ unsplittable resources have to\nbe assigned to $m$ players, each with a monotone submodular utility function\n$f_i$. The goal is to maximize $\\min_i f_i(S_i)$ where $S_1,\\dotsc,S_m$ is a\npartition of the resources. The result by Goemans et al. implies a polynomial\ntime $O(n^{1/2 +\\varepsilon})$-approximation algorithm. Since then progress on\nthis problem was limited to the linear case, that is, all $f_i$ are linear\nfunctions. In particular, a line of research has shown that there is a\npolynomial time constant approximation algorithm for linear valuation functions\nin the restricted assignment case. This is the special case where each player\nis given a set of desired resources $\\Gamma_i$ and the individual valuation\nfunctions are defined as $f_i(S) = f(S \\cap \\Gamma_i)$ for a global linear\nfunction $f$. This can also be interpreted as maximizing $\\min_i f(S_i)$ with\nadditional assignment restrictions, i.e., resources can only be assigned to\ncertain players. In this paper we make comparable progress for the submodular\nvariant. Namely, if $f$ is a monotone submodular function, we can in polynomial\ntime compute an $O(\\log\\log(n))$-approximate solution.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:33:23 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Bamas", "Etienne", ""], ["Garg", "Paritosh", ""], ["Rohwedder", "Lars", ""]]}, {"id": "2011.06959", "submitter": "Edouard Fouch\\'e", "authors": "Edouard Fouch\\'e, Florian Kalinke, Klemens B\\\"ohm", "title": "Efficient Subspace Search in Data Streams", "comments": "Accepted Manuscript to Information Systems, Volume 97, Elsevier.\n  Final authenticated version: https://doi.org/10.1016/j.is.2020.101705", "journal-ref": "In: Information Systems 97 (2021), p. 101705. ISSN: 0306-4379", "doi": "10.1016/j.is.2020.101705", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, data streams are ubiquitous -- think of network traffic or\nsensor data. Mining patterns, e.g., outliers or clusters, from such data must\ntake place in real time. This is challenging because (1) streams often have\nhigh dimensionality, and (2) the data characteristics may change over time.\nExisting approaches tend to focus on only one aspect, either high\ndimensionality or the specifics of the streaming setting. For static data, a\ncommon approach to deal with high dimensionality -- known as subspace search --\nextracts low-dimensional, `interesting' projections (subspaces), in which\npatterns are easier to find. In this paper, we address both Challenge (1) and\n(2) by generalising subspace search to data streams. Our approach, Streaming\nGreedy Maximum Random Deviation (SGMRD), monitors interesting subspaces in\nhigh-dimensional data streams. It leverages novel multivariate dependency\nestimators and monitoring techniques based on bandit theory. We show that the\nbenefits of SGMRD are twofold: (i) It monitors subspaces efficiently, and (ii)\nthis improves the results of downstream data mining tasks, such as outlier\ndetection. Our experiments, performed against synthetic and real-world data,\ndemonstrate that SGMRD outperforms its competitors by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:13:25 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 11:07:50 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Fouch\u00e9", "Edouard", ""], ["Kalinke", "Florian", ""], ["B\u00f6hm", "Klemens", ""]]}, {"id": "2011.06997", "submitter": "Navid Nouri", "authors": "Moses Charikar, Michael Kapralov, Navid Nouri and Paris Siminelakis", "title": "Kernel Density Estimation through Density Constrained Near Neighbor\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the kernel density estimation problem: given a\nkernel $K(x, y)$ and a dataset of $n$ points in high dimensional Euclidean\nspace, prepare a data structure that can quickly output, given a query $q$, a\n$(1+\\epsilon)$-approximation to $\\mu:=\\frac1{|P|}\\sum_{p\\in P} K(p, q)$. First,\nwe give a single data structure based on classical near neighbor search\ntechniques that improves upon or essentially matches the query time and space\ncomplexity for all radial kernels considered in the literature so far. We then\nshow how to improve both the query complexity and runtime by using recent\nadvances in data-dependent near neighbor search.\n  We achieve our results by giving a new implementation of the natural\nimportance sampling scheme. Unlike previous approaches, our algorithm first\nsamples the dataset uniformly (considering a geometric sequence of sampling\nrates), and then uses existing approximate near neighbor search techniques on\nthe resulting smaller dataset to retrieve the sampled points that lie at an\nappropriate distance from the query. We show that the resulting sampled dataset\nhas strong geometric structure, making approximate near neighbor search return\nthe required samples much more efficiently than for worst case datasets of the\nsame size. As an example application, we show that this approach yields a data\nstructure that achieves query time $\\mu^{-(1+o(1))/4}$ and space complexity\n$\\mu^{-(1+o(1))}$ for the Gaussian kernel. Our data dependent approach achieves\nquery time $\\mu^{-0.173-o(1)}$ and space $\\mu^{-(1+o(1))}$ for the Gaussian\nkernel. The data dependent analysis relies on new techniques for tracking the\ngeometric structure of the input datasets in a recursive hashing process that\nwe hope will be of interest in other applications in near neighbor search.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:26:52 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Charikar", "Moses", ""], ["Kapralov", "Michael", ""], ["Nouri", "Navid", ""], ["Siminelakis", "Paris", ""]]}, {"id": "2011.07001", "submitter": "Lukas Gianinazzi", "authors": "Tal Ben-Nun, Lukas Gianinazzi, Torsten Hoefler, Yishai Oltchik", "title": "Parametric Graph Templates: Properties and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical structure and repetition are prevalent in graphs originating\nfrom nature or engineering. These patterns can be represented by a class of\nparametric-structure graphs, which are defined by templates that generate\nstructure by way of repeated instantiation. We propose a class of parametric\ngraph templates that can succinctly represent a wide variety of graphs. Using\nparametric graph templates, we develop structurally-parametric algorithm\nvariants of maximum flow, minimum cut, and tree subgraph isomorphism. Our\nalgorithms are polynomial time for maximum flow and minimum cut and are\nfixed-parameter tractable for tree subgraph isomorphism when parameterized by\nthe size of the tree subgraph. By reasoning about the structure of the\nrepeating subgraphs, we avoid explicit construction of the instantiation.\nFurthermore, we show how parametric graph templates can be recovered from an\ninstantiated graph in quasi-polynomial time when certain parameters of the\ngraph are bounded. Parametric graph templates and the presented algorithmic\ntechniques thus create opportunities for reasoning about the generating\nstructure of a graph, rather than an instance of it.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:31:33 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Gianinazzi", "Lukas", ""], ["Hoefler", "Torsten", ""], ["Oltchik", "Yishai", ""]]}, {"id": "2011.07097", "submitter": "David Harris", "authors": "Nikhil Bansal, David G. Harris", "title": "Some remarks on hypergraph matching and the F\\\"{u}redi-Kahn-Seymour\n  conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic conjecture of F\\\"{u}redi, Kahn and Seymour (1993) states that given\nany hypergraph with non-negative edge weights $w(e)$, there exists a matching\n$M$ such that $\\sum_{e \\in M} (|e|-1+1/|e|)\\, w(e) \\geq w^*$, where $w^*$ is\nthe value of an optimum fractional matching. We show the conjecture is true for\nrank-3 hypergraphs, and is achieved by a natural iterated rounding algorithm.\nWhile the general conjecture remains open, we give several new improved bounds.\nIn particular, we show that the iterated rounding algorithm gives $\\sum_{e \\in\nM} (|e|-\\delta(e))\\, w(e) \\geq w^*$, where $\\delta(e) = |e|/(|e|^2+|e|-1)$,\nimproving upon the baseline guarantee of $\\sum_{e \\in M} |e|\\,w(e) \\geq w^*$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 19:30:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bansal", "Nikhil", ""], ["Harris", "David G.", ""]]}, {"id": "2011.07143", "submitter": "Gabriele Fici", "authors": "Gabriele Fici, Nicola Prezza, Rossano Venturini", "title": "Substring Query Complexity of String Reconstruction", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose an oracle knows a string $S$ that is unknown to us and that we want\nto determine. The oracle can answer queries of the form ``Is $s$ a substring of\n$S$?''. The \\emph{Substring Query Complexity} of a string $S$, denoted\n$\\chi(S)$, is the minimum number of adaptive substring queries that are needed\nto exactly reconstruct (or learn) $S$. It has been introduced in 1995 by Skiena\nand Sundaram, who showed that $\\chi(S) \\geq \\sigma n/4 -O(n)$ in the worst\ncase, where $\\sigma$ is the size of the alphabet of $S$ and $n$ its length, and\ngave an algorithm that spends $(\\sigma-1)n+O(\\sigma \\sqrt{n})$ queries to\nreconstruct $S$.\n  We show that for any binary string $S$, $\\chi(S)$ is asymptotically equal to\nthe Kolmogorov complexity of $S$ and therefore lower bounds any other measure\nof compressibility. However, since this result does not yield an efficient\nalgorithm for the reconstruction, we also present new algorithms which require\na number of substring queries bounded by other known measures of complexity,\ne.g., the number $rle$ of runs in $S$, the size $g$ of the smallest grammar\nproducing (only) $S$, or the size $z_{no}$ of the non-overlapping LZ77\nfactorization of $S$. We first show that any string of length $n$ over an\ninteger alphabet of size $\\sigma$ with $rle$ runs can be reconstructed with\n$q=O(rle (\\sigma + \\log \\frac{n}{rle}))$ substring queries in linear time and\nspace. We then present an algorithm that spends $q \\in O(\\sigma g\\log n)\n\\subseteq O(\\sigma z_{no}\\log (n/z_{no})\\log n)$ substring queries and runs in\n$O(n(\\log n + \\log \\sigma)+ q)$ time using linear space.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:57:18 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 15:15:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Fici", "Gabriele", ""], ["Prezza", "Nicola", ""], ["Venturini", "Rossano", ""]]}, {"id": "2011.07177", "submitter": "Maria Florina Balcan", "authors": "Maria-Florina Balcan", "title": "Data-driven Algorithm Design", "comments": "Chapter 29 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data driven algorithm design is an important aspect of modern data science\nand algorithm design. Rather than using off the shelf algorithms that only have\nworst case performance guarantees, practitioners often optimize over large\nfamilies of parametrized algorithms and tune the parameters of these algorithms\nusing a training set of problem instances from their domain to determine a\nconfiguration with high expected performance over future instances. However,\nmost of this work comes with no performance guarantees. The challenge is that\nfor many combinatorial problems of significant importance including\npartitioning, subset selection, and alignment problems, a small tweak to the\nparameters can cause a cascade of changes in the algorithm's behavior, so the\nalgorithm's performance is a discontinuous function of its parameters.\n  In this chapter, we survey recent work that helps put data-driven\ncombinatorial algorithm design on firm foundations. We provide strong\ncomputational and statistical performance guarantees, both for the batch and\nonline scenarios where a collection of typical problem instances from the given\napplication are presented either all at once or in an online fashion,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 00:51:57 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Balcan", "Maria-Florina", ""]]}, {"id": "2011.07278", "submitter": "Michel Schellekens", "authors": "Michel Schellekens", "title": "Entropy conservation for comparison-based algorithms", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison-based algorithms are algorithms for which the execution of each\noperation is solely based on the outcome of a series of comparisons between\nelements. Comparison-based computations can be naturally represented via the\nfollowing computational model: (a) model data structures as partially-ordered\nfinite sets; (b) model data on these by topological sorts; (c) considering\ncomputation states as finite multisets of such data; (d) represent computations\nby their induced transformations on states. In this view, an abstract\nspecification of a sorting algorithm has input state given by any possible\npermutation of a finite set of elements (represented, according to (a) and (b),\nby a discrete partially-ordered set together with its topological sorts given\nby all permutations) and output state a sorted list of elements (represented,\nagain according to (a) and (b), by a linearly-ordered finite set with its\nunique topological sort).\n  Entropy is a measure of \"randomness\" or \"disorder.\" Based on the\ncomputational model, we introduce an entropy conservation result for\ncomparison-based algorithms: \"quantitative order gained is proportional to\npositional order lost.\" Intuitively, the result bears some relation to the\nmessy office argument advocating a chaotic office where nothing is in the right\nplace yet each item's place is known to the owner, over the case where each\nitem is stored in the right order and yet the owner can no longer locate the\nitems. Formally, we generalize the result to the class of data structures\nrepresentable via series-parallel partial orders--a well-known computationally\ntractable class. The resulting \"denotational\" version of entropy conservation\nwill be extended in follow-up work to an \"operational\" version for a core part\nof our computational model.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 11:56:54 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Schellekens", "Michel", ""]]}, {"id": "2011.07378", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Matias Korman, Oliver Korten, Diane L. Souvaine and\n  Csaba D. T\\'oth", "title": "Reconfiguration of Connected Graph Partitions via Recombination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in gerrymandering detection, we study a\nreconfiguration problem on connected partitions of a connected graph $G$. A\npartition of $V(G)$ is \\emph{connected} if every part induces a connected\nsubgraph. In many applications, it is desirable to obtain parts of roughly the\nsame size, possibly with some slack $s$. A \\emph{Balanced Connected\n$k$-Partition with slack $s$}, denoted \\emph{$(k,s)$-BCP}, is a partition of\n$V(G)$ into $k$ nonempty subsets, of sizes $n_1,\\ldots , n_k$ with\n$|n_i-n/k|\\leq s$, each of which induces a connected subgraph (when $s=0$, the\n$k$ parts are perfectly balanced, and we call it \\emph{$k$-BCP} for short).\n  A \\emph{recombination} is an operation that takes a $(k,s)$-BCP of a graph\n$G$ and produces another by merging two adjacent subgraphs and repartitioning\nthem. Given two $k$-BCPs, $A$ and $B$, of $G$ and a slack $s\\geq 0$, we wish to\ndetermine whether there exists a sequence of recombinations that transform $A$\ninto $B$ via $(k,s)$-BCPs. We obtain four results related to this problem: (1)\nWhen $s$ is unbounded, the transformation is always possible using at most\n$6(k-1)$ recombinations. (2) If $G$ is Hamiltonian, the transformation is\npossible using $O(kn)$ recombinations for any $s \\ge n/k$, and (3) we provide\nnegative instances for $s \\leq n/(3k)$. (4) We show that the problem is\nPSPACE-complete when $k \\in O(n^{\\varepsilon})$ and $s \\in\nO(n^{1-\\varepsilon})$, for any constant $0 < \\varepsilon \\le 1$, even for\nrestricted settings such as when $G$ is an edge-maximal planar graph or when\n$k=3$ and $G$ is planar.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:52:25 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Korman", "Matias", ""], ["Korten", "Oliver", ""], ["Souvaine", "Diane L.", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "2011.07385", "submitter": "Thomas Erlebach", "authors": "Thomas Erlebach and Michael Hoffmann and Murilo S. de Lima and Nicole\n  Megow and Jens Schl\\\"oter", "title": "Untrusted Predictions Improve Trustable Query Policies", "comments": "52 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to utilize (possibly machine-learned) predictions in a model for\noptimization under uncertainty that allows an algorithm to query unknown data.\nThe goal is to minimize the number of queries needed to solve the problem.\nConsidering fundamental problems such as finding the minima of intersecting\nsets of elements or sorting them, as well as the minimum spanning tree problem,\nwe discuss different measures for the prediction accuracy and design algorithms\nwith performance guarantees that improve with the accuracy of predictions and\nthat are robust with respect to very poor prediction quality. We also provide\nnew structural insights for the minimum spanning tree problem that might be\nuseful in the context of explorable uncertainty regardless of predictions. Our\nresults prove that untrusted predictions can circumvent known lower bounds in\nthe model of explorable uncertainty. We complement our results by experiments\nthat empirically confirm the performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 20:50:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Erlebach", "Thomas", ""], ["Hoffmann", "Michael", ""], ["de Lima", "Murilo S.", ""], ["Megow", "Nicole", ""], ["Schl\u00f6ter", "Jens", ""]]}, {"id": "2011.07392", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Leszek G\\k{a}sieniec, Grzegorz Stachowiak, Przemys{\\l}aw Uzna\\'nski", "title": "Time and Space Optimal Exact Majority Population Protocols", "comments": "We combined this paper with arXiv:2012.15800 and have a new updated\n  version at arXiv:2106.10201", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study population protocols governed by the {\\em random\nscheduler}, which uniformly at random selects pairwise interactions between $n$\nagents. The main result of this paper is the first time and space optimal {\\em\nexact majority population protocol} which also works with high probability. The\nnew protocol operates in the optimal {\\em parallel time} $O(\\log n),$ which is\nequivalent to $O(n\\log n)$ sequential {\\em pairwise interactions}, where each\nagent utilises the optimal number of $O(\\log n)$ states.\n  The time optimality of the new majority protocol is possible thanks to the\nnovel concept of fixed-resolution phase clocks introduced and analysed in this\npaper. The new phase clock allows to count approximately constant parallel time\nin population protocols.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:23:47 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 15:03:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["G\u0105sieniec", "Leszek", ""], ["Stachowiak", "Grzegorz", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2011.07471", "submitter": "Samson Zhou", "authors": "David P. Woodruff, Samson Zhou", "title": "Adversarially Robust and Sliding Window Streaming Algorithms without the\n  Overhead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the adversarially robust streaming model, a stream of elements is\npresented to an algorithm and is allowed to depend on the output of the\nalgorithm at earlier times during the stream. In the classic insertion-only\nmodel of data streams, Ben-Eliezer et. al. (PODS 2020, best paper award) show\nhow to convert a non-robust algorithm into a robust one with a roughly\n$1/\\varepsilon$ factor overhead. This was subsequently improved to a\n$1/\\sqrt{\\varepsilon}$ factor overhead by Hassidim et. al. (NeurIPS 2020, oral\npresentation), suppressing logarithmic factors. For general functions the\nlatter is known to be best-possible, by a result of Kaplan et. al. (CRYPTO\n2021). We show how to bypass this impossibility result by developing data\nstream algorithms for a large class of streaming problems, with no overhead in\nthe approximation factor. Our class of streaming problems includes the most\nwell-studied problems such as the $L_2$-heavy hitters problem, $F_p$-moment\nestimation, as well as empirical entropy estimation. We substantially improve\nupon all prior work on these problems, giving the first optimal dependence on\nthe approximation factor.\n  As in previous work, we obtain a general transformation that applies to any\nnon-robust streaming algorithm and depends on the so-called flip number.\nHowever, the key technical innovation is that we apply the transformation to\nwhat we call a difference estimator for the streaming problem, rather than an\nestimator for the streaming problem itself. We then develop the first\ndifference estimators for a wide range of problems. Our difference estimator\nmethodology is not only applicable to the adversarially robust model, but to\nother streaming models where temporal properties of the data play a central\nrole. (Abstract shortened to meet arXiv limit.)\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 07:58:45 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 18:57:35 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 20:22:05 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "2011.07869", "submitter": "Tim Oosterwijk", "authors": "Jos\\'e Correa, Andr\\'es Cristi, Laurent Feuilloley, Tim Oosterwijk and\n  Alexandros Tsigonias-Dimitriadis", "title": "The Secretary Problem with Independent Sampling", "comments": "41 pages, 2 figures, shorter version published in proceedings of\n  SODA21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the secretary problem we are faced with an online sequence of elements\nwith values. Upon seeing an element we have to make an irrevocable\ntake-it-or-leave-it decision. The goal is to maximize the probability of\npicking the element of maximum value. The most classic version of the problem\nis that in which the elements arrive in random order and their values are\narbitrary. However, by varying the available information, new interesting\nproblems arise. Also the case in which the arrival order is adversarial instead\nof random leads to interesting variants that have been considered in the\nliterature.\n  In this paper we study both the random order and adversarial order secretary\nproblems with an additional twist. The values are arbitrary, but before\nstarting the online sequence we independently sample each element with a fixed\nprobability $p$. The sampled elements become our information or history set and\nthe game is played over the remaining elements. We call these problems the\nrandom order secretary problem with $p$-sampling (ROS$p$ for short) and the\nadversarial order secretary problem with $p$-sampling (AOS$p$ for short). Our\nmain result is to obtain best possible algorithms for both problems and all\nvalues of $p$. As $p$ grows to 1 the obtained guarantees converge to the\noptimal guarantees in the full information case. In the adversarial order\nsetting, the best possible algorithm turns out to be a simple fixed threshold\nalgorithm in which the optimal threshold is a function of $p$ only. In the\nrandom order setting we prove that the best possible algorithm is characterized\nby a fixed sequence of time thresholds, dictating at which point in time we\nshould start accepting a value that is both a maximum of the online sequence\nand has a given ranking within the sampled elements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 11:13:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Correa", "Jos\u00e9", ""], ["Cristi", "Andr\u00e9s", ""], ["Feuilloley", "Laurent", ""], ["Oosterwijk", "Tim", ""], ["Tsigonias-Dimitriadis", "Alexandros", ""]]}, {"id": "2011.07963", "submitter": "Marcel Van De Vel", "authors": "Marcel Van de Vel", "title": "Combining the Mersenne Twister and the Xorgens Designs", "comments": "8 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine the design of two \\emph{random number generators}, \\emph{Mersenne\nTwister} and \\emph{Xorgens}, to obtain a new class of generators with\nheavy-weight characteristic polynomials (exceeded only by the {\\sc well}\ngenerators) and high speed (comparable with the originals). Tables with\nparameter combinations are included for state sizes ranging from 521 to 44497\nbits and each of the word lengths 32, 64, 128. These generators passed all\ntests of the \\emph{TestU01}-package for each 32-bit integer part and each\n64-bit derived real part of the output. We determine \\emph{dimension gaps} for\n32-bit words, neglecting the non-linear tempering, and compare with an\nalternative experimental linear tempering.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 17:50:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Van de Vel", "Marcel", ""]]}, {"id": "2011.07999", "submitter": "Diego D\\'iaz-Dom\\'inguez", "authors": "Diego D\\'iaz-Dom\\'inguez and Gonzalo Navarro", "title": "A grammar compressor for collections of reads with applications to the\n  construction of the BWT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a grammar for DNA sequencing reads from which we can compute the\nBWT directly. Our motivation is to perform in succinct space genomic analyses\nthat require complex string queries not yet supported by repetition-based\nself-indexes. Our approach is to store the set of reads as a grammar, but when\nrequired, compute its BWT to carry out the analysis by using self-indexes. Our\nexperiments in real data showed that the space reduction we achieve with our\ncompressor is competitive with LZ-based methods and better than entropy-based\napproaches. Compared to other popular grammars, in this kind of data, we\nachieve, on average, 12\\% of extra compression and require less working space\nand time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 03:16:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["D\u00edaz-Dom\u00ednguez", "Diego", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "2011.08054", "submitter": "Leo Rannou", "authors": "L\\'eo Rannou, Cl\\'emence Magnien, Matthieu Latapy", "title": "Strongly Connected Components in Stream Graphs: Computation and\n  Experimentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream graphs model highly dynamic networks in which nodes and/or links\narrive and/or leave over time. Strongly connected components in stream graphs\nwere defined recently, but no algorithm was provided to compute them. We\npresent here several solutions with polynomial time and space complexities,\neach with its own strengths and weaknesses. We provide an implementation and\nexperimentally compare the algorithms in a wide variety of practical cases. In\naddition, we propose an approximation scheme that significantly reduces\ncomputation costs, and gives even more insight on the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 16:10:30 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 14:35:46 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rannou", "L\u00e9o", ""], ["Magnien", "Cl\u00e9mence", ""], ["Latapy", "Matthieu", ""]]}, {"id": "2011.08083", "submitter": "Pasin Manurangsi", "authors": "Jaros{\\l}aw Byrka, Szymon Dudycz, Pasin Manurangsi, Jan Marcinkowski,\n  Micha{\\l} W{\\l}odarczyk", "title": "To Close Is Easier Than To Open: Dual Parameterization To k-Median", "comments": "Appeared at WAOA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Median problem is one of the well-known optimization problems that\nformalize the task of data clustering. Here, we are given sets of facilities\n$F$ and clients $C$, and the goal is to open $k$ facilities from the set $F$,\nwhich provides the best division into clusters, that is, the sum of distances\nfrom each client to the closest open facility is minimized. In the Capacitated\n$k$-Median, the facilities are also assigned capacities specifying how many\nclients can be served by each facility.\n  Both problems have been extensively studied from the perspective of\napproximation algorithms. Recently, several surprising results have come from\nthe area of parameterized complexity, which provided better approximation\nfactors via algorithms with running times of the form $f(k)\\cdot poly(n)$. In\nthis work, we extend this line of research by studying a different choice of\nparameterization. We consider the parameter $\\ell = |F| - k$, that is, the\nnumber of facilities that remain closed. It turns out that such a\nparameterization reveals yet another behavior of $k$-Median. We observe that\nthe problem is W[1]-hard but it admits a parameterized approximation scheme.\nNamely, we present an algorithm with running time\n$2^{O(\\ell\\log(\\ell/\\epsilon))}\\cdot poly(n)$ that achieves a\n$(1+\\epsilon)$-approximation. On the other hand, we show that under the\nassumption of Gap Exponential Time Hypothesis, one cannot extend this result to\nthe capacitated version of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 16:29:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Dudycz", "Szymon", ""], ["Manurangsi", "Pasin", ""], ["Marcinkowski", "Jan", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "2011.08097", "submitter": "Calvin Beideman", "authors": "Calvin Beideman, Karthekeyan Chandrasekaran, Sagnik Mukhopadhyay,\n  Danupon Nanongkai", "title": "Faster connectivity in low-rank hypergraphs via expander decomposition", "comments": "Section 4.1 is new and contains an algorithm for finding the min-cut\n  in a hypergraph with an imbalanced min-cut. When one side of a min-cut has at\n  most $s$ vertices this algorithm finds a min-cut in time\n  $\\tilde{O}_r(s^{6r}p)$. Using this new algorithm as a subroutine in our\n  min-cut algorithm improves the overall runtime", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design an algorithm for computing connectivity in hypergraphs which runs\nin time $\\hat O_r(p + \\min\\{\\lambda^{\\frac{r-3}{r-1}} n^2,\nn^r/\\lambda^{r/(r-1)}\\})$ (the $\\hat O_r(\\cdot)$ hides the terms subpolynomial\nin the main parameter and terms that depend only on $r$) where $p$ is the size,\n$n$ is the number of vertices, and $r$ is the rank of the hypergraph. Our\nalgorithm is faster than existing algorithms when the the rank is constant and\nthe connectivity $\\lambda$ is $\\omega(1)$. At the heart of our algorithm is a\nstructural result regarding min-cuts in simple hypergraphs. We show a trade-off\nbetween the number of hyperedges taking part in all min-cuts and the size of\nthe smaller side of the min-cut. This structural result can be viewed as a\ngeneralization of a well-known structural theorem for simple graphs\n[Kawarabayashi-Thorup, JACM 19]. We extend the framework of expander\ndecomposition to simple hypergraphs in order to prove this structural result.\nWe also make the proof of the structural result constructive to obtain our\nfaster hypergraph connectivity algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 16:58:13 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 07:38:44 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:50:21 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Beideman", "Calvin", ""], ["Chandrasekaran", "Karthekeyan", ""], ["Mukhopadhyay", "Sagnik", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "2011.08119", "submitter": "Florian Sikora", "authors": "Riccardo Dondi and Florian Sikora", "title": "The Longest Run Subsequence Problem: Further Complexity Results", "comments": "Accepted in CPM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longest Run Subsequence is a problem introduced recently in the context of\nthe scaffolding phase of genome assembly (Schrinner et al., WABI 2020). The\nproblem asks for a maximum length subsequence of a given string that contains\nat most one run for each symbol (a run is a maximum substring of consecutive\nidentical symbols). The problem has been shown to be NP-hard and to be\nfixed-parameter tractable when the parameter is the size of the alphabet on\nwhich the input string is defined. In this paper we further investigate the\ncomplexity of the problem and we show that it is fixed-parameter tractable when\nit is parameterized by the number of runs in a solution, a smaller parameter.\nMoreover, we investigate the kernelization complexity of Longest Run\nSubsequence and we prove that it does not admit a polynomial kernel when\nparameterized by the size of the alphabet or by the number of runs. Finally, we\nconsider the restriction of Longest Run Subsequence when each symbol has at\nmost two occurrences in the input string and we show that it is APX-hard.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:31:51 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 10:10:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Dondi", "Riccardo", ""], ["Sikora", "Florian", ""]]}, {"id": "2011.08384", "submitter": "Jasper C.H. Lee", "authors": "Jasper C.H. Lee, Paul Valiant", "title": "Optimal Sub-Gaussian Mean Estimation in $\\mathbb{R}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of estimating the mean of a real-valued distribution,\npresenting a novel estimator with sub-Gaussian convergence: intuitively, \"our\nestimator, on any distribution, is as accurate as the sample mean is for the\nGaussian distribution of matching variance.\" Crucially, in contrast to prior\nworks, our estimator does not require prior knowledge of the variance, and\nworks across the entire gamut of distributions with bounded variance, including\nthose without any higher moments. Parameterized by the sample size $n$, the\nfailure probability $\\delta$, and the variance $\\sigma^2$, our estimator is\naccurate to within $\\sigma\\cdot(1+o(1))\\sqrt{\\frac{2\\log\\frac{1}{\\delta}}{n}}$,\ntight up to the $1+o(1)$ factor. Our estimator construction and analysis gives\na framework generalizable to other problems, tightly analyzing a sum of\ndependent random variables by viewing the sum implicitly as a 2-parameter\n$\\psi$-estimator, and constructing bounds using mathematical programming and\nduality techniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:47:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lee", "Jasper C. H.", ""], ["Valiant", "Paul", ""]]}, {"id": "2011.08447", "submitter": "Yash Khanna", "authors": "Yash Khanna", "title": "Exact recovery of Planted Cliques in Semi-random graphs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Planted Clique problem in a semi-random model.\nOur model is inspired from the Feige-Kilian model [FK01] which has been studied\nin many other works [FK00, Ste17, MMT20] for a variety of graph problems. Our\nalgorithm and analysis is on similar lines to the one studied for the Densest\n$k$-subgraph problem in the recent work of Khanna and Louis [KL20]. However\nsince our algorithm fully recovers the planted clique w.h.p. (for a \"large\"\nrange of input parameters), we require some new ideas.\n  As a by-product of our main result, we give an alternate SDP based rounding\nalgorithm (with matching guarantees) for solving the Planted Clique problem in\na random graph. Also, we are able to solve special cases of the models\nintroduced for the Densest $k$-subgraph problem in [KL20], when the planted\nsubgraph is a clique instead of an arbitrary $d$-regular graph.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:00:25 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 13:13:06 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 13:25:35 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 05:15:59 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Khanna", "Yash", ""]]}, {"id": "2011.08448", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Optimal diameter computation within bounded clique-width graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coudert et al. (SODA'18) proved that under the Strong Exponential-Time\nHypothesis, for any $\\epsilon >0$, there is no ${\\cal\nO}(2^{o(k)}n^{2-\\epsilon})$-time algorithm for computing the diameter within\nthe $n$-vertex cubic graphs of clique-width at most $k$. We present an\nalgorithm which given an $n$-vertex $m$-edge graph $G$ and a $k$-expression,\ncomputes all the eccentricities in ${\\cal O}(2^{{\\cal O}(k)}(n+m)^{1+o(1)})$\ntime, thus matching their conditional lower bound. It can be modified in order\nto compute the Wiener index and the median set of $G$ within the same amount of\ntime. On our way, we get a distance-labeling scheme for $n$-vertex $m$-edge\ngraphs of clique-width at most $k$, using ${\\cal O}(k\\log^2{n})$ bits per\nvertex and constructible in ${\\cal O}(k(n+m)\\log{n})$ time from a given\n$k$-expression. Doing so, we match the label size obtained by Courcelle and\nVanicat (DAM 2016), while we considerably improve the dependency on $k$ in\ntheir scheme. As a corollary, we get an ${\\cal O}(kn^2\\log{n})$-time algorithm\nfor computing All-Pairs Shortest-Paths on $n$-vertex graphs of clique-width at\nmost $k$. This partially answers an open question of Kratsch and Nelles\n(STACS'20).\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:01:13 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "2011.08557", "submitter": "Christopher Hojny", "authors": "Daniel Dadush, Christopher Hojny, Sophie Huiberts, Stefan Weltge", "title": "Simple Iterative Methods for Linear Optimization over Convex Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give simple iterative methods for computing approximately optimal primal\nand dual solutions for the problem of maximizing a linear functional over a\nconvex set $K$ given by a separation oracle. In contrast to prior work, our\nalgorithms directly output primal and dual solutions and avoid a common\nrequirement of binary search on the objective value.\n  Under the assumption that $K$ contains a ball of radius $r$ and is contained\ninside the origin centered ball of radius $R$, using $O(\\frac{R^4}{r^2\n\\varepsilon^2})$ iterations and calls to the oracle, our main method outputs a\npoint $x \\in K$ together with a non-negative combination of the inequalities\noutputted by the oracle and one inequality of the $R$-ball certifying that $x$\nis an additive $\\varepsilon$-approximate solution. In the setting where the\ninner $r$-ball is centered at the origin, we give a simplified variant which\noutputs a multiplicative $(1 + \\varepsilon)$-approximate primal and dual\nsolutions using $O(\\frac{R^2}{r^2 \\varepsilon^2})$ iterations and calls to the\noracle. Similar results hold for packing type problems. Our methods are based\non variants of the classical Von Neumann and Frank-Wolfe algorithms.\n  Our algorithms are also easy to implement, and we provide an experimental\nevaluation of their performance on a testbed of maximum matching and stable set\ninstances. We further compare variations of our method to the standard cut loop\nimplemented using Gurobi.This comparison reveals that in terms of iteration\ncount, our methods converge on average faster than the standard cut loop on our\ntest set.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:41:34 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Dadush", "Daniel", ""], ["Hojny", "Christopher", ""], ["Huiberts", "Sophie", ""], ["Weltge", "Stefan", ""]]}, {"id": "2011.08806", "submitter": "Arun Jambulapati", "authors": "Arun Jambulapati, Aaron Sidford", "title": "Ultrasparse Ultrasparsifiers and Faster Laplacian System Solvers", "comments": "52 pages, comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an $O(m (\\log \\log n)^{O(1)}\n\\log(1/\\epsilon))$-expected time algorithm for solving Laplacian systems on\n$n$-node $m$-edge graphs, improving improving upon the previous best expected\nruntime of $O(m \\sqrt{\\log n} (\\log \\log n)^{O(1)} \\log(1/\\epsilon))$ achieved\nby (Cohen, Kyng, Miller, Pachocki, Peng, Rao, Xu 2014). To obtain this result\nwe provide efficient constructions of $\\ell_p$-stretch graph approximations\nwith improved stretch and sparsity bounds. Additionally, as motivation for this\nwork, we show that for every set of vectors in $\\mathbb{R}^d$ (not just those\ninduced by graphs) and all $k > 0$ there exist an ultra-sparsifiers with $d-1 +\nO(d/k)$ re-weighted vectors of relative condition number at most $k^2$. For\nsmall $k$, this improves upon the previous best known multiplicative factor of\n$k \\cdot \\tilde{O}(\\log d)$, which is only known for the graph case.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:08:46 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jambulapati", "Arun", ""], ["Sidford", "Aaron", ""]]}, {"id": "2011.08944", "submitter": "Dror Dayan", "authors": "Dror Dayan, Kiril Solovey, Marco Pavone, Dan Halperin", "title": "Near-Optimal Multi-Robot Motion Planning with Finite Sampling", "comments": "To appear in the International Conference on Robotics and Automation\n  (ICRA), 2021. This is an extended version that includes full proofs and\n  additional details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An underlying structure in several sampling-based methods for continuous\nmulti-robot motion planning (MRMP) is the tensor roadmap (TR), which emerges\nfrom combining multiple PRM graphs constructed for the individual robots via a\ntensor product. We study the conditions under which the TR encodes a\nnear-optimal solution for MRMP -- satisfying these conditions implies near\noptimality for a variety of popular planners, including dRRT*, and the discrete\nmethods M* and CBS when applied to the continuous domain. We develop the first\nfinite-sample analysis of this kind, which specifies the number of samples,\ntheir deterministic distribution, and magnitude of the connection radii that\nshould be used by each individual PRM graph, to guarantee near-optimality using\nthe TR. This significantly improves upon a previous asymptotic analysis,\nwherein the number of samples tends to infinity. Our new finite sample-size\nanalysis supports guaranteed high-quality solutions in practice within finite\ntime. To achieve our new result, we first develop a sampling scheme, which we\ncall the staggered grid, for finite-sample motion planning for individual\nrobots, which requires significantly less samples than previous work. We then\nextend it to the much more involved MRMP setting which requires to account for\ninteractions among multiple robots. Finally, we report on a few experiments\nthat serve as a verification of our theoretical findings and raise interesting\nquestions for further investigation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 21:03:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 19:51:56 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 16:15:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dayan", "Dror", ""], ["Solovey", "Kiril", ""], ["Pavone", "Marco", ""], ["Halperin", "Dan", ""]]}, {"id": "2011.09076", "submitter": "Manish Purohit", "authors": "Nikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, Erik Vee", "title": "Scale-Free Allocation, Amortized Convexity, and Myopic Weighted Paging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by Belady's optimal algorithm for unweighted paging, we consider a\nnatural myopic model for weighted paging in which an algorithm has access to\nthe relative ordering of all pages with respect to the time of their next\narrival. We provide an $\\ell$-competitive deterministic and an $O(\\log\n\\ell)$-competitive randomized algorithm, where $\\ell$ is the number of distinct\nweight classes. Both these bounds are tight and imply an $O(\\log W)$ and\n$O(\\log \\log W)$-competitive ratio respectively when the page weights lie\nbetween $1$ and $W$. Our model and results also simultaneously generalize the\npaging with predictions (Lykouris and Vassilvitskii, 2018) and the interleaved\npaging (Barve et al., 2000; Cao et al., 1994; Kumar et al., 2019) settings to\nthe weighted case.\n  Perhaps more interestingly, our techniques shed light on a family of\ninterconnected geometric allocation problems. We show that the competitiveness\nof myopic weighted paging lies in between that of two similar problems called\nsoft and strict allocation. While variants of the allocation problem have been\nstudied previously in the context of the $k$-server problem, our formulation\nexhibits a certain scale-free property that poses significant technical\nchallenges. Consequently, simple multiplicative weight update algorithms do not\nseem to work here; instead, we are forced to use a novel technique that\ndecouples the rate of change of the allocation variables from their value. For\noffline unweighted paging, we also provide an alternate proof of the optimality\nof Belady's classic Farthest in Future algorithm, which may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 04:02:50 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bansal", "Nikhil", ""], ["Coester", "Christian", ""], ["Kumar", "Ravi", ""], ["Purohit", "Manish", ""], ["Vee", "Erik", ""]]}, {"id": "2011.09375", "submitter": "Markus Anders", "authors": "Markus Anders and Pascal Schweitzer", "title": "Engineering a Fast Probabilistic Isomorphism Test", "comments": "To appear at ALENEX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer a new probabilistic Monte-Carlo algorithm for isomorphism\ntesting. Most notably, as opposed to all other solvers, it implicitly exploits\nthe presence of symmetries without explicitly computing them.\n  We provide extensive benchmarks, showing that the algorithm outperforms all\nstate-of-the-art solutions for isomorphism testing on most inputs from the de\nfacto standard benchmark library for isomorphism testing. On many input types,\nour data not only show improved running times by an order of magnitude, but\nalso reflect a better asymptotic behavior.\n  Our results demonstrate that, with current algorithms, isomorphism testing is\nin practice easier than the related problems of computing the automorphism\ngroup or canonically labeling a graph. The results also show that probabilistic\nalgorithms for isomorphism testing can be engineered to outperform\ndeterministic approaches, even asymptotically.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:18:53 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Anders", "Markus", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "2011.09384", "submitter": "Dan Feldman PhD", "authors": "Dan Feldman", "title": "Introduction to Core-sets: an Updated Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In optimization or machine learning problems we are given a set of items,\nusually points in some metric space, and the goal is to minimize or maximize an\nobjective function over some space of candidate solutions. For example, in\nclustering problems, the input is a set of points in some metric space, and a\ncommon goal is to compute a set of centers in some other space (points, lines)\nthat will minimize the sum of distances to these points. In database queries,\nwe may need to compute such a some for a specific query set of $k$ centers.\n  However, traditional algorithms cannot handle modern systems that require\nparallel real-time computations of infinite distributed streams from sensors\nsuch as GPS, audio or video that arrive to a cloud, or networks of weaker\ndevices such as smartphones or robots.\n  Core-set is a \"small data\" summarization of the input \"big data\", where every\npossible query has approximately the same answer on both data sets. Generic\ntechniques enable efficient coreset \\changed{maintenance} of streaming,\ndistributed and dynamic data. Traditional algorithms can then be applied on\nthese coresets to maintain the approximated optimal solutions.\n  The challenge is to design coresets with provable tradeoff between their size\nand approximation error. This survey summarizes such constructions in a\nretrospective way, that aims to unified and simplify the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:31:34 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Feldman", "Dan", ""]]}, {"id": "2011.09406", "submitter": "Kira Goldner", "authors": "Shuchi Chawla, Kira Goldner, Anna R. Karlin, J. Benjamin Miller", "title": "Non-Adaptive Matroid Prophet Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate non-adaptive algorithms for matroid prophet inequalities.\nMatroid prophet inequalities have been considered resolved since 2012 when\n[KW12] introduced thresholds that guarantee a tight 2-approximation to the\nprophet; however, this algorithm is adaptive. Other approaches of [CHMS10] and\n[FSZ16] have used non-adaptive thresholds with a feasibility restriction;\nhowever, this translates to adaptively changing an item's threshold to infinity\nwhen it cannot be taken with respect to the additional feasibility constraint,\nhence the algorithm is not truly non-adaptive. A major application of prophet\ninequalities is in auction design, where non-adaptive prices possess a\nsignificant advantage: they convert to order-oblivious posted pricings, and are\nessential for translating a prophet inequality into a truthful mechanism for\nmulti-dimensional buyers. The existing matroid prophet inequalities do not\nsuffice for this application. We present the first non-adaptive constant-factor\nprophet inequality for graphic matroids.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:07:47 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Chawla", "Shuchi", ""], ["Goldner", "Kira", ""], ["Karlin", "Anna R.", ""], ["Miller", "J. Benjamin", ""]]}, {"id": "2011.09416", "submitter": "Bohdan Kivva", "authors": "Bohdan Kivva and Aaron Potechin", "title": "Exact nuclear norm, completion and decomposition for random overcomplete\n  tensors via degree-4 SOS", "comments": "132 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that simple semidefinite programs inspired by degree\n$4$ SOS can exactly solve the tensor nuclear norm, tensor decomposition, and\ntensor completion problems on tensors with random asymmetric components. More\nprecisely, for tensor nuclear norm and tensor decomposition, we show that\nw.h.p. these semidefinite programs can exactly find the nuclear norm and\ncomponents of an $(n\\times n\\times n)$-tensor $\\mathcal{T}$ with $m\\leq\nn^{3/2}/polylog(n)$ random asymmetric components. For tensor completion, we\nshow that w.h.p. the semidefinite program introduced by Potechin \\& Steurer\n(2017) can exactly recover an $(n\\times n\\times n)$-tensor $\\mathcal{T}$ with\n$m$ random asymmetric components from only $n^{3/2}m\\, polylog(n)$ randomly\nobserved entries. This gives the first theoretical guarantees for exact tensor\ncompletion in the overcomplete regime.\n  This matches the best known results for approximate versions of these\nproblems given by Barak \\& Moitra (2015) for tensor completion, and Ma, Shi \\&\nSteurer (2016) for tensor decomposition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:27:36 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kivva", "Bohdan", ""], ["Potechin", "Aaron", ""]]}, {"id": "2011.09439", "submitter": "Yangguang Shi", "authors": "Yuval Emek, Shay Kutten, Yangguang Shi", "title": "Online Paging with a Vanishing Regret", "comments": "25 pages. An extended abstract of this paper is to appear in the 12th\n  Innovations in Theoretical Computer Science conference (ITCS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a variant of the online paging problem, where the online\nalgorithm has access to multiple predictors, each producing a sequence of\npredictions for the page arrival times. The predictors may have occasional\nprediction errors and it is assumed that at least one of them makes a sublinear\nnumber of prediction errors in total. Our main result states that this\nassumption suffices for the design of a randomized online algorithm whose\ntime-average regret with respect to the optimal offline algorithm tends to zero\nas the time tends to infinity. This holds (with different regret bounds) for\nboth the full information access model, where in each round, the online\nalgorithm gets the predictions of all predictors, and the bandit access model,\nwhere in each round, the online algorithm queries a single predictor. While\nonline algorithms that exploit inaccurate predictions have been a topic of\ngrowing interest in the last few years, to the best of our knowledge, this is\nthe first paper that studies this topic in the context of multiple predictors\nfor an online problem with unbounded request sequences. Moreover, to the best\nof our knowledge, this is also the first paper that aims for (and achieves)\nonline algorithms with a vanishing regret for a classic online problem under\nreasonable assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:17:49 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 02:18:08 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Emek", "Yuval", ""], ["Kutten", "Shay", ""], ["Shi", "Yangguang", ""]]}, {"id": "2011.09504", "submitter": "Amariah Becker", "authors": "Amariah Becker and Justin Solomon", "title": "Redistricting Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why not have a computer just draw a map? This is something you hear a lot\nwhen people talk about gerrymandering, and it's easy to think at first that\nthis could solve redistricting altogether. But there are more than a couple\nproblems with this idea. In this chapter, two computer scientists survey what's\nbeen done in algorithmic redistricting, discuss what doesn't work and highlight\napproaches that show promise. This preprint was prepared as a chapter in the\nforthcoming edited volume Political Geometry, an interdisciplinary collection\nof essays on redistricting. (https://mggg.org/gerrybook)\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 19:19:20 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Becker", "Amariah", ""], ["Solomon", "Justin", ""]]}, {"id": "2011.09591", "submitter": "Yiming Zhao", "authors": "Haitao Wang and Yiming Zhao", "title": "Algorithms for Diameters of Unicycle Graphs and Diameter-Optimally\n  Augmenting Trees", "comments": "A preliminary version will appear in WALCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of computing the diameter of a unicycle graph (i.e.,\na graph with a unique cycle). We present an O(n) time algorithm for the\nproblem, where n is the number of vertices of the graph. This improves the\nprevious best O(n \\log n) time solution [Oh and Ahn, ISAAC 2016]. Using this\nalgorithm as a subroutine, we solve the problem of adding a shortcut to a tree\nso that the diameter of the new graph (which is a unicycle graph) is minimized;\nour algorithm takes O(n^2 \\log n) time and O(n) space. The previous best\nalgorithms solve the problem in O(n^2 \\log^3 n) time and O(n) space [Oh and\nAhn, ISAAC 2016], or in O(n^2) time and O(n^2) space [Bil\\`o, ISAAC 2018].\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 00:07:43 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wang", "Haitao", ""], ["Zhao", "Yiming", ""]]}, {"id": "2011.09761", "submitter": "Wojciech Janczewski", "authors": "Pawe{\\l} Gawrychowski, Wojciech Janczewski", "title": "Fully Dynamic Approximation of LIS in Polylogarithmic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of maintaining the longest increasing subsequence\n(LIS) of an array under (i) inserting an element, and (ii) deleting an element\nof an array. In a recent breakthrough, Mitzenmacher and Seddighin [STOC 2020]\ndesigned an algorithm that maintains an\n$\\mathcal{O}((1/\\epsilon)^{\\mathcal{O}(1/\\epsilon)})$-approximation of LIS\nunder both operations with worst-case update time $\\mathcal{\\tilde\nO}(n^{\\epsilon})$, for any constant $\\epsilon>0$. We exponentially improve on\ntheir result by designing an algorithm that maintains an\n$(1+\\epsilon)$-approximation of LIS under both operations with worst-case\nupdate time $\\mathcal{\\tilde O}(\\epsilon^{-5})$. Instead of working with the\ngrid packing technique introduced by Mitzenmacher and Seddighin, we take a\ndifferent approach building on a new tool that might be of independent\ninterest: LIS sparsification.\n  A particularly interesting consequence of our result is an improved solution\nfor the so-called Erd\\H{o}s-Szekeres partitioning, in which we seek a partition\nof a given permutation of $\\{1,2,\\ldots,n\\}$ into $\\mathcal{O}(\\sqrt{n})$\nmonotone subsequences. This problem has been repeatedly stated as one of the\nnatural examples in which we see a large gap between the decision-tree\ncomplexity and algorithmic complexity. The result of Mitzenmacher and Seddighin\nimplies an $\\mathcal{O}(n^{1+\\epsilon})$ time solution for this problem, for\nany $\\epsilon>0$. Our algorithm (in fact, its simpler decremental version)\nfurther improves this to $\\mathcal{\\tilde O}(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:39:58 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 12:13:19 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Janczewski", "Wojciech", ""]]}, {"id": "2011.09823", "submitter": "Troy Lee", "authors": "Simon Apers and Troy Lee", "title": "Quantum complexity of minimum cut", "comments": "15 pages; v2: improved bounds on query and time complexity; v3: fixes\n  typos, accepted to CCC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The minimum cut problem in an undirected and weighted graph $G$ is to find\nthe minimum total weight of a set of edges whose removal disconnects $G$. We\ncompletely characterize the quantum query and time complexity of the minimum\ncut problem in the adjacency matrix model. If $G$ has $n$ vertices and edge\nweights at least $1$ and at most $\\tau$, we give a quantum algorithm to solve\nthe minimum cut problem using $\\tilde O(n^{3/2}\\sqrt{\\tau})$ queries and time.\nMoreover, for every integer $1 \\le \\tau \\le n$ we give an example of a graph\n$G$ with edge weights $1$ and $\\tau$ such that solving the minimum cut problem\non $G$ requires $\\Omega(n^{3/2}\\sqrt{\\tau})$ many queries to the adjacency\nmatrix of $G$. These results contrast with the classical randomized case where\n$\\Omega(n^2)$ queries to the adjacency matrix are needed in the worst case even\nto decide if an unweighted graph is connected or not.\n  In the adjacency array model, when $G$ has $m$ edges the classical randomized\ncomplexity of the minimum cut problem is $\\tilde \\Theta(m)$. We show that the\nquantum query and time complexity are $\\tilde O(\\sqrt{mn\\tau})$ and $\\tilde\nO(\\sqrt{mn\\tau} + n^{3/2})$, respectively, where again the edge weights are\nbetween $1$ and $\\tau$. For dense graphs we give lower bounds on the quantum\nquery complexity of $\\Omega(n^{3/2})$ for $\\tau > 1$ and $\\Omega(\\tau n)$ for\nany $1 \\leq \\tau \\leq n$.\n  Our query algorithm uses a quantum algorithm for graph sparsification by\nApers and de Wolf (FOCS 2020) and results on the structure of near-minimum cuts\nby Kawarabayashi and Thorup (STOC 2015) and Rubinstein, Schramm and Weinberg\n(ITCS 2018). Our time efficient implementation builds on Karger's tree packing\ntechnique (STOC 1996).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:51:49 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 16:02:38 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 13:22:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Apers", "Simon", ""], ["Lee", "Troy", ""]]}, {"id": "2011.09932", "submitter": "Shmuel Onn", "authors": "Martin Koutecky, Shmuel Onn", "title": "Uniform and Monotone Line Sum Optimization", "comments": null, "journal-ref": "Discrete Applied Mathematics, 298:165--170, 2021", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em line sum optimization problem} asks for a $(0,1)$-matrix minimizing\nthe sum of given functions evaluated at its row and column sums. We show that\nthe {\\em uniform} problem, with identical row functions and identical column\nfunctions, and the {\\em monotone} problem, over matrices with nonincreasing row\nand column sums, are polynomial time solvable.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:16:17 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:25:49 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Koutecky", "Martin", ""], ["Onn", "Shmuel", ""]]}, {"id": "2011.09952", "submitter": "J. Carlos Martinez Mori", "authors": "J. Carlos Mart\\'inez Mori, Samitha Samaranayake", "title": "On the Request-Trip-Vehicle Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The request-trip-vehicle assignment problem is at the heart of a popular\ndecomposition strategy for online vehicle routing. In this framework,\nassignments are done in batches in order to exploit any shareability among\nvehicles and incoming travel requests. We study a natural ILP formulation and\nits LP relaxation. Our main result is an LP-based randomized rounding algorithm\nthat, whenever the instance is feasible, leverages mild assumptions to return\nan assignment whose: i) expected cost is at most that of an optimal solution,\nand ii) expected fraction of unassigned requests is at most $1/e$. If\ntrip-vehicle assignment costs are $\\alpha$-approximate, we pay an additional\nfactor of $\\alpha$ in the expected cost. We can relax the feasibility\nrequirement by considering the penalty version of the problem, in which a\npenalty is paid for each unassigned request. We find that, whenever a request\nis repeatedly unassigned after a number of rounds, with high probability it is\nso in accordance with the sequence of LP solutions and not because of a\nrounding error. We additionally introduce a deterministic rounding heuristic\ninspired by our randomized technique. Our computational experiments show that\nour rounding algorithms achieve a performance similar to that of the ILP at a\nreduced computation time, far improving on our theoretical guarantee. The\nreason for this is that, although the assignment problem is hard in theory, the\nnatural LP relaxation tends to be very tight in practice. We conclude with\nobservations and open questions related to this phenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:51:52 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 21:34:44 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Mori", "J. Carlos Mart\u00ednez", ""], ["Samaranayake", "Samitha", ""]]}, {"id": "2011.09973", "submitter": "Kevin Tian", "authors": "Ilias Diakonikolas, Daniel M. Kane, Daniel Kongsgaard, Jerry Li, Kevin\n  Tian", "title": "List-Decodable Mean Estimation in Nearly-PCA Time", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, robust statistics has focused on designing estimators tolerant\nto a minority of contaminated data. Robust list-decodable learning focuses on\nthe more challenging regime where only a minority $\\frac 1 k$ fraction of the\ndataset is drawn from the distribution of interest, and no assumptions are made\non the remaining data. We study the fundamental task of list-decodable mean\nestimation in high dimensions. Our main result is a new list-decodable mean\nestimation algorithm for bounded covariance distributions with optimal sample\ncomplexity and error rate, running in nearly-PCA time. Assuming the ground\ntruth distribution on $\\mathbb{R}^d$ has bounded covariance, our algorithm\noutputs a list of $O(k)$ candidate means, one of which is within distance\n$O(\\sqrt{k})$ from the truth. Our algorithm runs in time $\\widetilde{O}(ndk)$\nfor all $k = O(\\sqrt{d}) \\cup \\Omega(d)$, where $n$ is the size of the dataset.\nWe also show that a variant of our algorithm has runtime $\\widetilde{O}(ndk)$\nfor all $k$, at the expense of an $O(\\sqrt{\\log k})$ factor in the recovery\nguarantee. This runtime matches up to logarithmic factors the cost of\nperforming a single $k$-PCA on the data, which is a natural bottleneck of known\nalgorithms for (very) special cases of our problem, such as clustering\nwell-separated mixtures. Prior to our work, the fastest list-decodable mean\nestimation algorithms had runtimes $\\widetilde{O}(n^2 d k^2)$ and\n$\\widetilde{O}(nd k^{\\ge 6})$.\n  Our approach builds on a novel soft downweighting method, $\\mathsf{SIFT}$,\nwhich is arguably the simplest known polynomial-time mean estimation technique\nin the list-decodable learning setting. To develop our fast algorithms, we\nboost the computational cost of $\\mathsf{SIFT}$ via a careful \"win-win-win\"\nanalysis of an approximate Ky Fan matrix multiplicative weights procedure we\ndevelop, which we believe may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:21:37 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kongsgaard", "Daniel", ""], ["Li", "Jerry", ""], ["Tian", "Kevin", ""]]}, {"id": "2011.09986", "submitter": "Raj Kumar Maity", "authors": "Raj Kumar Maity and Cameron Musco", "title": "Estimation of Shortest Path Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the sample complexity of estimating the covariance matrix\n$\\mathbf{\\Sigma} \\in \\mathbb{R}^{d\\times d}$ of a distribution $\\mathcal D$\nover $\\mathbb{R}^d$ given independent samples, under the assumption that\n$\\mathbf{\\Sigma}$ is graph-structured. In particular, we focus on shortest path\ncovariance matrices, where the covariance between any two measurements is\ndetermined by the shortest path distance in an underlying graph with $d$ nodes.\nSuch matrices generalize Toeplitz and circulant covariance matrices and are\nwidely applied in signal processing applications, where the covariance between\ntwo measurements depends on the (shortest path) distance between them in time\nor space.\n  We focus on minimizing both the vector sample complexity: the number of\nsamples drawn from $\\mathcal{D}$ and the entry sample complexity: the number of\nentries read in each sample. The entry sample complexity corresponds to\nmeasurement equipment costs in signal processing applications. We give a very\nsimple algorithm for estimating $\\mathbf{\\Sigma}$ up to spectral norm error\n$\\epsilon \\left\\|\\mathbf{\\Sigma}\\right\\|_2$ using just $O(\\sqrt{D})$ entry\nsample complexity and $\\tilde O(r^2/\\epsilon^2)$ vector sample complexity,\nwhere $D$ is the diameter of the underlying graph and $r \\le d$ is the rank of\n$\\mathbf{\\Sigma}$. Our method is based on extending the widely applied idea of\nsparse rulers for Toeplitz covariance estimation to the graph setting.\n  In the special case when $\\mathbf{\\Sigma}$ is a low-rank Toeplitz matrix, our\nresult matches the state-of-the-art, with a far simpler proof. We also give an\ninformation theoretic lower bound matching our upper bound up to a factor $D$\nand discuss some directions towards closing this gap.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:37:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Maity", "Raj Kumar", ""], ["Musco", "Cameron", ""]]}, {"id": "2011.10004", "submitter": "Anas Elghafari", "authors": "Eugene Callahan, Robert Murphy, Anas Elghafari", "title": "Illustrating the Suitability of Greedy and Dynamic Algorithms Using The\n  Economics Concept of \"Opportunity Cost\"", "comments": "Preprint stage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students of Computer Science often wonder when, exactly, one can apply a\ngreedy algorithm to a problem, and when one must use the more complicated and\ntime-consuming techniques of dynamic programming. This paper argues that the\nexisting pedagogical literature does not offer clear guidance on this issue. We\nsuggest improving computer science pedagogy by importing a concept economists\nuse in their own implementations of dynamic programming. That economic concept\nis \"opportunity cost,\" and we explain how it can aid students in\ndifferentiating \"greedy problems\" from problems requiring dynamic programming\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:01:22 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Callahan", "Eugene", ""], ["Murphy", "Robert", ""], ["Elghafari", "Anas", ""]]}, {"id": "2011.10008", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "Subpath Queries on Compressed Graphs: a Survey", "comments": "Fixed some typos and references to Boyer-Moore-Galil's and\n  Apostolico-Giancarlo's algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text indexing is a classical algorithmic problem that has been studied for\nover four decades: given a text $T$, pre-process it off-line so that, later, we\ncan quickly count and locate the occurrences of any string (the query pattern)\nin $T$ in time proportional to the query's length. The earliest optimal-time\nsolution to the problem, the suffix tree, dates back to 1973 and requires up to\ntwo orders of magnitude more space than the plain text just to be stored. In\nthe year 2000, two breakthrough works showed that efficient queries can be\nachieved without this space overhead: a fast index be stored in a space\nproportional to the text's entropy. These contributions had an enormous impact\nin bioinformatics: nowadays, virtually any DNA aligner employs compressed\nindexes. Recent trends considered more powerful compression schemes (dictionary\ncompressors) and generalizations of the problem to labeled graphs: after all,\ntexts can be viewed as labeled directed paths. In turn, since finite state\nautomata can be considered as a particular case of labeled graphs, these\nfindings created a bridge between the fields of compressed indexing and regular\nlanguage theory, ultimately allowing to index regular languages and promising\nto shed new light on problems such as regular expression matching. This survey\nis a gentle introduction to the main landmarks of the fascinating journey that\ntook us from suffix trees to today's compressed indexes for labeled graphs and\nregular languages.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:07:53 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 10:01:03 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "2011.10014", "submitter": "Fabian Kuhn", "authors": "Salwa Faour and Fabian Kuhn", "title": "Approximate Bipartite Vertex Cover in the CONGEST Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give efficient distributed algorithms for the minimum vertex cover problem\nin bipartite graphs in the CONGEST model. From K\\H{o}nig's theorem, it is well\nknown that in bipartite graphs the size of a minimum vertex cover is equal to\nthe size of a maximum matching. We first show that together with an existing\n$O(n\\log n)$-round algorithm for computing a maximum matching, the constructive\nproof of K\\H{o}nig's theorem directly leads to a deterministic $O(n\\log\nn)$-round CONGEST algorithm for computing a minimum vertex cover. We then show\nthat by adapting the construction, we can also convert an \\emph{approximate}\nmaximum matching into an \\emph{approximate} minimum vertex cover. Given a\n$(1-\\delta)$-approximate matching for some $\\delta>1$, we show that a\n$(1+O(\\delta))$-approximate vertex cover can be computed in time\n$O(D+\\mathrm{poly}(\\frac{\\log n}{\\delta}))$, where $D$ is the diameter of the\ngraph. When combining with known graph clustering techniques, for any\n$\\varepsilon\\in(0,1]$, this leads to a $\\mathrm{poly}(\\frac{\\log\nn}{\\varepsilon})$-time deterministic and also to a slightly faster and simpler\nrandomized $O(\\frac{\\log n}{\\varepsilon^3})$-round CONGEST algorithm for\ncomputing a $(1+\\varepsilon)$-approximate vertex cover in bipartite graphs. For\nconstant $\\varepsilon$, the randomized time complexity matches the $\\Omega(\\log\nn)$ lower bound for computing a $(1+\\varepsilon)$-approximate vertex cover in\nbipartite graphs even in the LOCAL model. Our results are also in contrast to\nthe situation in general graphs, where it is known that computing an optimal\nvertex cover requires $\\tilde{\\Omega}(n^2)$ rounds in the CONGEST model and\nwhere it is not even known how to compute any $(2-\\varepsilon)$-approximation\nin time $o(n^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:28:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Faour", "Salwa", ""], ["Kuhn", "Fabian", ""]]}, {"id": "2011.10124", "submitter": "Haihao Lu", "authors": "Santiago Balseiro, Haihao Lu, Vahab Mirrokni", "title": "The Best of Many Worlds: Dual Mirror Descent for Online Allocation\n  Problems", "comments": "arXiv admin note: text overlap with arXiv:2002.10421", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online allocation problems with resource constraints are central problems in\nrevenue management and online advertising. In these problems, requests arrive\nsequentially during a finite horizon and, for each request, a decision maker\nneeds to choose an action that consumes a certain amount of resources and\ngenerates reward. The objective is to maximize cumulative rewards subject to a\nconstraint on the total consumption of resources. In this paper, we consider a\ndata-driven setting in which the reward and resource consumption of each\nrequest are generated using an input model that is unknown to the decision\nmaker.\n  We design a general class of algorithms that attain good performance in\nvarious inputs models without knowing which type of input they are facing. In\nparticular, our algorithms are asymptotically optimal under stochastic i.i.d.\ninput model as well as various non-stationary stochastic input models, and they\nattain an asymptotically optimal fixed competitive ratio when the input is\nadversarial. Our algorithms operate in the Lagrangian dual space: they maintain\na dual multiplier for each resource that is updated using online mirror\ndescent. By choosing the reference function accordingly, we recover dual\nsub-gradient descent and dual exponential weights algorithm. The resulting\nalgorithms are simple, fast, and have minimal requirements on the reward\nfunctions, consumption functions and the action space, in contrast to existing\nmethods for online allocation problems. We discuss applications to network\nrevenue management, online bidding in repeated auctions with budget\nconstraints, online proportional matching with high entropy, and personalized\nassortment optimization with limited inventories.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:39:17 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 15:51:56 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Balseiro", "Santiago", ""], ["Lu", "Haihao", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "2011.10355", "submitter": "Pedro Reviriego", "authors": "Pedro Reviriego, Pablo Adell and Daniel Ting", "title": "HyperLogLog (HLL) Security: Inflating Cardinality Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counting the number of distinct elements on a set is needed in many\napplications, for example to track the number of unique users in Internet\nservices or the number of distinct flows on a network. In many cases, an\nestimate rather than the exact value is sufficient and thus many algorithms for\ncardinality estimation that significantly reduce the memory and computation\nrequirements have been proposed. Among them, Hyperloglog has been widely\nadopted in both software and hardware implementations. The security of\nHyperloglog has been recently studied showing that an attacker can create a set\nof elements that produces a cardinality estimate that is much smaller than the\nreal cardinality of the set. This set can be used for example to evade\ndetection systems that use Hyperloglog. In this paper, the security of\nHyperloglog is considered from the opposite angle: the attacker wants to create\na small set that when inserted on the Hyperloglog produces a large cardinality\nestimate. This set can be used to trigger false alarms in detection systems\nthat use Hyperloglog but more interestingly, it can be potentially used to\ninflate the visits to websites or the number of hits of online advertisements.\nOur analysis shows that an attacker can create a set with a number of elements\nequal to the number of registers used in the Hyperloglog implementation that\nproduces any arbitrary cardinality estimate. This has been validated in two\ncommercial implementations of Hyperloglog: Presto and Redis. Based on those\nresults, we also consider the protection of Hyperloglog against such an attack.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 11:43:33 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Reviriego", "Pedro", ""], ["Adell", "Pablo", ""], ["Ting", "Daniel", ""]]}, {"id": "2011.10385", "submitter": "Felix Hommelsheim", "authors": "Tatsuhiko Hatanaka, Felix Hommelsheim, Takehiro Ito, Yusuke Kobayashi,\n  Moritz M\\\"uhlenthaler, Akira Suzuki", "title": "Fixed-Parameter Algorithms for Graph Constraint Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-deterministic constraint logic (NCL) is a simple model of computation\nbased on orientations of a constraint graph with edge weights and vertex\ndemands. NCL captures \\PSPACE\\xspace and has been a useful tool for proving\nalgorithmic hardness of many puzzles, games, and reconfiguration problems. In\nparticular, its usefulness stems from the fact that it remains \\PSPACE-complete\neven under severe restrictions of the weights (e.g., only edge-weights one and\ntwo are needed) and the structure of the constraint graph (e.g., planar\n\\textsc{and/or}\\xspace graphs of bounded bandwidth). While such restrictions on\nthe structure of constraint graphs do not seem to limit the expressiveness of\nNCL, the building blocks of the constraint graphs cannot be limited without\nlosing expressiveness: We consider as parameters the number of weight-one edges\nand the number of weight-two edges of a constraint graph, as well as the number\nof \\textsc{and}\\xspace or \\textsc{or}\\xspace vertices of an\n\\textsc{and/or}\\xspace constraint graph. We show that NCL is fixed-parameter\ntractable (FPT) for any of these parameters. In particular, for NCL\nparameterized by the number of weight-one edges or the number of\n\\textsc{and}\\xspace vertices, we obtain a linear kernel. It follows that, in a\nsense, NCL as introduced by Hearn and Demaine is defined in the most economical\nway for the purpose of capturing \\PSPACE.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:52:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hatanaka", "Tatsuhiko", ""], ["Hommelsheim", "Felix", ""], ["Ito", "Takehiro", ""], ["Kobayashi", "Yusuke", ""], ["M\u00fchlenthaler", "Moritz", ""], ["Suzuki", "Akira", ""]]}, {"id": "2011.10400", "submitter": "Tobias Z\\\"undorf", "authors": "Moritz Baum, Julian Dibbelt, Dorothea Wagner, Tobias Z\\\"undorf", "title": "Modeling and Engineering Constrained Shortest Path Algorithms for\n  Battery Electric Vehicles", "comments": null, "journal-ref": "In Transportation Science volume 54(6), pages 1571-1600, 2020", "doi": "10.1287/trsc.2020.0981", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing constrained shortest paths for battery\nelectric vehicles. Since battery capacities are limited, fastest routes are\noften infeasible. Instead, users are interested in fast routes on which the\nenergy consumption does not exceed the battery capacity. For that, drivers can\ndeliberately reduce speed to save energy. Hence, route planning should provide\nboth path and speed recommendations. To tackle the resulting NP-hard\noptimization problem, previous work trades correctness or accuracy of the\nunderlying model for practical running times. We present a novel framework to\ncompute optimal constrained shortest paths (without charging stops) for\nelectric vehicles that uses more realistic physical models, while taking speed\nadaptation into account. Careful algorithm engineering makes the approach\npractical even on large, realistic road networks: We compute optimal solutions\nin less than a second for typical battery capacities, matching the performance\nof previous inexact methods. For even faster query times, the approach can\neasily be extended with heuristics that provide high quality solutions within\nmilliseconds.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:35:28 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Baum", "Moritz", ""], ["Dibbelt", "Julian", ""], ["Wagner", "Dorothea", ""], ["Z\u00fcndorf", "Tobias", ""]]}, {"id": "2011.10446", "submitter": "Goran \\v{Z}u\\v{z}i\\'c", "authors": "Mohsen Ghaffari and Bernhard Haeupler and Goran Zuzic", "title": "Hop-Constrained Oblivious Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove the existence of an oblivious routing scheme that is\n$\\mathrm{poly}(\\log n)$-competitive in terms of $(congestion + dilation)$, thus\nresolving a well-known question in oblivious routing.\n  Concretely, consider an undirected network and a set of packets each with its\nown source and destination. The objective is to choose a path for each packet,\nfrom its source to its destination, so as to minimize $(congestion +\ndilation)$, defined as follows: The dilation is the maximum path hop-length,\nand the congestion is the maximum number of paths that include any single edge.\nThe routing scheme obliviously and randomly selects a path for each packet\nindependent of (the existence of) the other packets. Despite this\nobliviousness, the selected paths have $(congestion + dilation)$ within a\n$\\mathrm{poly}(\\log n)$ factor of the best possible value. More precisely, for\nany integer hop-bound $h$, this oblivious routing scheme selects paths of\nlength at most $h \\cdot \\mathrm{poly}(\\log n)$ and is $\\mathrm{poly}(\\log\nn)$-competitive in terms of $congestion$ in comparison to the best possible\n$congestion$ achievable via paths of length at most $h$ hops. These paths can\nbe sampled in polynomial time.\n  This result can be viewed as an analogue of the celebrated oblivious routing\nresults of R\\\"{a}cke [FOCS 2002, STOC 2008], which are $O(\\log n)$-competitive\nin terms of $congestion$, but are not competitive in terms of $dilation$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:20:23 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""], ["Zuzic", "Goran", ""]]}, {"id": "2011.10450", "submitter": "Yusuf Yigit Pilavci", "authors": "Yusuf Pilavci (GIPSA-GAIA), Pierre-Olivier Amblard (GIPSA-GAIA), Simon\n  Barthelme (GIPSA-GAIA), Nicolas Tremblay (GIPSA-GAIA)", "title": "Graph Tikhonov Regularization and Interpolation via Random Spanning\n  Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel Monte Carlo estimators are proposed to solve both the Tikhonov\nregularization (TR) and the interpolation problems on graphs. These estimators\nare based on random spanning forests (RSF), the theoretical properties of which\nenable to analyze the estimators' theoretical mean and variance. We also show\nhow to perform hyperparameter tuning for these RSF-based estimators. TR is a\ncomponent in many well-known algorithms, and we show how the proposed\nestimators can be easily adapted to avoid expensive intermediate steps in\ngeneralized semi-supervised learning, label propagation, Newton's method and\niteratively reweighted least squares. In the experiments, we illustrate the\nproposed methods on several problems and provide observations on their run\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:27:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:45:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Pilavci", "Yusuf", "", "GIPSA-GAIA"], ["Amblard", "Pierre-Olivier", "", "GIPSA-GAIA"], ["Barthelme", "Simon", "", "GIPSA-GAIA"], ["Tremblay", "Nicolas", "", "GIPSA-GAIA"]]}, {"id": "2011.10659", "submitter": "Mathilde Fekom", "authors": "Mathilde Fekom and Argyris Kalogeratos", "title": "Efficient stream-based Max-Min diversification with minimal failure rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The stream-based Max-Min diversification problem concerns the task of\nselecting a limited number of diverse instances from a data stream. The nature\nof the problem demands immediate and irrevocable decisions. The set-wise\ndiversity to be maximized is the minimum distance among any pair of the\nselected instances. Standard algorithmic approaches for sequential selection\ndisregard the possibility of selection failures, which is the situation where\nthe last instances of the stream are picked by default to prevent having an\nincomplete selection. This defect can be catastrophic for the Max-Min\ndiversification objective. In this paper we present the Failure Rate\nMinimization (FRM) algorithm that allows the selection of a set of disparate\ninstances while reducing significantly the probability of having failures. This\nis achieved by means of both analytical and empirical techniques. FRM is put in\ncomparison with relevant algorithms from the literature through simulations on\nreal datasets, where we demonstrate its efficiency and low time complexity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:20:16 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Fekom", "Mathilde", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "2011.10695", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski, Zhenyu Liao, Edgar Dobriban and Michael W.\n  Mahoney", "title": "Sparse sketches with small inversion bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a tall $n\\times d$ matrix $A$ and a random $m\\times n$ sketching matrix\n$S$, the sketched estimate of the inverse covariance matrix $(A^\\top A)^{-1}$\nis typically biased: $E[(\\tilde A^\\top\\tilde A)^{-1}]\\ne(A^\\top A)^{-1}$, where\n$\\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in\nstatistics and distributed optimization, when averaging multiple independently\nconstructed estimates of quantities that depend on the inverse covariance. We\ndevelop a framework for analyzing inversion bias, based on our proposed concept\nof an $(\\epsilon,\\delta)$-unbiased estimator for random matrices. We show that\nwhen the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,\nthen after simple rescaling, the estimator $(\\frac m{m-d}\\tilde A^\\top\\tilde\nA)^{-1}$ is $(\\epsilon,\\delta)$-unbiased for $(A^\\top A)^{-1}$ with a sketch of\nsize $m=O(d+\\sqrt d/\\epsilon)$. This implies that for $m=O(d)$, the inversion\nbias of this estimator is $O(1/\\sqrt d)$, which is much smaller than the\n$\\Theta(1)$ approximation error obtained as a consequence of the subspace\nembedding guarantee for sub-gaussian sketches. We then propose a new sketching\ntechnique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas\nfrom both data-oblivious sparse embeddings as well as data-aware leverage-based\nrow sampling methods, to get $\\epsilon$ inversion bias for sketch size\n$m=O(d\\log d+\\sqrt d/\\epsilon)$ in time $O(\\text{nnz}(A)\\log n+md^2)$, where\nnnz is the number of non-zeros. The key techniques enabling our analysis\ninclude an extension of a classical inequality of Bai and Silverstein for\nrandom quadratic forms, which we call the Restricted Bai-Silverstein\ninequality; and anti-concentration of the Binomial distribution via the\nPaley-Zygmund inequality, which we use to prove a lower bound showing that\nleverage score sampling sketches generally do not achieve small inversion bias.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:33:15 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 01:24:51 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Liao", "Zhenyu", ""], ["Dobriban", "Edgar", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2011.10870", "submitter": "Saeed Seddighin", "authors": "Michael Mitzenmacher and Saeed Seddighin", "title": "Erd\\\"{o}s-Szekeres Partitioning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this note, we present a substantial improvement on the computational\ncomplexity of the Erd\\\"{o}s-Szekeres partitioning problem and review recent\nworks on dynamic \\textsf{LIS}.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 21:32:05 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Seddighin", "Saeed", ""]]}, {"id": "2011.10874", "submitter": "Saeed Seddighin", "authors": "Tomasz Kociumaka and Saeed Seddighin", "title": "Improved Dynamic Algorithms for Longest Increasing Subsequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study dynamic algorithms for the longest increasing subsequence\n(\\textsf{LIS}) problem. A dynamic \\textsf{LIS} algorithm maintains a sequence\nsubject to operations of the following form arriving one by one: (i) insert an\nelement, (ii) delete an element, or (iii) substitute an element for another.\nAfter performing each operation, the algorithm must report the length of the\nlongest increasing subsequence of the current sequence.\n  Our main contribution is the first exact dynamic \\textsf{LIS} algorithm with\nsublinear update time. More precisely, we present a randomized algorithm that\nperforms each operation in time $\\tilde O(n^{2/3})$ and after each update,\nreports the answer to the \\textsf{LIS} problem correctly with high probability.\nWe use several novel techniques and observations for this algorithm that may\nfind their applications in future work.\n  In the second part of the paper, we study approximate dynamic \\textsf{LIS}\nalgorithms, which are allowed to underestimate the solution size within a\nbounded multiplicative factor. In this setting, we give a deterministic\nalgorithm with update time $O(n^{o(1)})$ and approximation factor $1-o(1)$.\nThis result substantially improves upon the previous work of Mitzenmacher and\nSeddighin (STOC'20) that presents an $\\Omega(\\epsilon\n^{O(1/\\epsilon)})$-approximation algorithm with update time $\\tilde\nO(n^\\epsilon)$ for any constant $\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 21:39:37 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 20:37:39 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Seddighin", "Saeed", ""]]}, {"id": "2011.10919", "submitter": "Kazem Jahanbakhsh", "authors": "Kazem Jahanbakhsh", "title": "Applying Multi-armed Bandit Algorithms to Computational Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last two decades, we have seen extensive industrial research in the\narea of computational advertising. In this paper, our goal is to study the\nperformance of various online learning algorithms to identify and display the\nbest ads/offers with the highest conversion rates to web users. We formulate\nour ad-selection problem as a Multi-Armed Bandit problem which is a classical\nparadigm in Machine Learning. We have been applying machine learning, data\nmining, probability, and statistics to analyze big data in the ad-tech space\nand devise efficient ad selection strategies. This article highlights some of\nour findings in the area of computational advertising from 2011 to 2015.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 03:23:13 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jahanbakhsh", "Kazem", ""]]}, {"id": "2011.10938", "submitter": "Songhua Li", "authors": "Songhua Li, Minming Li, Lingjie Duan and Victor C.S. Lee", "title": "Online Maximum $k$-Interval Coverage Problem", "comments": "An extended abstract of this full version is to appear in COCOA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online maximum coverage problem on a line, in which, given an\nonline sequence of sub-intervals (which may intersect among each other) of a\ntarget large interval and an integer $k$, we aim to select at most $k$ of the\nsub-intervals such that the total covered length of the target interval is\nmaximized. The decision to accept or reject each sub-interval is made\nimmediately and irrevocably (no preemption) right at the release timestamp of\nthe sub-interval. We comprehensively study different settings of this problem\nregarding both the length of a released sub-interval and the total number of\nreleased sub-intervals. We first present lower bounds on the competitive ratio\nfor the settings concerned in this paper, respectively. For the offline problem\nwhere the sequence of all the released sub-intervals is known in advance to the\ndecision-maker, we propose a dynamic-programming-based optimal approach as the\nbenchmark. For the online problem, we first propose a single-threshold-based\ndeterministic algorithm SOA by adding a sub-interval if the added length\nexceeds a certain threshold, achieving competitive ratios close to the lower\nbounds, respectively. Then, we extend to a double-thresholds-based algorithm\nDOA, by using the first threshold for exploration and the second threshold\n(larger than the first one) for exploitation. With the two thresholds solved by\nour proposed program, we show that DOA improves SOA in the worst-case\nperformance. Moreover, we prove that a deterministic algorithm that accepts\nsub-intervals by multi non-increasing thresholds cannot outperform even SOA.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 05:16:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Songhua", ""], ["Li", "Minming", ""], ["Duan", "Lingjie", ""], ["Lee", "Victor C. S.", ""]]}, {"id": "2011.10963", "submitter": "Eklavya Sharma", "authors": "Eklavya Sharma", "title": "Harmonic Algorithms for Packing d-dimensional Cuboids Into Bins", "comments": "Update 2: major refactoring; Update 1: fix typos, slightly improve\n  readability, use title-case for title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore approximation algorithms for the $d$-dimensional geometric bin\npacking problem ($d$BP). Caprara (MOR 2008) gave a harmonic-based algorithm for\n$d$BP having an asymptotic approximation ratio (AAR) of $T_{\\infty}^{d-1}$\n(where $T_{\\infty} \\approx 1.691$). However, their algorithm doesn't allow\nitems to be rotated. This is in contrast to some common applications of $d$BP,\nlike packing boxes into shipping containers. We give approximation algorithms\nfor $d$BP when items can be orthogonally rotated about all or a subset of axes.\nWe first give a fast and simple harmonic-based algorithm having AAR\n$T_{\\infty}^{d}$. We next give a more sophisticated harmonic-based algorithm,\nwhich we call $\\mathtt{HGaP}_k$, having AAR $T_{\\infty}^{d-1}(1+\\epsilon)$.\nThis gives an AAR of roughly $2.860 + \\epsilon$ for 3BP with rotations, which\nimproves upon the best-known AAR of $4.5$.\n  In addition, we study the multiple-choice bin packing problem that\ngeneralizes the rotational case. Here we are given $n$ sets of $d$-dimensional\ncuboidal items and we have to choose exactly one item from each set and then\npack the chosen items. Our algorithms also work for the multiple-choice bin\npacking problem. We also give fast and simple approximation algorithms for the\nmultiple-choice versions of $d$D strip packing and $d$D geometric knapsack.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 07:44:51 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 16:47:57 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 19:01:52 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sharma", "Eklavya", ""]]}, {"id": "2011.10983", "submitter": "Thomas Dybdahl Ahle", "authors": "Anders Aamand, Mikkel Abrahamsen, Thomas D. Ahle, Peter M. R.\n  Rasmussen", "title": "Tiling with Squares and Packing Dominos in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider planar tiling and packing problems with polyomino pieces and a\npolyomino container $P$. A polyomino is a polygonal region with axis parallel\nedges and corners of integral coordinates, which may have holes. We give two\npolynomial time algorithms, one for deciding if $P$ can be tiled with $2\\times\n2$ squares (that is, deciding if $P$ is the union of a set of non-overlapping\ncopies of the $2\\times 2$ square) and one for packing $P$ with a maximum number\nof non-overlapping and axis-parallel $2\\times 1$ dominos, allowing rotations of\n$90^\\circ$. As packing is more general than tiling, the latter algorithm can\nalso be used to decide if $P$ can be tiled by $2\\times 1$ dominos.\n  These are classical problems with important applications in VLSI design, and\nthe related problem of finding a maximum packing of $2\\times 2$ squares is\nknown to be NP-Hard [J.~Algorithms 1990]. For our three problems there are\nknown pseudo-polynomial time algorithms, that is, algorithms with running times\npolynomial in the \\emph{area} of $P$. However, the standard, compact way to\nrepresent a polygon is by listing the coordinates of the corners in binary. We\nuse this representation, and thus present the first polynomial time algorithms\nfor the problems. Concretely, we give a simple $O(n\\log n)$ algorithm for\ntiling with squares, and a more involved $O(n^4)$ algorithm for packing and\ntiling with dominos.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 10:57:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Aamand", "Anders", ""], ["Abrahamsen", "Mikkel", ""], ["Ahle", "Thomas D.", ""], ["Rasmussen", "Peter M. R.", ""]]}, {"id": "2011.10989", "submitter": "Ahmad T. Anaqreh", "authors": "Ahmad T. Anaqreh, Boglarka G.-Toth, Tamas Vinko", "title": "Algorithmic upper bounds for graph geodetic number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph theoretical problems based on shortest paths are at the core of\nresearch due to their theoretical importance and applicability. This paper\ndeals with the geodetic number which is a global measure for simple connected\ngraphs and it belongs to the path covering problems: what is the\nminimal-cardinality set of vertices, such that all shortest paths between its\nelements cover every vertex of the graph. Inspired by the exact 0-1 integer\nlinear programming formalism from the recent literature, we propose a new\nmethods to obtain upper bounds for the geodetic number in an algorithmic way.\nThe efficiency of these algorithms are demonstrated on a collection of\nstructurally different graphs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 11:23:21 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Anaqreh", "Ahmad T.", ""], ["-Toth", "Boglarka G.", ""], ["Vinko", "Tamas", ""]]}, {"id": "2011.11049", "submitter": "Hubert Garavel", "authors": "Pierre Bouvier (CONVECS), Hubert Garavel (CONVECS)", "title": "The VLSAT-1 Benchmark Suite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents VLSAT-1 (an acronym for \"Very Large Boolean\nSATisfiability problems\"), the first part of a benchmark suite to be used in\nscientificexperiments and software competitions addressing SAT-solving\nissues.VLSAT-1 contains 100~benchmarks of increasing complexity, proposed in\nDIMACSCNF format under a permissive Creative Commons license. These benchmarks\nhavebeen used by the 2020 International Competition on Model Counting.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:20:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bouvier", "Pierre", "", "CONVECS"], ["Garavel", "Hubert", "", "CONVECS"]]}, {"id": "2011.11129", "submitter": "Shahrzad Haddadan", "authors": "Cyrus Cousins, Shahrzad Haddadan, Eli Upfal", "title": "Making mean-estimation more efficient using an MCMC trace variance\n  approach: DynaMITE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel statistical measure for MCMC-mean estimation, the\ninter-trace variance ${\\rm trv}^{(\\tau_{rel})}({\\cal M},f)$, which depends on a\nMarkov chain ${\\cal M}$ and a function $f:S\\to [a,b]$. The inter-trace variance\ncan be efficiently estimated from observed data and leads to a more efficient\nMCMC-mean estimator. Prior MCMC mean-estimators receive, as input, upper-bounds\non $\\tau_{mix}$ or $\\tau_{rel}$, and often also the stationary variance, and\ntheir performance is highly dependent to the sharpness of these bounds. In\ncontrast, we introduce DynaMITE, which dynamically adjusts the sample size, it\nis less sensitive to the looseness of input upper-bounds on $\\tau_{rel}$, and\nrequires no bound on $v_{\\pi}$.\n  Receiving only an upper-bound ${\\cal T}_{rel}$ on $\\tau_{rel}$, DynaMITE\nestimates the mean of $f$ in $\\tilde{\\cal{O}}\\bigl(\\smash{\\frac{{\\cal T}_{rel}\nR}{\\varepsilon}}+\\frac{\\tau_{rel}\\cdot {\\rm\ntrv}^{(\\tau{{rel}})}}{\\varepsilon^{2}}\\bigr)$ steps, without a priori bounds on\nthe stationary variance $v_{\\pi}$ or the inter-trace variance ${\\rm trv}^{(\\tau\nrel)}$. Thus we depend minimally on the tightness of ${\\cal T}_{mix}$, as the\ncomplexity is dominated by $\\tau_{rel}\\rm{trv}^{(\\tau{rel})}$ as $\\varepsilon\n\\to 0$. Note that bounding $\\tau_{\\rm rel}$ is known to be prohibitively\ndifficult, however, DynaMITE is able to reduce its principal dependence on\n${\\cal T}_{rel}$ to $\\tau_{rel}$, simply by exploiting properties of the\ninter-trace variance. To compare our method to known variance-aware bounds, we\nshow ${\\rm trv}^{(\\tau{rel})}({\\cal M},f) \\leq v_{\\pi}$. Furthermore, we show\nwhen $f$'s image is distributed (semi)symmetrically on ${\\cal M}$'s traces, we\nhave ${\\rm trv}^{({\\tau{rel}})}({\\cal M},f)=o(v_{\\pi}(f))$, thus DynaMITE\noutperforms prior methods in these cases.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 22:38:09 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 03:26:08 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 23:39:50 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 01:49:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cousins", "Cyrus", ""], ["Haddadan", "Shahrzad", ""], ["Upfal", "Eli", ""]]}, {"id": "2011.11144", "submitter": "A. Yavuz Oruc", "authors": "Taeyoung An and A. Yavuz Oruc", "title": "Searching and Sorting with O(n^2) processors in O(1) time", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The proliferation of number of processing elements (PEs) in parallel computer\nsystems, along with the use of more extensive parallelization of algorithms\ncauses the interprocessor communications dominate VLSI chip space. This paper\nproposes a new architecture to overcome this issue by using simple crosspoint\nswitches to pair PEs instead of a complex interconnection network. Based on the\ncyclic permutation wiring idea described in \\cite{oruc2016self}, this pairing\nleads to a linear crosspoint array of $n(n-1)/2$ processing elements and as\nmany crosspoints. We demonstrate the versatility of this new parallel\narchitecture by designing fast searching and sorting algorithms for it. In\nparticular, we show that finding a minimum, maximum, and searching a list of\n$n$ elements can all be performed in $O(1)$ time with elementary logic gates\nwith $O(n)$ fan-in, and in $O(\\lg n)$ time with $O(1)$ fan-in. We further show\nthat sorting a list of $n$ elements can also be carried out in $O(1)$ time\nusing elementary logic gates with $O(n)$ fan-in and threshold logic gates. The\nsorting time increases to $O(\\lg n\\lg\\lg n)$ if only elementary logic gates\nwith $O(1)$ fan-in are used. The algorithm can find the maximum among $n$\nelements in $O(1)$ time, and sort $n$ elements in $O(\\lg n (\\lg\\lg n))$ time.\nIn addition, we show how other fundamental queries can be handled within the\nsame order of time complexities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:14:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["An", "Taeyoung", ""], ["Oruc", "A. Yavuz", ""]]}, {"id": "2011.11181", "submitter": "Sitan Chen", "authors": "Sitan Chen, Xiaoxiao Li, Zhao Song, Danyang Zhuo", "title": "On InstaHide, Phase Retrieval, and Sparse Matrix Factorization", "comments": "30 pages, to appear in ICLR 2021, v2: updated discussion of follow-up\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we examine the security of InstaHide, a scheme recently\nproposed by [Huang, Song, Li and Arora, ICML'20] for preserving the security of\nprivate datasets in the context of distributed learning. To generate a\nsynthetic training example to be shared among the distributed learners,\nInstaHide takes a convex combination of private feature vectors and randomly\nflips the sign of each entry of the resulting vector with probability 1/2. A\nsalient question is whether this scheme is secure in any provable sense,\nperhaps under a plausible hardness assumption and assuming the distributions\ngenerating the public and private data satisfy certain properties.\n  We show that the answer to this appears to be quite subtle and closely\nrelated to the average-case complexity of a new multi-task, missing-data\nversion of the classic problem of phase retrieval. Motivated by this\nconnection, we design a provable algorithm that can recover private vectors\nusing only the public vectors and synthetic vectors generated by InstaHide,\nunder the assumption that the private and public vectors are isotropic\nGaussian.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:47:08 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 00:08:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Chen", "Sitan", ""], ["Li", "Xiaoxiao", ""], ["Song", "Zhao", ""], ["Zhuo", "Danyang", ""]]}, {"id": "2011.11268", "submitter": "Eklavya Sharma", "authors": "Eklavya Sharma", "title": "An Approximation Algorithm for Covering Linear Programs and its\n  Application to Bin-Packing", "comments": "Update: added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $\\alpha(1+\\epsilon)$-approximation algorithm for solving covering\nLPs, assuming the presence of a $(1/\\alpha)$-approximation algorithm for a\ncertain optimization problem. Our algorithm is based on a simple modification\nof the Plotkin-Shmoys-Tardos algorithm (MOR 1995). We then apply our algorithm\nto $\\alpha(1+\\epsilon)$-approximately solve the configuration LP for a large\nclass of bin-packing problems, assuming the presence of a\n$(1/\\alpha)$-approximate algorithm for the corresponding knapsack problem (KS).\nPrevious results give us a PTAS for the configuration LP using a PTAS for KS.\nThose results don't extend to the case where KS is poorly approximated. Our\nalgorithm, however, works even for polynomially-large $\\alpha$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:15:23 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 16:43:21 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sharma", "Eklavya", ""]]}, {"id": "2011.11282", "submitter": "Tuukka Korhonen", "authors": "Tuukka Korhonen", "title": "Tight Bounds for Potential Maximal Cliques Parameterized by Vertex Cover", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a graph with $n$ vertices and vertex cover of size $k$ has at\nmost $4^k + n$ potential maximal cliques. We also show that for each positive\ninteger $k$, there exists a graph with vertex cover of size $k$, $O(k^2)$\nvertices, and $\\Omega(4^k)$ potential maximal cliques. Our results extend the\nresults of Fomin, Liedloff, Montealegre, and Todinca [Algorithmica,\n80(4):1146--1169, 2018], who proved an upper bound of $poly(n) 4^k$, but left\nthe lower bound as an open problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:55:17 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Korhonen", "Tuukka", ""]]}, {"id": "2011.11500", "submitter": "Luca Corinzia", "authors": "Luca Corinzia and Paolo Penna and Wojciech Szpankowski and Joachim M.\n  Buhmann", "title": "Statistical and computational thresholds for the planted $k$-densest\n  sub-hypergraph problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we consider the problem of recovery a planted $k$-densest\nsub-hypergraph on $d$-uniform hypergraphs. This fundamental problem appears in\ndifferent contexts, e.g., community detection, average-case complexity, and\nneuroscience applications as a structural variant of tensor-PCA problem. We\nprovide tight \\emph{information-theoretic} upper and lower bounds for the exact\nrecovery threshold by the maximum-likelihood estimator, as well as\n\\emph{algorithmic} bounds based on approximate message passing algorithms. The\nproblem exhibits a typical statistical-to-computational gap observed in\nanalogous sparse settings that widen with increasing sparsity of the problem.\nThe bounds show that the signal structure impacts the location of the\nstatistical and computational phase transition that the known existing bounds\nfor the tensor-PCA model do not capture. This effect is due to the generic\nplanted signal prior that this latter model addresses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:02:12 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 13:21:05 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Corinzia", "Luca", ""], ["Penna", "Paolo", ""], ["Szpankowski", "Wojciech", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "2011.11524", "submitter": "Viktor Krapivensky", "authors": "Viktor Krapivensky", "title": "Speeding up decimal multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decimal multiplication is the task of multiplying two numbers in base $10^N.$\nSpecifically, we focus on the number-theoretic transform (NTT) family of\nalgorithms. Using only portable techniques, we achieve a 3x-5x speedup over the\nmpdecimal library. In this paper we describe our implementation and discuss\nfurther possible optimizations. We also present a simple cache-efficient\nalgorithm for in-place $2n \\times n$ or $n \\times 2n$ matrix transposition, the\nneed for which arises in the \"six-step algorithm\" variation of the matrix\nFourier algorithm, and which does not seem to be widely known. Another finding\nis that use of two prime moduli instead of three makes sense even considering\nthe worst case of increasing the size of the input, and makes for simpler\nanswer recovery.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:35:53 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 06:06:19 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 09:46:27 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 20:26:35 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Krapivensky", "Viktor", ""]]}, {"id": "2011.11618", "submitter": "Eklavya Sharma", "authors": "Eklavya Sharma", "title": "Analysis of the Harmonic Function Used in Bin-Packing", "comments": "Update: fixed typo and added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The harmonic function was first introduced by Lee and Lee (JACM 1985) for\nanalyzing their online bin-packing algorithm. Subsequently, it has been used to\nobtain approximation algorithms for many different packing problems. Here we\nslightly generalize the harmonic function and give alternative proofs of its\nimportant properties.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:48:36 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 16:44:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sharma", "Eklavya", ""]]}, {"id": "2011.11706", "submitter": "Hossein Jowhari", "authors": "Hossein Jowhari", "title": "An Estimator for Matching Size in Low Arboricity Graphs with Two\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a new simple degree-based estimator for the size of\nmaximum matching in bounded arboricity graphs. When the arboricity of the graph\nis bounded by $\\alpha$, the estimator gives a $\\alpha+2$ factor approximation\nof the matching size. For planar graphs, we show the estimator does better and\nreturns a $3.5$ approximation of the matching size.\n  Using this estimator, we get new results for approximating the matching size\nof planar graphs in the streaming and distributed models of computation. In\nparticular, in the vertex-arrival streams, we get a randomized\n$O(\\frac{\\sqrt{n}}{\\epsilon^2}\\log n)$ space algorithm for approximating the\nmatching size within $(3.5+\\epsilon)$ factor in a planar graph on $n$ vertices.\nSimilarly, we get a simultaneous protocol in the vertex-partition model for\napproximating the matching size within $(3.5+\\epsilon)$ factor using\n$O(\\frac{n^{2/3}}{\\epsilon^2}\\log n)$ communication from each player.\n  In comparison with the previous estimators, the estimator in this paper does\nnot need to know the arboricity of the input graph and improves the\napproximation factor for the case of planar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:19:48 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 05:11:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Jowhari", "Hossein", ""]]}, {"id": "2011.11743", "submitter": "Chenyang Xu", "authors": "Thomas Lavastida, Benjamin Moseley, R. Ravi and Chenyang Xu", "title": "Learnable and Instance-Robust Predictions for Online Matching, Flows and\n  Load Balancing", "comments": "To appear in ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for augmenting algorithms with predictions by\nrequiring that they are formally learnable and instance robust. Learnability\nensures that predictions can be efficiently constructed from a reasonable\namount of past data. Instance robustness ensures that the prediction is robust\nto modest changes in the problem input, where the measure of the change may be\nproblem specific. Instance robustness insists on a smooth degradation in\nperformance as a function of the change. Ideally, the performance is never\nworse than worst-case bounds. This also allows predictions to be objectively\ncompared.\n  We design online algorithms with predictions for a network flow allocation\nproblem and restricted assignment makespan minimization. For both problems, two\nkey properties are established: high quality predictions can be learned from a\nsmall sample of prior instances and these predictions are robust to errors that\nsmoothly degrade as the underlying problem instance changes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:38:57 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 01:59:10 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Lavastida", "Thomas", ""], ["Moseley", "Benjamin", ""], ["Ravi", "R.", ""], ["Xu", "Chenyang", ""]]}, {"id": "2011.11772", "submitter": "Bryce Sandlund", "authors": "Bryce Sandlund and Lingyi Zhang", "title": "Selectable Heaps and Optimal Lazy Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the $O(\\log n)$ time extract minimum function of efficient priority\nqueues can be generalized to the extraction of the $k$ smallest elements in\n$O(k \\log(n/k))$ time, where we define $\\log(x)$ as $\\max(\\log_2(x), 1)$. We\nfirst show heap-ordered tree selection (Kaplan et al., SOSA '19) can be applied\non the heap-ordered trees of the classic Fibonacci heap to support the\nextraction in $O(k \\log(n/k))$ amortized time. We then show selection is\npossible in a priority queue with optimal worst-case guarantees by applying\nheap-ordered tree selection on Brodal queues (SODA '96), supporting the\noperation in $O(k \\log(n/k))$ worst-case time. Via a reduction from the\nmultiple selection problem, $\\Omega(k \\log(n/k))$ time is necessary if\ninsertion is supported in $o(\\log n)$ time.\n  We then apply the result to lazy search trees (Sandlund & Wild, FOCS '20),\ncreating a new interval data structure based on selectable heaps. This gives\noptimal $O(B+n)$ time lazy search tree performance, lowering insertion\ncomplexity into a gap $\\Delta_i$ to $O(\\log(n/|\\Delta_i|))$ time. An $O(1)$\ntime merge operation is also made possible when used as a priority queue, among\nother situations. If Brodal queues are used, runtimes of the lazy search tree\ncan be made worst-case in the general case of two-sided gaps. The presented\ndata structure makes fundamental use of soft heaps (Chazelle, J. ACM '00),\nbiased search trees, and efficient priority queues, approaching the\ntheoretically-best data structure for ordered data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:30:32 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Sandlund", "Bryce", ""], ["Zhang", "Lingyi", ""]]}, {"id": "2011.11877", "submitter": "Ruizhe Zhang", "authors": "Baihe Huang, Zhao Song, Runzhou Tao, Ruizhe Zhang, Danyang Zhuo", "title": "InstaHide's Sample Complexity When Mixing Two Private Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by InstaHide challenge [Huang, Song, Li and Arora'20], [Chen, Song\nand Zhuo'20] recently provides one mathematical formulation of InstaHide attack\nproblem under Gaussian images distribution. They show that it suffices to use\n$O(n_{\\mathsf{priv}}^{k_{\\mathsf{priv}} - 2/(k_{\\mathsf{priv}} + 1)})$ samples\nto recover one private image in $n_{\\mathsf{priv}}^{O(k_{\\mathsf{priv}})} +\n\\mathrm{poly}(n_{\\mathsf{pub}})$ time for any integer $k_{\\mathsf{priv}}$,\nwhere $n_{\\mathsf{priv}}$ and $n_{\\mathsf{pub}}$ denote the number of images\nused in the private and the public dataset to generate a mixed image sample.\n  Under the current setup for the InstaHide challenge of mixing two private\nimages ($k_{\\mathsf{priv}} = 2$), this means $n_{\\mathsf{priv}}^{4/3}$ samples\nare sufficient to recover a private image. In this work, we show that\n$n_{\\mathsf{priv}} \\log ( n_{\\mathsf{priv}} )$ samples are sufficient\n(information-theoretically) for recovering all the private images.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:41:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Huang", "Baihe", ""], ["Song", "Zhao", ""], ["Tao", "Runzhou", ""], ["Zhang", "Ruizhe", ""], ["Zhuo", "Danyang", ""]]}, {"id": "2011.11907", "submitter": "Huan Hu", "authors": "Huan Hu and Jianzhong Li", "title": "Efficient Approximate Nearest Neighbor Search for Multiple Weighted\n  $l_{p\\leq2}$ Distance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search is fundamental to a wide range of applications. Since\nthe exact nearest neighbor search suffers from the \"curse of dimensionality\",\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\nused to trade a little query accuracy for a much higher query efficiency. In\nmany scenarios, it is necessary to perform nearest neighbor search under\nmultiple weighted distance functions in high-dimensional spaces. This paper\nconsiders the important problem of supporting efficient approximate nearest\nneighbor search for multiple weighted distance functions in high-dimensional\nspaces. To the best of our knowledge, prior work can only solve the problem for\nthe $l_2$ distance. However, numerous studies have shown that the $l_p$\ndistance with $p\\in(0,2)$ could be more effective than the $l_2$ distance in\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\nproblem for the $l_p$ distance for $p\\in(0,2]$. WLSH takes the LSH approach and\ncan theoretically guarantee both the efficiency of processing queries and the\naccuracy of query results while minimizing the required total number of hash\ntables. We conduct extensive experiments on synthetic and real data sets, and\nthe results show that WLSH achieves high performance in terms of query\nefficiency, query accuracy and space consumption.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 05:56:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hu", "Huan", ""], ["Li", "Jianzhong", ""]]}, {"id": "2011.11918", "submitter": "Debajyoti Bera", "authors": "Sagnik Chatterjee and Debajyoti Bera", "title": "Applying the Quantum Alternating Operator Ansatz to the Graph Matching\n  Problem", "comments": "9 pages, 3 figures, 3 charts", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Quantum Alternating Operator Ansatz (QAOA+) framework has recently gained\nattention due to its ability to solve discrete optimization problems on noisy\nintermediate-scale quantum (NISQ) devices in a manner that is amenable to\nderivation of worst-case guarantees. We design a technique in this framework to\ntackle a few problems over maximal matchings in graphs. Even though maximum\nmatching is polynomial-time solvable, most counting and sampling versions are\n#P-hard.\n  We design a few algorithms that generates superpositions over matchings\nallowing us to sample from them. In particular, we get a superposition over all\npossible matchings when given the empty state as input and a superposition over\nall maximal matchings when given the W -states as input.\n  Our main result is that the expected size of the matchings corresponding to\nthe output states of our QAOA+ algorithm when ran on a 2-regular graph is\ngreater than the expected matching size obtained from a uniform distribution\nover all matchings. This algorithm uses a W -state as input and we prove that\nthis input state is better compared to using the empty matching as the input\nstate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 06:36:11 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Chatterjee", "Sagnik", ""], ["Bera", "Debajyoti", ""]]}, {"id": "2011.12155", "submitter": "Panagiotis Lionakis", "authors": "Panagiotis Lionakis, Giorgos Kritikakis, Ioannis G. Tollis", "title": "Algorithms and Experiments Comparing Two Hierarchical Drawing Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present algorithms that extend the path-based hierarchical drawing\nframework and give experimental results. Our algorithms run in $O(km)$ time,\nwhere $k$ is the number of paths and $m$ is the number of edges of the graph,\nand provide better upper bounds than the original path based framework: e.g.,\nthe height of the resulting drawings is equal to the length of the longest path\nof $G$, instead of $n-1$, where $n$ is the number of nodes. Additionally, we\nextend this framework, by bundling and drawing all the edges of the DAG in $O(m\n+ n \\log n)$ time, using minimum extra width per path. We also provide some\ncomparison to a well known hierarchical drawing framework, widely known as the\nSugiyama framework, as a proof of concept. The experimental results show that\nour algorithms produce drawings that are better in area and number of bends,\nbut worse for crossings in sparse graphs. Hence, our technique offers an\ninteresting alternative for drawing hierarchical graphs. Finally, we present an\n$O(m + k \\log k)$ time algorithm that computes a specific order of the paths in\norder to reduce the total edge length and number of crossings and bends.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:12:56 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lionakis", "Panagiotis", ""], ["Kritikakis", "Giorgos", ""], ["Tollis", "Ioannis G.", ""]]}, {"id": "2011.12169", "submitter": "Sandip Banerjee", "authors": "Sandip Banerjee, Rafail Ostrovsky, Yuval Rabani", "title": "Min-Sum Clustering (with Outliers)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a constant factor polynomial time pseudo-approximation algorithm for\nmin-sum clustering with or without outliers. The algorithm is allowed to\nexclude an arbitrarily small constant fraction of the points. For instance, we\nshow how to compute a solution that clusters 98\\% of the input data points and\npays no more than a constant factor times the optimal solution that clusters\n99\\% of the input data points. More generally, we give the following bicriteria\napproximation: For any $\\eps > 0$, for any instance with $n$ input points and\nfor any positive integer $n'\\le n$, we compute in polynomial time a clustering\nof at least $(1-\\eps) n'$ points of cost at most a constant factor greater than\nthe optimal cost of clustering $n'$ points. The approximation guarantee grows\nwith $\\frac{1}{\\eps}$. Our results apply to instances of points in real space\nendowed with squared Euclidean distance, as well as to points in a metric\nspace, where the number of clusters, and also the dimension if relevant, is\narbitrary (part of the input, not an absolute constant).\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:45:20 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Banerjee", "Sandip", ""], ["Ostrovsky", "Rafail", ""], ["Rabani", "Yuval", ""]]}, {"id": "2011.12196", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Huy Tuan Pham, Thuy Duong Vuong", "title": "Towards the sampling Lov\\'asz Local Lemma", "comments": "22 pages; comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\Phi = (V, \\mathcal{C})$ be a constraint satisfaction problem on\nvariables $v_1,\\dots, v_n$ such that each constraint depends on at most $k$\nvariables and such that each variable assumes values in an alphabet of size at\nmost $[q]$. Suppose that each constraint shares variables with at most $\\Delta$\nconstraints and that each constraint is violated with probability at most $p$\n(under the product measure on its variables). We show that for $k, q = O(1)$,\nthere is a deterministic, polynomial time algorithm to approximately count the\nnumber of satisfying assignments and a randomized, polynomial time algorithm to\nsample from approximately the uniform distribution on satisfying assignments,\nprovided that \\[C\\cdot q^{3}\\cdot k \\cdot p \\cdot \\Delta^{7} < 1, \\quad\n\\text{where }C \\text{ is an absolute constant.}\\] Previously, a result of this\nform was known essentially only in the special case when each constraint is\nviolated by exactly one assignment to its variables.\n  For the special case of $k$-CNF formulas, the term $\\Delta^{7}$ improves the\npreviously best known $\\Delta^{60}$ for deterministic algorithms [Moitra,\nJ.ACM, 2019] and $\\Delta^{13}$ for randomized algorithms [Feng et al., arXiv,\n2020]. For the special case of properly $q$-coloring $k$-uniform hypergraphs,\nthe term $\\Delta^{7}$ improves the previously best known $\\Delta^{14}$ for\ndeterministic algorithms [Guo et al., SICOMP, 2019] and $\\Delta^{9}$ for\nrandomized algorithms [Feng et al., arXiv, 2020].\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 16:40:50 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jain", "Vishesh", ""], ["Pham", "Huy Tuan", ""], ["Vuong", "Thuy Duong", ""]]}, {"id": "2011.12433", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Nilesh Tripuraneni, Peter L. Bartlett,\n  Michael I. Jordan", "title": "Optimal Mean Estimation without a Variance", "comments": "Fixed typographical errors in Theorem 1.2, Lemmas 4.3 and C.8", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of heavy-tailed mean estimation in settings where the\nvariance of the data-generating distribution does not exist. Concretely, given\na sample $\\mathbf{X} = \\{X_i\\}_{i = 1}^n$ from a distribution $\\mathcal{D}$\nover $\\mathbb{R}^d$ with mean $\\mu$ which satisfies the following\n\\emph{weak-moment} assumption for some ${\\alpha \\in [0, 1]}$: \\begin{equation*}\n\\forall \\|v\\| = 1: \\mathbb{E}_{X \\thicksim \\mathcal{D}}[\\lvert \\langle X - \\mu,\nv\\rangle \\rvert^{1 + \\alpha}] \\leq 1, \\end{equation*} and given a target\nfailure probability, $\\delta$, our goal is to design an estimator which attains\nthe smallest possible confidence interval as a function of $n,d,\\delta$. For\nthe specific case of $\\alpha = 1$, foundational work of Lugosi and Mendelson\nexhibits an estimator achieving subgaussian confidence intervals, and\nsubsequent work has led to computationally efficient versions of this\nestimator. Here, we study the case of general $\\alpha$, and establish the\nfollowing information-theoretic lower bound on the optimal attainable\nconfidence interval: \\begin{equation*} \\Omega \\left(\\sqrt{\\frac{d}{n}} +\n\\left(\\frac{d}{n}\\right)^{\\frac{\\alpha}{(1 + \\alpha)}} + \\left(\\frac{\\log 1 /\n\\delta}{n}\\right)^{\\frac{\\alpha}{(1 + \\alpha)}}\\right). \\end{equation*}\nMoreover, we devise a computationally-efficient estimator which achieves this\nlower bound.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:39:21 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 20:31:46 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Tripuraneni", "Nilesh", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.12439", "submitter": "Spyros Angelopoulos", "authors": "Spyros Angelopoulos and Shahin Kamali", "title": "Contract Scheduling With Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contract scheduling is a general technique that allows to design a system\nwith interruptible capabilities, given an algorithm that is not necessarily\ninterruptible. Previous work on this topic has largely assumed that the\ninterruption is a worst-case deadline that is unknown to the scheduler. In this\nwork, we study the setting in which there is a potentially erroneous prediction\nconcerning the interruption. Specifically, we consider the setting in which the\nprediction describes the time that the interruption occurs, as well as the\nsetting in which the prediction is obtained as a response to a single or\nmultiple binary queries. For both settings, we investigate tradeoffs between\nthe robustness (i.e., the worst-case performance assuming adversarial\nprediction) and the consistency (i.e, the performance assuming that the\nprediction is error-free), both from the side of positive and negative results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:00:04 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Kamali", "Shahin", ""]]}, {"id": "2011.12465", "submitter": "Sunipa Dev", "authors": "Sunipa Dev", "title": "The Geometry of Distributed Representations for Better Alignment,\n  Attenuated Bias, and Improved Interpretability", "comments": "PhD thesis, University of Utah (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional representations for words, text, images, knowledge graphs\nand other structured data are commonly used in different paradigms of machine\nlearning and data mining. These representations have different degrees of\ninterpretability, with efficient distributed representations coming at the cost\nof the loss of feature to dimension mapping. This implies that there is\nobfuscation in the way concepts are captured in these embedding spaces. Its\neffects are seen in many representations and tasks, one particularly\nproblematic one being in language representations where the societal biases,\nlearned from underlying data, are captured and occluded in unknown dimensions\nand subspaces. As a result, invalid associations (such as different races and\ntheir association with a polar notion of good versus bad) are made and\npropagated by the representations, leading to unfair outcomes in different\ntasks where they are used. This work addresses some of these problems\npertaining to the transparency and interpretability of such representations. A\nprimary focus is the detection, quantification, and mitigation of socially\nbiased associations in language representation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:04:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Dev", "Sunipa", ""]]}, {"id": "2011.12500", "submitter": "Bin Sheng", "authors": "Bin Sheng", "title": "Solving the r-pseudoforest Deletion Problem in Time Independent of r", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The feedback vertex set problem is one of the most studied parameterized\nproblems. Several generalizations of the problem have been studied where one is\nto delete vertices to obtain graphs close to acyclic. In this paper, we give an\nFPT algorithm for the problem of deleting at most $k$ vertices to get an\n$r$-pseudoforest. A graph is an $r$-pseudoforest if we can delete at most $r$\nedges from each component to get a forest. Philip et al. introduced this\nproblem and gave an $O^*(c_{r}^{k})$ algorithm for it, where $c_r$ depends on\n$r$ double exponentially. In comparison, our algorithm runs in time\n$O^*((10k)^{k})$, independent of $r$.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 03:42:35 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sheng", "Bin", ""]]}, {"id": "2011.12633", "submitter": "Matan Kraus", "authors": "Stav Ben-Nun, Tsvi Kopelowitz, Matan Kraus, Ely Porat", "title": "An $O(\\log^{3/2}n)$ Parallel Time Population Protocol for Majority with\n  $O(\\log n)$ States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population protocols, the underlying distributed network consists of $n$\nnodes (or agents), denoted by $V$, and a scheduler that continuously selects\nuniformly random pairs of nodes to interact. When two nodes interact, their\nstates are updated by applying a state transition function that depends only on\nthe states of the two nodes prior to the interaction. The efficiency of a\npopulation protocol is measured in terms of both time (which is the number of\ninteractions until the nodes collectively have a valid output) and the number\nof possible states of nodes used by the protocol. By convention, we consider\nthe parallel time cost, which is the time divided by $n$.\n  In this paper we consider the majority problem, where each node receives as\ninput a color that is either black or white, and the goal is to have all of the\nnodes output the color that is the majority of the input colors. We design a\npopulation protocol that solves the majority problem in $O(\\log^{3/2}n)$\nparallel time, both with high probability and in expectation, while using\n$O(\\log n)$ states. Our protocol improves on a recent protocol of Berenbrink et\nal. that runs in $O(\\log^{5/3}n)$ parallel time, both with high probability and\nin expectation, using $O(\\log n)$ states.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:48:05 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ben-Nun", "Stav", ""], ["Kopelowitz", "Tsvi", ""], ["Kraus", "Matan", ""], ["Porat", "Ely", ""]]}, {"id": "2011.12742", "submitter": "Maxime Crochemore", "authors": "Golnaz Badkobeh and Maxime Crochemore", "title": "Left Lyndon tree construction", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the left-to-right Lyndon factorisation of a word to the left Lyndon\ntree construction of a Lyndon word. It yields an algorithm to sort the prefixes\nof a Lyndon word according to the infinite ordering defined by Dolce et al.\n(2019). A straightforward variant computes the left Lyndon forest of a word.\nAll algorithms run in linear time on a general alphabet, that is, in the\nletter-comparison model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:52:42 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Badkobeh", "Golnaz", ""], ["Crochemore", "Maxime", ""]]}, {"id": "2011.12823", "submitter": "Harold Nieuwboer", "authors": "Joran van Apeldoorn and Sander Gribling and Yinan Li and Harold\n  Nieuwboer and Michael Walter and Ronald de Wolf", "title": "Quantum algorithms for matrix scaling and matrix balancing", "comments": "62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix scaling and matrix balancing are two basic linear-algebraic problems\nwith a wide variety of applications, such as approximating the permanent, and\npre-conditioning linear systems to make them more numerically stable. We study\nthe power and limitations of quantum algorithms for these problems.\n  We provide quantum implementations of two classical (in both senses of the\nword) methods: Sinkhorn's algorithm for matrix scaling and Osborne's algorithm\nfor matrix balancing. Using amplitude estimation as our main tool, our quantum\nimplementations both run in time $\\tilde O(\\sqrt{mn}/\\varepsilon^4)$ for\nscaling or balancing an $n \\times n$ matrix (given by an oracle) with $m$\nnon-zero entries to within $\\ell_1$-error $\\varepsilon$. Their classical\nanalogs use time $\\tilde O(m/\\varepsilon^2)$, and every classical algorithm for\nscaling or balancing with small constant $\\varepsilon$ requires $\\Omega(m)$\nqueries to the entries of the input matrix. We thus achieve a polynomial\nspeed-up in terms of $n$, at the expense of a worse polynomial dependence on\nthe obtained $\\ell_1$-error $\\varepsilon$. We emphasize that even for constant\n$\\varepsilon$ these problems are already non-trivial (and relevant in\napplications).\n  Along the way, we extend the classical analysis of Sinkhorn's and Osborne's\nalgorithm to allow for errors in the computation of marginals. We also adapt an\nimproved analysis of Sinkhorn's algorithm for entrywise-positive matrices to\nthe $\\ell_1$-setting, leading to an $\\tilde O(n^{1.5}/\\varepsilon^3)$-time\nquantum algorithm for $\\varepsilon$-$\\ell_1$-scaling in this case.\n  We also prove a lower bound, showing that our quantum algorithm for matrix\nscaling is essentially optimal for constant $\\varepsilon$: every quantum\nalgorithm for matrix scaling that achieves a constant $\\ell_1$-error with\nrespect to uniform marginals needs to make at least $\\Omega(\\sqrt{mn})$\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:26:59 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["van Apeldoorn", "Joran", ""], ["Gribling", "Sander", ""], ["Li", "Yinan", ""], ["Nieuwboer", "Harold", ""], ["Walter", "Michael", ""], ["de Wolf", "Ronald", ""]]}, {"id": "2011.12898", "submitter": "Daniel Saad Nogueira Nunes", "authors": "Daniel S. N. Nunes and Felipe A. Louza and Simon Gog and Mauricio\n  Ayala-Rinc\\'on and Gonzalo Navarro", "title": "Grammar Compression By Induced Suffix Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A grammar compression algorithm, called GCIS, is introduced in this work.\nGCIS is based on the induced suffix sorting algorithm SAIS, presented by Nong\net al. in 2009. The proposed solution builds on the factorization performed by\nSAIS during suffix sorting. A context-free grammar is used to replace factors\nby non-terminals. The algorithm is then recursively applied on the shorter\nsequence of non-terminals. The resulting grammar is encoded by exploiting some\nredundancies, such as common prefixes between right-hands of rules, sorted\naccording to SAIS. GCIS excels for its low space and time required for\ncompression while obtaining competitive compression ratios. Our experiments on\nregular and repetitive, moderate and very large texts, show that GCIS stands as\na very convenient choice compared to well-known compressors such as Gzip,\n7-Zip, and RePair, the gold standard in grammar compression. In exchange, GCIS\nis slow at decompressing. Yet, grammar compressors are more convenient than\nLempel-Ziv compressors in that one can access text substrings directly in\ncompressed form, without ever decompressing the text. We demonstrate that GCIS\nis an excellent candidate for this scenario because it shows to be competitive\namong its RePair based alternatives. We also show, how GCIS relation with SAIS\nmakes it a good intermediate structure to build the suffix array and the LCP\narray during decompression of the text.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:26:46 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Nunes", "Daniel S. N.", ""], ["Louza", "Felipe A.", ""], ["Gog", "Simon", ""], ["Ayala-Rinc\u00f3n", "Mauricio", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "2011.13248", "submitter": "Vishwa Prakash HV", "authors": "Aadityan Ganesh, Vishwa Prakash HV, Prajakta Nimbhorkar and\n  Geevarghese Philip", "title": "Disjoint Stable Matchings in Linear Time", "comments": "Conference: International Workshop on Graph-Theoretic Concepts in\n  Computer Science 2021 (https://wg2021.mimuw.edu.pl)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We show that given a SM instance G as input we can find a largest collection\nof pairwise edge-disjoint stable matchings of G in time linear in the input\nsize. This extends two classical results:\n  1. The Gale-Shapley algorithm, which can find at most two (\"extreme\")\npairwise edge-disjoint stable matchings of G in linear time, and\n  2. The polynomial-time algorithm for finding a largest collection of pairwise\nedge-disjoint perfect matchings (without the stability requirement) in a\nbipartite graph, obtained by combining K\\\"{o}nig's characterization with\nTutte's f-factor algorithm.\n  Moreover, we also give an algorithm to enumerate all maximum-length chains of\ndisjoint stable matchings in the lattice of stable matchings of a given\ninstance. This algorithm takes time polynomial in the input size for\nenumerating each chain. We also derive the expected number of such chains in a\nrandom instance of Stable Matching.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:39:30 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 07:25:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ganesh", "Aadityan", ""], ["HV", "Vishwa Prakash", ""], ["Nimbhorkar", "Prajakta", ""], ["Philip", "Geevarghese", ""]]}, {"id": "2011.13426", "submitter": "Michael P. Kim", "authors": "Cynthia Dwork and Michael P. Kim and Omer Reingold and Guy N. Rothblum\n  and Gal Yona", "title": "Outcome Indistinguishability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction algorithms assign numbers to individuals that are popularly\nunderstood as individual \"probabilities\" -- what is the probability of 5-year\nsurvival after cancer diagnosis? -- and which increasingly form the basis for\nlife-altering decisions. Drawing on an understanding of computational\nindistinguishability developed in complexity theory and cryptography, we\nintroduce Outcome Indistinguishability. Predictors that are Outcome\nIndistinguishable yield a generative model for outcomes that cannot be\nefficiently refuted on the basis of the real-life observations produced by\nNature. We investigate a hierarchy of Outcome Indistinguishability definitions,\nwhose stringency increases with the degree to which distinguishers may access\nthe predictor in question. Our findings reveal that Outcome\nIndistinguishability behaves qualitatively differently than previously studied\nnotions of indistinguishability. First, we provide constructions at all levels\nof the hierarchy. Then, leveraging recently-developed machinery for proving\naverage-case fine-grained hardness, we obtain lower bounds on the complexity of\nthe more stringent forms of Outcome Indistinguishability. This hardness result\nprovides the first scientific grounds for the political argument that, when\ninspecting algorithmic risk prediction instruments, auditors should be granted\noracle access to the algorithm, not simply historical predictions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:33:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Dwork", "Cynthia", ""], ["Kim", "Michael P.", ""], ["Reingold", "Omer", ""], ["Rothblum", "Guy N.", ""], ["Yona", "Gal", ""]]}, {"id": "2011.13476", "submitter": "Adiel Statman", "authors": "Adiel Statman, Liat Rozenberg, Dan Feldman", "title": "Faster Projective Clustering Approximation of Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In projective clustering we are given a set of n points in $R^d$ and wish to\ncluster them to a set $S$ of $k$ linear subspaces in $R^d$ according to some\ngiven distance function. An $\\eps$-coreset for this problem is a weighted\n(scaled) subset of the input points such that for every such possible $S$ the\nsum of these distances is approximated up to a factor of $(1+\\eps)$. We suggest\nto reduce the size of existing coresets by suggesting the first $O(\\log(m))$\napproximation for the case of $m$ lines clustering in $O(ndm)$ time, compared\nto the existing $\\exp(m)$ solution. We then project the points on these lines\nand prove that for a sufficiently large $m$ we obtain a coreset for projective\nclustering. Our algorithm also generalize to handle outliers. Experimental\nresults and open code are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 21:04:41 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Statman", "Adiel", ""], ["Rozenberg", "Liat", ""], ["Feldman", "Dan", ""]]}, {"id": "2011.13485", "submitter": "Zhiyang He", "authors": "Zhiyang He, Jason Li, Magnus Wahlstr\\\"om", "title": "Near-linear-time, Optimal Vertex Cut Sparsifiers in Directed Acyclic\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $G$ be a graph and $S, T \\subseteq V(G)$ be (possibly overlapping) sets\nof terminals, $|S|=|T|=k$. We are interested in computing a vertex sparsifier\nfor terminal cuts in $G$, i.e., a graph $H$ on a smallest possible number of\nvertices, where $S \\cup T \\subseteq V(H)$ and such that for every $A \\subseteq\nS$ and $B \\subseteq T$ the size of a minimum $(A,B)$-vertex cut is the same in\n$G$ as in $H$. We assume that our graphs are unweighted and that terminals may\nbe part of the min-cut. In previous work, Kratsch and Wahlstr\\\"om (FOCS\n2012/JACM 2020) used connections to matroid theory to show that a vertex\nsparsifier $H$ with $O(k^3)$ vertices can be computed in randomized polynomial\ntime, even for arbitrary digraphs $G$. However, since then, no improvements on\nthe size $O(k^3)$ have been shown.\n  In this paper, we draw inspiration from the renowned Bollob\\'as's\nTwo-Families Theorem in extremal combinatorics and introduce the use of total\norderings into Kratsch and Wahlstr\\\"om's methods. This new perspective allows\nus to construct a sparsifier $H$ of $\\Theta(k^2)$ vertices for the case that\n$G$ is a DAG. We also show how to compute $H$ in time near-linear in the size\nof $G$, improving on the previous $O(n^{\\omega+1})$. Furthermore, $H$ recovers\nthe closest min-cut in $G$ for every partition $(A,B)$, which was not\npreviously known. Finally, we show that a sparsifier of size $\\Omega(k^2)$ is\nrequired, both for DAGs and for undirected edge cuts.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 22:39:34 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 05:19:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["He", "Zhiyang", ""], ["Li", "Jason", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "2011.13550", "submitter": "Pasin Manurangsi", "authors": "Surbhi Goel, Adam Klivans, Pasin Manurangsi, Daniel Reichman", "title": "Tight Hardness Results for Training Depth-2 ReLU Networks", "comments": "To appear in ITCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove several hardness results for training depth-2 neural networks with\nthe ReLU activation function; these networks are simply weighted sums (that may\ninclude negative coefficients) of ReLUs. Our goal is to output a depth-2 neural\nnetwork that minimizes the square loss with respect to a given training set. We\nprove that this problem is NP-hard already for a network with a single ReLU. We\nalso prove NP-hardness for outputting a weighted sum of $k$ ReLUs minimizing\nthe squared error (for $k>1$) even in the realizable setting (i.e., when the\nlabels are consistent with an unknown depth-2 ReLU network). We are also able\nto obtain lower bounds on the running time in terms of the desired additive\nerror $\\epsilon$. To obtain our lower bounds, we use the Gap Exponential Time\nHypothesis (Gap-ETH) as well as a new hypothesis regarding the hardness of\napproximating the well known Densest $\\kappa$-Subgraph problem in\nsubexponential time (these hypotheses are used separately in proving different\nlower bounds). For example, we prove that under reasonable hardness\nassumptions, any proper learning algorithm for finding the best fitting ReLU\nmust run in time exponential in $1/\\epsilon^2$. Together with a previous work\nregarding improperly learning a ReLU (Goel et al., COLT'17), this implies the\nfirst separation between proper and improper algorithms for learning a ReLU. We\nalso study the problem of properly learning a depth-2 network of ReLUs with\nbounded weights giving new (worst-case) upper bounds on the running time needed\nto learn such networks both in the realizable and agnostic settings. Our upper\nbounds on the running time essentially matches our lower bounds in terms of the\ndependency on $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 04:18:00 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""], ["Manurangsi", "Pasin", ""], ["Reichman", "Daniel", ""]]}, {"id": "2011.13569", "submitter": "Junichi Teruyama", "authors": "Tetsuya Fujie, Yuya Higashikawa, Naoki Katoh, Junichi Teruyama, Yuki\n  Tokuni", "title": "Minmax Regret 1-Sink Location Problems on Dynamic Flow Path Networks\n  with Parametric Weights", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the minmax regret 1-sink location problem on dynamic\nflow path networks with parametric weights. We are given a dynamic flow network\nconsisting of an undirected path with positive edge lengths, positive edge\ncapacities, and nonnegative vertex weights. A path can be considered as a road,\nan edge length as the distance along the road and a vertex weight as the number\nof people at the site. An edge capacity limits the number of people that can\nenter the edge per unit time. We consider the problem of locating a sink in the\nnetwork, to which all the people evacuate from the vertices as quickly as\npossible. In our model, each weight is represented by a linear function in a\ncommon parameter $t$, and the decision maker who determines the location of a\nsink does not know the value of $t$. We formulate the sink location problem\nunder such uncertainty as the minmax regret problem. Given $t$ and a sink\nlocation $x$, the cost of $x$ under $t$ is the sum of arrival times at $x$ for\nall the people determined by $t$. The regret for $x$ under $t$ is the gap\nbetween the cost of $x$ under $t$ and the optimal cost under $t$. The task of\nthe problem is formulated as the one to find a sink location that minimizes the\nmaximum regret over all $t$. For the problem, we propose an $O(n^4\n2^{\\alpha(n)} \\alpha(n) \\log n)$ time algorithm where $n$ is the number of\nvertices in the network and $\\alpha(\\cdot)$ is the inverse Ackermann function.\nAlso for the special case in which every edge has the same capacity, we show\nthat the complexity can be reduced to $O(n^3 2^{\\alpha(n)} \\alpha(n) \\log n)$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 05:57:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Fujie", "Tetsuya", ""], ["Higashikawa", "Yuya", ""], ["Katoh", "Naoki", ""], ["Teruyama", "Junichi", ""], ["Tokuni", "Yuki", ""]]}, {"id": "2011.13702", "submitter": "Maximilian Probst Gutenberg", "authors": "Maximilian Probst Gutenberg", "title": "Near-Optimal Algorithms for Reachability, Strongly-Connected Components\n  and Shortest Paths in Partially Dynamic Digraphs", "comments": "Doctoral thesis; abstract shortened to meet ArXiv character limit;\n  university logo omitted to avoid compile problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this thesis, we present new techniques to deal with fundamental\nalgorithmic graph problems where graphs are directed and partially dynamic,\ni.e. undergo either a sequence of edge insertions or deletions:\n  - Single-Source Reachability (SSR),\n  - Strongly-Connected Components (SCCs), and\n  - Single-Source Shortest Paths (SSSP).\n  These problems have recently received an extraordinary amount of attention\ndue to their role as subproblems in various more complex and notoriously hard\ngraph problems, especially to compute flows, bipartite matchings and cuts.\n  Our techniques lead to the first near-optimal data structures for these\nproblems in various different settings. Letting $n$ denote the number of\nvertices in the graph and by $m$ the maximum number of edges in any version of\nthe graph, we obtain\n  - the first randomized data structure to maintain SSR and SCCs in\nnear-optimal total update time $\\tilde{O}(m)$ in a graph undergoing edge\ndeletions.\n  - the first randomized data structure to maintain SSSP in partially dynamic\ngraphs in total update time $\\tilde{O}(n^2)$ which is near-optimal in dense\ngraphs.\n  - the first deterministic data structures for SSR and SCC for graphs\nundergoing edge deletions, and for SSSP in partially dynamic graphs that\nimprove upon the $O(mn)$ total update time by Even and Shiloach from 1981 that\nis often considered to be a fundamental barrier.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:35:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Gutenberg", "Maximilian Probst", ""]]}, {"id": "2011.13908", "submitter": "Will Ma", "authors": "Will Ma, Pan Xu, Yifan Xu", "title": "Group-level Fairness Maximization in Online Bipartite Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the allocation of limited resources to heterogeneous customers\nwho arrive in an online fashion. We would like to allocate the resources\n\"fairly\", so that no group of customers is marginalized in terms of their\noverall service rate. We study whether this is possible to do so in an online\nfashion, and if so, what a good online allocation policy is.\n  We model this problem using online bipartite matching under stationary\narrivals, a fundamental model in the literature typically studied under the\nobjective of maximizing the total number of customers served. We instead study\nthe objective of maximizing the minimum service rate across all groups, and\npropose two notions of fairness: long-run and short-run.\n  For these fairness objectives, we analyze how competitive online algorithms\ncan be, in comparison to offline algorithms which know the sequence of demands\nin advance. For long-run fairness, we propose two online heuristics (Sampling\nand Pooling) which establish asymptotic optimality in different regimes (no\nspecialized supplies, no rare demand types, or imbalanced supply/demand). By\ncontrast, outside all of these regimes, we show that the competitive ratio of\nonline algorithms is between 0.632 and 0.732. For short-run fairness, we show\nfor complete bipartite graphs that the competitive ratio of online algorithms\nis between 0.863 and 0.942; we also derive a probabilistic rejection algorithm\nwhich is asymptotically optimal in the total demand.\n  Depending on the overall scarcity of resources, either our Sampling or\nPooling heuristics could be desirable. The most difficult situation for online\nallocation occurs when the total supply is just enough to serve the total\ndemand.\n  We simulate our algorithms on a public ride-hailing dataset, which both\ndemonstrates the efficacy of our heuristics and validates our managerial\ninsights.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:48:22 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 04:07:10 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 16:27:27 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ma", "Will", ""], ["Xu", "Pan", ""], ["Xu", "Yifan", ""]]}, {"id": "2011.14075", "submitter": "Benjamin Laufer", "authors": "Benjamin Laufer", "title": "Feedback Effects in Repeat-Use Criminal Risk Assessments", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2005.13404", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the criminal legal context, risk assessment algorithms are touted as\ndata-driven, well-tested tools. Studies known as validation tests are typically\ncited by practitioners to show that a particular risk assessment algorithm has\npredictive accuracy, establishes legitimate differences between risk groups,\nand maintains some measure of group fairness in treatment. To establish these\nimportant goals, most tests use a one-shot, single-point measurement. Using a\nPolya Urn model, we explore the implication of feedback effects in sequential\nscoring-decision processes. We show through simulation that risk can propagate\nover sequential decisions in ways that are not captured by one-shot tests. For\nexample, even a very small or undetectable level of bias in risk allocation can\namplify over sequential risk-based decisions, leading to observable group\ndifferences after a number of decision iterations. Risk assessment tools\noperate in a highly complex and path-dependent process, fraught with historical\ninequity. We conclude from this study that these tools do not properly account\nfor compounding effects, and require new approaches to development and\nauditing.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 06:40:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Laufer", "Benjamin", ""]]}, {"id": "2011.14144", "submitter": "Spyros Angelopoulos", "authors": "Spyros Angelopoulos and Malachi Voss", "title": "Online Search with Maximum Clearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the setting in which a mobile agent must locate a hidden target in a\nbounded or unbounded environment, with no information about the hider's\nposition. In particular, we consider online search, in which the performance of\nthe search strategy is evaluated by its worst case competitive ratio. We\nintroduce a multi-criteria search problem in which the searcher has a budget on\nits allotted search time, and the objective is to design strategies that are\ncompetitively efficient, respect the budget, and maximize the total searched\nground. We give analytically optimal strategies for the line and the star\nenvironments, and efficient heuristics for general networks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 15:15:34 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Voss", "Malachi", ""]]}, {"id": "2011.14192", "submitter": "Amatya Sharma", "authors": "Palash Dey, Arnab Maiti and Amatya Sharma", "title": "On Parameterized Complexity of Liquid Democracy", "comments": "Submitted to 7th Annual International Conference on Algorithms and\n  Discrete Applied Mathematics [CALDAM 2021]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.CY cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In liquid democracy, each voter either votes herself or delegates her vote to\nsome other voter. This gives rise to what is called a delegation graph. To\ndecide the voters who eventually votes along with the subset of voters whose\nvotes they give, we need to resolve the cycles in the delegation graph. This\ngives rise to the Resolve Delegation problem where we need to find an acyclic\nsub-graph of the delegation graph such that the number of voters whose votes\nthey give is bounded above by some integer {\\lambda}. Putting a cap on the\nnumber of voters whose votes a voter gives enable the system designer restrict\nthe power of any individual voter. The Resolve Delegation problem is already\nknown to be NP-hard. In this paper we study the parameterized complexity of\nthis problem. We show that Resolve Delegation is para-NP-hard with respect to\nparameters {\\lambda}, number of sink nodes and the maximum degree of the\ndelegation graph. We also show that Resolve Delegation is W[1]-hard even with\nrespect to the treewidth of the delegation graph. We complement our negative\nresults by exhibiting FPT algorithms with respect to some other parameters. We\nfinally show that a related problem, which we call Resolve Fractional\nDelegation, is polynomial time solvable.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:48:22 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dey", "Palash", ""], ["Maiti", "Arnab", ""], ["Sharma", "Amatya", ""]]}, {"id": "2011.14291", "submitter": "Ramesh Krishnan S. Pallavoor", "authors": "Amit Levi, Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova and\n  Nithin Varma", "title": "Erasure-Resilient Sublinear-Time Graph Algorithms", "comments": "To appear in the 12th Innovations in Theoretical Computer Science\n  (ITCS) conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate sublinear-time algorithms that take partially erased graphs\nrepresented by adjacency lists as input. Our algorithms make degree and\nneighbor queries to the input graph and work with a specified fraction of\nadversarial erasures in adjacency entries. We focus on two computational tasks:\ntesting if a graph is connected or $\\varepsilon$-far from connected and\nestimating the average degree. For testing connectedness, we discover a\nthreshold phenomenon: when the fraction of erasures is less than $\\varepsilon$,\nthis property can be tested efficiently (in time independent of the size of the\ngraph); when the fraction of erasures is at least $\\varepsilon,$ then a number\nof queries linear in the size of the graph representation is required. Our\nerasure-resilient algorithm (for the special case with no erasures) is an\nimprovement over the previously known algorithm for connectedness in the\nstandard property testing model and has optimal dependence on the proximity\nparameter $\\varepsilon$. For estimating the average degree, our results provide\nan \"interpolation\" between the query complexity for this computational task in\nthe model with no erasures in two different settings: with only degree queries,\ninvestigated by Feige (SIAM J. Comput. `06), and with degree queries and\nneighbor queries, investigated by Goldreich and Ron (Random Struct. Algorithms\n`08) and Eden et al. (ICALP `17). We conclude with a discussion of our model\nand open questions raised by our work.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:37:17 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Levi", "Amit", ""], ["Pallavoor", "Ramesh Krishnan S.", ""], ["Raskhodnikova", "Sofya", ""], ["Varma", "Nithin", ""]]}, {"id": "2011.14450", "submitter": "Tal Elbaz", "authors": "Noah Br\\\"ustle, Tal Elbaz, Hamed Hatami, Onur Kocer, Bingchan Ma", "title": "Approximation algorithms for hitting subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $H$ be a fixed undirected graph on $k$ vertices. The $H$-hitting set\nproblem asks for deleting a minimum number of vertices from a given graph $G$\nin such a way that the resulting graph has no copies of $H$ as a subgraph. This\nproblem is a special case of the hypergraph vertex cover problem on $k$-uniform\nhypergraphs, and thus admits an efficient $k$-factor approximation algorithm.\nThe purpose of this article is to investigate the question that for which\ngraphs $H$ this trivial approximation factor $k$ can be improved.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 22:08:28 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Br\u00fcstle", "Noah", ""], ["Elbaz", "Tal", ""], ["Hatami", "Hamed", ""], ["Kocer", "Onur", ""], ["Ma", "Bingchan", ""]]}, {"id": "2011.14460", "submitter": "Noah Bertram", "authors": "Joshua Sobel, Noah Bertram, Chen Ding, Fatemeh Nargesian, Daniel\n  Gildea", "title": "AWLCO: All-Window Length Co-Occurrence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing patterns in a sequence of events has applications in text analysis,\ncomputer programming, and genomics research. In this paper, we consider the\nall-window-length analysis model which analyzes a sequence of events with\nrespect to windows of all lengths. We study the exact co-occurrence counting\nproblem for the all-window-length analysis model. Our first algorithm is an\noffline algorithm that counts all-window-length co-occurrences by performing\nmultiple passes over a sequence and computing single-window-length\nco-occurrences. This algorithm has the time complexity $O(n)$ for each window\nlength and thus a total complexity of $O(n^2)$ and the space complexity\n$O(|I|)$ for a sequence of size n and an itemset of size $|I|$. We propose\nAWLCO, an online algorithm that computes all-window-length co-occurrences in a\nsingle pass with the expected time complexity of $O(n)$ and space complexity of\n$O( \\sqrt{ n|I| })$. Following this, we generalize our use case to patterns in\nwhich we propose an algorithm that computes all-window-length co-occurrence\nwith expected time complexity $O(n|I|)$ and space complexity $O( \\sqrt{n|I|} +\ne_{max}|I|)$, where $e_{max}$ is the length of the largest pattern.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 22:57:02 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sobel", "Joshua", ""], ["Bertram", "Noah", ""], ["Ding", "Chen", ""], ["Nargesian", "Fatemeh", ""], ["Gildea", "Daniel", ""]]}, {"id": "2011.14532", "submitter": "Cyrus Rashtchian", "authors": "Konstantin Makarychev, Miklos Z. Racz, Cyrus Rashtchian, Sergey\n  Yekhanin", "title": "Batch Optimization for DNA Synthesis", "comments": "Improved Theorem 1.2 and its proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.CO math.IT math.PR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pools of synthetic DNA molecules have been recently used to reliably\nstore significant volumes of digital data. While DNA as a storage medium has\nenormous potential because of its high storage density, its practical use is\ncurrently severely limited because of the high cost and low throughput of\navailable DNA synthesis technologies. We study the role of batch optimization\nin reducing the cost of large scale DNA synthesis, which translates to the\nfollowing algorithmic task. Given a large pool $\\mathcal{S}$ of random\nquaternary strings of fixed length, partition $\\mathcal{S}$ into batches in a\nway that minimizes the sum of the lengths of the shortest common supersequences\nacross batches. We introduce two ideas for batch optimization that both improve\n(in different ways) upon a naive baseline: (1) using both $(ACGT)^{*}$ and its\nreverse $(TGCA)^{*}$ as reference strands, and batching appropriately, and (2)\nbatching via the quantiles of an appropriate ordering of the strands. We also\nprove asymptotically matching lower bounds on the cost of DNA synthesis,\nshowing that one cannot improve upon these two ideas. Our results uncover a\nsurprising separation between two cases that naturally arise in the context of\nDNA data storage: the asymptotic cost savings of batch optimization are\nsignificantly greater in the case where strings in $\\mathcal{S}$ do not contain\nrepeats of the same character (homopolymers), as compared to the case where\nstrings in $\\mathcal{S}$ are unconstrained.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 03:56:25 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 01:28:37 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Racz", "Miklos Z.", ""], ["Rashtchian", "Cyrus", ""], ["Yekhanin", "Sergey", ""]]}, {"id": "2011.14580", "submitter": "Thao Nguyen", "authors": "Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Thao Nguyen", "title": "Robust and Private Learning of Halfspaces", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the trade-off between differential privacy and\nadversarial robustness under L2-perturbations in the context of learning\nhalfspaces. We prove nearly tight bounds on the sample complexity of robust\nprivate learning of halfspaces for a large regime of parameters. A highlight of\nour results is that robust and private learning is harder than robust or\nprivate learning alone. We complement our theoretical analysis with\nexperimental results on the MNIST and USPS datasets, for a learning algorithm\nthat is both differentially private and adversarially robust.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 06:59:20 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 23:20:21 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Ghazi", "Badih", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""], ["Nguyen", "Thao", ""]]}, {"id": "2011.14722", "submitter": "Shivam Garg", "authors": "Moses Charikar, Shivam Garg, Deborah M. Gordon, Kirankumar Shiragur", "title": "A Model for Ant Trail Formation and its Convergence Properties", "comments": "ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS nlin.AO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for ant trail formation, building upon previous work on\nbiologically feasible local algorithms that plausibly describe how ants\nmaintain trail networks. The model is a variant of a reinforced random walk on\na directed graph, where ants lay pheromone on edges as they traverse them and\nthe next edge to traverse is chosen based on the pheromone level; this\npheromone decays with time. There is a bidirectional flow of ants: the forward\nflow proceeds along forward edges from source (e.g. the nest) to sink (e.g. a\nfood source), and the backward flow in the opposite direction. Some fraction of\nants are lost as they pass through each node (modeling the loss of ants due to\nexploration). We initiate a theoretical study of this model.\n  We first consider the linear decision rule, where the flow divides itself\namong the next set of edges in proportion to their pheromone level. Here, we\nshow that the process converges to the path with minimum leakage when the\nforward and backward flows do not change over time. When the forward and\nbackward flows increase over time (caused by positive reinforcement from the\ndiscovery of a food source, for example), we show that the process converges to\nthe shortest path. These results are for graphs consisting of two parallel\npaths (a case that has been investigated before in experiments). Through\nsimulations, we show that these results hold for more general graphs drawn from\nvarious random graph models. Further, we consider a general family of decision\nrules, and show that there is no advantage of using a non-linear rule from this\nfamily, if the goal is to find the shortest or the minimum leakage path. We\nalso show that bidirectional flow is necessary for convergence to such paths.\nOur results provide a plausible explanation for field observations, and open up\nnew avenues for further theoretical and experimental investigation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:02:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Charikar", "Moses", ""], ["Garg", "Shivam", ""], ["Gordon", "Deborah M.", ""], ["Shiragur", "Kirankumar", ""]]}, {"id": "2011.14730", "submitter": "Daniel Neuen", "authors": "Daniel Neuen", "title": "Isomorphism Testing for Graphs Excluding Small Topological Subgraphs", "comments": "37 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2004.07671", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an isomorphism test that runs in time $n^{\\operatorname{polylog}(h)}$\non all $n$-vertex graphs excluding some $h$-vertex vertex graph as a\ntopological subgraph. Previous results state that isomorphism for such graphs\ncan be tested in time $n^{\\operatorname{polylog}(n)}$ (Babai, STOC 2016) and\n$n^{f(h)}$ for some function $f$ (Grohe and Marx, SIAM J. Comp., 2015).\n  Our result also unifies and extends previous isomorphism tests for graphs of\nmaximum degree $d$ running in time $n^{\\operatorname{polylog}(d)}$ (FOCS 2018)\nand for graphs of Hadwiger number $h$ running in time\n$n^{\\operatorname{polylog}(h)}$ (FOCS 2020).\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:14:08 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Neuen", "Daniel", ""]]}, {"id": "2011.14801", "submitter": "Guilherme de Castro Mendes Gomes", "authors": "Guilherme C. M. Gomes and Vinicius F. dos Santos", "title": "On structural parameterizations of the selective coloring problem", "comments": "14 pages, 2 figures. Submitted to LAGOS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Selective Coloring problem, we are given an integer $k$, a graph $G$,\nand a partition of $V(G)$ into $p$ parts, and the goal is to decide whether or\nnot we can pick exactly one vertex of each part and obtain a $k$-colorable\ninduced subgraph of $G$. This generalization of Vertex Coloring has only\nrecently begun to be studied by Demange et al. [Theoretical Computer Science,\n2014], motivated by scheduling problems on distributed systems, with Guo et al.\n[TAMC, 2020] discussing the first results on the parameterized complexity of\nthe problem. In this work, we study multiple structural parameterizations for\nSelective Coloring. We begin by revisiting the many hardness results of Demange\net al. and show how they may be used to provide intractability proofs for\nwidely used parameters such as pathwidth, distance to co-cluster, and max leaf\nnumber. Afterwards, we present fixed-parameter tractability algorithms when\nparameterizing by distance to cluster, or under the joint parameterizations\ntreewidth and number of parts, and co-treewidth and number of parts. Our main\ncontribution is a proof that, for every fixed $k \\geq 1$, Selective Coloring\ndoes not admit a polynomial kernel when jointly parameterized by the vertex\ncover number and the number of parts, which implies that Multicolored\nIndependent Set does not admit a polynomial kernel under the same\nparameterization.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:00:17 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gomes", "Guilherme C. M.", ""], ["Santos", "Vinicius F. dos", ""]]}, {"id": "2011.14849", "submitter": "Guilherme de Castro Mendes Gomes", "authors": "M\\'arcia R. Cappelle, Guilherme C. M. Gomes, Vinicius F. dos Santos", "title": "Parameterized algorithms for locating-dominating sets", "comments": "25 pages, 18 figures. Submitted to LAGOS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A locating-dominating set $D$ of a graph $G$ is a dominating set of $G$ where\neach vertex not in $D$ has a unique neighborhood in $D$, and the\nLocating-Dominating Set problem asks if $G$ contains such a dominating set of\nbounded size. This problem is known to be $\\mathsf{NP-hard}$ even on restricted\ngraph classes, such as interval graphs, split graphs, and planar bipartite\nsubcubic graphs. On the other hand, it is known to be solvable in polynomial\ntime for some graph classes, such as trees and, more generally, graphs of\nbounded cliquewidth. While these results have numerous implications on the\nparameterized complexity of the problem, little is known in terms of\nkernelization under structural parameterizations. In this work, we begin\nfilling this gap in the literature. Our first result shows that\nLocating-Dominating Set is $\\mathsf{W}[1]-\\mathsf{hard}$ when parameterized by\nthe size of a minimum clique cover. We present an exponential kernel for the\ndistance to cluster parameterization and show that, unless $\\mathsf{NP}\n\\subseteq \\mathsf{coNP/poly}$, no polynomial kernel exists for\nLocating-Dominating Set when parameterized by vertex cover nor when\nparameterized by distance to clique. We then turn our attention to parameters\nnot bounded by either of the previous two, and exhibit a linear kernel when\nparameterizing by the max leaf number; in this context, we leave the\nparameterization by feedback edge set as the primary open problem in our study.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:42:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cappelle", "M\u00e1rcia R.", ""], ["Gomes", "Guilherme C. M.", ""], ["Santos", "Vinicius F. dos", ""]]}, {"id": "2011.14929", "submitter": "Albert Zuo", "authors": "William Marshall, Nolan Miranda, Albert Zuo", "title": "Windowed Prophet Inequalities", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prophet inequalities problem has received significant study over the past\ndecades and has several applications such as to online auctions. In this paper,\nwe study two variants of the i.i.d. prophet inequalities problem, namely the\nwindowed prophet inequalities problem and the batched prophet inequalities\nproblem. For the windowed prophet inequalities problem, we show that for window\nsize $o(n)$, the optimal competitive ratio is $\\alpha \\approx 0.745$, the same\nas in the non-windowed case. In the case where the window size is $n/k$ for\nsome constant $k$, we show that $\\alpha_k < WIN_{n/k} \\le \\alpha_k + o_k(1)$\nwhere $WIN_{n/k}$ is the optimal competitive ratio for the window size $n/k$\nprophet inequalities problem and $\\alpha_k$ is the optimal competitive ratio\nfor the $k$ sample i.i.d. prophet inequalities problem. Finally, we prove an\nequivalence between the batched prophet inequalities problem and the i.i.d.\nprophet inequalities problem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:23:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Marshall", "William", ""], ["Miranda", "Nolan", ""], ["Zuo", "Albert", ""]]}, {"id": "2011.15019", "submitter": "Jes\\'us Garc\\'ia D\\'iaz", "authors": "Jes\\'us Garc\\'ia D\\'iaz", "title": "A simple approximation algorithm for the graph burning problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The graph burning problem is an NP-Hard optimization problem that may be used\nto model social contagion and the sequential spread of information on simple\ngraphs. This paper introduces a simple approximation algorithm for the graph\nburning problem over general graphs. The approximation factor of this algorithm\nis 3-2/b(G), where b(G) is the size of an optimal solution. The proposed\nalgorithm is based on farthest-first traversal, it is easy to implement, and\naccording to a brief empirical analysis, it generates competitive solutions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:23:06 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 23:22:08 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 22:30:54 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["D\u00edaz", "Jes\u00fas Garc\u00eda", ""]]}, {"id": "2011.15101", "submitter": "Yang P. Liu", "authors": "Yang P. Liu", "title": "Vertex Sparsification for Edge Connectivity in Polynomial Time", "comments": "16 pages, changed license", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important open question in the area of vertex sparsification is whether\n$(1+\\epsilon)$-approximate cut-preserving vertex sparsifiers with size close to\nthe number of terminals exist. The work Chalermsook et al. (SODA 2021)\nintroduced a relaxation called connectivity-$c$ mimicking networks, which asks\nto construct a vertex sparsifier which preserves connectivity among $k$\nterminals exactly up to the value of $c$, and showed applications to dynamic\nconnectivity data structures and survivable network design. We show that\nconnectivity-$c$ mimicking networks with $\\widetilde{O}(kc^3)$ edges exist and\ncan be constructed in polynomial time in $n$ and $c$, improving over the\nresults of Chalermsook et al. (SODA 2021) for any $c \\ge \\log n$, whose\nruntimes depended exponentially on $c$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:33:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:53:50 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Liu", "Yang P.", ""]]}]