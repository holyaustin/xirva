[{"id": "1812.00059", "submitter": "Carlos Cardonha", "authors": "David Bergman, Carlos Cardonha, Saharnaz Mehrani", "title": "Binary Decision Diagrams for Bin Packing with Minimum Color\n  Fragmentation", "comments": "10 pages, 2 figures, 1 table, currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin Packing with Minimum Color Fragmentation (BPMCF) is an extension of the\nBin Packing Problem in which each item has a size and a color and the goal is\nto minimize the sum of the number of bins containing items of each color. In\nthis work, we introduce BPMCF and present a decomposition strategy to solve the\nproblem, where the assignment of items to bins is formulated as a binary\ndecision diagram and an optimal integrated solutions is identified through a\nmixed-integer linear programming model. Our computational experiments show that\nthe proposed approach greatly outperforms a direct formulation of BPMCF and\nthat its performance is suitable for large instances of the problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 21:12:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Bergman", "David", ""], ["Cardonha", "Carlos", ""], ["Mehrani", "Saharnaz", ""]]}, {"id": "1812.00134", "submitter": "Manish Purohit", "authors": "Ravi Kumar, Manish Purohit, Aaron Schild, Zoya Svitkina, Erik Vee", "title": "Semi-Online Bipartite Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the \\emph{semi-online} model that generalizes the\nclassical online computational model. The semi-online model postulates that the\nunknown future has a predictable part and an adversarial part; these parts can\nbe arbitrarily interleaved. An algorithm in this model operates as in the\nstandard online model, i.e., makes an irrevocable decision at each step.\n  We consider bipartite matching in the semi-online model, for both integral\nand fractional cases. Our main contributions are competitive algorithms for\nthis problem that are close to or match a hardness bound. The competitive ratio\nof the algorithms nicely interpolates between the truly offline setting (no\nadversarial part) and the truly online setting (no predictable part).\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 03:29:06 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 21:28:37 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Kumar", "Ravi", ""], ["Purohit", "Manish", ""], ["Schild", "Aaron", ""], ["Svitkina", "Zoya", ""], ["Vee", "Erik", ""]]}, {"id": "1812.00241", "submitter": "Lin Yang", "authors": "Vladimir Braverman, Robert Krauthgamer, Lin F. Yang", "title": "Universal Streaming of Subset Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most known algorithms in the streaming model of computation aim to\napproximate a single function such as an $\\ell_p$-norm. In 2009, Nelson\n[\\url{https://sublinear.info}, Open Problem 30] asked if it possible to design\n\\emph{universal algorithms}, that simultaneously approximate multiple functions\nof the stream. In this paper we answer the question of Nelson for the class of\n\\emph{subset $\\ell_0$-norms} in the insertion-only frequency-vector model.\nGiven a family of subsets $\\mathcal{S}\\subset 2^{[n]}$, we provide a single\nstreaming algorithm that can $(1\\pm \\epsilon)$-approximate the subset-norm for\nevery $S\\in\\mathcal{S}$. Here, the subset-$\\ell_p$-norm of $v\\in \\mathbb{R}^n$\nwith respect to set $S\\subseteq [n]$ is the $\\ell_p$-norm of vector $v_{|S}$\n(which denotes restricting $v$ to $S$, by zeroing all other coordinates).\n  Our main result is a near-tight characterization of the space complexity of\nevery family $\\mathcal{S}\\subset 2^{[n]}$ of subset-$\\ell_0$-norms in\ninsertion-only streams, expressed in terms of the \"heavy-hitter dimension\" of\n$\\mathcal{S}$, a new combinatorial quantity that is related to the VC-dimension\nof $\\mathcal{S}$. In contrast, we show that the more general turnstile and\nsliding-window models require a much larger space usage. All these results\neasily extend to $\\ell_1$.\n  In addition, we design algorithms for two other subset-$\\ell_p$-norm\nvariants. These can be compared to the Priority Sampling algorithm of Duffield,\nLund and Thorup [JACM 2007], which achieves additive approximation\n$\\epsilon\\|{v}\\|$ for all possible subsets ($\\mathcal{S}=2^{[n]}$) in the\nentry-wise update model. One of our algorithms extends this algorithm to handle\nturnstile updates, and another one achieves multiplicative approximation given\na family $\\mathcal{S}$.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 19:08:06 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 05:04:09 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 03:32:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Braverman", "Vladimir", ""], ["Krauthgamer", "Robert", ""], ["Yang", "Lin F.", ""]]}, {"id": "1812.00359", "submitter": "Shay Golan", "authors": "Or Birenzwige, Shay Golan and Ely Porat", "title": "Locally Consistent Parsing for Text Indexing in Small Space", "comments": "Extended abstract to appear is SODA 2020", "journal-ref": null, "doi": "10.1137/1.9781611975994.37", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two closely related problems of text indexing in a sub-linear\nworking space. The first problem is the Sparse Suffix Tree (SST) construction\nof a set of suffixes $B$ using only $O(|B|)$ words of space. The second problem\nis the Longest Common Extension (LCE) problem, where for some parameter\n$1\\le\\tau\\le n$, the goal is to construct a data structure that uses $O(\\frac\n{n}{\\tau})$ words of space and can compute the longest common prefix length of\nany pair of suffixes. We show how to use ideas based on the Locally Consistent\nParsing technique, that was introduced by Sahinalp and Vishkin [STOC '94], in\nsome non-trivial ways in order to improve the known results for the above\nproblems. We introduce new Las-Vegas and deterministic algorithms for both\nproblems.\n  We introduce the first Las-Vegas SST construction algorithm that takes $O(n)$\ntime. This is an improvement over the last result of Gawrychowski and Kociumaka\n[SODA '17] who obtained $O(n)$ time for Monte-Carlo algorithm, and\n$O(n\\sqrt{\\log |B|})$ time for Las-Vegas algorithm. In addition, we introduce a\nrandomized Las-Vegas construction for an LCE data structure that can be\nconstructed in linear time and answers queries in $O(\\tau)$ time.\n  For the deterministic algorithms, we introduce an SST construction algorithm\nthat takes $O(n\\log \\frac{n}{|B|})$ time (for $|B|=\\Omega(\\log n)$). This is\nthe first almost linear time, $O(n\\cdot poly\\log{n})$, deterministic SST\nconstruction algorithm, where all previous algorithms take at least\n$\\Omega\\left(\\min\\{n|B|,\\frac{n^2}{|B|}\\}\\right)$ time. For the LCE problem, we\nintroduce a data structure that answers LCE queries in $O(\\tau\\sqrt{\\log^*n})$\ntime, with $O(n\\log\\tau)$ construction time (for $\\tau=O(\\frac{n}{\\log n})$).\nThis data structure improves both query time and construction time upon the\nresults of Tanimura et al. [CPM '16].\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 09:11:20 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 10:10:44 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Birenzwige", "Or", ""], ["Golan", "Shay", ""], ["Porat", "Ely", ""]]}, {"id": "1812.00369", "submitter": "Hao-Ting Wei", "authors": "Hao-Ting Wei, Sung-Hsien Hsieh, Wen-Liang Hwang, Chung-Shou Liao,\n  Chun-Shien Lu", "title": "Link Delay Estimation Using Sparse Recovery for Dynamic Network\n  Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the scale of communication networks has been growing rapidly in the past\ndecades, it becomes a critical challenge to extract fast and accurate\nestimation of key state parameters of network links, e.g., transmission delays\nand dropped packet rates, because such monitoring operations are usually\ntime-consuming. Based on the sparse recovery technique reported in [Wang et al.\n(2015) IEEE Trans. Information Theory, 61(2):1028--1044], which can infer link\ndelays from a limited number of measurements using compressed sensing, we\nparticularly extend to networks with dynamic changes including link insertion\nand deletion. Moreover, we propose a more efficient algorithm with a better\ntheoretical upper bound. The experimental result also demonstrates that our\nalgorithm outperforms the previous work in running time while maintaining a\nsimilar recovery performance, which shows its capability to cope with\nlarge-scale networks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 11:22:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wei", "Hao-Ting", ""], ["Hsieh", "Sung-Hsien", ""], ["Hwang", "Wen-Liang", ""], ["Liao", "Chung-Shou", ""], ["Lu", "Chun-Shien", ""]]}, {"id": "1812.00421", "submitter": "Simone Faro", "authors": "Domenico Cantone, Simone Faro and Arianna Pavone", "title": "Sequence Searching Allowing for Non-Overlapping Adjacent Unbalanced\n  Translocations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unbalanced translocations are among the most frequent chromosomal\nalterations, accounted for 30\\% of all losses of heterozygosity, a major\ngenetic event causing inactivation of tumor suppressor genes. Despite of their\ncentral role in genomic sequence analysis, little attention has been devoted to\nthe problem of matching sequences allowing for this kind of chromosomal\nalteration. In this paper we investigate the \\emph{approximate string matching}\nproblem when the edit operations are non-overlapping unbalanced translocations\nof adjacent factors. In particular, we first present a $O(nm^3)$-time and\n$O(m^2)$-space algorithm based on the dynamic-programming approach. Then we\nimprove our first result by designing a second solution which makes use of the\nDirected Acyclic Word Graph of the pattern. In particular, we show that under\nthe assumptions of equiprobability and independence of characters, our\nalgorithm has a $O(n\\log^2_{\\sigma} m)$ average time complexity, for an\nalphabet of size $\\sigma$, still maintaining the $O(nm^3)$-time and the\n$O(m^2)$-space complexity in the worst case. To the best of our knowledge this\nis the first solution in literature for the approximate string matching problem\nallowing for unbalanced translocations of factors.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 16:39:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Cantone", "Domenico", ""], ["Faro", "Simone", ""], ["Pavone", "Arianna", ""]]}, {"id": "1812.00793", "submitter": "Holden Lee", "authors": "Rong Ge, Holden Lee, Andrej Risteski", "title": "Simulated Tempering Langevin Monte Carlo II: An Improved Proof using\n  Soft Markov Chain Decomposition", "comments": "69 pages. arXiv admin note: text overlap with arXiv:1710.02736", "journal-ref": "Advances in Neural Information Processing Systems 31 (2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key task in Bayesian machine learning is sampling from distributions that\nare only specified up to a partition function (i.e., constant of\nproportionality). One prevalent example of this is sampling posteriors in\nparametric distributions, such as latent-variable generative models. However\nsampling (even very approximately) can be #P-hard.\n  Classical results going back to Bakry and \\'Emery (1985) on sampling focus on\nlog-concave distributions, and show a natural Markov chain called Langevin\ndiffusion mixes in polynomial time. However, all log-concave distributions are\nuni-modal, while in practice it is very common for the distribution of interest\nto have multiple modes. In this case, Langevin diffusion suffers from torpid\nmixing.\n  We address this problem by combining Langevin diffusion with simulated\ntempering. The result is a Markov chain that mixes more rapidly by\ntransitioning between different temperatures of the distribution. We analyze\nthis Markov chain for a mixture of (strongly) log-concave distributions of the\nsame shape. In particular, our technique applies to the canonical multi-modal\ndistribution: a mixture of gaussians (of equal variance). Our algorithm\nefficiently samples from these distributions given only access to the gradient\nof the log-pdf.\n  For the analysis, we introduce novel techniques for proving spectral gaps\nbased on decomposing the action of the generator of the diffusion. Previous\napproaches rely on decomposing the state space as a partition of sets, while\nour approach can be thought of as decomposing the stationary measure as a\nmixture of distributions (a \"soft partition\").\n  Additional materials for the paper can be found at\nhttp://holdenlee.github.io/Simulated%20tempering%20Langevin%20Monte%20Carlo.html.\nThe proof and results have been improved and generalized from the precursor at\narXiv:1710.02736.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 19:27:33 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 15:39:09 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 12:44:05 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Holden", ""], ["Risteski", "Andrej", ""]]}, {"id": "1812.00966", "submitter": "Dirk Sudholt", "authors": "Dirk Sudholt", "title": "Analysing the Robustness of Evolutionary Algorithms to Noise: Refined\n  Runtime Bounds and an Example Where Noise is Beneficial", "comments": "This is an extended version of a paper that appeared in the\n  Proceedings of the Genetic and Evolutionary Computation Conference (GECCO\n  2018), https://doi.org/10.1145/3205455.3205595", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the performance of well-known evolutionary algorithms (1+1)EA and\n(1+$\\lambda$)EA in the prior noise model, where in each fitness evaluation the\nsearch point is altered before evaluation with probability $p$. We present\nrefined results for the expected optimisation time of the (1+1)EA and the\n(1+$\\lambda$)EA on the function LeadingOnes, where bits have to be optimised in\nsequence. Previous work showed that the (1+1)EA on LeadingOnes runs in\npolynomial expected time if $p = O((\\log n)/n^2)$ and needs superpolynomial\nexpected time if $p = \\omega((\\log n)/n)$, leaving a huge gap for which no\nresults were known. We close this gap by showing that the expected optimisation\ntime is $\\Theta(n^2) \\cdot \\exp(\\Theta(\\min\\{pn^2, n\\}))$ for all $p \\le 1/2$,\nallowing for the first time to locate the threshold between polynomial and\nsuperpolynomial expected times at $p = \\Theta((\\log n)/n^2)$. Hence the (1+1)EA\non LeadingOnes is much more sensitive to noise than previously thought. We also\nshow that offspring populations of size $\\lambda \\ge 3.42\\log n$ can\neffectively deal with much higher noise than known before.\n  Finally, we present an example of a rugged landscape where prior noise can\nhelp to escape from local optima by blurring the landscape and allowing a hill\nclimber to see the underlying gradient. We prove that in this particular\nsetting noise can have a highly beneficial effect on performance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 18:41:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Sudholt", "Dirk", ""]]}, {"id": "1812.01115", "submitter": "Cristian Rusu", "authors": "Cristian Rusu", "title": "On learning with shift-invariant structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe new results and algorithms for two different, but related,\nproblems which deal with circulant matrices: learning shift-invariant\ncomponents from training data and calculating the shift (or alignment) between\ntwo given signals. In the first instance, we deal with the shift-invariant\ndictionary learning problem while the latter bears the name of (compressive)\nshift retrieval. We formulate these problems using circulant and convolutional\nmatrices (including unions of such matrices), define optimization problems that\ndescribe our goals and propose efficient ways to solve them. Based on these\nfindings, we also show how to learn a wavelet-like dictionary from training\ndata. We connect our work with various previous results from the literature and\nwe show the effectiveness of our proposed algorithms using synthetic, ECG\nsignals and images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 22:31:47 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 21:00:39 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Rusu", "Cristian", ""]]}, {"id": "1812.01241", "submitter": "Xiaofu Ma", "authors": "Xiaofu Ma, Qinghai Gao, Vuk Marojevic, Jeffrey H. Reed", "title": "Hypergraph matching for MU-MIMO user grouping in wireless LANs", "comments": "Ad Hoc Networks, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the user grouping problem of downlink wireless local\narea networks (WLANs) with multi-user MIMO (MU-MIMO). Particularly, we focus on\nthe problem of whether single user transmit beamforming (SU-TxBF) or MU-MIMO\nshould be utilized, and how many users and which users should be in a\nmulti-user (MU) group. We formulate the problem for maximizing the system\nthroughput subject to the multi-user air time fairness (MU-ATF) criterion. We\nshow that hypergraphs provide a suitable mathematical model and effective tool\nfor finding the optimal or close to optimal solution. We show that the optimal\ngrouping problem can be solved efficiently for the case where only SU-TxBF and\n2-user MU groups are allowed in the system. For the general case, where any\nnumber of users can be assigned to groups of different sizes, we develop an\nefficient graph matching algorithm (GMA) based on graph theory principles. We\nevaluate the proposed algorithm in terms of system throughput using an 802.11ac\nemulator, which is created using collected channel measurements from an indoor\nenvironment and simulated channel samples for outdoor scenarios. We show that\nour GMA achieves at least 93% of the optimal system throughput in all\nconsidered test cases.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 06:27:22 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Ma", "Xiaofu", ""], ["Gao", "Qinghai", ""], ["Marojevic", "Vuk", ""], ["Reed", "Jeffrey H.", ""]]}, {"id": "1812.01343", "submitter": "Paolo Penna", "authors": "Cong Chen and Paolo Penna and Yinfeng Xu", "title": "Online scheduling of jobs with favorite machines", "comments": null, "journal-ref": null, "doi": "10.1016/j.cor.2019.104868", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a natural variant of the online machine scheduling\nproblem on unrelated machines, which we refer to as the favorite machine model.\nIn this model, each job has a minimum processing time on a certain set of\nmachines, called favorite machines, and some longer processing times on other\nmachines. This type of costs (processing times) arise quite naturally in many\npractical problems. In the online version, jobs arrive one by one and must be\nallocated irrevocably upon each arrival without knowing the future jobs. We\nconsider online algorithms for allocating jobs in order to minimize the\nmakespan.\n  We obtain tight bounds on the competitive ratio of the greedy algorithm and\ncharacterize the optimal competitive ratio for the favorite machine model. Our\nbounds generalize the previous results of the greedy algorithm and the optimal\nalgorithm for the unrelated machines and the identical machines. We also study\na further restriction of the model, called the symmetric favorite machine\nmodel, where the machines are partitioned equally into two groups and each job\nhas one of the groups as favorite machines. We obtain a 2.675-competitive\nalgorithm for this case, and the best possible algorithm for the two machines\ncase.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 11:28:36 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Chen", "Cong", ""], ["Penna", "Paolo", ""], ["Xu", "Yinfeng", ""]]}, {"id": "1812.01450", "submitter": "Massimo Cafaro", "authors": "Marco Pulimeno, Italo Epicoco, Massimo Cafaro", "title": "Distributed mining of time--faded heavy hitters", "comments": "arXiv admin note: text overlap with arXiv:1806.06580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\textsc{P2PTFHH} (Peer--to--Peer Time--Faded Heavy Hitters) which,\nto the best of our knowledge, is the first distributed algorithm for mining\ntime--faded heavy hitters on unstructured P2P networks. \\textsc{P2PTFHH} is\nbased on the \\textsc{FDCMSS} (Forward Decay Count--Min Space-Saving) sequential\nalgorithm, and efficiently exploits an averaging gossip protocol, by merging in\neach interaction the involved peers' underlying data structures. We formally\nprove the convergence and correctness properties of our distributed algorithm\nand show that it is fast and simple to implement. Extensive experimental\nresults confirm that \\textsc{P2PTFHH} retains the extreme accuracy and error\nbound provided by \\textsc{FDCMSS} whilst showing excellent scalability. Our\ncontributions are three-fold: (i) we prove that the averaging gossip protocol\ncan be used jointly with our augmented sketch data structure for mining\ntime--faded heavy hitters; (ii) we prove the error bounds on frequency\nestimation; (iii) we experimentally prove that \\textsc{P2PTFHH} is extremely\naccurate and fast, allowing near real time processing of large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 20:53:16 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""], ["Cafaro", "Massimo", ""]]}, {"id": "1812.01459", "submitter": "N.S Narayanaswamy", "authors": "S.M.Dhannya, N.S. Narayanaswamy", "title": "Conflict-Free Colouring using Maximum Independent Set and Minimum\n  Colouring", "comments": "arXiv admin note: text overlap with arXiv:1707.05071", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypergraph $H$, the conflict-free colouring problem is to colour\nvertices of $H$ using minimum colours so that each hyperedge in $H$ sees a\nunique colour. We present a polynomial time reduction from the conflict-free\ncolouring problem in hypergraphs to the maximum independent set problem in a\nclass of simple graphs, which we refer to as \\textit{conflict graphs}. We also\npresent another characterization of the conflict-free colouring number in terms\nof the chromatic number of graphs in an associated family of simple graphs,\nwhich we refer to as \\textit{co-occurrence graphs}. We present perfectness\nresults for co-occurrence graphs and a special case of conflict graphs. Based\non these results and a linear program that returns an integer solution in\npolynomial time, we obtain a polynomial time algorithm to compute a minimum\nconflict-free colouring of interval hypergraphs, thus solving an open problem\ndue to Cheilaris et al.\\cite{CPLGARSS2014}. Finally, we use the co-occurrence\ngraph characterization to prove that for an interval hypergraph, the\nconflict-free colouring number is the minimum partition of its intervals into\nsets such that each set has an exact hitting set (a hitting set in which each\ninterval is hit exactly once).\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 14:46:13 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 16:33:11 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 05:03:13 GMT"}], "update_date": "2020-01-02", "authors_parsed": [["Dhannya", "S. M.", ""], ["Narayanaswamy", "N. S.", ""]]}, {"id": "1812.01467", "submitter": "Madelon de Kemp", "authors": "Madelon A. de Kemp, Michel Mandjes, Neil Olver", "title": "Performance of the smallest-variance-first rule in appointment\n  sequencing", "comments": "54 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical problem in appointment scheduling, with applications in health\ncare, concerns the determination of the patients' arrival times that minimize a\ncost function that is a weighted sum of mean waiting times and mean idle times.\nOne aspect of this problem is the sequencing problem, which focuses on ordering\nthe patients. We assess the performance of the smallest-variance-first (SVF)\nrule, which sequences patients in order of increasing variance of their service\ndurations. While it was known that SVF is not always optimal, it has been\nwidely observed that it performs well in practice and simulation. We provide a\ntheoretical justification for this observation by proving, in various settings,\nquantitative worst-case bounds on the ratio between the cost incurred by the\nSVF rule and the minimum attainable cost. We also show that, in great\ngenerality, SVF is asymptotically optimal, i.e., the ratio approaches 1 as the\nnumber of patients grows large. While evaluating policies by considering an\napproximation ratio is a standard approach in many algorithmic settings, our\nresults appear to be the first of this type in the appointment scheduling\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 14:53:43 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 08:34:15 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 09:12:55 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 09:54:40 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["de Kemp", "Madelon A.", ""], ["Mandjes", "Michel", ""], ["Olver", "Neil", ""]]}, {"id": "1812.01482", "submitter": "Rogers Mathew", "authors": "Suman Banerjee, Rogers Mathew, and Fahad Panolan", "title": "Target Set Selection parameterized by vertex cover and more", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple, undirected graph $G$ with a threshold function $\\tau:V(G)\n\\rightarrow \\mathbb{N}$, the \\textsc{Target Set Selection} (TSS) problem is\nabout choosing a minimum cardinality set, say $S \\subseteq V(G)$, such that\nstarting a diffusion process with $S$ as its seed set will eventually result in\nactivating all the nodes in $G$. For any non-negative integer $i$, we say a set\n$T\\subseteq V(G)$ is a \"degree-$i$ modulator\" of $G$ if the degree of any\nvertex in the graph $G-T$ is at most $i$. Degree-$0$ modulators of a graph are\nprecisely its vertex covers. Consider a graph $G$ on $n$ vertices and $m$\nedges. We have the following results on the TSS problem:\n  -> It was shown by Nichterlein et al. [Social Network Analysis and Mining,\n2013] that it is possible to compute an optimal-sized target set in\n$O(2^{(2^{t}+1)t}\\cdot m)$ time, where $t$ denotes the cardinality of a minimum\ndegree-$0$ modulator of $G$. We improve this result by designing an algorithm\nrunning in time $2^{O(t\\log t)}n^{O(1)}$.\n  -> We design a $2^{2^{O(t)}}n^{O(1)}$ time algorithm to compute an optimal\ntarget set for $G$, where $t$ is the size of a minimum degree-$1$ modulator of\n$G$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:23:48 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:55:23 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 05:04:11 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 07:51:30 GMT"}, {"version": "v5", "created": "Sun, 16 May 2021 12:55:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Banerjee", "Suman", ""], ["Mathew", "Rogers", ""], ["Panolan", "Fahad", ""]]}, {"id": "1812.01591", "submitter": "Huy Nguyen", "authors": "Alina Ene, Huy L. Nguyen, Adrian Vladu", "title": "A Parallel Double Greedy Algorithm for Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parallel algorithms for the problem of maximizing a non-negative\nsubmodular function. Our main result is an algorithm that achieves a\nnearly-optimal $1/2 -\\epsilon$ approximation using $O(\\log(1/\\epsilon) /\n\\epsilon)$ parallel rounds of function evaluations. Our algorithm is based on a\ncontinuous variant of the double greedy algorithm of Buchbinder et al. that\nachieves the optimal $1/2$ approximation in the sequential setting. Our\nalgorithm applies more generally to the problem of maximizing a continuous\ndiminishing-returns (DR) function.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:47:09 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Vladu", "Adrian", ""]]}, {"id": "1812.01602", "submitter": "Omer Gold", "authors": "Keerti Choudhary, Omer Gold", "title": "Diameter Spanners, Eccentricity Spanners, and Approximating Extremal\n  Distances", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diameter of a graph is one if its most important parameters, being used\nin many real-word applications. In particular, the diameter dictates how fast\ninformation can spread throughout data and communication networks. Thus, it is\na natural question to ask how much can we sparsify a graph and still guarantee\nthat its diameter remains preserved within an approximation $t$. This property\nis captured by the notion of extremal-distance spanners. Given a graph\n$G=(V,E)$, a subgraph $H=(V,E_H)$ is defined to be a $t$-diameter spanner if\nthe diameter of $H$ is at most $t$ times the diameter of $G$.\n  We show that for any $n$-vertex and $m$-edges directed graph $G$, we can\ncompute a sparse subgraph $H$ that is a $(1.5)$-diameter spanner of $G$, such\nthat $H$ contains at most $\\tilde O(n^{1.5})$ edges. We also show that the\nstretch factor cannot be improved to $(1.5-\\epsilon)$. For a graph whose\ndiameter is bounded by some constant, we show the existence of\n$\\frac{5}{3}$-diameter spanner that contains at most $\\tilde O(n^\\frac{4}{3})$\nedges. We also show that this bound is tight.\n  Additionally, we present other types of extremal-distance spanners, such as\n$2$-eccentricity spanners and $2$-radius spanners, both contain only $\\tilde\nO(n)$ edges and are computable in $\\tilde O(m)$ time.\n  Finally, we study extremal-distance spanners in the dynamic and\nfault-tolerant settings. An interesting implication of our work is the first\n$\\tilde O(m)$-time algorithm for computing $2$-approximation of vertex\neccentricities in general directed weighted graphs. Backurs et al. [STOC 2018]\ngave an $\\tilde O(m\\sqrt{n})$ time algorithm for this problem, and also showed\nthat no $O(n^{2-o(1)})$ time algorithm can achieve an approximation factor\nbetter than $2$ for graph eccentricities, unless SETH fails; this shows that\nour approximation factor is essentially tight.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:57:18 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 15:51:53 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Choudhary", "Keerti", ""], ["Gold", "Omer", ""]]}, {"id": "1812.01768", "submitter": "Rohan Ghuge", "authors": "Rohan Ghuge and Viswanath Nagarajan", "title": "Quasi-Polynomial Algorithms for Submodular Tree Orienteering and Other\n  Directed Network Design Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following general network design problem on directed graphs.\nThe input is an asymmetric metric $(V,c)$, root $r^{*}\\in V$, monotone\nsubmodular function $f:2^V\\rightarrow \\mathbb{R}_+$ and budget $B$. The goal is\nto find an $r^{*}$-rooted arborescence $T$ of cost at most $B$ that maximizes\n$f(T)$. Our main result is a simple quasi-polynomial time $O(\\frac{\\log\nk}{\\log\\log k})$-approximation algorithm for this problem, where $k\\le |V|$ is\nthe number of vertices in an optimal solution. To the best of our knowledge,\nthis is the first non-trivial approximation ratio for this problem. As a\nconsequence we obtain an $O(\\frac{\\log^2 k}{\\log\\log k})$-approximation\nalgorithm for directed (polymatroid) Steiner tree in quasi-polynomial time. We\nalso extend our main result to a setting with additional length bounds at\nvertices, which leads to improved $O(\\frac{\\log^2 k}{\\log\\log\nk})$-approximation algorithms for the single-source buy-at-bulk and priority\nSteiner tree problems. For the usual directed Steiner tree problem, our result\nmatches the best previous approximation ratio [GLL19]. Our algorithm has the\nadvantage of being deterministic and faster: the runtime is $\\exp(O(\\log n\\,\n\\log^{1+\\epsilon} k))$. For polymatroid Steiner tree and single-source\nbuy-at-bulk, our result improves prior approximation ratios by a logarithmic\nfactor. For directed priority Steiner tree, our result seems to be the first\nnon-trivial approximation ratio. All our approximation ratios are tight (up to\nconstant factors) for quasi-polynomial algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 01:07:31 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 19:17:38 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ghuge", "Rohan", ""], ["Nagarajan", "Viswanath", ""]]}, {"id": "1812.01789", "submitter": "Andrew Lucas", "authors": "Andrew Lucas", "title": "Hard combinatorial problems and minor embeddings on lattice graphs", "comments": "26+7 pages; 9+1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, hardware constraints are an important limitation on quantum adiabatic\noptimization algorithms. Firstly, computational problems must be formulated as\nquadratic unconstrained binary optimization (QUBO) in the presence of noisy\ncoupling constants. Secondly, the interaction graph of the QUBO must have an\neffective minor embedding into a two-dimensional nonplanar lattice graph. We\ndescribe new strategies for constructing QUBOs for NP-complete/hard\ncombinatorial problems that address both of these challenges. Our results\ninclude asymptotically improved embeddings for number partitioning, filling\nknapsacks, graph coloring, and finding Hamiltonian cycles. These embeddings can\nbe also be found with reduced computational effort. Our new embedding for\nnumber partitioning may be more effective on next-generation hardware.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 02:34:28 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lucas", "Andrew", ""]]}, {"id": "1812.01844", "submitter": "Jaiyam Sharma", "authors": "Jaiyam Sharma and Saket Navlakha", "title": "Improving Similarity Search with High-dimensional Locality-sensitive\n  Hashing", "comments": "12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new class of data-independent locality-sensitive hashing (LSH)\nalgorithms based on the fruit fly olfactory circuit. The fundamental difference\nof this approach is that, instead of assigning hashes as dense points in a low\ndimensional space, hashes are assigned in a high dimensional space, which\nenhances their separability. We show theoretically and empirically that this\nnew family of hash functions is locality-sensitive and preserves rank\nsimilarity for inputs in any `p space. We then analyze different variations on\nthis strategy and show empirically that they outperform existing LSH methods\nfor nearest-neighbors search on six benchmark datasets. Finally, we propose a\nmulti-probe version of our algorithm that achieves higher performance for the\nsame query time, or conversely, that maintains performance of prior approaches\nwhile taking significantly less indexing time and memory. Overall, our approach\nleverages the advantages of separability provided by high-dimensional spaces,\nwhile still remaining computationally efficient\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 07:41:53 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Sharma", "Jaiyam", ""], ["Navlakha", "Saket", ""]]}, {"id": "1812.01852", "submitter": "Du\\v{s}an Knop", "authors": "Du\\v{s}an Knop and Martin Kouteck\\'y and Matthias Mnich", "title": "Voting and Bribing in Single-Exponential Time", "comments": "extended abstract appeared in proceedings of STACS 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2017.46", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general problem about bribery in voting systems. In the\n$\\mathcal{R}$-Multi-Bribery problem, the goal is to bribe a set of voters at\nminimum cost such that a desired candidate wins the perturbed election under\nthe voting rule R. Voters assign prices for withdrawing their vote, for\nswapping the positions of two consecutive candidates in their preference order,\nand for perturbing their approval count to favour candidates.\n  As our main result, we show that $\\mathcal{R}$-Multi-Bribery is\nfixed-parameter tractable parameterized by the number of candidates for many\nnatural voting rules $\\mathcal{R}$, including Kemeny rule, all scoring\nprotocols, maximin rule, Bucklin rule, fallback rule, SP-AV, and any C1 rule.\nIn particular, our result resolves the parameterized complexity of\n$\\mathcal{R}$-Swap Bribery for all those voting rules, thereby solving a\nlong-standing open problem and \"Challenge #2\" of the \"Nine Research Challenges\nin Computational Social Choice\" by Bredereck et al.\n  Further, our algorithm runs in single-exponential time for arbitrary cost; it\nthus improves the earlier double-exponential time algorithm by Dorn and\nSchlotter that is restricted to the uniform-cost case for all scoring\nprotocols, the maximin rule, and Bucklin rule.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 08:28:14 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""], ["Mnich", "Matthias", ""]]}, {"id": "1812.02023", "submitter": "Andrew McGregor", "authors": "Kook Jin Ahn, Graham Cormode, Sudipto Guha, Andrew McGregor, Anthony\n  Wirth", "title": "Correlation Clustering in Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental tool for analyzing large data sets. A rich body\nof work has been devoted to designing data-stream algorithms for the relevant\noptimization problems such as $k$-center, $k$-median, and $k$-means. Such\nalgorithms need to be both time and and space efficient. In this paper, we\naddress the problem of correlation clustering in the dynamic data stream model.\nThe stream consists of updates to the edge weights of a graph on $n$ nodes and\nthe goal is to find a node-partition such that the end-points of\nnegative-weight edges are typically in different clusters whereas the\nend-points of positive-weight edges are typically in the same cluster. We\npresent polynomial-time, $O(n\\cdot \\ \\mbox{polylog}~n)$-space approximation\nalgorithms for natural problems that arise.\n  We first develop data structures based on linear sketches that allow the\n\"quality\" of a given node-partition to be measured. We then combine these data\nstructures with convex programming and sampling techniques to solve the\nrelevant approximation problem. Unfortunately, the standard LP and SDP\nformulations are not obviously solvable in $O(n\\cdot \\mbox{polylog}~n)$-space.\nOur work presents space-efficient algorithms for the convex programming\nrequired, as well as approaches to reduce the adaptivity of the sampling.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 14:47:58 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ahn", "Kook Jin", ""], ["Cormode", "Graham", ""], ["Guha", "Sudipto", ""], ["McGregor", "Andrew", ""], ["Wirth", "Anthony", ""]]}, {"id": "1812.02056", "submitter": "Crist\\'obal Camarero", "authors": "Crist\\'obal Camarero", "title": "Simple, Fast and Practicable Algorithms for Cholesky, LU and QR\n  Decomposition Using Fast Rectangular Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents fast Cholesky/LU/QR decomposition algorithms with\n$O(n^{2.529})$ time complexity when using the fastest known matrix\nmultiplication. The algorithms have potential application, since a quickly made\nimplementation using Strassen multiplication has lesser execution time than the\nemployed by the GNU Scientific Library for the same task in at least a few\nexamples.\n  The underlaying ideas are very simple. Despite this, I have been unable to\nfind these methods in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 15:46:21 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Camarero", "Crist\u00f3bal", ""]]}, {"id": "1812.02144", "submitter": "Aram Harrow", "authors": "Elizabeth Crosson, Aram W. Harrow", "title": "Rapid mixing of path integral Monte Carlo for 1D stoquastic Hamiltonians", "comments": "26 pages, 2 figures, version published in Quantum", "journal-ref": "Quantum 5, 395 (2021)", "doi": "10.22331/q-2021-02-11-395", "report-no": "MIT-CTP/5286", "categories": "quant-ph cond-mat.stat-mech cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Path integral quantum Monte Carlo (PIMC) is a method for estimating thermal\nequilibrium properties of stoquastic quantum spin systems by sampling from a\nclassical Gibbs distribution using Markov chain Monte Carlo. The PIMC method\nhas been widely used to study the physics of materials and for simulated\nquantum annealing, but these successful applications are rarely accompanied by\nformal proofs that the Markov chains underlying PIMC rapidly converge to the\ndesired equilibrium distribution. In this work we analyze the mixing time of\nPIMC for 1D stoquastic Hamiltonians, including disordered transverse Ising\nmodels (TIM) with long-range algebraically decaying interactions as well as\ndisordered XY spin chains with nearest-neighbor interactions. By bounding the\nconvergence time to the equilibrium distribution we rigorously justify the use\nof PIMC to approximate partition functions and expectations of observables for\nthese models at inverse temperatures that scale at most logarithmically with\nthe number of qubits. The mixing time analysis is based on the canonical paths\nmethod applied to the single-site Metropolis Markov chain for the Gibbs\ndistribution of 2D classical spin models with couplings related to the\ninteractions in the quantum Hamiltonian. Since the system has strongly\nnonisotropic couplings that grow with system size, it does not fall into the\nknown cases where 2D classical spin models are known to mix rapidly.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 18:02:01 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:02:29 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 19:28:13 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Crosson", "Elizabeth", ""], ["Harrow", "Aram W.", ""]]}, {"id": "1812.02160", "submitter": "Bruno Messias Farias de Resende", "authors": "Bruno Messias and Luciano da F. Costa", "title": "Characterization and space embedding of directed graphs and social\n  networks through magnetic Laplacians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though commonly found in the real world, directed networks have received\nrelatively less attention from the literature in which concerns their\ntopological and dynamical characteristics. In this work, we develop a magnetic\nLaplacian-based framework that can be used for studying directed complex\nnetworks. More specifically, we introduce a specific heat measurement that can\nhelp to characterize the network topology. It is shown that, by using this\napproach, it is possible to identify the types of several networks, as well as\nto infer parameters underlying specific network configurations. Then, we\nconsider the dynamics associated with the magnetic Laplacian as a means of\nembedding networks into a metric space, allowing the identification of\nmesoscopic structures in artificial networks or unravel the polarization on\npolitical blogosphere. By defining a coarse-graining procedure in this metric\nspace, we show how to connect the specific heat measurement and the positions\nof nodes in this space.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 18:55:09 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 13:29:01 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Messias", "Bruno", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1812.02237", "submitter": "Matias Siebert", "authors": "Matias Siebert, Shabbir Ahmed, and George Nemhauser", "title": "A Linear Programming Based Approach to the Steiner Tree Problem with a\n  Fixed Number of Terminals", "comments": null, "journal-ref": null, "doi": "10.1002/net.21913", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of integer programs (IPs) for the Steiner tree problem with\nthe property that the best solution obtained by solving all, provides an\noptimal Steiner tree. Each IP is polynomial in the size of the underlying graph\nand our main result is that the linear programming (LP) relaxation of each IP\nis integral so that it can be solved as a linear program. However, the number\nof IPs grows exponentially with the number of terminals in the Steiner tree. As\na consequence, we are able to solve the Steiner tree problem by solving a\npolynomial number of LPs, when the number of terminals is fixed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 21:44:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Siebert", "Matias", ""], ["Ahmed", "Shabbir", ""], ["Nemhauser", "George", ""]]}, {"id": "1812.02363", "submitter": "Muhammad Farhan", "authors": "Muhammad Farhan, Qing Wang, Yu Lin and Brendan Mckay", "title": "A Highly Scalable Labelling Approach for Exact Distance Queries in\n  Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering exact shortest path distance queries is a fundamental task in graph\ntheory. Despite a tremendous amount of research on the subject, there is still\nno satisfactory solution that can scale to billion-scale complex networks.\nLabelling-based methods are well-known for rendering fast response time to\ndistance queries; however, existing works can only construct labelling on\nmoderately large networks (million-scale) and cannot scale to large networks\n(billion-scale) due to their prohibitively large space requirements and very\nlong preprocessing time. In this work, we present novel techniques to\nefficiently construct distance labelling and process exact shortest path\ndistance queries for complex networks with billions of vertices and billions of\nedges. Our method is based on two ingredients: (i) a scalable labelling\nalgorithm for constructing minimal distance labelling, and (ii) a querying\nframework that supports fast distance-bounded search on a sparsified graph.\nThus, we first develop a novel labelling algorithm that can scale to graphs at\nthe billion-scale. Then, we formalize a querying framework for exact distance\nqueries, which combines our proposed highway cover distance labelling with\ndistance-bounded searches to enable fast distance computation. To speed up the\nlabelling construction process, we further propose a parallel labelling method\nthat can construct labelling simultaneously for multiple landmarks. We\nevaluated the performance of the proposed methods on 12 real-world networks.\nThe experiments show that the proposed methods can not only handle networks\nwith billions of vertices, but also be up to 70 times faster in constructing\nlabelling and save up to 90\\% of labelling space. In particular, our method can\nanswer distance queries on a billion-scale network of around 8B edges in less\nthan 1ms, on average.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 06:04:21 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 02:04:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Farhan", "Muhammad", ""], ["Wang", "Qing", ""], ["Lin", "Yu", ""], ["Mckay", "Brendan", ""]]}, {"id": "1812.02507", "submitter": "Lutz Oettershagen", "authors": "Petra Mutzel and Lutz Oettershagen", "title": "On the Enumeration and Counting of Bicriteria Temporal Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the complexity of path enumeration and counting in weighted\ntemporal graphs. In a weighted temporal graph, each edge has an availability\ntime, a traversal time and some real cost. We introduce two bicriteria temporal\nmin-cost path problems in which we are interested in the set of all efficient\npaths with low cost and short duration or early arrival time, respectively.\nHowever, the number of efficient paths can be exponential in the size of the\ninput. For the case of strictly positive edge costs we are able to provide\nalgorithms that enumerate the set of efficient paths with polynomial time delay\nand polynomial space. If we are only interested in the set of Pareto-optimal\nsolutions and not in the paths themselves, then these can be determined in\npolynomial time if all edge costs are nonnegative. In addition, for each\nPareto-optimal solution, we are able to find an efficient path in polynomial\ntime. On the negative side, we prove that counting the number of efficient\npaths is #P-complete, even in the non-weighted single criterion case.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:06:55 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 08:27:53 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Mutzel", "Petra", ""], ["Oettershagen", "Lutz", ""]]}, {"id": "1812.02565", "submitter": "Longfei Wang", "authors": "Xinhang Zhang, Haoyuan Hu, Longfei Wang, Zhijun Sun, Ying Zhang,\n  Kunpeng Han, Yinghui Xu", "title": "A Novel Bin Design Problem and High Performance Algorithm for E-commerce\n  Logistics System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing cost accounts for a large part of the e-commerce logistics cost.\nMining the patterns of customer orders and designing suitable packing bins help\nto reduce operating cost. In the classical bin packing problem, a given set of\ncuboid-shaped items should be packed into bins with given and fixed-sizes\n(length, width and height) to minimize the number of bins that are used.\nHowever, a novel bin design problem is proposed in this paper. The decision\nvariables are the geometric sizes of bins, and the objective is to minimize the\ntotal surface area. To solve the problem, a low computational-complexity,\nhigh-performance heuristic algorithm based on dynamic programming and\ndepth-first tree search, named DPTS, is developed. Based on real historical\ndata that are collected from logistics scenario, numerical experiments show\nthat the DPTS out-performed 5.8% than the greedy local search (GLS) algorithm\nin the total cost. What's more, DPTS algorithm requires only about 1/50 times\nof the computational resources compared to the GLS algorithm. This demonstrates\nthat DPTS algorithm is very efficient in bin design problem and can help\nlogistics companies to make appropriate design.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 05:26:23 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zhang", "Xinhang", ""], ["Hu", "Haoyuan", ""], ["Wang", "Longfei", ""], ["Sun", "Zhijun", ""], ["Zhang", "Ying", ""], ["Han", "Kunpeng", ""], ["Xu", "Yinghui", ""]]}, {"id": "1812.02570", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "Compression with wildcards: Abstract simplicial complexes", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the more handy terminology of abstract simplicial complexes SC, in\nits core this article is about antitone Boolean functions. Given the maximal\nfaces (=facets) of SC, our main algorithm, called Facets-To-Faces, outputs SC\nin a compressed format. The degree of compression of Facets-To-Faces, which is\nprogrammed in high-level Mathematica code, compares favorably to both the\nMathematica command BooleanConvert, and to the BDD's provided by Python. A\nnovel way to calculate the face-numbers from the facets is also presented. Both\nalgorithms can be parallelized and are applicable (e.g.) to reliability\nanalysis, combinatorial topology, and frequent set mining.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 14:51:25 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 18:14:23 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 21:15:00 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1812.02603", "submitter": "Tobias Christiani", "authors": "Tobias Christiani, Rasmus Pagh, Mikkel Thorup", "title": "Confirmation Sampling for Exact Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing (LSH), introduced by Indyk and Motwani in STOC\n'98, has been an extremely influential framework for nearest neighbor search in\nhigh-dimensional data sets. While theoretical work has focused on the\napproximate nearest neighbor problems, in practice LSH data structures with\nsuitably chosen parameters are used to solve the exact nearest neighbor problem\n(with some error probability). Sublinear query time is often possible in\npractice even for exact nearest neighbor search, intuitively because the\nnearest neighbor tends to be significantly closer than other data points.\nHowever, theory offers little advice on how to choose LSH parameters outside of\npre-specified worst-case settings.\n  We introduce the technique of confirmation sampling for solving the exact\nnearest neighbor problem using LSH. First, we give a general reduction that\ntransforms a sequence of data structures that each find the nearest neighbor\nwith a small, unknown probability, into a data structure that returns the\nnearest neighbor with probability $1-\\delta$, using as few queries as possible.\nSecond, we present a new query algorithm for the LSH Forest data structure with\n$L$ trees that is able to return the exact nearest neighbor of a query point\nwithin the same time bound as an LSH Forest of $\\Omega(L)$ trees with internal\nparameters specifically tuned to the query and data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:31:53 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Christiani", "Tobias", ""], ["Pagh", "Rasmus", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1812.02696", "submitter": "Saeed Sharifi-Malvajerdi", "authors": "Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron\n  Roth, Saeed Sharifi-Malvajerdi, Jonathan Ullman", "title": "Differentially Private Fair Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by settings in which predictive models may be required to be\nnon-discriminatory with respect to certain attributes (such as race), but even\ncollecting the sensitive attribute may be forbidden or restricted, we initiate\nthe study of fair learning under the constraint of differential privacy. We\ndesign two learning algorithms that simultaneously promise differential privacy\nand equalized odds, a 'fairness' condition that corresponds to equalizing false\npositive and negative rates across protected groups. Our first algorithm is a\nprivate implementation of the equalized odds post-processing approach of [Hardt\net al., 2016]. This algorithm is appealingly simple, but must be able to use\nprotected group membership explicitly at test time, which can be viewed as a\nform of 'disparate treatment'. Our second algorithm is a differentially private\nversion of the oracle-efficient in-processing approach of [Agarwal et al.,\n2018] that can be used to find the optimal fair classifier, given access to a\nsubroutine that can solve the original (not necessarily fair) learning problem.\nThis algorithm is more complex but need not have access to protected group\nmembership at test time. We identify new tradeoffs between fairness, accuracy,\nand privacy that emerge only when requiring all three properties, and show that\nthese tradeoffs can be milder if group membership may be used at test time. We\nconclude with a brief experimental evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:24:52 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 16:19:19 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 18:01:49 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Jagielski", "Matthew", ""], ["Kearns", "Michael", ""], ["Mao", "Jieming", ""], ["Oprea", "Alina", ""], ["Roth", "Aaron", ""], ["Sharifi-Malvajerdi", "Saeed", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1812.02715", "submitter": "Dingkang Wang", "authors": "Dingkang Wang, Yusu Wang", "title": "An Improved Cost Function for Hierarchical Cluster Trees", "comments": "32 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering has been a popular method in various data analysis\napplications. It partitions a data set into a hierarchical collection of\nclusters, and can provide a global view of (cluster) structure behind data\nacross different granularity levels. A hierarchical clustering (HC) of a data\nset can be naturally represented by a tree, called a HC-tree, where leaves\ncorrespond to input data and subtrees rooted at internal nodes correspond to\nclusters. Many hierarchical clustering algorithms used in practice are\ndeveloped in a procedure manner. Dasgupta proposed to study the hierarchical\nclustering problem from an optimization point of view, and introduced an\nintuitive cost function for similarity-based hierarchical clustering with nice\nproperties as well as natural approximation algorithms.\n  We observe that while Dasgupta's cost function is effective at\ndifferentiating a good HC-tree from a bad one for a fixed graph, the value of\nthis cost function does not reflect how well an input similarity graph is\nconsistent to a hierarchical structure. In this paper, we present a new cost\nfunction, which is developed based on Dasgupta's cost function, to address this\nissue. The optimal tree under the new cost function remains the same as the one\nunder Dasgupta's cost function. However, the value of our cost function is more\nmeaningful. The new way of formulating the cost function also leads to a\npolynomial time algorithm to compute the optimal cluster tree when the input\ngraph has a perfect HC-structure, or an approximation algorithm when the input\ngraph 'almost' has a perfect HC-structure. Finally, we provide further\nunderstanding of the new cost function by studying its behavior for random\ngraphs sampled from an edge probability matrix.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:50:35 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 20:08:47 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Wang", "Dingkang", ""], ["Wang", "Yusu", ""]]}, {"id": "1812.02841", "submitter": "Alex Wang", "authors": "Gary L. Miller, Noel J. Walkington, Alex L. Wang", "title": "Hardy-Muckenhoupt Bounds for Laplacian Eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two graph quantities Psi(G,S) and Psi_2(G) which give constant\nfactor estimates to the Dirichlet and Neumann eigenvalues, lambda(G,S) and\nlambda_2(G), respectively. Our techniques make use of a discrete Hardy-type\ninequality.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 22:48:47 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Miller", "Gary L.", ""], ["Walkington", "Noel J.", ""], ["Wang", "Alex L.", ""]]}, {"id": "1812.03074", "submitter": "Christine Dahn", "authors": "Christine Dahn, Nils M. Kriege, Petra Mutzel, Julian Schilling", "title": "Fixed-Parameter Algorithms for the Weighted Max-Cut Problem on Embedded\n  1-Planar Graphs", "comments": "This work is an extension of the conference version arXiv:1803.10983\n  , currently under review at TCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose two fixed-parameter tractable algorithms for the weighted Max-Cut\nproblem on embedded 1-planar graphs parameterized by the crossing number $k$ of\nthe given embedding. A graph is called 1-planar if it can be drawn in the plane\nwith at most one crossing per edge. Our algorithms recursively reduce a\n1-planar graph to at most $3^k$ planar graphs, using edge removal and node\ncontraction. Our main algorithm then solves the Max-Cut problem for the planar\ngraphs using the FCE-MaxCut introduced by Liers and Pardella [23]. In the case\nof non-negative edge weights, we suggest a variant that allows to solve the\nplanar instances with any planar Max-Cut algorithm. We show that a maximum cut\nin the given 1-planar graph can be derived from the solutions for the planar\ngraphs. Our algorithms compute a maximum cut in an embedded weighted 1-planar\ngraph with $n$ nodes and $k$ edge crossings in time $O(3^k \\cdot n^{3/2} \\log\nn)$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:17:47 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:39:51 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 11:31:34 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 11:02:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dahn", "Christine", ""], ["Kriege", "Nils M.", ""], ["Mutzel", "Petra", ""], ["Schilling", "Julian", ""]]}, {"id": "1812.03155", "submitter": "Holger Dell", "authors": "Holger Dell and D\\'aniel Marx", "title": "Kernelization of Packing Problems", "comments": "An extended abstract was presented at SODA 2012, but this full\n  version contains some new material: Sec. 5 and 6.2 are new, and the gadget in\n  Fig. 1 is simpler & better", "journal-ref": null, "doi": "10.1137/1.9781611973099.6", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernelization algorithms are polynomial-time reductions from a problem to\nitself that guarantee their output to have a size not exceeding some bound. For\nexample, d-Set Matching for integers d>2 is the problem of finding a matching\nof size at least k in a given d-uniform hypergraph and has kernels with O(k^d)\nedges. Bodlaender et al. [JCSS 2009], Fortnow and Santhanam [JCSS 2011], Dell\nand Van Melkebeek [JACM 2014] developed a framework for proving lower bounds on\nthe kernel size for certain problems, under the complexity-theoretic hypothesis\nthat coNP is not contained in NP/poly. Under the same hypothesis, we show tight\nlower bounds for the kernelization of d-Set Matching and other packing\nproblems.\n  Our bounds are tight for d-Set Matching: It does not have kernels with\nO(k^{d-{\\epsilon}}) edges for any {\\epsilon}>0 unless the hypothesis fails. By\nreduction, this transfers to a bound of O(k^{d-1-{\\epsilon}}) for the problem\nof finding k vertex-disjoint cliques of size d in standard graphs. Obtaining\ntight bounds for graph packing problems is challenging: We make first progress\nin this direction by showing non-trivial kernels with O(k^2.5) edges for the\nproblem of finding k vertex-disjoint paths of three edges each. If the paths\nhave d edges each, we improve the straightforward O(k^{d+1}) kernel to a\nuniform polynomial kernel where the exponent of the kernel size is independent\nof k.\n  Most of our lower bound proofs follow a general scheme that we discover: To\nexclude kernels of size O(k^{d-{\\epsilon}}) for a problem in d-uniform\nhypergraphs, one should reduce from a carefully chosen d-partite problem that\nis still NP-hard. As an illustration, we apply this scheme to the vertex cover\nproblem, which allows us to replace the number-theoretical construction by Dell\nand Van Melkebeek [JACM 2014] with shorter elementary arguments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 18:41:10 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Dell", "Holger", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1812.03160", "submitter": "Jure Slak", "authors": "Jure Slak and Gregor Kosec", "title": "On generation of node distributions for meshless PDE discretizations", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing, 2019, 41(5), A3202-A3229", "doi": "10.1137/18M1231456", "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm that is able to generate locally\nregular node layouts with spatially variable nodal density for interiors of\narbitrary domains in two, three and higher dimensions. It is demonstrated that\nthe generated node distributions are suitable to use in the RBF-FD method,\nwhich is demonstrated by solving thermo-fluid problem in 2D and 3D.\nAdditionally, local minimal spacing guarantees are proven for both uniform and\nvariable nodal densities. The presented algorithm has time complexity $O(N)$ to\ngenerate $N$ nodes with constant nodal spacing and $O(N \\log N)$ to generate\nvariably spaced nodes. Comparison with existing algorithms is performed in\nterms of node quality, time complexity, execution time and PDE solution\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 18:50:21 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 10:44:32 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 09:24:21 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Slak", "Jure", ""], ["Kosec", "Gregor", ""]]}, {"id": "1812.03174", "submitter": "Milica Bogicevic", "authors": "Milica Bogi\\'cevi\\'c, Milan Merkle", "title": "Approximate Calculation of Tukey's Depth and Median With\n  High-dimensional Data", "comments": null, "journal-ref": "Yugoslav Journal of Operations Research 28 (2018), Number 4,\n  475--499", "doi": "10.2298/YJOR180520022B", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a new fast approximate algorithm for Tukey (halfspace) depth level\nsets and its implementation-ABCDepth. Given a $d$-dimensional data set for any\n$d\\geq 1$, the algorithm is based on a representation of level sets as\nintersections of balls in $\\mathbb{R}^d$. Our approach does not need\ncalculations of projections of sample points to directions. This novel idea\nenables calculations of approximate level sets in very high dimensions with\ncomplexity which is linear in $d$, which provides a great advantage over all\nother approximate algorithms. Using different versions of this algorithm we\ndemonstrate approximate calculations of the deepest set of points (\"Tukey\nmedian\") and Tukey's depth of a sample point or out-of-sample point, all with a\nlinear in $d$ complexity. An additional theoretical advantage of this approach\nis that the data points are not assumed to be in \"general position\". Examples\nwith real and synthetic data show that the executing time of the algorithm in\nall mentioned versions in high dimensions is much smaller than the time of\nother implemented algorithms. Also, our algorithms can be used with thousands\nof multidimensional observations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 10:27:14 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bogi\u0107evi\u0107", "Milica", ""], ["Merkle", "Milan", ""]]}, {"id": "1812.03398", "submitter": "Yu Zhang", "authors": "Seyed-Vahid Sanei-Mehri, Yu Zhang, Ahmet Erdem Sariyuce, Srikanta\n  Tirthapura", "title": "FLEET: Butterfly Estimation from a Bipartite Graph Stream", "comments": "This is the author's version of the work. It is posted here by\n  permission of ACM for your personal use. Not for redistribution. The\n  definitive version was published in Seyed-Vahid Sanei-Mehri, Yu Zhang, Ahmet\n  Erdem Sariyuce and Srikanta Tirthapura. \"FLEET: Butterfly Estimation from a\n  Bipartite Graph Stream\". The 28th ACM International Conference on Information\n  and Knowledge Management", "journal-ref": null, "doi": "10.1145/3357384.3357983", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider space-efficient single-pass estimation of the number of\nbutterflies, a fundamental bipartite graph motif, from a massive bipartite\ngraph stream where each edge represents a connection between entities in two\ndifferent partitions. We present a space lower bound for any streaming\nalgorithm that can estimate the number of butterflies accurately, as well as\nFLEET, a suite of algorithms for accurately estimating the number of\nbutterflies in the graph stream. Estimates returned by the algorithms come with\nprovable guarantees on the approximation error, and experiments show good\ntradeoffs between the space used and the accuracy of approximation. We also\npresent space-efficient algorithms for estimating the number of butterflies\nwithin a sliding window of the most recent elements in the stream. While there\nis a significant body of work on counting subgraphs such as triangles in a\nunipartite graph stream, our work seems to be one of the few to tackle the case\nof bipartite graph streams.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 22:43:19 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 20:17:36 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Sanei-Mehri", "Seyed-Vahid", ""], ["Zhang", "Yu", ""], ["Sariyuce", "Ahmet Erdem", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1812.03535", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "On balanced clustering with tree-like structures over clusters", "comments": "15 pages, 15 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article addresses balanced clustering problems with an additional\nrequirement as a tree-like structure over the obtained balanced clusters. This\nkind of clustering problems can be useful in some applications (e.g., network\ndesign, management and routing). Various types of the initial elements are\nconsidered. Four basic greedy-like solving strategies (design framework) are\nconsidered: balancing-spanning strategy, spanning-balancing strategy, direct\nstrategy, and design of layered structures with balancing. An extended\ndescription of the spanning-balancing strategy is presented including four\nsolving schemes and an illustrative numerical example.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 18:03:46 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1812.03572", "submitter": "Alantha Newman", "authors": "Kevin L. Chang and Alantha Newman", "title": "Rounding semidefinite programs for large-domain problems via Brownian\n  motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new simple method for rounding a semidefinite programming\nrelaxation of a constraint satisfaction problem. We apply it to the problem of\napproximate angular synchronization. Specifically, we are given directed\ndistances on a circle (i.e., directed angles) between pairs of elements and our\ngoal is to assign the elements to positions on a circle so as to preserve these\ndistances as much as possible. The feasibility of our rounding scheme is based\non properties of the well-known stochastic process called Brownian motion.\nBased on computational and other evidence, we conjecture that this rounding\nscheme yields an approximation guarantee that is very close to the\nbest-possible guarantee (assuming the Unique-Games Conjecture).\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 23:01:26 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chang", "Kevin L.", ""], ["Newman", "Alantha", ""]]}, {"id": "1812.04165", "submitter": "Zhuo Feng", "authors": "Ying Zhang, Zhiqiang Zhao, and Zhuo Feng", "title": "A Unified Approach to Scalable Spectral Sparsification of Directed\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent spectral graph sparsification research allows constructing\nnearly-linear-sized subgraphs that can well preserve the spectral (structural)\nproperties of the original graph, such as the first few eigenvalues and\neigenvectors of the graph Laplacian, leading to the development of a variety of\nnearly-linear time numerical and graph algorithms. However, there is not a\nunified approach that allows for truly-scalable spectral sparsification of both\ndirected and undirected graphs. In this work, we prove the existence of\nlinear-sized spectral sparsifiers for general directed graphs and introduce a\npractically-efficient and unified spectral graph sparsification approach that\nallows sparsifying real-world, large-scale directed and undirected graphs with\nguaranteed preservation of the original graph spectra. By exploiting a\nhighly-scalable (nearly-linear complexity) spectral matrix perturbation\nanalysis framework for constructing nearly-linear sized (directed) subgraphs,\nit enables us to well preserve the key eigenvalues and eigenvectors of the\noriginal (directed) graph Laplacians. The proposed method has been validated\nusing various kinds of directed graphs obtained from public domain sparse\nmatrix collections, showing promising results for solving directed graph\nLaplacians, spectral embedding, and partitioning of general directed graphs, as\nwell as approximately computing (personalized) PageRank vectors.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 00:56:38 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 19:21:20 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zhang", "Ying", ""], ["Zhao", "Zhiqiang", ""], ["Feng", "Zhuo", ""]]}, {"id": "1812.04230", "submitter": "Jackson Abascal", "authors": "Jackson Abascal, Amadou Bah, Mario Banuelos, David Uminsky, Olivia\n  Vasquez", "title": "A Non-iterative Parallelizable Eigenbasis Algorithm for Johnson Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new $O(k^2 \\binom{n}{k}^2)$ method for generating an orthogonal\nbasis of eigenvectors for the Johnson graph $J(n,k)$. Unlike standard methods\nfor computing a full eigenbasis of sparse symmetric matrices, the algorithm\npresented here is non-iterative, and produces exact results under an\ninfinite-precision computation model. In addition, our method is highly\nparallelizable; given access to unlimited parallel processors, the eigenbasis\ncan be constructed in only $O(n)$ time given n and k. We also present an\nalgorithm for computing projections onto the eigenspaces of $J(n,k)$ in\nparallel time $O(n)$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 06:22:50 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Abascal", "Jackson", ""], ["Bah", "Amadou", ""], ["Banuelos", "Mario", ""], ["Uminsky", "David", ""], ["Vasquez", "Olivia", ""]]}, {"id": "1812.04261", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto and Yasuo Tabei", "title": "LZRR: LZ77 Parsing with Right Reference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossless data compression has been widely studied in computer science. One of\nthe most widely used lossless data compressions is Lempel-Zip(LZ) 77 parsing,\nwhich achieves a high compression ratio. Bidirectional (a.k.a. macro) parsing\nis a lossless data compression and computes a sequence of phrases copied from\nanother substring (target phrase) on either the left or the right position in\nan input string. Gagie et al.(LATIN 2018) recently showed that a large gap\nexists between the number of smallest bidirectional phrases of a given string\nand that of LZ77 phrases. In addition, finding the smallest bidirectional parse\nof a given text is NP-complete. Several variants of bidirectional parsing have\nbeen proposed thus far, but no prior work for bidirectional parsing has\nachieved high compression that is smaller than that of LZ77 phrasing for any\nstring. In this paper, we present the first practical bidirectional parsing\nnamed LZ77 parsing with right reference (LZRR), in which the number of LZRR\nphrases is theoretically guaranteed to be smaller than the number of LZ77\nphrases. Experimental results using benchmark strings show the number of LZRR\nphrases is approximately five percent smaller than that of LZ77 phrases.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 08:17:52 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["Tabei", "Yasuo", ""]]}, {"id": "1812.04431", "submitter": "Apostolos Rikos", "authors": "Apostolos I. Rikos", "title": "Distributed Weight Balancing in Directed Topologies", "comments": "doctoral thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This doctoral thesis concerns novel distributed algorithms for weight\nbalancing over directed (communication) topologies. A directed topology\n(digraph) with nonnegative (or positive) weights assigned on each edge is\nweight-balanced if, for each node, the sum of the weights of in-coming edges\nequals the sum of the weights of out-going edges. The novel algorithms\nintroduced in this thesis can facilitate the development of strategies for\ngenerating weight balanced digraphs, in a distributed manner, and find numerous\napplications in coordination and control of multi-component systems. In the\nfirst part of this thesis, we introduce a novel distributed algorithm that\noperates over a static topology and solves the weight balancing problem when\nthe weights are restricted to be nonnegative integers. In the second part of\nthe thesis, we present a novel distributed algorithm which solves the integer\nweight balancing problem in the presence of arbitrary (time-varying and\ninhomogeneous) delays that might affect the transmission at a particular link\nat a particular time. In the third part of this thesis, we present a novel\ndistributed algorithm for obtaining admissible and balanced integer weights for\nthe case when there are lower and upper weight constraints on the communication\nlinks. In the fourth part of this thesis we present a novel distributed\nalgorithm which solves the integer weight balancing problem under lower and\nupper weight constraints over the communication links for the case where\narbitrary (time-varying and inhomogeneous) time delays and possible packet\ndrops affect the transmission at a particular link at a particular time.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 00:06:54 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Rikos", "Apostolos I.", ""]]}, {"id": "1812.04543", "submitter": "Philipp Kindermann", "authors": "Therese Biedl and Philipp Kindermann", "title": "Finding Tutte paths in linear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that every planar graph has a Tutte path, i.e., a path $P$\nsuch that any component of $G-P$ has at most three attachment points on $P$.\nHowever, it was only recently shown that such Tutte paths can be found in\npolynomial time. In this paper, we give a new proof that 3-connected planar\ngraphs have Tutte paths, which leads to a linear-time algorithm to find Tutte\npaths. Furthermore, our Tutte path has special properties: it visits all\nexterior vertices, all components of $G-P$ have exactly three attachment\npoints, and we can assign distinct representatives to them that are interior\nvertices. Finally, our running time bound is slightly stronger; we can bound it\nin terms of the degrees of the faces that are incident to $P$. This allows us\nto find some applications of Tutte paths (such as binary spanning trees and\n2-walks) in linear time as well.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 17:01:32 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 23:40:42 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Biedl", "Therese", ""], ["Kindermann", "Philipp", ""]]}, {"id": "1812.04802", "submitter": "Mirza Galib Anwarul Husain Baig", "authors": "Mirza Galib Anwarul Husain Baig, Deepanjan Kesh and Chirag Sodani", "title": "An Improved Scheme in the Two Query Adaptive Bitprobe Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we look into the adaptive bitprobe model that stores subsets\nof size at most four from a universe of size m, and answers membership queries\nusing two bitprobes. We propose a scheme that stores arbitrary subsets of size\nfour using O(m^{5/6}) amount of space. This improves upon the non-explicit\nscheme proposed by Garg and Radhakrishnan [Garg2015] which uses O(m^{16/17})\namount of space, and the explicit scheme proposed by Garg [Thesis2015] which\nuses O(m^{14/15}) amount of space.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 04:22:55 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Baig", "Mirza Galib Anwarul Husain", ""], ["Kesh", "Deepanjan", ""], ["Sodani", "Chirag", ""]]}, {"id": "1812.05013", "submitter": "Nikhil Vyas", "authors": "Mitali Bafna, Jack Murtagh and Nikhil Vyas", "title": "Thwarting Adversarial Examples: An $L_0$-RobustSparse Fourier Transform", "comments": "Accepted at 32nd Conference on Neural Information Processing Systems\n  (NeurIPS 2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new algorithm for approximating the Discrete Fourier transform of\nan approximately sparse signal that has been corrupted by worst-case $L_0$\nnoise, namely a bounded number of coordinates of the signal have been corrupted\narbitrarily. Our techniques generalize to a wide range of linear\ntransformations that are used in data analysis such as the Discrete Cosine and\nSine transforms, the Hadamard transform, and their high-dimensional analogs. We\nuse our algorithm to successfully defend against well known $L_0$ adversaries\nin the setting of image classification. We give experimental results on the\nJacobian-based Saliency Map Attack (JSMA) and the Carlini Wagner (CW) $L_0$\nattack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch\non the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 16:36:14 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Bafna", "Mitali", ""], ["Murtagh", "Jack", ""], ["Vyas", "Nikhil", ""]]}, {"id": "1812.05189", "submitter": "Jason Altschuler", "authors": "Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed", "title": "Massively scalable Sinkhorn distances via the Nystr\\\"om method", "comments": "to appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sinkhorn \"distance\", a variant of the Wasserstein distance with entropic\nregularization, is an increasingly popular tool in machine learning and\nstatistical inference. However, the time and memory requirements of standard\nalgorithms for computing this distance grow quadratically with the size of the\ndata, making them prohibitively expensive on massive data sets. In this work,\nwe show that this challenge is surprisingly easy to circumvent: combining two\nsimple techniques---the Nystr\\\"om method and Sinkhorn scaling---provably yields\nan accurate approximation of the Sinkhorn distance with significantly lower\ntime and memory requirements than other approaches. We prove our results via\nnew, explicit analyses of the Nystr\\\"om method and of the stability properties\nof Sinkhorn scaling. We validate our claims experimentally by showing that our\napproach easily computes Sinkhorn distances on data sets hundreds of times\nlarger than can be handled by other techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 23:10:16 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 18:42:35 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 23:58:02 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Altschuler", "Jason", ""], ["Bach", "Francis", ""], ["Rudi", "Alessandro", ""], ["Niles-Weed", "Jonathan", ""]]}, {"id": "1812.05306", "submitter": "Wenjian Yu Prof.", "authors": "Dingcheng Yang, Wenjian Yu, Junhui Deng, Shenghua Liu", "title": "Optimal Algorithm for Profiling Dynamic Arrays with Finite Values", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How can one quickly answer the most and top popular objects at any time,\ngiven a large log stream in a system of billions of users? It is equivalent to\nfind the mode and top-frequent elements in a dynamic array corresponding to the\nlog stream. However, most existing work either restrain the dynamic array\nwithin a sliding window, or do not take advantages of only one element can be\nadded or removed in a log stream. Therefore, we propose a profiling algorithm,\nnamed S-Profile, which is of $O(1)$ time complexity for every updating of the\ndynamic array, and optimal in terms of computational complexity. With the\nprofiling results, answering the queries on the statistics of dynamic array\nbecomes trivial and fast. With the experiments of various settings of dynamic\narrays, our accurate S-Profile algorithm outperforms the well-known methods,\nshowing at least 2X speedup to the heap based approach and 13X or larger\nspeedup to the balanced tree based approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 08:04:09 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Yang", "Dingcheng", ""], ["Yu", "Wenjian", ""], ["Deng", "Junhui", ""], ["Liu", "Shenghua", ""]]}, {"id": "1812.05316", "submitter": "Martin Milani\\v{c}", "authors": "T{\\i}naz Ekim and Didem G\\\"oz\\\"upek and Ademir Hujdurovi\\'c and Martin\n  Milani\\v{c}", "title": "Mind the Independence Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The independence gap of a graph was introduced by Ekim et al. (2018) as a\nmeasure of how far a graph is from being well-covered. It is defined as the\ndifference between the maximum and minimum size of a maximal independent set.\n  We investigate the independence gap of a graph from structural and\nalgorithmic points of view, with a focus on classes of perfect graphs.\nGeneralizing results on well-covered graphs due to Dean and Zito (1994) and\nHujdurovi\\'c et al. (2018), we express the independence gap of a perfect graph\nin terms of clique partitions and use this characterization to develop a\npolynomial-time algorithm for recognizing graphs of constant independence gap\nin any class of perfect graphs of bounded clique number. Next, we introduce a\nhereditary variant of the parameter, which we call hereditary independence gap\nand which measures the maximum independence gap over all induced subgraphs of\nthe graph. We show that determining whether a given graph has hereditary\nindependence gap at most $k$ is polynomial-time solvable if $k$ is fixed and\nco-NP-complete if $k$ is part of input. We also investigate the complexity of\nthe independent set problem in graph classes related to independence gap,\nshowing that the problem is NP-complete in the class of graphs of independence\ngap at most one and polynomial-time solvable in any class of graphs with\nbounded hereditary independence gap. Combined with some known results on\nclaw-free graphs, our results imply that the independent domination problem is\nsolvable in polynomial time in the class of $\\{$claw, 2$P_3\\}$-free graphs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 08:55:01 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Ekim", "T\u0131naz", ""], ["G\u00f6z\u00fcpek", "Didem", ""], ["Hujdurovi\u0107", "Ademir", ""], ["Milani\u010d", "Martin", ""]]}, {"id": "1812.05352", "submitter": "Anisur Molla Rahaman", "authors": "Ajay D. Kshemkalyani and Anisur Rahaman Molla and Gokarna Sharma", "title": "Efficient Dispersion of Mobile Robots on Arbitrary Graphs and Grids", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mobile robot dispersion problem on graphs asks $k\\leq n$ robots placed\ninitially arbitrarily on the nodes of an $n$-node anonymous graph to reposition\nautonomously to reach a configuration in which each robot is on a distinct node\nof the graph. This problem is of significant interest due to its relationship\nto other fundamental robot coordination problems, such as exploration,\nscattering, load balancing, and relocation of self-driven electric cars\n(robots) to recharge stations (nodes). In this paper, we provide two novel\ndeterministic algorithms for dispersion, one for arbitrary graphs and another\nfor grid graphs, in a synchronous setting where all robots perform their\nactions in every time step. Our algorithm for arbitrary graphs has\n$O(\\min(m,k\\Delta) \\cdot \\log k)$ steps runtime using $O(\\log n)$ bits of\nmemory at each robot, where $m$ is the number of edges and $\\Delta$ is the\nmaximum degree of the graph. This is an exponential improvement over the\n$O(mk)$ steps best previously known algorithm. In particular, the runtime of\nour algorithm is optimal (up to a $O(\\log k)$ factor) in constant-degree\narbitrary graphs. Our algorithm for grid graphs has $O(\\min(k,\\sqrt{n}))$ steps\nruntime using $\\Theta(\\log k)$ bits at each robot. This is the first algorithm\nfor dispersion in grid graphs. Moreover, this algorithm is optimal for both\nmemory and time when $k=\\Omega(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 10:23:19 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 12:48:48 GMT"}, {"version": "v3", "created": "Sat, 27 Apr 2019 12:42:17 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kshemkalyani", "Ajay D.", ""], ["Molla", "Anisur Rahaman", ""], ["Sharma", "Gokarna", ""]]}, {"id": "1812.05419", "submitter": "Moritz M\\\"uhlenthaler", "authors": "Nicolas Bousquet and Tatsuhiko Hatanaka and Takehiro Ito and Moritz\n  M\\\"uhlenthaler", "title": "Shortest Reconfiguration of Matchings", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine that unlabelled tokens are placed on the edges of a graph, such that\nno two tokens are placed on incident edges. A token can jump to another edge if\nthe edges having tokens remain independent. We study the problem of determining\nthe distance between two token configurations (resp., the corresponding\nmatchings), which is given by the length of a shortest transformation. We give\na polynomial-time algorithm for the case that at least one of the two\nconfigurations is not inclusion-wise maximal and show that otherwise, the\nproblem admits no polynomial-time sublogarithmic-factor approximation unless P\n= NP. Furthermore, we show that the distance of two configurations in bipartite\ngraphs is fixed-parameter tractable parameterized by the size $d$ of the\nsymmetric difference of the source and target configurations, and obtain a\n$d^\\varepsilon$-factor approximation algorithm for every $\\varepsilon > 0$ if\nadditionally the configurations correspond to maximum matchings. Our two main\ntechnical tools are the Edmonds-Gallai decomposition and a close relation to\nthe Directed Steiner Tree problem. Using the former, we also characterize those\ngraphs whose corresponding configuration graphs are connected. Finally, we show\nthat deciding if the distance between two configurations is equal to a given\nnumber $\\ell$ is complete for the class $D^P$, and deciding if the diameter of\nthe graph of configurations is equal to $\\ell$ is $D^P$-hard.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 13:32:16 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Hatanaka", "Tatsuhiko", ""], ["Ito", "Takehiro", ""], ["M\u00fchlenthaler", "Moritz", ""]]}, {"id": "1812.05524", "submitter": "Anastasios Sidiropoulos", "authors": "Ilias Diakonikolas, Anastasios Sidiropoulos, Alistair Stewart", "title": "A Polynomial Time Algorithm for Maximum Likelihood Estimation of\n  Multivariate Log-concave Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing the maximum likelihood estimator (MLE) of\nmultivariate log-concave densities. Our main result is the first\ncomputationally efficient algorithm for this problem. In more detail, we give\nan algorithm that, on input a set of $n$ points in $\\mathbb{R}^d$ and an\naccuracy parameter $\\epsilon>0$, it runs in time $\\text{poly}(n, d,\n1/\\epsilon)$, and outputs a log-concave density that with high probability\nmaximizes the log-likelihood up to an additive $\\epsilon$. Our approach relies\non a natural convex optimization formulation of the underlying problem that can\nbe efficiently solved by a projected stochastic subgradient method. The main\nchallenge lies in showing that a stochastic subgradient of our objective\nfunction can be efficiently approximated. To achieve this, we rely on\nstructural results on approximation of log-concave densities and leverage\nclassical algorithmic tools on volume approximation of convex bodies and\nuniform sampling from convex sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:09:21 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Sidiropoulos", "Anastasios", ""], ["Stewart", "Alistair", ""]]}, {"id": "1812.05577", "submitter": "Guillem Perarnau", "authors": "Michelle Delcourt, Marc Heinrich and Guillem Perarnau", "title": "The Glauber dynamics for edge-colourings of trees", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T$ be a tree on $n$ vertices and with maximum degree $\\Delta$. We show\nthat for $k\\geq \\Delta+1$ the Glauber dynamics for $k$-edge-colourings of $T$\nmixes in polynomial time in $n$. The bound on the number of colours is best\npossible as the chain is not even ergodic for $k \\leq \\Delta$. Our proof uses a\nrecursive decomposition of the tree into subtrees; we bound the relaxation time\nof the original tree in terms of the relaxation time of its subtrees using\nblock dynamics and chain comparison techniques. Of independent interest, we\nalso introduce a monotonicity result for Glauber dynamics that simplifies our\nproof.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:50:02 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:19:20 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Delcourt", "Michelle", ""], ["Heinrich", "Marc", ""], ["Perarnau", "Guillem", ""]]}, {"id": "1812.05727", "submitter": "Roberto Palmieri", "authors": "Mohamed M. Saad and Masoomeh Javidi Kishi and Shihao Jing and Sandeep\n  Hans and Roberto Palmieri", "title": "Processing Transactions in a Predefined Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a high performance solution to the problem of\ncommitting transactions while enforcing a predefined order. We provide the\ndesign and implementation of three algorithms, which deploy a specialized\ncooperative transaction execution model. This model permits the propagation of\nwritten values along the chain of ordered transactions. We show that, even in\nthe presence of data conflicts, the proposed algorithms are able to outperform\nsingle-threaded execution, and other baseline and specialized state-of-the-art\ncompetitors (e.g., STMLite). The maximum speedup achieved in micro benchmarks,\nSTAMP, PARSEC and SPEC200 applications is in the range of 4.3x -- 16.5x.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 23:17:37 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Saad", "Mohamed M.", ""], ["Kishi", "Masoomeh Javidi", ""], ["Jing", "Shihao", ""], ["Hans", "Sandeep", ""], ["Palmieri", "Roberto", ""]]}, {"id": "1812.05778", "submitter": "Greg Bodwin", "authors": "Greg Bodwin, Shyamal Patel", "title": "A Trivial Yet Optimal Solution to Vertex Fault Tolerant Spanners", "comments": "PODC 2019. 3 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a short and easy upper bound on the worst-case size of fault tolerant\nspanners, which improves on all prior work and is fully optimal at least in the\nsetting of vertex faults.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 04:34:47 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 05:45:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bodwin", "Greg", ""], ["Patel", "Shyamal", ""]]}, {"id": "1812.05821", "submitter": "Umang Bhaskar", "authors": "Umang Bhaskar, Gunjan Kumar", "title": "Partial Function Extension with Applications to Learning and Property\n  Testing", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In partial function extension, we are given a partial function consisting of\n$n$ points from a domain and a function value at each point. Our objective is\nto determine if this partial function can be extended to a function defined on\nthe domain, that additionally satisfies a given property, such as convexity.\nThis basic problem underlies research questions in many areas, such as\nlearning, property testing, and game theory. We formally study the problem of\nextending partial functions to satisfy fundamental properties in combinatorial\noptimization, focusing on upper and lower bounds for extension and applications\nto learning and property testing.\n  (1) For subadditive functions, we show the extension problem is\ncoNP-complete, and we give tight bounds on the approximability. We also give an\nimproved lower bound for learning subadditive functions, and give the first\nnontrivial testers for subadditive and XOS functions.\n  (2) For submodular functions, we show that if a partial function can be\nextended to a submodular function on the lattice closure (the minimal set that\ncontains the partial function and is closed under union and intersection) of\nthe partial function, it can be extended to a submodular function on the entire\ndomain. We obtain algorithms for determining extendibility in a number of\ncases, including if $n$ is a constant, or the points are nearly the same size.\nThe complexity of extendibility is in general unresolved.\n  (3) Lastly, for convex functions in $\\mathbb{R}^m$, we show an interesting\njuxtaposition: while we can determine the existence of an extension\nefficiently, computing the value of a widely-studied convex extension at a\ngiven point is strongly NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 08:26:47 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Bhaskar", "Umang", ""], ["Kumar", "Gunjan", ""]]}, {"id": "1812.05913", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley and Michel Habib", "title": "Graph classes and forbidden patterns on three vertices", "comments": "Third version version. 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with graph classes characterization and recognition. A\npopular way to characterize a graph class is to list a minimal set of forbidden\ninduced subgraphs. Unfortunately this strategy usually does not lead to an\nefficient recognition algorithm. On the other hand, many graph classes can be\nefficiently recognized by techniques based on some interesting orderings of the\nnodes, such as the ones given by traversals.\n  We study specifically graph classes that have an ordering avoiding some\nordered structures. More precisely, we consider what we call patterns on three\nnodes, and the recognition complexity of the associated classes. In this\ndomain, there are two key previous works. Damashke started the study of the\nclasses defined by forbidden patterns, a set that contains interval, chordal\nand bipartite graphs among others. On the algorithmic side, Hell, Mohar and\nRafiey proved that any class defined by a set of forbidden patterns can be\nrecognized in polynomial time. We improve on these two works, by characterizing\nsystematically all the classes defined sets of forbidden patterns (on three\nnodes), and proving that among the 23 different classes (up to complementation)\nthat we find, 21 can actually be recognized in linear time.\n  Beyond this result, we consider that this type of characterization is very\nuseful, leads to a rich structure of classes, and generates a lot of open\nquestions worth investigating.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 13:35:20 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 11:57:41 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 14:06:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Habib", "Michel", ""]]}, {"id": "1812.06177", "submitter": "Sixue Liu", "authors": "S. Cliff Liu and Robert E. Tarjan", "title": "Simple Concurrent Labeling Algorithms for Connected Components", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a class of simple algorithms for concurrently computing the\nconnected components of an $n$-vertex, $m$-edge graph. Our algorithms are easy\nto implement in either the COMBINING CRCW PRAM or the MPC computing model. For\ntwo related algorithms in this class, we obtain $\\Theta(\\lg n)$ step and\n$\\Theta(m \\lg n)$ work bounds. For two others, we obtain $O(\\lg^2 n)$ step and\n$O(m \\lg^2 n)$ work bounds, which are tight for one of them. All our algorithms\nare simpler than related algorithms in the literature. We also point out some\ngaps and errors in the analysis of previous algorithms. Our results show that\neven a basic problem like connected components still has secrets to reveal.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:40:09 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 19:03:07 GMT"}, {"version": "v3", "created": "Sat, 19 Jan 2019 05:57:51 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 15:05:47 GMT"}, {"version": "v5", "created": "Tue, 3 Mar 2020 02:54:28 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Liu", "S. Cliff", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1812.06243", "submitter": "Zhao Song", "authors": "Yin Tat Lee, Zhao Song, Santosh S. Vempala", "title": "Algorithmic Theory of ODEs and Sampling from Well-conditioned Logconcave\n  Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sampling logconcave functions arising in statistics and machine learning has\nbeen a subject of intensive study. Recent developments include analyses for\nLangevin dynamics and Hamiltonian Monte Carlo (HMC). While both approaches have\ndimension-independent bounds for the underlying $\\mathit{continuous}$ processes\nunder sufficiently strong smoothness conditions, the resulting discrete\nalgorithms have complexity and number of function evaluations growing with the\ndimension. Motivated by this problem, in this paper, we give a general\nalgorithm for solving multivariate ordinary differential equations whose\nsolution is close to the span of a known basis of functions (e.g., polynomials\nor piecewise polynomials). The resulting algorithm has polylogarithmic depth\nand essentially tight runtime - it is nearly linear in the size of the\nrepresentation of the solution.\n  We apply this to the sampling problem to obtain a nearly linear\nimplementation of HMC for a broad class of smooth, strongly logconcave\ndensities, with the number of iterations (parallel depth) and gradient\nevaluations being $\\mathit{polylogarithmic}$ in the dimension (rather than\npolynomial as in previous work). This class includes the widely-used loss\nfunction for logistic regression with incoherent weight matrices and has been\nsubject of much study recently. We also give a faster algorithm with $\n\\mathit{polylogarithmic~depth}$ for the more general and standard class of\nstrongly convex functions with Lipschitz gradient. These results are based on\n(1) an improved contraction bound for the exact HMC process and (2) logarithmic\nbounds on the degree of polynomials that approximate solutions of the\ndifferential equations arising in implementing HMC.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 06:46:03 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Lee", "Yin Tat", ""], ["Song", "Zhao", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1812.06295", "submitter": "Kirankumar Shiragur", "authors": "Arun Jambulapati, Kirankumar Shiragur and Aaron Sidford", "title": "Efficient Structured Matrix Recovery and Nearly-Linear Time Algorithms\n  for Solving Inverse Symmetric $M$-Matrices", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to recover a spectral approximations to broad\nclasses of structured matrices using only a polylogarithmic number of adaptive\nlinear measurements to either the matrix or its inverse. Leveraging this result\nwe obtain faster algorithms for variety of linear algebraic problems. Key\nresults include:\n  $\\bullet$ A nearly linear time algorithm for solving the inverse of symmetric\n$M$-matrices, a strict superset of Laplacians and SDD matrices.\n  $\\bullet$ An $\\tilde{O}(n^2)$ time algorithm for solving $n \\times n$ linear\nsystems that are constant spectral approximations of Laplacians or more\ngenerally, SDD matrices.\n  $\\bullet$ An $\\tilde{O}(n^2)$ algorithm to recover a spectral approximation\nof a $n$-vertex graph using only $\\tilde{O}(1)$ matrix-vector multiplies with\nits Laplacian matrix.\n  The previous best results for each problem either used a trivial number of\nqueries to exactly recover the matrix or a trivial $O(n^\\omega)$ running time,\nwhere $\\omega$ is the matrix multiplication constant.\n  We achieve these results by generalizing recent semidefinite programming\nbased linear sized sparsifier results of Lee and Sun (2017) and providing\niterative methods inspired by the semistreaming sparsification results of\nKapralov, Lee, Musco, Musco and Sidford (2014) and input sparsity time linear\nsystem solving results of Li, Miller, and Peng (2013). We hope that by\ninitiating study of these natural problems, expanding the robustness and scope\nof recent nearly linear time linear system solving research, and providing\ngeneral matrix recovery machinery this work may serve as a stepping stone for\nfaster algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 14:37:28 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Jambulapati", "Arun", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "1812.06456", "submitter": "Rosario Scatamacchia", "authors": "Pierre Hosteins, Rosario Scatamacchia", "title": "The Stochastic Critical Node Problem over Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle a stochastic version of the Critical Node Problem (CNP) where the\ngoal is to minimize the pairwise connectivity of a graph by attacking a subset\nof its nodes. In the stochastic setting considered, the attacks on nodes can\nfail with a certain probability. In our work we focus on trees and demonstrate\nthat over trees the stochastic CNP actually generalizes to the stochastic\nCritical Element Detection Problem where attacks on edges can also fail with a\ncertain probability. We also prove the NP-completeness of the decision version\nof the problem when connection costs are one, while its deterministic\ncounterpart was proved to be polynomial. We then derive linear and nonlinear\nmodels for the considered CNP version. Moreover, we propose an exact approach\nbased on Benders decomposition and test its effectiveness on a large set of\ninstances. As a side result, we introduce an approximation algorithm for a\nproblem variant of interest.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 12:40:50 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 17:20:09 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Hosteins", "Pierre", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "1812.06653", "submitter": "Frank Gurski", "authors": "Frank Gurski and Carolin Rehs", "title": "Comparing Linear Width Parameters for Directed Graphs", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the linear clique-width, linear NLC-width,\nneighbourhood-width, and linear rank-width for directed graphs. We compare\nthese parameters with each other as well as with the previously defined\nparameters directed path-width and directed cut-width. It turns out that the\nparameters directed linear clique-width, directed linear NLC-width, directed\nneighbourhood-width, and directed linear rank-width are equivalent in that\nsense, that all of these parameters can be upper bounded by each of the others.\nFor the restriction to digraphs of bounded vertex degree directed path-width\nand directed cut-width are equivalent. Further for the restriction to\nsemicomplete digraphs of bounded vertex degree all six mentioned width\nparameters are equivalent. We also show close relations of the measures to\ntheir undirected versions of the underlying undirected graphs, which allow us\nto show the hardness of computing the considered linear width parameters for\ndirected graphs. Further we give first characterizations for directed graphs\ndefined by parameters of small width.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 08:52:20 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Gurski", "Frank", ""], ["Rehs", "Carolin", ""]]}, {"id": "1812.06778", "submitter": "Carlos Daganzo", "authors": "Carlos F. Daganzo", "title": "Minuet: A method to solve Sudoku puzzles by hand", "comments": "12 pages; Working Paper, University of California, Berkeley", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a systematic method to solve difficult 9 x 9 Sudoku\npuzzles by hand. While computer algorithms exist to solve these puzzles, these\nalgorithms are not good for human's to use because they involve too many steps\nand require too much memory. For humans, all one can find in the literature are\nindividual tricks, which used together in ad hoc ways can be used to solve some\npuzzles--but not all. To the author's knowledge, a systematic procedure made up\nof well-defined steps that can be carried out by hand and solve all puzzles has\nnot been devised. This paper proposes one such technique--the \"minuet\" method.\nIt is based on a new system of markings and a new way of simplifying the\npuzzles that can be easily carried out by hand--or by computer. The author has\nsolved hundreds of puzzles of the most difficult kind (\"evil\" in Sudoku's\nparlance) and never found one that could not be solved. The average time to\nsolve one of these puzzles is slightly over 1 hour. It is conjectured that this\nmethod can solve all well-posed 9 x 9 puzzles. The method's distinguishing\nfeature is a \"minuet\" strategy that is applied when the puzzle cannot be\nfurther simplified with basic tricks. The strategy consists in concurrently\ndeveloping two potential solutions, sometimes alone and sometimes in concert as\nif they were dancing a minuet.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 22:40:34 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 19:20:04 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 20:40:40 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Daganzo", "Carlos F.", ""]]}, {"id": "1812.07030", "submitter": "Saber Malekzadeh", "authors": "Saber Malekzadeh", "title": "A Fast Combination of AES Encryption and LZ4 Compression Algorithms", "comments": "Submitted to the 3rd International Conference on applied research in\n  Computer Science and Information Technology. in Farsi", "journal-ref": null, "doi": "10.13140/RG.2.2.33644.56960", "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a long time ago, beside encryption of data and making it secure,\ncompression packing it was also important that could make transmission of data\nfaster. In the past years need for improvement of encryption and compression\nfor a fast and easy transmission is more necessary. In this paper, a new method\nfor combination of LZ4 combination and AES encryption algorithms for a fast and\neasy packing, securing and compressing of data is presented. Choose of these\ntwo algorithms was for some special features of them about aim of this paper.\nThis paper also is introducing a method for Parallelism of compression and\nencryption in a special way for improvement of speed and security of data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:59:44 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Malekzadeh", "Saber", ""]]}, {"id": "1812.07075", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese and Eduardo Laber", "title": "Information theoretical clustering is hard to approximate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An impurity measures $I: \\mathbb{R}^d \\mapsto \\mathbb{R}^+$ is a function\nthat assigns a $d$-dimensional vector ${\\bf v}$ to a non-negative value $I({\\bf\nv})$ so that the more homogeneous ${\\bf v}$, with respect to the values of its\ncoordinates, the larger its impurity. A well known example of impurity measures\nis the Entropy impurity.\n  We study the problem of clustering based on impurity measures. Let $V$ be a\ncollection of $n$ many $d$-dimensional vectors with non-negative components.\nGiven $V$ and an impurity measure $I$, the goal is to find a partition\n${\\mathcal P}$ of $V$ into $k$ groups $V_1,\\ldots,V_k$ so as to minimize the\nsum of the impurities of the groups in ${\\cal P}$, i.e., $I({\\cal P})=\n\\sum_{i=1}^{k} I\\bigg(\\sum_{ {\\bf v} \\in V_i} {\\bf v} \\bigg).$\n  Impurity minimization has been widely used as quality assessment measure in\nprobability distribution clustering (KL-divergence) as well as in categorical\nclustering. However, in contrast to the case of metric based clustering, the\ncurrent knowledge of impurity measure based clustering in terms of\napproximation and inapproximability results is very limited. Here, we\ncontribute to change this scenario by proving that for the Entropy impurity\nmeasure the problem does not admit a PTAS even when all vectors have the same\n$\\ell_1$ norm. This result solves a question that remained open in previous\nwork on this topic [Chaudhuri and McGregor COLT 08; Ackermann et. al. ECCC 11].\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 22:25:55 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 22:04:29 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Laber", "Eduardo", ""]]}, {"id": "1812.07441", "submitter": "Renjie Chen", "authors": "Renjie Chen and Craig Gotsman", "title": "A Scalable Heuristic for Fastest-Path Computation on Very Large Road\n  Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fastest-path queries between two points in a very large road map is an\nincreasingly important primitive in modern transportation and navigation\nsystems, thus very efficient computation of these paths is critical for system\nperformance and throughput. We present a method to compute an effective\nheuristic for the fastest path travel time between two points on a road map,\nwhich can be used to significantly accelerate the classical A* algorithm when\ncomputing fastest paths. Our method is based on two hierarchical sets of\nseparators of the map represented by two binary trees. A preprocessing step\ncomputes a short vector of values per road junction based on the separator\ntrees, which is then stored with the map and used to efficiently compute the\nheuristic at the online query stage. We demonstrate experimentally that this\nmethod scales well to any map size, providing a better quality heuristic, thus\nmore efficient A* search, for fastest path queries between points at all\ndistances - especially small and medium range - relative to other known\nheuristics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 15:46:17 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 18:09:30 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 18:45:26 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Chen", "Renjie", ""], ["Gotsman", "Craig", ""]]}, {"id": "1812.07484", "submitter": "Ville Hyv\\\"onen", "authors": "Elias J\\\"a\\\"asaari, Ville Hyv\\\"onen, Teemu Roos", "title": "Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor\n  Search", "comments": "Accepted for the 23rd Pacific-Asia Conference on Knowledge Discovery\n  and Data Mining (PAKDD) 2019", "journal-ref": "Advances in Knowledge Discovery and Data Mining. PAKDD 2019.\n  Lecture Notes in Computer Science, vol 11440. Springer, Cham. pp. 590-602", "doi": "10.1007/978-3-030-16145-3_46", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor algorithms are used to speed up nearest neighbor\nsearch in a wide array of applications. However, current indexing methods\nfeature several hyperparameters that need to be tuned to reach an acceptable\naccuracy--speed trade-off. A grid search in the parameter space is often\nimpractically slow due to a time-consuming index-building procedure. Therefore,\nwe propose an algorithm for automatically tuning the hyperparameters of\nindexing methods based on randomized space-partitioning trees. In particular,\nwe present results using randomized k-d trees, random projection trees and\nrandomized PCA trees. The tuning algorithm adds minimal overhead to the\nindex-building process but is able to find the optimal hyperparameters\naccurately. We demonstrate that the algorithm is significantly faster than\nexisting approaches, and that the indexing methods used are competitive with\nthe state-of-the-art methods in query time while being faster to build.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:06:05 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["J\u00e4\u00e4saari", "Elias", ""], ["Hyv\u00f6nen", "Ville", ""], ["Roos", "Teemu", ""]]}, {"id": "1812.07532", "submitter": "Guus Regts", "authors": "Ferenc Bencs, Ewan Davies, Viresh Patel, Guus Regts", "title": "On zero-free regions for the anti-ferromagnetic Potts model on\n  bounded-degree graphs", "comments": "Some minor changes based on referee comments. Accepted for\n  publication in AIHPD. 22 pages; 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $G=(V,E)$, $k\\in \\mathbb{N}$, and a complex number $w$ the\npartition function of the univariate Potts model is defined as \\[ {\\bf\nZ}(G;k,w):=\\sum_{\\phi:V\\to [k]}\\prod_{\\substack{uv\\in E \\\\ \\phi(u)=\\phi(v)}}w,\n\\] where $[k]:=\\{1,\\ldots,k\\}$. In this paper we give zero-free regions for the\npartition function of the anti-ferromagnetic Potts model on bounded degree\ngraphs. In particular we show that for any $\\Delta\\in \\mathbb{N}$ and any\n$k\\geq e\\Delta+1$, there exists an open set $U$ in the complex plane that\ncontains the interval $[0,1)$ such that ${\\bf Z}(G;k,w)\\neq 0$ for any $w\\in U$\nand any graph $G$ of maximum degree at most $\\Delta$. (Here $e$ denotes the\nbase of the natural logarithm.) For small values of $\\Delta$ we are able to\ngive better results.\n  As an application of our results we obtain improved bounds on $k$ for the\nexistence of deterministic approximation algorithms for counting the number of\nproper $k$-colourings of graphs of small maximum degree.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:59:01 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 15:37:46 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 14:27:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bencs", "Ferenc", ""], ["Davies", "Ewan", ""], ["Patel", "Viresh", ""], ["Regts", "Guus", ""]]}, {"id": "1812.07721", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad", "title": "Approximation Schemes for Capacitated Clustering in Doubling Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in redistricting, we consider the uniform\ncapacitated k-median and uniform capacitated k-means problems in bounded\ndoubling metrics. We provide the first QPTAS for both problems and the first\nPTAS for the uniform capacitated k-median problem for points in R^2 . This is\nthe first improvement over the bicriteria QPTAS for capacitated k-median in\nlow-dimensional Euclidean space of Arora, Raghavan, Rao [STOC 1998] (1 +\n{\\epsilon}-approximation, 1 + {\\epsilon}-capacity violation) and arguably the\nfirst polynomial-time approximation algorithm for a non-trivial metric.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 01:14:09 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 14:57:34 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Cohen-Addad", "Vincent", ""]]}, {"id": "1812.07769", "submitter": "Sepehr Abbasi-Zadeh", "authors": "Sepehr Abbasi-Zadeh, Nikhil Bansal, Guru Guruganesh, Aleksandar\n  Nikolov, Roy Schwartz, and Mohit Singh", "title": "Sticky Brownian Rounding and its Applications to Constraint Satisfaction\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semidefinite programming is a powerful tool in the design and analysis of\napproximation algorithms for combinatorial optimization problems. In\nparticular, the random hyperplane rounding method of Goemans and Williamson has\nbeen extensively studied for more than two decades, resulting in various\nextensions to the original technique and beautiful algorithms for a wide range\nof applications. Despite the fact that this approach yields tight approximation\nguarantees for some problems, e.g., Max-Cut, for many others, e.g., Max-SAT and\nMax-DiCut, the tight approximation ratio is still unknown. One of the main\nreasons for this is the fact that very few techniques for rounding semidefinite\nrelaxations are known.\n  In this work, we present a new general and simple method for rounding\nsemi-definite programs, based on Brownian motion. Our approach is inspired by\nrecent results in algorithmic discrepancy theory. We develop and present tools\nfor analyzing our new rounding algorithms, utilizing mathematical machinery\nfrom the theory of Brownian motion, complex analysis, and partial differential\nequations. Focusing on constraint satisfaction problems, we apply our method to\nseveral classical problems, including Max-Cut, Max-2SAT, and MaxDiCut, and\nderive new algorithms that are competitive with the best known results. To\nillustrate the versatility and general applicability of our approach, we give\nnew approximation algorithms for the Max-Cut problem with side constraints that\ncrucially utilizes measure concentration results for the Sticky Brownian\nMotion, a feature missing from hyperplane rounding and its generalizations\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 06:09:07 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 00:13:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Abbasi-Zadeh", "Sepehr", ""], ["Bansal", "Nikhil", ""], ["Guruganesh", "Guru", ""], ["Nikolov", "Aleksandar", ""], ["Schwartz", "Roy", ""], ["Singh", "Mohit", ""]]}, {"id": "1812.07826", "submitter": "Adam Kasperski", "authors": "Marc Goerigk, Adam Kasperski, Pawel Zielinski", "title": "Two-stage Combinatorial Optimization Problems under Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of combinatorial optimization problems is discussed. It\nis assumed that a solution can be constructed in two stages. The current\nfirst-stage costs are precisely known, while the future second-stage costs are\nonly known to belong to an uncertainty set, which contains a finite number of\nscenarios with known probability distribution. A partial solution, chosen in\nthe first stage, can be completed by performing an optimal recourse action,\nafter the true second-stage scenario is revealed. A solution minimizing the\nConditional Value at Risk (CVaR) measure is computed. Since expectation and\nmaximum are boundary cases of CVaR, the model generalizes the traditional\nstochastic and robust two-stage approaches, previously discussed in the\nexisting literature. In this paper some new negative and positive results are\nprovided for basic combinatorial optimization problems such as the selection or\nnetwork problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 09:11:12 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Goerigk", "Marc", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1812.07903", "submitter": "Paul Pu Liang", "authors": "Hui Han Chin, Paul Pu Liang", "title": "An Empirical Evaluation of Sketched SVD and its Application to Leverage\n  Score Ordering", "comments": "ACML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power of randomized algorithms in numerical methods have led to fast\nsolutions which use the Singular Value Decomposition (SVD) as a core routine.\nHowever, given the large data size of modern and the modest runtime of SVD,\nmost practical algorithms would require some form of approximation, such as\nsketching, when running SVD. While these approximation methods satisfy many\ntheoretical guarantees, we provide the first algorithmic implementations for\nsketch-and-solve SVD problems on real-world, large-scale datasets. We provide a\ncomprehensive empirical evaluation of these algorithms and provide guidelines\non how to ensure accurate deployment to real-world data. As an application of\nsketched SVD, we present Sketched Leverage Score Ordering, a technique for\ndetermining the ordering of data in the training of neural networks. Our\ntechnique is based on the distributed computation of leverage scores using\nrandom projections. These computed leverage scores provide a flexible and\nefficient method to determine the optimal ordering of training data without\nmanual intervention or annotations. We present empirical results on an\nextensive set of experiments across image classification, language sentiment\nanalysis, and multi-modal sentiment analysis. Our method is faster compared to\nstandard randomized projection algorithms and shows improvements in convergence\nand results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 12:06:58 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chin", "Hui Han", ""], ["Liang", "Paul Pu", ""]]}, {"id": "1812.08003", "submitter": "Sebastian Siebertz", "authors": "Kord Eickmeyer, Jan van den Heuvel, Ken-ichi Kawarabayashi, Stephan\n  Kreutzer, Patrice Ossona de Mendez, Micha{\\l} Pilipczuk, Daniel A. Quiroz,\n  Roman Rabinovich, Sebastian Siebertz", "title": "Model-Checking on Ordered Structures", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.08516", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the model-checking problem for first- and monadic second-order logic\non finite relational structures. The problem of verifying whether a formula of\nthese logics is true on a given structure is considered intractable in general,\nbut it does become tractable on interesting classes of structures, such as on\nclasses whose Gaifman graphs have bounded treewidth. In this paper we continue\nthis line of research and study model-checking for first- and monadic\nsecond-order logic in the presence of an ordering on the input structure. We do\nso in two settings: the general ordered case, where the input structures are\nequipped with a fixed order or successor relation, and the order invariant\ncase, where the formulas may resort to an ordering, but their truth must be\nindependent of the particular choice of order. In the first setting we show\nvery strong intractability results for most interesting classes of structures.\nIn contrast, in the order invariant case we obtain tractability results for\norder-invariant monadic second-order formulas on the same classes of graphs as\nin the unordered case. For first-order logic, we obtain tractability of\nsuccessor-invariant formulas on classes whose Gaifman graphs have bounded\nexpansion. Furthermore, we show that model-checking for order-invariant\nfirst-order formulas is tractable on coloured posets of bounded width.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:21:15 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Eickmeyer", "Kord", ""], ["Heuvel", "Jan van den", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Kreutzer", "Stephan", ""], ["de Mendez", "Patrice Ossona", ""], ["Pilipczuk", "Micha\u0142", ""], ["Quiroz", "Daniel A.", ""], ["Rabinovich", "Roman", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1812.08101", "submitter": "Juliusz Straszy\\'nski", "authors": "Tomasz Kociumaka, Jakub Radoszewski, Wojciech Rytter, Juliusz\n  Straszy\\'nski, Tomasz Wale\\'n, Wiktor Zuba", "title": "Efficient Representation and Counting of Antipower Factors in Words", "comments": "Full version of a paper from LATA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-antipower (for $k \\ge 2$) is a concatenation of $k$ pairwise distinct\nwords of the same length. The study of fragments of a word being antipowers was\ninitiated by Fici et al. (ICALP 2016) and first algorithms for computing such\nfragments were presented by Badkobeh et al. (Inf. Process. Lett., 2018). We\naddress two open problems posed by Badkobeh et al. We propose efficient\nalgorithms for counting and reporting fragments of a word which are\n$k$-antipowers. They work in $\\mathcal{O}(nk \\log k)$ time and $\\mathcal{O}(nk\n\\log k + C)$ time, respectively, where $C$ is the number of reported fragments.\nFor $k=o(\\sqrt{n/\\log n})$, this improves the time complexity of\n$\\mathcal{O}(n^2/k)$ of the solution by Badkobeh et al. We also show that the\nnumber of different $k$-antipower factors of a word of length $n$ can be\ncomputed in $\\mathcal{O}(nk^4 \\log k \\log n)$ time. Our main algorithmic tools\nare runs and gapped repeats. Finally we present an improved data structure that\nchecks, for a given fragment of a word and an integer $k$, if the fragment is a\n$k$-antipower. This is a full and extended version of a paper from LATA 2019.\nIn particular, all results about counting different antipowers factors are\ncompletely new compared with the LATA proceedings version.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:22:32 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 21:12:50 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 02:41:42 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "1812.08277", "submitter": "Thibaut Vidal", "authors": "Ian Herszterg, Marcus Poggi, Thibaut Vidal", "title": "Two-Dimensional Phase Unwrapping via Balanced Spanning Forests", "comments": null, "journal-ref": null, "doi": "10.1287/ijoc.2018.0832", "report-no": null, "categories": "eess.SP cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase unwrapping is the process of recovering a continuous phase signal from\nan original signal wrapped in the ($-\\pi$,$\\pi$] interval. It is a critical\nstep of coherent signal processing, with applications such as synthetic\naperture radar, acoustic imaging, magnetic resonance, X-ray crystallography,\nand seismic processing. In the field of computational optics, this problem is\nclassically treated as a norm-minimization problem, in which one seeks to\nminimize the differences between the gradients of the original wrapped signal\nand those of the continuous unwrapped signal. When the L0-norm is considered,\nthe number of differences should be minimized, leading to a difficult\ncombinatorial optimization problem. We propose an approximate model for the\nL0-norm phase unwrapping problem in 2D, in which the singularities of the\nwrapped phase image are associated with a graph where the vertices have $-1$ or\n$+1$ polarities. The objective is to find a minimum-cost balanced spanning\nforest where the sum of the polarities is equal to zero in each tree. We\nintroduce a set of primal and dual heuristics, a branch-and-cut algorithm, and\na hybrid metaheuristic to efficiently find exact or heuristic solutions. These\napproaches move us one step closer to optimal solutions for 2D L0-norm phase\nunwrapping; such solutions were previously viewed, in the signal processing\nliterature, as highly desirable but not achievable.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 22:36:57 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 19:50:41 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Herszterg", "Ian", ""], ["Poggi", "Marcus", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1812.08480", "submitter": "Carola Doerr", "authors": "Peyman Afshani, Manindra Agrawal, Benjamin Doerr, Carola Doerr, Kasper\n  Green Larsen, Kurt Mehlhorn", "title": "The Query Complexity of a Permutation-Based Variant of Mastermind", "comments": "Full version of a result previously announced in 2013. Accepted\n  subject to minor revision at Discrete Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the query complexity of a permutation-based variant of the guessing\ngame Mastermind. In this variant, the secret is a pair $(z,\\pi)$ which consists\nof a binary string $z \\in \\{0,1\\}^n$ and a permutation $\\pi$ of $[n]$. The\nsecret must be unveiled by asking queries of the form $x \\in \\{0,1\\}^n$. For\neach such query, we are returned the score \\[ f_{z,\\pi}(x):= \\max \\{ i \\in\n[0..n]\\mid \\forall j \\leq i: z_{\\pi(j)} = x_{\\pi(j)}\\}\\,;\\] i.e., the score of\n$x$ is the length of the longest common prefix of $x$ and $z$ with respect to\nthe order imposed by $\\pi$. The goal is to minimize the number of queries\nneeded to identify $(z,\\pi)$. This problem originates from the study of\nblack-box optimization heuristics, where it is known as the\n\\textsc{LeadingOnes} problem.\n  In this work, we prove matching upper and lower bounds for the deterministic\nand randomized query complexity of this game, which are $\\Theta(n \\log n)$ and\n$\\Theta(n \\log \\log n)$, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 11:00:59 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Afshani", "Peyman", ""], ["Agrawal", "Manindra", ""], ["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Larsen", "Kasper Green", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1812.08615", "submitter": "Antoine Roux", "authors": "Julien Baste, Binh-Minh Bui-Xuan, Antoine Roux", "title": "Temporal Matching", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A link stream is a sequence of pairs of the form $(t,\\{u,v\\})$, where\n$t\\in\\mathbb N$ represents a time instant and $u\\neq v$. Given an integer\n$\\gamma$, the $\\gamma$-edge between vertices $u$ and $v$, starting at time $t$,\nis the set of temporally consecutive edges defined by $\\{(t',\\{u,v\\}) | t' \\in\n[t,t+\\gamma-1]\\}$. We introduce the notion of temporal matching of a link\nstream to be an independent $\\gamma$-edge set belonging to the link stream. We\nshow that the problem of computing a temporal matching of maximum size is\nNP-hard as soon as $\\gamma>1$. We depict a kernelization algorithm\nparameterized by the solution size for the problem. As a byproduct we also give\na $2$-approximation algorithm.\n  Both our $2$-approximation and kernelization algorithms are implemented and\nconfronted to link streams collected from real world graph data. We observe\nthat finding temporal matchings is a sensitive question when mining our data\nfrom such a perspective as: managing peer-working when any pair of peers $X$\nand $Y$ are to collaborate over a period of one month, at an average rate of at\nleast two email exchanges every week. We furthermore design a link stream\ngenerating process by mimicking the behaviour of a random moving group of\nparticles under natural simulation, and confront our algorithms to these\ngenerated instances of link streams. All the implementations are open source.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 14:51:49 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 10:24:52 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Baste", "Julien", ""], ["Bui-Xuan", "Binh-Minh", ""], ["Roux", "Antoine", ""]]}, {"id": "1812.08664", "submitter": "David Saulpic", "authors": "Vincent Cohen-Addad, Andreas Emil Feldmann, David Saulpic", "title": "Near-Linear Time Approximation Schemes for Clustering in Doubling\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic Facility Location, $k$-Median, and $k$-Means problems\nin metric spaces of doubling dimension $d$. We give nearly linear-time\napproximation schemes for each problem. The complexity of our algorithms is\n$2^{(\\log(1/\\eps)/\\eps)^{O(d^2)}} n \\log^4 n + 2^{O(d)} n \\log^9 n$, making a\nsignificant improvement over the state-of-the-art algorithms which run in time\n$n^{(d/\\eps)^{O(d)}}$.\n  Moreover, we show how to extend the techniques used to get the first\nefficient approximation schemes for the problems of prize-collecting\n$k$-Medians and $k$-Means, and efficient bicriteria approximation schemes for\n$k$-Medians with outliers, $k$-Means with outliers and $k$-Center.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 16:12:00 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 16:53:02 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 16:11:17 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 14:07:43 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Feldmann", "Andreas Emil", ""], ["Saulpic", "David", ""]]}, {"id": "1812.08712", "submitter": "Edoardo Galimberti", "authors": "Edoardo Galimberti, Francesco Bonchi, Francesco Gullo, and Tommaso\n  Lanciano", "title": "Core Decomposition in Multilayer Networks: Theory, Algorithms, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks are a powerful paradigm to model complex systems, where\nmultiple relations occur between the same entities. Despite the keen interest\nin a variety of tasks, algorithms, and analyses in this type of network, the\nproblem of extracting dense subgraphs has remained largely unexplored so far.\n  In this work we study the problem of core decomposition of a multilayer\nnetwork. The multilayer context is much challenging as no total order exists\namong multilayer cores; rather, they form a lattice whose size is exponential\nin the number of layers. In this setting we devise three algorithms which\ndiffer in the way they visit the core lattice and in their pruning techniques.\n  We then move a step forward and study the problem of extracting the\ninner-most (also known as maximal) cores, i.e., the cores that are not\ndominated by any other core in terms of their core index in all the layers.\nInner-most cores are typically orders of magnitude less than all the cores.\nMotivated by this, we devise an algorithm that effectively exploits the\nmaximality property and extracts inner-most cores directly, without first\ncomputing a complete decomposition.\n  Finally, we showcase the multilayer core-decomposition tool in a variety of\nscenarios and problems. We start by considering the problem of densest-subgraph\nextraction in multilayer networks. We introduce a definition of multilayer\ndensest subgraph that trades-off between high density and number of layers in\nwhich the high density holds, and exploit multilayer core decomposition to\napproximate this problem with quality guarantees. As further applications, we\nshow how to utilize multilayer core decomposition to speed-up the extraction of\nfrequent cross-graph quasi-cliques and to generalize the community-search\nproblem to the multilayer setting.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 17:34:13 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 16:41:33 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Galimberti", "Edoardo", ""], ["Bonchi", "Francesco", ""], ["Gullo", "Francesco", ""], ["Lanciano", "Tommaso", ""]]}, {"id": "1812.08723", "submitter": "Cameron Musco", "authors": "Haim Avron and Michael Kapralov and Cameron Musco and Christopher\n  Musco and Ameya Velingker and Amir Zandieh", "title": "A Universal Sampling Method for Reconstructing Signals with Simple\n  Fourier Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG eess.SP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing continuous signals from a small number of discrete samples is\na fundamental problem across science and engineering. In practice, we are often\ninterested in signals with 'simple' Fourier structure, such as bandlimited,\nmultiband, and Fourier sparse signals. More broadly, any prior knowledge about\na signal's Fourier power spectrum can constrain its complexity. Intuitively,\nsignals with more highly constrained Fourier structure require fewer samples to\nreconstruct.\n  We formalize this intuition by showing that, roughly, a continuous signal\nfrom a given class can be approximately reconstructed using a number of samples\nproportional to the *statistical dimension* of the allowed power spectrum of\nthat class. Further, in nearly all settings, this natural measure tightly\ncharacterizes the sample complexity of signal reconstruction.\n  Surprisingly, we also show that, up to logarithmic factors, a universal\nnon-uniform sampling strategy can achieve this optimal complexity for *any\nclass of signals*. We present a simple and efficient algorithm for recovering a\nsignal from the samples taken. For bandlimited and sparse signals, our method\nmatches the state-of-the-art. At the same time, it gives the first\ncomputationally and sample efficient solution to a broad range of problems,\nincluding multiband signal reconstruction and kriging and Gaussian process\nregression tasks in one dimension.\n  Our work is based on a novel connection between randomized linear algebra and\nsignal reconstruction with constrained Fourier structure. We extend tools based\non statistical leverage score sampling and column-based matrix reconstruction\nto the approximation of continuous linear operators that arise in signal\nreconstruction. We believe that these extensions are of independent interest\nand serve as a foundation for tackling a broad range of continuous time\nproblems using randomized methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 17:51:09 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Avron", "Haim", ""], ["Kapralov", "Michael", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Velingker", "Ameya", ""], ["Zandieh", "Amir", ""]]}, {"id": "1812.08731", "submitter": "Josh Alman", "authors": "Josh Alman", "title": "Limits on the Universal Method for Matrix Multiplication", "comments": "25 pages, to appear in 34th Computational Complexity Conference (CCC\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we prove limitations on the known methods for designing matrix\nmultiplication algorithms. Alman and Vassilevska Williams recently defined the\nUniversal Method, which substantially generalizes all the known approaches\nincluding Strassen's Laser Method and Cohn and Umans' Group Theoretic Method.\nWe prove concrete lower bounds on the algorithms one can design by applying the\nUniversal Method to many different tensors. Our proofs use new tools for upper\nbounding the asymptotic slice rank of a wide range of tensors. Our main result\nis that the Universal method applied to any Coppersmith-Winograd tensor $CW_q$\ncannot yield a bound on $\\omega$, the exponent of matrix multiplication, better\nthan $2.16805$. By comparison, it was previously only known that the weaker\n`Galactic Method' applied to $CW_q$ could not achieve an exponent of $2$.\n  We also study the Laser Method (which is, in principle, a highly special case\nof the Universal Method) and prove that it is \"complete\" for matrix\nmultiplication algorithms: when it applies to a tensor $T$, it achieves $\\omega\n= 2$ if and only if it is possible for the Universal method applied to $T$ to\nachieve $\\omega = 2$. Hence, the Laser Method, which was originally used as an\nalgorithmic tool, can also be seen as a lower bounding tool. For example, in\ntheir landmark paper, Coppersmith and Winograd achieved a bound of $\\omega \\leq\n2.376$, by applying the Laser Method to $CW_q$. By our result, the fact that\nthey did not achieve $\\omega=2$ implies a lower bound on the Universal Method\napplied to $CW_q$. Indeed, if it were possible for the Universal Method applied\nto $CW_q$ to achieve $\\omega=2$, then Coppersmith and Winograd's application of\nthe Laser Method would have achieved $\\omega=2$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 18:04:08 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 13:56:29 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Alman", "Josh", ""]]}, {"id": "1812.08942", "submitter": "Zhiqiang Zhao", "authors": "Zhiqiang Zhao, Yongyu Wang, Zhuo Feng", "title": "Nearly-Linear Time Spectral Graph Reduction for Scalable Graph\n  Partitioning and Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a scalable algorithmic framework for spectral reduction\nof large undirected graphs. The proposed method allows computing much smaller\ngraphs while preserving the key spectral (structural) properties of the\noriginal graph. Our framework is built upon the following two key components: a\nspectrum-preserving node aggregation (reduction) scheme, as well as a spectral\ngraph sparsification framework with iterative edge weight scaling. We show that\nthe resulting spectrally-reduced graphs can robustly preserve the first few\nnontrivial eigenvalues and eigenvectors of the original graph Laplacian. In\naddition, the spectral graph reduction method has been leveraged to develop\nmuch faster algorithms for multilevel spectral graph partitioning as well as\nt-distributed Stochastic Neighbor Embedding (t-SNE) of large data sets. We\nconducted extensive experiments using a variety of large graphs and data sets,\nand obtained very promising results. For instance, we are able to reduce the\n\"coPapersCiteseer\" graph with 0.43 million nodes and 16 million edges to a much\nsmaller graph with only 13K (32X fewer) nodes and 17K (950X fewer) edges in\nabout 16 seconds; the spectrally-reduced graphs also allow us to achieve up to\n1100X speedup for spectral graph partitioning and up to 60X speedup for t-SNE\nvisualization of large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 04:38:12 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Zhao", "Zhiqiang", ""], ["Wang", "Yongyu", ""], ["Feng", "Zhuo", ""]]}, {"id": "1812.08958", "submitter": "Thatchaphol Saranurak", "authors": "Thatchaphol Saranurak and Di Wang", "title": "Expander Decomposition and Pruning: Faster, Stronger, and Simpler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of graph clustering where the goal is to partition a\ngraph into clusters, i.e. disjoint subsets of vertices, such that each cluster\nis well connected internally while sparsely connected to the rest of the graph.\nIn particular, we use a natural bicriteria notion motivated by Kannan, Vempala,\nand Vetta which we refer to as {\\em expander decomposition}. Expander\ndecomposition has become one of the building blocks in the design of fast graph\nalgorithms, most notably in the nearly linear time Laplacian solver by Spielman\nand Teng, and it also has wide applications in practice.\n  We design algorithm for the parametrized version of expander decomposition,\nwhere given a graph $G$ of $m$ edges and a parameter $\\phi$, our algorithm\nfinds a partition of the vertices into clusters such that each cluster induces\na subgraph of conductance at least $\\phi$ (i.e. a $\\phi$ expander), and only a\n$\\widetilde{O}(\\phi)$ fraction of the edges in $G$ have endpoints across\ndifferent clusters. Our algorithm runs in $\\widetilde{O}(m/\\phi)$ time, and is\nthe first nearly linear time algorithm when $\\phi$ is at least $1/\\log^{O(1)}\nm$, which is the case in most practical settings and theoretical applications.\nPrevious results either take $\\Omega(m^{1+o(1)})$ time, or attain nearly linear\ntime but with a weaker expansion guarantee where each output cluster is\nguaranteed to be contained inside some unknown $\\phi$ expander. Our result\nachieve both nearly linear running time and the strong expander guarantee for\nclusters. Moreover, a main technique we develop for our result can be applied\nto obtain a much better \\emph{expander pruning} algorithm, which is the key\ntool for maintaining an expander decomposition on dynamic graphs. Finally, we\nnote that our algorithm is developed from first principles based on relatively\nsimple and basic techniques, thus making it very likely to be practical.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 05:43:48 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 17:16:50 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Saranurak", "Thatchaphol", ""], ["Wang", "Di", ""]]}, {"id": "1812.09094", "submitter": "Felipe A. Louza", "authors": "Felipe A. Louza", "title": "A Simple Algorithm for Computing the Document Array", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipl.2019.105887", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple algorithm for computing the document array given a string\ncollection and its suffix array as input. Our algorithm runs in linear time\nusing constant additional space for strings from constant alphabets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 13:03:06 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 13:54:54 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 18:17:50 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Louza", "Felipe A.", ""]]}, {"id": "1812.09120", "submitter": "Tatiana Starikovskaya", "authors": "Vincent Cohen-Addad (LIP6), Laurent Feuilloley (IRIF), Tatiana\n  Starikovskaya (DI-ENS)", "title": "Lower bounds for text indexing with mismatches and differences", "comments": null, "journal-ref": "SODA 2019, Jan 2019, San Diego, United States", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study lower bounds for the fundamental problem of text\nindexing with mismatches and differences. In this problem we are given a long\nstring of length $n$, the \"text\", and the task is to preprocess it into a data\nstructure such that given a query string $Q$, one can quickly identify\nsubstrings that are within Hamming or edit distance at most $k$ from $Q$. This\nproblem is at the core of various problems arising in biology and text\nprocessing. While exact text indexing allows linear-size data structures with\nlinear query time, text indexing with $k$ mismatches (or $k$ differences) seems\nto be much harder: All known data structures have exponential dependency on $k$\neither in the space, or in the time bound. We provide conditional and\npointer-machine lower bounds that make a step toward explaining this\nphenomenon. We start by demonstrating lower bounds for $k = \\Theta(\\log n)$. We\nshow that assuming the Strong Exponential Time Hypothesis, any data structure\nfor text indexing that can be constructed in polynomial time cannot have\n$\\mathcal{O}(n^{1-\\delta})$ query time, for any $\\delta>0$. This bound also\nextends to the setting where we only ask for $(1+\\varepsilon)$-approximate\nsolutions for text indexing. However, in many applications the value of $k$ is\nrather small, and one might hope that for small~$k$ we can develop more\nefficient solutions. We show that this would require a radically new approach\nas using the current methods one cannot avoid exponential dependency on $k$\neither in the space, or in the time bound for all even $\\frac{8}{\\sqrt{3}}\n\\sqrt{\\log n} \\le k = o(\\log n)$. Our lower bounds also apply to the dictionary\nlook-up problem, where instead of a text one is given a set of strings.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 13:52:54 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Cohen-Addad", "Vincent", "", "LIP6"], ["Feuilloley", "Laurent", "", "IRIF"], ["Starikovskaya", "Tatiana", "", "DI-ENS"]]}, {"id": "1812.09353", "submitter": "Guohui Lin", "authors": "Yong Chen, Randy Goebel, Guohui Lin, Longcheng Liu, Bing Su, Weitian\n  Tong, Yao Xu, An Zhang", "title": "A local search $4/3$-approximation algorithm for the minimum $3$-path\n  partition problem", "comments": "16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G = (V, E)$, the $3$-path partition problem is to find a\nminimum collection of vertex-disjoint paths each of order at most $3$ to cover\nall the vertices of $V$. It is different from but closely related to the\nwell-known $3$-set cover problem. The best known approximation algorithm for\nthe $3$-path partition problem was proposed recently and has a ratio $13/9$.\nHere we present a local search algorithm and show, by an amortized analysis,\nthat it is a $4/3$-approximation. This ratio matches up to the best\napproximation ratio for the $3$-set cover problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 19:45:54 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Chen", "Yong", ""], ["Goebel", "Randy", ""], ["Lin", "Guohui", ""], ["Liu", "Longcheng", ""], ["Su", "Bing", ""], ["Tong", "Weitian", ""], ["Xu", "Yao", ""], ["Zhang", "An", ""]]}, {"id": "1812.09372", "submitter": "Benjamin Woods", "authors": "Benjamin J. Q. Woods", "title": "Fast post-hoc method for updating moments of large datasets", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moments of large datasets utilise the mean of the dataset; consequently,\nupdating the dataset traditionally requires one to update the mean, which then\nrequires one to recalculate the moment. This means that metrics such as the\nstandard deviation, $R^2$ correlation, and other statistics have to be\n`refreshed' for dataset updates, requiring large data storage and taking long\ntimes to process. Here, a method is shown for updating moments that only\nrequires the previous moments (which are computationally cheaper to store), and\nthe new data to be appended. This leads to a dramatic decrease in data storage\nrequirements, and significant computational speed-up for large datasets or\nlow-order moments (n $\\lesssim$ 10).\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 21:10:00 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Woods", "Benjamin J. Q.", ""]]}, {"id": "1812.09519", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Stefan Mengel, Matthias Niewerth", "title": "Enumeration on Trees with Tractable Combined Complexity and Efficient\n  Updates", "comments": "16 pages of main material, 37 references, 11 pages of appendix. This\n  is the extended version with proofs of the PODS'19 paper. Except for minor\n  rephrasings and formatting differences, the contents are exactly the same as\n  the version published in the PODS'19 proceedings", "journal-ref": null, "doi": "10.1145/3294052.3319702", "report-no": null, "categories": "cs.DB cs.DS cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm to enumerate the results on trees of monadic\nsecond-order (MSO) queries represented by nondeterministic tree automata. After\nlinear time preprocessing (in the input tree), we can enumerate answers with\nlinear delay (in each answer). We allow updates on the tree to take place at\nany time, and we can then restart the enumeration after logarithmic time in the\ntree. Further, all our combined complexities are polynomial in the automaton.\n  Our result follows our previous circuit-based enumeration algorithms based on\ndeterministic tree automata, and is also inspired by our earlier result on\nwords and nondeterministic sequential extended variable-set automata in the\ncontext of document spanners. We extend these results and combine them with a\nrecent tree balancing scheme by Niewerth, so that our enumeration structure\nsupports updates to the underlying tree in logarithmic time (with leaf\ninsertions, leaf deletions, and node relabelings). Our result implies that, for\nMSO queries with free first-order variables, we can enumerate the results with\nlinear preprocessing and constant-delay and update the underlying tree in\nlogarithmic time, which improves on several known results for words and trees.\n  Building on lower bounds from data structure research, we also show\nunconditionally that up to a doubly logarithmic factor the update time of our\nalgorithm is optimal. Thus, unlike other settings, there can be no algorithm\nwith constant update time.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 12:16:35 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 16:14:02 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Mengel", "Stefan", ""], ["Niewerth", "Matthias", ""]]}, {"id": "1812.09526", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Ryan R. Curtin, Benjamin Moseley, Hung Q. Ngo,\n  XuanLong Nguyen, Dan Olteanu, Maximilian Schleich", "title": "Functional Aggregate Queries with Additive Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by fundamental applications in databases and relational machine\nlearning, we formulate and study the problem of answering functional aggregate\nqueries (FAQ) in which some of the input factors are defined by a collection of\nadditive inequalities between variables. We refer to these queries as FAQ-AI\nfor short.\n  To answer FAQ-AI in the Boolean semiring, we define relaxed tree\ndecompositions and relaxed submodular and fractional hypertree width\nparameters. We show that an extension of the InsideOut algorithm using\nChazelle's geometric data structure for solving the semigroup range search\nproblem can answer Boolean FAQ-AI in time given by these new width parameters.\nThis new algorithm achieves lower complexity than known solutions for FAQ-AI.\nIt also recovers some known results in database query answering.\n  Our second contribution is a relaxation of the set of polymatroids that gives\nrise to the counting version of the submodular width, denoted by #subw. This\nnew width is sandwiched between the submodular and the fractional hypertree\nwidths. Any FAQ and FAQ-AI over one semiring can be answered in time\nproportional to #subw and respectively to the relaxed version of #subw.\n  We present three applications of our FAQ-AI framework to relational machine\nlearning: k-means clustering, training linear support vector machines, and\ntraining models using non-polynomial loss. These optimization problems can be\nsolved over a database asymptotically faster than computing the join of the\ndatabase relations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 13:05:18 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 10:59:32 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 23:08:57 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 05:20:46 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Curtin", "Ryan R.", ""], ["Moseley", "Benjamin", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""], ["Olteanu", "Dan", ""], ["Schleich", "Maximilian", ""]]}, {"id": "1812.09583", "submitter": "Zunjing Wang", "authors": "Xiao-Feng Xie and Zunjing Jenipher Wang", "title": "Uncovering Urban Mobility and City Dynamics from Large-Scale Taxi\n  Origin-Destination (O-D) Trips: Case Study in Washington DC Area", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "WIO-TR-18-003", "categories": "stat.CO cs.DS physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We perform a systematic analysis on the large-scale taxi trip data to uncover\nurban mobility and city dynamics in multimodal urban transportation\nenvironments. As a case study, we use the taxi origin-destination trip data and\nsome additional data sources in Washington DC area. We first study basic\ncharacteristics of taxi trips, then focus on five important aspects. Three of\nthem concern urban mobility, which are respectively mobility and cost including\neffect of traffic congestion, trip safety, and multimodal connectivity; the\nother two pertain to city dynamics, which are respectively transportation\nresilience and the relation between trip patterns and land use. For these\naspects, we use appropriate statistical methods and geographic techniques to\nmine patterns and characteristics from taxi trip data for better understanding\nqualitative and quantitative impacts of the inputs from key stakeholders on\navailable measures of effectiveness on urban mobility and city dynamics, where\nkey stakeholders include road users, system operators, and city. Finally, we\nbriefly summarize our findings and discuss some critical roles and implications\nof the uncovered patterns and characteristics from the relation between taxi\nsystem and key stakeholders. The results can support road users by providing\nevidence-based information of trip cost, mobility, safety, multimodal\nconnectivity and transportation resilience, can assist taxi drivers and\noperators to deliver transportation services in a higher quality of mobility,\nsafety and operational efficiency, and can also help city planners and policy\nmakers to transform multimodal transportation and to manage urban resources in\na more effective and better way.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 19:13:59 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Xie", "Xiao-Feng", ""], ["Wang", "Zunjing Jenipher", ""]]}, {"id": "1812.09674", "submitter": "Gal Mendelson", "authors": "Gal Mendelson, Shay Vargaftik, Katherine Barabash, Dean Lorenz, Isaac\n  Keslassy, Ariel Orda", "title": "AnchorHash: A Scalable Consistent Hash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistent hashing (CH) is a central building block in many networking\napplications, from datacenter load-balancing to distributed storage.\nUnfortunately, state-of-the-art CH solutions cannot ensure full consistency\nunder arbitrary changes and/or cannot scale while maintaining reasonable memory\nfootprints and update times. We present AnchorHash, a scalable and\nfully-consistent hashing algorithm. AnchorHash achieves high key lookup rates,\na low memory footprint, and low update times. We formally establish its strong\ntheoretical guarantees, and present advanced implementations with a memory\nfootprint of only a few bytes per resource. Moreover, extensive evaluations\nindicate that it outperforms state-of-the-art algorithms, and that it can scale\non a single core to 100 million resources while still achieving a key lookup\nrate of more than 15 million keys per second.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 08:58:51 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 13:11:41 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mendelson", "Gal", ""], ["Vargaftik", "Shay", ""], ["Barabash", "Katherine", ""], ["Lorenz", "Dean", ""], ["Keslassy", "Isaac", ""], ["Orda", "Ariel", ""]]}, {"id": "1812.09824", "submitter": "Shikha Singh", "authors": "Michael A. Bender, Jonathan W. Berry, Martin Farach-Colton, Rob\n  Johnson, Thomas M. Kroeger, Prashant Pandey, Cynthia A. Phillips and Shikha\n  Singh", "title": "The Online Event-Detection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a stream $S = (s_1, s_2, ..., s_N)$, a $\\phi$-heavy hitter is an item\n$s_i$ that occurs at least $\\phi N$ times in $S$. The problem of finding\nheavy-hitters has been extensively studied in the database literature. In this\npaper, we study a related problem. We say that there is a $\\phi$-event at time\n$t$ if $s_t$ occurs exactly $\\phi N$ times in $(s_1, s_2, ..., s_t)$. Thus, for\neach $\\phi$-heavy hitter there is a single $\\phi$-event which occurs when its\ncount reaches the reporting threshold $\\phi N$. We define the online\nevent-detection problem (OEDP) as: given $\\phi$ and a stream $S$, report all\n$\\phi$-events as soon as they occur.\n  Many real-world monitoring systems demand event detection where all events\nmust be reported (no false negatives), in a timely manner, with no non-events\nreported (no false positives), and a low reporting threshold. As a result, the\nOEDP requires a large amount of space (Omega(N) words) and is not solvable in\nthe streaming model or via standard sampling-based approaches.\n  Since OEDP requires large space, we focus on cache-efficient algorithms in\nthe external-memory model.\n  We provide algorithms for the OEDP that are within a log factor of optimal.\nOur algorithms are tunable: its parameters can be set to allow for a bounded\nfalse-positives and a bounded delay in reporting. None of our relaxations allow\nfalse negatives since reporting all events is a strict requirement of our\napplications. Finally, we show improved results when the count of items in the\ninput stream follows a power-law distribution.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 04:16:53 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Bender", "Michael A.", ""], ["Berry", "Jonathan W.", ""], ["Farach-Colton", "Martin", ""], ["Johnson", "Rob", ""], ["Kroeger", "Thomas M.", ""], ["Pandey", "Prashant", ""], ["Phillips", "Cynthia A.", ""], ["Singh", "Shikha", ""]]}, {"id": "1812.09859", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Jan Vondrak", "title": "Generalization Bounds for Uniformly Stable Algorithms", "comments": "Appeared in Neural Information Processing Systems (NeurIPS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform stability of a learning algorithm is a classical notion of\nalgorithmic stability introduced to derive high-probability bounds on the\ngeneralization error (Bousquet and Elisseeff, 2002). Specifically, for a loss\nfunction with range bounded in $[0,1]$, the generalization error of a\n$\\gamma$-uniformly stable learning algorithm on $n$ samples is known to be\nwithin $O((\\gamma +1/n) \\sqrt{n \\log(1/\\delta)})$ of the empirical error with\nprobability at least $1-\\delta$. Unfortunately, this bound does not lead to\nmeaningful generalization bounds in many common settings where $\\gamma \\geq\n1/\\sqrt{n}$. At the same time the bound is known to be tight only when $\\gamma\n= O(1/n)$.\n  We substantially improve generalization bounds for uniformly stable\nalgorithms without making any additional assumptions. First, we show that the\nbound in this setting is $O(\\sqrt{(\\gamma + 1/n) \\log(1/\\delta)})$ with\nprobability at least $1-\\delta$. In addition, we prove a tight bound of\n$O(\\gamma^2 + 1/n)$ on the second moment of the estimation error. The best\nprevious bound on the second moment is $O(\\gamma + 1/n)$. Our proofs are based\non new analysis techniques and our results imply substantially stronger\ngeneralization guarantees for several well-studied algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 07:55:45 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 05:12:35 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Feldman", "Vitaly", ""], ["Vondrak", "Jan", ""]]}, {"id": "1812.09880", "submitter": "Zeev Nutov", "authors": "Zeev Nutov and Eli Shalom", "title": "Approximating activation edge-cover and facility location problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What approximation ratio can we achieve for the Facility Location problem if\nwhenever a client $u$ connects to a facility $v$,the opening cost of $v$ is at\nmost $\\theta$ times the service cost of $u$? We show that this and many other\nproblems are a particular case of the Activation Edge-Cover problem. Here we\nare given a multigraph $G=(V,E)$, a set $R \\subseteq V$ of terminals, and\nthresholds $\\{t^e_u,t^e_v\\}$ for each $uv$-edge $e \\in E$. The goal is to find\nan assignment ${\\bf a}=\\{a_v:v \\in V\\}$ to the nodes minimizing $\\sum_{v \\in V}\na_v$, such that the edge set $E_{\\bf a}=\\{e=uv: a_u \\geq t^e_u, a_v \\geq\nt^e_v\\}$ activated by ${\\bf a}$ covers $R$. We obtain ratio $1+\\omega(\\theta)\n\\approx \\ln \\theta-\\ln \\ln \\theta$ for the problem, where $\\omega(\\theta)$ is\nthe root of the equation $x+1=\\ln(\\theta/x)$ and $\\theta$ is a problem\nparameter. This result is based on a simple generic algorithm for the problem\nof minimizing a sum of a decreasing and a sub-additive set functions, which is\nof independent interest. As an application, we get that the above variant of\nFacility Location admits ratio $1+\\omega(\\theta)$; if for each facility all\nservice costs are identical then we show a better ratio $\\displaystyle\n1+\\max_{k \\geq 1} \\frac{H_k-1}{1+k/\\theta}$, where $H_k=\\sum_{i=1}^k 1/i$. For\nthe Min-Power Edge-Cover problem we improve the ratio $1.406$ of Calinescu et.\nal. (achieved by iterative randomized rounding) to $1+\\omega(1)<1.2785$. For\nunit thresholds we improve the ratio $73/60 \\approx 1.217$ to\n$\\frac{1555}{1347} \\approx 1.155$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 10:28:49 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Nutov", "Zeev", ""], ["Shalom", "Eli", ""]]}, {"id": "1812.09967", "submitter": "Tselil Schramm", "authors": "Ryan O'Donnell and Tselil Schramm", "title": "Sherali--Adams Strikes Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be any $n$-vertex graph whose random walk matrix has its nontrivial\neigenvalues bounded in magnitude by $1/\\sqrt{\\Delta}$ (for example, a random\ngraph $G$ of average degree~$\\Theta(\\Delta)$ typically has this property). We\nshow that the $\\exp\\Big(c \\frac{\\log n}{\\log \\Delta}\\Big)$-round Sherali--Adams\nlinear programming hierarchy certifies that the maximum cut in such a~$G$ is at\nmost $50.1\\%$ (in fact, at most $\\tfrac12 + 2^{-\\Omega(c)}$). For example, in\nrandom graphs with $n^{1.01}$ edges, $O(1)$ rounds suffice; in random graphs\nwith $n \\cdot \\text{polylog}(n)$ edges, $n^{O(1/\\log \\log n)} = n^{o(1)}$\nrounds suffice.\n  Our results stand in contrast to the conventional beliefs that linear\nprogramming hierarchies perform poorly for \\maxcut and other CSPs, and that\neigenvalue/SDP methods are needed for effective refutation. Indeed, our results\nimply that constant-round Sherali--Adams can strongly refute random Boolean\n$k$-CSP instances with $n^{\\lceil k/2 \\rceil + \\delta}$ constraints; previously\nthis had only been done with spectral algorithms or the SOS SDP hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 19:23:52 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Schramm", "Tselil", ""]]}, {"id": "1812.10131", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Till Fluschnik and Oxana Yu. Tsidulko", "title": "On approximate data reduction for the Rural Postman Problem: Theory and\n  experiments", "comments": "Added plot, definition of parameterized optimization problem,\n  argument against PSAKS for parameter b", "journal-ref": "Networks 76(4):485-508, 2020", "doi": "10.1002/net.21985", "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph with edge weights and a subset $R$ of its edges,\nthe Rural Postman Problem (RPP) is to find a closed walk of minimum total\nweight containing all edges of $R$. We prove that RPP is WK[1]-complete\nparameterized by the number and cost $d$ of edges traversed additionally to the\nrequired ones. Thus, in particular, RPP instances cannot be polynomial-time\ncompressed to instances of size polynomial in $d$ unless the polynomial-time\nhierarchy collapses. In contrast, denoting by $b\\leq 2d$ the number of vertices\nincident to an odd number of edges of $R$ and by $c\\leq d$ the number of\nconnected components formed by the edges in $R$, we show how to reduce any RPP\ninstance $I$ to an RPP instance $I'$ with $2b+O(c/\\varepsilon)$ vertices in\n$O(n^3)$ time so that any $\\alpha$-approximate solution for $I'$ gives an\n$\\alpha(1+\\varepsilon)$-approximate solution for $I$, for any $\\alpha\\geq 1$\nand $\\varepsilon>0$. That is, we provide a polynomial-size approximate\nkernelization scheme (PSAKS). We experimentally evaluate it on wide-spread\nbenchmark data sets as well as on two real snow plowing instances from Berlin.\nOn instances with few connected components, the number of vertices and required\nedges is reduced to about $50\\,\\%$ at a $1\\,\\%$ solution quality loss. We also\nmake first steps towards a PSAKS for the parameter $c$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 16:26:14 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:35:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 07:51:33 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Fluschnik", "Till", ""], ["Tsidulko", "Oxana Yu.", ""]]}, {"id": "1812.10215", "submitter": "James Drain", "authors": "James Drain", "title": "Two algorithms for the package-exchange robot-routing problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze two new algorithms for the package-exchange\nrobot-routing problem (PERR): restriction to inidividual paths (RIP) and\nbubbletree. RIP provably produces a makespan that is $O(\\text{SIC}+k^2)$, where\nSIC is the sum of the lengths of the individual paths and $k$ is the number of\nrobots. Bubbletree produces a makespan that is $O(n)$, where $n$ is the number\nof nodes. With optimizations bubbletree can also achieve a makespan of\n$O((k+l)\\text{log}k)$, where $l$ is the longest path from start to goal in the\nbubbletree subgraph.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 04:07:43 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Drain", "James", ""]]}, {"id": "1812.10309", "submitter": "Fotis Iliopoulos", "authors": "Fotis Iliopoulos and Alistair Sinclair", "title": "Efficiently list-edge coloring multigraphs asymptotically optimally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give polynomial time algorithms for the seminal results of Kahn, who\nshowed that the Goldberg-Seymour and List-Coloring conjectures for (list-)edge\ncoloring multigraphs hold asymptotically. Kahn's arguments are based on the\nprobabilistic method and are non-constructive. Our key insight is to show that\nthe main result of Achlioptas, Iliopoulos and Kolmogorov for analyzing local\nsearch algorithms can be used to make constructive applications of a powerful\nversion of the so-called Lopsided Lovasz Local Lemma. In particular, we use it\nto design algorithms that exploit the fact that correlations in the probability\nspaces on matchings used by Kahn decay with distance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 13:39:20 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 09:17:47 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Iliopoulos", "Fotis", ""], ["Sinclair", "Alistair", ""]]}, {"id": "1812.10334", "submitter": "Sergey Belim", "authors": "S.V. Belim, S.Yu. Belim", "title": "Implementation of Simplex Channels in the Blom's Keys Pre-Distribution\n  Scheme", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1210/1/012008", "report-no": null, "categories": "cs.CR cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In article the modification of the Blom's keys preliminary distribution\nscheme, considering the direction of information streams is suggested. For this\nmodification it is necessary to use function from three variables. Function of\nformation of key materials will be the asymmetrical. The exponential form of\nthis function which does not increase the volume of key materials is suggested.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 14:56:48 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Belim", "S. V.", ""], ["Belim", "S. Yu.", ""]]}, {"id": "1812.10431", "submitter": "Vijay Garg", "authors": "Vijay K. Garg", "title": "Applying Predicate Detection to the Constrained Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method to design parallel algorithms for constrained\ncombinatorial optimization problems. Our method solves and generalizes many\nclassical combinatorial optimization problems including the stable marriage\nproblem, the shortest path problem and the market clearing price problem. These\nthree problems are solved in the literature using Gale-Shapley algorithm,\nDijkstra's algorithm, and Demange, Gale, Sotomayor algorithm. Our method solves\nall these problems by casting them as searching for an element that satisfies\nan appropriate predicate in a distributive lattice. Moreover, it solves\ngeneralizations of all these problems - namely finding the optimal solution\nsatisfying additional constraints called {\\em lattice-linear} predicates. For\nstable marriage problems, an example of such a constraint is that Peter's\nregret is less than that of Paul. For shortest path problems, an example of\nsuch a constraint is that cost of reaching vertex $v_1$ is at least the cost of\nreaching vertex $v_2$. For the market clearing price problem, an example of\nsuch a constraint is that $item_1$ is priced at least as much as $item_2$. In\naddition to finding the optimal solution, our method is useful in enumerating\nall constrained stable matchings, and all constrained market clearing price\nvectors.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 17:57:02 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Garg", "Vijay K.", ""]]}, {"id": "1812.10499", "submitter": "Vijay Garg", "authors": "Vijay K. Garg", "title": "Removing Sequential Bottleneck of Dijkstra's Algorithm for the Shortest\n  Path Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  All traditional methods of computing shortest paths depend upon\nedge-relaxation where the cost of reaching a vertex from a source vertex is\npossibly decreased if that edge is used. We introduce a method which maintains\nlower bounds as well as upper bounds for reaching a vertex. This method enables\none to find the optimal cost for multiple vertices in one iteration and thereby\nreduces the sequential bottleneck in Dijkstra's algorithm.\n  We present four algorithms in this paper --- $SP_1$, $SP_2$, $SP_3$ and\n$SP_4$. $SP_1$ and $SP_2$ reduce the number of heap operations in Dijkstra's\nalgorithm. For directed acyclic graphs, or directed unweighted graphs they have\nthe optimal complexity of $O(e)$ where $e$ is the number of edges in the graph\nwhich is better than that of Dijkstra's algorithm. For general graphs, their\nworst case complexity matches that of Dijkstra's algorithm for a sequential\nimplementation but allows for greater parallelism. Algorithms $SP_3$ and $SP_4$\nallow for even more parallelism but with higher work complexity. Algorithm\n$SP_3$ requires $O(n + e(\\max(\\log n, \\Delta)))$ work where $n$ is the number\nof vertices and $\\Delta$ is the maximum in-degree of a node. Algorithm $SP_4$\nhas the most parallelism. It requires $O(ne)$ work. These algorithms generalize\nthe work by Crauser, Mehlhorn, Meyer, and Sanders on parallelizing Dijkstra's\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 19:00:15 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Garg", "Vijay K.", ""]]}, {"id": "1812.10563", "submitter": "Jack Wang", "authors": "Jack Wang", "title": "The Prophet Inequality Can Be Solved Optimally with a Single Set of\n  Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The setting of the classic prophet inequality is as follows: a gambler is\nshown the probability distributions of $n$ independent, non-negative random\nvariables with finite expectations. In their indexed order, a value is drawn\nfrom each distribution, and after every draw the gambler may choose to accept\nthe value and end the game, or discard the value permanently and continue the\ngame. What is the best performance that the gambler can achieve in comparison\nto a prophet who can always choose the highest value? Krengel, Sucheston, and\nGarling solved this problem in 1978, showing that there exists a strategy for\nwhich the gambler can achieve half as much reward as the prophet in\nexpectation. Furthermore, this result is tight.\n  In this work, we consider a setting in which the gambler is allowed much less\ninformation. Suppose that the gambler can only take one sample from each of the\ndistributions before playing the game, instead of knowing the full\ndistributions. We provide a simple and intuitive algorithm that recovers the\noriginal approximation of $\\frac{1}{2}$. Our algorithm works against even an\nalmighty adversary who always chooses a worst-case ordering, rather than the\nstandard offline adversary. The result also has implications for mechanism\ndesign -- there is much interest in designing competitive auctions with a\nfinite number of samples from value distributions rather than full\ndistributional knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 22:31:36 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Wang", "Jack", ""]]}, {"id": "1812.10582", "submitter": "Vaggos Chatziafratis", "authors": "Moses Charikar, Vaggos Chatziafratis, Rad Niazadeh, Grigory\n  Yaroslavtsev", "title": "Hierarchical Clustering for Euclidean Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on Hierarchical Clustering (HC), a well-studied problem in\nexploratory data analysis, have focused on optimizing various objective\nfunctions for this problem under arbitrary similarity measures. In this paper\nwe take the first step and give novel scalable algorithms for this problem\ntailored to Euclidean data in R^d and under vector-based similarity measures, a\nprevalent model in several typical machine learning applications. We focus\nprimarily on the popular Gaussian kernel and other related measures, presenting\nour results through the lens of the objective introduced recently by Moseley\nand Wang [2017]. We show that the approximation factor in Moseley and Wang\n[2017] can be improved for Euclidean data. We further demonstrate both\ntheoretically and experimentally that our algorithms scale to very high\ndimension d, while outperforming average-linkage and showing competitive\nresults against other less scalable approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 00:49:30 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Charikar", "Moses", ""], ["Chatziafratis", "Vaggos", ""], ["Niazadeh", "Rad", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1812.10629", "submitter": "Tatsuhiko Hatanaka", "authors": "Tatsuhiko Hatanaka, Takehiro Ito, Xiao Zhou", "title": "Complexity of Reconfiguration Problems for Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint satisfaction problem (CSP) is a well-studied combinatorial search\nproblem, in which we are asked to find an assignment of values to given\nvariables so as to satisfy all of given constraints. We study a reconfiguration\nvariant of CSP, in which we are given an instance of CSP together with its two\nsatisfying assignments, and asked to determine whether one assignment can be\ntransformed into the other by changing a single variable assignment at a time,\nwhile always remaining satisfying assignment. This problem generalizes several\nwell-studied reconfiguration problems such as Boolean satisfiability\nreconfiguration, vertex coloring reconfiguration, homomorphism reconfiguration.\nIn this paper, we study the problem from the viewpoints of polynomial-time\nsolvability and parameterized complexity, and give several interesting\nboundaries of tractable and intractable cases.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 05:19:03 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Hatanaka", "Tatsuhiko", ""], ["Ito", "Takehiro", ""], ["Zhou", "Xiao", ""]]}, {"id": "1812.10770", "submitter": "Alantha Newman", "authors": "Alantha Newman", "title": "Complex Semidefinite Programming and Max-k-Cut", "comments": "Appeared in Proceedings of SOSA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a second seminal paper on the application of semidefinite programming to\ngraph partitioning problems, Goemans and Williamson showed how to formulate and\nround a complex semidefinite program to give what is to date still the\nbest-known approximation guarantee of .836008 for Max-$3$-Cut. (This\napproximation ratio was also achieved independently by De Klerk et al.) Goemans\nand Williamson left open the problem of how to apply their techniques to\nMax-$k$-Cut for general $k$. They point out that it does not seem\nstraightforward or even possible to formulate a good quality complex\nsemidefinite program for the general Max-$k$-Cut problem, which presents a\nbarrier for the further application of their techniques.\n  We present a simple rounding algorithm for the standard semidefinite\nprogrammming relaxation of Max-$k$-Cut and show that it is equivalent to the\nrounding of Goemans and Williamson in the case of Max-$3$-Cut. This allows us\nto transfer the elegant analysis of Goemans and Williamson for Max-3-Cut to\nMax-$k$-Cut. For $k \\geq 4$, the resulting approximation ratios are about $.01$\nworse than the best known guarantees. Finally, we present a generalization of\nour rounding algorithm and conjecture (based on computational observations)\nthat it matches the best-known guarantees of De Klerk et al.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 17:02:48 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Newman", "Alantha", ""]]}, {"id": "1812.10797", "submitter": "Jian Lin", "authors": "Jian Lin, Zhong Yuan Lai, Xiaopeng Li", "title": "Quantum Adiabatic Algorithm Design using Reinforcement Learning", "comments": "11 pages, 10 figures", "journal-ref": "Phys. Rev. A 101, 052327 (2020)", "doi": "10.1103/PhysRevA.101.052327", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum algorithm design plays a crucial role in exploiting the computational\nadvantage of quantum devices. Here we develop a deep-reinforcement-learning\nbased approach for quantum adiabatic algorithm design. Our approach is\ngenerically applicable to a class of problems with solution hard-to-find but\neasy-to-verify, e.g., searching and NP-complete problems. We benchmark this\napproach in Grover-search and 3-SAT problems, and find that the\nadiabatic-algorithm obtained by our RL approach leads to significant\nimprovement in the resultant success probability. In application to Grover\nsearch, our RL-design automatically produces an adiabatic quantum algorithm\nthat has the quadratic speedup. We find for all our studied cases that\nquantitatively the RL-designed algorithm has a better performance compared to\nthe analytically constructed non-linear Hamiltonian path when the encoding\nHamiltonian is solvable, and that this RL-design approach remains applicable\neven when the non-linear Hamiltonian path is not analytically available. In\n3-SAT, we find RL-design has fascinating transferability---the adiabatic\nalgorithm obtained by training on a specific choice of clause number leads to\nbetter performance consistently over the linear algorithm on different clause\nnumbers. These findings suggest the applicability of reinforcement learning for\nautomated quantum adiabatic algorithm design. Further considering the\nestablished complexity-equivalence of circuit and adiabatic quantum algorithms,\nwe expect the RL-designed adiabatic algorithm to inspire novel circuit\nalgorithms as well. Our approach is potentially applicable to different quantum\nhardwares from trapped-ions and optical-lattices to superconducting-qubit\ndevices.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:00:03 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 04:50:35 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 04:42:26 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lin", "Jian", ""], ["Lai", "Zhong Yuan", ""], ["Li", "Xiaopeng", ""]]}, {"id": "1812.10808", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Above guarantee parameterization for vertex cover on graphs with maximum\n  degree 4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the vertex cover problem, the input is a graph $G$ and an integer $k$, and\nthe goal is to decide whether there is a set of vertices $S$ of size at most\n$k$ such that every edge of $G$ is incident on at least one vertex in $S$. We\nstudy the vertex cover problem on graphs with maximum degree 4 and minimum\ndegree at least 2, parameterized by $r = k-n/3$. We give an algorithm for this\nproblem whose running time is $O^*(1.6253^r)$. As a corollary, we obtain an\n$O^*(1.2403^k)$-time algorithm for vertex cover on graphs with maximum degree\n4.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:45:43 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1812.10837", "submitter": "Jesse Goodman", "authors": "Alexandre Bayen, Jesse Goodman, Eugene Vinitsky", "title": "On the Approximability of Time Disjoint Walks", "comments": "20 pages; extended (full) version; preliminary version appeared in\n  COCOA 2018; new results in the extended version include those listed in the\n  second paragraph of the abstract", "journal-ref": "Journal of Combinatorial Optimization (2020)", "doi": "10.1007/s10878-020-00525-z", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the combinatorial optimization problem Time Disjoint Walks\n(TDW), which has applications in collision-free routing of discrete objects\n(e.g., autonomous vehicles) over a network. This problem takes as input a\ndigraph $G$ with positive integer arc lengths, and $k$ pairs of vertices that\neach represent a trip demand from a source to a destination. The goal is to\nfind a walk and delay for each demand so that no two trips occupy the same\nvertex at the same time, and so that a min-max or min-sum objective over the\ntrip durations is realized.\n  We focus here on the min-sum variant of Time Disjoint Walks, although most of\nour results carry over to the min-max case. We restrict our study to various\nsubclasses of DAGs, and observe that there is a sharp complexity boundary\nbetween Time Disjoint Walks on oriented stars and on oriented stars with the\ncentral vertex replaced by a path. In particular, we present a poly-time\nalgorithm for min-sum and min-max TDW on the former, but show that min-sum TDW\non the latter is NP-hard.\n  Our main hardness result is that for DAGs with max degree $\\Delta\\leq3$,\nmin-sum Time Disjoint Walks is APX-hard. We present a natural approximation\nalgorithm for the same class, and provide a tight analysis. In particular, we\nprove that it achieves an approximation ratio of $\\Theta(k/\\log k)$ on\nbounded-degree DAGs, and $\\Theta(k)$ on DAGs and bounded-degree digraphs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 21:56:02 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 07:46:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bayen", "Alexandre", ""], ["Goodman", "Jesse", ""], ["Vinitsky", "Eugene", ""]]}, {"id": "1812.10854", "submitter": "Chris Schwiegelshohn", "authors": "Melanie Schmidt and Chris Schwiegelshohn and Christian Sohler", "title": "Fair Coresets and Streaming Algorithms for Fair k-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fair clustering problems as proposed by Chierichetti et al. (NIPS\n2017). Here, points have a sensitive attribute and all clusters in the solution\nare required to be balanced with respect to it (to counteract any form of\ndata-inherent bias). Previous algorithms for fair clustering do not scale well.\n  We show how to model and compute so-called coresets for fair clustering\nproblems, which can be used to significantly reduce the input data size. We\nprove that the coresets are composable and show how to compute them in a\nstreaming setting. Furthermore, we propose a variant of Lloyd's algorithm that\ncomputes fair clusterings and extend it to a fair k-means++ clustering\nalgorithm. We implement these algorithms and provide empirical evidence that\nthe combination of our approximation algorithms and the coreset construction\nyields a scalable algorithm for fair k-means clustering.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 00:51:19 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 10:29:13 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 23:54:05 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 12:07:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Schmidt", "Melanie", ""], ["Schwiegelshohn", "Chris", ""], ["Sohler", "Christian", ""]]}, {"id": "1812.10950", "submitter": "Torben Hagerup", "authors": "Torben Hagerup", "title": "Fast Breadth-First Search in Still Less Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that a breadth-first search in a directed or undirected graph\nwith $n$ vertices and $m$ edges can be carried out in $O(n+m)$ time with\n$n\\log_2 3+O((\\log n)^2)$ bits of working memory.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 10:43:12 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 14:16:45 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Hagerup", "Torben", ""]]}, {"id": "1812.10974", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa and Adri\\'an G\\'omez-Brand\\'on and Miguel A.\n  Mart\\'inez-Prieto and Jos\\'e R. Param\\'a", "title": "A Grammar-based Compressed Representation of 3D Trajectories", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "String Processing and Information Retrieval: 25th International\n  Symposium, SPIRE 2018, Lima, Peru, October 9-11, 2018, Proceedings. Springer\n  International Publishing. pp 102-116. ISBN: 9783030004781", "doi": "10.1007/978-3-030-00479-8_9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research has been published about trajectory management on the ground or\nat the sea, but compression or indexing of flight trajectories have usually\nbeen less explored. However, air traffic management is a challenge because\nairspace is becoming more and more congested, and large flight data collections\nmust be preserved and exploited for varied purposes. This paper proposes\n3DGraCT, a new method for representing these flight trajectories. It extends\nthe GraCT compact data structure to cope with a third dimension (altitude),\nwhile retaining its space/time complexities. 3DGraCT improves space\nrequirements of traditional spatio-temporal data structures by two orders of\nmagnitude, being competitive for the considered types of queries, even leading\nthe comparison for a particular one.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 12:24:18 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Param\u00e1", "Jos\u00e9 R.", ""]]}, {"id": "1812.10977", "submitter": "Susana Ladra", "authors": "Sandra \\'Alvarez-Garc\\'ia, Borja Freire, Susana Ladra, \\'Oscar\n  Pedreira", "title": "Compact and Efficient Representation of General Graph Databases", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Knowledge and Information Systems, 2018", "doi": "10.1007/s10115-018-1275-x", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a compact data structure to store labeled\nattributed graphs based on the k2-tree, which is a very compact data structure\ndesigned to represent a simple directed graph. The idea we propose can be seen\nas an extension of the k2-tree to support property graphs. In addition to the\nstatic approach, we also propose a dynamic version of the storage\nrepresentation, which allows exible schemas and insertion or deletion of data.\nWe provide an implementation of a basic set of operations, which can be\ncombined to form complex queries over these graphs with attributes. We evaluate\nthe performance of our proposal with existing graph database systems and prove\nthat our compact attributed graph representation obtains also competitive time\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 12:46:15 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["\u00c1lvarez-Garc\u00eda", "Sandra", ""], ["Freire", "Borja", ""], ["Ladra", "Susana", ""], ["Pedreira", "\u00d3scar", ""]]}, {"id": "1812.11003", "submitter": "Thomas Powell", "authors": "Thomas Powell", "title": "Sequential algorithms and the computational content of classical proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a correspondence between the theory of sequential algorithms and\nclassical reasoning, via Kreisel's no-counterexample interpretation. Our\nframework views realizers of the no-counterexample interpretation as dynamic\nprocesses which interact with an oracle, and allows these processes to be\nmodelled at any given level of abstraction. We discuss general constructions on\nalgorithms which represent specific patterns which often appear in classical\nreasoning, and in particular, we develop a computational interpretation of the\nrule of dependent choice which is phrased purely on the level of algorithms,\ngiving us a clearer insight into the computational meaning of proofs in\nclassical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 14:31:32 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Powell", "Thomas", ""]]}, {"id": "1812.11244", "submitter": "Antonio Fari\\~na", "authors": "Nieves R. Brisaboa and Diego Caro and Antonio Fari\\~na and M. Andrea\n  Rodriguez", "title": "Using Compressed Suffix-Arrays for a Compact Representation of\n  Temporal-Graphs", "comments": "41 pages, Information Sciences", "journal-ref": "Information Sciences Volume 465, October 2018, Pages 459-483", "doi": "10.1016/j.ins.2018.07.023", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs represent binary relationships that change along time. They\ncan model the dynamism of, for example, social and communication networks.\nTemporal graphs are defined as sets of contacts that are edges tagged with the\ntemporal intervals when they are active. This work explores the use of the\nCompressed Suffix Array (CSA), a well-known compact and self-indexed data\nstructure in the area of text indexing, to represent large temporal graphs. The\nnew structure, called Temporal Graph CSA (TGCSA), is experimentally compared\nwith the most competitive compact data structures in the state-of-the-art,\nnamely, EDGELOG and CET. The experimental results show that TGCSA obtains a\ngood space-time trade-off. It uses a reasonable space and is efficient for\nsolving complex temporal queries. Furthermore, TGCSA has wider expressive\ncapabilities than EDGELOG and CET, because it is able to represent temporal\ngraphs where contacts on an edge can temporally overlap.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 23:09:59 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Caro", "Diego", ""], ["Fari\u00f1a", "Antonio", ""], ["Rodriguez", "M. Andrea", ""]]}, {"id": "1812.11249", "submitter": "Antonio Fari\\~na", "authors": "Nieves R. Brisaboa and Antonio Fari\\~na and Daniil Galaktionov and M.\n  Andrea Rodriguez", "title": "A Compact Representation for Trips over Networks built on self-indexes", "comments": "42 pages", "journal-ref": "Information Systems, Volume 78, November 2018, Pages 1-22", "doi": "10.1016/j.is.2018.06.010", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the movements of objects (trips) over a network in a compact way\nwhile retaining the capability of exploiting such data effectively is an\nimportant challenge of real applications. We present a new Compact Trip\nRepresentation (CTR) that handles the spatio-temporal data associated with\nusers' trips over transportation networks. Depending on the network and types\nof queries, nodes in the network can represent intersections, stops, or even\nstreet segments.\n  CTR represents separately sequences of nodes and the time instants when users\ntraverse these nodes. The spatial component is handled with a data structure\nbased on the well-known Compressed Suffix Array (CSA), which provides both a\ncompact representation and interesting indexing capabilities. The temporal\ncomponent is self-indexed with either a Hu-Tucker-shaped Wavelet-tree or a\nWavelet Matrix that solve range-interval queries efficiently. We show how CTR\ncan solve relevant counting-based spatial, temporal, and spatio-temporal\nqueries over large sets of trips. Experimental results show the space\nrequirements (around 50-70% of the space needed by a compact non-indexed\nbaseline) and query efficiency (most queries are solved in the range of 1-1000\nmicroseconds) of CTR.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 23:57:14 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Fari\u00f1a", "Antonio", ""], ["Galaktionov", "Daniil", ""], ["Rodriguez", "M. Andrea", ""]]}, {"id": "1812.11254", "submitter": "Faisal Abu-Khzam", "authors": "Faisal N. Abu-Khzam and Bachir M. Chahine", "title": "A Dynamically Turbo-Charged Greedy Heuristic for Graph Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic version of the graph coloring problem and prove its\nfixed-parameter tractability with respect to the edit-parameter. This is used\nto present a {\\em turbo-charged} heuristic for the problem that works by\ncombining the turbo-charging technique with other standard heuristic tools,\nincluding greedy coloring. The recently introduced turbo-charging idea is\nfurther enhanced in this paper by introducing a dynamic version of the so\ncalled {\\em moment of regret} and {\\em rollback points}. Experiments comparing\nour turbo-charging algorithm to other heuristics demonstrate its effectiveness.\nOur algorithm often produced results that were either exact or better than all\nthe other available heuristics.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 00:47:50 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 23:00:06 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Chahine", "Bachir M.", ""]]}, {"id": "1812.11476", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Himanshu Tyagi", "title": "Inference under Information Constraints I: Lower Bounds from Chi-Square\n  Contraction", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple players are each given one independent sample, about which they can\nonly provide limited information to a central referee. Each player is allowed\nto describe its observed sample to the referee using a channel from a family of\nchannels $\\mathcal{W}$, which can be instantiated to capture both the\ncommunication- and privacy-constrained settings and beyond. The referee uses\nthe messages from players to solve an inference problem for the unknown\ndistribution that generated the samples. We derive lower bounds for sample\ncomplexity of learning and testing discrete distributions in this\ninformation-constrained setting.\n  Underlying our bounds is a characterization of the contraction in chi-square\ndistances between the observed distributions of the samples when information\nconstraints are placed. This contraction is captured in a local neighborhood in\nterms of chi-square and decoupled chi-square fluctuations of a given channel,\ntwo quantities we introduce. The former captures the average distance between\ndistributions of channel output for two product distributions on the input, and\nthe latter for a product distribution and a mixture of product distribution on\nthe input. Our bounds are tight for both public- and private-coin protocols.\nInterestingly, the sample complexity of testing is order-wise higher when\nrestricted to private-coin protocols.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 06:29:14 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 18:04:55 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 01:55:18 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 04:30:54 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1812.11564", "submitter": "Sandeep Silwal", "authors": "Sandeep Silwal, Jonathan Tidor", "title": "Spectral methods for testing cluster structure of graphs", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of graph property testing, we study the problem of\ndetermining if a graph admits a cluster structure. We say that a graph is $(k,\n\\phi)$-clusterable if it can be partitioned into at most $k$ parts such that\neach part has conductance at least $\\phi$. We present an algorithm that accepts\nall graphs that are $(2, \\phi)$-clusterable with probability at least\n$\\frac{2}3$ and rejects all graphs that are $\\epsilon$-far from $(2,\n\\phi^*)$-clusterable for $\\phi^* \\le \\mu \\phi^2 \\epsilon^2$ with probability at\nleast $\\frac{2}3$ where $\\mu > 0$ is a parameter that affects the query\ncomplexity. This improves upon the work of Czumaj, Peng, and Sohler by removing\na $\\log n$ factor from the denominator of the bound on $\\phi^*$ for the case of\n$k=2$. Our work was concurrent with the work of Chiplunkar et al.\\@ who\nachieved the same improvement for all values of $k$. Our approach for the case\n$k=2$ relies on the geometric structure of the eigenvectors of the graph\nLaplacian and results in an algorithm with query complexity $O(n^{1/2+O(1)\\mu}\n\\cdot \\text{poly}(1/\\epsilon, 1/\\phi,\\log n))$.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 16:08:13 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Silwal", "Sandeep", ""], ["Tidor", "Jonathan", ""]]}, {"id": "1812.11594", "submitter": "Burcak Otlu", "authors": "Burcak Otlu and Tolga Can", "title": "Joint Overlap Analysis of Multiple Genomic Interval Sets", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing (NGS) technologies have produced large volumes of\ngenomic data. One common operation on heterogeneous genomic data is genomic\ninterval intersection. Most of the existing tools impose restrictions such as\nnot allowing nested intervals or requiring intervals to be sorted when finding\noverlaps in two or more interval sets. We proposed segment tree (ST) and\nindexed segment tree forest (ISTF) based solutions for intersection of multiple\ngenomic interval sets in parallel. We developed these methods as a tool, Joint\nOverlap Analysis (JOA), which takes n interval sets and finds overlapping\nintervals with no constraints on the given intervals. The proposed indexed\nsegment tree forest is a novel composite data structure, which leverages on\nindexing and natural binning of a segment tree. We also presented construction\nand search algorithms for this novel data structure. We compared JOA ST and JOA\nISTF with each other, and with other interval intersection tools for\nverification of its correctness and for showing that it attains comparable\nexecution times. We implemented JOA in Java using the fork/join framework which\nspeeds up parallel processing by taking advantage of all available processor\ncores. We compared JOA ST with JOA ISTF and showed that segment tree and\nindexed segment tree forest methods are comparable with each other in terms of\nexecution time and memory usage. We also carried out execution time comparison\nanalysis for JOA and other tools and demonstrated that JOA has comparable\nexecution time and is able to further reduce its running time by using more\nprocessors per node. JOA can be run using its GUI or as a command line tool.\nJOA is available with source code at https://github.com/burcakotlu/JOA/. A user\nmanual is provided at https://joa.readthedocs.org\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 19:14:41 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Otlu", "Burcak", ""], ["Can", "Tolga", ""]]}, {"id": "1812.11774", "submitter": "Uriel Feige", "authors": "Uriel Feige", "title": "Tighter bounds for online bipartite matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online bipartite matching problem, introduced by Karp, Vazirani\nand Vazirani [1990]. For bipartite graphs with matchings of size $n$, it is\nknown that the Ranking randomized algorithm matches at least $(1 -\n\\frac{1}{e})n$ edges in expectation. It is also known that no online algorithm\nmatches more than $(1 - \\frac{1}{e})n + O(1)$ edges in expectation, when the\ninput is chosen from a certain distribution that we refer to as $D_n$. This\nupper bound also applies to fractional matchings. We review the known proofs\nfor this last statement. In passing we observe that the $O(1)$ additive term\n(in the upper bound for fractional matching) is $\\frac{1}{2} - \\frac{1}{2e} +\nO(\\frac{1}{n})$, and that this term is tight: the online algorithm known as\nBalance indeed produces a fractional matching of this size. We provide a new\nproof that exactly characterizes the expected cardinality of the (integral)\nmatching produced by Ranking when the input graph comes from the support of\n$D_n$. This expectation turns out to be $(1 - \\frac{1}{e})n + 1 - \\frac{2}{e} +\nO(\\frac{1}{n!})$, and serves as an upper bound on the performance ratio of any\nonline (integral) matching algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 12:23:27 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Feige", "Uriel", ""]]}, {"id": "1812.11896", "submitter": "Inbal Talgam-Cohen", "authors": "Tim Roughgarden and Inbal Talgam-Cohen", "title": "Approximately Optimal Mechanism Design", "comments": "Preprint; final version to appear in Annual Reviews of Economics,\n  August 2019. August 2020: Fixed small typo in funding info. arXiv admin note:\n  text overlap with arXiv:1406.6773", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal mechanism design enjoys a beautiful and well-developed theory, and\nalso a number of killer applications. Rules of thumb produced by the field\ninfluence everything from how governments sell wireless spectrum licenses to\nhow the major search engines auction off online advertising. There are,\nhowever, some basic problems for which the traditional optimal mechanism design\napproach is ill-suited---either because it makes overly strong assumptions, or\nbecause it advocates overly complex designs. This survey reviews several common\nissues with optimal mechanisms, including exorbitant communication,\ncomputation, and informational requirements; and it presents several examples\ndemonstrating that passing to the relaxed goal of an approximately optimal\nmechanism allows us to reason about fundamental questions that seem out of\nreach of the traditional theory.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 16:54:47 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 19:41:17 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Roughgarden", "Tim", ""], ["Talgam-Cohen", "Inbal", ""]]}]