[{"id": "1902.00179", "submitter": "Ryan Spring", "authors": "Ryan Spring, Anastasios Kyrillidis, Vijai Mohan, Anshumali Shrivastava", "title": "Compressing Gradient Optimizers via Count-Sketches", "comments": "Initially submitted to WWW 2019 (November 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many popular first-order optimization methods (e.g., Momentum, AdaGrad, Adam)\naccelerate the convergence rate of deep learning models. However, these\nalgorithms require auxiliary parameters, which cost additional memory\nproportional to the number of parameters in the model. The problem is becoming\nmore severe as deep learning models continue to grow larger in order to learn\nfrom complex, large-scale datasets. Our proposed solution is to maintain a\nlinear sketch to compress the auxiliary variables. We demonstrate that our\ntechnique has the same performance as the full-sized baseline, while using\nsignificantly less space for the auxiliary variables. Theoretically, we prove\nthat count-sketch optimization maintains the SGD convergence rate, while\ngracefully reducing memory usage for large-models. On the large-scale 1-Billion\nWord dataset, we save 25% of the memory used during training (8.6 GB instead of\n11.7 GB) by compressing the Adam optimizer in the Embedding and Softmax layers\nwith negligible accuracy and performance loss. For an Amazon extreme\nclassification task with over 49.5 million classes, we also reduce the training\ntime by 38%, by increasing the mini-batch size 3.5x using our count-sketch\noptimizer.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 04:43:08 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 14:58:26 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Spring", "Ryan", ""], ["Kyrillidis", "Anastasios", ""], ["Mohan", "Vijai", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1902.00216", "submitter": "Katsuhito Nakashima", "authors": "Katsuhito Nakashima and Diptarama Hendrian and Ryo Yoshinaka and Ayumi\n  Shinohara", "title": "An Extension of Linear-size Suffix Tries for Parameterized Strings", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new indexing structure for parameterized strings\nwhich we call PLSTs, by generalizing linear-size suffix tries for ordinary\nstrings. Two parameterized strings are said to match if there is a bijection on\nthe symbol set that makes the two coincide. PLSTs are applicable to the\nparameterized pattern matching problem, which is to decide whether the input\nparameterized text has a substring that matches the input parameterized\npattern. The size of PLSTs is linear in the text size, with which our algorithm\nsolves the parameterized pattern matching problem in linear time in the pattern\nsize. PLSTs can be seen as a compacted version of parameterized suffix tries\nand a combination of linear-size suffix tries and parameterized suffix trees.\nWe experimentally show that PLSTs are more space efficient than parameterized\nsuffix trees for highly repetitive strings.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 08:20:26 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 03:35:56 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 04:37:56 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Nakashima", "Katsuhito", ""], ["Hendrian", "Diptarama", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1902.00256", "submitter": "Feiyang Chen", "authors": "Feiyang Chen, Nan Chen, Hanyang Mao, Hanlin Hu", "title": "The Application of Bipartite Matching in Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimized assignment of staff is of great significance for improving the\nproduction efficiency of the society. For specific tasks, the key to optimizing\nstaffing is personnel scheduling. The assignment problem is classical in the\npersonnel scheduling. In this paper, we abstract it as an optimal matching\nmodel of a bipartite graph and propose the Ultimate Hungarian Algorithm(UHA).\nBy introducing feasible labels, iteratively searching for the augmenting path\nto get the optimal match(maximum-weight matching). And we compare the algorithm\nwith the traditional brute force method, then conclude that our algorithm has\nlower time complexity and can solve the problems of maximum-weight matching\nmore effectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 10:02:03 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Chen", "Feiyang", ""], ["Chen", "Nan", ""], ["Mao", "Hanyang", ""], ["Hu", "Hanlin", ""]]}, {"id": "1902.00257", "submitter": "Feiyang Chen", "authors": "Feiyang Chen, Nan Chen, Hanyang Mao, Hanlin Hu", "title": "An efficient sorting algorithm - Ultimate Heapsort(UHS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the development of computer theory, the sorting algorithm is\nemerging in an endless stream. Inspired by decrease and conquer method, we\npropose a brand new sorting algorithmUltimately Heapsort. The algorithm\nconsists of two parts: building a heap and adjusting a heap. Through the\nasymptotic analysis and experimental analysis of the algorithm, the time\ncomplexity of our algorithm can reach O(nlogn) under any condition. Moreover,\nits space complexity is only O(1). It can be seen that our algorithm is\nsuperior to all previous algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 10:06:52 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Chen", "Feiyang", ""], ["Chen", "Nan", ""], ["Mao", "Hanyang", ""], ["Hu", "Hanlin", ""]]}, {"id": "1902.00340", "submitter": "Anastasiia Koloskova", "authors": "Anastasia Koloskova, Sebastian U. Stich, Martin Jaggi", "title": "Decentralized Stochastic Optimization and Gossip Algorithms with\n  Compressed Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized stochastic optimization with the objective function\n(e.g. data samples for machine learning task) being distributed over $n$\nmachines that can only communicate to their neighbors on a fixed communication\ngraph. To reduce the communication bottleneck, the nodes compress (e.g.\nquantize or sparsify) their model updates. We cover both unbiased and biased\ncompression operators with quality denoted by $\\omega \\leq 1$ ($\\omega=1$\nmeaning no compression). We (i) propose a novel gossip-based stochastic\ngradient descent algorithm, CHOCO-SGD, that converges at rate\n$\\mathcal{O}\\left(1/(nT) + 1/(T \\delta^2 \\omega)^2\\right)$ for strongly convex\nobjectives, where $T$ denotes the number of iterations and $\\delta$ the\neigengap of the connectivity matrix. Despite compression quality and network\nconnectivity affecting the higher order terms, the first term in the rate,\n$\\mathcal{O}(1/(nT))$, is the same as for the centralized baseline with exact\ncommunication. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the\naverage consensus problem that converges in time\n$\\mathcal{O}(1/(\\delta^2\\omega) \\log (1/\\epsilon))$ for accuracy $\\epsilon >\n0$. This is (up to our knowledge) the first gossip algorithm that supports\narbitrary compressed messages for $\\omega > 0$ and still exhibits linear\nconvergence. We (iii) show in experiments that both of our algorithms do\noutperform the respective state-of-the-art baselines and CHOCO-SGD can reduce\ncommunication by at least two orders of magnitudes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:11:20 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Koloskova", "Anastasia", ""], ["Stich", "Sebastian U.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1902.00488", "submitter": "Rahul Jain", "authors": "Rahul Jain and Raghunath Tewari", "title": "Grid Graph Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reachability problem is to determine if there exists a path from one\nvertex to another in a graph. Grid graphs are the class of graphs where\nvertices are present on the lattice points of a two-dimensional grid, and an\nedge can occur between a vertex and its immediate horizontal or vertical\nneighbor only. Asano et al. presented the first simultaneous time space bound\nfor reachability in grid graphs by presenting an algorithm that solves the\nproblem in polynomial time and $O(n^{1/2 + \\epsilon})$ space. In 2018, the\nspace bound was improved to $\\tilde{O}(n^{1/3})$ by Ashida and Nakagawa. In\nthis paper, we show that reachability in an $n$ vertex grid graph can be\ndecided by an algorithm using $O(n^{1/4 + \\epsilon})$ space and polynomial time\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:22:11 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 09:33:00 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Jain", "Rahul", ""], ["Tewari", "Raghunath", ""]]}, {"id": "1902.00490", "submitter": "Artem Lutov", "authors": "Artem Lutov, Soheil Roshankish, Mourad Khayati and Philippe\n  Cudr\\'e-Mauroux", "title": "StaTIX - Statistical Type Inference on Linked Data", "comments": "Application sources and executables:\n  https://github.com/eXascaleInfolab/StaTIX", "journal-ref": "2018 IEEE International Conference on Big Data", "doi": "10.1109/BigData.2018.8622285", "report-no": null, "categories": "stat.AP cs.DS cs.SI physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large knowledge bases typically contain data adhering to various schemas with\nincomplete and/or noisy type information. This seriously complicates further\nintegration and post-processing efforts, as type information is crucial in\ncorrectly handling the data. In this paper, we introduce a novel statistical\ntype inference method, called StaTIX, to effectively infer instance types in\nLinked Data sets in a fully unsupervised manner. Our inference technique\nleverages a new hierarchical clustering algorithm that is robust, highly\neffective, and scalable. We introduce a novel approach to reduce the processing\ncomplexity of the similarity matrix specifying the relations between various\ninstances in the knowledge base. This approach speeds up the inference process\nwhile also improving the correctness of the inferred types due to the noise\nattenuation in the input data. We further optimize the clustering process by\nintroducing a dedicated hash function that speeds up the inference process by\norders of magnitude without negatively affecting its accuracy. Finally, we\ndescribe a new technique to identify representative clusters from the\nmulti-scale output of our clustering algorithm to further improve the accuracy\nof the inferred types. We empirically evaluate our approach on several\nreal-world datasets and compare it to the state of the art. Our results show\nthat StaTIX is more efficient than existing methods (both in terms of speed and\nmemory consumption) as well as more effective. StaTIX reduces the F1-score\nerror of the predicted types by about 40% on average compared to the state of\nthe art and improves the execution time by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:25:08 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 09:48:23 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Lutov", "Artem", ""], ["Roshankish", "Soheil", ""], ["Khayati", "Mourad", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1902.00732", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Scheduling with Predictions and the Price of Misprediction", "comments": "14 pages, submitted; version 2 has updated references, some\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many traditional job scheduling settings, it is assumed that one knows the\ntime it will take for a job to complete service. In such cases, strategies such\nas shortest job first can be used to improve performance in terms of measures\nsuch as the average time a job waits in the system. We consider the setting\nwhere the service time is not known, but is predicted by for example a machine\nlearning algorithm. Our main result is the derivation, under natural\nassumptions, of formulae for the performance of several strategies for queueing\nsystems that use predictions for service times in order to schedule jobs. As\npart of our analysis, we suggest the framework of the \"price of misprediction,\"\nwhich offers a measure of the cost of using predicted information.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 14:56:16 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 15:35:49 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1902.00804", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Itemsets for Real-valued Datasets", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2013.138", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern mining is one of the most well-studied subfields in exploratory data\nanalysis. While there is a significant amount of literature on how to discover\nand rank itemsets efficiently from binary data, there is surprisingly little\nresearch done in mining patterns from real-valued data. In this paper we\npropose a family of quality scores for real-valued itemsets. We approach the\nproblem by considering casting the dataset into a binary data and computing the\nsupport from this data. This naive approach requires us to select thresholds.\nTo remedy this, instead of selecting one set of thresholds, we treat thresholds\nas random variables and compute the average support. We show that we can\ncompute this support efficiently, and we also introduce two normalisations,\nnamely comparing the support against the independence assumption and, more\ngenerally, against the partition assumption. Our experimental evaluation\ndemonstrates that we can discover statistically significant patterns\nefficiently.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 22:21:53 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.00846", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Micheal Houle, Micheal Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M\n  Databases", "comments": "Northeast Database Data 2019 (MIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large scale networks requires high performance streaming updates of\ngraph representations of these data. Associative arrays are mathematical\nobjects combining properties of spreadsheets, databases, matrices, and graphs,\nand are well-suited for representing and analyzing streaming network data. The\nDynamic Distributed Dimensional Data Model (D4M) library implements associative\narrays in a variety of languages (Python, Julia, and Matlab/Octave) and\nprovides a lightweight in-memory database. Associative arrays are designed for\nblock updates. Streaming updates to a large associative array requires a\nhierarchical implementation to optimize the performance of the memory\nhierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on\n1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of\n1,900,000,000 updates per second. This capability allows the MIT SuperCloud to\nanalyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 04:58:07 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Micheal", ""], ["Jones", "Micheal", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1902.00911", "submitter": "Nidhal Jelassi", "authors": "M. Nidhal Jelassi", "title": "Study, representation and applications of hypergraph minimal\n  transversals", "comments": "PhD thesis, in French, University of Tunis El Manar (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is part of the field of the hypergraph theory and focuses on\nhypergraph minimal transversal. The problem of extracting the minimal\ntransversals from a hypergraph received the interest of many researchers as\nshown the number of algorithms proposed in the literature, and this is mainly\ndue to the solutions offered by the minimal transversal in various application\nareas such as databases, artificial intelligence, e-commerce, semantic web,\netc. In view of the wide range of fields of minimal transversal application and\nthe interest they generate, the objective of this thesis is to explore new\napplication paths of minimal transversal by proposing methods to optimize the\nextraction. This has led to three proposed contributions in this thesis. The\nfirst approach takes advantage of the emergence of Web 2.0 and, therefore,\nsocial networks using minimal transversal for the detection of important actors\nwithin these networks. The second part of research in this thesis has focused\non reducing the number of hypergraph minimal transversal. A concise and\naccurate representation of minimal transversal was proposed and is based on the\nconstruction of an irredundant hypergraph, hence are calculated the irredundant\nminimal transversal of the initial hypergraph. An application of this\nrepresentation to the dependency inference problem is presented to illustrate\nthe usefulness of this approach. The last approach includes the hypergraph\ndecomposition into partial hypergraph the local minimal transversal are\ncalculated and their Cartesian product can generate all the hypergraph\ntransversal sets. Different experimental studies have shown the value of these\nproposed approaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 15:41:09 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Jelassi", "M. Nidhal", ""]]}, {"id": "1902.00919", "submitter": "Wenxin Li", "authors": "Wenxin Li, Joohyun Lee", "title": "A Faster FPTAS for Knapsack Problem With Cardinality Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $K$-item knapsack problem (i.e., $1.5$-dimensional KP), which is\na generalization of the famous 0-1 knapsack problem (i.e., $1$-dimensional KP)\nin which an upper bound $K$ is imposed on the number of items selected. This\nproblem is of fundamental importance and is known to have a broad range of\napplications in various fields. It is well known that, there is no FPTAS for\nthe $d$-dimensional knapsack problem when $d\\geq 2$, unless P $=$ NP. While the\n$K$-item knapsack problem is known to admit an FPTAS, the complexity of all\nexisting FPTASs have a high dependency on the cardinality bound $K$ and\napproximation error $\\varepsilon$, which could result in inefficiencies\nespecially when $K$ and $\\varepsilon^{-1}$ increase. The current best results\nare due to Mastrolilli and Hutter (2006), in which two schemes are presented\nexhibiting a space-time tradeoff--one scheme with time complexity\n$O(n+Kz^{2}/\\varepsilon^{2})$ and space complexity $O(n+z^{3}/\\varepsilon)$,\nwhile another scheme requires $O(n+(Kz^{2}+z^{4})/\\varepsilon^{2})$ run-time\nbut only needs $O(n+z^{2}/\\varepsilon)$ space, where\n$z=\\min\\{K,1/\\varepsilon\\}$. In this paper we close the space-time tradeoff\nexhibited in the state-of-the-art by designing a new FPTAS with a run-time of\n$\\widetilde{O}(n+z^{2}/\\varepsilon^{2})$, while simultaneously reaching the\n$O(n+z^{2}/\\varepsilon)$ space bound. Our scheme provides $\\widetilde{O}(K)$\nand $O(z)$ improvements on the state-of-the-art algorithms in time and space\ncomplexity respectively, and is the first scheme that achieves a run-time that\nis independent of cardinality bound $K$ (up to logarithmic factors) under fixed\n$\\varepsilon$. Another salient feature of our scheme is that it is the first\nFPTAS that achieves better time and space complexity bounds than the very first\nstandard FPTAS over all parameter regimes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 16:29:01 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 04:12:25 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 17:22:13 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 07:27:40 GMT"}, {"version": "v5", "created": "Sat, 12 Dec 2020 12:41:39 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Wenxin", ""], ["Lee", "Joohyun", ""]]}, {"id": "1902.01002", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Ranking Episodes using a Partition Model", "comments": null, "journal-ref": "Data Min Knowl Disc (2015) 29: 1312", "doi": "10.1007/s10618-015-0419-9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest setbacks in traditional frequent pattern mining is that\noverwhelmingly many of the discovered patterns are redundant. A prototypical\nexample of such redundancy is a freerider pattern where the pattern contains a\ntrue pattern and some additional noise events. A technique for filtering\nfreerider patterns that has proved to be efficient in ranking itemsets is to\nuse a partition model where a pattern is divided into two subpatterns and the\nobserved support is compared to the expected support under the assumption that\nthese two subpatterns occur independently.\n  In this paper we develop a partition model for episodes, patterns discovered\nfrom sequential data. An episode is essentially a set of events, with possible\nrestrictions on the order of events. Unlike with itemset mining, computing the\nexpected support of an episode requires surprisingly sophisticated methods. In\norder to construct the model, we partition the episode into two subepisodes. We\nthen model how likely the events in each subepisode occur close to each other.\nIf this probability is high---which is often the case if the subepisode has a\nhigh support---then we can expect that when one event from a subepisode occurs,\nthen the remaining events occur also close by. This approach increases the\nexpected support of the episode, and if this increase explains the observed\nsupport, then we can deem the episode uninteresting. We demonstrate in our\nexperiments that using the partition model can effectively and efficiently\nreduce the redundancy in episodes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 01:30:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.01028", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "Can SGD Learn Recurrent Neural Networks with Provable Generalization?", "comments": "V2 polishes writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are among the most popular models in\nsequential data analysis. Yet, in the foundational PAC learning language, what\nconcept class can it learn? Moreover, how can the same recurrent unit\nsimultaneously learn functions from different input tokens to different output\ntokens, without affecting each other? Existing generalization bounds for RNN\nscale exponentially with the input length, significantly limiting their\npractical implications.\n  In this paper, we show using the vanilla stochastic gradient descent (SGD),\nRNN can actually learn some notable concept class efficiently, meaning that\nboth time and sample complexity scale polynomially in the input length (or\nalmost polynomially, depending on the concept). This concept class at least\nincludes functions where each output token is generated from inputs of earlier\ntokens using a smooth two-layer neural network.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 04:21:00 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 10:43:43 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1902.01088", "submitter": "Nicola Prezza", "authors": "Jarno Alanko, Giovanna D'Agostino, Alberto Policriti, Nicola Prezza", "title": "Regular Languages meet Prefix Sorting", "comments": "added minimization theorems; uploaded submitted version; New version\n  with new results (W-MH theorem, linear determinization), added author:\n  Giovanna D'Agostino", "journal-ref": null, "doi": "10.1137/1.9781611975994.55", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing strings via prefix (or suffix) sorting is, arguably, one of the most\nsuccessful algorithmic techniques developed in the last decades. Can indexing\nbe extended to languages? The main contribution of this paper is to initiate\nthe study of the sub-class of regular languages accepted by an automaton whose\nstates can be prefix-sorted. Starting from the recent notion of Wheeler graph\n[Gagie et al., TCS 2017]-which extends naturally the concept of prefix sorting\nto labeled graphs-we investigate the properties of Wheeler languages, that is,\nregular languages admitting an accepting Wheeler finite automaton.\nInterestingly, we characterize this family as the natural extension of regular\nlanguages endowed with the co-lexicographic ordering: when sorted, the strings\nbelonging to a Wheeler language are partitioned into a finite number of\nco-lexicographic intervals, each formed by elements from a single Myhill-Nerode\nequivalence class. Moreover: (i) We show that every Wheeler NFA (WNFA) with $n$\nstates admits an equivalent Wheeler DFA (WDFA) with at most $2n-1-|\\Sigma|$\nstates that can be computed in $O(n^3)$ time. This is in sharp contrast with\ngeneral NFAs. (ii) We describe a quadratic algorithm to prefix-sort a proper\nsuperset of the WDFAs, a $O(n\\log n)$-time online algorithm to sort acyclic\nWDFAs, and an optimal linear-time offline algorithm to sort general WDFAs. By\ncontribution (i), our algorithms can also be used to index any WNFA at the\nmoderate price of doubling the automaton's size. (iii) We provide a\nminimization theorem that characterizes the smallest WDFA recognizing the same\nlanguage of any input WDFA. The corresponding constructive algorithm runs in\noptimal linear time in the acyclic case, and in $O(n\\log n)$ time in the\ngeneral case. (iv) We show how to compute the smallest WDFA equivalent to any\nacyclic DFA in nearly-optimal time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 09:00:36 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 08:21:50 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 15:06:55 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 07:53:59 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Alanko", "Jarno", ""], ["D'Agostino", "Giovanna", ""], ["Policriti", "Alberto", ""], ["Prezza", "Nicola", ""]]}, {"id": "1902.01128", "submitter": "Kui Zhao", "authors": "Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, Cheng Yang", "title": "A Unified Framework for Marketing Budget Allocation", "comments": "KDD'19, 11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While marketing budget allocation has been studied for decades in traditional\nbusiness, nowadays online business brings much more challenges due to the\ndynamic environment and complex decision-making process. In this paper, we\npresent a novel unified framework for marketing budget allocation. By\nleveraging abundant data, the proposed data-driven approach can help us to\novercome the challenges and make more informed decisions. In our approach, a\nsemi-black-box model is built to forecast the dynamic market response and an\nefficient optimization method is proposed to solve the complex allocation task.\nFirst, the response in each market-segment is forecasted by exploring\nhistorical data through a semi-black-box model, where the capability of logit\ndemand curve is enhanced by neural networks. The response model reveals\nrelationship between sales and marketing cost. Based on the learned model,\nbudget allocation is then formulated as an optimization problem, and we design\nefficient algorithms to solve it in both continuous and discrete settings.\nSeveral kinds of business constraints are supported in one unified optimization\nparadigm, including cost upper bound, profit lower bound, or ROI lower bound.\nThe proposed framework is easy to implement and readily to handle large-scale\nproblems. It has been successfully applied to many scenarios in Alibaba Group.\nThe results of both offline experiments and online A/B testing demonstrate its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 11:27:11 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 03:29:12 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 04:36:22 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Zhao", "Kui", ""], ["Hua", "Junhao", ""], ["Yan", "Ling", ""], ["Zhang", "Qi", ""], ["Xu", "Huan", ""], ["Yang", "Cheng", ""]]}, {"id": "1902.01280", "submitter": "Giovanni Manzini", "authors": "Raffaele Giancarlo, Giovanni Manzini, Giovanna Rosone, Marinella\n  Sciortino", "title": "A New Class of Searchable and Provably Highly Compressible String\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform is a string transformation that plays a\nfundamental role for the design of self-indexing compressed data structures.\nOver the years, researchers have successfully extended this transformation\noutside the domains of strings. However, efforts to find non-trivial\nalternatives of the original, now 25 years old, Burrows-Wheeler string\ntransformation have met limited success. In this paper we bring new lymph to\nthis area by introducing a whole new family of transformations that have all\nthe myriad virtues of the BWT: they can be computed and inverted in linear\ntime, they produce provably highly compressible strings, and they support\nlinear time pattern search directly on the transformed string. This new family\nis a special case of a more general class of transformations based on context\nadaptive alphabet orderings, a concept introduced here. This more general class\nincludes also the Alternating BWT, another invertible string transforms\nrecently introduced in connection with a generalization of Lyndon words.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:15:38 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Giancarlo", "Raffaele", ""], ["Manzini", "Giovanni", ""], ["Rosone", "Giovanna", ""], ["Sciortino", "Marinella", ""]]}, {"id": "1902.01331", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Safe projections of binary data sets", "comments": null, "journal-ref": null, "doi": "10.1007/s00236-006-0009-9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectivity estimation of a boolean query based on frequent itemsets can be\nsolved by describing the problem by a linear program. However, the number of\nvariables in the equations is exponential, rendering the approach tractable\nonly for small-dimensional cases. One natural approach would be to project the\ndata to the variables occurring in the query. This can, however, change the\noutcome of the linear program.\n  We introduce the concept of safe sets: projecting the data to a safe set does\nnot change the outcome of the linear program. We characterise safe sets using\ngraph theoretic concepts and give an algorithm for finding minimal safe sets\ncontaining given attributes. We describe a heuristic algorithm for finding\nalmost-safe sets given a size restriction, and show empirically that these sets\noutperform the trivial projection.\n  We also show a connection between safe sets and Markov Random Fields and use\nit to further reduce the number of variables in the linear program, given some\nregularity assumptions on the frequent itemsets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:42:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.01334", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Distances between Data Sets Based on Summary Statistics", "comments": null, "journal-ref": "JMLR 8, 2007, 131-154", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concepts of similarity and distance are crucial in data mining. We\nconsider the problem of defining the distance between two data sets by\ncomparing summary statistics computed from the data sets. The initial\ndefinition of our distance is based on geometrical notions of certain sets of\ndistributions. We show that this distance can be computed in cubic time and\nthat it has several intuitive properties. We also show that this distance is\nthe unique Mahalanobis distance satisfying certain assumptions. We also\ndemonstrate that if we are dealing with binary data sets, then the distance can\nbe represented naturally by certain parity functions, and that it can be\nevaluated in linear time. Our empirical tests with real world data show that\nthe distance works well.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:45:55 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.01461", "submitter": "Goran \\v{Z}u\\v{z}i\\'c", "authors": "Domagoj Bradac and Sahil Singla and Goran Zuzic", "title": "(Near) Optimal Adaptivity Gaps for Stochastic Multi-Value Probing", "comments": "Added the hyperlinks (they got removed for some reason in the last\n  submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a kidney-exchange application where we want to find a max-matching\nin a random graph. To find whether an edge $e$ exists, we need to perform an\nexpensive test, in which case the edge $e$ appears independently with a\n\\emph{known} probability $p_e$. Given a budget on the total cost of the tests,\nour goal is to find a testing strategy that maximizes the expected maximum\nmatching size.\n  The above application is an example of the stochastic probing problem. In\ngeneral the optimal stochastic probing strategy is difficult to find because it\nis \\emph{adaptive}---decides on the next edge to probe based on the outcomes of\nthe probed edges. An alternate approach is to show the \\emph{adaptivity gap} is\nsmall, i.e., the best \\emph{non-adaptive} strategy always has a value close to\nthe best adaptive strategy. This allows us to focus on designing non-adaptive\nstrategies that are much simpler. Previous works, however, have focused on\nBernoulli random variables that can only capture whether an edge appears or\nnot. In this work we introduce a multi-value stochastic probing problem, which\ncan also model situations where the weight of an edge has a probability\ndistribution over multiple values.\n  Our main technical contribution is to obtain (near) optimal bounds for the\n(worst-case) adaptivity gaps for multi-value stochastic probing over\nprefix-closed constraints. For a monotone submodular function, we show the\nadaptivity gap is at most $2$ and provide a matching lower bound. For a\nweighted rank function of a $k$-extendible system (a generalization of\nintersection of $k$ matroids), we show the adaptivity gap is between $O(k\\log\nk)$ and $k$. None of these results were known even in the Bernoulli case where\nboth our upper and lower bounds also apply, thereby resolving an open question\nof Gupta et al.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 21:24:43 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 02:49:31 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Bradac", "Domagoj", ""], ["Singla", "Sahil", ""], ["Zuzic", "Goran", ""]]}, {"id": "1902.01477", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Faster way to agony: Discovering hierarchies in directed graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-44845-8_11", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world phenomena exhibit strong hierarchical structure.\nConsequently, in many real-world directed social networks vertices do not play\nequal role. Instead, vertices form a hierarchy such that the edges appear\nmainly from upper levels to lower levels. Discovering hierarchies from such\ngraphs is a challenging problem that has gained attention. Formally, given a\ndirected graph, we want to partition vertices into levels such that ideally\nthere are only edges from upper levels to lower levels. From computational\npoint of view, the ideal case is when the underlying directed graph is acyclic.\nIn such case, we can partition the vertices into a hierarchy such that there\nare only edges from upper levels to lower edges. In practice, graphs are rarely\nacyclic, hence we need to penalize the edges that violate the hierarchy. One\npractical approach is agony, where each violating edge is penalized based on\nthe severity of the violation. The fastest algorithm for computing agony\nrequires $O(nm^2)$ time. In the paper we present an algorithm for computing\nagony that has better theoretical bound, namely $O(m^2)$. We also show that in\npractice the obtained bound is pessimistic and that we can use our algorithm to\ncompute agony for large datasets. Moreover, our algorithm can be used as\nany-time algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 22:12:16 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.01483", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti and Aristides Gionis", "title": "Discovering Nested Communities", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-40991-2_3", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding communities in graphs is one of the most well-studied problems in\ndata mining and social-network analysis. In many real applications, the\nunderlying graph does not have a clear community structure. In those cases,\nselecting a single community turns out to be a fairly ill-posed problem, as the\noptimization criterion has to make a difficult choice between selecting a tight\nbut small community or a more inclusive but sparser community.\n  In order to avoid the problem of selecting only a single community we propose\ndiscovering a sequence of nested communities. More formally, given a graph and\na starting set, our goal is to discover a sequence of communities all\ncontaining the starting set, and each community forming a denser subgraph than\nthe next. Discovering an optimal sequence of communities is a complex\noptimization problem, and hence we divide it into two subproblems: 1) discover\nthe optimal sequence for a fixed order of graph vertices, a subproblem that we\ncan solve efficiently, and 2) find a good order. We employ a simple heuristic\nfor discovering an order and we provide empirical and theoretical evidence that\nour order is good.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 22:30:08 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Gionis", "Aristides", ""]]}, {"id": "1902.01499", "submitter": "Hassan Jameel Asghar", "authors": "Hassan Jameel Asghar and Ming Ding and Thierry Rakotoarivelo and\n  Sirine Mrabet and Mohamed Ali Kaafar", "title": "Differentially Private Release of High-Dimensional Datasets using the\n  Gaussian Copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic mechanism to efficiently release differentially private\nsynthetic versions of high-dimensional datasets with high utility. The core\ntechnique in our mechanism is the use of copulas. Specifically, we use the\nGaussian copula to define dependencies of attributes in the input dataset,\nwhose rows are modelled as samples from an unknown multivariate distribution,\nand then sample synthetic records through this copula. Despite the inherently\nnumerical nature of Gaussian correlations we construct a method that is\napplicable to both numerical and categorical attributes alike. Our mechanism is\nefficient in that it only takes time proportional to the square of the number\nof attributes in the dataset. We propose a differentially private way of\nconstructing the Gaussian copula without compromising computational efficiency.\nThrough experiments on three real-world datasets, we show that we can obtain\nhighly accurate answers to the set of all one-way marginal, and two-and\nthree-way positive conjunction queries, with 99\\% of the query answers having\nabsolute (fractional) error rates between 0.01 to 3\\%. Furthermore, for a\nmajority of two-way and three-way queries, we outperform independent noise\naddition through the well-known Laplace mechanism. In terms of computational\ntime we demonstrate that our mechanism can output synthetic datasets in around\n6 minutes 47 seconds on average with an input dataset of about 200 binary\nattributes and more than 32,000 rows, and about 2 hours 30 mins to execute a\nmuch larger dataset of about 700 binary attributes and more than 5 million\nrows. To further demonstrate scalability, we ran the mechanism on larger\n(artificial) datasets with 1,000 and 2,000 binary attributes (and 5 million\nrows) obtaining synthetic outputs in approximately 6 and 19 hours,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 23:57:49 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Asghar", "Hassan Jameel", ""], ["Ding", "Ming", ""], ["Rakotoarivelo", "Thierry", ""], ["Mrabet", "Sirine", ""], ["Kaafar", "Mohamed Ali", ""]]}, {"id": "1902.01543", "submitter": "Md Anwarul Patwary", "authors": "Md Anwarul kaium Patwary, Saurabh Garg, Byeong Kang", "title": "Window-based Streaming Graph Partitioning Algorithm", "comments": null, "journal-ref": null, "doi": "10.1145/3290688.3290711", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, the scale of graph datasets has increased to such a\ndegree that a single machine is not capable of efficiently processing large\ngraphs. Thereby, efficient graph partitioning is necessary for those large\ngraph applications. Traditional graph partitioning generally loads the whole\ngraph data into the memory before performing partitioning; this is not only a\ntime consuming task but it also creates memory bottlenecks. These issues of\nmemory limitation and enormous time complexity can be resolved using\nstream-based graph partitioning. A streaming graph partitioning algorithm reads\nvertices once and assigns that vertex to a partition accordingly. This is also\ncalled an one-pass algorithm. This paper proposes an efficient window-based\nstreaming graph partitioning algorithm called WStream. The WStream algorithm is\nan edge-cut partitioning algorithm, which distributes a vertex among the\npartitions. Our results suggest that the WStream algorithm is able to partition\nlarge graph data efficiently while keeping the load balanced across different\npartitions, and communication to a minimum. Evaluation results with real\nworkloads also prove the effectiveness of our proposed algorithm, and it\nachieves a significant reduction in load imbalance and edge-cut with different\nranges of dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 04:48:14 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Patwary", "Md Anwarul kaium", ""], ["Garg", "Saurabh", ""], ["Kang", "Byeong", ""]]}, {"id": "1902.01609", "submitter": "Julian Wellman", "authors": "Josh Brunner, Julian Wellman", "title": "An Optimal Algorithm for Online Freeze-tag", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the freeze-tag problem, one active robot must wake up many frozen robots.\nThe robots are considered as points in a metric space, where active robots move\nat a constant rate and activate other robots by visiting them. In the\n(time-dependent) online variant of the problem, frozen robots are not revealed\nuntil a specified time. Hammar, Nilsson, and Persson have shown that no online\nalgorithm can achieve a competitive ratio better than $7/3$ for online\nfreeze-tag, and asked whether there is any $O(1)$-competitive algorithm. In\nthis paper, we provide a $(1+\\sqrt{2})$-competitive algorithm for online\ntime-dependent freeze-tag, and show that no algorithm can achieve a lower\ncompetitive ratio on every metric space.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 09:39:55 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Brunner", "Josh", ""], ["Wellman", "Julian", ""]]}, {"id": "1902.01635", "submitter": "Boris Shustin", "authors": "Boris Shustin and Haim Avron", "title": "Preconditioned Riemannian Optimization on the Generalized Stiefel\n  Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems on the generalized Stiefel manifold (and products of\nit) are prevalent across science and engineering. For example, in computational\nscience they arise in the symmetric (generalized) eigenvalue problem, in\nnonlinear eigenvalue problems, and in electronic structures computations, to\nname a few problems. In statistics and machine learning, they arise, for\nexample, in various dimensionality reduction techniques such as canonical\ncorrelation analysis. In deep learning, regularization and improved stability\ncan be obtained by constraining some layers to have parameter matrices that\nbelong to the Stiefel manifold. Solving problems on the generalized Stiefel\nmanifold can be approached via the tools of Riemannian optimization. However,\nusing the standard geometric components for the generalized Stiefel manifold\nhas two possible shortcoming: computing some of the geometric components can be\ntoo expensive and converge can be rather slow in certain cases. Both\nshortcomings can be addressed using a technique called Riemannian\npreconditioning, which amounts to using geometric components derived using a\nprecoditioner that defines a Riemannian metric on the constraint manifold. In\nthis paper we develop the geometric components required to perform Riemannian\noptimization on the generalized Stiefel manifold equipped with a non-standard\nmetric, and illustrate theoretically and numerically the use of those\ncomponents and the effect of Riemannian preconditioning for solving\noptimization problems on the generalized Stiefel manifold.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 10:42:00 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 06:32:05 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 16:17:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Shustin", "Boris", ""], ["Avron", "Haim", ""]]}, {"id": "1902.01691", "submitter": "Artem Lutov", "authors": "Artem Lutov, Mourad Khayati and Philippe Cudr\\'e-Mauroux", "title": "Accuracy Evaluation of Overlapping and Multi-resolution Clustering\n  Algorithms on Large Datasets", "comments": "The application executable and sources:\n  https://github.com/eXascaleInfolab/xmeasures", "journal-ref": "2019 IEEE International Conference on Big Data and Smart Computing", "doi": null, "report-no": null, "categories": "cs.DS physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance of clustering algorithms is evaluated with the help of accuracy\nmetrics. There is a great diversity of clustering algorithms, which are key\ncomponents of many data analysis and exploration systems. However, there exist\nonly few metrics for the accuracy measurement of overlapping and\nmulti-resolution clustering algorithms on large datasets. In this paper, we\nfirst discuss existing metrics, how they satisfy a set of formal constraints,\nand how they can be applied to specific cases. Then, we propose several\noptimizations and extensions of these metrics. More specifically, we introduce\na new indexing technique to reduce both the runtime and the memory complexity\nof the Mean F1 score evaluation. Our technique can be applied on large datasets\nand it is faster on a single CPU than state-of-the-art implementations running\non high-performance servers. In addition, we propose several extensions of the\ndiscussed metrics to improve their effectiveness and satisfaction to formal\nconstraints without affecting their efficiency. All the metrics discussed in\nthis paper are implemented in C++ and are available for free as open-source\npackages that can be used either as stand-alone tools or as part of a\nbenchmarking system to compare various clustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:02:09 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 22:53:35 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Lutov", "Artem", ""], ["Khayati", "Mourad", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1902.01698", "submitter": "Alathea Jensen PhD", "authors": "Alathea Jensen", "title": "Stochastic Enumeration with Importance Sampling", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability, December 2018,\n  Volume 20, Issue 4, pp 1259-1284", "doi": "10.1007/s11009-018-9619-2", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many hard problems in the computational sciences are equivalent to counting\nthe leaves of a decision tree, or, more generally, summing a cost function over\nthe nodes. These problems include calculating the permanent of a matrix,\nfinding the volume of a convex polyhedron, and counting the number of linear\nextensions of a partially ordered set. Many approximation algorithms exist to\nestimate such sums. One of the most recent is Stochastic Enumeration (SE),\nintroduced in 2013 by Rubinstein. In 2015, Vaisman and Kroese provided a\nrigorous analysis of the variance of SE, and showed that SE can be extended to\na fully polynomial randomized approximation scheme for certain cost functions\non random trees. We present an algorithm that incorporates an importance\nfunction into SE, and provide theoretical analysis of its efficacy. We also\npresent the results of numerical experiments to measure the variance of an\napplication of the algorithm to the problem of counting linear extensions of a\nposet, and show that introducing importance sampling results in a significant\nreduction of variance as compared to the original version of SE.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 20:37:21 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Jensen", "Alathea", ""]]}, {"id": "1902.01701", "submitter": "Lan N. Nguyen", "authors": "Lan N. Nguyen, My T. Thai", "title": "Network Resilience Assessment via QoS Degradation Metrics: An\n  Algorithmic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on network resilience to perturbation of edge weight.\nOther than connectivity, many network applications nowadays rely upon some\nmeasure of network distance between a pair of connected nodes. In these\nsystems, a metric related to network functionality is associated to each edge.\nA pair of nodes only being functional if the weighted, shortest-path distance\nbetween the pair is below a given threshold \\texttt{T}. Consequently, a natural\nquestion is on which degree the change of edge weights can damage the network\nfunctionality? With this motivation, we study a new problem, \\textit{Quality of\nService Degradation}: given a set of pairs, find a minimum budget to increase\nthe edge weights which ensures the distance between each pair exceeds\n$\\mathtt{T}$. We introduce four algorithms with theoretical performance\nguarantees for this problem. Each of them has its own strength in trade-off\nbetween effectiveness and running time, which are illustrated both in theory\nand comprehensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:32:35 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Nguyen", "Lan N.", ""], ["Thai", "My T.", ""]]}, {"id": "1902.01704", "submitter": "Alathea Jensen PhD", "authors": "Isabel Beichl, Alathea Jensen", "title": "A Sequential Importance Sampling Algorithm for Estimating Linear\n  Extensions", "comments": null, "journal-ref": null, "doi": "10.1145/3385650", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent decades, a number of profound theorems concerning approximation of\nhard counting problems have appeared. These include estimation of the\npermanent, estimating the volume of a convex polyhedron, and counting\n(approximately) the number of linear extensions of a partially ordered set. All\nof these results have been achieved using probabilistic sampling methods,\nspecifically Monte Carlo Markov Chain (MCMC) techniques. In each case, a\nrapidly mixing Markov chain is defined that is guaranteed to produce, with high\nprobability, an accurate result after only a polynomial number of operations.\n  Although of polynomial complexity, none of these results lead to a practical\ncomputational technique, nor do they claim to. The polynomials are of high\ndegree and a non-trivial amount of computing is required to get even a single\nsample. Our aim in this paper is to present practical Monte Carlo methods for\none of these problems, counting linear extensions. Like related work on\nestimating the coefficients of the reliability polynomial, our technique is\nbased on improving the so-called Knuth counting algorithm by incorporating an\nimportance function into the node selection technique giving a sequential\nimportance sampling (SIS) method. We define and report performance on two\nimportance functions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 21:04:15 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 00:16:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Beichl", "Isabel", ""], ["Jensen", "Alathea", ""]]}, {"id": "1902.01727", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Discovering bursts revisited: guaranteed optimization of the model\n  parameters", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611974973.93", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classic data mining tasks is to discover bursts, time intervals,\nwhere events occur at abnormally high rate. In this paper we revisit\nKleinberg's seminal work, where bursts are discovered by using exponential\ndistribution with a varying rate parameter: the regions where it is more\nadvantageous to set the rate higher are deemed bursty. The model depends on two\nparameters, the initial rate and the change rate. The initial rate, that is,\nthe rate that is used when there are no burstiness was set to the average rate\nover the whole sequence. The change rate is provided by the user.\n  We argue that these choices are suboptimal: it leads to worse likelihood, and\nmay lead to missing some existing bursts. We propose an alternative problem\nsetting, where the model parameters are selected by optimizing the likelihood\nof the model. While this tweak is trivial from the problem definition point of\nview, this changes the optimization problem greatly. To solve the problem in\npractice, we propose efficient ($1 + \\epsilon$) approximation schemes. Finally,\nwe demonstrate empirically that with this setting we are able to discover\nbursts that would have otherwise be undetected.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:00:23 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.01745", "submitter": "L\\'aszl\\'o Kozma", "authors": "Bart M.P. Jansen, L\\'aszl\\'o Kozma, Jesper Nederlof", "title": "Hamiltonicity below Dirac's condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirac's theorem (1952) is a classical result of graph theory, stating that an\n$n$-vertex graph ($n \\geq 3$) is Hamiltonian if every vertex has degree at\nleast $n/2$. Both the value $n/2$ and the requirement for every vertex to have\nhigh degree are necessary for the theorem to hold.\n  In this work we give efficient algorithms for determining Hamiltonicity when\neither of the two conditions are relaxed. More precisely, we show that the\nHamiltonian cycle problem can be solved in time $c^k \\cdot n^{O(1)}$, for some\nfixed constant $c$, if at least $n-k$ vertices have degree at least $n/2$, or\nif all vertices have degree at least $n/2-k$. The running time is, in both\ncases, asymptotically optimal, under the exponential-time hypothesis (ETH).\n  The results extend the range of tractability of the Hamiltonian cycle\nproblem, showing that it is fixed-parameter tractable when parameterized below\na natural bound. In addition, for the first parameterization we show that a\nkernel with $O(k)$ vertices can be found in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:40:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1902.01829", "submitter": "Wajih Halim Boukaram", "authors": "Wajih Halim Boukaram, George Turkiyyah, David E. Keyes", "title": "Hierarchical Matrix Operations on GPUs: Matrix-Vector Multiplication and\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical matrices are space and time efficient representations of dense\nmatrices that exploit the low rank structure of matrix blocks at different\nlevels of granularity. The hierarchically low rank block partitioning produces\nrepresentations that can be stored and operated on in near-linear complexity\ninstead of the usual polynomial complexity of dense matrices. In this paper, we\npresent high performance implementations of matrix vector multiplication and\ncompression operations for the $\\mathcal{H}^2$ variant of hierarchical matrices\non GPUs. This variant exploits, in addition to the hierarchical block\npartitioning, hierarchical bases for the block representations and results in a\nscheme that requires only $O(n)$ storage and $O(n)$ complexity for the mat-vec\nand compression kernels. These two operations are at the core of algebraic\noperations for hierarchical matrices, the mat-vec being a ubiquitous operation\nin numerical algorithms while compression/recompression represents a key\nbuilding block for other algebraic operations, which require periodic\nrecompression during execution. The difficulties in developing efficient GPU\nalgorithms come primarily from the irregular tree data structures that underlie\nthe hierarchical representations, and the key to performance is to recast the\ncomputations on flattened trees in ways that allow batched linear algebra\noperations to be performed. This requires marshaling the irregularly laid out\ndata in a way that allows them to be used by the batched routines. Marshaling\noperations only involve pointer arithmetic with no data movement and as a\nresult have minimal overhead. Our numerical results on covariance matrices from\n2D and 3D problems from spatial statistics show the high efficiency our\nroutines achieve---over 550GB/s for the bandwidth-limited mat-vec and over\n850GFLOPS/s in sustained performance for the compression on the P100 Pascal\nGPU.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:59:51 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Boukaram", "Wajih Halim", ""], ["Turkiyyah", "George", ""], ["Keyes", "David E.", ""]]}, {"id": "1902.01832", "submitter": "Nikolaj Tatti", "authors": "Polina Rozenshtein, Nikolaj Tatti, Aristides Gionis", "title": "Inferring the strength of social ties: a community-driven approach", "comments": null, "journal-ref": null, "doi": "10.1145/3097983.3098199", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks are growing and becoming denser. The social\nconnections of a given person may have very high variability: from close\nfriends and relatives to acquaintances to people who hardly know. Inferring the\nstrength of social ties is an important ingredient for modeling the interaction\nof users in a network and understanding their behavior. Furthermore, the\nproblem has applications in computational social science, viral marketing, and\npeople recommendation.\n  In this paper we study the problem of inferring the strength of social ties\nin a given network. Our work is motivated by a recent approach [27], which\nleverages the strong triadic closure (STC) principle, a hypothesis rooted in\nsocial psychology [13]. To guide our inference process, in addition to the\nnetwork structure, we also consider as input a collection of tight communities.\nThose are sets of vertices that we expect to be connected via strong ties. Such\ncommunities appear in different situations, e.g., when being part of a\ncommunity implies a strong connection to one of the existing members.\n  We consider two related problem formalizations that reflect the assumptions\nof our setting: small number of STC violations and strong-tie connectivity in\nthe input communities. We show that both problem formulations are NP-hard. We\nalso show that one problem formulation is hard to approximate, while for the\nsecond we develop an algorithm with approximation guarantee. We validate the\nproposed method on real-world datasets by comparing with baselines that\noptimize STC violations and community connectivity separately.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 18:11:11 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Rozenshtein", "Polina", ""], ["Tatti", "Nikolaj", ""], ["Gionis", "Aristides", ""]]}, {"id": "1902.01873", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Dynamic hierarchies in temporal directed networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-10928-8_4", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outcome of interactions in many real-world systems can be often explained\nby a hierarchy between the participants. Discovering hierarchy from a given\ndirected network can be formulated as follows: partition vertices into levels\nsuch that, ideally, there are only forward edges, that is, edges from upper\nlevels to lower levels. In practice, the ideal case is impossible, so instead\nwe minimize some penalty function on the backward edges. One practical option\nfor such a penalty is agony, where the penalty depends on the severity of the\nviolation. In this paper we extend the definition of agony to temporal\nnetworks. In this setup we are given a directed network with time stamped\nedges, and we allow the rank assignment to vary over time. We propose 2\nstrategies for controlling the variation of individual ranks. In our first\nvariant, we penalize the fluctuation of the rankings over time by adding a\npenalty directly to the optimization function. In our second variant we allow\nthe rank change at most once. We show that the first variant can be solved\nexactly in polynomial time while the second variant is NP-hard, and in fact\ninapproximable. However, we develop an iterative method, where we first fix the\nchange point and optimize the ranks, and then fix the ranks and optimize the\nchange points, and reiterate until convergence. We show empirically that the\nalgorithms are reasonably fast in practice, and that the obtained rankings are\nsensible.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 19:12:09 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.01874", "submitter": "Ararat Harutyunyan", "authors": "Tom Denat, Ararat Harutyunyan, Vangelis Th. Paschos", "title": "Average-case complexity of a branch-and-bound algorithm for min\n  dominating set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average-case complexity of a branch-and-bound algorithms for Minimum\nDominating Set problem in random graphs in the G(n,p) model is studied. We\nidentify phase transitions between subexponential and exponential average-case\ncomplexities, depending on the growth of the probability p with respect to the\nnumber n of nodes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 19:13:12 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Denat", "Tom", ""], ["Harutyunyan", "Ararat", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1902.01896", "submitter": "Sepideh Aghamolaei", "authors": "Sepideh Aghamolaei, Mohammad Ghodsi", "title": "A Composable Coreset for k-Center in Doubling Metrics", "comments": "The preliminary version of this paper has appeared in Proceedings of\n  the 30th Canadian Conference on Computational Geometry, (CCCG 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of points $P$ in a metric space and a constant integer $k$ are given.\nThe $k$-center problem finds $k$ points as centers among $P$, such that the\nmaximum distance of any point of $P$ to their closest centers $(r)$ is\nminimized.\n  Doubling metrics are metric spaces in which for any $r$, a ball of radius $r$\ncan be covered using a constant number of balls of radius $r/2$. Fixed\ndimensional Euclidean spaces are doubling metrics. The lower bound on the\napproximation factor of $k$-center is $1.822$ in Euclidean spaces, however,\n$(1+\\epsilon)$-approximation algorithms with exponential dependency on\n$\\frac{1}{\\epsilon}$ and $k$ exist.\n  For a given set of sets $P_1,\\ldots,P_L$, a composable coreset independently\ncomputes subsets $C_1\\subset P_1, \\ldots, C_L\\subset P_L$, such that\n$\\cup_{i=1}^L C_i$ contains an approximation of a measure of the set\n$\\cup_{i=1}^L P_i$.\n  We introduce a $(1+\\epsilon)$-approximation composable coreset for\n$k$-center, which in doubling metrics has size sublinear in $|P|$. This results\nin a $(2+\\epsilon)$-approximation algorithm for $k$-center in MapReduce with a\nconstant number of rounds in doubling metrics for any $\\epsilon>0$ and\nsublinear communications, which is based on parametric pruning.\n  We prove the exponential nature of the trade-off between the number of\ncenters $(k)$ and the radius $(r)$, and give a composable coreset for a related\nproblem called dual clustering. Also, we give a new version of the parametric\npruning algorithm with $O(\\frac{nk}{\\epsilon})$ running time, $O(n)$ space and\n$2+\\epsilon$ approximation factor for metric $k$-center.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:14:33 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 17:11:59 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Aghamolaei", "Sepideh", ""], ["Ghodsi", "Mohammad", ""]]}, {"id": "1902.01998", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Nicolas Flammarion, Peter L. Bartlett", "title": "Fast Mean Estimation with Sub-Gaussian Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimator for the mean of a random vector in $\\mathbb{R}^d$\nthat can be computed in time $O(n^4+n^2d)$ for $n$ i.i.d.~samples and that has\nerror bounds matching the sub-Gaussian case. The only assumptions we make about\nthe data distribution are that it has finite mean and covariance; in\nparticular, we make no assumptions about higher-order moments. Like the\npolynomial time estimator introduced by Hopkins, 2018, which is based on the\nsum-of-squares hierarchy, our estimator achieves optimal statistical efficiency\nin this challenging setting, but it has a significantly faster runtime and a\nsimpler analysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 01:33:05 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Flammarion", "Nicolas", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1902.01999", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Peter L. Bartlett", "title": "Testing Markov Chains without Hitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identity testing of markov chains. In this setting,\nwe are given access to a single trajectory from a markov chain with unknown\ntransition matrix $Q$ and the goal is to determine whether $Q = P$ for some\nknown matrix $P$ or $\\text{Dist}(P, Q) \\geq \\epsilon$ where $\\text{Dist}$ is\nsuitably defined. In recent work by Daskalakis, Dikkala and Gravin, 2018, it\nwas shown that it is possible to distinguish between the two cases provided the\nlength of the observed trajectory is at least super-linear in the hitting time\nof $P$ which may be arbitrarily large.\n  In this paper, we propose an algorithm that avoids this dependence on hitting\ntime thus enabling efficient testing of markov chains even in cases where it is\ninfeasible to observe every state in the chain. Our algorithm is based on\ncombining classical ideas from approximation algorithms with techniques for the\nspectral analysis of markov chains.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 01:33:43 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1902.02159", "submitter": "Bertrand Jouve", "authors": "Pierre Coupechoux, Marc Demange, David Ellison, Bertrand Jouve", "title": "Firefighting on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Firefighter problem, introduced by Hartnell in 1995, a fire spreads\nthrough a graph while a player chooses which vertices to protect in order to\ncontain it. In this paper, we focus on the case of trees and we consider as\nwell the Fractional Firefighter game where the amount of protection allocated\nto a vertex lies between 0 and 1. While most of the work in this area deals\nwith a constant amount of firefighters available at each turn, we consider\nthree research questions which arise when including the sequence of\nfirefighters as part of the instance. We first introduce the online version of\nboth Firefighter and Fractional Firefighter, in which the number of\nfirefighters available at each turn is revealed over time. We show that a\ngreedy algorithm on finite trees is 1/2-competitive for both online versions,\nwhich generalises a result previously known for special cases of Firefighter.\nWe also show that the optimal competitive ratio of online Firefighter ranges\nbetween 1/2 and the inverse of the golden ratio. Next, given two firefighter\nsequences, we discuss sufficient conditions for the existence of an infinite\ntree that separates them, in the sense that the fire can be contained with one\nsequence but not with the other. To this aim, we study a new purely numerical\ngame called targeting game. Finally, we give sufficient conditions for the fire\nto be contained, expressed as the asymptotic comparison of the number of\nfirefighters and the size of the tree levels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 13:24:06 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Coupechoux", "Pierre", ""], ["Demange", "Marc", ""], ["Ellison", "David", ""], ["Jouve", "Bertrand", ""]]}, {"id": "1902.02187", "submitter": "Philip Bille", "authors": "Philip Bille, Inge Li G{\\o}rtz, Pawe{\\l} Gawrychowski, Gad M. Landau,\n  and Oren Weimann", "title": "Top Tree Compression of Tries", "comments": "Extended abstract appeared at ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a compressed representation of tries based on top tree compression\n[ICALP 2013] that works on a standard, comparison-based, pointer machine model\nof computation and supports efficient prefix search queries. Namely, we show\nhow to preprocess a set of strings of total length $n$ over an alphabet of size\n$\\sigma$ into a compressed data structure of worst-case optimal size\n$O(n/\\log_\\sigma n)$ that given a pattern string $P$ of length $m$ determines\nif $P$ is a prefix of one of the strings in time $O(\\min(m\\log \\sigma,m + \\log\nn))$. We show that this query time is in fact optimal regardless of the size of\nthe data structure.\n  Existing solutions either use $\\Omega(n)$ space or rely on word RAM\ntechniques, such as tabulation, hashing, address arithmetic, or word-level\nparallelism, and hence do not work on a pointer machine. Our result is the\nfirst solution on a pointer machine that achieves worst-case $o(n)$ space.\nAlong the way, we develop several interesting data structures that work on a\npointer machine and are of independent interest. These include an optimal data\nstructures for random access to a grammar-compressed string and an optimal data\nstructure for a variant of the level ancestor problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 14:01:45 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 11:09:45 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Landau", "Gad M.", ""], ["Weimann", "Oren", ""]]}, {"id": "1902.02201", "submitter": "Akbar Rafiey", "authors": "Akbar Rafiey, Arash Rafiey, and Thiago Santos", "title": "Toward a Dichotomy for Approximation of $H$-coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two (di)graphs G, H and a cost function $c:V(G)\\times V(H) \\to\n\\mathbb{Q}_{\\geq 0}\\cup\\{+\\infty\\}$, in the minimum cost homomorphism problem,\nMinHOM(H), goal is finding a homomorphism $f:V(G)\\to V(H)$ (a.k.a H-coloring)\nthat minimizes $\\sum\\limits_{v\\in V(G)}c(v,f(v))$. The complexity of exact\nminimization of this problem is well understood [34], and the class of digraphs\nH, for which the MinHOM(H) is polynomial time solvable is a small subset of all\ndigraphs.\n  In this paper, we consider the approximation of MinHOM within a constant\nfactor. For digraphs, MinHOM(H) is not approximable if H contains a digraph\nasteroidal triple (DAT). We take a major step toward a dichotomy classification\nof approximable cases. We give a dichotomy classification for approximating the\nMinHOM(H) when H is a graph. For digraphs, we provide constant factor\napproximation algorithms for two important classes of digraphs, namely bi-arc\ndigraphs (digraphs with a conservative semi-lattice polymorphism or\nmin-ordering), and k-arc digraphs (digraphs with an extended min-ordering).\nSpecifically, we show that:\n  1. \\textbf{Dichotomy for Graphs:} MinHOM(H) has a $2|V(H)|$-approximation\nalgorithm if graph H admits a conservative majority polymorphims (i.e. H is a\nbi-arc graph), otherwise, it is inapproximable;\n  2. MinHOM(H) has a $|V(H)|^2$-approximation algorithm if H is a bi-arc\ndigraph;\n  3. MinHOM(H) has a $|V(H)|^2$-approximation algorithm if H is a k-arc\ndigraph.\n  In conclusion, we show the importance of these results and provide insights\nfor achieving a dichotomy classification of approximable cases. Our constant\nfactors depend on the size of H. However, the implementation of our algorithms\nprovides a much better approximation ratio. It leaves open to investigate a\nclassification of digraphs H, where MinHOM(H) admits a constant factor\napproximation algorithm that is independent of H.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 14:27:25 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 03:30:34 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 04:51:23 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Rafiey", "Akbar", ""], ["Rafiey", "Arash", ""], ["Santos", "Thiago", ""]]}, {"id": "1902.02304", "submitter": "Stefan Neumann", "authors": "Sayan Bhattacharya, Monika Henzinger, Stefan Neumann", "title": "New Amortized Cell-Probe Lower Bounds for Dynamic Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build upon the recent papers by Weinstein and Yu (FOCS'16), Larsen\n(FOCS'12), and Clifford et al. (FOCS'15) to present a general framework that\ngives amortized lower bounds on the update and query times of dynamic data\nstructures. Using our framework, we present two concrete results.\n  (1) For the dynamic polynomial evaluation problem, where the polynomial is\ndefined over a finite field of size $n^{1+\\Omega(1)}$ and has degree $n$, any\ndynamic data structure must either have an amortized update time of\n$\\Omega((\\lg n/\\lg \\lg n)^2)$ or an amortized query time of $\\Omega((\\lg n/\\lg\n\\lg n)^2)$.\n  (2) For the dynamic online matrix vector multiplication problem, where we get\nan $n \\times n$ matrix whose entires are drawn from a finite field of size\n$n^{\\Theta(1)}$, any dynamic data structure must either have an amortized\nupdate time of $\\Omega((\\lg n/\\lg \\lg n)^2)$ or an amortized query time of\n$\\Omega(n \\cdot (\\lg n/\\lg \\lg n)^2)$.\n  For these two problems, the previous works by Larsen (FOCS'12) and Clifford\net al. (FOCS'15) gave the same lower bounds, but only for worst case update and\nquery times. Our bounds match the highest unconditional lower bounds known till\ndate for any dynamic problem in the cell-probe model.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 17:59:38 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Neumann", "Stefan", ""]]}, {"id": "1902.02392", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Jilles Vreeken", "title": "Finding Good Itemsets by Packing Data", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2008.39", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of selecting small groups of itemsets that represent the data\nwell has recently gained a lot of attention. We approach the problem by\nsearching for the itemsets that compress the data efficiently. As a compression\ntechnique we use decision trees combined with a refined version of MDL. More\nformally, assuming that the items are ordered, we create a decision tree for\neach item that may only depend on the previous items. Our approach allows us to\nfind complex interactions between the attributes, not just co-occurrences of\n1s. Further, we present a link between the itemsets and the decision trees and\nuse this link to export the itemsets from the decision trees. In this paper we\npresent two algorithms. The first one is a simple greedy approach that builds a\nfamily of itemsets directly from data. The second one, given a collection of\ncandidate itemsets, selects a small subset of these itemsets. Our experiments\nshow that these approaches result in compact and high quality descriptions of\nthe data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 20:38:24 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1902.02459", "submitter": "Erik Waingarten", "authors": "Jerry Li, Aleksandar Nikolov, Ilya Razenshteyn, Erik Waingarten", "title": "On Mean Estimation for General Norms with Statistical Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of mean estimation for high-dimensional distributions,\nassuming access to a statistical query oracle for the distribution. For a\nnormed space $X = (\\mathbb{R}^d, \\|\\cdot\\|_X)$ and a distribution supported on\nvectors $x \\in \\mathbb{R}^d$ with $\\|x\\|_{X} \\leq 1$, the task is to output an\nestimate $\\hat{\\mu} \\in \\mathbb{R}^d$ which is $\\epsilon$-close in the distance\ninduced by $\\|\\cdot\\|_X$ to the true mean of the distribution. We obtain sharp\nupper and lower bounds for the statistical query complexity of this problem\nwhen the the underlying norm is symmetric as well as for Schatten-$p$ norms,\nanswering two questions raised by Feldman, Guzm\\'{a}n, and Vempala (SODA 2017).\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 03:30:05 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Li", "Jerry", ""], ["Nikolov", "Aleksandar", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1902.02499", "submitter": "Pavel S. Ruzankin", "authors": "Pavel S. Ruzankin", "title": "A fast algorithm for constructing balanced binary search trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a new non-recursive algorithm for constructing a binary search\ntree given an array of numbers. The algorithm has $O(N)$ time and $O(1)$ memory\ncomplexity if the given array of $N$ numbers is sorted. The resulting tree is\nof minimal height and can be transformed to a complete binary search tree\n(retaining minimal height) with $O(\\log N)$ time and $O(1)$ memory.\n  The algorithm allows simple and effective parallelization.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 07:21:11 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 12:19:06 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 11:46:26 GMT"}, {"version": "v4", "created": "Sat, 2 Mar 2019 03:02:40 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Ruzankin", "Pavel S.", ""]]}, {"id": "1902.02526", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, Daniel Lokshtanov, Fahad Panolan,\n  Saket Saurabh and Meirav Zehavi", "title": "Going Far From Degeneracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An undirected graph G is d-degenerate if every subgraph of G has a vertex of\ndegree at most d. By the classical theorem of Erd\\H{o}s and Gallai from 1959,\nevery graph of degeneracy d>1 contains a cycle of length at least d+1. The\nproof of Erd\\H{o}s and Gallai is constructive and can be turned into a\npolynomial time algorithm constructing a cycle of length at least d+1. But can\nwe decide in polynomial time whether a graph contains a cycle of length at\nleast d+2? An easy reduction from Hamiltonian Cycle provides a negative answer\nto this question: deciding whether a graph has a cycle of length at least d+2\nis NP-complete. Surprisingly, the complexity of the problem changes drastically\nwhen the input graph is 2-connected. In this case we prove that deciding\nwhether G contains a cycle of length at least d+k can be done in time\n2^{O(k)}|V(G)|^{O(1)}. In other words, deciding whether a 2-connected n-vertex\nG contains a cycle of length at least d+log n can be done in polynomial time.\n  Similar algorithmic results hold for long paths in graphs. We observe that\ndeciding whether a graph has a path of length at least d+1 is NP-complete.\nHowever, we prove that if graph G is connected, then deciding whether G\ncontains a path of length at least d+k can be done in time 2^{O(k)}n^{O(1)}. We\ncomplement these results by showing that the choice of degeneracy as the `above\nguarantee parameterization' is optimal in the following sense: For any\n\\epsilon>0 it is NP-complete to decide whether a connected (2-connected) graph\nof degeneracy d has a path (cycle) of length at least (1+\\epsilon)d.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 09:00:23 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 09:18:46 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1902.02755", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Significance of Episodes Based on Minimal Windows", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2009.23", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering episodes, frequent sets of events from a sequence has been an\nactive field in pattern mining. Traditionally, a level-wise approach is used to\ndiscover all frequent episodes. While this technique is computationally\nfeasible it may result in a vast number of patterns, especially when low\nthresholds are used.\n  In this paper we propose a new quality measure for episodes. We say that an\nepisode is significant if the average length of its minimal windows deviates\ngreatly when compared to the expected length according to the independence\nmodel. We can apply this measure as a post-pruning step to test whether the\ndiscovered frequent episodes are truly interesting and consequently to reduce\nthe number of output.\n  As a main contribution we introduce a technique that allows us to compute the\ndistribution of lengths of minimal windows using the independence model. Such a\ncomputation task is surpisingly complex and in order to solve it we compute the\ndistribution iteratively starting from simple episodes and progressively moving\ntowards the more complex ones. In our experiments we discover candidate\nepisodes that have a sufficient amount of minimal windows and test each\ncandidate for significance. The experimental results demonstrate that our\napproach finds significant episodes while ignoring uninteresting ones.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:26:50 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.02834", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Jilles Vreeken", "title": "The Long and the Short of It: Summarising Event Sequences with Serial\n  Episodes", "comments": null, "journal-ref": null, "doi": "10.1145/2339530.2339606", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ideal outcome of pattern mining is a small set of informative patterns,\ncontaining no redundancy or noise, that identifies the key structure of the\ndata at hand. Standard frequent pattern miners do not achieve this goal, as due\nto the pattern explosion typically very large numbers of highly redundant\npatterns are returned.\n  We pursue the ideal for sequential data, by employing a pattern set mining\napproach-an approach where, instead of ranking patterns individually, we\nconsider results as a whole. Pattern set mining has been successfully applied\nto transactional data, but has been surprisingly under studied for sequential\ndata.\n  In this paper, we employ the MDL principle to identify the set of sequential\npatterns that summarises the data best. In particular, we formalise how to\nencode sequential data using sets of serial episodes, and use the encoded\nlength as a quality score. As search strategy, we propose two approaches: the\nfirst algorithm selects a good pattern set from a large candidate set, while\nthe second is a parameter-free any-time algorithm that mines pattern sets\ndirectly from the data. Experimentation on synthetic and real data demonstrates\nwe efficiently discover small sets of informative patterns.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 20:25:50 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1902.02861", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Jilles Vreeken", "title": "Discovering Descriptive Tile Trees by Mining Optimal Geometric Subtiles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-33460-3_6", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analysing binary data, the ease at which one can interpret results is\nvery important. Many existing methods, however, discover either models that are\ndifficult to read, or return so many results interpretation becomes impossible.\nHere, we study a fully automated approach for mining easily interpretable\nmodels for binary data. We model data hierarchically with noisy\ntiles-rectangles with significantly different density than their parent tile.\nTo identify good trees, we employ the Minimum Description Length principle.\n  We propose STIJL, a greedy any-time algorithm for mining good tile trees from\nbinary data. Iteratively, it finds the locally optimal addition to the current\ntree, allowing overlap with tiles of the same parent. A major result of this\npaper is that we find the optimal tile in only $\\Theta(NM\\min(N, M))$ time.\nSTIJL can either be employed as a top-$k$ miner, or by MDL we can identify the\ntree that describes the data best.\n  Experiments show we find succinct models that accurately summarise the data,\nand, by their hierarchical property are easily interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 21:56:09 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1902.02889", "submitter": "Felipe A. Louza", "authors": "Lavinia Egidi, Felipe A. Louza, Giovanni Manzini", "title": "Space-efficient merging of succinct de Bruijn graphs", "comments": "Accepted to SPIRE'19", "journal-ref": null, "doi": "10.1007/978-3-030-32686-9_24", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for merging succinct representations of de Bruijn\ngraphs introduced in [Bowe et al. WABI 2012]. Our algorithm is based on the\nlightweight BWT merging approach by Holt and McMillan [Bionformatics 2014,\nACM-BCB 2014]. Our algorithm has the same asymptotic cost of the state of the\nart tool for the same problem presented by Muggli et al. [bioRxiv 2017,\nBioinformatics 2019], but it uses less than half of its working space. A novel\nimportant feature of our algorithm, not found in any of the existing tools, is\nthat it can compute the Variable Order succinct representation of the union\ngraph within the same asymptotic time/space bounds.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 23:56:50 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 22:31:00 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 08:17:21 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 12:23:24 GMT"}, {"version": "v5", "created": "Fri, 26 Jul 2019 19:05:01 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Egidi", "Lavinia", ""], ["Louza", "Felipe A.", ""], ["Manzini", "Giovanni", ""]]}, {"id": "1902.02921", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Are your Items in Order?", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611972818.36", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Items in many datasets can be arranged to a natural order. Such orders are\nuseful since they can provide new knowledge about the data and may ease further\ndata exploration and visualization. Our goal in this paper is to define a\nstatistically well-founded and an objective score measuring the quality of an\norder. Such a measure can be used for determining whether the current order has\nany valuable information or can it be discarded.\n  Intuitively, we say that the order is good if dependent attributes are close\nto each other. To define the order score we fit an order-sensitive model to the\ndataset. Our model resembles a Markov chain model, that is, the attributes\ndepend only on the immediate neighbors. The score of the order is the BIC score\nof the best model. For computing the measure we introduce a fast dynamic\nprogram. The score is then compared against random orders: if it is better than\nthe scores of the random orders, we say that the order is good. We also show\nthe asymptotic connection between the score function and the number of free\nparameters of the model. In addition, we introduce a simple greedy approach for\nfinding an order with a good score. We evaluate the score for synthetic and\nreal datasets using different spectral orders and the orders obtained with the\ngreedy method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 02:59:15 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.03102", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Michael Mampaey", "title": "Using Background Knowledge to Rank Itemsets", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-010-0188-4", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the quality of discovered results is an important open problem in\ndata mining. Such assessment is particularly vital when mining itemsets, since\ncommonly many of the discovered patterns can be easily explained by background\nknowledge. The simplest approach to screen uninteresting patterns is to compare\nthe observed frequency against the independence model. Since the parameters for\nthe independence model are the column margins, we can view such screening as a\nway of using the column margins as background knowledge.\n  In this paper we study techniques for more flexible approaches for infusing\nbackground knowledge. Namely, we show that we can efficiently use additional\nknowledge such as row margins, lazarus counts, and bounds of ones. We\ndemonstrate that these statistics describe forms of data that occur in practice\nand have been studied in data mining.\n  To infuse the information efficiently we use a maximum entropy approach. In\nits general setting, solving a maximum entropy model is infeasible, but we\ndemonstrate that for our setting it can be solved in polynomial time.\nExperiments show that more sophisticated models fit the data better and that\nusing more information improves the frequency prediction of itemsets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:36:13 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Mampaey", "Michael", ""]]}, {"id": "1902.03274", "submitter": "Manuel C\\'aceres", "authors": "Manuel C\\'aceres and Gonzalo Navarro", "title": "Faster Repetition-Aware Compressed Suffix Trees based on Block Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix trees are a fundamental data structure in stringology, but their space\nusage, though linear, is an important problem for its applications. We design\nand implement a new compressed suffix tree targeted to highly repetitive texts,\nsuch as large genomic collections of the same species. Our suffix tree tree\nbuilds on Block Trees, a recent Lempel-Ziv-bounded data structure that captures\nthe repetitiveness of its input. We use Block Trees to compress the topology of\nthe suffix tree, and augment the Block Tree nodes with data that speeds up\nsuffix tree navigation.\n  Our compressed suffix tree is slightly larger than previous repetition-aware\nsuffix trees based on grammars, but outperforms them in time, often by orders\nof magnitude. The component that represents the tree topology achieves a speed\ncomparable to that of general-purpose compressed trees, while using 2.3--10\ntimes less space, and might be of interest in other scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 20:08:03 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["C\u00e1ceres", "Manuel", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1902.03285", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Fast Sequence Segmentation using Log-Linear Models", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-012-0301-y", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence segmentation is a well-studied problem, where given a sequence of\nelements, an integer K, and some measure of homogeneity, the task is to split\nthe sequence into K contiguous segments that are maximally homogeneous. A\nclassic approach to find the optimal solution is by using a dynamic program.\nUnfortunately, the execution time of this program is quadratic with respect to\nthe length of the input sequence. This makes the algorithm slow for a sequence\nof non-trivial length. In this paper we study segmentations whose measure of\ngoodness is based on log-linear models, a rich family that contains many of the\nstandard distributions. We present a theoretical result allowing us to prune\nmany suboptimal segmentations. Using this result, we modify the standard\ndynamic program for one-dimensional log-linear models, and by doing so reduce\nthe computational time. We demonstrate empirically, that this approach can\nsignificantly reduce the computational burden of finding the optimal\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 20:42:07 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.03297", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Probably the Best Itemsets", "comments": null, "journal-ref": null, "doi": "10.1145/1835804.1835843", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main current challenges in itemset mining is to discover a small\nset of high-quality itemsets. In this paper we propose a new and general\napproach for measuring the quality of itemsets. The method is solidly founded\nin Bayesian statistics and decreases monotonically, allowing for efficient\ndiscovery of all interesting itemsets. The measure is defined by connecting\nstatistical models and collections of itemsets. This allows us to score\nindividual itemsets with the probability of them occuring in random models\nbuilt on the data.\n  As a concrete example of this framework we use exponential models. This class\nof models possesses many desirable properties. Most importantly, Occam's razor\nin Bayesian model selection provides a defence for the pattern explosion. As\ngeneral exponential models are infeasible in practice, we use decomposable\nmodels; a large sub-class for which the measure is solvable. For the actual\ncomputation of the score we sample models from the posterior distribution using\nan MCMC approach.\n  Experimentation on our method demonstrates the measure works in practice and\nresults in interpretable and insightful itemsets for both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:51:59 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.03422", "submitter": "Srikrishnan Divakaran", "authors": "Srikrishnan Divakaran", "title": "Fast Approximation Schemes for Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new approximation schemes for bin packing based on the following\ntwo approaches: (1) partitioning the given problem into mostly identical\nsub-problems of constant size and then construct a solution by combining the\nsolutions of these constant size sub-problems obtained through PTAS or exact\nmethods; (2) solving bin packing using irregular sized bins, a generalization\nof bin packing, that facilitates the design of simple and efficient recursive\nalgorithms that solve a problem in terms of smaller sub-problems such that the\nunused space in bins used by an earlier solved sub-problem is available to\nsubsequently solved sub-problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 13:16:52 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Divakaran", "Srikrishnan", ""]]}, {"id": "1902.03428", "submitter": "Malin Rau", "authors": "Klaus Jansen, Malin Rau", "title": "Linear Time Algorithms for Multiple Cluster Scheduling and Multiple\n  Strip Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Multiple Cluster Scheduling problem and the Multiple Strip\nPacking problem. For both problems, there is no algorithm with approximation\nratio better than $2$ unless $P = NP$. In this paper, we present an algorithm\nwith approximation ratio $2$ and running time $O(n)$ for both problems. While a\n$2$ approximation was known before, the running time of the algorithm is at\nleast $\\Omega(n^{256})$ in the worst case. Therefore, an $O(n)$ algorithm is\nsurprising and the best possible. We archive this result by calling an AEPTAS\nwith approximation guarantee $(1+\\varepsilon)OPT +p_{\\max}$ and running time of\nthe form $O(n\\log(1/\\varepsilon)+ f(1/\\varepsilon))$ with a constant\n$\\varepsilon$ to schedule the jobs on a single cluster. This schedule is then\ndistributed on the $N$ clusters in $O(n)$. Moreover, this distribution\ntechnique can be applied to any variant of of Multi Cluster Scheduling for\nwhich there exists an AEPTAS with additive term $p_{\\max}$.\n  While the above result is strong from a theoretical point of view, it might\nnot be very practical due to a large hidden constant caused by calling an\nAEPTAS with a constant $\\varepsilon \\geq 1/8$ as subroutine. Nevertheless, we\npoint out that the general approach of finding first a schedule on one cluster\nand then distributing it onto the other clusters might come in handy in\npractical approaches. We demonstrate this by presenting a practical algorithm\nwith running time $O(n\\log(n))$, with out hidden constants, that is a\n$9/4$-approximation for one third of all possible instances, i.e, all instances\nwhere the number of clusters is dividable by $3$, and has an approximation\nratio of at most $2.3$ for all instances with at least $9$ clusters.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 14:25:23 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jansen", "Klaus", ""], ["Rau", "Malin", ""]]}, {"id": "1902.03519", "submitter": "Ali Vakilian", "authors": "Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali\n  Vakilian, Tal Wagner", "title": "Scalable Fair Clustering", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fair variant of the classic $k$-median problem introduced by\nChierichetti et al. [2017]. In the standard $k$-median problem, given an input\npointset $P$, the goal is to find $k$ centers $C$ and assign each input point\nto one of the centers in $C$ such that the average distance of points to their\ncluster center is minimized.\n  In the fair variant of $k$-median, the points are colored, and the goal is to\nminimize the same average distance objective while ensuring that all clusters\nhave an \"approximately equal\" number of points of each color.\n  Chierichetti et al. proposed a two-phase algorithm for fair $k$-clustering.\nIn the first step, the pointset is partitioned into subsets called fairlets\nthat satisfy the fairness requirement and approximately preserve the $k$-median\nobjective. In the second step, fairlets are merged into $k$ clusters by one of\nthe existing $k$-median algorithms. The running time of this algorithm is\ndominated by the first step, which takes super-quadratic time.\n  In this paper, we present a practical approximate fairlet decomposition\nalgorithm that runs in nearly linear time. Our algorithm additionally allows\nfor finer control over the balance of resulting clusters than the original\nwork. We complement our theoretical bounds with empirical evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 00:04:34 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 18:19:34 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""], ["Onak", "Krzysztof", ""], ["Schieber", "Baruch", ""], ["Vakilian", "Ali", ""], ["Wagner", "Tal", ""]]}, {"id": "1902.03522", "submitter": "Dmitrii Avdiukhin", "authors": "Dmitrii Avdiukhin, Sergey Pupyrev, Grigory Yaroslavtsev", "title": "Multi-Dimensional Balanced Graph Partitioning via Projected Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by performance optimization of large-scale graph processing systems\nthat distribute the graph across multiple machines, we consider the balanced\ngraph partitioning problem. Compared to the previous work, we study the\nmulti-dimensional variant when balance according to multiple weight functions\nis required. As we demonstrate by experimental evaluation, such\nmulti-dimensional balance is important for achieving performance improvements\nfor typical distributed graph processing workloads. We propose a new scalable\ntechnique for the multidimensional balanced graph partitioning problem. The\nmethod is based on applying randomized projected gradient descent to a\nnon-convex continuous relaxation of the objective. We show how to implement the\nnew algorithm efficiently in both theory and practice utilizing various\napproaches for projection. Experiments with large-scale social networks\ncontaining up to hundreds of billions of edges indicate that our algorithm has\nsuperior performance compared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 00:23:16 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 00:25:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Avdiukhin", "Dmitrii", ""], ["Pupyrev", "Sergey", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1902.03534", "submitter": "Ali Vakilian", "authors": "Piotr Indyk, Sepideh Mahabadi, Ronitt Rubinfeld, Ali Vakilian, Anak\n  Yodpinyanee", "title": "Set Cover in Sub-linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic set cover problem from the perspective of sub-linear\nalgorithms. Given access to a collection of $m$ sets over $n$ elements in the\nquery model, we show that sub-linear algorithms derived from existing\ntechniques have almost tight query complexities.\n  On one hand, first we show an adaptation of the streaming algorithm presented\nin Har-Peled et al. [2016] to the sub-linear query model, that returns an\n$\\alpha$-approximate cover using $\\tilde{O}(m(n/k)^{1/(\\alpha-1)} + nk)$\nqueries to the input, where $k$ denotes the value of a minimum set cover. We\nthen complement this upper bound by proving that for lower values of $k$, the\nrequired number of queries is $\\tilde{\\Omega}(m(n/k)^{1/(2\\alpha)})$, even for\nestimating the optimal cover size. Moreover, we prove that even checking\nwhether a given collection of sets covers all the elements would require\n$\\Omega(nk)$ queries. These two lower bounds provide strong evidence that the\nupper bound is almost tight for certain values of the parameter $k$.\n  On the other hand, we show that this bound is not optimal for larger values\nof the parameter $k$, as there exists a $(1+\\varepsilon)$-approximation\nalgorithm with $\\tilde{O}(mn/k\\varepsilon^2)$ queries. We show that this bound\nis essentially tight for sufficiently small constant $\\varepsilon$, by\nestablishing a lower bound of $\\tilde{\\Omega}(mn/k)$ query complexity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 04:10:34 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Indyk", "Piotr", ""], ["Mahabadi", "Sepideh", ""], ["Rubinfeld", "Ronitt", ""], ["Vakilian", "Ali", ""], ["Yodpinyanee", "Anak", ""]]}, {"id": "1902.03548", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "Approximating $k$-connected $m$-dominating sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A subset $S$ of nodes in a graph $G$ is a $k$-connected $m$-dominating set\n($(k,m)$-cds) if the subgraph $G[S]$ induced by $S$ is $k$-connected and every\n$v \\in V \\setminus S$ has at least $m$ neighbors in $S$. In the $k$-Connected\n$m$-Dominating Set ($(k,m)$-CDS) problem the goal is to find a minimum weight\n$(k,m)$-cds in a node-weighted graph. For $m \\geq k$ we obtain the following\napproximation ratios. For general graphs our ratio $O(k \\ln n)$ improves the\nprevious best ratio $O(k^2 \\ln n)$ and matches the best known ratio for unit\nweights. For unit disc graphs we improve the ratio $O(k \\ln k)$ to\n$\\min\\left\\{\\frac{m}{m-k},k^{2/3}\\right\\} \\cdot O(\\ln^2 k)$ -- this is the\nfirst sublinear ratio for the problem, and the first polylogarithmic ratio\n$O(\\ln^2 k)/\\epsilon$ when $m \\geq (1+\\epsilon)k$; furthermore, we obtain ratio\n$\\min\\left\\{\\frac{m}{m-k},\\sqrt{k}\\right\\} \\cdot O(\\ln^2 k)$ for uniform\nweights. These results are obtained by showing the same ratios for the Subset\n$k$-Connectivity problem when the set $T$ of terminals is an $m$-dominating set\nwith $m \\geq k$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 07:07:27 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "1902.03549", "submitter": "Moustapha Diaby", "authors": "Moustapha Diaby, Mark H. Karwan, and Lei Sun", "title": "On modeling hard combinatorial optimization problems as linear programs:\n  Refutations of the \"unconditional impossibility\" claims", "comments": "17 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a series of developments in the recent literature (by\nessentially a same \"circle\" of authors) with the absolute/unconditioned\n(implicit or explicit) claim that there exists no abstraction of an NP-Complete\ncombinatorial optimization problem in which the defining combinatorial\nconfigurations (such as \"tours\" in the case of the traveling salesman problem\n(TSP) for example) can be modeled by a polynomial-sized system of linear\nconstraints. The purpose of this paper is to provide general as well as\nspecific refutations for these recent claims.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 07:09:22 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Diaby", "Moustapha", ""], ["Karwan", "Mark H.", ""], ["Sun", "Lei", ""]]}, {"id": "1902.03560", "submitter": "Massimo Equi", "authors": "Massimo Equi, Roberto Grossi, Alexandru I. Tomescu, Veli M\\\"akinen", "title": "On the Complexity of Exact Pattern Matching in Graphs: Determinism and\n  Zig-Zag Matching", "comments": "Further developments on our previous work: arXiv:1901.05264", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact pattern matching in labeled graphs is the problem of searching paths of\na graph $G=(V,E)$ that spell the same string as the given pattern $P[1..m]$.\nThis basic problem can be found at the heart of more complex operations on\nvariation graphs in computational biology, query operations in graph databases,\nand analysis of heterogeneous networks, where the nodes of some paths must\nmatch a sequence of labels or types. In our recent work we described a\nconditional lower bound stating that the exact pattern matching problem in\nlabeled graphs cannot be solved in less than quadratic time, namely, $O(|E|^{1\n- \\epsilon} \\, m)$ time or $O(|E| \\, m^{1 - \\epsilon})$ time for any constant\n$\\epsilon>0$, unless the Strong Exponential Time Hypothesis (SETH) is false.\nThe result holds even if node labels and pattern $P$ are drawn from a binary\nalphabet, and $G$ is restricted to undirected graphs of maximum degree three or\ndirected acyclic graphs of maximum sum of indegree and outdegree three. It was\nleft open what happens on undirected graphs of maximum degree two, i.e., when\nthe pattern can have a zig-zag match in a (cyclic) bidirectional string. Also,\nthe reduction created a non-determistic directed acyclic graph, and it was left\nopen if determinism would make the problem easier. In this work, we show\nthrough the Orthogonal Vectors hypothesis (OV) that the same conditional lower\nbound holds even for these restricted cases.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 09:43:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Equi", "Massimo", ""], ["Grossi", "Roberto", ""], ["Tomescu", "Alexandru I.", ""], ["M\u00e4kinen", "Veli", ""]]}, {"id": "1902.03568", "submitter": "Markus Lohrey", "authors": "Moses Ganardi, Artur Je\\.z and Markus Lohrey", "title": "Balancing Straight-Line Programs", "comments": "An extended abstract of this paper appears in the Proceedings of FOCS\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that a context-free grammar of size $m$ that produces a single\nstring $w$ (such a grammar is also called a string straight-line program) can\nbe transformed in linear time into a context-free grammar for $w$ of size\n$\\mathcal{O}(m)$, whose unique derivation tree has depth $\\mathcal{O}(\\log\n|w|)$. This solves an open problem in the area of grammar-based compression.\nSimilar results are shown for two formalism for grammar-based tree compression:\ntop dags and forest straight-line programs. These balancing results are all\ndeduced from a single meta theorem stating that the depth of an algebraic\ncircuit over an algebra with a certain finite base property can be reduced to\n$\\mathcal{O}(\\log n)$ with the cost of a constant multiplicative size increase.\nHere, $n$ refers to the size of the unfolding (or unravelling) of the circuit.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 10:30:46 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 05:20:18 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 18:37:57 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 11:58:08 GMT"}, {"version": "v5", "created": "Wed, 1 Jul 2020 17:38:07 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Ganardi", "Moses", ""], ["Je\u017c", "Artur", ""], ["Lohrey", "Markus", ""]]}, {"id": "1902.04023", "submitter": "Ted Dunning", "authors": "Ted Dunning and Otmar Ertl", "title": "Computing Extremely Accurate Quantiles Using t-Digests", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present on-line algorithms for computing approximations of rank-based\nstatistics that give high accuracy, particularly near the tails of a\ndistribution, with very small sketches. Notably, the method allows a quantile\n$q$ to be computed with an accuracy relative to $\\max(q, 1-q)$ rather than\nabsolute accuracy as with most other methods. This new algorithm is robust with\nrespect to skewed distributions or ordered datasets and allows separately\ncomputed summaries to be combined with no loss in accuracy.\n  An open-source Java implementation of this algorithm is available from the\nauthor. Independent implementations in Go and Python are also available.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 17:57:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Dunning", "Ted", ""], ["Ertl", "Otmar", ""]]}, {"id": "1902.04045", "submitter": "Panos Giannopoulos", "authors": "Mikkel Abrahamsen and Panos Giannopoulos and Maarten L\\\"offler and\n  G\\\"unter Rote", "title": "Geometric Multicut", "comments": "24 pages, 15 figures", "journal-ref": "Discrete & Computational Geometry 64 (2020), 575-607", "doi": "10.1007/s00454-020-00232-w", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following separation problem: Given a collection of colored\nobjects in the plane, compute a shortest \"fence\" $F$, i.e., a union of curves\nof minimum total length, that separates every two objects of different colors.\nTwo objects are separated if $F$ contains a simple closed curve that has one\nobject in the interior and the other in the exterior. We refer to the problem\nas GEOMETRIC $k$-CUT, where $k$ is the number of different colors, as it can be\nseen as a geometric analogue to the well-studied multicut problem on graphs. We\nfirst give an $O(n^4\\log^3 n)$-time algorithm that computes an optimal fence\nfor the case where the input consists of polygons of two colors and $n$ corners\nin total. We then show that the problem is NP-hard for the case of three\ncolors. Finally, we give a $(2-4/3k)$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 18:44:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Abrahamsen", "Mikkel", ""], ["Giannopoulos", "Panos", ""], ["L\u00f6ffler", "Maarten", ""], ["Rote", "G\u00fcnter", ""]]}, {"id": "1902.04121", "submitter": "Evangelos Kipouridis", "authors": "Evangelos Kipouridis, Kostas Tsichlas", "title": "On the Convergence of Network Systems", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The apparent disconnection between the microscopic and the macroscopic is a\nmajor issue in the understanding of complex systems. To this extend, we study\nthe convergence of repeatedly applying local rules on a network, and touch on\nthe expressive power of this model. We look at network systems and study their\nbehavior when different types of local rules are applied on them. For a very\ngeneral class of local rules, we prove convergence and provide a certain member\nof this class that, when applied on a graph, efficiently computes its k-core\nand its (k-1)-crust giving hints on the expressive power of such a model.\nFurthermore, we provide guarantees on the speed of convergence for an important\nsubclass of the aforementioned class. We also study more general rules, and\nshow that they do not converge. Our counterexamples resolve an open question of\n(Zhang, Wang, Wang, Zhou, KDD- 2009) as well, concerning whether a certain\nprocess converges. Finally, we show the universality of our network system, by\nproviding a local rule under which it is Turing-Complete.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 20:02:28 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 17:15:52 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 11:02:34 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kipouridis", "Evangelos", ""], ["Tsichlas", "Kostas", ""]]}, {"id": "1902.04256", "submitter": "Mingda Qiao", "authors": "Mingda Qiao, Gregory Valiant", "title": "A Theory of Selective Prediction", "comments": "COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model of selective prediction, where the prediction algorithm\nis given a data sequence in an online fashion and asked to predict a\npre-specified statistic of the upcoming data points. The algorithm is allowed\nto choose when to make the prediction as well as the length of the prediction\nwindow, possibly depending on the observations so far. We prove that, even\nwithout any distributional assumption on the input data stream, a large family\nof statistics can be estimated to non-trivial accuracy. To give one concrete\nexample, suppose that we are given access to an arbitrary binary sequence $x_1,\n\\ldots, x_n$ of length $n$. Our goal is to accurately predict the average\nobservation, and we are allowed to choose the window over which the prediction\nis made: for some $t < n$ and $m \\le n - t$, after seeing $t$ observations we\npredict the average of $x_{t+1}, \\ldots, x_{t+m}$. This particular problem was\nfirst studied in Drucker (2013) and referred to as the \"density prediction\ngame\". We show that the expected squared error of our prediction can be bounded\nby $O(\\frac{1}{\\log n})$ and prove a matching lower bound, which resolves an\nopen question raised in Drucker (2013). This result holds for any sequence\n(that is not adaptive to when the prediction is made, or the predicted value),\nand the expectation of the error is with respect to the randomness of the\nprediction algorithm. Our results apply to more general statistics of a\nsequence of observations, and we highlight several open directions for future\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:45:07 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 03:12:27 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 22:00:58 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Qiao", "Mingda", ""], ["Valiant", "Gregory", ""]]}, {"id": "1902.04427", "submitter": "Oren Weimann", "authors": "Pawe{\\l} Gawrychowski, Seungbum Jo, Shay Mozes, Oren Weimann", "title": "Compressed Range Minimum Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $S$ of $n$ integers in $[0,\\sigma)$, a range minimum query\nRMQ$(i, j)$ asks for the index of the smallest integer in $S[i \\dots j]$. It is\nwell known that the problem can be solved with a succinct data structure of\nsize $2n + o(n)$ and constant query-time. In this paper we show how to\npreprocess $S$ into a compressed representation that allows fast range minimum\nqueries. This allows for sublinear size data structures with logarithmic query\ntime. The most natural approach is to use string compression and construct a\ndata structure for answering range minimum queries directly on the compressed\nstring. We investigate this approach in the context of grammar compression. We\nthen consider an alternative approach. Instead of compressing $S$ using string\ncompression, we compress the Cartesian tree of $S$ using tree compression. We\nshow that this approach can be exponentially better than the former, is never\nworse by more than an $O(\\sigma)$ factor (i.e. for constant alphabets it is\nnever asymptotically worse), and can in fact be worse by an $\\Omega(\\sigma)$\nfactor.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 15:05:59 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 09:59:23 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Jo", "Seungbum", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1902.04495", "submitter": "Linjun Zhang", "authors": "T. Tony Cai, Yichen Wang and Linjun Zhang", "title": "The Cost of Privacy: Optimal Rates of Convergence for Parameter\n  Estimation with Differential Privacy", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving data analysis is a rising challenge in contemporary\nstatistics, as the privacy guarantees of statistical methods are often achieved\nat the expense of accuracy. In this paper, we investigate the tradeoff between\nstatistical accuracy and privacy in mean estimation and linear regression,\nunder both the classical low-dimensional and modern high-dimensional settings.\nA primary focus is to establish minimax optimality for statistical estimation\nwith the $(\\varepsilon,\\delta)$-differential privacy constraint. To this end,\nwe find that classical lower bound arguments fail to yield sharp results, and\nnew technical tools are called for.\n  By refining the \"tracing adversary\" technique for lower bounds in the\ntheoretical computer science literature, we formulate a general lower bound\nargument for minimax risks with differential privacy constraints, and apply\nthis argument to high-dimensional mean estimation and linear regression\nproblems. We also design computationally efficient algorithms that attain the\nminimax lower bounds up to a logarithmic factor. In particular, for the\nhigh-dimensional linear regression, a novel private iterative hard thresholding\npursuit algorithm is proposed, based on a privately truncated version of\nstochastic gradient descent. The numerical performance of these algorithms is\ndemonstrated by simulation studies and applications to real data containing\nsensitive information, for which privacy-preserving statistical methods are\nnecessary.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 16:59:15 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 01:09:36 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 20:29:28 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 14:16:51 GMT"}, {"version": "v5", "created": "Tue, 10 Nov 2020 16:57:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Cai", "T. Tony", ""], ["Wang", "Yichen", ""], ["Zhang", "Linjun", ""]]}, {"id": "1902.04629", "submitter": "Lev Reyzin", "authors": "Shelby Heinecke and Lev Reyzin", "title": "Crowdsourced PAC Learning under Classification Noise", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze PAC learnability from labels produced by\ncrowdsourcing. In our setting, unlabeled examples are drawn from a distribution\nand labels are crowdsourced from workers who operate under classification\nnoise, each with their own noise parameter. We develop an end-to-end\ncrowdsourced PAC learning algorithm that takes unlabeled data points as input\nand outputs a trained classifier. Our three-step algorithm incorporates\nmajority voting, pure-exploration bandits, and noisy-PAC learning. We prove\nseveral guarantees on the number of tasks labeled by workers for PAC learning\nin this setting and show that our algorithm improves upon the baseline by\nreducing the total number of tasks given to workers. We demonstrate the\nrobustness of our algorithm by exploring its application to additional\nrealistic crowdsourcing settings.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 20:54:44 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Heinecke", "Shelby", ""], ["Reyzin", "Lev", ""]]}, {"id": "1902.04728", "submitter": "Surbhi Goel", "authors": "Surbhi Goel, Daniel M. Kane, Adam R. Klivans", "title": "Learning Ising Models with Independent Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first efficient algorithm for learning the structure of an Ising\nmodel that tolerates independent failures; that is, each entry of the observed\nsample is missing with some unknown probability p. Our algorithm matches the\nessentially optimal runtime and sample complexity bounds of recent work for\nlearning Ising models due to Klivans and Meka (2017).\n  We devise a novel unbiased estimator for the gradient of the Interaction\nScreening Objective (ISO) due to Vuffray et al. (2016) and apply a stochastic\nmultiplicative gradient descent algorithm to minimize this objective. Solutions\nto this minimization recover the neighborhood information of the underlying\nIsing model on a node by node basis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 03:37:44 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Goel", "Surbhi", ""], ["Kane", "Daniel M.", ""], ["Klivans", "Adam R.", ""]]}, {"id": "1902.04738", "submitter": "Emery Berger", "authors": "Bobby Powers, David Tench, Emery D. Berger, Andrew McGregor", "title": "Mesh: Compacting Memory Management for C/C++ Applications", "comments": "Draft version, accepted at PLDI 2019", "journal-ref": null, "doi": "10.1145/3314221.3314582", "report-no": null, "categories": "cs.PL cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs written in C/C++ can suffer from serious memory fragmentation,\nleading to low utilization of memory, degraded performance, and application\nfailure due to memory exhaustion. This paper introduces Mesh, a plug-in\nreplacement for malloc that, for the first time, eliminates fragmentation in\nunmodified C/C++ applications. Mesh combines novel randomized algorithms with\nwidely-supported virtual memory operations to provably reduce fragmentation,\nbreaking the classical Robson bounds with high probability. Mesh generally\nmatches the runtime performance of state-of-the-art memory allocators while\nreducing memory consumption; in particular, it reduces the memory of\nconsumption of Firefox by 16% and Redis by 39%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 04:40:05 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 19:03:03 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Powers", "Bobby", ""], ["Tench", "David", ""], ["Berger", "Emery D.", ""], ["McGregor", "Andrew", ""]]}, {"id": "1902.04740", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Sivakanth Gopi and Venkatesan Guruswami", "title": "CSPs with Global Modular Constraints: Algorithms and Hardness via\n  Polynomial Representations", "comments": "52 pages; to appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT cs.LO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of Boolean constraint satisfaction problems (CSPs)\nwhen the assignment must have Hamming weight in some congruence class modulo M,\nfor various choices of the modulus M. Due to the known classification of\ntractable Boolean CSPs, this mainly reduces to the study of three cases: 2-SAT,\nHORN-SAT, and LIN-2 (linear equations mod 2). We classify the moduli M for\nwhich these respective problems are polynomial time solvable, and when they are\nnot (assuming the ETH). Our study reveals that this modular constraint lends a\nsurprising richness to these classic, well-studied problems, with interesting\nbroader connections to complexity theory and coding theory. The HORN-SAT case\nis connected to the covering complexity of polynomials representing the NAND\nfunction mod M. The LIN-2 case is tied to the sparsity of polynomials\nrepresenting the OR function mod M, which in turn has connections to modular\nweight distribution properties of linear codes and locally decodable codes. In\nboth cases, the analysis of our algorithm as well as the hardness reduction\nrely on these polynomial representations, highlighting an interesting algebraic\ncommon ground between hard cases for our algorithms and the gadgets which show\nhardness. These new complexity measures of polynomial representations merit\nfurther study.\n  The inspiration for our study comes from a recent work by N\\\"agele, Sudakov,\nand Zenklusen on submodular minimization with a global congruence constraint.\nOur algorithm for HORN-SAT has strong similarities to their algorithm, and in\nparticular identical kind of set systems arise in both cases. Our connection to\npolynomial representations leads to a simpler analysis of such set systems, and\nalso sheds light on (but does not resolve) the complexity of submodular\nminimization with a congruency requirement modulo a composite M.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 04:49:40 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Gopi", "Sivakanth", ""], ["Guruswami", "Venkatesan", ""]]}, {"id": "1902.04741", "submitter": "Shay Moran", "authors": "Alon Cohen and Avinatan Hassidim and Haim Kaplan and Yishay Mansour\n  and Shay Moran", "title": "Learning to Screen", "comments": "15 pages, 1 figure. Extended the model and the results, changed the\n  title, and added a new lower bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a large firm with multiple departments that plans a large\nrecruitment. Candidates arrive one-by-one, and for each candidate the firm\ndecides, based on her data (CV, skills, experience, etc), whether to summon her\nfor an interview. The firm wants to recruit the best candidates while\nminimizing the number of interviews. We model such scenarios as an assignment\nproblem between items (candidates) and categories (departments): the items\narrive one-by-one in an online manner, and upon processing each item the\nalgorithm decides, based on its value and the categories it can be matched\nwith, whether to retain or discard it (this decision is irrevocable). The goal\nis to retain as few items as possible while guaranteeing that the set of\nretained items contains an optimal matching.\n  We consider two variants of this problem: (i) in the first variant it is\nassumed that the $n$ items are drawn independently from an unknown distribution\n$D$. (ii) In the second variant it is assumed that before the process starts,\nthe algorithm has an access to a training set of $n$ items drawn independently\nfrom the same unknown distribution (e.g.\\ data of candidates from previous\nrecruitment seasons). We give tight bounds on the minimum possible number of\nretained items in each of these variants. These results demonstrate that one\ncan retain exponentially less items in the second variant (with the training\nset).\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 05:02:12 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 20:51:00 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 14:44:59 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Cohen", "Alon", ""], ["Hassidim", "Avinatan", ""], ["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""], ["Moran", "Shay", ""]]}, {"id": "1902.04785", "submitter": "Solon Pissis", "authors": "Lorraine A.K. Ayad, Golnaz Badkobeh, Gabriele Fici, Alice H\\'eliou,\n  and Solon P. Pissis", "title": "Constructing Antidictionaries in Output-Sensitive Space", "comments": "Version accepted to DCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A word $x$ that is absent from a word $y$ is called minimal if all its proper\nfactors occur in $y$. Given a collection of $k$ words $y_1,y_2,\\ldots,y_k$ over\nan alphabet $\\Sigma$, we are asked to compute the set\n$\\mathrm{M}^{\\ell}_{y_{1}\\#\\ldots\\#y_{k}}$ of minimal absent words of length at\nmost $\\ell$ of word $y=y_1\\#y_2\\#\\ldots\\#y_k$, $\\#\\notin\\Sigma$. In data\ncompression, this corresponds to computing the antidictionary of $k$ documents.\nIn bioinformatics, it corresponds to computing words that are absent from a\ngenome of $k$ chromosomes. This computation generally requires $\\Omega(n)$\nspace for $n=|y|$ using any of the plenty available $\\mathcal{O}(n)$-time\nalgorithms. This is because an $\\Omega(n)$-sized text index is constructed over\n$y$ which can be impractical for large $n$. We do the identical computation\nincrementally using output-sensitive space. This goal is reasonable when\n$||\\mathrm{M}^{\\ell}_{y_{1}\\#\\ldots\\#y_{N}}||=o(n)$, for all $N\\in[1,k]$. For\ninstance, in the human genome, $n \\approx 3\\times 10^9$ but\n$||\\mathrm{M}^{12}_{y_{1}\\#\\ldots\\#y_{k}}|| \\approx 10^6$. We consider a\nconstant-sized alphabet for stating our results. We show that all\n$\\mathrm{M}^{\\ell}_{y_{1}},\\ldots,\\mathrm{M}^{\\ell}_{y_{1}\\#\\ldots\\#y_{k}}$ can\nbe computed in\n$\\mathcal{O}(kn+\\sum^{k}_{N=1}||\\mathrm{M}^{\\ell}_{y_{1}\\#\\ldots\\#y_{N}}||)$\ntotal time using $\\mathcal{O}(\\mathrm{MaxIn}+\\mathrm{MaxOut})$ space, where\n$\\mathrm{MaxIn}$ is the length of the longest word in $\\{y_1,\\ldots,y_{k}\\}$\nand\n$\\mathrm{MaxOut}=\\max\\{||\\mathrm{M}^{\\ell}_{y_{1}\\#\\ldots\\#y_{N}}||:N\\in[1,k]\\}$.\nProof-of-concept experimental results are also provided confirming our\ntheoretical findings and justifying our contribution.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 08:45:51 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ayad", "Lorraine A. K.", ""], ["Badkobeh", "Golnaz", ""], ["Fici", "Gabriele", ""], ["H\u00e9liou", "Alice", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1902.04805", "submitter": "Charles Gueunet", "authors": "Charles Gueunet (LIP6), P. Fortin (LLR), J Jomier, J Tierny", "title": "Task-based Augmented Contour Trees with Fibonacci Heaps", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, Institute\n  of Electrical and Electronics Engineers, In press", "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DM cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for the fast, shared memory, multi-core\ncomputation of augmented contour trees on triangulations. In contrast to most\nexisting parallel algorithms our technique computes augmented trees, enabling\nthe full extent of contour tree based applications including data segmentation.\nOur approach completely revisits the traditional, sequential contour tree\nalgorithm to re-formulate all the steps of the computation as a set of\nindependent local tasks. This includes a new computation procedure based on\nFibonacci heaps for the join and split trees, two intermediate data structures\nused to compute the contour tree, whose constructions are efficiently carried\nout concurrently thanks to the dynamic scheduling of task parallelism. We also\nintroduce a new parallel algorithm for the combination of these two trees into\nthe output global contour tree. Overall, this results in superior time\nperformance in practice, both in sequential and in parallel thanks to the\nOpenMP task runtime. We report performance numbers that compare our approach to\nreference sequential and multi-threaded implementations for the computation of\naugmented merge and contour trees. These experiments demonstrate the run-time\nefficiency of our approach and its scalability on common workstations. We\ndemonstrate the utility of our approach in data segmentation applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 09:31:03 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Gueunet", "Charles", "", "LIP6"], ["Fortin", "P.", "", "LLR"], ["Jomier", "J", ""], ["Tierny", "J", ""]]}, {"id": "1902.04919", "submitter": "Ioannis Katsikarelis", "authors": "R\\'emy Belmonte, Tesshu Hanaka, Ioannis Katsikarelis, Eun Jung Kim,\n  Michael Lampis", "title": "New Results on Directed Edge Dominating Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of generalizations of Edge Dominating Set on directed\ngraphs called Directed $(p,q)$-Edge Dominating Set. In this problem an arc\n$(u,v)$ is said to dominate itself, as well as all arcs which are at distance\nat most $q$ from $v$, or at distance at most $p$ to $u$.\n  First, we give significantly improved FPT algorithms for the two most\nimportant cases of the problem, $(0,1)$-dEDS and $(1,1)$-dEDS (that correspond\nto versions of Dominating Set on line graphs), as well as polynomial kernels.\nWe also improve the best-known approximation for these cases from logarithmic\nto constant. In addition, we show that $(p,q)$-dEDS is FPT parameterized by\n$p+q+tw$, but W-hard parameterized by $tw$ (even if the size of the optimal is\nadded as a second parameter), where $tw$ is the treewidth of the underlying\ngraph of the input.\n  We then go on to focus on the complexity of the problem on tournaments. Here,\nwe provide a complete classification for every possible fixed value of $p,q$,\nwhich shows that the problem exhibits a surprising behavior, including cases\nwhich are in P; cases which are solvable in quasi-polynomial time but not in P;\nand a single case $(p=q=1)$ which is NP-hard (under randomized reductions) and\ncannot be solved in sub-exponential time, under standard assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:30:41 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 16:44:45 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Hanaka", "Tesshu", ""], ["Katsikarelis", "Ioannis", ""], ["Kim", "Eun Jung", ""], ["Lampis", "Michael", ""]]}, {"id": "1902.05101", "submitter": "Sami Davies", "authors": "Sami Davies, Miklos Z. Racz, Cyrus Rashtchian", "title": "Reconstructing Trees from Traces", "comments": "Major revisions in the new version including algorithm descriptions,\n  more details in section 3.1, and several new figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a node-labeled tree given independent traces\nfrom an appropriately defined deletion channel. This problem, tree trace\nreconstruction, generalizes string trace reconstruction, which corresponds to\nthe tree being a path. For many classes of trees, including complete trees and\nspiders, we provide algorithms that reconstruct the labels using only a\npolynomial number of traces. This exhibits a stark contrast to known results on\nstring trace reconstruction, which require exponentially many traces, and where\na central open problem is to determine whether a polynomial number of traces\nsuffice. Our techniques combine novel combinatorial and complex analytic\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 19:59:36 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 22:38:46 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 23:16:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Davies", "Sami", ""], ["Racz", "Miklos Z.", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1902.05134", "submitter": "Lefteris Zervakis", "authors": "Lefteris Zervakis, Vinay Setty, Christos Tryfonopoulos, Katja Hose", "title": "Efficient Continuous Multi-Query Processing over Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are ubiquitous and ever-present data structures that have a wide range\nof applications involving social networks, knowledge bases and biological\ninteractions. The evolution of a graph in such scenarios can yield important\ninsights about the nature and activities of the underlying network, which can\nthen be utilized for applications such as news dissemination, network\nmonitoring, and content curation. Capturing the continuous evolution of a graph\ncan be achieved by long-standing sub-graph queries. Although, for many\napplications this can only be achieved by a set of queries, state-of-the-art\napproaches focus on a single query scenario. In this paper, we therefore\nintroduce the notion of continuous multi-query processing over graph streams\nand discuss its application to a number of use cases. To this end, we designed\nand developed a novel algorithmic solution for efficient multi-query evaluation\nagainst a stream of graph updates and experimentally demonstrated its\napplicability. Our results against two baseline approaches using real-world, as\nwell as synthetic datasets, confirm a two orders of magnitude improvement of\nthe proposed solution.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 21:14:10 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zervakis", "Lefteris", ""], ["Setty", "Vinay", ""], ["Tryfonopoulos", "Christos", ""], ["Hose", "Katja", ""]]}, {"id": "1902.05166", "submitter": "Bryce Sandlund", "authors": "J. Ian Munro, Bryce Sandlund, Corwin Sinnamon", "title": "Space-Efficient Data Structures for Lattices", "comments": "Accepted in SWAT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lattice is a partially-ordered set in which every pair of elements has a\nunique meet (greatest lower bound) and join (least upper bound). We present new\ndata structures for lattices that are simple, efficient, and nearly optimal in\nterms of space complexity.\n  Our first data structure can answer partial order queries in constant time\nand find the meet or join of two elements in $O(n^{3/4})$ time, where $n$ is\nthe number of elements in the lattice. It occupies $O(n^{3/2}\\log n)$ bits of\nspace, which is only a $\\Theta(\\log n)$ factor from the $\\Theta(n^{3/2})$-bit\nlower bound for storing lattices. The preprocessing time is $O(n^2)$. This\nstructure admits a simple space-time tradeoff so that, for any $c \\in\n[\\frac{1}{2}, 1]$, the data structure supports meet and join queries in\n$O(n^{1-c/2})$ time, occupies $O(n^{1+c}\\log n)$ bits of space, and can be\nconstructed in $O(n^2 + n^{1+3c/2})$ time.\n  Our second data structure uses $O(n^{3/2}\\log n)$ bits of space and supports\nmeet and join in $O(d \\frac{\\log n}{\\log d})$ time, where $d$ is the maximum\ndegree of any element in the transitive reduction graph of the lattice. This\nstructure is much faster for lattices with low-degree elements.\n  This paper also identifies an error in a long-standing solution to the\nproblem of representing lattices. We discuss the issue with this previous work.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 23:47:23 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 04:59:57 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Munro", "J. Ian", ""], ["Sandlund", "Bryce", ""], ["Sinnamon", "Corwin", ""]]}, {"id": "1902.05224", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto and Yasuo Tabei", "title": "Conversion from RLBWT to LZ77", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Converting a compressed format of a string into another compressed format\nwithout an explicit decompression is one of the central research topics in\nstring processing. We discuss the problem of converting the run-length\nBurrows-Wheeler Transform (RLBWT) of a string to Lempel-Ziv 77 (LZ77) phrases\nof the reversed string. The first results with Policriti and Prezza's\nconversion algorithm [Algorithmica 2018] were $O(n \\log r)$ time and $O(r)$\nworking space for length of the string $n$, number of runs $r$ in the RLBWT,\nand number of LZ77 phrases $z$. Recent results with Kempa's conversion\nalgorithm [SODA 2019] are $O(n / \\log n + r \\log^{9} n + z \\log^{9} n)$ time\nand $O(n / \\log_{\\sigma} n + r \\log^{8} n)$ working space for the alphabet size\n$\\sigma$ of the RLBWT. In this paper, we present a new conversion algorithm by\nimproving Policriti and Prezza's conversion algorithm where dynamic data\nstructures for general purpose are used. We argue that these dynamic data\nstructures can be replaced and present new data structures for faster\nconversion. The time and working space of our conversion algorithm with new\ndata structures are $O(n \\min \\{ \\log \\log n, \\sqrt{\\frac{\\log r}{\\log\\log r}}\n\\})$ and $O(r)$, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 05:09:57 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["Tabei", "Yasuo", ""]]}, {"id": "1902.05432", "submitter": "Thomas Lidbetter Dr", "authors": "Thomas Lidbetter", "title": "Search and Rescue in the Face of Uncertain Threats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a search problem in which one or more targets must be rescued by\na search party, or Searcher. The targets may be survivors of some natural\ndisaster, or prisoners held by an adversary. The targets are hidden among a\nfinite set of locations, but when a location is searched, there is a known\nprobability that the search will come to an end, perhaps because the Searcher\nbecomes trapped herself, or is captured by the adversary. If this happens\nbefore all the targets have been recovered, then the rescue attempt is deemed a\nfailure. The objective is to find the search that maximizes the probability of\nrecovering all the targets. We present and solve a game theoretic model for\nthis problem, by placing it in a more general framework that encompasses\nanother game previously introduced by the author. We also consider an extension\nto the game in which the targets are hidden on the vertices of a graph. In the\ncase that there is only one target, we give a solution of the game played on a\ntree.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 02:11:03 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 19:40:47 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Lidbetter", "Thomas", ""]]}, {"id": "1902.05638", "submitter": "Abhinav Mishra", "authors": "Abhinav Mishra", "title": "Finding Nearest Neighbors in graphs locally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed learning techniques have been motivated by the increasing\nsize of datasets and their inability to fit into main memory on a single\nmachine. We propose an algorithm that finds the nearest neighbor in a graph\nlocally without the need of visiting the whole graph. Our algorithm is\ndistributed which further encourage scalability. We prove the convergence of\nthe algorithm\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 22:51:48 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Mishra", "Abhinav", ""]]}, {"id": "1902.05659", "submitter": "Julian Yarkony", "authors": "Margret Keuper, Jovita Lukasik, Maneesh Singh, Julian Yarkony", "title": "Massively Parallel Benders Decomposition for Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of graph partitioning for image segmentation using\ncorrelation clustering (CC), which we treat as an integer linear program (ILP).\nWe reformulate optimization in the ILP so as to admit efficient optimization\nvia Benders decomposition, a classic technique from operations research. Our\nBenders decomposition formulation has many subproblems, each associated with a\nnode in the CC instance's graph, which are solved in parallel. Each Benders\nsubproblem enforces the cycle inequalities corresponding to the negative weight\nedges attached to its corresponding node in the CC instance. We generate\nMagnanti-Wong Benders rows in addition to standard Benders rows, to accelerate\noptimization. Our Benders decomposition approach provides a promising new\navenue to accelerate optimization for CC, and allows for massive\nparallelization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 01:50:07 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 17:44:22 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Keuper", "Margret", ""], ["Lukasik", "Jovita", ""], ["Singh", "Maneesh", ""], ["Yarkony", "Julian", ""]]}, {"id": "1902.05678", "submitter": "Shuichi Miyazaki", "authors": "Koki Hamada, Shuichi Miyazaki, Hiroki Yanagisawa", "title": "Strategy-Proof Approximation Algorithms for the Stable Marriage Problem\n  with Ties and Incomplete Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stable marriage problem (SM), a mechanism that always outputs a stable\nmatching is called a stable mechanism. One of the well-known stable mechanisms\nis the man-oriented Gale-Shapley algorithm (MGS). MGS has a good property that\nit is strategy-proof to the men's side, i.e., no man can obtain a better\noutcome by falsifying a preference list. We call such a mechanism a\nman-strategy-proof mechanism. Unfortunately, MGS is not a woman-strategy-proof\nmechanism. Roth has shown that there is no stable mechanism that is\nsimultaneously man-strategy-proof and woman-strategy-proof, which is known as\nRoth's impossibility theorem.\n  In this paper, we extend these results to the stable marriage problem with\nties and incomplete lists (SMTI). Since SMTI is an extension of SM, Roth's\nimpossibility theorem takes over to SMTI. Therefore, we focus on the\none-sided-strategy-proofness. In SMTI, one instance can have stable matchings\nof different sizes, and it is natural to consider the problem of finding a\nlargest stable matching, known as MAX SMTI. Thus we incorporate the notion of\napproximation ratio used in the theory of approximation algorithms. We say that\na stable-mechanism is $c$-approximate-stable mechanism if it always returns a\nstable matching of size at least $1/c$ of a largest one. We also consider a\nrestricted variant of MAX SMTI, which we call MAX SMTI-1TM, where only men's\nlists can contain ties.\n  Our results are summarized as follows: (i) MAX SMTI admits both a\nman-strategy-proof 2-approximate-stable mechanism and a woman-strategy-proof\n2-approximate-stable mechanism. (ii) MAX SMTI-1TM admits a woman-strategy-proof\n2-approximate-stable mechanism. (iii) MAX SMTI-1TM admits a man-strategy-proof\n1.5-approximate-stable mechanism. All these results are tight in terms of\napproximation ratios. Also, all these strategy-proofness results apply for\nstrategy-proofness against coalitions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 04:06:50 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Hamada", "Koki", ""], ["Miyazaki", "Shuichi", ""], ["Yanagisawa", "Hiroki", ""]]}, {"id": "1902.05877", "submitter": "Alex Pothen", "authors": "Ahmed Al-Herz and Alex Pothen", "title": "A 2/3-Approximation Algorithm for Vertex-weighted Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum vertex-weighted matching problem (MVM) for\nnon-bipartite graphs. In earlier work we have described a 2/3-approximation\nalgorithm for the MVM on bipartite graphs (Dobrian, Halappanavar, Pothen and\nAl-Herz, SIAM J. Scientific Computing, 2019). Here we show that a\n2/3-approximation algorithm for MVM on non-bipartite graphs can be obtained by\nrestricting the length of augmenting paths to at most three. The algorithm has\ntime complexity $O(m \\log \\Delta + n \\log n)$, where $n$ is the number of\nvertices, $m$ is the number of edges, and $\\Delta$ is the maximum degree of a\nvertex.\n  The approximation ratio of the algorithm is obtained by considering failed\nvertices, i.e., vertices that the approximation algorithm fails to match but\nthe exact algorithm does. We show that there are two distinct heavier matched\nvertices that we can charge each failed vertex to. Our proof techniques\ncharacterize the structure of augmenting paths in a novel way.\n  We have implemented the 2/3-approximation algorithm and show that it runs in\nunder a minute on graphs with tens of millions of vertices and hundreds of\nmillions of edges. We compare its performance with five other algorithms: an\nexact algorithm for MVM, an exact algorithm for the maximum edge-weighted\nmatching (MEM) problem, as well as three approximation algorithms. In our test\nset of nineteen problems, there are graphs on which the exact algorithms fail\nto terminate in 100 hours. The new 2/3-approximation algorithm for MVM\noutperforms the other approximation algorithms by either being faster (often by\norders of magnitude) or obtaining better weights.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 16:25:02 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Al-Herz", "Ahmed", ""], ["Pothen", "Alex", ""]]}, {"id": "1902.05981", "submitter": "Ehsan Kazemi", "authors": "Marko Mitrovic and Ehsan Kazemi and Moran Feldman and Andreas Krause\n  and Amin Karbasi", "title": "Adaptive Sequence Submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, one needs to interactively select a\nsequence of items (e.g., recommending movies based on a user's feedback) or\nmake sequential decisions in a certain order (e.g., guiding an agent through a\nseries of states). Not only do sequences already pose a dauntingly large search\nspace, but we must also take into account past observations, as well as the\nuncertainty of future outcomes. Without further structure, finding an optimal\nsequence is notoriously challenging, if not completely intractable. In this\npaper, we view the problem of adaptive and sequential decision making through\nthe lens of submodularity and propose an adaptive greedy policy with strong\ntheoretical guarantees. Additionally, to demonstrate the practical utility of\nour results, we run experiments on Amazon product recommendation and Wikipedia\nlink prediction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 20:37:14 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 16:04:03 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Mitrovic", "Marko", ""], ["Kazemi", "Ehsan", ""], ["Feldman", "Moran", ""], ["Krause", "Andreas", ""], ["Karbasi", "Amin", ""]]}, {"id": "1902.06039", "submitter": "Chen Dingding", "authors": "Yanchen Deng, Ziyu Chen, Dingding Chen, Xingqiong Jiang, Qiang Li", "title": "PT-ISABB: A Hybrid Tree-based Complete Algorithm to Solve Asymmetric\n  Distributed Constraint Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric Distributed Constraint Optimization Problems (ADCOPs) have emerged\nas an important formalism in multi-agent community due to their ability to\ncapture personal preferences. However, the existing search-based complete\nalgorithms for ADCOPs can only use local knowledge to compute lower bounds,\nwhich leads to inefficient pruning and prohibits them from solving large scale\nproblems. On the other hand, inference-based complete algorithms (e.g., DPOP)\nfor Distributed Constraint Optimization Problems (DCOPs) require only a linear\nnumber of messages, but they cannot be directly applied into ADCOPs due to a\nprivacy concern. Therefore, in the paper, we consider the possibility of\ncombining inference and search to effectively solve ADCOPs at an acceptable\nloss of privacy. Specifically, we propose a hybrid complete algorithm called\nPT-ISABB which uses a tailored inference algorithm to provide tight lower\nbounds and a tree-based complete search algorithm to exhaust the search space.\nWe prove the correctness of our algorithm and the experimental results\ndemonstrate its superiority over other state-of-the-art complete algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 04:30:05 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 03:45:23 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 14:25:30 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 02:54:29 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Deng", "Yanchen", ""], ["Chen", "Ziyu", ""], ["Chen", "Dingding", ""], ["Jiang", "Xingqiong", ""], ["Li", "Qiang", ""]]}, {"id": "1902.06090", "submitter": "Andrzej Pelc", "authors": "Andrzej Pelc, Ram Narayan Yadav", "title": "Cost vs. Information Tradeoffs for Treasure Hunt in the Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile agent has to find an inert treasure hidden in the plane. Both the\nagent and the treasure are modeled as points. This is a variant of the task\nknown as treasure hunt. The treasure is at a distance at most $D$ from the\ninitial position of the agent, and the agent finds the treasure when it gets at\ndistance $r$ from it, called the {\\em vision radius}. However, the agent does\nnot know the location of the treasure and does not know the parameters $D$ and\n$r$. The cost of finding the treasure is the length of the trajectory of the\nagent. We investigate the tradeoffs between the amount of information held {\\em\na priori} by the agent and the cost of treasure hunt. Following the\nwell-established paradigm of {\\em algorithms with advice}, this information is\ngiven to the agent in advance as a binary string, by an oracle cooperating with\nthe agent and knowing the location of the treasure and the initial position of\nthe agent. The size of advice given to the agent is the length of this binary\nstring.\n  For any size $z$ of advice and any $D$ and $r$, let $OPT(z,D,r)$ be the\noptimal cost of finding the treasure for parameters $z$, $D$ and $r$, if the\nagent has only an advice string of length $z$ as input. We design treasure hunt\nalgorithms working with advice of size $z$ at cost $O(OPT(z,D,r))$ whenever\n$r\\leq 1$ or $r\\geq 0.9D$. For intermediate values of $r$, i.e., $1<r<0.9D$, we\ndesign an almost optimal scheme of algorithms: for any constant $\\alpha>0$, the\ntreasure can be found at cost $O(OPT(z,D,r)^{1+\\alpha})$.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 11:17:30 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Pelc", "Andrzej", ""], ["Yadav", "Ram Narayan", ""]]}, {"id": "1902.06103", "submitter": "Birgit Vogtenhuber", "authors": "Oswin Aichholzer and Jean Cardinal and Tony Huynh and Kolja Knauer and\n  Torsten M\\\"utze and Raphael Steiner and Birgit Vogtenhuber", "title": "Flip distances between graph orientations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flip graphs are a ubiquitous class of graphs, which encode relations induced\non a set of combinatorial objects by elementary, local changes. Skeletons of\nassociahedra, for instance, are the graphs induced by quadrilateral flips in\ntriangulations of a convex polygon. For some definition of a flip graph, a\nnatural computational problem to consider is the flip distance: Given two\nobjects, what is the minimum number of flips needed to transform one into the\nother?\n  We consider flip graphs on orientations of simple graphs, where flips consist\nof reversing the direction of some edges. More precisely, we consider so-called\n$\\alpha$-orientations of a graph $G$, in which every vertex $v$ has a specified\noutdegree $\\alpha(v)$, and a flip consists of reversing all edges of a directed\ncycle. We prove that deciding whether the flip distance between two\n$\\alpha$-orientations of a planar graph $G$ is at most two is \\NP-complete.\nThis also holds in the special case of perfect matchings, where flips involve\nalternating cycles. This problem amounts to finding geodesics on the common\nbase polytope of two partition matroids, or, alternatively, on an alcoved\npolytope. It therefore provides an interesting example of a flip distance\nquestion that is computationally intractable despite having a natural\ninterpretation as a geodesic on a nicely structured combinatorial polytope.\n  We also consider the dual question of the flip distance betwe en graph\norientations in which every cycle has a specified number of forward edges, and\na flip is the reversal of all edges in a minimal directed cut. In general, the\nproblem remains hard. However, if we restrict to flips that only change sinks\ninto sources, or vice-versa, then the problem can be solved in polynomial time.\nHere we exploit the fact that the flip graph is the cover graph of a\ndistributive lattice. This generalizes a recent result from Zhang, Qian, and\nZhang.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 13:47:27 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Aichholzer", "Oswin", ""], ["Cardinal", "Jean", ""], ["Huynh", "Tony", ""], ["Knauer", "Kolja", ""], ["M\u00fctze", "Torsten", ""], ["Steiner", "Raphael", ""], ["Vogtenhuber", "Birgit", ""]]}, {"id": "1902.06179", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle", "title": "Interlaced Greedy Algorithm for Maximization of Submodular Functions in\n  Nearly Linear Time", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deterministic approximation algorithm is presented for the maximization of\nnon-monotone submodular functions over a ground set of size $n$ subject to\ncardinality constraint $k$; the algorithm is based upon the idea of interlacing\ntwo greedy procedures. The algorithm uses interlaced, thresholded greedy\nprocedures to obtain tight ratio $1/4 - \\epsilon$ in $O \\left(\n\\frac{n}{\\epsilon} \\log \\left( \\frac{k}{\\epsilon} \\right) \\right)$ queries of\nthe objective function, which improves upon both the ratio and the quadratic\ntime complexity of the previously fastest deterministic algorithm for this\nproblem. The algorithm is validated in the context of two applications of\nnon-monotone submodular maximization, on which it outperforms the fastest\ndeterministic and randomized algorithms in prior literature.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 00:51:55 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 21:00:59 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 19:28:40 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Kuhnle", "Alan", ""]]}, {"id": "1902.06196", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Nearest neighbor decoding for Tardos fingerprinting codes", "comments": "6 pages, 1 figure, 2 tables", "journal-ref": "ACM Workshop on Information Hiding and Multimedia Security\n  (IH&MMSec), pp. 182-187, 2019", "doi": "10.1145/3335203.3335732", "report-no": null, "categories": "cs.CR cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, various improvements have been made to Tardos'\ncollusion-resistant fingerprinting scheme [Tardos, STOC 2003], ultimately\nresulting in a good understanding of what is the minimum code length required\nto achieve collusion-resistance. In contrast, decreasing the cost of the actual\ndecoding algorithm for identifying the potential colluders has received less\nattention, even though previous results have shown that using joint decoding\nstrategies, deemed too expensive for decoding, may lead to better code lengths.\nMoreover, in dynamic settings a fast decoder may be required to provide answers\nin real-time, further raising the question whether the decoding costs of\nscore-based fingerprinting schemes can be decreased with a smarter decoding\nalgorithm. In this paper we show how to model the decoding step of score-based\nfingerprinting as a nearest neighbor search problem, and how this relation\nallows us to apply techniques from the field of (approximate) nearest neighbor\nsearching to obtain decoding times which are sublinear in the total number of\nusers. As this does not affect the encoding and embedding steps, this decoding\nmechanism can easily be deployed within existing fingerprinting schemes, and\nthis may bring a truly efficient joint decoder closer to reality. Besides the\napplication to fingerprinting, similar techniques can be used to decrease the\ndecoding costs of group testing methods, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 03:35:23 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1902.06243", "submitter": "Nick Gravin", "authors": "Nick Gravin and Hongao Wang", "title": "Prophet inequality for bipartite matching: merits of being simple and\n  non adaptive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian online selection problem of a matching in bipartite\ngraphs, i.e., online weighted matching problem with edge arrivals where online\nalgorithm knows distributions of weights, that corresponds to the intersection\nof two matroids in [Kleinberg and Wienberg STOC 12] model. We consider a simple\nclass of non adaptive vertex-additive policies that assign static prices to all\nvertices in the graph and accept each edge only if its weight exceeds the sum\nof the prices of the edge's endpoints. We show existence of a vertex-additive\npolicy with the expected payoff of at least one third of the prophet's payoff\nand present gradient decent type algorithm that quickly converges to the\ndesired vector of vertex prices. This improves the adaptive online policy of\n[Kleinberg and Wienberg STOC 12] for the intersection of two matroids in two\nways: our policy is non adaptive and has better approximation guarantee of $3$\ninstead of previous guarantee of $5.82$ against the prophet. We give a\ncomplementary lower bound of $2.25$ for any online algorithm in the bipartite\nmatching setting.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 10:51:49 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Gravin", "Nick", ""], ["Wang", "Hongao", ""]]}, {"id": "1902.06332", "submitter": "Lin Chen", "authors": "Mingrui Zhang, Lin Chen, Aryan Mokhtari, Hamed Hassani, Amin Karbasi", "title": "Quantized Frank-Wolfe: Faster Optimization, Lower Communication, and\n  Projection Free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we efficiently mitigate the overhead of gradient communications in\ndistributed optimization? This problem is at the heart of training scalable\nmachine learning models and has been mainly studied in the unconstrained\nsetting. In this paper, we propose Quantized-Frank-Wolfe (QFW), the first\nprojection-free and communication-efficient algorithm for solving constrained\noptimization problems at scale. We consider both convex and non-convex\nobjective functions, expressed as a finite-sum or more generally a stochastic\noptimization problem, and provide strong theoretical guarantees on the\nconvergence rate of QFW. This is accomplished by proposing novel quantization\nschemes that efficiently compress gradients while controlling the noise\nvariance introduced during this process. Finally, we empirically validate the\nefficiency of QFW in terms of communication and the quality of returned\nsolution against natural baselines.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 21:43:10 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 15:36:18 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 20:15:16 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Zhang", "Mingrui", ""], ["Chen", "Lin", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1902.06380", "submitter": "Gregory Rosenthal", "authors": "Gregory Rosenthal", "title": "Beating Treewidth for Average-Case Subgraph Isomorphism", "comments": "31 pages. International Symposium on Parameterized and Exact\n  Computation (IPEC) 2019", "journal-ref": null, "doi": "10.4230/LIPIcs.IPEC.2019.24", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any fixed graph $G$, the subgraph isomorphism problem asks whether an\n$n$-vertex input graph has a subgraph isomorphic to $G$. A well-known algorithm\nof Alon, Yuster and Zwick (1995) efficiently reduces this to the \"colored\"\nversion of the problem, denoted $G$-$\\mathsf{SUB}$, and then solves\n$G$-$\\mathsf{SUB}$ in time $O(n^{tw(G)+1})$ where $tw(G)$ is the treewidth of\n$G$. Marx (2010) conjectured that $G$-$\\mathsf{SUB}$ requires time\n$\\Omega(n^{\\mathrm{const}\\cdot tw(G)})$ and, assuming the Exponential Time\nHypothesis, proved a lower bound of $\\Omega(n^{\\mathrm{const}\\cdot emb(G)})$\nfor a certain graph parameter $emb(G) \\ge \\Omega(tw(G)/\\log tw(G))$. With\nrespect to the size of $\\mathrm{AC}^0$ circuits solving $G$-$\\mathsf{SUB}$ in\nthe average case, Li, Razborov and Rossman (2017) proved (unconditional) upper\nand lower bounds of $O(n^{2\\kappa(G)+\\mathrm{const}})$ and\n$\\Omega(n^{\\kappa(G)})$ for a different graph parameter $\\kappa(G) \\ge\n\\Omega(tw(G)/\\log tw(G))$.\n  Our contributions are as follows. First, we prove that $emb(G)$ is\n$O(\\kappa(G))$ for all graphs $G$. Next, we show that $\\kappa(G)$ can be\nasymptotically less than $tw(G)$; for example, if $G$ is a hypercube then\n$\\kappa(G)$ is $\\Theta\\big(tw(G)\\big/\\sqrt{\\log tw(G)}\\big)$. This implies that\nthe average-case complexity of $G$-$\\mathsf{SUB}$ is $n^{o(tw(G))}$ when $G$ is\na hypercube. Finally, we construct $\\mathrm{AC}^0$ circuits of size\n$O(n^{\\kappa(G)+\\mathrm{const}})$ that solve $G$-$\\mathsf{SUB}$ in the average\ncase, closing the gap between the upper and lower bounds of Li et al.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 02:53:32 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 14:45:20 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 17:00:22 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 05:01:26 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Rosenthal", "Gregory", ""]]}, {"id": "1902.06391", "submitter": "Adrian Vladu", "authors": "Alina Ene, Adrian Vladu", "title": "Improved Convergence for $\\ell_\\infty$ and $\\ell_1$ Regression via\n  Iteratively Reweighted Least Squares", "comments": "Appears in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iteratively reweighted least squares method (IRLS) is a popular technique\nused in practice for solving regression problems. Various versions of this\nmethod have been proposed, but their theoretical analyses failed to capture the\ngood practical performance.\n  In this paper we propose a simple and natural version of IRLS for solving\n$\\ell_\\infty$ and $\\ell_1$ regression, which provably converges to a\n$(1+\\epsilon)$-approximate solution in\n$O(m^{1/3}\\log(1/\\epsilon)/\\epsilon^{2/3} + \\log m/\\epsilon^2)$ iterations,\nwhere $m$ is the number of rows of the input matrix. Interestingly, this\nrunning time is independent of the conditioning of the input, and the dominant\nterm of the running time depends sublinearly in $\\epsilon^{-1}$, which is\natypical for the optimization of non-smooth functions.\n  This improves upon the more complex algorithms of Chin et al. (ITCS '12), and\nChristiano et al. (STOC '11) by a factor of at least $1/\\epsilon^2$, and yields\na truly efficient natural algorithm for the slime mold dynamics\n(Straszak-Vishnoi, SODA '16, ITCS '16, ITCS '17).\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 03:45:08 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 16:10:04 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Ene", "Alina", ""], ["Vladu", "Adrian", ""]]}, {"id": "1902.06473", "submitter": "Jean Cardinal", "authors": "Jean Cardinal, Gwena\\\"el Joret, J\\'er\\'emie Roland", "title": "Information-theoretic lower bounds for quantum sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the quantum query complexity of sorting under partial information.\nIn this problem, we are given a partially ordered set $P$ and are asked to\nidentify a linear extension of $P$ using pairwise comparisons. For the standard\nsorting problem, in which $P$ is empty, it is known that the quantum query\ncomplexity is not asymptotically smaller than the classical\ninformation-theoretic lower bound. We prove that this holds for a wide class of\npartially ordered sets, thereby improving on a result from Yao (STOC'04).\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 09:16:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Cardinal", "Jean", ""], ["Joret", "Gwena\u00ebl", ""], ["Roland", "J\u00e9r\u00e9mie", ""]]}, {"id": "1902.06575", "submitter": "Fabrizio Frati", "authors": "Giordano Da Lozzo, Giuseppe Di Battista, Fabrizio Frati", "title": "Extending Upward Planar Graph Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the computational complexity of the Upward Planarity\nExtension problem, which takes in input an upward planar drawing $\\Gamma_H$ of\na subgraph $H$ of a directed graph $G$ and asks whether $\\Gamma_H$ can be\nextended to an upward planar drawing of $G$. Our study fits into the line of\nresearch on the extensibility of partial representations, which has recently\nbecome a mainstream in Graph Drawing.\n  We show the following results.\n  First, we prove that the Upward Planarity Extension problem is NP-complete,\neven if $G$ has a prescribed upward embedding, the vertex set of $H$ coincides\nwith the one of $G$, and $H$ contains no edge.\n  Second, we show that the Upward Planarity Extension problem can be solved in\n$O(n \\log n)$ time if $G$ is an $n$-vertex upward planar $st$-graph. This\nresult improves upon a known $O(n^2)$-time algorithm, which however applies to\nall $n$-vertex single-source upward planar graphs.\n  Finally, we show how to solve in polynomial time a surprisingly difficult\nversion of the Upward Planarity Extension problem, in which $G$ is a directed\npath or cycle with a prescribed upward embedding, $H$ contains no edges, and no\ntwo vertices share the same $y$-coordinate in $\\Gamma_H$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 14:03:25 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""]]}, {"id": "1902.06687", "submitter": "Benjamin Coleman", "authors": "Benjamin Coleman, Richard G. Baraniuk, Anshumali Shrivastava", "title": "Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data", "comments": "Published in ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first sublinear memory sketch that can be queried to find the\nnearest neighbors in a dataset. Our online sketching algorithm compresses an N\nelement dataset to a sketch of size $O(N^b \\log^3 N)$ in $O(N^{(b+1)} \\log^3\nN)$ time, where $b < 1$. This sketch can correctly report the nearest neighbors\nof any query that satisfies a stability condition parameterized by $b$. We\nachieve sublinear memory performance on stable queries by combining recent\nadvances in locality sensitive hash (LSH)-based estimators, online kernel\ndensity estimation, and compressed sensing. Our theoretical results shed new\nlight on the memory-accuracy tradeoff for nearest neighbor search, and our\nsketch, which consists entirely of short integer arrays, has a variety of\nattractive features in practice. We evaluate the memory-recall tradeoff of our\nmethod on a friend recommendation task in the Google Plus social media network.\nWe obtain orders of magnitude better compression than the random projection\nbased alternative while retaining the ability to report the nearest neighbors\nof practical queries.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 17:53:28 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:33:03 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 14:23:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Coleman", "Benjamin", ""], ["Baraniuk", "Richard G.", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1902.06713", "submitter": "C\\'icero A. de Lima", "authors": "C\\'icero A. de Lima", "title": "SFCM-R: A novel algorithm for the hamiltonian sequence problem", "comments": "38 pages; 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hamiltonian sequence is a path walk $P$ that can be a hamiltonian path or\nhamiltonian circuit. Determining whether such hamiltonian sequence exists in a\ngiven graph \\G is a NP-Complete problem. In this paper, a novel algorithm for\nhamiltonian sequence problem is proposed. The proposed algorithm assumes that\n$G$ has potential forbidden minors that prevent a potential hamiltonian\nsequence $P^\\prime$ from being a hamiltonian sequence. The algorithm's goal is\nto degenerate such potential forbidden minors in a two-phrase process. In first\nphrase, the algorithm passes through $G$ in order to construct a potential\nhamiltonian sequence $P^\\prime$ with the aim of degenerating these potential\nforbidden minors. The algorithm, in turn, tries to reconstruct $P^\\prime$ in\nsecond phrase by using a goal-oriented approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 18:31:58 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 17:30:32 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 17:59:25 GMT"}, {"version": "v4", "created": "Mon, 17 Jun 2019 18:00:48 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["de Lima", "C\u00edcero A.", ""]]}, {"id": "1902.06749", "submitter": "Pablo Antonio Moreno Casares", "authors": "P. A. M. Casares and M. A. Martin-Delgado", "title": "A Quantum Interior-Point Predictor-Corrector Algorithm for Linear\n  Programming", "comments": "Revtex file, color figures. Minor changes and typo correction from\n  previous version", "journal-ref": "Journal of Physics A: Mathematical and Theretical 2020", "doi": "10.1088/1751-8121/abb439", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new quantum optimization algorithm for dense Linear\nProgramming problems, which can be seen as the quantization of the Interior\nPoint Predictor-Corrector algorithm \\cite{Predictor-Corrector} using a Quantum\nLinear System Algorithm \\cite{DenseHHL}. The (worst case) work complexity of\nour method is, up to polylogarithmic factors,\n$O(L\\sqrt{n}(n+m)\\overline{||M||_F}\\bar{\\kappa}^2\\epsilon^{-2})$ for $n$ the\nnumber of variables in the cost function, $m$ the number of constraints,\n$\\epsilon^{-1}$ the target precision, $L$ the bit length of the input data,\n$\\overline{||M||_F}$ an upper bound to the Frobenius norm of the linear systems\nof equations that appear, $||M||_F$, and $\\bar{\\kappa}$ an upper bound to the\ncondition number $\\kappa$ of those systems of equations. This represents a\nquantum speed-up in the number $n$ of variables in the cost function with\nrespect to the comparable classical Interior Point algorithms when the initial\nmatrix of the problem $A$ is dense: if we substitute the quantum part of the\nalgorithm by classical algorithms such as Conjugate Gradient Descent, that\nwould mean the whole algorithm has complexity $O(L\\sqrt{n}(n+m)^2\\bar{\\kappa}\n\\log(\\epsilon^{-1}))$, or with exact methods, at least\n$O(L\\sqrt{n}(n+m)^{2.373})$. Also, in contrast with any Quantum Linear System\nAlgorithm, the algorithm described in this article outputs a classical\ndescription of the solution vector, and the value of the optimal solution.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 19:00:01 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 14:19:07 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 16:59:44 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 14:42:49 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 10:14:22 GMT"}, {"version": "v6", "created": "Mon, 6 Jul 2020 11:55:48 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Casares", "P. A. M.", ""], ["Martin-Delgado", "M. A.", ""]]}, {"id": "1902.06796", "submitter": "Roman Plotnikov", "authors": "Roman Plotnikov and Adil Erzin", "title": "Constructive Heuristics for Min-Power Bounded-Hops Symmetric\n  Connectivity Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Min-Power Bounded-Hops Symmetric Connectivity problem that\nconsists in the construction of communication spanning tree on a given graph,\nwhere the total energy consumption spent for the data transmission is minimized\nand the maximum number of hops between two nodes is bounded by some predefined\nconstant. We focus on the planar Euclidian case of this problem where the nodes\nare placed at the random uniformly spread points on a square and the power cost\nnecessary for the communication between two network elements is proportional to\nthe squared distance between them. Since this is an NP-hard problem, we propose\ndifferent polynomial heuristic algorithms for the approximation solution to\nthis problem. We perform a posteriori comparative analysis of the proposed\nalgorithms and present the obtained results in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 08:26:59 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Plotnikov", "Roman", ""], ["Erzin", "Adil", ""]]}, {"id": "1902.06808", "submitter": "Samuel Gutekunst", "authors": "Samuel C. Gutekunst and David P. Williamson", "title": "Characterizing the Integrality Gap of the Subtour LP for the Circulant\n  Traveling Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the integrality gap of the subtour LP relaxation of the Traveling\nSalesman Problem restricted to circulant instances. De Klerk and Dobre\nconjectured that the value of the optimal solution to the subtour LP on these\ninstances is equal to an entirely combinatorial lower bound from Van der Veen,\nVan Dal, and Sierksma. We prove this conjecture by giving an explicit optimal\nsolution to the subtour LP. We then use it to show that the integrality gap of\nthe subtour LP is 2 on circulant instances, making such instances one of the\nfew non-trivial classes of TSP instances for which the integrality gap of the\nsubtour LP is exactly known. We also show that the degree constraints do not\nstrengthen the subtour LP on circulant instances, mimicking the parsimonious\nproperty of metric, symmetric TSP instances shown in Goemans and Bertsimas in a\ndistinctly non-metric set of instances.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 21:49:03 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 17:43:24 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 15:17:26 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gutekunst", "Samuel C.", ""], ["Williamson", "David P.", ""]]}, {"id": "1902.06812", "submitter": "Anisse Ismaili", "authors": "Anisse Ismaili", "title": "The Complexity of Max-Min $k$-Partitioning", "comments": "Personal part of a submission to AAMAS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a max-min $k$-partition problem on a weighted graph,\nthat could model a robust $k$-coalition formation. We settle the computational\ncomplexity of this problem as complete for class $\\Sigma_2^P$. This hardness\nholds even for $k=2$ and arbitrary weights, or $k=3$ and non-negative weights,\nwhich matches what was known on \\textsc{MaxCut} and \\textsc{Min-3-Cut} one\nlevel higher in the polynomial hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 12:45:43 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Ismaili", "Anisse", ""]]}, {"id": "1902.06864", "submitter": "Lech Duraj", "authors": "Lech Duraj", "title": "A sub-quadratic algorithm for the longest common increasing subsequence\n  problem", "comments": "21 pages; STACS 2020 version -- heavily corrected from the previous\n  one, might actually be readable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Longest Common Increasing Subsequence problem (LCIS) is a natural variant\nof the celebrated Longest Common Subsequence (LCS) problem. For LCIS, as well\nas for LCS, there is an $O(n^2)$-time algorithm and a SETH-based conditional\nlower bound of $O(n^{2-\\varepsilon})$. For LCS, there is also the\nMasek-Paterson $O(n^2 / \\log{n})$-time algorithm, which does not seem to adapt\nto LCIS in any obvious way. Hence, a natural question arises: does any\n(slightly) sub-quadratic algorithm exist for the Longest Common Increasing\nSubsequence problem? We answer this question positively, presenting a $O(n^2 /\n\\log^a{n})$-time algorithm for $a = \\frac{1}{6}-o(1)$. The algorithm is not\nbased on memorizing small chunks of data (often used for logarithmic speedups,\nincluding the \"Four Russians Trick\" in LCS), but rather utilizes a new\ntechnique, bounding the number of significant symbol matches between the two\nsequences.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 02:32:32 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 13:10:34 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 19:20:08 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Duraj", "Lech", ""]]}, {"id": "1902.06957", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, Daniel Lokshtanov, Saket Saurabh,\n  Meirav Zehavi", "title": "Covering Vectors by Spaces in Perturbed Graphic Matroids and Their Duals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perturbed graphic matroids are binary matroids that can be obtained from a\ngraphic matroid by adding a noise of small rank. More precisely, r-rank\nperturbed graphic matroid M is a binary matroid that can be represented in the\nform I +P, where I is the incidence matrix of some graph and P is a binary\nmatrix of rank at most r. Such matroids naturally appear in a number of\ntheoretical and applied settings. The main motivation behind our work is an\nattempt to understand which parameterized algorithms for various problems on\ngraphs could be lifted to perturbed graphic matroids.\n  We study the parameterized complexity of a natural generalization (for\nmatroids) of the following fundamental problems on graphs: Steiner Tree and\nMultiway Cut. In this generalization, called the Space Cover problem, we are\ngiven a binary matroid M with a ground set E, a set of terminals T\\subseteq E,\nand a non-negative integer k. The task is to decide whether T can be spanned by\na subset of E\\setminus T of size at most k.\n  We prove that on graphic matroid perturbations, for every fixed r, Space\nCover is fixed-parameter tractable parameterized by k. On the other hand, the\nproblem becomes W[1]-hard when parameterized by r+k+|T| and it is NP-complete\nfor r\\leq 2 and |T|\\leq 2.\n  On cographic matroids, that are the duals of graphic matroids, Space Cover\ngeneralizes another fundamental and well-studied problem, namely Multiway Cut.\nWe show that on the duals of perturbed graphic matroids the Space Cover problem\nis fixed-parameter tractable parameterized by r+k.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 09:15:13 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1902.07004", "submitter": "Oscar Defrain", "authors": "Oscar Defrain, Lhouari Nourine and Takeaki Uno", "title": "On the dualization in distributive lattices and related problems", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the dualization in distributive lattices, a\ngeneralization of the well-known hypergraph dualization problem. We in\nparticular propose equivalent formulations of the problem in terms of graphs,\nhypergraphs, and posets. It is known that hypergraph dualization amounts to\ngenerate all minimal transversals of a hypergraph, or all minimal dominating\nsets of a graph. In this new framework, a poset on vertices is given together\nwith the input (hyper)graph, and minimal ``ideal solutions'' are to be\ngenerated. This in particular allows us to study the complexity of the problem\nunder various combined restrictions on graph classes and poset types, including\nbipartite, split, and co-bipartite graphs, and variants of neighborhood\ninclusion posets. We for example show that while the enumeration of minimal\ndominating sets is possible with linear delay in split graphs, the problem,\nwithin the same class, gets as hard as for general graphs when generalized to\nthis framework. More surprisingly, this result holds even when the poset is\nonly comparing vertices of included neighborhoods in the graph. If both the\nposet and the graph class are sufficiently restricted, we show that the\ndualization is tractable relying on existing algorithms from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 11:38:15 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 13:33:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Defrain", "Oscar", ""], ["Nourine", "Lhouari", ""], ["Uno", "Takeaki", ""]]}, {"id": "1902.07040", "submitter": "Andreas Emil Feldmann", "authors": "Yann Disser, Andreas Emil Feldmann, Max Klimm, and Jochen Konemann", "title": "Travelling on Graphs with Small Highway Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Travelling Salesperson (TSP) and the Steiner Tree problem (STP)\nin graphs of low highway dimension. This graph parameter was introduced by\nAbraham et al. [SODA 2010] as a model for transportation networks, on which TSP\nand STP naturally occur for various applications in logistics. It was\npreviously shown [Feldmann et al. ICALP 2015] that these problems admit a\nquasi-polynomial time approximation scheme (QPTAS) on graphs of constant\nhighway dimension. We demonstrate that a significant improvement is possible in\nthe special case when the highway dimension is 1, for which we present a\nfully-polynomial time approximation scheme (FPTAS). We also prove that STP is\nweakly NP-hard for these restricted graphs. For TSP we show NP-hardness for\ngraphs of highway dimension 6, which answers an open problem posed in [Feldmann\net al. ICALP 2015].\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 13:23:18 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 15:36:23 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 13:18:35 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Disser", "Yann", ""], ["Feldmann", "Andreas Emil", ""], ["Klimm", "Max", ""], ["Konemann", "Jochen", ""]]}, {"id": "1902.07055", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Adrian Kosowski, Przemys{\\l}aw Uzna\\'nski, Laurent Viennot", "title": "Hardness of Exact Distance Queries in Sparse Graphs Through Hub Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distance labeling scheme is an assignment of bit-labels to the vertices of\nan undirected, unweighted graph such that the distance between any pair of\nvertices can be decoded solely from their labels. An important class of\ndistance labeling schemes is that of hub labelings, where a node $v \\in G$\nstores its distance to the so-called hubs $S_v \\subseteq V$, chosen so that for\nany $u,v \\in V$ there is $w \\in S_u \\cap S_v$ belonging to some shortest $uv$\npath. Notice that for most existing graph classes, the best distance labelling\nconstructions existing use at some point a hub labeling scheme at least as a\nkey building block. Our interest lies in hub labelings of sparse graphs, i.e.,\nthose with $|E(G)| = O(n)$, for which we show a lowerbound of\n$\\frac{n}{2^{O(\\sqrt{\\log n})}}$ for the average size of the hubsets.\nAdditionally, we show a hub-labeling construction for sparse graphs of average\nsize $O(\\frac{n}{RS(n)^{c}})$ for some $0 < c < 1$, where $RS(n)$ is the\nso-called Ruzsa-Szemer{\\'e}di function, linked to structure of induced\nmatchings in dense graphs. This implies that further improving the lower bound\non hub labeling size to $\\frac{n}{2^{(\\log n)^{o(1)}}}$ would require a\nbreakthrough in the study of lower bounds on $RS(n)$, which have resisted\nsubstantial improvement in the last 70 years. For general distance labeling of\nsparse graphs, we show a lowerbound of $\\frac{1}{2^{O(\\sqrt{\\log n})}}\nSumIndex(n)$, where $SumIndex(n)$ is the communication complexity of the\nSum-Index problem over $Z_n$. Our results suggest that the best achievable\nhub-label size and distance-label size in sparse graphs may be\n$\\Theta(\\frac{n}{2^{(\\log n)^c}})$ for some $0<c < 1$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 13:57:29 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 20:16:04 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 18:28:46 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""], ["Viennot", "Laurent", ""]]}, {"id": "1902.07324", "submitter": "Dmitriy Kunisky", "authors": "Afonso S. Bandeira, Dmitriy Kunisky, Alexander S. Wein", "title": "Computational Hardness of Certifying Bounds on Constrained PCA Problems", "comments": "Submitted version (minor text revisions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random $n \\times n$ symmetric matrix $\\boldsymbol W$ drawn from the\nGaussian orthogonal ensemble (GOE), we consider the problem of certifying an\nupper bound on the maximum value of the quadratic form $\\boldsymbol x^\\top\n\\boldsymbol W \\boldsymbol x$ over all vectors $\\boldsymbol x$ in a constraint\nset $\\mathcal{S} \\subset \\mathbb{R}^n$. For a certain class of normalized\nconstraint sets $\\mathcal{S}$ we show that, conditional on certain\ncomplexity-theoretic assumptions, there is no polynomial-time algorithm\ncertifying a better upper bound than the largest eigenvalue of $\\boldsymbol W$.\nA notable special case included in our results is the hypercube $\\mathcal{S} =\n\\{ \\pm 1 / \\sqrt{n}\\}^n$, which corresponds to the problem of certifying bounds\non the Hamiltonian of the Sherrington-Kirkpatrick spin glass model from\nstatistical physics.\n  Our proof proceeds in two steps. First, we give a reduction from the\ndetection problem in the negatively-spiked Wishart model to the above\ncertification problem. We then give evidence that this Wishart detection\nproblem is computationally hard below the classical spectral threshold, by\nshowing that no low-degree polynomial can (in expectation) distinguish the\nspiked and unspiked models. This method for identifying computational\nthresholds was proposed in a sequence of recent works on the sum-of-squares\nhierarchy, and is believed to be correct for a large class of problems. Our\nproof can be seen as constructing a distribution over symmetric matrices that\nappears computationally indistinguishable from the GOE, yet is supported on\nmatrices whose maximum quadratic form over $\\boldsymbol x \\in \\mathcal{S}$ is\nmuch larger than that of a GOE matrix.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 22:18:46 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 04:46:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1902.07353", "submitter": "Ethan Madison", "authors": "Ethan Madison and Zachary Zipper", "title": "In oder Aus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bloom filters are data structures used to determine set membership of\nelements, with applications from string matching to networking and security\nproblems. These structures are favored because of their reduced memory\nconsumption and fast wallclock and asymptotic time bounds. Generally, Bloom\nfilters maintain constant membership query time, making them very fast in their\nniche. However, they are limited in their lack of a removal operation, as well\nas by their probabilistic nature. In this paper, we discuss various iterations\nof and alternatives to the generic Bloom filter that have been researched and\nimplemented to overcome their inherent limitations. Bloom filters, especially\nwhen used in conjunction with other data structures, are still powerful and\nefficient data structures; we further discuss their use in industy and research\nto optimize resource utilization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 23:58:11 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Madison", "Ethan", ""], ["Zipper", "Zachary", ""]]}, {"id": "1902.07354", "submitter": "Abdolhamid Ghodselahi", "authors": "Abdolhamid Ghodselahi, Fabian Kuhn, Volker Turau", "title": "Concurrent Distributed Serving with Mobile Servers", "comments": "34 pages, conference version in ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new resource allocation problem in distributed\ncomputing called distributed serving with mobile servers (DSMS). In DSMS, there\nare $k$ identical mobile servers residing at the processors of a network. At\narbitrary points of time, any subset of processors can invoke one or more\nrequests. To serve a request, one of the servers must move to the processor\nthat invoked the request. Resource allocation is performed in a distributed\nmanner since only the processor that invoked the request initially knows about\nit. All processors cooperate by passing messages to achieve correct resource\nallocation. They do this with the goal to minimize the communication cost.\n  Routing servers in large-scale distributed systems requires a scalable\nlocation service. We introduce the distributed protocol GNN that solves the\nDSMS problem on overlay trees. We prove that GNN is starvation-free and\ncorrectly integrates locating the servers and synchronizing the concurrent\naccess to servers despite asynchrony, even when the requests are invoked over\ntime. Further, we analyze GNN for \"one-shot\" executions, i.e., all requests are\ninvoked simultaneously. We prove that when running GNN on top of a special\nfamily of tree topologies---known as hierarchically well-separated trees\n(HSTs)---we obtain a randomized distributed protocol with an expected\ncompetitive ratio of $O(\\log n)$ on general network topologies with $n$\nprocessors. From a technical point of view, our main result is that GNN\noptimally solves the DSMS problem on HSTs for one-shot executions, even if\ncommunication is asynchronous. Further, we present a lower bound of\n$\\Omega(\\max\\{k, \\log n/\\log\\log n\\})$ on the competitive ratio for DSMS. The\nlower bound even holds when communication is synchronous and requests are\ninvoked sequentially.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 00:10:02 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 22:59:53 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ghodselahi", "Abdolhamid", ""], ["Kuhn", "Fabian", ""], ["Turau", "Volker", ""]]}, {"id": "1902.07554", "submitter": "Daniel Funke", "authors": "Daniel Funke and Peter Sanders and Vincent Winkler", "title": "Load-Balancing for Parallel Delaunay Triangulations", "comments": "Short version submitted to EuroPar 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the Delaunay triangulation (DT) of a given point set in\n$\\mathbb{R}^D$ is one of the fundamental operations in computational geometry.\nRecently, Funke and Sanders (2017) presented a divide-and-conquer DT algorithm\nthat merges two partial triangulations by re-triangulating a small subset of\ntheir vertices - the border vertices - and combining the three triangulations\nefficiently via parallel hash table lookups. The input point division should\ntherefore yield roughly equal-sized partitions for good load-balancing and also\nresult in a small number of border vertices for fast merging. In this paper, we\npresent a novel divide-step based on partitioning the triangulation of a small\nsample of the input points. In experiments on synthetic and real-world data\nsets, we achieve nearly perfectly balanced partitions and small border\ntriangulations. This almost cuts running time in half compared to\nnon-data-sensitive division schemes on inputs exhibiting an exploitable\nunderlying structure.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 13:47:57 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Funke", "Daniel", ""], ["Sanders", "Peter", ""], ["Winkler", "Vincent", ""]]}, {"id": "1902.07568", "submitter": "Petr Kolman", "authors": "Kate\\v{r}ina Altmanov\\'a, Petr Kolman, Jan Voborn\\'ik", "title": "On Polynomial-Time Combinatorial Algorithms for Maximum $L$-Bounded Flow", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ with two distinguished vertices $s,t\\in V$ and an\ninteger $L$, an {\\em $L$-bounded flow} is a flow between $s$ and $t$ that can\nbe decomposed into paths of length at most $L$. In the {\\em maximum $L$-bounded\nflow problem} the task is to find a maximum $L$-bounded flow between a given\npair of vertices in the input graph.\n  The problem can be solved in polynomial time using linear programming.\nHowever, as far as we know, no polynomial-time combinatorial algorithm for the\n$L$-bounded flow is known. The only attempt, that we are aware of, to describe\na combinatorial algorithm for the maximum $L$-bounded flow problem was done by\nKoubek and \\v{R}\\'i ha in 1981. Unfortunately, their paper contains\nsubstantional flaws and the algorithm does not work; in the first part of this\npaper, we describe these problems.\n  In the second part of this paper we describe a combinatorial algorithm based\non the exponential length method that finds a $(1+\\epsilon)$-approximation of\nthe maximum $L$-bounded flow in time $O(\\epsilon^{-2}m^2 L\\log L)$ where $m$ is\nthe number of edges in the graph. Moreover, we show that this approach works\neven for the NP-hard generalization of the maximum $L$-bounded flow problem in\nwhich each edge has a length.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 14:21:01 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Altmanov\u00e1", "Kate\u0159ina", ""], ["Kolman", "Petr", ""], ["Voborn\u00edk", "Jan", ""]]}, {"id": "1902.07599", "submitter": "Dustin Cobas", "authors": "Dustin Cobas and Gonzalo Navarro", "title": "Fast, Small, and Simple Document Listing on Repetitive Text Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document listing on string collections is the task of finding all documents\nwhere a pattern appears. It is regarded as the most fundamental document\nretrieval problem, and is useful in various applications. Many of the\nfastest-growing string collections are composed of very similar documents, such\nas versioned code and document collections, genome repositories, etc. Plain\npattern-matching indexes designed for repetitive text collections achieve\norders-of-magnitude reductions in space. Instead, there are not many analogous\nindexes for document retrieval. In this paper we present a simple document\nlisting index for repetitive string collections of total length $n$ that lists\nthe $ndoc$ distinct documents where a pattern of length $m$ appears in time\n$\\mathcal{O}(m+ndoc \\cdot \\log n)$. We exploit the repetitiveness of the\ndocument array (i.e., the suffix array coarsened to document identifiers) to\ngrammar-compress it while precomputing the answers to nonterminals, and store\nthem in grammar-compressed form as well. Our experimental results show that our\nindex sharply outperforms existing alternatives in the space/time tradeoff map.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:34:06 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Cobas", "Dustin", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1902.07660", "submitter": "Malte Skambath", "authors": "Max Bannach, Malte Skambath, Till Tantau", "title": "Towards Work-Efficient Parallel Parameterized Algorithms", "comments": "Prior full version of the paper that will appear in Proceedings of\n  the 13th International Conference and Workshops on Algorithms and Computation\n  (WALCOM 2019), February 27 - March 02, 2019, Guwahati, India. The final\n  authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-10564-8_27", "journal-ref": null, "doi": "10.1007/978-3-030-10564-8_27", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel parameterized complexity theory studies how fixed-parameter\ntractable (fpt) problems can be solved in parallel. Previous theoretical work\nfocused on parallel algorithms that are very fast in principle, but did not\ntake into account that when we only have a small number of processors (between\n2 and, say, 1024), it is more important that the parallel algorithms are\nwork-efficient. In the present paper we investigate how work-efficient fpt\nalgorithms can be designed. We review standard methods from fpt theory, like\nkernelization, search trees, and interleaving, and prove trade-offs for them\nbetween work efficiency and runtime improvements. This results in a toolbox for\ndeveloping work-efficient parallel fpt algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 17:16:39 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Bannach", "Max", ""], ["Skambath", "Malte", ""], ["Tantau", "Till", ""]]}, {"id": "1902.07785", "submitter": "Ashish Dwivedi", "authors": "Ashish Dwivedi, Rajat Mittal, Nitin Saxena", "title": "Counting basic-irreducible factors mod $p^k$ in deterministic poly-time\n  and $p$-adic applications", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an irreducible factor, of a polynomial $f(x)$ modulo a prime $p$, is\nnot known to be in deterministic polynomial time. Though there is such a\nclassical algorithm that {\\em counts} the number of irreducible factors of\n$f\\bmod p$. We can ask the same question modulo prime-powers $p^k$. The\nirreducible factors of $f\\bmod p^k$ blow up exponentially in number; making it\nhard to describe them. Can we count those irreducible factors $\\bmod~p^k$ that\nremain irreducible mod $p$? These are called {\\em basic-irreducible}. A simple\nexample is in $f=x^2+px \\bmod p^2$; it has $p$ many basic-irreducible factors.\nAlso note that, $x^2+p \\bmod p^2$ is irreducible but not basic-irreducible!\n  We give an algorithm to count the number of basic-irreducible factors of\n$f\\bmod p^k$ in deterministic poly(deg$(f),k\\log p$)-time. This solves the open\nquestions posed in (Cheng et al, ANTS'18 \\& Kopp et al, Math.Comp.'19). In\nparticular, we are counting roots $\\bmod\\ p^k$; which gives the first\ndeterministic poly-time algorithm to compute Igusa zeta function of $f$. Also,\nour algorithm efficiently partitions the set of all basic-irreducible factors\n(possibly exponential) into merely deg$(f)$-many disjoint sets, using a compact\ntree data structure and {\\em split} ideals.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 21:33:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Dwivedi", "Ashish", ""], ["Mittal", "Rajat", ""], ["Saxena", "Nitin", ""]]}, {"id": "1902.07812", "submitter": "Therese Biedl", "authors": "Therese Biedl", "title": "Finding big matchings in planar graphs quickly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that every $n$-vertex planar graph with minimum degree 3 has\na matching of size at least $\\frac{n}{3}$. But all proofs of this use the\nTutte-Berge-formula for the size of a maximum matching. Hence these proofs are\nnot directly algorithmic, and to find such a matching one must apply a\ngeneral-purposes maximum matching algorithm, which has run-time\n$O(n^{1.5}\\alpha(n))$ for planar graphs. In contrast to this, this paper gives\na linear-time algorithm that finds a matching of size at least $\\frac{n}{3}$ in\nany planar graph with minimum degree 3.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 23:39:40 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Biedl", "Therese", ""]]}, {"id": "1902.07823", "submitter": "Huang Lingxiao", "authors": "Lingxiao Huang and Nisheeth K. Vishnoi", "title": "Stable and Fair Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fair classification has been a topic of intense study in machine learning,\nand several algorithms have been proposed towards this important task. However,\nin a recent study, Friedler et al. observed that fair classification algorithms\nmay not be stable with respect to variations in the training dataset -- a\ncrucial consideration in several real-world applications. Motivated by their\nwork, we study the problem of designing classification algorithms that are both\nfair and stable. We propose an extended framework based on fair classification\nalgorithms that are formulated as optimization problems, by introducing a\nstability-focused regularization term. Theoretically, we prove a stability\nguarantee, that was lacking in fair classification algorithms, and also provide\nan accuracy guarantee for our extended framework. Our accuracy guarantee can be\nused to inform the selection of the regularization parameter in our framework.\nTo the best of our knowledge, this is the first work that combines stability\nand fairness in automated decision-making tasks. We assess the benefits of our\napproach empirically by extending several fair classification algorithms that\nare shown to achieve the best balance between fairness and accuracy over the\nAdult dataset. Our empirical results show that our framework indeed improves\nthe stability at only a slight sacrifice in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 00:56:14 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 14:15:32 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 16:22:18 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 12:32:22 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Huang", "Lingxiao", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1902.07856", "submitter": "Haotian Jiang", "authors": "Anupam Gupta, Haotian Jiang, Ziv Scully, and Sahil Singla", "title": "The Markovian Price of Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose there are $n$ Markov chains and we need to pay a per-step\n\\emph{price} to advance them. The \"destination\" states of the Markov chains\ncontain rewards; however, we can only get rewards for a subset of them that\nsatisfy a combinatorial constraint, e.g., at most $k$ of them, or they are\nacyclic in an underlying graph. What strategy should we choose to advance the\nMarkov chains if our goal is to maximize the total reward \\emph{minus} the\ntotal price that we pay?\n  In this paper we introduce a Markovian price of information model to capture\nsettings such as the above, where the input parameters of a combinatorial\noptimization problem are given via Markov chains. We design\noptimal/approximation algorithms that jointly optimize the value of the\ncombinatorial problem and the total paid price. We also study \\emph{robustness}\nof our algorithms to the distribution parameters and how to handle the\n\\emph{commitment} constraint.\n  Our work brings together two classical lines of investigation: getting\noptimal strategies for Markovian multi-armed bandits, and getting exact and\napproximation algorithms for discrete optimization problems using combinatorial\nas well as linear-programming relaxation ideas.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 03:40:09 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Gupta", "Anupam", ""], ["Jiang", "Haotian", ""], ["Scully", "Ziv", ""], ["Singla", "Sahil", ""]]}, {"id": "1902.07928", "submitter": "Ben Karsin", "authors": "John Iacono, Varunkumar Jayapaul, Ben Karsin", "title": "Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of modern computation is characterized by locality of\nreference, that is, it is faster to access data that is close in address space\nto data that been accessed recently than a random piece of data. This is due to\nmany architectural features including caches, lookahead, address translation\nand the physical properties of a hard disk drive; attempting to model all the\ncomponents that constitute the performance of a modern machine is impossible,\nespecially for general algorithm design purposes. What if one could prove an\nalgorithm is asymptotically optimal on all systems that reward locality of\nreference, no matter how it manifests itself within reasonable limits? We show\nthat this is possible, and that excluding some pathological cases, algorithms\nthat are asymptotically optimal in the cache-oblivious model are asymptotically\noptimal in any reasonable locality-of-reference rewarding setting. This is\nsurprising as the cache-oblivious model envisions a particular architectural\nmodel involving blocked memory transfer into a multi-level hierarchy of caches\nof varying sizes, and was not designed to directly model locality-of-reference\ncorrelated performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 09:21:57 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 09:39:03 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Iacono", "John", ""], ["Jayapaul", "Varunkumar", ""], ["Karsin", "Ben", ""]]}, {"id": "1902.08042", "submitter": "Will Rosenbaum", "authors": "Johannes Bund, Christoph Lenzen, Will Rosenbaum", "title": "Fault Tolerant Gradient Clock Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronizing clocks in distributed systems is well-understood, both in terms\nof fault-tolerance in fully connected systems and the dependence of local and\nglobal worst-case skews (i.e., maximum clock difference between neighbors and\narbitrary pairs of nodes, respectively) on the diameter of fault-free systems.\nHowever, so far nothing non-trivial is known about the local skew that can be\nachieved in topologies that are not fully connected even under a single\nByzantine fault. Put simply, in this work we show that the most powerful known\ntechniques for fault-tolerant and gradient clock synchronization are\ncompatible, in the sense that the best of both worlds can be achieved\nsimultaneously.\n  Concretely, we combine the Lynch-Welch algorithm [Welch1988] for\nsynchronizing a clique of $n$ nodes despite up to $f<n/3$ Byzantine faults with\nthe gradient clock synchronization (GCS) algorithm by Lenzen et al.\n[Lenzen2010] in order to render the latter resilient to faults. As this is not\npossible on general graphs, we augment an input graph $\\mathcal{G}$ by\nreplacing each node by $3f+1$ fully connected copies, which execute an instance\nof the Lynch-Welch algorithm. We then interpret these clusters as supernodes\nexecuting the GCS algorithm, where for each cluster its correct nodes'\nLynch-Welch clocks provide estimates of the logical clock of the supernode in\nthe GCS algorithm. By connecting clusters corresponding to neighbors in\n$\\mathcal{G}$ in a fully bipartite manner, supernodes can inform each other\nabout (estimates of) their logical clock values. This way, we achieve\nasymptotically optimal local skew, granted that no cluster contains more than\n$f$ faulty nodes, at factor $O(f)$ and $O(f^2)$ overheads in terms of nodes and\nedges, respectively. Note that tolerating $f$ faulty neighbors trivially\nrequires degree larger than $f$, so this is asymptotically optimal as well.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:35:08 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Bund", "Johannes", ""], ["Lenzen", "Christoph", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1902.08069", "submitter": "Will Rosenbaum", "authors": "Avery Miller, Boaz Patt-Shamir, Will Rosenbaum", "title": "With Great Speed Come Small Buffers: Space-Bandwidth Tradeoffs for\n  Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Adversarial Queuing Theory (AQT) model, where packet arrivals\nare subject to a maximum average rate $0\\le\\rho\\le1$ and burstiness\n$\\sigma\\ge0$. In this model, we analyze the size of buffers required to avoid\noverflows in the basic case of a path. Our main results characterize the space\nrequired by the average rate and the number of distinct destinations: we show\nthat $O(k d^{1/k})$ space suffice, where $d$ is the number of distinct\ndestinations and $k=\\lfloor 1/\\rho \\rfloor$; and we show that $\\Omega(\\frac 1 k\nd^{1/k})$ space is necessary. For directed trees, we describe an algorithm\nwhose buffer space requirement is at most $1 + d' + \\sigma$ where $d'$ is the\nmaximum number of destinations on any root-leaf path.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 14:31:25 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Miller", "Avery", ""], ["Patt-Shamir", "Boaz", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1902.08086", "submitter": "Will Rosenbaum", "authors": "Talya Eden, Dana Ron, Will Rosenbaum", "title": "The Arboricity Captures the Complexity of Sampling Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the problem of sampling edges in an unknown graph\n$G = (V, E)$ from a distribution that is (pointwise) almost uniform over $E$.\nWe consider the case where there is some a priori upper bound on the arboriciy\nof $G$. Given query access to a graph $G$ over $n$ vertices and of average\ndegree $d$ and arboricity at most $\\alpha$, we design an algorithm that\nperforms $O\\!\\left(\\frac{\\alpha}{d} \\cdot \\frac{\\log^3 n}{\\varepsilon}\\right)$\nqueries in expectation and returns an edge in the graph such that every edge $e\n\\in E$ is sampled with probability $(1 \\pm \\varepsilon)/m$. The algorithm\nperforms two types of queries: degree queries and neighbor queries. We show\nthat the upper bound is tight (up to poly-logarithmic factors and the\ndependence in $\\varepsilon$), as $\\Omega\\!\\left(\\frac{\\alpha}{d} \\right)$\nqueries are necessary for the easier task of sampling edges from any\ndistribution over $E$ that is close to uniform in total variational distance.\nWe also prove that even if $G$ is a tree (i.e., $\\alpha = 1$ so that\n$\\frac{\\alpha}{d}=\\Theta(1)$), $\\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)$\nqueries are necessary to sample an edge from any distribution that is pointwise\nclose to uniform, thus establishing that a $\\mathrm{poly}(\\log n)$ factor is\nnecessary for constant $\\alpha$. Finally we show how our algorithm can be\napplied to obtain a new result on approximately counting subgraphs, based on\nthe recent work of Assadi, Kapralov, and Khanna (ITCS, 2019).\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 15:03:44 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Eden", "Talya", ""], ["Ron", "Dana", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1902.08091", "submitter": "Ross Duncan", "authors": "Alexander Cowtan, Silas Dilkes, Ross Duncan, Alexandre Krajenbrink,\n  Will Simmons, Seyon Sivarajah", "title": "On the qubit routing problem", "comments": "v2: wrong chart replaced with correct version; minor edits for\n  clarity", "journal-ref": null, "doi": "10.4230/LIPIcs.TQC.2019.5", "report-no": null, "categories": "quant-ph cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new architecture-agnostic methodology for mapping abstract\nquantum circuits to realistic quantum computing devices with restricted qubit\nconnectivity, as implemented by Cambridge Quantum Computing's tket compiler. We\npresent empirical results showing the effectiveness of this method in terms of\nreducing two-qubit gate depth and two-qubit gate count, compared to other\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 15:09:20 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 19:06:27 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Cowtan", "Alexander", ""], ["Dilkes", "Silas", ""], ["Duncan", "Ross", ""], ["Krajenbrink", "Alexandre", ""], ["Simmons", "Will", ""], ["Sivarajah", "Seyon", ""]]}, {"id": "1902.08179", "submitter": "Holden Lee", "authors": "Holden Lee, Oren Mangoubi, Nisheeth K. Vishnoi", "title": "Online Sampling from Log-Concave Distributions", "comments": "42 pages", "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence of convex functions $f_0, f_1, \\ldots, f_T$, we study the\nproblem of sampling from the Gibbs distribution $\\pi_t \\propto\ne^{-\\sum_{k=0}^tf_k}$ for each epoch $t$ in an online manner. Interest in this\nproblem derives from applications in machine learning, Bayesian statistics, and\noptimization where, rather than obtaining all the observations at once, one\nconstantly acquires new data, and must continuously update the distribution.\nOur main result is an algorithm that generates roughly independent samples from\n$\\pi_t$ for every epoch $t$ and, under mild assumptions, makes\n$\\mathrm{polylog}(T)$ gradient evaluations per epoch. All previous results\nimply a bound on the number of gradient or function evaluations which is at\nleast linear in $T$. Motivated by real-world applications, we assume that\nfunctions are smooth, their associated distributions have a bounded second\nmoment, and their minimizer drifts in a bounded manner, but do not assume they\nare strongly convex. In particular, our assumptions hold for online Bayesian\nlogistic regression, when the data satisfy natural regularity properties,\ngiving a sampling algorithm with updates that are poly-logarithmic in $T$. In\nsimulations, our algorithm achieves accuracy comparable to an algorithm\nspecialized to logistic regression. Key to our algorithm is a novel stochastic\ngradient Langevin dynamics Markov chain with a carefully designed variance\nreduction step and constant batch size. Technically, lack of strong convexity\nis a significant barrier to analysis and, here, our main contribution is a\nmartingale exit time argument that shows our Markov chain remains in a ball of\nradius roughly poly-logarithmic in $T$ for enough time to reach within\n$\\varepsilon$ of $\\pi_t$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 18:42:14 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 01:25:58 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 15:50:02 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 23:52:58 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lee", "Holden", ""], ["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1902.08218", "submitter": "Lin Chen", "authors": "Lin Chen, Daniel Marx", "title": "Covering a tree with rooted subtrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multiple traveling salesman problem on a weighted tree. In\nthis problem there are $m$ salesmen located at the root initially. Each of them\nwill visit a subset of vertices and return to the root. The goal is to assign a\ntour to every salesman such that every vertex is visited and the longest tour\namong all salesmen is minimized. The problem is equivalent to the subtree cover\nproblem, in which we cover a tree with rooted subtrees such that the weight of\nthe maximum weighted subtree is minimized. The classical machine scheduling\nproblem can be viewed as a special case of our problem when the given tree is a\nstar. We observe that, the problem remains NP-hard even if tree height and edge\nweight are constant, and present an FPT algorithm for this problem\nparameterized by the largest tour length. To achieve the FPT algorithm, we show\na more general result. We prove that, integer linear programming that has a\ntree-fold structure is in FPT, which extends the FPT result for the $n$-fold\ninteger programming by Hemmecke, Onn and Romanchuk.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 19:09:46 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Chen", "Lin", ""], ["Marx", "Daniel", ""]]}, {"id": "1902.08266", "submitter": "Ali Vakilian", "authors": "Merav Parter, Ronitt Rubinfeld, Ali Vakilian, Anak Yodpinyanee", "title": "Local Computation Algorithms for Spanners", "comments": "An extended abstract appeared in the proceedings of ITCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph spanner is a fundamental graph structure that faithfully preserves\nthe pairwise distances in the input graph up to a small multiplicative stretch.\nThe common objective in the computation of spanners is to achieve the\nbest-known existential size-stretch trade-off efficiently.\n  Classical models and algorithmic analysis of graph spanners essentially\nassume that the algorithm can read the input graph, construct the desired\nspanner, and write the answer to the output tape. However, when considering\nmassive graphs containing millions or even billions of nodes not only the input\ngraph, but also the output spanner might be too large for a single processor to\nstore.\n  To tackle this challenge, we initiate the study of local computation\nalgorithms (LCAs) for graph spanners in general graphs, where the algorithm\nshould locally decide whether a given edge $(u,v) \\in E$ belongs to the output\nspanner. Such LCAs give the user the `illusion' that a specific sparse spanner\nfor the graph is maintained, without ever fully computing it. We present the\nfollowing results:\n  -For general $n$-vertex graphs and $r \\in \\{2,3\\}$, there exists an LCA for\n$(2r-1)$-spanners with $\\widetilde{O}(n^{1+1/r})$ edges and sublinear probe\ncomplexity of $\\widetilde{O}(n^{1-1/2r})$. These size/stretch tradeoffs are\nbest possible (up to polylogarithmic factors).\n  -For every $k \\geq 1$ and $n$-vertex graph with maximum degree $\\Delta$,\nthere exists an LCA for $O(k^2)$ spanners with $\\widetilde{O}(n^{1+1/k})$\nedges, probe complexity of $\\widetilde{O}(\\Delta^4 n^{2/3})$, and random seed\nof size $\\mathrm{polylog}(n)$. This improves upon, and extends the work of\n[Lenzen-Levi, 2018].\n  We also complement our results by providing a polynomial lower bound on the\nprobe complexity of LCAs for graph spanners that holds even for the simpler\ntask of computing a sparse connected subgraph with $o(m)$ edges.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 21:03:22 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Parter", "Merav", ""], ["Rubinfeld", "Ronitt", ""], ["Vakilian", "Ali", ""], ["Yodpinyanee", "Anak", ""]]}, {"id": "1902.08340", "submitter": "Noah Stephens-Davidowitz", "authors": "Noah Stephens-Davidowitz", "title": "A time-distance trade-off for GDD with preprocessing---Instantiating the\n  DLW heuristic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $0 \\leq \\alpha \\leq 1/2$, we show an algorithm that does the following.\nGiven appropriate preprocessing $P(\\mathcal{L})$ consisting of $N_\\alpha :=\n2^{O(n^{1-2\\alpha} + \\log n)}$ vectors in some lattice $\\mathcal{L} \\subset\n\\mathbb{R}^n$ and a target vector $\\boldsymbol{t}\\in \\mathbb{R}^n$, the\nalgorithm finds $\\boldsymbol{y} \\in \\mathcal{L}$ such that $\\|\\boldsymbol{y}-\n\\boldsymbol{t}\\| \\leq n^{1/2 + \\alpha} \\eta(\\mathcal{L})$ in time\n$\\mathrm{poly}(n) \\cdot N_\\alpha$, where $\\eta(\\mathcal{L})$ is the smoothing\nparameter of the lattice.\n  The algorithm itself is very simple and was originally studied by\nDoulgerakis, Laarhoven, and de Weger (to appear in PQCrypto, 2019), who proved\nits correctness under certain reasonable heuristic assumptions on the\npreprocessing $P(\\mathcal{L})$ and target $\\boldsymbol{t}$. Our primary\ncontribution is a choice of preprocessing that allows us to prove correctness\nwithout any heuristic assumptions.\n  Our main motivation for studying this is the recent breakthrough algorithm\nfor IdealSVP due to Hanrot, Pellet--Mary, and Stehl\\'e (to appear in Eurocrypt,\n2019), which uses the DLW algorithm as a key subprocedure. In particular, our\nresult implies that the HPS IdealSVP algorithm can be made to work with fewer\nheuristic assumptions.\n  Our only technical tool is the discrete Gaussian distribution over\n$\\mathcal{L}$, and in particular, a lemma showing that the one-dimensional\nprojections of this distribution behave very similarly to the continuous\nGaussian. This lemma might be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 02:29:17 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 02:28:30 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1902.08366", "submitter": "Farhad Shahrokhi", "authors": "Farhad Shahrokhi", "title": "A simple upper bound for trace function of a hypergraph with\n  applications", "comments": "One of lemmas 2.2 is wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${H}=(V, {E})$ be a hypergraph on the vertex set $V$ and edge set\n${E}\\subseteq 2^V$. We show that number of distinct {\\it traces} on any $k-$\nsubset of $V$, is most $k.{\\hat \\alpha}(H)$, where ${\\hat \\alpha}(H)$ is the\n{\\it degeneracy} of $H$. The result significantly improves/generalizes some of\nrelated results. For instance, the $vc$ dimension $H$ (or $vc(H)$) is shown to\nbe at most $\\log({\\hat \\alpha}(H))+1$ which was not known before. As a\nconsequence $vc(H)$ can be computed in computed in $n^{O( {\\rm log}({\\hat\n\\delta}(H)))}$ time. When applied to the neighborhood systems of a graphs\nexcluding a fixed minor, it reduces the known linear upper bound on the $VC$\ndimension to a logarithmic one, in the size of the minor. When applied to the\nlocation domination and identifying code numbers of any $n$ vertex graph $G$,\none gets the new lower bound of $\\Omega(n/({\\hat \\alpha}(G))$, where ${\\hat\n\\alpha}(G)$ is the degeneracy of $G$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 05:17:57 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 20:32:29 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Shahrokhi", "Farhad", ""]]}, {"id": "1902.08384", "submitter": "Aleksandar Nikolov", "authors": "Andrey Boris Khesin, Aleksandar Nikolov, Dmitry Paramonov", "title": "Preconditioning for the Geometric Transportation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the geometric transportation problem, we are given a collection of points\n$P$ in $d$-dimensional Euclidean space, and each point is given a supply of\n$\\mu(p)$ units of mass, where $\\mu(p)$ could be a positive or a negative\ninteger, and the total sum of the supplies is $0$. The goal is to find a flow\n(called a transportation map) that transports $\\mu(p)$ units from any point $p$\nwith $\\mu(p) > 0$, and transports $-\\mu(p)$ units into any point $p$ with\n$\\mu(p) < 0$. Moreover, the flow should minimize the total distance traveled by\nthe transported mass. The optimal value is known as the transportation cost, or\nthe Earth Mover's Distance (from the points with positive supply to those with\nnegative supply). This problem has been widely studied in many fields of\ncomputer science: from theoretical work in computational geometry, to\napplications in computer vision, graphics, and machine learning.\n  In this work we study approximation algorithms for the geometric\ntransportation problem. We give an algorithm which, for any fixed dimension\n$d$, finds a $(1+\\varepsilon)$-approximate transportation map in time\nnearly-linear in $n$, and polynomial in $\\varepsilon^{-1}$ and in the logarithm\nof the total supply. This is the first approximation scheme for the problem\nwhose running time depends on $n$ as $n\\cdot \\mathrm{polylog}(n)$. Our\ntechniques combine the generalized preconditioning framework of Sherman, which\nis grounded in continuous optimization, with simple geometric arguments to\nfirst reduce the problem to a minimum cost flow problem on a sparse graph, and\nthen to design a good preconditioner for this latter problem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 07:20:07 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Khesin", "Andrey Boris", ""], ["Nikolov", "Aleksandar", ""], ["Paramonov", "Dmitry", ""]]}, {"id": "1902.08452", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Nonconvex sampling with the Metropolis-adjusted Langevin algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Langevin Markov chain algorithms are widely deployed methods to sample\nfrom distributions in challenging high-dimensional and non-convex statistics\nand machine learning applications. Despite this, current bounds for the\nLangevin algorithms are slower than those of competing algorithms in many\nimportant situations, for instance when sampling from weakly log-concave\ndistributions, or when sampling or optimizing non-convex log-densities. In this\npaper, we obtain improved bounds in many of these situations, showing that the\nMetropolis-adjusted Langevin algorithm (MALA) is faster than the best bounds\nfor its competitor algorithms when the target distribution satisfies weak\nthird- and fourth- order regularity properties associated with the input data.\nIn many settings, our regularity conditions are weaker than the usual Euclidean\noperator norm regularity properties, allowing us to show faster bounds for a\nmuch larger class of distributions than would be possible with the usual\nEuclidean operator norm approach, including in statistics and machine learning\napplications where the data satisfy a certain incoherence condition. In\nparticular, we show that using our regularity conditions one can obtain faster\nbounds for applications which include sampling problems in Bayesian logistic\nregression with weakly convex priors, and the nonconvex optimization problem of\nlearning linear classifiers with zero-one loss functions.\n  Our main technical contribution in this paper is our analysis of the\nMetropolis acceptance probability of MALA in terms of its \"energy-conservation\nerror,\" and our bound for this error in terms of third- and fourth- order\nregularity conditions. Our combination of this higher-order analysis of the\nenergy conservation error with the conductance method is key to obtaining\nbounds which have a sub-linear dependence on the dimension $d$ in the\nnon-strongly logconcave setting.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 12:05:58 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 16:40:25 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1902.08559", "submitter": "Kirill Simonov", "authors": "Fedor V. Fomin, Petr A. Golovach, Kirill Simonov", "title": "Parameterized k-Clustering: The distance matters!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-Clustering problem, which is for a given multiset of $n$\nvectors $X\\subset \\mathbb{Z}^d$ and a nonnegative number $D$, to decide whether\n$X$ can be partitioned into $k$ clusters $C_1, \\dots, C_k$ such that the cost\n  \\[\\sum_{i=1}^k \\min_{c_i\\in \\mathbb{R}^d}\\sum_{x \\in C_i} \\|x-c_i\\|_p^p \\leq\nD,\\] where $\\|\\cdot\\|_p$ is the Minkowski ($L_p$) norm of order $p$. For $p=1$,\n$k$-Clustering is the well-known $k$-Median. For $p=2$, the case of the\nEuclidean distance, $k$-Clustering is $k$-Means. We show that the parameterized\ncomplexity of $k$-Clustering strongly depends on the distance order $p$. In\nparticular, we prove that for every $p\\in (0,1]$, $k$-Clustering is solvable in\ntime $2^{O(D \\log{D})} (nd)^{O(1)}$, and hence is fixed-parameter tractable\nwhen parameterized by $D$. On the other hand, we prove that for distances of\norders $p=0$ and $p=\\infty$, no such algorithm exists, unless FPT=W[1].\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 17:05:18 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Simonov", "Kirill", ""]]}, {"id": "1902.08679", "submitter": "Philip Milton", "authors": "Philip Milton, Emanuele Giorgi, Samir Bhatt", "title": "Spatial Analysis Made Easy with Linear Regression and Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are an incredibly popular technique for extending linear\nmodels to non-linear problems via a mapping to an implicit, high-dimensional\nfeature space. While kernel methods are computationally cheaper than an\nexplicit feature mapping, they are still subject to cubic cost on the number of\npoints. Given only a few thousand locations, this computational cost rapidly\noutstrips the currently available computational power. This paper aims to\nprovide an overview of kernel methods from first-principals (with a focus on\nridge regression), before progressing to a review of random Fourier features\n(RFF), a set of methods that enable the scaling of kernel methods to big\ndatasets. At each stage, the associated R code is provided. We begin by\nillustrating how the dual representation of ridge regression relies solely on\ninner products and permits the use of kernels to map the data into\nhigh-dimensional spaces. We progress to RFFs, showing how only a few lines of\ncode provides a significant computational speed-up for a negligible cost to\naccuracy. We provide an example of the implementation of RFFs on a simulated\nspatial data set to illustrate these properties. Lastly, we summarise the main\nissues with RFFs and highlight some of the advanced techniques aimed at\nalleviating them.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 21:39:29 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Milton", "Philip", ""], ["Giorgi", "Emanuele", ""], ["Bhatt", "Samir", ""]]}, {"id": "1902.08698", "submitter": "Manuel Torres", "authors": "Chandra Chekuri, Kent Quanrud, Manuel R. Torres", "title": "$\\ell_1$-sparsity Approximation Bounds for Packing Integer Programs", "comments": "To appear in IPCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximation algorithms for packing integer programs (PIPs) of\nthe form $\\max\\{\\langle c, x\\rangle : Ax \\le b, x \\in \\{0,1\\}^n\\}$ where $c$,\n$A$, and $b$ are nonnegative. We let $W = \\min_{i,j} b_i / A_{i,j}$ denote the\nwidth of $A$ which is at least $1$. Previous work by Bansal et al.\n\\cite{bansal-sparse} obtained an $\\Omega(\\frac{1}{\\Delta_0^{1/\\lfloor W\n\\rfloor}})$-approximation ratio where $\\Delta_0$ is the maximum number of\nnonzeroes in any column of $A$ (in other words the $\\ell_0$-column sparsity of\n$A$). They raised the question of obtaining approximation ratios based on the\n$\\ell_1$-column sparsity of $A$ (denoted by $\\Delta_1$) which can be much\nsmaller than $\\Delta_0$. Motivated by recent work on covering integer programs\n(CIPs) \\cite{cq,chs-16} we show that simple algorithms based on randomized\nrounding followed by alteration, similar to those of Bansal et al.\n\\cite{bansal-sparse} (but with a twist), yield approximation ratios for PIPs\nbased on $\\Delta_1$. First, following an integrality gap example from\n\\cite{bansal-sparse}, we observe that the case of $W=1$ is as hard as maximum\nindependent set even when $\\Delta_1 \\le 2$. In sharp contrast to this negative\nresult, as soon as width is strictly larger than one, we obtain positive\nresults via the natural LP relaxation. For PIPs with width $W = 1 + \\epsilon$\nwhere $\\epsilon \\in (0,1]$, we obtain an\n$\\Omega(\\epsilon^2/\\Delta_1)$-approximation. In the large width regime, when $W\n\\ge 2$, we obtain an $\\Omega((\\frac{1}{1 +\n\\Delta_1/W})^{1/(W-1)})$-approximation. We also obtain a\n$(1-\\epsilon)$-approximation when $W = \\Omega(\\frac{\\log\n(\\Delta_1/\\epsilon)}{\\epsilon^2})$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 23:11:58 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""], ["Torres", "Manuel R.", ""]]}, {"id": "1902.08723", "submitter": "Saket Saurabh", "authors": "Daniel Lokshtanov and Daniel Marx and Saket Saurabh", "title": "Slightly Superexponential Parameterized Problems", "comments": null, "journal-ref": "SIAM Journal on Computing, 2018", "doi": "10.1137/16M1104834", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in parameterized algorithms is to obtain algorithms\n  with running time $f(k)\\cdot n^{O(1)}$ such that $f$ is as slow growing\nfunction of the parameter $k$ as possible. In particular, a large number of\nbasic parameterized problems admit parameterized algorithms where $f(k)$ is\nsingle-exponential, that is, $c^k$ for some constant $c$, which makes aiming\nfor such a running time a natural goal for other problems as well. However\nthere are still plenty of problems where the $f(k)$ appearing in the best known\nrunning time is worse than single-exponential and it remained ``slightly\nsuperexponential'' even after serious attempts to bring it down. A natural\nquestion to ask is whether the $f(k)$ appearing in the running time of the\nbest-known algorithms is optimal for any of these problems.\n  In this paper, we examine parameterized problems where $f(k)$ is\n$k^{O(k)}=2^{O(k\\log k)}$ in the best known running time and for a number of\nsuch problems, we show that the dependence on $k$ in the running time cannot be\nimproved to single exponential. (See paper for the longer abstract.)\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 03:06:26 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Marx", "Daniel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1902.08742", "submitter": "Takanori Maehara", "authors": "Takanori Maehara, Kazutoshi Ando", "title": "Optimal Algorithm to Reconstruct a Tree from a Subtree Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding a representation of a subtree\ndistance, which is an extension of the tree metric. We show that a minimal\nrepresentation is uniquely determined by a given subtree distance, and give a\nlinear time algorithm that finds such a representation. This algorithm achieves\nthe optimal time complexity.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 06:04:30 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Maehara", "Takanori", ""], ["Ando", "Kazutoshi", ""]]}, {"id": "1902.08744", "submitter": "Martianus Frederic Ezerman", "authors": "Zuling Chang, Martianus Frederic Ezerman and Adamas Aqsa Fahreza", "title": "On Greedy Algorithms for Binary de Bruijn Sequences", "comments": null, "journal-ref": "Applicable Algebra in Engineering, Communication and Computing,\n  2021", "doi": "10.1007/s00200-020-00459-3", "report-no": null, "categories": "cs.IT cs.DS math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general greedy algorithm for binary de Bruijn sequences, called\nGeneralized Prefer-Opposite (GPO) Algorithm, and its modifications. By\nidentifying specific feedback functions and initial states, we demonstrate that\nmost previously-known greedy algorithms that generate binary de Bruijn\nsequences are particular cases of our new algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 06:10:34 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 15:30:50 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chang", "Zuling", ""], ["Ezerman", "Martianus Frederic", ""], ["Fahreza", "Adamas Aqsa", ""]]}, {"id": "1902.08809", "submitter": "L\\'aszl\\'o Kozma", "authors": "L\\'aszl\\'o Kozma", "title": "Faster and simpler algorithms for finding large patterns in permutations", "comments": "The second analysis of Algorithm~S was mistaken. The corrected bound\n  is 1.618^n instead of 1.414^n", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation patterns and pattern avoidance have been intensively studied in\ncombinatorics and computer science, going back at least to the seminal work of\nKnuth on stack-sorting (1968). Perhaps the most natural algorithmic question in\nthis area is deciding whether a given permutation of length $n$ contains a\ngiven pattern of length $k$.\n  In this work we give two new algorithms for this well-studied problem, one\nwhose running time is $n^{0.44k+o(k)}$, and one whose running time is the\nbetter of $O(1.6181^n)$ and $n^{k/2+o(k)}$. These results improve the earlier\nbest bounds of Ahal and Rabinovich (2000), and Bruner and Lackner (2012), and\nare the fastest algorithms for the problem when $k = \\Omega(\\log n)$. When $k =\no(\\log n)$, the parameterized algorithm of Guillemot and Marx (2013) dominates.\n  Our second algorithm uses polynomial space and is significantly simpler than\nall previous approaches with comparable running times, including an\n$n^{k/2+o(k)}$ algorithm proposed by Guillemot and Marx. Our approach can be\nsummarized as follows: \"for every matching of the even-valued entries of the\npattern, try to match all odd-valued entries left-to-right\". For the special\ncase of patterns that are Jordan-permutations, we show an improved,\nsubexponential running time.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 16:24:20 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 15:52:03 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kozma", "L\u00e1szl\u00f3", ""]]}, {"id": "1902.09009", "submitter": "Lydia Zakynthinou", "authors": "Huy L. Nguyen, Jonathan Ullman, Lydia Zakynthinou", "title": "Efficient Private Algorithms for Learning Large-Margin Halfspaces", "comments": "changed title, added references and remarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new differentially private algorithms for learning a large-margin\nhalfspace. In contrast to previous algorithms, which are based on either\ndifferentially private simulations of the statistical query model or on private\nconvex optimization, the sample complexity of our algorithms depends only on\nthe margin of the data, and not on the dimension. We complement our results\nwith a lower bound, showing that the dependence of our upper bounds on the\nmargin is optimal.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 20:14:58 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 02:50:55 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Nguyen", "Huy L.", ""], ["Ullman", "Jonathan", ""], ["Zakynthinou", "Lydia", ""]]}, {"id": "1902.09015", "submitter": "Gabriel Cardona", "authors": "Gabriel Cardona, Joan Carles Pons, Celine Scornavacca", "title": "Generation of Tree-Child phylogenetic networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1007347", "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks generalize phylogenetic trees by allowing the\nmodelization of events of reticulate evolution. Among the different kinds of\nphylogenetic networks that have been proposed in the literature, the subclass\nof binary tree-child networks is one of the most studied ones. However, very\nlittle is known about the combinatorial structure of these networks. In this\npaper we address the problem of generating all possible binary tree-child\nnetworks with a given number of leaves in an efficient way via\nreduction/augmentation operations that extend and generalize analogous\noperations for phylogenetic trees and are biologically relevant. Since our\nsolution is recursive, this also provides us with a recurrence relation giving\nan upper bound on the number of such networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 20:50:38 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Cardona", "Gabriel", ""], ["Pons", "Joan Carles", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1902.09102", "submitter": "Eddie Schoute", "authors": "Andrew M. Childs and Eddie Schoute and Cem M. Unsal", "title": "Circuit Transformations for Quantum Architectures", "comments": "29 pages, 1 figure. LuaTeX paper source available at\n  https://gitlab.umiacs.umd.edu/eschoute/arct-paper ; updated with changes from\n  published version, tightened lower bound for modular architectures", "journal-ref": "Proceedings of TQC 2019, LIPIcs, vol. 135, 3:1-3:24 (2019)", "doi": "10.4230/LIPIcs.TQC.2019.3", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computer architectures impose restrictions on qubit interactions. We\npropose efficient circuit transformations that modify a given quantum circuit\nto fit an architecture, allowing for any initial and final mapping of circuit\nqubits to architecture qubits. To achieve this, we first consider the qubit\nmovement subproblem and use the routing via matchings framework to prove\ntighter bounds on parallel routing. In practice, we only need to perform\npartial permutations, so we generalize routing via matchings to that setting.\nWe give new routing procedures for common architecture graphs and for the\ngeneralized hierarchical product of graphs, which produces subgraphs of the\nCartesian product. Secondly, for serial routing, we consider the token swapping\nframework and extend a 4-approximation algorithm for general graphs to support\npartial permutations. We apply these routing procedures to give several circuit\ntransformations, using various heuristic qubit placement subroutines. We\nimplement these transformations in software and compare their performance for\nlarge quantum circuits on grid and modular architectures, identifying\nstrategies that work well in practice.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 06:15:31 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 21:54:51 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Childs", "Andrew M.", ""], ["Schoute", "Eddie", ""], ["Unsal", "Cem M.", ""]]}, {"id": "1902.09114", "submitter": "Bai Zonglei", "authors": "Zonglei Bai, Yongzhi Cao and Hanpin Wang", "title": "FPRAS for the Potts Model and the Number of $k$-colorings", "comments": "We find an error in the analysis of the algorithm, and it makes the\n  results wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a sampling algorithm for the Potts model using Markov\nchains. Based on the sampling algorithm, we give \\emph{FPRAS}es for the Potts\nmodel and the number of $k$-colorings of the graph.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 07:17:58 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 07:36:32 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 10:59:39 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Bai", "Zonglei", ""], ["Cao", "Yongzhi", ""], ["Wang", "Hanpin", ""]]}, {"id": "1902.09228", "submitter": "Seungbum Jo", "authors": "H\\\"useyin Acan, Sankardeep Chakraborty, Seungbum Jo and Srinivasa Rao\n  Satti", "title": "Succinct Data Structures for Families of Interval Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing succinct data structures for interval\ngraphs with $n$ vertices while supporting degree, adjacency, neighborhood and\nshortest path queries in optimal time in the $\\Theta(\\log n)$-bit word RAM\nmodel. The degree query reports the number of incident edges to a given vertex\nin constant time, the adjacency query returns true if there is an edge between\ntwo vertices in constant time, the neighborhood query reports the set of all\nadjacent vertices in time proportional to the degree of the queried vertex, and\nthe shortest path query returns a shortest path in time proportional to its\nlength, thus the running times of these queries are optimal. Towards showing\nsuccinctness, we first show that at least $n\\log{n} - 2n\\log\\log n - O(n)$ bits\nare necessary to represent any unlabeled interval graph $G$ with $n$ vertices,\nanswering an open problem of Yang and Pippenger [Proc. Amer. Math. Soc. 2017].\nThis is augmented by a data structure of size $n\\log{n} +O(n)$ bits while\nsupporting not only the aforementioned queries optimally but also capable of\nexecuting various combinatorial algorithms (like proper coloring, maximum\nindependent set etc.) on the input interval graph efficiently. Finally, we\nextend our ideas to other variants of interval graphs, for example, proper/unit\ninterval graphs, k-proper and k-improper interval graphs, and circular-arc\ngraphs, and design succinct/compact data structures for these graph classes as\nwell along with supporting queries on them efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 12:29:11 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 06:19:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Acan", "H\u00fcseyin", ""], ["Chakraborty", "Sankardeep", ""], ["Jo", "Seungbum", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1902.09377", "submitter": "Ran Ben Basat", "authors": "Ran Ben-Basat and Guy Even and Ken-ichi Kawarabayashi and Gregory\n  Schwartzman", "title": "Optimal Distributed Covering Algorithms", "comments": "This paper extends and improves arXiv:1808.05809 which we keep\n  separate as it is considerably simpler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a time-optimal deterministic distributed algorithm for\napproximating a minimum weight vertex cover in hypergraphs of rank $f$. This\nproblem is equivalent to the Minimum Weight Set Cover problem in which the\nfrequency of every element is bounded by $f$. The approximation factor of our\nalgorithm is $(f+\\epsilon)$. Our algorithm runs in the CONGEST model and\nrequires $O(\\log\\Delta/ \\log\\log\\Delta)$ rounds, for constants\n$\\epsilon\\in(0,1]$ and $f\\in N^+$. This is the first distributed algorithm for\nthis problem whose running time does not depend on the vertex weights nor the\nnumber of vertices. For constant values of $f$ and $\\epsilon$, our algorithm\nimproves over the $(f+\\epsilon)$-approximation algorithm of KMW06 whose running\ntime is $O(\\log \\Delta + \\log W)$, where $W$ is the ratio between the largest\nand smallest vertex weights in the graph. Our algorithm also achieves an\n$f$-approximation for the problem in $O(f\\log n)$ rounds, improving over the\nclassical result of KVY94 that achieves a running time of $O(f\\log^2 n)$.\nFinally, for weighted vertex cover ($f=2$) our algorithm achieves a\n\\emph{deterministic} running time of $O(\\log n)$, matching the\n\\emph{randomized} previously best result of KY11. We also show that integer\ncovering-programs can be reduced to the Minimum Weight Set Cover problem in the\ndistributed setting. This allows us to achieve an $(f+\\epsilon)$-approximate\nintegral solution in $O(\\frac{\\log\\Delta}{\\log\\log\\Delta}+(f\\cdot\\log\nM)^{1.01}\\log\\epsilon^{-1}(\\log\\Delta)^{0.01})$ rounds, where $f$ bounds the\nnumber of variables in a constraint, $\\Delta$ bounds the number of constraints\na variable appears in, and $M=\\max \\{1, 1/a_{\\min}\\}$, where $a_{min}$ is the\nsmallest normalized constraint coefficient. This improves over the results of\nKMW06 for the integral case, which runs in $O(\\epsilon^{-4}\\cdot f^4\\cdot \\log\nf\\cdot\\log(M\\cdot\\Delta))$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:45:13 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 12:37:30 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Even", "Guy", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1902.09465", "submitter": "Daniel LeJeune", "authors": "Daniel LeJeune, Richard G. Baraniuk, Reinhard Heckel", "title": "Adaptive Estimation for Approximate k-Nearest-Neighbor Computations", "comments": "11 pages, 2 figures. To appear in AISTATS 2019", "journal-ref": "Proceedings of Machine Learning Research 89 (2019):3099-3107", "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithms often carry out equally many computations for \"easy\" and \"hard\"\nproblem instances. In particular, algorithms for finding nearest neighbors\ntypically have the same running time regardless of the particular problem\ninstance. In this paper, we consider the approximate k-nearest-neighbor\nproblem, which is the problem of finding a subset of O(k) points in a given set\nof points that contains the set of k nearest neighbors of a given query point.\nWe propose an algorithm based on adaptively estimating the distances, and show\nthat it is essentially optimal out of algorithms that are only allowed to\nadaptively estimate distances. We then demonstrate both theoretically and\nexperimentally that the algorithm can achieve significant speedups relative to\nthe naive method.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 17:30:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["LeJeune", "Daniel", ""], ["Baraniuk", "Richard G.", ""], ["Heckel", "Reinhard", ""]]}, {"id": "1902.09702", "submitter": "Gecia Bravo-Hermsdorff", "authors": "Gecia Bravo-Hermsdorff and Lee M. Gunderson", "title": "A Unifying Framework for Spectrum-Preserving Graph Sparsification and\n  Coarsening", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019), pp. 7734-7746", "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How might one \"reduce\" a graph? That is, generate a smaller graph that\npreserves the global structure at the expense of discarding local details?\nThere has been extensive work on both graph sparsification (removing edges) and\ngraph coarsening (merging nodes, often by edge contraction); however, these\noperations are currently treated separately. Interestingly, for a planar graph,\nedge deletion corresponds to edge contraction in its planar dual (and more\ngenerally, for a graphical matroid and its dual). Moreover, with respect to the\ndynamics induced by the graph Laplacian (e.g., diffusion), deletion and\ncontraction are physical manifestations of two reciprocal limits: edge weights\nof $0$ and $\\infty$, respectively. In this work, we provide a unifying\nframework that captures both of these operations, allowing one to\nsimultaneously sparsify and coarsen a graph while preserving its large-scale\nstructure. The limit of infinite edge weight is rarely considered, as many\nclassical notions of graph similarity diverge. However, its algebraic,\ngeometric, and physical interpretations are reflected in the Laplacian\npseudoinverse $\\mathbf{\\mathit{L}}^{\\dagger}$, which remains finite in this\nlimit. Motivated by this insight, we provide a probabilistic algorithm that\nreduces graphs while preserving $\\mathbf{\\mathit{L}}^{\\dagger}$, using an\nunbiased procedure that minimizes its variance. We compare our algorithm with\nseveral existing sparsification and coarsening algorithms using real-world\ndatasets, and demonstrate that it more accurately preserves the large-scale\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:02:23 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 05:29:51 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 17:49:15 GMT"}, {"version": "v4", "created": "Fri, 23 Aug 2019 05:36:52 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2020 03:12:21 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Bravo-Hermsdorff", "Gecia", ""], ["Gunderson", "Lee M.", ""]]}, {"id": "1902.09958", "submitter": "Sebastian Brandt", "authors": "Sebastian Brandt", "title": "An Automatic Speedup Theorem for Distributed Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Brandt et al. [STOC'16] proved a lower bound for the distributed\nLov\\'asz Local Lemma, which has been conjectured to be tight for sufficiently\nrelaxed LLL criteria by Chang and Pettie [FOCS'17]. At the heart of their\nresult lies a speedup technique that, for graphs of girth at least $2t+2$,\ntransforms any $t$-round algorithm for one specific LLL problem into a\n$(t-1)$-round algorithm for the same problem. We substantially improve on this\ntechnique by showing that such a speedup exists for any locally checkable\nproblem $\\Pi$, with the difference that the problem $\\Pi_1$ the inferred\n$(t-1)$-round algorithm solves is not (necessarily) the same problem as $\\Pi$.\nOur speedup is automatic in the sense that there is a fixed procedure that\ntransforms a description for $\\Pi$ into a description for $\\Pi_1$ and\nreversible in the sense that any $(t-1)$-round algorithm for $\\Pi_1$ can be\ntransformed into a $t$-round algorithm for $\\Pi$. In particular, for any\nlocally checkable problem $\\Pi$ with exact deterministic time complexity $T(n,\n\\Delta) \\leq t$ on graphs with $n$ nodes, maximum node degree $\\Delta$, and\ngirth at least $2t+2$, there is a sequence of problems $\\Pi_1, \\Pi_2, \\dots$\nwith time complexities $T(n, \\Delta)-1, T(n, \\Delta)-2, \\dots$, that can be\ninferred from $\\Pi$.\n  As a first application of our generalized speedup, we solve a long-standing\nopen problem of Naor and Stockmeyer [STOC'93]: we show that weak $2$-coloring\nin odd-degree graphs cannot be solved in $o(\\log^* \\Delta)$ rounds, thereby\nproviding a matching lower bound to their upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 14:27:52 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Brandt", "Sebastian", ""]]}, {"id": "1902.10046", "submitter": "Jeremy Alm", "authors": "Jeremy F Alm", "title": "Arithmetic Progressions of Length Three in Multiplicative Subgroups of\n  $\\mathbb{F}_p$", "comments": "8 pages, 2 figures. The proof of Theorem 2 follows closely the proof\n  of Theorem 4 from arXiv:1609.01817", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give an algorithm for detecting non-trivial 3-APs in\nmultiplicative subgroups of $\\mathbb{F}_p^\\times$ that is substantially more\nefficient than the naive approach. It follows that certain Var der Waerden-like\nnumbers can be computed in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 16:43:59 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Alm", "Jeremy F", ""]]}, {"id": "1902.10132", "submitter": "Pan Li", "authors": "Pan Li, Niao He, Olgica Milenkovic", "title": "Quadratic Decomposable Submodular Function Minimization: Theory and\n  Practice (Computation and Analysis of PageRank over Hypergraphs)", "comments": "A part of the work appeared in NeurIPS 2018. The current version is\n  to appear in JMLR (Revise some typos). arXiv admin note: substantial text\n  overlap with arXiv:1806.09842", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new convex optimization problem, termed quadratic decomposable\nsubmodular function minimization (QDSFM), which allows to model a number of\nlearning tasks on graphs and hypergraphs. The problem exhibits close ties to\ndecomposable submodular function minimization (DSFM), yet is much more\nchallenging to solve. We approach the problem via a new dual strategy and\nformulate an objective that can be optimized through a number of double-loop\nalgorithms. The outer-loop uses either random coordinate descent (RCD) or\nalternative projection (AP) methods, for both of which we prove linear\nconvergence rates. The inner-loop computes projections onto cones generated by\nbase polytopes of the submodular functions, via the modified min-norm-point or\nFrank-Wolfe algorithm. We also describe two new applications of QDSFM:\nhypergraph-adapted PageRank and semi-supervised learning. The proposed\nhypergraph-based PageRank algorithm can be used for local hypergraph\npartitioning, and comes with provable performance guarantees. For\nhypergraph-adapted semi-supervised learning, we provide numerical experiments\ndemonstrating the efficiency of our QDSFM solvers and their significant\nimprovements on prediction accuracy when compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 08:08:40 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 09:23:38 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 00:10:29 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 15:33:50 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Pan", ""], ["He", "Niao", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1902.10140", "submitter": "Tom Zahavy", "authors": "Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour", "title": "Planning in Hierarchical Reinforcement Learning: Guarantees for Using\n  Local Policies", "comments": "Extends previous paper (arXiv:1803.04674) by the same authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a settings of hierarchical reinforcement learning, in which the\nreward is a sum of components. For each component we are given a policy that\nmaximizes it and our goal is to assemble a policy from the individual policies\nthat maximizes the sum of the components. We provide theoretical guarantees for\nassembling such policies in deterministic MDPs with collectible rewards. Our\napproach builds on formulating this problem as a traveling salesman problem\nwith discounted reward. We focus on local solutions, i.e., policies that only\nuse information from the current state; thus, they are easy to implement and do\nnot require substantial computational resources. We propose three local\nstochastic policies and prove that they guarantee better performance than any\ndeterministic local policy in the worst case; experimental results suggest that\nthey also perform better on average.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:04:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 15:59:35 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Zahavy", "Tom", ""], ["Hasidim", "Avinatan", ""], ["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""]]}, {"id": "1902.10176", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "A Memoization Framework for Scaling Submodular Optimization to Large\n  Scale Problems", "comments": "To Appear in Proc. AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by large scale submodular optimization problems, where\nstandard algorithms that treat the submodular functions in the \\emph{value\noracle model} do not scale. In this paper, we present a model called the\n\\emph{precomputational complexity model}, along with a unifying memoization\nbased framework, which looks at the specific form of the given submodular\nfunction. A key ingredient in this framework is the notion of a\n\\emph{precomputed statistic}, which is maintained in the course of the\nalgorithms. We show that we can easily integrate this idea into a large class\nof submodular optimization problems including constrained and unconstrained\nsubmodular maximization, minimization, difference of submodular optimization,\noptimization with submodular constraints and several other related optimization\nproblems. Moreover, memoization can be integrated in both discrete and\ncontinuous relaxation flavors of algorithms for these problems. We demonstrate\nthis idea for several commonly occurring submodular functions, and show how the\nprecomputational model provides significant speedups compared to the value\noracle model. Finally, we empirically demonstrate this for large scale machine\nlearning problems of data subset selection and summarization.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 19:22:57 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1902.10271", "submitter": "Victor Verdugo", "authors": "Hans Raj Tiwary, Victor Verdugo and Andreas Wiese", "title": "On the extension complexity of scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear programming is a powerful method in combinatorial optimization with\nmany applications in theory and practice. For solving a linear program quickly\nit is desirable to have a formulation of small size for the given problem. A\nuseful approach for this is the construction of an extended formulation, which\nis a linear program in a higher dimensional space whose projection yields the\noriginal linear program. For many problems it is known that a small extended\nformulation cannot exist. However, most of these problems are either\n$\\mathsf{NP}$-hard (like TSP), or only quite complicated polynomial time\nalgorithms are known for them (like for the matching problem). In this work we\nstudy the minimum makespan problem on identical machines in which we want to\nassign a set of $n$ given jobs to $m$ machines in order to minimize the maximum\nload over the machines. We prove that the canonical formulation for this\nproblem has extension complexity $2^{\\Omega(n/\\log n)}$, even if each job has\nsize 1 or 2 and the optimal makespan is 2. This is a case that a trivial greedy\nalgorithm can solve optimally! For the more powerful configuration integer\nprogram, we even prove a lower bound of $2^{\\Omega(n)}$. On the other hand, we\nshow that there is an abstraction of the configuration integer program\nadmitting an extended formulation of size $f(\\text{opt})\\cdot\n\\text{poly}(n,m)$. In addition, we give an $O(\\log n)$-approximate integral\nformulation of polynomial size, even for arbitrary processing times and for the\nfar more general setting of unrelated machines.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 00:14:59 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Tiwary", "Hans Raj", ""], ["Verdugo", "Victor", ""], ["Wiese", "Andreas", ""]]}, {"id": "1902.10328", "submitter": "Nadiia Chepurko", "authors": "Ainesh Bakshi, Nadiia Chepurko, David P. Woodruff", "title": "Weighted Maximum Independent Set of Geometric Objects in Turnstile\n  Streams", "comments": "The lower bound for arbitrary length intervals in the previous\n  version contains a bug, we are updating the submission to reflect this", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Maximum Independent Set problem for geometric objects given in\nthe data stream model. A set of geometric objects is said to be independent if\nthe objects are pairwise disjoint. We consider geometric objects in one and two\ndimensions, i.e., intervals and disks. Let $\\alpha$ be the cardinality of the\nlargest independent set. Our goal is to estimate $\\alpha$ in a small amount of\nspace, given that the input is received as a one-pass stream. We also consider\na generalization of this problem by assigning weights to each object and\nestimating $\\beta$, the largest value of a weighted independent set. We\ninitialize the study of this problem in the turnstile streaming model\n(insertions and deletions) and provide the first algorithms for estimating\n$\\alpha$ and $\\beta$.\n  For unit-length intervals, we obtain a $(2+\\epsilon)$-approximation to\n$\\alpha$ and $\\beta$ in poly$(\\frac{\\log(n)}{\\epsilon})$ space. We also show a\nmatching lower bound. Combined with the $3/2$-approximation for insertion-only\nstreams by Cabello and Perez-Lanterno [CP15], our result implies a separation\nbetween the insertion-only and turnstile model. For unit-radius disks, we\nobtain a $\\left(\\frac{8\\sqrt{3}}{\\pi}\\right)$-approximation to $\\alpha$ and\n$\\beta$ in poly$(\\log(n), \\epsilon^{-1})$ space, which is closely related to\nthe hexagonal circle packing constant.\n  We provide algorithms for estimating $\\alpha$ for arbitrary-length intervals\nunder a bounded intersection assumption and study the parameterized space\ncomplexity of estimating $\\alpha$ and $\\beta$, where the parameter is the ratio\nof maximum to minimum interval length.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 04:49:58 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 19:46:44 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Chepurko", "Nadiia", ""], ["Woodruff", "David P.", ""]]}, {"id": "1902.10349", "submitter": "Michael Haythorpe", "authors": "Jerzy A Filar, Michael Haythorpe and Richard Taylor", "title": "Linearly-growing Reductions of Karp's 21 NP-complete Problems", "comments": null, "journal-ref": "Numerical Algebra, Control and Optimization, 8(1):1-16, 2018", "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the question of whether it may be worthwhile to convert certain,\nnow classical, NP-complete problems to one of a smaller number of kernel\nNP-complete problems. In particular, we show that Karp's classical set of 21\nNP-complete problems contains a kernel subset of six problems with the property\nthat each problem in the larger set can be converted to one of these six\nproblems with only linear growth in problem size. This finding has potential\napplications in optimisation theory because the kernel subset includes 0-1\ninteger programming, job sequencing and undirected Hamiltonian cycle problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 06:17:56 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Filar", "Jerzy A", ""], ["Haythorpe", "Michael", ""], ["Taylor", "Richard", ""]]}, {"id": "1902.10369", "submitter": "Yael Hitron", "authors": "Yael Hitron, Merav Parter", "title": "Counting to Ten with Two Fingers: Compressed Counting with Spiking\n  Neurons", "comments": "Accepted to ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of measuring time with probabilistic threshold gates\nimplemented by bio-inspired spiking neurons. In the model of spiking neural\nnetworks, network evolves in discrete rounds, where in each round, neurons fire\nin pulses in response to a sufficiently high membrane potential. This potential\nis induced by spikes from neighboring neurons that fired in the previous round,\nwhich can have either an excitatory or inhibitory effect. We first consider a\ndeterministic implementation of a neural timer and show that $\\Theta(\\log t)$\n(deterministic) threshold gates are both sufficient and necessary. This raised\nthe question of whether randomness can be leveraged to reduce the number of\nneurons. We answer this question in the affirmative by considering neural\ntimers with spiking neurons where the neuron $y$ is required to fire for $t$\nconsecutive rounds with probability at least $1-\\delta$, and should stop firing\nafter at most $2t$ rounds with probability $1-\\delta$ for some input parameter\n$\\delta \\in (0,1)$. Our key result is a construction of a neural timer with\n$O(\\log\\log 1/\\delta)$ spiking neurons. Interestingly, this construction uses\nonly one spiking neuron, while the remaining neurons can be deterministic\nthreshold gates. We complement this construction with a matching lower bound of\n$\\Omega(\\min\\{\\log\\log 1/\\delta, \\log t\\})$ neurons. This provides the first\nseparation between deterministic and randomized constructions in the setting of\nspiking neural networks. Finally, we demonstrate the usefulness of compressed\ncounting networks for synchronizing neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:39:17 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 15:21:15 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 09:22:08 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hitron", "Yael", ""], ["Parter", "Merav", ""]]}, {"id": "1902.10419", "submitter": "Bruno Ordozgoiti", "authors": "Bruno Ordozgoiti and Aristides Gionis", "title": "Reconciliation k-median: Clustering with Non-Polarized Representatives", "comments": "The Web Conference 2019", "journal-ref": null, "doi": "10.1145/3308558.3313475", "report-no": null, "categories": "cs.DS cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new variant of the k-median problem, where the objective\nfunction models not only the cost of assigning data points to cluster\nrepresentatives, but also a penalty term for disagreement among the\nrepresentatives. We motivate this novel problem by applications where we are\ninterested in clustering data while avoiding selecting representatives that are\ntoo far from each other. For example, we may want to summarize a set of news\nsources, but avoid selecting ideologically-extreme articles in order to reduce\npolarization.\n  To solve the proposed k-median formulation we adopt the local-search\nalgorithm of Arya et al. We show that the algorithm provides a provable\napproximation guarantee, which becomes constant under an assumption on the\nminimum number of points for each cluster. We experimentally evaluate our\nproblem formulation and proposed algorithm on datasets inspired by the\nmotivating applications. In particular, we experiment with data extracted from\nTwitter, the US Congress voting records, and popular news sources. The results\nshow that our objective can lead to choosing less polarized groups of\nrepresentatives without significant loss in representation fidelity.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 09:57:51 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 08:59:19 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ordozgoiti", "Bruno", ""], ["Gionis", "Aristides", ""]]}, {"id": "1902.10489", "submitter": "William Moses Jr.", "authors": "Anisur Rahaman Molla and William K. Moses Jr", "title": "Dispersion of Mobile Robots: The Power of Randomness", "comments": "20 pages, 1 table. Accepted at TAMC 2019: Theory & Applications of\n  Models of Computation. The final authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-14812-6_30", "journal-ref": null, "doi": "10.1007/978-3-030-14812-6_30", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider cooperation among insects, modeled as cooperation between mobile\nrobots on a graph. Within this setting, we consider the problem of mobile robot\ndispersion on graphs. The study of mobile robots on a graph is an interesting\nparadigm with many interesting problems and applications. The problem of\ndispersion in this context, introduced by Augustine and Moses Jr., asks that\n$n$ robots, initially placed arbitrarily on an $n$ node graph, work together to\nquickly reach a configuration with exactly one robot at each node. Previous\nwork on this problem has looked at the trade-off between the time to achieve\ndispersion and the amount of memory required by each robot. However, the\ntrade-off was analyzed for \\textit{deterministic algorithms} and the minimum\nmemory required to achieve dispersion was found to be $\\Omega(\\log n)$ bits at\neach robot. In this paper, we show that by harnessing the power of\n\\textit{randomness}, one can achieve dispersion with $O(\\log \\Delta)$ bits of\nmemory at each robot, where $\\Delta$ is the maximum degree of the graph.\nFurthermore, we show a matching lower bound of $\\Omega(\\log \\Delta)$ bits for\nany \\textit{randomized algorithm} to solve dispersion.\n  We further extend the problem to a general $k$-dispersion problem where $k>\nn$ robots need to disperse over $n$ nodes such that at most $\\lceil k/n \\rceil$\nrobots are at each node in the final configuration.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 12:46:14 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Moses", "William K.", "Jr"]]}, {"id": "1902.10582", "submitter": "Yuko Kuroki", "authors": "Yuko Kuroki, Liyuan Xu, Atsushi Miyauchi, Junya Honda, Masashi\n  Sugiyama", "title": "Polynomial-time Algorithms for Multiple-arm Identification with\n  Full-bandit Feedback", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of stochastic combinatorial pure exploration (CPE),\nwhere an agent sequentially pulls a set of single arms (a.k.a. a super arm) and\ntries to find the best super arm. Among a variety of problem settings of the\nCPE, we focus on the full-bandit setting, where we cannot observe the reward of\neach single arm, but only the sum of the rewards. Although we can regard the\nCPE with full-bandit feedback as a special case of pure exploration in linear\nbandits, an approach based on linear bandits is not computationally feasible\nsince the number of super arms may be exponential. In this paper, we first\npropose a polynomial-time bandit algorithm for the CPE under general\ncombinatorial constraints and provide an upper bound of the sample complexity.\nSecond, we design an approximation algorithm for the 0-1 quadratic maximization\nproblem, which arises in many bandit algorithms with confidence ellipsoids.\nBased on our approximation algorithm, we propose novel bandit algorithms for\nthe top-k selection problem, and prove that our algorithms run in polynomial\ntime. Finally, we conduct experiments on synthetic and real-world datasets, and\nconfirm the validity of our theoretical analysis in terms of both the\ncomputation time and the sample complexity.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 15:20:09 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 12:45:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kuroki", "Yuko", ""], ["Xu", "Liyuan", ""], ["Miyauchi", "Atsushi", ""], ["Honda", "Junya", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1902.10633", "submitter": "Amir Zandieh", "authors": "Michael Kapralov, Ameya Velingker, Amir Zandieh", "title": "Dimension-independent Sparse Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Discrete Fourier Transform (DFT) is a fundamental computational\nprimitive, and the fastest known algorithm for computing the DFT is the FFT\n(Fast Fourier Transform) algorithm. One remarkable feature of FFT is the fact\nthat its runtime depends only on the size $N$ of the input vector, but not on\nthe dimensionality of the input domain: FFT runs in time $O(N\\log N)$\nirrespective of whether the DFT in question is on $\\mathbb{Z}_N$ or\n$\\mathbb{Z}_n^d$ for some $d>1$, where $N=n^d$.\n  The state of the art for Sparse FFT, i.e. the problem of computing the DFT of\na signal that has at most $k$ nonzeros in Fourier domain, is very different:\nall current techniques for sublinear time computation of Sparse FFT incur an\nexponential dependence on the dimension $d$ in the runtime. In this paper we\ngive the first algorithm that computes the DFT of a $k$-sparse signal in time\n$\\text{poly}(k, \\log N)$ in any dimension $d$, avoiding the curse of\ndimensionality inherent in all previously known techniques. Our main tool is a\nnew class of filters that we refer to as adaptive aliasing filters: these\nfilters allow isolating frequencies of a $k$-Fourier sparse signal using $O(k)$\nsamples in time domain and $O(k\\log N)$ runtime per frequency, in any dimension\n$d$.\n  We also investigate natural average case models of the input signal: (1)\nworst case support in Fourier domain with randomized coefficients and (2)\nrandom locations in Fourier domain with worst case coefficients. Our techniques\nlead to an $\\widetilde O(k^2)$ time algorithm for the former and an $\\widetilde\nO(k)$ time algorithm for the latter.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 16:53:56 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Kapralov", "Michael", ""], ["Velingker", "Ameya", ""], ["Zandieh", "Amir", ""]]}, {"id": "1902.10710", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Jan Vondrak", "title": "High probability generalization bounds for uniformly stable algorithms\n  with nearly optimal rate", "comments": "this is a follow-up to and has minor text overlap with\n  arXiv:1812.09859; v2: minor revision following acceptance for presentation at\n  COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic stability is a classical approach to understanding and analysis\nof the generalization error of learning algorithms. A notable weakness of most\nstability-based generalization bounds is that they hold only in expectation.\nGeneralization with high probability has been established in a landmark paper\nof Bousquet and Elisseeff (2002) albeit at the expense of an additional\n$\\sqrt{n}$ factor in the bound. Specifically, their bound on the estimation\nerror of any $\\gamma$-uniformly stable learning algorithm on $n$ samples and\nrange in $[0,1]$ is $O(\\gamma \\sqrt{n \\log(1/\\delta)} +\n\\sqrt{\\log(1/\\delta)/n})$ with probability $\\geq 1-\\delta$. The $\\sqrt{n}$\noverhead makes the bound vacuous in the common settings where $\\gamma \\geq\n1/\\sqrt{n}$. A stronger bound was recently proved by the authors (Feldman and\nVondrak, 2018) that reduces the overhead to at most $O(n^{1/4})$. Still, both\nof these results give optimal generalization bounds only when $\\gamma =\nO(1/n)$.\n  We prove a nearly tight bound of $O(\\gamma \\log(n)\\log(n/\\delta) +\n\\sqrt{\\log(1/\\delta)/n})$ on the estimation error of any $\\gamma$-uniformly\nstable algorithm. It implies that for algorithms that are uniformly stable with\n$\\gamma = O(1/\\sqrt{n})$, estimation error is essentially the same as the\nsampling error. Our result leads to the first high-probability generalization\nbounds for multi-pass stochastic gradient descent and regularized ERM for\nstochastic convex problems with nearly optimal rate --- resolving open problems\nin prior work. Our proof technique is new and we introduce several analysis\ntools that might find additional applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 18:50:28 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 05:48:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Feldman", "Vitaly", ""], ["Vondrak", "Jan", ""]]}, {"id": "1902.10765", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Matthew D. Jones, Matias Korman, Christopher\n  Meierfrankenfeld, Michael J. Munje, Diane L. Souvaine, Michael Thramann,\n  Csaba D. T\\'oth", "title": "Reconfiguration of Connected Graph Partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent computational models for redistricting and detection of\ngerrymandering, we study the following problem on graph partitions. Given a\ngraph $G$ and an integer $k\\geq 1$, a $k$-district map of $G$ is a partition of\n$V(G)$ into $k$ nonempty subsets, called districts, each of which induces a\nconnected subgraph of $G$. A switch is an operation that modifies a\n$k$-district map by reassigning a subset of vertices from one district to an\nadjacent district; a 1-switch is a switch that moves a single vertex. We study\nthe connectivity of the configuration space of all $k$-district maps of a graph\n$G$ under 1-switch operations. We give a combinatorial characterization for the\nconnectedness of this space that can be tested efficiently. We prove that it is\nNP-complete to decide whether there exists a sequence of 1-switches that takes\na given $k$-district map into another; and NP-hard to find the shortest such\nsequence (even if a sequence of polynomial length is known to exist). We also\npresent efficient algorithms for computing a sequence of 1-switches that takes\na given $k$-district map into another when the space is connected, and show\nthat these algorithms perform a worst-case optimal number of switches up to\nconstant factors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 20:18:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:29:29 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Jones", "Matthew D.", ""], ["Korman", "Matias", ""], ["Meierfrankenfeld", "Christopher", ""], ["Munje", "Michael J.", ""], ["Souvaine", "Diane L.", ""], ["Thramann", "Michael", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1902.10812", "submitter": "Vladan Majerech", "authors": "Vladan Majerech", "title": "Padovan heaps", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze priority queues of Fibonacci family. The paper is inspired by\nViolation heap [1], where A. Elmasry saves one pointer in representation of\nFibonacci heap nodes while achieving the same amortized bounds as Fibonacci\nheaps [2] of M. L. Fredman and R. E. Tarjan. Unfortunately author forces the\nheaps to be wide, what goes against optimal heap principles. Our goal is to\nachieve the same result, but with much narrower heaps. We follow the principle\nof superexpensive comparison so we try to remember results of all comparisons\nand never compare elements that cannot be minimal. We delay comparisons as long\nas possible. Actually I have always want to share superexpensive comparison\nprinciple ideas, discovery of Padovan heaps allowed me to do so. Of course\nsaving one pointer is not that big goal, but I hope the presented reasoning and\namortized analysis of the resulting heaps is worth a publication.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 22:27:12 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Majerech", "Vladan", ""]]}, {"id": "1902.10829", "submitter": "Sanchit Kalhan", "authors": "Sanchit Kalhan, Konstantin Makarychev and Timothy Zhou", "title": "Improved algorithms for Correlation Clustering with local objectives", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation Clustering is a powerful graph partitioning model that aims to\ncluster items based on the notion of similarity between items. An instance of\nthe Correlation Clustering problem consists of a graph $G$ (not necessarily\ncomplete) whose edges are labeled by a binary classifier as ``similar'' and\n``dissimilar''. An objective which has received a lot of attention in\nliterature is that of minimizing the number of disagreements: an edge is in\ndisagreement if it is a ``similar'' edge and is present across clusters or if\nit is a ``dissimilar'' edge and is present within a cluster. Define the\ndisagreements vector to be an $n$ dimensional vector indexed by the vertices,\nwhere the $v$-th index is the number of disagreements at vertex $v$. Recently,\nPuleo and Milenkovic (ICML '16) initiated the study of the Correlation\nClustering framework in which the objectives were more general functions of the\ndisagreements vector. In this paper, we study algorithms for minimizing\n$\\ell_q$ norms $(q \\geq 1)$ of the disagreements vector for both arbitrary and\ncomplete graphs. We present the first known algorithm for minimizing the\n$\\ell_q$ norm of the disagreements vector on arbitrary graphs and also provide\nan improved algorithm for minimizing the $\\ell_q$ norm $(q \\geq 1)$ of the\ndisagreements vector on complete graphs. We also study an alternate\ncluster-wise local objective introduced by Ahmadi, Khuller and Saha (IPCO '19),\nwhich aims to minimize the maximum number of disagreements associated with a\ncluster. We also present an improved ($2 + \\varepsilon$) approximation\nalgorithm for this objective. Finally, we compliment our algorithmic results\nfor minimizing the $\\ell_q$ norm of the disagreements vector with some hardness\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 23:23:17 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 22:53:04 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Kalhan", "Sanchit", ""], ["Makarychev", "Konstantin", ""], ["Zhou", "Timothy", ""]]}, {"id": "1902.10935", "submitter": "Lior Kamma", "authors": "Peyman Afshani, Casper Benjamin Freksen, Lior Kamma, Kasper Green\n  Larsen", "title": "Lower Bounds for Multiplication via Network Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication is one of the most fundamental computational problems, yet its\ntrue complexity remains elusive. The best known upper bound, by F\\\"{u}rer,\nshows that two $n$-bit numbers can be multiplied via a boolean circuit of size\n$O(n \\lg n \\cdot 4^{\\lg^*n})$, where $\\lg^*n$ is the very slowly growing\niterated logarithm. In this work, we prove that if a central conjecture in the\narea of network coding is true, then any constant degree boolean circuit for\nmultiplication must have size $\\Omega(n \\lg n)$, thus almost completely\nsettling the complexity of multiplication circuits. We additionally revisit\nclassic conjectures in circuit complexity, due to Valiant, and show that the\nnetwork coding conjecture also implies one of Valiant's conjectures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 07:35:19 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Afshani", "Peyman", ""], ["Freksen", "Casper Benjamin", ""], ["Kamma", "Lior", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1902.10966", "submitter": "Amer Krivo\\v{s}ija", "authors": "Amer Krivo\\v{s}ija and Alexander Munteanu", "title": "Probabilistic smallest enclosing ball in high dimensions via subgradient\n  sampling", "comments": "20 pages; SoCG 2019 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the median problem for a collection of point sets in\nhigh dimensions. This generalizes the geometric median as well as the\n(probabilistic) smallest enclosing ball (pSEB) problems. Our main objective and\nmotivation is to improve the previously best algorithm for the pSEB problem by\nreducing its exponential dependence on the dimension to linear. This is\nachieved via a novel combination of sampling techniques for clustering problems\nin metric spaces with the framework of stochastic subgradient descent. As a\nresult, the algorithm becomes applicable to shape fitting problems in Hilbert\nspaces of unbounded dimension via kernel functions. We present an exemplary\napplication by extending the support vector data description (SVDD) shape\nfitting method to the probabilistic case. This is done by simulating the pSEB\nalgorithm implicitly in the feature space induced by the kernel function.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:22:46 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Krivo\u0161ija", "Amer", ""], ["Munteanu", "Alexander", ""]]}, {"id": "1902.10983", "submitter": "Florin Manea", "authors": "Katrin Casel, Joel D. Day, Pamela Fleischmann, Tomasz Kociumaka,\n  Florin Manea, Markus L. Schmid", "title": "Graph and String Parameters: Connections Between Pathwidth, Cutwidth and\n  the Locality Number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the locality number, a recently introduced structural\nparameter for strings (with applications in pattern matching with variables),\nand its connection to two important graph-parameters, cutwidth and pathwidth.\nThese connections allow us to show that computing the locality number is\nNP-hard but fixed parameter tractable (when the locality number or the alphabet\nsize is treated as a parameter), and can be approximated with ratio\n$O(\\sqrt{\\log{opt}} \\log n)$. As a by-product, we also relate cutwidth via the\nlocality number to pathwidth, which is of independent interest, since it\nimproves the currently best known approximation algorithm for cutwidth. In\naddition to these main results, we also consider the possibility of\ngreedy-based approximation algorithms for the locality number.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:01:40 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Casel", "Katrin", ""], ["Day", "Joel D.", ""], ["Fleischmann", "Pamela", ""], ["Kociumaka", "Tomasz", ""], ["Manea", "Florin", ""], ["Schmid", "Markus L.", ""]]}, {"id": "1902.10995", "submitter": "Arik Rinberg", "authors": "Arik Rinberg, Alexander Spiegelman, Edward Bortnikov, Eshcar Hillel,\n  Idit Keidar, Lee Rhodes, and Hadar Serviansky", "title": "Fast Concurrent Data Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sketches are approximate succinct summaries of long streams. They are\nwidely used for processing massive amounts of data and answering statistical\nqueries about it in real-time. Existing libraries producing sketches are very\nfast, but do not allow parallelism for creating sketches using multiple threads\nor querying them while they are being built. We present a generic approach to\nparallelising data sketches efficiently, while bounding the error that such\nparallelism introduces. Utilising relaxed semantics and the notion of strong\nlinearisability we prove our algorithm's correctness and analyse the error it\ninduces in two specific sketches. Our implementation achieves high scalability\nwhile keeping the error small.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:29:35 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 08:57:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rinberg", "Arik", ""], ["Spiegelman", "Alexander", ""], ["Bortnikov", "Edward", ""], ["Hillel", "Eshcar", ""], ["Keidar", "Idit", ""], ["Rhodes", "Lee", ""], ["Serviansky", "Hadar", ""]]}, {"id": "1902.11044", "submitter": "Fabrizio Frati", "authors": "Barbara Covella, Fabrizio Frati and Maurizio Patrignani", "title": "On the Area Requirements of Planar Straight-Line Orthogonal Drawings of\n  Ternary Trees", "comments": "Combines the results from a GD '07 paper and a IWOCA '18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the area requirements of planar straight-line\northogonal drawings of ternary trees. We prove that every ternary tree admits\nsuch a drawing in sub-quadratic area. Further, we present upper bounds, the\noutcomes of an experimental evaluation, and a conjecture on the area\nrequirements of planar straight-line orthogonal drawings of complete ternary\ntrees. Finally, we present a polynomial lower bound on the length of the\nminimum side of any planar straight-line orthogonal drawing of a complete\nternary tree.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:18:42 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Covella", "Barbara", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1902.11047", "submitter": "Kurt Mehlhorn", "authors": "Hannaneh Akrami and Kurt Mehlhorn and Tommy Odland", "title": "Ratio-Balanced Maximum Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a loan is approved for a person or company, the bank is subject to\n\\emph{credit risk}; the risk that the lender defaults. To mitigate this risk, a\nbank will require some form of \\emph{security}, which will be collected if the\nlender defaults. Accounts can be secured by several securities and a security\ncan be used for several accounts. The goal is to fractionally assign the\nsecurities to the accounts so as to balance the risk.\n  This situation can be modelled by a bipartite graph. We have a set $S$ of\nsecurities and a set $A$ of accounts. Each security has a \\emph{value} $v_i$\nand each account has an \\emph{exposure} $e_j$. If a security $i$ can be used to\nsecure an account $j$, we have an edge from $i$ to $j$. Let $f_{ij}$ be part of\nsecurity $i$'s value used to secure account $j$. We are searching for a maximum\nflow that send at most $v_i$ units out of node $i \\in S$ and at most $e_j$\nunits into node $j \\in A$. Then $s_j = e_j - \\sum_i f_{ij}$ is the unsecured\npart of account $j$. We are searching for the maximum flow that minimizes\n$\\sum_j s_j^2/e_j$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:30:32 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Akrami", "Hannaneh", ""], ["Mehlhorn", "Kurt", ""], ["Odland", "Tommy", ""]]}, {"id": "1902.11105", "submitter": "Jingbo Wang", "authors": "Callum Schofield, Jingbo B. Wang, Yuying Li", "title": "Quantum walk inspired algorithm for graph similarity and isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale complex systems, such as social networks, electrical power grid,\ndatabase structure, consumption pattern or brain connectivity, are often\nmodeled using network graphs. Valuable insight can be gained by measuring the\nsimilarity between network graphs in order to make quantitative comparisons.\nSince these networks can be very large, scalability and efficiency of the\nalgorithm are key concerns. More importantly, for graphs with unknown labeling,\nthis graph similarity problem requires exponential time to solve using existing\nalgorithms. In this paper, we propose a quantum walk inspired algorithm, which\nprovides a solution to the graph similarity problem without prior knowledge on\ngraph labeling. This algorithm is capable of distinguishing between minor\nstructural differences, such as between strongly regular graphs with the same\nparameters. The algorithm has polynomial complexity, scaling with $O(n^9)$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 14:39:19 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Schofield", "Callum", ""], ["Wang", "Jingbo B.", ""], ["Li", "Yuying", ""]]}, {"id": "1902.11169", "submitter": "Riko Jacob", "authors": "Riko Jacob, Gerth St{\\o}lting Brodal", "title": "Dynamic Planar Convex Hull", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we determine the amortized computational complexity of the\nplanar dynamic convex hull problem by querying.\n  We present a data structure that maintains a set of n points in the plane\nunder the insertion and deletion of points in amortized O(log n) time per\noperation. The space usage of the data structure is O(n). The data structure\nsupports extreme point queries in a given direction, tangent queries through a\ngiven point, and queries for the neighboring points on the convex hull in O(log\nn) time. The extreme point queries can be used to decide whether or not a given\nline intersects the convex hull, and the tangent queries to determine whether a\ngiven point is inside the convex hull.\n  We give a lower bound on the amortized asymptotic time complexity that\nmatches the performance of this data structure.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 15:49:06 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Jacob", "Riko", ""], ["Brodal", "Gerth St\u00f8lting", ""]]}, {"id": "1902.11281", "submitter": "Uthaipon Tantipongpipat", "authors": "Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie\n  Morgenstern, Santosh Vempala", "title": "Multi-Criteria Dimensionality Reduction with Applications to Fairness", "comments": "The preliminary version appeared in NeurIPS2019. This version\n  combines the motivation from \"The Price of Fair PCA: One Extra Dimension\"\n  (NeurIPS 2018) by the same set of author, adds new a motivation, and\n  introduces new heuristics and more experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a classical technique widely used for data\nanalysis. One foundational instantiation is Principal Component Analysis (PCA),\nwhich minimizes the average reconstruction error. In this paper, we introduce\nthe \"multi-criteria dimensionality reduction\" problem where we are given\nmultiple objectives that need to be optimized simultaneously. As an\napplication, our model captures several fairness criteria for dimensionality\nreduction such as our novel Fair-PCA problem and the Nash Social Welfare (NSW)\nproblem. In Fair-PCA, the input data is divided into $k$ groups, and the goal\nis to find a single $d$-dimensional representation for all groups for which the\nminimum variance of any one group is maximized. In NSW, the goal is to maximize\nthe product of the individual variances of the groups achieved by the common\nlow-dimensional space.\n  Our main result is an exact polynomial-time algorithm for the two-criterion\ndimensionality reduction problem when the two criteria are increasing concave\nfunctions. As an application of this result, we obtain a polynomial time\nalgorithm for Fair-PCA for $k=2$ groups and a polynomial time algorithm for NSW\nobjective for $k=2$ groups. We also give approximation algorithms for $k>2$.\nOur technical contribution in the above results is to prove new low-rank\nproperties of extreme point solutions to semi-definite programs. We conclude\nwith experiments indicating the effectiveness of algorithms based on extreme\npoint solutions of semi-definite programs on several real-world data sets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:37:18 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 17:36:00 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 13:40:52 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Tantipongpipat", "Uthaipon", ""], ["Samadi", "Samira", ""], ["Singh", "Mohit", ""], ["Morgenstern", "Jamie", ""], ["Vempala", "Santosh", ""]]}]