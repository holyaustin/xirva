[{"id": "1608.00117", "submitter": "Jack Fitzsimons", "authors": "J. K. Fitzsimons, M. A. Osborne, S. J. Roberts and J. F. Fitzsimons", "title": "Improved stochastic trace estimation using mutually unbiased bases", "comments": "5 pages, 1 figure, 2 tables. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS math.NA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of estimating the trace of a matrix $A$ when given\naccess to an oracle which computes $x^\\dagger A x$ for an input vector $x$. We\nmake use of the basis vectors from a set of mutually unbiased bases, widely\nstudied in the field of quantum information processing, in the selection of\nprobing vectors $x$. This approach offers a new state of the art single shot\nsampling variance while requiring only $O(\\log(n))$ random bits to generate\neach vector. This significantly improves on traditional methods such as\nHutchinson's and Gaussian estimators in terms of the number of random bits\nrequired and worst case sample variance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 13:06:05 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Fitzsimons", "J. K.", ""], ["Osborne", "M. A.", ""], ["Roberts", "S. J.", ""], ["Fitzsimons", "J. F.", ""]]}, {"id": "1608.00180", "submitter": "Venkata Gandikota", "authors": "Karthekeyan Chandrasekaran, Mahdi Cheraghchi, Venkata Gandikota, Elena\n  Grigorescu", "title": "Local Testing for Membership in Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the structural analogies between point lattices and linear\nerror-correcting codes, and by the mature theory on locally testable codes, we\ninitiate a systematic study of local testing for membership in lattices.\nTesting membership in lattices is also motivated in practice, by applications\nto integer programming, error detection in lattice-based communication, and\ncryptography.\n  Apart from establishing the conceptual foundations of lattice testing, our\nresults include the following:\n  1. We demonstrate upper and lower bounds on the query complexity of local\ntesting for the well-known family of code formula lattices. Furthermore, we\ninstantiate our results with code formula lattices constructed from Reed-Muller\ncodes, and obtain nearly-tight bounds.\n  2. We show that in order to achieve low query complexity, it is sufficient to\ndesign one-sided non-adaptive canonical tests. This result is akin to, and\nbased on an analogous result for error-correcting codes due to Ben-Sasson et\nal. (SIAM J. Computing 35(1) pp1-21).\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 03:48:22 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Cheraghchi", "Mahdi", ""], ["Gandikota", "Venkata", ""], ["Grigorescu", "Elena", ""]]}, {"id": "1608.00191", "submitter": "Ankit Singh Rawat", "authors": "Venkatesan Guruswami, Ankit Singh Rawat", "title": "New MDS codes with small sub-packetization and near-optimal repair\n  bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n, M)$ vector code $\\mathcal{C} \\subseteq \\mathbb{F}^n$ is a collection\nof $M$ codewords where $n$ elements (from the field $\\mathbb{F}$) in each of\nthe codewords are referred to as code blocks. Assuming that $\\mathbb{F} \\cong\n\\mathbb{B}^{\\ell}$, the code blocks are treated as $\\ell$-length vectors over\nthe base field $\\mathbb{B}$. Equivalently, the code is said to have the\nsub-packetization level $\\ell$. This paper addresses the problem of\nconstructing MDS vector codes which enable exact reconstruction of each code\nblock by downloading small amount of information from the remaining code\nblocks. The repair bandwidth of a code measures the information flow from the\nremaining code blocks during the reconstruction of a single code block. This\nproblem naturally arises in the context of distributed storage systems as the\nnode repair problem [4]. Assuming that $M = |\\mathbb{B}|^{k\\ell}$, the repair\nbandwidth of an MDS vector code is lower bounded by $\\big(\\frac{n - 1}{n -\nk}\\big)\\cdot \\ell$ symbols (over the base field $\\mathbb{B}$) which is also\nreferred to as the cut-set bound [4]. For all values of $n$ and $k$, the MDS\nvector codes that attain the cut-set bound with the sub-packetization level\n$\\ell = (n-k)^{\\lceil{{n}/{(n-k)}}\\rceil}$ are known in the literature [23,\n35].\n  This paper presents a construction for MDS vector codes which simultaneously\nensures both small repair bandwidth and small sub-packetization level. The\nobtained codes have the smallest possible sub-packetization level $\\ell = O(n -\nk)$ for an MDS vector code and the repair bandwidth which is at most twice the\ncut-set bound. The paper then generalizes this code construction so that the\nrepair bandwidth of the obtained codes approach the cut-set bound at the cost\nof increased sub-packetization level. The constructions presented in this paper\ngive MDS vector codes which are linear over the base field $\\mathbb{B}$.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 06:30:54 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Rawat", "Ankit Singh", ""]]}, {"id": "1608.00196", "submitter": "Zhi-Zhong Chen", "authors": "Zhi-Zhong Chen, Youta Harada, Lusheng Wang", "title": "An Approximation Algorithm for Maximum Internal Spanning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G, the {\\em maximum internal spanning tree problem} (MIST for\nshort) asks for computing a spanning tree T of G such that the number of\ninternal vertices in T is maximized. MIST has possible applications in the\ndesign of cost-efficient communication networks and water supply networks and\nhence has been extensively studied in the literature. MIST is NP-hard and hence\na number of polynomial-time approximation algorithms have been designed for\nMIST in the literature. The previously best polynomial-time approximation\nalgorithm for MIST achieves a ratio of 3/4. In this paper, we first design a\nsimpler algorithm that achieves the same ratio and the same time complexity as\nthe previous best. We then refine the algorithm into a new approximation\nalgorithm that achieves a better ratio (namely, 13/17) with the same time\ncomplexity. Our new algorithm explores much deeper structure of the problem\nthan the previous best. The discovered structure may be used to design even\nbetter approximation or parameterized algorithms for the problem in the future.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 07:22:44 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Chen", "Zhi-Zhong", ""], ["Harada", "Youta", ""], ["Wang", "Lusheng", ""]]}, {"id": "1608.00212", "submitter": "Oren Weimann", "authors": "Ofer Freedman, Pawe{\\l} Gawrychowski, Patrick K. Nicholson, and Oren\n  Weimann", "title": "Optimal Distance Labeling Schemes for Trees", "comments": "23 pages, 6 figures, to appear in PODC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling schemes seek to assign a short label to each node in a network, so\nthat a function on two nodes can be computed by examining their labels alone.\nFor the particular case of trees, optimal bounds (up to low order terms) were\nrecently obtained for adjacency labeling [FOCS'15], nearest common ancestor\nlabeling [SODA'14], and ancestry labeling [SICOMP'06]. In this paper we obtain\noptimal bounds for distance labeling. We present labels of size\n$1/4\\log^2n+o(\\log^2n)$, matching (up to low order terms) the recent\n$1/4\\log^2n-O(\\log n)$ lower bound [ICALP'16].\n  Prior to our work, all distance labeling schemes for trees could be\nreinterpreted as universal trees. A tree $T$ is said to be universal if any\ntree on $n$ nodes can be found as a subtree of $T$. A universal tree with $|T|$\nnodes implies a distance labeling scheme with label size $\\log |T|$. In 1981,\nChung et al. proved that any distance labeling scheme based on universal trees\nrequires labels of size $1/2\\log^2 n-\\log n\\cdot\\log\\log n+O(\\log n)$. Our\nscheme is the first to break this lower bound, showing a separation between\ndistance labeling and universal trees.\n  The $\\Theta(\\log^2 n)$ barrier has led researchers to consider distances\nbounded by $k$. The size of such labels was improved from $\\log n+O(k\\sqrt{\\log\nn})$ [WADS'01] to $\\log n+O(k^2(\\log(k\\log n))$ [SODA'03] and then to $\\log\nn+O(k\\log(k\\log(n/k)))$ [PODC'07]. We show how to construct labels whose size\nis $\\min\\{\\log n+O(k\\log((\\log n)/k)),O(\\log n\\cdot\\log(k/\\log n))\\}$. We\ncomplement this with almost tight lower bounds of $\\log n+\\Omega(k\\log(\\log\nn/(k\\log k)))$ and $\\Omega(\\log n\\cdot\\log(k/\\log n))$. Finally, we consider\n$(1+\\varepsilon)$-approximate distances. We show that the labeling scheme of\n[ICALP'16] can be easily modified to obtain an $O(\\log(1/\\varepsilon)\\cdot\\log\nn)$ upper bound and we prove a $\\Omega(\\log(1/\\varepsilon)\\cdot\\log n)$ lower\nbound.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 12:30:27 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 18:54:55 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Freedman", "Ofer", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Nicholson", "Patrick K.", ""], ["Weimann", "Oren", ""]]}, {"id": "1608.00271", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy and Alina Ene", "title": "On Approximating Maximum Independent Set of Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Maximum Independent Set of Rectangles (MISR) problem: given a\nset of $n$ axis-parallel rectangles, find a largest-cardinality subset of the\nrectangles, such that no two of them overlap. MISR is a basic geometric\noptimization problem with many applications, that has been studied extensively.\nUntil recently, the best approximation algorithm for it achieved an $O(\\log\n\\log n)$-approximation factor. In a recent breakthrough, Adamaszek and Wiese\nprovided a quasi-polynomial time approximation scheme: a\n$(1-\\epsilon)$-approximation algorithm with running time\n$n^{O(\\operatorname{poly}(\\log n)/\\epsilon)}$. Despite this result, obtaining a\nPTAS or even a polynomial-time constant-factor approximation remains a\nchallenging open problem. In this paper we make progress towards this goal by\nproviding an algorithm for MISR that achieves a $(1 - \\epsilon)$-approximation\nin time $n^{O(\\operatorname{poly}(\\log\\log{n} / \\epsilon))}$. We introduce\nseveral new technical ideas, that we hope will lead to further progress on this\nand related problems.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:15:31 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Ene", "Alina", ""]]}, {"id": "1608.00497", "submitter": "Madhur Tulsiani", "authors": "Mrinalkanti Ghosh, Madhur Tulsiani", "title": "From Weak to Strong LP Gaps for all CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximability of constraint satisfaction problems (CSPs) by\nlinear programming (LP) relaxations. We show that for every CSP, the\napproximation obtained by a basic LP relaxation, is no weaker than the\napproximation obtained using relaxations given by $\\Omega\\left(\\frac{\\log\nn}{\\log \\log n}\\right)$ levels of the Sherali-Adams hierarchy on instances of\nsize $n$.\n  It was proved by Chan et al. [FOCS 2013] that any polynomial size LP extended\nformulation is no stronger than relaxations obtained by a super-constant levels\nof the Sherali-Adams hierarchy.. Combining this with our result also implies\nthat any polynomial size LP extended formulation is no stronger than the basic\nLP.\n  Using our techniques, we also simplify and strengthen the result by Khot et\nal. [STOC 2014] on (strong) approximation resistance for LPs. They provided a\nnecessary and sufficient condition under which $\\Omega(\\log \\log n)$ levels of\nthe Sherali-Adams hierarchy cannot achieve an approximation better than a\nrandom assignment. We simplify their proof and strengthen the bound to\n$\\Omega\\left(\\frac{\\log n}{\\log \\log n}\\right)$ levels.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 17:14:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ghosh", "Mrinalkanti", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1608.00529", "submitter": "V\\'it Jel\\'inek", "authors": "V\\'it Jel\\'inek, Jan Kyn\\v{c}l", "title": "Hardness of Permutation Pattern Matching", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation Pattern Matching (or PPM) is a decision problem whose input is a\npair of permutations $\\pi$ and $\\tau$, represented as sequences of integers,\nand the task is to determine whether $\\tau$ contains a subsequence\norder-isomorphic to $\\pi$. Bose, Buss and Lubiw proved that PPM is NP-complete\non general inputs.\n  We show that PPM is NP-complete even when $\\pi$ has no decreasing subsequence\nof length 3 and $\\tau$ has no decreasing subsequence of length 4. This provides\nthe first known example of PPM being hard when one or both of $\\pi$ and\n$\\sigma$ are restricted to a proper hereditary class of permutations.\n  This hardness result is tight in the sense that PPM is known to be polynomial\nwhen both $\\pi$ and $\\tau$ avoid a decreasing subsequence of length 3, as well\nas when $\\pi$ avoids a decreasing subsequence of length 2. The result is also\ntight in another sense: we will show that for any hereditary proper subclass C\nof the class of permutations avoiding a decreasing sequence of length 3, there\nis a polynomial algorithm solving PPM instances where $\\pi$ is from C and\n$\\tau$ is arbitrary.\n  We also obtain analogous hardness and tractability results for the class of\nso-called skew-merged patterns.\n  From these results, we deduce a complexity dichotomy for the PPM problem\nrestricted to $\\pi$ belonging to $Av(\\rho)$, where $Av(\\rho)$ denotes the class\nof permutations avoiding a permutation $\\rho$. Specifically, we show that the\nproblem is polynomial when $\\rho$ is in the set {1, 12, 21, 132, 213, 231,\n312}, and it is NP-complete for any other $\\rho$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:11:30 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Jel\u00ednek", "V\u00edt", ""], ["Kyn\u010dl", "Jan", ""]]}, {"id": "1608.00550", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Theory of the GMM Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop some theoretical results for a robust similarity measure named\n\"generalized min-max\" (GMM). This similarity has direct applications in machine\nlearning as a positive definite kernel and can be efficiently computed via\nprobabilistic hashing. Owing to the discrete nature, the hashed values can also\nbe used for efficient near neighbor search. We prove the theoretical limit of\nGMM and the consistency result, assuming that the data follow an elliptical\ndistribution, which is a very general family of distributions and includes the\nmultivariate $t$-distribution as a special case. The consistency result holds\nas long as the data have bounded first moment (an assumption which essentially\nholds for datasets commonly encountered in practice). Furthermore, we establish\nthe asymptotic normality of GMM. Compared to the \"cosine\" similarity which is\nroutinely adopted in current practice in statistics and machine learning, the\nconsistency of GMM requires much weaker conditions. Interestingly, when the\ndata follow the $t$-distribution with $\\nu$ degrees of freedom, GMM typically\nprovides a better measure of similarity than \"cosine\" roughly when $\\nu<8$\n(which is already very close to normal). These theoretical results will help\nexplain the recent success of GMM in learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:45:57 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1608.00554", "submitter": "Tarun Kathuria", "authors": "L. Elisa Celis and Amit Deshpande and Tarun Kathuria and Damian\n  Straszak and Nisheeth K. Vishnoi", "title": "On the Complexity of Constrained Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal Point Processes (DPPs) are probabilistic models that arise in\nquantum physics and random matrix theory and have recently found numerous\napplications in computer science. DPPs define distributions over subsets of a\ngiven ground set, they exhibit interesting properties such as negative\ncorrelation, and, unlike other models, have efficient algorithms for sampling.\nWhen applied to kernel methods in machine learning, DPPs favor subsets of the\ngiven data with more diverse features. However, many real-world applications\nrequire efficient algorithms to sample from DPPs with additional constraints on\nthe subset, e.g., partition or matroid constraints that are important to ensure\npriors, resource or fairness constraints on the sampled subset. Whether one can\nefficiently sample from DPPs in such constrained settings is an important\nproblem that was first raised in a survey of DPPs by \\cite{KuleszaTaskar12} and\nstudied in some recent works in the machine learning literature.\n  The main contribution of our paper is the first resolution of the complexity\nof sampling from DPPs with constraints. We give exact efficient algorithms for\nsampling from constrained DPPs when their description is in unary. Furthermore,\nwe prove that when the constraints are specified in binary, this problem is\n#P-hard via a reduction from the problem of computing mixed discriminants\nimplying that it may be unlikely that there is an FPRAS. Our results benefit\nfrom viewing the constrained sampling problem via the lens of polynomials.\nConsequently, we obtain a few algorithms of independent interest: 1) to count\nover the base polytope of regular matroids when there are additional (succinct)\nbudget constraints and, 2) to evaluate and compute the mixed characteristic\npolynomials, that played a central role in the resolution of the Kadison-Singer\nproblem, for certain special cases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:58:05 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 19:12:46 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 12:38:48 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Celis", "L. Elisa", ""], ["Deshpande", "Amit", ""], ["Kathuria", "Tarun", ""], ["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1608.00567", "submitter": "Changchuan Yin Dr.", "authors": "Changchuan Yin", "title": "Identification of repeats in DNA sequences using nucleotide distribution\n  uniformity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repetitive elements are important in genomic structures, functions and\nregulations, yet effective methods in precisely identifying repetitive elements\nin DNA sequences are not fully accessible, and the relationship between\nrepetitive elements and periodicities of genomes is not clearly understood. We\npresent an $\\textit{ab initio}$ method to quantitatively detect repetitive\nelements and infer the consensus repeat pattern in repetitive elements. The\nmethod uses the measure of the distribution uniformity of nucleotides at\nperiodic positions in DNA sequences or genomes. It can identify periodicities,\nconsensus repeat patterns, copy numbers and perfect levels of repetitive\nelements. The results of using the method on different DNA sequences and\ngenomes demonstrate efficacy and accuracy in identifying repeat patterns and\nperiodicities. The complexity of the method is linear with respect to the\nlengths of the analyzed sequences.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 21:16:18 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Yin", "Changchuan", ""]]}, {"id": "1608.00673", "submitter": "Sahil Singla", "authors": "Anupam Gupta, Viswanath Nagarajan, Sahil Singla", "title": "Adaptivity Gaps for Stochastic Probing: Submodular and XOS Functions", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are given a submodular function $f$ over a set of elements, and we\nwant to maximize its value subject to certain constraints. Good approximation\nalgorithms are known for such problems under both monotone and non-monotone\nsubmodular functions. We consider these problems in a stochastic setting, where\nelements are not all active and we can only get value from active elements.\nEach element $e$ is active independently with some known probability $p_e$, but\nwe don't know the element's status \\emph{a priori}. We find it out only when we\n\\emph{probe} the element $e$---probing reveals whether it's active or not,\nwhereafter we can use this information to decide which other elements to probe.\nEventually, if we have a probed set $S$ and a subset $\\text{active}(S)$ of\nactive elements in $S$, we can pick any $T \\subseteq \\text{active}(S)$ and get\nvalue $f(T)$. Moreover, the sequence of elements we probe must satisfy a given\n\\emph{prefix-closed constraint}---e.g., these may be given by a matroid, or an\norienteering constraint, or deadline, or precedence constraint, or an arbitrary\ndownward-closed constraint---if we can probe some sequence of elements we can\nprobe any prefix of it. What is a good strategy to probe elements to maximize\nthe expected value?\n  In this paper we study the gap between adaptive and non-adaptive strategies\nfor $f$ being a submodular or a fractionally subadditive (XOS) function. If\nthis gap is small, we can focus on finding good non-adaptive strategies\ninstead, which are easier to find as well as to represent. We show that the\nadaptivity gap is a constant for monotone and non-monotone submodular\nfunctions, and logarithmic for XOS functions of small \\emph{width}. These\nbounds are nearly tight. Our techniques show new ways of arguing about the\noptimal adaptive decision tree for stochastic problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 02:07:51 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Gupta", "Anupam", ""], ["Nagarajan", "Viswanath", ""], ["Singla", "Sahil", ""]]}, {"id": "1608.00687", "submitter": "Xibo Jin", "authors": "Xibo Jin, Fa Zhang, Athanasios V. Vasilakos, and Zhiyong Liu", "title": "Green Data Centers: A Survey, Perspectives, and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, a major concern regarding data centers is their extremely high\nenergy consumption and carbon dioxide emissions. However, because of the\nover-provisioning of resources, the utilization of existing data centers is, in\nfact, remarkably low, leading to considerable energy waste. Therefore, over the\npast few years, many research efforts have been devoted to increasing\nefficiency for the construction of green data centers. The goal of these\nefforts is to efficiently utilize available resources and to reduce energy\nconsumption and thermal cooling costs. In this paper, we provide a survey of\nthe state-of-the-art research on green data center techniques, including energy\nefficiency, resource management, thermal control and green metrics.\nAdditionally, we present a detailed comparison of the reviewed proposals. We\nfurther discuss the key challenges for future research and highlight some\nfuture research issues for addressing the problem of building green data\ncenters.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 03:28:30 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Jin", "Xibo", ""], ["Zhang", "Fa", ""], ["Vasilakos", "Athanasios V.", ""], ["Liu", "Zhiyong", ""]]}, {"id": "1608.00724", "submitter": "Darren Strash", "authors": "Darren Strash", "title": "On the Power of Simple Reductions for the Maximum Independent Set\n  Problem", "comments": "13 pages; 1 figure; 3 tables; Appeared at the 22nd International\n  Conference on Computing and Combinatorics (COCOON 2016)", "journal-ref": null, "doi": "10.1007/978-3-319-42634-1_28", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reductions---rules that reduce input size while maintaining the ability to\ncompute an optimal solution---are critical for developing efficient maximum\nindependent set algorithms in both theory and practice. While several simple\nreductions have previously been shown to make small domain-specific instances\ntractable in practice, it was only recently shown that advanced reductions (in\na measure-and-conquer approach) can be used to solve real-world networks on\nmillions of vertices [Akiba and Iwata, TCS 2016]. In this paper we compare\nthese state-of-the-art reductions against a small suite of simple reductions,\nand come to two conclusions: just two simple reductions---vertex folding and\nisolated vertex removal---are sufficient for many real-world instances, and\nfurther, the power of the advanced rules comes largely from their initial\napplication (i.e., kernelization), and not their repeated application during\nbranch-and-bound. As a part of our comparison, we give the first experimental\nevaluation of a reduction based on maximum critical independent sets, and show\nit is highly effective in practice for medium-sized networks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:15:21 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Strash", "Darren", ""]]}, {"id": "1608.00726", "submitter": "Sebastien Tixeuil", "authors": "Dianne Foreback, Mikhail Nesterenko, S\\'ebastien Tixeuil (NPA, IUF,\n  LINCS)", "title": "Infinite Unlimited Churn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unlimited infinite churn in peer-to-peer overlay networks. Under\nthis churn, arbitrary many peers may concurrently request to join or leave the\noverlay network; moreover these requests may never stop coming. We prove that\nunlimited adversarial churn, where processes may just exit the overlay network,\nis unsolvable. We focus on cooperative churn where exiting processes\nparticipate in the churn handling algorithm. We define the problem of unlimited\ninfinite churn in this setting. We distinguish the fair version of the problem,\nwhere each request is eventually satisfied, from the unfair version that just\nguarantees progress. We focus on local solutions to the problem, and prove that\na local solution to the Fair Infinite Unlimited Churn is impossible. We then\npresent and prove correct an algorithm UIUC that solves the Unfair Infinite\nUnlimited Churn Problem for a linearized peer-to-peer overlay network. We\nextend this solution to skip lists and skip graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:25:13 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Foreback", "Dianne", "", "NPA, IUF,\n  LINCS"], ["Nesterenko", "Mikhail", "", "NPA, IUF,\n  LINCS"], ["Tixeuil", "S\u00e9bastien", "", "NPA, IUF,\n  LINCS"]]}, {"id": "1608.00865", "submitter": "Tomasz Kociumaka", "authors": "Pawe{\\l} Gawrychowski, Tomasz Kociumaka", "title": "Sparse Suffix Tree Construction in Optimal Time and Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix tree (and the closely related suffix array) are fundamental structures\ncapturing all substrings of a given text essentially by storing all its\nsuffixes in the lexicographical order. In some applications, we work with a\nsubset of $b$ interesting suffixes, which are stored in the so-called sparse\nsuffix tree. Because the size of this structure is $\\Theta(b)$, it is natural\nto seek a construction algorithm using only $O(b)$ words of space assuming\nread-only random access to the text. We design a linear-time Monte Carlo\nalgorithm for this problem, hence resolving an open question explicitly stated\nby Bille et al. [TALG 2016]. The best previously known algorithm by I et al.\n[STACS 2014] works in $O(n\\log b)$ time. Our solution proceeds in $n/b$ rounds;\nin the $r$-th round, we consider all suffixes starting at positions congruent\nto $r$ modulo $n/b$. By maintaining rolling hashes, we lexicographically sort\nall interesting suffixes starting at such positions, and then we merge them\nwith the already considered suffixes. For efficient merging, we also need to\nanswer LCE queries in small space. By plugging in the structure of Bille et al.\n[CPM 2015] we obtain $O(n+b\\log b)$ time complexity. We improve this structure,\nwhich implies a linear-time sparse suffix tree construction algorithm. We\ncomplement our Monte Carlo algorithm with a deterministic verification\nprocedure. The verification takes $O(n\\sqrt{\\log b})$ time, which improves upon\nthe bound of $O(n\\log b)$ obtained by I et al. [STACS 2014]. This is obtained\nby first observing that the pruning done inside the previous solution has a\nrather clean description using the notion of graph spanners with small\nmultiplicative stretch. Then, we are able to decrease the verification time by\napplying difference covers twice. Combined with the Monte Carlo algorithm, this\ngives us an $O(n\\sqrt{\\log b})$-time and $O(b)$-space Las Vegas algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:25:52 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Kociumaka", "Tomasz", ""]]}, {"id": "1608.01031", "submitter": "Eugene Goltsman", "authors": "Jarrod A. Chapman, Isaac Y. Ho, Eugene Goltsman, Daniel S. Rokhsar", "title": "Meraculous2: fast accurate short-read assembly of large polymorphic\n  genomes", "comments": "Supplementary notes included with the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Meraculous2, an update to the Meraculous short-read assembler that\nincludes (1) handling of allelic variation using \"bubble\" structures within the\nde Bruijn graph, (2) improved gap closing, and (3) an improved scaffolding\nalgorithm that produces more complete assemblies without compromising\nscaffolding accuracy. The speed and bandwidth efficiency of the new parallel\nimplementation have also been substantially improved, allowing the assembly of\na human genome to be accomplished in 24 hours on the JGI/NERSC Genepool system.\nTo highlight the features of Meraculous2 we present here the assembly of the\ndiploid human genome NA12878, and compare it with previously published\nassemblies of the same data using other algorithms. The Meraculous2 assemblies\nare shown to have better completeness, contiguity, and accuracy than other\npublished assemblies for these data. Practical considerations including\npre-assembly analyses of polymorphism and repetitiveness are described.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 23:49:21 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 19:59:13 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Chapman", "Jarrod A.", ""], ["Ho", "Isaac Y.", ""], ["Goltsman", "Eugene", ""], ["Rokhsar", "Daniel S.", ""]]}, {"id": "1608.01036", "submitter": "Michael J. Steindorfer", "authors": "Michael J. Steindorfer and Jurgen J. Vinju", "title": "Fast and Lean Immutable Multi-Maps on the JVM based on Heterogeneous\n  Hash-Array Mapped Tries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An immutable multi-map is a many-to-many thread-friendly map data structure\nwith expected fast insert and lookup operations. This data structure is used\nfor applications processing graphs or many-to-many relations as applied in\nstatic analysis of object-oriented systems. When processing such big data sets\nthe memory overhead of the data structure encoding itself is a memory usage\nbottleneck. Motivated by reuse and type-safety, libraries for Java, Scala and\nClojure typically implement immutable multi-maps by nesting sets as the values\nwith the keys of a trie map. Like this, based on our measurements the expected\nbyte overhead for a sparse multi-map per stored entry adds up to around 65B,\nwhich renders it unfeasible to compute with effectively on the JVM.\n  In this paper we propose a general framework for Hash-Array Mapped Tries on\nthe JVM which can store type-heterogeneous keys and values: a Heterogeneous\nHash-Array Mapped Trie (HHAMT). Among other applications, this allows for a\nhighly efficient multi-map encoding by (a) not reserving space for empty value\nsets and (b) inlining the values of singleton sets while maintaining a (c)\ntype-safe API.\n  We detail the necessary encoding and optimizations to mitigate the overhead\nof storing and retrieving heterogeneous data in a hash-trie. Furthermore, we\nevaluate HHAMT specifically for the application to multi-maps, comparing them\nto state-of-the-art encodings of multi-maps in Java, Scala and Clojure. We\nisolate key differences using microbenchmarks and validate the resulting\nconclusions on a real world case in static analysis. The new encoding brings\nthe per key-value storage overhead down to 30B: a 2x improvement. With\nadditional inlining of primitive values it reaches a 4x improvement.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 00:35:42 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Steindorfer", "Michael J.", ""], ["Vinju", "Jurgen J.", ""]]}, {"id": "1608.01275", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Arnab Bhattacharyya and Suprovat Ghoshal", "title": "Testing Sparsity over Known and Unknown Bases", "comments": "This version subsumes the previous one; 25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity is a basic property of real vectors that is exploited in a wide\nvariety of applications. In this work, we describe property testing algorithms\nfor sparsity that observe a low-dimensional projection of the input.\n  We consider two settings. In the first setting, for a given design matrix A\nin R^{d x m}, we test whether an input vector y in R^d equals Ax for some\nk-sparse unit vector x. Our algorithm projects the input onto O(k \\eps^{-2} log\nm) dimensions, accepts if the property holds, rejects if ||y - Ax|| > \\eps for\nany O(k/\\eps^2)-sparse vector x, and runs in time polynomial in m. Our\nalgorithm is based on the approximate Caratheodory's theorem. Previously known\nalgorithms that solve the problem for arbitrary A with qualitatively similar\nguarantees run in exponential time.\n  In the second setting, the design matrix A is unknown. Given input vectors\ny_1, y_2,...,y_p in R^d whose concatenation as columns forms Y in R^{d x p} ,\nthe goal is to decide whether Y=AX for matrices A in R^{d x m} and X in R^{m x\np} such that each column of X is k-sparse, or whether Y is \"far\" from having\nsuch a decomposition. We give such a testing algorithm which projects the input\nvectors to O(log p/\\eps^2) dimensions and assumes that the unknown A satisfies\nk-restricted isometry. Our analysis gives a new robust characterization of\ngaussian width in terms of sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:19:31 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 02:34:51 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Barman", "Siddharth", ""], ["Bhattacharyya", "Arnab", ""], ["Ghoshal", "Suprovat", ""]]}, {"id": "1608.01350", "submitter": "Mikkel Thorup", "authors": "Vahab Mirrokni and Mikkel Thorup and Morteza Zadimoghaddam", "title": "Consistent Hashing with Bounded Loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing algorithms for balanced allocation of clients to servers in dynamic\nsettings is a challenging problem for a variety of reasons. Both servers and\nclients may be added and/or removed from the system periodically, and the main\nobjectives of allocation algorithms are: the uniformity of the allocation, and\nthe number of moves after adding or removing a server or a client. The most\npopular solution for our dynamic settings is Consistent Hashing. However, the\nload balancing of consistent hashing is no better than a random assignment of\nclients to servers, so with $n$ of each, we expect many servers to be\noverloaded with $\\Theta(\\log n/ \\log\\log n)$ clients. In this paper, with $n$\nclients and $n$ servers, we get a guaranteed max-load of 2 while only moving an\nexpected constant number of clients for each update.\n  We take an arbitrary user specified balancing parameter $c=1+\\epsilon>1$.\nWith $m$ balls and $n$ bins in the system, we want no load above $\\lceil\ncm/n\\rceil$. Meanwhile we want to bound the expected number of balls that have\nto be moved when a ball or server is added or removed. Compared with general\nlower bounds without capacity constraints, we show that in our algorithm when a\nball or bin is inserted or deleted, the expected number of balls that have to\nbe moved is increased only by a multiplicative factor $O({1\\over \\epsilon^2})$\nfor $\\epsilon \\le 1$ (Theorem 4) and by a factor $1+O(\\frac{\\log c}c)$ for\n$\\epsilon\\ge 1$ (Theorem 3). Technically, the latter bound is the most\nchallenging to prove. It implies that we for superconstant $c$ only pay a\nnegligible cost in extra moves. We also get the same bounds for the simpler\nproblem where we instead of a user specified balancing parameter have a fixed\nbin capacity $C$ for all bins.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 20:45:09 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 20:50:23 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 16:19:26 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Mirrokni", "Vahab", ""], ["Thorup", "Mikkel", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1608.01396", "submitter": "Vijay Sridhar", "authors": "Facundo M\\'emoli, Anastasios Sidiropoulos and Vijay Sridhar", "title": "Quasimetric embeddings and their applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalizations of classical metric embedding results to the case of\nquasimetric spaces; that is, spaces that do not necessarily satisfy symmetry.\nQuasimetric spaces arise naturally from the shortest-path distances on directed\ngraphs. Perhaps surprisingly, very little is known about low-distortion\nembeddings for quasimetric spaces.\n  Random embeddings into ultrametric spaces are arguably one of the most\nsuccessful geometric tools in the context of algorithm design. We extend this\nto the quasimetric case as follows. We show that any $n$-point quasimetric\nspace supported on a graph of treewidth $t$ admits a random embedding into\nquasiultrametric spaces with distortion $O(t \\log^2 n)$, where\nquasiultrametrics are a natural generalization of ultrametrics. This result\nallows us to obtain $t\\log^{O(1)} n$-approximation algorithms for the Directed\nNon-Bipartite Sparsest-Cut and the Directed Multicut problems on $n$-vertex\ngraphs of treewidth $t$, with running time polynomial in both $n$ and $t$.\n  The above results are obtained by considering a generalization of random\npartitions to the quasimetric case, which we refer to as random\nquasipartitions. Using this definition and a construction of [Chuzhoy and\nKhanna 2009] we derive a polynomial lower bound on the distortion of random\nembeddings of general quasimetric spaces into quasiultrametric spaces. Finally,\nwe establish a lower bound for embedding the shortest-path quasimetric of a\ngraph $G$ into graphs that exclude $G$ as a minor. This lower bound is used to\nshow that several embedding results from the metric case do not have natural\nanalogues in the quasimetric setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 23:28:23 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["M\u00e9moli", "Facundo", ""], ["Sidiropoulos", "Anastasios", ""], ["Sridhar", "Vijay", ""]]}, {"id": "1608.01426", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Solving Laplacian Systems in Logarithmic Space", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the space complexity of solving linear systems of equations.\nWhile all known deterministic or randomized algorithms solving a square system\nof $n$ linear equations in $n$ variables require $\\Omega(\\log^2 n)$ space,\nTa-Shma (STOC 2013) recently showed that on a quantum computer an approximate\nsolution can be computed in logarithmic space, giving the first explicit\ncomputational task for which quantum computation seems to outperform classical\ncomputation with respect to space complexity. In this paper we show that for\nsystems of linear equations in the Laplacian matrix of graphs, the same\nlogarithmic space complexity can actually be achieved by a classical (i.e.,\nnon-quantum) algorithm. More precisely, given a system of linear equations\n$Lx=b$, where $L$ is the (normalized) Laplacian matrix of a graph on $n$\nvertices and $b$ is a unit-norm vector, our algorithm outputs a vector $\\tilde\nx$ such that $\\left\\lVert\\tilde x -x\\right\\rVert\\le 1/\\mathrm{poly}(n)$ and\nuses only $O(\\log n)$ space if the underlying graph has polynomially bounded\nweights. We also show how to estimate, again in logarithmic space, the smallest\nnon-zero eigenvalue of $L$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 04:41:48 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1608.01463", "submitter": "Yoichi Iwata", "authors": "Yoichi Iwata", "title": "Linear-time Kernelization for Feedback Vertex Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an algorithm that, given an undirected graph $G$ of\n$m$ edges and an integer $k$, computes a graph $G'$ and an integer $k'$ in\n$O(k^4 m)$ time such that (1) the size of the graph $G'$ is $O(k^2)$, (2)\n$k'\\leq k$, and (3) $G$ has a feedback vertex set of size at most $k$ if and\nonly if $G'$ has a feedback vertex set of size at most $k'$. This is the first\nlinear-time polynomial-size kernel for Feedback Vertex Set. The size of our\nkernel is $2k^2+k$ vertices and $4k^2$ edges, which is smaller than the\nprevious best of $4k^2$ vertices and $8k^2$ edges. Thus, we improve the size\nand the running time simultaneously. We note that under the assumption of\n$\\mathrm{NP}\\not\\subseteq\\mathrm{coNP}/\\mathrm{poly}$, Feedback Vertex Set does\nnot admit an $O(k^{2-\\epsilon})$-size kernel for any $\\epsilon>0$.\n  Our kernel exploits $k$-submodular relaxation, which is a recently developed\ntechnique for obtaining efficient FPT algorithms for various problems. The dual\nof $k$-submodular relaxation of Feedback Vertex Set can be seen as a\nhalf-integral variant of $A$-path packing, and to obtain the linear-time\ncomplexity, we propose an efficient augmenting-path algorithm for this problem.\nWe believe that this combinatorial algorithm is of independent interest.\n  A solver based on the proposed method won first place in the 1st\nParameterized Algorithms and Computational Experiments (PACE) challenge.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 08:40:37 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 04:52:17 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2017 09:32:22 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Iwata", "Yoichi", ""]]}, {"id": "1608.01487", "submitter": "Samson Zhou", "authors": "Elena Grigorescu, Morteza Monemizadeh, Samson Zhou", "title": "Streaming Weighted Matchings: Optimal Meets Greedy", "comments": "Contains an error that we have not been able to fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating a maximum weighted matching, when\nthe edges of an underlying weighted graph $G(V,E)$ are revealed in a streaming\nfashion. We analyze a variant of the previously best-known\n$(4+\\epsilon)$-approximation algorithm due to Crouch and Stubbs (APPROX, 2014),\nand prove their conjecture that it achieves a tight approximation factor of\n$3.5+\\epsilon$.\n  The algorithm splits the stream into substreams on which it runs a greedy\nmaximum matching algorithm. At the end of the stream, the selected edges are\ngiven as input to an optimal maximum weighted matching algorithm. To analyze\nthe approximation guarantee, we develop a novel charging argument in which we\ndecompose the edges of a maximum weighted matching of $G$ into a few natural\nclasses, and then charge them separately to the edges of the matching output by\nour algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 10:33:53 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 17:22:52 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Grigorescu", "Elena", ""], ["Monemizadeh", "Morteza", ""], ["Zhou", "Samson", ""]]}, {"id": "1608.01612", "submitter": "James Lee", "authors": "James R. Lee", "title": "Separators in region intersection graphs", "comments": "Minor fixes; references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For undirected graphs $G=(V,E)$ and $G_0=(V_0,E_0)$, say that $G$ is a region\nintersection graph over $G_0$ if there is a family of connected subsets $\\{ R_u\n\\subseteq V_0 : u \\in V \\}$ of $G_0$ such that $\\{u,v\\} \\in E \\iff R_u \\cap R_v\n\\neq \\emptyset$.\n  We show if $G_0$ excludes the complete graph $K_h$ as a minor for some $h\n\\geq 1$, then every region intersection graph $G$ over $G_0$ with $m$ edges has\na balanced separator with at most $c_h \\sqrt{m}$ nodes, where $c_h$ is a\nconstant depending only on $h$. If $G$ additionally has uniformly bounded\nvertex degrees, then such a separator is found by spectral partitioning.\n  A string graph is the intersection graph of continuous arcs in the plane. The\npreceding result implies that every string graph with $m$ edges has a balanced\nseparator of size $O(\\sqrt{m})$. This bound is optimal, as it generalizes the\nplanar separator theorem. It confirms a conjecture of Fox and Pach (2010), and\nimproves over the $O(\\sqrt{m} \\log m)$ bound of Matousek (2013).\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 17:00:35 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 10:00:38 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 06:40:46 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Lee", "James R.", ""]]}, {"id": "1608.01670", "submitter": "Dimitri Bertsekas", "authors": "Dimitri Bertsekas", "title": "Robust Shortest Path Planning and Semicontractive Dynamic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIT Report LIDS - 2915", "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider shortest path problems in a directed graph where\nthe transitions between nodes are subject to uncertainty. We use a minimax\nformulation, where the objective is to guarantee that a special destination\nstate is reached with a minimum cost path under the worst possible instance of\nthe uncertainty. Problems of this type arise, among others, in planning and\npursuit-evasion contexts, and in model predictive control. Our analysis makes\nuse of the recently developed theory of abstract semicontractive dynamic\nprogramming models. We investigate questions of existence and uniqueness of\nsolution of the optimality equation, existence of optimal paths, and the\nvalidity of various algorithms patterned after the classical methods of value\nand policy iteration, as well as a Dijkstra-like algorithm for problems with\nnonnegative arc lengths.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 23:07:24 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Bertsekas", "Dimitri", ""]]}, {"id": "1608.01689", "submitter": "Gregory Schwartzman", "authors": "Keren Censor-Hillel, Merav Parter, Gregory Schwartzman", "title": "Derandomizing Local Distributed Algorithms under Bandwidth Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the cornerstone family of \\emph{local problems} in\ndistributed computing, and investigates the curious gap between randomized and\ndeterministic solutions under bandwidth restrictions.\n  Our main contribution is in providing tools for derandomizing solutions to\nlocal problems, when the $n$ nodes can only send $O(\\log n)$-bit messages in\neach round of communication. We combine bounded independence, which we show to\nbe sufficient for some algorithms, with the method of conditional expectations\nand with additional machinery, to obtain the following results.\n  Our techniques give a deterministic maximal independent set (MIS) algorithm\nin the CONGEST model, where the communication graph is identical to the input\ngraph, in $O(D\\log^2 n)$ rounds, where $D$ is the diameter of the graph. The\nbest known running time in terms of $n$ alone is $2^{O(\\sqrt{\\log n})}$, which\nis super-polylogarithmic, and requires large messages. For the CONGEST model,\nthe only known previous solution is a coloring-based $O(\\Delta + \\log^*\nn)$-round algorithm, where $\\Delta$ is the maximal degree in the graph.\n  On the way to obtaining the above, we show that in the \\emph{Congested\nClique} model, which allows all-to-all communication, there is a deterministic\nMIS algorithm that runs in $O(\\log \\Delta \\log n)$ rounds.%, where $\\Delta$ is\nthe maximum degree. When $\\Delta=O(n^{1/3})$, the bound improves to $O(\\log\n\\Delta)$ and holds also for $(\\Delta+1)$-coloring.\n  In addition, we deterministically construct a $(2k-1)$-spanner with\n$O(kn^{1+1/k}\\log n)$ edges in $O(k \\log n)$ rounds. For comparison, in the\nmore stringent CONGEST model, the best deterministic algorithm for constructing\na $(2k-1)$-spanner with $O(kn^{1+1/k})$ edges runs in $O(n^{1-1/k})$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 20:16:18 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Parter", "Merav", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1608.01700", "submitter": "Chaitanya Swamy", "authors": "Sara Ahmadian and Chaitanya Swamy", "title": "Approximation Algorithms for Clustering Problems with Lower Bounds and\n  Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider clustering problems with {\\em non-uniform lower bounds and\noutliers}, and obtain the {\\em first approximation guarantees} for these\nproblems. We have a set $\\F$ of facilities with lower bounds $\\{L_i\\}_{i\\in\\F}$\nand a set $\\D$ of clients located in a common metric space\n$\\{c(i,j)\\}_{i,j\\in\\F\\cup\\D}$, and bounds $k$, $m$. A feasible solution is a\npair $\\bigl(S\\sse\\F,\\sigma:\\D\\mapsto S\\cup\\{\\mathsf{out}\\}\\bigr)$, where\n$\\sigma$ specifies the client assignments, such that $|S|\\leq k$,\n$|\\sigma^{-1}(i)|\\geq L_i$ for all $i\\in S$, and\n$|\\sigma^{-1}(\\mathsf{out})|\\leq m$. In the {\\em lower-bounded min-sum-of-radii\nwith outliers} (\\lbksro) problem, the objective is to minimize $\\sum_{i\\in\nS}\\max_{j\\in\\sigma^{-1}(i)}c(i,j)$, and in the {\\em lower-bounded $k$-supplier\nwith outliers} (\\lbkso) problem, the objective is to minimize $\\max_{i\\in\nS}\\max_{j\\in\\sigma^{-1}(i)}c(i,j)$.\n  We obtain an approximation factor of $12.365$ for \\lbksro, which improves to\n$3.83$ for the non-outlier version (i.e., $m=0$). These also constitute the\n{\\em first} approximation bounds for the min-sum-of-radii objective when we\nconsider lower bounds and outliers {\\em separately}. We apply the primal-dual\nmethod to the relaxation where we Lagrangify the $|S|\\leq k$ constraint. The\nchief technical contribution and novelty of our algorithm is that, departing\nfrom the standard paradigm used for such constrained problems, we obtain an\n$O(1)$-approximation {\\em despite the fact that we do not obtain a\nLagrangian-multiplier-preserving algorithm for the Lagrangian relaxation}. We\nbelieve that our ideas have {broader applicability to other clustering problems\nwith outliers as well.}\n  We obtain approximation factors of $5$ and $3$ respectively for \\lbkso and\nits non-outlier version. These are the {\\em first} approximation results for\n$k$-supplier with {\\em non-uniform} lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 21:12:49 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 04:35:01 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 13:44:22 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Ahmadian", "Sara", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1608.01716", "submitter": "Hiroki Sayama", "authors": "Ali Jazayeri and Hiroki Sayama", "title": "A Polynomial-Time Deterministic Approach to the Traveling Salesperson\n  Problem", "comments": "8 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new polynomial-time deterministic algorithm that produces an\napproximated solution for the traveling salesperson problem. The proposed\nalgorithm ranks cities based on their priorities calculated using a power\nfunction of means and standard deviations of their distances from other cities\nand then connects the cities to their neighbors in the order of their\npriorities. When connecting a city, a neighbor is selected based on their\nneighbors' priorities calculated as another power function that additionally\nincludes their distance from the focal city to be connected. This repeats until\nall the cities are connected into a single loop. The time complexity of the\nproposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical\nevaluation shows that, despite its simplicity, the proposed algorithm produces\nshorter tours with less time complexity than other conventional tour\nconstruction heuristics. The proposed algorithm can be used by itself or as an\ninitial tour generator for other more complex heuristic optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 23:18:47 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:27:26 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 03:46:44 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 22:42:16 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Jazayeri", "Ali", ""], ["Sayama", "Hiroki", ""]]}, {"id": "1608.01721", "submitter": "Lehilton Pedrosa", "authors": "Cristina G. Fernandes, Samuel P. de Paula, Lehilton L. C. Pedrosa", "title": "Improved Approximation Algorithms for Capacitated Fault-Tolerant\n  k-Center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-center problem, given a metric space V and a positive integer k, one\nwants to select k elements (centers) of V and an assignment from V to centers,\nminimizing the maximum distance between an element of V and its assigned\ncenter. One of the most general variants is the capacitated\n{\\alpha}-fault-tolerant k-center, where centers have a limit on the number of\nassigned elements, and, if {\\alpha} centers fail, there is a reassignment from\nV to non-faulty centers. In this paper, we present a new approach to tackle\nfault tolerance, by selecting and pre-opening a set of backup centers, then\nsolving the obtained residual instance. For the {0,L}-capacitated case, we give\napproximations with factor 6 for the basic problem, and 7 for the so called\nconservative variant, when only clients whose centers failed may be reassigned.\nOur algorithms improve on the previously best known factors of 9 and 17,\nrespectively. Moreover, we consider the case with general capacities. Assuming\n{\\alpha} is constant, our method leads to the first approximations for this\ncase. We also derive approximations for the capacitated fault- tolerant\nk-supplier problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 23:59:31 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Fernandes", "Cristina G.", ""], ["de Paula", "Samuel P.", ""], ["Pedrosa", "Lehilton L. C.", ""]]}, {"id": "1608.02282", "submitter": "Piyush Srivastava", "authors": "Nicholas J. A. Harvey, Piyush Srivastava, Jan Vondr\\'ak", "title": "Computing the Independence Polynomial: from the Tree Threshold down to\n  the Roots", "comments": "35 pages. Extended abstract to appear in Proceedings of ACM-SIAM\n  SODA, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an algorithm for approximating the multivariate independence\npolynomial $Z(\\mathbf{z})$, with negative and complex arguments, an object that\nhas strong connections to combinatorics and to statistical physics. In\nparticular, the independence polynomial with negative arguments,\n$Z(-\\mathbf{p})$, determines the Shearer region, the maximal region of\nprobabilities to which the Lovasz Local Lemma (LLL) can be extended (Shearer\n1985). In statistical physics, complex zeros of the independence polynomial\nrelate to existence of phase transitions.\n  Our main result is a deterministic algorithm to compute approximately the\nindependence polynomial in any root-free complex polydisc centered at the\norigin. Our algorithm is essentially the same as Weitz's algorithm for positive\nparameters up to the tree uniqueness threshold, and the core of our analysis is\na novel multivariate form of the correlation decay technique, which can handle\nnon-uniform complex parameters. In particular, in the univariate real setting\nour work implies that Weitz's algorithm works in an interval between two\ncritical points $(\\lambda'_c(d), \\lambda_c(d))$, and outside of this interval\nan approximation of $Z(\\mathbf{z})$ is known to be NP-hard.\n  As an application, we give a sub-exponential time algorithm for testing\napproximate membership in the Shearer region. We also give a new rounding based\ndeterministic algorithm for Shearer's lemma (an extension of the LLL), which,\nhowever, runs in sub-exponential time. On the hardness side, we prove that\nevaluating $Z(\\mathbf{z})$ at an arbitrary point in Shearer's region, and\ntesting membership in Shearer's region, are #P-hard problems. We also establish\nthe best possible dependence of the exponent of the run time of Weitz's\ncorrelation decay technique in the negative regime on the distance to the\nboundary of the Shearer region.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 23:49:42 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 20:46:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Harvey", "Nicholas J. A.", ""], ["Srivastava", "Piyush", ""], ["Vondr\u00e1k", "Jan", ""]]}, {"id": "1608.02413", "submitter": "Christopher Maximilian Pockrandt", "authors": "Christopher Pockrandt, Marcel Ehrhardt and Knut Reinert", "title": "EPR-dictionaries: A practical and fast data structure for constant time\n  searches in unidirectional and bidirectional FM-indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, practical method for conducting an exact search in a uni-\nand bidirectional FM index in $O(1)$ time per step while using $O(\\log \\sigma *\nn) + o(\\log \\sigma * \\sigma * n)$ bits of space. This is done by replacing the\nbinary wavelet tree by a new data structure, the Enhanced Prefixsum Rank\ndictionary (EPR-dictionary). We implemented this method in the SeqAn C++\nlibrary and experimentally validated our theoretical results. In addition we\ncompared our implementation with other freely available implementations of\nbidirectional indices and show that we are between $\\approx 2.6-4.8$ times\nfaster. This will have a large impact for many bioinformatics applications that\nrely on practical implementations of (2)FM indices e.g. for read mapping. To\nour knowledge this is the first implementation of a constant time method for a\nsearch step in 2FM indices.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 12:47:27 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 17:10:54 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Pockrandt", "Christopher", ""], ["Ehrhardt", "Marcel", ""], ["Reinert", "Knut", ""]]}, {"id": "1608.02451", "submitter": "Igor Shinkar", "authors": "Subhash Khot and Igor Shinkar", "title": "An $\\widetilde{O}(n)$ Queries Adaptive Tester for Unateness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive tester for the unateness property of Boolean\nfunctions. Given a function $f:\\{0,1\\}^n \\to \\{0,1\\}$ the tester makes $O(n\n\\log(n)/\\epsilon)$ adaptive queries to the function. The tester always accepts\na unate function, and rejects with probability at least 0.9 if a function is\n$\\epsilon$-far from being unate.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 14:32:47 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Khot", "Subhash", ""], ["Shinkar", "Igor", ""]]}, {"id": "1608.02515", "submitter": "Chandra Chekuri", "authors": "Chandra Chekuri and Thapanapong Rukkanchanunt", "title": "A note on the Survivable Network Design Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider the survivable network design problem (SNDP) in\nundirected graphs. We make two contributions. The first is a new counting\nargument in the iterated rounding based 2-approximation for edge-connectivity\nSNDP (EC-SNDP) originally due to Jain. The second is to make some additional\nconnections between hypergraphic version of SNDP introduced by Zhao, Nagamochi\nand Ibaraki and edge and node-weighted versions of EC-SNDP and\nelement-connectivity SNDP (Elem-SNDP). One useful consequence of this\nconnection is a 2-approximation for Elem-SNDP that avoids the use of set-pair\nbased relaxation and analysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 16:50:45 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Chekuri", "Chandra", ""], ["Rukkanchanunt", "Thapanapong", ""]]}, {"id": "1608.02615", "submitter": "Adnan Mohammed", "authors": "Adnan Saher Mohammed, \\c{S}ahin Emrah Amrahov, Fatih V. \\c{C}elebi", "title": "Bidirectional Conditional Insertion Sort algorithm; An efficient\n  progress on the classical insertion sort", "comments": "references not appeared cause arxiv system does not uploaded .bib\n  files", "journal-ref": null, "doi": "10.1016/j.future.2017.01.034", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a new efficient sorting algorithm based on\ninsertion sort concept. The proposed algorithm called Bidirectional Conditional\nInsertion Sort (BCIS). It is in-place sorting algorithm and it has remarkably\nefficient average case time complexity when compared with classical insertion\nsort (IS). By comparing our new proposed algorithm with the Quicksort\nalgorithm, BCIS indicated faster average case time for relatively small size\narrays up to 1500 elements. Furthermore, BCIS was observed to be faster than\nQuicksort within high rate of duplicated elements even for large size array.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 20:36:53 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 22:55:47 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Mohammed", "Adnan Saher", ""], ["Amrahov", "\u015eahin Emrah", ""], ["\u00c7elebi", "Fatih V.", ""]]}, {"id": "1608.02653", "submitter": "Sergey Bereg", "authors": "Lev Nachmanson, Arlind Nocaj, Sergey Bereg, Leishi Zhang, Alexander\n  Holroyd", "title": "Node Overlap Removal by Growing a Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node overlap removal is a necessary step in many scenarios including laying\nout a graph, or visualizing a tag cloud. Our contribution is a new overlap\nremoval algorithm that iteratively builds a Minimum Spanning Tree on a Delaunay\ntriangulation of the node centers and removes the node overlaps by \"growing\"\nthe tree. The algorithm is simple to implement yet produces high quality\nlayouts. According to our experiments it runs several times faster than the\ncurrent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 23:16:38 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Nachmanson", "Lev", ""], ["Nocaj", "Arlind", ""], ["Bereg", "Sergey", ""], ["Zhang", "Leishi", ""], ["Holroyd", "Alexander", ""]]}, {"id": "1608.02674", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Further Algebraic Algorithms in the Congested Clique Model and\n  Applications to Graph-Theoretic Problems", "comments": "29 pages; accepted to DISC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Censor-Hillel et al. [PODC'15] recently showed how to efficiently implement\ncentralized algebraic algorithms for matrix multiplication in the congested\nclique model, a model of distributed computing that has received increasing\nattention in the past few years. This paper develops further algebraic\ntechniques for designing algorithms in this model. We present deterministic and\nrandomized algorithms, in the congested clique model, for efficiently computing\nmultiple independent instances of matrix products, computing the determinant,\nthe rank and the inverse of a matrix, and solving systems of linear equations.\nAs applications of these techniques, we obtain more efficient algorithms for\nthe computation, again in the congested clique model, of the all-pairs shortest\npaths and the diameter in directed and undirected graphs with small weights,\nimproving over Censor-Hillel et al.'s work. We also obtain algorithms for\nseveral other graph-theoretic problems such as computing the number of edges in\na maximum matching and the Gallai-Edmonds decomposition of a simple graph, and\ncomputing a minimum vertex cover of a bipartite graph.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 02:08:28 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1608.02683", "submitter": "Shishir Kolathaya", "authors": "Shishir Kolathaya, Benjamin J. Morris, Ryan W. Sinnet, Aaron D. Ames", "title": "System Identification and Control of Valkyrie through SVA--Based\n  Regressor Computation", "comments": "8 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates simultaneous identification and control of the\nhumanoid robot, Valkyrie, utilizing Spatial Vector Algebra (SVA). In\nparticular, the inertia, Coriolis-centrifugal and gravity terms for the\ndynamics of a robot are computed using spatial inertia tensors. With the\nassumption that the link lengths or the distance between the joint axes are\naccurately known, it will be shown that inertial properties of a robot can be\ndirectly evaluated from the inertia tensor. An algorithm is proposed to\nevaluate the regressor, yielding a run time of $O(n^2)$. The efficiency of this\nalgorithm yields a means for online system identification via the SVA--based\nregressor and, as a byproduct, a method for accurate model-based control.\nExperimental validation of the proposed method is provided through its\nimplementation in three case studies: offline identification of a double\npendulum and a $4$-DOF robotic leg, and online identification and control of a\n$4$-DOF robotic arm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 03:10:05 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 16:10:17 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 15:55:24 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kolathaya", "Shishir", ""], ["Morris", "Benjamin J.", ""], ["Sinnet", "Ryan W.", ""], ["Ames", "Aaron D.", ""]]}, {"id": "1608.02709", "submitter": "Feng Shi", "authors": "Feng Shi, Jianer Chen, Qilong Feng and Jianxin Wang", "title": "Parameterized Algorithms for the Maximum Agreement Forest Problem on\n  Multiple Rooted Multifurcating Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Agreement Forest problem has been extensively studied in\nphylogenetics. Most previous work is on two binary phylogenetic trees. In this\npaper, we study a generalized version of the problem: the Maximum Agreement\nForest problem on multiple rooted multifurcating phylogenetic trees, from the\nperspective of fixed-parameter algorithms. By taking advantage of a new\nbranch-and-bound strategy, two parameterized algorithms, with running times\n$O(2.42^k m^3 n^4)$ and $O(2.74^k m^3 n^5)$, respectively, are presented for\nthe hard version and the soft version of the problem, which correspond to two\ndifferent biological meanings to the polytomies in multifurcating phylogenetic\ntrees.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 07:39:01 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 06:57:11 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Shi", "Feng", ""], ["Chen", "Jianer", ""], ["Feng", "Qilong", ""], ["Wang", "Jianxin", ""]]}, {"id": "1608.02895", "submitter": "Ori Gurel-Gurevich", "authors": "Raaz Dwivedi, Ohad N. Feldheim, Ori Gurel-Gurevich and Aaditya Ramdas", "title": "The power of online thinning in reducing discrepancy", "comments": "22 pages, 3 figures. Expanded version including multidimensional\n  results. Some results regarding 1+\\beta\\ thinning were deferred to a separate\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an infinite sequence of independent, uniformly chosen points from\n$[0,1]^d$. After looking at each point in the sequence, an overseer is allowed\nto either keep it or reject it, and this choice may depend on the locations of\nall previously kept points. However, the overseer must keep at least one of\nevery two consecutive points. We call a sequence generated in this fashion a\n\\emph{two-thinning} sequence. Here, the purpose of the overseer is to control\nthe discrepancy of the empirical distribution of points, that is, after\nselecting $n$ points, to reduce the maximal deviation of the number of points\ninside any axis-parallel hyper-rectangle of volume $A$ from $nA$. Our main\nresult is an explicit low complexity two-thinning strategy which guarantees\ndiscrepancy of $O(\\log^{2d+1} n)$ for all $n$ with high probability (compare\nwith $\\Theta(\\sqrt{n\\log\\log n})$ without thinning). The case $d=1$ of this\nresult answers a question of Benjamini.\n  We also extend the construction to achieve the same asymptotic bound for\n($1+\\beta$)-thinning, a set-up in which rejecting is only allowed with\nprobability $\\beta$ independently for each point. In addition, we suggest an\nimproved and simplified strategy which we conjecture to guarantee discrepancy\nof $O(\\log^{d+1} n)$ (compare with $\\theta(\\log^d n)$, the best known\nconstruction of a low discrepancy sequence). Finally, we provide theoretical\nand empirical evidence for our conjecture, and provide simulations supporting\nthe viability of our construction for applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 18:07:43 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 08:16:26 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 18:29:24 GMT"}, {"version": "v4", "created": "Mon, 4 Sep 2017 10:46:34 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Feldheim", "Ohad N.", ""], ["Gurel-Gurevich", "Ori", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1608.03017", "submitter": "Yuichi Yoshida", "authors": "Hubie Chen, Matt Valeriote, Yuichi Yoshida", "title": "Testing Assignments to Constraint Satisfaction Problems", "comments": "An extended abstract will appear in the proceedings of FOCS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finite relational structure A, let CSP(A) denote the CSP instances\nwhose constraint relations are taken from A. The resulting family of problems\nCSP(A) has been considered heavily in a variety of computational contexts. In\nthis article, we consider this family from the perspective of property testing:\ngiven an instance of a CSP and query access to an assignment, one wants to\ndecide whether the assignment satisfies the instance, or is far from so doing.\nWhile previous works on this scenario studied concrete templates or restricted\nclasses of structures, this article presents comprehensive classification\ntheorems.\n  Our first contribution is a dichotomy theorem completely characterizing the\nstructures A such that CSP(A) is constant-query testable: (i) If A has a\nmajority polymorphism and a Maltsev polymorphism, then CSP(A) is constant-query\ntestable with one-sided error. (ii) Else, testing CSP(A) requires a\nsuper-constant number of queries. Let $\\exists$CSP(A) denote the extension of\nCSP(A) to instances which may include existentially quantified variables.\n  Our second contribution is to classify all structures A in terms of the\nnumber of queries needed to test assignments to instances of $\\exists$CSP(A),\nwith one-sided error. More specifically, we show the following trichotomy: (i)\nIf A has a majority polymorphism and a Maltsev polymorphism, then\n$\\exists$CSP(A) is constant-query testable with one-sided error. (ii) Else, if\nA has a $(k + 1)$-ary near-unanimity polymorphism for some $k \\geq 2$, and no\nMaltsev polymorphism then $\\exists$CSP(A) is not constant-query testable (even\nwith two-sided error) but is sublinear-query testable with one-sided error.\n(iii) Else, testing $\\exists$CSP(A) with one-sided error requires a linear\nnumber of queries.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:15:42 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Chen", "Hubie", ""], ["Valeriote", "Matt", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1608.03078", "submitter": "Grzegorz Gutowski", "authors": "Grzegorz Gutowski, Patryk Mikos", "title": "Lower Bounds for On-line Interval Coloring with Vector and Cardinality\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two strategies for Presenter in the on-line interval graph\ncoloring games. Specifically, we consider a setting in which each interval is\nassociated with a $d$-dimensional vector of weights and the coloring needs to\nsatisfy the $d$-dimensional bandwidth constraint, and the $k$-cardinality\nconstraint. Such a variant was first introduced by Epstein and Levy and it is a\nnatural model for resource-aware task scheduling with $d$ different shared\nresources where at most $k$ tasks can be scheduled simultaneously on a single\nmachine.\n  The first strategy forces any on-line interval coloring algorithm to use at\nleast $(5m-3)\\frac{d}{\\log d + 3}$ different colors on an $m(\\frac{d}{k} +\n\\log{d} + 3)$-colorable set of intervals. The second strategy forces any\non-line interval coloring algorithm to use at least\n$\\lfloor\\frac{5m}{2}\\rfloor\\frac{d}{\\log d + 3}$ different colors on an\n$m(\\frac{d}{k} + \\log{d} + 3)$-colorable set of unit intervals.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 08:31:25 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Gutowski", "Grzegorz", ""], ["Mikos", "Patryk", ""]]}, {"id": "1608.03118", "submitter": "Graham Cormode", "authors": "Graham Cormode and Hossein Jowhari and Morteza Monemizadeh and S.\n  Muthukrishnan", "title": "The Sparse Awakens: Streaming Algorithms for Matching Size Estimation in\n  Sparse Graphs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the size of the maximum matching is a canonical problem in graph\nalgorithms, and one that has attracted extensive study over a range of\ndifferent computational models. We present improved streaming algorithms for\napproximating the size of maximum matching with sparse (bounded arboricity)\ngraphs.\n  * Insert-Only Streams: We present a one-pass algorithm that takes O(c log^2\nn) space and approximates the size of the maximum matching in graphs with\narboricity c within a factor of O(c). This improves significantly on the\nstate-of-the-art O~(cn^{2/3})-space streaming algorithms.\n  * Dynamic Streams: Given a dynamic graph stream (i.e., inserts and deletes)\nof edges of an underlying c-bounded arboricity graph, we present a one-pass\nalgorithm that uses space O~(c^{10/3}n^{2/3}) and returns an O(c)-estimator for\nthe size of the maximum matching. This algorithm improves the state-of-the-art\nO~(cn^{4/5})-space algorithms, where the O~(.) notation hides logarithmic in\n$n$ dependencies.\n  In contrast to the previous works, our results take more advantage of the\nstreaming access to the input and characterize the matching size based on the\nordering of the edges in the stream in addition to the degree distributions and\nstructural properties of the sparse graphs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 10:40:22 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 18:41:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cormode", "Graham", ""], ["Jowhari", "Hossein", ""], ["Monemizadeh", "Morteza", ""], ["Muthukrishnan", "S.", ""]]}, {"id": "1608.03182", "submitter": "Allan Borodin", "authors": "Nicolas Pena and Allan Borodin", "title": "On the limitations of deterministic de-randomizations for online\n  bipartite matching and max-sat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surprising results of Karp, Vazirani and Vazirani and (respectively)\nBuchbinder et al are examples where rather simple randomizations provide\nprovably better approximations than the corresponding deterministic\ncounterparts for online bipartite matching and (respectively) unconstrained\nnon-monotone submodular maximization. We show that seemingly strong extensions\nof the deterministic online computation model can at best match the performance\nof naive randomization. More specifically, for bipartite matching, we show that\nin the priority model (allowing very general ways to order the input stream),\nwe cannot improve upon the trivial 1/2-approximation achieved by any greedy\nmaximal matching algorithm and likewise cannot improve upon this approximation\nby any log n/log log n number of online algorithms running in parallel. The\nlatter result yields an improved log log n - log log log n lower bound for the\nnumber of advice bits needed. For max-sat, we adapt the recent de-randomization\napproach of Buchbinder and Feldman applied to the Buchbinbder et al algorithm\nfor max-sat to obtain a deterministic 3/4-approximation algorithm using width\n2n parallelism. In order to improve upon this approximation, we show that\nexponential width parallelism of online algorithms is necessary (in a model\nthat is more general than what is needed for the width 2n algorithm).\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 14:07:14 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Pena", "Nicolas", ""], ["Borodin", "Allan", ""]]}, {"id": "1608.03220", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari and Hsin-Hao Su", "title": "Distributed Degree Splitting, Edge Coloring, and Orientations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of closely-related distributed graph problems, which we\ncall degree splitting, where roughly speaking the objective is to partition (or\norient) the edges such that each node's degree is split almost uniformly. Our\nfindings lead to answers for a number of problems, a sampling of which\nincludes:\n  -- We present a $poly(\\log n)$ round deterministic algorithm for\n$(2\\Delta-1)\\cdot (1+o(1))$-edge-coloring, where $\\Delta$ denotes the maximum\ndegree. Modulo the $1+o(1)$ factor, this settles one of the long-standing open\nproblems of the area from the 1990's (see e.g. Panconesi and Srinivasan\n[PODC'92]). Indeed, a weaker requirement of $(2\\Delta-1)\\cdot poly(\\log\n\\Delta)$-edge-coloring in $poly(\\log n)$ rounds was asked for in the 4th open\nquestion in the Distributed Graph Coloring book by Barenboim and Elkin.\n  -- We show that sinkless orientation---i.e., orienting edges such that each\nnode has at least one outgoing edge---on $\\Delta$-regular graphs can be solved\nin $O(\\log_{\\Delta} \\log n)$ rounds randomized and in $O(\\log_{\\Delta} n)$\nrounds deterministically. These prove the corresponding lower bounds by Brandt\net al. [STOC'16] and Chang, Kopelowitz, and Pettie [FOCS'16] to be tight.\nMoreover, these show that sinkless orientation exhibits an exponential\nseparation between its randomized and deterministic complexities, akin to the\nresults of Chang et al. for $\\Delta$-coloring $\\Delta$-regular trees.\n  -- We present a randomized $O(\\log^4 n)$ round algorithm for orienting\n$a$-arboricity graphs with maximum out-degree $a(1+\\epsilon)$. This can be also\nturned into a decomposition into $a (1+\\epsilon)$ forests when $a=\\Omega(\\log\nn)$ and into $a (1+\\epsilon)$ pseduo-forests when $a=o(\\log n)$. Obtaining an\nefficient distributed decomposition into less than $2a$ forests was stated as\nthe 10th open problem in the book by Barenboim and Elkin.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 15:53:24 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1608.03270", "submitter": "Richard Peng", "authors": "Michael B. Cohen, Jon Kelner, John Peebles, Richard Peng, Aaron\n  Sidford, Adrian Vladu", "title": "Faster Algorithms for Computing the Stationary Distribution, Simulating\n  Random Walks, and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide faster algorithms for computing various fundamental\nquantities associated with random walks on a directed graph, including the\nstationary distribution, personalized PageRank vectors, hitting times, and\nescape probabilities. In particular, on a directed graph with $n$ vertices and\n$m$ edges, we show how to compute each quantity in time\n$\\tilde{O}(m^{3/4}n+mn^{2/3})$, where the $\\tilde{O}$ notation suppresses\npolylogarithmic factors in $n$, the desired accuracy, and the appropriate\ncondition number (i.e. the mixing time or restart probability).\n  Our result improves upon the previous fastest running times for these\nproblems; previous results either invoke a general purpose linear system solver\non a $n\\times n$ matrix with $m$ non-zero entries, or depend polynomially on\nthe desired error or natural condition number associated with the problem (i.e.\nthe mixing time or restart probability). For sparse graphs, we obtain a running\ntime of $\\tilde{O}(n^{7/4})$, breaking the $O(n^{2})$ barrier of the best\nrunning time one could hope to achieve using fast matrix multiplication.\n  We achieve our result by providing a similar running time improvement for\nsolving directed Laplacian systems, a natural directed or asymmetric analog of\nthe well studied symmetric or undirected Laplacian systems. We show how to\nsolve such systems in time $\\tilde{O}(m^{3/4}n+mn^{2/3})$, and efficiently\nreduce a broad range of problems to solving $\\tilde{O}(1)$ directed Laplacian\nsystems on Eulerian graphs. We hope these results and our analysis open the\ndoor for further study into directed spectral graph theory.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 19:58:57 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 19:58:26 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Cohen", "Michael B.", ""], ["Kelner", "Jon", ""], ["Peebles", "John", ""], ["Peng", "Richard", ""], ["Sidford", "Aaron", ""], ["Vladu", "Adrian", ""]]}, {"id": "1608.03299", "submitter": "Guohui Lin", "authors": "Zhi-Zhong Chen, Guohui Lin, Lusheng Wang, Yong Chen, Dan Wang", "title": "Approximation algorithms for the maximum weight internal spanning tree\n  problem", "comments": "Revised complete version, with 27 pages, 27 figures; an extended\n  abstract appears in the Proceedings of COCOON 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex-weighted connected graph $G = (V, E)$, the maximum weight\ninternal spanning tree (MwIST for short) problem asks for a spanning tree $T$\nof $G$ such that the total weight of the internal vertices in $T$ is maximized.\nThe un-weighted variant, denoted as MIST, is NP-hard and APX-hard, and the\ncurrently best approximation algorithm has a proven performance ratio $13/17$.\nThe currently best approximation algorithm for MwIST only has a performance\nratio $1/3 - \\epsilon$, for any $\\epsilon > 0$. In this paper, we present a\nsimple algorithm based on a novel relationship between MwIST and the maximum\nweight matching, and show that it achieves a better approximation ratio of\n$1/2$. When restricted to claw-free graphs, a special case been previously\nstudied, we design a $7/12$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 20:43:39 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 20:14:49 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Chen", "Zhi-Zhong", ""], ["Lin", "Guohui", ""], ["Wang", "Lusheng", ""], ["Chen", "Yong", ""], ["Wang", "Dan", ""]]}, {"id": "1608.03368", "submitter": "Akbar Rafiey", "authors": "Pavol Hell and Akbar Rafiey and Arash Rafiey", "title": "Bi-Arc Digraphs and Conservative Polymorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the class of bi-arc digraphs, important from two\nseemingly unrelated perspectives. On the one hand, they are precisely the\ndigraphs that admit certain polymorphisms of interest in the study of\nconstraint satisfaction problems; on the other hand, they are a very broad\ngeneralization of interval graphs.\n  Bi-arc digraphs is the class of digraphs that admit conservative semilattice\npolymorphisms. There is much interest in understanding structures that admit\nparticular types of polymorphisms, and especially in their recognition\nalgorithms. (Such problems are referred to as metaproblems.) Surprisingly, the\nclass of bi-arc digraphs also describes the class of digraphs that admit\ncertain other kinds of conservative polymorphisms. Thus solving the recognition\nproblem for bi-arc digraphs solves the metaproblem for digraphs for several\ntypes of conservative polymorphisms. The complexity of the recognition problem\nfor digraphs with conservative semilattice polymorphisms was an open problem,\nwhile it was known to be NP-complete for certain more complex relational\nstructures. We complement our result by providing a complete dichotomy\nclassification of which general relational structures have polynomial or\nNP-complete recognition problems for the existence of conservative semilattice\npolymorphisms.\n  Bi-arc digraphs also generalizes the class of interval graphs; in fact it\nreduces to the class of interval graphs for symmetric and reflexive digraphs.\nIt is much broader than interval graphs and includes other generalizations of\ninterval graphs such as co-threshold tolerance graphs and adjusted interval\ndigraphs. Yet, it is still a reasonable extension of interval graphs, in the\nsense that it keeps much of the appeal of interval graphs.\n  Our main result is a forbidden obstruction characterization of, and a\npolynomial recognition for, the class of bi-arc digraphs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 04:53:53 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 23:57:57 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2016 02:53:32 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 20:24:57 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2020 09:27:13 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hell", "Pavol", ""], ["Rafiey", "Akbar", ""], ["Rafiey", "Arash", ""]]}, {"id": "1608.03439", "submitter": "Jesper Nederlof", "authors": "Jesper Nederlof", "title": "Finding Large Set Covers Faster via the Representation Method", "comments": "20 pages, to appear at ESA'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worst-case fastest known algorithm for the Set Cover problem on universes\nwith $n$ elements still essentially is the simple $O^*(2^n)$-time dynamic\nprogramming algorithm, and no non-trivial consequences of an $O^*(1.01^n)$-time\nalgorithm are known. Motivated by this chasm, we study the following natural\nquestion: Which instances of Set Cover can we solve faster than the simple\ndynamic programming algorithm? Specifically, we give a Monte Carlo algorithm\nthat determines the existence of a set cover of size $\\sigma n$ in\n$O^*(2^{(1-\\Omega(\\sigma^4))n})$ time. Our approach is also applicable to Set\nCover instances with exponentially many sets: By reducing the task of finding\nthe chromatic number $\\chi(G)$ of a given $n$-vertex graph $G$ to Set Cover in\nthe natural way, we show there is an $O^*(2^{(1-\\Omega(\\sigma^4))n})$-time\nrandomized algorithm that given integer $s=\\sigma n$, outputs NO if $\\chi(G) >\ns$ and YES with constant probability if $\\chi(G)\\leq s-1$.\n  On a high level, our results are inspired by the `representation method' of\nHowgrave-Graham and Joux~[EUROCRYPT'10] and obtained by only evaluating a\nrandomly sampled subset of the table entries of a dynamic programming\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 12:48:39 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Nederlof", "Jesper", ""]]}, {"id": "1608.03580", "submitter": "Erik Waingarten", "authors": "Alexandr Andoni and Thijs Laarhoven and Ilya Razenshteyn and Erik\n  Waingarten", "title": "Optimal Hashing-based Time-Space Trade-offs for Approximate Near\n  Neighbors", "comments": "62 pages, 5 figures; a merger of arXiv:1511.07527 [cs.DS] and\n  arXiv:1605.02701 [cs.DS], which subsumes both of the preprints. New version\n  contains more elaborated proofs and fixed some typos", "journal-ref": null, "doi": "10.1137/1.9781611974782.4", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n$c$-Approximate Near Neighbor Search problem. For the $d$-dimensional Euclidean\nspace and $n$-point datasets, we develop a data structure with space $n^{1 +\n\\rho_u + o(1)} + O(dn)$ and query time $n^{\\rho_q + o(1)} + d n^{o(1)}$ for\nevery $\\rho_u, \\rho_q \\geq 0$ such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor $c > 1$, improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for $\\rho_q = 0$, improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:50:00 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 16:57:47 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Andoni", "Alexandr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1608.03611", "submitter": "Huy Nguyen", "authors": "Alina Ene, Huy L. Nguyen", "title": "Constrained Submodular Maximization: Beyond 1/e", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new algorithm for maximizing a non-monotone\nsubmodular function subject to a general constraint. Our algorithm finds an\napproximate fractional solution for maximizing the multilinear extension of the\nfunction over a down-closed polytope. The approximation guarantee is 0.372 and\nit is the first improvement over the 1/e approximation achieved by the unified\nContinuous Greedy algorithm [Feldman et al., FOCS 2011].\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 20:36:45 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1608.03643", "submitter": "Santosh Vempala", "authors": "Ravi Kannan and Santosh Vempala", "title": "Chi-squared Amplification: Identifying Hidden Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following general hidden hubs model: an $n \\times n$ random\nmatrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows\noutside $S$ are generated from the probability distribution $p_0 \\sim\nN(0,\\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated\nfrom $p_1 \\sim N(0,\\sigma_1^2)$, $\\sigma_1>\\sigma_0$, and the rest of the\nentries from $p_0$. The problem is to identify the high-degree hubs\nefficiently. This model includes and significantly generalizes the planted\nGaussian Submatrix Model, where the special entries are all in a $k \\times k$\nsubmatrix. There are two well-known barriers: if $k\\geq c\\sqrt{n\\ln n}$, just\nthe row sums are sufficient to find $S$ in the general model. For the submatrix\nproblem, this can be improved by a $\\sqrt{\\ln n}$ factor to $k \\ge c\\sqrt{n}$\nby spectral methods or combinatorial methods. In the variant with $p_0=\\pm 1$\n(with probability $1/2$ each) and $p_1\\equiv 1$, neither barrier has been\nbroken.\n  We give a polynomial-time algorithm to identify all the hidden hubs with high\nprobability for $k \\ge n^{0.5-\\delta}$ for some $\\delta >0$, when\n$\\sigma_1^2>2\\sigma_0^2$. The algorithm extends to the setting where planted\nentries might have different variances each at least as large as $\\sigma_1^2$.\nWe also show a nearly matching lower bound: for $\\sigma_1^2 \\le 2\\sigma_0^2$,\nthere is no polynomial-time Statistical Query algorithm for distinguishing\nbetween a matrix whose entries are all from $N(0,\\sigma_0^2)$ and a matrix with\n$k=n^{0.5-\\delta}$ hidden hubs for any $\\delta >0$. The lower bound as well as\nthe algorithm are related to whether the chi-squared distance of the two\ndistributions diverges. At the critical value $\\sigma_1^2=2\\sigma_0^2$, we show\nthat the general hidden hubs problem can be solved for $k\\geq c\\sqrt n(\\ln\nn)^{1/4}$, improving on the naive row sum-based method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 00:36:42 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 16:31:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kannan", "Ravi", ""], ["Vempala", "Santosh", ""]]}, {"id": "1608.03680", "submitter": "Hung-I Yu", "authors": "Hung-I Yu and Tien-Ching Lin and D. T. Lee", "title": "The $(1|1)_R$-Centroid Problem on the Plane", "comments": "27 pages, 6 figures. A preliminary version of this paper has been\n  submitted to ISAAC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1982, Drezner proposed the (1|1)-centroid problem on the plane, in which\ntwo players, called the leader and the follower, open facilities to provide\nservice to customers in a competitive manner. The leader opens the first\nfacility, and then the follower opens the second. Each customer will patronize\nthe facility closest to him (ties broken in favor of the leader's one), thereby\ndecides the market share of the two players. The goal is to find the best\nposition for the leader's facility so that his market share is maximized. The\nbest algorithm for this problem is an $O(n^2 \\log n)$-time parametric search\napproach, which searches over the space of possible market share values.\n  In the same paper, Drezner also proposed a general version of (1|1)-centroid\nproblem by introducing a minimal distance constraint $R$, such that the\nfollower's facility is not allowed to be located within a distance $R$ from the\nleader's. He proposed an $O(n^5 \\log n)$-time algorithm for this general\nversion by identifying $O(n^4)$ points as the candidates of the optimal\nsolution and checking the market share for each of them. In this paper, we\ndevelop a new parametric search approach searching over the $O(n^4)$ candidate\npoints, and present an $O(n^2 \\log n)$-time algorithm for the general version,\nthereby close the $O(n^3)$ gap between the two bounds.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 05:09:05 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Yu", "Hung-I", ""], ["Lin", "Tien-Ching", ""], ["Lee", "D. T.", ""]]}, {"id": "1608.04036", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Greedy Maximization Framework for Graph-based Influence Functions", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of graph-based submodular maximization problems was initiated in a\nseminal work of Kempe, Kleinberg, and Tardos (2003): An {\\em influence}\nfunction of subsets of nodes is defined by the graph structure and the aim is\nto find subsets of seed nodes with (approximately) optimal tradeoff of size and\ninfluence. Applications include viral marketing, monitoring, and active\nlearning of node labels. This powerful formulation was studied for\n(generalized) {\\em coverage} functions, where the influence of a seed set on a\nnode is the maximum utility of a seed item to the node, and for pairwise {\\em\nutility} based on reachability, distances, or reverse ranks.\n  We define a rich class of influence functions which unifies and extends\nprevious work beyond coverage functions and specific utility functions. We\npresent a meta-algorithm for approximate greedy maximization with strong\napproximation quality guarantees and worst-case near-linear computation for all\nfunctions in our class. Our meta-algorithm generalizes a recent design by Cohen\net al (2014) that was specific for distance-based coverage functions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 23:38:42 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 19:11:53 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1608.04132", "submitter": "Carlo Comin", "authors": "Massimo Cairo, Carlo Comin, Romeo Rizzi", "title": "Instantaneous Reaction-Time in Dynamic-Consistency Checking of\n  Conditional Simple Temporal Networks -- Extended version with an Improved\n  Upper Bound --", "comments": "23rd International Symposium on Temporal Representation and Reasoning\n  (TIME2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CSTNs is a constraint-based graph-formalism for conditional temporal\nplanning. In order to address the DC-Checking problem, in [Comin and Rizzi,\nTIME 2015] we introduced epsilon-DC (a refined, more realistic, notion of DC),\nand provided an algorithmic solution to it. The epsilon-DC notion is\ninteresting per se, and the epsilon-DC-Checking algorithm in [Comin and Rizzi,\nTIME 2015] rests on the assumption that the reaction-time satisfies epsilon >\n0; leaving unsolved the question of what happens when epsilon = 0. In this\nwork, we introduce and study pi-DC, a sound notion of DC with an instantaneous\nreaction-time (i.e. one in which the planner can react to any observation at\nthe same instant of time in which the observation is made). Firstly, we\ndemonstrate by a counter-example that pi-DC is not equivalent to 0-DC, and that\n0-DC is actually inadequate for modeling DC with an instantaneous\nreaction-time. This shows that the main results obtained in our previous work\ndo not apply directly, as they were formulated, to the case of epsilon=0.\nMotivated by this observation, as a second contribution, our previous tools are\nextended in order to handle pi-DC, and the notion of ps-tree is introduced,\nalso pointing out a relationship between pi-DC and HyTN-Consistency. Thirdly, a\nsimple reduction from pi-DC-Checking to DC-Checking is identified. This allows\nus to design and to analyze the first sound-and-complete pi-DC-Checking\nprocedure. Remarkably, the time complexity of the proposed algorithm remains\n(pseudo) singly-exponential in the number of propositional letters. Finally, it\nis observed that the technique can be leveraged to actually reduce from pi-DC\nto 1-DC, this allows us to further improve the exponents in the time complexity\nof pi-DC-Checking.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 19:54:16 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 15:04:47 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 19:52:13 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2018 17:16:33 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Cairo", "Massimo", ""], ["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1608.04223", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "A Faster Approximation Algorithm for the Gibbs Partition Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the partition function $Z(\\beta)=\\sum_x\n\\exp(-\\beta(H(x))$ of a Gibbs distribution with a Hamilton $H(\\cdot)$, or more\nprecisely the logarithm of the ratio $q=\\ln Z(0)/Z(\\beta)$. It has been\nrecently shown how to approximate $q$ with high probability assuming the\nexistence of an oracle that produces samples from the Gibbs distribution for a\ngiven parameter value in $[0,\\beta]$. The current best known approach due to\nHuber [9] uses $O(q\\ln n\\cdot[\\ln q + \\ln \\ln n+\\varepsilon^{-2}])$ oracle\ncalls on average where $\\varepsilon$ is the desired accuracy of approximation\nand $H(\\cdot)$ is assumed to lie in $\\{0\\}\\cup[1,n]$. We improve the complexity\nto $O(q\\ln n\\cdot\\varepsilon^{-2})$ oracle calls. We also show that the same\ncomplexity can be achieved if exact oracles are replaced with approximate\nsampling oracles that are within $O(\\frac{\\varepsilon^2}{q\\ln n})$ variation\ndistance from exact oracles. Finally, we prove a lower bound of $\\Omega(q\\cdot\n\\varepsilon^{-2})$ oracle calls under a natural model of computation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 10:05:44 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 06:49:39 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 09:14:07 GMT"}, {"version": "v4", "created": "Wed, 27 Dec 2017 10:26:12 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "1608.04355", "submitter": "Josh Alman", "authors": "Josh Alman, Timothy M. Chan and Ryan Williams", "title": "Polynomial Representations of Threshold Functions and Algorithmic\n  Applications", "comments": "30 pages. To appear in 57th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design new polynomials for representing threshold functions in three\ndifferent regimes: probabilistic polynomials of low degree, which need far less\nrandomness than previous constructions, polynomial threshold functions (PTFs)\nwith \"nice\" threshold behavior and degree almost as low as the probabilistic\npolynomials, and a new notion of probabilistic PTFs where we combine the above\ntechniques to achieve even lower degree with similar \"nice\" threshold behavior.\nUtilizing these polynomial constructions, we design faster algorithms for a\nvariety of problems:\n  $\\bullet$ Offline Hamming Nearest (and Furthest) Neighbors: Given $n$ red and\n$n$ blue points in $d$-dimensional Hamming space for $d=c\\log n$, we can find\nan (exact) nearest (or furthest) blue neighbor for every red point in\nrandomized time $n^{2-1/O(\\sqrt{c}\\log^{2/3}c)}$ or deterministic time\n$n^{2-1/O(c\\log^2c)}$. These also lead to faster MAX-SAT algorithms for sparse\nCNFs.\n  $\\bullet$ Offline Approximate Nearest (and Furthest) Neighbors: Given $n$ red\nand $n$ blue points in $d$-dimensional $\\ell_1$ or Euclidean space, we can find\na $(1+\\epsilon)$-approximate nearest (or furthest) blue neighbor for each red\npoint in randomized time near\n$dn+n^{2-\\Omega(\\epsilon^{1/3}/\\log(1/\\epsilon))}$.\n  $\\bullet$ SAT Algorithms and Lower Bounds for Circuits With Linear Threshold\nFunctions: We give a satisfiability algorithm for $AC^0[m]\\circ LTF\\circ LTF$\ncircuits with a subquadratic number of linear threshold gates on the bottom\nlayer, and a subexponential number of gates on the other layers, that runs in\ndeterministic $2^{n-n^\\epsilon}$ time. This also implies new circuit lower\nbounds for threshold circuits. We also give a randomized\n$2^{n-n^\\epsilon}$-time SAT algorithm for subexponential-size $MAJ\\circ\nAC^0\\circ LTF\\circ AC^0\\circ LTF$ circuits, where the top $MAJ$ gate and middle\n$LTF$ gates have $O(n^{6/5-\\delta})$ fan-in.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:28:47 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Alman", "Josh", ""], ["Chan", "Timothy M.", ""], ["Williams", "Ryan", ""]]}, {"id": "1608.04431", "submitter": "Richard Barnes", "authors": "Richard Barnes", "title": "Parallel Non-divergent Flow Accumulation For Trillion Cell Digital\n  Elevation Models On Desktops Or Clusters", "comments": "23 pages (double-spaced), 4 figures, 2 tables. arXiv admin note:\n  substantial text overlap with arXiv:1606.06204", "journal-ref": "2017. Environmental Modelling & Software 92, 202-212", "doi": "10.1016/j.envsoft.2017.02.022", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continent-scale datasets challenge hydrological algorithms for processing\ndigital elevation models. Flow accumulation is an important input for many such\nalgorithms; here, I parallelize its calculation. The new algorithm works on one\nor many cores, or multiple machines, and can take advantage of large memories\nor cope with small ones. Unlike previous parallel algorithms, the new algorithm\nguarantees a fixed number of memory access and communication events per raster\ncell. In testing, the new algorithm ran faster and used fewer resources than\nprevious algorithms, exhibiting ~30% strong and weak scaling efficiencies up to\n48 cores and linear scaling across datasets ranging over three orders of\nmagnitude. The largest dataset tested had two trillion (2*10^12) cells. With 48\ncores, processing required 24 minutes wall-time (14.5 compute-hours). This test\nis three orders of magnitude larger than any previously performed in the\nliterature. Complete, well-commented source code and correctness tests are\navailable on Github.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:50:48 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 01:50:55 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Barnes", "Richard", ""]]}, {"id": "1608.04456", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "An Improved Algorithm for Diameter-Optimally Augmenting Paths in a\n  Metric Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a path graph of $n$ vertices embedded in a metric space. We\nconsider the problem of adding a new edge to $P$ such that the diameter of the\nresulting graph is minimized. Previously (in ICALP 2015) the problem was solved\nin $O(n\\log^3 n)$ time. In this paper, based on new observations and different\nalgorithmic techniques, we present an $O(n\\log n)$ time algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 01:35:46 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1608.04481", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Lecture Notes on Randomized Linear Algebra", "comments": "188 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are lecture notes that are based on the lectures from a class I taught\non the topic of Randomized Linear Algebra (RLA) at UC Berkeley during the Fall\n2013 semester.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 04:55:26 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1608.04535", "submitter": "Hang Zhou", "authors": "Fabrice Benhamouda, Tancr\\`ede Lepoint, Claire Mathieu, Hang Zhou", "title": "Optimization of Bootstrapping in Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2009, Gentry proposed the first Fully Homomorphic Encryption (FHE) scheme,\nan extremely powerful cryptographic primitive that enables to perform\ncomputations, i.e., to evaluate circuits, on encrypted data without decrypting\nthem first. This has many applications, in particular in cloud computing.\n  In all currently known FHE schemes, encryptions are associated to some\n(non-negative integer) noise level, and at each evaluation of an AND gate, the\nnoise level increases. This is problematic because decryption can only work if\nthe noise level stays below some maximum level $L$ at every gate of the\ncircuit. To ensure that property, it is possible to perform an operation called\n\\emph{bootstrapping} to reduce the noise level. However, bootstrapping is\ntime-consuming and has been identified as a critical operation. This motivates\na new problem in discrete optimization, that of choosing where in the circuit\nto perform bootstrapping operations so as to control the noise level; the goal\nis to minimize the number of bootstrappings in circuits.\n  In this paper, we formally define the \\emph{bootstrap problem}, we design a\npolynomial-time $L$-approximation algorithm using a novel method of rounding of\na linear program, and we show a matching hardness result:\n$(L-\\epsilon)$-inapproximability for any $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 09:59:35 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Benhamouda", "Fabrice", ""], ["Lepoint", "Tancr\u00e8de", ""], ["Mathieu", "Claire", ""], ["Zhou", "Hang", ""]]}, {"id": "1608.04759", "submitter": "Emmanouil Zampetakis", "authors": "Themistoklis Gouleakis, Christos Tzamos and Manolis Zampetakis", "title": "Faster Sublinear Algorithms using Conditional Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional sampling oracle for a probability distribution D returns\nsamples from the conditional distribution of D restricted to a specified subset\nof the domain. A recent line of work (Chakraborty et al. 2013 and Cannone et\nal. 2014) has shown that having access to such a conditional sampling oracle\nrequires only polylogarithmic or even constant number of samples to solve\ndistribution testing problems like identity and uniformity. This significantly\nimproves over the standard sampling model where polynomially many samples are\nnecessary.\n  Inspired by these results, we introduce a computational model based on\nconditional sampling to develop sublinear algorithms with exponentially faster\nruntimes compared to standard sublinear algorithms. We focus on geometric\noptimization problems over points in high dimensional Euclidean space. Access\nto these points is provided via a conditional sampling oracle that takes as\ninput a succinct representation of a subset of the domain and outputs a\nuniformly random point in that subset. We study two well studied problems:\nk-means clustering and estimating the weight of the minimum spanning tree. In\ncontrast to prior algorithms for the classic model, our algorithms have time,\nspace and sample complexity that is polynomial in the dimension and\npolylogarithmic in the number of points.\n  Finally, we comment on the applicability of the model and compare with\nexisting ones like streaming, parallel and distributed computational models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:03:58 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Gouleakis", "Themistoklis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1608.04769", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o, Luciano Gual\\`a, Stefano Leucci, Guido Proietti", "title": "Compact and Fast Sensitivity Oracles for Single-Source Distances", "comments": "19 pages, 3 figures. ESA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $s$ denote a distinguished source vertex of a non-negatively real\nweighted and undirected graph $G$ with $n$ vertices and $m$ edges. In this\npaper we present two efficient \\emph{single-source approximate-distance\nsensitivity oracles}, namely \\emph{compact} data structures which are able to\n\\emph{quickly} report an approximate (by a multiplicative stretch factor)\ndistance from $s$ to any node of $G$ following the failure of any edge in $G$.\nMore precisely, we first present a sensitivity oracle of size $O(n)$ which is\nable to report 2-approximate distances from the source in $O(1)$ time. Then, we\nfurther develop our construction by building, for any $0<\\epsilon<1$, another\nsensitivity oracle having size $O\\left(n\\cdot \\frac{1}{\\epsilon} \\log\n\\frac{1}{\\epsilon}\\right)$, and which is able to report a\n$(1+\\epsilon)$-approximate distance from $s$ to any vertex of $G$ in\n$O\\left(\\log n\\cdot \\frac{1}{\\epsilon} \\log \\frac{1}{\\epsilon}\\right)$ time.\nThus, this latter oracle is essentially optimal as far as size and stretch are\nconcerned, and it only asks for a logarithmic query time. Finally, our results\nare complemented with a space lower bound for the related class of\nsingle-source \\emph{additively-stretched} sensitivity oracles, which is helpful\nto realize the hardness of designing compact oracles of this type.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:38:20 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1608.04773", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev\n  Approximation", "comments": "title changed and minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve principal component regression (PCR), up to a multiplicative\naccuracy $1+\\gamma$, by reducing the problem to $\\tilde{O}(\\gamma^{-1})$\nblack-box calls of ridge regression. Therefore, our algorithm does not require\nany explicit construction of the top principal components, and is suitable for\nlarge-scale PCR instances. In contrast, previous result requires\n$\\tilde{O}(\\gamma^{-2})$ such black-box calls.\n  We obtain this result by developing a general stable recurrence formula for\nmatrix Chebyshev polynomials, and a degree-optimal polynomial approximation to\nthe matrix sign function. Our techniques may be of independent interests,\nespecially when designing iterative methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:48:02 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 19:35:38 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1608.04845", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Lecture Notes on Spectral Graph Methods", "comments": "257 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are lecture notes that are based on the lectures from a class I taught\non the topic of Spectral Graph Methods at UC Berkeley during the Spring 2015\nsemester.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:38:37 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1608.04852", "submitter": "Noriyuki Kurosawa", "authors": "Noriyuki Kurosawa", "title": "Quicksort with median of medians is considered practical", "comments": "6 pages, program source code in the C language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The linear pivot selection algorithm, known as median-of-medians, makes the\nworst case complexity of quicksort be $\\mathrm{O}(n\\ln n)$. Nevertheless, it\nhas often been said that this algorithm is too expensive to use in quicksort.\nIn this article, we show that we can make the quicksort with this kind of pivot\nselection approach be efficient.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 04:43:38 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Kurosawa", "Noriyuki", ""]]}, {"id": "1608.04906", "submitter": "Sebastian Wild", "authors": "Sebastian Wild", "title": "Quicksort Is Optimal For Many Equal Keys", "comments": "v4 is a major reorganization of sections; a shortened version appears\n  in the proceedings of ANALCO 2018", "journal-ref": null, "doi": "10.1137/1.9781611975062.2", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I prove that the average number of comparisons for median-of-$k$ Quicksort\n(with fat-pivot a.k.a. three-way partitioning) is asymptotically only a\nconstant $\\alpha_k$ times worse than the lower bound for sorting random\nmultisets with $\\Omega(n^\\varepsilon)$ duplicates of each value (for any\n$\\varepsilon>0$). The constant is $\\alpha_k = \\ln(2) /\n\\bigl(H_{k+1}-H_{(k+1)/2} \\bigr)$, which converges to 1 as $k\\to\\infty$, so\nQuicksort is asymptotically optimal for inputs with many duplicates. This\nresolves a conjecture by Sedgewick and Bentley (1999, 2002) and constitutes the\nfirst progress on the analysis of Quicksort with equal elements since\nSedgewick's 1977 article.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 09:41:53 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 11:27:59 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 13:48:23 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 17:00:19 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wild", "Sebastian", ""]]}, {"id": "1608.05100", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "In-Place Sparse Suffix Sorting", "comments": "ACM-SIAM Symposium on Discrete Algorithms 2018; arXiv admin note:\n  text overlap with arXiv:1607.06660 Comment: new style (lipics); using\n  Heath-Brown theorem for number of primes in Z; improved bounds for LCP array\n  computation and sparse suffix sorting; added construction of the LCE\n  structure using radix sort; added reference to lower bound for LCE query\n  times; uploaded version accepted at SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix arrays encode the lexicographical order of all suffixes of a text and\nare often combined with the Longest Common Prefix array (LCP) to simulate\nnavigational queries on the suffix tree in reduced space. In space-critical\napplications such as sparse and compressed text indexing, only information\nregarding the lexicographical order of a size-$b$ subset of all $n$ text\nsuffixes is often needed. Such information can be stored space-efficiently (in\n$b$ words) in the sparse suffix array (SSA). The SSA and its relative sparse\nLCP array (SLCP) can be used as a space-efficient substitute of the sparse\nsuffix tree. Very recently, Gawrychowski and Kociumaka [SODA 2017] showed that\nthe sparse suffix tree (and therefore SSA and SLCP) can be built in\nasymptotically optimal $O(b)$ space with a Monte Carlo algorithm running in\n$O(n)$ time. The main reason for using the SSA and SLCP arrays in place of the\nsparse suffix tree is, however, their reduced space of $b$ words each. This\nleads naturally to the quest for in-place algorithms building these arrays.\nFranceschini and Muthukrishnan [ICALP 2007] showed that the full suffix array\ncan be built in-place and in optimal running time. On the other hand, finding\nsub-quadratic in-place algorithms for building the SSA and SLCP for\n\\emph{general} subsets of suffixes has been an elusive task for decades. In\nthis paper, we give the first solution to this problem. We provide the first\nin-place algorithm building the full LCP array in $O(n\\log n)$ expected time\nand the first Monte Carlo in-place algorithms building the SSA and SLCP in $O(n\n+ b\\log^2 n)$ expected time. We moreover describe the first in-place solution\nfor the suffix selection problem: to compute the $i$-th smallest text suffix.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 20:54:07 GMT"}, {"version": "v10", "created": "Tue, 3 Oct 2017 07:40:44 GMT"}, {"version": "v11", "created": "Wed, 1 Nov 2017 10:57:39 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 14:53:16 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 09:03:17 GMT"}, {"version": "v4", "created": "Tue, 11 Oct 2016 11:04:06 GMT"}, {"version": "v5", "created": "Wed, 19 Oct 2016 15:45:00 GMT"}, {"version": "v6", "created": "Wed, 2 Nov 2016 13:54:27 GMT"}, {"version": "v7", "created": "Tue, 14 Feb 2017 13:36:18 GMT"}, {"version": "v8", "created": "Thu, 16 Feb 2017 10:29:45 GMT"}, {"version": "v9", "created": "Tue, 28 Feb 2017 12:42:08 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "1608.05152", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "Conditional Sparse Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and statistics typically focus on building models that\ncapture the vast majority of the data, possibly ignoring a small subset of data\nas \"noise\" or \"outliers.\" By contrast, here we consider the problem of jointly\nidentifying a significant (but perhaps small) segment of a population in which\nthere is a highly sparse linear regression fit, together with the coefficients\nfor the linear fit. We contend that such tasks are of interest both because the\nmodels themselves may be able to achieve better predictions in such special\ncases, but also because they may aid our understanding of the data. We give\nalgorithms for such problems under the sup norm, when this unknown segment of\nthe population is described by a k-DNF condition and the regression fit is\ns-sparse for constant k and s. For the variants of this problem when the\nregression fit is not so sparse or using expected error, we also give a\npreliminary algorithm and highlight the question as a challenge for future\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 01:30:49 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1608.05205", "submitter": "Ioannis Fudos", "authors": "Ioannis Fudos, Christoph M. Hoffmann, Robert Joan-Arinyo", "title": "Tree-decomposable and Underconstrained Geometric Constraint Problems", "comments": "This work was accepted to appear as a chapter, pending minor\n  revision, to the \"Handbook of Geometric Constraints Principles\", edited by\n  Meera Sitharam, Audrey St.John and Jessica Sidman, Mathematics series, CRC\n  press (Taylor and Francis group). To appear in 2017. Universitat Politecnica\n  de Catalunya, Research Report: http://hdl.handle.net/2117/91041", "journal-ref": null, "doi": null, "report-no": "University of Ioannina, CSE DEPT: TR-2016-3", "categories": "cs.GR cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with geometric constraint solvers, i.e., with\nprograms that find one or more solutions of a geometric constraint problem. If\nno solution exists, the solver is expected to announce that no solution has\nbeen found. Owing to the complexity, type or difficulty of a constraint\nproblem, it is possible that the solver does not find a solution even though\none may exist. Thus, there may be false negatives, but there should never be\nfalse positives. Intuitively, the ability to find solutions can be considered a\nmeasure of solver's competence. We consider static constraint problems and\ntheir solvers. We do not consider dynamic constraint solvers, also known as\ndynamic geometry programs, in which specific geometric elements are moved,\ninteractively or along prescribed trajectories, while continually maintaining\nall stipulated constraints. However, if we have a solver for static constraint\nproblems that is sufficiently fast and competent, we can build a dynamic\ngeometry program from it by solving the static problem for a sufficiently dense\nsampling of the trajectory of the moving element(s). The work we survey has its\nroots in applications, especially in mechanical computer-aided design (MCAD).\nThe constraint solvers used in MCAD took a quantum leap in the 1990s. These\napproaches solve a geometric constraint problem by an initial, graph-based\nstructural analysis that extracts generic subproblems and determines how they\nwould combine to form a complete solution. These subproblems are then handed to\nan algebraic solver that solves the specific instances of the generic\nsubproblems and combines them.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 08:25:42 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 14:33:43 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 09:38:55 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Fudos", "Ioannis", ""], ["Hoffmann", "Christoph M.", ""], ["Joan-Arinyo", "Robert", ""]]}, {"id": "1608.05390", "submitter": "Thomas Lidbetter Dr", "authors": "Steve Alpern and Thomas Lidbetter", "title": "Constant Factor Approximate Solutions for Expanding Search on General\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical problem introduced by R. Isaacs and S. Gal of\nminimizing the time to find a hidden point $H$ on a network $Q$ moving from a\nknown starting point. Rather than adopting the traditional continuous unit\nspeed path paradigm, we use the ``expanding search'' paradigm recently\nintroduced by the authors. Here the regions $S\\left( t\\right) $ that have been\nsearched by time $t$ are increasing from the starting point and have total\nlength $t$. Roughly speaking the search follows a sequence of arcs $a_{i}$ such\nthat each one starts at some point of an earlier one. This type of search is\noften carried out by real life search teams in the hunt for missing persons,\nescaped convicts, terrorists or lost airplanes. The paper which introduced this\ntype of search solved the adversarial problem (where $H$ is hidden to take a\nlong time to find) for the cases where $Q$ is a tree or is 2-arc-connected.\nThis paper solves the game on some additional families of networks. However the\nmain contribution is to give strategy classes which can be used on any network\nand have expected search times which are within a factor close to 1 of the\nvalue of the game (minimax search time). We identify cases where our strategies\nare in fact optimal.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 19:47:10 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Alpern", "Steve", ""], ["Lidbetter", "Thomas", ""]]}, {"id": "1608.05550", "submitter": "Yuto Nakashima", "authors": "Yuto Nakashima, Hiroe Inoue, Takuya Mieno, Shunsuke Inenaga, Hideo\n  Bannai and Masayuki Takeda", "title": "Shortest unique palindromic substring queries in optimal time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A palindrome is a string that reads the same forward and backward. A\npalindromic substring $P$ of a string $S$ is called a shortest unique\npalindromic substring ($\\mathit{SUPS}$) for an interval $[x, y]$ in $S$, if $P$\noccurs exactly once in $S$, this occurrence of $P$ contains interval $[x, y]$,\nand every palindromic substring of $S$ which contains interval $[x, y]$ and is\nshorter than $P$ occurs at least twice in $S$. The $\\mathit{SUPS}$ problem is,\ngiven a string $S$, to preprocess $S$ so that for any subsequent query interval\n$[x, y]$ all the $\\mathit{SUPS}\\mbox{s}$ for interval $[x, y]$ can be answered\nquickly. We present an optimal solution to this problem. Namely, we show how to\npreprocess a given string $S$ of length $n$ in $O(n)$ time and space so that\nall $\\mathit{SUPS}\\mbox{s}$ for any subsequent query interval can be answered\nin $O(k+1)$ time, where $k$ is the number of outputs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 09:35:52 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 10:23:59 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Nakashima", "Yuto", ""], ["Inoue", "Hiroe", ""], ["Mieno", "Takuya", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1608.05634", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Michael Axtmann, Emanuel J\\\"obstl, Sebastian Lamm,\n  Huyen Chau Nguyen, Alexander Noe, Sebastian Schlag, Matthias Stumpp, Tobias\n  Sturm, and Peter Sanders", "title": "Thrill: High-Performance Algorithmic Distributed Batch Data Processing\n  with C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and a first performance evaluation of Thrill -- a\nprototype of a general purpose big data processing framework with a convenient\ndata-flow style programming interface. Thrill is somewhat similar to Apache\nSpark and Apache Flink with at least two main differences. First, Thrill is\nbased on C++ which enables performance advantages due to direct native code\ncompilation, a more cache-friendly memory layout, and explicit memory\nmanagement. In particular, Thrill uses template meta-programming to compile\nchains of subsequent local operations into a single binary routine without\nintermediate buffering and with minimal indirections. Second, Thrill uses\narrays rather than multisets as its primary data structure which enables\nadditional operations like sorting, prefix sums, window scans, or combining\ncorresponding fields of several arrays (zipping). We compare Thrill with Apache\nSpark and Apache Flink using five kernels from the HiBench suite. Thrill is\nconsistently faster and often several times faster than the other frameworks.\nAt the same time, the source codes have a similar level of simplicity and\nabstraction\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 15:13:31 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Bingmann", "Timo", ""], ["Axtmann", "Michael", ""], ["J\u00f6bstl", "Emanuel", ""], ["Lamm", "Sebastian", ""], ["Nguyen", "Huyen Chau", ""], ["Noe", "Alexander", ""], ["Schlag", "Sebastian", ""], ["Stumpp", "Matthias", ""], ["Sturm", "Tobias", ""], ["Sanders", "Peter", ""]]}, {"id": "1608.05816", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao", "title": "Linear Kernels for Separating a Graph into Components of Bounded Size", "comments": null, "journal-ref": "Journal of Computer and System Sciences 88 (2017): 260-270", "doi": "10.1016/j.jcss.2017.04.004", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph separation and partitioning are fundamental problems that have been\nextensively studied both in theory and practice. The \\textsc{$p$-Size\nSeparator} problem, closely related to the \\textsc{Balanced Separator} problem,\nis to check whether we can delete at most $k$ vertices in a given graph $G$\nsuch that each connected component of the remaining graph has at most $p$\nvertices. This problem is NP-hard for each fixed integer $p\\geq 1$ and it\nbecomes the famous \\textsc{Vertex Cover} problem when $p=1$. It is known that\nthe problem with parameter $k$ is W[1]-hard for unfixed $p$. In this paper, we\nprove a kernel of $O(pk)$ vertices for this problem, i.e., a linear vertex\nkernel for each fixed $p \\geq 1$. In fact, we first obtain an $O(p^2k)$ vertex\nkernel by using a nontrivial extension of the expansion lemma. Then we further\nreduce the kernel size to $O(pk)$ by using some `local adjustment' techniques.\nOur proofs are based on extremal combinatorial arguments and the main result\ncan be regarded as a generalization of the Nemhauser and Trotter's theorem for\nthe \\textsc{Vertex Cover} problem. These techniques are possible to be used to\nimprove kernel sizes for more problems, especially problems with kernelization\nalgorithms based on techniques similar to the expansion lemma or crown\ndecompositions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 13:02:58 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Xiao", "Mingyu", ""]]}, {"id": "1608.06001", "submitter": "Brett Stevens", "authors": "Mark Cooke, Chris North, Megan Dewar, Brett Stevens", "title": "A note on Beckett-Gray codes and the relationship of Gray codes to data\n  structures", "comments": "6 pages, 3 tables. Revisions requested from Journal of Combinatorial\n  Mathematics and Combinatorial Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we introduce a natural mathematical structure derived from\nSamuel Beckett's play \"Quad\". We call this structure a binary Beckett-Gray\ncode. We enumerate all codes for $n \\leq 6$ and give examples for $n=7,8$.\nBeckett-Gray codes can be realized as successive states of a queue data\nstructure. We show that the binary reflected Gray code can be realized as\nsuccessive states of two stack data structures.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 21:12:40 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 20:55:42 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Cooke", "Mark", ""], ["North", "Chris", ""], ["Dewar", "Megan", ""], ["Stevens", "Brett", ""]]}, {"id": "1608.06009", "submitter": "Matthew Hammer", "authors": "Kyle Headley, Matthew A. Hammer", "title": "The Random Access Zipper: Simple, Purely-Functional Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Random Access Zipper (RAZ), a simple, purely-functional data\nstructure for editable sequences. A RAZ combines the structure of a zipper with\nthat of a tree: like a zipper, edits at the cursor require constant time; by\nleveraging tree structure, relocating the edit cursor in the sequence requires\nlogarithmic time. While existing data structures provide these time bounds,\nnone do so with the same simplicity and brevity of code as the RAZ. The\nsimplicity of the RAZ provides the opportunity for more programmers to extend\nthe structure to their own needs, and we provide some suggestions for how to do\nso.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:15:40 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Headley", "Kyle", ""], ["Hammer", "Matthew A.", ""]]}, {"id": "1608.06016", "submitter": "Aleksander M\\k{a}dry", "authors": "Aleksander Madry", "title": "Computing Maximum Flow with Augmenting Electrical Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $\\tilde{O}\\left(m^{\\frac{10}{7}}U^{\\frac{1}{7}}\\right)$-time\nalgorithm for the maximum $s$-$t$ flow problem and the minimum $s$-$t$ cut\nproblem in directed graphs with $m$ arcs and largest integer capacity $U$. This\nmatches the running time of the\n$\\tilde{O}\\left((mU)^{\\frac{10}{7}}\\right)$-time algorithm of M\\k{a}dry (FOCS\n2013) in the unit-capacity case, and improves over it, as well as over the\n$\\tilde{O}\\left(m \\sqrt{n} \\log U\\right)$-time algorithm of Lee and Sidford\n(FOCS 2014), whenever $U$ is moderately large and the graph is sufficiently\nsparse. By well-known reductions, this also gives similar running time\nimprovements for the maximum-cardinality bipartite $b$-matching problem.\n  One of the advantages of our algorithm is that it is significantly simpler\nthan the ones presented in Madry (FOCS 2013) and Lee and Sidford (FOCS 2014).\nIn particular, these algorithms employ a sophisticated interior-point method\nframework, while our algorithm is cast directly in the classic augmenting path\nsetting that almost all the combinatorial maximum flow algorithms use. At a\nhigh level, the presented algorithm takes a primal dual approach in which each\niteration uses electrical flows computations both to find an augmenting $s$-$t$\nflow in the current residual graph and to update the dual solution. We show\nthat by maintain certain careful coupling of these primal and dual solutions we\nare always guaranteed to make significant progress.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:53:22 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Madry", "Aleksander", ""]]}, {"id": "1608.06031", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Jian Li, Mingda Qiao", "title": "Towards Instance Optimal Bounds for Best Arm Identification", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical best arm identification (Best-$1$-Arm) problem, we are given\n$n$ stochastic bandit arms, each associated with a reward distribution with an\nunknown mean. We would like to identify the arm with the largest mean with\nprobability at least $1-\\delta$, using as few samples as possible.\nUnderstanding the sample complexity of Best-$1$-Arm has attracted significant\nattention since the last decade. However, the exact sample complexity of the\nproblem is still unknown.\n  Recently, Chen and Li made the gap-entropy conjecture concerning the instance\nsample complexity of Best-$1$-Arm. Given an instance $I$, let $\\mu_{[i]}$ be\nthe $i$th largest mean and $\\Delta_{[i]}=\\mu_{[1]}-\\mu_{[i]}$ be the\ncorresponding gap. $H(I)=\\sum_{i=2}^n\\Delta_{[i]}^{-2}$ is the complexity of\nthe instance. The gap-entropy conjecture states that\n$\\Omega\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}+\\mathsf{Ent}(I)\\right)\\right)$ is an\ninstance lower bound, where $\\mathsf{Ent}(I)$ is an entropy-like term\ndetermined by the gaps, and there is a $\\delta$-correct algorithm for\nBest-$1$-Arm with sample complexity\n$O\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}+\\mathsf{Ent}(I)\\right)+\\Delta_{[2]}^{-2}\\ln\\ln\\Delta_{[2]}^{-1}\\right)$.\nIf the conjecture is true, we would have a complete understanding of the\ninstance-wise sample complexity of Best-$1$-Arm.\n  We make significant progress towards the resolution of the gap-entropy\nconjecture. For the upper bound, we provide a highly nontrivial algorithm which\nrequires \\[O\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}\n+\\mathsf{Ent}(I)\\right)+\\Delta_{[2]}^{-2}\\ln\\ln\\Delta_{[2]}^{-1}\\mathrm{polylog}(n,\\delta^{-1})\\right)\\]\nsamples in expectation. For the lower bound, we show that for any Gaussian\nBest-$1$-Arm instance with gaps of the form $2^{-k}$, any $\\delta$-correct\nmonotone algorithm requires $\\Omega\\left(H(I)\\cdot\\left(\\ln\\delta^{-1} +\n\\mathsf{Ent}(I)\\right)\\right)$ samples in expectation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 02:05:10 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 02:19:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chen", "Lijie", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1608.06136", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Dieter Kratsch, Dani\\\"el Paulusma, and Anthony\n  Stewart", "title": "A Linear Kernel for Finding Square Roots of Almost Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph H is a square root of a graph G if G can be obtained from H by the\naddition of edges between any two vertices in H that are of distance 2 from\neach other. The Square Root problem is that of deciding whether a given graph\nadmits a square root. We consider this problem for planar graphs in the context\nof the \"distance from triviality\" framework. For an integer k, a planar+kv\ngraph (or k-apex graph) is a graph that can be made planar by the removal of at\nmost k vertices. We prove that a generalization of Square Root, in which some\nedges are prescribed to be either in or out of any solution, has a kernel of\nsize O(k) for planar+kv graphs, when parameterized by k. Our result is based on\na new edge reduction rule which, as we shall also show, has a wider\napplicability for the Square Root problem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:03:53 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Golovach", "Petr A.", ""], ["Kratsch", "Dieter", ""], ["Paulusma", "Dani\u00ebl", ""], ["Stewart", "Anthony", ""]]}, {"id": "1608.06142", "submitter": "Petr Golovach", "authors": "Manfred Cochefert, Jean-Fran\\c{c}ois Couturier, Petr A. Golovach,\n  Dieter Kratsch, Dani\\\"el Paulusma, and Anthony Stewart", "title": "Squares of Low Maximum Degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph H is a square root of a graph G if G can be obtained from H by adding\nan edge between any two vertices in H that are of distance 2. The Square Root\nproblem is that of deciding whether a given graph admits a square root. This\nproblem is only known to be NP-complete for chordal graphs and polynomial-time\nsolvable for non-trivial minor-closed graph classes and a very limited number\nof other graph classes. We prove that Square Root is O(n)-time solvable for\ngraphs of maximum degree 5 and O(n^4)-time solvable for graphs of maximum\ndegree at most 6.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:18:30 GMT"}, {"version": "v2", "created": "Sat, 27 Aug 2016 18:01:44 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Cochefert", "Manfred", ""], ["Couturier", "Jean-Fran\u00e7ois", ""], ["Golovach", "Petr A.", ""], ["Kratsch", "Dieter", ""], ["Paulusma", "Dani\u00ebl", ""], ["Stewart", "Anthony", ""]]}, {"id": "1608.06325", "submitter": "T-H. Hubert Chan", "authors": "T-H. Hubert Chan and Shuguang Hu and Shaofeng H.-C. Jiang", "title": "A PTAS for the Steiner Forest Problem in Doubling Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We achieve a (randomized) polynomial-time approximation scheme (PTAS) for the\nSteiner Forest Problem in doubling metrics. Before our work, a PTAS is given\nonly for the Euclidean plane in [FOCS 2008: Borradaile, Klein and Mathieu]. Our\nPTAS also shares similarities with the dynamic programming for sparse instances\nused in [STOC 2012: Bartal, Gottlieb and Krauthgamer] and [SODA 2016: Chan and\nJiang]. However, extending previous approaches requires overcoming several\nnon-trivial hurdles, and we make the following technical contributions.\n  (1) We prove a technical lemma showing that Steiner points have to be \"near\"\nthe terminals in an optimal Steiner tree. This enables us to define a heuristic\nto estimate the local behavior of the optimal solution, even though the Steiner\npoints are unknown in advance. This lemma also generalizes previous results in\nthe Euclidean plane, and may be of independent interest for related problems\ninvolving Steiner points.\n  (2) We develop a novel algorithmic technique known as \"adaptive cells\" to\novercome the difficulty of keeping track of multiple components in a solution.\nOur idea is based on but significantly different from the previously proposed\n\"uniform cells\" in the FOCS 2008 paper, whose techniques cannot be readily\napplied to doubling metrics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 21:50:07 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chan", "T-H. Hubert", ""], ["Hu", "Shuguang", ""], ["Jiang", "Shaofeng H. -C.", ""]]}, {"id": "1608.06415", "submitter": "Leah Epstein", "authors": "J\\'anos Balogh, J\\'ozsef B\\'ek\\'esi, Gy\\\"orgy D\\'osa, Leah Epstein,\n  Asaf Levin", "title": "Online bin packing with cardinality constraints resolved", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality constrained bin packing or bin packing with cardinality\nconstraints is a basic bin packing problem. In the online version with the\nparameter k \\geq 2, items having sizes in (0,1] associated with them are\npresented one by one to be packed into unit capacity bins, such that the\ncapacities of bins are not exceeded, and no bin receives more than k items. We\nresolve the online problem in the sense that we prove a lower bound of 2 on the\noverall asymptotic competitive ratio. This closes this long standing open\nproblem, since an algorithm of an absolute competitive ratio 2 is known.\nAdditionally, we significantly improve the known lower bounds on the asymptotic\ncompetitive ratio for every specific value of k. The novelty of our\nconstructions is based on full adaptivity that creates large gaps between item\nsizes. Thus, our lower bound inputs do not follow the common practice for\nonline bin packing problems of having a known in advance input consisting of\nbatches for which the algorithm needs to be competitive on every prefix of the\ninput.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 08:24:00 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Balogh", "J\u00e1nos", ""], ["B\u00e9k\u00e9si", "J\u00f3zsef", ""], ["D\u00f3sa", "Gy\u00f6rgy", ""], ["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "1608.06458", "submitter": "Fabrizio Frati", "authors": "Vida Dujmovi\\'c and Fabrizio Frati", "title": "Stack and Queue Layouts via Layered Separators", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that every proper minor-closed class of graphs has bounded\nstack-number (a.k.a. book thickness and page number). While this includes\nnotable graph families such as planar graphs and graphs of bounded genus, many\nother graph families are not closed under taking minors. For fixed $g$ and $k$,\nwe show that every $n$-vertex graph that can be embedded on a surface of genus\n$g$ with at most $k$ crossings per edge has stack-number $\\mathcal{O}(\\log n)$;\nthis includes $k$-planar graphs. The previously best known bound for the\nstack-number of these families was $\\mathcal{O}(\\sqrt{n})$, except in the case\nof $1$-planar graphs. Analogous results are proved for map graphs that can be\nembedded on a surface of fixed genus. None of these families is closed under\ntaking minors. The main ingredient in the proof of these results is a\nconstruction proving that $n$-vertex graphs that admit constant layered\nseparators have $\\mathcal{O}(\\log n)$ stack-number.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 10:48:18 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Dujmovi\u0107", "Vida", ""], ["Frati", "Fabrizio", ""]]}, {"id": "1608.06462", "submitter": "Aikaterini Karanasiou", "authors": "Loukas Georgiadis, Aikaterini Karanasiou, Giannis Konstantinos, Luigi\n  Laura", "title": "On Low-High Orders of Directed Graphs: Incremental Algorithms and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flow graph $G=(V,E,s)$ is a directed graph with a distinguished start\nvertex $s$. The dominator tree $D$ of $G$ is a tree rooted at $s$, such that a\nvertex $v$ is an ancestor of a vertex $w$ if and only if all paths from $s$ to\n$w$ include $v$. The dominator tree is a central tool in program optimization\nand code generation and has many applications in other diverse areas including\nconstraint programming, circuit testing, biology, and in algorithms for graph\nconnectivity problems. A low-high order of $G$ is a preorder $\\delta$ of $D$\nthat certifies the correctness of $D$ and has further applications in\nconnectivity and path-determination problems. In this paper, we first consider\nhow to maintain efficiently a low-high order of a flow graph incrementally\nunder edge insertions. We present algorithms that run in $O(mn)$ total time for\na sequence of $m$ edge insertions in an initially empty flow graph with $n$\nvertices.These immediately provide the first incremental certifying algorithms\nfor maintaining the dominator tree in $O(mn)$ total time, and also imply\nincremental algorithms for other problems. Hence, we provide a substantial\nimprovement over the $O(m^2)$ simple-minded algorithms, which recompute the\nsolution from scratch after each edge insertion. We also show how to apply\nlow-high orders to obtain a linear-time $2$-approximation algorithm for the\nsmallest $2$-vertex-connected spanning subgraph problem (2VCSS). Finally, we\npresent efficient implementations of our new algorithms for the incremental\nlow-high and 2VCSS problems and conduct an extensive experimental study on\nreal-world graphs taken from a variety of application areas. The experimental\nresults show that our algorithms perform very well in practice.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 11:10:10 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Karanasiou", "Aikaterini", ""], ["Konstantinos", "Giannis", ""], ["Laura", "Luigi", ""]]}, {"id": "1608.06498", "submitter": "Alexander Stollenwerk", "authors": "Sjoerd Dirksen, Alexander Stollenwerk", "title": "Fast binary embeddings with Gaussian circulant matrices: improved bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of encoding a finite set of vectors into a small\nnumber of bits while approximately retaining information on the angular\ndistances between the vectors. By deriving improved variance bounds related to\nbinary Gaussian circulant embeddings, we largely fix a gap in the proof of the\nbest known fast binary embedding method. Our bounds also show that\nwell-spreadness assumptions on the data vectors, which were needed in earlier\nwork on variance bounds, are unnecessary. In addition, we propose a new binary\nembedding with a faster running time on sparse data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 13:13:16 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 15:23:13 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Dirksen", "Sjoerd", ""], ["Stollenwerk", "Alexander", ""]]}, {"id": "1608.06617", "submitter": "Stacey Jeffery", "authors": "Stacey Jeffery and Fran\\c{c}ois Le Gall", "title": "Quantum Communication Complexity of Distributed Set Joins", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing set joins of two inputs is a common task in database theory.\nRecently, Van Gucht, Williams, Woodruff and Zhang [PODS 2015] considered the\ncomplexity of such problems in the natural model of (classical) two-party\ncommunication complexity and obtained tight bounds for the complexity of\nseveral important distributed set joins.\n  In this paper we initiate the study of the *quantum* communication complexity\nof distributed set joins. We design a quantum protocol for distributed Boolean\nmatrix multiplication, which corresponds to computing the composition join of\ntwo databases, showing that the product of two $n\\times n$ Boolean matrices,\neach owned by one of two respective parties, can be computed with\n$\\widetilde{O}(\\sqrt{n}\\ell^{3/4})$ qubits of communication, where $\\ell$\ndenotes the number of non-zero entries of the product. Since Van Gucht et al.\nshowed that the classical communication complexity of this problem is\n$\\widetilde{\\Theta}(n\\sqrt{\\ell})$, our quantum algorithm outperforms classical\nprotocols whenever the output matrix is sparse. We also show a quantum lower\nbound and a matching classical upper bound on the communication complexity of\ndistributed matrix multiplication over $\\mathbb{F}_2$.\n  Besides their applications to database theory, the communication complexity\nof set joins is interesting due to its connections to direct product theorems\nin communication complexity. In this work we also introduce a notion of\n*all-pairs* product theorem, and relate this notion to standard direct product\ntheorems in communication complexity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 19:45:38 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Jeffery", "Stacey", ""], ["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1608.06666", "submitter": "Carlos Ochoa", "authors": "J\\'er\\'emy Barbay, Carlos Ochoa, Srinivasa Rao Satti", "title": "Synergistic Sorting, MultiSelection and Deferred Data Structures on\n  MultiSets", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karp et al. (1988) described Deferred Data Structures for Multisets as \"lazy\"\ndata structures which partially sort data to support online rank and select\nqueries, with the minimum amount of work in the worst case over instances of\nsize $n$ and number of queries $q$ fixed (i.e., the query size). Barbay et al.\n(2016) refined this approach to take advantage of the gaps between the\npositions hit by the queries (i.e., the structure in the queries). We develop\nnew techniques in order to further refine this approach and to take advantage\nall at once of the structure (i.e., the multiplicities of the elements), the\nlocal order (i.e., the number and sizes of runs) and the global order (i.e.,\nthe number and positions of existing pivots) in the input; and of the structure\nand order in the sequence of queries. Our main result is a synergistic deferred\ndata structure which performs much better on large classes of instances, while\nperforming always asymptotically as good as previous solutions. As intermediate\nresults, we describe two new synergistic sorting algorithms, which take\nadvantage of the structure and order (local and global) in the input, improving\nupon previous results which take advantage only of the structure (Munro and\nSpira 1979) or of the local order (Takaoka 1997) in the input; and one new\nmultiselection algorithm which takes advantage of not only the order and\nstructure in the input, but also of the structure in the queries. We described\ntwo compressed data structures to represent a multiset taking advantage of both\nthe local order and structure, while supporting the operators rank and select\non the multiset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 23:04:51 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 19:44:30 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["Ochoa", "Carlos", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1608.06819", "submitter": "Thodoris Lykouris", "authors": "Siddhartha Banerjee, Daniel Freund, Thodoris Lykouris", "title": "Pricing and Optimization in Shared Vehicle Systems: An Approximation\n  Framework", "comments": "The current version represents the content that will appear in\n  Operations Research. A one-page abstract of the paper appeared at the 18th\n  ACM Conference on Economics and Computation (EC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing shared vehicle systems (bike/scooter/car/ride-sharing) is more\nchallenging compared to traditional resource allocation settings due to the\npresence of \\emph{complex network externalities} -- changes in the\ndemand/supply at any location affect future supply throughout the system within\nshort timescales. These externalities are well captured by steady-state\nMarkovian models, which are therefore widely used to analyze such systems.\nHowever, using such models to design pricing and other control policies is\ncomputationally difficult since the resulting optimization problems are\nhigh-dimensional and non-convex.\n  To this end, we develop a \\emph{rigorous approximation framework} for shared\nvehicle systems, providing a unified approach for a wide range of controls\n(pricing, matching, rebalancing), objective functions (throughput, revenue,\nwelfare), and system constraints (travel-times, welfare benchmarks,\nposted-price constraints). Our approach is based on the analysis of natural\nconvex relaxations, and obtains as special cases existing approximate-optimal\npolicies for limited settings, asymptotic-optimality results, and heuristic\npolicies. The resulting guarantees are non-asymptotic and parametric, and\nprovide operational insights into the design of real-world systems. In\nparticular, for any shared vehicle system with $n$ stations and $m$ vehicles,\nour framework obtains an approximation ratio of $1+(n-1)/m$, which is\nparticularly meaningful when $m/n$, the average number of vehicles per station,\nis large, as is often the case in practice.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 14:01:08 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 22:27:12 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 22:49:22 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 22:02:43 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Banerjee", "Siddhartha", ""], ["Freund", "Daniel", ""], ["Lykouris", "Thodoris", ""]]}, {"id": "1608.06980", "submitter": "C. Seshadhri", "authors": "Deeparnab Chakrabarty and C. Seshadhri", "title": "A $\\widetilde{O}(n)$ Non-Adaptive Tester for Unateness", "comments": "We mention the relation of our algorithm to Levin's investment\n  strategy, as pointed out by Oded Goldreich", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Khot and Shinkar (RANDOM, 2016) recently describe an adaptive, $O(n\n\\log(n)/\\varepsilon)$-query tester for unateness of Boolean functions\n$f:\\{0,1\\}^n \\to \\{0,1\\}$. In this note we describe a simple non-adaptive, $O(n\n\\log(n/\\varepsilon)/\\varepsilon)$ -query tester for unateness for functions\nover the hypercube with any ordered range.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 22:20:43 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 21:49:36 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Seshadhri", "C.", ""]]}, {"id": "1608.07002", "submitter": "Xiaodong Wang", "authors": "Daxin Zhu, Lei Wang, Tinran Wang, and Xiaodong Wang", "title": "A simple linear space algorithm for computing a longest common\n  increasing subsequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reformulates the problem of finding a longest common increasing\nsubsequence of the two given input sequences in a very succinct way. An\nextremely simple linear space algorithm based on the new formula can find a\nlongest common increasing subsequence of sizes $n$ and $m$ respectively, in\ntime $O(nm)$ using additional $\\min\\{n,m\\}+1$ space.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 01:56:40 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Zhu", "Daxin", ""], ["Wang", "Lei", ""], ["Wang", "Tinran", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1608.07022", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao and Shaowei Kou", "title": "Kernelization and Parameterized Algorithms for 3-Path Vertex Cover", "comments": "in TAMC 2016, LNCS 9796, 2016", "journal-ref": "TAMC 2017, LNCS 10185, 654-668", "doi": "10.1007/978-3-319-55911-7_47", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 3-path vertex cover in a graph is a vertex subset $C$ such that every path\nof three vertices contains at least one vertex from $C$. The parameterized\n3-path vertex cover problem asks whether a graph has a 3-path vertex cover of\nsize at most $k$. In this paper, we give a kernel of $5k$ vertices and an\n$O^*(1.7485^k)$-time and polynomial-space algorithm for this problem, both new\nresults improve previous known bounds.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 06:16:32 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Xiao", "Mingyu", ""], ["Kou", "Shaowei", ""]]}, {"id": "1608.07179", "submitter": "Yuichi Yoshida", "authors": "Kohei Hayashi, Yuichi Yoshida", "title": "Minimizing Quadratic Functions in Constant Time", "comments": "An extended abstract will appear in the proceedings of NIPS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sampling-based optimization method for quadratic functions is proposed. Our\nmethod approximately solves the following $n$-dimensional quadratic\nminimization problem in constant time, which is independent of $n$:\n$z^*=\\min_{\\mathbf{v} \\in \\mathbb{R}^n}\\langle\\mathbf{v}, A \\mathbf{v}\\rangle +\nn\\langle\\mathbf{v}, \\mathrm{diag}(\\mathbf{d})\\mathbf{v}\\rangle +\nn\\langle\\mathbf{b}, \\mathbf{v}\\rangle$, where $A \\in \\mathbb{R}^{n \\times n}$\nis a matrix and $\\mathbf{d},\\mathbf{b} \\in \\mathbb{R}^n$ are vectors. Our\ntheoretical analysis specifies the number of samples $k(\\delta, \\epsilon)$ such\nthat the approximated solution $z$ satisfies $|z - z^*| = O(\\epsilon n^2)$ with\nprobability $1-\\delta$. The empirical performance (accuracy and runtime) is\npositively confirmed by numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 14:43:17 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Hayashi", "Kohei", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1608.07336", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Alistair Stewart", "title": "Playing Anonymous Games using Simple Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of computing approximate Nash equilibria in\nanonymous games. Our main algorithmic result is the following: For any\n$n$-player anonymous game with a bounded number of strategies and any constant\n$\\delta>0$, an $O(1/n^{1-\\delta})$-approximate Nash equilibrium can be computed\nin polynomial time. Complementing this positive result, we show that if there\nexists any constant $\\delta>0$ such that an $O(1/n^{1+\\delta})$-approximate\nequilibrium can be computed in polynomial time, then there is a fully\npolynomial-time approximation scheme for this problem.\n  We also present a faster algorithm that, for any $n$-player $k$-strategy\nanonymous game, runs in time $\\tilde O((n+k) k n^k)$ and computes an $\\tilde\nO(n^{-1/3} k^{11/3})$-approximate equilibrium. This algorithm follows from the\nexistence of simple approximate equilibria of anonymous games, where each\nplayer plays one strategy with probability $1-\\delta$, for some small $\\delta$,\nand plays uniformly at random with probability $\\delta$.\n  Our approach exploits the connection between Nash equilibria in anonymous\ngames and Poisson multinomial distributions (PMDs). Specifically, we prove a\nnew probabilistic lemma establishing the following: Two PMDs, with large\nvariance in each direction, whose first few moments are approximately matching\nare close in total variation distance. Our structural result strengthens\nprevious work by providing a smooth tradeoff between the variance bound and the\nnumber of matching moments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 23:25:57 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Stewart", "Alistair", ""]]}, {"id": "1608.07413", "submitter": "Nicolas Trotignon", "authors": "Lan Anh Pham and Nicolas Trotignon", "title": "$\\chi$-bounds, operations and chords", "comments": null, "journal-ref": null, "doi": "10.1002/jgt.22214", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{long unichord} in a graph is an edge that is the unique chord of some\ncycle of length at least 5. A graph is \\emph{long-unichord-free} if it does not\ncontain any long-unichord. We prove a structure theorem for long-unichord-free\ngraph. We give an $O(n^4m)$-time algorithm to recognize them. We show that any\nlong-unichord-free graph $G$ can be colored with at most $O(\\omega^3)$ colors,\nwhere $\\omega$ is the maximum number of pairwise adjacent vertices in $G$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 10:12:27 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 08:46:01 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Pham", "Lan Anh", ""], ["Trotignon", "Nicolas", ""]]}, {"id": "1608.07505", "submitter": "Tilo Wiedera", "authors": "Markus Chimani, Karsten Klein, and Tilo Wiedera", "title": "A Note on the Practicality of Maximal Planar Subgraph Algorithms", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, the NP-hard Maximum Planar Subgraph problem (MPS) asks for\na planar subgraph of $G$ with the maximum number of edges. There are several\nheuristic, approximative, and exact algorithms to tackle the problem, but---to\nthe best of our knowledge---they have never been compared competitively in\npractice. We report on an exploratory study on the relative merits of the\ndiverse approaches, focusing on practical runtime, solution quality, and\nimplementation complexity. Surprisingly, a seemingly only theoretically strong\napproximation forms the building block of the strongest choice.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 16:21:15 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Chimani", "Markus", ""], ["Klein", "Karsten", ""], ["Wiedera", "Tilo", ""]]}, {"id": "1608.07568", "submitter": "Daniel Kral", "authors": "Zdenek Dvorak, Daniel Kral, Bojan Mohar", "title": "Graphic TSP in cubic graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial-time 9/7-approximation algorithm for the graphic TSP\nfor cubic graphs, which improves the previously best approximation factor of\n1.3 for 2-connected cubic graphs and drops the requirement of 2-connectivity at\nthe same time. To design our algorithm, we prove that every simple 2-connected\ncubic n-vertex graph contains a spanning closed walk of length at most 9n/7-1,\nand that such a walk can be found in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 19:39:42 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 13:23:23 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 15:42:17 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Dvorak", "Zdenek", ""], ["Kral", "Daniel", ""], ["Mohar", "Bojan", ""]]}, {"id": "1608.07575", "submitter": "Mircea Adrian Digulescu", "authors": "Mircea Adrian Digulescu", "title": "Strategic play in stable marriage problem", "comments": "89 pages, 8 sections. Main result in Section 3. Other results in\n  other sections", "journal-ref": null, "doi": "10.13140/RG.2.2.20331.75041", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stable marriage problem, as addressed by Gale and Shapely [1] consists of\nproviding a bipartite matching between n \" boys \" and n \" girls \"-each of whom\nhave a totally ordered preference list over the other set-such that there\nexists no \" boy \" and no \" girl \" that would prefer each other over their\npartner in the matching. In this paper, we analyze the cases of strategic play\nby the \" boys \" in the game directly inspired by this problem. We provide an\nO(n^3) algorithm for determining a matching which is not necessarily stable in\nthe Gale-Shapely sense, but it is coalition-stable, in that no player has a\nselfish interest to leave the resulting grand coalition to join any potential\nalternative one which might feasibly form, and is also man-optimal. Thus, under\na realistic assumption set, no player has an interest to \" destabilize \" the\nmatching, even though he theoretically could. The resulting matching is often\nbetter than the na\\\"ive Gale-Shapely one for some (not all) of the \" boys \" ,\nbeing no worse for the rest. This matching is more realistic (stable) than the\none produced by top-trading-cycles method, thus offering a qualitative\nimprovement over the latter. Furthermore, we analyze the situation when players\nare allowed to make strategic threats (i.e. be willing to sacrifice their own\noutcome to hurt others), offer a relevant example to illustrate the benefits of\nthis form of play, and ultimately provide an exponential time algorithm which\ntries to determine a good threat-making strategy. We then briefly examine a few\nother non-conventional possibilities a player has to affect his outcome. Most\ncommon variations to the game model are also described and analyzed with regard\nto applicability of the methods in this paper. Finally, a few examples of\nreal-life problems which can be modeled and solved with the methods in this\npaper are presented.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 18:07:25 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Digulescu", "Mircea Adrian", ""]]}, {"id": "1608.07596", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow", "title": "An algorithm for dividing two complex numbers", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a rationalized algorithm for calculating the quotient of two\ncomplex numbers is presented which reduces the number of underlying real\nmultiplications. The performing of a complex number division using the naive\nmethod takes 4 multiplications, 3 additions, 2 squarings and 2 divisions of\nreal numbers while the proposed algorithm can compute the same result in only 3\nmultiplications ( or multipliers in hardware implementation case), 6 additions,\n2 squarings and 2 divisions of real numbers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 20:21:47 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 11:21:45 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Cariow", "Aleksandr", ""]]}, {"id": "1608.07647", "submitter": "Yu Hin Au", "authors": "Yu Hin Au, Levent Tun\\c{c}el", "title": "Elementary polytopes with high lift-and-project ranks for strong\n  positive semidefinite operators", "comments": null, "journal-ref": "Discrete Optimization 27 (2018), 103-129", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider operators acting on convex subsets of the unit hypercube. These\noperators are used in constructing convex relaxations of combinatorial\noptimization problems presented as a 0,1 integer programming problem or a 0,1\npolynomial optimization problem. Our focus is mostly on operators that, when\nexpressed as a lift-and-project operator, involve the use of semidefiniteness\nconstraints in the lifted space, including operators due to Lasserre and\nvariants of the Sherali--Adams and Bienstock--Zuckerberg operators. We study\nthe performance of these semidefinite-optimization-based lift-and-project\noperators on some elementary polytopes --- hypercubes that are chipped (at\nleast one vertex of the hypercube removed by intersection with a closed\nhalfspace) or cropped (all $2^n$ vertices of the hypercube removed by\nintersection with $2^n$ closed halfspaces) to varying degrees of severity\n$\\rho$. We prove bounds on $\\rho$ where these operators would perform badly on\nthe aforementioned examples. We also show that the integrality gap of the\nchipped hypercube is invariant under the application of several\nlift-and-project operators of varying strengths.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 02:20:21 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Au", "Yu Hin", ""], ["Tun\u00e7el", "Levent", ""]]}, {"id": "1608.07652", "submitter": "Ramesh Krishnan S. Pallavoor", "authors": "Roksana Baleshzar, Meiram Murzabulatov, Ramesh Krishnan S. Pallavoor,\n  Sofya Raskhodnikova", "title": "Testing Unateness of Real-Valued Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a unateness tester for functions of the form $f:[n]^d\\rightarrow R$,\nwhere $n,d\\in \\mathbb{N}$ and $R\\subseteq \\mathbb{R}$ with query complexity\n$O(\\frac{d\\log (\\max(d,n))}{\\epsilon})$. Previously known unateness testers\nwork only for Boolean functions over the domain $\\{0,1\\}^d$. We show that every\nunateness tester for real-valued functions over hypergrid has query complexity\n$\\Omega(\\min\\{d, |R|^2\\})$. Consequently, our tester is nearly optimal for\nreal-valued functions over $\\{0,1\\}^d$. We also prove that every nonadaptive,\n1-sided error unateness tester for Boolean functions needs\n$\\Omega(\\sqrt{d}/\\epsilon)$ queries. Previously, no lower bounds for testing\nunateness were known.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 02:49:18 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Baleshzar", "Roksana", ""], ["Murzabulatov", "Meiram", ""], ["Pallavoor", "Ramesh Krishnan S.", ""], ["Raskhodnikova", "Sofya", ""]]}, {"id": "1608.07662", "submitter": "Seok-Hee Hong", "authors": "Seok-Hee Hong, Hiroshi Nagamochi", "title": "Re-embedding a 1-Plane Graph into a Straight-line Drawing in Linear Time", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016). This is an extended\n  abstract. For a full version of this paper, see Hong S-H, Nagamochi H.:\n  Re-embedding a 1-Plane Graph into a Straight-line Drawing in Linear Time,\n  Technical Report TR 2016-002, Department of Applied Mathematics and Physics,\n  Kyoto University (2016)", "journal-ref": null, "doi": null, "report-no": "Technical Report TR 2016-002, Department of Applied Mathematics and\n  Physics, Kyoto University (2016)", "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thomassen characterized some 1-plane embedding as the forbidden configuration\nsuch that a given 1-plane embedding of a graph is drawable in straight-lines if\nand only if it does not contain the configuration [C. Thomassen, Rectilinear\ndrawings of graphs, J. Graph Theory, 10(3), 335-341, 1988].\n  In this paper, we characterize some 1-plane embedding as the forbidden\nconfiguration such that a given 1-plane embedding of a graph can be re-embedded\ninto a straight-line drawable 1-plane embedding of the same graph if and only\nif it does not contain the configuration. Re-embedding of a 1-plane embedding\npreserves the same set of pairs of crossing edges.\n  We give a linear-time algorithm for finding a straight-line drawable 1-plane\nre-embedding or the forbidden configuration.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 05:29:29 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 14:47:27 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Hong", "Seok-Hee", ""], ["Nagamochi", "Hiroshi", ""]]}, {"id": "1608.07847", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Roman Kolpakov and Mathieu Raffinot", "title": "Indexing and querying color sets of images", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.tcs.2016.07.041", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to study the set of color sets of continuous regions of an image given\nas a matrix of $m$ rows over $n\\geq m$ columns where each element in the matrix\nis an integer from $[1,\\sigma]$ named a {\\em color}.\n  The set of distinct colors in a region is called fingerprint. We aim to\ncompute, index and query the fingerprints of all rectangular regions named\nrectangles. The set of all such fingerprints is denoted by ${\\cal F}$. A\nrectangle is {\\em maximal} if it is not contained in a greater rectangle with\nthe same fingerprint. The set of all locations of maximal rectangles is denoted\nby $\\mathcal{L}.$ We first explain how to determine all the $|\\mathcal{L}|$\nmaximal locations with their fingerprints in expected time $O(nm^2\\sigma)$\nusing a Monte Carlo algorithm (with polynomially small probability of error) or\nwithin deterministic $O(nm^2\\sigma\\log(\\frac{|\\mathcal{L}|}{nm^2}+2))$ time. We\nthen show how to build a data structure which occupies $O(nm\\log\nn+\\mathcal{|L|})$ space such that a query which asks for all the maximal\nlocations with a given fingerprint $f$ can be answered in time $O(|f|+\\log\\log\nn+k)$, where $k$ is the number of maximal locations with fingerprint $f$. If\nthe query asks only for the presence of the fingerprint, then the space usage\nbecomes $O(nm\\log n+|{\\cal F}|)$ while the query time becomes $O(|f|+\\log\\log\nn)$. We eventually consider the special case of squared regions (squares).\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 19:52:09 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Kolpakov", "Roman", ""], ["Raffinot", "Mathieu", ""]]}, {"id": "1608.08027", "submitter": "Martin Gronemann", "authors": "Martin Gronemann, Michael J\\\"unger, Frauke Liers, Francesco Mambelli", "title": "Crossing Minimization in Storyline Visualization", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A storyline visualization is a layout that represents the temporal dynamics\nof social interactions along time by the convergence of chronological lines.\nAmong the criteria oriented at improving aesthetics and legibility of a\nrepresentation of this type, a small number of line crossings is the hardest to\nachieve. We model the crossing minimization in the storyline visualization\nproblem as a multi-layer crossing minimization problem with tree constraints.\nOur algorithm can compute a layout with the minimum number of crossings of the\nchronological lines. Computational results demonstrate that it can solve\ninstances with more than 100 interactions and with more than 100 chronological\nlines to optimality.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:43:36 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Gronemann", "Martin", ""], ["J\u00fcnger", "Michael", ""], ["Liers", "Frauke", ""], ["Mambelli", "Francesco", ""]]}, {"id": "1608.08161", "submitter": "Martin Fink", "authors": "Md. Jawaherul Alam and Martin Fink and Sergey Pupyrev", "title": "The Bundled Crossing Number", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithmic aspect of edge bundling. A bundled crossing in a\ndrawing of a graph is a group of crossings between two sets of parallel edges.\nThe bundled crossing number is the minimum number of bundled crossings that\ngroup all crossings in a drawing of the graph.\n  We show that the bundled crossing number is closely related to the orientable\ngenus of the graph. If multiple crossings and self-intersections of edges are\nallowed, the two values are identical; otherwise, the bundled crossing number\ncan be higher than the genus.\n  We then investigate the problem of minimizing the number of bundled\ncrossings. For circular graph layouts with a fixed order of vertices, we\npresent a constant-factor approximation algorithm. When the circular order is\nnot prescribed, we get a $\\frac{6c}{c-2}$ approximation for a graph with $n$\nvertices having at least $cn$ edges for $c>2$. For general graph layouts, we\ndevelop an algorithm with an approximation factor of $\\frac{6c}{c-3}$ for\ngraphs with at least $cn$ edges for $c > 3$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:07:48 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 18:08:54 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Alam", "Md. Jawaherul", ""], ["Fink", "Martin", ""], ["Pupyrev", "Sergey", ""]]}, {"id": "1608.08346", "submitter": "Igor Zavadskyi", "authors": "Igor O. Zavadskyi (Taras Shevchenko National University of Kyiv)", "title": "A family of fast exact pattern matching algorithms", "comments": "6 pages, 1 figure, submitted to 'Bulletin of Taras Shevchenko\n  NationalUniversity of Kyiv'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of comparison-based exact pattern matching algorithms is described.\nThey utilize multi-dimensional arrays in order to process more than one\nadjacent text window in each iteration of the search cycle. This approach leads\nto a lower average time complexity by the cost of space. The algorithms of this\nfamily perform well for short patterns and middle size alphabets. In such case\nthe shift of the window by several pattern lengths at once is quite probable,\nwhich is the main factor of algorithm success. Our algorithms outperform the\nBoyer-Moore-Horspool algorithm, either in the original version or with Sunday's\nQuick search modification, in a wide area of pattern length - alphabet size\nplane. In some subareas the proposed algorithms are the fastest among all known\nexact pattern matching algorithms. Namely, they perform best when alphabet size\nis about 30-40 and pattern length is about 4-10. Such parameters are typical\nfor search in natural language text databases.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 06:52:52 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Zavadskyi", "Igor O.", "", "Taras Shevchenko National University of Kyiv"]]}, {"id": "1608.08385", "submitter": "Malte Skambath", "authors": "Malte Skambath and Till Tantau", "title": "Offline Drawing of Dynamic Trees: Algorithmics and Document Integration", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the algorithmic drawing of static trees is well-understood and\nwell-supported by software tools, creating animations depicting how a tree\nchanges over time is currently difficult: software support, if available at\nall, is not integrated into a document production workflow and algorithmic\napproaches only rarely take temporal information into consideration. During the\nproduction of a presentation or a paper, most users will visualize how, say, a\nsearch tree evolves over time by manually drawing a sequence of trees. We\npresent an extension of the popular $\\TeX$ typesetting system that allows users\nto specify dynamic trees inside their documents, together with a new algorithm\nfor drawing them. Running $\\TeX$ on the documents then results in documents in\nthe SVG format with visually pleasing embedded animations. Our algorithm\nproduces animations that satisfy a set of natural aesthetic criteria when\npossible. On the negative side, we show that one cannot always satisfy all\ncriteria simultaneously and that minimizing their violations is NP-complete.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 09:34:11 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Skambath", "Malte", ""], ["Tantau", "Till", ""]]}, {"id": "1608.08427", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini and Steven Chaplick and Sabine Cornelsen and\n  Giordano Da Lozzo and Giuseppe Di Battista and Peter Eades and Philipp\n  Kindermann and Jan Kratochvil and Fabian Lipp and and Ignaz Rutter", "title": "Simultaneous Orthogonal Planarity", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the $\\textit{OrthoSEFE}-k$ problem: Given $k$ planar\ngraphs each with maximum degree 4 and the same vertex set, do they admit an\nOrthoSEFE, that is, is there an assignment of the vertices to grid points and\nof the edges to paths on the grid such that the same edges in distinct graphs\nare assigned the same path and such that the assignment induces a planar\northogonal drawing of each of the $k$ graphs?\n  We show that the problem is NP-complete for $k \\geq 3$ even if the shared\ngraph is a Hamiltonian cycle and has sunflower intersection and for $k \\geq 2$\neven if the shared graph consists of a cycle and of isolated vertices. Whereas\nthe problem is polynomial-time solvable for $k=2$ when the union graph has\nmaximum degree five and the shared graph is biconnected. Further, when the\nshared graph is biconnected and has sunflower intersection, we show that every\npositive instance has an OrthoSEFE with at most three bends per edge.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 12:47:16 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Angelini", "Patrizio", ""], ["Chaplick", "Steven", ""], ["Cornelsen", "Sabine", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Eades", "Peter", ""], ["Kindermann", "Philipp", ""], ["Kratochvil", "Jan", ""], ["Lipp", "Fabian", ""], ["Rutter", "and Ignaz", ""]]}, {"id": "1608.08500", "submitter": "Daniel Graf", "authors": "Andreas B\\\"artschi, J\\'er\\'emie Chalopin, Shantanu Das, Yann Disser,\n  Barbara Geissmann, Daniel Graf, Arnaud Labourel, Mat\\'u\\v{s} Mihal\\'ak", "title": "Collaborative Delivery with Energy-Constrained Mobile Robots", "comments": "19 pages. An extended abstract of this paper was published at the\n  23rd International Colloquium on Structural Information and Communication\n  Complexity 2016, SIROCCO'16", "journal-ref": "Theoretical Computer Science 810:2-14, 2020", "doi": "10.1016/j.tcs.2017.04.018", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of collectively delivering some message from a\nspecified source to a designated target location in a graph, using multiple\nmobile agents. Each agent has a limited energy which constrains the distance it\ncan move. Hence multiple agents need to collaborate to move the message, each\nagent handing over the message to the next agent to carry it forward. Given the\npositions of the agents in the graph and their respective budgets, the problem\nof finding a feasible movement schedule for the agents can be challenging. We\nconsider two variants of the problem: in non-returning delivery, the agents can\nstop anywhere; whereas in returning delivery, each agent needs to return to its\nstarting location, a variant which has not been studied before.\n  We first provide a polynomial-time algorithm for returning delivery on trees,\nwhich is in contrast to the known (weak) NP-hardness of the non-returning\nversion. In addition, we give resource-augmented algorithms for returning\ndelivery in general graphs. Finally, we give tight lower bounds on the required\nresource augmentation for both variants of the problem. In this sense, our\nresults close the gap left by previous research.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:25:59 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["B\u00e4rtschi", "Andreas", ""], ["Chalopin", "J\u00e9r\u00e9mie", ""], ["Das", "Shantanu", ""], ["Disser", "Yann", ""], ["Geissmann", "Barbara", ""], ["Graf", "Daniel", ""], ["Labourel", "Arnaud", ""], ["Mihal\u00e1k", "Mat\u00fa\u0161", ""]]}, {"id": "1608.08505", "submitter": "Carla Binucci", "authors": "Carla Binucci, Markus Chimani, Walter Didimo, Giuseppe Liotta,\n  Fabrizio Montecchiani", "title": "Placing Arrows in Directed Graph Drawings", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of placing arrow heads in directed graph drawings\nwithout them overlapping other drawn objects. This gives drawings where edge\ndirections can be deduced unambiguously. We show hardness of the problem,\npresent exact and heuristic algorithms, and report on a practical study.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:33:56 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Binucci", "Carla", ""], ["Chimani", "Markus", ""], ["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "1608.08527", "submitter": "Grigorios Koumoutsos", "authors": "Nikhil Bansal, Marek Eli\\'a\\v{s}, {\\L}ukasz Je\\.z, Grigorios\n  Koumoutsos", "title": "The $(h,k)$-Server Problem on Bounded Depth Trees", "comments": "Appeared in SODA 2017", "journal-ref": null, "doi": "10.1137/1.9781611974782.65", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $k$-server problem in the resource augmentation setting i.e.,\nwhen the performance of the online algorithm with $k$ servers is compared to\nthe offline optimal solution with $h \\leq k$ servers. The problem is very\npoorly understood beyond uniform metrics. For this special case, the classic\n$k$-server algorithms are roughly $(1+1/\\epsilon)$-competitive when\n$k=(1+\\epsilon) h$, for any $\\epsilon >0$. Surprisingly however, no\n$o(h)$-competitive algorithm is known even for HSTs of depth 2 and even when\n$k/h$ is arbitrarily large.\n  We obtain several new results for the problem. First we show that the known\n$k$-server algorithms do not work even on very simple metrics. In particular,\nthe Double Coverage algorithm has competitive ratio $\\Omega(h)$ irrespective of\nthe value of $k$, even for depth-2 HSTs. Similarly the Work Function Algorithm,\nthat is believed to be optimal for all metric spaces when $k=h$, has\ncompetitive ratio $\\Omega(h)$ on depth-3 HSTs even if $k=2h$. Our main result\nis a new algorithm that is $O(1)$-competitive for constant depth trees,\nwhenever $k =(1+\\epsilon )h$ for any $\\epsilon > 0$. Finally, we give a general\nlower bound that any deterministic online algorithm has competitive ratio at\nleast 2.4 even for depth-2 HSTs and when $k/h$ is arbitrarily large. This gives\na surprising qualitative separation between uniform metrics and depth-2 HSTs\nfor the $(h,k)$-server problem, and gives the strongest known lower bound for\nthe problem on general metrics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 16:01:31 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 16:03:27 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Bansal", "Nikhil", ""], ["Eli\u00e1\u0161", "Marek", ""], ["Je\u017c", "\u0141ukasz", ""], ["Koumoutsos", "Grigorios", ""]]}, {"id": "1608.08538", "submitter": "Michael Bekos", "authors": "Patrizio Angelini, Michael A. Bekos, Till Bruckdorfer, Jaroslav\n  Han\\v{c}l Jr., Michael Kaufmann, Stephen Kobourov, Antonios Symvonis, Pavel\n  Valtr", "title": "Low Ply Drawings of Trees", "comments": "This is a complete access version of a paper that will appear in the\n  proceedings of GD2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recently introduced model of \\emph{low ply graph drawing}, in\nwhich the ply-disks of the vertices do not have many common overlaps, which\nresults in a good distribution of the vertices in the plane. The\n\\emph{ply-disk} of a vertex in a straight-line drawing is the disk centered at\nit whose radius is half the length of its longest incident edge. The largest\nnumber of ply-disks having a common overlap is called the \\emph{ply-number} of\nthe drawing.\n  We focus on trees. We first consider drawings of trees with constant\nply-number, proving that they may require exponential area, even for stars, and\nthat they may not even exist for bounded-degree trees. Then, we turn our\nattention to drawings with logarithmic ply-number and show that trees with\nmaximum degree $6$ always admit such drawings in polynomial area.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 16:14:42 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 09:16:32 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bekos", "Michael A.", ""], ["Bruckdorfer", "Till", ""], ["Han\u010dl", "Jaroslav", "Jr."], ["Kaufmann", "Michael", ""], ["Kobourov", "Stephen", ""], ["Symvonis", "Antonios", ""], ["Valtr", "Pavel", ""]]}, {"id": "1608.08545", "submitter": "Massimo Cairo", "authors": "Massimo Cairo and Romeo Rizzi", "title": "Dynamic Controllability of Conditional Simple Temporal Networks is\n  PSPACE-complete", "comments": "Accepted to TIME2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even after the proposal of various solution algorithms, the precise\ncomputational complexity of checking whether a Conditional Temporal Network is\nDynamically Controllable had still remained widely open. This issue gets\nsettled in this paper which provides constructions, algorithms, and bridging\nlemmas and arguments to formally prove that: (1) the problem is PSPACE-hard,\nand (2) the problem lies in PSPACE.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 16:28:12 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Cairo", "Massimo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1608.08578", "submitter": "Martin Gronemann", "authors": "Martin Gronemann", "title": "Bitonic st-orderings for Upward Planar Graphs", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical orderings serve as the basis for many incremental planar drawing\nalgorithms. All these techniques, however, have in common that they are limited\nto undirected graphs. While $st$-orderings do extend to directed graphs,\nespecially planar $st$-graphs, they do not offer the same properties as\ncanonical orderings. In this work we extend the so called bitonic\n$st$-orderings to directed graphs. We fully characterize planar $st$-graphs\nthat admit such an ordering and provide a linear-time algorithm for recognition\nand ordering. If for a graph no bitonic $st$-ordering exists, we show how to\nfind in linear time a minimum set of edges to split such that the resulting\ngraph admits one. With this new technique we are able to draw every upward\nplanar graph on $n$ vertices by using at most one bend per edge, at most $n -\n3$ bends in total and within quadratic area.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 17:56:03 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Gronemann", "Martin", ""]]}, {"id": "1608.08648", "submitter": "Alexandros Gerbessiotis", "authors": "Alexandros V Gerbessiotis", "title": "Using parallelism techniques to improve sequential and multi-core\n  sorting performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new sequential sorting operations by adapting techniques and\nmethods used for designing parallel sorting algorithms. Although the norm is to\nparallelize a sequential algorithm to improve performance, we adapt a\ncontrarian approach: we employ parallel computing techniques to speed up\nsequential sorting. Our methods can also work for multi-core sorting with minor\nadjustments that do not necessarily require full parallelization of the\noriginal sequential algorithm. The proposed approach leads to the development\nof asymptotically efficient deterministic and randomized sorting operations\nwhose practical sequential and multi-core performance, as witnessed by an\nexperimental study, matches or surpasses existing optimized sorting algorithm\nimplementations.\n  We utilize parallel sorting techniques such as deterministic regular sampling\nand random oversampling. We extend the notion of deterministic regular sampling\ninto deterministic regular oversampling for sequential and multi-core sorting\nand demonstrate its potential. We then show how these techniques can be used\nfor sequential sorting and also lead to better multi-core sorting algorithm\nperformance as witnessed by the undertaken experimental study.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 20:23:46 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Gerbessiotis", "Alexandros V", ""]]}, {"id": "1608.08691", "submitter": "Muhammad Ali Raza Anjum", "authors": "Muhammad Ali Raza Anjum", "title": "One-Minute Derivation of The Conjugate Gradient Algorithm", "comments": null, "journal-ref": "Journal of Telematics and Informatics, Vol 4, No 1 (2016): March\n  2016", "doi": "10.12928/jti.v4i1", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the great triumphs in the history of numerical methods was the\ndiscovery of the Conjugate Gradient (CG) algorithm. It could solve a symmetric\npositive-definite system of linear equations of dimension N in exactly N steps.\nAs many practical problems at that time belonged to this category, CG algorithm\nbecame rapidly popular. It remains popular even today due to its immense\ncomputational power. But despite its amazing computational ability, mathematics\nof this algorithm is not easy to learn. Lengthy derivations, redundant\nnotations, and over-emphasis on formal presentation make it much difficult for\na beginner to master this algorithm. This paper aims to serve as a starting\npoint for such readers. It provides a curt, easy-to-follow but minimalist\nderivation of the algorithm by keeping the sufficient steps only, maintaining a\nuniform notation, and focusing entirely on the ease of reader.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 00:06:31 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Anjum", "Muhammad Ali Raza", ""]]}, {"id": "1608.08791", "submitter": "Tamara Mchedlidze David", "authors": "David Bremner and Olivier Devillers and Marc Glisse and Sylvain Lazard\n  and Giuseppe Liotta and Tamara Mchedlidze and Sue Whitesides and Stephen\n  Wismath", "title": "Monotone Simultaneous Embeddings of Paths in R^d", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem: Given $k$ paths that share the same vertex\nset, is there a simultaneous geometric embedding of these paths such that each\nindividual drawing is monotone in some direction? We prove that for any\ndimension $d \\geq 2$, there is a set of $d+1$ paths that does not admit a\nmonotone simultaneous geometric embedding.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 09:45:05 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Bremner", "David", ""], ["Devillers", "Olivier", ""], ["Glisse", "Marc", ""], ["Lazard", "Sylvain", ""], ["Liotta", "Giuseppe", ""], ["Mchedlidze", "Tamara", ""], ["Whitesides", "Sue", ""], ["Wismath", "Stephen", ""]]}, {"id": "1608.08909", "submitter": "Mark Ortmann", "authors": "Mark Ortmann, Mirza Klimenta and Ulrik Brandes", "title": "A Sparse Stress Model", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Force-directed layout methods constitute the most common approach to draw\ngeneral graphs. Among them, stress minimization produces layouts of\ncomparatively high quality but also imposes comparatively high computational\ndemands. We propose a speed-up method based on the aggregation of terms in the\nobjective function. It is akin to aggregate repulsion from far-away nodes\nduring spring embedding but transfers the idea from the layout space into a\npreprocessing phase. An initial experimental study informs a method to select\nrepresentatives, and subsequent more extensive experiments indicate that our\nmethod yields better approximations of minimum-stress layouts in less time than\nrelated methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 15:30:30 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 12:56:58 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 09:41:12 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ortmann", "Mark", ""], ["Klimenta", "Mirza", ""], ["Brandes", "Ulrik", ""]]}, {"id": "1608.08927", "submitter": "Payam Siyari", "authors": "Payam Siyari and Matthias Gall\\'e", "title": "The Generalized Smallest Grammar Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Smallest Grammar Problem -- the problem of finding the smallest\ncontext-free grammar that generates exactly one given sequence -- has never\nbeen successfully applied to grammatical inference. We investigate the reasons\nand propose an extended formulation that seeks to minimize non-recursive\ngrammars, instead of straight-line programs. In addition, we provide very\nefficient algorithms that approximate the minimization problem of this class of\ngrammars. Our empirical evaluation shows that we are able to find smaller\nmodels than the current best approximations to the Smallest Grammar Problem on\nstandard benchmarks, and that the inferred rules capture much better the\nsyntactic structure of natural language.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:23:07 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Siyari", "Payam", ""], ["Gall\u00e9", "Matthias", ""]]}, {"id": "1608.08952", "submitter": "Maurizio Patrignani", "authors": "Giordano Da Lozzo, Giuseppe Di Battista, Fabrizio Frati, Maurizio\n  Patrignani", "title": "Computing NodeTrix Representations of Clustered Graphs", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NodeTrix representations are a popular way to visualize clustered graphs;\nthey represent clusters as adjacency matrices and inter-cluster edges as curves\nconnecting the matrix boundaries. We study the complexity of constructing\nNodeTrix representations focusing on planarity testing problems, and we show\nseveral NP-completeness results and some polynomial-time algorithms. Building\non such algorithms we develop a JavaScript library for NodeTrix representations\naimed at reducing the crossings between edges incident to the same matrix.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:19:33 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 13:32:42 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1608.08970", "submitter": "Md. Jawaherul Alam", "authors": "Md. Jawaherul Alam, Michael T. Goodrich, and Timothy Johnson", "title": "J-Viz: Sibling-First Recursive Graph Drawing for Visualizing Java\n  Bytecode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a graph visualization tool for visualizing Java bytecode. Our\ntool, which we call J-Viz, visualizes connected directed graphs according to a\ncanonical node ordering, which we call the sibling-first recursive (SFR)\nnumbering. The particular graphs we consider are derived from applying Shiver's\nk-CFA framework to Java bytecode, and our visualizer includes helpful links\nbetween the nodes of an input graph and the Java bytecode that produced it, as\nwell as a decompiled version of that Java bytecode. We show through several\ncase studies that the canonical drawing paradigm used in J-Viz is effective for\nidentifying potential security vulnerabilities and repeated use of the same\ncode in Java applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:03:18 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Alam", "Md. Jawaherul", ""], ["Goodrich", "Michael T.", ""], ["Johnson", "Timothy", ""]]}]