[{"id": "1709.00164", "submitter": "Cole Lyman", "authors": "Cole A. Lyman, M. Stanley Fujimoto, Anton Suvorov, Paul M. Bodily,\n  Quinn Snell, Keith A. Crandall, Seth M. Bybee and Mark J. Clement", "title": "Whole Genome Phylogenetic Tree Reconstruction Using Colored de Bruijn\n  Graphs", "comments": "6 pages, 3 figures, accepted at BIBE 2017. Minor modifications to the\n  text due to reviewer feedback and fixed typos", "journal-ref": null, "doi": "10.1109/BIBE.2017.00-44", "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present kleuren, a novel assembly-free method to reconstruct phylogenetic\ntrees using the Colored de Bruijn Graph. kleuren works by constructing the\nColored de Bruijn Graph and then traversing it, finding bubble structures in\nthe graph that provide phylogenetic signal. The bubbles are then aligned and\nconcatenated to form a supermatrix, from which a phylogenetic tree is inferred.\nWe introduce the algorithms that kleuren uses to accomplish this task, and show\nits performance on reconstructing the phylogenetic tree of 12 Drosophila\nspecies. kleuren reconstructed the established phylogenetic tree accurately,\nand is a viable tool for phylogenetic tree reconstruction using whole genome\nsequences. Software package available at: https://github.com/Colelyman/kleuren\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 05:38:32 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 02:14:24 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lyman", "Cole A.", ""], ["Fujimoto", "M. Stanley", ""], ["Suvorov", "Anton", ""], ["Bodily", "Paul M.", ""], ["Snell", "Quinn", ""], ["Crandall", "Keith A.", ""], ["Bybee", "Seth M.", ""], ["Clement", "Mark J.", ""]]}, {"id": "1709.00228", "submitter": "Yang Cai", "authors": "Yang Cai, Constantinos Daskalakis", "title": "Learning Multi-item Auctions with (or without) Samples", "comments": "Appears in FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide algorithms that learn simple auctions whose revenue is\napproximately optimal in multi-item multi-bidder settings, for a wide range of\nvaluations including unit-demand, additive, constrained additive, XOS, and\nsubadditive. We obtain our learning results in two settings. The first is the\ncommonly studied setting where sample access to the bidders' distributions over\nvaluations is given, for both regular distributions and arbitrary distributions\nwith bounded support. Our algorithms require polynomially many samples in the\nnumber of items and bidders. The second is a more general max-min learning\nsetting that we introduce, where we are given \"approximate distributions,\" and\nwe seek to compute an auction whose revenue is approximately optimal\nsimultaneously for all \"true distributions\" that are close to the given ones.\nThese results are more general in that they imply the sample-based results, and\nare also applicable in settings where we have no sample access to the\nunderlying distributions but have estimated them indirectly via market research\nor by observation of previously run, potentially non-truthful auctions.\n  Our results hold for valuation distributions satisfying the standard (and\nnecessary) independence-across-items property. They also generalize and improve\nupon recent works, which have provided algorithms that learn approximately\noptimal auctions in more restricted settings with additive, subadditive and\nunit-demand valuations using sample access to distributions. We generalize\nthese results to the complete unit-demand, additive, and XOS setting, to i.i.d.\nsubadditive bidders, and to the max-min setting.\n  Our results are enabled by new uniform convergence bounds for hypotheses\nclasses under product measures. Our bounds result in exponential savings in\nsample complexity compared to bounds derived by bounding the VC dimension, and\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 10:07:18 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""]]}, {"id": "1709.00247", "submitter": "Tae Woo Kim", "authors": "Tae Woo Kim", "title": "A Simple Balanced Search Tree with No Balance Criterion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that maintains a balanced binary search tree without\nusing any tree balance criterion at all, with the ultimate aim of maximum\nsimplicity. In fact, our method is highly intuitive, and we only need to add\nminimal extra code and a simple partial-rebuilding algorithm to a naive binary\nsearch tree. Our method will be suitable as a highly simple and short solution\nwhen amortized logarithmic costs are enough.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 11:14:39 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 21:12:02 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Kim", "Tae Woo", ""]]}, {"id": "1709.00252", "submitter": "Tony Wauters", "authors": "Fabio Salassa, Wim Vancroonenburg, Tony Wauters, Federico Della Croce,\n  Greet Vanden Berghe", "title": "MILP and Max-Clique based heuristics for the Eternity II puzzle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper considers a hybrid local search approach to the Eternity II\npuzzle and to unsigned, rectangular, edge matching puzzles in general. Both an\noriginal mixed-integer linear programming (MILP) formulation and a novel\nMax-Clique formulation are presented for this NP-hard problem. Although the\npresented formulations remain computationally intractable for medium and large\nsized instances, they can serve as the basis for developing heuristic\ndecompositions and very large scale neighbourhoods. As a side product of the\nMax-Clique formulation, new hard-to-solve instances are published for the\nacademic research community. Two reasonably well performing MILP-based\nconstructive methods are presented and used for determining the initial\nsolution of a multi-neighbourhood local search approach. Experimental results\nconfirm that this local search can further improve the results obtained by the\nconstructive heuristics and is quite competitive with the state of the art\nprocedures.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 11:27:55 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 11:01:37 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Salassa", "Fabio", ""], ["Vancroonenburg", "Wim", ""], ["Wauters", "Tony", ""], ["Della Croce", "Federico", ""], ["Berghe", "Greet Vanden", ""]]}, {"id": "1709.00378", "submitter": "Yanlin Chen", "authors": "Yanlin Chen, Kai-Min Chung, Ching-Yi Lai", "title": "Space-efficient classical and quantum algorithms for the shortest vector\n  problem", "comments": null, "journal-ref": "QIC, Vol. 18 No.3&4 , 0285-0307 (2018)", "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lattice is the integer span of some linearly independent vectors. Lattice\nproblems have many significant applications in coding theory and cryptographic\nsystems for their conjectured hardness. The Shortest Vector Problem (SVP),\nwhich is to find the shortest non-zero vector in a lattice, is one of the\nwell-known problems that are believed to be hard to solve, even with a quantum\ncomputer. In this paper we propose space-efficient classical and quantum\nalgorithms for solving SVP. Currently the best time-efficient algorithm for\nsolving SVP takes $2^{n+o(n)}$ time and $2^{n+o(n)}$ space. Our classical\nalgorithm takes $2^{2.05n+o(n)}$ time to solve SVP with only $2^{0.5n+o(n)}$\nspace. We then modify our classical algorithm to a quantum version, which can\nsolve SVP in time $2^{1.2553n+o(n)}$ with $2^{0.5n+o(n)}$ classical space and\nonly poly(n) qubits.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 14:41:29 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 12:09:47 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Chen", "Yanlin", ""], ["Chung", "Kai-Min", ""], ["Lai", "Ching-Yi", ""]]}, {"id": "1709.00553", "submitter": "Daniel Martin PhD", "authors": "Daniel P. Martin", "title": "Dynamic Shortest Path and Transitive Closure Algorithms: A Survey", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms which compute properties over graphs have always been of interest\nin computer science, with some of the fundamental algorithms, such as\nDijkstra's algorithm, dating back to the 50s. Since the 70s there as been\ninterest in computing over graphs which are constantly changing, in a way which\nis more efficient than simple recomputing after each time the graph changes. In\nthis paper we provide a survey of both the foundational, and the state of the\nart, algorithms which solve either shortest path or transitive closure problems\nin either fully or partially dynamic graphs. We balance this with the known\nconditional lowerbounds.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 09:55:18 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 15:15:14 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Martin", "Daniel P.", ""]]}, {"id": "1709.00644", "submitter": "Sanmukh Rao Kuppannagari", "authors": "Sanmukh R. Kuppannagari, Rajgopal Kannan and Viktor K. Prasanna", "title": "Optimal Net-Load Balancing in Smart Grids with High PV Penetration", "comments": "11 pages. To be published in the 4th ACM International Conference on\n  Systems for Energy-Efficient Built Environments (BuildSys 17) Changes from\n  previous version: Fixed a bug in Algorithm 1 which was causing some min cost\n  solutions to be missed", "journal-ref": null, "doi": "10.1145/3137133.3137145", "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitigating Supply-Demand mismatch is critical for smooth power grid\noperation. Traditionally, load curtailment techniques such as Demand Response\n(DR) have been used for this purpose. However, these cannot be the only\ncomponent of a net-load balancing framework for Smart Grids with high PV\npenetration. These grids can sometimes exhibit supply surplus causing\nover-voltages. Supply curtailment techniques such as Volt-Var Optimizations are\ncomplex and computationally expensive. This increases the complexity of\nnet-load balancing systems used by the grid operator and limits their\nscalability. Recently new technologies have been developed that enable the\nrapid and selective connection of PV modules of an installation to the grid.\nTaking advantage of these advancements, we develop a unified optimal net-load\nbalancing framework which performs both load and solar curtailment. We show\nthat when the available curtailment values are discrete, this problem is\nNP-hard and develop bounded approximation algorithms for minimizing the\ncurtailment cost. Our algorithms produce fast solutions, given the tight timing\nconstraints required for grid operation. We also incorporate the notion of\nfairness to ensure that curtailment is evenly distributed among all the nodes.\nFinally, we develop an online algorithm which performs net-load balancing using\nonly data available for the current interval. Using both theoretical analysis\nand practical evaluations, we show that our net-load balancing algorithms\nprovide solutions which are close to optimal in a small amount of time.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 23:18:11 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 22:31:47 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 20:28:17 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Kuppannagari", "Sanmukh R.", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor K.", ""]]}, {"id": "1709.00722", "submitter": "Kjell Winblad", "authors": "Kjell Winblad", "title": "Faster Concurrent Range Queries with Contention Adapting Search Trees\n  Using Immutable Data", "comments": "12 pages, 21 figures, To be published in 2017 Imperial College\n  Computing Student Workshop (ICCSW 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for scalable concurrent ordered set data structures with\nlinearizable range query support is increasing due to the rise of multicore\ncomputers, data processing platforms and in-memory databases. This paper\npresents a new concurrent ordered set with linearizable range query support.\nThe new data structure is based on the contention adapting search tree and an\nimmutable data structure. Experimental results show that the new data structure\nis as much as three times faster compared to related data structures. The data\nstructure scales well due to its ability to adapt the sizes of its immutable\nparts to the contention level and the sizes of the range queries.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 14:25:24 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Winblad", "Kjell", ""]]}, {"id": "1709.00869", "submitter": "Anna Ben-Hamou", "authors": "Anna Ben-Hamou, Roberto I. Oliveira and Yuval Peres", "title": "Estimating graph parameters with random walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm observes the trajectories of random walks over an unknown graph\n$G$, starting from the same vertex $x$, as well as the degrees along the\ntrajectories. For all finite connected graphs, one can estimate the number of\nedges $m$ up to a bounded factor in\n$O\\left(t_{\\mathrm{rel}}^{3/4}\\sqrt{m/d}\\right)$ steps, where\n$t_{\\mathrm{rel}}$ is the relaxation time of the lazy random walk on $G$ and\n$d$ is the minimum degree in $G$. Alternatively, $m$ can be estimated in\n$O\\left(t_{\\mathrm{unif}} +t_{\\mathrm{rel}}^{5/6}\\sqrt{n}\\right)$, where $n$ is\nthe number of vertices and $t_{\\mathrm{unif}}$ is the uniform mixing time on\n$G$. The number of vertices $n$ can then be estimated up to a bounded factor in\nan additional $O\\left(t_{\\mathrm{unif}}\\frac{m}{n}\\right)$ steps. Our\nalgorithms are based on counting the number of intersections of random walk\npaths $X,Y$, i.e. the number of pairs $(t,s)$ such that $X_t=Y_s$. This\nimproves on previous estimates which only consider collisions (i.e., times $t$\nwith $X_t=Y_t$). We also show that the complexity of our algorithms is optimal,\neven when restricting to graphs with a prescribed relaxation time. Finally, we\nshow that, given either $m$ or the mixing time of $G$, we can compute the\n\"other parameter\" with a self-stopping algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 08:58:43 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 16:23:33 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Ben-Hamou", "Anna", ""], ["Oliveira", "Roberto I.", ""], ["Peres", "Yuval", ""]]}, {"id": "1709.00901", "submitter": "Christopher Purcell", "authors": "Jukka Kohonen, Janne H. Korhonen, Christopher Purcell, Jukka Suomela,\n  Przemys{\\l}aw Uzna\\'nski", "title": "Distributed Colour Reduction Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new, simple distributed algorithm for graph colouring in paths and\ncycles. Our algorithm is fast and self-contained, it does not need any globally\nconsistent orientation, and it reduces the number of colours from $10^{100}$ to\n$3$ in three iterations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 11:17:46 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kohonen", "Jukka", ""], ["Korhonen", "Janne H.", ""], ["Purcell", "Christopher", ""], ["Suomela", "Jukka", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1709.01123", "submitter": "Natali Ruchansky", "authors": "Natali Ruchansky, Francesco Bonchi, David Garcia-Soriano, Francesco\n  Gullo, Nicolas Kourtellis", "title": "To Be Connected, or Not to Be Connected: That is the Minimum\n  Inefficiency Subgraph Problem", "comments": "In Proceedings of the 26th ACM conference on Information and\n  Knowledge Management (CIKM 2017)", "journal-ref": null, "doi": "10.1145/3132847.3132991", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of extracting a selective connector for a given set of\nquery vertices $Q \\subseteq V$ in a graph $G = (V,E)$. A selective connector is\na subgraph of $G$ which exhibits some cohesiveness property, and contains the\nquery vertices but does not necessarily connect them all. Relaxing the\nconnectedness requirement allows the connector to detect multiple communities\nand to be tolerant to outliers. We achieve this by introducing the new measure\nof network inefficiency and by instantiating our search for a selective\nconnector as the problem of finding the minimum inefficiency subgraph.\n  We show that the minimum inefficiency subgraph problem is NP-hard, and devise\nefficient algorithms to approximate it. By means of several case studies in a\nvariety of application domains (such as human brain, cancer, and food\nnetworks), we show that our minimum inefficiency subgraph produces high-quality\nsolutions, exhibiting all the desired behaviors of a selective connector.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 19:09:57 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Ruchansky", "Natali", ""], ["Bonchi", "Francesco", ""], ["Garcia-Soriano", "David", ""], ["Gullo", "Francesco", ""], ["Kourtellis", "Nicolas", ""]]}, {"id": "1709.01152", "submitter": "L\\'aszl\\'o Kozma", "authors": "Dani Dorfman, Haim Kaplan, L\\'aszl\\'o Kozma, Uri Zwick", "title": "Pairing heaps: the forward variant", "comments": "small fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pairing heap is a classical heap data structure introduced in 1986 by\nFredman, Sedgewick, Sleator, and Tarjan. It is remarkable both for its\nsimplicity and for its excellent performance in practice. The \"magic\" of\npairing heaps lies in the restructuring that happens after the deletion of the\nsmallest item. The resulting collection of trees is consolidated in two rounds:\na left-to-right pairing round, followed by a right-to-left accumulation round.\nFredman et al. showed, via an elegant correspondence to splay trees, that in a\npairing heap of size $n$ all operations take $O(\\log{n})$ amortized time. They\nalso proposed an arguably more natural variant, where both pairing and\naccumulation are performed in a combined left-to-right round (called the\nforward variant of pairing heaps). The analogy to splaying breaks down in this\ncase, and the analysis of the forward variant was left open.\n  In this paper we show that inserting an item and deleting the minimum in a\nforward-variant pairing heap both take amortized time $O(\\log{n} \\cdot\n4^{\\sqrt{\\log{n}}} )$. This is the first improvement over the $O(\\sqrt{n})$\nbound showed by Fredman et al. three decades ago. Our analysis relies on a new\npotential function that tracks parent-child rank-differences in the heap.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 20:57:44 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 10:56:43 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Dorfman", "Dani", ""], ["Kaplan", "Haim", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Zwick", "Uri", ""]]}, {"id": "1709.01190", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Anshumali Shrivastava, Jonathan Wang, Junghee Ryu", "title": "FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High\n  Dimensional Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FLASH (\\textbf{F}ast \\textbf{L}SH \\textbf{A}lgorithm for\n\\textbf{S}imilarity search accelerated with \\textbf{H}PC), a similarity search\nsystem for ultra-high dimensional datasets on a single machine, that does not\nrequire similarity computations and is tailored for high-performance computing\nplatforms. By leveraging a LSH style randomized indexing procedure and\ncombining it with several principled techniques, such as reservoir sampling,\nrecent advances in one-pass minwise hashing, and count based estimations, we\nreduce the computational and parallelization costs of similarity search, while\nretaining sound theoretical guarantees.\n  We evaluate FLASH on several real, high-dimensional datasets from different\ndomains, including text, malicious URL, click-through prediction, social\nnetworks, etc. Our experiments shed new light on the difficulties associated\nwith datasets having several million dimensions. Current state-of-the-art\nimplementations either fail on the presented scale or are orders of magnitude\nslower than FLASH. FLASH is capable of computing an approximate k-NN graph,\nfrom scratch, over the full webspam dataset (1.3 billion nonzeros) in less than\n10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam\ndataset, using brute-force ($n^2D$), will require at least 20 teraflops. We\nprovide CPU and GPU implementations of FLASH for replicability of our results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:09:19 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 07:09:23 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Yiqiu", ""], ["Shrivastava", "Anshumali", ""], ["Wang", "Jonathan", ""], ["Ryu", "Junghee", ""]]}, {"id": "1709.01535", "submitter": "Noah Stephens-Davidowitz", "authors": "Divesh Aggarwal and Noah Stephens-Davidowitz", "title": "Just Take the Average! An Embarrassingly Simple $2^n$-Time Algorithm for\n  SVP (and CVP)", "comments": null, "journal-ref": "SOSA 2018", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a $2^{n+o(n)}$-time (and space) algorithm for the Shortest Vector\nProblem on lattices (SVP) that works by repeatedly running an embarrassingly\nsimple \"pair and average\" sieving-like procedure on a list of lattice vectors.\nThis matches the running time (and space) of the current fastest known\nalgorithm, due to Aggarwal, Dadush, Regev, and Stephens-Davidowitz (ADRS, in\nSTOC, 2015), with a far simpler algorithm. Our algorithm is in fact a\nmodification of the ADRS algorithm, with a certain careful rejection sampling\nstep removed.\n  The correctness of our algorithm follows from a more general \"meta-theorem,\"\nshowing that such rejection sampling steps are unnecessary for a certain class\nof algorithms and use cases. In particular, this also applies to the related\n$2^{n + o(n)}$-time algorithm for the Closest Vector Problem (CVP), due to\nAggarwal, Dadush, and Stephens-Davidowitz (ADS, in FOCS, 2015), yielding a\nsimilar embarrassingly simple algorithm for $\\gamma$-approximate CVP for any\n$\\gamma = 1+2^{-o(n/\\log n)}$. (We can also remove the rejection sampling\nprocedure from the $2^{n+o(n)}$-time ADS algorithm for exact CVP, but the\nresulting algorithm is still quite complicated.)\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 18:06:21 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1709.01670", "submitter": "Ren\\'e van Bevern", "authors": "Matthias Mnich, Ren\\'e van Bevern", "title": "Parameterized complexity of machine scheduling: 15 open problems", "comments": "Version accepted to Computers & Operations Research", "journal-ref": "Computers & Operations Research, 100:254--261, 2018", "doi": "10.1016/j.cor.2018.07.020", "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine scheduling problems are a long-time key domain of algorithms and\ncomplexity research. A novel approach to machine scheduling problems are\nfixed-parameter algorithms. To stimulate this thriving research direction, we\npropose 15 open questions in this area whose resolution we expect to lead to\nthe discovery of new approaches and techniques both in scheduling and\nparameterized complexity theory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 04:40:02 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 09:03:03 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 01:05:59 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Mnich", "Matthias", ""], ["van Bevern", "Ren\u00e9", ""]]}, {"id": "1709.01960", "submitter": "Arnold Filtser", "authors": "Stephen Alstrup, S{\\o}ren Dahlgaard, Arnold Filtser, Morten St\\\"ockel,\n  Christian Wulff-Nilsen", "title": "Constructing Light Spanners Deterministically in Near-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph spanners are well-studied and widely used both in theory and practice.\nIn a recent breakthrough, Chechik and Wulff-Nilsen [CW16] improved the\nstate-of-the-art for light spanners by constructing a\n$(2k-1)(1+\\epsilon)$-spanner with $O(n^{1+1/k})$ edges and\n$O_\\epsilon(n^{1/k})$ lightness. Soon after, Filtser and Solomon [FS16] showed\nthat the classic greedy spanner construction achieves the same bounds The major\ndrawback of the greedy spanner is its running time of $O(mn^{1+1/k})$ (which is\nfaster than [CW16]). This makes the construction impractical even for graphs of\nmoderate size. Much faster spanner constructions do exist but they only achieve\nlightness $\\Omega_\\epsilon(kn^{1/k})$, even when randomization is used.\n  The contribution of this paper is deterministic spanner constructions that\nare fast, and achieve similar bounds as the state-of-the-art slower\nconstructions. Our first result is an $O_\\epsilon(n^{2+1/k+\\epsilon'})$ time\nspanner construction which achieves the state-of-the-art bounds. Our second\nresult is an $O_\\epsilon(m + n\\log n)$ time construction of a spanner with\n$(2k-1)(1+\\epsilon)$ stretch, $O(\\log k\\cdot n^{1+1/k})$ edges and\n$O_\\epsilon(\\log k\\cdot n^{1/k})$ lightness. This is an exponential improvement\nin the dependence on $k$ compared to the previous result with such running\ntime. Finally, for the important special case where $k=\\log n$, for every\nconstant $\\epsilon>0$, we provide an $O(m+n^{1+\\epsilon})$ time construction\nthat produces an $O(\\log n)$-spanner with $O(n)$ edges and $O(1)$ lightness\nwhich is asymptotically optimal. This is the first known sub-quadratic\nconstruction of such a spanner for any $k = \\omega(1)$. To achieve our\nconstructions, we show a novel deterministic incremental approximate distance\noracle, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 18:32:05 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Alstrup", "Stephen", ""], ["Dahlgaard", "S\u00f8ren", ""], ["Filtser", "Arnold", ""], ["St\u00f6ckel", "Morten", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1709.01982", "submitter": "Zhuan Khye Koh", "authors": "Zhuan Khye Koh, Laura Sanit\\`a", "title": "Stabilizing Weighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An edge-weighted graph $G=(V,E)$ is called stable if the value of a\nmaximum-weight matching equals the value of a maximum-weight fractional\nmatching. Stable graphs play an important role in some interesting game theory\nproblems, such as network bargaining games and cooperative matching games,\nbecause they characterize instances which admit stable outcomes. Motivated by\nthis, in the last few years many researchers have investigated the algorithmic\nproblem of turning a given graph into a stable one, via edge- and\nvertex-removal operations. However, all the algorithmic results developed in\nthe literature so far only hold for unweighted instances, i.e., assuming unit\nweights on the edges of $G$.\n  We give the first polynomial-time algorithm to find a minimum cardinality\nsubset of vertices whose removal from $G$ yields a stable graph, for any\nweighted graph $G$. The algorithm is combinatorial and exploits new structural\nproperties of basic fractional matchings, which are of independent interest. In\nparticular, one of the main ingredients of our result is the development of a\npolynomial-time algorithm to compute a basic maximum-weight fractional matching\nwith minimum number of odd cycles in its support. This generalizes a\nfundamental and classical result on unweighted matchings given by Balas more\nthan 30 years ago, which we expect to prove useful beyond this particular\napplication.\n  In contrast, we show that the problem of finding a minimum cardinality subset\nof edges whose removal from a weighted graph $G$ yields a stable graph, does\nnot admit any constant-factor approximation algorithm, unless $P=NP$. In this\nsetting, we develop an $O(\\Delta)$-approximation algorithm for the problem,\nwhere $\\Delta$ is the maximum degree of a node in $G$.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 20:21:55 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 00:14:01 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Koh", "Zhuan Khye", ""], ["Sanit\u00e0", "Laura", ""]]}, {"id": "1709.02087", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Sharp Bounds for Generalized Uniformity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of generalized uniformity testing \\cite{BC17} of a\ndiscrete probability distribution: Given samples from a probability\ndistribution $p$ over an {\\em unknown} discrete domain $\\mathbf{\\Omega}$, we\nwant to distinguish, with probability at least $2/3$, between the case that $p$\nis uniform on some {\\em subset} of $\\mathbf{\\Omega}$ versus $\\epsilon$-far, in\ntotal variation distance, from any such uniform distribution.\n  We establish tight bounds on the sample complexity of generalized uniformity\ntesting. In more detail, we present a computationally efficient tester whose\nsample complexity is optimal, up to constant factors, and a matching\ninformation-theoretic lower bound. Specifically, we show that the sample\ncomplexity of generalized uniformity testing is\n$\\Theta\\left(1/(\\epsilon^{4/3}\\|p\\|_3) + 1/(\\epsilon^{2} \\|p\\|_2) \\right)$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 06:16:08 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1709.02099", "submitter": "EPTCS", "authors": "Massimo Benerecetti (Universit\\`a degli Studi di Napoli Federico II),\n  Daniele Dell'Erba (Universit\\`a degli Studi di Napoli Federico II), Fabio\n  Mogavero (Universit\\`a degli Studi di Verona)", "title": "Robust Exponential Worst Cases for Divide-et-Impera Algorithms for\n  Parity Games", "comments": "In Proceedings GandALF 2017, arXiv:1709.01761", "journal-ref": "EPTCS 256, 2017, pp. 121-135", "doi": "10.4204/EPTCS.256.9", "report-no": null, "categories": "cs.LO cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The McNaughton-Zielonka divide et impera algorithm is the simplest and most\nflexible approach available in the literature for determining the winner in a\nparity game. Despite its theoretical worst-case complexity and the negative\nreputation as a poorly effective algorithm in practice, it has been shown to\nrank among the best techniques for the solution of such games. Also, it proved\nto be resistant to a lower bound attack, even more than the strategy\nimprovements approaches, and only recently a family of games on which the\nalgorithm requires exponential time has been provided by Friedmann. An easy\nanalysis of this family shows that a simple memoization technique can help the\nalgorithm solve the family in polynomial time. The same result can also be\nachieved by exploiting an approach based on the dominion-decomposition\ntechniques proposed in the literature. These observations raise the question\nwhether a suitable combination of dynamic programming and game-decomposition\ntechniques can improve on the exponential worst case of the original algorithm.\nIn this paper we answer this question negatively, by providing a robustly\nexponential worst case, showing that no intertwining of the above mentioned\ntechniques can help mitigating the exponential nature of the divide et impera\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 06:56:46 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Benerecetti", "Massimo", "", "Universit\u00e0 degli Studi di Napoli Federico II"], ["Dell'Erba", "Daniele", "", "Universit\u00e0 degli Studi di Napoli Federico II"], ["Mogavero", "Fabio", "", "Universit\u00e0 degli Studi di Verona"]]}, {"id": "1709.02225", "submitter": "Niv Gabso", "authors": "Assaf Yifrach, Niv Gabso", "title": "Enhancing KiWi - Scalable Concurrent Key-Value Map", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a relatively fresh wait-free, concurrent sorted map called KiWi, fix\nand enhance it. First, we test its linearizability by fuzzing and applying\nWing&Gong [2] linearizability test. After fixing a few bugs in the algorithm\ndesign and its implementation, we enhance it. We design, implement and test two\nnew linearizable operations sizeLowerBound() and sizeUpperBound(). We further\ncompose these operations to create more useful operations. Last, we evaluate\nthe map performance because previous evaluations became obsolete due to our bug\ncorrections.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 13:23:27 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 06:02:36 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Yifrach", "Assaf", ""], ["Gabso", "Niv", ""]]}, {"id": "1709.02311", "submitter": "Radu Curticapean", "authors": "Radu Curticapean, Nathan Lindzey, Jesper Nederlof", "title": "A Tight Lower Bound for Counting Hamiltonian Cycles via Matrix Rank", "comments": "improved lower bounds modulo primes, improved figures, to appear in\n  SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For even $k$, the matchings connectivity matrix $\\mathbf{M}_k$ encodes which\npairs of perfect matchings on $k$ vertices form a single cycle. Cygan et al.\n(STOC 2013) showed that the rank of $\\mathbf{M}_k$ over $\\mathbb{Z}_2$ is\n$\\Theta(\\sqrt 2^k)$ and used this to give an $O^*((2+\\sqrt{2})^{\\mathsf{pw}})$\ntime algorithm for counting Hamiltonian cycles modulo $2$ on graphs of\npathwidth $\\mathsf{pw}$. The same authors complemented their algorithm by an\nessentially tight lower bound under the Strong Exponential Time Hypothesis\n(SETH). This bound crucially relied on a large permutation submatrix within\n$\\mathbf{M}_k$, which enabled a \"pattern propagation\" commonly used in previous\nrelated lower bounds, as initiated by Lokshtanov et al. (SODA 2011).\n  We present a new technique for a similar pattern propagation when only a\nblack-box lower bound on the asymptotic rank of $\\mathbf{M}_k$ is given; no\nstronger structural insights such as the existence of large permutation\nsubmatrices in $\\mathbf{M}_k$ are needed. Given appropriate rank bounds, our\ntechnique yields lower bounds for counting Hamiltonian cycles (also modulo\nfixed primes $p$) parameterized by pathwidth.\n  To apply this technique, we prove that the rank of $\\mathbf{M}_k$ over the\nrationals is $4^k / \\mathrm{poly}(k)$. We also show that the rank of\n$\\mathbf{M}_k$ over $\\mathbb{Z}_p$ is $\\Omega(1.97^k)$ for any prime $p\\neq 2$\nand even $\\Omega(2.15^k)$ for some primes.\n  As a consequence, we obtain that Hamiltonian cycles cannot be counted in time\n$O^*((6-\\epsilon)^{\\mathsf{pw}})$ for any $\\epsilon>0$ unless SETH fails. This\nbound is tight due to a $O^*(6^{\\mathsf{pw}})$ time algorithm by Bodlaender et\nal. (ICALP 2013). Under SETH, we also obtain that Hamiltonian cycles cannot be\ncounted modulo primes $p\\neq 2$ in time $O^*(3.97^\\mathsf{pw})$, indicating\nthat the modulus can affect the complexity in intricate ways.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 15:29:11 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 17:17:10 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Curticapean", "Radu", ""], ["Lindzey", "Nathan", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1709.02334", "submitter": "Romain Aza\\\"is", "authors": "Romain Aza\\\"is", "title": "Nearest Embedded and Embedding Self-Nested Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-nested trees present a systematic form of redundancy in their subtrees\nand thus achieve optimal compression rates by DAG compression. A method for\nquantifying the degree of self-similarity of plants through self-nested trees\nhas been introduced by Godin and Ferraro in 2010. The procedure consists in\ncomputing a self-nested approximation, called the nearest embedding self-nested\ntree, that both embeds the plant and is the closest to it. In this paper, we\npropose a new algorithm that computes the nearest embedding self-nested tree\nwith a smaller overall complexity, but also the nearest embedded self-nested\ntree. We show from simulations that the latter is mostly the closest to the\ninitial data, which suggests that this better approximation should be used as a\nprivileged measure of the degree of self-similarity of plants.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:17:09 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 08:55:51 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Aza\u00efs", "Romain", ""]]}, {"id": "1709.02377", "submitter": "Torben Hagerup", "authors": "Tim Baumann and Torben Hagerup", "title": "Rank-Select Indices Without Tears", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rank-select index for a sequence $B=(b_1,\\ldots,b_n)$ of $n$ bits is a data\nstructure that, if provided with an operation to access $\\Theta(\\log n)$\narbitrary consecutive bits of $B$ in constant time (thus $B$ is stored outside\nof the data structure), can compute $\\mathit{rank}_B(j)=\\sum_{i=1}^j b_i$ for\ngiven $j\\in\\{0,\\ldots,n\\}$ and\n$\\mathit{select}_B(k)=\\min\\{j:\\mathit{rank}_B(j)\\ge k\\}$ for given\n$k\\in\\{1,\\ldots,\\sum_{i=1}^n b_i\\}$. We describe a new rank-select index that,\nlike previous rank-select indices, occupies $O(n\\log\\log n/\\log n)$ bits and\nexecutes $\\mathit{rank}$ and $\\mathit{select}$ queries in constant time. Its\nderivation is intended to be particularly easy to follow and largely free of\ntedious low-level detail, its operations are given by straight-line code, and\nwe show that it can be constructed in $O(n/\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:53:44 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Baumann", "Tim", ""], ["Hagerup", "Torben", ""]]}, {"id": "1709.02500", "submitter": "Mark Amo-Boateng PhD.", "authors": "Mark Amo-Boateng", "title": "Super-speeds with Zero-RAM: Next Generation Large-Scale Optimization in\n  Your Laptop!", "comments": "7 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the novel breakthrough general purpose algorithm for\nlarge scale optimization problems. The novel algorithm is capable of achieving\nbreakthrough speeds for very large-scale optimization on general purpose\nlaptops and embedded systems. Application of the algorithm to the Griewank\nfunction was possible in up to 1 billion decision variables in double precision\ntook only 64485 seconds (~18 hours) to solve, while consuming 7,630 MB (7.6 GB)\nor RAM on a single threaded laptop CPU. It shows that the algorithm is\ncomputationally and memory (space) linearly efficient, and can find the optimal\nor near-optimal solution in a fraction of the time and memory that many\nconventional algorithms require. It is envisaged that this will open up new\npossibilities of real-time large-scale problems on personal laptops and\nembedded systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 01:47:56 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 01:14:46 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Amo-Boateng", "Mark", ""]]}, {"id": "1709.02579", "submitter": "Yoshio Okamoto", "authors": "Paz Carmi, Man Kwun Chiu, Matthew J. Katz, Matias Korman, Yoshio\n  Okamoto, Andr\\'e van Renssen, Marcel Roeloffzen, Taichi Shiitada, Shakhar\n  Smorodinsky", "title": "Balanced Line Separators of Unit Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a geometric version of the graph separator theorem for the unit disk\nintersection graph: for any set of $n$ unit disks in the plane there exists a\nline $\\ell$ such that $\\ell$ intersects at most $O(\\sqrt{(m+n)\\log{n}})$ disks\nand each of the halfplanes determined by $\\ell$ contains at most $2n/3$ unit\ndisks from the set, where $m$ is the number of intersecting pairs of disks. We\nalso show that an axis-parallel line intersecting $O(\\sqrt{m+n})$ disks exists,\nbut each halfplane may contain up to $4n/5$ disks. We give an almost tight\nlower bound (up to sublogarithmic factors) for our approach, and also show that\nno line-separator of sublinear size in $n$ exists when we look at disks of\narbitrary radii, even when $m=0$. Proofs are constructive and suggest simple\nalgorithms that run in linear time. Experimental evaluation has also been\nconducted, which shows that for random instances our method outperforms the\nmethod by Fox and Pach (whose separator has size $O(\\sqrt{m})$).\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 07:54:32 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 02:07:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Carmi", "Paz", ""], ["Chiu", "Man Kwun", ""], ["Katz", "Matthew J.", ""], ["Korman", "Matias", ""], ["Okamoto", "Yoshio", ""], ["van Renssen", "Andr\u00e9", ""], ["Roeloffzen", "Marcel", ""], ["Shiitada", "Taichi", ""], ["Smorodinsky", "Shakhar", ""]]}, {"id": "1709.02592", "submitter": "Christoph D\\\"urr", "authors": "Christoph D\\\"urr, Thomas Erlebach, Nicole Megow and Julie Mei{\\ss}ner", "title": "An Adversarial Model for Scheduling with Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel adversarial model for scheduling with explorable\nuncertainty. In this model, the processing time of a job can potentially be\nreduced (by an a priori unknown amount) by testing the job. Testing a job $j$\ntakes one unit of time and may reduce its processing time from the given upper\nlimit $\\bar{p}_j$ (which is the time taken to execute the job if it is not\ntested) to any value between $0$ and $\\bar{p}_j$. This setting is motivated\ne.g. by applications where a code optimizer can be run on a job before\nexecuting it. We consider the objective of minimizing the sum of completion\ntimes on a single machine. All jobs are available from the start, but the\nreduction in their processing times as a result of testing is unknown, making\nthis an online problem that is amenable to competitive analysis. The need to\nbalance the time spent on tests and the time spent on job executions adds a\nnovel flavor to the problem. We give the first and nearly tight lower and upper\nbounds on the competitive ratio for deterministic and randomized algorithms. We\nalso show that minimizing the makespan is a considerably easier problem for\nwhich we give optimal deterministic and randomized online algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 08:53:18 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 11:25:31 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 08:55:03 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["D\u00fcrr", "Christoph", ""], ["Erlebach", "Thomas", ""], ["Megow", "Nicole", ""], ["Mei\u00dfner", "Julie", ""]]}, {"id": "1709.02599", "submitter": "Stepan Kochemazov", "authors": "Stepan Kochemazov, Eduard Vatutin, Oleg Zaikin", "title": "Fast Algorithm for Enumerating Diagonal Latin Squares of Small Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an algorithm for enumerating diagonal Latin squares\nof small order. It relies on specific properties of diagonal Latin squares to\nemploy symmetry breaking techniques, and on several heuristic optimizations and\nbit arithmetic techniques to make use of computational power of\nstate-of-the-art CPUs. Using this approach we enumerated diagonal Latin squares\nof order at most 9, and vertically symmetric diagonal Latin squares of order at\nmost 10.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 09:07:14 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Kochemazov", "Stepan", ""], ["Vatutin", "Eduard", ""], ["Zaikin", "Oleg", ""]]}, {"id": "1709.02674", "submitter": "Pu Gao", "authors": "Pu Gao and Nicholas Wormald", "title": "Uniform generation of random graphs with power-law degree sequences", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a linear-time algorithm that approximately uniformly generates a\nrandom simple graph with a power-law degree sequence whose exponent is at least\n2.8811. While sampling graphs with power-law degree sequence of exponent at\nleast 3 is fairly easy, and many samplers work efficiently in this case, the\nproblem becomes dramatically more difficult when the exponent drops below 3;\nours is the first provably practicable sampler for this case. We also show that\nwith an appropriate rejection scheme, our algorithm can be tuned into an exact\nuniform sampler. The running time of the exact sampler is O(n^{2.107}) with\nhigh probability, and O(n^{4.081}) in expectation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 12:25:33 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 04:56:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Gao", "Pu", ""], ["Wormald", "Nicholas", ""]]}, {"id": "1709.02837", "submitter": "Heather Guarnera", "authors": "Feodor F. Dragan and Heather M. Guarnera", "title": "Obstructions to a small hyperbolicity in Helly graphs", "comments": "18 pages, 16 figures", "journal-ref": "Discrete Mathematics. 342 (2019) 326-338", "doi": "10.1016/j.disc.2018.10.017", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that for every graph $G$ there exists the smallest Helly graph\n$\\cal H(G)$ into which $G$ isometrically embeds ($\\cal H(G)$ is called the\ninjective hull of $G$) such that the hyperbolicity of $\\cal H(G)$ is equal to\nthe hyperbolicity of $G$. Motivated by this, we investigate structural\nproperties of Helly graphs that govern their hyperbolicity and identify three\nisometric subgraphs of the King-grid as structural obstructions to a small\nhyperbolicity in Helly graphs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 19:14:00 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 19:24:02 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Dragan", "Feodor F.", ""], ["Guarnera", "Heather M.", ""]]}, {"id": "1709.02850", "submitter": "Piotr Skowron", "authors": "Robert Bredereck, Piotr Faliszewski, Rolf Niedermeier, Piotr Skowron,\n  Nimrod Talmon", "title": "Mixed Integer Programming with Convex/Concave Constraints:\n  Fixed-Parameter Tractability and Applications to Multicovering and Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic result of Lenstra [Math.~Oper.~Res.~1983] says that an integer\nlinear program can be solved in fixed-parameter tractable (FPT) time for the\nparameter being the number of variables. We extend this result by incorporating\nnon-decreasing piecewise linear convex or concave functions to our (mixed)\ninteger programs. This general technique allows us to establish parameterized\ncomplexity of a number of classic computational problems. In particular, we\nprove that Weighted Set Multicover is in FPT when parameterized by the number\nof elements to cover, and that there exists an FPT-time approximation scheme\nfor Multiset Multicover for the same parameter. Further, we use our general\ntechnique to prove that a number of problems from computational social choice\n(e.g., problems related to bribery and control in elections) are in FPT when\nparameterized by the number of candidates. For bribery, this resolves a nearly\n10-year old family of open problems, and for weighted electoral control of\nApproval voting, this improves some previously known XP-memberships to\nFPT-memberships.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 20:11:50 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 22:45:36 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Bredereck", "Robert", ""], ["Faliszewski", "Piotr", ""], ["Niedermeier", "Rolf", ""], ["Skowron", "Piotr", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1709.02910", "submitter": "Tasuku Soma", "authors": "Tasuku Soma, Yuichi Yoshida", "title": "A New Approximation Guarantee for Monotone Submodular Function\n  Maximization via Discrete Convexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In monotone submodular function maximization, approximation guarantees based\non the curvature of the objective function have been extensively studied in the\nliterature. However, the notion of curvature is often pessimistic, and we\nrarely obtain improved approximation guarantees, even for very simple objective\nfunctions.\n  In this paper, we provide a novel approximation guarantee by extracting an\nM$^\\natural$-concave function $h:2^E \\to \\mathbb R_+$, a notion in discrete\nconvex analysis, from the objective function $f:2^E \\to \\mathbb R_+$. We\nintroduce the notion of $h$-curvature, which measures how much $f$ deviates\nfrom $h$, and show that we can obtain a $(1-\\gamma/e-\\epsilon)$-approximation\nto the problem of maximizing $f$ under a cardinality constraint in polynomial\ntime for any constant $\\epsilon > 0$. Then, we show that we can obtain\nnontrivial approximation guarantees for various problems by applying the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 04:59:59 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Soma", "Tasuku", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1709.02917", "submitter": "Vasileios Nakos", "authors": "Yi Li and Vasileios Nakos", "title": "Sublinear-Time Algorithms for Compressive Phase Retrieval", "comments": "The ell_2/ell_2 algorithm was substituted by a modification of the\n  ell_infty/ell_2 algorithm which strictly subsumes it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the compressive phase retrieval problem, or phaseless compressed sensing,\nor compressed sensing from intensity only measurements, the goal is to\nreconstruct a sparse or approximately $k$-sparse vector $x \\in \\mathbb{R}^n$\ngiven access to $y= |\\Phi x|$, where $|v|$ denotes the vector obtained from\ntaking the absolute value of $v\\in\\mathbb{R}^n$ coordinate-wise. In this paper\nwe present sublinear-time algorithms for different variants of the compressive\nphase retrieval problem which are akin to the variants considered for the\nclassical compressive sensing problem in theoretical computer science. Our\nalgorithms use pure combinatorial techniques and near-optimal number of\nmeasurements.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 06:21:36 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 05:07:03 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 19:17:58 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 17:41:41 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Li", "Yi", ""], ["Nakos", "Vasileios", ""]]}, {"id": "1709.02919", "submitter": "Vasileios Nakos", "authors": "Yi Li, Vasileios Nakos and David Woodruff", "title": "On Low-Risk Heavy Hitters and Sparse Recovery Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the heavy hitters and related sparse recovery problems in the\nlow-failure probability regime. This regime is not well-understood, and has\nonly been studied for non-adaptive schemes. The main previous work is one on\nsparse recovery by Gilbert et al.(ICALP'13). We recognize an error in their\nanalysis, improve their results, and contribute new non-adaptive and adaptive\nsparse recovery algorithms, as well as provide upper and lower bounds for the\nheavy hitters problem with low failure probability.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 06:48:48 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 20:07:37 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2019 22:17:22 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Li", "Yi", ""], ["Nakos", "Vasileios", ""], ["Woodruff", "David", ""]]}, {"id": "1709.03033", "submitter": "Jianan Zhang", "authors": "Jianan Zhang, Eytan Modiano", "title": "Robust Routing in Interdependent Networks", "comments": "Preliminary version was presented at INFOCOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model of two interdependent networks, where every node in one\nnetwork depends on one or more supply nodes in the other network and a node\nfails if it loses all of its supply nodes. We develop algorithms to compute the\nfailure probability of a path, and obtain the most reliable path between a pair\nof nodes in a network, under the condition that each supply node fails\nindependently with a given probability. Our work generalizes the classical\nshared risk group model, by considering multiple risks associated with a node\nand letting a node fail if all the risks occur. Moreover, we study the diverse\nrouting problem by considering two paths between a pair of nodes. We define two\npaths to be $d$-failure resilient if at least one path survives after removing\n$d$ or fewer supply nodes, which generalizes the concept of disjoint paths in a\nsingle network, and risk-disjoint paths in a classical shared risk group model.\nWe compute the probability that both paths fail, and develop algorithms to\ncompute the most reliable pair of paths.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 03:24:52 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Jianan", ""], ["Modiano", "Eytan", ""]]}, {"id": "1709.03147", "submitter": "Kijung Shin", "authors": "Kijung Shin", "title": "WRS: Waiting Room Sampling for Accurate Triangle Counting in Real Graph\n  Streams", "comments": "to be published in IEEE International Conference on Data Mining 2017\n  (ICDM-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we cannot store all edges in a graph stream, which edges should we store\nto estimate the triangle count accurately?\n  Counting triangles (i.e., cycles of length three) is a fundamental graph\nproblem with many applications in social network analysis, web mining, anomaly\ndetection, etc. Recently, much effort has been made to accurately estimate\nglobal and local triangle counts in streaming settings with limited space.\nAlthough existing methods use sampling techniques without considering temporal\ndependencies in edges, we observe temporal locality in real dynamic graphs.\nThat is, future edges are more likely to form triangles with recent edges than\nwith older edges.\n  In this work, we propose a single-pass streaming algorithm called\nWaiting-Room Sampling (WRS) for global and local triangle counting. WRS\nexploits the temporal locality by always storing the most recent edges, which\nfuture edges are more likely to form triangles with, in the waiting room, while\nit uses reservoir sampling for the remaining edges. Our theoretical and\nempirical analyses show that WRS is: (a) Fast and 'any time': runs in linear\ntime, always maintaining and updating estimates while new edges arrive, (b)\nEffective: yields up to 47% smaller estimation error than its best competitors,\nand (c) Theoretically sound: gives unbiased estimates with small variances\nunder the temporal locality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 17:47:04 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 12:40:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Shin", "Kijung", ""]]}, {"id": "1709.03309", "submitter": "Yann Dauxais", "authors": "Yann Dauxais (UR1, LACODAM), Thomas Guyet (LACODAM), David\n  Gross-Amblard (DRUID), Andr\\'e Happe", "title": "Discriminant chronicles mining: Application to care pathways analytics", "comments": "Artificial Intelligence in Medicine, Jun 2017, Vienna, Austria. 2017,\n  16th Conference on Artificial Intelligence in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pharmaco-epidemiology (PE) is the study of uses and effects of drugs in well\ndefined populations. As medico-administrative databases cover a large part of\nthe population, they have become very interesting to carry PE studies. Such\ndatabases provide longitudinal care pathways in real condition containing\ntimestamped care events, especially drug deliveries. Temporal pattern mining\nbecomes a strategic choice to gain valuable insights about drug uses. In this\npaper we propose DCM, a new discriminant temporal pattern mining algorithm. It\nextracts chronicle patterns that occur more in a studied population than in a\ncontrol population. We present results on the identification of possible\nassociations between hospitalizations for seizure and anti-epileptic drug\nswitches in care pathway of epileptic patients.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 09:37:07 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Dauxais", "Yann", "", "UR1, LACODAM"], ["Guyet", "Thomas", "", "LACODAM"], ["Gross-Amblard", "David", "", "DRUID"], ["Happe", "Andr\u00e9", ""]]}, {"id": "1709.03517", "submitter": "Teresa Brooks", "authors": "Teresa Nicole Brooks, Rania Almajalid", "title": "Multi-Level Spherical Locality Sensitive Hashing For Approximate Near\n  Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces \"Multi-Level Spherical LSH\": parameter-free, a\nmulti-level, data-dependant Locality Sensitive Hashing data structure for\nsolving the Approximate Near Neighbors Problem (ANN). This data structure uses\na modified version of a multi-probe adaptive querying algorithm, with the\npotential of achieving a $O(n^p + t)$ query run time, for all inputs n where $t\n<= n$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 18:00:28 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 21:52:24 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Brooks", "Teresa Nicole", ""], ["Almajalid", "Rania", ""]]}, {"id": "1709.03565", "submitter": "Hung Nguyen", "authors": "Hung T. Nguyen and Tri P. Nguyen and NhatHai Phan and Thang N. Dinh", "title": "Importance Sketching of Influence Dynamics in Billion-scale Networks", "comments": "12 pages, to appear in ICDM 2017 as a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blooming availability of traces for social, biological, and communication\nnetworks opens up unprecedented opportunities in analyzing diffusion processes\nin networks. However, the sheer sizes of the nowadays networks raise serious\nchallenges in computational efficiency and scalability.\n  In this paper, we propose a new hyper-graph sketching framework for inflence\ndynamics in networks. The central of our sketching framework, called SKIS, is\nan efficient importance sampling algorithm that returns only non-singular\nreverse cascades in the network. Comparing to previously developed sketches\nlike RIS and SKIM, our sketch significantly enhances estimation quality while\nsubstantially reducing processing time and memory-footprint. Further, we\npresent general strategies of using SKIS to enhance existing algorithms for\ninfluence estimation and influence maximization which are motivated by\npractical applications like viral marketing. Using SKIS, we design high-quality\ninfluence oracle for seed sets with average estimation error up to 10x times\nsmaller than those using RIS and 6x times smaller than SKIM. In addition, our\ninfluence maximization using SKIS substantially improves the quality of\nsolutions for greedy algorithms. It achieves up to 10x times speed-up and 4x\nmemory reduction for the fastest RIS-based DSSA algorithm, while maintaining\nthe same theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 20:02:36 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Nguyen", "Hung T.", ""], ["Nguyen", "Tri P.", ""], ["Phan", "NhatHai", ""], ["Dinh", "Thang N.", ""]]}, {"id": "1709.03867", "submitter": "Chi-Yeh Chen", "authors": "Chi-Yeh Chen", "title": "An Efficient Approximation Algorithm for the Steiner Tree Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Steiner tree problem is one of the classic and most fundamental\n$\\mathcal{NP}$-hard problems: given an arbitrary weighted graph, seek a\nminimum-cost tree spanning a given subset of the vertices (terminals). Byrka\n\\emph{et al}. proposed a $1.3863+\\epsilon$-approximation algorithm in which the\nlinear program is solved at every iteration after contracting a component.\nGoemans \\emph{et al}. shown that it is possible to achieve the same\napproximation guarantee while only solving hypergraphic LP relaxation once.\nHowever, optimizing hypergraphic LP relaxation exactly is strongly NP-hard.\nThis article presents an efficient two-phase heuristic in greedy strategy that\nachieves an approximation ratio of $1.4295$.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 14:41:34 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 16:14:33 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 03:20:50 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 15:55:24 GMT"}, {"version": "v5", "created": "Thu, 1 Nov 2018 08:12:13 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Chen", "Chi-Yeh", ""]]}, {"id": "1709.04015", "submitter": "Daniel DiTursi", "authors": "Daniel J. DiTursi, Gregorios A. Katsios, Petko Bogdanov", "title": "Network Clocks: Detecting the Temporal Scale of Information Diffusion", "comments": "extended version of paper from ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information diffusion models typically assume a discrete timeline in which an\ninformation token spreads in the network. Since users in real-world networks\nvary significantly in their intensity and periods of activity, our objective in\nthis work is to answer: How to determine a temporal scale that best agrees with\nthe observed information propagation within a network? A key limitation of\nexisting approaches is that they aggregate the timeline into fixed-size\nwindows, which may not fit all network nodes' activity periods. We propose the\nnotion of a heterogeneous network clock: a mapping of events to discrete\ntimestamps that best explains their occurrence according to a given cascade\npropagation model. We focus on the widely-adopted independent cascade (IC)\nmodel and formalize the optimal clock as the one that maximizes the likelihood\nof all observed cascades. The single optimal clock (OC) problem can be solved\nexactly in polynomial time. However, we prove that learning multiple optimal\nclocks(kOC), corresponding to temporal patterns of groups of network nodes, is\nNP-hard. We propose scalable solutions that run in almost linear time in the\ntotal number of cascade activations and discuss approximation guarantees for\neach variant. Our algorithms and their detected clocks enable improved cascade\nsize classification (up to 8 percent F1 lift) and improved missing cascade data\ninference (0.15 better recall). We also demonstrate that the network clocks\nexhibit consistency within the type of content diffusing in the network and are\nrobust with respect to the propagation probability parameters of the IC model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 18:48:16 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["DiTursi", "Daniel J.", ""], ["Katsios", "Gregorios A.", ""], ["Bogdanov", "Petko", ""]]}, {"id": "1709.04048", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Data Sketches for Disaggregated Subset Sum and Frequent Item Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a new data sketch for processing massive datasets. It\naddresses two common problems: 1) computing a sum given arbitrary filter\nconditions and 2) identifying the frequent items or heavy hitters in a data\nset. For the former, the sketch provides unbiased estimates with state of the\nart accuracy. It handles the challenging scenario when the data is\ndisaggregated so that computing the per unit metric of interest requires an\nexpensive aggregation. For example, the metric of interest may be total clicks\nper user while the raw data is a click stream with multiple rows per user. Thus\nthe sketch is suitable for use in a wide range of applications including\ncomputing historical click through rates for ad prediction, reporting user\nmetrics from event streams, and measuring network traffic for IP flows.\n  We prove and empirically show the sketch has good properties for both the\ndisaggregated subset sum estimation and frequent item problems. On i.i.d. data,\nit not only picks out the frequent items but gives strongly consistent\nestimates for the proportion of each frequent item. The resulting sketch\nasymptotically draws a probability proportional to size sample that is optimal\nfor estimating sums over the data. For non i.i.d. data, we show that it\ntypically does much better than random sampling for the frequent item problem\nand never does worse. For subset sum estimation, we show that even for\npathological sequences, the variance is close to that of an optimal sampling\ndesign. Empirically, despite the disadvantage of operating on disaggregated\ndata, our method matches or bests priority sampling, a state of the art method\nfor pre-aggregated data and performs orders of magnitude better on skewed data\ncompared to uniform sampling. We propose extensions to the sketch that allow it\nto be used in combining multiple data sets, in distributed systems, and for\ntime decayed aggregation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 20:19:25 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "1709.04123", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Lada Adamic, Jon Kleinberg", "title": "Mitigating Overexposure in Viral Marketing", "comments": "In AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional models for word-of-mouth recommendations and viral marketing,\nthe objective function has generally been based on reaching as many people as\npossible. However, a number of studies have shown that the indiscriminate\nspread of a product by word-of-mouth can result in overexposure, reaching\npeople who evaluate it negatively. This can lead to an effect in which the\nover-promotion of a product can produce negative reputational effects, by\nreaching a part of the audience that is not receptive to it.\n  How should one make use of social influence when there is a risk of\noverexposure? In this paper, we develop and analyze a theoretical model for\nthis process; we show how it captures a number of the qualitative phenomena\nassociated with overexposure, and for the main formulation of our model, we\nprovide a polynomial-time algorithm to find the optimal marketing strategy. We\nalso present simulations of the model on real network topologies, quantifying\nthe extent to which our optimal strategies outperform natural baselines\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 03:32:12 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 23:23:55 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Abebe", "Rediet", ""], ["Adamic", "Lada", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1709.04161", "submitter": "Nimrod Talmon", "authors": "Danny Hermelin, Judith-Madeleine Kubitza, Dvir Shabtay, Nimrod Talmon,\n  Gerhard Woeginger", "title": "Scheduling Two Agents on a Single Machine: A Parameterized Analysis of\n  NP-hard Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling theory is an old and well-established area in combinatorial\noptimization, whereas the much younger area of parameterized complexity has\nonly recently gained the attention of the community. Our aim is to bring these\ntwo areas closer together by studying the parameterized complexity of a class\nof single-machine two-agent scheduling problems. Our analysis focuses on the\ncase where the number of jobs belonging to the second agent is considerably\nsmaller than the number of jobs belonging to the first agent, and thus can be\nconsidered as a fixed parameter k. We study a variety of combinations of\nscheduling criteria for the two agents, and for each such combination we\npinpoint its parameterized complexity with respect to the parameter k. The\nscheduling criteria that we analyze include the total weighted completion time,\nthe total weighted number of tardy jobs, and the total weighted number of\njust-in-time jobs. Our analysis draws a borderline between tractable and\nintractable variants of these problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 07:14:31 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Hermelin", "Danny", ""], ["Kubitza", "Judith-Madeleine", ""], ["Shabtay", "Dvir", ""], ["Talmon", "Nimrod", ""], ["Woeginger", "Gerhard", ""]]}, {"id": "1709.04169", "submitter": "Nimrod Talmon", "authors": "Danny Hermelin, Dvir Shabtay and Nimrod Talmon", "title": "On The Parameterized Tractability of the Just-In-Time Flow-Shop\n  Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its development in the early 90's, parameterized complexity has been\nwidely used to analyze the tractability of many NP-hard combinatorial\noptimization problems with respect to various types of problem parameters.\nWhile the generic nature of the framework allows the analysis of any\ncombinatorial problem, the main focus along the years was on analyzing graph\nproblems. In this paper we diverge from this trend by studying the\nparameterized complexity of Just-In-Time (JIT) flow-shop scheduling problems.\nOur analysis focuses on the case where the number of due dates is considerably\nsmaller than the number of jobs, and can thus be considered as a parameter. We\nprove that the two-machine problem is W[1]-hard with respect to this parameter,\neven if all processing times on the second machine are of unit length, while\nthe problem is in XP even for a parameterized number of machines. We then move\non to study the tractability of the problem when combining the different number\nof due dates with either the number of different weights or the number of\ndifferent processing times on the first machine. We prove that in both cases\nthe problem is fixed-parameter tractable for the two machine case, and is\nW[1]-hard for three or more machines.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 07:30:13 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Hermelin", "Danny", ""], ["Shabtay", "Dvir", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1709.04228", "submitter": "Fabrizio Frati", "authors": "Fabrizio Frati and Kwan-Liu Ma", "title": "Proceedings of the 25th International Symposium on Graph Drawing and\n  Network Visualization (GD 2017)", "comments": "Electronic self-archived proceedings. Proceedings are also to be\n  published by Springer in the Lecture Notes in Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.HC cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the arXiv index for the electronic proceedings of the 25th\nInternational Symposium on Graph Drawing and Network Visualization (GD 2017),\nwhich was held in Boston, U.S.A., September 25-27 2017. It contains the\npeer-reviewed and revised accepted papers with an optional appendix.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 10:05:37 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Frati", "Fabrizio", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1709.04459", "submitter": "Damir Hasi\\'c", "authors": "Damir Hasic, Eric Tannier", "title": "Gene tree reconciliation including transfers with replacement is hard\n  and FPT", "comments": "34 pages, 11 figures. J Comb Optim (2019)", "journal-ref": null, "doi": "10.1007/s10878-019-00396-z", "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees illustrate the evolutionary history of genes and species.\nIn most cases, although genes evolve along with the species they belong to, a\nspecies tree and gene tree are not identical, because of evolutionary events at\nthe gene level like duplication or transfer. These differences are handled by\nphylogenetic reconciliation, which formally is a mapping between gene tree\nnodes and species tree nodes and branches. We investigate models of\nreconciliation with a gene transfer that replaces existing gene, which is a\nbiological important event but never included in reconciliation models. Also\nthe problem is close to a dated version of the classical subtree prune and\nregraft (SPR) distance problem, where a pruned subtree has to be regrafted only\non a branch closer to the root. We prove that the reconciliation problem\nincluding transfer and replacement is NP-hard, and that if speciations and\ntransfers with replacement are the only allowed evolutionary events, then it is\nfixed-parameter tractable (FPT) with respect to the reconciliation's weight. We\nprove that the results extend to the dated SPR problem.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:58:20 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Hasic", "Damir", ""], ["Tannier", "Eric", ""]]}, {"id": "1709.04599", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi", "title": "Simple Round Compression for Parallel Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Czumaj et.al. (arXiv 2017) presented a parallel (almost)\n$2$-approximation algorithm for the maximum matching problem in only\n$O({(\\log\\log{n})^2})$ rounds of the massive parallel computation (MPC)\nframework, when the memory per machine is $O(n)$. The main approach in their\nwork is a way of compressing $O(\\log{n})$ rounds of a distributed algorithm for\nmaximum matching into only $O({(\\log\\log{n})^2})$ MPC rounds.\n  In this note, we present a similar algorithm for the closely related problem\nof approximating the minimum vertex cover in the MPC framework. We show that\none can achieve an $O(\\log{n})$ approximation to minimum vertex cover in only\n$O(\\log\\log{n})$ MPC rounds when the memory per machine is $O(n)$. Our\nalgorithm for vertex cover is similar to the maximum matching algorithm of\nCzumaj et.al. but avoids many of the intricacies in their approach and as a\nresult admits a considerably simpler analysis (at a cost of a worse\napproximation guarantee). We obtain this result by modifying a previous\nparallel algorithm by Khanna and the author (SPAA 2017) for vertex cover that\nallowed for compressing $O(\\log{n})$ rounds of a distributed algorithm into\nconstant MPC rounds when the memory allowed per machine is $O(n\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 02:49:01 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Assadi", "Sepehr", ""]]}, {"id": "1709.04688", "submitter": "Dimitrios Letsios", "authors": "Dimitrios Letsios, Georgia Kouyialis, Ruth Misener", "title": "Heuristics with Performance Guarantees for the Minimum Number of Matches\n  Problem in Heat Recovery Network Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat exchanger network synthesis exploits excess heat by integrating process\nhot and cold streams and improves energy efficiency by reducing utility usage.\nDetermining provably good solutions to the minimum number of matches is a\nbottleneck of designing a heat recovery network using the sequential method.\nThis subproblem is an NP-hard mixed-integer linear program exhibiting\ncombinatorial explosion in the possible hot and cold stream configurations. We\nexplore this challenging optimization problem from a graph theoretic\nperspective and correlate it with other special optimization problems such as\ncost flow network and packing problems. In the case of a single temperature\ninterval, we develop a new optimization formulation without problematic big-M\nparameters. We develop heuristic methods with performance guarantees using\nthree approaches: (i) relaxation rounding, (ii) water filling, and (iii) greedy\npacking. Numerical results from a collection of 51 instances substantiate the\nstrength of the methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 09:58:38 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 13:23:59 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Letsios", "Dimitrios", ""], ["Kouyialis", "Georgia", ""], ["Misener", "Ruth", ""]]}, {"id": "1709.04711", "submitter": "Salvatore Pontarelli", "authors": "Salvatore Pontarelli, Pedro Reviriego, Michael Mitzenmacher", "title": "EMOMA: Exact Match in One Memory Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important function in modern routers and switches is to perform a lookup\nfor a key. Hash-based methods, and in particular cuckoo hash tables, are\npopular for such lookup operations, but for large structures stored in off-chip\nmemory, such methods have the downside that they may require more than one\noff-chip memory access to perform the key lookup. Although the number of\noff-chip memory accesses can be reduced using on-chip approximate membership\nstructures such as Bloom filters, some lookups may still require more than one\noff-chip memory access. This can be problematic for some hardware\nimplementations, as having only a single off-chip memory access enables a\npredictable processing of lookups and avoids the need to queue pending\nrequests.\n  We provide a data structure for hash-based lookups based on cuckoo hashing\nthat uses only one off-chip memory access per lookup, by utilizing an on-chip\npre-filter to determine which of multiple locations holds a key. We make\nparticular use of the flexibility to move elements within a cuckoo hash table\nto ensure the pre-filter always gives the correct response. While this requires\na slightly more complex insertion procedure and some additional memory accesses\nduring insertions, it is suitable for most packet processing applications where\nkey lookups are much more frequent than insertions. An important feature of our\napproach is its simplicity. Our approach is based on simple logic that can be\neasily implemented in hardware, and hardware implementations would benefit most\nfrom the single off-chip memory access per lookup.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:32:52 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Pontarelli", "Salvatore", ""], ["Reviriego", "Pedro", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1709.04885", "submitter": "Ori Gurel-Gurevich", "authors": "Yoel Grinshpon, Ori Gurel-Gurevich", "title": "Optimal broadcasting in networks with faulty nodes", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large computer networks are an essential part of modern technology, and quite\noften information needs to be broadcast to all the computers in the network. If\nall computers work perfectly all the time, this is simple. Suppose, however,\nthat some of the computers fail occasionally. What is the fastest way to ensure\nthat with high probability all working computers get the information?\n  In this paper, we analyze three algorithms to do so. All algorithms terminate\nin logarithmic time, assuming computers fail with probability $1-p$\nindependently of each other. We prove that the third algorithm, which runs in\ntime $(1+o(1))(\\frac{\\log N}{\\log(1+p)})$, is asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 17:18:53 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Grinshpon", "Yoel", ""], ["Gurel-Gurevich", "Ori", ""]]}, {"id": "1709.05000", "submitter": "Cedric Bentz", "authors": "C\\'edric Bentz", "title": "Weighted and locally bounded list-colorings in split graphs, cographs,\n  and partial k-trees", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a fixed number of colors, we show that, in node-weighted split graphs,\ncographs, and graphs of bounded tree-width, one can determine in polynomial\ntime whether a proper list-coloring of the vertices of a graph such that the\ntotal weight of vertices of each color equals a given value in each part of a\nfixed partition of the vertices exists. We also show that this result is tight\nin some sense, by providing hardness results for the cases where any one of the\nassumptions does not hold. The edge-coloring variant is also studied, as well\nas special cases of cographs and split graphs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 22:47:42 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 00:23:57 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Bentz", "C\u00e9dric", ""]]}, {"id": "1709.05061", "submitter": "Mo Sha", "authors": "Mo Sha, Yuchen Li, Bingsheng He and Kian-Lee Tan", "title": "Technical Report: Accelerating Dynamic Graph Analytics on GPUs", "comments": "34 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As graph analytics often involves compute-intensive operations, GPUs have\nbeen extensively used to accelerate the processing. However, in many\napplications such as social networks, cyber security, and fraud detection,\ntheir representative graphs evolve frequently and one has to perform a rebuild\nof the graph structure on GPUs to incorporate the updates. Hence, rebuilding\nthe graphs becomes the bottleneck of processing high-speed graph streams. In\nthis paper, we propose a GPU-based dynamic graph storage scheme to support\nexisting graph algorithms easily. Furthermore, we propose parallel update\nalgorithms to support efficient stream updates so that the maintained graph is\nimmediately available for high-speed analytic processing on GPUs. Our extensive\nexperiments with three streaming applications on large-scale real and synthetic\ndatasets demonstrate the superior performance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 05:08:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 06:13:41 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Sha", "Mo", ""], ["Li", "Yuchen", ""], ["He", "Bingsheng", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1709.05282", "submitter": "Ryan Williams", "authors": "Ryan Williams", "title": "On the Difference Between Closest, Furthest, and Orthogonal Pairs:\n  Nearly-Linear vs Barely-Subquadratic Complexity in Computational Geometry", "comments": "13 pages", "journal-ref": "In ACM-SIAM Symposium on Discrete Algorithms (SODA), 2018", "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point location problems for $n$ points in $d$-dimensional Euclidean space\n(and $\\ell_p$ spaces more generally) have typically had two kinds of\nrunning-time solutions:\n  * (Nearly-Linear) less than $d^{poly(d)} \\cdot n \\log^{O(d)} n$ time, or\n  * (Barely-Subquadratic) $f(d) \\cdot n^{2-1/\\Theta(d)}$ time, for various $f$.\n  For small $d$ and large $n$, \"nearly-linear\" running times are generally\nfeasible, while \"barely-subquadratic\" times are generally infeasible. For\nexample, in the Euclidean metric, finding a Closest Pair among $n$ points in\n${\\mathbb R}^d$ is nearly-linear, solvable in $2^{O(d)} \\cdot n \\log^{O(1)} n$\ntime, while known algorithms for Furthest Pair (the diameter of the point set)\nare only barely-subquadratic, requiring $\\Omega(n^{2-1/\\Theta(d)})$ time. Why\ndo these proximity problems have such different time complexities? Is there a\nbarrier to obtaining nearly-linear algorithms for problems which are currently\nonly barely-subquadratic?\n  We give a novel exact and deterministic self-reduction for the Orthogonal\nVectors problem on $n$ vectors in $\\{0,1\\}^d$ to $n$ vectors in ${\\mathbb\nZ}^{\\omega(\\log d)}$ that runs in $2^{o(d)}$ time. As a consequence,\nbarely-subquadratic problems such as Euclidean diameter, Euclidean bichromatic\nclosest pair, ray shooting, and incidence detection do not have\n$O(n^{2-\\epsilon})$ time algorithms (in Turing models of computation) for\ndimensionality $d = \\omega(\\log \\log n)^2$, unless the popular Orthogonal\nVectors Conjecture and the Strong Exponential Time Hypothesis are false. That\nis, while poly-log-log-dimensional Closest Pair is in $n^{1+o(1)}$ time, the\nanalogous case of Furthest Pair can encode larger-dimensional problems\nconjectured to require $n^{2-o(1)}$ time. We also show that the All-Nearest\nNeighbors problem in $\\omega(\\log n)$ dimensions requires $n^{2-o(1)}$ time to\nsolve, assuming either of the above conjectures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:53:59 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Williams", "Ryan", ""]]}, {"id": "1709.05294", "submitter": "Ryan Williams", "authors": "Daniel Kane, Ryan Williams", "title": "The Orthogonal Vectors Conjecture for Branching Programs and Formulas", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Orthogonal Vectors (OV) problem, we wish to determine if there is an\northogonal pair of vectors among $n$ Boolean vectors in $d$ dimensions. The OV\nConjecture (OVC) posits that OV requires $n^{2-o(1)}$ time to solve, for all\n$d=\\omega(\\log n)$. Assuming the OVC, optimal time lower bounds have been\nproved for many prominent problems in $P$.\n  We prove that OVC is true in several computational models of interest:\n  * For all sufficiently large $n$ and $d$, OV for $n$ vectors in $\\{0,1\\}^d$\nhas branching program complexity $\\tilde{\\Theta}(n\\cdot \\min(n,2^d))$. In\nparticular, the lower bounds match the upper bounds up to polylog factors.\n  * OV has Boolean formula complexity $\\tilde{\\Theta}(n\\cdot \\min(n,2^d))$,\nover all complete bases of $O(1)$ fan-in.\n  * OV requires $\\tilde{\\Theta}(n\\cdot \\min(n,2^d))$ wires, in formulas\ncomprised of gates computing arbitrary symmetric functions of unbounded fan-in.\n  Our lower bounds basically match the best known (quadratic) lower bounds for\nany explicit function in those models. Analogous lower bounds hold for many\nrelated problems shown to be hard under OVC, such as Batch Partial Match, Batch\nSubset Queries, and Batch Hamming Nearest Neighbors, all of which have very\nsuccinct reductions to OV.\n  The proofs use a certain kind of input restriction that is different from\ntypical random restrictions where variables are assigned independently. We give\na sense in which independent random restrictions cannot be used to show\nhardness, in that OVC is false in the \"average case\" even for $AC^0$ formulas:\n  * For every fixed $p \\in (0,1)$ there is an $\\epsilon_p > 0$ such that for\nevery $n$ and $d$, OV instances where input bits are independently set to $1$\nwith probability $p$ (and $0$ otherwise) can be solved with $AC^0$ formulas of\nsize $O(n^{2-\\epsilon_p})$, on all but a $o_n(1)$ fraction of instances.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:26:34 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Kane", "Daniel", ""], ["Williams", "Ryan", ""]]}, {"id": "1709.05314", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "String Attractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $S$ be a string of length $n$. In this paper we introduce the notion of\n\\emph{string attractor}: a subset of the string's positions $[1,n]$ such that\nevery distinct substring of $S$ has an occurrence crossing one of the\nattractor's elements. We first show that the minimum attractor's size yields\nupper-bounds to the string's repetitiveness as measured by its linguistic\ncomplexity and by the length of its longest repeated substring. We then prove\nthat all known compressors for repetitive strings induce a string attractor\nwhose size is bounded by their associated repetitiveness measure, and can\ntherefore be considered as approximations of the smallest one. Using further\nreductions, we derive the approximation ratios of these compressors with\nrespect to the smallest attractor and solve several open problems related to\nthe asymptotic relations between repetitiveness measures (in particular,\nbetween the the sizes of the Lempel-Ziv factorization, the run-length\nBurrows-Wheeler transform, the smallest grammar, and the smallest macro\nscheme). These reductions directly provide approximation algorithms for the\nsmallest string attractor. We then apply string attractors to solve efficiently\na fundamental problem in the field of compressed computation: we present a\nuniversal compressed data structure for text extraction that improves existing\nstrategies simultaneously for \\emph{all} known dictionary compressors and that,\nby recent lower bounds, almost matches the optimal running time within the\nresulting space. To conclude, we consider generalizations of string attractors\nto labeled graphs, show that the attractor problem is NP-complete on trees, and\nprovide a logarithmic approximation computable in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:09:54 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 07:44:08 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "1709.05396", "submitter": "Victor Balcer", "authors": "Victor Balcer and Salil Vadhan", "title": "Differential Privacy on Finite Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing and analyzing differentially private\nalgorithms that can be implemented on {\\em discrete} models of computation in\n{\\em strict} polynomial time, motivated by known attacks on floating point\nimplementations of real-arithmetic differentially private algorithms (Mironov,\nCCS 2012) and the potential for timing attacks on expected polynomial-time\nalgorithms. As a case study, we examine the basic problem of approximating the\nhistogram of a categorical dataset over a possibly large data universe\n$\\mathcal{X}$. The classic Laplace Mechanism (Dwork, McSherry, Nissim, Smith,\nTCC 2006 and J. Privacy \\& Confidentiality 2017) does not satisfy our\nrequirements, as it is based on real arithmetic, and natural discrete\nanalogues, such as the Geometric Mechanism (Ghosh, Roughgarden, Sundarajan,\nSTOC 2009 and SICOMP 2012), take time at least linear in $|\\mathcal{X}|$, which\ncan be exponential in the bit length of the input.\n  In this paper, we provide strict polynomial-time discrete algorithms for\napproximate histograms whose simultaneous accuracy (the maximum error over all\nbins) matches that of the Laplace Mechanism up to constant factors, while\nretaining the same (pure) differential privacy guarantee. One of our algorithms\nproduces a sparse histogram as output. Its \"per-bin accuracy\" (the error on\nindividual bins) is worse than that of the Laplace Mechanism by a factor of\n$\\log|\\mathcal{X}|$, but we prove a lower bound showing that this is necessary\nfor any algorithm that produces a sparse histogram. A second algorithm avoids\nthis lower bound, and matches the per-bin accuracy of the Laplace Mechanism, by\nproducing a compact and efficiently computable representation of a dense\nhistogram, it is based on an $(n+1)$-wise independent implementation of an\nappropriately clamped version of the Discrete Geometric Mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 20:43:20 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 14:02:14 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 17:01:08 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Balcer", "Victor", ""], ["Vadhan", "Salil", ""]]}, {"id": "1709.05440", "submitter": "Shuhong Chen", "authors": "Shuhong Chen, Sen Yang, Moliang Zhou, Randall S. Burd, Ivan Marsic", "title": "Process-oriented Iterative Multiple Alignment for Medical Process Mining", "comments": "accepted at ICDMW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapted from biological sequence alignment, trace alignment is a process\nmining technique used to visualize and analyze workflow data. Any analysis done\nwith this method, however, is affected by the alignment quality. The best\nexisting trace alignment techniques use progressive guide-trees to\nheuristically approximate the optimal alignment in O(N2L2) time. These\nalgorithms are heavily dependent on the selected guide-tree metric, often\nreturn sum-of-pairs-score-reducing errors that interfere with interpretation,\nand are computationally intensive for large datasets. To alleviate these\nissues, we propose process-oriented iterative multiple alignment (PIMA), which\ncontains specialized optimizations to better handle workflow data. We\ndemonstrate that PIMA is a flexible framework capable of achieving better\nsum-of-pairs score than existing trace alignment algorithms in only O(NL2)\ntime. We applied PIMA to analyzing medical workflow data, showing how iterative\nalignment can better represent the data and facilitate the extraction of\ninsights from data visualization.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 00:51:39 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Chen", "Shuhong", ""], ["Yang", "Sen", ""], ["Zhou", "Moliang", ""], ["Burd", "Randall S.", ""], ["Marsic", "Ivan", ""]]}, {"id": "1709.05510", "submitter": "Sainyam Galhotra Mr", "authors": "Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal, Barna Saha", "title": "The Geometric Block Model", "comments": "A shorter version of this paper has appeared in 32nd AAAI Conference\n  on Artificial Intelligence. The AAAI proceedings version as well as the\n  previous version in arxiv contained some errors that have been corrected in\n  this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To capture the inherent geometric features of many community detection\nproblems, we propose to use a new random graph model of communities that we\ncall a Geometric Block Model. The geometric block model generalizes the random\ngeometric graphs in the same way that the well-studied stochastic block model\ngeneralizes the Erdos-Renyi random graphs. It is also a natural extension of\nrandom community models inspired by the recent theoretical and practical\nadvancement in community detection. While being a topic of fundamental\ntheoretical interest, our main contribution is to show that many practical\ncommunity structures are better explained by the geometric block model. We also\nshow that a simple triangle-counting algorithm to detect communities in the\ngeometric block model is near-optimal. Indeed, even in the regime where the\naverage degree of the graph grows only logarithmically with the number of\nvertices (sparse-graph), we show that this algorithm performs extremely well,\nboth theoretically and practically. In contrast, the triangle-counting\nalgorithm is far from being optimum for the stochastic block model. We simulate\nour results on both real and synthetic datasets to show superior performance of\nboth the new model as well as our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 13:38:03 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 16:26:40 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Mazumdar", "Arya", ""], ["Pal", "Soumyabrata", ""], ["Saha", "Barna", ""]]}, {"id": "1709.05709", "submitter": "K. Alex Mills", "authors": "K. Alex Mills, R. Chandrasekaran, Neeraj Mittal", "title": "Lexico-minimum Replica Placement in Multitrees", "comments": "18 pages, 7 figures accepted for publication in COCOA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of placing replicas in a data center or\nstorage area network, represented as a digraph, so as to lexico-minimize a\npreviously proposed reliability measure which minimizes the impact of all\nfailure events in the model in decreasing order of severity. Prior work focuses\non the special case in which the digraph is an arborescence. In this work, we\nconsider the broader class of multitrees: digraphs in which the subgraph\ninduced by vertices reachable from a fixed node forms a tree. We parameterize\nmultitrees by their number of \"roots\" (nodes with in-degree zero), and rule out\nmembership in the class of fixed-parameter tractable problems (FPT) by showing\nthat finding optimal replica placements in multitrees with 3 roots is NP-hard.\nOn the positive side, we show that the problem of finding optimal replica\nplacements in the class of \\emph{untangled} multitrees is FPT, as parameterized\nby the replication factor $\\rho$ and the number of roots $k$. Our approach\ncombines dynamic programming (DP) with a novel tree decomposition to find an\noptimal placement of $\\rho$ replicas on the leaves of a multitree with $n$\nnodes and $k$ roots in $O(n^2\\rho^{2k+3})$ time.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 19:41:58 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Mills", "K. Alex", ""], ["Chandrasekaran", "R.", ""], ["Mittal", "Neeraj", ""]]}, {"id": "1709.05751", "submitter": "Dvir Shabtay", "authors": "Danny Hermelin, Shlomo Karhi, Mike Pinedo, and Dvir Shabtay", "title": "New Algorithms for Minimizing the Weighted Number of Tardy Jobs On a\n  Single Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the classical single machine scheduling problem where\nthe objective is to minimize the total weight of tardy jobs. Our analysis\nfocuses on the case where one or more of three natural parameters is either\nconstant or is taken as a parameter in the sense of parameterized complexity.\nThese three parameters are the number of different due dates, processing times,\nand weights in our set of input jobs. We show that the problem belongs to the\nclass of fixed parameter tractable (FPT)problems when combining any two of\nthese three parameters. We also show that the problem is polynomial-time\nsolvable when the latter two parameters are constant, complementing Karp's\nresult who showed that the problem is NP-hard already for a single due date.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:46:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Hermelin", "Danny", ""], ["Karhi", "Shlomo", ""], ["Pinedo", "Mike", ""], ["Shabtay", "Dvir", ""]]}, {"id": "1709.05876", "submitter": "Khaled Elbassioni", "authors": "Areg Karapetyan, Khaled Elbassioni, Majid Khonji, Chi-Kin Chau", "title": "Approximations for Generalized Unsplittable Flow on Paths with\n  Application to Power Systems Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\textit{Unsplittable Flow on a Path} (UFP) problem has sparked remarkable\nattention as a challenging combinatorial problem with profound practical\nimplications. Steered by its prominent application in \\textit{power\nengineering}, the present work formulates a novel generalization of UFP,\nwherein demands and capacities in the input instance are monotone step\nfunctions over the set of edges. As an initial step towards tackling this\ngeneralization, we draw on and extend ideas from prior research to devise a a\nquasi-polynomial time approximation scheme (QPTAS) under the premise that the\ndemands and capacities lie in a quasi-polynomial range. Second, retaining the\nsame assumption, an efficient logarithmic approximation is introduced for the\nsingle-source variant of the problem. Finally, we round up the contributions by\ndesigning a (kind of) black-box reduction that, under some mild conditions,\nallows to translate LP-based approximation algorithms for the studied problem\ninto their counterparts for the \\textit{Alternating Current Optimal Power Flow}\n(AC OPF) problem -- a fundamental workflow in operation and control of power\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:50:46 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 08:40:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Karapetyan", "Areg", ""], ["Elbassioni", "Khaled", ""], ["Khonji", "Majid", ""], ["Chau", "Chi-Kin", ""]]}, {"id": "1709.05896", "submitter": "Alexander M\\\"acker", "authors": "Alexander M\\\"acker, Manuel Malatyali, Friedhelm Meyer auf der Heide,\n  S\\\"oren Riechers", "title": "Non-Clairvoyant Scheduling to Minimize Max Flow Time on a Machine with\n  Setup Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a problem in which $n$ jobs that are classified into $k$ types\narrive over time at their release times and are to be scheduled on a single\nmachine so as to minimize the maximum flow time. The machine requires a setup\ntaking $s$ time units whenever it switches from processing jobs of one type to\njobs of a different type. We consider the problem as an online problem where\neach job is only known to the scheduler as soon as it arrives and where the\nprocessing time of a job only becomes known upon its completion\n(non-clairvoyance).\n  We are interested in the potential of simple \"greedy-like\" algorithms. We\nanalyze a modification of the FIFO strategy and show its competitiveness to be\n$\\Theta(\\sqrt{n})$, which is optimal for the considered class of algorithms.\nFor $k=2$ types it achieves a constant competitiveness. Our main insight is\nobtained by an analysis of the smoothed competitiveness. If processing times\n$p_j$ are independently perturbed to $\\hat p_j = (1+X_j)p_j$, we obtain a\ncompetitiveness of $O(\\sigma^{-2} \\log^2 n)$ when $X_j$ is drawn from a uniform\nor a (truncated) normal distribution with standard deviation $\\sigma$. The\nresult proves that bad instances are fragile and \"practically\" one might expect\na much better performance than given by the $\\Omega(\\sqrt{n})$-bound.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 12:37:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["M\u00e4cker", "Alexander", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""], ["Riechers", "S\u00f6ren", ""]]}, {"id": "1709.06010", "submitter": "Surbhi Goel", "authors": "Surbhi Goel and Adam Klivans", "title": "Learning Neural Networks with Two Nonlinear Layers in Polynomial Time", "comments": "Changed title, included new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial-time algorithm for learning neural networks with one\nlayer of sigmoids feeding into any Lipschitz, monotone activation function\n(e.g., sigmoid or ReLU). We make no assumptions on the structure of the\nnetwork, and the algorithm succeeds with respect to {\\em any} distribution on\nthe unit ball in $n$ dimensions (hidden weight vectors also have unit norm).\nThis is the first assumption-free, provably efficient algorithm for learning\nneural networks with two nonlinear layers.\n  Our algorithm-- {\\em Alphatron}-- is a simple, iterative update rule that\ncombines isotonic regression with kernel methods. It outputs a hypothesis that\nyields efficient oracle access to interpretable features. It also suggests a\nnew approach to Boolean learning problems via real-valued conditional-mean\nfunctions, sidestepping traditional hardness results from computational\nlearning theory.\n  Along these lines, we subsume and improve many longstanding results for PAC\nlearning Boolean functions to the more general, real-valued setting of {\\em\nprobabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d.\nnoise-tolerance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 15:36:06 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 17:29:51 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 17:14:38 GMT"}, {"version": "v4", "created": "Fri, 20 Apr 2018 20:16:02 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""]]}, {"id": "1709.06056", "submitter": "Aleksandar Prokopec", "authors": "Aleksandar Prokopec, Phil Bagwell, Martin Odersky", "title": "Cache-Aware Lock-Free Concurrent Hash Tries", "comments": null, "journal-ref": null, "doi": null, "report-no": "EPFL-REPORT-166908", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes an implementation of a non-blocking concurrent\nshared-memory hash trie based on single-word compare-and-swap instructions.\nInsert, lookup and remove operations modifying different parts of the hash trie\ncan be run independent of each other and do not contend. Remove operations\nensure that the unneeded memory is freed and that the trie is kept compact. A\npseudocode for these operations is presented and a proof of correctness is\ngiven -- we show that the implementation is linearizable and lock-free.\nFinally, benchmarks are presented which compare concurrent hash trie operations\nagainst the corresponding operations on other concurrent data structures,\nshowing their performance and scalability.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 17:22:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Prokopec", "Aleksandar", ""], ["Bagwell", "Phil", ""], ["Odersky", "Martin", ""]]}, {"id": "1709.06113", "submitter": "Siddharth Gupta", "authors": "David Eppstein, Siddharth Gupta", "title": "Crossing Patterns in Nonplanar Road Networks", "comments": "9 pages, 4 figures. To appear at the 25th ACM SIGSPATIAL\n  International Conference on Advances in Geographic Information Systems(ACM\n  SIGSPATIAL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the crossing graph of a given embedded graph (such as a road\nnetwork) to be a graph with a vertex for each edge of the embedding, with two\ncrossing graph vertices adjacent when the corresponding two edges of the\nembedding cross each other. In this paper, we study the sparsity properties of\ncrossing graphs of real-world road networks. We show that, in large road\nnetworks (the Urban Road Network Dataset), the crossing graphs have connected\ncomponents that are primarily trees, and that the remaining non-tree components\nare typically sparse (technically, that they have bounded degeneracy). We prove\ntheoretically that when an embedded graph has a sparse crossing graph, it has\nother desirable properties that lead to fast algorithms for shortest paths and\nother algorithms important in geographic information systems. Notably, these\ngraphs have polynomial expansion, meaning that they and all their subgraphs\nhave small separators.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 18:15:41 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Eppstein", "David", ""], ["Gupta", "Siddharth", ""]]}, {"id": "1709.06169", "submitter": "Safa Jammali", "authors": "Jean-David Aguilar, Safa Jammali, Esaie Kuitche, A\\\"ida Ouangraoua", "title": "Improving spliced alignment for identification of ortholog groups and\n  multiple CDS alignment", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Spliced Alignment Problem (SAP) that consists in finding an optimal\nsemi-global alignment of a spliced RNA sequence on an unspliced genomic\nsequence has been largely considered for the prediction and the annotation of\ngene structures in genomes. Here, we re-visit it for the purpose of identifying\nCDS ortholog groups within a set of CDS from homologous genes and for computing\nmultiple CDS alignments. We introduce a new constrained version of the spliced\nalignment problem together with an algorithm that exploits full information on\nthe exon-intron structure of the input RNA and gene sequences in order to\ncompute high-coverage accurate alignments. We show how pairwise spliced\nalignments between the CDS and the gene sequences of a gene family can be\ndirectly used in order to clusterize the set of CDS of the gene family into a\nset of ortholog groups. We also introduce an extension of the spliced alignment\nproblem called Multiple Spliced Alignment Problem (MSAP) that consists in\naligning simultaneously several RNA sequences on several genes from the same\ngene family. We develop a heuristic algorithmic solution for the problem. We\nshow how to exploit multiple spliced alignments for the clustering of\nhomologous CDS into ortholog and close paralog groups, and for the construction\nof multiple CDS alignments. An implementation of the method in Python is\navailable on demande to SFA@USherbrooke.ca.\n  Keywords: Spliced alignment, CDS ortholog groups, Multiple CDS alignment,\nGene structure, Gene family.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:56:29 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Aguilar", "Jean-David", ""], ["Jammali", "Safa", ""], ["Kuitche", "Esaie", ""], ["Ouangraoua", "A\u00efda", ""]]}, {"id": "1709.06299", "submitter": "Arne Schmidt", "authors": "Aaron T. Becker, S\\'andor P. Fekete, Phillip Keldenich, Dominik\n  Krupke, Christian Rieck, Christian Scheffer, Arne Schmidt", "title": "Tilt Assembly: Algorithms for Micro-Factories That Build Objects with\n  Uniform External Forces", "comments": "17 pages, 17 figures, 1 table, full version of extended abstract that\n  is to appear in ISAAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithmic results for the parallel assembly of many micro-scale\nobjects in two and three dimensions from tiny particles, which has been\nproposed in the context of programmable matter and self-assembly for building\nhigh-yield micro-factories. The underlying model has particles moving under the\ninfluence of uniform external forces until they hit an obstacle; particles can\nbond when being forced together with another appropriate particle. Due to the\nphysical and geometric constraints, not all shapes can be built in this manner;\nthis gives rise to the Tilt Assembly Problem (TAP) of deciding\nconstructibility. For simply-connected polyominoes $P$ in 2D consisting of $N$\nunit-squares (\"tiles\"), we prove that TAP can be decided in $O(N\\log N)$ time.\nFor the optimization variant MaxTAP (in which the objective is to construct a\nsubshape of maximum possible size), we show polyAPX-hardness: unless P=NP,\nMaxTAP cannot be approximated within a factor of $\\Omega(N^{\\frac{1}{3}})$; for\ntree-shaped structures, we give an $O(N^{\\frac{1}{2}})$-approximation\nalgorithm. For the efficiency of the assembly process itself, we show that any\nconstructible shape allows pipelined assembly, which produces copies of $P$ in\n$O(1)$ amortized time, i.e., $N$ copies of $P$ in $O(N)$ time steps. These\nconsiderations can be extended to three-dimensional objects: For the class of\npolycubes $P$ we prove that it is NP-hard to decide whether it is possible to\nconstruct a path between two points of $P$; it is also NP-hard to decide\nconstructibility of a polycube $P$. Moreover, it is expAPX-hard to maximize a\npath from a given start point.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 08:52:23 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Becker", "Aaron T.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Keldenich", "Phillip", ""], ["Krupke", "Dominik", ""], ["Rieck", "Christian", ""], ["Scheffer", "Christian", ""], ["Schmidt", "Arne", ""]]}, {"id": "1709.06525", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Yash Deshpande, Andrea Montanari", "title": "Inference in Graphical Models via Semidefinite Programming Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum A posteriori Probability (MAP) inference in graphical models amounts\nto solving a graph-structured combinatorial optimization problem. Popular\ninference algorithms such as belief propagation (BP) and generalized belief\npropagation (GBP) are intimately related to linear programming (LP) relaxation\nwithin the Sherali-Adams hierarchy. Despite the popularity of these algorithms,\nit is well understood that the Sum-of-Squares (SOS) hierarchy based on\nsemidefinite programming (SDP) can provide superior guarantees. Unfortunately,\nSOS relaxations for a graph with $n$ vertices require solving an SDP with\n$n^{\\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In\npractice, for $d\\ge 4$, this approach does not scale beyond a few tens of\nvariables. In this paper, we propose binary SDP relaxations for MAP inference\nusing the SOS hierarchy with two innovations focused on computational\nefficiency. Firstly, in analogy to BP and its variants, we only introduce\ndecision variables corresponding to contiguous regions in the graphical model.\nSecondly, we solve the resulting SDP using a non-convex Burer-Monteiro style\nmethod, and develop a sequential rounding procedure. We demonstrate that the\nresulting algorithm can solve problems with tens of thousands of variables\nwithin minutes, and outperforms BP and GBP on practical problems such as image\ndenoising and Ising spin glasses. Finally, for specific graph types, we\nestablish a sufficient condition for the tightness of the proposed partial SOS\nrelaxation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 16:45:30 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Deshpande", "Yash", ""], ["Montanari", "Andrea", ""]]}, {"id": "1709.06534", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "BIOS ORAM: Improved Privacy-Preserving Data Access for Parameterized\n  Outsourced Storage", "comments": "Full version of paper appearing in WPES 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for oblivious random access machine (ORAM) simulation allow a\nclient, Alice, to obfuscate a pattern of data accesses with a server, Bob, who\nis maintaining Alice's outsourced data while trying to learn information about\nher data. We present a novel ORAM scheme that improves the asymptotic I/O\noverhead of previous schemes for a wide range of size parameters for\nclient-side private memory and message blocks, from logarithmic to polynomial.\nOur method achieves statistical security for hiding Alice's access pattern and,\nwith high probability, achieves an I/O overhead that ranges from $O(1)$ to\n$O(\\log^2 n/(\\log\\log n)^2)$, depending on these size parameters, where $n$ is\nthe size of Alice's outsourced memory. Our scheme, which we call BIOS ORAM,\ncombines multiple uses of B-trees with a reduction of ORAM simulation to\nisogrammic access sequences.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 17:24:01 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "1709.06810", "submitter": "Lijun Chang", "authors": "Lijun Chang, Xing Feng, Xuemin Lin, Lu Qin, Wenjie Zhang", "title": "Efficient Graph Edit Distance Computation and Verification via\n  Anchor-aware Lower Bound Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph edit distance (GED) is an important similarity measure adopted in a\nsimilarity-based analysis between two graphs, and computing GED is a primitive\noperator in graph database analysis. Partially due to the NP-hardness, the\nexisting techniques for computing GED are only able to process very small\ngraphs with less than 30 vertices. Motivated by this, in this paper we\nsystematically study the problems of both GED computation, and GED verification\n(i.e., verify whether the GED between two graphs is no larger than a user-given\nthreshold). Firstly, we develop a unified framework that can be instantiated\ninto either a best-first search approach AStar+ or a depth-first search\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\ntechniques to compute tighter lower bounds for intermediate search states,\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\nsmaller search space. Extensive empirical studies validate that AStar+ performs\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\nstate-of-the-art technique by more than four orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 10:55:22 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 02:52:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chang", "Lijun", ""], ["Feng", "Xing", ""], ["Lin", "Xuemin", ""], ["Qin", "Lu", ""], ["Zhang", "Wenjie", ""]]}, {"id": "1709.06995", "submitter": "David Harris", "authors": "David G. Harris, Thomas Pensyl, Aravind Srinivasan, Khoa Trinh", "title": "Dependent randomized rounding for clustering and partition systems with\n  knapsack constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering problems are fundamental to unsupervised learning. There is an\nincreased emphasis on fairness in machine learning and AI; one representative\nnotion of fairness is that no single demographic group should be\nover-represented among the cluster-centers. This, and much more general\nclustering problems, can be formulated with \"knapsack\" and \"partition\"\nconstraints. We develop new randomized algorithms targeting such problems, and\nstudy two in particular: multi-knapsack median and multi-knapsack center. Our\nrounding algorithms give new approximation and pseudo-approximation algorithms\nfor these problems. One key technical tool, which may be of independent\ninterest, is a new tail bound analogous to Feige (2006) for sums of random\nvariables with unbounded variances. Such bounds can be useful in inferring\nproperties of large networks using few samples.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 12:38:38 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 15:10:58 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 13:38:22 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 19:41:22 GMT"}, {"version": "v5", "created": "Thu, 4 Jul 2019 09:50:36 GMT"}, {"version": "v6", "created": "Sat, 12 Oct 2019 12:47:25 GMT"}, {"version": "v7", "created": "Tue, 25 Feb 2020 20:28:22 GMT"}, {"version": "v8", "created": "Wed, 1 Apr 2020 13:46:26 GMT"}, {"version": "v9", "created": "Wed, 30 Dec 2020 19:29:12 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Harris", "David G.", ""], ["Pensyl", "Thomas", ""], ["Srinivasan", "Aravind", ""], ["Trinh", "Khoa", ""]]}, {"id": "1709.07093", "submitter": "Xingguo Li", "authors": "Jarvis Haupt and Xingguo Li and David P. Woodruff", "title": "Near Optimal Sketching of Low-Rank Tensor Regression", "comments": "36 pages, 2 figures, 2 tables, Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the least squares regression problem \\begin{align*} \\min_{\\Theta \\in\n\\mathcal{S}_{\\odot D,R}} \\|A\\Theta-b\\|_2, \\end{align*} where\n$\\mathcal{S}_{\\odot D,R}$ is the set of $\\Theta$ for which $\\Theta =\n\\sum_{r=1}^{R} \\theta_1^{(r)} \\circ \\cdots \\circ \\theta_D^{(r)}$ for vectors\n$\\theta_d^{(r)} \\in \\mathbb{R}^{p_d}$ for all $r \\in [R]$ and $d \\in [D]$, and\n$\\circ$ denotes the outer product of vectors. That is, $\\Theta$ is a\nlow-dimensional, low-rank tensor. This is motivated by the fact that the number\nof parameters in $\\Theta$ is only $R \\cdot \\sum_{d=1}^D p_d$, which is\nsignificantly smaller than the $\\prod_{d=1}^{D} p_d$ number of parameters in\nordinary least squares regression. We consider the above CP decomposition model\nof tensors $\\Theta$, as well as the Tucker decomposition. For both models we\nshow how to apply data dimensionality reduction techniques based on {\\it\nsparse} random projections $\\Phi \\in \\mathbb{R}^{m \\times n}$, with $m \\ll n$,\nto reduce the problem to a much smaller problem $\\min_{\\Theta} \\|\\Phi A \\Theta\n- \\Phi b\\|_2$, for which if $\\Theta'$ is a near-optimum to the smaller problem,\nthen it is also a near optimum to the original problem. We obtain significantly\nsmaller dimension and sparsity in $\\Phi$ than is possible for ordinary least\nsquares regression, and we also provide a number of numerical simulations\nsupporting our theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 22:05:49 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Haupt", "Jarvis", ""], ["Li", "Xingguo", ""], ["Woodruff", "David P.", ""]]}, {"id": "1709.07122", "submitter": "Kartik Lakhotia", "authors": "Kartik Lakhotia, Rajgopal Kannan, Viktor Prasanna", "title": "Accelerating PageRank using Partition-Centric Processing", "comments": "Added acknowledgments. In proceedings of USENIX ATC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PageRank is a fundamental link analysis algorithm that also functions as a\nkey representative of the performance of Sparse Matrix-Vector (SpMV)\nmultiplication. The traditional PageRank implementation generates fine\ngranularity random memory accesses resulting in large amount of wasteful DRAM\ntraffic and poor bandwidth utilization. In this paper, we present a novel\nPartition-Centric Processing Methodology (PCPM) to compute PageRank, that\ndrastically reduces the amount of DRAM communication while achieving high\nsustained memory bandwidth. PCPM uses a Partition-centric abstraction coupled\nwith the Gather-Apply-Scatter (GAS) programming model. By carefully examining\nhow a PCPM based implementation impacts communication characteristics of the\nalgorithm, we propose several system optimizations that improve the execution\ntime substantially. More specifically, we develop (1) a new data layout that\nsignificantly reduces communication and random DRAM accesses, and (2) branch\navoidance mechanisms to get rid of unpredictable data-dependent branches.\n  We perform detailed analytical and experimental evaluation of our approach\nusing 6 large graphs and demonstrate an average 2.7x speedup in execution time\nand 1.7x reduction in communication volume, compared to the state-of-the-art.\nWe also show that unlike other GAS based implementations, PCPM is able to\nfurther reduce main memory traffic by taking advantage of intelligent node\nlabeling that enhances locality. Although we use PageRank as the target\napplication in this paper, our approach can be applied to generic SpMV\ncomputation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 01:41:34 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 19:26:08 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 09:02:35 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 20:32:23 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Lakhotia", "Kartik", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1709.07249", "submitter": "Stefano Leucci", "authors": "Barbara Geissmann, Stefano Leucci, Chih-Hung Liu, Paolo Penna", "title": "Sorting with Recurrent Comparison Errors", "comments": "15 pages, ISAAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sorting algorithm for the case of recurrent random comparison\nerrors. The algorithm essentially achieves simultaneously good properties of\nprevious algorithms for sorting $n$ distinct elements in this model. In\nparticular, it runs in $O(n^2)$ time, the maximum dislocation of the elements\nin the output is $O(\\log n)$, while the total dislocation is $O(n)$. These\nguarantees are the best possible since we prove that even randomized algorithms\ncannot achieve $o(\\log n)$ maximum dislocation with high probability, or $o(n)$\ntotal dislocation in expectation, regardless of their running time.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 10:38:28 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Geissmann", "Barbara", ""], ["Leucci", "Stefano", ""], ["Liu", "Chih-Hung", ""], ["Penna", "Paolo", ""]]}, {"id": "1709.07259", "submitter": "Manuel Malatyali", "authors": "Felix Biermeier, Bj\\\"orn Feldkord, Manuel Malatyali, Friedhelm Meyer\n  auf der Heide", "title": "A Communication-Efficient Distributed Data Structure for Top-k and\n  k-Select Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the scenario of $n$ sensor nodes observing streams of data. The\nnodes are connected to a central server whose task it is to compute some\nfunction over all data items observed by the nodes. In our case, there exists a\ntotal order on the data items observed by the nodes. Our goal is to compute the\n$k$ currently lowest observed values or a value with rank in\n$[(1-\\varepsilon)k,(1+\\varepsilon)k]$ with probability $(1-\\delta)$. We propose\nsolutions for these problems in an extension of the distributed monitoring\nmodel where the server can send broadcast messages to all nodes for unit cost.\nWe want to minimize communication over multiple time steps where there are $m$\nupdates to a node's value in between queries. The result is composed of two\nmain parts, which each may be of independent interest: (1) Protocols which\nanswer Top-k and k-Select queries. These protocols are memoryless in the sense\nthat they gather all information at the time of the request. (2) A dynamic data\nstructure which tracks for every $k$ an element close to $k$.\n  We describe how to combine the two parts to receive a protocol answering the\nstated queries over multiple time steps. Overall, for Top-$k$ queries we use\n$O(k + \\log m + \\log \\log n)$ and for $k$-Select queries\n$O(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} + \\log m + \\log^2 \\log n)$\nmessages in expectation. These results are shown to be asymptotically tight if\n$m$ is not too small.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 11:06:04 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Biermeier", "Felix", ""], ["Feldkord", "Bj\u00f6rn", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1709.07308", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis, Michael Mitzenmacher, Kasper Green Larsen,\n  Jaros{\\l}aw B{\\l}asiok, Ben Lawson, Preetum Nakkiran, Vasileios Nakos", "title": "Predicting Positive and Negative Links with Noisy Queries: Theory &\n  Practice", "comments": "arXiv admin note: text overlap with arXiv:1609.00750", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks involve both positive and negative relationships, which can\nbe captured in signed graphs. The {\\em edge sign prediction problem} aims to\npredict whether an interaction between a pair of nodes will be positive or\nnegative. We provide theoretical results for this problem that motivate natural\nimprovements to recent heuristics.\n  The edge sign prediction problem is related to correlation clustering; a\npositive relationship means being in the same cluster. We consider the\nfollowing model for two clusters: we are allowed to query any pair of nodes\nwhether they belong to the same cluster or not, but the answer to the query is\ncorrupted with some probability $0<q<\\frac{1}{2}$. Let $\\delta=1-2q$ be the\nbias. We provide an algorithm that recovers all signs correctly with high\nprobability in the presence of noise with $O(\\frac{n\\log\nn}{\\delta^2}+\\frac{\\log^2 n}{\\delta^6})$ queries. This is the best known result\nfor this problem for all but tiny $\\delta$, improving on the recent work of\nMazumdar and Saha \\cite{mazumdar2017clustering}. We also provide an algorithm\nthat performs $O(\\frac{n\\log n}{\\delta^4})$ queries, and uses breadth first\nsearch as its main algorithmic primitive. While both the running time and the\nnumber of queries for this algorithm are sub-optimal, our result relies on\nnovel theoretical techniques, and naturally suggests the use of edge-disjoint\npaths as a feature for predicting signs in online social networks.\nCorrespondingly, we experiment with using edge disjoint $s-t$ paths of short\nlength as a feature for predicting the sign of edge $(s,t)$ in real-world\nsigned networks. Empirical findings suggest that the use of such paths improves\nthe classification accuracy, especially for pairs of nodes with no common\nneighbors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 20:38:10 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 21:28:34 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2020 21:54:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""], ["Mitzenmacher", "Michael", ""], ["Larsen", "Kasper Green", ""], ["B\u0142asiok", "Jaros\u0142aw", ""], ["Lawson", "Ben", ""], ["Nakkiran", "Preetum", ""], ["Nakos", "Vasileios", ""]]}, {"id": "1709.07358", "submitter": "Toshio Suzuki", "authors": "Toshio Suzuki", "title": "Non-Depth-First Search against Independent Distributions on an AND-OR\n  Tree", "comments": "12 pages, 1 figure", "journal-ref": "Inf. Process. Lett. 139, pp.13-17 (2018)", "doi": "10.1016/j.ipl.2018.06.013", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suzuki and Niida (Ann. Pure. Appl. Logic, 2015) showed the following results\non independent distributions (IDs) on an AND-OR tree, where they took only\ndepth-first algorithms into consideration. (1) Among IDs such that probability\nof the root having value 0 is fixed as a given r such that 0 < r < 1, if d is a\nmaximizer of cost of the best algorithm then d is an independent and identical\ndistribution (IID). (2) Among all IDs, if d is a maximizer of cost of the best\nalgorithm then d is an IID. In the case where non-depth-first algorithms are\ntaken into consideration, the counter parts of (1) and (2) are left open in the\nabove work. Peng et al. (Inform. Process. Lett., 2017) extended (1) and (2) to\nmulti-branching trees, where in (2) they put an additional hypothesis on IDs\nthat probability of the root having value 0 is neither 0 nor 1. We give\npositive answers for the two questions of Suzuki-Niida. A key to the proof is\nthat if ID d achieves the equilibrium among IDs then we can chose an algorithm\nof the best cost against d from depth-first algorithms. In addition, we extend\nthe result of Peng et al. to the case where non-depth-first algorithms are\ntaken into consideration.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:54:52 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Suzuki", "Toshio", ""]]}, {"id": "1709.07601", "submitter": "Yasushi Kawase", "authors": "Yasushi Kawase", "title": "Stochastic Input Models in Online Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study twelve stochastic input models for online problems\nand reveal the relationships among the competitive ratios for the models. The\ncompetitive ratio is defined as the worst ratio between the expected optimal\nvalue and the expected profit of the solution obtained by the online algorithm\nwhere the input distribution is restricted according to the model. To handle a\nbroad class of online problems, we use a framework called request-answer games\nthat is introduced by Ben-David et al. The stochastic input models consist of\ntwo types: known distribution and unknown distribution. For each type, we\nconsider six classes of distributions: dependent distributions, deterministic\ninput, independent distributions, identical independent distribution, random\norder of a deterministic input, and random order of independent distributions.\nAs an application of the models, we consider two basic online problems, which\nare variants of the secretary problem and the prophet inequality problem, under\nthe twelve stochastic input models. We see the difference of the competitive\nratios through these problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:30:53 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Kawase", "Yasushi", ""]]}, {"id": "1709.07712", "submitter": "Lucas Pastor", "authors": "T. Karthick, Fr\\'ed\\'eric Maffray, Lucas Pastor", "title": "Polynomial Cases for the Vertex Coloring Problem", "comments": "This article contains results from arXiv:1707.08918", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of the Vertex Coloring problem is known for all\nhereditary classes of graphs defined by forbidding two connected five-vertex\ninduced subgraphs, except for seven cases. We prove the polynomial-time\nsolvability of four of these problems: for ($P_5$, dart)-free graphs, ($P_5$,\nbanner)-free graphs, ($P_5$, bull)-free graphs, and (fork, bull)-free graphs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 12:27:54 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 12:24:49 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Karthick", "T.", ""], ["Maffray", "Fr\u00e9d\u00e9ric", ""], ["Pastor", "Lucas", ""]]}, {"id": "1709.07797", "submitter": "Timothy Chu", "authors": "Timothy Chu, Gary Miller, Donald Sheehy", "title": "Exact Computation of a Manifold Metric, via Lipschitz Embeddings and\n  Shortest Paths on a Graph", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-sensitive metrics adapt distances locally based the density of data\npoints with the goal of aligning distances and some notion of similarity. In\nthis paper, we give the first exact algorithm for computing a data-sensitive\nmetric called the nearest neighbor metric. In fact, we prove the surprising\nresult that a previously published $3$-approximation is an exact algorithm.\n  The nearest neighbor metric can be viewed as a special case of a\ndensity-based distance used in machine learning, or it can be seen as an\nexample of a manifold metric. Previous computational research on such metrics\ndespaired of computing exact distances on account of the apparent difficulty of\nminimizing over all continuous paths between a pair of points. We leverage the\nexact computation of the nearest neighbor metric to compute sparse spanners and\npersistent homology. We also explore the behavior of the metric built from\npoint sets drawn from an underlying distribution and consider the more general\ncase of inputs that are finite collections of path-connected compact sets.\n  The main results connect several classical theories such as the conformal\nchange of Riemannian metrics, the theory of positive definite functions of\nSchoenberg, and screw function theory of Schoenberg and Von Neumann. We develop\nnovel proof techniques based on the combination of screw functions and\nLipschitz extensions that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:58:01 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 05:56:43 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2018 20:29:59 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2020 06:29:52 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chu", "Timothy", ""], ["Miller", "Gary", ""], ["Sheehy", "Donald", ""]]}, {"id": "1709.07822", "submitter": "Nima Anari", "authors": "Nima Anari and Vijay V. Vazirani", "title": "Planar Graph Perfect Matching is in NC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is perfect matching in NC? That is, is there a deterministic fast parallel\nalgorithm for it? This has been an outstanding open question in theoretical\ncomputer science for over three decades, ever since the discovery of RNC\nmatching algorithms. Within this question, the case of planar graphs has\nremained an enigma: On the one hand, counting the number of perfect matchings\nis far harder than finding one (the former is #P-complete and the latter is in\nP), and on the other, for planar graphs, counting has long been known to be in\nNC whereas finding one has resisted a solution.\n  In this paper, we give an NC algorithm for finding a perfect matching in a\nplanar graph. Our algorithm uses the above-stated fact about counting matchings\nin a crucial way. Our main new idea is an NC algorithm for finding a face of\nthe perfect matching polytope at which $\\Omega(n)$ new conditions, involving\nconstraints of the polytope, are simultaneously satisfied. Several other ideas\nare also needed, such as finding a point in the interior of the minimum weight\nface of this polytope and finding a balanced tight odd set in NC.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:56:53 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 00:50:32 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 22:02:03 GMT"}, {"version": "v4", "created": "Sat, 21 Apr 2018 21:49:54 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Anari", "Nima", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1709.07869", "submitter": "Piotr Sankowski", "authors": "Piotr Sankowski", "title": "NC Algorithms for Weighted Planar Perfect Matching and Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a planar graph $G=(V,E)$ with polynomially bounded edge weight\nfunction $w:E\\to [0, poly(n)]$. The main results of this paper are NC\nalgorithms for the following problems:\n  - minimum weight perfect matching in $G$,\n  - maximum cardinality and maximum weight matching in $G$ when $G$ is\nbipartite,\n  - maximum multiple-source multiple-sink flow in $G$ where $c:E\\to [1,\npoly(n)]$ is a polynomially bounded edge capacity function,\n  - minimum weight $f$-factor in $G$ where $f:V\\to [1, poly(n)]$,\n  - min-cost flow in $G$ where $c:E\\to [1, poly(n)]$ is a polynomially bounded\nedge capacity function and $b:V\\to [1, poly(n)]$ is a polynomially bounded\nvertex demand function.\n  There have been no known NC algorithms for any of these problems previously\n(Before this and independent paper by Anari and Vazirani). In order to solve\nthese problems we develop a new relatively simple but versatile framework that\nis combinatorial in spirit. It handles the combinatorial structure of matchings\ndirectly and needs to only know weights of appropriately defined matchings from\nalgebraic subroutines.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:48:47 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 09:34:39 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 09:59:49 GMT"}, {"version": "v4", "created": "Wed, 18 Apr 2018 20:14:25 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Sankowski", "Piotr", ""]]}, {"id": "1709.08088", "submitter": "Jonathan Jedwab", "authors": "Jonathan Jedwab, Tara Petrie, Samuel Simon", "title": "An infinite class of unsaturated rooted trees corresponding to\n  designable RNA secondary structures", "comments": "Minor corrections and clarification of scope", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An RNA secondary structure is designable if there is an RNA sequence which\ncan attain its maximum number of base pairs only by adopting that structure.\nThe combinatorial RNA design problem, introduced by Hale\\v{s} et al. in 2016,\nis to determine whether or not a given RNA secondary structure is designable.\nHale\\v{s} et al. identified certain classes of designable and non-designable\nsecondary structures by reference to their corresponding rooted trees. We\nintroduce an infinite class of rooted trees containing unpaired nucleotides at\nthe greatest depth, and prove constructively that their corresponding secondary\nstructures are designable. This complements previous results for the\ncombinatorial RNA design problem.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 17:57:19 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 13:57:12 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 03:19:31 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 04:54:42 GMT"}, {"version": "v5", "created": "Fri, 8 May 2020 23:26:21 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Jedwab", "Jonathan", ""], ["Petrie", "Tara", ""], ["Simon", "Samuel", ""]]}, {"id": "1709.08139", "submitter": "Victor Amelkin", "authors": "Victor Amelkin, Ambuj K. Singh", "title": "Disabling External Influence in Social Networks via Edge Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS cs.MA math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing socio-psychological studies suggest that users of a social network\nform their opinions relying on the opinions of their neighbors. According to\nDeGroot opinion formation model, one value of particular importance is the\nasymptotic consensus value---the sum of user opinions weighted by the users'\neigenvector centralities. This value plays the role of an attractor for the\nopinions in the network and is a lucrative target for external influence.\nHowever, since any potentially malicious control of the opinion distribution in\na social network is clearly undesirable, it is important to design methods to\nprevent the external attempts to strategically change the asymptotic consensus\nvalue.\n  In this work, we assume that the adversary wants to maximize the asymptotic\nconsensus value by altering the opinions of some users in a network; we, then,\nstate DIVER---an NP-hard problem of disabling such external influence attempts\nby strategically adding a limited number of edges to the network. Relying on\nthe theory of Markov chains, we provide perturbation analysis that shows how\neigenvector centrality and, hence, DIVER's objective function change in\nresponse to an edge's addition to the network. The latter leads to the design\nof a pseudo-linear-time heuristic for DIVER, whose computation relies on\nefficient estimation of mean first passage times in a Markov chain. We confirm\nour theoretical findings in experiments.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 02:49:02 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Amelkin", "Victor", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1709.08431", "submitter": "Sid Chi-Kin Chau", "authors": "Majid Khonji, Sid Chi-Kin Chau, and Khaled Elbassion", "title": "Combinatorial Optimization of AC Optimal Power Flow with Discrete\n  Demands in Radial Networks", "comments": "To be published in IEEE Transactions on Control Networked Systems", "journal-ref": "IEEE Transactions on Control of Network Systems (Volume: 7 ,\n  Issue: 2 , June 2020) Page(s): 887 - 898", "doi": "10.1109/TCNS.2019.2951657", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AC Optimal power flow (OPF) problem is one of the most fundamental\nproblems in power systems engineering. For the past decades, researchers have\nbeen relying on unproven heuristics to tackle OPF. The hardness of OPF stems\nfrom two issues: (1) non-convexity and (2) combinatoric constraints (e.g.,\ndiscrete power extraction constraints). The recent advances in providing\nsufficient conditions on the exactness of convex relaxation of OPF can address\nthe issue of non-convexity. To complete the understanding of OPF, this paper\npresents a polynomial-time approximation algorithm to solve the convex-relaxed\nOPF with discrete demands as combinatoric constraints, which has a provably\nsmall parameterized approximation ratio (also known as PTAS algorithm).\nTogether with the sufficient conditions on the exactness of the convex\nrelaxation, we provide an efficient approximation algorithm to solve OPF with\ndiscrete demands, when the underlying network is radial with a fixed size and\none feeder. The running time of PTAS is $O(n^{4m/\\epsilon}T)$, where $T$ is the\ntime required to solve a convex relaxation of the problem, and $m, \\epsilon$\nare fixed constants. Based on prior hardness results of OPF, our PTAS is among\nthe best achievable in theory. Simulations show our algorithm can produce\nclose-to-optimal solutions in practice.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 11:29:35 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 06:39:39 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 05:10:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Khonji", "Majid", ""], ["Chau", "Sid Chi-Kin", ""], ["Elbassion", "Khaled", ""]]}, {"id": "1709.08436", "submitter": "Zhijie Zhang", "authors": "Xiaoming Sun, Jialin Zhang, Zhijie Zhang", "title": "A Linear Algorithm for Finding the Sink of Unique Sink Orientations on\n  Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orientation of a grid is called unique sink orientation (USO) if each of\nits nonempty subgrids has a unique sink. Particularly, the original grid itself\nhas a unique global sink. In this work we investigate the problem of how to\nfind the global sink using minimum number of queries to an oracle. There are\ntwo different oracle models: the vertex query model where the orientation of\nall edges incident to the queried vertex are provided, and the edge query model\nwhere the orientation of the queried edge is provided. In the 2-dimensional\ncase, we design an optimal linear deterministic algorithm for the vertex query\nmodel and an almost linear deterministic algorithm for the edge query model,\npreviously the best known algorithms run in O(N logN) time for the vertex query\nmodel and O(N^1.404) time for the edge query model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 11:49:16 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""], ["Zhang", "Zhijie", ""]]}, {"id": "1709.08459", "submitter": "Antti Ukkonen", "authors": "Antti Ukkonen", "title": "Crowdsourced correlation clustering with relative distance comparisons", "comments": "short version published at IEEE ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced, or human computation based clustering algorithms usually rely\non relative distance comparisons, as these are easier to elicit from human\nworkers than absolute distance information. A relative distance comparison is a\nstatement of the form \"item A is closer to item B than to item C\". However,\nmany existing clustering algorithms that use relative distances are rather\ncomplex. They are often based on a two-step approach, where the relative\ndistances are first used to learn either a distance matrix, or an embedding of\nthe items, and then some standard clustering method is applied in a second\nstep. In this paper we argue that it should be possible to compute a clustering\ndirectly from relative distance comparisons. Our ideas are built upon existing\nwork on correlation clustering, a well-known non-parametric approach to\nclustering. The technical contribution of this work is twofold. We first define\na novel variant of correlation clustering that is based on relative distance\ncomparisons, and hence suitable for human computation. We go on to show that\nour new problem is closely related to basic correlation clustering, and use\nthis property to design an approximation algorithm for our problem. As a second\ncontribution, we propose a more practical algorithm, which we empirically\ncompare against existing methods from literature. Experiments with synthetic\ndata suggest that our approach can outperform more complex methods. Also, our\nmethod efficiently finds good and intuitive clusterings from real relative\ndistance comparison data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 12:51:46 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Ukkonen", "Antti", ""]]}, {"id": "1709.08561", "submitter": "Heng Guo", "authors": "Heng Guo, Mark Jerrum", "title": "A polynomial-time approximation algorithm for all-terminal network\n  reliability", "comments": "accepted to SICOMP", "journal-ref": "SIAM J. Comput. 48 (2019), no. 3, 964-978", "doi": "10.1137/18M1201846", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fully polynomial-time randomized approximation scheme (FPRAS) for\nthe all-terminal network reliability problem, which is to determine the\nprobability that, in a undirected graph, assuming each edge fails\nindependently, the remaining graph is still connected. Our main contribution is\nto confirm a conjecture by Gorodezky and Pak (Random Struct. Algorithms, 2014),\nthat the expected running time of the \"cluster-popping\" algorithm in\nbi-directed graphs is bounded by a polynomial in the size of the input.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:52:14 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 18:14:10 GMT"}, {"version": "v3", "created": "Sun, 10 Feb 2019 23:11:32 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Guo", "Heng", ""], ["Jerrum", "Mark", ""]]}, {"id": "1709.08563", "submitter": "Christian Schulz", "authors": "Orlando Moreira, Merten Popp, Christian Schulz", "title": "Evolutionary Acyclic Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed graphs are widely used to model data flow and execution dependencies\nin streaming applications. This enables the utilization of graph partitioning\nalgorithms for the problem of parallelizing computation for multiprocessor\narchitectures. However due to resource restrictions, an acyclicity constraint\non the partition is necessary when mapping streaming applications to an\nembedded multiprocessor. Here, we contribute a multi-level algorithm for the\nacyclic graph partitioning problem. Based on this, we engineer an evolutionary\nalgorithm to further reduce communication cost, as well as to improve load\nbalancing and the scheduling makespan on embedded multiprocessor architectures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:54:25 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Moreira", "Orlando", ""], ["Popp", "Merten", ""], ["Schulz", "Christian", ""]]}, {"id": "1709.08858", "submitter": "Mitsuo Yoshida", "authors": "Kana Oomoto, Haruka Oikawa, Eiko Yamamoto, Mitsuo Yoshida, Masayuki\n  Okabe, Kyoji Umemura", "title": "Polysemy Detection in Distributed Representation of Word Sense", "comments": "The 9th International Conference on Knowledge and Smart Technology\n  (KST-2017)", "journal-ref": null, "doi": "10.1109/KST.2017.7886073", "report-no": null, "categories": "cs.DS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a statistical test to determine whether a given\nword is used as a polysemic word or not. The statistic of the word in this test\nroughly corresponds to the fluctuation in the senses of the neighboring words a\nnd the word itself. Even though the sense of a word corresponds to a single\nvector, we discuss how polysemy of the words affects the position of vectors.\nFinally, we also explain the method to detect this effect.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 06:51:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Oomoto", "Kana", ""], ["Oikawa", "Haruka", ""], ["Yamamoto", "Eiko", ""], ["Yoshida", "Mitsuo", ""], ["Okabe", "Masayuki", ""], ["Umemura", "Kyoji", ""]]}, {"id": "1709.08900", "submitter": "Keisuke Goto", "authors": "Takashi Katoh, Keisuke Goto", "title": "In-Place Initializable Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An initializable array is an array that supports the read and write\noperations for any element and the initialization of the entire array. This\npaper proposes a simple in-place algorithm to implement an initializable array\nof length $N$ containing $\\ell \\in O(w)$ bits entries on the word RAM model\nwith $w$ bits word size, which supports all the operations in constant\nworst-case time. The algorithm requires $N \\ell+1$ bits in total, namely, which\nrequires only 1 extra bit on top of a normal array of length $N$ containing\n$\\ell$ bits entries. The time and space complexities are optimal since it was\nalready proven that there is no implementation of an initializable array with\nno extra bit supporting all the operations in constant worst-case time [Hagerup\nand Kammer, ISAAC 2017]. Our algorithm significantly improves upon the best\nalgorithm presented in the earlier studies [Navarro, CSUR 2014] which uses $N +\no(N)$ extra bits to support all the operations in constant worst-case time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 09:09:17 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 13:24:29 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 00:35:22 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 05:56:35 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Katoh", "Takashi", ""], ["Goto", "Keisuke", ""]]}, {"id": "1709.08949", "submitter": "Ben Strasser", "authors": "Ben Strasser", "title": "Computing Tree Decompositions with FlowCutter: PACE 2017 Submission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the algorithm behind our PACE 2017 submission to the heuristic\ntree decomposition computation track. It was the only competitor to solve all\ninstances and won a tight second place. The algorithm was originally developed\nin the context of accelerating shortest path computation on road graphs using\nmultilevel partitions. We illustrate how this seemingly unrelated field fits\ninto tree decomposition and parameterized complexity theory.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 11:34:38 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Strasser", "Ben", ""]]}, {"id": "1709.09099", "submitter": "Chiwan Park", "authors": "Chiwan Park, Ha-Myung Park, Minji Yoon, U Kang", "title": "PMV: Pre-partitioned Generalized Matrix-Vector Multiplication for\n  Scalable Graph Mining", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we analyze enormous networks including the Web and social networks\nwhich have hundreds of billions of nodes and edges? Network analyses have been\nconducted by various graph mining methods including shortest path computation,\nPageRank, connected component computation, random walk with restart, etc. These\ngraph mining methods can be expressed as generalized matrix-vector\nmultiplication which consists of few operations inspired by typical\nmatrix-vector multiplication. Recently, several graph processing systems based\non matrix-vector multiplication or their own primitives have been proposed to\ndeal with large graphs; however, they all have failed on Web-scale graphs due\nto insufficient memory space or the lack of consideration for I/O costs. In\nthis paper, we propose PMV (Pre-partitioned generalized Matrix-Vector\nmultiplication), a scalable distributed graph mining method based on\ngeneralized matrix-vector multiplication on distributed systems. PMV\nsignificantly decreases the communication cost, which is the main bottleneck of\ndistributed systems, by partitioning the input graph in advance and judiciously\napplying execution strategies based on the density of the pre-partitioned\nsub-matrices. Experiments show that PMV succeeds in processing up to 16x larger\ngraphs than existing distributed memory-based graph mining methods, and\nrequires 9x less time than previous disk-based graph mining methods by reducing\nI/O costs significantly.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:53:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Park", "Chiwan", ""], ["Park", "Ha-Myung", ""], ["Yoon", "Minji", ""], ["Kang", "U", ""]]}, {"id": "1709.09209", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Radoslav Fulek and Csaba D. T\\'oth", "title": "Recognizing Weak Embeddings of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for a problem in the interface between\nclustering and graph embeddings. An embedding $\\varphi:G\\rightarrow M$ of a\ngraph $G$ into a 2-manifold $M$ maps the vertices in $V(G)$ to distinct points\nand the edges in $E(G)$ to interior-disjoint Jordan arcs between the\ncorresponding vertices. In applications in clustering, cartography, and\nvisualization, nearby vertices and edges are often bundled to the same point or\noverlapping arcs, due to data compression or low resolution. This raises the\ncomputational problem of deciding whether a given map $\\varphi:G\\rightarrow M$\ncomes from an embedding. A map $\\varphi:G\\rightarrow M$ is a \\textbf{weak\nembedding} if it can be perturbed into an embedding\n$\\psi_\\varepsilon:G\\rightarrow M$ with\n$\\|\\varphi-\\psi_\\varepsilon\\|<\\varepsilon$ for every $\\varepsilon>0$, where\n$\\|.\\|$ is the unform norm. A polynomial-time algorithm for recognizing weak\nembeddings has recently been found by Fulek and Kyn\\v{c}l. It reduces the\nproblem to solving a system of linear equations over $\\mathbb{Z}_2$. It runs in\n$O(n^{2\\omega})\\leq O(n^{4.75})$ time, where $\\omega\\in [2,2.373)$ is the\nmatrix multiplication exponent and $n$ is the number of vertices and edges of\n$G$. We improve the running time to $O(n\\log n)$. Our algorithm is also\nconceptually simpler: We perform a sequence of \\emph{local operations} that\ngradually \"untangles\" the image $\\varphi(G)$ into an embedding $\\psi(G)$, or\nreports that $\\varphi$ is not a weak embedding. It combines local constraints\non the orientation of subgraphs directly, thereby eliminating the need for\nsolving large systems of linear equations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 18:21:59 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 16:04:11 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Fulek", "Radoslav", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1709.09307", "submitter": "Georgina Hall", "authors": "Amir Ali Ahmadi and Georgina Hall", "title": "On the construction of converging hierarchies for polynomial\n  optimization based on certificates of global positivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.SY math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, techniques based on convex optimization and real algebra\nthat produce converging hierarchies of lower bounds for polynomial minimization\nproblems have gained much popularity. At their heart, these hierarchies rely\ncrucially on Positivstellens\\\"atze from the late 20th century (e.g., due to\nStengle, Putinar, or Schm\\\"udgen) that certify positivity of a polynomial on an\narbitrary closed basic semialgebraic set. In this paper, we show that such\nhierarchies could in fact be designed from much more limited\nPositivstellens\\\"atze dating back to the early 20th century that only certify\npositivity of a polynomial globally. More precisely, we show that any inner\napproximation to the cone of positive homogeneous polynomials that is\narbitrarily tight can be turned into a converging hierarchy of lower bounds for\ngeneral polynomial minimization problems with compact feasible sets. This in\nparticular leads to a semidefinite programming-based hierarchy that relies\nsolely on Artin's solution to Hilbert's 17th problem. We also use a classical\nresult of Poly\\'a on global positivity of even forms to construct an\n\"optimization-free\" converging hierarchy for general polynomial minimization\nproblems. This hierarchy only requires polynomial multiplication and checking\nnonnegativity of coefficients of certain fixed polynomials. As a corollary, we\nobtain new linear programming and second-order cone programming-based\nhierarchies for polynomial minimization problems that rely on the recently\nintroduced concepts of dsos and sdsos polynomials. We remark that the scope of\nthis paper is theoretical at this stage as our hierarchies-though they involve\nat most two sum of squares constraints or only basic arithmetic at each\nlevel-require the use of bisection and increase the number of variables (resp.\ndegree) of the problem by the number of inequality constraints plus three\n(resp. by a factor of two).\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 02:11:51 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 10:02:40 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Hall", "Georgina", ""]]}, {"id": "1709.09574", "submitter": "Jelani Nelson", "authors": "Jacob Teo Por Loong, Jelani Nelson, Huacheng Yu", "title": "Fillable arrays with constant time operations and a single bit of\n  redundancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fillable array problem one must maintain an array A[1..n] of $w$-bit\nentries subject to random access reads and writes, and also a\n$\\texttt{fill}(\\Delta)$ operation which sets every entry of to some\n$\\Delta\\in\\{0,\\ldots,2^w-1\\}$. We show that with just one bit of redundancy,\ni.e. a data structure using $nw+1$ bits of memory,\n$\\texttt{read}/\\texttt{fill}$ can be implemented in worst case constant time,\nand $\\texttt{write}$ can be implemented in either amortized constant time\n(deterministically) or worst case expected constant (randomized). In the latter\ncase, we need to store an additional $O(\\log n)$ random bits to specify a\npermutation drawn from an $1/n^2$-almost pairwise independent family.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 15:15:14 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Loong", "Jacob Teo Por", ""], ["Nelson", "Jelani", ""], ["Yu", "Huacheng", ""]]}, {"id": "1709.09737", "submitter": "Jean-Florent Raymond", "authors": "Fedor V. Fomin, Petr A. Golovach, Jean-Florent Raymond", "title": "On the tractability of optimization problems on H-graphs", "comments": "42 pages, 4 figures. Accepted by Algorithmica in 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $H$, a graph $G$ is an $H$-graph if it is an intersection graph\nof connected subgraphs of some subdivision of $H$. $H$-graphs naturally\ngeneralize several important graph classes like interval or circular-arc graph.\nThis class was introduced in the early 1990s by B\\'ir\\'o, Hujter, and Tuza.\nRecently, Chaplick et al. initiated the algorithmic study of $H$-graphs by\nshowing that a number of fundamental optimization problems are solvable in\npolynomial time on $H$-graphs. We extend and complement these algorithmic\nfindings in several directions.\n  First we show that for every fixed $H$, the class of $H$-graphs is of\nlogarithmically-bounded boolean-width (via mim-width). Pipelined with the\nplethora of known algorithms on graphs of bounded boolean-width, this describes\na large class of problems solvable in polynomial time on $H$-graphs. We also\nobserve that $H$-graphs are graphs with polynomially many minimal separators.\nCombined with the work of Fomin, Todinca and Villanger on algorithmic\nproperties of such classes of graphs, this identify another wide class of\nproblems solvable in polynomial time on $H$-graphs.\n  The most fundamental optimization problems among the problems solvable in\npolynomial time on $H$-graphs are Maximum Clique, Maximum Independent Set, and\nMinimum Dominating Set. We provide a more refined complexity analysis of these\nproblems from the perspective of Parameterized Complexity. We show that Maximum\nIndependent Set and Minimum Dominating Set are W[1]-hard being parameterized by\nthe size of $H$ plus the size of the solution. On the other hand, we prove that\nwhen $H$ is a tree, then Minimum Dominating Set is fixed-parameter tractable\nparameterized (FPT) by the size of $H$. For Maximum Clique we show that it\nadmits a polynomial kernel parameterized by $H$ and the solution size.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 21:21:56 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 14:25:23 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2018 14:24:29 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 16:30:08 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Raymond", "Jean-Florent", ""]]}, {"id": "1709.09767", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen", "title": "A Nearly-linear Time Algorithm for Submodular Maximization with a\n  Knapsack Constraint", "comments": "The matroid results included in v2 are now part of a separate arxiv\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a monotone submodular function subject\nto a knapsack constraint. Our main contribution is an algorithm that achieves a\nnearly-optimal, $1 - 1/e - \\epsilon$ approximation, using\n$(1/\\epsilon)^{O(1/\\epsilon^4)} n \\log^2{n}$ function evaluations and\narithmetic operations. Our algorithm is impractical but theoretically\ninteresting, since it overcomes a fundamental running time bottleneck of the\nmultilinear extension relaxation framework. This is the main approach for\nobtaining nearly-optimal approximation guarantees for important classes of\nconstraints but it leads to $\\Omega(n^2)$ running times, since evaluating the\nmultilinear extension is expensive. Our algorithm maintains a fractional\nsolution with only a constant number of entries that are strictly fractional,\nwhich allows us to overcome this obstacle.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 00:26:16 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 16:02:02 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 02:27:38 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1709.09778", "submitter": "Benjamin Rubinstein", "authors": "Benjamin Fish, Lev Reyzin, Benjamin I. P. Rubinstein", "title": "Sampling Without Compromising Accuracy in Adaptive Data Analysis", "comments": "Appearing in the 31st International Conference on Algorithmic\n  Learning Theory (ALT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study how to use sampling to speed up mechanisms for\nanswering adaptive queries into datasets without reducing the accuracy of those\nmechanisms. This is important to do when both the datasets and the number of\nqueries asked are very large. In particular, we describe a mechanism that\nprovides a polynomial speed-up per query over previous mechanisms, without\nneeding to increase the total amount of data required to maintain the same\ngeneralization error as before. We prove that this speed-up holds for arbitrary\nstatistical queries. We also provide an even faster method for achieving\nstatistically-meaningful responses wherein the mechanism is only allowed to see\na constant number of samples from the data per query. Finally, we show that our\ngeneral results yield a simple, fast, and unified approach for adaptively\noptimizing convex and strongly convex functions over a dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 01:45:02 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 20:34:52 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 00:18:42 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Fish", "Benjamin", ""], ["Reyzin", "Lev", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1709.09808", "submitter": "Alexandre Blondin Mass\\'e Ph.D.", "authors": "Alexandre Blondin Mass\\'e, Julien de Carufel, Alain Goupil, M\\'elodie\n  Lapointe, \\'Emile Nadeau, \\'Elise Vandomme", "title": "Fully leafed induced subtrees", "comments": "16 pages, 8 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a simple graph on $n$ vertices. We consider the problem LIS of\ndeciding whether there exists an induced subtree with exactly $i \\leq n$\nvertices and $\\ell$ leaves in $G$. We study the associated optimization\nproblem, that consists in computing the maximal number of leaves, denoted by\n$L_G(i)$, realized by an induced subtree with $i$ vertices, for $0 \\le i \\le\nn$. We begin by proving that the LIS problem is NP-complete in general and then\nwe compute the values of the map $L_G$ for some classical families of graphs\nand in particular for the $d$-dimensional hypercubic graphs $Q_d$, for $2 \\leq\nd \\leq 6$. We also describe a nontrivial branch and bound algorithm that\ncomputes the function $L_G$ for any simple graph $G$. In the special case where\n$G$ is a tree of maximum degree $\\Delta$, we provide a $\\mathcal{O}(n^3\\Delta)$\ntime and $\\mathcal{O}(n^2)$ space algorithm to compute the function $L_G$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 05:29:51 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 01:55:38 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 21:16:46 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Mass\u00e9", "Alexandre Blondin", ""], ["de Carufel", "Julien", ""], ["Goupil", "Alain", ""], ["Lapointe", "M\u00e9lodie", ""], ["Nadeau", "\u00c9mile", ""], ["Vandomme", "\u00c9lise", ""]]}, {"id": "1709.10061", "submitter": "Shuran Zheng", "authors": "Shuran Zheng, Bo Waggoner, Yang Liu and Yiling Chen", "title": "Active Information Acquisition for Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider partially-specified optimization problems where the goal is to\nactively, but efficiently, acquire missing information about the problem in\norder to solve it. An algorithm designer wishes to solve a linear program (LP),\n$\\max \\mathbf{c}^T \\mathbf{x}$ s.t. $\\mathbf{A}\\mathbf{x} \\leq \\mathbf{b},\n\\mathbf{x} \\ge \\mathbf{0}$, but does not initially know some of the parameters.\nThe algorithm can iteratively choose an unknown parameter and gather\ninformation in the form of a noisy sample centered at the parameter's (unknown)\nvalue. The goal is to find an approximately feasible and optimal solution to\nthe underlying LP with high probability while drawing a small number of\nsamples. We focus on two cases. (1) When the parameters $\\mathbf{c}$ of the\nobjective are initially unknown, we take an information-theoretic approach and\ngive roughly matching upper and lower sample complexity bounds, with an\n(inefficient) successive-elimination algorithm. (2) When the parameters\n$\\mathbf{b}$ of the constraints are initially unknown, we propose an efficient\nalgorithm combining techniques from the ellipsoid method for LP and\nconfidence-bound approaches from bandit algorithms. The algorithm adaptively\ngathers information about constraints only as needed in order to make progress.\nWe give sample complexity bounds for the algorithm and demonstrate its\nimprovement over a naive approach via simulation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 17:03:48 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 02:20:41 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Zheng", "Shuran", ""], ["Waggoner", "Bo", ""], ["Liu", "Yang", ""], ["Chen", "Yiling", ""]]}, {"id": "1709.10146", "submitter": "Dariusz Dereniowski", "authors": "Shantanu Das, Dariusz Dereniowski and Przemys{\\l}aw Uzna\\'nski", "title": "Energy Constrained Depth First Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth first search is a natural algorithmic technique for constructing a\nclosed route that visits all vertices of a graph. The length of such route\nequals, in an edge-weighted tree, twice the total weight of all edges of the\ntree and this is asymptotically optimal over all exploration strategies. This\npaper considers a variant of such search strategies where the length of each\nroute is bounded by a positive integer $B$ (e.g. due to limited energy\nresources of the searcher). The objective is to cover all the edges of a tree\n$T$ using the minimum number of routes, each starting and ending at the root\nand each being of length at most $B$. To this end, we analyze the following\nnatural greedy tree traversal process that is based on decomposing a depth\nfirst search traversal into a sequence of limited length routes. Given any\narbitrary depth first search traversal $R$ of the tree $T$, we cover $R$ with\nroutes $R_1,\\ldots,R_l$, each of length at most $B$ such that: $R_i$ starts at\nthe root, reaches directly the farthest point of $R$ visited by $R_{i-1}$, then\n$R_i$ continues along the path $R$ as far as possible, and finally $R_i$\nreturns to the root. We call the above algorithm \\emph{piecemeal-DFS} and we\nprove that it achieves the asymptotically minimal number of routes $l$,\nregardless of the choice of $R$. Our analysis also shows that the total length\nof the traversal (and thus the traversal time) of piecemeal-DFS is\nasymptotically minimum over all energy-constrained exploration strategies. The\nfact that $R$ can be chosen arbitrarily means that the exploration strategy can\nbe constructed in an online fashion when the input tree $T$ is not known in\nadvance. Surprisingly, our results show that depth first search is efficient\nfor energy constrained exploration of trees, even though it is known that the\nsame does not hold for energy constrained exploration of arbitrary graphs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 19:50:10 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 13:46:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Das", "Shantanu", ""], ["Dereniowski", "Dariusz", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1709.10254", "submitter": "Noam Ravid", "authors": "Noam Ravid, Dori Medini and Benny Kimelfeld", "title": "Ranked Enumeration of Minimal Triangulations", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree decomposition of a graph facilitates computations by grouping vertices\ninto bags that are interconnected in an acyclic structure, hence their\nimportance in a plethora of problems such as query evaluation over databases\nand inference over probabilistic graphical models. The relative benefit from\ndifferent tree decompositions is measured by diverse (sometime complex) cost\nfunctions that vary from one application to another. For generic cost functions\nlike width and fill-in, an optimal tree decomposition can be efficiently\ncomputed in some cases, notably when the number of minimal separators is\nbounded by a polynomial (due to Bouchitte and Todinca), we refer to this\nassumption as \"poly-MS.\" To cover the variety of cost functions in need, it has\nrecently been proposed to devise algorithms for enumerating many decomposition\ncandidates for applications to choose from using specialized, or even\nmachine-learned, cost functions.\n  We explore the ability to produce a large collection of \"high quality\" tree\ndecompositions. We present the first algorithm for ranked enumeration of the\nproper (non-redundant) tree decompositions, or equivalently minimal\ntriangulations, under a wide class of cost functions that substantially\ngeneralizes the above generic ones. On the theoretical side, we establish the\nguarantee of polynomial delay if poly-MS is assumed, or if we are interested in\ntree decompositions of a width bounded by a constant. We describe an\nexperimental evaluation on graphs of various domains (including join queries,\nBayesian networks, treewidth benchmarks and random), and explore both the\napplicability of the poly-MS assumption and the performance of our algorithm\nrelative to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 06:58:26 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 11:27:57 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ravid", "Noam", ""], ["Medini", "Dori", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1709.10258", "submitter": "Brahim Chaourar", "authors": "Brahim Chaourar", "title": "An improved algorithm for recognizing matroids", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1703.03744", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M$ be a matroid defined on a finite set $E$ and $L\\subset E$. $L$ is\nlocked in $M$ if $M|L$ and $M^*|(E\\backslash L)$ are 2-connected, and\n$min\\{r(L), r^*(E\\backslash L)\\} \\geq 2$. Locked subsets characterize\nnontrivial facets of the bases polytope. In this paper, we give a new axiom\nsystem for matroids based on locked subsets. We deduce an algorithm for\nrecognizing matroids improving the running time complexity of the best known\ntill today. This algorithm induces a polynomial time algorithm for recognizing\nuniform matroids. This latter problem is intractable if we use an independence\noracle.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 07:19:57 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 03:00:15 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chaourar", "Brahim", ""]]}, {"id": "1709.10285", "submitter": "Stefan R\\\"ummele", "authors": "Serge Gaspers, Joachim Gudmundsson, Juli\\'an Mestre, Stefan R\\\"ummele", "title": "Barrier Coverage with Non-uniform Lengths to Minimize Aggregate\n  Movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a line segment $I=[0,L]$, the so-called barrier, and a set of $n$\nsensors with varying ranges positioned on the line containing $I$, the barrier\ncoverage problem is to move the sensors so that they cover $I$, while\nminimising the total movement. In the case when all the sensors have the same\nradius the problem can be solved in $O(n \\log n)$ time (Andrews and Wang,\nAlgorithmica 2017). If the sensors have different radii the problem is known to\nbe NP-hard to approximate within a constant factor (Czyzowicz et al., ADHOC-NOW\n2009). We strengthen this result and prove that no polynomial time\n$\\rho^{1-\\varepsilon}$-approximation algorithm exists unless $P=NP$, where\n$\\rho$ is the ratio between the largest radius and the smallest radius. Even\nwhen we restrict the number of sensors that are allowed to move by a parameter\n$k$, the problem turns out to be W[1]-hard. On the positive side we show that a\n$((2+\\varepsilon)\\rho+2/\\varepsilon)$-approximation can be computed in\n$O(n^3/\\varepsilon^2)$ time and we prove fixed-parameter tractability when\nparameterized by the total movement assuming all numbers in the input are\nintegers.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 08:38:48 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Gaspers", "Serge", ""], ["Gudmundsson", "Joachim", ""], ["Mestre", "Juli\u00e1n", ""], ["R\u00fcmmele", "Stefan", ""]]}, {"id": "1709.10305", "submitter": "Xiaoyang Chen", "authors": "Xiaoyang Chen, Hongwei Huo, Jun Huan and Jeffrey Scott Vitter", "title": "Fast Computation of Graph Edit Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph edit distance (GED) is a well-established distance measure widely\nused in many applications. However, existing methods for the GED computation\nsuffer from several drawbacks including oversized search space, huge memory\nconsumption, and lots of expensive backtracking. In this paper, we present\nBSS_GED, a novel vertex-based mapping method for the GED computation. First, we\ncreate a small search space by reducing the number of invalid and redundant\nmappings involved in the GED computation. Then, we utilize beam-stack search\ncombined with two heuristics to efficiently compute GED, achieving a flexible\ntrade-off between available memory and expensive backtracking. Extensive\nexperiments demonstrate that BSS GED is highly efficient for the GED\ncomputation on sparse as well as dense graphs and outperforms the\nstate-of-the-art GED methods. In addition, we also apply BSS_GED to the graph\nsimilarity search problem and the practical results confirm its efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 09:51:33 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Chen", "Xiaoyang", ""], ["Huo", "Hongwei", ""], ["Huan", "Jun", ""], ["Vitter", "Jeffrey Scott", ""]]}, {"id": "1709.10307", "submitter": "Mordecai Golin", "authors": "Mordecai J. Golin, Hadi Khodabande and Bo Qin", "title": "Non-approximability and Polylogarithmic Approximations of the\n  Single-Sink Unsplittable and Confluent Dynamic Flow Problems", "comments": "Full version of paper appearing in Isaac'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Flows were introduced by Ford and Fulkerson in 1958 to model flows\nover time. They define edge capacities to be the total amount of flow that can\nenter an edge {\\em in one time unit}. Each edge also has a length, representing\nthe time needed to traverse it. Dynamic Flows have been used to model many\nproblems including traffic congestion, hop-routing of packets and evacuation\nprotocols in buildings. While the basic problem of moving the maximal amount of\nsupplies from sources to sinks is polynomial time solvable, natural minor\nmodifications can make it NP-hard.\n  One such modification is that flows be confluent, i.e., all flows leaving a\nvertex must leave along the same edge. This corresponds to natural conditions\nin, e.g., evacuation planning and hop routing.\n  We investigate the single-sink Confluent Quickest Flow problem. The input is\na graph with edge capacities and lengths, sources with supplies and a sink. The\nproblem is to find a confluent flow minimizing the time required to send\nsupplies to the sink. Our main results include: a) Logarithmic\nNon-Approximability: Directed Confluent Quickest Flows cannot be approximated\nin polynomial time with an $O(\\log n)$ approximation factor, unless $P=NP$.\n  b) Polylogarithmic Bicriteria Approximations: Polynomial time $(O(\\log^8 n),\nO(\\log^2 \\kappa))$ bicritera approximation algorithms for the Confluent\nQuickest Flow problem where $\\kappa$ is the number of sinks, in both directed\nand undirected graphs.\n  Corresponding results are also developed for the Confluent Maximum Flow over\ntimeproblem. The techniques developed also improve recent approximation\nalgorithms for static confluent flows.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 10:00:19 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Golin", "Mordecai J.", ""], ["Khodabande", "Hadi", ""], ["Qin", "Bo", ""]]}, {"id": "1709.10405", "submitter": "Jan-Hendrik Lorenz", "authors": "Jan-Hendrik Lorenz", "title": "Runtime Distributions and Criteria for Restarts", "comments": null, "journal-ref": "SOFSEM 2018: Theory and Practice of Computer Science. SOFSEM 2018.\n  Lecture Notes in Computer Science, vol 10706, (2018), 493-507", "doi": "10.1007/978-3-319-73117-9_35", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms sometimes employ a restart strategy. After a certain\nnumber of steps, the current computation is aborted and restarted with a new,\nindependent random seed. In some cases, this results in an improved overall\nexpected runtime. This work introduces properties of the underlying runtime\ndistribution which determine whether restarts are advantageous. The most\ncommonly used probability distributions admit the use of a scale and a location\nparameter. Location parameters shift the density function to the right, while\nscale parameters affect the spread of the distribution. It is shown that for\nall distributions scale parameters do not influence the usefulness of restarts\nand that location parameters only have a limited influence. This result\nsimplifies the analysis of the usefulness of restarts. The most important\nruntime probability distributions are the log-normal, the Weibull, and the\nPareto distribution. In this work, these distributions are analyzed for the\nusefulness of restarts. Secondly, a condition for the optimal restart time (if\nit exists) is provided. The log-normal, the Weibull, and the generalized Pareto\ndistribution are analyzed in this respect. Moreover, it is shown that the\noptimal restart time is also not influenced by scale parameters and that the\ninfluence of location parameters is only linear.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 13:39:39 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Lorenz", "Jan-Hendrik", ""]]}, {"id": "1709.10455", "submitter": "Nathaniel Kell", "authors": "Sungjin Im, Nathaniel Kell, Debmalya Panigrahi, Maryam Shadloo", "title": "Online Load Balancing for Related Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the load balancing problem, introduced by Graham in the 1960s (SIAM J. of\nAppl. Math. 1966, 1969), jobs arriving online have to be assigned to machines\nso to minimize an objective defined on machine loads. A long line of work has\naddressed this problem for both the makespan norm and arbitrary $\\ell_q$-norms\nof machine loads. Recent literature (e.g., Azar et al., STOC 2013; Im et al.,\nFOCS 2015) has further expanded the scope of this problem to vector loads, to\ncapture jobs with multi-dimensional resource requirements in applications such\nas data centers. In this paper, we completely resolve the job scheduling\nproblem for both scalar and vector jobs on related machines, i.e., where each\nmachine has a given speed and the time taken to process a job is inversely\nproportional to the speed of the machine it is assigned on.\n  We show the following results. For scalar scheduling, we give a constant\ncompetitive algorithm for optimizing any $\\ell_q$-norm for related machines.\nThe only previously known result was for the makespan norm. For vector\nscheduling, there are two natural variants for vector scheduling, depending on\nwhether the speed of a machine is dimension-dependent or not. We show a sharp\ncontrast between these two variants, proving that they are respectively\nequivalent to unrelated machines and identical machines for the makespan norm.\nWe also extend these results to arbitrary $\\ell_q$-norms of the machine loads.\nNo previous results were known for vector scheduling on related machines.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 15:31:46 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Im", "Sungjin", ""], ["Kell", "Nathaniel", ""], ["Panigrahi", "Debmalya", ""], ["Shadloo", "Maryam", ""]]}, {"id": "1709.10477", "submitter": "Frank Kammer", "authors": "Torben Hagerup and Frank Kammer", "title": "On-the-Fly Array Initialization in Less Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for all given $n,t,w \\in \\{1,2,...\\}$ with $n<2^w$, an array of\n$n$ entries of $w$ bits each can be represented on a word RAM with a word\nlength of $w$ bits in at most $nw+\\lceil n(t/(2 w))^t\\rceil$ bits of\nuninitialized memory to support constant-time initialization of the whole array\nand $O(t)$-time reading and writing of individual array entries. At one end of\nthis tradeoff, we achieve initialization and access (i.e., reading and writing)\nin constant time with $nw+\\lceil n/w^t\\rceil$ bits for arbitrary fixed $t$, to\nbe compared with $nw+\\Theta(n)$ bits for the best previous solution, and at the\nopposite end, still with constant-time initialization, we support $O(\\log\nn)$-time access with just $nw+1$ bits, which is optimal for arbitrary access\ntimes if the initialization executes fewer than $n$ steps.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 16:13:52 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Hagerup", "Torben", ""], ["Kammer", "Frank", ""]]}]