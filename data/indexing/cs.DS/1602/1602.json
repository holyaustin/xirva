[{"id": "1602.00023", "submitter": "J\\'er\\'emy Barbay", "authors": "J\\'er\\'emy Barbay", "title": "Optimal Prefix Free Codes With Partial Sorting", "comments": "13 pages, no figures. arXiv admin note: text overlap with\n  arXiv:1204.5801", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an algorithm computing an optimal prefix free code for $n$\nunsorted positive weights in time within $O(n(1+\\lg \\alpha))\\subseteq O(n\\lg\nn)$, where the alternation $\\alpha\\in[1..n-1]$ measures the amount of sorting\nrequired by the computation. This asymptotical complexity is within a constant\nfactor of the optimal in the algebraic decision tree computational model, in\nthe worst case over all instances of size $n$ and alternation $\\alpha$. Such\nresults refine the state of the art complexity of $\\Theta(n\\lg n)$ in the worst\ncase over instances of size $n$ in the same computational model, a landmark in\ncompression and coding since 1952, by the mere combination of van Leeuwen's\nalgorithm to compute optimal prefix free codes from sorted weights (known since\n1976), with Deferred Data Structures to partially sort a multiset depending on\nthe queries on it (known since 1988).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 21:58:42 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""]]}, {"id": "1602.00078", "submitter": "Hau-tieng Wu", "authors": "Ronen Talmon and Hau-tieng Wu", "title": "Latent common manifold learning with alternating diffusion: analysis and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of data sets arising from multiple sensors has drawn significant\nresearch attention over the years. Traditional methods, including kernel-based\nmethods, are typically incapable of capturing nonlinear geometric structures.\nWe introduce a latent common manifold model underlying multiple sensor\nobservations for the purpose of multimodal data fusion. A method based on\nalternating diffusion is presented and analyzed; we provide theoretical\nanalysis of the method under the latent common manifold model. To exemplify the\npower of the proposed framework, experimental results in several applications\nare reported.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 06:14:24 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 00:12:13 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Talmon", "Ronen", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1602.00192", "submitter": "Xian Qiu", "authors": "Xian Qiu, Walter Kern", "title": "On the Factor Revealing LP Approach for Facility Location with Penalties", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in the opening cost constraint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the uncapacitated facility location problem with (linear) penalty\nfunction and show that a modified JMS algorithm, combined with a randomized LP\nrounding technique due to Byrka-Aardal[1], Li[14] and Li et al.[16] yields\n1.488 approximation, improving the factor 1.5148 due to Li et al.[16]. This\ncloses the current gap between the classical facility location problem and this\npenalized variant. Main ingredient is a straightforward adaptation of the JMS\nalgorithm to the penalty setting plus a consistent use of the upper bounding\ntechnique for factor revealing LPs due to Fernandes et al.[7]. In contrast to\nthe bounds in [12], our factor revealing LP is monotone.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 03:12:57 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 02:40:32 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 12:35:38 GMT"}, {"version": "v4", "created": "Tue, 5 Jul 2016 01:53:49 GMT"}, {"version": "v5", "created": "Wed, 28 Sep 2016 05:36:21 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Qiu", "Xian", ""], ["Kern", "Walter", ""]]}, {"id": "1602.00329", "submitter": "Dominik Kempa", "authors": "Djamal Belazzougui, Juha K\\\"arkk\\\"ainen, Dominik Kempa, Simon J.\n  Puglisi", "title": "Lempel-Ziv Decoding in External Memory", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-38851-9_5", "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple and fast decoding is one of the main advantages of LZ77-type text\nencoding used in many popular file compressors such as gzip and 7zip. With the\nrecent introduction of external memory algorithms for Lempel-Ziv factorization\nthere is a need for external memory LZ77 decoding but the standard algorithm\nmakes random accesses to the text and cannot be trivially modified for external\nmemory computation. We describe the first external memory algorithms for LZ77\ndecoding, prove that their I/O complexity is optimal, and demonstrate that they\nare very fast in practice, only about three times slower than in-memory\ndecoding (when reading input and writing output is included in the time).\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 21:43:40 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Belazzougui", "Djamal", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["Kempa", "Dominik", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1602.00412", "submitter": "Mina Ghashami", "authors": "Mina Ghashami, Edo Liberty, Jeff M. Phillips", "title": "Efficient Frequent Directions Algorithm for Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Sparse Frequent Directions, a variant of Frequent\nDirections for sketching sparse matrices. It resembles the original algorithm\nin many ways: both receive the rows of an input matrix $A^{n \\times d}$ one by\none in the streaming setting and compute a small sketch $B \\in R^{\\ell \\times\nd}$. Both share the same strong (provably optimal) asymptotic guarantees with\nrespect to the space-accuracy tradeoff in the streaming setting. However,\nunlike Frequent Directions which runs in $O(nd\\ell)$ time regardless of the\nsparsity of the input matrix $A$, Sparse Frequent Directions runs in $\\tilde{O}\n(nnz(A)\\ell + n\\ell^2)$ time. Our analysis loosens the dependence on computing\nthe Singular Value Decomposition (SVD) as a black box within the Frequent\nDirections algorithm. Our bounds require recent results on the properties of\nfast approximate SVD computations. Finally, we empirically demonstrate that\nthese asymptotic improvements are practical and significant on real and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 07:33:02 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 08:23:13 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Ghashami", "Mina", ""], ["Liberty", "Edo", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1602.00422", "submitter": "Takuya Takagi", "authors": "Takuya Takagi, Shunsuke Inenaga, Kunihiko Sadakane and Hiroki Arimura", "title": "Packed Compact Tries: A Fast and Efficient Data Structure for Online\n  String Processing", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": "10.1587/transfun.E100.A.1785", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new data structure called the packed compact trie\n(packed c-trie) which stores a set $S$ of $k$ strings of total length $n$ in $n\n\\log\\sigma + O(k \\log n)$ bits of space and supports fast pattern matching\nqueries and updates, where $\\sigma$ is the size of an alphabet. Assume that\n$\\alpha = \\log_\\sigma n$ letters are packed in a single machine word on the\nstandard word RAM model, and let $f(k,n)$ denote the query and update times of\nthe dynamic predecessor/successor data structure of our choice which stores $k$\nintegers from universe $[1,n]$ in $O(k \\log n)$ bits of space. Then, given a\nstring of length $m$, our packed c-tries support pattern matching queries and\ninsert/delete operations in $O(\\frac{m}{\\alpha} f(k,n))$ worst-case time and in\n$O(\\frac{m}{\\alpha} + f(k,n))$ expected time. Our experiments show that our\npacked c-tries are faster than the standard compact tries (a.k.a. Patricia\ntrees) on real data sets. As an application of our packed c-trie, we show that\nthe sparse suffix tree for a string of length $n$ over prefix codes with $k$\nsampled positions, such as evenly-spaced and word delimited sparse suffix\ntrees, can be constructed online in $O((\\frac{n}{\\alpha} + k) f(k,n))$\nworst-case time and $O(\\frac{n}{\\alpha} + k f(k,n))$ expected time with $n \\log\n\\sigma + O(k \\log n)$ bits of space. When $k = O(\\frac{n}{\\alpha})$, by using\nthe state-of-the-art dynamic predecessor/successor data structures, we obtain\nsub-linear time construction algorithms using only $O(\\frac{n}{\\alpha})$ bits\nof space in both cases. We also discuss an application of our packed c-tries to\nonline LZD factorization.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 08:17:52 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Takagi", "Takuya", ""], ["Inenaga", "Shunsuke", ""], ["Sadakane", "Kunihiko", ""], ["Arimura", "Hiroki", ""]]}, {"id": "1602.00435", "submitter": "Rasmus Pagh", "authors": "Leszek Gasieniec and Christos Levcopoulos and Andrzej Lingas and\n  Rasmus Pagh and Takeshi Tokuyama", "title": "Efficiently Correcting Matrix Products", "comments": "Fixed invalid reference to figure in v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently correcting an erroneous product of two\n$n\\times n$ matrices over a ring. Among other things, we provide a randomized\nalgorithm for correcting a matrix product with at most $k$ erroneous entries\nrunning in $\\tilde{O}(n^2+kn)$ time and a deterministic $\\tilde{O}(kn^2)$-time\nalgorithm for this problem (where the notation $\\tilde{O}$ suppresses\npolylogarithmic terms in $n$ and $k$).\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 09:17:13 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 15:19:30 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Gasieniec", "Leszek", ""], ["Levcopoulos", "Christos", ""], ["Lingas", "Andrzej", ""], ["Pagh", "Rasmus", ""], ["Tokuyama", "Takeshi", ""]]}, {"id": "1602.00447", "submitter": "Tomasz Kociumaka", "authors": "Pawe{\\l} Gawrychowski, Tomasz Kociumaka, Wojciech Rytter, and Tomasz\n  Wale\\'n", "title": "Faster Longest Common Extension Queries in Strings over General\n  Alphabets", "comments": "Accepted to CPM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longest common extension queries (often called longest common prefix queries)\nconstitute a fundamental building block in multiple string algorithms, for\nexample computing runs and approximate pattern matching. We show that a\nsequence of $q$ LCE queries for a string of size $n$ over a general ordered\nalphabet can be realized in $O(q \\log \\log n+n\\log^*n)$ time making only\n$O(q+n)$ symbol comparisons. Consequently, all runs in a string over a general\nordered alphabet can be computed in $O(n \\log \\log n)$ time making $O(n)$\nsymbol comparisons. Our results improve upon a solution by Kosolobov\n(Information Processing Letters, 2016), who gave an algorithm with $O(n\n\\log^{2/3} n)$ running time and conjectured that $O(n)$ time is possible. We\nmake a significant progress towards resolving this conjecture. Our techniques\nextend to the case of general unordered alphabets, when the time increases to\n$O(q\\log n + n\\log^*n)$. The main tools are difference covers and the\ndisjoint-sets data structure.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 09:47:44 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 07:23:47 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Kociumaka", "Tomasz", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1602.00621", "submitter": "Marius Nicolae", "authors": "Marius Nicolae and Sanguthevar Rajasekaran", "title": "On pattern matching with k mismatches and few don't cares", "comments": "Information Processing Letters, Available online 27 October 2016,\n  ISSN 0020-0190", "journal-ref": null, "doi": "10.1016/j.ipl.2016.10.003", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of pattern matching with $k$ mismatches, where there\ncan be don't care or wild card characters in the pattern. Specifically, given a\npattern $P$ of length $m$ and a text $T$ of length $n$, we want to find all\noccurrences of $P$ in $T$ that have no more than $k$ mismatches. The pattern\ncan have don't care characters, which match any character. Without don't cares,\nthe best known algorithm for pattern matching with $k$ mismatches has a runtime\nof $O(n\\sqrt{k \\log k})$. With don't cares in the pattern, the best\ndeterministic algorithm has a runtime of $O(nk polylog m)$. Therefore, there is\nan important gap between the versions with and without don't cares.\n  In this paper we give an algorithm whose runtime increases with the number of\ndon't cares. We define an {\\em island} to be a maximal length substring of $P$\nthat does not contain don't cares. Let $q$ be the number of islands in $P$. We\npresent an algorithm that runs in $O(n\\sqrt{k\\log m}+n\\min\\{\\sqrt[3]{qk\\log^2\nm},\\sqrt{q\\log m}\\})$ time. If the number of islands $q$ is $O(k)$ this runtime\nbecomes $O(n\\sqrt{k\\log m})$, which essentially matches the best known runtime\nfor pattern matching with $k$ mismatches without don't cares. If the number of\nislands $q$ is $O(k^2)$, this algorithm is asymptotically faster than the\nprevious best algorithm for pattern matching with $k$ mismatches with don't\ncares in the pattern.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 18:08:47 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 18:12:13 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Nicolae", "Marius", ""], ["Rajasekaran", "Sanguthevar", ""]]}, {"id": "1602.00849", "submitter": "Dario  Izzo", "authors": "Dario Izzo, Daniel Hennes, Marcus M\\\"artens, Ingmar Getzner, Krzysztof\n  Nowak, Anna Heffernan, Stefano Campagnola, Chit Hong Yam, Naoya Ozaki,\n  Yoshihide Sugimoto", "title": "GTOC8: Results and Methods of ESA Advanced Concepts Team and JAXA-ISAS", "comments": "Presented at the 26th AAS/AIAA Space Flight Mechanics Meeting, Napa,\n  CA. Paper AAS 16-275", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.space-ph astro-ph.IM cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the interplanetary trajectory design problem posed by the 8th\nedition of the Global Trajectory Optimization Competition and present the\nend-to-end strategy developed by the team ACT-ISAS (a collaboration between the\nEuropean Space Agency's Advanced Concepts Team and JAXA's Institute of Space\nand Astronautical Science). The resulting interplanetary trajectory won 1st\nplace in the competition, achieving a final mission value of $J=146.33$ [Mkm].\nSeveral new algorithms were developed in this context but have an interest that\ngo beyond the particular problem considered, thus, they are discussed in some\ndetail. These include the Moon-targeting technique, allowing one to target a\nMoon encounter from a low Earth orbit; the 1-$k$ and 2-$k$ fly-by targeting\ntechniques, enabling one to design resonant fly-bys while ensuring a targeted\nfuture formation plane% is acquired at some point after the manoeuvre ; the\ndistributed low-thrust targeting technique, admitting one to control the\nspacecraft formation plane at 1,000,000 [km]; and the low-thrust optimization\ntechnique, permitting one to enforce the formation plane's orientations as path\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 09:38:22 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 10:38:12 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Izzo", "Dario", ""], ["Hennes", "Daniel", ""], ["M\u00e4rtens", "Marcus", ""], ["Getzner", "Ingmar", ""], ["Nowak", "Krzysztof", ""], ["Heffernan", "Anna", ""], ["Campagnola", "Stefano", ""], ["Yam", "Chit Hong", ""], ["Ozaki", "Naoya", ""], ["Sugimoto", "Yoshihide", ""]]}, {"id": "1602.00963", "submitter": "Flavio Vella", "authors": "Flavio Vella, Giancarlo Carbone and Massimo Bernaschi", "title": "Algorithms and Heuristics for Scalable Betweenness Centrality\n  Computation on Multi-GPU Systems", "comments": null, "journal-ref": "Journal of Experimental Algorithmics (JEA) 2018", "doi": "10.1145/3182656", "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness Centrality (BC) is steadily growing in popularity as a metrics of\nthe influence of a vertex in a graph. The BC score of a vertex is proportional\nto the number of all-pairs-shortest-paths passing through it. However, complete\nand exact BC computation for a large-scale graph is an extraordinary challenge\nthat requires high performance computing techniques to provide results in a\nreasonable amount of time. Our approach combines bi-dimensional (2-D)\ndecomposition of the graph and multi-level parallelism together with a suitable\ndata-thread mapping that overcomes most of the difficulties caused by the\nirregularity of the computation on GPUs. Furthermore, we propose novel\nheuristics which exploit the topology information of the graph in order to\nreduce time and space requirements of BC computation. Experimental results on\nsynthetic and real-world graphs show that the proposed techniques allow the BC\ncomputation of graphs which are too large to fit in the memory of a single\ncomputational node along with a significant reduction of the computing time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 15:08:29 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Vella", "Flavio", ""], ["Carbone", "Giancarlo", ""], ["Bernaschi", "Massimo", ""]]}, {"id": "1602.01016", "submitter": "Thang Dinh", "authors": "Thang N. Dinh, Xiang Li, and My T. Thai", "title": "Network Clustering via Maximizing Modularity: Approximation Algorithms\n  and Theoretical Limits", "comments": "Appeared in IEEE ICDM 2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.139", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social networks and complex systems are found to be naturally divided\ninto clusters of densely connected nodes, known as community structure (CS).\nFinding CS is one of fundamental yet challenging topics in network science. One\nof the most popular classes of methods for this problem is to maximize Newman's\nmodularity. However, there is a little understood on how well we can\napproximate the maximum modularity as well as the implications of finding\ncommunity structure with provable guarantees. In this paper, we settle\ndefinitely the approximability of modularity clustering, proving that\napproximating the problem within any (multiplicative) positive factor is\nintractable, unless P = NP. Yet we propose the first additive approximation\nalgorithm for modularity clustering with a constant factor. Moreover, we\nprovide a rigorous proof that a CS with modularity arbitrary close to maximum\nmodularity QOPT might bear no similarity to the optimal CS of maximum\nmodularity. Thus even when CS with near-optimal modularity are found, other\nverification methods are needed to confirm the significance of the structure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 17:23:59 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Dinh", "Thang N.", ""], ["Li", "Xiang", ""], ["Thai", "My T.", ""]]}, {"id": "1602.01116", "submitter": "Jakub Radoszewski", "authors": "Carl Barton, Tomasz Kociumaka, Solon P. Pissis and Jakub Radoszewski", "title": "Efficient Index for Weighted Sequences", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding factors of a text string which are identical or\nsimilar to a given pattern string is a central problem in computer science. A\ngeneralised version of this problem consists in implementing an index over the\ntext to support efficient on-line pattern queries. We study this problem in the\ncase where the text is weighted: for every position of the text and every\nletter of the alphabet a probability of occurrence of this letter at this\nposition is given. Sequences of this type, also called position weight\nmatrices, are commonly used to represent imprecise or uncertain data. A\nweighted sequence may represent many different strings, each with probability\nof occurrence equal to the product of probabilities of its letters at\nsubsequent positions. Given a probability threshold $1/z$, we say that a\npattern string $P$ matches a weighted text at position $i$ if the product of\nprobabilities of the letters of $P$ at positions $i,\\ldots,i+|P|-1$ in the text\nis at least $1/z$. In this article, we present an $O(nz)$-time construction of\nan $O(nz)$-sized index that can answer pattern matching queries in a weighted\ntext in optimal time improving upon the state of the art by a factor of $z \\log\nz$. Other applications of this data structure include an $O(nz)$-time\nconstruction of the weighted prefix table and an $O(nz)$-time computation of\nall covers of a weighted sequence, which improve upon the state of the art by\nthe same factor.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 21:13:53 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Barton", "Carl", ""], ["Kociumaka", "Tomasz", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1602.01295", "submitter": "Petteri Kaski", "authors": "Andreas Bj\\\"orklund and Petteri Kaski", "title": "How proofs are prepared at Camelot", "comments": "42 pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a design framework for robust, independently verifiable, and\nworkload-balanced distributed algorithms working on a common input. An\nalgorithm based on the framework is essentially a distributed encoding\nprocedure for a Reed--Solomon code, which enables (a) robustness against\nbyzantine failures with intrinsic error-correction and identification of failed\nnodes, and (b) independent randomized verification to check the entire\ncomputation for correctness, which takes essentially no more resources than\neach node individually contributes to the computation. The framework builds on\nrecent Merlin--Arthur proofs of batch evaluation of Williams~[{\\em Electron.\\\nColloq.\\ Comput.\\ Complexity}, Report TR16-002, January 2016] with the\nobservation that {\\em Merlin's magic is not needed} for batch evaluation---mere\nKnights can prepare the proof, in parallel, and with intrinsic\nerror-correction.\n  The contribution of this paper is to show that in many cases the verifiable\nbatch evaluation framework admits algorithms that match in total resource\nconsumption the best known sequential algorithm for solving the problem. As our\nmain result, we show that the $k$-cliques in an $n$-vertex graph can be counted\n{\\em and} verified in per-node $O(n^{(\\omega+\\epsilon)k/6})$ time and space on\n$O(n^{(\\omega+\\epsilon)k/6})$ compute nodes, for any constant $\\epsilon>0$ and\npositive integer $k$ divisible by $6$, where $2\\leq\\omega<2.3728639$ is the\nexponent of matrix multiplication. This matches in total running time the best\nknown sequential algorithm, due to Ne{\\v{s}}et{\\v{r}}il and Poljak [{\\em\nComment.~Math.~Univ.~Carolin.}~26 (1985) 415--419], and considerably improves\nits space usage and parallelizability. Further results include novel algorithms\nfor counting triangles in sparse graphs, computing the chromatic polynomial of\na graph, and computing the Tutte polynomial of a graph.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 13:45:12 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kaski", "Petteri", ""]]}, {"id": "1602.01342", "submitter": "Frederik Mallmann-Trenn", "authors": "Petra Berenbrink, Tom Friedetzky, Peter Kling, Frederik\n  Mallmann-Trenn, Chris Wastell", "title": "Plurality Consensus via Shuffling: Lessons Learned from Load Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider \\emph{plurality consensus} in a network of $n$ nodes. Initially,\neach node has one of $k$ opinions. The nodes execute a (randomized) distributed\nprotocol to agree on the plurality opinion (the opinion initially supported by\nthe most nodes). Nodes in such networks are often quite cheap and simple, and\nhence one seeks protocols that are not only fast but also simple and space\nefficient. Typically, protocols depend heavily on the employed communication\nmechanism, which ranges from sequential (only one pair of nodes communicates at\nany time) to fully parallel (all nodes communicate with all their neighbors at\nonce) communication and everything in-between.\n  We propose a framework to design protocols for a multitude of communication\nmechanisms. We introduce protocols that solve the plurality consensus problem\nand are with probability 1-o(1) both time and space efficient. Our protocols\nare based on an interesting relationship between plurality consensus and\ndistributed load balancing. This relationship allows us to design protocols\nthat generalize the state of the art for a large range of problem parameters.\nIn particular, we obtain the same bounds as the recent result of Alistarh et\nal. (who consider only two opinions on a clique) using a much simpler protocol\nthat generalizes naturally to general graphs and multiple opinions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 15:39:49 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Berenbrink", "Petra", ""], ["Friedetzky", "Tom", ""], ["Kling", "Peter", ""], ["Mallmann-Trenn", "Frederik", ""], ["Wastell", "Chris", ""]]}, {"id": "1602.01376", "submitter": "William March", "authors": "Chenhan D. Yu, William B. March, Bo Xiao, and George Biros", "title": "Inv-ASKIT: A Parallel Fast Diret Solver for Kernel Matrices", "comments": "11 pages, 2 figures, to appear in IPDPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm for computing the approximate factorization\nof an $N$-by-$N$ kernel matrix. Once this factorization has been constructed\n(with $N \\log^2 N $ work), we can solve linear systems with this matrix with $N\n\\log N $ work. Kernel matrices represent pairwise interactions of points in\nmetric spaces. They appear in machine learning, approximation theory, and\ncomputational physics. Kernel matrices are typically dense (matrix\nmultiplication scales quadratically with $N$) and ill-conditioned (solves can\nrequire 100s of Krylov iterations). Thus, fast algorithms for matrix\nmultiplication and factorization are critical for scalability.\n  Recently we introduced ASKIT, a new method for approximating a kernel matrix\nthat resembles N-body methods. Here we introduce INV-ASKIT, a factorization\nscheme based on ASKIT. We describe the new method, derive complexity estimates,\nand conduct an empirical study of its accuracy and scalability. We report\nresults on real-world datasets including \"COVTYPE\" ($0.5$M points in 54\ndimensions), \"SUSY\" ($4.5$M points in 8 dimensions) and \"MNIST\" (2M points in\n784 dimensions) using shared and distributed memory parallelism. In our largest\nrun we approximately factorize a dense matrix of size 32M $\\times$ 32M\n(generated from points in 64 dimensions) on 4,096 Sandy-Bridge cores. To our\nknowledge these results improve the state of the art by several orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 17:23:24 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Yu", "Chenhan D.", ""], ["March", "William B.", ""], ["Xiao", "Bo", ""], ["Biros", "George", ""]]}, {"id": "1602.01560", "submitter": "Rahul Vaze", "authors": "Aditya Deshmukh and Rahul Vaze", "title": "Online energy efficient packet scheduling for a common deadline with and\n  without energy harvesting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of online packet scheduling to minimize the required conventional\ngrid energy for transmitting a fixed number of packets given a common deadline\nis considered. The total number of packets arriving within the deadline is\nknown, but the packet arrival times are unknown, and can be arbitrary. The\nproposed algorithm tries to finish the transmission of each packet assuming all\nfuture packets are going to arrive at equal time intervals within the left-over\ntime. The proposed online algorithm is shown to have competitive ratio that is\nlogarithmic in the number of packet arrivals. The hybrid energy paradigm is\nalso considered, where in addition to grid energy, energy is also available via\nextraction from renewable sources. The objective here is to minimize the grid\nenergy use. A suitably modified version of the previous algorithm is also shown\nto have competitive ratio that is logarithmic in the number of packet arrivals.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 05:24:01 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Deshmukh", "Aditya", ""], ["Vaze", "Rahul", ""]]}, {"id": "1602.01659", "submitter": "Darren Strash", "authors": "Jakob Dahlum, Sebastian Lamm, Peter Sanders, Christian Schulz, Darren\n  Strash and Renato F. Werneck", "title": "Accelerating Local Search for the Maximum Independent Set Problem", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing high-quality independent sets quickly is an important problem in\ncombinatorial optimization. Several recent algorithms have shown that\nkernelization techniques can be used to find exact maximum independent sets in\nmedium-sized sparse graphs, as well as high-quality independent sets in huge\nsparse graphs that are intractable for exact (exponential-time) algorithms.\nHowever, a major drawback of these algorithms is that they require significant\npreprocessing overhead, and therefore cannot be used to find a high-quality\nindependent set quickly.\n  In this paper, we show that performing simple kernelization techniques in an\nonline fashion significantly boosts the performance of local search, and is\nmuch faster than pre-computing a kernel using advanced techniques. In addition,\nwe show that cutting high-degree vertices can boost local search performance\neven further, especially on huge (sparse) complex networks. Our experiments\nshow that we can drastically speed up the computation of large independent sets\ncompared to other state-of-the-art algorithms, while also producing results\nthat are very close to the best known solutions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 12:42:59 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Dahlum", "Jakob", ""], ["Lamm", "Sebastian", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["Werneck", "Renato F.", ""]]}, {"id": "1602.01764", "submitter": "Olivier Spanjaard", "authors": "Hugo Gilbert, Olivier Spanjaard", "title": "A double oracle approach for minmax regret optimization problems with\n  interval data", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a generic anytime lower bounding procedure for\nminmax regret optimization problems. We show that the lower bound obtained is\nalways at least as accurate as the lower bound recently proposed by Chassein\nand Goerigk (2015). This lower bound can be viewed as the optimal value of a\nlinear programming relaxation of a mixed integer programming formulation of\nminmax regret optimization, but the contribution of the paper is to compute\nthis lower bound via a double oracle algorithm (McMahan et al., 2003) that we\nspecify. The double oracle algorithm is designed by relying on a game theoretic\nview of robust optimization, similar to the one developed by Mastin et al.\n(2015), and it can be efficiently implemented for any minmax regret\noptimization problem whose standard version is \"easy\". We describe how to\nefficiently embed this lower bound in a branch and bound procedure. Finally, we\napply our approach to the robust shortest path problem. Our numerical results\nshow a significant gain in the computation times compared to previous\napproaches in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 17:57:30 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 14:14:51 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 13:37:37 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Gilbert", "Hugo", ""], ["Spanjaard", "Olivier", ""]]}, {"id": "1602.01777", "submitter": "Moritz Baum", "authors": "Moritz Baum, Thomas Bl\\\"asius, Andreas Gemsa, Ignaz Rutter, Franziska\n  Wegner", "title": "Scalable Isocontour Visualization in Road Networks via Minimum-Link\n  Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isocontours in road networks represent the area that is reachable from a\nsource within a given resource limit. We study the problem of computing\naccurate isocontours in realistic, large-scale networks. We propose polygons\nwith minimum number of segments that separate reachable and unreachable\ncomponents of the network. Since the resulting problem is not known to be\nsolvable in polynomial time, we introduce several heuristics that are simple\nenough to be implemented in practice. A key ingredient is a new practical\nlinear-time algorithm for minimum-link paths in simple polygons. Experiments in\na challenging realistic setting show excellent performance of our algorithms in\npractice, answering queries in a few milliseconds on average even for long\nranges.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 18:17:44 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Baum", "Moritz", ""], ["Bl\u00e4sius", "Thomas", ""], ["Gemsa", "Andreas", ""], ["Rutter", "Ignaz", ""], ["Wegner", "Franziska", ""]]}, {"id": "1602.01819", "submitter": "Jesper Nederlof", "authors": "Jesper Nederlof", "title": "A short note on Merlin-Arthur protocols for subset sum", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the subset sum problem we are given n positive integers along with a\ntarget integer t. A solution is a subset of these integers summing to t. In\nthis short note we show that for a given subset sum instance there is a proof\nof size $O^*(\\sqrt{t})$ of what the number of solutions is that can be\nconstructed in $O^*(t)$ time and can be probabilistically verified in time\n$O^*(\\sqrt{t})$ with at most constant error probability. Here, the $O^*()$\nnotation omits factors polynomial in the input size $n\\log(t)$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 20:32:51 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Nederlof", "Jesper", ""]]}, {"id": "1602.01867", "submitter": "Hanna Furmanczyk", "authors": "H. Furmanczyk, M. Kubale", "title": "Scheduling of unit-length jobs with bipartite incompatibility graphs on\n  four uniform machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we consider the problem of scheduling $n$ identical jobs on 4\nuniform machines with speeds $s_1 \\geq s_2 \\geq s_3 \\geq s_4,$ respectively.\nOur aim is to find a schedule with a minimum possible length. We assume that\njobs are subject to some kind of mutual exclusion constraints modeled by a\nbipartite incompatibility graph of degree $\\Delta$, where two incompatible jobs\ncannot be processed on the same machine. We show that the problem is NP-hard\neven if $s_1=s_2=s_3$. If, however, $\\Delta \\leq 4$ and $s_1 \\geq 12 s_2$,\n$s_2=s_3=s_4$, then the problem can be solved to optimality in time\n$O(n^{1.5})$. The same algorithm returns a solution of value at most 2 times\noptimal provided that $s_1 \\geq 2s_2$. Finally, we study the case $s_1 \\geq s_2\n\\geq s_3=s_4$ and give an $O(n^{1.5})$-time $32/15$-approximation algorithm in\nall such situations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 22:10:25 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Furmanczyk", "H.", ""], ["Kubale", "M.", ""]]}, {"id": "1602.01956", "submitter": "David Manlove", "authors": "David F. Manlove, Iain McBride and James Trimble", "title": "\"Almost-stable\" matchings in the Hospitals / Residents problem with\n  Couples", "comments": "A shortened version of this paper will appear at CP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hospitals / Residents problem with Couples (HRC) models the allocation of\nintending junior doctors to hospitals where couples are allowed to submit joint\npreference lists over pairs of (typically geographically close) hospitals. It\nis known that a stable matching need not exist, so we consider MIN BP HRC, the\nproblem of finding a matching that admits the minimum number of blocking pairs\n(i.e., is \"as stable as possible\"). We show that this problem is NP-hard and\ndifficult to approximate even in the highly restricted case that each couple\nfinds only one hospital pair acceptable. However if we further assume that the\npreference list of each single resident and hospital is of length at most 2, we\ngive a polynomial-time algorithm for this case. We then present the first\nInteger Programming (IP) and Constraint Programming (CP) models for MIN BP HRC.\nFinally, we discuss an empirical evaluation of these models applied to\nrandomly-generated instances of MIN BP HRC. We find that on average, the CP\nmodel is about 1.15 times faster than the IP model, and when presolving is\napplied to the CP model, it is on average 8.14 times faster. We further observe\nthat the number of blocking pairs admitted by a solution is very small, i.e.,\nusually at most 1, and never more than 2, for the (28,000) instances\nconsidered.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 08:56:39 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 08:34:36 GMT"}, {"version": "v3", "created": "Sat, 18 Jun 2016 18:55:01 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Manlove", "David F.", ""], ["McBride", "Iain", ""], ["Trimble", "James", ""]]}, {"id": "1602.02018", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay, Gilles Puy, Remi Gribonval, Pierre Vandergheynst", "title": "Compressive Spectral Clustering", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has become a popular technique due to its high\nperformance in many contexts. It comprises three main steps: create a\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\nof its Laplacian matrix to define a feature vector for each object, and run\nk-means on these features to separate objects into k classes. Each of these\nthree steps becomes computationally intensive for large N and/or k. We propose\nto speed up the last two steps based on recent results in the emerging field of\ngraph signal processing: graph filtering of random signals, and random sampling\nof bandlimited graph signals. We prove that our method, with a gain in\ncomputation time that can reach several orders of magnitude, is in fact an\napproximation of spectral clustering, for which we are able to control the\nerror. We test the performance of our method on artificial and real-world\nnetwork data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 13:42:27 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 13:21:56 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Puy", "Gilles", ""], ["Gribonval", "Remi", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1602.02120", "submitter": "Yihan Sun", "authors": "Guy Blelloch and Daniel Ferizovic and Yihan Sun", "title": "Parallel Ordered Sets Using Join", "comments": null, "journal-ref": null, "doi": "10.1145/2935764.2935768", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ordered set is one of the most important data type in both theoretical\nalgorithm design and analysis and practical programming. In this paper we study\nthe set operations on two ordered sets, including Union, Intersect and\nDifference, based on four types of balanced Binary Search Trees (BST) including\nAVL trees, red-black trees, weight balanced trees and treaps. We introduced\nonly one subroutine Join that needs to be implemented differently for each\nbalanced BST, and on top of which we can implement generic, simple and\nefficient parallel functions for ordered sets. We first prove the\nwork-efficiency of these Join-based set functions using a generic proof working\nfor all the four types of balanced BSTs.\n  We also implemented and tested our algorithm on all the four balancing\nschemes. Interestingly the implementations on all four data structures and\nthree set functions perform similarly in time and speedup (more than 45x on 64\ncores). We also compare the performance of our implementation to other existing\nlibraries and algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:03:08 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 19:00:04 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 05:05:19 GMT"}, {"version": "v4", "created": "Sat, 12 Nov 2016 21:46:28 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Blelloch", "Guy", ""], ["Ferizovic", "Daniel", ""], ["Sun", "Yihan", ""]]}, {"id": "1602.02262", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang, Andrej Risteski", "title": "Recovery guarantee of weighted low-rank approximation via alternating\n  minimization", "comments": "40 pages. Updated with the ICML 2016 camera ready version, together\n  with an additional algorithm which needs less assumptions in Appendix C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require recovering a ground truth low-rank matrix from\nnoisy observations of the entries, which in practice is typically formulated as\na weighted low-rank approximation problem and solved by non-convex optimization\nheuristics such as alternating minimization. In this paper, we provide provable\nrecovery guarantee of weighted low-rank via a simple alternating minimization\nalgorithm. In particular, for a natural class of matrices and weights and\nwithout any assumption on the noise, we bound the spectral norm of the\ndifference between the recovered matrix and the ground truth, by the spectral\nnorm of the weighted noise plus an additive error that decreases exponentially\nwith the number of rounds of alternating minimization, from either\ninitialization by SVD or, more importantly, random initialization. These\nprovide the first theoretical results for weighted low-rank via alternating\nminimization with non-binary deterministic weights, significantly generalizing\nthose for matrix completion, the special case with binary weights, since our\nassumptions are similar or weaker than those made in existing works.\nFurthermore, this is achieved by a very simple algorithm that improves the\nvanilla alternating minimization with a simple clipping step.\n  The key technical challenge is that under non-binary deterministic weights,\nna\\\"ive alternating steps will destroy the incoherence and spectral properties\nof the intermediate solutions, which are needed for making progress towards the\nground truth. We show that the properties only need to hold in an average sense\nand can be achieved by the clipping step.\n  We further provide an alternating algorithm that uses a whitening step that\nkeeps the properties via SDP and Rademacher rounding and thus requires weaker\nassumptions. This technique can potentially be applied in some other\napplications and is of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 14:55:12 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 17:05:41 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1602.02293", "submitter": "Ofer Neiman", "authors": "Michael Elkin, Ofer Neiman", "title": "On Efficient Distributed Construction of Near Optimal Routing Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a distributed network represented by a weighted undirected graph\n$G=(V,E)$ on $n$ vertices, and a parameter $k$, we devise a distributed\nalgorithm that computes a routing scheme in $(n^{1/2+1/k}+D)\\cdot n^{o(1)}$\nrounds, where $D$ is the hop-diameter of the network. The running time matches\nthe lower bound of $\\tilde{\\Omega}(n^{1/2}+D)$ rounds (which holds for any\nscheme with polynomial stretch), up to lower order terms. The routing tables\nare of size $\\tilde{O}(n^{1/k})$, the labels are of size $O(k\\log^2n)$, and\nevery packet is routed on a path suffering stretch at most $4k-5+o(1)$. Our\nconstruction nearly matches the state-of-the-art for routing schemes built in a\ncentralized sequential manner. The previous best algorithms for building\nrouting tables in a distributed small messages model were by \\cite[STOC\n2013]{LP13} and \\cite[PODC 2015]{LP15}. The former has similar properties but\nsuffers from substantially larger routing tables of size $O(n^{1/2+1/k})$,\nwhile the latter has sub-optimal running time of\n$\\tilde{O}(\\min\\{(nD)^{1/2}\\cdot n^{1/k},n^{2/3+2/(3k)}+D\\})$.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 19:34:26 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 09:26:22 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1602.02362", "submitter": "Igor Sergeev S.", "authors": "Igor S. Sergeev", "title": "On the circuit complexity of the standard and the Karatsuba methods of\n  multiplying integers", "comments": "6 pages, published in Russian in Proc. XXII Conf. \"Information means\n  and technology\" (Moscow, November 18--20, 2014). Vol. 3. Moscow, MPEI, 2014,\n  180--187", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide accurate upper bounds on the Boolean circuit complexity of the\nstandard and the Karatsuba methods of integer multiplication\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 11:54:08 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1602.02377", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "Find an Optimal Path in Static System and Dynamical System within\n  Polynomial Runtime", "comments": "27 pages, 9720 words,10 figures,5 trials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM cs.RO math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an ancient problem that in a static or dynamical system, sought an\noptimal path, which the context always means within an extremal condition. In\nfact, through those discussions about this theme, we established a universal\nessential calculated model to serve for these complex systems. Meanwhile we\nutilize the sample space to character the system. These contents in this paper\nwould involve in several major areas including the geometry, probability, graph\nalgorithms and some prior approaches, which stands the ultimately subtle linear\nalgorithm to solve this class problem. Along with our progress, our discussion\nwould demonstrate more general meaning and robust character, which provides\nclear ideas or notion to support our concrete applications, who work in a more\npopular complex system.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 14:50:45 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1602.02610", "submitter": "Petr Golovach", "authors": "R\\'emy Belmonte, Fedor V. Fomin, Petr A. Golovach, and M. S. Ramanujan", "title": "Metric Dimension of Bounded Tree-length Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of resolving sets in a graph was introduced by Slater (1975) and\nHarary and Melter (1976) as a way of uniquely identifying every vertex in a\ngraph. A set of vertices in a graph is a resolving set if for any pair of\nvertices x and y there is a vertex in the set which has distinct distances to x\nand y. A smallest resolving set in a graph is called a metric basis and its\nsize, the metric dimension of the graph. The problem of computing the metric\ndimension of a graph is a well-known NP-hard problem and while it was known to\nbe polynomial time solvable on trees, it is only recently that efforts have\nbeen made to understand its computational complexity on various restricted\ngraph classes. In recent work, Foucaud et al. (2015) showed that this problem\nis NP-complete even on interval graphs. They complemented this result by also\nshowing that it is fixed-parameter tractable (FPT) parameterized by the metric\ndimension of the graph. In this work, we show that this FPT result can in fact\nbe extended to all graphs of bounded tree-length. This includes well-known\nclasses like chordal graphs, AT-free graphs and permutation graphs. We also\nshow that this problem is FPT parameterized by the modular-width of the input\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 15:40:03 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Ramanujan", "M. S.", ""]]}, {"id": "1602.02620", "submitter": "Ninh Pham", "authors": "Ninh Pham, Rasmus Pagh", "title": "Scalability and Total Recall with Fast CoveringLSH", "comments": "Short version appears in Proceedings of CIKM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave \\emph{false negatives}, i.e., the recall is less than 100\\%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical \"CoveringLSH\" construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called \\emph{Fast CoveringLSH (fcLSH)}. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from $\\mathcal{O}(dL)$ to\n$\\mathcal{O}(d + L\\log{L})$, where $d$ is the dimensionality of data and $L$ is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that \\emph{fcLSH} is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 16:03:11 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 10:46:19 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Pham", "Ninh", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1602.02670", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Krishnendu Chatterjee and Wolfgang Dvo\\v{r}\\'ak and Monika Henzinger\n  and Veronika Loitzenbauer", "title": "Model and Objective Separation with Conditional Lower Bounds:\n  Disjunction is Harder than Conjunction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a model of a system and an objective, the model-checking question asks\nwhether the model satisfies the objective. We study polynomial-time problems in\ntwo classical models, graphs and Markov Decision Processes (MDPs), with respect\nto several fundamental $\\omega$-regular objectives, e.g., Rabin and Streett\nobjectives. For many of these problems the best-known upper bounds are\nquadratic or cubic, yet no super-linear lower bounds are known. In this work\nour contributions are two-fold: First, we present several improved algorithms,\nand second, we present the first conditional super-linear lower bounds based on\nwidely believed assumptions about the complexity of CNF-SAT and combinatorial\nBoolean matrix multiplication. A separation result for two models with respect\nto an objective means a conditional lower bound for one model that is strictly\nhigher than the existing upper bound for the other model, and similarly for two\nobjectives with respect to a model. Our results establish the following\nseparation results: (1) A separation of models (graphs and MDPs) for\ndisjunctive queries of reachability and B\\\"uchi objectives. (2) Two kinds of\nseparations of objectives, both for graphs and MDPs, namely, (2a) the\nseparation of dual objectives such as reachability/safety (for disjunctive\nquestions) and Streett/Rabin objectives, and (2b) the separation of conjunction\nand disjunction of multiple objectives of the same type such as safety,\nB\\\"uchi, and coB\\\"uchi. In summary, our results establish the first model and\nobjective separation results for graphs and MDPs for various classical\n$\\omega$-regular objectives. Quite strikingly, we establish conditional lower\nbounds for the disjunction of objectives that are strictly higher than the\nexisting upper bounds for the conjunction of the same objectives.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:51:52 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1602.02739", "submitter": "Megan Owen", "authors": "Maria Anaya, Olga Anipchenko-Ulaj, Aisha Ashfaq, Joyce Chiu, Mahedi\n  Kaiser, Max Shoji Ohsawa, Megan Owen, Ella Pavlechko, Katherine St. John,\n  Shivam Suleria, Keith Thompson, Corrine Yap", "title": "On Determining if Tree-based Networks Contain Fixed Trees", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an open question of Francis and Steel about phylogenetic networks\nand trees. They give a polynomial time algorithm to decide if a phylogenetic\nnetwork, N, is tree-based and pose the problem: given a fixed tree T and\nnetwork N, is N based on T? We show that it is NP-hard to decide, by reduction\nfrom 3-Dimensional Matching (3DM), and further, that the problem is fixed\nparameter tractable.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 20:47:31 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Anaya", "Maria", ""], ["Anipchenko-Ulaj", "Olga", ""], ["Ashfaq", "Aisha", ""], ["Chiu", "Joyce", ""], ["Kaiser", "Mahedi", ""], ["Ohsawa", "Max Shoji", ""], ["Owen", "Megan", ""], ["Pavlechko", "Ella", ""], ["John", "Katherine St.", ""], ["Suleria", "Shivam", ""], ["Thompson", "Keith", ""], ["Yap", "Corrine", ""]]}, {"id": "1602.02747", "submitter": "Endre Cs\\'oka", "authors": "Endre Cs\\'oka", "title": "Independent sets and cuts in large-girth regular graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a local algorithm producing an independent set of expected size\n$0.44533n$ on large-girth 3-regular graphs and $0.40407n$ on large-girth\n4-regular graphs. We also construct a cut (or bisection or bipartite subgraph)\nwith $1.34105n$ edges on large-girth 3-regular graphs. These decrease the gaps\nbetween the best known upper and lower bounds from $0.0178$ to $0.01$, from\n$0.0242$ to $0.0123$ and from $0.0724$ to $0.0616$, respectively. We are using\nlocal algorithms, therefore, the method also provides upper bounds for the\nfractional coloring numbers of $1 / 0.44533 \\approx 2.24554$ and $1 / 0.40407\n\\approx 2.4748$ and fractional edge coloring number $1.5 / 1.34105 \\approx\n1.1185$. Our algorithms are applications of the technique introduced by Hoppen\nand Wormald.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 00:40:42 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Cs\u00f3ka", "Endre", ""]]}, {"id": "1602.02841", "submitter": "Max Alekseyev", "authors": "Nikita Alexeev and Max A. Alekseyev", "title": "Combinatorial Scoring of Phylogenetic Networks", "comments": "12 pages; 3 figures", "journal-ref": "Lecture Notes in Computer Science 9797 (2016), 560-572", "doi": "10.1007/978-3-319-42634-1_45", "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Construction of phylogenetic trees and networks for extant species from their\ncharacters represents one of the key problems in phylogenomics. While solution\nto this problem is not always uniquely defined and there exist multiple methods\nfor tree/network construction, it becomes important to measure how well the\nconstructed networks capture the given character relationship across the\nspecies.\n  In the current study, we propose a novel method for measuring the specificity\nof a given phylogenetic network in terms of the total number of distributions\nof character states at the leaves that the network may impose. While for binary\nphylogenetic trees, this number has an exact formula and depends only on the\nnumber of leaves and character states but not on the tree topology, the\nsituation is much more complicated for non-binary trees or networks.\nNevertheless, we develop an algorithm for combinatorial enumeration of such\ndistributions, which is applicable for arbitrary trees and networks under some\nreasonable assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 02:30:12 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 21:29:15 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Alexeev", "Nikita", ""], ["Alekseyev", "Max A.", ""]]}, {"id": "1602.02991", "submitter": "Saeed Akhoondian Amiri", "authors": "Saeed Akhoondian Amiri, Stefan Schmid, Sebastian Siebertz", "title": "A local constant factor approximation for the minimum dominating set\n  problem on bounded genus graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Dominating Set (MDS) problem is not only one of the most\nfundamental problems in distributed computing, it is also one of the most\nchallenging ones. While it is well-known that minimum dominating sets cannot be\napproximated locally on general graphs, over the last years, several\nbreakthroughs have been made on computing local approximations on sparse\ngraphs.\n  This paper presents a deterministic and local constant factor approximation\nfor minimum dominating sets on bounded genus graphs, a very large family of\nsparse graphs. Our main technical contribution is a new analysis of a slightly\nmodified, first-order definable variant of an existing algorithm by Lenzen et\nal. Interestingly, unlike existing proofs for planar graphs, our analysis does\nnot rely on any topological arguments. We believe that our techniques can be\nuseful for the study of local problems on sparse graphs beyond the scope of\nthis paper.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:17:55 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 09:11:16 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Schmid", "Stefan", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1602.03086", "submitter": "Sebastian Deorowicz", "authors": "Maciej Dlugosz and Sebastian Deorowicz", "title": "RECKONER: Read Error Corrector Based on KMC", "comments": "7 pages + 24 pages of supplementary material", "journal-ref": null, "doi": "10.1093/bioinformatics/btw746", "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Next-generation sequencing tools have enabled producing of huge\namount of genomic information at low cost. Unfortunately, presence of\nsequencing errors in such data affects quality of downstream analyzes. Accuracy\nof them can be improved by performing error correction. Because of huge amount\nof such data correction algorithms have to: be fast, memory-frugal, and provide\nhigh accuracy of error detection and elimination for variously-sized organisms.\n  Results: We introduce a new algorithm for genomic data correction, capable of\nprocessing eucaryotic 300 Mbp-genome-size, high error-rated data using less\nthan 4 GB of RAM in less than 40 minutes on 16-core CPU. The algorithm allows\nto correct sequencing data at better or comparable level than competitors. This\nwas achieved by using very robust KMC~2 $k$-mer counter, new method of\nerroneous regions correction based on both $k$-mer counts and FASTQ quality\nindicators as well as careful optimization. Availability: Program is freely\navailable at http://sun.aei.posl.pl/REFRESH/reckoner. Contact:\nsebastian.deorowicz@polsl.pl\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 17:27:41 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Dlugosz", "Maciej", ""], ["Deorowicz", "Sebastian", ""]]}, {"id": "1602.03105", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Hung Bui, Mohammad Ghavamzadeh, Georgios\n  Theocharous, S. Muthukrishnan, and Siqi Sun", "title": "Graphical Model Sketch", "comments": "Proceedings of the European Conference on Machine Learning and\n  Knowledge Discovery in Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured high-cardinality data arises in many domains, and poses a major\nchallenge for both modeling and inference. Graphical models are a popular\napproach to modeling structured data but they are unsuitable for\nhigh-cardinality variables. The count-min (CM) sketch is a popular approach to\nestimating probabilities in high-cardinality data but it does not scale well\nbeyond a few variables. In this work, we bring together the ideas of graphical\nmodels and count sketches; and propose and analyze several approaches to\nestimating probabilities in structured high-cardinality streams of data. The\nkey idea of our approximations is to use the structure of a graphical model and\napproximately estimate its factors by \"sketches\", which hash high-cardinality\nvariables using random projections. Our approximations are computationally\nefficient and their space complexity is independent of the cardinality of\nvariables. Our error bounds are multiplicative and significantly improve upon\nthose of the CM sketch, a state-of-the-art approach to estimating probabilities\nin streams. We evaluate our approximations on synthetic and real-world\nproblems, and report an order of magnitude improvements over the CM sketch.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 18:07:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 05:48:03 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kveton", "Branislav", ""], ["Bui", "Hung", ""], ["Ghavamzadeh", "Mohammad", ""], ["Theocharous", "Georgios", ""], ["Muthukrishnan", "S.", ""], ["Sun", "Siqi", ""]]}, {"id": "1602.03247", "submitter": "Carlos Pi\\~na-Garc\\'ia", "authors": "C.A. Pi\\~na-Garc\\'ia, Dongbing Gu, J. Mario Siqueiros-Garc\\'ia,\n  Gustavo Carre\\'on and Carlos Gershenson", "title": "Exploring Dynamic Environments Using Stochastic Search Strategies", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we conduct a literature review of laws of motion based on\nstochastic search strategies which are mainly focused on exploring highly\ndynamic environments. In this regard, stochastic search strategies represent an\ninteresting alternative to cope with uncertainty and reduced perceptual\ncapabilities. This study aims to present an introductory overview of research\nin terms of directional rules and searching methods mainly based on\nbio-inspired approaches. This study critically examines the role of animal\nsearching behavior applied to random walk models using stochastic rules and\nkinesis or taxis. The aim of this study is to examine existing techniques and\nto select relevant work on random walks and analyze their actual contributions.\nIn this regard, we cover a wide range of displacement events with an\norientation mechanism given by a reactive behavior or a source-seeking\nbehavior. Finally, we conclude with a discussion concerning the usefulness of\nusing optimal foraging strategies as a reliable methodology.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 02:46:39 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Pi\u00f1a-Garc\u00eda", "C. A.", ""], ["Gu", "Dongbing", ""], ["Siqueiros-Garc\u00eda", "J. Mario", ""], ["Carre\u00f3n", "Gustavo", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1602.03320", "submitter": "Arlei Lopes Da Silva", "authors": "Arlei Silva, Xuan-Hong Dang, Prithwish Basu, Ambuj K Singh, Ananthram\n  Swami", "title": "Graph Wavelets via Sparse Cuts: Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling information that resides on vertices of large graphs is a key\nproblem in several real-life applications, ranging from social networks to the\nInternet-of-things. Signal Processing on Graphs and, in particular, graph\nwavelets can exploit the intrinsic smoothness of these datasets in order to\nrepresent them in a both compact and accurate manner. However, how to discover\nwavelet bases that capture the geometry of the data with respect to the signal\nas well as the graph structure remains an open question. In this paper, we\nstudy the problem of computing graph wavelet bases via sparse cuts in order to\nproduce low-dimensional encodings of data-driven bases. This problem is\nconnected to known hard problems in graph theory (e.g. multiway cuts) and thus\nrequires an efficient heuristic. We formulate the basis discovery task as a\nrelaxation of a vector optimization problem, which leads to an elegant solution\nas a regularized eigenvalue computation. Moreover, we propose several\nstrategies in order to scale our algorithm to large graphs. Experimental\nresults show that the proposed algorithm can effectively encode both the graph\nstructure and signal, producing compressed and accurate representations for\nvertex values in a wide range of datasets (e.g. sensor and gene networks) and\nsignificantly outperforming the best baseline.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 10:34:41 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 04:21:13 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 07:08:36 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2016 01:01:45 GMT"}, {"version": "v5", "created": "Mon, 13 Jun 2016 02:31:07 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Silva", "Arlei", ""], ["Dang", "Xuan-Hong", ""], ["Basu", "Prithwish", ""], ["Singh", "Ambuj K", ""], ["Swami", "Ananthram", ""]]}, {"id": "1602.03333", "submitter": "Timo Beller", "authors": "Timo Beller and Enno Ohlebusch", "title": "A representation of a compressed de Bruijn graph for pan-genome analysis\n  that enables search", "comments": "Submitted to Algorithmica special issue of CPM2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Marcus et al. (Bioinformatics 2014) proposed to use a compressed de\nBruijn graph to describe the relationship between the genomes of many\nindividuals/strains of the same or closely related species. They devised an\n$O(n \\log g)$ time algorithm called splitMEM that constructs this graph\ndirectly (i.e., without using the uncompressed de Bruijn graph) based on a\nsuffix tree, where $n$ is the total length of the genomes and $g$ is the length\nof the longest genome. In this paper, we present a construction algorithm that\noutperforms their algorithm in theory and in practice. Moreover, we propose a\nnew space-efficient representation of the compressed de Bruijn graph that adds\nthe possibility to search for a pattern (e.g. an allele - a variant form of a\ngene) within the pan-genome.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 11:41:06 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Beller", "Timo", ""], ["Ohlebusch", "Enno", ""]]}, {"id": "1602.03713", "submitter": "Keren Censor-Hillel", "authors": "Reuven Bar-Yehuda and Keren Censor-Hillel and Gregory Schwartzman", "title": "A Distributed $(2+\\epsilon)$-Approximation for Vertex Cover in\n  $O(\\log{\\Delta}/\\epsilon\\log\\log{\\Delta})$ Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple deterministic distributed $(2+\\epsilon)$-approximation\nalgorithm for minimum weight vertex cover, which completes in\n$O(\\log{\\Delta}/\\epsilon\\log\\log{\\Delta})$ rounds, where $\\Delta$ is the\nmaximum degree in the graph, for any $\\epsilon>0$ which is at most $O(1)$. For\na constant $\\epsilon$, this implies a constant approximation in\n$O(\\log{\\Delta}/\\log\\log{\\Delta})$ rounds, which contradicts the lower bound of\n[KMW10].\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 13:00:04 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 07:51:02 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Bar-Yehuda", "Reuven", ""], ["Censor-Hillel", "Keren", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1602.03718", "submitter": "Keren Censor-Hillel", "authors": "Keren Censor-Hillel and Eldar Fischer and Gregory Schwartzman and Yadu\n  Vasudev", "title": "Fast Distributed Algorithms for Testing Graph Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a thorough study of \\emph{distributed property testing} --\nproducing algorithms for the approximation problems of property testing in the\nCONGEST model. In particular, for the so-called \\emph{dense} testing model we\nemulate sequential tests for nearly all graph properties having $1$-sided\ntests, while in the \\emph{sparse} and \\emph{general} models we obtain faster\ntests for triangle-freeness and bipartiteness respectively.\n  In most cases, aided by parallelism, the distributed algorithms have a much\nshorter running time as compared to their counterparts from the sequential\nquerying model of traditional property testing. The simplest property testing\nalgorithms allow a relatively smooth transitioning to the distributed model.\nFor the more complex tasks we develop new machinery that is of independent\ninterest. This includes a method for distributed maintenance of multiple random\nwalks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 13:20:06 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 21:21:15 GMT"}, {"version": "v3", "created": "Mon, 2 May 2016 12:47:13 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Fischer", "Eldar", ""], ["Schwartzman", "Gregory", ""], ["Vasudev", "Yadu", ""]]}, {"id": "1602.03881", "submitter": "Yan Gu", "authors": "Guy E. Blelloch, Yan Gu, Yihan Sun and Kanat Tangwongsan", "title": "Parallel Shortest-Paths Using Radius Stepping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The single-source shortest path problem (SSSP) with nonnegative edge weights\nis a notoriously difficult problem to solve efficiently in parallel---it is one\nof the graph problems said to suffer from the transitive-closure bottleneck. In\npractice, the $\\Delta$-stepping algorithm of Meyer and Sanders (J. Algorithms,\n2003) often works efficiently but has no known theoretical bounds on general\ngraphs. The algorithm takes a sequence of steps, each increasing the radius by\na user-specified value $\\Delta$. Each step settles the vertices in its annulus\nbut can take $\\Theta(n)$ substeps, each requiring $\\Theta(m)$ work ($n$\nvertices and $m$ edges).\n  In this paper, we describe Radius-Stepping, an algorithm with the best-known\ntradeoff between work and depth bounds for SSSP with nearly-linear\n($\\otilde(m)$) work. The algorithm is a $\\Delta$-stepping-like algorithm but\nuses a variable instead of fixed-size increase in radii, allowing us to prove a\nbound on the number of steps. In particular, by using what we define as a\nvertex $k$-radius, each step takes at most $k+2$ substeps. Furthermore, we\ndefine a $(k, \\rho)$-graph property and show that if an undirected graph has\nthis property, then the number of steps can be bounded by $O(\\frac{n}{\\rho}\n\\log \\rho L)$, for a total of $O(\\frac{kn}{\\rho} \\log \\rho L)$ substeps, each\nparallel. We describe how to preprocess a graph to have this property.\nAltogether, Radius-Stepping takes $O((m+n\\log n)\\log \\frac{n}{\\rho})$ work and\n$O(\\frac{n}{\\rho}\\log n \\log (\\rho{}L))$ depth per source after preprocessing.\nThe preprocessing step can be done in $O(m\\log n + n\\rho^2)$ work and\n$O(\\rho^2)$ depth or in $O(m\\log n + n\\rho^2\\log n)$ work and $O(\\rho\\log\n\\rho)$ depth, and adds no more than $O(n\\rho)$ edges.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 20:51:47 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 00:31:26 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Gu", "Yan", ""], ["Sun", "Yihan", ""], ["Tangwongsan", "Kanat", ""]]}, {"id": "1602.03942", "submitter": "Koji Yamamoto", "authors": "Koji Yamamoto and Taka Matsutsuka", "title": "Efficient Call Path Detection for Android-OS Size of Huge Source Code", "comments": "in Sixth International Conference on Computer Science, Engineering\n  and Applications (CCSEA 2016), Dubai, UAE, January 23~24, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today most developers utilize source code written by other parties. Because\nthe code is modified frequently, the developers need to grasp the impact of the\nmodification repeatedly. A call graph and especially its special type, a call\npath, help the developers comprehend the modification. Source code written by\nother parties, however, becomes too huge to be held in memory in the form of\nparsed data for a call graph or path. This paper offers a bidirectional search\nalgorithm for a call graph of too huge amount of source code to store all parse\nresults of the code in memory. It refers to a method definition in source code\ncorresponding to the visited node in the call graph. The significant feature of\nthe algorithm is the referenced information is used not in order to select a\nprioritized node to visit next but in order to select a node to postpone\nvisiting. It reduces path extraction time by 8% for a case in which ordinary\npath search algorithms do not reduce the time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 01:29:08 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Yamamoto", "Koji", ""], ["Matsutsuka", "Taka", ""]]}, {"id": "1602.04031", "submitter": "Daniel Krenn", "authors": "Martin Aum\\\"uller, Martin Dietzfelbinger, Clemens Heuberger, Daniel\n  Krenn, Helmut Prodinger", "title": "Counting Zeros in Random Walks on the Integers and Analysis of Optimal\n  Dual-Pivot Quicksort", "comments": "extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an average case analysis of two variants of dual-pivot quicksort,\none with a non-algorithmic comparison-optimal partitioning strategy, the other\nwith a closely related algorithmic strategy. For both we calculate the expected\nnumber of comparisons exactly as well as asymptotically, in particular, we\nprovide exact expressions for the linear, logarithmic, and constant terms. An\nessential step is the analysis of zeros of lattice paths in a certain\nprobability model. Along the way a combinatorial identity is proven.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 12:34:37 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 13:00:02 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Dietzfelbinger", "Martin", ""], ["Heuberger", "Clemens", ""], ["Krenn", "Daniel", ""], ["Prodinger", "Helmut", ""]]}, {"id": "1602.04095", "submitter": "Pedro Montealegre", "authors": "Pedro Montealegre and Ioan Todinca", "title": "Deterministic graph connectivity in the broadcast congested clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deterministic constant-round protocols for the graph connectivity\nproblem in the model where each of the $n$ nodes of a graph receives a row of\nthe adjacency matrix, and broadcasts a single sublinear size message to all\nother nodes. Communication rounds are synchronous. This model is sometimes\ncalled the broadcast congested clique. Specifically, we exhibit a deterministic\nprotocol that computes the connected components of the input graph in $\\lceil\n1/\\epsilon \\rceil$ rounds, each player communicating $\\mathcal{O}(n^{\\epsilon}\n\\cdot \\log n)$ bits per round, with $0 < \\epsilon \\leq 1$.\n  We also provide a deterministic one-round protocol for connectivity, in the\nmodel when each node receives as input the graph induced by the nodes at\ndistance at most $r>0$, and communicates $\\mathcal{O}(n^{1/r} \\cdot \\log n)$\nbits. This result is based on a $d$-pruning protocol, which consists in\nsuccessively removing nodes of degree at most $d$ until obtaining a graph with\nminimum degree larger than $d$. Our technical novelty is the introduction of\ndeterministic sparse linear sketches: a linear compression function that\npermits to recover sparse Boolean vectors deterministically.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:04:14 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 20:34:56 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 19:37:35 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Montealegre", "Pedro", ""], ["Todinca", "Ioan", ""]]}, {"id": "1602.04181", "submitter": "Soheil Feizi", "authors": "Soheil Feizi, Gerald Quon, Mariana Recamonde-Mendoza, Muriel Medard,\n  Manolis Kellis, Ali Jadbabaie", "title": "Spectral Alignment of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph alignment refers to the problem of finding a bijective mapping across\nvertices of two graphs such that, if two nodes are connected in the first\ngraph, their images are connected in the second graph. This problem arises in\nmany fields such as computational biology, social sciences, and computer vision\nand is often cast as a quadratic assignment problem (QAP). Most standard graph\nalignment methods consider an optimization that maximizes the number of matches\nbetween the two graphs, ignoring the effect of mismatches. We propose a\ngeneralized graph alignment formulation that considers both matches and\nmismatches in a standard QAP formulation. This modification can have a major\nimpact in aligning graphs with different sizes and heterogenous edge densities.\nMoreover, we propose two methods for solving the generalized graph alignment\nproblem based on spectral decomposition of matrices. We compare the performance\nof proposed methods with some existing graph alignment algorithms including\nNatalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite\nprogramming-based method over various synthetic and real graph models. Our\nproposed method based on simultaneous alignment of multiple eigenvectors leads\nto consistently good performance in different graph models. In particular, in\nthe alignment of regular graph structures which is one of the most difficult\ngraph alignment cases, our proposed method significantly outperforms other\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 19:38:33 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 00:35:32 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Feizi", "Soheil", ""], ["Quon", "Gerald", ""], ["Recamonde-Mendoza", "Mariana", ""], ["Medard", "Muriel", ""], ["Kellis", "Manolis", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1602.04270", "submitter": "Bonnie Kirkpatrick", "authors": "Bonnie Kirkpatrick", "title": "Haplotype Inference for Pedigrees with Few Recombinations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedigrees, or family trees, are graphs of family relationships that are used\nto study inheritance. A fundamental problem in computational biology is to\nfind, for a pedigree with $n$ individuals genotyped at every site, a set of\nMendelian-consistent haplotypes that have the minimum number of recombinations.\nThis is an NP-hard problem and some pedigrees can have thousands of individuals\nand hundreds of thousands of sites.\n  This paper formulates this problem as a optimization on a graph and\nintroduces a tailored algorithm with a running time of O(n^{(k+2)}m^{6k}) for n\nindividuals, m sites, and k recombinations. Since there are generally only 1-2\nrecombinations per chromosome in each meiosis, k is small enough to make this\nalgorithm practically relevant.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 00:25:27 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Kirkpatrick", "Bonnie", ""]]}, {"id": "1602.04274", "submitter": "Arman Zaribafiyan", "authors": "Arman Zaribafiyan, Dominic J.J. Marchand, Seyed Saeed Changiz Rezaei", "title": "Systematic and Deterministic Graph-Minor Embedding for Cartesian\n  Products of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limited connectivity of current and next-generation quantum annealers\nmotivates the need for efficient graph-minor embedding methods. These methods\nallow non-native problems to be adapted to the target annealer's architecture.\nThe overhead of the widely used heuristic techniques is quickly proving to be a\nsignificant bottleneck for solving real-world applications. To alleviate this\ndifficulty, we propose a systematic and deterministic embedding method,\nexploiting the structures of both the input graph of the specific problem and\nthe quantum annealer. We focus on the specific case of the Cartesian product of\ntwo complete graphs, a regular structure that occurs in many problems. We\ndivide the embedding problem by first embedding one of the factors of the\nCartesian product in a repeatable pattern. The resulting simplified problem\nconsists of the placement and connecting together of these copies to reach a\nvalid solution. Aside from the obvious advantage of a systematic and\ndeterministic approach with respect to speed and efficiency, the embeddings\nproduced are easily scaled for larger processors and show desirable properties\nfor the number of qubits used and the chain length distribution. To conclude,\nwe briefly address the problem of circumventing inoperable qubits by presenting\npossible extensions of the method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:05:01 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 01:28:40 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Zaribafiyan", "Arman", ""], ["Marchand", "Dominic J. J.", ""], ["Rezaei", "Seyed Saeed Changiz", ""]]}, {"id": "1602.04358", "submitter": "Leonid Gugel", "authors": "Leonid Gugel, Yoel Shkolnisky, Shai Dekel", "title": "Machine olfaction using time scattering of sensor multiresolution graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct a learning architecture for high dimensional time\nseries sampled by sensor arrangements. Using a redundant wavelet decomposition\non a graph constructed over the sensor locations, our algorithm is able to\nconstruct discriminative features that exploit the mutual information between\nthe sensors. The algorithm then applies scattering networks to the time series\ngraphs to create the feature space. We demonstrate our method on a machine\nolfaction problem, where one needs to classify the gas type and the location\nwhere it originates from data sampled by an array of sensors. Our experimental\nresults clearly demonstrate that our method outperforms classical machine\nlearning techniques used in previous studies.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 17:25:03 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Gugel", "Leonid", ""], ["Shkolnisky", "Yoel", ""], ["Dekel", "Shai", ""]]}, {"id": "1602.04365", "submitter": "Christoph D\\\"urr", "authors": "Christoph D\\\"urr, Zden\\v{e}k Hanz\\'alek, Christian Konrad, Yasmina\n  Seddik, Ren\\'e Sitters, \\'Oscar C. V\\'asquez, Gerhard Woeginger", "title": "The triangle scheduling problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel scheduling problem, where jobs occupy a\ntriangular shape on the time line. This problem is motivated by scheduling jobs\nwith different criticality levels. A measure is introduced, namely the binary\ntree ratio. It is shown that the greedy algorithm solves the problem to\noptimality when the binary tree ratio of the input instance is at most 2. We\nalso show that the problem is unary NP-hard for instances with binary tree\nratio strictly larger than 2, and provide a quasi polynomial time approximation\nscheme (QPTAS). The approximation ratio of Greedy on general instances is shown\nto be between 1.5 and 1.05.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 19:05:21 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 19:57:49 GMT"}, {"version": "v3", "created": "Sun, 1 May 2016 12:41:32 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["D\u00fcrr", "Christoph", ""], ["Hanz\u00e1lek", "Zden\u011bk", ""], ["Konrad", "Christian", ""], ["Seddik", "Yasmina", ""], ["Sitters", "Ren\u00e9", ""], ["V\u00e1squez", "\u00d3scar C.", ""], ["Woeginger", "Gerhard", ""]]}, {"id": "1602.04368", "submitter": "Bonnie Kirkpatrick", "authors": "Bonnie Kirkpatrick", "title": "Fast Computation of the Kinship Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For families, kinship coefficients are quantifications of the amount of\ngenetic sharing between a pair of individuals. These coefficients are critical\nfor understanding the breeding habits and genetic diversity of diploid\npopulations. Historically, computations of the inbreeding coefficient were used\nto prohibit inbred marriages and prohibit breeding of some pairs of pedigree\nanimals. Such prohibitions foster genetic diversity and help prevent recessive\nMendelian disease at a population level.\n  This paper gives the fastest known algorithms for computing the kinship\ncoefficient of a set of individuals with a known pedigree. The algorithms given\nhere consider the possibility that the founders of the known pedigree may\nthemselves be inbred, and they compute the appropriate inbreeding-adjusted\nkinship coefficients. The exact kinship algorithm has running-time $O(n^2)$ for\nan $n$-individual pedigree. The recursive-cut exact kinship algorithm has\nrunning time $O(s^2m)$ where $s$ is the number of individuals in the largest\nsegment of the pedigree and $m$ is the number of cuts. The approximate\nalgorithm has running-time $O(n)$ for an $n$-individual pedigree on which to\nestimate the kinship coefficients of $\\sqrt{n}$ individuals from $\\sqrt{n}$\nfounder kinship coefficients.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 19:38:44 GMT"}], "update_date": "2016-02-20", "authors_parsed": [["Kirkpatrick", "Bonnie", ""]]}, {"id": "1602.04399", "submitter": "Pablo P\\'erez-Lantero", "authors": "Gilberto Guti\\'errez, Pablo P\\'erez-Lantero and Claudio Torres", "title": "Linear Separability in Spatial Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two point sets $R$ and $B$ in the plane, with cardinalities $m$ and\n$n$, respectively, and each set stored in a separate R-tree, we present an\nalgorithm to decide whether $R$ and $B$ are linearly separable. Our algorithm\nexploits the structure of the R-trees, loading into the main memory only\nrelevant data, and runs in $O(m\\log m + n\\log n)$ time in the worst case. As\nexperimental results, we implement the proposed algorithm and executed it on\nseveral real and synthetic point sets, showing that the percentage of nodes of\nthe R-trees that are accessed and the memory usage are low in these cases. We\nalso present an algorithm to compute the convex hull of $n$ planar points given\nin an R-tree, running in $O(n\\log n)$ time in the worst case.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 23:12:25 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Guti\u00e9rrez", "Gilberto", ""], ["P\u00e9rez-Lantero", "Pablo", ""], ["Torres", "Claudio", ""]]}, {"id": "1602.04421", "submitter": "Yitong Yin", "authors": "Mingmou Liu, Xiaoyin Pan, Yitong Yin", "title": "Randomized approximate nearest neighbor search with limited adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of approximate nearest neighbor search in\n$d$-dimensional Hamming space $\\{0,1\\}^d$. We study the complexity of the\nproblem in the famous cell-probe model, a classic model for data structures. We\nconsider algorithms in the cell-probe model with limited adaptivity, where the\nalgorithm makes $k$ rounds of parallel accesses to the data structure for a\ngiven $k$. For any $k\\ge 1$, we give a simple randomized algorithm solving the\napproximate nearest neighbor search using $k$ rounds of parallel memory\naccesses, with $O(k(\\log d)^{1/k})$ accesses in total. We also give a more\nsophisticated randomized algorithm using $O(k+(\\frac{1}{k}\\log d)^{O(1/k)})$\nmemory accesses in $k$ rounds for large enough $k$. Both algorithms use data\nstructures of size polynomial in $n$, the number of points in the database.\n  For the lower bound, we prove an $\\Omega(\\frac{1}{k}(\\log d)^{1/k})$ lower\nbound for the total number of memory accesses required by any randomized\nalgorithm solving the approximate nearest neighbor search within\n$k\\le\\frac{\\log\\log d}{2\\log\\log\\log d}$ rounds of parallel memory accesses on\nany data structures of polynomial size. This lower bound shows that our first\nalgorithm is asymptotically optimal for any constant round $k$. And our second\nalgorithm approaches the asymptotically optimal tradeoff between rounds and\nmemory accesses, in a sense that the lower bound of memory accesses for any\n$k_1$ rounds can be matched by the algorithm within $k_2=O(k_1)$ rounds. In the\nextreme, for some large enough $k=\\Theta\\left(\\frac{\\log\\log d}{\\log\\log\\log\nd}\\right)$, our second algorithm matches the $\\Theta\\left(\\frac{\\log\\log\nd}{\\log\\log\\log d}\\right)$ tight bound for fully adaptive algorithms for\napproximate nearest neighbor search due to Chakrabarti and Regev.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 06:36:17 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Liu", "Mingmou", ""], ["Pan", "Xiaoyin", ""], ["Yin", "Yitong", ""]]}, {"id": "1602.04478", "submitter": "Michael Kapralov", "authors": "Venkatesan T. Chakaravarthy, Michael Kapralov, Prakash Murali,\n  Fabrizio Petrini, Xinyu Que, Yogish Sabharwal, Baruch Schieber", "title": "Subgraph Counting: Color Coding Beyond Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counting occurrences of query graphs in a large data graph,\nknown as subgraph counting, is fundamental to several domains such as genomics\nand social network analysis. Many important special cases (e.g. triangle\ncounting) have received significant attention. Color coding is a very general\nand powerful algorithmic technique for subgraph counting. Color coding has been\nshown to be effective in several applications, but scalable implementations are\nonly known for the special case of {\\em tree queries} (i.e. queries of\ntreewidth one).\n  In this paper we present the first efficient distributed implementation for\ncolor coding that goes beyond tree queries: our algorithm applies to any query\ngraph of treewidth $2$. Since tree queries can be solved in time linear in the\nsize of the data graph, our contribution is the first step into the realm of\ncolour coding for queries that require superlinear running time in the worst\ncase. This superlinear complexity leads to significant load balancing problems\non graphs with heavy tailed degree distributions. Our algorithm structures the\ncomputation to work around high degree nodes in the data graph, and achieves\nvery good runtime and scalability on a diverse collection of data and query\ngraph pairs as a result. We also provide theoretical analysis of our\nalgorithmic techniques, showing asymptotic improvements in runtime on random\ngraphs with power law degree distributions, a popular model for real world\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 17:54:25 GMT"}, {"version": "v2", "created": "Sat, 2 Apr 2016 08:53:20 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Chakaravarthy", "Venkatesan T.", ""], ["Kapralov", "Michael", ""], ["Murali", "Prakash", ""], ["Petrini", "Fabrizio", ""], ["Que", "Xinyu", ""], ["Sabharwal", "Yogish", ""], ["Schieber", "Baruch", ""]]}, {"id": "1602.04505", "submitter": "Martin Grohe", "authors": "Martin Grohe", "title": "Quasi-4-Connected Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new decomposition of a graphs into quasi-4-connected\ncomponents, where we call a graph quasi-4-connected if it is 3-connected and it\nonly has separations of order 3 that remove a single vertex. Moreover, we give\na cubic time algorithm computing the decomposition of a given graph.\n  Our decomposition into quasi-4-connected components refines the well-known\ndecompositions of graphs into biconnected and triconnected components. We\nrelate our decomposition to Robertson and Seymour's theory of tangles by\nestablishing a correspondence between the quasi-4-connected components of a\ngraph and its tangles of order 4.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 20:42:24 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Grohe", "Martin", ""]]}, {"id": "1602.04652", "submitter": "Alan Frieze", "authors": "Alan Frieze, Tony Johansson", "title": "On the insertion time of random walk cuckoo hashing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo Hashing is a hashing scheme invented by Pagh and Rodler. It uses\n$d\\geq 2$ distinct hash functions to insert items into the hash table. It has\nbeen an open question for some time as to the expected time for Random Walk\nInsertion to add items. We show that if the number of hash functions $d=O(1)$\nis sufficiently large, then the expected insertion time is $O(1)$ per item.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 12:33:12 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 15:26:08 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 12:58:56 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2016 16:23:07 GMT"}, {"version": "v5", "created": "Sun, 28 Feb 2016 18:03:34 GMT"}, {"version": "v6", "created": "Tue, 15 Mar 2016 16:37:15 GMT"}, {"version": "v7", "created": "Mon, 18 Apr 2016 13:45:05 GMT"}, {"version": "v8", "created": "Wed, 6 Jul 2016 20:12:11 GMT"}, {"version": "v9", "created": "Sun, 8 Jan 2017 15:05:34 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Frieze", "Alan", ""], ["Johansson", "Tony", ""]]}, {"id": "1602.04667", "submitter": "Dominik Kaaser", "authors": "Robert Els\\\"asser, Tom Friedetzky, Dominik Kaaser, Frederik\n  Mallmann-Trenn, Horst Trinker", "title": "Rapid Asynchronous Plurality Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed plurality consensus in a complete graph of size $n$\nwith $k$ initial opinions. We design an efficient and simple protocol in the\nasynchronous communication model that ensures that all nodes eventually agree\non the initially most frequent opinion. In this model, each node is equipped\nwith a random Poisson clock with parameter $\\lambda=1$. Whenever a node's clock\nticks, it samples some neighbors, uniformly at random and with replacement, and\nadjusts its opinion according to the sample.\n  A prominent example is the so-called two-choices algorithm in the synchronous\nmodel, where in each round, every node chooses two neighbors uniformly at\nrandom, and if the two sampled opinions coincide, then that opinion is adopted.\nThis protocol is very efficient and well-studied when $k=2$. If\n$k=O(n^\\varepsilon)$ for some small $\\varepsilon$, we show that it converges to\nthe initial plurality opinion within $O(k \\cdot \\log{n})$ rounds, w.h.p., as\nlong as the initial difference between the largest and second largest opinion\nis $\\Omega(\\sqrt{n \\log n})$. On the other side, we show that there are cases\nin which $\\Omega(k)$ rounds are needed, w.h.p.\n  One can beat this lower bound in the synchronous model by combining the\ntwo-choices protocol with randomized broadcasting. Our main contribution is a\nnon-trivial adaptation of this approach to the asynchronous model. If the\nsupport of the most frequent opinion is at least $(1+\\varepsilon)$ times that\nof the second-most frequent one and $k=O(\\exp(\\log{n}/\\log \\log{n}))$, then our\nprotocol achieves the best possible run time of $O(\\log n)$, w.h.p. We relax\nfull synchronicity by allowing $o(n)$ nodes to be poorly synchronized, and the\nwell synchronized nodes are only required to be within a certain time\ndifference from one another. We enforce this synchronicity by introducing a\nnovel gadget into the protocol.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 13:23:10 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 14:52:05 GMT"}, {"version": "v3", "created": "Sun, 6 Mar 2016 20:53:44 GMT"}, {"version": "v4", "created": "Tue, 31 May 2016 15:49:48 GMT"}, {"version": "v5", "created": "Wed, 22 Feb 2017 16:38:55 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Els\u00e4sser", "Robert", ""], ["Friedetzky", "Tom", ""], ["Kaaser", "Dominik", ""], ["Mallmann-Trenn", "Frederik", ""], ["Trinker", "Horst", ""]]}, {"id": "1602.04781", "submitter": "Mathias Hauptmann", "authors": "Mathias Hauptmann", "title": "On Alternation and the Union Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the assumption $P=\\Sigma_2^p$, we prove a new variant of the Union\nTheorem of McCreight and Meyer for the class $\\Sigma_2^p$. This yields a union\nfunction $F$ which is computable in time $F(n)^c$ for some constant $c$ and\nsatisfies $P=DTIME(F)=\\Sigma_2(F)=\\Sigma_2^p$ with respect to a subfamily\n$(\\tilde{S}_i)$ of $\\Sigma_2$-machines. We show that this subfamily does not\nchange the complexity classes $P$ and $\\Sigma_2^p$. Moreover, a padding\nconstruction shows that this also implies $DTIME(F^c)=\\Sigma_2(F^c)$. On the\nother hand, we prove a variant of Gupta's result who showed that\n$DTIME(t)\\subsetneq\\Sigma_2(t)$ for time-constructible functions $t(n)$. Our\nvariant of this result holds with respect to the subfamily $(\\tilde{S}_i)$ of\n$\\Sigma_2$-machines. We show that these two results contradict each other.\nHence the assumption $P=\\Sigma_2^p$ cannot hold.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 19:56:58 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 14:15:46 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 10:27:37 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Hauptmann", "Mathias", ""]]}, {"id": "1602.04800", "submitter": "Florian Hauer", "authors": "Florian Hauer and Panagiotis Tsiotras", "title": "Reduced Complexity Multi-Scale Path-Planning on Probabilistic Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several modifications to the previously proposed MSPP algorithm\nthat can speed-up its execution considerably. The MSPP algorithm leverages a\nmultiscale representation of the environment in $n$ dimensions. The information\nof the environment is stored in a tree data structure representing a recursive\ndyadic partitioning of the search space. The information used by the algorithm\nis the probability that a node in the tree corresponds to an obstacle in the\nsearch space. Such trees are often created from mainstream perception\nalgorithms, and correspond to quadtrees and octrees in two and three\ndimensions, respectively. We first present a new method to compute the graph\nneighbors in order to reduce the complexity of each iteration, from $O(| V|^2)$\nto $O(| V| \\log |V|)$. We then show how to delay expensive intermediate\ncomputations until we know that new information will be required, hence saving\ntime by not operating on information that is never used during the search.\nFinally, we present a way to remove the very expensive need to calculate a full\nmulti-scale map with the use of sampling and derive an theoretical upperbound\nof the probability of failure as a function of the number of samples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:48:23 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Hauer", "Florian", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "1602.04847", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Yin-Tat Lee", "title": "Black-box optimization with a politician", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for black-box convex optimization which is\nwell-suited for situations where gradient computations are expensive. We derive\na new method for this framework which leverages several concepts from convex\noptimization, from standard first-order methods (e.g. gradient descent or\nquasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant\nbarriers). We demonstrate empirically that our new technique compares favorably\nwith state of the art algorithms (such as BFGS).\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 21:35:58 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin-Tat", ""]]}, {"id": "1602.04876", "submitter": "Filipe Brand\\~ao M.Sc", "authors": "Filipe Brand\\~ao", "title": "VPSolver 3: Multiple-choice Vector Packing Solver", "comments": "8 pages. arXiv admin note: text overlap with arXiv:1310.6887", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VPSolver is a vector packing solver based on an arc-flow formulation with\ngraph compression. In this paper, we present the algorithm introduced in\nVPSolver 3.0.0 for building compressed arc-flow models for the multiple-choice\nvector packing problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 00:53:44 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Brand\u00e3o", "Filipe", ""]]}, {"id": "1602.04995", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Michael Kaufmann, Chrysanthi N. Raftopoulou", "title": "On the Density of non-Simple 3-Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{$k$-planar graph} is a graph that can be drawn in the plane such that\nevery edge is crossed at most $k$ times. For $k \\leq 4$, Pach and T\\'oth proved\na bound of $(k+3)(n-2)$ on the total number of edges of a $k$-planar graph,\nwhich is tight for $k=1,2$. For $k=3$, the bound of $6n-12$ has been improved\nto $\\frac{11}{2}n-11$ and has been shown to be optimal up to an additive\nconstant for simple graphs. In this paper, we prove that the bound of\n$\\frac{11}{2}n-11$ edges also holds for non-simple $3$-planar graphs that admit\ndrawings in which non-homotopic parallel edges and self-loops are allowed.\nBased on this result, a characterization of \\emph{optimal $3$-planar graphs}\n(that is, $3$-planar graphs with $n$ vertices and exactly $\\frac{11}{2}n-11$\nedges) might be possible, as to the best of our knowledge the densest known\nsimple $3$-planar is not known to be optimal.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 11:45:25 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 20:38:40 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2016 07:50:34 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Bekos", "Michael A.", ""], ["Kaufmann", "Michael", ""], ["Raftopoulou", "Chrysanthi N.", ""]]}, {"id": "1602.05016", "submitter": "Alexander Golovnev", "authors": "Marek Cygan, Fedor V. Fomin, Alexander Golovnev, Alexander S. Kulikov,\n  Ivan Mihajlin, Jakub Pachocki, and Arkadiusz Soca{\\l}a", "title": "Tight Lower Bounds on Graph Embedding Problems", "comments": "23 pages. arXiv admin note: substantial text overlap with\n  arXiv:1502.05447, arXiv:1507.03738", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that unless the Exponential Time Hypothesis (ETH) fails, deciding if\nthere is a homomorphism from graph $G$ to graph $H$ cannot be done in time\n$|V(H)|^{o(|V(G)|)}$. We also show an exponential-time reduction from Graph\nHomomorphism to Subgraph Isomorphism. This rules out (subject to ETH) a\npossibility of $|V(H)|^{o(|V(H)|)}$-time algorithm deciding if graph $G$ is a\nsubgraph of $H$. For both problems our lower bounds asymptotically match the\nrunning time of brute-force algorithms trying all possible mappings of one\ngraph into another. Thus, our work closes the gap in the known complexity of\nthese fundamental problems.\n  Moreover, as a consequence of our reductions conditional lower bounds follow\nfor other related problems such as Locally Injective Homomorphism, Graph\nMinors, Topological Graph Minors, Minimum Distortion Embedding and Quadratic\nAssignment Problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 13:37:46 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Cygan", "Marek", ""], ["Fomin", "Fedor V.", ""], ["Golovnev", "Alexander", ""], ["Kulikov", "Alexander S.", ""], ["Mihajlin", "Ivan", ""], ["Pachocki", "Jakub", ""], ["Soca\u0142a", "Arkadiusz", ""]]}, {"id": "1602.05136", "submitter": "David Caissy", "authors": "David Caissy and Andrzej Pelc", "title": "Exploration of Faulty Hamiltonian Graphs", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exploration of networks, some of whose edges are\nfaulty. A mobile agent, situated at a starting node and unaware of which edges\nare faulty, has to explore the connected fault-free component of this node by\nvisiting all of its nodes. The cost of the exploration is the number of edge\ntraversals. For a given network and given starting node, the overhead of an\nexploration algorithm is the worst-case ratio (taken over all fault\nconfigurations) of its cost to the cost of an optimal algorithm which knows\nwhere faults are situated. An exploration algorithm, for a given network and\ngiven starting node, is called perfectly competitive if its overhead is the\nsmallest among all exploration algorithms not knowing the location of faults.\nWe design a perfectly competitive exploration algorithm for any ring, and show\nthat, for networks modeled by hamiltonian graphs, the overhead of any DFS\nexploration is at most 10/9 times larger than that of a perfectly competitive\nalgorithm. Moreover, for hamiltonian graphs of size at least 24, this overhead\nis less than 6% larger than that of a perfectly competitive algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:50:51 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Caissy", "David", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1602.05232", "submitter": "Srikanta Tirthapura", "authors": "Natcha Simsiri and Kanat Tangwongsan and Srikanta Tirthapura and\n  Kun-Lung Wu", "title": "Work-Efficient Parallel and Incremental Graph Connectivity", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On an evolving graph that is continuously updated by a high-velocity stream\nof edges, how can one efficiently maintain if two vertices are connected? This\nis the connectivity problem, a fundamental and widely studied problem on\ngraphs. We present the first shared-memory parallel algorithm for incremental\ngraph connectivity that is both provably work-efficient and has polylogarithmic\nparallel depth. We also present a simpler algorithm with slightly worse\ntheoretical properties, but which is easier to implement and has good practical\nperformance. Our experiments show a throughput of hundreds of millions of edges\nper second on a $20$-core machine.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 22:30:15 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Simsiri", "Natcha", ""], ["Tangwongsan", "Kanat", ""], ["Tirthapura", "Srikanta", ""], ["Wu", "Kun-Lung", ""]]}, {"id": "1602.05242", "submitter": "Alireza Rezaei", "authors": "Nima Anari, Shayan Oveis Gharan, Alireza Rezaei", "title": "Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh\n  Distributions and Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly Rayleigh distributions are natural generalizations of product and\ndeterminantal probability distributions and satisfy strongest form of negative\ndependence properties. We show that the \"natural\" Monte Carlo Markov Chain\n(MCMC) is rapidly mixing in the support of a {\\em homogeneous} strongly\nRayleigh distribution. As a byproduct, our proof implies Markov chains can be\nused to efficiently generate approximate samples of a $k$-determinantal point\nprocess. This answers an open question raised by Deshpande and Rademacher.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 23:32:53 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 04:08:26 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 19:58:03 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1602.05263", "submitter": "Georgios Zois", "authors": "Dimitris Fotakis, Ioannis Milis, Orestis Papadigenopoulos, Vasilis\n  Vassalos, Georgios Zois", "title": "Scheduling MapReduce Jobs under Multi-Round Precedences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider non-preemptive scheduling of MapReduce jobs with multiple tasks\nin the practical scenario where each job requires several map-reduce rounds. We\nseek to minimize the average weighted completion time and consider scheduling\non identical and unrelated parallel processors. For identical processors, we\npresent LP-based O(1)-approximation algorithms. For unrelated processors, the\napproximation ratio naturally depends on the maximum number of rounds of any\njob. Since the number of rounds per job in typical MapReduce algorithms is a\nsmall constant, our scheduling algorithms achieve a small approximation ratio\nin practice. For the single-round case, we substantially improve on previously\nbest known approximation guarantees for both identical and unrelated\nprocessors. Moreover, we conduct an experimental analysis and compare the\nperformance of our algorithms against a fast heuristic and a lower bound on the\noptimal solution, thus demonstrating their promising practical performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 01:38:08 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Milis", "Ioannis", ""], ["Papadigenopoulos", "Orestis", ""], ["Vassalos", "Vasilis", ""], ["Zois", "Georgios", ""]]}, {"id": "1602.05391", "submitter": "Yitong Yin", "authors": "Yitong Yin", "title": "Simple average-case lower bounds for approximate near-neighbor from\n  isoperimetric inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an $\\Omega(d/\\log \\frac{sw}{nd})$ lower bound for the average-case\ncell-probe complexity of deterministic or Las Vegas randomized algorithms\nsolving approximate near-neighbor (ANN) problem in $d$-dimensional Hamming\nspace in the cell-probe model with $w$-bit cells, using a table of size $s$.\nThis lower bound matches the highest known worst-case cell-probe lower bounds\nfor any static data structure problems.\n  This average-case cell-probe lower bound is proved in a general framework\nwhich relates the cell-probe complexity of ANN to isoperimetric inequalities in\nthe underlying metric space. A tighter connection between ANN lower bounds and\nisoperimetric inequalities is established by a stronger richness lemma proved\nby cell-sampling techniques.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 12:52:27 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 08:11:34 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Yin", "Yitong", ""]]}, {"id": "1602.05437", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Distributed Strong Diameter Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a pair of positive parameters $D,\\chi$, a partition ${\\cal P}$ of the\nvertex set $V$ of an $n$-vertex graph $G = (V,E)$ into disjoint clusters of\ndiameter at most $D$ each is called a $(D,\\chi)$ network decomposition, if the\nsupergraph ${\\cal G}({\\cal P})$, obtained by contracting each of the clusters\nof ${\\cal P}$, can be properly $\\chi$-colored. The decomposition ${\\cal P}$ is\nsaid to be strong (resp., weak) if each of the clusters has strong (resp.,\nweak) diameter at most $D$, i.e., if for every cluster $C \\in {\\cal P}$ and\nevery two vertices $u,v \\in C$, the distance between them in the induced graph\n$G(C)$ of $C$ (resp., in $G$) is at most $D$.\n  Network decomposition is a powerful construct, very useful in distributed\ncomputing and beyond. It was shown by Awerbuch \\etal \\cite{AGLP89} and\nPanconesi and Srinivasan \\cite{PS92}, that strong $(2^{O(\\sqrt{\\log\nn})},2^{O(\\sqrt{\\log n})})$ network decompositions can be computed in\n$2^{O(\\sqrt{\\log n})}$ distributed time. Linial and Saks \\cite{LS93} devised an\ningenious randomized algorithm that constructs {\\em weak} $(O(\\log n),O(\\log\nn))$ network decompositions in $O(\\log^2 n)$ time. It was however open till now\nif {\\em strong} network decompositions with both parameters $2^{o(\\sqrt{\\log\nn})}$ can be constructed in distributed $2^{o(\\sqrt{\\log n})}$ time.\n  In this paper we answer this long-standing open question in the affirmative,\nand show that strong $(O(\\log n),O(\\log n))$ network decompositions can be\ncomputed in $O(\\log^2 n)$ time. We also present a tradeoff between parameters\nof our network decomposition. Our work is inspired by and relies on the\n\"shifted shortest path approach\", due to Blelloch \\etal \\cite{BGKMPT11}, and\nMiller \\etal \\cite{MPX13}. These authors developed this approach for PRAM\nalgorithms for padded partitions. We adapt their approach to network\ndecompositions in the distributed model of computation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:43:40 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1602.05561", "submitter": "Payam Siyari", "authors": "Payam Siyari, Bistra Dilkina, Constantine Dovrolis", "title": "Lexis: An Optimization Framework for Discovering the Hierarchical\n  Structure of Sequential Data", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939741", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data represented as strings abounds in biology, linguistics, document mining,\nweb search and many other fields. Such data often have a hierarchical\nstructure, either because they were artificially designed and composed in a\nhierarchical manner or because there is an underlying evolutionary process that\ncreates repeatedly more complex strings from simpler substrings. We propose a\nframework, referred to as \"Lexis\", that produces an optimized hierarchical\nrepresentation of a given set of \"target\" strings. The resulting hierarchy,\n\"Lexis-DAG\", shows how to construct each target through the concatenation of\nintermediate substrings, minimizing the total number of such concatenations or\nDAG edges. The Lexis optimization problem is related to the smallest grammar\nproblem. After we prove its NP-Hardness for two cost formulations, we propose\nan efficient greedy algorithm for the construction of Lexis-DAGs. We also\nconsider the problem of identifying the set of intermediate nodes (substrings)\nthat collectively form the \"core\" of a Lexis-DAG, which is important in the\nanalysis of Lexis-DAGs. We show that the Lexis framework can be applied in\ndiverse applications such as optimized synthesis of DNA fragments in genomic\nlibraries, hierarchical structure discovery in protein sequences,\ndictionary-based text compression, and feature extraction from a set of\ndocuments.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 20:36:28 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 21:15:54 GMT"}, {"version": "v3", "created": "Sat, 11 Jun 2016 05:52:26 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Siyari", "Payam", ""], ["Dilkina", "Bistra", ""], ["Dovrolis", "Constantine", ""]]}, {"id": "1602.05608", "submitter": "Lukasz Kowalik", "authors": "{\\L}ukasz Kowalik, Juho Lauri, Arkadiusz Soca{\\l}a", "title": "On the fine-grained complexity of rainbow coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rainbow k-Coloring problem asks whether the edges of a given graph can be\ncolored in $k$ colors so that every pair of vertices is connected by a rainbow\npath, i.e., a path with all edges of different colors. Our main result states\nthat for any $k\\ge 2$, there is no algorithm for Rainbow k-Coloring running in\ntime $2^{o(n^{3/2})}$, unless ETH fails.\n  Motivated by this negative result we consider two parameterized variants of\nthe problem. In Subset Rainbow k-Coloring problem, introduced by Chakraborty et\nal. [STACS 2009, J. Comb. Opt. 2009], we are additionally given a set $S$ of\npairs of vertices and we ask if there is a coloring in which all the pairs in\n$S$ are connected by rainbow paths. We show that Subset Rainbow k-Coloring is\nFPT when parameterized by $|S|$. We also study Maximum Rainbow k-Coloring\nproblem, where we are additionally given an integer $q$ and we ask if there is\na coloring in which at least $q$ anti-edges are connected by rainbow paths. We\nshow that the problem is FPT when parameterized by $q$ and has a kernel of size\n$O(q)$ for every $k\\ge 2$ (thus proving that the problem is FPT), extending the\nresult of Ananth et al. [FSTTCS 2011].\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 21:49:16 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Kowalik", "\u0141ukasz", ""], ["Lauri", "Juho", ""], ["Soca\u0142a", "Arkadiusz", ""]]}, {"id": "1602.05622", "submitter": "Michael Horton", "authors": "Kevin Buchin, Maike Buchin, Joachim Gudmundsson, Michael Horton, Stef\n  Sijben", "title": "Compact Flow Diagrams for State Sequences", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": "10.1145/3150525", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of compactly representing a large number of state\nsequences, e.g., sequences of activities, as a flow diagram. We argue that the\nflow diagram representation gives an intuitive summary that allows the user to\ndetect patterns among large sets of state sequences. Simplified, our aim is to\ngenerate a small flow diagram that models the flow of states of all the state\nsequences given as input. For a small number of state sequences we present\nefficient algorithms to compute a minimal flow diagram. For a large number of\nstate sequences we show that it is unlikely that efficient algorithms exist.\nMore specifically, the problem is W[1]-hard if the number of state sequences is\ntaken as a parameter. We thus introduce several heuristics for this problem. We\nargue about the usefulness of the flow diagram by applying the algorithms to\ntwo problems in sports analysis. We evaluate the performance of our algorithms\non a football data set and generated data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 22:56:59 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Buchin", "Kevin", ""], ["Buchin", "Maike", ""], ["Gudmundsson", "Joachim", ""], ["Horton", "Michael", ""], ["Sijben", "Stef", ""]]}, {"id": "1602.05719", "submitter": "Jelani Nelson", "authors": "Jaros{\\l}aw B{\\l}asiok, Jelani Nelson", "title": "An improved analysis of the ER-SpUD dictionary learning algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"dictionary learning\" we observe $Y = AX + E$ for some\n$Y\\in\\mathbb{R}^{n\\times p}$, $A \\in\\mathbb{R}^{m\\times n}$, and\n$X\\in\\mathbb{R}^{m\\times p}$. The matrix $Y$ is observed, and $A, X, E$ are\nunknown. Here $E$ is \"noise\" of small norm, and $X$ is column-wise sparse. The\nmatrix $A$ is referred to as a {\\em dictionary}, and its columns as {\\em\natoms}. Then, given some small number $p$ of samples, i.e.\\ columns of $Y$, the\ngoal is to learn the dictionary $A$ up to small error, as well as $X$. The\nmotivation is that in many applications data is expected to sparse when\nrepresented by atoms in the \"right\" dictionary $A$ (e.g.\\ images in the Haar\nwavelet basis), and the goal is to learn $A$ from the data to then use it for\nother applications.\n  Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD with\nprovable guarantees when $E = 0$ and $m = n$. They showed if $X$ has\nindependent entries with an expected $s$ non-zeroes per column for $1 \\lesssim\ns \\lesssim \\sqrt{n}$, and with non-zero entries being subgaussian, then for\n$p\\gtrsim n^2\\log^2 n$ with high probability ER-SpUD outputs matrices $A', X'$\nwhich equal $A, X$ up to permuting and scaling columns (resp.\\ rows) of $A$\n(resp.\\ $X$). They conjectured $p\\gtrsim n\\log n$ suffices, which they showed\nwas information theoretically necessary for {\\em any} algorithm to succeed when\n$s \\simeq 1$. Significant progress was later obtained in [LV15].\n  We show that for a slight variant of ER-SpUD, $p\\gtrsim n\\log(n/\\delta)$\nsamples suffice for successful recovery with probability $1-\\delta$. We also\nshow that for the unmodified ER-SpUD, $p\\gtrsim n^{1.99}$ samples are required\neven to learn $A, X$ with polynomially small success probability. This resolves\nthe main conjecture of [SWW12], and contradicts the main result of [LV15],\nwhich claimed that $p\\gtrsim n\\log^4 n$ guarantees success whp.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 08:51:08 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Nelson", "Jelani", ""]]}, {"id": "1602.05837", "submitter": "Arturs Backurs", "authors": "Arturs Backurs and Nishanth Dikkala and Christos Tzamos", "title": "Tight Hardness Results for Maximum Weight Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ weighted points (positive or negative) in $d$ dimensions, what is\nthe axis-aligned box which maximizes the total weight of the points it\ncontains?\n  The best known algorithm for this problem is based on a reduction to a\nrelated problem, the Weighted Depth problem [T. M. Chan, FOCS'13], and runs in\ntime $O(n^d)$. It was conjectured [Barbay et al., CCCG'13] that this runtime is\ntight up to subpolynomial factors. We answer this conjecture affirmatively by\nproviding a matching conditional lower bound. We also provide conditional lower\nbounds for the special case when points are arranged in a grid (a well studied\nproblem known as Maximum Subarray problem) as well as for other related\nproblems.\n  All our lower bounds are based on assumptions that the best known algorithms\nfor the All-Pairs Shortest Paths problem (APSP) and for the Max-Weight k-Clique\nproblem in edge-weighted graphs are essentially optimal.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 15:24:22 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 00:12:38 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Backurs", "Arturs", ""], ["Dikkala", "Nishanth", ""], ["Tzamos", "Christos", ""]]}, {"id": "1602.05852", "submitter": "Kyrill Winkler", "authors": "Kyrill Winkler, Manfred Schwarz and Ulrich Schmid", "title": "Consensus in Rooted Dynamic Networks with Short-Lived Stability", "comments": "14 pages, 2 figures", "journal-ref": "Distrib. Comput. (2019)", "doi": "10.1007/s00446-019-00348-0", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving consensus using deterministic algorithms\nin a synchronous dynamic network with unreliable, directional point-to-point\nlinks, which are under the control of a message adversary. In contrast to a\nlarge body of existing work that focuses on oblivious message adversaries where\nthe communication graphs are picked from a predefined set, we consider message\nadversaries where guarantees about stable periods that occur only eventually\ncan be expressed. We reveal to what extent such eventual stability is necessary\nand sufficient, that is, we present the shortest period of stability that\npermits solving consensus, a result that should prove quite useful in systems\nthat exhibit erratic boot-up phases or recover after repeatedly occurring,\nmassive transient faults. Contrary to the case of longer stability periods,\nwhere we show how standard algorithmic techniques for solving consensus can be\nemployed, the short-lived nature of the stability phase forces us to use more\nunusual algorithmic methods that avoid waiting explicitly for the stability\nperiod to occur.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 16:08:36 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 15:53:31 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 13:57:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Winkler", "Kyrill", ""], ["Schwarz", "Manfred", ""], ["Schmid", "Ulrich", ""]]}, {"id": "1602.05856", "submitter": "Ilia Minkin", "authors": "Ilia Minkin, Son Pham and Paul Medvedev", "title": "TwoPaCo: An efficient algorithm to build the compacted de Bruijn graph\n  from many complete genomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: De Bruijn graphs have been proposed as a data structure to\nfacilitate the analysis of related whole genome sequences, in both a population\nand comparative genomic settings. However, current approaches do not scale well\nto many genomes of large size (such as mammalian genomes). Results: In this\npaper, we present TwoPaCo, a simple and scalable low memory algorithm for the\ndirect construction of the compacted de Bruijn graph from a set of complete\ngenomes. We demonstrate that it can construct the graph for 100 simulated human\ngenomes in less then a day and eight real primates in less than two hours, on a\ntypical shared-memory machine. We believe that this progress will enable novel\nbiological analyses of hundreds of mammalian-sized genomes. Availability: Our\ncode and data is available for download from github.com/medvedevgroup/TwoPaCo\nContact: ium125@psu.edu\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 16:11:50 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Minkin", "Ilia", ""], ["Pham", "Son", ""], ["Medvedev", "Paul", ""]]}, {"id": "1602.05866", "submitter": "Matteo Riondato", "authors": "Matteo Riondato and Eli Upfal", "title": "ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs\n  with Rademacher Averages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ABRA, a suite of algorithms that compute and maintain\nprobabilistically-guaranteed, high-quality, approximations of the betweenness\ncentrality of all nodes (or edges) on both static and fully dynamic graphs. Our\nalgorithms rely on random sampling and their analysis leverages on Rademacher\naverages and pseudodimension, fundamental concepts from statistical learning\ntheory. To our knowledge, this is the first application of these concepts to\nthe field of graph analysis. The results of our experimental evaluation show\nthat our approach is much faster than exact methods, and vastly outperforms, in\nboth speed and number of samples, current state-of-the-art algorithms with the\nsame quality guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 16:37:24 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Riondato", "Matteo", ""], ["Upfal", "Eli", ""]]}, {"id": "1602.05889", "submitter": "Jarek Duda dr", "authors": "Jarek Duda", "title": "Distortion-Resistant Hashing for rapid search of similar DNA subsequence", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the basic tasks in bioinformatics is localizing a short subsequence\n$S$, read while sequencing, in a long reference sequence $R$, like the human\ngeneome. A natural rapid approach would be finding a hash value for $S$ and\ncompare it with a prepared database of hash values for each of length $|S|$\nsubsequences of $R$. The problem with such approach is that it would only spot\na perfect match, while in reality there are lots of small changes:\nsubstitutions, deletions and insertions.\n  This issue could be repaired if having a hash function designed to tolerate\nsome small distortion accordingly to an alignment metric (like\nNeedleman-Wunch): designed to make that two similar sequences should most\nlikely give the same hash value. This paper discusses construction of\nDistortion-Resistant Hashing (DRH) to generate such fingerprints for rapid\nsearch of similar subsequences. The proposed approach is based on the rate\ndistortion theory: in a nearly uniform subset of length $|S|$ sequences, the\nhash value represents the closest sequence to $S$. This gives some control of\nthe distance of collisions: sequences having the same hash value.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 17:42:54 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "1602.05897", "submitter": "Amit Daniely", "authors": "Amit Daniely and Roy Frostig and Yoram Singer", "title": "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 18:14:19 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 18:39:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Daniely", "Amit", ""], ["Frostig", "Roy", ""], ["Singer", "Yoram", ""]]}, {"id": "1602.05899", "submitter": "Leah Epstein", "authors": "Ron Adar and Leah Epstein", "title": "An algorithm for the weighted metric dimension of two-dimensional grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-dimensional grid consists of vertices of the form (i,j) for 1 \\leq i\n\\leq m and 1 \\leq j \\leq n, for fixed m,n > 1. Two vertices are adjacent if the\n\\ell_1 distance between their vectors is equal to 1. A landmark set is a subset\nof vertices L \\subseteq V, such that for any distinct pair of vertices u,v \\in\nV, there exists a vertex of L whose distances to u and v are not equal. We\ndesign an efficient algorithm for finding a minimum landmark set with respect\nto total cost in a grid graph with non-negative costs defined on the vertices.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 18:19:40 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Adar", "Ron", ""], ["Epstein", "Leah", ""]]}, {"id": "1602.05914", "submitter": "Shahar Dobzinski", "authors": "Shahar Dobzinski", "title": "Breaking the Logarithmic Barrier for Truthful Combinatorial Auctions\n  with Submodular Bidders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a central problem in Algorithmic Mechanism Design: constructing\ntruthful mechanisms for welfare maximization in combinatorial auctions with\nsubmodular bidders. Dobzinski, Nisan, and Schapira provided the first mechanism\nthat guarantees a non-trivial approximation ratio of $O(\\log^2 m)$ [STOC'06],\nwhere $m$ is the number of items. This was subsequently improved to $O(\\log\nm\\log \\log m)$ [Dobzinski, APPROX'07] and then to $O(\\log m)$ [Krysta and\nVocking, ICALP'12].\n  In this paper we develop the first mechanism that breaks the logarithmic\nbarrier. Specifically, the mechanism provides an approximation ratio of\n$O(\\sqrt {\\log m})$. Similarly to previous constructions, our mechanism uses\npolynomially many value and demand queries, and in fact provides the same\napproximation ratio for the larger class of XOS (a.k.a. fractionally\nsubadditive) valuations.\n  We also develop a computationally efficient implementation of the mechanism\nfor combinatorial auctions with budget additive bidders. Although in general\ncomputing a demand query is NP-hard for budget additive valuations, we observe\nthat the specific form of demand queries that our mechanism uses can be\nefficiently computed when bidders are budget additive.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 19:08:06 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 09:36:20 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Dobzinski", "Shahar", ""]]}, {"id": "1602.06058", "submitter": "Sung-Il Pae", "authors": "Sung-il Pae", "title": "Binarization Trees and Random Number Generation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An m-extracting procedure produces unbiased random bits from a loaded dice\nwith m faces. A binarization takes inputs from an m-faced dice and produce bit\nsequences to be fed into a (binary) extracting procedure to obtain random bits.\nThus, binary extracting procedures give rise to an m-extracting procedure via a\nbinarization. An entropy- preserving binarization is to be called complete, and\nsuch a procedure has been proposed by Zhou and Bruck. We show that there exist\ncomplete binarizations in abundance as naturally arising from binary trees with\nm leaves. The well-known leaf entropy theorem and a closely related structure\nlemma play important roles in the arguments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 07:12:02 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 21:58:45 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Pae", "Sung-il", ""]]}, {"id": "1602.06159", "submitter": "Adi Rosen", "authors": "Guy Even, Reut Levi, Moti Medina, Adi Rosen", "title": "Sublinear Random Access Generators for Preferential Attachment Graphs", "comments": "ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a distribution on graphs,\nspecifically when the distribution is defined by an evolving graph model, and\nconsider the time, space and randomness complexities of such samplers.\n  In the standard approach, the whole graph is chosen randomly according to the\nrandomized evolving process, stored in full, and then queries on the sampled\ngraph are answered by simply accessing the stored graph. This may require\nprohibitive amounts of time, space and random bits, especially when only a\nsmall number of queries are actually issued. Instead, we propose to generate\nthe graph on-the-fly, in response to queries, and therefore to require amounts\nof time, space, and random bits which are a function of the actual number of\nqueries.\n  We focus on two random graph models: the Barab{\\'{a}}si-Albert Preferential\nAttachment model (BA-graphs) and the random recursive tree model. We give\non-the-fly generation algorithms for both models. With probability\n$1-1/\\mbox{poly}(n)$, each and every query is answered in $\\mbox{polylog}(n)$\ntime, and the increase in space and the number of random bits consumed by any\nsingle query are both $\\mbox{polylog}(n)$, where $n$ denotes the number of\nvertices in the graph.\n  Our results show that, although the BA random graph model is defined by a\nsequential process, efficient random access to the graph's nodes is possible.\nIn addition to the conceptual contribution, efficient on-the-fly generation of\nrandom graphs can serve as a tool for the efficient simulation of sublinear\nalgorithms over large BA-graphs, and the efficient estimation of their\nperformance on such graphs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 14:16:41 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 09:05:23 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 10:27:12 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Even", "Guy", ""], ["Levi", "Reut", ""], ["Medina", "Moti", ""], ["Rosen", "Adi", ""]]}, {"id": "1602.06169", "submitter": "Moti Medina", "authors": "Guy Even, Moti Medina, Boaz Patt-Shamir", "title": "Competitive Path Computation and Function Placement in SDNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a task of serving requests that arrive in an online fashion in\nSoftware-Defined Networks (SDNs) with network function virtualization (NFV).\nEach request specifies an abstract routing and processing \"plan\" for a flow.\nEach processing function can be performed by a specified subset of servers in\nthe system. The algorithm needs to either reject the request or admit it and\nreturn detailed routing (a.k.a. \"path computation\") and processing assignment\n(\"function placement\"). Each request also specifies the communication bandwidth\nand the processing load it requires. Components in the system (links and\nprocessors) have bounded capacity; a feasible solution may not violate the\ncapacity constraints. Requests have benefits and the goal is to maximize the\ntotal benefit of accepted requests.\n  In this paper we first formalize the problem, and propose a new service model\nthat allows us to cope with requests with unknown duration. The new service\nmodel augments the traditional accept/reject schemes with a new possible\nresponse of \"stand by.\" Our main result is an online algorithm for path\ncomputation and function placement that guarantees, in each time step,\nthroughput of at least $\\Omega\\left(\\frac{\\text{OPT}^*}{\\log n}\\right)$, where\n$n$ is the system size and $\\text{OPT}^*$ is an upper bound on the maximal\npossible throughput. The guarantee holds assuming that requests ask for at most\nan $O\\left(1/{\\log n}\\right)$-fraction of the capacity of any component in the\nsystem. Furthermore, the guarantee holds even though our algorithm serves\nrequests in an all-or-nothing fashion using a single path and never preempts\naccepted flows, while $\\text{OPT}^*$ may serve fractional requests, may split\nthe allocation over multiple paths, and may arbitrarily preempt and resume\nservice of requests.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 14:46:13 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Even", "Guy", ""], ["Medina", "Moti", ""], ["Patt-Shamir", "Boaz", ""]]}, {"id": "1602.06174", "submitter": "Adi Rosen", "authors": "Guy Even, Moti Medina, Adi Ros\\'en", "title": "A Constant Approximation Algorithm for Scheduling Packets on Line\n  Networks", "comments": null, "journal-ref": "ESA 2016", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we improve the approximation ratio for the problem of\nscheduling packets on line networks with bounded buffers, where the aim is that\nof maximizing the throughput. Each node in the network has a local buffer of\nbounded size $B$, and each edge (or link) can transmit a limited number, $c$,\nof packets in every time unit. The input to the problem consists of a set of\npacket requests, each defined by a source node, a destination node, and a\nrelease time. We denote by $n$ the size of the network. A solution for this\nproblem is a schedule that delivers (some of the) packets to their destinations\nwithout violating the capacity constraints of the network (buffers or edges).\nOur goal is to design an efficient algorithm that computes a schedule that\nmaximizes the number of packets that arrive to their respective destinations.\n  We give a randomized approximation algorithm with constant approximation\nratio for the case where $B=\\Theta(c)$. This improves over the previously best\nresult of $O(\\log^* n)$ (R\\\"acke and Ros\\'en, Theory Comput. Syst., 49(4),\n2011). Our improvement is based on a new combinatorial lemma that we prove,\nstating, roughly speaking, that if packets are allowed to stay put in buffers\nonly a limited number of time steps, $2d$, where $d$ is the longest\nsource-destination distance of any input packet, then the cardinality of the\noptimal solution is decreased by only a constant factor. This claim was not\npreviously known in the directed integral (i.e., unsplittable, zero-one) case,\nand may find additional applications for routing and scheduling algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 15:06:11 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 12:36:10 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Even", "Guy", ""], ["Medina", "Moti", ""], ["Ros\u00e9n", "Adi", ""]]}, {"id": "1602.06283", "submitter": "Carlo Comin", "authors": "Carlo Comin, Anthony Labarre, Romeo Rizzi, St\\'ephane Vialette", "title": "Sorting With Forbidden Intermediates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of applications, most notably in comparative genomics, involve\nthe computation of a shortest sorting sequence of operations for a given\npermutation, where the set of allowed operations is fixed beforehand. Such\nsequences are useful for instance when reconstructing potential scenarios of\nevolution between species, or when trying to assess their similarity. We\nrevisit those problems by adding a new constraint on the sequences to be\ncomputed: they must \\emph{avoid} a given set of \\emph{forbidden intermediates},\nwhich correspond to species that cannot exist because the mutations that would\nbe involved in their creation are lethal. We initiate this study by focusing on\nthe case where the only mutations that can occur are exchanges of any two\nelements in the permutations, and give a polynomial time algorithm for solving\nthat problem when the permutation to sort is an involution.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 20:38:56 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 10:45:21 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Comin", "Carlo", ""], ["Labarre", "Anthony", ""], ["Rizzi", "Romeo", ""], ["Vialette", "St\u00e9phane", ""]]}, {"id": "1602.06295", "submitter": "Michel Fliess", "authors": "C\\'edric Join, Michel Fliess, Cyril Voyant, Fr\\'ed\\'eric Chaxel", "title": "Solar energy production: Short-term forecasting and risk management", "comments": "8th IFAC Conference on Manufacturing Modelling, Management & Control\n  (Troyes, France, June 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity production via solar energy is tackled via short-term forecasts\nand risk management. Our main tool is a new setting on time series. It allows\nthe definition of \"confidence bands\" where the Gaussian assumption, which is\nnot satisfied by our concrete data, may be abandoned. Those bands are quite\nconvenient and easily implementable. Numerous computer simulations are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 16:25:16 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Join", "C\u00e9dric", ""], ["Fliess", "Michel", ""], ["Voyant", "Cyril", ""], ["Chaxel", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1602.06401", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, John Liagouris, Maria Krommyda, George Papastefanatos,\n  Timos Sellis", "title": "graphVizdb: A Scalable Platform for Interactive Large Graph\n  Visualization", "comments": "32nd IEEE International Conference on Data Engineering (ICDE '16)", "journal-ref": null, "doi": "10.1109/ICDE.2016.7498340", "report-no": null, "categories": "cs.HC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel platform for the interactive visualization of very large\ngraphs. The platform enables the user to interact with the visualized graph in\na way that is very similar to the exploration of maps at multiple levels. Our\napproach involves an offline preprocessing phase that builds the layout of the\ngraph by assigning coordinates to its nodes with respect to a Euclidean plane.\nThe respective points are indexed with a spatial data structure, i.e., an\nR-tree, and stored in a database. Multiple abstraction layers of the graph\nbased on various criteria are also created offline, and they are indexed\nsimilarly so that the user can explore the dataset at different levels of\ngranularity, depending on her particular needs. Then, our system translates\nuser operations into simple and very efficient spatial operations (i.e., window\nqueries) in the backend. This technique allows for a fine-grained access to\nvery large graphs with extremely low latency and memory requirements and\nwithout compromising the functionality of the tool. Our web-based prototype\nsupports three main operations: (1) interactive navigation, (2) multi-level\nexploration, and (3) keyword search on the graph metadata.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 12:49:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bikakis", "Nikos", ""], ["Liagouris", "John", ""], ["Krommyda", "Maria", ""], ["Papastefanatos", "George", ""], ["Sellis", "Timos", ""]]}, {"id": "1602.06411", "submitter": "Kyriakos Axiotis", "authors": "Kyriakos Axiotis, Dimitris Fotakis", "title": "On the Size and the Approximability of Minimum Temporally Connected\n  Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider temporal graphs with discrete time labels and investigate the\nsize and the approximability of minimum temporally connected spanning\nsubgraphs. We present a family of minimally connected temporal graphs with $n$\nvertices and $\\Omega(n^2)$ edges, thus resolving an open question of (Kempe,\nKleinberg, Kumar, JCSS 64, 2002) about the existence of sparse temporal\nconnectivity certificates. Next, we consider the problem of computing a minimum\nweight subset of temporal edges that preserve connectivity of a given temporal\ngraph either from a given vertex r (r-MTC problem) or among all vertex pairs\n(MTC problem). We show that the approximability of r-MTC is closely related to\nthe approximability of Directed Steiner Tree and that r-MTC can be solved in\npolynomial time if the underlying graph has bounded treewidth. We also show\nthat the best approximation ratio for MTC is at least $O(2^{\\log^{1-\\epsilon}\nn})$ and at most $O(\\min\\{n^{1+\\epsilon}, (\\Delta M)^{2/3+\\epsilon}\\})$, for\nany constant $\\epsilon > 0$, where $M$ is the number of temporal edges and\n$\\Delta$ is the maximum degree of the underlying graph. Furthermore, we prove\nthat the unweighted version of MTC is APX-hard and that MTC is efficiently\nsolvable in trees and $2$-approximable in cycles.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 14:24:13 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Axiotis", "Kyriakos", ""], ["Fotakis", "Dimitris", ""]]}, {"id": "1602.06426", "submitter": "Cassio Neri", "authors": "Cassio Neri", "title": "A loopless and branchless $O(1)$ algorithm to generate the next Dyck\n  word", "comments": "First published on 19 July 2014 at https://github.com/cassioneri/Dyck", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Let integer be any C/C++ unsigned integer type up to 64-bits long. Given a\nDyck word the following code returns the next Dyck word of the same size,\nprovided it exists.\n  integer next_dyck_word(integer w) {\n  integer const a = w & -w;\n  integer const b = w + a;\n  integer c = w ^ b;\n  c = (c / a >> 2) + 1;\n  c = ((c * c - 1) & 0xaaaaaaaaaaaaaaaa) | b;\n  return c;\n  }\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 16:24:30 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:54:58 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Neri", "Cassio", ""]]}, {"id": "1602.06577", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "2-Bit Random Projections, NonLinear Estimators, and Approximate Near\n  Neighbor Search", "comments": "arXiv admin note: text overlap with arXiv:1403.8144", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of random projections has become a standard tool for machine\nlearning, data mining, and search with massive data at Web scale. The effective\nuse of random projections requires efficient coding schemes for quantizing\n(real-valued) projected data into integers. In this paper, we focus on a simple\n2-bit coding scheme. In particular, we develop accurate nonlinear estimators of\ndata similarity based on the 2-bit strategy. This work will have important\npractical applications. For example, in the task of near neighbor search, a\ncrucial step (often called re-ranking) is to compute or estimate data\nsimilarities once a set of candidate data points have been identified by hash\ntable techniques. This re-ranking step can take advantage of the proposed\ncoding scheme and estimator.\n  As a related task, in this paper, we also study a simple uniform quantization\nscheme for the purpose of building hash tables with projected data. Our\nanalysis shows that typically only a small number of bits are needed. For\nexample, when the target similarity level is high, 2 or 3 bits might be\nsufficient. When the target similarity level is not so high, it is preferable\nto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good\nchoice for the task of sublinear time approximate near neighbor search via hash\ntables.\n  Combining these results, we conclude that 2-bit random projections should be\nrecommended for approximate near neighbor search and similarity estimation.\nExtensive experimental results are provided.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 20:46:13 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1602.06589", "submitter": "Edoardo Di Napoli", "authors": "Edoardo Di Napoli (1 and 4), Elmar Peise (2), Markus Hrywniak (3),\n  Paolo Bientinesi (2) ((1) J\\\"ulich Supercomputing Centre, (2) AICES, RWTH\n  Aachen University, (3) GRS, RWTH Aachen University, (4) J\\\"ulich Aachen\n  Research Alliance -- High-performance Computing)", "title": "High-performance generation of the Hamiltonian and Overlap matrices in\n  FLAPW methods", "comments": "Second revised version. Corrected notation. Added acknowledgment. 30\n  pages, 2 figures and two tables. Submitted to a Special Issue of Computer\n  Physics Communication", "journal-ref": null, "doi": "10.1016/j.cpc.2016.10.003", "report-no": null, "categories": "cs.CE cs.DS cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest efforts of computational scientists is to translate the\nmathematical model describing a class of physical phenomena into large and\ncomplex codes. Many of these codes face the difficulty of implementing the\nmathematical operations in the model in terms of low level optimized kernels\noffering both performance and portability. Legacy codes suffer from the\nadditional curse of rigid design choices based on outdated performance metrics\n(e.g. minimization of memory footprint). Using a representative code from the\nMaterials Science community, we propose a methodology to restructure the most\nexpensive operations in terms of an optimized combination of dense linear\nalgebra kernels. The resulting algorithm guarantees an increased performance\nand an extended life span of this code enabling larger scale simulations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 22:09:30 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 08:24:02 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2016 19:40:16 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Di Napoli", "Edoardo", "", "1 and 4"], ["Peise", "Elmar", ""], ["Hrywniak", "Markus", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1602.06612", "submitter": "Soledad Villar", "authors": "Dustin G. Mixon, Soledad Villar, Rachel Ward", "title": "Clustering subgaussian mixtures by semidefinite programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-free relax-and-round algorithm for k-means clustering\nbased on a semidefinite relaxation due to Peng and Wei. The algorithm\ninterprets the SDP output as a denoised version of the original data and then\nrounds this output to a hard clustering. We provide a generic method for\nproving performance guarantees for this algorithm, and we analyze the algorithm\nin the context of subgaussian mixture models. We also study the fundamental\nlimits of estimating Gaussian centers by k-means clustering in order to compare\nour approximation guarantee to the theoretically optimal k-means clustering\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 00:19:20 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 19:38:37 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Mixon", "Dustin G.", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1602.06659", "submitter": "Fu-Hong Liu", "authors": "Fu-Hong Liu, Hsiang-Hsuan Liu and Prudence W.H. Wong", "title": "Non-preemptive Scheduling in a Smart Grid Model and its Implications on\n  Machine Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a scheduling problem arising in demand response management in smart\ngrid. Consumers send in power requests with a flexible feasible time interval\nduring which their requests can be served. The grid controller, upon receiving\npower requests, schedules each request within the specified interval. The\nelectricity cost is measured by a convex function of the load in each timeslot.\nThe objective is to schedule all requests with the minimum total electricity\ncost. Previous work has studied cases where jobs have unit power requirement\nand unit duration. We extend the study to arbitrary power requirement and\nduration, which has been shown to be NP-hard. We give the first online\nalgorithm for the general problem, and prove that the problem is fixed\nparameter tractable. We also show that the online algorithm is asymptotically\noptimal when the objective is to minimize the peak load. In addition, we\nobserve that the classical non-preemptive machine minimization problem is a\nspecial case of the smart grid problem with min-peak objective, and show that\nwe can solve the non-preemptive machine minimization problem asymptotically\noptimally.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 06:39:10 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 07:43:10 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 09:13:14 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Liu", "Fu-Hong", ""], ["Liu", "Hsiang-Hsuan", ""], ["Wong", "Prudence W. H.", ""]]}, {"id": "1602.06688", "submitter": "Hiroshi Sakamoto", "authors": "Yoshimasa Takabatake, Kenta Nakashima, Tetsuji Kuboyama, Yasuo Tabei,\n  Hiroshi Sakamoto", "title": "siEDM: an efficient string index and search algorithm for edit distance\n  with moves", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several self-indexes for highly repetitive text collections exist,\ndeveloping an index and search algorithm with editing operations remains a\nchallenge. Edit distance with moves (EDM) is a string-to-string distance\nmeasure that includes substring moves in addition to ordinal editing operations\nto turn one string into another. Although the problem of computing EDM is\nintractable, it has a wide range of potential applications, especially in\napproximate string retrieval. Despite the importance of computing EDM, there\nhas been no efficient method for indexing and searching large text collections\nbased on the EDM measure. We propose the first algorithm, named string index\nfor edit distance with moves (siEDM), for indexing and searching strings with\nEDM. The siEDM algorithm builds an index structure by leveraging the idea\nbehind the edit sensitive parsing (ESP), an efficient algorithm enabling\napproximately computing EDM with guarantees of upper and lower bounds for the\nexact EDM. siEDM efficiently prunes the space for searching query strings by\nthe proposed method, which enables fast query searches with the same guarantee\nas ESP. We experimentally tested the ability of siEDM to index and search\nstrings on benchmark datasets, and we showed siEDM's efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:02:44 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 05:23:27 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Takabatake", "Yoshimasa", ""], ["Nakashima", "Kenta", ""], ["Kuboyama", "Tetsuji", ""], ["Tabei", "Yasuo", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1602.06705", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard", "title": "On the Hardness of Partially Dynamic Graph Problems and Connections to\n  Diameter", "comments": "To appear at ICALP'16. Abstract truncated to fit arXiv limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional lower bounds for dynamic graph problems has received a great deal\nof attention in recent years. While many results are now known for the\nfully-dynamic case and such bounds often imply worst-case bounds for the\npartially dynamic setting, it seems much more difficult to prove amortized\nbounds for incremental and decremental algorithms. In this paper we consider\npartially dynamic versions of three classic problems in graph theory. Based on\npopular conjectures we show that:\n  -- No algorithm with amortized update time $O(n^{1-\\varepsilon})$ exists for\nincremental or decremental maximum cardinality bipartite matching. This\nsignificantly improves on the $O(m^{1/2-\\varepsilon})$ bound for sparse graphs\nof Henzinger et al. [STOC'15] and $O(n^{1/3-\\varepsilon})$ bound of Kopelowitz,\nPettie and Porat. Our linear bound also appears more natural. In addition, the\nresult we present separates the node-addition model from the edge insertion\nmodel, as an algorithm with total update time $O(m\\sqrt{n})$ exists for the\nformer by Bosek et al. [FOCS'14].\n  -- No algorithm with amortized update time $O(m^{1-\\varepsilon})$ exists for\nincremental or decremental maximum flow in directed and weighted sparse graphs.\nNo such lower bound was known for partially dynamic maximum flow previously.\nFurthermore no algorithm with amortized update time $O(n^{1-\\varepsilon})$\nexists for directed and unweighted graphs or undirected and weighted graphs.\n  -- No algorithm with amortized update time $O(n^{1/2 - \\varepsilon})$ exists\nfor incremental or decremental $(4/3-\\varepsilon')$-approximating the diameter\nof an unweighted graph. We also show a slightly stronger bound if node\nadditions are allowed. [...]\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 10:07:27 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 10:05:16 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""]]}, {"id": "1602.06708", "submitter": "Joan Boyar", "authors": "Joan Boyar, Leah Epstein, Lene M. Favrholdt, Kim S. Larsen and Asaf\n  Levin", "title": "Online Bounded Analysis", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though competitive analysis is often a very good tool for the analysis of\nonline algorithms, sometimes it does not give any insight and sometimes it\ngives counter-intuitive results. Much work has gone into exploring other\nperformance measures, in particular targeted at what seems to be the core\nproblem with competitive analysis: the comparison of the performance of an\nonline algorithm is made to a too powerful adversary. We consider a new\napproach to restricting the power of the adversary, by requiring that when\njudging a given online algorithm, the optimal offline algorithm must perform as\nwell as the online algorithm, not just on the entire final request sequence,\nbut also on any prefix of that sequence. This is limiting the adversary's usual\nadvantage of being able to exploit that it knows the sequence is continuing\nbeyond the current request. Through a collection of online problems, including\nmachine scheduling, bin packing, dual bin packing, and seat reservation, we\ninvestigate the significance of this particular offline advantage.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 10:28:27 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 05:41:16 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Boyar", "Joan", ""], ["Epstein", "Leah", ""], ["Favrholdt", "Lene M.", ""], ["Larsen", "Kim S.", ""], ["Levin", "Asaf", ""]]}, {"id": "1602.06819", "submitter": "Thibault Debatty", "authors": "Thibault Debatty, Pietro Michiardi and Wim Mees", "title": "Fast Online k-nn Graph Building", "comments": "Submitted to ACM SIGKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an online approximate k-nn graph building algorithm,\nwhich is able to quickly update a k-nn graph using a flow of data points. One\nvery important step of the algorithm consists in using the current distributed\ngraph to search for the neighbors of a new node. Hence we also propose a\ndistributed partitioning method based on balanced k-medoids clustering, that we\nuse to optimize the distributed search process. Finally, we present the\nimproved sequential search procedure that is used inside each partition.\n  We also perform an experimental evaluation of the different algorithms, where\nwe study the influence of the parameters and compare the result of our\nalgorithms to existing state of the art. This experimental evaluation confirms\nthat the fast online k-nn graph building algorithm produces a graph that is\nhighly similar to the graph produced by an offline exhaustive algorithm, while\nit requires less similarity computations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 15:33:57 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Debatty", "Thibault", ""], ["Michiardi", "Pietro", ""], ["Mees", "Wim", ""]]}, {"id": "1602.06872", "submitter": "Christopher Musco", "authors": "Roy Frostig, Cameron Musco, Christopher Musco, Aaron Sidford", "title": "Principal Component Projection Without Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to efficiently project a vector onto the top principal components\nof a matrix, without explicitly computing these components. Specifically, we\nintroduce an iterative algorithm that provably computes the projection using\nfew calls to any black-box routine for ridge regression.\n  By avoiding explicit principal component analysis (PCA), our algorithm is the\nfirst with no runtime dependence on the number of top principal components. We\nshow that it can be used to give a fast iterative method for the popular\nprincipal component regression problem, giving the first major runtime\nimprovement over the naive method of combining PCA with regression.\n  To achieve our results, we first observe that ridge regression can be used to\nobtain a \"smooth projection\" onto the top principal components. We then sharpen\nthis approximation to true projection using a low-degree polynomial\napproximation to the matrix step function. Step function approximation is a\ntopic of long-term interest in scientific computing. We extend prior theory by\nconstructing polynomials with simple iterative structure and rigorously\nanalyzing their behavior under limited precision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 17:52:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 17:29:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Frostig", "Roy", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Sidford", "Aaron", ""]]}, {"id": "1602.06922", "submitter": "Christopher Kennedy", "authors": "Christopher Kennedy, Rachel Ward", "title": "Fast Cross-Polytope Locality-Sensitive Hashing", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a variant of cross-polytope locality sensitive hashing with\nrespect to angular distance which is provably optimal in asymptotic sensitivity\nand enjoys $\\mathcal{O}(d \\ln d )$ hash computation time. Building on a recent\nresult (by Andoni, Indyk, Laarhoven, Razenshteyn, Schmidt, 2015), we show that\noptimal asymptotic sensitivity for cross-polytope LSH is retained even when the\ndense Gaussian matrix is replaced by a fast Johnson-Lindenstrauss transform\nfollowed by discrete pseudo-rotation, reducing the hash computation time from\n$\\mathcal{O}(d^2)$ to $\\mathcal{O}(d \\ln d )$. Moreover, our scheme achieves\nthe optimal rate of convergence for sensitivity. By incorporating a\nlow-randomness Johnson-Lindenstrauss transform, our scheme can be modified to\nrequire only $\\mathcal{O}(\\ln^9(d))$ random bits\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 20:24:03 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 16:22:35 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 23:46:21 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Kennedy", "Christopher", ""], ["Ward", "Rachel", ""]]}, {"id": "1602.06929", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli\n  and Aaron Sidford", "title": "Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample\n  Guarantees for Oja's Algorithm", "comments": "Updated title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides improved guarantees for streaming principle component\nanalysis (PCA). Given $A_1, \\ldots, A_n\\in \\mathbb{R}^{d\\times d}$ sampled\nindependently from distributions satisfying $\\mathbb{E}[A_i] = \\Sigma$ for\n$\\Sigma \\succeq \\mathbf{0}$, this work provides an $O(d)$-space linear-time\nsingle-pass streaming algorithm for estimating the top eigenvector of $\\Sigma$.\nThe algorithm nearly matches (and in certain cases improves upon) the accuracy\nobtained by the standard batch method that computes top eigenvector of the\nempirical covariance $\\frac{1}{n} \\sum_{i \\in [n]} A_i$ as analyzed by the\nmatrix Bernstein inequality. Moreover, to achieve constant accuracy, our\nalgorithm improves upon the best previous known sample complexities of\nstreaming algorithms by either a multiplicative factor of $O(d)$ or\n$1/\\mathrm{gap}$ where $\\mathrm{gap}$ is the relative distance between the top\ntwo eigenvalues of $\\Sigma$.\n  These results are achieved through a novel analysis of the classic Oja's\nalgorithm, one of the oldest and most popular algorithms for streaming PCA. In\nparticular, this work shows that simply picking a random initial point $w_0$\nand applying the update rule $w_{i + 1} = w_i + \\eta_i A_i w_i$ suffices to\naccurately estimate the top eigenvector, with a suitable choice of $\\eta_i$. We\nbelieve our result sheds light on how to efficiently perform streaming PCA both\nin theory and in practice and we hope that our analysis may serve as the basis\nfor analyzing many variants and extensions of streaming PCA.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 20:30:37 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 17:45:51 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Jain", "Prateek", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1602.07008", "submitter": "Pavel Dourbal", "authors": "Pavel Dourbal", "title": "Synthesis of fast multiplication algorithms for arbitrary tensors", "comments": "79 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method of fast linear transform algorithm synthesis for an arbitrary\ntensor, matrix, or vector is proposed. The method is based on factorization of\na tensor and using the factors for building computational structures performing\nfast tensor - vector multiplication on a computer or dedicated hardware\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 01:27:54 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Dourbal", "Pavel", ""]]}, {"id": "1602.07013", "submitter": "Adam Karczmarz", "authors": "Pawe{\\l} Gawrychowski and Adam Karczmarz", "title": "Improved Bounds for Shortest Paths in Dense Distance Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing shortest paths in so-called dense distance\ngraphs. Every planar graph $G$ on $n$ vertices can be partitioned into a set of\n$O(n/r)$ edge-disjoint regions (called an $r$-division) with $O(r)$ vertices\neach, such that each region has $O(\\sqrt{r})$ vertices (called boundary\nvertices) in common with other regions. A dense distance graph of a region is a\ncomplete graph containing all-pairs distances between its boundary nodes. A\ndense distance graph of an $r$-division is the union of the $O(n/r)$ dense\ndistance graphs of the individual pieces. Since the introduction of dense\ndistance graphs by Fakcharoenphol and Rao, computing single-source shortest\npaths in dense distance graphs has found numerous applications in fundamental\nplanar graph algorithms.\n  Fakcharoenphol and Rao proposed an algorithm (later called FR-Dijkstra) for\ncomputing single-source shortest paths in a dense distance graph in\n$O\\left(\\frac{n}{\\sqrt{r}}\\log{n}\\log{r}\\right)$ time. We show an\n$O\\left(\\frac{n}{\\sqrt{r}}\\left(\\frac{\\log^2{r}}{\\log^2\\log{r}}+\\log{n}\\log^{\\epsilon}{r}\\right)\\right)$\ntime algorithm for this problem, which is the first improvement to date over\nFR-Dijkstra for the important case when $r$ is polynomial in $n$. In this case,\nour algorithm is faster by a factor of $O(\\log^2{\\log{n}})$ and implies\nimproved upper bounds for such planar graph problems as multiple-source\nmultiple-sink maximum flow, single-source all-sinks maximum flow, and (dynamic)\nexact distance oracles.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 02:28:06 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 12:10:46 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Karczmarz", "Adam", ""]]}, {"id": "1602.07040", "submitter": "Eleni Rozaki", "authors": "Eleni Rozaki", "title": "Clustering Optimisation Techniques in Mobile Networks", "comments": "8 pages, 4 figures", "journal-ref": "(IJRITCC), February 2016, Volume 4, Issue 2, PP:22-29", "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of mobile phones has exploded over the past years,abundantly through\nthe introduction of smartphones and the rapidly expanding use of mobile data.\nThis has resulted in a spiraling problem of ensuring quality of service for\nusers of mobile networks. Hence, mobile carriers and service providers need to\ndetermine how to prioritise expansion decisions and optimise network faults to\nensure customer satisfaction and optimal network performance. To assist in that\ndecision-making process, this research employs data mining classification of\ndifferent Key Performance Indicator datasets to develop a monitoring scheme for\nmobile networks as a means of identifying the causes of network malfunctions.\nThen, the data are clustered to observe the characteristics of the technical\nareas with the use of k-means clustering. The data output is further trained\nwith decision tree classification algorithms. The end result was that this\nmethod of network optimisation allowed for significantly improved fault\ndetection performance\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 14:17:05 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Rozaki", "Eleni", ""]]}, {"id": "1602.07106", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "Scalable Generation of Scale-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain how massive instances of scale-free graphs following the\nBarabasi-Albert model can be generated very quickly in an embarrassingly\nparallel way. This makes this popular model available for studying big data\ngraph problems. As a demonstration, we generated a Petaedge graph in less than\nan hour.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 10:21:06 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1602.07154", "submitter": "Christoph D\\\"urr", "authors": "Christoph D\\\"urr, Christian Konrad, Marc Renault", "title": "On the Power of Advice and Randomization for Online Bipartite Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While randomized online algorithms have access to a sequence of uniform\nrandom bits, deterministic online algorithms with advice have access to a\nsequence of advice bits, i.e., bits that are set by an all powerful oracle\nprior to the processing of the request sequence. Advice bits are at least as\nhelpful as random bits, but how helpful are they? In this work, we investigate\nthe power of advice bits and random bits for online maximum bipartite matching\n(MBM).\n  The well-known Karp-Vazirani-Vazirani algorithm is an optimal randomized\n$(1-\\frac{1}{e})$-competitive algorithm for \\textsc{MBM} that requires access\nto $\\Theta(n \\log n)$ uniform random bits. We show that\n$\\Omega(\\log(\\frac{1}{\\epsilon}) n)$ advice bits are necessary and\n$O(\\frac{1}{\\epsilon^5} n)$ sufficient in order to obtain a\n$(1-\\epsilon)$-competitive deterministic advice algorithm. Furthermore, for a\nlarge natural class of deterministic advice algorithms, we prove that\n$\\Omega(\\log \\log \\log n)$ advice bits are required in order to improve on the\n$\\frac{1}{2}$-competitiveness of the best deterministic online algorithm, while\nit is known that $O(\\log n)$ bits are sufficient.\n  Last, we give a randomized online algorithm that uses $c n$ random bits, for\nintegers $c \\ge 1$, and a competitive ratio that approaches $1-\\frac{1}{e}$\nvery quickly as $c$ is increasing. For example if $c = 10$, then the difference\nbetween $1-\\frac{1}{e}$ and the achieved competitive ratio is less than\n$0.0002$.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 13:54:54 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 14:06:58 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["D\u00fcrr", "Christoph", ""], ["Konrad", "Christian", ""], ["Renault", "Marc", ""]]}, {"id": "1602.07194", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner and Ulrike von Luxburg", "title": "Lens depth function and k-relative neighborhood graph: versatile tools\n  for ordinal data analysis", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(58):1-52, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years it has become popular to study machine learning problems in a\nsetting of ordinal distance information rather than numerical distance\nmeasurements. By ordinal distance information we refer to binary answers to\ndistance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine\nlearning and statistics it is unclear how to solve them in such a scenario. Up\nto now, the main approach is to explicitly construct an ordinal embedding of\nthe data points in the Euclidean space, an approach that has a number of\ndrawbacks. In this paper, we propose algorithms for the problems of medoid\nestimation, outlier identification, classification, and clustering when given\nonly ordinal data. They are based on estimating the lens depth function and the\n$k$-relative neighborhood graph on a data set. Our algorithms are simple, are\nmuch faster than an ordinal embedding approach and avoid some of its drawbacks,\nand can easily be parallelized.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:30:46 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 11:52:01 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1602.07195", "submitter": "Sharayu Moharir", "authors": "Rahul Vaze and Sharayu Moharir", "title": "Paging with Multiple Caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern content delivery networks consist of one or more back-end servers\nwhich store the entire content catalog, assisted by multiple front-end servers\nwith limited storage and service capacities located near the end-users.\nAppropriate replication of content on the front-end servers is key to maximize\nthe fraction of requests served by the front-end servers. Motivated by this, a\nmultiple cache variant of the classical single cache paging problem is studied,\nwhich is referred to as the Multiple Cache Paging (MCP) problem. In each\ntime-slot, a batch of content requests arrive that have to be served by a bank\nof caches, and each cache can serve exactly one request. If a content is not\nfound in the bank, it is fetched from the back-end server, and one currently\nstored content is ejected, and counted as fault. As in the classical paging\nproblem, the goal is to minimize the total number of faults. The competitive\nratio of any online algorithm for the MCP problem is shown to be unbounded for\narbitrary input, thus concluding that the MCP problem is fundamentally\ndifferent from the classical paging problem. Consequently, stochastic arrivals\nsetting is considered, where requests arrive according to a known/unknown\nstochastic process. It is shown that near optimal performance can be achieved\nwith simple policies that require no co-ordination across the caches.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:32:30 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Vaze", "Rahul", ""], ["Moharir", "Sharayu", ""]]}, {"id": "1602.07210", "submitter": "Nils Kriege", "authors": "Andre Droschinsky, Nils M. Kriege, Petra Mutzel", "title": "Faster Algorithms for the Maximum Common Subtree Isomorphism Problem", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.MFCS.2016.33", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum common subtree isomorphism problem asks for the largest possible\nisomorphism between subtrees of two given input trees. This problem is a\nnatural restriction of the maximum common subgraph problem, which is ${\\sf\nNP}$-hard in general graphs. Confining to trees renders polynomial time\nalgorithms possible and is of fundamental importance for approaches on more\ngeneral graph classes. Various variants of this problem in trees have been\nintensively studied. We consider the general case, where trees are neither\nrooted nor ordered and the isomorphism is maximum w.r.t. a weight function on\nthe mapped vertices and edges. For trees of order $n$ and maximum degree\n$\\Delta$ our algorithm achieves a running time of $\\mathcal{O}(n^2\\Delta)$ by\nexploiting the structure of the matching instances arising as subproblems. Thus\nour algorithm outperforms the best previously known approaches. No faster\nalgorithm is possible for trees of bounded degree and for trees of unbounded\ndegree we show that a further reduction of the running time would directly\nimprove the best known approach to the assignment problem. Combining a\npolynomial-delay algorithm for the enumeration of all maximum common subtree\nisomorphisms with central ideas of our new algorithm leads to an improvement of\nits running time from $\\mathcal{O}(n^6+Tn^2)$ to $\\mathcal{O}(n^3+Tn\\Delta)$,\nwhere $n$ is the order of the larger tree, $T$ is the number of different\nsolutions, and $\\Delta$ is the minimum of the maximum degrees of the input\ntrees. Our theoretical results are supplemented by an experimental evaluation\non synthetic and real-world instances.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:48:25 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 15:32:22 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 15:01:42 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 15:08:46 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Droschinsky", "Andre", ""], ["Kriege", "Nils M.", ""], ["Mutzel", "Petra", ""]]}, {"id": "1602.07241", "submitter": "Tatiana Starikovskaya", "authors": "Raphael Clifford, Tatiana Starikovskaya", "title": "Approximate Hamming distance in a stream", "comments": "Submitted to ICALP' 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a $(1+\\epsilon)$-approximation of the\nHamming distance between a pattern of length $n$ and successive substrings of a\nstream. We first look at the one-way randomised communication complexity of\nthis problem, giving Alice the first half of the stream and Bob the second\nhalf. We show the following: (1) If Alice and Bob both share the pattern then\nthere is an $O(\\epsilon^{-4} \\log^2 n)$ bit randomised one-way communication\nprotocol. (2) If only Alice has the pattern then there is an\n$O(\\epsilon^{-2}\\sqrt{n}\\log n)$ bit randomised one-way communication protocol.\n  We then go on to develop small space streaming algorithms for\n$(1+\\epsilon)$-approximate Hamming distance which give worst case running time\nguarantees per arriving symbol. (1) For binary input alphabets there is an\n$O(\\epsilon^{-3} \\sqrt{n} \\log^{2} n)$ space and $O(\\epsilon^{-2} \\log{n})$\ntime streaming $(1+\\epsilon)$-approximate Hamming distance algorithm. (2) For\ngeneral input alphabets there is an $O(\\epsilon^{-5} \\sqrt{n} \\log^{4} n)$\nspace and $O(\\epsilon^{-4} \\log^3 {n})$ time streaming\n$(1+\\epsilon)$-approximate Hamming distance algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 17:21:26 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Clifford", "Raphael", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1602.07351", "submitter": "Xin Li", "authors": "Amitabh Basu, Michael Dinitz, Xin Li", "title": "Computing approximate PSD factorizations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for computing approximate PSD factorizations of\nnonnegative matrices. The running time of the algorithm is polynomial in the\ndimensions of the input matrix, but exponential in the PSD rank and the\napproximation error. The main ingredient is an exact factorization algorithm\nwhen the rows and columns of the factors are constrained to lie in a general\npolyhedron. This strictly generalizes nonnegative matrix factorizations which\ncan be captured by letting this polyhedron to be the nonnegative orthant.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 23:09:36 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Basu", "Amitabh", ""], ["Dinitz", "Michael", ""], ["Li", "Xin", ""]]}, {"id": "1602.07422", "submitter": "Adam Kasperski", "authors": "Mikita Hradovich, Adam Kasperski, Pawel Zielinski", "title": "The robust recoverable spanning tree problem with interval costs is\n  polynomially solvable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the robust recoverable spanning tree problem with interval edge\ncosts is considered. The complexity of this problem has remained open to date.\nIt is shown that the problem is polynomially solvable, by using an iterative\nrelaxation method. A generalization of this idea to the robust recoverable\nmatroid basis problem is also presented. Polynomial algorithms for both robust\nrecoverable problems are proposed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 07:29:06 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 09:32:20 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Hradovich", "Mikita", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1602.07424", "submitter": "Matteo Riondato", "authors": "Lorenzo De Stefani, Alessandro Epasto, Matteo Riondato, Eli Upfal", "title": "TRI\\`EST: Counting Local and Global Triangles in Fully-dynamic Streams\n  with Fixed Memory Size", "comments": "49 pages, 7 figures, extended version of the paper appeared at ACM\n  KDD'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TRI\\`EST, a suite of one-pass streaming algorithms to compute\nunbiased, low-variance, high-quality approximations of the global and local\n(i.e., incident to each vertex) number of triangles in a fully-dynamic graph\nrepresented as an adversarial stream of edge insertions and deletions. Our\nalgorithms use reservoir sampling and its variants to exploit the\nuser-specified memory space at all times. This is in contrast with previous\napproaches which use hard-to-choose parameters (e.g., a fixed sampling\nprobability) and offer no guarantees on the amount of memory they will use. We\nshow a full analysis of the variance of the estimations and novel concentration\nbounds for these quantities. Our experimental results on very large graphs show\nthat TRI\\`EST outperforms state-of-the-art approaches in accuracy and exhibits\na small update time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 07:39:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 18:51:06 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["De Stefani", "Lorenzo", ""], ["Epasto", "Alessandro", ""], ["Riondato", "Matteo", ""], ["Upfal", "Eli", ""]]}, {"id": "1602.07504", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Pinar Heggernes and Dieter Kratsch", "title": "Enumeration and Maximum Number of Minimal Connected Vertex Covers in\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected Vertex Cover is one of the classical problems of computer science,\nalready mentioned in the monograph of Garey and Johnson. Although the\noptimization and decision variants of finding connected vertex covers of\nminimum size or weight are well studied, surprisingly there is no work on the\nenumeration or maximum number of minimal connected vertex covers of a graph. In\nthis paper we show that the maximum number of minimal connected vertex covers\nof a graph is at most 1.8668^n, and these can be enumerated in time\nO(1.8668^n). For graphs of chordality at most 5, we are able to give a better\nupper bound, and for chordal graphs and distance-hereditary graphs we are able\nto give tight bounds on the maximum number of minimal connected vertex covers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 13:50:01 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Kratsch", "Dieter", ""]]}, {"id": "1602.07570", "submitter": "Aleksandrs Slivkins", "authors": "Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, Zhiwei Steven\n  Wu", "title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "comments": "All revisions focused on presentation; all results (except Appendix\n  C) have been present since the initial version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a ubiquitous scenario in the Internet economy when individual\ndecision-makers (henceforth, agents) both produce and consume information as\nthey make strategic choices in an uncertain environment. This creates a\nthree-way tradeoff between exploration (trying out insufficiently explored\nalternatives to help others in the future), exploitation (making optimal\ndecisions given the information discovered by other agents), and incentives of\nthe agents (who are myopically interested in exploitation, while preferring the\nothers to explore). We posit a principal who controls the flow of information\nfrom agents that came before, and strives to coordinate the agents towards a\nsocially optimal balance between exploration and exploitation, not using any\nmonetary transfers. The goal is to design a recommendation policy for the\nprincipal which respects agents' incentives and minimizes a suitable notion of\nregret.\n  We extend prior work in this direction to allow the agents to interact with\none another in a shared environment: at each time step, multiple agents arrive\nto play a Bayesian game, receive recommendations, choose their actions, receive\ntheir payoffs, and then leave the game forever. The agents now face two sources\nof uncertainty: the actions of the other agents and the parameters of the\nuncertain game environment.\n  Our main contribution is to show that the principal can achieve constant\nregret when the utilities are deterministic (where the constant depends on the\nprior distribution, but not on the time horizon), and logarithmic regret when\nthe utilities are stochastic. As a key technical tool, we introduce the concept\nof explorable actions, the actions which some incentive-compatible policy can\nrecommend with non-zero probability. We show how the principal can identify\n(and explore) all explorable actions, and use the revealed information to\nperform optimally.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 15:57:28 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 00:53:19 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 17:48:48 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 02:23:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mansour", "Yishay", ""], ["Slivkins", "Aleksandrs", ""], ["Syrgkanis", "Vasilis", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1602.07616", "submitter": "Anindya De", "authors": "Anindya De and Michael Saks and Sijian Tang", "title": "Noisy population recovery in polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the noisy population recovery problem of Dvir et al., the goal is to learn\nan unknown distribution $f$ on binary strings of length $n$ from noisy samples.\nFor some parameter $\\mu \\in [0,1]$, a noisy sample is generated by flipping\neach coordinate of a sample from $f$ independently with probability\n$(1-\\mu)/2$. We assume an upper bound $k$ on the size of the support of the\ndistribution, and the goal is to estimate the probability of any string to\nwithin some given error $\\varepsilon$. It is known that the algorithmic\ncomplexity and sample complexity of this problem are polynomially related to\neach other.\n  We show that for $\\mu > 0$, the sample complexity (and hence the algorithmic\ncomplexity) is bounded by a polynomial in $k$, $n$ and $1/\\varepsilon$\nimproving upon the previous best result of $\\mathsf{poly}(k^{\\log\\log\nk},n,1/\\varepsilon)$ due to Lovett and Zhang.\n  Our proof combines ideas from Lovett and Zhang with a \\emph{noise attenuated}\nversion of M\\\"{o}bius inversion. In turn, the latter crucially uses the\nconstruction of \\emph{robust local inverse} due to Moitra and Saks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 17:46:30 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["De", "Anindya", ""], ["Saks", "Michael", ""], ["Tang", "Sijian", ""]]}, {"id": "1602.07726", "submitter": "Zhiwei Steven Wu", "authors": "Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, Zhiwei\n  Steven Wu", "title": "Adaptive Learning with Robust Generalization Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional notion of generalization---i.e., learning a hypothesis whose\nempirical error is close to its true error---is surprisingly brittle. As has\nrecently been noted in [DFH+15b], even if several algorithms have this\nguarantee in isolation, the guarantee need not hold if the algorithms are\ncomposed adaptively. In this paper, we study three notions of\ngeneralization---increasing in strength---that are robust to postprocessing and\namenable to adaptive composition, and examine the relationships between them.\nWe call the weakest such notion Robust Generalization. A second, intermediate,\nnotion is the stability guarantee known as differential privacy. The strongest\nguarantee we consider we call Perfect Generalization. We prove that every\nhypothesis class that is PAC learnable is also PAC learnable in a robustly\ngeneralizing fashion, with almost the same sample complexity. It was previously\nknown that differentially private algorithms satisfy robust generalization. In\nthis paper, we show that robust generalization is a strictly weaker concept,\nand that there is a learning task that can be carried out subject to robust\ngeneralization guarantees, yet cannot be carried out subject to differential\nprivacy. We also show that perfect generalization is a strictly stronger\nguarantee than differential privacy, but that, nevertheless, many learning\ntasks can be carried out subject to the guarantees of perfect generalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 21:59:30 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 00:07:01 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Cummings", "Rachel", ""], ["Ligett", "Katrina", ""], ["Nissim", "Kobbi", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1602.07750", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen, Wen-Hung Huang, Geoffrey Nelissen", "title": "A Note on Modeling Self-Suspending Time as Blocking Time in Real-Time\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents a proof to support the correctness of the schedulability\ntest for self-suspending real-time task systems proposed by Jane W. S. Liu in\nher book titled \"Real-Time Systems\" (Pages 164-165). The same concept was also\nimplicitly used by Rajkumar, Sha, and Lehoczky in RTSS 1988 (Page 267) for\nanalyzing self-suspending behaviour due to synchronization protocols in\nmultiprocessor systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 23:30:04 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Huang", "Wen-Hung", ""], ["Nelissen", "Geoffrey", ""]]}, {"id": "1602.07876", "submitter": "Jan Arne Telle", "authors": "Serge Gaspers, Christos Papadimitriou, Sigve Hortemo Saether, Jan Arne\n  Telle", "title": "On Satisfiability Problems with a Linear Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown \\cite{STV} that satisfiability is polynomially solvable\nwhen the incidence graph is an interval bipartite graph (an interval graph\nturned into a bipartite graph by omitting all edges within each partite set).\nHere we relax this condition in several directions: First, we show that it\nholds for $k$-interval bigraphs, bipartite graphs which can be converted to\ninterval bipartite graphs by adding to each node of one side at most $k$ edges;\nthe same result holds for the counting and the weighted maximization version of\nsatisfiability. Second, given two linear orders, one for the variables and one\nfor the clauses, we show how to find, in polynomial time, the smallest $k$ such\nthat there is a $k$-interval bigraph compatible with these two orders. On the\nnegative side we prove that, barring complexity collapses, no such extensions\nare possible for CSPs more general than satisfiability. We also show\nNP-hardness of recognizing 1-interval bigraphs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 10:40:28 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Gaspers", "Serge", ""], ["Papadimitriou", "Christos", ""], ["Saether", "Sigve Hortemo", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1602.07985", "submitter": "Alexandros A. Voudouris", "authors": "Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris", "title": "How effective can simple ordinal peer grading be?", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal peer grading has been proposed as a simple and scalable solution for\ncomputing reliable information about student performance in massive open online\ncourses. The idea is to outsource the grading task to the students themselves\nas follows. After the end of an exam, each student is asked to rank -- in terms\nof quality -- a bundle of exam papers by fellow students. An aggregation rule\nthen combines the individual rankings into a global one that contains all\nstudents. We define a broad class of simple aggregation rules, which we call\ntype-ordering aggregation rules, and present a theoretical framework for\nassessing their effectiveness. When statistical information about the grading\nbehaviour of students is available (in terms of a noise matrix that\ncharacterizes the grading behaviour of the average student from a student\npopulation), the framework can be used to compute the optimal rule from this\nclass with respect to a series of performance objectives that compare the\nranking returned by the aggregation rule to the underlying ground truth\nranking. For example, a natural rule known as Borda is proved to be optimal\nwhen students grade correctly. In addition, we present extensive simulations\nthat validate our theory and prove it to be extremely accurate in predicting\nthe performance of aggregation rules even when only rough information about\ngrading behaviour (i.e., an approximation of the noise matrix) is available.\nBoth in the application of our theoretical framework and in our simulations, we\nexploit data about grading behaviour of students that have been extracted from\ntwo field experiments in the University of Patras.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 16:37:57 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 16:23:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Krimpas", "George A.", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1602.08034", "submitter": "Hiroki Morizumi", "authors": "Hiroki Morizumi", "title": "Zero-Suppressed Computation: A New Computation Inspired by ZDDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-suppressed binary decision diagrams (ZDDs) are a data structure\nrepresenting Boolean functions, and one of the most successful variants of\nbinary decision diagrams (BDDs). On the other hand, BDDs are also called\nbranching programs in computational complexity theory, and have been studied as\na computation model. In this paper, we consider ZDDs from the viewpoint of\ncomputational complexity theory. Our main proposal of this paper is that we\nregard the basic idea of ZDDs as a new computation, which we call\nzero-suppressed computation. We consider the zero-suppressed version of two\nclassical computation models, decision trees and branching programs, and show\nsome results. Although this paper is mainly written from the viewpoint of\ncomputational complexity theory, the concept of zero-suppressed computation can\nbe widely applied to various areas.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 18:58:28 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Morizumi", "Hiroki", ""]]}, {"id": "1602.08156", "submitter": "Chien-Chun Ni", "authors": "Chien-Chun Ni, Zhengyu Su, Jie Gao and Xianfeng David Gu", "title": "Capacitated Kinetic Clustering in Mobile Networks by Optimal\n  Transportation Theory", "comments": "9 pages, 10 figures. To be appear in INFOCOM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CG cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of capacitated kinetic clustering in which $n$ mobile\nterminals and $k$ base stations with respective operating capacities are given.\nThe task is to assign the mobile terminals to the base stations such that the\ntotal squared distance from each terminal to its assigned base station is\nminimized and the capacity constraints are satisfied. This paper focuses on the\ndevelopment of \\emph{distributed} and computationally efficient algorithms that\nadapt to the motion of both terminals and base stations. Suggested by the\noptimal transportation theory, we exploit the structural property of the\noptimal solution, which can be represented by a power diagram on the base\nstations such that the total usage of nodes within each power cell equals the\ncapacity of the corresponding base station. We show by using the kinetic data\nstructure framework the first analytical upper bound on the number of changes\nin the optimal solution, i.e., its stability. On the algorithm side, using the\npower diagram formulation we show that the solution can be represented in size\nproportional to the number of base stations and can be solved by an iterative,\nlocal algorithm. In particular, this algorithm can naturally exploit the\ncontinuity of motion and has orders of magnitude faster than existing solutions\nusing min-cost matching and linear programming, and thus is able to handle\nlarge scale data under mobility.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 00:11:42 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ni", "Chien-Chun", ""], ["Su", "Zhengyu", ""], ["Gao", "Jie", ""], ["Gu", "Xianfeng David", ""]]}, {"id": "1602.08162", "submitter": "Samira Daruki", "authors": "Amirali Abdullah, Samira Daruki, Chitradeep Dutta Roy, Suresh\n  Venkatasubramanian", "title": "Streaming Verification of Graph Properties", "comments": "26 pages, 2 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming interactive proofs (SIPs) are a framework for outsourced\ncomputation. A computationally limited streaming client (the verifier) hands\nover a large data set to an untrusted server (the prover) in the cloud and the\ntwo parties run a protocol to confirm the correctness of result with high\nprobability. SIPs are particularly interesting for problems that are hard to\nsolve (or even approximate) well in a streaming setting. The most notable of\nthese problems is finding maximum matchings, which has received intense\ninterest in recent years but has strong lower bounds even for constant factor\napproximations.\n  In this paper, we present efficient streaming interactive proofs that can\nverify maximum matchings exactly. Our results cover all flavors of matchings\n(bipartite/non-bipartite and weighted). In addition, we also present streaming\nverifiers for approximate metric TSP. In particular, these are the first\nefficient results for weighted matchings and for metric TSP in any streaming\nverification model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 01:16:51 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 00:48:12 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Abdullah", "Amirali", ""], ["Daruki", "Samira", ""], ["Roy", "Chitradeep Dutta", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1602.08166", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang and Tsvi Kopelowitz and Seth Pettie", "title": "An Exponential Separation Between Randomized and Deterministic\n  Complexity in the LOCAL Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past 30 years numerous algorithms have been designed for symmetry\nbreaking problems in the LOCAL model, such as maximal matching, MIS, vertex\ncoloring, and edge-coloring. For most problems the best randomized algorithm is\nat least exponentially faster than the best deterministic algorithm. In this\npaper we prove that these exponential gaps are necessary and establish\nconnections between the deterministic and randomized complexities in the LOCAL\nmodel. Each result has a very compelling take-away message:\n  1. Fast $\\Delta$-coloring of trees requires random bits: Building on the\nrecent lower bounds of Brandt et al., we prove that the randomized complexity\nof $\\Delta$-coloring a tree with maximum degree $\\Delta\\ge 55$ is\n$\\Theta(\\log_\\Delta\\log n)$, whereas its deterministic complexity is\n$\\Theta(\\log_\\Delta n)$ for any $\\Delta\\ge 3$. This also establishes a large\nseparation between the deterministic complexity of $\\Delta$-coloring and\n$(\\Delta+1)$-coloring trees.\n  2. Randomized lower bounds imply deterministic lower bounds: We prove that\nany deterministic algorithm for a natural class of problems that runs in\n$O(1)+o(\\log_\\Delta n)$ rounds can be transformed to run in\n$O(\\log^*n-\\log^*\\Delta+1)$ rounds. If the transformed algorithm violates a\nlower bound (even allowing randomization), then one can conclude that the\nproblem requires $\\Omega(\\log_\\Delta n)$ time deterministically.\n  3. Deterministic lower bounds imply randomized lower bounds: We prove that\nthe randomized complexity of any natural problem on instances of size $n$ is at\nleast its deterministic complexity on instances of size $\\sqrt{\\log n}$. This\nshows that a deterministic $\\Omega(\\log_\\Delta n)$ lower bound for any problem\nimplies a randomized $\\Omega(\\log_\\Delta\\log n)$ lower bound. It also\nillustrates that the graph shattering technique is absolutely essential to the\nLOCAL model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 01:31:56 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 00:11:49 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Kopelowitz", "Tsvi", ""], ["Pettie", "Seth", ""]]}, {"id": "1602.08230", "submitter": "Matthias Mnich", "authors": "Matthias Mnich and Ildik\\'o Schlotter", "title": "Stable Marriage with Covering Constraints: A Complete Computational\n  Trichotomy", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Stable Marriage with Covering Constraints (SMC): in this variant\nof Stable Marriage, we distinguish a subset of women as well as a subset of\nmen, and we seek a matching with fewest number of blocking pairs that matches\nall of the distinguished people. We investigate how a set of natural\nparameters, namely the maximum length of preference lists for men and women,\nthe number of distinguished men and women, and the number of blocking pairs\nallowed determine the computational tractability of this problem. Our main\nresult is a complete complexity trichotomy that, for each choice of the studied\nparameters, classifies SMC as polynomial-time solvable, NP-hard and\nfixed-parameter tractable, or NP-hard and W[1]-hard. We also classify all cases\nof one-sided constraints where only women may be distinguished.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 08:07:11 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 16:22:36 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 16:34:29 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Mnich", "Matthias", ""], ["Schlotter", "Ildik\u00f3", ""]]}, {"id": "1602.08254", "submitter": "Melanie Schmidt", "authors": "Johannes Bl\\\"omer, Christiane Lammersen, Melanie Schmidt, Christian\n  Sohler", "title": "Theoretical Analysis of the $k$-Means Algorithm - A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is one of the most widely used clustering heuristics.\nDespite its simplicity, analyzing its running time and quality of approximation\nis surprisingly difficult and can lead to deep insights that can be used to\nimprove the algorithm. In this paper we survey the recent results in this\ndirection as well as several extension of the basic $k$-means method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 09:39:50 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Lammersen", "Christiane", ""], ["Schmidt", "Melanie", ""], ["Sohler", "Christian", ""]]}, {"id": "1602.08268", "submitter": "Marc Hellmuth", "authors": "Marc Hellmuth and Nicolas Wieseke", "title": "Construction of Gene and Species Trees from Sequence Data incl.\n  Orthologs, Paralogs, and Xenologs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic reconstruction aims at finding plausible hypotheses of the\nevolutionary history of genes or species based on genomic sequence information.\nThe distinction of orthologous genes (genes that having a common ancestry and\ndiverged after a speciation) is crucial and lies at the heart of many genomic\nstudies. However, existing methods that rely only on 1:1 orthologs to infer\nspecies trees are strongly restricted to a small set of allowed genes that\nprovide information about the species tree. The use of larger gene sets that\nconsist in addition of non-orthologous genes (e.g. so-called paralogous or\nxenologous genes) considerably increases the information about the evolutionary\nhistory of the respective species. In this work, we introduce a novel method to\ncompute species phylogenies based on sequence data including orthologs,\nparalogs or even xenologs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 10:23:26 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Hellmuth", "Marc", ""], ["Wieseke", "Nicolas", ""]]}, {"id": "1602.08298", "submitter": "William Moses Jr.", "authors": "John Augustine, William K. Moses Jr., Amanda Redlich and Eli Upfal", "title": "Balanced Allocation: Patience is not a Virtue", "comments": "26 pages, preliminary version accepted at SODA 2016", "journal-ref": "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on\n  Discrete Algorithms (SODA 2016), 655-671", "doi": "10.1137/1.9781611974331.ch48", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing is a well-studied problem, with balls-in-bins being the\nprimary framework. The greedy algorithm $\\mathsf{Greedy}[d]$ of Azar et al.\nplaces each ball by probing $d > 1$ random bins and placing the ball in the\nleast loaded of them. With high probability, the maximum load under\n$\\mathsf{Greedy}[d]$ is exponentially lower than the result when balls are\nplaced uniformly randomly. V\\\"ocking showed that a slightly asymmetric variant,\n$\\mathsf{Left}[d]$, provides a further significant improvement. However, this\nimprovement comes at an additional computational cost of imposing structure on\nthe bins.\n  Here, we present a fully decentralized and easy-to-implement algorithm called\n$\\mathsf{FirstDiff}[d]$ that combines the simplicity of $\\mathsf{Greedy}[d]$\nand the improved balance of $\\mathsf{Left}[d]$. The key idea in\n$\\mathsf{FirstDiff}[d]$ is to probe until a different bin size from the first\nobservation is located, then place the ball. Although the number of probes\ncould be quite large for some of the balls, we show that\n$\\mathsf{FirstDiff}[d]$ requires only at most $d$ probes on average per ball\n(in both the standard and the heavily-loaded settings). Thus the number of\nprobes is no greater than either that of $\\mathsf{Greedy}[d]$ or\n$\\mathsf{Left}[d]$. More importantly, we show that $\\mathsf{FirstDiff}[d]$\nclosely matches the improved maximum load ensured by $\\mathsf{Left}[d]$ in both\nthe standard and heavily-loaded settings. We further provide a tight lower\nbound on the maximum load up to $O(\\log \\log \\log n)$ terms. We additionally\ngive experimental data that $\\mathsf{FirstDiff}[d]$ is indeed as good as\n$\\mathsf{Left}[d]$, if not better, in practice.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 12:45:25 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 20:23:49 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Augustine", "John", ""], ["Moses", "William K.", "Jr."], ["Redlich", "Amanda", ""], ["Upfal", "Eli", ""]]}, {"id": "1602.08357", "submitter": "Samantha Petti", "authors": "Christos Papadimitrou, Samantha Petti, Santosh Vempala", "title": "Cortical Computation via Iterative Constructions", "comments": "40 pages, COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Boolean functions of an arbitrary number of input variables that can\nbe realized by simple iterative constructions based on constant-size\nprimitives. This restricted type of construction needs little global\ncoordination or control and thus is a candidate for neurally feasible\ncomputation. Valiant's construction of a majority function can be realized in\nthis manner and, as we show, can be generalized to any uniform threshold\nfunction. We study the rate of convergence, finding that while linear\nconvergence to the correct function can be achieved for any threshold using a\nfixed set of primitives, for quadratic convergence, the size of the primitives\nmust grow as the threshold approaches 0 or 1. We also study finite realizations\nof this process and the learnability of the functions realized. We show that\nthe constructions realized are accurate outside a small interval near the\ntarget threshold, where the size of the construction grows as the inverse\nsquare of the interval width. This phenomenon, that errors are higher closer to\nthresholds (and thresholds closer to the boundary are harder to represent), is\na well-known cognitive finding.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 15:01:08 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 03:16:18 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Papadimitrou", "Christos", ""], ["Petti", "Samantha", ""], ["Vempala", "Santosh", ""]]}, {"id": "1602.08361", "submitter": "Sebastien Tixeuil", "authors": "Pierre Courtieu (CEDRIC), Lionel Rieg, S\\'ebastien Tixeuil (LINCS,\n  NPA, IUF), Xavier Urbain (ENSIIE, LRI)", "title": "Certified Universal Gathering in $R^2$ for Oblivious Mobile Robots", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.01603", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified formal framework for expressing mobile robots models,\nprotocols, and proofs, and devise a protocol design/proof methodology dedicated\nto mobile robots that takes advantage of this formal framework. As a case\nstudy, we present the first formally certified protocol for oblivious mobile\nrobots evolving in a two-dimensional Euclidean space. In more details, we\nprovide a new algorithm for the problem of universal gathering mobile oblivious\nrobots (that is, starting from any initial configuration that is not bivalent,\nusing any number of robots, the robots reach in a finite number of steps the\nsame position, not known beforehand) without relying on a common orientation\nnor chirality. We give very strong guaranties on the correctness of our\nalgorithm by proving formally that it is correct, using the COQ proof\nassistant. This result demonstrates both the effectiveness of the approach to\nobtain new algorithms that use as few assumptions as necessary, and its\nmanageability since the amount of developed code remains human readable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 15:16:21 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Courtieu", "Pierre", "", "CEDRIC"], ["Rieg", "Lionel", "", "LINCS,\n  NPA, IUF"], ["Tixeuil", "S\u00e9bastien", "", "LINCS,\n  NPA, IUF"], ["Urbain", "Xavier", "", "ENSIIE, LRI"]]}, {"id": "1602.08369", "submitter": "Mikael Gast", "authors": "Mikael Gast, Mathias Hauptmann and Marek Karpinski", "title": "Approximation Complexity of Max-Cut on Power Law Graphs", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the MAX-CUT problem on power law graphs (PLGs) with\npower law exponent $\\beta$. We prove some new approximability results on that\nproblem. In particular we show that there exist polynomial time approximation\nschemes (PTAS) for MAX-CUT on PLGs for the power law exponent $\\beta$ in the\ninterval $(0,2)$. For $\\beta>2$ we show that for some $\\epsilon>0$, MAX-CUT is\nNP-hard to approximate within approximation ratio $1+\\epsilon$, ruling out the\nexistence of a PTAS in this case. Moreover we give an approximation algorithm\nwith improved constant approximation ratio for the case of $\\beta>2$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 15:32:29 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Gast", "Mikael", ""], ["Hauptmann", "Mathias", ""], ["Karpinski", "Marek", ""]]}, {"id": "1602.08371", "submitter": "Daniel Neuen", "authors": "Daniel Neuen", "title": "Graph Isomorphism for unit square graphs", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades for more and more graph classes the Graph Isomorphism\nProblem was shown to be solvable in polynomial time. An interesting family of\ngraph classes arises from intersection graphs of geometric objects. In this\nwork we show that the Graph Isomorphism Problem for unit square graphs,\nintersection graphs of axis-parallel unit squares in the plane, can be solved\nin polynomial time. Since the recognition problem for this class of graphs is\nNP-hard we can not rely on standard techniques for geometric graphs based on\nconstructing a canonical realization. Instead, we develop new techniques which\ncombine structural insights into the class of unit square graphs with\nunderstanding of the automorphism group of such graphs. For the latter we\nintroduce a generalization of bounded degree graphs which is used to capture\nthe main structure of unit square graphs. Using group theoretic algorithms we\nobtain sufficient information to solve the isomorphism problem for unit square\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 15:36:43 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 14:32:51 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Neuen", "Daniel", ""]]}, {"id": "1602.08393", "submitter": "Anshumali Shrivastava", "authors": "Anshumali Shrivastava", "title": "Exact Weighted Minwise Hashing in Constant Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, $O(d)$ ($d$ is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes $k$ independent and unbiased WMH in\ntime $O(k)$ instead of $O(dk)$ required by Ioffe's method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around $64$ bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe's method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient \"densified\" one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 16:55:08 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Shrivastava", "Anshumali", ""]]}, {"id": "1602.08481", "submitter": "Peter Robinson", "authors": "Gopal Pandurangan, Peter Robinson, Michele Scquizzato", "title": "On the Distributed Complexity of Large-Scale Graph Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing need to understand the distributed algorithmic\nfoundations of large-scale graph computations, we study some fundamental graph\nproblems in a message-passing model for distributed computing where $k \\geq 2$\nmachines jointly perform computations on graphs with $n$ nodes (typically, $n\n\\gg k$). The input graph is assumed to be initially randomly partitioned among\nthe $k$ machines, a common implementation in many real-world systems.\nCommunication is point-to-point, and the goal is to minimize the number of\ncommunication {\\em rounds} of the computation.\n  Our main contribution is the {\\em General Lower Bound Theorem}, a theorem\nthat can be used to show non-trivial lower bounds on the round complexity of\ndistributed large-scale data computations. The General Lower Bound Theorem is\nestablished via an information-theoretic approach that relates the round\ncomplexity to the minimal amount of information required by machines to solve\nthe problem. Our approach is generic and this theorem can be used in a\n\"cookbook\" fashion to show distributed lower bounds in the context of several\nproblems, including non-graph problems. We present two applications by showing\n(almost) tight lower bounds for the round complexity of two fundamental graph\nproblems, namely {\\em PageRank computation} and {\\em triangle enumeration}. Our\napproach, as demonstrated in the case of PageRank, can yield tight lower bounds\nfor problems (including, and especially, under a stochastic partition of the\ninput) where communication complexity techniques are not obvious.\n  Our approach, as demonstrated in the case of triangle enumeration, can yield\nstronger round lower bounds as well as message-round tradeoffs compared to\napproaches that use communication complexity techniques.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 20:57:30 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 23:42:15 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 17:46:11 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 10:26:59 GMT"}, {"version": "v5", "created": "Sat, 25 Nov 2017 15:59:14 GMT"}, {"version": "v6", "created": "Thu, 17 May 2018 00:10:05 GMT"}, {"version": "v7", "created": "Thu, 26 Jul 2018 13:58:09 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""], ["Scquizzato", "Michele", ""]]}, {"id": "1602.08515", "submitter": "Qie He", "authors": "Shabbir Ahmed, Qie He, Shi Li, George Nemhauser", "title": "On the computational complexity of minimum-concave-cost flow in a\n  two-dimensional grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum-concave-cost flow problem on a two-dimensional grid. We\ncharacterize the computational complexity of this problem based on the number\nof rows and columns of the grid, the number of different capacities over all\narcs, and the location of sources and sinks. The concave cost over each arc is\nassumed to be evaluated through an oracle machine, i.e., the oracle machine\nreturns the cost over an arc in a single computational step, given the flow\nvalue and the arc index. We propose an algorithm whose running time is\npolynomial in the number of columns of the grid, for the following cases: (1)\nthe grid has a constant number of rows, a constant number of different\ncapacities over all arcs, and sources and sinks in at most two rows; (2) the\ngrid has two rows and a constant number of different capacities over all arcs\nconnecting rows; (3) the grid has a constant number of rows and all sources in\none row, with infinite capacity over each arc. These are presumably the most\ngeneral polynomially solvable cases, since we show the problem becomes NP-hard\nwhen any condition in these cases is removed. Our results apply to abundant\nvariants and generalizations of the dynamic lot sizing model, and answer\nseveral questions raised in serial supply chain optimization.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 21:52:10 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Ahmed", "Shabbir", ""], ["He", "Qie", ""], ["Li", "Shi", ""], ["Nemhauser", "George", ""]]}, {"id": "1602.08519", "submitter": "Samuel Hetterich", "authors": "Samuel Hetterich", "title": "Analysing Survey Propagation Guided Decimation on Random Formulas", "comments": "arXiv admin note: substantial text overlap with arXiv:1007.1328 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\varPhi$ be a uniformly distributed random $k$-SAT formula with $n$\nvariables and $m$ clauses. For clauses/variables ratio $m/n \\leq\nr_{k\\text{-SAT}} \\sim 2^k\\ln2$ the formula $\\varPhi$ is satisfiable with high\nprobability. However, no efficient algorithm is known to provably find a\nsatisfying assignment beyond $m/n \\sim 2k \\ln(k)/k$ with a non-vanishing\nprobability. Non-rigorous statistical mechanics work on $k$-CNF led to the\ndevelopment of a new efficient \"message passing algorithm\" called \\emph{Survey\nPropagation Guided Decimation} [M\\'ezard et al., Science 2002]. Experiments\nconducted for $k=3,4,5$ suggest that the algorithm finds satisfying assignments\nclose to $r_{k\\text{-SAT}}$. However, in the present paper we prove that the\nbasic version of Survey Propagation Guided Decimation fails to solve random\n$k$-SAT formulas efficiently already for $m/n=2^k(1+\\varepsilon_k)\\ln(k)/k$\nwith $\\lim_{k\\to\\infty}\\varepsilon_k= 0$ almost a factor $k$ below\n$r_{k\\text{-SAT}}$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 11:32:25 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Hetterich", "Samuel", ""]]}, {"id": "1602.08563", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Jan Marcinkowski, Maciej Pacut, Stefan Schmid,\n  Aleksandra Spyra", "title": "Online Tree Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of a natural and practically relevant new variant of\nonline caching where the to-be-cached items can have dependencies. We assume\nthat the universe is a tree T and items are tree nodes; we require that if a\nnode v is cached then the whole subtree T(v) rooted at v is cached as well.\nThis theoretical problem finds an immediate application in the context of\nforwarding table optimization in IP routing and software-defined networks.\n  We present an elegant online deterministic algorithm TC for this problem, and\nrigorously prove that its competitive ratio is O(height(T) *\nk_ALG/(k_ALG-k_OPT+1)), where k_ALG and k_OPT denote the cache sizes of an\nonline and the optimal offline algorithm, respectively. The result is optimal\nup to a factor of O(height(T)).\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 07:35:49 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 23:17:40 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Marcinkowski", "Jan", ""], ["Pacut", "Maciej", ""], ["Schmid", "Stefan", ""], ["Spyra", "Aleksandra", ""]]}, {"id": "1602.08730", "submitter": "David Harris", "authors": "David G. Harris and Aravind Srinivasan", "title": "Improved bounds and algorithms for graph cuts and network reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karger (SIAM Journal on Computing, 1999) developed the first fully-polynomial\napproximation scheme to estimate the probability that a graph $G$ becomes\ndisconnected, given that its edges are removed independently with probability\n$p$. This algorithm runs in $n^{5+o(1)} \\epsilon^{-3}$ time to obtain an\nestimate within relative error $\\epsilon$.\n  We improve this run-time through algorithmic and graph-theoretic advances.\nFirst, there is a certain key sub-problem encountered by Karger, for which a\ngeneric estimation procedure is employed, we show that this has a special\nstructure for which a much more efficient algorithm can be used. Second, we\nshow better bounds on the number of edge cuts which are likely to fail. Here,\nKarger's analysis uses a variety of bounds for various graph parameters, we\nshow that these bounds cannot be simultaneously tight. We describe a new graph\nparameter, which simultaneously influences all the bounds used by Karger, and\nobtain much tighter estimates of the cut structure of $G$. These techniques\nallow us to improve the runtime to $n^{3+o(1)} \\epsilon^{-2}$, our results also\nrigorously prove certain experimental observations of Karger & Tai (Proc.\nACM-SIAM Symposium on Discrete Algorithms, 1997). Our rigorous proofs are\nmotivated by certain non-rigorous differential-equation approximations which,\nhowever, provably track the worst-case trajectories of the relevant parameters.\n  A key driver of Karger's approach (and other cut-related results) is a bound\non the number of small cuts: we improve these estimates when the min-cut size\nis \"small\" and odd, augmenting, in part, a result of Bixby (Bulletin of the\nAMS, 1974).\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 15:32:20 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 22:46:48 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 14:47:01 GMT"}, {"version": "v4", "created": "Sat, 8 Jul 2017 20:04:51 GMT"}, {"version": "v5", "created": "Fri, 4 Aug 2017 23:07:24 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Harris", "David G.", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1602.08820", "submitter": "Sergey Pupyrev", "authors": "Laxman Dhulipala and Igor Kabiljo and Brian Karrer and Giuseppe\n  Ottaviano and Sergey Pupyrev and Alon Shalita", "title": "Compressing Graphs and Indexes with Recursive Graph Bisection", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939862", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph reordering is a powerful technique to increase the locality of the\nrepresentations of graphs, which can be helpful in several applications. We\nstudy how the technique can be used to improve compression of graphs and\ninverted indexes.\n  We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for\ngraph compression, and show how it can be employed for compression-friendly\nreordering of social networks and web graphs and for assigning document\nidentifiers in inverted indexes. We design and implement a novel theoretically\nsound reordering algorithm that is based on recursive graph bisection.\n  Our experiments show a significant improvement of the compression rate of\ngraph and indexes over existing heuristics. The new method is relatively simple\nand allows efficient parallel and distributed implementations, which is\ndemonstrated on graphs with billions of vertices and hundreds of billions of\nedges.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 04:38:50 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Kabiljo", "Igor", ""], ["Karrer", "Brian", ""], ["Ottaviano", "Giuseppe", ""], ["Pupyrev", "Sergey", ""], ["Shalita", "Alon", ""]]}, {"id": "1602.08829", "submitter": "Matthias Petri", "authors": "Matthias Petri, Alistair Moffat, P.C. Nagesh, Anthony Wirth", "title": "Access Time Tradeoffs in Archive Compression", "comments": "Note that the final published version of this paper prepared by\n  Springer/LNCS introduced errors in the publication process in Figures 1, 2,\n  and 3 that are not present in this preprint. In all other regards the\n  preprint and the published version are identical in their content", "journal-ref": "Asia Information Retrieval Societies Conference (AIRS), LNCS vol.\n  9460, pages 15-28, 2015", "doi": "10.1007/978-3-319-28940-3_2", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archives, query and proxy logs, and so on, can all be very large and\nhighly repetitive; and are accessed only sporadically and partially, rather\nthan continually and holistically. This type of data is ideal for\ncompression-based archiving, provided that random-access to small fragments of\nthe original data can be achieved without needing to decompress everything. The\nrecent RLZ (relative Lempel Ziv) compression approach uses a semi-static model\nextracted from the text to be compressed, together with a greedy factorization\nof the whole text encoded using static integer codes. Here we demonstrate more\nprecisely than before the scenarios in which RLZ excels. We contrast RLZ with\nalternatives based on block-based adaptive methods, including approaches that\n\"prime\" the encoding for each block, and measure a range of implementation\noptions using both hard-disk (HDD) and solid-state disk (SSD) drives. For HDD,\nthe dominant factor affecting access speed is the compression rate achieved,\neven when this involves larger dictionaries and larger blocks. When the data is\non SSD the same effects are present, but not as markedly, and more complex\ntrade-offs apply.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 05:40:31 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Petri", "Matthias", ""], ["Moffat", "Alistair", ""], ["Nagesh", "P. C.", ""], ["Wirth", "Anthony", ""]]}, {"id": "1602.09037", "submitter": "Paul Kirchner", "authors": "Paul Kirchner", "title": "Algorithms on Ideal over Complex Multiplication order", "comments": "Full version of a paper submitted to ANTS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DM math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show in this paper that the Gentry-Szydlo algorithm for cyclotomic orders,\npreviously revisited by Lenstra-Silverberg, can be extended to\ncomplex-multiplication (CM) orders, and even to a more general structure. This\nalgorithm allows to test equality over the polarized ideal class group, and\nfinds a generator of the polarized ideal in polynomial time. Also, the\nalgorithm allows to solve the norm equation over CM orders and the recent\nreduction of principal ideals to the real suborder can also be performed in\npolynomial time. Furthermore, we can also compute in polynomial time a unit of\nan order of any number field given a (not very precise) approximation of it.\nOur description of the Gentry-Szydlo algorithm is different from the original\nand Lenstra- Silverberg's variant and we hope the simplifications made will\nallow a deeper understanding. Finally, we show that the well-known speed-up for\nenumeration and sieve algorithms for ideal lattices over power of two\ncyclotomics can be generalized to any number field with many roots of unity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 16:40:54 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Kirchner", "Paul", ""]]}]