[{"id": "1308.0068", "submitter": "Michael Christ", "authors": "Michael Christ, James Demmel, Nicholas Knight, Thomas Scanlon, and\n  Katherine Yelick", "title": "Communication lower bounds and optimal algorithms for programs that\n  reference arrays -- Part 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The movement of data (communication) between levels of a memory hierarchy, or\nbetween parallel processors on a network, can greatly dominate the cost of\ncomputation, so algorithms that minimize communication are of interest.\nMotivated by this, attainable lower bounds for the amount of communication\nrequired by algorithms were established by several groups for a variety of\nalgorithms, including matrix computations. Prior work of\nBallard-Demmel-Holtz-Schwartz relied on a geometric inequality of Loomis and\nWhitney for this purpose. In this paper the general theory of discrete\nmultilinear Holder-Brascamp-Lieb (HBL) inequalities is used to establish\ncommunication lower bounds for a much wider class of algorithms. In some cases,\nalgorithms are presented which attain these lower bounds.\n  Several contributions are made to the theory of HBL inequalities proper. The\noptimal constant in such an inequality for torsion-free Abelian groups is shown\nto equal one whenever it is finite. Bennett-Carbery-Christ-Tao had\ncharacterized the tuples of exponents for which such an inequality is valid as\nthe convex polyhedron defined by a certain finite list of inequalities. The\nproblem of constructing an algorithm to decide whether a given inequality is on\nthis list, is shown to be equivalent to Hilbert's Tenth Problem over the\nrationals, which remains open. Nonetheless, an algorithm which computes the\npolyhedron itself is constructed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 00:15:42 GMT"}], "update_date": "2013-08-03", "authors_parsed": [["Christ", "Michael", ""], ["Demmel", "James", ""], ["Knight", "Nicholas", ""], ["Scanlon", "Thomas", ""], ["Yelick", "Katherine", ""]]}, {"id": "1308.0085", "submitter": "Yinglei Song", "authors": "Yinglei Song", "title": "An Improved Parameterized Algorithm for the Independent Feedback Vertex\n  Set Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new parameterized algorithm for the {\\sc\nIndependent Feedback Vertex Set} (IFVS) problem. Given a graph $G=(V,E)$, the\ngoal of the problem is to determine whether there exists a vertex subset\n$F\\subseteq V$ such that $V-F$ induces a forest in $G$ and $F$ is an\nindependent set. We show that there exists a parameterized algorithm that can\ndetermine whether a graph contains an IFVS of size $k$ or not in time\n$O(4^kn^{2})$. To our best knowledge, this result improves the known upper\nbound for this problem, which is $O^{*}(5^{k}n^{O(1)})$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 03:26:10 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Song", "Yinglei", ""]]}, {"id": "1308.0183", "submitter": "Patrick Prosser", "authors": "Chris Unsworth and Patrick Prosser", "title": "An n-ary Constraint for the Stable Marriage Problem", "comments": "7 pages. The Fifth Workshop on Modelling and Solving Problems with\n  Constraints, held at the 19th International Joint Conference on Artificial\n  Intelligence (IJCAI 2005)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an n-ary constraint for the stable marriage problem. This\nconstraint acts between two sets of integer variables where the domains of\nthose variables represent preferences. Our constraint enforces stability and\ndisallows bigamy. For a stable marriage instance with $n$ men and $n$ women we\nrequire only one of these constraints, and the complexity of enforcing\narc-consistency is $O(n^2)$ which is optimal in the size of input. Our\ncomputational studies show that our n-ary constraint is significantly faster\nand more space efficient than the encodings presented in \\cite{cp01}. We also\nintroduce a new problem to the constraint community, the sex-equal stable\nmarriage problem.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 12:56:47 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Unsworth", "Chris", ""], ["Prosser", "Patrick", ""]]}, {"id": "1308.0299", "submitter": "Leonardo Borba", "authors": "Leonardo Borba and Marcus Ritt", "title": "Exact and Heuristic Methods for the Assembly Line Worker Assignment and\n  Balancing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional assembly lines, it is reasonable to assume that task execution\ntimes are the same for each worker. However, in sheltered work centres for\ndisabled this assumption is not valid: some workers may execute some tasks\nconsiderably slower or even be incapable of executing them. Worker\nheterogeneity leads to a problem called the assembly line worker assignment and\nbalancing problem (ALWABP). For a fixed number of workers the problem is to\nmaximize the production rate of an assembly line by assigning workers to\nstations and tasks to workers, while satisfying precedence constraints between\nthe tasks. This paper introduces new heuristic and exact methods to solve this\nproblem. We present a new MIP model, propose a novel heuristic algorithm based\non beam search, as well as a task-oriented branch-and-bound procedure which\nuses new reduction rules and lower bounds for solving the problem. Extensive\ncomputational tests on a large set of instances show that these methods are\neffective and improve over existing ones.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2013 18:49:07 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Borba", "Leonardo", ""], ["Ritt", "Marcus", ""]]}, {"id": "1308.0482", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Gabriele Muciaccia, Anders Yeo", "title": "Parameterized Complexity of k-Chinese Postman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem called the $k$-Chinese Postman Problem\n($k$-CPP): given a connected edge-weighted graph $G$ and integers $p$ and $k$,\ndecide whether there are at least $k$ closed walks such that every edge of $G$\nis contained in at least one of them and the total weight of the edges in the\nwalks is at most $p$? The problem $k$-CPP is NP-complete, and van Bevern et al.\n(to appear) and Sorge (2013) asked whether the $k$-CPP is fixed-parameter\ntractable when parameterized by $k$. We prove that the $k$-CPP is indeed\nfixed-parameter tractable. In fact, we prove a stronger result: the problem\nadmits a kernel with $O(k^2\\log k)$ vertices. We prove that the directed\nversion of $k$-CPP is NP-complete and ask whether the directed version is\nfixed-parameter tractable when parameterized by $k$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 12:43:48 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 08:36:41 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Gutin", "Gregory", ""], ["Muciaccia", "Gabriele", ""], ["Yeo", "Anders", ""]]}, {"id": "1308.0626", "submitter": "C. Seshadhri", "authors": "M. Saks and C. Seshadhri", "title": "Estimating the longest increasing sequence in polylogarithmic time", "comments": "Full version of FOCS 2010 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the length of the longest increasing subsequence (LIS) is a classic\nalgorithmic problem. Let $n$ denote the size of the array. Simple $O(n\\log n)$\nalgorithms are known for this problem. We develop a polylogarithmic time\nrandomized algorithm that for any constant $\\delta > 0$, estimates the length\nof the LIS of an array to within an additive error of $\\delta n$. More\nprecisely, the running time of the algorithm is $(\\log n)^c\n(1/\\delta)^{O(1/\\delta)}$ where the exponent $c$ is independent of $\\delta$.\nPreviously, the best known polylogarithmic time algorithms could only achieve\nan additive $n/2$ approximation. With a suitable choice of parameters, our\nalgorithm also gives, for any fixed $\\tau>0$, a multiplicative\n$(1+\\tau)$-approximation to the distance to monotonicity $\\varepsilon_f$ (the\nfraction of entries not in the LIS), whose running time is polynomial in\n$\\log(n)$ and $1/varepsilon_f$. The best previously known algorithm could only\nguarantee an approximation within a factor (arbitrarily close to) 2.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 21:57:52 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Saks", "M.", ""], ["Seshadhri", "C.", ""]]}, {"id": "1308.0761", "submitter": "Alexander Semenov", "authors": "Alexander Semenov, Oleg Zaikin", "title": "On estimating total time to solve SAT in distributed computing\n  environments: Application to the SAT@home project", "comments": "This paper was submitted to SAT-2013 conference. Its materials were\n  reported in a poster session (the paper in its full variant was not\n  accepted). 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to estimate the total time required to solve SAT\nin distributed environments via partitioning approach. It is based on the\nobservation that for some simple forms of problem partitioning one can use the\nMonte Carlo approach to estimate the time required to solve an original\nproblem. The method proposed is based on an algorithm for searching for\npartitioning with an optimal solving time estimation. We applied this method to\nestimate the time required to perform logical cryptanalysis of the widely known\nstream ciphers A5/1 and Bivium. The paper also describes a volunteer computing\nproject SAT@home aimed at solving hard combinatorial problems reduced to SAT.\nIn this project during several months there were solved 10 problems of logical\ncryptanalysis of the A5/1 cipher thatcould not be solved using known rainbow\ntables.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 00:30:09 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Semenov", "Alexander", ""], ["Zaikin", "Oleg", ""]]}, {"id": "1308.0776", "submitter": "Sebastian Krinninger", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", "title": "Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier\n  and Derandomization", "comments": "A preliminary version was presented at the 2013 IEEE 54th Annual\n  Symposium on Foundations of Computer Science (FOCS 2013)", "journal-ref": "SIAM Journal on Computing 45(3): 947-1006 (2016)", "doi": "10.1137/140957299", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic $(1+\\epsilon)$-approximation algorithms for the all-pairs\nshortest paths problem in unweighted undirected $n$-node $m$-edge graphs under\nedge deletions. The fastest algorithm for this problem is a randomized\nalgorithm with a total update time of $\\tilde O(mn/\\epsilon)$ and constant\nquery time by Roditty and Zwick [FOCS 2004]. The fastest deterministic\nalgorithm is from a 1981 paper by Even and Shiloach [JACM 1981]; it has a total\nupdate time of $O(mn^2)$ and constant query time. We improve these results as\nfollows: (1) We present an algorithm with a total update time of $\\tilde\nO(n^{5/2}/\\epsilon)$ and constant query time that has an additive error of $2$\nin addition to the $1+\\epsilon$ multiplicative error. This beats the previous\n$\\tilde O(mn/\\epsilon)$ time when $m=\\Omega(n^{3/2})$. Note that the additive\nerror is unavoidable since, even in the static case, an $O(n^{3-\\delta})$-time\n(a so-called truly subcubic) combinatorial algorithm with $1+\\epsilon$\nmultiplicative error cannot have an additive error less than $2-\\epsilon$,\nunless we make a major breakthrough for Boolean matrix multiplication [Dor et\nal. FOCS 1996] and many other long-standing problems [Vassilevska Williams and\nWilliams FOCS 2010]. The algorithm can also be turned into a\n$(2+\\epsilon)$-approximation algorithm (without an additive error) with the\nsame time guarantees, improving the recent $(3+\\epsilon)$-approximation\nalgorithm with $\\tilde O(n^{5/2+O(\\sqrt{\\log{(1/\\epsilon)}/\\log n})})$ running\ntime of Bernstein and Roditty [SODA 2011] in terms of both approximation and\ntime guarantees. (2) We present a deterministic algorithm with a total update\ntime of $\\tilde O(mn/\\epsilon)$ and a query time of $O(\\log\\log n)$. The\nalgorithm has a multiplicative error of $1+\\epsilon$ and gives the first\nimproved deterministic algorithm since 1981. It also answers an open question\nraised by Bernstein [STOC 2013].\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 04:11:38 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 08:28:26 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1308.0833", "submitter": "Maximilian Fillinger", "authors": "Maximilian Fillinger", "title": "Data Structures in Classical and Quantum Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey summarizes several results about quantum computing related to\n(mostly static) data structures. First, we describe classical data structures\nfor the set membership and the predecessor search problems: Perfect Hash tables\nfor set membership by Fredman, Koml\\'{o}s and Szemer\\'{e}di and a data\nstructure by Beame and Fich for predecessor search. We also prove results about\ntheir space complexity (how many bits are required) and time complexity (how\nmany bits have to be read to answer a query). After that, we turn our attention\nto classical data structures with quantum access. In the quantum access model,\ndata is stored in classical bits, but they can be accessed in a quantum way: We\nmay read several bits in superposition for unit cost. We give proofs for lower\nbounds in this setting that show that the classical data structures from the\nfirst section are, in some sense, asymptotically optimal - even in the quantum\nmodel. In fact, these proofs are simpler and give stronger results than\nprevious proofs for the classical model of computation. The lower bound for set\nmembership was proved by Radhakrishnan, Sen and Venkatesh and the result for\nthe predecessor problem by Sen and Venkatesh. Finally, we examine fully quantum\ndata structures. Instead of encoding the data in classical bits, we now encode\nit in qubits. We allow any unitary operation or measurement in order to answer\nqueries. We describe one data structure by de Wolf for the set membership\nproblem and also a general framework using fully quantum data structures in\nquantum walks by Jeffery, Kothari and Magniez.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 18:31:30 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Fillinger", "Maximilian", ""]]}, {"id": "1308.0907", "submitter": "Shailesh Vaya", "authors": "Shailesh Vaya", "title": "The complexity of resolving conflicts on MAC", "comments": "Xerox internal report 27th July; 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of multiple stations competing to\ntransmit on a multiple access channel (MAC). We are given $n$ stations out of\nwhich at most $d$ are active and intend to transmit a message to other stations\nusing MAC. All stations are assumed to be synchronized according to a time\nclock. If $l$ stations node transmit in the same round, then the MAC provides\nthe feedback whether $l=0$, $l=2$ (collision occurred) or $l=1$. When $l=1$,\nthen a single station is indeed able to successfully transmit a message, which\nis received by all other nodes. For the above problem the active stations have\nto schedule their transmissions so that they can singly, transmit their\nmessages on MAC, based only on the feedback received from the MAC in previous\nround.\n  For the above problem it was shown in [Greenberg, Winograd, {\\em A Lower\nbound on the Time Needed in the Worst Case to Resolve Conflicts\nDeterministically in Multiple Access Channels}, Journal of ACM 1985] that every\ndeterministic adaptive algorithm should take $\\Omega(d (\\lg n)/(\\lg d))$ rounds\nin the worst case. The fastest known deterministic adaptive algorithm requires\n$O(d \\lg n)$ rounds. The gap between the upper and lower bound is $O(\\lg d)$\nround. It is substantial for most values of $d$: When $d = $ constant and $d\n\\in O(n^{\\epsilon})$ (for any constant $\\epsilon \\leq 1$, the lower bound is\nrespectively $O(\\lg n)$ and O(n), which is trivial in both cases. Nevertheless,\nthe above lower bound is interesting indeed when $d \\in$ poly($\\lg n$). In this\nwork, we present a novel counting argument to prove a tight lower bound of\n$\\Omega(d \\lg n)$ rounds for all deterministic, adaptive algorithms, closing\nthis long standing open question.}\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 08:58:32 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Vaya", "Shailesh", ""]]}, {"id": "1308.1006", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer, Stefanie Jegelka and Jeff Bilmes", "title": "Fast Semidifferential-based Submodular Function Optimization", "comments": "This work appeared in Proc. International Conference of Machine\n  Learning (ICML, 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical and powerful new framework for both unconstrained and\nconstrained submodular function optimization based on discrete\nsemidifferentials (sub- and super-differentials). The resulting algorithms,\nwhich repeatedly compute and then efficiently optimize submodular\nsemigradients, offer new and generalize many old methods for submodular\noptimization. Our approach, moreover, takes steps towards providing a unifying\nparadigm applicable to both submodular min- imization and maximization,\nproblems that historically have been treated quite distinctly. The practicality\nof our algorithms is important since interest in submodularity, owing to its\nnatural and wide applicability, has recently been in ascendance within machine\nlearning. We analyze theoretical properties of our algorithms for minimization\nand maximization, and show that many state-of-the-art maximization algorithms\nare special cases. Lastly, we complement our theoretical analyses with\nsupporting empirical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 15:19:48 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Iyer", "Rishabh", ""], ["Jegelka", "Stefanie", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1308.1009", "submitter": "Ping Li", "authors": "Ping Li, Gennady Samorodnitsky, John Hopcroft", "title": "Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of stable random projections is popular for efficiently computing\nthe Lp distances in high dimension (where 0<p<=2), using small space. Because\nit adopts nonadaptive linear projections, this method is naturally suitable\nwhen the data are collected in a dynamic streaming fashion (i.e., turnstile\ndata streams). In this paper, we propose to use only the signs of the projected\ndata and analyze the probability of collision (i.e., when the two signs\ndiffer). We derive a bound of the collision probability which is exact when p=2\nand becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,\nCauchy random projections), we show that the probability of collision can be\naccurately approximated as functions of the chi-square similarity. For example,\nwhen the (un-normalized) data are binary, the maximum approximation error of\nthe collision probability is smaller than 0.0192. In text and vision\napplications, the chi-square similarity is a popular measure for nonnegative\ndata when the features are generated from histograms. Our experiments confirm\nthat the proposed method is promising for large-scale learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 15:25:51 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Li", "Ping", ""], ["Samorodnitsky", "Gennady", ""], ["Hopcroft", "John", ""]]}, {"id": "1308.1068", "submitter": "Laszlo Egri", "authors": "Rajesh Chitnis, Laszlo Egri, Daniel Marx", "title": "List H-Coloring a Graph by Removing Few Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the deletion version of the list homomorphism problem, we are given graphs\nG and H, a list L(v) that is a subset of V(H) for each vertex v of G, and an\ninteger k. The task is to decide whether there exists a subset W of V(G) of\nsize at most k such that there is a homomorphism from G \\ W to H respecting the\nlists. We show that DL-Hom(H), parameterized by k and |H|, is fixed-parameter\ntractable for any (P6, C6)-free bipartite graph H; already for this restricted\nclass of graphs, the problem generalizes Vertex Cover, Odd Cycle Transversal,\nand Vertex Multiway Cut parameterized by the size of the cutset and the number\nof terminals. We conjecture that DL-Hom(H) is fixed-parameter tractable for the\nclass of graphs H for which the list homomorphism problem (without deletions)\nis polynomial-time solvable; by a result of Feder, Hell and Huang (1999), a\ngraph H belongs to this class precisely if it is a bipartite graph whose\ncomplement is a circular arc graph. We show that this conjecture is equivalent\nto the fixed-parameter tractability of a single fairly natural satisfiability\nproblem, Clause Deletion Chain-SAT.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 18:56:38 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Egri", "Laszlo", ""], ["Marx", "Daniel", ""]]}, {"id": "1308.1212", "submitter": "Rahul Vaze", "authors": "Andrew Thangaraj and Rahul Vaze", "title": "Online Algorithms for Basestation Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of {\\it online algorithms} for assigning mobile users to basestations\nis considered with the objective of maximizing the sum-rate, when all users\nassociated to any one basestation equally share each basestation's resources.\nEach user on its arrival reveals the rates it can obtain if connected to each\nof the basestations, and the problem is to assign each user to any one\nbasestation irrevocably so that the sum-rate is maximized at the end of all\nuser arrivals, without knowing the future user arrival or rate information or\nits statistics at each user arrival. Online algorithms with constant factor\nloss in comparison to offline algorithms (that know both the user arrival and\nuser rates profile in advance) are derived. The proposed online algorithms are\nmotivated from the famous online k-secretary problem and online maximum weight\nmatching problem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 09:19:50 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Thangaraj", "Andrew", ""], ["Vaze", "Rahul", ""]]}, {"id": "1308.1351", "submitter": "Davis Issac", "authors": "Davis Issac and Ragesh Jaiswal", "title": "An $O^*(1.0821^n)$-Time Algorithm for Computing Maximum Independent Set\n  in Graphs with Bounded Degree 3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O^*(1.0821^n)$-time, polynomial space algorithm for computing\nMaximum Independent Set in graphs with bounded degree 3. This improves all the\nprevious running time bounds known for the problem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 17:08:50 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 13:56:42 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 15:32:53 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Issac", "Davis", ""], ["Jaiswal", "Ragesh", ""]]}, {"id": "1308.1385", "submitter": "Aleksandar Nikolov", "authors": "Cynthia Dwork, Aleksandar Nikolov, Kunal Talwar", "title": "Efficient Algorithms for Privately Releasing Marginals via Convex\n  Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a database of $n$ people, each represented by a bit-string of length\n$d$ corresponding to the setting of $d$ binary attributes. A $k$-way marginal\nquery is specified by a subset $S$ of $k$ attributes, and a $|S|$-dimensional\nbinary vector $\\beta$ specifying their values. The result for this query is a\ncount of the number of people in the database whose attribute vector restricted\nto $S$ agrees with $\\beta$.\n  Privately releasing approximate answers to a set of $k$-way marginal queries\nis one of the most important and well-motivated problems in differential\nprivacy. Information theoretically, the error complexity of marginal queries is\nwell-understood: the per-query additive error is known to be at least\n$\\Omega(\\min\\{\\sqrt{n},d^{\\frac{k}{2}}\\})$ and at most\n$\\tilde{O}(\\min\\{\\sqrt{n} d^{1/4},d^{\\frac{k}{2}}\\})$. However, no polynomial\ntime algorithm with error complexity as low as the information theoretic upper\nbound is known for small $n$. In this work we present a polynomial time\nalgorithm that, for any distribution on marginal queries, achieves average\nerror at most $\\tilde{O}(\\sqrt{n} d^{\\frac{\\lceil k/2 \\rceil}{4}})$. This error\nbound is as good as the best known information theoretic upper bounds for\n$k=2$. This bound is an improvement over previous work on efficiently releasing\nmarginals when $k$ is small and when error $o(n)$ is desirable. Using private\nboosting we are also able to give nearly matching worst-case error bounds.\n  Our algorithms are based on the geometric techniques of Nikolov, Talwar, and\nZhang. The main new ingredients are convex relaxations and careful use of the\nFrank-Wolfe algorithm for constrained convex minimization. To design our\nrelaxations, we rely on the Grothendieck inequality from functional analysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 19:29:17 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Dwork", "Cynthia", ""], ["Nikolov", "Aleksandar", ""], ["Talwar", "Kunal", ""]]}, {"id": "1308.1556", "submitter": "Yinglei Song", "authors": "Yinglei Song", "title": "On the Independent Set and Common Subgraph Problems in Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we develop efficient exact and approximate algorithms for\ncomputing a maximum independent set in random graphs. In a random graph $G$,\neach pair of vertices are joined by an edge with a probability $p$, where $p$\nis a constant between $0$ and $1$. We show that, a maximum independent set in a\nrandom graph that contains $n$ vertices can be computed in expected computation\ntime $2^{O(\\log_{2}^{2}{n})}$. Using techniques based on enumeration, we\ndevelop an algorithm that can find a largest common subgraph in two random\ngraphs in $n$ and $m$ vertices ($m \\leq n$) in expected computation time\n$2^{O(n^{\\frac{1}{2}}\\log_{2}^{\\frac{5}{3}}{n})}$. In addition, we show that,\nwith high probability, the parameterized independent set problem is fixed\nparameter tractable in random graphs and the maximum independent set in a\nrandom graph in $n$ vertices can be approximated within a ratio of\n$\\frac{2n}{2^{\\sqrt{\\log_{2}{n}}}}$ in expected polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 12:55:41 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Song", "Yinglei", ""]]}, {"id": "1308.1568", "submitter": "Airat Urakov", "authors": "Urakov and Timeryaev", "title": "All-Pairs Shortest Paths Algorithm for High-dimensional Sparse Graphs", "comments": "8 pages, 3 figures, 2 tables. A more detailed text on Russian:\n  http://www.lib.tsu.ru/mminfo/000349342/19/image/19-084.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here the All-pairs shortest path problem on weighted undirected sparse graphs\nis being considered. For the problem considered, we propose ``disassembly and\nassembly of a graph'' algorithm which uses a solution of the problem on a\nsmall-dimensional graph to obtain the solution for the given graph. The\nproposed algorithm has been compared to one of the fastest classic algorithms\non data from an open public source.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 13:32:38 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Urakov", "", ""], ["Timeryaev", "", ""]]}, {"id": "1308.1643", "submitter": "Hu Fu", "authors": "Hu Fu and Robert Kleinberg", "title": "Improved Lower Bounds for Testing Triangle-freeness in Boolean Functions\n  via Fast Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the query complexity for testing linear-invariant properties\nhas been a central open problem in the study of algebraic property testing.\nTriangle-freeness in Boolean functions is a simple property whose testing\ncomplexity is unknown. Three Boolean functions $f_1$, $f_2$ and $f_3:\n\\mathbb{F}_2^k \\to \\{0, 1\\}$ are said to be triangle free if there is no $x, y\n\\in \\mathbb{F}_2^k$ such that $f_1(x) = f_2(y) = f_3(x + y) = 1$. This property\nis known to be strongly testable (Green 2005), but the number of queries needed\nis upper-bounded only by a tower of twos whose height is polynomial in $1 /\n\\epsislon$, where $\\epsislon$ is the distance between the tested function\ntriple and triangle-freeness, i.e., the minimum fraction of function values\nthat need to be modified to make the triple triangle free. A lower bound of $(1\n/ \\epsilon)^{2.423}$ for any one-sided tester was given by Bhattacharyya and\nXie (2010). In this work we improve this bound to $(1 / \\epsilon)^{6.619}$.\nInterestingly, we prove this by way of a combinatorial construction called\n\\emph{uniquely solvable puzzles} that was at the heart of Coppersmith and\nWinograd's renowned matrix multiplication algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 17:42:45 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Fu", "Hu", ""], ["Kleinberg", "Robert", ""]]}, {"id": "1308.1762", "submitter": "Piyush Srivastava", "authors": "Alistair Sinclair, Piyush Srivastava, Yitong Yin", "title": "Spatial mixing and approximation algorithms for graphs with bounded\n  connective constant", "comments": "26 pages. In October 2014, this paper was superseded by\n  arxiv:1410.2595. Before that, an extended abstract of this paper appeared in\n  Proc. IEEE Symposium on the Foundations of Computer Science (FOCS), 2013, pp.\n  300-309", "journal-ref": null, "doi": "10.1109/FOCS.2013.40", "report-no": null, "categories": "cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hard core model in statistical physics is a probability distribution on\nindependent sets in a graph in which the weight of any independent set I is\nproportional to lambda^(|I|), where lambda > 0 is the vertex activity. We show\nthat there is an intimate connection between the connective constant of a graph\nand the phenomenon of strong spatial mixing (decay of correlations) for the\nhard core model; specifically, we prove that the hard core model with vertex\nactivity lambda < lambda_c(Delta + 1) exhibits strong spatial mixing on any\ngraph of connective constant Delta, irrespective of its maximum degree, and\nhence derive an FPTAS for the partition function of the hard core model on such\ngraphs. Here lambda_c(d) := d^d/(d-1)^(d+1) is the critical activity for the\nuniqueness of the Gibbs measure of the hard core model on the infinite d-ary\ntree. As an application, we show that the partition function can be efficiently\napproximated with high probability on graphs drawn from the random graph model\nG(n,d/n) for all lambda < e/d, even though the maximum degree of such graphs is\nunbounded with high probability.\n  We also improve upon Weitz's bounds for strong spatial mixing on bounded\ndegree graphs (Weitz, 2006) by providing a computationally simple method which\nuses known estimates of the connective constant of a lattice to obtain bounds\non the vertex activities lambda for which the hard core model on the lattice\nexhibits strong spatial mixing. Using this framework, we improve upon these\nbounds for several lattices including the Cartesian lattice in dimensions 3 and\nhigher.\n  Our techniques also allow us to relate the threshold for the uniqueness of\nthe Gibbs measure on a general tree to its branching factor (Lyons, 1989).\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 06:08:22 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 00:34:43 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""], ["Yin", "Yitong", ""]]}, {"id": "1308.1802", "submitter": "Petr Golovach", "authors": "Petr A. Golovach", "title": "Editing to a Connected Graph of Given Degrees", "comments": "Some proofs are simplified and typos are corrected in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of edge editing or modification problems is to change a given graph\nby adding and deleting of a small number of edges in order to satisfy a certain\nproperty. We consider the Edge Editing to a Connected Graph of Given Degrees\nproblem that asks for a graph G, non-negative integers d,k and a function\n\\delta:V(G)->{1,...,d}, whether it is possible to obtain a connected graph G'\nfrom G such that the degree of v is \\delta(v) for any vertex v by at most k\nedge editing operations. As the problem is NP-complete even if \\delta(v)=2, we\nare interested in the parameterized complexity and show that Edge Editing to a\nConnected Graph of Given Degrees admits a polynomial kernel when parameterized\nby d+k. For the special case \\delta(v)=d, i.e., when the aim is to obtain a\nconnected d-regular graph, the problem is shown to be fixed parameter tractable\nwhen parameterized by k only.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 09:51:02 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 09:20:58 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Golovach", "Petr A.", ""]]}, {"id": "1308.1820", "submitter": "Gregory Gutin", "authors": "Gregory Gutin and Mark Jones", "title": "Parameterized Algorithms for Load Coloring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to state the Load Coloring Problem (LCP) is as follows. Let $G=(V,E)$\nbe graph and let $f:V\\rightarrow \\{{\\rm red}, {\\rm blue}\\}$ be a 2-coloring. An\nedge $e\\in E$ is called red (blue) if both end-vertices of $e$ are red (blue).\nFor a 2-coloring $f$, let $r'_f$ and $b'_f$ be the number of red and blue edges\nand let $\\mu_f(G)=\\min\\{r'_f,b'_f\\}$. Let $\\mu(G)$ be the maximum of $\\mu_f(G)$\nover all 2-colorings.\n  We introduce the parameterized problem $k$-LCP of deciding whether $\\mu(G)\\ge\nk$, where $k$ is the parameter. We prove that this problem admits a kernel with\nat most $7k$. Ahuja et al. (2007) proved that one can find an optimal\n2-coloring on trees in polynomial time. We generalize this by showing that an\noptimal 2-coloring on graphs with tree decomposition of width $t$ can be found\nin time $O^*(2^t)$. We also show that either $G$ is a Yes-instance of $k$-LCP\nor the treewidth of $G$ is at most $2k$. Thus, $k$-LCP can be solved in time\n$O^*(4^k).$\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 11:41:30 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 15:34:56 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Gutin", "Gregory", ""], ["Jones", "Mark", ""]]}, {"id": "1308.1911", "submitter": "Saurabh Aggarwal", "authors": "Saurabh Aggarwal, Joy Kuri, Rahul Vaze", "title": "Social optimum in Social Groups with Give-and-Take criterion", "comments": "Submitted for review to INFOCOM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a \"Social Group\" of networked nodes, seeking a \"universe\" of\nsegments. Each node has subset of the universe, and access to an expensive\nresource for downloading data. Alternatively, nodes can also acquire the\nuniverse by exchanging segments among themselves, at low cost, using a local\nnetwork interface. While local exchanges ensure minimum cost, \"free riders\" in\nthe group can exploit the system. To prohibit free riding, we propose the\n\"Give-and-Take\" criterion, where exchange is allowed if each node has segments\nunavailable with the other. Under this criterion, we consider the problem of\nmaximizing the aggregate cardinality of the nodes' segment sets. First, we\npresent a randomized algorithm, whose analysis yields a lower bound on the\nexpected aggregate cardinality, as well as an approximation ratio of 1/4 under\nsome conditions. Four other algorithms are presented and analyzed. We identify\nconditions under which some of these algorithms are optimal\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 17:35:58 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Aggarwal", "Saurabh", ""], ["Kuri", "Joy", ""], ["Vaze", "Rahul", ""]]}, {"id": "1308.1971", "submitter": "Ji Zhu", "authors": "Ji Zhu and Bruce Hajek", "title": "Tree dynamics for peer-to-peer streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an asynchronous distributed algorithm to manage multiple\ntrees for peer-to-peer streaming in a flow level model. It is assumed that\nvideos are cut into substreams, with or without source coding, to be\ndistributed to all nodes. The algorithm guarantees that each node receives\nsufficiently many substreams within delay logarithmic in the number of peers.\nThe algorithm works by constantly updating the topology so that each substream\nis distributed through trees to as many nodes as possible without interference.\nCompetition among trees for limited upload capacity is managed so that both\ncoverage and balance are achieved. The algorithm is robust in that it\nefficiently eliminates cycles and maintains tree structures in a distributed\nway. The algorithm favors nodes with higher degree, so it not only works for\nlive streaming and video on demand, but also in the case a few nodes with large\ndegree act as servers and other nodes act as clients.\n  A proof of convergence of the algorithm is given assuming instantaneous\nupdate of depth information, and for the case of a single tree it is shown that\nthe convergence time is stochastically tightly bounded by a small constant\ntimes the log of the number of nodes. These theoretical results are\ncomplemented by simulations showing that the algorithm works well even when\nmost assumptions for the theoretical tractability do not hold.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 20:38:25 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Zhu", "Ji", ""], ["Hajek", "Bruce", ""]]}, {"id": "1308.1978", "submitter": "Marek Szyku{\\l}a", "authors": "Jakub Kowalski, Marek Szyku{\\l}a", "title": "A New Heuristic Synchronizing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new heuristic algorithm finding reset words. The algorithm\ncalled CutOff-IBFS is based on a simple idea of inverse breadth-first-search in\nthe power automaton. We perform an experimental investigation of effectiveness\ncompared to other algorithms existing in literature, which yields that our\nmethod generally finds a shorter word in an average case and works well in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 20:49:47 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Kowalski", "Jakub", ""], ["Szyku\u0142a", "Marek", ""]]}, {"id": "1308.2144", "submitter": "Sebastiano Vigna", "authors": "Paolo Boldi and Sebastiano Vigna", "title": "In-Core Computation of Geometric Centralities with HyperBall: A Hundred\n  Billion Nodes and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a social network, which of its nodes are more central? This question\nhas been asked many times in sociology, psychology and computer science, and a\nwhole plethora of centrality measures (a.k.a. centrality indices, or rankings)\nwere proposed to account for the importance of the nodes of a network. In this\npaper, we approach the problem of computing geometric centralities, such as\ncloseness and harmonic centrality, on very large graphs; traditionally this\ntask requires an all-pairs shortest-path computation in the exact case, or a\nnumber of breadth-first traversals for approximated computations, but these\ntechniques yield very weak statistical guarantees on highly disconnected\ngraphs. We rather assume that the graph is accessed in a semi-streaming\nfashion, that is, that adjacency lists are scanned almost sequentially, and\nthat a very small amount of memory (in the order of a dozen bytes) per node is\navailable in core memory. We leverage the newly discovered algorithms based on\nHyperLogLog counters, making it possible to approximate a number of geometric\ncentralities at a very high speed and with high accuracy. While the application\nof similar algorithms for the approximation of closeness was attempted in the\nMapReduce framework, our exploitation of HyperLogLog counters reduces\nexponentially the memory footprint, paving the way for in-core processing of\nnetworks with a hundred billion nodes using \"just\" 2TiB of RAM. Moreover, the\ncomputations we describe are inherently parallelizable, and scale linearly with\nthe number of available cores.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 14:56:55 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 10:25:46 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Boldi", "Paolo", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1308.2166", "submitter": "Kanat Tangwongsan", "authors": "Kanat Tangwongsan, A. Pavan, and Srikanta Tirthapura", "title": "Parallel Triangle Counting in Massive Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of triangles in a graph is a fundamental metric, used in social\nnetwork analysis, link classification and recommendation, and more. Driven by\nthese applications and the trend that modern graph datasets are both large and\ndynamic, we present the design and implementation of a fast and cache-efficient\nparallel algorithm for estimating the number of triangles in a massive\nundirected graph whose edges arrive as a stream. It brings together the\nbenefits of streaming algorithms and parallel algorithms. By building on the\nstreaming algorithms framework, the algorithm has a small memory footprint. By\nleveraging the paralell cache-oblivious framework, it makes efficient use of\nthe memory hierarchy of modern multicore machines without needing to know its\nspecific parameters. We prove theoretical bounds on accuracy, memory access\ncost, and parallel runtime complexity, as well as showing empirically that the\nalgorithm yields accurate results and substantial speedups compared to an\noptimized sequential implementation.\n  (This is an expanded version of a CIKM'13 paper of the same title.)\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 15:54:22 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Pavan", "A.", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1308.2218", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "Coding for Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of random projections has become very popular for large-scale\napplications in statistical learning, information retrieval, bio-informatics\nand other applications. Using a well-designed coding scheme for the projected\ndata, which determines the number of bits needed for each projected value and\nhow to allocate these bits, can significantly improve the effectiveness of the\nalgorithm, in storage cost as well as computational speed. In this paper, we\nstudy a number of simple coding schemes, focusing on the task of similarity\nestimation and on an application to training linear classifiers. We demonstrate\nthat uniform quantization outperforms the standard existing influential method\n(Datar et. al. 2004). Indeed, we argue that in many cases coding with just a\nsmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bit\ncoding scheme that generally performs well in practice, as confirmed by our\nexperiments on training linear support vector machines (SVM).\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 19:50:24 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1308.2400", "submitter": "Niraj Singh", "authors": "Niraj Kumar Singh, Soubhik Chakraborty, Dheeresh Kumar Mallick", "title": "An Adaptable Fast Matrix Multiplication Algorithm, Going Beyond the Myth\n  of Decimal War", "comments": "No of tables is 1. No of diagrams is 01", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an adaptable fast matrix multiplication (AFMM)\nalgorithm, for two nxn dense matrices which computes the product matrix with\naverage complexity Tavg(n) = d1d2n3 with the acknowledgement that the average\ncount is obtained for addition as the basic operation rather than\nmultiplication which is probably the unquestionable choice for basic operation\nin existing matrix multiplication algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 14:55:14 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Singh", "Niraj Kumar", ""], ["Chakraborty", "Soubhik", ""], ["Mallick", "Dheeresh Kumar", ""]]}, {"id": "1308.2409", "submitter": "Amer Mouawad", "authors": "Amer E. Mouawad, Naomi Nishimura, Venkatesh Raman, Narges Simjour,\n  Akira Suzuki", "title": "On the Parameterized Complexity of Reconfiguration Problems", "comments": "IPEC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first results on the parameterized complexity of\nreconfiguration problems, where a reconfiguration version of an optimization\nproblem $Q$ takes as input two feasible solutions $S$ and $T$ and determines if\nthere is a sequence of {\\em reconfiguration steps} that can be applied to\ntransform $S$ into $T$ such that each step results in a feasible solution to\n$Q$. For most of the results in this paper, $S$ and $T$ are subsets of vertices\nof a given graph and a reconfiguration step adds or deletes a vertex. Our study\nis motivated by recent results establishing that for most NP-hard problems, the\nclassical complexity of reconfiguration is PSPACE-complete. We address the\nquestion for several important graph properties under two natural\nparameterizations: $k$, the size of the solutions, and $\\ell$, the length of\nthe sequence of steps. Our first general result is an algorithmic paradigm, the\n{\\em reconfiguration kernel}, used to obtain fixed-parameter algorithms for the\nreconfiguration versions of {\\sc Vertex Cover} and, more generally, {\\sc\nBounded Hitting Set} and {\\sc Feedback Vertex Set}, all parameterized by $k$.\nIn contrast, we show that reconfiguring {\\sc Unbounded Hitting Set} is\n$W[2]$-hard when parameterized by $k+\\ell$. We also demonstrate the\n$W[1]$-hardness of the reconfiguration versions of a large class of\nmaximization problems parameterized by $k+\\ell$, and of their corresponding\ndeletion problems parameterized by $\\ell$; in doing so, we show that there\nexist problems in FPT when parameterized by $k$, but whose reconfiguration\nversions are $W[1]$-hard when parameterized by $k+\\ell$.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 17:00:13 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 18:17:56 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""], ["Raman", "Venkatesh", ""], ["Simjour", "Narges", ""], ["Suzuki", "Akira", ""]]}, {"id": "1308.2473", "submitter": "Sriram Pemmaraju", "authors": "Andrew Berns, James Hegeman, Sriram V. Pemmaraju", "title": "Super-Fast Distributed Algorithms for Metric Facility Location", "comments": "15 pages, 2 figures. This is the full version of a paper that\n  appeared in ICALP 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed O(1)-approximation algorithm, with\nexpected-$O(\\log \\log n)$ running time, in the $\\mathcal{CONGEST}$ model for\nthe metric facility location problem on a size-$n$ clique network. Though\nmetric facility location has been considered by a number of researchers in\nlow-diameter settings, this is the first sub-logarithmic-round algorithm for\nthe problem that yields an O(1)-approximation in the setting of non-uniform\nfacility opening costs. In order to obtain this result, our paper makes three\nmain technical contributions. First, we show a new lower bound for metric\nfacility location, extending the lower bound of B\\u{a}doiu et al. (ICALP 2005)\nthat applies only to the special case of uniform facility opening costs. Next,\nwe demonstrate a reduction of the distributed metric facility location problem\nto the problem of computing an O(1)-ruling set of an appropriate spanning\nsubgraph. Finally, we present a sub-logarithmic-round (in expectation)\nalgorithm for computing a 2-ruling set in a spanning subgraph of a clique. Our\nalgorithm accomplishes this by using a combination of randomized and\ndeterministic sparsification.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 06:30:09 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Berns", "Andrew", ""], ["Hegeman", "James", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1308.2572", "submitter": "Blesson Varghese", "authors": "A. K. Bahl, O. Baltzer, A. Rau-Chaplin, B. Varghese and A. Whiteway", "title": "Achieving Speedup in Aggregate Risk Analysis using Multiple GPUs", "comments": "Workshop Proceedings of International Conference on Parallel\n  Processing, Lyon, France, 2013, 8 pages. arXiv admin note: text overlap with\n  arXiv:1308.2066", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DS q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation techniques employed for the analysis of portfolios of\ninsurance/reinsurance risk, often referred to as `Aggregate Risk Analysis', can\nbenefit from exploiting state-of-the-art high-performance computing platforms.\nIn this paper, parallel methods to speed-up aggregate risk analysis for\nsupporting real-time pricing are explored. An algorithm for analysing aggregate\nrisk is proposed and implemented for multi-core CPUs and for many-core GPUs.\nExperimental studies indicate that GPUs offer a feasible alternative solution\nover traditional high-performance computing systems. A simulation of 1,000,000\ntrials with 1,000 catastrophic events per trial on a typical exposure set and\ncontract structure is performed in less than 5 seconds on a multiple GPU\nplatform. The key result is that the multiple GPU implementation can be used in\nreal-time pricing scenarios as it is approximately 77x times faster than the\nsequential counterpart implemented on a CPU.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 14:09:45 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Bahl", "A. K.", ""], ["Baltzer", "O.", ""], ["Rau-Chaplin", "A.", ""], ["Varghese", "B.", ""], ["Whiteway", "A.", ""]]}, {"id": "1308.2581", "submitter": "Craig Vercueil", "authors": "Craig Vercueil", "title": "A Simple Circle Discretization Algorithm With Applications", "comments": "9 pages,2 figures,Java code, fixed typos,improved typesetting,results\n  unchanged,revision2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In CNC manufacturing,there often arises the need to create G-Code programs\nwhich require the calculation of discrete x-y coordinate pairs(2D).An example\nof this situation is when the programmer needs to create a program to machine a\nhelix(or thread).The required toolpath will be a set of points on a helix\ncurve.The problem now entails calculating the number of points along this\ncurve.Too few points and the toolpath will not be smooth.Too many points and\nthe program becomes too big.This article will serve to provide a simple way to\ndivide a circle into discrete points,with a notion of \"dimensional tolerance\"\nbuilt into the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 14:36:24 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 15:11:15 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Vercueil", "Craig", ""]]}, {"id": "1308.2599", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Magnus Wahlstrom, Anders Yeo", "title": "Parameterized Rural Postman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Directed Rural Postman Problem (DRPP) can be formulated as follows: given\na strongly connected directed multigraph $D=(V,A)$ with nonnegative integral\nweights on the arcs, a subset $R$ of $A$ and a nonnegative integer $\\ell$,\ndecide whether $D$ has a closed directed walk containing every arc of $R$ and\nof total weight at most $\\ell$. Let $k$ be the number of weakly connected\ncomponents in the the subgraph of $D$ induced by $R$. Sorge et al. (2012) ask\nwhether the DRPP is fixed-parameter tractable (FPT) when parameterized by $k$,\ni.e., whether there is an algorithm of running time $O^*(f(k))$ where $f$ is a\nfunction of $k$ only and the $O^*$ notation suppresses polynomial factors.\nSorge et al. (2012) note that this question is of significant practical\nrelevance and has been open for more than thirty years. Using an algebraic\napproach, we prove that DRPP has a randomized algorithm of running time\n$O^*(2^k)$ when $\\ell$ is bounded by a polynomial in the number of vertices in\n$D$. We also show that the same result holds for the undirected version of\nDRPP, where $D$ is a connected undirected multigraph.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 15:45:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 15:44:29 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2013 07:20:22 GMT"}, {"version": "v4", "created": "Mon, 31 Mar 2014 15:29:59 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Gutin", "Gregory", ""], ["Wahlstrom", "Magnus", ""], ["Yeo", "Anders", ""]]}, {"id": "1308.2617", "submitter": "Bundit Laekhanukit", "authors": "Parinya Chalermsook, Bundit Laekhanukit, Danupon Nanongkai", "title": "Independent Set, Induced Matching, and Pricing: Connections and Tight\n  (Subexponential Time) Approximation Hardnesses", "comments": "The full version of FOCS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a series of almost settled inapproximability results for three\nfundamental problems. The first in our series is the subexponential-time\ninapproximability of the maximum independent set problem, a question studied in\nthe area of parameterized complexity. The second is the hardness of\napproximating the maximum induced matching problem on bounded-degree bipartite\ngraphs. The last in our series is the tight hardness of approximating the\nk-hypergraph pricing problem, a fundamental problem arising from the area of\nalgorithmic game theory. In particular, assuming the Exponential Time\nHypothesis, our two main results are:\n  - For any r larger than some constant, any r-approximation algorithm for the\nmaximum independent set problem must run in at least\n2^{n^{1-\\epsilon}/r^{1+\\epsilon}} time. This nearly matches the upper bound of\n2^{n/r} (Cygan et al., 2008). It also improves some hardness results in the\ndomain of parameterized complexity (e.g., Escoffier et al., 2012 and Chitnis et\nal., 2013)\n  - For any k larger than some constant, there is no polynomial time min\n(k^{1-\\epsilon}, n^{1/2-\\epsilon})-approximation algorithm for the k-hypergraph\npricing problem, where n is the number of vertices in an input graph. This\nalmost matches the upper bound of min (O(k), \\tilde O(\\sqrt{n})) (by Balcan and\nBlum, 2007 and an algorithm in this paper).\n  We note an interesting fact that, in contrast to n^{1/2-\\epsilon} hardness\nfor polynomial-time algorithms, the k-hypergraph pricing problem admits\nn^{\\delta} approximation for any \\delta >0 in quasi-polynomial time. This puts\nthis problem in a rare approximability class in which approximability\nthresholds can be improved significantly by allowing algorithms to run in\nquasi-polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 16:39:11 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2013 22:23:03 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Laekhanukit", "Bundit", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1308.2858", "submitter": "Sebastian Ordyniak", "authors": "Jakub Gajarsk\\'y and Michael Lampis and Sebastian Ordyniak", "title": "Parameterized Algorithms for Modular-Width", "comments": "to appear in IPEC 2013. arXiv admin note: text overlap with\n  arXiv:1304.5479 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that a number of natural graph problems which are FPT\nparameterized by treewidth become W-hard when parameterized by clique-width. It\nis therefore desirable to find a different structural graph parameter which is\nas general as possible, covers dense graphs but does not incur such a heavy\nalgorithmic penalty.\n  The main contribution of this paper is to consider a parameter called\nmodular-width, defined using the well-known notion of modular decompositions.\nUsing a combination of ILPs and dynamic programming we manage to design FPT\nalgorithms for Coloring and Partitioning into paths (and hence Hamiltonian path\nand Hamiltonian cycle), which are W-hard for both clique-width and its recently\nintroduced restriction, shrub-depth. We thus argue that modular-width occupies\na sweet spot as a graph parameter, generalizing several simpler notions on\ndense graphs but still evading the \"price of generality\" paid by clique-width.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 13:22:02 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 10:21:27 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Gajarsk\u00fd", "Jakub", ""], ["Lampis", "Michael", ""], ["Ordyniak", "Sebastian", ""]]}, {"id": "1308.2891", "submitter": "N. A. Carella", "authors": "N. A. Carella", "title": "Deterministic Integer Factorization Algorithms", "comments": "Six Pages, Improved Version. arXiv admin note: substantial text\n  overlap with arXiv:1003.3261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents a deterministic integer factorization algorithm of running\ntime complexity O(N^(1/6+e)), e > 0. This improves the current performances of\ndeterministic integer factorization algorithms rated at O(N^(1/4+e)) arithmetic\noperations. Equivalently, given the least (log N)/6 bits of a factor of N = pq,\nthe algorithm factors the integer in polynomial time O(log(N)^c), c > 0\nconstant.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 22:06:03 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 14:14:04 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Carella", "N. A.", ""]]}, {"id": "1308.2954", "submitter": "Robert Kleinberg", "authors": "Bruno Abrahao and Flavio Chierichetti and Robert Kleinberg and\n  Alessandro Panconesi", "title": "Trace Complexity of Network Inference", "comments": "25 pages, preliminary version appeared in Proceedings of the 19th ACM\n  SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\n  2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network inference problem consists of reconstructing the edge set of a\nnetwork given traces representing the chronology of infection times as\nepidemics spread through the network. This problem is a paradigmatic\nrepresentative of prediction tasks in machine learning that require deducing a\nlatent structure from observed patterns of activity in a network, which often\nrequire an unrealistically large number of resources (e.g., amount of available\ndata, or computational time). A fundamental question is to understand which\nproperties we can predict with a reasonable degree of accuracy with the\navailable resources, and which we cannot. We define the trace complexity as the\nnumber of distinct traces required to achieve high fidelity in reconstructing\nthe topology of the unobserved network or, more generally, some of its\nproperties. We give algorithms that are competitive with, while being simpler\nand more efficient than, existing network inference approaches. Moreover, we\nprove that our algorithms are nearly optimal, by proving an\ninformation-theoretic lower bound on the number of traces that an optimal\ninference algorithm requires for performing this task in the general case.\nGiven these strong lower bounds, we turn our attention to special cases, such\nas trees and bounded-degree graphs, and to property recovery tasks, such as\nreconstructing the degree distribution without inferring the network. We show\nthat these problems require a much smaller (and more realistic) number of\ntraces, making them potentially solvable in practice.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 19:36:53 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Abrahao", "Bruno", ""], ["Chierichetti", "Flavio", ""], ["Kleinberg", "Robert", ""], ["Panconesi", "Alessandro", ""]]}, {"id": "1308.3326", "submitter": "Gelin Zhou", "authors": "Gelin Zhou", "title": "Sorted Range Reporting Revisited", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two-dimensional sorted range reporting problem. Our data\nstructure requires O(n lglg n) words of space and O(lglg n + k lglg n) query\ntime, where k is the number of points in the query range. This data structure\nimproves a recent result of Nekrich and Navarro [8] by a factor of O(lglg n) in\nquery time, and matches the state of the art for unsorted range reporting [1].\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 07:38:16 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Zhou", "Gelin", ""]]}, {"id": "1308.3336", "submitter": "Piotr Sankowski", "authors": "Jakub {\\L}\\k{a}cki and Jakub O\\'cwieja and Marcin Pilipczuk and Piotr\n  Sankowski and Anna Zych", "title": "The Power of Dynamic Distance Oracles: Efficient Dynamic Algorithms for\n  the Steiner Tree", "comments": "Full version of the paper accepted to STOC'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the Steiner tree problem over a dynamic set of\nterminals. We consider the model where we are given an $n$-vertex graph\n$G=(V,E,w)$ with positive real edge weights, and our goal is to maintain a tree\nwhich is a good approximation of the minimum Steiner tree spanning a terminal\nset $S \\subseteq V$, which changes over time. The changes applied to the\nterminal set are either terminal additions (incremental scenario), terminal\nremovals (decremental scenario), or both (fully dynamic scenario). Our task\nhere is twofold. We want to support updates in sublinear $o(n)$ time, and keep\nthe approximation factor of the algorithm as small as possible. We show that we\ncan maintain a $(6+\\varepsilon)$-approximate Steiner tree of a general graph in\n$\\tilde{O}(\\sqrt{n} \\log D)$ time per terminal addition or removal. Here, $D$\ndenotes the stretch of the metric induced by $G$. For planar graphs we achieve\nthe same running time and the approximation ratio of $(2+\\varepsilon)$.\nMoreover, we show faster algorithms for incremental and decremental scenarios.\nFinally, we show that if we allow higher approximation ratio, even more\nefficient algorithms are possible. In particular we show a polylogarithmic time\n$(4+\\varepsilon)$-approximate algorithm for planar graphs.\n  One of the main building blocks of our algorithms are dynamic distance\noracles for vertex-labeled graphs, which are of independent interest. We also\nimprove and use the online algorithms for the Steiner tree problem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 08:32:39 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 09:03:38 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 08:34:25 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 11:06:23 GMT"}, {"version": "v5", "created": "Fri, 24 Jun 2016 10:55:49 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["O\u0107wieja", "Jakub", ""], ["Pilipczuk", "Marcin", ""], ["Sankowski", "Piotr", ""], ["Zych", "Anna", ""]]}, {"id": "1308.3370", "submitter": "Tamara Mchedlidze David", "authors": "Tamara Mchedlidze, Martin N\\\"ollenburg, Ignaz Rutter", "title": "Drawing Planar Graphs with a Prescribed Inner Face", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a plane graph $G$ (i.e., a planar graph with a fixed planar embedding)\nand a simple cycle $C$ in $G$ whose vertices are mapped to a convex polygon, we\nconsider the question whether this drawing can be extended to a planar\nstraight-line drawing of $G$. We characterize when this is possible in terms of\nsimple necessary conditions, which we prove to be sufficient. This also leads\nto a linear-time testing algorithm. If a drawing extension exists, it can be\ncomputed in the same running time.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 12:07:03 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Mchedlidze", "Tamara", ""], ["N\u00f6llenburg", "Martin", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1308.3405", "submitter": "David Williamson", "authors": "Matthias Poloczek and David P. Williamson and Anke van Zuylen", "title": "On Some Recent MAX SAT Approximation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a number of randomized 3/4-approximation algorithms for MAX SAT have\nbeen proposed that all work in the same way: given a fixed ordering of the\nvariables, the algorithm makes a random assignment to each variable in\nsequence, in which the probability of assigning each variable true or false\ndepends on the current set of satisfied (or unsatisfied) clauses. To our\nknowledge, the first such algorithm was proposed by Poloczek and Schnitger; Van\nZuylen subsequently gave an algorithm that set the probabilities differently\nand had a simpler analysis. She also set up a framework for deriving such\nalgorithms. Buchbinder, Feldman, Naor, and Schwartz, as a special case of their\nwork on maximizing submodular functions, also give a randomized\n3/4-approximation algorithm for MAX SAT with the same structure as these\nprevious algorithms. In this note we give a gloss on the Buchbinder et al.\nalgorithm that makes it even simpler, and show that in fact it is equivalent to\nthe previous algorithm of Van Zuylen. We also show how it extends to a\ndeterministic LP rounding algorithm; such an algorithm was also given by Van\nZuylen.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 14:20:51 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Poloczek", "Matthias", ""], ["Williamson", "David P.", ""], ["van Zuylen", "Anke", ""]]}, {"id": "1308.3466", "submitter": "Frederik Mallmann-Trenn", "authors": "Petra Berenbrink, Funda Erg\\\"un, Frederik Mallmann-Trenn, and Erfan\n  Sadeqi Azer", "title": "Palindrome Recognition In The Streaming Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Palindrome Problem one tries to find all palindromes (palindromic\nsubstrings) in a given string. A palindrome is defined as a string which reads\nforwards the same as backwards, e.g., the string \"racecar\". A related problem\nis the Longest Palindromic Substring Problem in which finding an arbitrary one\nof the longest palindromes in the given string suffices. We regard the\nstreaming version of both problems. In the streaming model the input arrives\nover time and at every point in time we are only allowed to use sublinear\nspace. The main algorithms in this paper are the following: The first one is a\none-pass randomized algorithm that solves the Palindrome Problem. It has an\nadditive error and uses $O(\\sqrt n$) space. The second algorithm is a two-pass\nalgorithm which determines the exact locations of all longest palindromes. It\nuses the first algorithm as the first pass. The third algorithm is again a\none-pass randomized algorithm, which solves the Longest Palindromic Substring\nProblem. It has a multiplicative error using only $O(\\log(n))$ space. We also\ngive two variants of the first algorithm which solve other related practical\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 18:04:24 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 09:24:29 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 11:55:49 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Berenbrink", "Petra", ""], ["Erg\u00fcn", "Funda", ""], ["Mallmann-Trenn", "Frederik", ""], ["Azer", "Erfan Sadeqi", ""]]}, {"id": "1308.3520", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, MohammadTaghi Hajiaghayi and Guy Kortsarz", "title": "Fixed-Parameter and Approximation Algorithms: A New Look", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Fixed-Parameter Tractable (\\FPT) $\\rho$-approximation algorithm for a\nminimization (resp. maximization) parameterized problem $P$ is an FPT algorithm\nthat, given an instance $(x, k)\\in P$ computes a solution of cost at most $k\n\\cdot \\rho(k)$ (resp. $k/\\rho(k)$) if a solution of cost at most (resp. at\nleast) $k$ exists; otherwise the output can be arbitrary. For well-known\nintractable problems such as the W[1]-hard {Clique} and W[2]-hard {Set Cover}\nproblems, the natural question is whether we can get any \\FPT-approximation. It\nis widely believed that both {Clique} and {Set-Cover} admit no FPT\n$\\rho$-approximation algorithm, for any increasing function $\\rho$. Assuming\nstandard conjectures such as the Exponential Time Hypothesis (ETH)\n\\cite{eth-paturi} and the Projection Games Conjecture (PGC) \\cite{r3}, we make\nthe first progress towards proving this conjecture by showing that\n  1. Under the ETH and PGC, there exist constants $F_1, F_2 >0$ such that the\n{Set Cover} problem does not admit an FPT approximation algorithm with ratio\n$k^{F_1}$ in $2^{k^{F_2}}\\cdot \\text{poly}(N,M)$ time, where $N$ is the size of\nthe universe and $M$ is the number of sets.\n  2. Unless $\\NP\\subseteq \\SUBEXP$, for every $1> \\delta > 0$ there exists a\nconstant $F(\\delta)>0$ such that {Clique} has no FPT cost approximation with\nratio $k^{1-\\delta}$ in $2^{k^{F}}\\cdot \\text{poly}(n)$ time, where $n$ is the\nnumber of vertices in the graph.\n  In the second part of the paper we consider various W[1]-hard problems such\nas {\\dst}, {\\dsf}, Directed Steiner Network and {\\mec}. For all these problem\nwe give polynomial time $f(\\text{OPT})$-approximation algorithms for some small\nfunction $f$ (the largest approximation ratio we give is $\\text{OPT}^2$).\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 22:17:59 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Kortsarz", "Guy", ""]]}, {"id": "1308.3613", "submitter": "Liang Ding", "authors": "Liang Ding, Abdul Samad, Xingran Xue, Xiuzhen Huang, and Liming Cai", "title": "Polynomial kernels collapse the W-hierarchy", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that, for many parameterized problems in the class FPT, the\nexistence of polynomial kernels implies the collapse of the W-hierarchy (i.e.,\nW[P] = FPT). The collapsing results are also extended to assumed exponential\nkernels for problems in the class FPT. In particular, we establish a close\nrelationship between polynomial (and exponential) kernelizability and the\nexistence of sub-exponential time algorithms for a spectrum of circuit\nsatisfiability problems in FPT. To the best of our knowledge, this is the first\nwork that connects hardness for polynomial kernelizability of FPT problems to\nparameterized intractability. Our work also offers some new insights into the\nclass FPT.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 18:51:44 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Ding", "Liang", ""], ["Samad", "Abdul", ""], ["Xue", "Xingran", ""], ["Huang", "Xiuzhen", ""], ["Cai", "Liming", ""]]}, {"id": "1308.3665", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen", "title": "On Sparsification for Computing Treewidth", "comments": "21 pages. Full version of the extended abstract presented at IPEC\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We investigate whether an n-vertex instance (G,k) of Treewidth, asking\nwhether the graph G has treewidth at most k, can efficiently be made sparse\nwithout changing its answer. By giving a special form of OR-cross-composition,\nwe prove that this is unlikely: if there is an e > 0 and a polynomial-time\nalgorithm that reduces n-vertex Treewidth instances to equivalent instances, of\nan arbitrary problem, with O(n^{2-e}) bits, then NP is in coNP/poly and the\npolynomial hierarchy collapses to its third level.\n  Our sparsification lower bound has implications for structural\nparameterizations of Treewidth: parameterizations by measures that do not\nexceed the vertex count, cannot have kernels with O(k^{2-e}) bits for any e >\n0, unless NP is in coNP/poly. Motivated by the question of determining the\noptimal kernel size for Treewidth parameterized by vertex cover, we improve the\nO(k^3)-vertex kernel from Bodlaender et al. (STACS 2011) to a kernel with\nO(k^2) vertices. Our improved kernel is based on a novel form of\ntreewidth-invariant set. We use the q-expansion lemma of Fomin et al. (STACS\n2011) to find such sets efficiently in graphs whose vertex count is\nsuperquadratic in their vertex cover number.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 16:47:07 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Jansen", "Bart M. P.", ""]]}, {"id": "1308.3822", "submitter": "Steven Kearns", "authors": "Steven M. Kearns", "title": "Sublinear Matching With Finite Automata Using Reverse Suffix Scanning", "comments": "This version of the paper is a streamlined presentation that includes\n  the definition of Offsetting Finite Automata, which replaces the name\n  Accelerated Finite Automata in previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms to accelerate the computation of deterministic finite\nautomata (DFA) by calculating the state of a DFA n positions ahead utilizing a\nreverse scan of the next n characters. Often this requires scanning fewer than\nn characters resulting in a fraction of the input being skipped and a\ncommensurate increase in processing speed. The skipped fraction is > 80% in\nseveral of our examples. We introduce offsetting finite automata (OFA) to\nencode the accelerated computation. OFA generalize DFA by adding an integer\noffset to the current input index at each state transition. We give algorithms\nfor constructing an OFA that accepts the same language as a DFA while possibly\nskipping input, and for matching with an OFA. Compared to previous algorithms\nthat attempt to skip some of the input, the new matching algorithm can skip\nmore often and can skip farther. In the worst case the new matching algorithm\nscans the same number of characters as a simple forward scan, whereas previous\napproaches often scan more, so the new algorithm can be used as a reliable\nreplacement for the simple forward scan. Additionally, the new algorithm adapts\nto available memory and time constraints.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 01:45:04 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 00:42:10 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 18:23:44 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Kearns", "Steven M.", ""]]}, {"id": "1308.3829", "submitter": "Igor Razgon", "authors": "Igor Razgon", "title": "On OBDDs for CNFs of bounded treewidth", "comments": "Corollary 3 is added, separating OBDD and SDD in the classical\n  (non-parameterized) sense", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that a CNF cannot be compiled into an Ordered Binary\nDecision Diagram (OBDD) of fixed-parameter size parameterized by the primal\ngraph treewidth of the CNF. Thus we provide a parameterized separation between\nOBDDs and Sentential Decision Diagrams (SDDs) for which such fixed-parameter\ncompilation is possible. In fact, we demonstrate that the proposed lower bound\nalso yields a classical (non-parameterized) separation of OBDDs and SDDs. We\nalso show that the best existing parameterized upper bound for OBDDs in fact\nholds for incidence graph treewidth parameterization.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 04:21:54 GMT"}, {"version": "v2", "created": "Sat, 5 Jul 2014 20:18:29 GMT"}, {"version": "v3", "created": "Wed, 30 Jul 2014 15:26:00 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Razgon", "Igor", ""]]}, {"id": "1308.3946", "submitter": "Gregory Valiant", "authors": "Siu-On Chan and Ilias Diakonikolas and Gregory Valiant and Paul\n  Valiant", "title": "Optimal Algorithms for Testing Closeness of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of closeness testing for two discrete distributions.\nMore precisely, given samples from two distributions $p$ and $q$ over an\n$n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least\n$\\eps$-far from $q$, in either $\\ell_1$ or $\\ell_2$ distance. Batu et al. gave\nthe first sub-linear time algorithms for these problems, which matched the\nlower bounds of Valiant up to a logarithmic factor in $n$, and a polynomial\nfactor of $\\eps.$\n  In this work, we present simple (and new) testers for both the $\\ell_1$ and\n$\\ell_2$ settings, with sample complexity that is information-theoretically\noptimal, to constant factors, both in the dependence on $n$, and the dependence\non $\\eps$; for the $\\ell_1$ testing problem we establish that the sample\ncomplexity is $\\Theta(\\max\\{n^{2/3}/\\eps^{4/3}, n^{1/2}/\\eps^2 \\}).$\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 07:45:07 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "1308.4064", "submitter": "Augustine  Kwanashie", "authors": "Augustine Kwanashie and David F. Manlove", "title": "An Integer Programming Approach to the Hospital/Residents Problem with\n  Ties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical Hospitals/Residents problem (HR) models the assignment of\njunior doctors to hospitals based on their preferences over one another. In an\ninstance of this problem, a stable matching M is sought which ensures that no\nblocking pair can exist in which a resident r and hospital h can improve\nrelative to M by becoming assigned to each other. Such a situation is\nundesirable as it could naturally lead to r and h forming a private arrangement\noutside of the matching. The original HR model assumes that preference lists\nare strictly ordered. However in practice, this may be an unreasonable\nassumption: an agent may find two or more agents equally acceptable, giving\nrise to ties in its preference list. We thus obtain the Hospitals/Residents\nproblem with Ties (HRT). In such an instance, stable matchings may have\ndifferent sizes and MAX HRT, the problem of finding a maximum cardinality\nstable matching, is NP-hard. In this paper we describe an Integer Programming\n(IP) model for MAX HRT. We also provide some details on the implementation of\nthe model. Finally we present results obtained from an empirical evaluation of\nthe IP model based on real-world and randomly generated problem instances.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 15:46:38 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2013 17:14:03 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Kwanashie", "Augustine", ""], ["Manlove", "David F.", ""]]}, {"id": "1308.4291", "submitter": "Vincenzo Roselli", "authors": "Patrizio Angelini, Fabrizio Frati, Maurizio Patrignani, Vincenzo\n  Roselli", "title": "Morphing Planar Graphs Drawings Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A morph between two straight-line planar drawings of the same graph is a\ncontinuous transformation from the first to the second drawing such that\nplanarity is preserved at all times. Each step of the morph moves each vertex\nat constant speed along a straight line. Although the existence of a morph\nbetween any two drawings was established several decades ago, only recently it\nhas been proved that a polynomial number of steps suffices to morph any two\nplanar straight-line drawings. Namely, at SODA 2013, Alamdari et al.[1] proved\nthat any two planar straight-line drawings of a planar graph can be morphed in\nO(n^4) steps, while O(n^2) steps suffice if we restrict to maximal planar\ngraphs.\n  In this paper, we improve upon such results, by showing an algorithm to morph\nany two planar straight-line drawings of a planar graph in O(n^2) steps;\nfurther, we show that a morph with O(n) steps exists between any two planar\nstraight-line drawings of a series-parallel graph.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2013 12:51:45 GMT"}], "update_date": "2013-08-21", "authors_parsed": [["Angelini", "Patrizio", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1308.4469", "submitter": "Craig Dillabaugh Dr", "authors": "Craig Dillabaugh", "title": "External Memory Algorithms For Path Traversal in Graphs", "comments": "181 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents a number of results related to path traversal in trees\nand graphs. In particular, we focus on data structures which allow such\ntraversals to be performed efficiently in the external memory setting. In\naddition, for trees and planar graphs the data structures we present are\nsuccinct. Our tree structures permit efficient bottom-up path traversal in\nrooted trees of arbitrary degree and efficient top-down path traversal in\nbinary trees. In the graph setting, we permit efficient traversal of an\narbitrary path in bounded degree planar graphs. Our data structures for both\ntrees and graphs match or slightly improve current best results for external\nmemory path traversal in these settings while at the same time improving space\nbounds due to the succinct nature of our data structures. Employing our path\ntraversal structure for bounded degree planar graphs, we describe a number of\nuseful applications of this technique for triangular meshes in R^2. As an\nextension of the R^2 representation for triangular meshes we also present an\nefficient external memory representation for well-shaped tetrahedral meshes in\nR^3. The external memory representation we present is based on a partitioning\nscheme that matches the current best-known results for well-shaped tetrahedral\nmeshes. We describe applications of path traversal in tetrahedral meshes which\nare made efficient in the external memory setting using our structure. Finally,\nwe present a result on using jump-and-walk point location in well-shaped meshes\nin both R^2 and R^3. We demonstrate that, given an approximate nearest\nneighbour from among the vertices of a mesh, locating the simplex containing\nthe query point involves a constant length walk (path traversal) in the mesh.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 01:56:50 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Dillabaugh", "Craig", ""]]}, {"id": "1308.4534", "submitter": "Iain McBride", "authors": "P. Biro, D. F. Manlove, I. McBride", "title": "The Hospitals / Residents Problem with Couples: Complexity and Integer\n  Programming Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hospitals / Residents problem with Couples (HRC) is a generalisation of\nthe classical Hospitals / Resident problem (HR) that is important in practical\napplications because it models the case where couples submit joint preference\nlists over pairs of (typically geographically close) hospitals. In this paper\nwe give a new NP-completeness result for the problem of deciding whether a\nstable matching exists, in highly restricted instances of HRC. Further, we\npresent an Integer Programming (IP) model for HRC and extend it the case where\npreference lists can include ties. Also, we describe an empirical study of an\nIP model or HRC and its extension to the case where preference lists can\ninclude ties. This model was applied to randomly generated instances and also\nreal-world instances arising from previous matching runs of the Scottish\nFoundation Allocation Scheme, used to allocate junior doctors to hospitals in\nScotland.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 10:58:03 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 12:15:41 GMT"}, {"version": "v3", "created": "Tue, 27 May 2014 08:06:16 GMT"}, {"version": "v4", "created": "Wed, 28 May 2014 11:55:50 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Biro", "P.", ""], ["Manlove", "D. F.", ""], ["McBride", "I.", ""]]}, {"id": "1308.4670", "submitter": "Stephan Friedrichs", "authors": "S\\'andor P. Fekete and Stephan Friedrichs and Alexander Kr\\\"oller and\n  Christiane Schmidt", "title": "Facets for Art Gallery Problems", "comments": "29 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Art Gallery Problem (AGP) asks for placing a minimum number of stationary\nguards in a polygonal region P, such that all points in P are guarded. The\nproblem is known to be NP-hard, and its inherent continuous structure (with\nboth the set of points that need to be guarded and the set of points that can\nbe used for guarding being uncountably infinite) makes it difficult to apply a\nstraightforward formulation as an Integer Linear Program. We use an iterative\nprimal-dual relaxation approach for solving AGP instances to optimality. At\neach stage, a pair of LP relaxations for a finite candidate subset of primal\ncovering and dual packing constraints and variables is considered; these\ncorrespond to possible guard positions and points that are to be guarded.\n  Particularly useful are cutting planes for eliminating fractional solutions.\nWe identify two classes of facets, based on Edge Cover and Set Cover (SC)\ninequalities. Solving the separation problem for the latter is NP-complete, but\nexploiting the underlying geometric structure, we show that large subclasses of\nfractional SC solutions cannot occur for the AGP. This allows us to separate\nthe relevant subset of facets in polynomial time. We also characterize all\nfacets for finite AGP relaxations with coefficients in {0, 1, 2}.\n  Finally, we demonstrate the practical usefulness of our approach. Our cutting\nplane technique yields a significant improvement in terms of speed and solution\nquality due to considerably reduced integrality gaps as compared to the\napproach by Kr\\\"oller et al.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 19:32:06 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 10:23:04 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Friedrichs", "Stephan", ""], ["Kr\u00f6ller", "Alexander", ""], ["Schmidt", "Christiane", ""]]}, {"id": "1308.5207", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Christopher Kennedy and Amit Singer", "title": "Approximating the Little Grothendieck Problem over the Orthogonal and\n  Unitary Groups", "comments": "Updates in version 2: extension to the complex valued (unitary group)\n  case, sharper lower bounds on the approximation ratios, matching integrality\n  gap, and a generalized rank constrained version of the problem. Updates in\n  version 3: Improvement on the exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The little Grothendieck problem consists of maximizing\n$\\sum_{ij}C_{ij}x_ix_j$ over binary variables $x_i\\in\\{\\pm1\\}$, where C is a\npositive semidefinite matrix. In this paper we focus on a natural\ngeneralization of this problem, the little Grothendieck problem over the\northogonal group. Given C a dn x dn positive semidefinite matrix, the objective\nis to maximize $\\sum_{ij}Tr (C_{ij}^TO_iO_j^T)$ restricting $O_i$ to take\nvalues in the group of orthogonal matrices, where $C_{ij}$ denotes the (ij)-th\nd x d block of C. We propose an approximation algorithm, which we refer to as\nOrthogonal-Cut, to solve this problem and show a constant approximation ratio.\nOur method is based on semidefinite programming. For a given $d\\geq 1$, we show\na constant approximation ratio of $\\alpha_{R}(d)^2$, where $\\alpha_{R}(d)$ is\nthe expected average singular value of a d x d matrix with random Gaussian\n$N(0,1/d)$ i.i.d. entries. For d=1 we recover the known $\\alpha_{R}(1)^2=2/\\pi$\napproximation guarantee for the classical little Grothendieck problem. Our\nalgorithm and analysis naturally extends to the complex valued case also\nproviding a constant approximation ratio for the analogous problem over the\nUnitary Group.\n  Orthogonal-Cut also serves as an approximation algorithm for several\napplications, including the Procrustes problem where it improves over the best\npreviously known approximation ratio of~$\\frac1{2\\sqrt{2}}$. The little\nGrothendieck problem falls under the class of problems approximated by a recent\nalgorithm proposed in the context of the non-commutative Grothendieck\ninequality. Nonetheless, our approach is simpler and it provides a more\nefficient algorithm with better approximation ratios and matching integrality\ngaps.\n  Finally, we also provide an improved approximation algorithm for the more\ngeneral little Grothendieck problem over the orthogonal (or unitary) group with\nrank constraints.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 18:53:30 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 16:20:05 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2013 04:49:21 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2015 20:07:56 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Kennedy", "Christopher", ""], ["Singer", "Amit", ""]]}, {"id": "1308.5256", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Moses Charikar and Amit Singer and Andy Zhu", "title": "Multireference Alignment using Semidefinite Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multireference alignment problem consists of estimating a signal from\nmultiple noisy shifted observations. Inspired by existing Unique-Games\napproximation algorithms, we provide a semidefinite program (SDP) based\nrelaxation which approximates the maximum likelihood estimator (MLE) for the\nmultireference alignment problem. Although we show that the MLE problem is\nUnique-Games hard to approximate within any constant, we observe that our\npoly-time approximation algorithm for the MLE appears to perform quite well in\ntypical instances, outperforming existing methods. In an attempt to explain\nthis behavior we provide stability guarantees for our SDP under a random noise\nmodel on the observations. This case is more challenging to analyze than\ntraditional semi-random instances of Unique-Games: the noise model is on\nvertices of a graph and translates into dependent noise on the edges.\nInterestingly, we show that if certain positivity constraints in the SDP are\ndropped, its solution becomes equivalent to performing phase correlation, a\npopular method used for pairwise alignment in imaging applications. Finally, we\nshow how symmetry reduction techniques from matrix representation theory can\nsimplify the analysis and computation of the SDP, greatly decreasing its\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 22:19:33 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Charikar", "Moses", ""], ["Singer", "Amit", ""], ["Zhu", "Andy", ""]]}, {"id": "1308.5261", "submitter": "Francesco Sorrentino Dr.", "authors": "Afroza Shirin, Dionicio F. Rios, and Francesco Sorrentino", "title": "Observability of Nonlinear Complex Networks in the Presence of\n  Symmetries: A Graphical Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.PS cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the states of the nodes of a dynamical network is a problem of\nfundamental importance in the study of neuronal and genetic networks. An\nunderlying related problem is that of observability, i.e., identifying the\nconditions under which such a reconstruction is possible. In this paper we\nstudy observability of complex dynamical networks, where we consider the\neffects of network symmetries on observability. We present an efficient\nalgorithm that returns a minimal set of necessary sensor nodes for\nobservability in the presence of symmetries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 22:59:02 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 05:28:22 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 09:46:08 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Shirin", "Afroza", ""], ["Rios", "Dionicio F.", ""], ["Sorrentino", "Francesco", ""]]}, {"id": "1308.5444", "submitter": "Rad Niazadeh", "authors": "Rad Niazadeh, Robert D. Kleinberg", "title": "A Unified Approach to Online Allocation Algorithms via Randomized Dual\n  Fitting", "comments": "22 pages, submitted to WINE'13:The 9th Conference on Web and Internet\n  Economics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for designing and analyzing algorithms for\nonline budgeted allocation problems (including online matching) and their\ngeneralization, the Online Generalized Assignment Problem (OnGAP). These\nproblems have been intensively studied as models of how to allocate impressions\nfor online advertising. In contrast to previous analyses of online budgeted\nallocation algorithms (the so-called \"balance\" or \"water-filling\" family of\nalgorithms) our analysis is based on the method of randomized dual fitting,\nanalogous to the recent analysis of the RANKING algorithm for online matching\ndue to Devanur et al. Our main contribution is thus to provide a unified method\nof proof that simultaneously derives the optimal competitive ratio bounds for\nonline matching and online fractional budgeted allocation. The same method of\nproof also supplies $(1-1/e)$ competitive ratio bounds for greedy algorithms\nfor both problems, in the random order arrival model; this simplifies existing\nanalyses of greedy online allocation algorithms with random order of arrivals,\nwhile also strengthening them to apply to a larger family of greedy algorithms.\nFinally, for the more general OnGAP problem, we show that no algorithm can be\nconstant-competitive; instead we present an algorithm whose competitive ratio\ndepends logarithmically on a certain parameter of the problem instance, and we\nshow that this dependence cannot be improved.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2013 20:14:08 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Niazadeh", "Rad", ""], ["Kleinberg", "Robert D.", ""]]}, {"id": "1308.5741", "submitter": "Michael Bannister", "authors": "Michael J. Bannister and David Eppstein and Joseph A. Simons", "title": "Fixed parameter tractability of crossing minimization of almost-trees", "comments": "Graph Drawing 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate exact crossing minimization for graphs that differ from trees\nby a small number of additional edges, for several variants of the crossing\nminimization problem. In particular, we provide fixed parameter tractable\nalgorithms for the 1-page book crossing number, the 2-page book crossing\nnumber, and the minimum number of crossed edges in 1-page and 2-page book\ndrawings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 02:45:22 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Bannister", "Michael J.", ""], ["Eppstein", "David", ""], ["Simons", "Joseph A.", ""]]}, {"id": "1308.6145", "submitter": "Lars Bonnichsen", "authors": "Lars F. Bonnichsen, Sven Karlsson, Christian W. Probst", "title": "ELB-Trees, An Efficient and Lock-free B-tree Derivative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report is an extension of the paper of the same title, which\nis to appear at MUCOCOS'13. The technical report proves correctness of the\nELB-trees operations' semantics and that the operations are lock-free.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 12:45:42 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Bonnichsen", "Lars F.", ""], ["Karlsson", "Sven", ""], ["Probst", "Christian W.", ""]]}, {"id": "1308.6203", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou", "title": "Adaptive detection and severity level characterization algorithm for\n  Obstructive Sleep Apnea Hypopnea Syndrome (OSAHS) via oximetry signal\n  analysis", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": "HG/BIOINF.0813.28v1", "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, an abstract definition and formal specification is presented\nfor the task of adaptive-threshold OSAHS events detection and severity\ncharacterization. Specifically, a low-level pseudocode is designed for the\nalgorithm of raw oximetry signal pre-processing, calculation of the 'drop' and\n'rise' frames in the related time series, detection of valid apnea/hypopnea\nevents via SpO2 saturation level tracking, as well as calculation of\ncorresponding event rates for OSAHS severity characterization. The designed\nalgorithm can be used as the first module in a machine learning application\nwhere these data can be used as inputs or encoded into higher-level statistics\n(features) for pattern classifiers, in the context of computer-aided or fully\nautomated diagnosis of OSAHS and related pathologies.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 16:23:46 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Georgiou", "Harris V.", ""]]}, {"id": "1308.6273", "submitter": "Rong Ge", "authors": "Sanjeev Arora and Rong Ge and Ankur Moitra", "title": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse recovery we are given a matrix $A$ (the dictionary) and a vector of\nthe form $A X$ where $X$ is sparse, and the goal is to recover $X$. This is a\ncentral notion in signal processing, statistics and machine learning. But in\napplications such as sparse coding, edge detection, compression and super\nresolution, the dictionary $A$ is unknown and has to be learned from random\nexamples of the form $Y = AX$ where $X$ is drawn from an appropriate\ndistribution --- this is the dictionary learning problem. In most settings, $A$\nis overcomplete: it has more columns than rows. This paper presents a\npolynomial-time algorithm for learning overcomplete dictionaries; the only\npreviously known algorithm with provable guarantees is the recent work of\nSpielman, Wang and Wright who gave an algorithm for the full-rank case, which\nis rarely the case in applications. Our algorithm applies to incoherent\ndictionaries which have been a central object of study since they were\nintroduced in seminal work of Donoho and Huo. In particular, a dictionary is\n$\\mu$-incoherent if each pair of columns has inner product at most $\\mu /\n\\sqrt{n}$.\n  The algorithm makes natural stochastic assumptions about the unknown sparse\nvector $X$, which can contain $k \\leq c \\min(\\sqrt{n}/\\mu \\log n, m^{1/2\n-\\eta})$ non-zero entries (for any $\\eta > 0$). This is close to the best $k$\nallowable by the best sparse recovery algorithms even if one knows the\ndictionary $A$ exactly. Moreover, both the running time and sample complexity\ndepend on $\\log 1/\\epsilon$, where $\\epsilon$ is the target accuracy, and so\nour algorithms converge very quickly to the true dictionary. Our algorithm can\nalso tolerate substantial amounts of noise provided it is incoherent with\nrespect to the dictionary (e.g., Gaussian). In the noisy setting, our running\ntime and sample complexity depend polynomially on $1/\\epsilon$, and this is\nnecessary.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 19:57:31 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 19:46:05 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2013 19:34:59 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 18:35:17 GMT"}, {"version": "v5", "created": "Mon, 26 May 2014 17:38:58 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Moitra", "Ankur", ""]]}, {"id": "1308.6384", "submitter": "Carola Doerr", "authors": "Benjamin Doerr and Carola Doerr", "title": "Collecting Coupons with Random Initial Stake", "comments": null, "journal-ref": "Algorithmica 75 (2016), 529-553", "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a problem in the theory of randomized search heuristics, we give\na very precise analysis for the coupon collector problem where the collector\nstarts with a random set of coupons (chosen uniformly from all sets).\n  We show that the expected number of rounds until we have a coupon of each\ntype is $nH_{n/2} - 1/2 \\pm o(1)$, where $H_{n/2}$ denotes the $(n/2)$th\nharmonic number when $n$ is even, and $H_{n/2}:= (1/2) H_{\\lfloor n/2 \\rfloor}\n+ (1/2) H_{\\lceil n/2 \\rceil}$ when $n$ is odd. Consequently, the coupon\ncollector with random initial stake is by half a round faster than the one\nstarting with exactly $n/2$ coupons (apart from additive $o(1)$ terms).\n  This result implies that classic simple heuristic called \\emph{randomized\nlocal search} needs an expected number of $nH_{n/2} - 1/2 \\pm o(1)$ iterations\nto find the optimum of any monotonic function defined on bit-strings of length\n$n$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 07:45:51 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""]]}, {"id": "1308.6509", "submitter": "Pawel Gawrychowski", "authors": "Pawel Gawrychowski and Damian Straszak", "title": "Beating O(nm) in approximate LZW-compressed pattern matching", "comments": "This is the full version of an extended abstract to appear in\n  ISAAC'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an LZW/LZ78 compressed text, we want to find an approximate occurrence\nof a given pattern of length m. The goal is to achieve time complexity\ndepending on the size n of the compressed representation of the text instead of\nits length. We consider two specific definitions of approximate matching,\nnamely the Hamming distance and the edit distance, and show how to achieve\nO(nm^0.5k^2) and O(nm^0.5k^3) running time, respectively, where k is the bound\non the distance. Both algorithms use just linear space. Even for very small\nvalues of k, the best previously known solutions required O(nm) time. Our main\ncontribution is applying a periodicity-based argument in a way that is\ncomputationally effective even if we need to operate on a compressed\nrepresentation of a string, while the previous solutions were either based on a\ndynamic programming, or a black-box application of tools developed for\nuncompressed strings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 16:16:45 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2013 21:20:11 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Straszak", "Damian", ""]]}, {"id": "1308.6627", "submitter": "James Y. Zhao", "authors": "James Y. Zhao", "title": "Expand and Contract: Sampling graphs with given degrees and other\n  combinatorial families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from combinatorial families can be difficult. However, complicated\nfamilies can often be embedded within larger, simpler ones, for which easy\nsampling algorithms are known. We take advantage of such a relationship to\ndescribe a sampling algorithm for the smaller family, via a Markov chain\nstarted at a random sample of the larger family. The utility of the method is\ndemonstrated via several examples, with particular emphasis on sampling\nlabelled graphs with given degree sequence, a well-studied problem for which\nexisting algorithms leave much room for improvement. For graphs with given\ndegrees, with maximum degree $O(m^{1/4})$ where $m$ is the number of edges, we\nobtain an asymptotically uniform sample in $O(m)$ steps, which substantially\nimproves upon existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 23:35:14 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Zhao", "James Y.", ""]]}, {"id": "1308.6635", "submitter": "Rui Ferreira", "authors": "Rui Ferreira", "title": "Efficiently Listing Combinatorial Patterns in Graphs", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are extremely versatile and ubiquitous mathematical structures with\npotential to model a wide range of domains. For this reason, graph problems\nhave been of interest since the early days of computer science. Some of these\nproblems consider substructures of a graph that have certain properties. These\nsubstructures of interest, generally called patterns, are often meaningful in\nthe domain being modeled. Classic examples of patterns include spanning trees,\ncycles and subgraphs.\n  This thesis focuses on the topic of explicitly listing all the patterns\nexisting in an input graph. One of the defining features of this problem is\nthat the number of patterns is frequently exponential on the size of the input\ngraph. Thus, the time complexity of listing algorithms is parameterized by the\nsize of the output.\n  The main contribution of this work is the presentation of optimal algorithms\nfor four different problems of listing patterns in graphs, namely the listing\nof k-subtrees, k-subgraphs, st-paths and cycles. The algorithms presented are\nframed within the same generic approach, based in a recursive partition of the\nsearch space that divides the problem into subproblems. The key to an efficient\nimplementation of this approach is to avoid recursing into subproblems that do\nnot list any patterns. With this goal in sight, a dynamic data structure,\ncalled the certificate, is introduced and maintained throughout the recursion.\nMoreover, properties of the recursion tree and lower bounds on the number of\npatterns are used to amortize the cost of the algorithm on the size of the\noutput.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 02:10:19 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Ferreira", "Rui", ""]]}, {"id": "1308.6711", "submitter": "Pawe{\\l} Pszona", "authors": "Michael T. Goodrich and Pawe{\\l} Pszona", "title": "Streamed Graph Drawing and the File Maintenance Problem", "comments": "16 pages, 9 figures; to appear at the 21st International Symposium on\n  Graph Drawing (GD 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In streamed graph drawing, a planar graph, G, is given incrementally as a\ndata stream and a straight-line drawing of G must be updated after each new\nedge is released. To preserve the mental map, changes to the drawing should be\nminimized after each update, and Binucci et al.show that exponential area is\nnecessary and sufficient for a number of streamed graph drawings for trees if\nedges are not allowed to move at all. We show that a number of streamed graph\ndrawings can, in fact, be done with polynomial area, including planar streamed\ngraph drawings of trees, tree-maps, and outerplanar graphs, if we allow for a\nsmall number of coordinate movements after each update. Our algorithms involve\nan interesting connection to a classic algorithmic problem - the file\nmaintenance problem - and we also give new algorithms for this problem in a\nframework where bulk memory moves are allowed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 11:10:39 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Pszona", "Pawe\u0142", ""]]}, {"id": "1308.6730", "submitter": "Pawe{\\l} Pszona", "authors": "Michael T. Goodrich and Pawe{\\l} Pszona", "title": "Achieving Good Angular Resolution in 3D Arc Diagrams", "comments": "12 pages, 5 figures; to appear at the 21st International Symposium on\n  Graph Drawing (GD 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a three-dimensional analogue to the well-known graph visualization\napproach known as arc diagrams. We provide several algorithms that achieve good\nangular resolution for 3D arc diagrams, even for cases when the arcs must\nproject to a given 2D straight-line drawing of the input graph. Our methods\nmake use of various graph coloring algorithms, including an algorithm for a new\ncoloring problem, which we call localized edge coloring.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 12:56:55 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Pszona", "Pawe\u0142", ""]]}, {"id": "1308.6778", "submitter": "Benjamin Niedermann", "authors": "Therese Biedl, Thomas Bl\\\"asius, Benjamin Niedermann, Martin\n  N\\\"ollenburg, Roman Prutkin, Ignaz Rutter", "title": "Using ILP/SAT to determine pathwidth, visibility representations, and\n  other grid-based graph drawings", "comments": "Full version of GD 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and versatile formulation of grid-based graph\nrepresentation problems as an integer linear program (ILP) and a corresponding\nSAT instance. In a grid-based representation vertices and edges correspond to\naxis-parallel boxes on an underlying integer grid; boxes can be further\nconstrained in their shapes and interactions by additional problem-specific\nconstraints. We describe a general d-dimensional model for grid representation\nproblems. This model can be used to solve a variety of NP-hard graph problems,\nincluding pathwidth, bandwidth, optimum st-orientation, area-minimal (bar-k)\nvisibility representation, boxicity-k graphs and others. We implemented\nSAT-models for all of the above problems and evaluated them on the Rome graphs\ncollection. The experiments show that our model successfully solves NP-hard\nproblems within few minutes on small to medium-size Rome graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 15:46:54 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 10:16:18 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Biedl", "Therese", ""], ["Bl\u00e4sius", "Thomas", ""], ["Niedermann", "Benjamin", ""], ["N\u00f6llenburg", "Martin", ""], ["Prutkin", "Roman", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1308.6801", "submitter": "Ignaz Rutter", "authors": "Michael A. Bekos and Sabine Cornelsen and Martin Fink and Seokhee Hong\n  and Michael Kaufmann and Martin N\\\"ollenburg and Ignaz Rutter and Antonios\n  Symvonis", "title": "Many-to-One Boundary Labeling with Backbones", "comments": "23 pages, 10 figures, this is the full version of a paper that is\n  about to appear in GD'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study \\emph{many-to-one boundary labeling with backbone\nleaders}. In this new many-to-one model, a horizontal backbone reaches out of\neach label into the feature-enclosing rectangle. Feature points that need to be\nconnected to this label are linked via vertical line segments to the backbone.\nWe present dynamic programming algorithms for label number and total leader\nlength minimization of crossing-free backbone labelings. When crossings are\nallowed, we aim to obtain solutions with the minimum number of crossings. This\ncan be achieved efficiently in the case of fixed label order, however, in the\ncase of flexible label order we show that minimizing the number of leader\ncrossings is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 17:13:57 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Bekos", "Michael A.", ""], ["Cornelsen", "Sabine", ""], ["Fink", "Martin", ""], ["Hong", "Seokhee", ""], ["Kaufmann", "Michael", ""], ["N\u00f6llenburg", "Martin", ""], ["Rutter", "Ignaz", ""], ["Symvonis", "Antonios", ""]]}]