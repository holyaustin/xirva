[{"id": "1909.00171", "submitter": "Brian Axelrod", "authors": "Brian Axelrod, Yang P. Liu, Aaron Sidford", "title": "Near-optimal Approximate Discrete and Continuous Submodular Function\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide improved running times and oracle complexities for\napproximately minimizing a submodular function. Our main result is a randomized\nalgorithm, which given any submodular function defined on $n$-elements with\nrange $[-1, 1]$, computes an $\\epsilon$-additive approximate minimizer in\n$\\tilde{O}(n/\\epsilon^2)$ oracle evaluations with high probability. This\nimproves over the $\\tilde{O}(n^{5/3}/\\epsilon^2)$ oracle evaluation algorithm\nof Chakrabarty \\etal~(STOC 2017) and the $\\tilde{O}(n^{3/2}/\\epsilon^2)$ oracle\nevaluation algorithm of Hamoudi \\etal.\n  Further, we leverage a generalization of this result to obtain efficient\nalgorithms for minimizing a broad class of nonconvex functions. For any\nfunction $f$ with domain $[0, 1]^n$ that satisfies $\\frac{\\partial^2f}{\\partial\nx_i \\partial x_j} \\le 0$ for all $i \\neq j$ and is $L$-Lipschitz with respect\nto the $L^\\infty$-norm we give an algorithm that computes an\n$\\epsilon$-additive approximate minimizer with $\\tilde{O}(n \\cdot\n\\mathrm{poly}(L/\\epsilon))$ function evaluation with high probability.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 09:09:08 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Axelrod", "Brian", ""], ["Liu", "Yang P.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1909.00398", "submitter": "Yair Censor", "authors": "Yair Censor and Eliahu Levy", "title": "An analysis of the superiorization method via the principle of\n  concentration of measure", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superiorization methodology is intended to work with input data of\nconstrained minimization problems, i.e., a target function and a constraints\nset. However, it is based on an antipodal way of thinking to the thinking that\nleads constrained minimization methods. Instead of adapting unconstrained\nminimization algorithms to handling constraints, it adapts feasibility-seeking\nalgorithms to reduce (not necessarily minimize) target function values. This is\ndone while retaining the feasibility-seeking nature of the algorithm and\nwithout paying a high computational price. A guarantee that the local target\nfunction reduction steps properly accumulate to a global target function value\nreduction is still missing in spite of an ever-growing body of publications\nthat supply evidence of the success of the superiorization method in various\nproblems. We propose an analysis based on the principle of concentration of\nmeasure that attempts to alleviate the guarantee question of the\nsuperiorization method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 13:32:15 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Censor", "Yair", ""], ["Levy", "Eliahu", ""]]}, {"id": "1909.00530", "submitter": "Avery Miller", "authors": "Shahin Kamali, Avery Miller, Kenny Zhang", "title": "Burning Two Worlds: Algorithms for Burning Dense and Tree-like Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph burning is a simple model for the spread of social influence in\nnetworks. The objective is to measure how quickly a fire (e.g., a piece of fake\nnews) can be spread in a network. The burning process takes place in discrete\nrounds. In each round, a new fire breaks out at a selected vertex and burns it.\nMeanwhile, the old fires extend to their neighbours and burn them. A burning\nschedule selects where the new fire breaks out in each round, and the burning\nproblem asks for a schedule that burns all vertices in a minimum number of\nrounds, termed the burning number of the graph. The burning problem is known to\nbe NP-hard even when the graph is a tree or a disjoint set of paths. For\nconnected graphs, it has been conjectured that burning takes at most $\\lceil\n\\sqrt{n} \\rceil$ rounds.\n  We approach the algorithmic study of graph burning from two directions.\nFirst, we consider graphs with minimum degree $\\delta$. We present an algorithm\nthat burns any graph of size $n$ in at most $\\sqrt{\\frac{24n}{\\delta+1}}$\nrounds. In particular, for dense graphs with $\\delta \\in \\Theta(n)$, all\nvertices are burned in a constant number of rounds. More interestingly, even\nwhen $\\delta$ is a constant that is independent of the graph size, our\nalgorithm answers the graph-burning conjecture in the affirmative by burning\nthe graph in at most $\\lceil \\sqrt{n} \\rceil$ rounds. Next, we consider burning\ngraphs with bounded path-length or tree-length. These include many graph\nfamilies including connected interval graphs and connected chordal graphs. We\nshow that any graph with path-length $pl$ and diameter $d$ can be burned in\n$\\lceil \\sqrt{d-1} \\rceil + pl$ rounds. Our algorithm ensures an approximation\nratio of $1+o(1)$ for graphs of bounded path-length. We introduce another\nalgorithm that achieves an approximation ratio of $2+o(1)$ for burning graphs\nof bounded tree-length.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 03:47:57 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 02:50:01 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kamali", "Shahin", ""], ["Miller", "Avery", ""], ["Zhang", "Kenny", ""]]}, {"id": "1909.00740", "submitter": "Haris Aziz", "authors": "Haris Aziz and Herve Moulin and Fedor Sandomirskiy", "title": "A polynomial-time algorithm for computing a Pareto optimal and almost\n  proportional allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fair allocation of indivisible items under additive utilities.\nWhen the utilities can be negative, the existence and complexity of an\nallocation that satisfies Pareto optimality and proportionality up to one item\n(PROP1) is an open problem. We show that there exists a strongly\npolynomial-time algorithm that always computes an allocation satisfying Pareto\noptimality and proportionality up to one item even if the utilities are mixed\nand the agents have asymmetric weights. We point out that the result does not\nhold if either of Pareto optimality or PROP1 is replaced with slightly stronger\nconcepts.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:41:15 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 23:31:54 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Aziz", "Haris", ""], ["Moulin", "Herve", ""], ["Sandomirskiy", "Fedor", ""]]}, {"id": "1909.00844", "submitter": "Krzysztof Nowicki", "authors": "Mohsen Ghaffari, Krzysztof Nowicki, Mikkel Thorup", "title": "Faster Algorithms for Edge Connectivity via Random $2$-Out Contractions", "comments": "algorithms and data structures, graph algorithms, edge connectivity,\n  out-contractions, randomized algorithms, distributed algorithms, massively\n  parallel computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple new randomized contraction approach to the global minimum\ncut problem for simple undirected graphs. The contractions exploit 2-out edge\nsampling from each vertex rather than the standard uniform edge sampling. We\ndemonstrate the power of our new approach by obtaining better algorithms for\nsequential, distributed, and parallel models of computation. Our end results\ninclude the following randomized algorithms for computing edge connectivity\nwith high probability:\n  -- Two sequential algorithms with complexities $O(m \\log n)$ and $O(m+n\n\\log^3 n)$. These improve on a long line of developments including a celebrated\n$O(m \\log^3 n)$ algorithm of Karger [STOC'96] and the state of the art $O(m\n\\log^2 n (\\log\\log n)^2)$ algorithm of Henzinger et al. [SODA'17]. Moreover,\nour $O(m+n \\log^3 n)$ algorithm is optimal whenever $m = \\Omega(n \\log^3 n)$.\nWithin our new time bounds, whp, we can also construct the cactus\nrepresentation of all minimal cuts.\n  -- An $\\~O(n^{0.8} D^{0.2} + n^{0.9})$ round distributed algorithm, where D\ndenotes the graph diameter. This improves substantially on a recent\nbreakthrough of Daga et al. [STOC'19], which achieved a round complexity of\n$\\~O(n^{1-1/353}D^{1/353} + n^{1-1/706})$, hence providing the first sublinear\ndistributed algorithm for exactly computing the edge connectivity.\n  -- The first $O(1)$ round algorithm for the massively parallel computation\nsetting with linear memory per machine.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:49:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Nowicki", "Krzysztof", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1909.00899", "submitter": "Roman Snytsar", "authors": "Roman Snytsar", "title": "De(con)struction of the lazy-F loop: improving performance of Smith\n  Waterman alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Striped variation of the Smith-Waterman algorithm is known as extremely\nefficient and easily adaptable for the SIMD architectures. However, the\npotential for improvement has not been exhausted yet. The popular Lazy-F loop\nheuristic requires additional memory access operations, and the worst-case\nperformance of the loop could be as bad as the nonvectorized version. We\ndemonstrate the progression of the lazy-F loop transformations that improve the\nloop performance, and ultimately eliminate the loop completely. Our algorithm\nachieves the best asymptotic performance of all scan-based SW algorithms\nO(n/p+log(p)), and is very efficient in practice.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:37:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Snytsar", "Roman", ""]]}, {"id": "1909.01060", "submitter": "Florian Adriaens", "authors": "Florian Adriaens, Cigdem Aslay, Tijl De Bie, Aristides Gionis, Jefrey\n  Lijffijt", "title": "Discovering Interesting Cycles in Directed Graphs", "comments": "Accepted for CIKM'19", "journal-ref": null, "doi": "10.1145/3357384.3357970", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cycles in graphs often signify interesting processes. For example, cyclic\ntrading patterns can indicate inefficiencies or economic dependencies in trade\nnetworks, cycles in food webs can identify fragile dependencies in ecosystems,\nand cycles in financial transaction networks can be an indication of money\nlaundering. Identifying such interesting cycles, which can also be constrained\nto contain a given set of query nodes, although not extensively studied, is\nthus a problem of considerable importance. In this paper, we introduce the\nproblem of discovering interesting cycles in graphs. We first address the\nproblem of quantifying the extent to which a given cycle is interesting for a\nparticular analyst. We then show that finding cycles according to this\ninterestingness measure is related to the longest cycle and maximum mean-weight\ncycle problems (in the unconstrained setting) and to the maximum Steiner cycle\nand maximum mean Steiner cycle problems (in the constrained setting). A\ncomplexity analysis shows that finding interesting cycles is NP-hard, and is\nNP-hard to approximate within a constant factor in the unconstrained setting,\nand within a factor polynomial in the input size for the constrained setting.\nThe latter inapproximability result implies a similar result for the maximum\nSteiner cycle and maximum mean Steiner cycle problems. Motivated by these\nhardness results, we propose a number of efficient heuristic algorithms. We\nverify the effectiveness of the proposed methods and demonstrate their\npractical utility on two real-world use cases: a food web and an international\ntrade-network dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:58:03 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Adriaens", "Florian", ""], ["Aslay", "Cigdem", ""], ["De Bie", "Tijl", ""], ["Gionis", "Aristides", ""], ["Lijffijt", "Jefrey", ""]]}, {"id": "1909.01152", "submitter": "Anne-Sophie Himmel", "authors": "Anne-Sophie Himmel, Matthias Bentert, Andr\\'e Nichterlein, and Rolf\n  Niedermeier", "title": "Efficient Computation of Optimal Temporal Walks under Waiting-Time\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node connectivity plays a central role in temporal network analysis. We\nprovide a comprehensive study of various concepts of walks in temporal graphs,\nthat is, graphs with fixed vertex sets but edge sets changing over time. Taking\ninto account the temporal aspect leads to a rich set of optimization criteria\nfor \"shortest\" walks. Extending and significantly broadening state-of-the-art\nwork of Wu et al. [IEEE TKDE 2016], we provide an algorithm for computing\noptimal walks that is capable to deal with various optimization criteria and\nany linear combination of these. It runs in $O (|V| + |E| \\log |E|)$ time where\n$|V|$ is the number of vertices and $|E|$ is the number of time edges. A\ncentral distinguishing factor to Wu et al.'s work is that our model allows to,\nmotivated by real-world applications, respect waiting-time constraints for\nvertices, that is, the minimum and maximum waiting time allowed in intermediate\nvertices of a walk. Moreover, other than Wu et al. our algorithm also allows to\nsearch for walks that pass multiple subsequent edges in one time step, and it\ncan optimize a richer set of optimization criteria. Our experimental studies\nindicate that our richer modeling can be achieved without significantly\nworsening the running time when compared to Wu et al.'s algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 17:00:31 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 17:02:46 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Himmel", "Anne-Sophie", ""], ["Bentert", "Matthias", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1909.01410", "submitter": "Amir Zandieh", "authors": "Thomas D. Ahle, Michael Kapralov, Jakob B. T. Knudsen, Rasmus Pagh,\n  Ameya Velingker, David Woodruff, Amir Zandieh", "title": "Oblivious Sketching of High-Degree Polynomial Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are fundamental tools in machine learning that allow detection\nof non-linear dependencies between data without explicitly constructing feature\nvectors in high dimensional spaces. A major disadvantage of kernel methods is\ntheir poor scalability: primitives such as kernel PCA or kernel ridge\nregression generally take prohibitively large quadratic space and (at least)\nquadratic time, as kernel matrices are usually dense. Some methods for speeding\nup kernel linear algebra are known, but they all invariably take time\nexponential in either the dimension of the input point set (e.g., fast\nmultipole methods suffer from the curse of dimensionality) or in the degree of\nthe kernel function.\n  Oblivious sketching has emerged as a powerful approach to speeding up\nnumerical linear algebra over the past decade, but our understanding of\noblivious sketching solutions for kernel matrices has remained quite limited,\nsuffering from the aforementioned exponential dependence on input parameters.\nOur main contribution is a general method for applying sketching solutions\ndeveloped in numerical linear algebra over the past decade to a tensoring of\ndata points without forming the tensoring explicitly. This leads to the first\noblivious sketch for the polynomial kernel with a target dimension that is only\npolynomially dependent on the degree of the kernel function, as well as the\nfirst oblivious sketch for the Gaussian kernel on bounded datasets that does\nnot suffer from an exponential dependence on the dimensionality of input data\npoints.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 19:20:00 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 15:26:17 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 15:05:15 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 09:00:04 GMT"}, {"version": "v5", "created": "Tue, 22 Dec 2020 09:19:11 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Kapralov", "Michael", ""], ["Knudsen", "Jakob B. T.", ""], ["Pagh", "Rasmus", ""], ["Velingker", "Ameya", ""], ["Woodruff", "David", ""], ["Zandieh", "Amir", ""]]}, {"id": "1909.01554", "submitter": "Matti Karppa", "authors": "Matti Karppa and Petteri Kaski", "title": "Engineering Boolean Matrix Multiplication for Multiple-Accelerator\n  Shared-Memory Architectures", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study the problem of multiplying two bit matrices with entries either over\nthe Boolean algebra $(0,1,\\vee,\\wedge)$ or over the binary field\n$(0,1,+,\\cdot)$. We engineer high-performance open-source algorithm\nimplementations for contemporary multiple-accelerator shared-memory\narchitectures, with the objective of time-and-energy-efficient scaling up to\ninput sizes close to the available shared memory capacity. For example, given\ntwo terabinary-bit square matrices as input, our implementations compute the\nBoolean product in approximately 2100 seconds (1.0 Pbop/s at 3.3 pJ/bop for a\ntotal of 2.1 kWh/product) and the binary product in less than 950 seconds (2.4\neffective Pbop/s at 1.5 effective pJ/bop for a total of 0.92 kWh/product) on an\nNVIDIA DGX-1 with power consumption at peak system power (3.5 kW).\n  Our contributions are (a) for the binary product, we use alternative-basis\ntechniques of Karstadt and Schwartz [SPAA '17] to design novel\nalternative-basis variants of Strassen's recurrence for $2\\times 2$ block\nmultiplication [Numer. Math. 13 (1969)] that have been optimized for both the\nnumber of additions and low working memory, (b) structuring the parallel block\nrecurrences and the memory layout for coalescent and register-localized\nexecution on accelerator hardware, (c) low-level engineering of the innermost\nblock products for the specific target hardware, and (d) structuring the\ntop-level shared-memory implementation to feed the accelerators with data and\nintegrate the results for input and output sizes beyond the aggregate memory\ncapacity of the available accelerators.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 04:57:24 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Karppa", "Matti", ""], ["Kaski", "Petteri", ""]]}, {"id": "1909.01583", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Gerrymandering: A Briber's Perspective", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of bribery problem in the context of gerrymandering and\nreverse gerrymandering. In our most general problem, the input is a set of\nvoters having votes over a set of alternatives, a graph on the voters, a\npartition of voters into connected districts, cost of every voter for changing\nher district, a budget for the briber, and a favorite alternative of the\nbriber. The briber needs to compute if the given partition can be modified so\nthat (i) the favorite alternative of the briber wins the resulting election,\n(ii) the modification is budget feasible, and (iii) every new district is\nconnected. We study four natural variants of the above problem -- the graph on\nvoter being arbitrary vs complete graph (corresponds to removing connectedness\nrequirement for districts) and the cost of bribing every voter being uniform vs\nnon-uniform. We show that all the four problems are NP-complete even under\nquite restrictive scenarios. Hence our results show that district based\nelections are quite resistant under this new kind of electoral attack. We\ncomplement our hardness results with polynomial time algorithms in some other\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:21:15 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1909.01599", "submitter": "Motoki Ikeda", "authors": "Hiroshi Hirai, Motoki Ikeda", "title": "A Cost-Scaling Algorithm for Minimum-Cost Node-Capacitated Multiflow\n  Problem", "comments": "A preliminary version of this paper was presented in at the 11th\n  Hungarian-Japanese Symposium on Discrete Mathematics and Its Applications,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the minimum-cost node-capacitated multiflow problem\nin an undirected network. For this problem, Babenko and Karzanov (2012) showed\nstrongly polynomial-time solvability via the ellipsoid method. Our result is\nthe first combinatorial weakly polynomial-time algorithm for this problem. Our\nalgorithm finds a half-integral minimum-cost maximum multiflow in $O(m\n\\log(nCD)\\mathrm{SF}(kn, m, k))$ time, where $n$ is the number of nodes, $m$ is\nthe number of edges, $k$ is the number of terminals, $C$ is the maximum node\ncapacity, $D$ is the maximum edge cost, and $\\mathrm{SF}(n', m', \\eta)$ is the\ntime complexity of solving the submodular flow problem in a network of $n'$\nnodes, $m'$ edges, and a submodular function with $\\eta$-time-computable\nexchange capacity. Our algorithm is built on discrete convex analysis on graph\nstructures and the concept of reducible bisubmodular flows.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:55:03 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Hirai", "Hiroshi", ""], ["Ikeda", "Motoki", ""]]}, {"id": "1909.01721", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Chrysanthi N. Raftopoulou", "title": "On a Conjecture of Lov\\'asz on Circle-Representations of Simple\n  4-Regular Planar Graphs", "comments": null, "journal-ref": null, "doi": "10.20382/jocg.v6i1a1", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lov\\'asz conjectured that every connected 4-regular planar graph G admits a\nrealization as a system of circles, i.e., it can be drawn on the plane\nutilizing a set of circles, such that the vertices of G correspond to the\nintersection and touching points of the circles and the edges of G are the arc\nsegments among pairs of intersection and touching points of the circles. In\nthis paper, we settle this conjecture. In particular, (a) we first provide\ntight upper and lower bounds on the number of circles needed in a realization\nof any simple 4-regular planar graph, (b) we affirmatively answer Lov\\'asz's\nconjecture, if G is 3-connected, and (c) we demonstrate an infinite class of\nsimple connected 4-regular planar graphs which are not 3-connected (i.e.,\neither simply connected or biconnected) and do not admit realizations as a\nsystem of circles.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:15:04 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Bekos", "Michael A.", ""], ["Raftopoulou", "Chrysanthi N.", ""]]}, {"id": "1909.01747", "submitter": "EPTCS", "authors": "Isabela Dr\\u{a}mnesc (Department of Computer Science West University\n  Timisoara, Romania), Tudor Jebelean (Research Institute for Symbolic\n  Computation, Johannes Kepler University, Linz, Austria)", "title": "Proof-Based Synthesis of Sorting Algorithms Using Multisets in Theorema", "comments": "In Proceedings FROM 2019, arXiv:1909.00584", "journal-ref": "EPTCS 303, 2019, pp. 76-91", "doi": "10.4204/EPTCS.303.6", "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using multisets, we develop novel techniques for mechanizing the proofs of\nthe synthesis conjectures for list-sorting algorithms, and we demonstrate them\nin the Theorema system. We use the classical principle of extracting the\nalgorithm as a set of rewrite rules based on the witnesses found in the proof\nof the synthesis conjecture produced from the specification of the desired\nfunction (input and output conditions). The proofs are in natural style, using\nstandard rules, but most importantly domain specific inference rules and\nstrategies. In particular the use of multisets allows us to develop powerful\nstrategies for the synthesis of arbitrarily structured recursive algorithms by\ngeneral Noetherian induction, as well as for the automatic generation of the\nspecifications of all necessary auxiliary functions (insert, merge, split),\nwhose synthesis is performed using the same method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:49:23 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Dr\u0103mnesc", "Isabela", "", "Department of Computer Science West University\n  Timisoara, Romania"], ["Jebelean", "Tudor", "", "Research Institute for Symbolic\n  Computation, Johannes Kepler University, Linz, Austria"]]}, {"id": "1909.01795", "submitter": "Shaojie Tang", "authors": "Shaojie Tang", "title": "Stochastic Submodular Probing with State-Dependent Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new stochastic submodular maximization problem with\nstate-dependent costs and rejections. The input of our problem is a budget\nconstraint $B$, and a set of items whose states (i.e., the marginal\ncontribution and the cost of an item) are drawn from a known probability\ndistribution. The only way to know the realized state of an item is to probe\nthe item. We allow rejections, i.e., after probing an item and knowing its\nactual state, we must decide immediately and irrevocably whether to add that\nitem to our solution or not. Our objective is to maximize the objective\nfunction subject to a budget constraint on the total cost of the selected\nitems. We present a constant approximate solution to this problem. We show that\nour solution is also applied to an online setting.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 22:50:51 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tang", "Shaojie", ""]]}, {"id": "1909.01802", "submitter": "Chiara Ravazzi", "authors": "Diego Valsesia, Sophie Marie Fosson, Chiara Ravazzi, Tiziano Bianchi,\n  Enrico Magli", "title": "Analysis of SparseHash: an efficient embedding of set-similarity via\n  sparse projections", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Embeddings provide compact representations of signals in order to perform\nefficient inference in a wide variety of tasks. In particular, random\nprojections are common tools to construct Euclidean distance-preserving\nembeddings, while hashing techniques are extensively used to embed\nset-similarity metrics, such as the Jaccard coefficient. In this letter, we\ntheoretically prove that a class of random projections based on sparse\nmatrices, called SparseHash, can preserve the Jaccard coefficient between the\nsupports of sparse signals, which can be used to estimate set similarities.\nMoreover, besides the analysis, we provide an efficient implementation and we\ntest the performance in several numerical experiments, both on synthetic and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 15:41:54 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Valsesia", "Diego", ""], ["Fosson", "Sophie Marie", ""], ["Ravazzi", "Chiara", ""], ["Bianchi", "Tiziano", ""], ["Magli", "Enrico", ""]]}, {"id": "1909.01812", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Alexandros G. Dimakis, Sujay Sanghavi", "title": "Learning Distributions Generated by One-Layer ReLU Networks", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters of a $d$-dimensional\nrectified Gaussian distribution from i.i.d. samples. A rectified Gaussian\ndistribution is defined by passing a standard Gaussian distribution through a\none-layer ReLU neural network. We give a simple algorithm to estimate the\nparameters (i.e., the weight matrix and bias vector of the ReLU neural network)\nup to an error $\\epsilon||W||_F$ using $\\tilde{O}(1/\\epsilon^2)$ samples and\n$\\tilde{O}(d^2/\\epsilon^2)$ time (log factors are ignored for simplicity). This\nimplies that we can estimate the distribution up to $\\epsilon$ in total\nvariation distance using $\\tilde{O}(\\kappa^2d^2/\\epsilon^2)$ samples, where\n$\\kappa$ is the condition number of the covariance matrix. Our only assumption\nis that the bias vector is non-negative. Without this non-negativity\nassumption, we show that estimating the bias vector within any error requires\nthe number of samples at least exponential in the infinity norm of the bias\nvector. Our algorithm is based on the key observation that vector norms and\npairwise angles can be estimated separately. We use a recent result on learning\nfrom truncated samples. We also prove two sample complexity lower bounds:\n$\\Omega(1/\\epsilon^2)$ samples are required to estimate the parameters up to\nerror $\\epsilon$, while $\\Omega(d/\\epsilon^2)$ samples are necessary to\nestimate the distribution up to $\\epsilon$ in total variation distance. The\nfirst lower bound implies that our algorithm is optimal for parameter\nestimation. Finally, we show an interesting connection between learning a\ntwo-layer generative model and non-negative matrix factorization. Experimental\nresults are provided to support our analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:04:46 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 16:04:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Wu", "Shanshan", ""], ["Dimakis", "Alexandros G.", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1909.01821", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas D. Ahle, Jakob B. T. Knudsen", "title": "Almost Optimal Tensor Sketch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a matrix $M\\in R^{m\\otimes d^c}$ with just\n$m=O(c\\,\\lambda\\,\\varepsilon^{-2}\\text{poly}\\log1/\\varepsilon\\delta)$ rows,\nwhich preserves the norm $\\|Mx\\|_2=(1\\pm\\varepsilon)\\|x\\|_2$ of all $x$ in any\ngiven $\\lambda$ dimensional subspace of $ R^d$ with probability at least\n$1-\\delta$. This matrix can be applied to tensors $x^{(1)}\\otimes\\dots\\otimes\nx^{(c)}\\in R^{d^c}$ in $O(c\\, m \\min\\{d,m\\})$ time -- hence the name \"Tensor\nSketch\". (Here $x\\otimes y = \\text{asvec}(xy^T) = [x_1y_1,\nx_1y_2,\\dots,x_1y_m,x_2y_1,\\dots,x_ny_m]\\in R^{nm}$.)\n  This improves upon earlier Tensor Sketch constructions by Pagh and Pham~[TOCT\n2013, SIGKDD 2013] and Avron et al.~[NIPS 2014] which require\n$m=\\Omega(3^c\\lambda^2\\delta^{-1})$ rows for the same guarantees. The factors\nof $\\lambda$, $\\varepsilon^{-2}$ and $\\log1/\\delta$ can all be shown to be\nnecessary making our sketch optimal up to log factors.\n  With another construction we get $\\lambda$ times more rows $m=\\tilde\nO(c\\,\\lambda^2\\,\\varepsilon^{-2}(\\log1/\\delta)^3)$, but the matrix can be\napplied to any vector $x^{(1)}\\otimes\\dots\\otimes x^{(c)}\\in R^{d^c}$ in just\n$\\tilde O(c\\, (d+m))$ time. This matches the application time of Tensor Sketch\nwhile still improving the exponential dependencies in $c$ and $\\log1/\\delta$.\n  Technically, we show two main lemmas: (1) For many Johnson Lindenstrauss (JL)\nconstructions, if $Q,Q'\\in R^{m\\times d}$ are independent JL matrices, the\nelement-wise product $Qx \\circ Q'y$ equals $M(x\\otimes y)$ for some $M\\in\nR^{m\\times d^2}$ which is itself a JL matrix. (2) If $M^{(i)}\\in R^{m\\times\nmd}$ are independent JL matrices, then $M^{(1)}(x \\otimes (M^{(2)}y \\otimes\n\\dots)) = M(x\\otimes y\\otimes \\dots)$ for some $M\\in R^{m\\times d^c}$ which is\nitself a JL matrix. Combining these two results give an efficient sketch for\ntensors of any size.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:56:59 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Knudsen", "Jakob B. T.", ""]]}, {"id": "1909.01957", "submitter": "Anisur Molla Rahaman", "authors": "Ajay D. Kshemkalyani and Anisur Rahaman Molla and Gokarna Sharma", "title": "Dispersion of Mobile Robots in the Global Communication Model", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dispersion problem on graphs asks $k\\leq n$ robots placed initially\narbitrarily on the nodes of an $n$-node anonymous graph to reposition\nautonomously to reach a configuration in which each robot is on a distinct node\nof the graph. This problem is of significant interest due to its relationship\nto other fundamental robot coordination problems, such as exploration,\nscattering, load balancing etc. In this paper, we consider dispersion in the\n{\\em global communication} model where a robot can communicate with any other\nrobot in the graph (but the graph is unknown to robots). We provide three novel\ndeterministic algorithms, two for arbitrary graphs and one for arbitrary trees,\nin a synchronous setting where all robots perform their actions in every time\nstep. For arbitrary graphs, our first algorithm is based on a DFS traversal and\nguarantees $O(\\min(m,k\\Delta))$ steps runtime using $\\Theta(\\log\n(\\max(k,\\Delta)))$ bits at each robot, where $m$ is the number of edges and\n$\\Delta$ is the maximum degree of the graph. The second algorithm for arbitrary\ngraphs is based on a BFS traversal and guarantees $O( \\max(D,k) \\Delta\n(D+\\Delta))$ steps runtime using $O(\\max(D,\\Delta \\log k))$ bits at each robot,\nwhere $D$ is the diameter of the graph. The algorithm for arbitrary trees is\nalso based on a BFS travesal and guarantees $O(D\\max(D,k))$ steps runtime using\n$O(\\max(D,\\Delta \\log k))$ bits at each robot. Our results are significant\nimprovements compared to the existing results established in the {\\em local\ncommunication} model where a robot can communication only with other robots\npresent at the same node. Particularly, the DFS-based algorithm is optimal for\nboth memory and time in constant-degree arbitrary graphs. The BFS-based\nalgorithm for arbitrary trees is optimal with respect to runtime when $k\\leq\nO(D)$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:33:09 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kshemkalyani", "Ajay D.", ""], ["Molla", "Anisur Rahaman", ""], ["Sharma", "Gokarna", ""]]}, {"id": "1909.02198", "submitter": "Chanyeol Yoo", "authors": "James Ju Heon Lee, Chanyeol Yoo, Stuart Anstee and Robert Fitch", "title": "Efficient Optimal Planning in non-FIFO Time-Dependent Flow Fields", "comments": "10 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for solving the time-dependent shortest path problem\nin flow fields where the FIFO (first-in-first-out) assumption is violated. This\nproblem variant is important for autonomous vehicles in the ocean, for example,\nthat cannot arbitrarily hover in a fixed position and that are strongly\ninfluenced by time-varying ocean currents. Although polynomial-time solutions\nare available for discrete-time problems, the continuous-time non-FIFO case is\nNP-hard with no known relevant special cases. Our main result is to show that\nthis problem can be solved in polynomial time if the edge travel time functions\nare piecewise-constant, agreeing with existing worst-case bounds for FIFO\nproblems with restricted slopes. We present a minimum-time algorithm for graphs\nthat allows for paths with finite-length cycles, and then embed this algorithm\nwithin an asymptotically optimal sampling-based framework to find time-optimal\npaths in flows. The algorithm relies on an efficient data structure to\nrepresent and manipulate piecewise-constant functions and is straightforward to\nimplement. We illustrate the behaviour of the algorithm in an example based on\na common ocean vortex model in addition to simpler graph-based examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 03:36:11 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Lee", "James Ju Heon", ""], ["Yoo", "Chanyeol", ""], ["Anstee", "Stuart", ""], ["Fitch", "Robert", ""]]}, {"id": "1909.02364", "submitter": "Carl Barton", "authors": "Carl Barton, Ewan Birney, Tomas Fitzgerald", "title": "A Simple Reduction for Full-Permuted Pattern Matching Problems on\n  Multi-Track Strings", "comments": "Basic error made in lemma on sorting suffixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a variant of string pattern matching which deals with\ntuples of strings known as \\textit{multi-track strings}. Multi-track strings\nare a generalisation of strings (or \\textit{single-track strings}) that have\nprimarily found uses in problems related to searching multiple genomes and\nmusic information retrieval. A multi-track string $\\mathcal{T} = (t_1, t_2,\nt_3, \\ldots , t_N)$ of length $n$ and track count $N$ is a multi-set of $N$\nstrings of length $n$ with characters drawn from a common alphabet of size\n$\\sigma_U$. Given two multi-track strings $\\mathcal{T} = (t_1, t_2, t_3, \\ldots\n, t_N)$ and $ \\mathcal{P} = (p_1, p_2, p_3, \\ldots , p_N)$ of length $n$ and\ntrack count $N$, there is a \\textit{full-permuted-match} between $\\mathcal{P}$\nand $\\mathcal{T}$ if $t_{r_i} = p_i$ for all $i \\in \\{1,2,3,\\ldots N \\}$ and\nsome permutation $(r_1, r_2, r_3\\ldots,r_N)$ of $(1, 2, 3,\\ldots,N)$, we denote\nthis $\\mathcal{P}\\asymp\\mathcal{T}$.\n  Efficient algorithms for some full-permuted-match problems on multi-track\nstrings have recently been presented. In this paper we show a reduction from a\nmulti-track string of length $n$ and track count $N$ with alphabet size\n$\\sigma_U$, to a single-track string of length $2n-1$ with alphabet size\n$\\sigma_U^N$. Through this reduction we allow any string algorithm to be used\non multi-track string problems using $\\asymp$ as the match relation. For\npolynomial time algorithms on single-track strings of length $n$ there is a\nmultiplicative penalty of not more than $\\mathcal{O}(N)$-time for the same\nalgorithm on mt-strings of length $n$ and track count $N$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 12:47:54 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 18:18:49 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 18:19:44 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 09:59:01 GMT"}, {"version": "v5", "created": "Thu, 28 Nov 2019 11:29:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Barton", "Carl", ""], ["Birney", "Ewan", ""], ["Fitzgerald", "Tomas", ""]]}, {"id": "1909.02400", "submitter": "Ching-Lueh Chang", "authors": "Ching-Lueh Chang", "title": "On ultrametric $1$-median selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of finding a point in an ultrametric space with the\nminimum average distance to all points. We give this problem a Monte Carlo\n$O((\\log^2(1/\\epsilon))/\\epsilon^3)$-time $(1+\\epsilon)$-approximation\nalgorithm for all $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:34:45 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Chang", "Ching-Lueh", ""]]}, {"id": "1909.02545", "submitter": "Asish Mukhopadhyay", "authors": "Md. Zamilur Rahman and Asish Mukhopadhyay", "title": "Strongly Chordal Graph Generation using Intersection Graph\n  Characterisation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly chordal graphs are a subclass of chordal graphs. Farber also\nestablished a number of different characterisations for this class of graphs.\nThese include an intersection graph characterisation that is analogous to a\nsimilar characterisation for chordal graphs. Seker et al. exploited this\ncharacterisation of chordal graphs to obtain an algorithm for generating them.\nIn this paper, we propose an algorithm to show that strongly chordal graphs can\nalso be generated using their intersection graph characterisation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:47:08 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Rahman", "Md. Zamilur", ""], ["Mukhopadhyay", "Asish", ""]]}, {"id": "1909.02629", "submitter": "Ming-Hung Shih", "authors": "Trong Duc Nguyen, Ming-Hung Shih, Sai Sree Parvathaneni, Bojian Xu,\n  Divesh Srivastava, Srikanta Tirthapura", "title": "Random Sampling for Group-By Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling has been widely used in approximate query processing on large\ndatabases, due to its potential to significantly reduce resource usage and\nresponse times, at the cost of a small approximation error. We consider random\nsampling for answering the ubiquitous class of group-by queries, which first\ngroup data according to one or more attributes, and then aggregate within each\ngroup after filtering through a predicate. The challenge with group-by queries\nis that a sampling method cannot focus on optimizing the quality of a single\nanswer (e.g. the mean of selected data), but must simultaneously optimize the\nquality of a set of answers (one per group).\n  We present CVOPT, a query- and data-driven sampling framework for a set of\ngroup-by queries. To evaluate the quality of a sample, CVOPT defines a metric\nbased on the norm (e.g. $\\ell_2$ or $\\ell_\\infty$) of the coefficients of\nvariation (CVs) of different answers, and constructs a stratified sample that\nprovably optimizes the metric. CVOPT can handle group-by queries on data where\ngroups have vastly different statistical characteristics, such as frequencies,\nmeans, or variances. CVOPT jointly optimizes for multiple aggregations and\nmultiple group-by clauses, and provides a way to prioritize specific groups or\naggregates. It can be tuned to cases when partial information about a query\nworkload is known, such as a data warehouse where queries are scheduled to run\nperiodically.\n  Our experimental results show that CVOPT outperforms current state-of-the-art\non sample quality and estimation accuracy for group-by queries. On a set of\nqueries on two real-world data sets, CVOPT yields relative errors that are 5x\nsmaller than competing approaches, under the same space budget.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:52:42 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 16:15:22 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 07:14:41 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nguyen", "Trong Duc", ""], ["Shih", "Ming-Hung", ""], ["Parvathaneni", "Sai Sree", ""], ["Xu", "Bojian", ""], ["Srivastava", "Divesh", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1909.02737", "submitter": "Haroldo Gambini Santos D.Sc.", "authors": "Janniele A.S.Araujo, Haroldo Gambini Santos, Bernard Gendron, Sanjay\n  Dominik Jena, Samuel S.Brito, Danilo S.Souzaa", "title": "Strong Bounds for Resource Constrained Project Scheduling: Preprocessing\n  and Cutting Planes", "comments": "-", "journal-ref": "Computers & Operations Research (2019)", "doi": "10.1016/j.cor.2019.104782", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resource Constrained Project Scheduling Problems (RCPSPs) without preemption\nare well-known NP-hard combinatorial optimization problems. A feasible RCPSP\nsolution consists of a time-ordered schedule of jobs with corresponding\nexecution modes, respecting precedence and resources constraints. In this\npaper, we propose a cutting plane algorithm to separate five different cut\nfamilies, as well as a new preprocessing routine to strengthen resource-related\nconstraints. New lifted versions of the well-known precedence and cover\ninequalities are employed. At each iteration, a dense conflict graph is built\nconsidering feasibility and optimality conditions to separate cliques,\nodd-holes and strengthened Chv\\'atal-Gomory cuts. The proposed strategies\nconsiderably improve the linear relaxation bounds, allowing a state-of-the-art\nmixed-integer linear programming solver to find provably optimal solutions for\n754 previously open instances of different variants of the RCPSPs, which was\nnot possible using the original linear programming formulations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:02:36 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Araujo", "Janniele A. S.", ""], ["Santos", "Haroldo Gambini", ""], ["Gendron", "Bernard", ""], ["Jena", "Sanjay Dominik", ""], ["Brito", "Samuel S.", ""], ["Souzaa", "Danilo S.", ""]]}, {"id": "1909.02804", "submitter": "Takuya Mieno", "authors": "Takuya Mieno, Yuki Kuhara, Tooru Akagi, Yuta Fujishige, Yuto\n  Nakashima, Shunsuke Inenaga, Hideo Bannai, Masayuki Takeda", "title": "Minimal Unique Substrings and Minimal Absent Words in a Sliding Window", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substring $u$ of a string $T$ is called a minimal unique substring (MUS) of\n$T$ if $u$ occurs exactly once in $T$ and any proper substring of $u$ occurs at\nleast twice in $T$. A string $w$ is called a minimal absent word (MAW) of $T$\nif $w$ does not occur in $T$ and any proper substring of $w$ occurs in $T$. In\nthis paper, we study the problems of computing MUSs and MAWs in a sliding\nwindow over a given string $T$. We first show how the set of MUSs can change in\na sliding window over $T$, and present an $O(n\\log\\sigma)$-time and\n$O(d)$-space algorithm to compute MUSs in a sliding window of width $d$ over\n$T$, where $\\sigma$ is the maximum number of distinct characters in every\nwindow. We then give tight upper and lower bounds on the maximum number of\nchanges in the set of MAWs in a sliding window over $T$. Our bounds improve on\nthe previous results in [Crochemore et al., 2017].\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:17:30 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 06:16:46 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Mieno", "Takuya", ""], ["Kuhara", "Yuki", ""], ["Akagi", "Tooru", ""], ["Fujishige", "Yuta", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1909.02852", "submitter": "Nachshon Cohen", "authors": "Yoav Zuriel, Michal Friedman, Gali Sheffi, Nachshon Cohen, Erez\n  Petrank", "title": "Efficient Lock-Free Durable Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-volatile memory is expected to co-exist or replace DRAM in upcoming\narchitectures. Durable concurrent data structures for non-volatile memories are\nessential building blocks for constructing adequate software for use with these\narchitectures. In this paper, we propose a new approach for durable concurrent\nsets and use this approach to build the most efficient durable hash tables\navailable today. Evaluation shows a performance improvement factor of up to\n3.3x over existing technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:21:12 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 16:56:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zuriel", "Yoav", ""], ["Friedman", "Michal", ""], ["Sheffi", "Gali", ""], ["Cohen", "Nachshon", ""], ["Petrank", "Erez", ""]]}, {"id": "1909.03152", "submitter": "Keaton Hamm", "authors": "Reyan Ahmed, Greg Bodwin, Faryad Darabi Sahneh, Keaton Hamm, Mohammad\n  Javad Latifi Jebelli, Stephen Kobourov, Richard Spence", "title": "Graph Spanners: A Tutorial Review", "comments": "Many more papers added from the previous version, as well as\n  significant expansion of results presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial review provides a guiding reference to researchers who want to\nhave an overview of the large body of literature about graph spanners. It\nreviews the current literature covering various research streams about graph\nspanners, such as different formulations, sparsity and lightness results,\ncomputational complexity, dynamic algorithms, and applications. As an\nadditional contribution, we offer a list of open problems on graph spanners.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 00:10:41 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 01:55:19 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ahmed", "Reyan", ""], ["Bodwin", "Greg", ""], ["Sahneh", "Faryad Darabi", ""], ["Hamm", "Keaton", ""], ["Jebelli", "Mohammad Javad Latifi", ""], ["Kobourov", "Stephen", ""], ["Spence", "Richard", ""]]}, {"id": "1909.03162", "submitter": "Palash Dey", "authors": "Aditya Anand and Palash Dey", "title": "Distance Restricted Manipulation in Voting", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of {\\em Distance Restricted Manipulation}, where\ncolluding manipulator(s) need to compute if there exist votes which make their\npreferred alternative win the election when their knowledge about the others'\nvotes is a little inaccurate. We use the Kendall-Tau distance to model the\nmanipulators' confidence in the non-manipulators' votes. To this end, we study\nthis problem in two settings - one where the manipulators need to compute a\nmanipulating vote that succeeds irrespective of perturbations in others' votes\n({\\em Distance Restricted Strong Manipulation}), and the second where the\nmanipulators need to compute a manipulating vote that succeeds for at least one\npossible vote profile of the others ({\\em Distance Restricted Weak\nManipulation}). We show that {\\em Distance Restricted Strong Manipulation}\nadmits polynomial-time algorithms for every scoring rule, maximin, Bucklin, and\nsimplified Bucklin voting rules for a single manipulator, and for the\n$k$-approval rule for any number of manipulators, but becomes intractable for\nthe Copeland$^\\alpha$ voting rule for every $\\alpha\\in[0,1]$ even for a single\nmanipulator. In contrast, {\\em Distance Restricted Weak Manipulation} is\nintractable for almost all the common voting rules, with the exception of the\nplurality rule. For a constant number of alternatives, we show that both the\nproblems are polynomial-time solvable for every anonymous and efficient voting\nrule.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 01:29:32 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 11:19:49 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anand", "Aditya", ""], ["Dey", "Palash", ""]]}, {"id": "1909.03350", "submitter": "Faez Ahmed", "authors": "Saba Ahmadi, Faez Ahmed, John P. Dickerson, Mark Fuge and Samir\n  Khuller", "title": "An Algorithm for Multi-Attribute Diverse Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite b-matching, where agents on one side of a market are matched to one\nor more agents or items on the other, is a classical model that is used in\nmyriad application areas such as healthcare, advertising, education, and\ngeneral resource allocation. Traditionally, the primary goal of such models is\nto maximize a linear function of the constituent matches (e.g., linear social\nwelfare maximization) subject to some constraints. Recent work has studied a\nnew goal of balancing whole-match diversity and economic efficiency, where the\nobjective is instead a monotone submodular function over the matching. Basic\nversions of this problem are solvable in polynomial time. In this work, we\nprove that the problem of simultaneously maximizing diversity along several\nfeatures (e.g., country of citizenship, gender, skills) is NP-hard. To address\nthis problem, we develop the first combinatorial algorithm that constructs\nprovably-optimal diverse b-matchings in pseudo-polynomial time. We also provide\na Mixed-Integer Quadratic formulation for the same problem and show that our\nmethod guarantees optimal solutions and takes less computation time for a\nreviewer assignment application.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 23:29:47 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 06:54:09 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 05:17:03 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Ahmadi", "Saba", ""], ["Ahmed", "Faez", ""], ["Dickerson", "John P.", ""], ["Fuge", "Mark", ""], ["Khuller", "Samir", ""]]}, {"id": "1909.03391", "submitter": "Noah Fleming", "authors": "Noah Fleming, Yuichi Yoshida", "title": "Distribution-Free Testing of Linear Functions on R^n", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing whether a function f:R^n->R is linear (i.e.,\nboth additive and homogeneous) in the distribution-free property testing model,\nwhere the distance between functions is measured with respect to an unknown\nprobability distribution over R. We show that, given query access to f,\nsampling access to the unknown distribution as well as the standard Gaussian,\nand eps>0, we can distinguish additive functions from functions that are\neps-far from additive functions with O((1/eps)log(1/eps)) queries, independent\nof n. Furthermore, under the assumption that f is a continuous function, the\nadditivity tester can be extended to a distribution-free tester for linearity\nusing the same number of queries. On the other hand, we show that if we are\nonly allowed to get values of f on sampled points, then any distribution-free\ntester requires Omega(n) samples, even if the underlying distribution is the\nstandard Gaussian.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 05:42:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Fleming", "Noah", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1909.03422", "submitter": "Shimon Kogan", "authors": "Uriel Feige and Shimon Kogan", "title": "Target Set Selection for Conservative Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G = (V,E)$ be a graph on $n$ vertices, where $d_v$ denotes the degree of\nvertex $v$, and $t_v$ is a threshold associated with $v$. We consider a process\nin which initially a set $S$ of vertices becomes active, and thereafter, in\ndiscrete time steps, every vertex $v$ that has at least $t_v$ active neighbors\nbecomes active as well. The set $S$ is contagious if eventually all $V$ becomes\nactive. The target set selection problem TSS asks for the smallest contagious\nset. TSS is NP-hard and moreover, notoriously difficult to approximate.\n  In the conservative special case of TSS, $t_v > \\frac{1}{2}d_v$ for every $v\n\\in V$. In this special case, TSS can be approximated within a ratio of\n$O(\\Delta)$, where $\\Delta = \\max_{v \\in V}[d_v]$. In this work we introduce a\nmore general class of TSS instances that we refer to as conservative on average\n(CoA), that satisfy the condition $\\sum_{v\\in V} t_v > \\frac{1}{2}\\sum_{v \\in\nV} d_v$. We design approximation algorithms for some subclasses of CoA. For\nexample, if $t_v \\geq \\frac{1}{2}d_v$ for every $v \\in V$, we can find in\npolynomial time a contagious set of size $\\tilde{O}\\left(\\Delta \\cdot OPT^2\n\\right)$, where $OPT$ is the size of a smallest contagious set in $G$. We also\nprovide several hardness of approximation results. For example, assuming the\nunique games conjecture, we prove that TSS on CoA instances with $\\Delta \\le 3$\ncannot be approximated within any constant factor.\n  We also present results concerning the fixed parameter tractability of CoA\nTSS instances, and approximation algorithms for a related problem, that of TSS\nwith partial incentives.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 10:20:21 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Feige", "Uriel", ""], ["Kogan", "Shimon", ""]]}, {"id": "1909.03445", "submitter": "Tianyi Zhang", "authors": "Shiri Chechik and Tianyi Zhang", "title": "Fully Dynamic Maximal Independent Set in Expected Poly-Log Update Time", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fully dynamic maximal independent set (MIS) problem our goal is to\nmaintain an MIS in a given graph $G$ while edges are inserted and deleted from\nthe graph. The first non-trivial algorithm for this problem was presented by\nAssadi, Onak, Schieber, and Solomon [STOC 2018] who obtained a deterministic\nfully dynamic MIS with $O(m^{3/4})$ update time. Later, this was independently\nimproved by Du and Zhang and by Gupta and Khan [arXiv 2018] to\n$\\tilde{O}(m^{2/3})$ update time. Du and Zhang [arXiv 2018] also presented a\nrandomized algorithm against an oblivious adversary with $\\tilde{O}(\\sqrt{m})$\nupdate time.\n  The current state of art is by Assadi, Onak, Schieber, and Solomon [SODA\n2019] who obtained randomized algorithms against oblivious adversary with\n$\\tilde{O}(\\sqrt{n})$ and $\\tilde{O}(m^{1/3})$ update times.\n  In this paper, we propose a dynamic randomized algorithm against oblivious\nadversary with expected worst-case update time of $O(\\log^4n)$. As a direct\ncorollary, one can apply the black-box reduction from a recent work by\nBernstein, Forster, and Henzinger [SODA 2019] to achieve $O(\\log^6n)$\nworst-case update time with high probability. This is the first dynamic MIS\nalgorithm with very fast update time of poly-log.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:27:37 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:06:37 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chechik", "Shiri", ""], ["Zhang", "Tianyi", ""]]}, {"id": "1909.03478", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Cliff\n  Stein, Madhu Sudan", "title": "Fully Dynamic Maximal Independent Set with Polylogarithmic Update Time", "comments": "A preliminary version of this paper is to appear in the proceedings\n  of The 60th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first algorithm for maintaining a maximal independent set\n(MIS) of a fully dynamic graph---which undergoes both edge insertions and\ndeletions---in polylogarithmic time. Our algorithm is randomized and, per\nupdate, takes $O(\\log^2 \\Delta \\cdot \\log^2 n)$ expected time. Furthermore, the\nalgorithm can be adjusted to have $O(\\log^2 \\Delta \\cdot \\log^4 n)$ worst-case\nupdate-time with high probability. Here, $n$ denotes the number of vertices and\n$\\Delta$ is the maximum degree in the graph.\n  The MIS problem in fully dynamic graphs has attracted significant attention\nafter a breakthrough result of Assadi, Onak, Schieber, and Solomon [STOC'18]\nwho presented an algorithm with $O(m^{3/4})$ update-time (and thus broke the\nnatural $\\Omega(m)$ barrier) where $m$ denotes the number of edges in the\ngraph. This result was improved in a series of subsequent papers, though, the\nupdate-time remained polynomial. In particular, the fastest algorithm prior to\nour work had $\\widetilde{O}(\\min\\{\\sqrt{n}, m^{1/3}\\})$ update-time [Assadi et\nal. SODA'19].\n  Our algorithm maintains the lexicographically first MIS over a random order\nof the vertices. As a result, the same algorithm also maintains a\n3-approximation of correlation clustering. We also show that a simpler variant\nof our algorithm can be used to maintain a random-order lexicographically first\nmaximal matching in the same update-time.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 15:06:28 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Stein", "Cliff", ""], ["Sudan", "Madhu", ""]]}, {"id": "1909.03547", "submitter": "Shay Moran", "authors": "Mark Braverman and Gillat Kol and Shay Moran and Raghuvansh R. Saxena", "title": "Convex Set Disjointness, Distributed Learning of Halfspaces, and LP\n  Feasibility", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Convex Set Disjointness (CSD) problem, where two players have\ninput sets taken from an arbitrary fixed domain~$U\\subseteq \\mathbb{R}^d$ of\nsize $\\lvert U\\rvert = n$. Their mutual goal is to decide using minimum\ncommunication whether the convex hulls of their sets intersect (equivalently,\nwhether their sets can be separated by a hyperplane).\n  Different forms of this problem naturally arise in distributed learning and\noptimization: it is equivalent to {\\em Distributed Linear Program (LP)\nFeasibility} -- a basic task in distributed optimization, and it is tightly\nlinked to {\\it Distributed Learning of Halfdpaces in $\\mathbb{R}^d$}. In\n{communication complexity theory}, CSD can be viewed as a geometric\ninterpolation between the classical problems of {Set Disjointness} (when~$d\\geq\nn-1$) and {Greater-Than} (when $d=1$).\n  We establish a nearly tight bound of $\\tilde \\Theta(d\\log n)$ on the\ncommunication complexity of learning halfspaces in $\\mathbb{R}^d$. For Convex\nSet Disjointness (and the equivalent task of distributed LP feasibility) we\nderive upper and lower bounds of $\\tilde O(d^2\\log n)$ and~$\\Omega(d\\log n)$.\nThese results improve upon several previous works in distributed learning and\noptimization.\n  Unlike typical works in communication complexity, the main technical\ncontribution of this work lies in the upper bounds. In particular, our\nprotocols are based on a {\\it Container Lemma for Halfspaces} and on two\nvariants of {\\it Carath\\'eodory's Theorem}, which may be of independent\ninterest. These geometric statements are used by our protocols to provide a\ncompressed summary of the players' input.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 21:19:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Braverman", "Mark", ""], ["Kol", "Gillat", ""], ["Moran", "Shay", ""], ["Saxena", "Raghuvansh R.", ""]]}, {"id": "1909.03636", "submitter": "Marek Chrobak", "authors": "Marek Chrobak, Kevin Costello, Leszek Gasieniec", "title": "Information Gathering in Ad-Hoc Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the ad-hoc radio network model, nodes communicate with their neighbors via\nradio signals, without knowing the topology of the graph. We study the\ninformation gathering problem, where each node has a piece of information\ncalled a rumor, and the objective is to transmit all rumors to a designated\ntarget node. We provide an O(n^1.5*polylog(n)) deteministic protocol for\ninformation gathering in ad-hoc radio networks, significantly improving the\ntrivial bound of O(n^2).\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 05:37:13 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 10:43:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chrobak", "Marek", ""], ["Costello", "Kevin", ""], ["Gasieniec", "Leszek", ""]]}, {"id": "1909.03829", "submitter": "William Pettersson", "authors": "William Pettersson and Melih Ozlen", "title": "Multi-Objective Mixed Integer Programming: An Objective Space Algorithm", "comments": "6 pages, presented at LeGO International Global Optimization\n  Workshop. At time of submission, no competing algorithm was known, but a\n  competing algorithm was published between submission and presentation of this\n  work", "journal-ref": null, "doi": "10.1063/1.5090006", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first objective space algorithm which can exactly\nfind all supported and non-supported non-dominated solutions to a mixed-integer\nmulti-objective linear program with an arbitrary number of objective functions.\nThis algorithm is presented in three phases. First it builds up a super-set\nwhich contains the Pareto front. This super-set is then modified to not contain\nany intersecting polytopes. Once this is achieved, the algorithm efficiently\ncalculates which portions of the super-set are not part of the Pareto front and\nremoves them, leaving exactly the Pareto front.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:14:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Pettersson", "William", ""], ["Ozlen", "Melih", ""]]}, {"id": "1909.03859", "submitter": "Muhammad Omer Bin Saeed", "authors": "Muhammad Omer Bin Saeed", "title": "A Complete Transient Analysis for the Incremental LMS Algorithm", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incremental least mean square (ILMS) algorithm was presented in\n\\cite{Lopes2007}. The article included theoretical analysis of the algorithm\nalong with simulation results under different scenarios. However, the transient\nanalysis was left incomplete. This work presents the complete transient\nanalysis, including the learning behavior. The analysis results are verified\nthrough several experimental results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:46:25 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Saeed", "Muhammad Omer Bin", ""]]}, {"id": "1909.03951", "submitter": "Vikrant Singhal", "authors": "Gautam Kamath, Or Sheffet, Vikrant Singhal, Jonathan Ullman", "title": "Differentially Private Algorithms for Learning Mixtures of Separated\n  Gaussians", "comments": "To appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the parameters of Gaussian mixture models is a fundamental and\nwidely studied problem with numerous applications. In this work, we give new\nalgorithms for learning the parameters of a high-dimensional, well separated,\nGaussian mixture model subject to the strong constraint of differential\nprivacy. In particular, we give a differentially private analogue of the\nalgorithm of Achlioptas and McSherry. Our algorithm has two key properties not\nachieved by prior work: (1) The algorithm's sample complexity matches that of\nthe corresponding non-private algorithm up to lower order terms in a wide range\nof parameters. (2) The algorithm does not require strong a priori bounds on the\nparameters of the mixture components.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:58:52 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 21:48:40 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kamath", "Gautam", ""], ["Sheffet", "Or", ""], ["Singhal", "Vikrant", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1909.04016", "submitter": "Justin Sybrandt", "authors": "Justin Sybrandt, Ruslan Shaydulin, Ilya Safro", "title": "Hypergraph Partitioning With Embeddings", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2020.3017120", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems in scientific computing, such as distributing large sparse matrix\noperations, have analogous formulations as hypergraph partitioning problems. A\nhypergraph is a generalization of a traditional graph wherein \"hyperedges\" may\nconnect any number of nodes. As a result, hypergraph partitioning is an NP-Hard\nproblem to both solve or approximate. State-of-the-art algorithms that solve\nthis problem follow the multilevel paradigm, which begins by iteratively\n\"coarsening\" the input hypergraph to smaller problem instances that share key\nstructural features. Once identifying an approximate problem that is small\nenough to be solved directly, that solution can be interpolated and refined to\nthe original problem. While this strategy represents an excellent trade off\nbetween quality and running time, it is sensitive to coarsening strategy. In\nthis work we propose using graph embeddings of the initial hypergraph in order\nto ensure that coarsened problem instances retrain key structural features. Our\napproach prioritizes coarsening within self-similar regions within the input\ngraph, and leads to significantly improved solution quality across a range of\nconsidered hypergraphs. Reproducibility: All source code, plots and\nexperimental data are available at https://sybrandt.com/2019/partition.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:55:23 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 13:18:07 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 15:05:52 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2020 16:36:23 GMT"}, {"version": "v5", "created": "Tue, 25 Aug 2020 21:02:16 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Sybrandt", "Justin", ""], ["Shaydulin", "Ruslan", ""], ["Safro", "Ilya", ""]]}, {"id": "1909.04244", "submitter": "Shuai Shao", "authors": "Shuai Shao and Yuxin Sun", "title": "Contraction: a Unified Perspective of Correlation Decay and\n  Zero-Freeness of 2-Spin Systems", "comments": "21 pages, 3 figures. Update: two correlation decay sets were added; a\n  discussion with an independent work by Liu with similar results was given", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cond-mat.stat-mech cs.DS math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study complex zeros of the partition function of 2-spin systems, viewed as\na multivariate polynomial in terms of the edge interaction parameters and the\nuniform external field. We obtain new zero-free regions in which all these\nparameters are complex-valued. Crucially based on the zero-freeness, we show\nthe existence of correlation decay in these regions. As a consequence, we\nobtain an FPTAS for computing the partition function of 2-spin systems on\ngraphs of bounded degree for these parameter settings. We introduce the\ncontraction property as a unified sufficient condition to devise FPTAS via\neither Weitz's algorithm or Barvinok's algorithm. Our main technical\ncontribution is a very simple but general approach to extend any real parameter\nof which the 2-spin system exhibits correlation decay to its complex\nneighborhood where the partition function is zero-free and correlation decay\nstill exists. This result formally establishes the inherent connection between\ntwo distinct notions of phase transition for 2-spin systems: the existence of\ncorrelation decay and the zero-freeness of the partition function via a unified\nperspective, contraction.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 02:36:07 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 21:03:32 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 23:16:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Shao", "Shuai", ""], ["Sun", "Yuxin", ""]]}, {"id": "1909.04268", "submitter": "Shaddin Dughmi", "authors": "Shaddin Dughmi", "title": "The Outer Limits of Contention Resolution on Matroids and Connections to\n  the Secretary Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contention resolution schemes have proven to be a useful and unifying\nabstraction for a variety of constrained optimization problems, in both offline\nand online arrival models. Much of prior work restricts attention to product\ndistributions for the input set of elements, and studies contention resolution\nfor increasingly general packing constraints, both offline and online. In this\npaper, we instead focus on generalizing the input distribution, restricting\nattention to matroid constraints in both the offline and online random arrival\nmodels. In particular, we study contention resolution when the input set is\narbitrarily distributed, and may exhibit positive and/or negative correlations\nbetween elements. We characterize the distributions for which offline\ncontention resolution is possible, and establish some of their basic closure\nproperties. Our characterization can be interpreted as a distributional\ngeneralization of the matroid covering theorem. For the online random arrival\nmodel, we show that contention resolution is intimately tied to the secretary\nproblem via two results. First, we show that a competitive algorithm for the\nmatroid secretary problem implies that online contention resolution is\nessentially as powerful as offline contention resolution for matroids, so long\nas the algorithm is given the input distribution. Second, we reduce the matroid\nsecretary problem to the design of an online contention resolution scheme of a\nparticular form.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 03:49:20 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 04:49:16 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 22:00:45 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Dughmi", "Shaddin", ""]]}, {"id": "1909.04396", "submitter": "Tapani Toivonen Mr.", "authors": "Tapani Toivonen and Janne Karttunen", "title": "Constant factor approximation of MAX CLIQUE", "comments": "the reduction does not preserve the approximation ratio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MAX CLIQUE problem (MCP) is an NPO problem, which asks to find the largest\ncomplete sub-graph in a graph $G, G = (V, E)$ (directed or undirected). MCP is\nwell known to be $NP-Hard$ to approximate in polynomial time with an\napproximation ratio of $1 + \\epsilon$, for every $\\epsilon > 0$ [9] (and even a\npolynomial time approximation algorithm with a ratio $n^{1 - \\epsilon}$ has\nbeen conjectured to be non-existent [2] for MCP). Up to this date, the best\nknown approximation ratio for MCP of a polynomial time algorithm is\n$O(n(log_2(log_2(n)))^2 / (log_2(n))^3)$ given by Feige [1]. In this paper, we\nshow that MCP can be approximated with a constant factor in polynomial time\nthrough approximation ratio preserving reductions from MCP to MAX DNF and from\nMAX DNF to MIN SAT. A 2-approximation algorithm for MIN SAT was presented in\n[6]. An approximation ratio preserving reduction from MIN SAT to min vertex\ncover improves the approximation ratio to $2 - \\Theta(1/ \\sqrt{n})$ [10]. Hence\nwe prove false the infamous conjecture, which argues that there cannot be a\npolynomial time algorithm for MCP with an approximation ratio of any constant\nfactor.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:42:37 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 06:04:07 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 14:43:08 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 18:54:00 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Toivonen", "Tapani", ""], ["Karttunen", "Janne", ""]]}, {"id": "1909.04463", "submitter": "Markus Sinnl", "authors": "Markus Sinnl", "title": "Algorithmic expedients for the S-labeling problem", "comments": "Accepted for publication in Computers & Operations Research; doi:\n  10.1016/j.cor.2019.04.014", "journal-ref": "Computers & Operations Research, Volume 108, 2019, Pages 201-212", "doi": "10.1016/j.cor.2019.04.014", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph labeling problems have been widely studied in the last decades and have\na vast area of application. In this work, we study the recently introduced\nS-labeling problem, in which the nodes get labeled using labels from 1 to |V |\nand for each edge the contribution to the objective function, called S-labeling\nnumber of the graph, is the minimum label of its end-nodes. The goal is to find\na labeling with minimum value. The problem is NP-hard for planar subcubic\ngraphs, although for many other graph classes the complexity status is still\nunknown. In this paper, we present different algorithmic approaches for\ntackling this problem: We develop an exact solution framework based on\nMixed-Integer Programming (MIP) which is enhanced with valid inequalities,\nstarting and primal heuristics and specialized branching rules. We show that\nour MIP formulation has no integrality gap for paths, cycles and perfect n-ary\ntrees, and, to the best of our knowledge, we give the first polynomial-time\nalgorithm for the problem on n-ary trees as well as a closed formula for the\nS-labeling number for such trees. Moreover, we also present a Lagrangian\nheuristic and a constraint programming approach. A computational study is\ncarried out in order to (i) investigate if there may be other special graph\nclasses, where our MIP formulation has no integrality gap, and (ii) assess the\neffectiveness of the proposed solution approaches for solving the problem on a\ndataset consisting of general graphs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:17:17 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 08:20:16 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Sinnl", "Markus", ""]]}, {"id": "1909.04481", "submitter": "Xiaowei Wu", "authors": "Bo Li, Minming Li and Xiaowei Wu", "title": "Well-behaved Online Load Balancing Against Strategic Jobs", "comments": "A preliminary version appeared in AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online load balancing problem on related machines, we have a set of\njobs (with different sizes) arriving online, and we need to assign each job to\na machine immediately upon its arrival, so as to minimize the makespan, i.e.,\nthe maximum completion time. In classic mechanism design problems, we assume\nthat the jobs are controlled by selfish agents, with the sizes being their\nprivate information. Each job (agent) aims at minimizing its own cost, which is\nits completion time plus the payment charged by the mechanism. Truthful\nmechanisms guaranteeing that every job minimizes its cost by reporting its true\nsize have been well-studied [Aspnes et al. JACM 1997, Feldman et al. EC 2017].\n  In this paper, we study truthful online load balancing mechanisms that are\nwell-behaved [Epstein et al., MOR 2016]. Well-behavior is important as it\nguarantees fairness between machines, and implies truthfulness in some cases\nwhen machines are controlled by selfish agents. Unfortunately, existing\ntruthful online load balancing mechanisms are not well-behaved. We first show\nthat to guarantee producing a well-behaved schedule, any online algorithm (even\nnon-truthful) has a competitive ratio at least $\\Omega(\\sqrt{m})$, where m is\nthe number of machines. Then we propose a mechanism that guarantees\ntruthfulness of the online jobs, and produces a schedule that is almost\nwell-behaved. We show that our algorithm has a competitive ratio of $O(\\log\nm)$. Moreover, for the case when the sizes of online jobs are bounded, the\ncompetitive ratio of our algorithm improves to $O(1)$. Interestingly, we show\nseveral cases for which our mechanism is actually truthful against selfish\nmachines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:50:39 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Li", "Bo", ""], ["Li", "Minming", ""], ["Wu", "Xiaowei", ""]]}, {"id": "1909.04611", "submitter": "Andrew van der Poel", "authors": "Brian Lavallee, Hayley Russell, Blair D. Sullivan, Andrew van der Poel", "title": "Approximating Vertex Cover using Structural Rounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we provide the first practical evaluation of the structural\nrounding framework for approximation algorithms. Structural rounding works by\nfirst editing to a well-structured class, efficiently solving the edited\ninstance, and \"lifting\" the partial solution to recover an approximation on the\ninput. We focus on the well-studied Vertex Cover problem, and edit to the class\nof bipartite graphs (where Vertex Cover has an exact polynomial time\nalgorithm). In addition to the naive lifting strategy for Vertex Cover\ndescribed by Demaine et al., we introduce a suite of new lifting strategies and\nmeasure their effectiveness on a large corpus of synthetic graphs. We find that\nin this setting, structural rounding significantly outperforms standard\n2-approximations. Further, simpler lifting strategies are extremely competitive\nwith the more sophisticated approaches. The implementations are available as an\nopen-source Python package, and all experiments are replicable.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:41:20 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 18:01:12 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Lavallee", "Brian", ""], ["Russell", "Hayley", ""], ["Sullivan", "Blair D.", ""], ["van der Poel", "Andrew", ""]]}, {"id": "1909.04613", "submitter": "Daniel Stilck Franca", "authors": "Fernando G.S L. Brand\\~ao, Richard Kueng, Daniel Stilck Fran\\c{c}a", "title": "Faster quantum and classical SDP approximations for quadratic binary\n  optimization", "comments": "31 pages, one figure. Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a quantum speedup for solving the canonical semidefinite programming\nrelaxation for binary quadratic optimization. This class of relaxations for\ncombinatorial optimization has so far eluded quantum speedups. Our methods\ncombine ideas from quantum Gibbs sampling and matrix exponent updates. A\nde-quantization of the algorithm also leads to a faster classical solver. For\ngeneric instances, our quantum solver gives a nearly quadratic speedup over\nstate-of-the-art algorithms. We also provide an efficient randomized rounding\nprocedure that converts approximately optimal SDP solutions into constant\nfactor approximations of the original quadratic optimization problem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:43:35 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 07:06:51 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Brand\u00e3o", "Fernando G. S L.", ""], ["Kueng", "Richard", ""], ["Fran\u00e7a", "Daniel Stilck", ""]]}, {"id": "1909.04759", "submitter": "Ali Khodabakhsh", "authors": "Swati Gupta, Ali Khodabakhsh, Hassan Mortagy, Evdokia Nikolova", "title": "Electrical Flows over Spanning Trees", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network reconfiguration problem seeks to find a rooted tree $T$ such that\nthe energy of the (unique) feasible electrical flow over $T$ is minimized. The\ntree requirement on the support of the flow is motivated by operational\nconstraints in electricity distribution networks. The bulk of existing results\non convex optimization over vertices of polytopes and on the structure of\nelectrical flows do not easily give guarantees for this problem, while many\nheuristic methods have been developed in the power systems community as early\nas 1989. Our main contribution is to give the first provable approximation\nguarantees for the network reconfiguration problem. We provide novel lower\nbounds and corresponding approximation factors for various settings ranging\nfrom $\\min\\{O(m-n), O(n)\\}$ for general graphs, to $O(\\sqrt{n})$ over grids\nwith uniform resistances on edges, and $O(1)$ for grids with uniform edge\nresistances and demands. To obtain the result for general graphs, we propose a\nnew method for (approximate) spectral graph sparsification, which may be of\nindependent interest. Using insights from our theoretical results, we propose a\ngeneral heuristic for the network reconfiguration problem that is orders of\nmagnitude faster than existing methods in the literature, while obtaining\ncomparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:20:11 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 03:09:24 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 19:04:58 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Gupta", "Swati", ""], ["Khodabakhsh", "Ali", ""], ["Mortagy", "Hassan", ""], ["Nikolova", "Evdokia", ""]]}, {"id": "1909.04905", "submitter": "Carlo Pinciroli", "authors": "Nathalie Majcherczyk and Carlo Pinciroli", "title": "SwarmMesh: A Distributed Data Structure for Cooperative Multi-Robot\n  Applications", "comments": "8 pages, submitted to RAL-ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to the distributed storage of data across a swarm of\nmobile robots that forms a shared global memory. We assume that external\nstorage infrastructure is absent, and that each robot is capable of devoting a\nquota of memory and bandwidth to distributed storage. Our approach is motivated\nby the insight that in many applications data is collected at the periphery of\na swarm topology, but the periphery also happens to be the most dangerous\nlocation for storing data, especially in exploration missions. Our approach is\ndesigned to promote data storage in the locations in the swarm that best suit a\nspecific feature of interest in the data, while accounting for the constantly\nchanging topology due to individual motion. We analyze two possible features of\ninterest: the data type and the data item position in the environment. We\nassess the performance of our approach in a large set of simulated experiments.\nThe evaluation shows that our approach is capable of storing quantities of data\nthat exceed the memory of individual robots, while maintaining near-perfect\ndata retention in high-load conditions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 08:20:18 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Majcherczyk", "Nathalie", ""], ["Pinciroli", "Carlo", ""]]}, {"id": "1909.04910", "submitter": "Markus Sinnl", "authors": "Eduardo \\'Alvarez-Miranda and Markus Sinnl", "title": "An exact solution framework for the multiple gradual cover location\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facility and covering location models are key elements in many decision aid\ntools in logistics, supply chain design, telecommunications, public\ninfrastructure planning, and many other industrial and public sectors. In many\napplications, it is likely that customers are not dichotomously covered by\nfacilities, but gradually covered according to, e.g., the distance to the open\nfacilities. Moreover, customers are not served by a single facility, but by a\ncollection of them, which jointly serve them. In this paper we study the\nrecently introduced multiple gradual cover location problem (MGCLP). The MGCLP\naddresses both of the issues described above.\n  We provide four different mixed-integer programming formulations for the\nMGCLP, all of them exploiting the submodularity of the objective function and\ndeveloped a branch-and-cut framework based one these formulations. The\nframework is further enhanced by starting and primal heuristics and\ninitialization procedures.\n  The computational results show that our approach allows to effectively\naddress different sets of instances. We provide optimal solution values for 13\ninstances from literature, where the optimal solution was not known, and\nadditionally provide improved solution values for seven instances. Many of\nthese instances can be solved within a minute. We also analyze the dependence\nof the solution-structure on instance-characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 08:30:43 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["\u00c1lvarez-Miranda", "Eduardo", ""], ["Sinnl", "Markus", ""]]}, {"id": "1909.05007", "submitter": "Daron Anderson", "authors": "Daron Anderson, Douglas Leith", "title": "Optimality of the Subgradient Algorithm in the Stochastic Setting", "comments": "6 figures, Corrected off-by-one errors coming from proof in Appendix\n  A. Replaced with newer Version April 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG cs.SY eess.SY math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Subgradient algorithm is universal for online learning on\nthe simplex in the sense that it simultaneously achieves $O(\\sqrt N)$ regret\nfor adversarial costs and $O(1)$ pseudo-regret for i.i.d costs. To the best of\nour knowledge this is the first demonstration of a universal algorithm on the\nsimplex that is not a variant of Hedge. Since Subgradient is a popular and\nwidely used algorithm our results have immediate broad application.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:44:30 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 14:33:20 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 12:22:22 GMT"}, {"version": "v4", "created": "Wed, 30 Oct 2019 12:02:47 GMT"}, {"version": "v5", "created": "Thu, 31 Oct 2019 12:50:45 GMT"}, {"version": "v6", "created": "Fri, 3 Apr 2020 17:04:11 GMT"}, {"version": "v7", "created": "Fri, 27 Nov 2020 14:31:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anderson", "Daron", ""], ["Leith", "Douglas", ""]]}, {"id": "1909.05023", "submitter": "Johannes Bausch", "authors": "Johannes Bausch, Sathyawageeswar Subramanian, Stephen Piddock", "title": "A Quantum Search Decoder for Natural Language Processing", "comments": "39 pages, 16 figures, 2 algorithms", "journal-ref": "Quantum Mach. Intell. 3, 16 (2021)", "doi": "10.1007/s42484-021-00041-1", "report-no": null, "categories": "quant-ph cs.CL cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic language models, e.g. those based on an LSTM, often face the\nproblem of finding a high probability prediction from a sequence of random\nvariables over a set of tokens. This is commonly addressed using a form of\ngreedy decoding such as beam search, where a limited number of\nhighest-likelihood paths (the beam width) of the decoder are kept, and at the\nend the maximum-likelihood path is chosen. In this work, we construct a quantum\nalgorithm to find the globally optimal parse (i.e. for infinite beam width)\nwith high constant success probability. When the input to the decoder is\ndistributed as a power-law with exponent $k>0$, our algorithm has runtime $R^{n\nf(R,k)}$, where $R$ is the alphabet size, $n$ the input length; here $f<1/2$,\nand $f\\rightarrow 0$ exponentially fast with increasing $k$, hence making our\nalgorithm always more than quadratically faster than its classical counterpart.\nWe further modify our procedure to recover a finite beam width variant, which\nenables an even stronger empirical speedup while still retaining higher\naccuracy than possible classically. Finally, we apply this quantum beam search\ndecoder to Mozilla's implementation of Baidu's DeepSpeech neural net, which we\nshow to exhibit such a power law word rank frequency.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:59:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 16:51:11 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Bausch", "Johannes", ""], ["Subramanian", "Sathyawageeswar", ""], ["Piddock", "Stephen", ""]]}, {"id": "1909.05270", "submitter": "Raban Iten", "authors": "Raban Iten, Romain Moyard, Tony Metger, David Sutter, Stefan Woerner", "title": "Exact and practical pattern matching for quantum circuit optimization", "comments": "Raban Iten and Romain Moyard contributed equally to this work. Major\n  updates: Added numerical analysis of the pattern matching algorithm; fixed\n  two special cases that were missed by our algorithm and updated the\n  worst-case complexity analysis. 10 pages summary + 23 pages main text + 7\n  pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computations are typically compiled into a circuit of basic quantum\ngates. Just like for classical circuits, a quantum compiler should optimize the\nquantum circuit, e.g. by minimizing the number of required gates. Optimizing\nquantum circuits is not only relevant for improving the runtime of quantum\nalgorithms in the long term, but is also particularly important for near-term\nquantum devices that can only implement a small number of quantum gates before\nnoise renders the computation useless. An important building block for many\nquantum circuit optimization techniques is pattern matching, where given a\nlarge and a small quantum circuit, we are interested in finding all maximal\nmatches of the small circuit, called pattern, in the large circuit, considering\npairwise commutation of quantum gates.\n  In this work, we present a classical algorithm for pattern matching that\nprovably finds all maximal matches in time polynomial in the circuit size (for\na fixed pattern size). Our algorithm works for both quantum and reversible\nclassical circuits. We demonstrate numerically that our algorithm, implemented\nin the open-source library Qiskit, scales considerably better than suggested by\nthe theoretical worst-case complexity and is practical to use for circuit sizes\ntypical for near-term quantum devices. Using our pattern matching algorithm as\nthe basis for known circuit optimization techniques such as template matching\nand peephole optimization, we demonstrate a significant (~30%) reduction in\ngate count for random quantum circuits, and are able to further improve\npractically relevant quantum circuits that were already optimized with\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 18:00:27 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 06:51:30 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Iten", "Raban", ""], ["Moyard", "Romain", ""], ["Metger", "Tony", ""], ["Sutter", "David", ""], ["Woerner", "Stefan", ""]]}, {"id": "1909.05499", "submitter": "Xiaocheng Li", "authors": "Xiaocheng Li, Yinyu Ye", "title": "Online Linear Programming: Dual Convergence, New Algorithms, and Regret\n  Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online linear programming (OLP) problem under a random input\nmodel in which the columns of the constraint matrix along with the\ncorresponding coefficients in the objective function are generated i.i.d. from\nan unknown distribution and revealed sequentially over time. Virtually all\npre-existing online algorithms were based on learning the dual optimal\nsolutions/prices of the linear programs (LP), and their analyses were focused\non the aggregate objective value and solving the packing LP where all\ncoefficients in the constraint matrix and objective are nonnegative. However,\ntwo major open questions were: (i) Does the set of LP optimal dual prices\nlearned in the pre-existing algorithms converge to those of the \"offline\" LP,\nand (ii) Could the results be extended to general LP problems where the\ncoefficients can be either positive or negative. We resolve these two questions\nby establishing convergence results for the dual prices under moderate\nregularity conditions for general LP problems. Specifically, we identify an\nequivalent form of the dual problem which relates the dual LP with a sample\naverage approximation to a stochastic program. Furthermore, we propose a new\ntype of OLP algorithm, Action-History-Dependent Learning Algorithm, which\nimproves the previous algorithm performances by taking into account the past\ninput data as well as decisions/actions already made. We derive an $O(\\log n\n\\log \\log n)$ regret bound (under a locally strong convexity and smoothness\ncondition) for the proposed algorithm, against the $O(\\sqrt{n})$ bound for\ntypical dual-price learning algorithms, where $n$ is the number of decision\nvariables. Numerical experiments demonstrate the effectiveness of the proposed\nalgorithm and the action-history-dependent design.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:18:44 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 23:33:44 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 03:09:07 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 22:31:15 GMT"}, {"version": "v5", "created": "Mon, 19 Apr 2021 10:19:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Xiaocheng", ""], ["Ye", "Yinyu", ""]]}, {"id": "1909.05503", "submitter": "Ruoqi Shen", "authors": "Ruoqi Shen, Yin Tat Lee", "title": "The Randomized Midpoint Method for Log-Concave Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from log-concave distributions is a well researched problem that has\nmany applications in statistics and machine learning. We study the\ndistributions of the form $p^{*}\\propto\\exp(-f(x))$, where\n$f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ has an $L$-Lipschitz gradient and is\n$m$-strongly convex. In our paper, we propose a Markov chain Monte Carlo (MCMC)\nalgorithm based on the underdamped Langevin diffusion (ULD). It can achieve\n$\\epsilon\\cdot D$ error (in 2-Wasserstein distance) in\n$\\tilde{O}\\left(\\kappa^{7/6}/\\epsilon^{1/3}+\\kappa/\\epsilon^{2/3}\\right)$\nsteps, where $D\\overset{\\mathrm{def}}{=}\\sqrt{\\frac{d}{m}}$ is the effective\ndiameter of the problem and $\\kappa\\overset{\\mathrm{def}}{=}\\frac{L}{m}$ is the\ncondition number. Our algorithm performs significantly faster than the\npreviously best known algorithm for solving this problem, which requires\n$\\tilde{O}\\left(\\kappa^{1.5}/\\epsilon\\right)$ steps. Moreover, our algorithm\ncan be easily parallelized to require only $O(\\kappa\\log\\frac{1}{\\epsilon})$\nparallel steps.\n  To solve the sampling problem, we propose a new framework to discretize\nstochastic differential equations. We apply this framework to discretize and\nsimulate ULD, which converges to the target distribution $p^{*}$. The framework\ncan be used to solve not only the log-concave sampling problem, but any problem\nthat involves simulating (stochastic) differential equations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:32:39 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Shen", "Ruoqi", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1909.05711", "submitter": "Federico Cor\\`o", "authors": "Francesco Betti Sorbelli and Stefano Carpin and Federico Cor\\`o and\n  Alfredo Navarra and Cristina M. Pinotti", "title": "Optimal Routing Schedules for Robots Operating in Aisle-Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Constant-cost Orienteering Problem (COP) where\na robot, constrained by a limited travel budget, aims at selecting a path with\nthe largest reward in an aisle-graph. The aisle-graph consists of a set of\nloosely connected rows where the robot can change lane only at either end, but\nnot in the middle. Even when considering this special type of graphs, the\norienteering problem is known to be NP-hard. We optimally solve in polynomial\ntime two special cases, COP-FR where the robot can only traverse full rows, and\nCOP-SC where the robot can access the rows only from one side. To solve the\ngeneral COP, we then apply our special case algorithms as well as a new\nheuristic that suitably combines them. Despite its light computational\ncomplexity and being confined into a very limited class of paths, the optimal\nsolutions for COP-FR turn out to be competitive even for COP in both real and\nsynthetic scenarios. Furthermore, our new heuristic for the general case\noutperforms state-of-art algorithms, especially for input with highly\nunbalanced rewards.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:24:45 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 18:12:21 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Sorbelli", "Francesco Betti", ""], ["Carpin", "Stefano", ""], ["Cor\u00f2", "Federico", ""], ["Navarra", "Alfredo", ""], ["Pinotti", "Cristina M.", ""]]}, {"id": "1909.06226", "submitter": "Albert Einstein Fernandes Muritiba", "authors": "Albert Einstein Fernandes Muritiba and Tib\\'erius O. Bonates and\n  St\\^enio Oliveira Da Silva and Manuel Iori", "title": "Branch-and-cut and iterated local search for the weighted $k$-traveling\n  repairman problem: an application to the maintenance of speed cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private enterprises and governments around the world use speed cameras to\ncontrol traffic flow and limit speed excess. Cameras may be exposed to\ndifficult weather conditions and typically require frequent maintenance. When\ndeciding the order in which maintenance should be performed, one has to\nconsider both the traveling times between the cameras and the traffic flow that\neach camera is supposed to monitor. In this paper, we study the problem of\nrouting a set of technicians to repair cameras by minimizing the total weighted\nlatency, that is, the sum of the weighted waiting times of each camera, where\nthe weight is a parameter proportional to the monitored traffic. The resulting\nproblem, called weighted k-traveling repairman problem (wkTRP), is a\ngeneralization of the well-known traveling repairman problem and can be used to\nmodel a variety of real-world applications. To solve the wkTRP, we propose an\niterated local search heuristic and an exact branch-and-cut algorithm enriched\nwith valid inequalities. The effectiveness of the two methods is proved by\nextensive computational experiments performed both on instances derived from a\nreal-world case study, as well as on benchmark instances from the literature on\nthe wkTRP and on related problems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:36:10 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Muritiba", "Albert Einstein Fernandes", ""], ["Bonates", "Tib\u00e9rius O.", ""], ["Da Silva", "St\u00eanio Oliveira", ""], ["Iori", "Manuel", ""]]}, {"id": "1909.06292", "submitter": "Hendrik Molter", "authors": "Hendrik Molter and Rolf Niedermeier and Malte Renken", "title": "Enumerating Isolated Cliques in Temporal Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-36683-4_42", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isolation is a concept from the world of clique enumeration that is mostly\nused to model communities that do not have much contact to the outside world.\nHerein, a clique is considered isolated if it has few edges connecting it to\nthe rest of the graph. Motivated by recent work on enumerating cliques in\ntemporal networks, we lift the isolation concept to this setting. We discover\nthat the addition of the time dimension leads to six distinct natural isolation\nconcepts. Our main contribution is the development of fixed-parameter\nenumeration algorithms for five of these six clique types employing the\nparameter \"degree of isolation\". On the empirical side, we implement and test\nthese algorithms on (temporal) social network data, obtaining encouraging\npreliminary results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:25:59 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 15:17:11 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""]]}, {"id": "1909.06363", "submitter": "Matthew Tsao", "authors": "Matthew Tsao, Kiril Solovey, Marco Pavone", "title": "Sample Complexity of Probabilistic Roadmaps via $\\epsilon$-nets", "comments": "14 pages, 4 figures, submitted to International Conference of\n  Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fundamental theoretical aspects of probabilistic roadmaps (PRM) in\nthe finite time (non-asymptotic) regime. In particular, we investigate how\ncompleteness and optimality guarantees of the approach are influenced by the\nunderlying deterministic sampling distribution ${\\mathcal{X}}$ and connection\nradius ${r>0}$. We develop the notion of ${(\\delta,\\epsilon)}$-completeness of\nthe parameters ${\\mathcal{X}, r}$, which indicates that for every\nmotion-planning problem of clearance at least ${\\delta>0}$, PRM using\n${\\mathcal{X}, r}$ returns a solution no longer than ${1+\\epsilon}$ times the\nshortest ${\\delta}$-clear path. Leveraging the concept of ${\\epsilon}$-nets, we\ncharacterize in terms of lower and upper bounds the number of samples needed to\nguarantee ${(\\delta,\\epsilon)}$-completeness. This is in contrast with previous\nwork which mostly considered the asymptotic regime in which the number of\nsamples tends to infinity. In practice, we propose a sampling distribution\ninspired by ${\\epsilon}$-nets that achieves nearly the same coverage as grids\nwhile using significantly fewer samples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 04:43:50 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 18:25:58 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Tsao", "Matthew", ""], ["Solovey", "Kiril", ""], ["Pavone", "Marco", ""]]}, {"id": "1909.06413", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci", "title": "Dynamic Graph Algorithms and Graph Sparsification: New Techniques and\n  Connections", "comments": "Doctoral thesis; abstract shortened to respect the arXiv limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs naturally appear in several real-world contexts including social\nnetworks, the web network, and telecommunication networks. While the analysis\nand the understanding of graph structures have been a central area of study in\nalgorithm design, the rapid increase of data sets over the last decades has\nposed new challenges for designing efficient algorithms that process\nlarge-scale graphs. These challenges arise from two usual assumptions in\nclassical algorithm design, namely that graphs are static and that they fit\ninto a single machine. However, in many application domains, graphs are subject\nto frequent changes over time, and their massive size makes them infeasible to\nbe stored in the memory of a single machine.\n  Driven by the need to devise new tools for overcoming such challenges, this\nthesis focuses on two areas of modern algorithm design that directly deal with\nprocessing massive graphs, namely dynamic graph algorithms and graph\nsparsification. We develop new algorithmic techniques from both dynamic and\nsparsification perspective for a multitude of graph-based optimization problems\nwhich lie at the core of Spectral Graph Theory, Graph Partitioning, and Metric\nEmbeddings. Our algorithms are faster than any previous one and design smaller\nsparsifiers with better (approximation) quality. More importantly, this work\nintroduces novel reduction techniques that show unexpected connections between\nseemingly different areas such as dynamic graph algorithms and graph\nsparsification.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 19:08:32 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Goranci", "Gramoz", ""]]}, {"id": "1909.06435", "submitter": "Camilo Rocha", "authors": "Carlos Pinz\\'on, Camilo Rocha, Jorge Finke", "title": "A Random Network Model for the Analysis of Blockchain Designs with\n  Communication Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a random network model for blockchains, a distributed\nhierarchical data structure of blocks that has found several applications in\nvarious industries. The model is parametric on two probability distribution\nfunctions governing block production and communication delay, which are key to\ncapture the complexity of the mechanism used to synchronize the many\ndistributed local copies of a blockchain. The proposed model is equipped with\nsimulation algorithms for both bounded and unbounded number of distributed\ncopies of the blockchain. They are used to study fast blockchain systems, i.e.,\nblockchains in which the average time of block production can match the average\ntime of message broadcasting used for blockchain synchronization. In\nparticular, the model and the algorithms are useful to understand efficiency\ncriteria associated with fast blockchains for identifying, e.g., when\nincreasing the block production will have negative impact on the stability of\nthe distributed data structure given the network's broadcast delay.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:26:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Pinz\u00f3n", "Carlos", ""], ["Rocha", "Camilo", ""], ["Finke", "Jorge", ""]]}, {"id": "1909.06444", "submitter": "Shashank Vatedka", "authors": "Shashank Vatedka and Aslan Tchamkerten", "title": "Local Decode and Update for Big Data Compression", "comments": "19 pages, 4 figures. v2: updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates data compression that simultaneously allows local\ndecoding and local update. The main result is a universal compression scheme\nfor memoryless sources with the following features. The rate can be made\narbitrarily close to the entropy of the underlying source, contiguous fragments\nof the source can be recovered or updated by probing or modifying a number of\ncodeword bits that is on average linear in the size of the fragment, and the\noverall encoding and decoding complexity is quasilinear in the blocklength of\nthe source. In particular, the local decoding or update of a single message\nsymbol can be performed by probing or modifying a constant number of codeword\nbits. This latter part improves over previous best known results for which\nlocal decodability or update efficiency grows logarithmically with blocklength.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:51:30 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 20:06:46 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 07:57:50 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Vatedka", "Shashank", ""], ["Tchamkerten", "Aslan", ""]]}, {"id": "1909.06485", "submitter": "Vahan Huroyan", "authors": "Md Iqbal Hossain and Vahan Huroyan and Stephen Kobourov and Raymundo\n  Navarrete", "title": "Multi-Perspective, Simultaneous Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe MPSE: a Multi-Perspective Simultaneous Embedding method for\nvisualizing high-dimensional data, based on multiple pairwise distances between\nthe data points. Specifically, MPSE computes positions for the points in 3D and\nprovides different views into the data by means of 2D projections (planes) that\npreserve each of the given distance matrices. We consider two versions of the\nproblem: fixed projections and variable projections. MPSE with fixed\nprojections takes as input a set of pairwise distance matrices defined on the\ndata points, along with the same number of projections and embeds the points in\n3D so that the pairwise distances are preserved in the given projections. MPSE\nwith variable projections takes as input a set of pairwise distance matrices\nand embeds the points in 3D while also computing the appropriate projections\nthat preserve the pairwise distances. The proposed approach can be useful in\nmultiple scenarios: from creating simultaneous embedding of multiple graphs on\nthe same set of vertices, to reconstructing a 3D object from multiple 2D\nsnapshots, to analyzing data from multiple points of view. We provide a\nfunctional prototype of MPSE that is based on an adaptive and stochastic\ngeneralization of multi-dimensional scaling to multiple distances and multiple\nvariable projections. We provide an extensive quantitative evaluation with\ndatasets of different sizes and using different number of projections, as well\nas several examples that illustrate the quality of the resulting solutions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 23:20:17 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 10:41:39 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 00:55:29 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hossain", "Md Iqbal", ""], ["Huroyan", "Vahan", ""], ["Kobourov", "Stephen", ""], ["Navarrete", "Raymundo", ""]]}, {"id": "1909.06524", "submitter": "Alex Rusciano", "authors": "Grey Ballard, James Demmel, Ioana Dumitriu, Alexander Rusciano", "title": "A Generalized Randomized Rank-Revealing Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Generalized Randomized QR-decomposition that may be applied to\narbitrary products of matrices and their inverses, without needing to\nexplicitly compute the products or inverses. This factorization is a critical\npart of a communication-optimal spectral divide-and-conquer algorithm for the\nnonsymmetric eigenvalue problem. In this paper, we establish that this\nrandomized QR-factorization satisfies the strong rank-revealing properties. We\nalso formally prove its stability, making it suitable in applications. Finally,\nwe present numerical experiments which demonstrate that our theoretical bounds\ncapture the empirical behavior of the factorization.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 03:50:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Dumitriu", "Ioana", ""], ["Rusciano", "Alexander", ""]]}, {"id": "1909.06535", "submitter": "Zhimin Gao", "authors": "Zhimin Gao, Lei Xu, Keshav Kasichainula, Lin Chen, Bogdan Carbunar,\n  Weidong Shi", "title": "Private and Atomic Exchange of Assets over Zero Knowledge Based Payment\n  Ledger", "comments": "Single column, 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin brings a new type of digital currency that does not rely on a central\nsystem to maintain transactions. By benefiting from the concept of\ndecentralized ledger, users who do not know or trust each other can still\nconduct transactions in a peer-to-peer manner. Inspired by Bitcoin, other\ncryptocurrencies were invented in recent years such as Ethereum, Dash, Zcash,\nMonero, Grin, etc. Some of these focus on enhancing privacy for instance crypto\nnote or systems that apply the similar concept of encrypted notes used for\ntransactions to enhance privacy (e.g., Zcash, Monero). However, there are few\nmechanisms to support the exchange of privacy-enhanced notes or assets on the\nchain, and at the same time preserving the privacy of the exchange operations.\nExisting approaches for fair exchanges of assets with privacy mostly rely on\noff-chain/side-chain, escrow or centralized services. Thus, we propose a\nsolution that supports oblivious and privacy-protected fair exchange of crypto\nnotes or privacy enhanced crypto assets. The technology is demonstrated by\nextending zero-knowledge based crypto notes. To address \"privacy\" and\n\"multi-currency\", we build a new zero-knowledge proving system and extend note\nformat with new property to represent various types of tokenized assets or\ncryptocurrencies. By extending the payment protocol, exchange operations are\nrealized through privacy enhanced transactions (e.g., shielded transactions).\nBased on the possible scenarios during the exchange operation, we add new\nconstraints and conditions to the zero-knowledge proving system used for\nvalidating transactions publicly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 05:14:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gao", "Zhimin", ""], ["Xu", "Lei", ""], ["Kasichainula", "Keshav", ""], ["Chen", "Lin", ""], ["Carbunar", "Bogdan", ""], ["Shi", "Weidong", ""]]}, {"id": "1909.06653", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Monika Henzinger, Dariusz Leniowski", "title": "A Tree Structure For Dynamic Facility Location", "comments": "An extended abstract appeared at the 26th Annual European Symposium\n  on Algorithms (ESA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the metric facility location problem with client insertions and\ndeletions. This setting differs from the classic dynamic facility location\nproblem, where the set of clients remains the same, but the metric space can\nchange over time. We show a deterministic algorithm that maintains a constant\nfactor approximation to the optimal solution in worst-case time $\\tilde\nO(2^{O(\\kappa^2)})$ per client insertion or deletion in metric spaces while\nanswering queries about the cost in $O(1)$ time, where $\\kappa$ denotes the\ndoubling dimension of the metric. For metric spaces with bounded doubling\ndimension, the update time is polylogarithmic in the parameters of the problem.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 18:48:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Leniowski", "Dariusz", ""]]}, {"id": "1909.06747", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao and Jiaxing Ling", "title": "Algorithms for Manipulating Sequential Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential allocation is a simple and widely studied mechanism to allocate\nindivisible items in turns to agents according to a pre-specified picking\nsequence of agents. At each turn, the current agent in the picking sequence\npicks its most preferred item among all items having not been allocated yet.\nThis problem is well-known to be not strategyproof, i.e., an agent may get more\nutility by reporting an untruthful preference ranking of items. It arises the\nproblem: how to find the best response of an agent?\n  It is known that this problem is polynomially solvable for only two agents\nand NP-complete for arbitrary number of agents.\n  The computational complexity of this problem with three agents was left as an\nopen problem. In this paper, we give a novel algorithm that solves the problem\nin polynomial time for each fixed number of agents. We also show that an agent\ncan always get at least half of its optimal utility by simply using its\ntruthful preference as the response.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 06:45:48 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Xiao", "Mingyu", ""], ["Ling", "Jiaxing", ""]]}, {"id": "1909.06794", "submitter": "N. Jesper Larsson", "authors": "N. Jesper Larsson", "title": "Run-Length Encoding in a Finite Universe", "comments": "SPIRE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text compression schemes and compact data structures usually combine\nsophisticated probability models with basic coding methods whose average\ncodeword length closely match the entropy of known distributions. In the\nfrequent case where basic coding represents run-lengths of outcomes that have\nprobability $p$, i.e. the geometric distribution $\\Pr(i)=p^i(1-p)$, a\n\\emph{Golomb code} is an optimal instantaneous code, which has the additional\nadvantage that codewords can be computed using only an integer parameter\ncalculated from $p$, without need for a large or sophisticated data structure.\nGolomb coding does not, however, gracefully handle the case where run-lengths\nare bounded by a known integer~$n$. In this case, codewords allocated for the\ncase $i>n$ are wasted. While negligible for large $n$, this makes Golomb coding\nunattractive in situations where $n$ is recurrently small, e.g., when\nrepresenting many short lists of integers drawn from limited ranges, or when\nthe range of $n$ is narrowed down by a recursive algorithm. We address the\nproblem of choosing a code for this case, considering efficiency from both\ninformation-theoretic and computational perspectives, and arrive at a simple\ncode that allows computing a codeword using only $O(1)$ simple computer\noperations and $O(1)$ machine words. We demonstrate experimentally that the\nresulting representation length is very close (equal in a majority of tested\ncases) to the optimal Huffman code, to the extent that the expected difference\nis practically negligible. We describe efficient branch-free implementation of\nencoding and decoding.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 12:48:00 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 05:13:05 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Larsson", "N. Jesper", ""]]}, {"id": "1909.06811", "submitter": "Ran Gelles", "authors": "Yagel Ashkenazi, Ran Gelles, Amir Leshem", "title": "Noisy Beeping Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce noisy beeping networks, where nodes have limited communication\ncapabilities, namely, they can only emit energy or sense the channel for\nenergy. Furthermore, imperfections may cause devices to malfunction with some\nfixed probability when sensing the channel, which amounts to deducing a noisy\nreceived transmission. Such noisy networks have implications for\nultra-lightweight sensor networks and biological systems.\n  We show how to compute tasks in a noise-resilient manner over noisy beeping\nnetworks of arbitrary structure. In particular, we transform any algorithm that\nassumes a noiseless beeping network (of size $n$) into a noise-resilient\nversion while incurring a multiplicative overhead of only $O(\\log n)$ in its\nround complexity, with high probability. We show that our coding is optimal for\nsome tasks, such as node-coloring of a clique.\n  We further show how to simulate a large family of algorithms designed for\ndistributed networks in the CONGEST($B$) model over a noisy beeping network.\nThe simulation succeeds with high probability and incurs an asymptotic\nmultiplicative overhead of $O(B\\cdot \\Delta \\cdot \\min(n,\\Delta^2))$ in the\nround complexity, where $\\Delta$ is the maximal degree of the network. The\noverhead is tight for certain graphs, e.g., a clique. Further, this simulation\nimplies a constant overhead coding for constant-degree networks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 15:00:36 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ashkenazi", "Yagel", ""], ["Gelles", "Ran", ""], ["Leshem", "Amir", ""]]}, {"id": "1909.06835", "submitter": "Manuel Iori Dr", "authors": "Jean-Fran\\c{c}ois C\\^ot\\'e and Mohamed Haouari and Manuel Iori", "title": "A Primal Decomposition Algorithm for the Two-dimensional Bin Packing\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Two-dimensional Bin Packing Problem calls for packing a set of\nrectangular items into a minimal set of larger rectangular bins. Items must be\npacked with their edges parallel to the borders of the bins, cannot be rotated\nand cannot overlap among them. The problem is of interest because it models\nmany real-world applications, including production, warehouse management and\ntransportation. It is, unfortunately, very difficult, and instances with just\n40 items are unsolved to proven optimality, despite many attempts, since the\n1990s. In this paper, we solve the problem with a combinatorial Benders\ndecomposition that is based on a simple model in which the two-dimensional\nitems and bins are just represented by their areas, and infeasible packings are\nimposed by means of exponentially-many no-good cuts. The basic decomposition\nscheme is quite naive, but we enrich it with a number of preprocessing\ntechniques, valid inequalities, lower bounding methods, and enhanced algorithms\nto produce the strongest possible cuts. The resulting algorithm behaved very\nwell on the benchmark sets of instances, improving on average upon previous\nalgorithms from the literature and solving for the first time a number of open\ninstances.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 16:56:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["C\u00f4t\u00e9", "Jean-Fran\u00e7ois", ""], ["Haouari", "Mohamed", ""], ["Iori", "Manuel", ""]]}, {"id": "1909.06988", "submitter": "Sidhanth Mohanty", "authors": "Sidhanth Mohanty, Ryan O'Donnell, Pedro Paredes", "title": "Explicit near-Ramanujan graphs of every degree", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every constant $d \\geq 3$ and $\\epsilon > 0$, we give a deterministic\n$\\mathrm{poly}(n)$-time algorithm that outputs a $d$-regular graph on\n$\\Theta(n)$ vertices that is $\\epsilon$-near-Ramanujan; i.e., its eigenvalues\nare bounded in magnitude by $2\\sqrt{d-1} + \\epsilon$ (excluding the single\ntrivial eigenvalue of~$d$).\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:03:38 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 22:03:49 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Mohanty", "Sidhanth", ""], ["O'Donnell", "Ryan", ""], ["Paredes", "Pedro", ""]]}, {"id": "1909.07013", "submitter": "Csaba D. Toth", "authors": "Daniel Archambault, Csaba D. T\\'oth", "title": "Proceedings of the 27th International Symposium on Graph Drawing and\n  Network Visualization (GD 2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.HC cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the arXiv index for the electronic proceedings of GD 2019, which\ncontains the peer-reviewed and revised accepted papers with an optional\nappendix. Proceedings (without appendices) are also to be published by Springer\nin the Lecture Notes in Computer Science series.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 06:29:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Archambault", "Daniel", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1909.07041", "submitter": "Fei Ma", "authors": "Xudong Luo, Fei Ma and Wentao Xu", "title": "Exact solutions for geodesic distance on treelike models with some\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic distance, commonly called shortest path length, has proved useful in\na great variety of disciplines. It has been playing a significant role in\nsearch engine at present and so attracted considerable attention at the last\nfew decades, particularly, almost all data structures and corresponding\nalgorithms suitable to searching information generated based on treelike\nmodels. Hence, we, in this paper, study in detail geodesic distance on some\ntreelike models which can be generated by three different types of operations,\nincluding first-order subdivision, ($1,m$)-star-fractal operation and\n$m$-vertex-operation. Compared to the most best used approaches for calculating\ngeodesic distance on graphs, for instance, enumeration method and matrix\nmultiplication, we take useful advantage of a novel method consisting in spirit\nof the concept of vertex cover in the language of graph theory and mapping. For\neach kind of treelike model addressed here, we certainly obtain an exact\nsolution for its geodesic distance using our method. With the help of computer\nsimulations, we confirm that the analytical results are in perfect agreement\nwith simulations. In addition, we also report some intriguing structure\nproperties on treelike models of two types among them. The one obeys\nexponential degree distribution seen in many complex networks, by contrast, the\nother possesses all but leaf vertices with identical degree and shows more\nhomogeneous topological structure than the former. Besides that, the both have,\nin some sense, self-similar feature but instead the latter exhibits fractal\nproperty.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:47:56 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Luo", "Xudong", ""], ["Ma", "Fei", ""], ["Xu", "Wentao", ""]]}, {"id": "1909.07059", "submitter": "Andreas Galanis", "authors": "Charilaos Efthymiou, Andreas Galanis, Thomas P. Hayes, Daniel\n  Stefankovic, Eric Vigoda", "title": "Improved Strong Spatial Mixing for Colorings on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong spatial mixing (SSM) is a form of correlation decay that has played an\nessential role in the design of approximate counting algorithms for spin\nsystems. A notable example is the algorithm of Weitz (2006) for the hard-core\nmodel on weighted independent sets. We study SSM for the $q$-colorings problem\non the infinite $(d+1)$-regular tree. Weak spatial mixing (WSM) captures\nwhether the influence of the leaves on the root vanishes as the height of the\ntree grows. Jonasson (2002) established WSM when $q>d+1$. In contrast, in SSM,\nwe first fix a coloring on a subset of internal vertices, and we again ask if\nthe influence of the leaves on the root is vanishing. It was known that SSM\nholds on the $(d+1)$-regular tree when $q>\\alpha d$ where $\\alpha\\approx\n1.763...$ is a constant that has arisen in a variety of results concerning\nrandom colorings. Here we improve on this bound by showing SSM for $q>1.59d$.\nOur proof establishes an $L^2$ contraction for the BP operator. For the\ncontraction we bound the norm of the BP Jacobian by exploiting combinatorial\nproperties of the coloring of the tree.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:41:40 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Efthymiou", "Charilaos", ""], ["Galanis", "Andreas", ""], ["Hayes", "Thomas P.", ""], ["Stefankovic", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "1909.07093", "submitter": "Fabrizio Frati", "authors": "Fidel Barrera-Cruz, Manuel Borrazzo, Giordano Da Lozzo, Giuseppe Di\n  Battista, Fabrizio Frati, Maurizio Patrignani, and Vincenzo Roselli", "title": "How to Morph a Tree on a Small Grid", "comments": "A preliminary version of this paper appears in WADS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study planar morphs between straight-line planar grid\ndrawings of trees. A morph consists of a sequence of morphing steps, where in a\nmorphing step vertices move along straight-line trajectories at constant speed.\nWe show how to construct planar morphs that simultaneously achieve a reduced\nnumber of morphing steps and a polynomially-bounded resolution. We assume that\nboth the initial and final drawings lie on the grid and we ensure that each\nmorphing step produces a grid drawing; further, we consider both upward\ndrawings of rooted trees and drawings of arbitrary trees.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:54:20 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 13:19:14 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Barrera-Cruz", "Fidel", ""], ["Borrazzo", "Manuel", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1909.07251", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Note on distributed certification of minimum spanning trees", "comments": "A note explaining a known result. (New version, with minor changes.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed proof (also known as local certification, or proof-labeling\nscheme) is a mechanism to certify that the solution to a graph problem is\ncorrect. It takes the form of an assignment of labels to the nodes, that can be\nchecked locally. There exists such a proof for the minimum spanning tree\nproblem, using $O(\\log n \\log W)$ bit labels (where $n$ is the number of nodes\nin the graph, and $W$ is the largest weight of an edge). This is due to Korman\nand Kutten who describe it in concise and formal manner in [Korman and Kutten\n07]. In this note, we propose a more intuitive description of the result, as\nwell as a gentle introduction to the problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:55:31 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 15:36:31 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "1909.07313", "submitter": "Edwin Lock", "authors": "Elizabeth Baldwin, Paul W. Goldberg, Paul Klemperer, Edwin Lock", "title": "Solving Strong-Substitutes Product-Mix Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops algorithms to solve strong-substitutes product-mix\nauctions. That is, it finds competitive equilibrium prices and quantities for\nagents who use this auction's bidding language to truthfully express their\nstrong-substitutes preferences over an arbitrary number of goods, each of which\nis available in multiple discrete units. (Strong substitutes preferences are\nalso known, in other literatures, as $M^\\natural$-concave, matroidal and\nwell-layered maps, and valuated matroids). Our use of the bidding language, and\nthe information it provides, contrasts with existing algorithms that rely on\naccess to a valuation or demand oracle to find equilibrium.\n  We compute market-clearing prices using algorithms that apply existing\nsubmodular minimisation methods. Allocating the supply among the bidders at\nthese prices then requires solving a novel constrained matching problem. Our\nalgorithm iteratively simplifies the allocation problem, perturbing bids and\nprices in a way that resolves tie-breaking choices created by bids that can be\naccepted on more than one good. We provide practical running time bounds on\nboth price-finding and allocation, and illustrate experimentally that our\nallocation mechanism is practical.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:18:12 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 09:26:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Baldwin", "Elizabeth", ""], ["Goldberg", "Paul W.", ""], ["Klemperer", "Paul", ""], ["Lock", "Edwin", ""]]}, {"id": "1909.07326", "submitter": "Martin Kouteck\\'y", "authors": "Du\\v{s}an Knop, Martin Kouteck\\'y, Asaf Levin, Matthias Mnich, Shmuel\n  Onn", "title": "Multitype Integer Monoid Optimization and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Configuration integer programs (IP) have been key in the design of algorithms\nfor NP-hard high-multiplicity problems since the pioneering work of Gilmore and\nGomory [Oper. Res., 1961]. Configuration IPs have a variable for each possible\nconfiguration, which describes a placement of items into a location, and whose\nvalue corresponds to the number of locations with that placement. In high\nmultiplicity problems items come in types, and are represented succinctly by a\nvector of multiplicities; solving the configuration IP then amounts to deciding\nwhether the input vector of multiplicities of items of each type can be\ndecomposed into a given number of configurations.\n  We make this implicit notion explicit by observing that the set of all input\nvectors decomposable into configurations forms a monoid, and solving the\nconfiguration IP is the Monoid Decomposition problem. Motivated by\napplications, we enrich this problem in two ways. First, sometimes each\nconfiguration additionally has an objective value, yielding an optimization\nproblem of finding a \"best\" decomposition under the given objective. Second,\nthere are often different types of configurations for different types of\nlocations. The resulting problem is to optimize over decompositions of the\ninput multiplicity vector into configurations of several types, and we call it\nMultitype Integer Monoid Optimization, or MIMO.\n  We develop fast exact algorithms for various MIMO with few or many location\ntypes and with various objectives. Our algorithms build on a novel proximity\ntheorem connecting the solutions of a certain configuration IP to those of its\ncontinuous relaxation. We then cast several fundamental scheduling and bin\npacking problems as MIMOs, and thereby obtain new or substantially faster\nalgorithms for them.\n  We complement our positive algorithmic results by hardness results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:40:11 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""], ["Levin", "Asaf", ""], ["Mnich", "Matthias", ""], ["Onn", "Shmuel", ""]]}, {"id": "1909.07420", "submitter": "Marco Fiorucci", "authors": "Marco Fiorucci", "title": "Regular Partitions and Their Use in Structural Pattern Recognition", "comments": "PhD Thesis (Mar 2019), Ca Foscari University, Venice. arXiv admin\n  note: text overlap with arXiv:1704.07114 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years are characterized by an unprecedented quantity of available\nnetwork data which are produced at an astonishing rate by an heterogeneous\nvariety of interconnected sensors and devices. This high-throughput generation\ncalls for the development of new effective methods to store, retrieve,\nunderstand and process massive network data. In this thesis, we tackle this\nchallenge by introducing a framework to summarize large graphs based on\nSzemer\\'edi's Regularity Remma (RL), which roughly states that any sufficiently\nlarge graph can almost entirely be partitioned into a bounded number of\nrandom-like bipartite graphs. The partition resulting from the RL gives rise to\na summary, which inherits many of the essential structural properties of the\noriginal graph. We first extend an heuristic version of the RL to improve its\nefficiency and its robustness. We use the proposed algorithm to address\ngraph-based clustering and image segmentation tasks. In the second part of the\nthesis, we introduce a new heuristic algorithm which is characterized by an\nimprovement of the summary quality both in terms of reconstruction error and of\nnoise filtering. We use the proposed heuristic to address the graph search\nproblem defined under a similarity measure. Finally, we study the linkage among\nthe regularity lemma, the stochastic block model and the minimum description\nlength. This study provide us a principled way to develop a graph decomposition\nalgorithm based on stochastic block model which is fitted using likelihood\nmaximization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 18:14:05 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 21:15:53 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Fiorucci", "Marco", ""]]}, {"id": "1909.07446", "submitter": "Hsueh-I Lu", "authors": "Kai-Yuan Lai, Hsueh-I Lu, and Mikkel Thorup", "title": "Three-in-a-Tree in Near Linear Time", "comments": "46 pages, 12 figures, accepted to STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-in-a-tree problem is to determine if a simple undirected graph\ncontains an induced subgraph which is a tree connecting three given vertices.\nBased on a beautiful characterization that is proved in more than twenty pages,\nChudnovsky and Seymour [Combinatorica 2010] gave the previously only known\npolynomial-time algorithm, running in $O(mn^2)$ time, to solve the\nthree-in-a-tree problem on an $n$-vertex $m$-edge graph. Their three-in-a-tree\nalgorithm has become a critical subroutine in several state-of-the-art graph\nrecognition and detection algorithms.\n  In this paper we solve the three-in-a-tree problem in $\\tilde{O}(m)$ time,\nleading to improved algorithms for recognizing perfect graphs and detecting\nthetas, pyramids, beetles, and odd and even holes. Our result is based on a new\nand more constructive characterization than that of Chudnovsky and Seymour. Our\nnew characterization is stronger than the original, and our proof implies a new\nsimpler proof for the original characterization. The improved characterization\ngains the first factor $n$ in speed. The remaining improvement is based on\ndynamic graph algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:33:28 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 18:57:24 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 19:36:44 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Lai", "Kai-Yuan", ""], ["Lu", "Hsueh-I", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1909.07511", "submitter": "Ragesh Jaiswal", "authors": "Dishant Goyal, Ragesh Jaiswal, Amit Kumar", "title": "Streaming PTAS for Constrained k-Means", "comments": "Changes from previous version: (i) added discussion on coreset, and\n  (ii) fixed few typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalise the results of Bhattacharya et al. (Journal of Computing\nSystems, 62(1):93-115, 2018) for the list-$k$-means problem defined as -- for a\n(unknown) partition $X_1, ..., X_k$ of the dataset $X \\subseteq \\mathbb{R}^d$,\nfind a list of $k$-center sets (each element in the list is a set of $k$\ncenters) such that at least one of $k$-center sets $\\{c_1, ..., c_k\\}$ in the\nlist gives an $(1+\\varepsilon)$-approximation with respect to the cost function\n$\\min_{\\textrm{permutation } \\pi} \\left[ \\sum_{i=1}^{k} \\sum_{x \\in X_i} ||x -\nc_{\\pi(i)}||^2 \\right]$. The list-$k$-means problem is important for the\nconstrained $k$-means problem since algorithms for the former can be converted\nto PTAS for various versions of the latter. Following are the consequences of\nour generalisations:\n  - Streaming algorithm: Our $D^2$-sampling based algorithm running in a single\niteration allows us to design a 2-pass, logspace streaming algorithm for the\nlist-$k$-means problem. This can be converted to a 4-pass, logspace streaming\nPTAS for various constrained versions of the $k$-means problem.\n  - Faster PTAS under stability: Our generalisation is also useful in $k$-means\nclustering scenarios where finding good centers becomes easy once good centers\nfor a few \"bad\" clusters have been chosen. One such scenario is clustering\nunder stability where the number of such bad clusters is a constant. Using the\nabove idea, we significantly improve the running time of the known algorithm\nfrom $O(dn^3) (k \\log{n})^{poly(\\frac{1}{\\beta}, \\frac{1}{\\varepsilon})}$ to $O\n\\left(dn^3 k^{\\tilde{O}_{\\beta \\varepsilon}(\\frac{1}{\\beta \\varepsilon})}\n\\right)$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:49:54 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 01:39:33 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Goyal", "Dishant", ""], ["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "1909.07515", "submitter": "Ragesh Jaiswal", "authors": "Ragesh Jaiswal and Amit Kumar", "title": "Multiplicative Rank-1 Approximation using Length-Squared Sampling", "comments": "A section on open problems added in the new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the span of $\\Omega(\\frac{1}{\\varepsilon^4})$ rows of any matrix\n$A \\subset \\mathbb{R}^{n \\times d}$ sampled according to the length-squared\ndistribution contains a rank-$1$ matrix $\\tilde{A}$ such that $||A -\n\\tilde{A}||_F^2 \\leq (1 + \\varepsilon) \\cdot ||A - \\pi_1(A)||_F^2$, where\n$\\pi_1(A)$ denotes the best rank-$1$ approximation of $A$ under the Frobenius\nnorm. Length-squared sampling has previously been used in the context of\nrank-$k$ approximation. However, the approximation obtained was additive in\nnature. We obtain a multiplicative approximation albeit only for rank-$1$\napproximation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:56:36 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 20:38:59 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "1909.07538", "submitter": "Diptarama Hendrian", "authors": "Diptarama Hendrian", "title": "Generalized Dictionary Matching under Substring Consistent Equivalence\n  Relations", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of patterns called a dictionary and a text, the dictionary\nmatching problem is a task to find all occurrence positions of all patterns in\nthe text. The dictionary matching problem can be solved efficiently by using\nthe Aho-Corasick algorithm. Recently, Matsuoka et al. [TCS, 2016] proposed a\ngeneralization of pattern matching problem under substring consistent\nequivalence relations and presented a generalization of the Knuth-Morris-Pratt\nalgorithm to solve this problem. An equivalence relation $\\approx$ is a\nsubstring consistent equivalence relation (SCER) if for two strings $X,Y$, $X\n\\approx Y$ implies $|X| = |Y|$ and $X[i:j] \\approx Y[i:j]$ for all $1 \\le i \\le\nj \\le |X|$. In this paper, we propose a generalization of the dictionary\nmatching problem and present a generalization of the Aho-Corasick algorithm for\nthe dictionary matching under SCER. We present an algorithm that constructs\nSCER automata and an algorithm that performs dictionary matching under SCER by\nusing the automata. Moreover, we show the time and space complexity of our\nalgorithms with respect to the size of input strings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 01:08:27 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 07:36:36 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Hendrian", "Diptarama", ""]]}, {"id": "1909.07557", "submitter": "Mingyu Xiao", "authors": "Sen Huang and Mingyu Xiao", "title": "Object Reachability via Swaps under Strict and Weak Preferences", "comments": "This version is to appear in Autonomous Agents and Multi-Agent\n  Systems", "journal-ref": "A previous version presented at AAAI 2019 with the title 'Object\n  Reachability via Swaps along a Line'", "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\textsc{Housing Market} problem is a widely studied resource allocation\nproblem. In this problem, each agent can only receive a single object and has\npreferences over all objects. Starting from an initial endowment, we want to\nreach a certain assignment via a sequence of rational trades. We first consider\nwhether an object is reachable for a given agent under a social network, where\na trade between two agents is allowed if they are neighbors in the network and\nno participant has a deficit from the trade. Assume that the preferences of the\nagents are strict (no tie among objects is allowed). This problem is\npolynomially solvable in a star-network and NP-complete in a tree-network. It\nis left as a challenging open problem whether the problem is polynomially\nsolvable when the network is a path. We answer this open problem positively by\ngiving a polynomial-time algorithm. Then we show that when the preferences of\nthe agents are weak (ties among objects are allowed), the problem becomes\nNP-hard even when the network is a path. In addition, we consider the\ncomputational complexity of finding different optimal assignments for the\nproblem under the network being a path or a star.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:36:41 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 08:40:52 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Huang", "Sen", ""], ["Xiao", "Mingyu", ""]]}, {"id": "1909.07633", "submitter": "Ziyue Huang", "authors": "Ziyue Huang, Ke Yi", "title": "Communication-Efficient Weighted Sampling and Quantile Summary for GBDT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting decision tree (GBDT) is a powerful and widely-used machine\nlearning model, which has achieved state-of-the-art performance in many\nacademic areas and production environment. However, communication overhead is\nthe main bottleneck in distributed training which can handle the massive data\nnowadays. In this paper, we propose two novel communication-efficient methods\nover distributed dataset to mitigate this problem, a weighted sampling approach\nby which we can estimate the information gain over a small subset efficiently,\nand distributed protocols for weighted quantile problem used in approximate\ntree learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 07:55:04 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Huang", "Ziyue", ""], ["Yi", "Ke", ""]]}, {"id": "1909.07647", "submitter": "Hisao Tamaki", "authors": "Hisao Tamaki", "title": "A heuristic use of dynamic programming to upperbound treewidth", "comments": "14 pages, 2 tables. In v2, some typographical errors, as well as an\n  incorrect statement in Proposition 3.6, are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $G$, let $\\Pi(G)$ denote the set of all potential maximal cliques\nof $G$. For each subset $\\Pi$ of $\\Pi(G)$, let $\\tw(G, \\Pi)$ denote the\nsmallest $k$ such that there is a tree-decomposition of $G$ of width $k$ whose\nbags all belong to $\\Pi$. Bouchitt\\'{e} and Todinca observed in 2001 that\n$\\tw(G, \\Pi(G))$ is exactly the treewidth of $G$ and developed a dynamic\nprogramming algorithm to compute it. Indeed, their algorithm can readily be\napplied to an arbitrary non-empty subset $\\Pi$ of $\\Pi(G)$ and computes $\\tw(G,\n\\Pi)$, or reports that it is undefined, in time $|\\Pi||V(G)|^{O(1)}$. This\nefficient tool for computing $\\tw(G, \\Pi)$ allows us to conceive of an\niterative improvement procedure for treewidth upper bounds which maintains, as\nthe current solution, a set of potential maximal cliques rather than a\ntree-decomposition.\n  We design and implement an algorithm along this approach. Experiments show\nthat our algorithm vastly outperforms previously implemented heuristic\nalgorithms for treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 08:35:25 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 07:54:20 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Tamaki", "Hisao", ""]]}, {"id": "1909.07703", "submitter": "J\\\"urgen Scherer", "authors": "J\\\"urgen Scherer, Bernhard Rinner", "title": "Multi-robot persistent surveillance with connectivity constraints", "comments": null, "journal-ref": "IEEE Access (2020)", "doi": "10.1109/ACCESS.2020.2967650", "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots, especially unmanned aerial vehicles (UAVs), are of increasing\ninterest for surveillance and disaster response scenarios. We consider the\nproblem of multi-robot persistent surveillance with connectivity constraints\nwhere robots have to visit sensing locations periodically and maintain a\nmulti-hop connection to a base station. We formally define several problem\ninstances closely related to multi-robot persistent surveillance with\nconnectivity constraints, i.e., connectivity-constrained multi-robot persistent\nsurveillance (CMPS), connectivity-constrained multi-robot reachability (CMR),\nand connectivity-constrained multi-robot reachability with relay dropping\n(CMRD), and show that they are all NP-hard on general graph. We introduce three\nheuristics with different planning horizons for convex grid graphs and combine\nthese with a tree traversal approach which can be applied to a partitioning of\nnon-convex grid graphs (CMPS with tree traversal, CMPSTT). In simulation\nstudies we show that a short horizon greedy approach, which requires parameters\nto be optimized beforehand, can outperform a full horizon approach, which\nrequires a tour through all sensing locations, if the number of robots is\nlarger than the minimum number of robots required to reach all sensing\nlocations. The minimum number required is the number of robots necessary for\nbuilding a chain to the farthest sensing location from the base station.\nFurthermore, we show that partitioning the area and applying the tree traversal\napproach can achieve a performance similar to the unpartitioned case up to a\ncertain number of robots but requires less optimization time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 10:36:11 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Scherer", "J\u00fcrgen", ""], ["Rinner", "Bernhard", ""]]}, {"id": "1909.07780", "submitter": "Samuel Brito", "authors": "Samuel S. Brito and Haroldo G. Santos", "title": "Preprocessing and Cutting Planes with Conflict Graphs", "comments": "Preprint submitted to Computers & Operations Research in September\n  14, 2019", "journal-ref": "Computers & Operations Research, 128, 105176 (2020)", "doi": "10.1016/j.cor.2020.105176", "report-no": null, "categories": "cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the development of conflict graph-based algorithms and\ndata structures into the COIN-OR Branch-and-Cut (CBC) solver, including: $(i)$\nan efficient infrastructure for the construction and manipulation of conflict\ngraphs; $(ii)$ a preprocessing routine based on a clique strengthening scheme\nthat can both reduce the number of constraints and produce stronger\nformulations; $(iii)$ a clique cut separator capable of obtaining dual bounds\nat the root node LP relaxation that are $19.65\\%$ stronger than those provided\nby the equivalent cut generator of a state-of-the-art commercial solver, $3.62$\ntimes better than those attained by the clique cut separator of the GLPK solver\nand $4.22$ times stronger than the dual bounds obtained by the clique\nseparation routine of the COIN-OR Cut Generation Library; and $(iv)$ an\nodd-cycle cut separator with a new lifting module to produce valid odd-wheel\ninequalities. The average gap closed by this new version of CBC was up to four\ntimes better than its previous version. Moreover, the number of mixed-integer\nprograms solved by CBC in a time limit of three hours was increased by\n$23.53\\%$.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:33:52 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 19:47:46 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 22:13:06 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Brito", "Samuel S.", ""], ["Santos", "Haroldo G.", ""]]}, {"id": "1909.07816", "submitter": "Jiawei Gao", "authors": "Jiawei Gao", "title": "The Computational Complexity of Fire Emblem Series and similar Tactical\n  Role-Playing Games", "comments": "Updated the abstract and the first paragraph of \"main results\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fire Emblem (FE) is a popular turn-based tactical role-playing game (TRPG)\nseries on the Nintendo gaming consoles. This paper studies the computational\ncomplexity of a simplified version of FE (only floor tiles and wall tiles, the\nHP and other attributes of characters are constants at most 8, the movement\ndistance per character each turn is fixed to 6 tiles), and proves that: 1.\nSimplified FE is PSPACE-complete (Thus actual FE is at least as hard). 2.\nPoly-round FE is NP-complete, even when the map is cycle-free, without healing\nunits, and the weapon durability is a small constant. Poly-round FE is to\ndecide whether the player can win the game in a certain number of rounds that\nis polynomial to the map size. A map is called cycle-free if its corresponding\nplanar graph is cycle-free. These hardness results also hold for other similar\nTRPG series, such as Final Fantasy Tactics, Tactics Ogre and Disgaea.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:30:30 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 05:09:31 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Gao", "Jiawei", ""]]}, {"id": "1909.07854", "submitter": "N.S Narayanaswamy", "authors": "Manas Jyoti Kashyop, N. S. Narayanaswamy, Meghana Nasre and Sai Mohith\n  Potluri", "title": "Trade-offs in dynamic coloring for bipartite and general graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present trade-offs in the incremental and fully dynamic settings to\nmaintian a proper coloring. For any fully dynamic $2$-coloring algorithm, the\nmaximum of the update time, number of recolorings, and query time is\n$\\Omega(\\log n)$. We present a deterministic fully dynamic $2$-coloring\nalgorithm with $O(\\log^2 n)$ amortized update time, $O(\\log n)$ amortized query\ntime, and one recoloring in the worst case. For any incremental $2$-coloring\nalgorithm which explicitly maintains the color of every vertex after each\nupdate, the amortized update time and the amortized number of recolorings is\n$\\Omega(\\log n)$. For such an algorithm, in the worst case the update time and\nthe number of recolorings is $\\Omega(n)$. We then design a deterministic\nincremental $2$-coloring algorithm which explicitly maintains the color of\nevery vertex after each update, with amortized $O(\\log n)$ update time and\namortized $O(\\log n)$ many recolorings. Further, in the worst case the update\ntime and the number of recolorings is $O(n)$. Further, we present a\ndeterministic incremental $(1+2 \\log n)$-coloring algorithm which explicitly\nmaintains the color of every vertex after each update, with $O(\\alpha(n))$\namortized update time, at most one recoloring and $O(1)$ query time. We then\nshow a deterministic incremental $2$-coloring algorithm which does not maintain\ncolor of every vertex after each update, with amortized $O(\\alpha(n))$ update\ntime, amortized $O(\\alpha(n))$ recolorings, and amortized $O(\\alpha(n))$ query\ntime.\n  For general graphs and graphs of bounded arboricity $\\gamma$ and maximum\ndegree $\\Delta$ we present a deterministic $(\\Delta+1)$-coloring algorithm with\n$O(\\sqrt{m})$ update time, $O(1)$ query time, and one recoloring. Finally, we\nshow a deterministic $(\\Delta+1)$-coloring algorithm with amortized $O(\\gamma +\n\\log{n})$ update time, $O(1)$ query time, and one recoloring.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:44:11 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 14:58:13 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 08:27:30 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Kashyop", "Manas Jyoti", ""], ["Narayanaswamy", "N. S.", ""], ["Nasre", "Meghana", ""], ["Potluri", "Sai Mohith", ""]]}, {"id": "1909.07919", "submitter": "Yu Yokoi Dr.", "authors": "Satoru Iwata and Yu Yokoi", "title": "Combinatorial Algorithms for Edge-Disjoint $T$-Paths and Integer Free\n  Multiflow", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a multigraph with a set $T\\subseteq V$ of terminals. A path\nin $G$ is called a $T$-path if its ends are distinct vertices in $T$ and no\ninternal vertices belong to $T$. In 1978, Mader showed a characterization of\nthe maximum number of edge-disjoint $T$-paths. The original proof was not\nconstructive, and hence it did not suggest an efficient algorithm.\n  In this paper, we provide a combinatorial, deterministic algorithm for\nfinding the maximum number of edge-disjoint $T$-paths. The algorithm adopts an\naugmenting path approach. More specifically, we introduce a novel concept of\naugmenting walks in auxiliary labeled graphs to capture a possible augmentation\nof the number of edge-disjoint $T$-paths. To design a search procedure for an\naugmenting walk, we introduce blossoms analogously to the matching algorithm of\nEdmonds (1965), while it is neither a special case nor a generalization of the\npresent problem. When the search procedure terminates without finding an\naugmenting walk, the algorithm provides a certificate for the optimality of the\ncurrent edge-disjoint $T$-paths. Thus the correctness argument of the algorithm\nserves as an alternative direct proof of Mader's theorem on edge-disjoint\n$T$-paths. The algorithm runs in $O(|V|\\cdot |E|^2)$ time, which is much faster\nthan the best known deterministic algorithm based on a reduction to the linear\nmatroid parity problem.\n  We also present a strongly polynomial algorithm for solving the integer free\nmultiflow problem, which asks for a nonnegative integer combination of\n$T$-paths maximizing the sum of the coefficients subject to capacity\nconstraints on the edges.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 04:25:39 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Iwata", "Satoru", ""], ["Yokoi", "Yu", ""]]}, {"id": "1909.08006", "submitter": "Yuanjing Shi", "authors": "Yuanjing Shi, Zhaoxing Li", "title": "Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with\n  Limited Memory", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is the one of the fundamental tasks of modern data management\nsystems. With Disk I/O being the most-accused performance bottleneck and more\ncomputation-intensive workloads, it has come to our attention that in\nheterogeneous environment, performance bottleneck may vary among different\ninfrastructure. As a result, sort kernels need to be adaptive to changing\nhardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and\nefficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with\nutilization of local thread-level CPU cache and efficient disk/memory I/O.\nLeyenda is capable of performing either internal or external sort efficiently,\nbased on different I/O and processing conditions. We benchmarked Leyenda with\nthree different workloads from Sort Benchmark, targeting three unique use\ncases, including internal, partially in-memory and external sort, and we found\nLeyenda to outperform GNU's parallel in-memory quick/merge sort implementations\nby up to three times. Leyenda is also ranked the second best external sort\nalgorithm on ACM 2019 SIGMOD programming contest and forth overall.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:10:04 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Shi", "Yuanjing", ""], ["Li", "Zhaoxing", ""]]}, {"id": "1909.08065", "submitter": "David Harris", "authors": "David G. Harris", "title": "Deterministic algorithms for the Lovasz Local Lemma: simpler, more\n  general, and more parallel", "comments": "This superseded arxiv:1807.06672", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) is a keystone principle in probability\ntheory, guaranteeing the existence of configurations which avoid a collection\n$\\mathcal B$ of \"bad\" events which are mostly independent and have low\nprobability. In its simplest \"symmetric\" form, it asserts that whenever a\nbad-event has probability $p$ and affects at most $d$ bad-events, and $e p d <\n1$, then a configuration avoiding all $\\mathcal B$ exists.\n  A seminal algorithm of Moser & Tardos (2010) gives nearly-automatic\nrandomized algorithms for most constructions based on the LLL. However,\ndeterministic algorithms have lagged behind. We address three specific\nshortcomings of the prior deterministic algorithms. First, our algorithm\napplies to the LLL criterion of Shearer (1985); this is more powerful than\nalternate LLL criteria and also removes a number of nuisance parameters and\nleads to cleaner and more legible bounds. Second, we provide parallel\nalgorithms with much greater flexibility in the functional form of of the\nbad-events. Third, we provide a derandomized version of the MT-distribution,\nthat is, the distribution of the variables at the termination of the MT\nalgorithm.\n  We show applications to non-repetitive vertex coloring, independent\ntransversals, strong coloring, and other problems. These give deterministic\nalgorithms which essentially match the best previous randomized sequential and\nparallel algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 20:02:39 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 14:41:58 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1909.08263", "submitter": "EPTCS", "authors": "Marco De Bortoli (Graz University of Technology)", "title": "Distributed Answer Set Coloring: Stable Models Computation via Graph\n  Coloring", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 441-451", "doi": "10.4204/EPTCS.306.60", "report-no": null, "categories": "cs.DC cs.AI cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a famous logic language for knowledge\nrepresentation, which has been really successful in the last years, as\nwitnessed by the great interest into the development of efficient solvers for\nASP. Yet, the great request of resources for certain types of problems, as the\nplanning ones, still constitutes a big limitation for problem solving.\nParticularly, in the case the program is grounded before the resolving phase,\nan exponential blow up of the grounding can generate a huge ground file,\ninfeasible for single machines with limited resources, thus preventing even the\ndiscovering of a single non-optimal solution. To address this problem, in this\npaper we present a distributed approach to ASP solving, exploiting distributed\ncomputation benefits in order to overcome the just explained limitations. The\nhere presented tool, which is called Distributed Answer Set Coloring (DASC), is\na pure solver based on the well-known Graph Coloring algorithm. DASC is part of\na bigger project aiming to bring logic programming into a distributed system,\nstarted in 2017 by Federico Igne with mASPreduce and continued in 2018 by\nPietro Totis with a distributed grounder. In this paper we present a low level\nimplementation of the Graph Coloring algorithm, via the Boost and MPI libraries\nfor C++. Finally, we provide a few results of the very first working version of\nour tool, at the moment without any strong optimization or heuristic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:16:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["De Bortoli", "Marco", "", "Graz University of Technology"]]}, {"id": "1909.08307", "submitter": "Vladimir Manuilov", "authors": "Alexander Mishchenko, Vladimir Manuilov, Chao You, Han Yang", "title": "Hypergraph partitions", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a reduction of the combinatorial problem of hypergraph\npartitioning to a continuous optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:21:36 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Mishchenko", "Alexander", ""], ["Manuilov", "Vladimir", ""], ["You", "Chao", ""], ["Yang", "Han", ""]]}, {"id": "1909.08369", "submitter": "Taisuke Izumi", "authors": "Shimon Bitton, Yuval Emek, Taisuke Izumi, Shay Kutten", "title": "Message Reduction in the Local Model is a Free Lunch", "comments": "Appeared at International Symposium on Distributed Computing (DISC)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new \\emph{spanner} construction algorithm is presented, working under the\n\\emph{LOCAL} model with unique edge IDs. Given an $n$-node communication graph,\na spanner with a constant stretch and $O (n^{1 + \\varepsilon})$ edges (for an\narbitrarily small constant $\\varepsilon > 0$) is constructed in a constant\nnumber of rounds sending $O (n^{1 + \\varepsilon})$ messages whp. Consequently,\nwe conclude that every $t$-round LOCAL algorithm can be transformed into an $O\n(t)$-round LOCAL algorithm that sends $O (t \\cdot n^{1 + \\varepsilon})$\nmessages whp. This improves upon all previous message-reduction schemes for\nLOCAL algorithms that incur a $\\log^{\\Omega (1)} n$ blow-up of the round\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 11:33:59 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Bitton", "Shimon", ""], ["Emek", "Yuval", ""], ["Izumi", "Taisuke", ""], ["Kutten", "Shay", ""]]}, {"id": "1909.08426", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Nicolas Bousquet, St\\'ephan Thomass\\'e, R\\'emi\n  Watrigant", "title": "When Maximum Stable Set can be solved in FPT time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum Independent Set (MIS for short) is in general graphs the paradigmatic\n$W[1]$-hard problem. In stark contrast, polynomial-time algorithms are known\nwhen the inputs are restricted to structured graph classes such as, for\ninstance, perfect graphs (which includes bipartite graphs, chordal graphs,\nco-graphs, etc.) or claw-free graphs. In this paper, we introduce some variants\nof co-graphs with parameterized noise, that is, graphs that can be made into\ndisjoint unions or complete sums by the removal of a certain number of vertices\nand the addition/deletion of a certain number of edges per incident vertex,\nboth controlled by the parameter. We give a series of FPT Turing-reductions on\nthese classes and use them to make some progress on the parameterized\ncomplexity of MIS in $H$-free graphs. We show that for every fixed $t \\geqslant\n1$, MIS is FPT in $P(1,t,t,t)$-free graphs, where $P(1,t,t,t)$ is the graph\nobtained by substituting all the vertices of a four-vertex path but one end of\nthe path by cliques of size $t$. We also provide randomized FPT algorithms in\ndart-free graphs and in cricket-free graphs. This settles the FPT/W[1]-hard\ndichotomy for five-vertex graphs $H$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:04:39 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Bousquet", "Nicolas", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1909.08519", "submitter": "Tobias Z\\\"undorf", "authors": "Jonas Sauer, Dorothea Wagner, Tobias Z\\\"undorf", "title": "Efficient Computation of Multi-Modal Public Transit Traffic Assignments\n  using ULTRA", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing public transit traffic assignments in a\nmulti-modal setting: Given a public transit timetable, an additional\nunrestricted transfer mode (in our case walking), and a set of\norigin-destination pairs, we aim to compute the utilization of every vehicle in\nthe network. While it has been shown that considering unrestricted transfers\ncan significantly improve journeys, computing such journeys efficiently remains\nalgorithmically challenging. Since traffic assignments require the computation\nof millions of shortest paths, using a multi-modal network has previously not\nbeen feasible. A recently proposed approach (ULTRA) enables efficient\nalgorithms with UnLimited TRAnsfers at the cost of a short preprocessing phase.\nIn this work we combine the ULTRA approach with a state-of-the-art assignment\nalgorithm, making multi-modal assignments practical. Careful algorithm\nengineering results in a new public transit traffic assignment algorithm that\neven outperforms the algorithm it builds upon, while enabling unlimited walking\nwhich has not been considered previously. We conclude our work with an\nextensive evaluation of the algorithm, showing its versatility and efficiency.\nOn our real world instance, the algorithm computes over 15 million unique\njourneys in less than 17 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 15:57:31 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Sauer", "Jonas", ""], ["Wagner", "Dorothea", ""], ["Z\u00fcndorf", "Tobias", ""]]}, {"id": "1909.08608", "submitter": "Renos Karamanis", "authors": "Renos Karamanis, Eleftherios Anastasiadis, Panagiotis Angeloudis and\n  Marc Stettler", "title": "Assignment and Pricing of Shared Rides in Ride-Sourcing using\n  Combinatorial Double Auctions", "comments": "12 pages, Ride-Sharing, Combinatorial Double Auctions, Assignment,\n  Dynamic Pricing", "journal-ref": null, "doi": "10.1109/TITS.2020.2988356", "report-no": null, "categories": "cs.CC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation Network Companies employ dynamic pricing methods at periods of\npeak travel to incentivise driver participation and balance supply and demand\nfor rides. Surge pricing multipliers are commonly used and are applied\nfollowing demand and estimates of customer and driver trip valuations.\nCombinatorial double auctions have been identified as a suitable alternative,\nas they can achieve maximum social welfare in the allocation by relying on\ncustomers and drivers stating their valuations. A shortcoming of current\nmodels, however, is that they fail to account for the effects of trip detours\nthat take place in shared trips and their impact on the accuracy of pricing\nestimates. To resolve this, we formulate a new shared-ride assignment and\npricing algorithm using combinatorial double auctions. We demonstrate that this\nmodel is reduced to a maximum weighted independent set model, which is known to\nbe APX-hard. A fast local search heuristic is also presented, which is capable\nof producing results that lie within 10% of the exact approach for practical\nimplementations. Our proposed algorithm could be used as a fast and reliable\nassignment and pricing mechanism of ride-sharing requests to vehicles during\npeak travel times.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:56:29 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 20:39:32 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 09:46:49 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Karamanis", "Renos", ""], ["Anastasiadis", "Eleftherios", ""], ["Angeloudis", "Panagiotis", ""], ["Stettler", "Marc", ""]]}, {"id": "1909.08660", "submitter": "Shai Vardi", "authors": "Thomas Kesselheim, Alexandros Psomas, Shai Vardi", "title": "How to Hire Secretaries with Stochastic Departures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the secretary problem, where decisions do not\nhave to be made immediately upon candidates' arrivals. After arriving, each\ncandidate stays in the system for some (random) amount of time and then leaves,\nwhereupon the algorithm has to decide irrevocably whether to select this\ncandidate or not. The goal is to maximize the probability of selecting the best\ncandidate overall. We assume that the arrival and waiting times are drawn from\nknown distributions.\n  Our first main result is a characterization of the optimal policy for this\nsetting. We show that when deciding whether to select a candidate it suffices\nto know only the time and the number of candidates that have arrived so far.\nFurthermore, the policy is monotone non-decreasing in the number of candidates\nseen so far, and, under certain natural conditions, monotone non-increasing in\nthe time. Our second main result is proving that when the number of candidates\nis large, a single threshold policy is almost optimal.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 18:57:24 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Kesselheim", "Thomas", ""], ["Psomas", "Alexandros", ""], ["Vardi", "Shai", ""]]}, {"id": "1909.08786", "submitter": "Artem Lutov", "authors": "Artem Lutov, Mourad Khayati and Philippe Cudr\\'e-Mauroux", "title": "DAOC: Stable Clustering of Large Networks", "comments": "IEEE BigData'19, Special Session on Intelligent Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering is a crucial component of many data mining systems involving the\nanalysis and exploration of various data. Data diversity calls for clustering\nalgorithms to be accurate while providing stable (i.e., deterministic and\nrobust) results on arbitrary input networks. Moreover, modern systems often\noperate with large datasets, which implicitly constrains the complexity of the\nclustering algorithm. Existing clustering techniques are only partially stable,\nhowever, as they guarantee either determinism or robustness. To address this\nissue, we introduce DAOC, a Deterministic and Agglomerative Overlapping\nClustering algorithm. DAOC leverages a new technique called Overlap\nDecomposition to identify fine-grained clusters in a deterministic way\ncapturing multiple optima. In addition, it leverages a novel consensus\napproach, Mutual Maximal Gain, to ensure robustness and further improve the\nstability of the results while still being capable of identifying micro-scale\nclusters. Our empirical results on both synthetic and real-world networks show\nthat DAOC yields stable clusters while being on average 25% more accurate than\nstate-of-the-art deterministic algorithms without requiring any tuning. Our\napproach has the ambition to greatly simplify and speed up data analysis tasks\ninvolving iterative processing (need for determinism) as well as data\nfluctuations (need for robustness) and to provide accurate and reproducible\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 03:12:05 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 20:47:56 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Lutov", "Artem", ""], ["Khayati", "Mourad", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1909.08795", "submitter": "Dibyayan Chakraborty", "authors": "Dibyayan Chakraborty, Florent Foucaud, Harmender Gahlawat, Subir Kumar\n  Ghosh, Bodhayan Roy", "title": "Hardness and approximation for the geodetic set problem in some graph\n  classes", "comments": null, "journal-ref": "Proceedings CALDAM 2020. Lecture Notes in Computer Science\n  12016:102-115, 2020", "doi": "10.1007/978-3-030-39219-2_9", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the computational complexity of finding the\n\\emph{geodetic number} of graphs. A set of vertices $S$ of a graph $G$ is a\n\\emph{geodetic set} if any vertex of $G$ lies in some shortest path between\nsome pair of vertices from $S$. The \\textsc{Minimum Geodetic Set (MGS)} problem\nis to find a geodetic set with minimum cardinality. In this paper, we prove\nthat solving the \\textsc{MGS} problem is NP-hard on planar graphs with a\nmaximum degree six and line graphs. We also show that unless $P=NP$, there is\nno polynomial time algorithm to solve the \\textsc{MGS} problem with\nsublogarithmic approximation factor (in terms of the number of vertices) even\non graphs with diameter $2$. On the positive side, we give an\n$O\\left(\\sqrt[3]{n}\\log n\\right)$-approximation algorithm for the \\textsc{MGS}\nproblem on general graphs of order $n$. We also give a $3$-approximation\nalgorithm for the \\textsc{MGS} problem on the family of solid grid graphs which\nis a subclass of planar graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 04:08:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chakraborty", "Dibyayan", ""], ["Foucaud", "Florent", ""], ["Gahlawat", "Harmender", ""], ["Ghosh", "Subir Kumar", ""], ["Roy", "Bodhayan", ""]]}, {"id": "1909.08801", "submitter": "Samuel Fischer", "authors": "Samuel M. Fischer", "title": "Locally optimal routes for route choice sets", "comments": "Keywords: alternative paths; choice set; local optimality; road\n  network; route choice", "journal-ref": "Transportation Research Part B: Methodological 141 (2020): 240-266", "doi": "10.1016/j.trb.2020.09.007", "report-no": null, "categories": "eess.SY cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Route choice is often modelled as a two-step procedure in which travellers\nchoose their routes from small sets of promising candidates. Many methods\ndeveloped to identify such choice sets rely on assumptions about the mechanisms\nbehind the route choice and require corresponding data sets. Furthermore,\nexisting approaches often involve considerable complexity or perform many\nrepeated shortest path queries. This makes it difficult to apply these methods\nin comprehensive models with numerous origin-destination pairs. In this paper,\nwe address these issues by developing an algorithm that efficiently identifies\nlocally optimal routes. Such paths arise from travellers acting rationally on\nlocal scales, whereas unknown factors may affect the routes on larger scales.\nThough methods identifying locally optimal routes are available already, these\nalgorithms rely on approximations and return only few, heuristically chosen\npaths for specific origin-destination pairs. This conflicts with the demands of\nroute choice models, where an exhaustive search for many origins and\ndestinations would be necessary. We therefore extend existing algorithms to\nreturn (almost) all admissible paths between a large number of\norigin-destination pairs. We test our algorithm on a road network modelling the\nCanadian province British Columbia and analyze the distribution of locally\noptimal paths in the province.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 04:35:18 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 09:18:20 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 09:48:12 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fischer", "Samuel M.", ""]]}, {"id": "1909.08846", "submitter": "Sevag Gharibian", "authors": "Sevag Gharibian, Ojas Parekh", "title": "Almost optimal classical approximation algorithms for a quantum\n  generalization of Max-Cut", "comments": "17 pages, published version", "journal-ref": "Proceedings of the 22nd International Workshop on Approximation\n  Algorithms for Combinatorial Optimization Problems (APPROX), volume 145 of\n  Leibniz International Proceedings in Informatics (LIPIcs), pages 31:1-31:17,\n  2019", "doi": "10.4230/LIPIcs.APPROX-RANDOM.2019.31", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation algorithms for constraint satisfaction problems (CSPs) are a\ncentral direction of study in theoretical computer science. In this work, we\nstudy classical product state approximation algorithms for a physically\nmotivated quantum generalization of Max-Cut, known as the quantum Heisenberg\nmodel. This model is notoriously difficult to solve exactly, even on bipartite\ngraphs, in stark contrast to the classical setting of Max-Cut. Here we show,\nfor any interaction graph, how to classically and efficiently obtain\napproximation ratios 0.649 (anti-ferromagnetic XY model) and 0.498\n(anti-ferromagnetic Heisenberg XYZ model). These are almost optimal; we show\nthat the best possible ratios achievable by a product state for these models is\n2/3 and 1/2, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:15:48 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gharibian", "Sevag", ""], ["Parekh", "Ojas", ""]]}, {"id": "1909.09298", "submitter": "Will Perkins", "authors": "Christian Borgs, Jennifer Chayes, Tyler Helmuth, Will Perkins, Prasad\n  Tetali", "title": "Efficient sampling and counting algorithms for the Potts model on\n  $\\mathbb Z^d$ at all temperatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $d \\ge 2$ and all $q\\geq q_{0}(d)$ we give an efficient algorithm to\napproximately sample from the $q$-state ferromagnetic Potts and random cluster\nmodels on the torus $(\\mathbb Z / n \\mathbb Z )^d$ for any inverse temperature\n$\\beta\\geq 0$. This stands in contrast to Markov chain mixing time results: the\nGlauber dynamics mix slowly at and below the critical temperature, and the\nSwendsen--Wang dynamics mix slowly at the critical temperature. We also provide\nan efficient algorithm (an FPRAS) for approximating the partition functions of\nthese models.\n  Our algorithms are based on representing the random cluster model as a\ncontour model using Pirogov-Sinai theory, and then computing an accurate\napproximation of the logarithm of the partition function by inductively\ntruncating the resulting cluster expansion. The main innovation of our approach\nis an algorithmic treatment of unstable ground states; this is essential for\nour algorithms to apply to all inverse temperatures $\\beta$. By treating\nunstable ground states our work gives a general template for converting\nprobabilistic applications of Pirogov--Sinai theory to efficient algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 02:42:47 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 21:58:43 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Helmuth", "Tyler", ""], ["Perkins", "Will", ""], ["Tetali", "Prasad", ""]]}, {"id": "1909.09371", "submitter": "Yota Otachi", "authors": "Yusuke Kobayashi, Yoshio Okamoto, Yota Otachi, Yushi Uno", "title": "Linear-Time Recognition of Double-Threshold Graphs", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G = (V,E)$ is a double-threshold graph if there exist a\nvertex-weight function $w \\colon V \\to \\mathbb{R}$ and two real numbers\n$\\mathtt{lb}, \\mathtt{ub} \\in \\mathbb{R}$ such that $uv \\in E$ if and only if\n$\\mathtt{lb} \\le \\mathtt{w}(u) + \\mathtt{w}(v) \\le \\mathtt{ub}$. In the\nliterature, those graphs are studied as the pairwise compatibility graphs that\nhave stars as their underlying trees. We give a new characterization of\ndouble-threshold graphs, which gives connections to bipartite permutation\ngraphs. Using the new characterization, we present a linear-time algorithm for\nrecognizing double-threshold graphs. Prior to our work, the fastest known\nalgorithm by Xiao and Nagamochi [COCOON 2018] ran in $O(n^6)$ time, where $n$\nis the number of vertices.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 08:37:37 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Kobayashi", "Yusuke", ""], ["Okamoto", "Yoshio", ""], ["Otachi", "Yota", ""], ["Uno", "Yushi", ""]]}, {"id": "1909.09630", "submitter": "Jonathan Ullman", "authors": "Albert Cheu and Adam Smith and Jonathan Ullman", "title": "Manipulation Attacks in Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy is a widely studied restriction on distributed\nalgorithms that collect aggregates about sensitive user data, and is now\ndeployed in several large systems. We initiate a systematic study of a\nfundamental limitation of locally differentially private protocols: they are\nhighly vulnerable to adversarial manipulation. While any algorithm can be\nmanipulated by adversaries who lie about their inputs, we show that any\nnon-interactive locally differentially private protocol can be manipulated to a\nmuch greater extent. Namely, when the privacy level is high or the input domain\nis large, an attacker who controls a small fraction of the users in the\nprotocol can completely obscure the distribution of the users' inputs. We also\nshow that existing protocols differ greatly in their resistance to\nmanipulation, even when they offer the same accuracy guarantee with honest\nexecution. Our results suggest caution when deploying local differential\nprivacy and reinforce the importance of efficient cryptographic techniques for\nemulating mechanisms from central differential privacy in distributed settings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 17:42:22 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Cheu", "Albert", ""], ["Smith", "Adam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1909.09791", "submitter": "Saachi Jain", "authors": "Saachi Jain, Matei Zaharia", "title": "Spectral Lower Bounds on the I/O Complexity of Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding lower bounds on the I/O complexity of\narbitrary computations in a two level memory hierarchy. Executions of complex\ncomputations can be formalized as an evaluation order over the underlying\ncomputation graph. However, prior methods for finding I/O lower bounds leverage\nthe graph structures for specific problems (e.g matrix multiplication) which\ncannot be applied to arbitrary graphs. In this paper, we first present a novel\nmethod to bound the I/O of any computation graph using the first few\neigenvalues of the graph's Laplacian. We further extend this bound to the\nparallel setting. This spectral bound is not only efficiently computable by\npower iteration, but can also be computed in closed form for graphs with known\nspectra. We apply our spectral method to compute closed-form analytical bounds\non two computation graphs (the Bellman-Held-Karp algorithm for the traveling\nsalesman problem and the Fast Fourier Transform), as well as provide a\nprobabilistic bound for random Erdos Renyi graphs. We empirically validate our\nbound on four computation graphs, and find that our method provides tighter\nbounds than current empirical methods and behaves similarly to previously\npublished I/O bounds.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 08:04:03 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 22:39:53 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Jain", "Saachi", ""], ["Zaharia", "Matei", ""]]}, {"id": "1909.09912", "submitter": "Charalampos Tsourakakis", "authors": "Kasper Green Larsen, Michael Mitzenmacher, Charalampos E. Tsourakakis", "title": "Optimal Learning of Joint Alignments with a Faulty Oracle", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem, which is useful in applications such as\njoint image and shape alignment. The goal is to recover $n$ discrete variables\n$g_i \\in \\{0, \\ldots, k-1\\}$ (up to some global offset) given noisy\nobservations of a set of their pairwise differences $\\{(g_i - g_j) \\bmod k\\}$;\nspecifically, with probability $\\frac{1}{k}+\\delta$ for some $\\delta > 0$ one\nobtains the correct answer, and with the remaining probability one obtains a\nuniformly random incorrect answer. We consider a learning-based formulation\nwhere one can perform a query to observe a pairwise difference, and the goal is\nto perform as few queries as possible while obtaining the exact joint\nalignment. We provide an easy-to-implement, time efficient algorithm that\nperforms $O\\big(\\frac{n \\lg n}{k \\delta^2}\\big)$ queries, and recovers the\njoint alignment with high probability. We also show that our algorithm is\noptimal by proving a general lower bound that holds for all non-adaptive\nalgorithms. Our work improves significantly recent work by Chen and Cand\\'{e}s\n\\cite{chen2016projected}, who view the problem as a constrained principal\ncomponents analysis problem that can be solved using the power method.\nSpecifically, our approach is simpler both in the algorithm and the analysis,\nand provides additional insights into the problem structure.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 23:41:19 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Mitzenmacher", "Michael", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1909.10016", "submitter": "Yasushi Kawase", "authors": "Xin Han, Yasushi Kawase, Kazuhisa Makino, Haruki Yokomaku", "title": "Online Knapsack Problems with a Resource Buffer", "comments": "Accepted by ISAAC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce online knapsack problems with a resource buffer.\nIn the problems, we are given a knapsack with capacity $1$, a buffer with\ncapacity $R\\ge 1$, and items that arrive one by one. Each arriving item has to\nbe taken into the buffer or discarded on its arrival irrevocably. When every\nitem has arrived, we transfer a subset of items in the current buffer into the\nknapsack. Our goal is to maximize the total value of the items in the knapsack.\nWe consider four variants depending on whether items in the buffer are\nremovable (i.e., we can remove items in the buffer) or non-removable, and\nproportional (i.e., the value of each item is proportional to its size) or\ngeneral. For the general&non-removable case, we observe that no constant\ncompetitive algorithm exists for any $R\\ge 1$. For the\nproportional&non-removable case, we show that a simple greedy algorithm is\noptimal for every $R\\ge 1$. For the general&removable and the\nproportional&removable cases, we present optimal algorithms for small $R$ and\ngive asymptotically nearly optimal algorithms for general $R$.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 14:37:33 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Han", "Xin", ""], ["Kawase", "Yasushi", ""], ["Makino", "Kazuhisa", ""], ["Yokomaku", "Haruki", ""]]}, {"id": "1909.10261", "submitter": "Danny Hucke", "authors": "Moses Ganardi, Danny Hucke, Markus Lohrey, Tatiana Starikovskaya", "title": "Sliding window property testing for regular languages", "comments": "A short version of this paper was accepted for presentation at ISAAC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognizing regular languages in a variant of the\nstreaming model of computation, called the sliding window model. In this model,\nwe are given a size of the sliding window $n$ and a stream of symbols. At each\ntime instant, we must decide whether the suffix of length $n$ of the current\nstream (\"the active window\") belongs to a given regular language.\n  Recent works showed that the space complexity of an optimal deterministic\nsliding window algorithm for this problem is either constant, logarithmic or\nlinear in the window size $n$ and provided natural language theoretic\ncharacterizations of the space complexity classes. Subsequently, those results\nwere extended to randomized algorithms to show that any such algorithm admits\neither constant, double logarithmic, logarithmic or linear space complexity.\n  In this work, we make an important step forward and combine the sliding\nwindow model with the property testing setting, which results in\nultra-efficient algorithms for all regular languages. Informally, a sliding\nwindow property tester must accept the active window if it belongs to the\nlanguage and reject it if it is far from the language. We consider\ndeterministic and randomized sliding window property testers with one-sided and\ntwo-sided errors. In particular, we show that for any regular language, there\nis a deterministic sliding window property tester that uses logarithmic space\nand a randomized sliding window property tester with two-sided error that uses\nconstant space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 10:12:13 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ganardi", "Moses", ""], ["Hucke", "Danny", ""], ["Lohrey", "Markus", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1909.10323", "submitter": "Siddharth Bhandari", "authors": "Siddharth Bhandari, Sayantan Chakraborty", "title": "Improved Bounds for Perfect Sampling of $k$-Colorings in Graphs", "comments": "Rewrite of previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized algorithm that takes as input an undirected\n$n$-vertex graph $G$ with maximum degree $\\Delta$ and an integer $k > 3\\Delta$,\nand returns a random proper $k$-coloring of $G$. The distribution of the\ncoloring is \\emph{perfectly} uniform over the set of all proper $k$-colorings;\nthe expected running time of the algorithm is\n$\\mathrm{poly}(k,n)=\\widetilde{O}(n\\Delta^2\\cdot \\log(k))$. This improves upon\na result of Huber~(STOC 1998) who obtained a polynomial time perfect sampling\nalgorithm for $k>\\Delta^2+2\\Delta$. Prior to our work, no algorithm with\nexpected running time $\\mathrm{poly}(k,n)$ was known to guarantee perfectly\nsampling with sub-quadratic number of colors in general. Our algorithm (like\nseveral other perfect sampling algorithms including Huber's) is based on the\nCoupling from the Past method. Inspired by the \\emph{bounding chain} approach,\npioneered independently by Huber~(STOC 1998) and H\\\"aggstr\\\"om \\&\nNelander~(Scand.{} J.{} Statist., 1999), we employ a novel bounding chain to\nderive our result for the graph coloring problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 12:26:44 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 18:48:58 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 10:55:15 GMT"}, {"version": "v4", "created": "Wed, 6 Nov 2019 09:38:26 GMT"}, {"version": "v5", "created": "Thu, 21 May 2020 12:13:26 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Bhandari", "Siddharth", ""], ["Chakraborty", "Sayantan", ""]]}, {"id": "1909.10354", "submitter": "Bruno Escoffier", "authors": "Evripidis Bampis, Bruno Escoffier, Alexander Kononov", "title": "LP-based algorithms for multistage minimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multistage framework introduced recently where, given a time\nhorizon t=1,2,...,T, the input is a sequence of instances of a (static)\ncombinatorial optimization problem I_1,I_2,...,I_T, (one for each time step),\nand the goal is to find a sequence of solutions S_1,S_2,...,S_T (one for each\ntime step) reaching a tradeoff between the quality of the solutions in each\ntime step and the stability/similarity of the solutions in consecutive time\nsteps. For several polynomial-time solvable problems, such as Minimum Cost\nPerfect Matching, the multistage variant becomes hard to approximate (even for\ntwo time steps for Minimum Cost Perfect Matching). In this paper, we study the\nmultistage variants of some important discrete minimization problems (including\nMinimum Cut, Vertex Cover, Set Cover, Prize-Collecting Steiner Tree,\nPrize-Collecting Traveling Salesman). We focus on the natural question of\nwhether linear-programming-based methods may help in developing good\napproximation algorithms in this framework. We first show that Min Cut remains\npolytime solvable in its multistage variant, and Vertex Cover remains\n2-approximable, as particular case of a more general statement which easily\nfollows from the work of (Hochbaum, EJOR 2002) on monotone and IP2 problems.\nThen, we tackle other problems and for this we introduce a new two-threshold\nrounding scheme, tailored for multistage problems. As a first application, we\nshow that this rounding scheme gives a 2$f$-approximation algorithm for the\nmultistage variant of the f-Set Cover problem, where each element belongs to at\nmost f sets. More interestingly, we are able to use our rounding scheme in\norder to propose a 3.53-approximation algorithm for the multistage variant of\nthe Prize-Collecting Steiner Tree problem, and a 3.034-approximation algorithm\nfor the multistage variant of the Prize-Collecting Traveling Salesman problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:15:27 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Bampis", "Evripidis", ""], ["Escoffier", "Bruno", ""], ["Kononov", "Alexander", ""]]}, {"id": "1909.10647", "submitter": "Artur Czumaj", "authors": "Artur Czumaj and Christian Sohler", "title": "A characterization of graph properties testable for general planar\n  graphs with one-sided error (It is all about forbidden subgraphs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of characterizing testable graph properties (properties that can\nbe tested with a number of queries independent of the input size) is a\nfundamental problem in the area of property testing. While there has been some\nextensive prior research characterizing testable graph properties in the dense\ngraphs model and we have good understanding of the bounded degree graphs model,\nno similar characterization has been known for general graphs, with no degree\nbounds. In this paper we take on this major challenge and consider the problem\nof characterizing all testable graph properties in general planar graphs.\n  We consider the model in which a general planar graph can be accessed by the\nrandom neighbor oracle that allows access to any given vertex and access to a\nrandom neighbor of a given vertex. We show that, informally, a graph property\n$P$ is testable with one-sided error for general planar graphs if and only if\ntesting $P$ can be reduced to testing for a finite family of finite forbidden\nsubgraphs. While our presentation focuses on planar graphs, our approach\nextends easily to general minor-free graphs.\n  Our analysis of the necessary condition relies on a recent construction of\ncanonical testers in the random neighbor oracle model that is applied here to\nthe one-sided error model for testing in planar graphs. The sufficient\ncondition in the characterization reduces the problem to the task of testing\n$H$-freeness in planar graphs, and is the main and most challenging technical\ncontribution of the paper: we show that for planar graphs (with arbitrary\ndegrees), the property of being $H$-free is testable with one-sided error for\nevery finite graph $H$, in the random neighbor oracle model.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 23:03:07 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Czumaj", "Artur", ""], ["Sohler", "Christian", ""]]}, {"id": "1909.10668", "submitter": "Taisuke Yasuda", "authors": "Manuel Fernandez, David P. Woodruff, Taisuke Yasuda", "title": "The Query Complexity of Mastermind with $\\ell_p$ Distances", "comments": "Full version of APPROX 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a variant of the Mastermind game in which queries are $\\ell_p$\ndistances, rather than the usual Hamming distance. That is, a codemaker chooses\na hidden vector $\\mathbf{y}\\in\\{-k,-k+1,\\dots,k-1,k\\}^n$ and answers to queries\nof the form $\\Vert\\mathbf{y}-\\mathbf{x}\\Vert_p$ where\n$\\mathbf{x}\\in\\{-k,-k+1,\\dots,k-1,k\\}^n$. The goal is to minimize the number of\nqueries made in order to correctly guess $\\mathbf{y}$.\n  Motivated by this question, in this work, we develop a nonadaptive polynomial\ntime algorithm that works for a natural class of separable distance measures,\ni.e.\\ coordinate-wise sums of functions of the absolute value. This in\nparticular includes distances such as the smooth max (LogSumExp) as well as\nmany widely-studied $M$-estimator losses, such as $\\ell_p$ norms, the\n$\\ell_1$-$\\ell_2$ loss, the Huber loss, and the Fair estimator loss. When we\napply this result to $\\ell_p$ queries, we obtain an upper bound of\n$O\\left(\\min\\left\\{n,\\frac{n\\log k}{\\log n}\\right\\}\\right)$ queries for any\nreal $1\\leq p<\\infty$. We also show matching lower bounds up to constant\nfactors for the $\\ell_p$ problem, even for adaptive algorithms for the\napproximation version of the problem, in which the problem is to output\n$\\mathbf{y}'$ such that $\\Vert\\mathbf{y}'-\\mathbf{y}\\Vert_p\\leq R$ for any\n$R\\leq k^{1-\\varepsilon}n^{1/p}$ for constant $\\varepsilon>0$. Thus,\nessentially any approximation of this problem is as hard as finding the hidden\nvector exactly, up to constant factors. Finally, we show that for the noisy\nversion of the problem, i.e. the setting when the codemaker answers queries\nwith any $q = (1\\pm\\varepsilon)\\Vert\\mathbf{y}-\\mathbf{x}\\Vert_p$, there is no\nquery efficient algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 01:01:44 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Fernandez", "Manuel", ""], ["Woodruff", "David P.", ""], ["Yasuda", "Taisuke", ""]]}, {"id": "1909.10683", "submitter": "Amirbehshad Shahrasbi", "authors": "Venkatesan Guruswami, Bernhard Haeupler, and Amirbehshad Shahrasbi", "title": "Optimally Resilient Codes for List-Decoding from Insertions and\n  Deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete answer to the following basic question: \"What is the\nmaximal fraction of deletions or insertions tolerable by $q$-ary list-decodable\ncodes with non-vanishing information rate?\"\n  This question has been open even for binary codes, including the restriction\nto the binary insertion-only setting, where the best-known result was that a\n$\\gamma\\leq 0.707$ fraction of insertions is tolerable by some binary code\nfamily.\n  For any desired $\\epsilon > 0$, we construct a family of binary codes of\npositive rate which can be efficiently list-decoded from any combination of\n$\\gamma$ fraction of insertions and $\\delta$ fraction of deletions as long as $\n\\gamma+2\\delta\\leq 1-\\epsilon$. On the other hand, for any $\\gamma,\\delta$ with\n$\\gamma+2\\delta=1$ list-decoding is impossible. Our result thus precisely\ncharacterizes the feasibility region of binary list-decodable codes for\ninsertions and deletions.\n  We further generalize our result to codes over any finite alphabet of size\n$q$. Surprisingly, our work reveals that the feasibility region for $q>2$ is\nnot the natural generalization of the binary bound above. We provide tight\nupper and lower bounds that precisely pin down the feasibility region, which\nturns out to have a $(q-1)$-piece-wise linear boundary whose $q$ corner-points\nlie on a quadratic curve.\n  The main technical work in our results is proving the existence of code\nfamilies of sufficiently large size with good list-decoding properties for any\ncombination of $\\delta,\\gamma$ within the claimed feasibility region. We\nachieve this via an intricate analysis of codes introduced by [Bukh, Ma; SIAM\nJ. Discrete Math; 2014]. Finally, we give a simple yet powerful concatenation\nscheme for list-decodable insertion-deletion codes which transforms any such\n(non-efficient) code family (with vanishing information rate) into an\nefficiently decodable code family with constant rate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 02:35:23 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 07:26:49 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 06:24:21 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Haeupler", "Bernhard", ""], ["Shahrasbi", "Amirbehshad", ""]]}, {"id": "1909.10719", "submitter": "Muhammad Irfan Yousuf Dr.", "authors": "Raheel Anwar, Muhammad Irfan Yousuf, Muhammad Abid", "title": "Analysis of a Model for Generating Weakly Scale-free Networks", "comments": "16 Pages, 4 Figures, Pre-print", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 22 no. 1\n  (February 24, 2020) dmtcs:6089", "doi": "10.23638/DMTCS-22-1-7", "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is commonly believed that real networks are scale-free and fraction of\nnodes $P(k)$ with degree $k$ satisfies the power law $P(k) \\propto k^{-\\gamma}\n\\text{ for } k > k_{min} > 0$. Preferential attachment is the mechanism that\nhas been considered responsible for such organization of these networks. In\nmany real networks, degree distribution before the $k_{min}$ varies very slowly\nto the extent of being uniform as compared with the degree distribution for $k\n> k_{min}$ . In this paper, we proposed a model that describe this particular\ndegree distribution for the whole range of $k>0$. We adopt a two step approach.\nIn the first step, at every time stamp we add a new node to the network and\nattach it with an existing node using preferential attachment method. In the\nsecond step, we add edges between existing pairs of nodes with the node\nselection based on the uniform probability distribution. Our approach generates\nweakly scale-free networks that closely follow the degree distribution of\nreal-world networks. We perform comprehensive mathematical analysis of the\nmodel in the discrete domain and compare the degree distribution generated by\nthese models with that of real-world networks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 05:49:30 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 08:45:06 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 07:18:32 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 19:12:57 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Anwar", "Raheel", ""], ["Yousuf", "Muhammad Irfan", ""], ["Abid", "Muhammad", ""]]}, {"id": "1909.10766", "submitter": "Rasmus Pagh", "authors": "Rasmus Pagh, Johan Sivertsen", "title": "The space complexity of inner product filters", "comments": "To appear at ICDT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of filtering candidate pairs in inner product\nsimilarity joins we study the following inner product estimation problem: Given\nparameters $d\\in {\\bf N}$, $\\alpha>\\beta\\geq 0$ and unit vectors $x,y\\in {\\bf\nR}^{d}$ consider the task of distinguishing between the cases $\\langle x,\ny\\rangle\\leq\\beta$ and $\\langle x, y\\rangle\\geq \\alpha$ where $\\langle x,\ny\\rangle = \\sum_{i=1}^d x_i y_i$ is the inner product of vectors $x$ and $y$.\nThe goal is to distinguish these cases based on information on each vector\nencoded independently in a bit string of the shortest length possible. In\ncontrast to much work on compressing vectors using randomized dimensionality\nreduction, we seek to solve the problem deterministically, with no probability\nof error. Inner product estimation can be solved in general via estimating\n$\\langle x, y\\rangle$ with an additive error bounded by $\\varepsilon = \\alpha -\n\\beta$. We show that $d \\log_2 \\left(\\tfrac{\\sqrt{1-\\beta}}{\\varepsilon}\\right)\n\\pm \\Theta(d)$ bits of information about each vector is necessary and\nsufficient. Our upper bound is constructive and improves a known upper bound of\n$d \\log_2(1/\\varepsilon) + O(d)$ by up to a factor of 2 when $\\beta$ is close\nto $1$. The lower bound holds even in a stronger model where one of the vectors\nis known exactly, and an arbitrary estimation function is allowed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:02:28 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 11:45:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Pagh", "Rasmus", ""], ["Sivertsen", "Johan", ""]]}, {"id": "1909.10850", "submitter": "Jan van den Brand", "authors": "Jan van den Brand, Danupon Nanongkai", "title": "Dynamic Approximate Shortest Paths and Beyond: Subquadratic and\n  Worst-Case Update Time", "comments": "To appear in FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following distance query for an $n$-node graph $G$ undergoing\nedge insertions and deletions: given two sets of nodes $I$ and $J$, return the\ndistances between every pair of nodes in $I\\times J$. This query is rather\ngeneral and captures several versions of the dynamic shortest paths problem. In\nthis paper, we develop an efficient $(1+\\epsilon)$-approximation algorithm for\nthis query using fast matrix multiplication. Our algorithm leads to answers for\nsome open problems for Single-Source and All-Pairs Shortest Paths (SSSP and\nAPSP), as well as for Diameter, Radius, and Eccentricities. Below are some\nhighlights. Note that all our algorithms guarantee worst-case update time and\nare randomized (Monte Carlo), but do not need the oblivious adversary\nassumption.\n  Subquadratic update time for SSSP, Diameter, Centralities, ect.: When we want\nto maintain distances from a single node explicitly (without queries), a\nfundamental question is to beat trivially calling Dijkstra's static algorithm\nafter each update, taking $\\Theta(n^2)$ update time on dense graphs. It was\nknown to be improbable for exact algorithms and for combinatorial\nany-approximation algorithms to polynomially beat the $\\Omega(n^2)$ bound\n(under some conjectures) [Roditty, Zwick, ESA'04; Abboud, V. Williams,\nFOCS'14]. Our algorithm with $I=\\{s\\}$ and $J=V(G)$ implies a\n$(1+\\epsilon)$-approximation algorithm for this, guaranteeing $\\tilde\nO(n^{1.823}/\\epsilon^2)$ worst-case update time for directed graphs with\npositive real weights in $[1, W]$. With ideas from [Roditty, V. Williams,\nSTOC'13], we also obtain the first subquadratic worst-case update time for\n$(5/3+\\epsilon)$-approximating the eccentricities and\n$(1.5+\\epsilon)$-approximating the diameter and radius for unweighted graphs\n(with small additive errors).\n  [...]\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:51:57 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 18:26:54 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Brand", "Jan van den", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1909.11073", "submitter": "Badih Ghazi", "authors": "Badih Ghazi, Pasin Manurangsi, Rasmus Pagh, Ameya Velingker", "title": "Private Aggregation from Fewer Anonymous Messages", "comments": "31 pages; 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the setup where $n$ parties are each given a number $x_i \\in\n\\mathbb{F}_q$ and the goal is to compute the sum $\\sum_i x_i$ in a secure\nfashion and with as little communication as possible. We study this problem in\nthe anonymized model of Ishai et al. (FOCS 2006) where each party may broadcast\nanonymous messages on an insecure channel.\n  We present a new analysis of the one-round \"split and mix\" protocol of Ishai\net al. In order to achieve the same security parameter, our analysis reduces\nthe required number of messages by a $\\Theta(\\log n)$ multiplicative factor. We\ncomplement our positive result with lower bounds showing that the dependence of\nthe number of messages on the domain size, the number of parties, and the\nsecurity parameter is essentially tight.\n  Using a reduction of Balle et al. (2019), our improved analysis of the\nprotocol of Ishai et al. yields, in the same model, an $\\left(\\varepsilon,\n\\delta\\right)$-differentially private protocol for aggregation that, for any\nconstant $\\varepsilon > 0$ and any $\\delta = \\frac{1}{\\mathrm{poly}(n)}$,\nincurs only a constant error and requires only a constant number of messages\nper party. Previously, such a protocol was known only for $\\Omega(\\log n)$\nmessages per party.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:52:14 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 18:06:28 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ghazi", "Badih", ""], ["Manurangsi", "Pasin", ""], ["Pagh", "Rasmus", ""], ["Velingker", "Ameya", ""]]}, {"id": "1909.11086", "submitter": "Tamas Kis", "authors": "Peter Gyorgyi and Tamas Kis", "title": "A common approximation framework for the early work, the late work, and\n  resource leveling problems with unit time jobs", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2020.03.032", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximability of two related machine scheduling problems. In\nthe late work minimization problem, there are identical parallel machines and\nthe jobs have a common due date. The objective is to minimize the late work,\ndefined as the sum of the portion of the jobs done after the due date. A\nrelated problem is the maximization of the early work, defined as the sum of\nthe portion of the jobs done before the due date. We describe a polynomial time\napproximation scheme for the early work maximization problem, and we extended\nit to the late work minimization problem after shifting the objective function\nby a positive value that depends on the problem data. We also prove an\ninapproximability result for the latter problem if the objective function is\nshifted by a constant which does not depend on the input. These results remain\nvalid even if the number of the jobs assigned to the same machine is bounded.\nThis leads to an extension of our approximation scheme to some variants of the\nresource leveling problem, for which no approximation algorithms were known.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:25:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Gyorgyi", "Peter", ""], ["Kis", "Tamas", ""]]}, {"id": "1909.11123", "submitter": "Vasileios Nakos", "authors": "Vasileios Nakos, Zhao Song, Zhengyu Wang", "title": "(Nearly) Sample-Optimal Sparse Fourier Transform in Any Dimension;\n  RIPless and Filterless", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the extensively studied problem of computing a\n$k$-sparse approximation to the $d$-dimensional Fourier transform of a length\n$n$ signal. Our algorithm uses $O(k \\log k \\log n)$ samples, is dimension-free,\noperates for any universe size, and achieves the strongest $\\ell_\\infty/\\ell_2$\nguarantee, while running in a time comparable to the Fast Fourier Transform. In\ncontrast to previous algorithms which proceed either via the Restricted\nIsometry Property or via filter functions, our approach offers a fresh\nperspective to the sparse Fourier Transform problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 18:49:41 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Nakos", "Vasileios", ""], ["Song", "Zhao", ""], ["Wang", "Zhengyu", ""]]}, {"id": "1909.11147", "submitter": "Uri Zwick", "authors": "Jacob Holm, Valerie King, Mikkel Thorup, Or Zamir, Uri Zwick", "title": "Random $k$-out subgraph leaves only $O(n/k)$ inter-component edges", "comments": "22 pages, 1 figure, to appear at FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each vertex of an arbitrary simple graph on $n$ vertices chooses $k$ random\nincident edges. What is the expected number of edges in the original graph that\nconnect different connected components of the sampled subgraph? We prove that\nthe answer is $O(n/k)$, when $k\\ge c\\log n$, for some large enough $c$. We\nconjecture that the same holds for smaller values of $k$, possibly for any\n$k\\ge 2$. Such a result is best possible for any $k\\ge 2$. As an application,\nwe use this sampling result to obtain a one-way communication protocol with\n\\emph{private} randomness for finding a spanning forest of a graph in which\neach vertex sends only ${O}(\\sqrt{n}\\log n)$ bits to a referee.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:34:38 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Holm", "Jacob", ""], ["King", "Valerie", ""], ["Thorup", "Mikkel", ""], ["Zamir", "Or", ""], ["Zwick", "Uri", ""]]}, {"id": "1909.11336", "submitter": "Jakub Radoszewski", "authors": "Patryk Czajka and Jakub Radoszewski", "title": "Experimental Evaluation of Algorithms for Computing Quasiperiods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasiperiodicity is a generalization of periodicity that was introduced in\nthe early 1990s. Since then, dozens of algorithms for computing various types\nof quasiperiodicity were proposed. Our work is a step towards answering the\nquestion: \"Which algorithm for computing quasiperiods to choose in practice?\".\nThe central notions of quasiperiodicity are covers and seeds. We implement\nalgorithms for computing covers and seeds in the original and in new simplified\nversions and compare their efficiency on various types of data. We also discuss\nother known types of quasiperiodicity, distinguish partial covers as currently\nthe most promising for large real-world data, and check their effectiveness\nusing real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 08:22:35 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Czajka", "Patryk", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1909.11426", "submitter": "Abhinav Srivastav", "authors": "Nguyen Kim Thang and Abhinav Srivastav", "title": "Online Non-Monotone DR-submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study fundamental problems of maximizing DR-submodular\ncontinuous functions that have real-world applications in the domain of machine\nlearning, economics, operations research and communication systems. It captures\na subclass of non-convex optimization that provides both theoretical and\npractical guarantees. Here, we focus on minimizing regret for online arriving\nnon-monotone DR-submodular functions over different types of convex sets:\nhypercube, down-closed and general convex sets.\n  First, we present an online algorithm that achieves a $1/e$-approximation\nratio with the regret of $O(T^{2/3})$ for maximizing DR-submodular functions\nover any down-closed convex set. Note that, the approximation ratio of $1/e$\nmatches the best-known guarantee for the offline version of the problem.\nMoreover, when the convex set is the hypercube, we propose a tight\n1/2-approximation algorithm with regret bound of $O(\\sqrt{T})$. Next, we give\nan online algorithm that achieves an approximation guarantee (depending on the\nsearch space) for the problem of maximizing non-monotone continuous\nDR-submodular functions over a \\emph{general} convex set (not necessarily\ndown-closed). To best of our knowledge, no prior algorithm with approximation\nguarantee was known for non-monotone DR-submodular maximization in the online\nsetting. Finally we run experiments to verify the performance of our algorithms\non problems arising in machine learning domain with the real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 12:06:39 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 18:22:16 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Thang", "Nguyen Kim", ""], ["Srivastav", "Abhinav", ""]]}, {"id": "1909.11433", "submitter": "Juliusz Straszy\\'nski", "authors": "Panagiotis Charalampopoulos, Tomasz Kociumaka, Solon P. Pissis, Jakub\n  Radoszewski, Wojciech Rytter, Juliusz Straszy\\'nski, Tomasz Wale\\'n and\n  Wiktor Zuba", "title": "Weighted Shortest Common Supersequence Problem Revisited", "comments": "Accepted to SPIRE'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted string, also known as a position weight matrix, is a sequence of\nprobability distributions over some alphabet. We revisit the Weighted Shortest\nCommon Supersequence (WSCS) problem, introduced by Amir et al. [SPIRE 2011],\nthat is, the SCS problem on weighted strings. In the WSCS problem, we are given\ntwo weighted strings $W_1$ and $W_2$ and a threshold $\\mathit{Freq}$ on\nprobability, and we are asked to compute the shortest (standard) string $S$\nsuch that both $W_1$ and $W_2$ match subsequences of $S$ (not necessarily the\nsame) with probability at least $\\mathit{Freq}$. Amir et al. showed that this\nproblem is NP-complete if the probabilities, including the threshold\n$\\mathit{Freq}$, are represented by their logarithms (encoded in binary). We\npresent an algorithm that solves the WSCS problem for two weighted strings of\nlength $n$ over a constant-sized alphabet in $\\mathcal{O}(n^2\\sqrt{z} \\log{z})$\ntime. Notably, our upper bound matches known conditional lower bounds stating\nthat the WSCS problem cannot be solved in $\\mathcal{O}(n^{2-\\varepsilon})$ time\nor in $\\mathcal{O}^*(z^{0.5-\\varepsilon})$ time unless there is a breakthrough\nimproving upon long-standing upper bounds for fundamental NP-hard problems\n(CNF-SAT and Subset Sum, respectively). We also discover a fundamental\ndifference between the WSCS problem and the Weighted Longest Common Subsequence\n(WLCS) problem, introduced by Amir et al. [JDA 2010]. We show that the WLCS\nproblem cannot be solved in $\\mathcal{O}(n^{f(z)})$ time, for any function\n$f(z)$, unless $\\mathrm{P}=\\mathrm{NP}$.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 12:19:38 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Kociumaka", "Tomasz", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "1909.11554", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Li-Chun Wang, Zhu Han", "title": "Data-Driven 3D Placement of UAV Base Stations for Arbitrarily\n  Distributed Crowds", "comments": "6 pages, 3 figures, accepted by 2019 IEEE Global Communications\n  Conference: Wireless Communications (Globecom2019 WC)", "journal-ref": null, "doi": "10.1109/GLOBECOM38437.2019.9014210", "report-no": null, "categories": "cs.NI cs.CC cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an Unmanned Aerial Vehicle (UAV)-assisted cellular\nsystem which consists of multiple UAV base stations (BSs) cooperating the\nterrestrial BSs. In such a heterogeneous network, for cellular operators, the\nproblem is how to determine the appropriate number, locations, and altitudes of\nUAV-BSs to improve the system sumrate as well as satisfy the demands of\narbitrarily flash crowds on data rates. We propose a data-driven 3D placement\nof UAV-BSs for providing an effective placement result with a feasible\ncomputational cost. The proposed algorithm searches for the appropriate number,\nlocation, coverage, and altitude of each UAV-BS in the serving area with the\nmaximized system sumrate in polynomial time so as to guarantee the minimum data\nrate requirement of UE. The simulation results show that the proposed approach\ncan improve system sumrate in comparison with the case without UAV-BSs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 15:33:19 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Wang", "Li-Chun", ""], ["Han", "Zhu", ""]]}, {"id": "1909.11564", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Analytical confidence intervals for the number of different objects in\n  data streams", "comments": "accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper develops a new mathematical-statistical approach to analyze a\nclass of Flajolet-Martin algorithms (FMa), and provides analytical confidence\nintervals for the number F0 of distinct elements in a stream, based on Chernoff\nbounds. The class of FMa has reached a significant popularity in bigdata stream\nlearning, and the attention of the literature has mainly been based on\nalgorithmic aspects, basically complexity optimality, while the statistical\nanalysis of these class of algorithms has been often faced heuristically. The\nanalysis provided here shows deep connections with mathematical special\nfunctions and with extreme value theory. The latter connection may help in\nexplaining heuristic considerations, while the first opens many numerical\nissues, faced at the end of the present paper. Finally, the algorithms are\ntested on an anonymized real data stream and MonteCarlo simulations are\nprovided to support our analytical choice in this context.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 15:46:11 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 10:37:18 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 16:50:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1909.11577", "submitter": "Jakub Radoszewski", "authors": "Panagiotis Charalampopoulos, Tomasz Kociumaka, Manal Mohamed, Jakub\n  Radoszewski, Wojciech Rytter and Tomasz Wale\\'n", "title": "Internal Dictionary Matching", "comments": "A short version of this paper was accepted for presentation at ISAAC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce data structures answering queries concerning the occurrences of\npatterns from a given dictionary $\\mathcal{D}$ in fragments of a given string\n$T$ of length $n$. The dictionary is internal in the sense that each pattern in\n$\\mathcal{D}$ is given as a fragment of $T$. This way, $\\mathcal{D}$ takes\nspace proportional to the number of patterns $d=|\\mathcal{D}|$ rather than\ntheir total length, which could be $\\Theta(n\\cdot d)$.\n  In particular, we consider the following types of queries: reporting and\ncounting all occurrences of patterns from $\\mathcal{D}$ in a fragment $T[i..j]$\nand reporting distinct patterns from $\\mathcal{D}$ that occur in $T[i..j]$. We\nshow how to construct, in $\\mathcal{O}((n+d) \\log^{\\mathcal{O}(1)} n)$ time, a\ndata structure that answers each of these queries in time\n$\\mathcal{O}(\\log^{\\mathcal{O}(1)} n+|output|)$.\n  The case of counting patterns is much more involved and needs a combination\nof a locally consistent parsing with orthogonal range searching. Reporting\ndistinct patterns, on the other hand, uses the structure of maximal repetitions\nin strings. Finally, we provide tight---up to subpolynomial factors---upper and\nlower bounds for the case of a dynamic dictionary.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:08:19 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Kociumaka", "Tomasz", ""], ["Mohamed", "Manal", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1909.11600", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Danupon Nanongkai", "title": "A New Deterministic Algorithm for Dynamic Set Cover", "comments": "To appear in FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic dynamic algorithm for maintaining a\n$(1+\\epsilon)f$-approximate minimum cost set cover with\n$O(f\\log(Cn)/\\epsilon^2)$ amortized update time, when the input set system is\nundergoing element insertions and deletions. Here, $n$ denotes the number of\nelements, each element appears in at most $f$ sets, and the cost of each set\nlies in the range $[1/C, 1]$. Our result, together with that of Gupta et al.\n[STOC`17], implies that there is a deterministic algorithm for this problem\nwith $O(f\\log(Cn))$ amortized update time and $O(\\min(\\log n,\nf))$-approximation ratio, which nearly matches the polynomial-time hardness of\napproximation for minimum set cover in the static setting. Our update time is\nonly $O(\\log (Cn))$ away from a trivial lower bound.\n  Prior to our work, the previous best approximation ratio guaranteed by\ndeterministic algorithms was $O(f^2)$, which was due to Bhattacharya et al.\n[ICALP`15]. In contrast, the only result that guaranteed $O(f)$-approximation\nwas obtained very recently by Abboud et al. [STOC`19], who designed a dynamic\nalgorithm with $(1+\\epsilon)f$-approximation ratio and $O(f^2 \\log n/\\epsilon)$\namortized update time. Besides the extra $O(f)$ factor in the update time\ncompared to our and Gupta et al.'s results, the Abboud et al. algorithm is\nrandomized, and works only when the adversary is oblivious and the sets are\nunweighted (each set has the same cost).\n  We achieve our result via the primal-dual approach, by maintaining a\nfractional packing solution as a dual certificate. Unlike previous primal-dual\nalgorithms that try to satisfy some local constraints for individual sets at\nall time, our algorithm basically waits until the dual solution changes\nsignificantly globally, and fixes the solution only where the fix is needed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:35:43 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1909.11744", "submitter": "Ragesh Jaiswal", "authors": "Anup Bhattacharya and Dishant Goyal and Ragesh Jaiswal and Amit Kumar", "title": "Streaming PTAS for Binary $\\ell_0$-Low Rank Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a 3-pass, polylog-space streaming PTAS for the constrained binary\n$k$-means problem and a 4-pass, polylog-space streaming PTAS for the binary\n$\\ell_0$-low rank approximation problem. The connection between the above two\nproblems has recently been studied. We design a streaming PTAS for the former\nand use this connection to obtain streaming PTAS for the latter. This is the\nfirst constant pass, polylog-space streaming algorithm for either of the two\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 20:15:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Goyal", "Dishant", ""], ["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "1909.11778", "submitter": "Zhuolun Xiang", "authors": "Zhuolun Xiang, Bolin Ding, Xi He and Jingren Zhou", "title": "Linear and Range Counting under Metric-based Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) enables private data sharing and analytics\nwithout the need for a trusted data collector. Error-optimal primitives (for,\ne.g., estimating means and item frequencies) under LDP have been well studied.\nFor analytical tasks such as range queries, however, the best known error bound\nis dependent on the domain size of private data, which is potentially\nprohibitive. This deficiency is inherent as LDP protects the same level of\nindistinguishability between any pair of private data values for each data\ndowner.\n  In this paper, we utilize an extension of $\\epsilon$-LDP called Metric-LDP or\n$E$-LDP, where a metric $E$ defines heterogeneous privacy guarantees for\ndifferent pairs of private data values and thus provides a more flexible knob\nthan $\\epsilon$ does to relax LDP and tune utility-privacy trade-offs. We show\nthat, under such privacy relaxations, for analytical workloads such as linear\ncounting, multi-dimensional range counting queries, and quantile queries, we\ncan achieve significant gains in utility. In particular, for range queries\nunder $E$-LDP where the metric $E$ is the $L^1$-distance function scaled by\n$\\epsilon$, we design mechanisms with errors independent on the domain sizes;\ninstead, their errors depend on the metric $E$, which specifies in what\ngranularity the private data is protected. We believe that the primitives we\ndesign for $E$-LDP will be useful in developing mechanisms for other analytical\ntasks, and encourage the adoption of LDP in practice.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 21:22:03 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 03:56:48 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 13:04:09 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Xiang", "Zhuolun", ""], ["Ding", "Bolin", ""], ["He", "Xi", ""], ["Zhou", "Jingren", ""]]}, {"id": "1909.11930", "submitter": "Teresa Anna Steiner", "authors": "Philip Bille and Inge Li G{\\o}rtz and Teresa Anna Steiner", "title": "String Indexing with Compressed Patterns", "comments": "Draft of full version, extended abstract appeared at STACS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $S$ of length $n$, the classic string indexing problem is to\npreprocess $S$ into a compact data structure that supports efficient subsequent\npattern queries. In this paper we consider the basic variant where the pattern\nis given in compressed form and the goal is to achieve query time that is fast\nin terms of the compressed size of the pattern. This captures the common\nclient-server scenario, where a client submits a query and communicates it in\ncompressed form to a server. Instead of the server decompressing the query\nbefore processing it, we consider how to efficiently process the compressed\nquery directly. Our main result is a novel linear space data structure that\nachieves near-optimal query time for patterns compressed with the classic\nLempel-Ziv compression scheme. Along the way we develop several data structural\ntechniques of independent interest, including a novel data structure that\ncompactly encodes all LZ77 compressed suffixes of a string in linear space and\na general decomposition of tries that reduces the search time from logarithmic\nin the size of the trie to logarithmic in the length of the pattern.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:34:01 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 12:05:06 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 08:47:59 GMT"}, {"version": "v4", "created": "Fri, 23 Apr 2021 14:23:22 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Steiner", "Teresa Anna", ""]]}, {"id": "1909.11970", "submitter": "Marten Maack", "authors": "Klaus Jansen and Alexandra Lassota and Marten Maack", "title": "Approximation Algorithms for Scheduling with Class Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning jobs onto identical machines with the objective to minimize the\nmaximal load is one of the most basic problems in combinatorial optimization.\nMotivated by product planing and data placement, we study a natural extension\ncalled Class Constrainted Scheduling (CCS). In this problem, each job\nadditionally belongs to a class and each machine can only schedule jobs from at\nmost $c$ different classes. Even though this problem is closely related to the\nClass Constraint Bin Packing, the Class Constraint Knapsack and the Cardinality\nConstraint variants, CCS lacks results regarding approximation algorithms, even\nthough it is also well-known to be NP-hard. We fill this gap by analyzing the\nproblem considering three different ways to feasibly allot the jobs: The\nsplittable case, where we can split and allot the jobs arbitrarily; the\npreemptive case, where jobs pieces belonging to the same job are not allowed to\nbe scheduled in parallel; and finally the non-preemptive case, where no\nsplitting is allowed at all. For each case we introduce the first PTAS where\nneither $c$ nor the number of all classes have to be a constant. In order to\nachieve this goal, we give new insights about the structure of optimal\nsolutions. This allows us to preprocess the instance appropriately and by\nadditionally grouping variables to set up a configuration Integer Linear\nProgram (ILP) with N-fold structure. This N-fold structure allows us to solve\nthe ILP efficiently. Further we developed the first simple approximation\nalgorithms with a constant approximation ratio running in strongly polynomial\ntime. The splittable and the preemptive case admit algorithms with ratio $2$\nand a running time of $O(n^2 \\log(n))$. The algorithm for the non-preemptive\ncase has a ratio of $7/3$ and a running time of $O(n^2 \\log^2(n))$. All results\neven hold if the number of machines cannot be bounded by a polynomial in $n$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:21:08 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Jansen", "Klaus", ""], ["Lassota", "Alexandra", ""], ["Maack", "Marten", ""]]}, {"id": "1909.12025", "submitter": "Xianghui Zhong", "authors": "Stefan Hougardy, Fabian Zaiser, Xianghui Zhong", "title": "The Approximation Ratio of the 2-Opt Heuristic for the Metric Traveling\n  Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2-Opt heuristic is one of the simplest algorithms for finding good\nsolutions to the metric Traveling Salesman Problem. It is the key ingredient to\nthe well-known Lin-Kernighan algorithm and often used in practice. So far, only\nupper and lower bounds on the approximation ratio of the 2-Opt heuristic for\nthe metric TSP were known. We prove that for the metric TSP with $n$ cities,\nthe approximation ratio of the 2-Opt heuristic is $\\sqrt{n/2}$ and that this\nbound is tight.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 10:58:33 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 12:33:13 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 11:13:47 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hougardy", "Stefan", ""], ["Zaiser", "Fabian", ""], ["Zhong", "Xianghui", ""]]}, {"id": "1909.12044", "submitter": "Sandor Kisfaludi-Bak", "authors": "S\\'andor Kisfaludi-Bak, D\\'aniel Marx, Tom C. van der Zanden", "title": "How does object fatness impact the complexity of packing in d\n  dimensions?", "comments": "Short version appears in ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing is a classical problem where one is given a set of subsets of\nEuclidean space called objects, and the goal is to find a maximum size subset\nof objects that are pairwise non-intersecting. The problem is also known as the\nIndependent Set problem on the intersection graph defined by the objects.\nAlthough the problem is NP-complete, there are several subexponential\nalgorithms in the literature. One of the key assumptions of such algorithms has\nbeen that the objects are fat, with a few exceptions in two dimensions; for\nexample, the packing problem of a set of polygons in the plane surprisingly\nadmits a subexponential algorithm. In this paper we give tight running time\nbounds for packing similarly-sized non-fat objects in higher dimensions.\n  We propose an alternative and very weak measure of fatness called the\nstabbing number, and show that the packing problem in Euclidean space of\nconstant dimension $d \\geq 3$ for a family of similarly sized objects with\nstabbing number $\\alpha$ can be solved in $2^{O(n^{1-1/d}\\alpha)}$ time. We\nprove that even in the case of axis-parallel boxes of fixed shape, there is no\n$2^{o(n^{1-1/d}\\alpha)}$ algorithm under ETH. This result smoothly bridges the\nwhole range of having constant-fat objects on one extreme ($\\alpha=1$) and a\nsubexponential algorithm of the usual running time, and having very \"skinny\"\nobjects on the other extreme ($\\alpha=n^{1/d}$), where we cannot hope to\nimprove upon the brute force running time of $2^{O(n)}$, and thereby\ncharacterizes the impact of fatness on the complexity of packing in case of\nsimilarly sized objects. We also study the same problem when parameterized by\nthe solution size $k$, and give a $n^{O(k^{1-1/d}\\alpha)}$ algorithm, with an\nalmost matching lower bound.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:08:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kisfaludi-Bak", "S\u00e1ndor", ""], ["Marx", "D\u00e1niel", ""], ["van der Zanden", "Tom C.", ""]]}, {"id": "1909.12209", "submitter": "Abdurrahman Ya\\c{s}ar", "authors": "Abdurrahman Ya\\c{s}ar and \\\"Umit V. \\c{C}ataly\\\"urek", "title": "Heuristics for Symmetric Rectilinear Matrix Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning sparse matrices and graphs is a common and important problem in\nmany scientific and graph analytics applications. In this work, we are\nconcerned with a spatial partitioning called rectilinear partitioning (also\nknown as generalized block distribution) of sparse matrices, which is needed\nfor tiled (or {\\em blocked}) execution of sparse matrix and graph analytics\nkernels. More specifically, in this work, we address the problem of symmetric\nrectilinear partitioning of square matrices. By symmetric, we mean having the\nsame partition on rows and columns of the matrix, yielding a special tiling\nwhere the diagonal tiles (blocks) will be squares. We propose five heuristics\nto solve two different variants of this problem, and present a thorough\nexperimental evaluation showing the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:00:13 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 16:13:55 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ya\u015far", "Abdurrahman", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""]]}, {"id": "1909.12328", "submitter": "Dimitrios Letsios", "authors": "Dimitrios Letsios, Radu Baltean-Lugojan, Francesco Ceccon, Miten\n  Mistry, Johannes Wiebe, Ruth Misener", "title": "Approximation Algorithms for Process Systems Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and analyzing algorithms with provable performance guarantees\nenables efficient optimization problem solving in different application\ndomains, e.g.\\ communication networks, transportation, economics, and\nmanufacturing. Despite the significant contributions of approximation\nalgorithms in engineering, only limited and isolated works contribute from this\nperspective in process systems engineering. The current paper discusses three\nrepresentative, NP-hard problems in process systems engineering: (i) pooling,\n(ii) process scheduling, and (iii) heat exchanger network synthesis. We survey\nrelevant results and raise major open questions. Further, we present\napproximation algorithms applications which are relevant to process systems\nengineering: (i) better mathematical modeling, (ii) problem classification,\n(iii) designing solution methods, and (iv) dealing with uncertainty. This paper\naims to motivate further research at the intersection of approximation\nalgorithms and process systems engineering.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 18:38:25 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Letsios", "Dimitrios", ""], ["Baltean-Lugojan", "Radu", ""], ["Ceccon", "Francesco", ""], ["Mistry", "Miten", ""], ["Wiebe", "Johannes", ""], ["Misener", "Ruth", ""]]}, {"id": "1909.12353", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate, Cosmin Bonchis and Mircea Marin", "title": "Interactive Particle Systems on Hypergraphs, Drift Analysis and the\n  WalkSAT algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the expected running time of WalkSAT, a well-known local search\nprocedure for satisfiability solving, on satisfiable instances of the k-XOR SAT\nproblem. We obtain estimates of this expected running time by reducing the\nproblem to a setting amenable to classical techniques from drift analysis.\n  A crucial ingredient of this reduction is the definition of (new, explosive)\nhypergraph versions of interacting particle systems, notably of coalescing and\nannihilating random walks as well as the voter model. The use of these tools\nallows to show that the expected running time of WalkSAT depends on structural\nparameter (we call odd Cheeger drift) of the dual of the formula hypergraph.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:38:42 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Istrate", "Gabriel", ""], ["Bonchis", "Cosmin", ""], ["Marin", "Mircea", ""]]}, {"id": "1909.12379", "submitter": "Vicent Cholvi", "authors": "Vicent Cholvi, Pawe{\\l} Garncarek, Tomasz Jurdzinski and Dariusz R.\n  Kowalski", "title": "Optimal Packet-oblivious Stable Routing in Multi-hop Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability is an important issue in order to characterize the performance of a\nnetwork, and it has become a major topic of study in the last decade. Roughly\nspeaking, a communication network system is said to be stable if the number of\npackets waiting to be delivered (backlog) is finitely bounded at any one time.\n  In this paper, we introduce a new family of combinatorial structures, which\nwe call universally strong selectors, that are used to provide a set of\ntransmission schedules. Making use of these structures, combined with some\nknown queuing policies, we propose a packet-oblivious routing algorithm which\nis working without using any global topological information, and guarantees\nstability for certain injection rates. We show that this protocol is\nasymptotically optimal regarding the injection rate for which stability is\nguaranteed.\n  Furthermore, we also introduce a packet-oblivious routing algorithm that\nguarantees stability for higher traffic. This algorithm is optimal regarding\nthe injection rate for which stability is guaranteed. However, it needs to use\nsome global information of the system topology.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:40:47 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 07:25:28 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 11:12:35 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Cholvi", "Vicent", ""], ["Garncarek", "Pawe\u0142", ""], ["Jurdzinski", "Tomasz", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1909.12387", "submitter": "Digvijay Boob", "authors": "Digvijay Boob, Saurabh Sawlani, Di Wang", "title": "Faster width-dependent algorithm for mixed packing and covering LPs", "comments": "Accepted for oral presentation at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a faster width-dependent algorithm for mixed\npacking-covering LPs. Mixed packing-covering LPs are fundamental to\ncombinatorial optimization in computer science and operations research. Our\nalgorithm finds a $1+\\eps$ approximate solution in time $O(Nw/ \\eps)$, where\n$N$ is number of nonzero entries in the constraint matrix and $w$ is the\nmaximum number of nonzeros in any constraint. This run-time is better than\nNesterov's smoothing algorithm which requires $O(N\\sqrt{n}w/ \\eps)$ where $n$\nis the dimension of the problem. Our work utilizes the framework of area\nconvexity introduced in [Sherman-FOCS'17] to obtain the best dependence on\n$\\eps$ while breaking the infamous $\\ell_{\\infty}$ barrier to eliminate the\nfactor of $\\sqrt{n}$. The current best width-independent algorithm for this\nproblem runs in time $O(N/\\eps^2)$ [Young-arXiv-14] and hence has worse running\ntime dependence on $\\eps$. Many real life instances of the mixed\npacking-covering problems exhibit small width and for such cases, our algorithm\ncan report higher precision results when compared to width-independent\nalgorithms. As a special case of our result, we report a $1+\\eps$ approximation\nalgorithm for the densest subgraph problem which runs in time $O(md/ \\eps)$,\nwhere $m$ is the number of edges in the graph and $d$ is the maximum graph\ndegree.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 21:02:51 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Boob", "Digvijay", ""], ["Sawlani", "Saurabh", ""], ["Wang", "Di", ""]]}, {"id": "1909.12441", "submitter": "Xin Yang", "authors": "Huaian Diao, Zhao Song, David P. Woodruff, Xin Yang", "title": "Total Least Squares Regression in Input Sparsity Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the total least squares problem, one is given an $m \\times n$ matrix $A$,\nand an $m \\times d$ matrix $B$, and one seeks to \"correct\" both $A$ and $B$,\nobtaining matrices $\\hat{A}$ and $\\hat{B}$, so that there exists an $X$\nsatisfying the equation $\\hat{A}X = \\hat{B}$. Typically the problem is\noverconstrained, meaning that $m \\gg \\max(n,d)$. The cost of the solution\n$\\hat{A}, \\hat{B}$ is given by $\\|A-\\hat{A}\\|_F^2 + \\|B - \\hat{B}\\|_F^2$. We\ngive an algorithm for finding a solution $X$ to the linear system\n$\\hat{A}X=\\hat{B}$ for which the cost $\\|A-\\hat{A}\\|_F^2 + \\|B-\\hat{B}\\|_F^2$\nis at most a multiplicative $(1+\\epsilon)$ factor times the optimal cost, up to\nan additive error $\\eta$ that may be an arbitrarily small function of $n$.\nImportantly, our running time is $\\tilde{O}( \\mathrm{nnz}(A) + \\mathrm{nnz}(B)\n) + \\mathrm{poly}(n/\\epsilon) \\cdot d$, where for a matrix $C$,\n$\\mathrm{nnz}(C)$ denotes its number of non-zero entries. Importantly, our\nrunning time does not directly depend on the large parameter $m$. As total\nleast squares regression is known to be solvable via low rank approximation, a\nnatural approach is to invoke fast algorithms for approximate low rank\napproximation, obtaining matrices $\\hat{A}$ and $\\hat{B}$ from this low rank\napproximation, and then solving for $X$ so that $\\hat{A}X = \\hat{B}$. However,\nexisting algorithms do not apply since in total least squares the rank of the\nlow rank approximation needs to be $n$, and so the running time of known\nmethods would be at least $mn^2$. In contrast, we are able to achieve a much\nfaster running time for finding $X$ by never explicitly forming the equation\n$\\hat{A} X = \\hat{B}$, but instead solving for an $X$ which is a solution to an\nimplicit such equation. Finally, we generalize our algorithm to the total least\nsquares problem with regularization.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 00:02:57 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Diao", "Huaian", ""], ["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Yang", "Xin", ""]]}, {"id": "1909.12518", "submitter": "Lior Kamma", "authors": "Allan Gr{\\o}nlund, Lior Kamma, Kasper Green Larsen, Alexander\n  Mathiasen, Jelani Nelson", "title": "Margin-Based Generalization Lower Bounds for Boosted Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is one of the most successful ideas in machine learning. The most\nwell-accepted explanations for the low generalization error of boosting\nalgorithms such as AdaBoost stem from margin theory. The study of margins in\nthe context of boosting algorithms was initiated by Schapire, Freund, Bartlett\nand Lee (1998) and has inspired numerous boosting algorithms and generalization\nbounds. To date, the strongest known generalization (upper bound) is the $k$th\nmargin bound of Gao and Zhou (2013). Despite the numerous generalization upper\nbounds that have been proved over the last two decades, nothing is known about\nthe tightness of these bounds. In this paper, we give the first margin-based\nlower bounds on the generalization error of boosted classifiers. Our lower\nbounds nearly match the $k$th margin bound and thus almost settle the\ngeneralization performance of boosted classifiers in terms of margins.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 06:56:47 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 14:39:11 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 08:07:50 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 05:56:19 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Kamma", "Lior", ""], ["Larsen", "Kasper Green", ""], ["Mathiasen", "Alexander", ""], ["Nelson", "Jelani", ""]]}, {"id": "1909.12658", "submitter": "Seiichiro Tani", "authors": "Seiichiro Tani", "title": "Quantum Algorithm for Finding the Optimal Variable Ordering for Binary\n  Decision Diagrams", "comments": "8 figures added; typos corrected", "journal-ref": "Proc. 17th Scandinavian Symposium and Workshops on Algorithm\n  Theory (SWAT 2020), LIPIcs 162, pp. 36:1--36:19", "doi": "10.4230/LIPIcs.SWAT.2020.36", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ordered binary decision diagram (OBDD) is a directed acyclic graph that\nrepresents a Boolean function. OBDDs are also known as special cases of\noblivious read-once branching programs in the field of complexity theory. Since\nOBDDs have many nice properties as data structures, they have been extensively\nstudied for decades in both theoretical and practical fields, such as VLSI\ndesign, formal verification, machine learning, and combinatorial problems.\nArguably, the most crucial problem in using OBDDs is that they may vary\nexponentially in size depending on their variable ordering (i.e., the order in\nwhich the variable are to read) when they represent the same function. Indeed,\nit is NP hard to find an optimal variable ordering that minimizes an OBDD for a\ngiven function. Hence, numerous studies have sought heuristics to find an\noptimal variable ordering. From practical as well as theoretical points of\nview, it is also important to seek algorithms that output optimal solutions\nwith lower (exponential) time complexity than trivial brute-force algorithms\ndo. Friedman and Supowit provided a clever deterministic algorithm with\ntime/space complexity $O^\\ast(3^n)$, where $n$ is the number of variables of\nthe function, which is much better than the trivial brute-force bound\n$O^\\ast(n!2^n)$. This paper shows that a further speedup is possible with\nquantum computers by demonstrating the existence of a quantum algorithm that\nproduces a minimum OBDD together with the corresponding variable ordering in\n$O^\\ast(2.77286^n)$ time and space with an exponentially small error. Moreover,\nthis algorithm can be adapted to constructing other minimum decision diagrams\nsuch as zero-suppressed BDDs, which provide compact representations of sparse\nsets and are often used in the field of discrete optimization and enumeration.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 12:52:07 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 04:15:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tani", "Seiichiro", ""]]}, {"id": "1909.12755", "submitter": "Xianghui Zhong", "authors": "Xianghui Zhong", "title": "On the Approximation Ratio of the $k$-Opt and Lin-Kernighan Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Opt and Lin-Kernighan algorithm are two of the most important local\nsearch approaches for the Metric TSP. Both start with an arbitrary tour and\nmake local improvements in each step to get a shorter tour. We show that for\nany fixed $k\\geq 3$ the approximation ratio of the $k$-Opt algorithm for Metric\nTSP is $O(\\sqrt[k]{n})$. Assuming the Erd\\H{o}s girth conjecture, we prove a\nmatching lower bound of $\\Omega(\\sqrt[k]{n})$. Unconditionally, we obtain\nmatching bounds for $k=3,4,6$ and a lower bound of\n$\\Omega(n^{\\frac{2}{3k-3}})$. Our most general bounds depend on the values of a\nfunction from extremal graph theory and are tight up to a factor logarithmic in\nthe number of vertices unconditionally. Moreover, all the upper bounds also\napply to a parameterized version of the Lin-Kernighan algorithm with\nappropriate parameters. We also show that the approximation ratio of $k$-Opt\nfor Graph TSP is $\\Omega\\left(\\frac{\\log(n)}{\\log\\log(n)}\\right)$ and\n$O\\left(\\left(\\frac{\\log(n)}{\\log\\log(n)}\\right)^{\\log_2(9)+\\epsilon}\\right)$\nfor all $\\epsilon>0$. For the (1,2)-TSP we give a lower bound of\n$\\frac{11}{10}$ on the approximation ratio of the $k$-improv and $k$-Opt\nalgorithm for arbitrary fixed $k$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 15:47:54 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 17:16:42 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 08:51:15 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 18:09:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhong", "Xianghui", ""]]}, {"id": "1909.12760", "submitter": "Buddhima Gamlath", "authors": "Buddhima Gamlath, Sagar Kale, Ola Svensson", "title": "Beating Greedy for Stochastic Bipartite Matching", "comments": "Published in ACM-SIAM Symposium on Discrete Algorithms (SODA19)", "journal-ref": null, "doi": "10.1137/1.9781611975482.176", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum bipartite matching problem in stochastic settings,\nnamely the query-commit and price-of-information models. In the query-commit\nmodel, an edge e independently exists with probability $p_e$. We can query\nwhether an edge exists or not, but if it does exist, then we have to take it\ninto our solution. In the unweighted case, one can query edges in the order\ngiven by the classical online algorithm of Karp, Vazirani, and Vazirani to get\na $(1-1/e)$-approximation. In contrast, the previously best known algorithm in\nthe weighted case is the $(1/2)$-approximation achieved by the greedy algorithm\nthat sorts the edges according to their weights and queries in that order.\n  Improving upon the basic greedy, we give a $(1-1/e)$-approximation algorithm\nin the weighted query-commit model. We use a linear program (LP) to upper bound\nthe optimum achieved by any strategy. The proposed LP admits several structural\nproperties that play a crucial role in the design and analysis of our\nalgorithm. We also extend these techniques to get a $(1-1/e)$-approximation\nalgorithm for maximum bipartite matching in the price-of-information model\nintroduced by Singla, who also used the basic greedy algorithm to give a\n$(1/2)$-approximation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 16:00:00 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 11:12:14 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Gamlath", "Buddhima", ""], ["Kale", "Sagar", ""], ["Svensson", "Ola", ""]]}, {"id": "1909.12877", "submitter": "Jens Stoye", "authors": "Eyla Willing and Jens Stoye and Mar\\'ilia D. V. Braga", "title": "Computing the Inversion-Indel Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inversion distance, that is the distance between two unichromosomal\ngenomes with the same content allowing only inversions of DNA segments, can be\nexactly computed thanks to a pioneering approach of Hannenhalli and Pevzner\nfrom 1995. In 2000, El-Mabrouk extended the inversion model to perform the\ncomparison of unichromosomal genomes with unequal contents, combining\ninversions with insertions and deletions (indels) of DNA segments, giving rise\nto the inversion-indel distance. However, only a heuristic was provided for its\ncomputation. In 2005, Yancopoulos, Attie and Friedberg started a new branch of\nresearch by introducing the generic double cut and join (DCJ) operation, that\ncan represent several genome rearrangements (including inversions). In 2006,\nBergeron, Mixtacki and Stoye showed that the DCJ distance can be computed in\nlinear time with a very simple procedure. As a consequence, in 2010 we gave a\nlinear-time algorithm to compute the DCJ-indel distance. This result allowed\nthe inversion-indel model to be revisited from another angle. In 2013, we could\nshow that, when the diagram that represents the relation between the two\ncompared genomes has no bad components, the inversion-indel distance is equal\nto the DCJ-indel distance. In the present work we complete the study of the\ninversion-indel distance by giving the first algorithm to compute it exactly\neven in the presence of bad components.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 19:13:02 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Willing", "Eyla", ""], ["Stoye", "Jens", ""], ["Braga", "Mar\u00edlia D. V.", ""]]}, {"id": "1909.13029", "submitter": "Guozhen Rong", "authors": "Guozhen Rong, Yixin Cao, Jianxin Wang", "title": "Characterization and Linear-time Recognition of Paired Threshold Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a paired threshold graph, each vertex has a weight, and two vertices are\nadjacent if their weight sum is large enough and their weight difference is\nsmall enough. It generalizes threshold graphs and unit interval graphs, both\nvery well studied. We present a vertex ordering characterization of this graph\nclass, which enables us to prove that it is a subclass of interval graphs.\nFurther study of clique paths of paired threshold graphs leads to a simple\nlinear-time recognition algorithm for the class.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:41:12 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Rong", "Guozhen", ""], ["Cao", "Yixin", ""], ["Wang", "Jianxin", ""]]}, {"id": "1909.13344", "submitter": "Dominik Goeke", "authors": "Dominik Goeke and Michael Schneider", "title": "Modeling Single Picker Routing Problems in Classical and Modern\n  Warehouses", "comments": null, "journal-ref": null, "doi": null, "report-no": "Working Paper DPO-2018-11 (version 1, 04.11.2018)", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard single picker routing problem (SPRP) seeks the cost-minimal tour\nto collect a set of given articles in a rectangular single-block warehouse with\nparallel picking aisles and a dedicated storage policy, i.e, each SKU is only\navailable from one storage location in the warehouse. We present a compact\nformulation that forgoes classical subtour elimination constraints by directly\nexploiting two of the properties of an optimal picking tour used in the dynamic\nprogramming algorithm of Ratliff and Rosenthal (1983). We extend the\nformulation to three important settings prevalent in modern e-commerce\nwarehouses: scattered storage, decoupling of picker and cart, and multiple end\ndepots. In numerical studies, our formulation outperforms existing standard\nSPRP formulations from the literature and proves able to solve large instances\nwithin short runtimes. Realistically sized instances of the three problem\nextensions can also be solved with low computational effort. We find that\ndecoupling of picker and cart can lead to substantial cost savings depending on\nthe speed and capacity of the picker when traveling alone, whereas additional\nend depots have rather limited benefits in a single-block warehouse.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 19:15:50 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Goeke", "Dominik", ""], ["Schneider", "Michael", ""]]}, {"id": "1909.13345", "submitter": "Antonios Antoniadis", "authors": "Antonios Antoniadis and Naveen Garg and Gunjan Kumar and Nikhil Kumar", "title": "Parallel Machine Scheduling to Minimize Energy Consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n jobs with release dates, deadlines and processing times we consider\nthe problem of scheduling them on m parallel machines so as to minimize the\ntotal energy consumed. Machines can enter a sleep state and they consume no\nenergy in this state. Each machine requires Q units of energy to awaken from\nthe sleep state and in its active state the machine can process jobs and\nconsumes a unit of energy per unit time. We allow for preemption and migration\nof jobs and provide the first constant approximation algorithm for this\nproblem.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 19:20:27 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Garg", "Naveen", ""], ["Kumar", "Gunjan", ""], ["Kumar", "Nikhil", ""]]}, {"id": "1909.13384", "submitter": "Rajesh Jayaram", "authors": "Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, David P. Woodruff", "title": "Optimal Sketching for Kronecker Product Regression and Low Rank\n  Approximation", "comments": "A preliminary version of this paper appeared in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Kronecker product regression problem, in which the design matrix\nis a Kronecker product of two or more matrices. Given $A_i \\in \\mathbb{R}^{n_i\n\\times d_i}$ for $i=1,2,\\dots,q$ where $n_i \\gg d_i$ for each $i$, and $b \\in\n\\mathbb{R}^{n_1 n_2 \\cdots n_q}$, let $\\mathcal{A} = A_1 \\otimes A_2 \\otimes\n\\cdots \\otimes A_q$. Then for $p \\in [1,2]$, the goal is to find $x \\in\n\\mathbb{R}^{d_1 \\cdots d_q}$ that approximately minimizes $\\|\\mathcal{A}x -\nb\\|_p$. Recently, Diao, Song, Sun, and Woodruff (AISTATS, 2018) gave an\nalgorithm which is faster than forming the Kronecker product $\\mathcal{A}$\nSpecifically, for $p=2$ their running time is $O(\\sum_{i=1}^q \\text{nnz}(A_i) +\n\\text{nnz}(b))$, where nnz$(A_i)$ is the number of non-zero entries in $A_i$.\nNote that nnz$(b)$ can be as large as $n_1 \\cdots n_q$. For $p=1,$ $q=2$ and\n$n_1 = n_2$, they achieve a worse bound of $O(n_1^{3/2} \\text{poly}(d_1d_2) +\n\\text{nnz}(b))$. In this work, we provide significantly faster algorithms. For\n$p=2$, our running time is $O(\\sum_{i=1}^q \\text{nnz}(A_i) )$, which has no\ndependence on nnz$(b)$. For $p<2$, our running time is $O(\\sum_{i=1}^q\n\\text{nnz}(A_i) + \\text{nnz}(b))$, which matches the prior best running time\nfor $p=2$. We also consider the related all-pairs regression problem, where\ngiven $A \\in \\mathbb{R}^{n \\times d}, b \\in \\mathbb{R}^n$, we want to solve\n$\\min_{x} \\|\\bar{A}x - \\bar{b}\\|_p$, where $\\bar{A} \\in \\mathbb{R}^{n^2 \\times\nd}, \\bar{b} \\in \\mathbb{R}^{n^2}$ consist of all pairwise differences of the\nrows of $A,b$. We give an $O(\\text{nnz}(A))$ time algorithm for $p \\in[1,2]$,\nimproving the $\\Omega(n^2)$ time needed to form $\\bar{A}$. Finally, we initiate\nthe study of Kronecker product low rank and low $t$-rank approximation. For\ninput $\\mathcal{A}$ as above, we give $O(\\sum_{i=1}^q \\text{nnz}(A_i))$ time\nalgorithms, which is much faster than computing $\\mathcal{A}$.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 22:24:28 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Diao", "Huaian", ""], ["Jayaram", "Rajesh", ""], ["Song", "Zhao", ""], ["Sun", "Wen", ""], ["Woodruff", "David P.", ""]]}, {"id": "1909.13459", "submitter": "Jie Liu", "authors": "Jie Liu, Xiao Yan, Xinyan Dai, Zhirong Li, James Cheng, Ming-Chang\n  Yang", "title": "Understanding and Improving Proximity Graph based Maximum Inner Product\n  Search", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inner-product navigable small world graph (ip-NSW) represents the\nstate-of-the-art method for approximate maximum inner product search (MIPS) and\nit can achieve an order of magnitude speedup over the fastest baseline.\nHowever, to date it is still unclear where its exceptional performance comes\nfrom. In this paper, we show that there is a strong norm bias in the MIPS\nproblem, which means that the large norm items are very likely to become the\nresult of MIPS. Then we explain the good performance of ip-NSW as matching the\nnorm bias of the MIPS problem - large norm items have big in-degrees in the\nip-NSW proximity graph and a walk on the graph spends the majority of\ncomputation on these items, thus effectively avoids unnecessary computation on\nsmall norm items. Furthermore, we propose the ip-NSW+ algorithm, which improves\nip-NSW by introducing an additional angular proximity graph. Search is first\nconducted on the angular graph to find the angular neighbors of a query and\nthen the MIPS neighbors of these angular neighbors are used to initialize the\ncandidate pool for search on the inner-product proximity graph. Experiment\nresults show that ip-NSW+ consistently and significantly outperforms ip-NSW and\nprovides more robust performance under different data distributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 05:12:49 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 14:19:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Liu", "Jie", ""], ["Yan", "Xiao", ""], ["Dai", "Xinyan", ""], ["Li", "Zhirong", ""], ["Cheng", "James", ""], ["Yang", "Ming-Chang", ""]]}, {"id": "1909.13472", "submitter": "Martin Royer", "authors": "Martin Royer (DATASHAPE), Fr\\'ed\\'eric Chazal (DATASHAPE), Cl\\'ement\n  Levrard (LPSM (UMR\\_8001)), Umeda Yuhei, Ike Yuichi", "title": "ATOL: Measure Vectorization for Automatic Topologically-Oriented\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust topological information commonly comes in the form of a set of\npersistence diagrams, finite measures that are in nature uneasy to affix to\ngeneric machine learning frameworks. We introduce a fast, learnt, unsupervised\nvectorization method for measures in Euclidean spaces and use it for reflecting\nunderlying changes in topological behaviour in machine learning contexts. The\nalgorithm is simple and efficiently discriminates important space regions where\nmeaningful differences to the mean measure arise. It is proven to be able to\nseparate clusters of persistence diagrams. We showcase the strength and\nrobustness of our approach on a number of applications, from emulous and modern\ngraph collections where the method reaches state-of-the-art performance to a\ngeometric synthetic dynamical orbits problem. The proposed methodology comes\nwith a single high level tuning parameter: the total measure encoding budget.\nWe provide a completely open access software.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:30:33 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 16:21:18 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 08:13:12 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Royer", "Martin", "", "DATASHAPE"], ["Chazal", "Fr\u00e9d\u00e9ric", "", "DATASHAPE"], ["Levrard", "Cl\u00e9ment", "", "LPSM"], ["Yuhei", "Umeda", ""], ["Yuichi", "Ike", ""]]}, {"id": "1909.13541", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain, Vincent Froese, David Schultz", "title": "An Average-Compress Algorithm for the Sample Mean Problem under Dynamic\n  Time Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing a sample mean of time series under dynamic time warping (DTW) is\nNP-hard. Consequently, there is an ongoing research effort to devise efficient\nheuristics. The majority of heuristics have been developed for the constrained\nsample mean problem that assumes a solution of predefined length. In contrast,\nresearch on the unconstrained sample mean problem is underdeveloped. In this\narticle, we propose a generic average-compress (AC) algorithm for solving the\nunconstrained problem. The algorithm alternates between averaging (A-step) and\ncompression (C-step). The A-step takes an initial guess as input and returns an\napproximation of a sample mean. Then the C-step reduces the length of the\napproximate solution. The compressed approximation serves as initial guess of\nthe A-step in the next iteration. The purpose of the C-step is to direct the\nalgorithm to more promising solutions of shorter length. The proposed algorithm\nis generic in the sense that any averaging and any compression method can be\nused. Experimental results show that the AC algorithm substantially outperforms\ncurrent state-of-the-art algorithms for time series averaging.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 09:10:13 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 14:23:54 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Jain", "Brijnesh", ""], ["Froese", "Vincent", ""], ["Schultz", "David", ""]]}, {"id": "1909.13670", "submitter": "Se Kwon Lee", "authors": "Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, Vijay\n  Chidambaram", "title": "RECIPE : Converting Concurrent DRAM Indexes to Persistent-Memory Indexes", "comments": "3pages: Added one more reference", "journal-ref": null, "doi": "10.1145/3341301.3359635", "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Recipe, a principled approach for converting concurrent DRAM\nindexes into crash-consistent indexes for persistent memory (PM). The main\ninsight behind Recipe is that isolation provided by a certain class of\nconcurrent in-memory indexes can be translated with small changes to\ncrash-consistency when the same index is used in PM. We present a set of\nconditions that enable the identification of this class of DRAM indexes, and\nthe actions to be taken to convert each index to be persistent. Based on these\nconditions and conversion actions, we modify five different DRAM indexes based\non B+ trees, tries, radix trees, and hash tables to their crash-consistent PM\ncounterparts. The effort involved in this conversion is minimal, requiring\n30-200 lines of code. We evaluated the converted PM indexes on Intel DC\nPersistent Memory, and found that they outperform state-of-the-art,\nhand-crafted PM indexes in multi-threaded workloads by up-to 5.2x. For example,\nwe built P-CLHT, our PM implementation of the CLHT hash table by modifying only\n30 LOC. When running YCSB workloads, P-CLHT performs up to 2.4x better than\nCacheline-Conscious Extendible Hashing (CCEH), the state-of-the-art PM hash\ntable.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:21:18 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:59:31 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 04:03:04 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2019 18:23:08 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Lee", "Se Kwon", ""], ["Mohan", "Jayashree", ""], ["Kashyap", "Sanidhya", ""], ["Kim", "Taesoo", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1909.13676", "submitter": "Alexander Robey", "authors": "Alexander Robey, Arman Adibi, Brent Schlotfeldt, George J. Pappas,\n  Hamed Hassani", "title": "Optimal Algorithms for Submodular Maximization with Distributed\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of discrete optimization problems that aim to maximize a\nsubmodular objective function subject to a distributed partition matroid\nconstraint. More precisely, we consider a networked scenario in which multiple\nagents choose actions from local strategy sets with the goal of maximizing a\nsubmodular objective function defined over the set of all possible actions.\nGiven this distributed setting, we develop Constraint-Distributed Continuous\nGreedy (CDCG), a message passing algorithm that converges to the tight\n$(1-1/e)$ approximation factor of the optimum global solution using only local\ncomputation and communication. It is known that a sequential greedy algorithm\ncan only achieve a $1/2$ multiplicative approximation of the optimal solution\nfor this class of problems in the distributed setting. Our framework relies on\nlifting the discrete problem to a continuous domain and developing a consensus\nalgorithm that achieves the tight $(1-1/e)$ approximation guarantee of the\nglobal discrete solution once a proper rounding scheme is applied. We also\noffer empirical results from a multi-agent area coverage problem to show that\nthe proposed method significantly outperforms the state-of-the-art sequential\ngreedy method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:26:05 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 00:21:21 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 01:40:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Robey", "Alexander", ""], ["Adibi", "Arman", ""], ["Schlotfeldt", "Brent", ""], ["Pappas", "George J.", ""], ["Hassani", "Hamed", ""]]}, {"id": "1909.13830", "submitter": "Ryan Rogers", "authors": "Jinshuo Dong, David Durfee, Ryan Rogers", "title": "Optimal Differential Privacy Composition for Exponential Mechanisms and\n  the Cost of Adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composition is one of the most important properties of differential privacy\n(DP), as it allows algorithm designers to build complex private algorithms from\nDP primitives. We consider precise composition bounds of the overall privacy\nloss for exponential mechanisms, one of the fundamental classes of mechanisms\nin DP. We give explicit formulations of the optimal privacy loss for both the\nadaptive and non-adaptive settings. For the non-adaptive setting in which each\nmechanism has the same privacy parameter, we give an efficiently computable\nformulation of the optimal privacy loss. Furthermore, we show that there is a\ndifference in the privacy loss when the exponential mechanism is chosen\nadaptively versus non-adaptively. To our knowledge, it was previously unknown\nwhether such a gap existed for any DP mechanisms with fixed privacy parameters,\nand we demonstrate the gap for a widely used class of mechanism in a natural\nsetting. We then improve upon the best previously known upper bounds for\nadaptive composition of exponential mechanisms with efficiently computable\nformulations and show the improvement.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 16:45:50 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 19:39:52 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Dong", "Jinshuo", ""], ["Durfee", "David", ""], ["Rogers", "Ryan", ""]]}, {"id": "1909.13848", "submitter": "Ignasi Sau", "authors": "Raul Lopes, Ignasi Sau", "title": "A relaxation of the Directed Disjoint Paths problem: a global congestion\n  metric helps", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Directed Disjoint Paths problem, we are given a digraph $D$ and a set\nof requests $\\{(s_1, t_1), \\ldots, (s_k, t_k)\\}$, and the task is to find a\ncollection of pairwise vertex-disjoint paths $\\{P_1, \\ldots, P_k\\}$ such that\neach $P_i$ is a path from $s_i$ to $t_i$ in $D$. This problem is NP-complete\nfor fixed $k=2$ and W[1]-hard with parameter $k$ in DAGs. A few positive\nresults are known under restrictions on the input digraph, such as being planar\nor having bounded directed tree-width, or under relaxations of the problem,\nsuch as allowing for vertex congestion. Good news are scarce, however, for\ngeneral digraphs. In this article we propose a novel global congestion metric\nfor the problem: we only require the paths to be \"disjoint enough\", in the\nsense that they must behave properly not in the whole graph, but in an\nunspecified large part of it. Namely, in the Disjoint Enough Directed Paths\nproblem, given an $n$-vertex digraph $D$, a set of $k$ requests, and\nnon-negative integers $d$ and $s$, the task is to find a collection of paths\nconnecting the requests such that at least $d$ vertices of $D$ occur in at most\n$s$ paths of the collection. We study the parameterized complexity of this\nproblem for a number of choices of the parameter, including the directed\ntree-width of $D$. Among other results, we show that the problem is W[1]-hard\nin DAGs with parameter $d$ and, on the positive side, we give an algorithm in\ntime $O(n^d \\cdot k^{d\\cdot s})$ and a kernel of size $d \\cdot 2^{k-s}\\cdot\n\\binom{k}{s} + 2k$ in general digraphs. The latter result, which is our main\ncontribution, has consequences for the Steiner Network problem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:12:37 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Lopes", "Raul", ""], ["Sau", "Ignasi", ""]]}]