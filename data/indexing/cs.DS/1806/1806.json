[{"id": "1806.00040", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Weihao Kong, Alistair Stewart", "title": "Efficient Algorithms and Lower Bounds for Robust Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional linear regression in a robust model\nwhere an $\\epsilon$-fraction of the samples can be adversarially corrupted. We\nfocus on the fundamental setting where the covariates of the uncorrupted\nsamples are drawn from a Gaussian distribution $\\mathcal{N}(0, \\Sigma)$ on\n$\\mathbb{R}^d$. We give nearly tight upper bounds and computational lower\nbounds for this problem. Specifically, our main contributions are as follows:\n  For the case that the covariance matrix is known to be the identity, we give\na sample near-optimal and computationally efficient algorithm that outputs a\ncandidate hypothesis vector $\\widehat{\\beta}$ which approximates the unknown\nregression vector $\\beta$ within $\\ell_2$-norm $O(\\epsilon \\log(1/\\epsilon)\n\\sigma)$, where $\\sigma$ is the standard deviation of the random observation\nnoise. An error of $\\Omega (\\epsilon \\sigma)$ is information-theoretically\nnecessary, even with infinite sample size. Prior work gave an algorithm for\nthis problem with sample complexity $\\tilde{\\Omega}(d^2/\\epsilon^2)$ whose\nerror guarantee scales with the $\\ell_2$-norm of $\\beta$.\n  For the case of unknown covariance, we show that we can efficiently achieve\nthe same error guarantee as in the known covariance case using an additional\n$\\tilde{O}(d^2/\\epsilon^2)$ unlabeled examples. On the other hand, an error of\n$O(\\epsilon \\sigma)$ can be information-theoretically attained with\n$O(d/\\epsilon^2)$ samples. We prove a Statistical Query (SQ) lower bound\nproviding evidence that this quadratic tradeoff in the sample size is inherent.\nMore specifically, we show that any polynomial time SQ learning algorithm for\nrobust linear regression (in Huber's contamination model) with estimation\ncomplexity $O(d^{2-c})$, where $c>0$ is an arbitrarily small constant, must\nincur an error of $\\Omega(\\sqrt{\\epsilon} \\sigma)$.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:23:29 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kong", "Weihao", ""], ["Stewart", "Alistair", ""]]}, {"id": "1806.00188", "submitter": "Yulun Tian", "authors": "Yulun Tian, Kasra Khosoussi, Matthew Giamou, Jonathan P. How and\n  Jonathan Kelly", "title": "Near-Optimal Budgeted Data Exchange for Distributed Loop Closure\n  Detection", "comments": "RSS 2018 Extended Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-robot loop closure detection is a core problem in collaborative SLAM\n(CSLAM). Establishing inter-robot loop closures is a resource-demanding\nprocess, during which robots must consume a substantial amount of\nmission-critical resources (e.g., battery and bandwidth) to exchange sensory\ndata. However, even with the most resource-efficient techniques, the resources\navailable onboard may be insufficient for verifying every potential loop\nclosure. This work addresses this critical challenge by proposing a\nresource-adaptive framework for distributed loop closure detection. We seek to\nmaximize task-oriented objectives subject to a budget constraint on total data\ntransmission. This problem is in general NP-hard. We approach this problem from\ndifferent perspectives and leverage existing results on monotone submodular\nmaximization to provide efficient approximation algorithms with performance\nguarantees. The proposed approach is extensively evaluated using the KITTI\nodometry benchmark dataset and synthetic Manhattan-like datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 04:35:16 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Tian", "Yulun", ""], ["Khosoussi", "Kasra", ""], ["Giamou", "Matthew", ""], ["How", "Jonathan P.", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1806.00198", "submitter": "Keisuke Goto", "authors": "Keisuke Goto, Tomohiro I, Hideo Bannai, Shunsuke Inenaga", "title": "Block Palindromes: A New Generalization of Palindromes", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new generalization of palindromes and gapped palindromes called\nblock palindromes. A block palindrome is a string that becomes a palindrome\nwhen identical substrings are replaced with a distinct character. We\ninvestigate several properties of block palindromes and in particular, study\nsubstrings of a string which are block palindromes. In so doing, we introduce\nthe notion of a \\emph{maximal block palindrome}, which leads to a compact\nrepresentation of all block palindromes that occur in a string. We also propose\nan algorithm which enumerates all maximal block palindromes that appear in a\ngiven string $T$ in $O(|T| + \\|\\mathit{MBP}(T)\\|)$ time, where\n$\\|\\mathit{MBP}(T)\\|$ is the output size, which is optimal unless all the\nmaximal block palindromes can be represented in a more compact way.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 05:22:55 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 08:24:24 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 07:50:30 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Goto", "Keisuke", ""], ["I", "Tomohiro", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""]]}, {"id": "1806.00305", "submitter": "Jos\\'e A. R. Fonollosa", "authors": "Jos\\'e A. R. Fonollosa", "title": "Joint Size and Depth Optimization of Sorting Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting networks are oblivious sorting algorithms with many interesting\ntheoretical properties and practical applications. One of the related classical\nchallenges is the search of optimal networks respect to size (number of\ncomparators) of depth (number of layers). However, up to our knowledge, the\njoint size-depth optimality of small sorting networks has not been addressed\nbefore. This paper presents size-depth optimality results for networks up to\n$12$ channels. Our results show that there are sorting networks for $n\\leq9$\ninputs that are optimal in both size and depth, but this is not the case for\n$10$ and $12$ channels. For $n=10$ inputs, we were able to proof that\noptimal-depth optimal sorting networks with $7$ layers require $31$ comparators\nwhile optimal-size networks with $29$ comparators need $8$ layers. For $n=11$\ninputs we show that networks with $8$ or $9$ layers require at least $35$\ncomparators (the best known upper bound for the minimal size). And for networks\nwith $n=12$ inputs and $8$ layers we need $40$ comparators, while for $9$\nlayers the best known size is $39$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 12:20:45 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1806.00534", "submitter": "Anastasios Kyrillidis", "authors": "Tayo Ajayi, David Mildebrath, Anastasios Kyrillidis, Shashanka Ubaru,\n  Georgios Kollias, Kristofer Bouchard", "title": "Provably convergent acceleration in factored gradient descent with\n  applications in matrix sensing", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present theoretical results on the convergence of \\emph{non-convex}\naccelerated gradient descent in matrix factorization models with $\\ell_2$-norm\nloss. The purpose of this work is to study the effects of acceleration in\nnon-convex settings, where provable convergence with acceleration should not be\nconsidered a \\emph{de facto} property. The technique is applied to matrix\nsensing problems, for the estimation of a rank $r$ optimal solution $X^\\star\n\\in \\mathbb{R}^{n \\times n}$. Our contributions can be summarized as follows.\n$i)$ We show that acceleration in factored gradient descent converges at a\nlinear rate; this fact is novel for non-convex matrix factorization settings,\nunder common assumptions. $ii)$ Our proof technique requires the acceleration\nparameter to be carefully selected, based on the properties of the problem,\nsuch as the condition number of $X^\\star$ and the condition number of objective\nfunction. $iii)$ Currently, our proof leads to the same dependence on the\ncondition number(s) in the contraction parameter, similar to recent results on\nnon-accelerated algorithms. $iv)$ Acceleration is observed in practice, both in\nsynthetic examples and in two real applications: neuronal multi-unit activities\nrecovery from single electrode recordings, and quantum state tomography on\nquantum computing simulators.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 20:29:47 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 00:27:55 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 11:46:46 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 18:57:26 GMT"}, {"version": "v5", "created": "Sat, 21 Sep 2019 19:43:17 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ajayi", "Tayo", ""], ["Mildebrath", "David", ""], ["Kyrillidis", "Anastasios", ""], ["Ubaru", "Shashanka", ""], ["Kollias", "Georgios", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "1806.00588", "submitter": "Xing Shi", "authors": "Xing Shi, Shizhen Xu, Kevin Knight", "title": "Fast Locality Sensitive Hashing for Beam Search on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up\nbeam search for sequence models. We utilize the winner-take-all (WTA) hash,\nwhich is based on relative ranking order of hidden dimensions and thus\nresilient to perturbations in numerical values. Our algorithm is designed by\nfully considering the underling architecture of CUDA-enabled GPUs\n(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied\nfor LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are\nshared across beams to maximize the parallelism; 3) Top frequent words are\nmerged into candidate lists to improve performance. Experiments on 4\nlarge-scale neural machine translation models demonstrate that our algorithm\ncan achieve up to 4x speedup on softmax module, and 2x overall speedup without\nhurting BLEU on GPU.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 06:18:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Shi", "Xing", ""], ["Xu", "Shizhen", ""], ["Knight", "Kevin", ""]]}, {"id": "1806.00638", "submitter": "Ishay Haviv", "authors": "Ishay Haviv", "title": "On Minrank and Forbidden Subgraphs", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minrank over a field $\\mathbb{F}$ of a graph $G$ on the vertex set\n$\\{1,2,\\ldots,n\\}$ is the minimum possible rank of a matrix $M \\in\n\\mathbb{F}^{n \\times n}$ such that $M_{i,i} \\neq 0$ for every $i$, and\n$M_{i,j}=0$ for every distinct non-adjacent vertices $i$ and $j$ in $G$. For an\ninteger $n$, a graph $H$, and a field $\\mathbb{F}$, let $g(n,H,\\mathbb{F})$\ndenote the maximum possible minrank over $\\mathbb{F}$ of an $n$-vertex graph\nwhose complement contains no copy of $H$. In this paper we study this quantity\nfor various graphs $H$ and fields $\\mathbb{F}$. For finite fields, we prove by\na probabilistic argument a general lower bound on $g(n,H,\\mathbb{F})$, which\nyields a nearly tight bound of $\\Omega(\\sqrt{n}/\\log n)$ for the triangle\n$H=K_3$. For the real field, we prove by an explicit construction that for\nevery non-bipartite graph $H$, $g(n,H,\\mathbb{R}) \\geq n^\\delta$ for some\n$\\delta = \\delta(H)>0$. As a by-product of this construction, we disprove a\nconjecture of Codenotti, Pudl\\'ak, and Resta. The results are motivated by\nquestions in information theory, circuit complexity, and geometry.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 14:05:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Haviv", "Ishay", ""]]}, {"id": "1806.00814", "submitter": "Tsunehiko Kameda", "authors": "Binay Bhattacharya, Yuya Higashikawa, Tsunehiko Kameda, Naoki Katoh", "title": "Minmax Regret 1-Sink for Aggregate Evacuation Time on Path Networks", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evacuation in emergency situations can be modeled by a dynamic flow network.\nTwo criteria have been used before: one is the evacuation completion time and\nthe other is the aggregate evacuation time of individual evacuees. The aim of\nthis paper is to optimize the aggregate evacuation time in the simplest case,\nwhere the network is a path and only one evacuation center (called a sink) is\nto be introduced. The evacuees are initially located at the vertices, but their\nprecise numbers are unknown, and are given by upper and lower bounds. Under\nthis assumption, we compute the sink location that minimizes the maximum\n\"regret.\" We present an $O(n^2\\log n)$ time algorithm to solve this problem,\nimproving upon the previously fastest $O(n^3)$ time algorithm, where $n$ is the\nnumber of vertices.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 15:25:01 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bhattacharya", "Binay", ""], ["Higashikawa", "Yuya", ""], ["Kameda", "Tsunehiko", ""], ["Katoh", "Naoki", ""]]}, {"id": "1806.00917", "submitter": "Roger Paredes", "authors": "R. Paredes, L. Duenas-Osorio, K.S. Meel, M.Y. Vardi", "title": "Principled Network Reliability Approximation: A Counting-Based Approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2019.04.025", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As engineered systems expand, become more interdependent, and operate in\nreal-time, reliability assessment is indispensable to support investment and\ndecision making. However, network reliability problems are known to be\n#P-complete, a computational complexity class largely believed to be\nintractable. The computational intractability of network reliability motivates\nour quest for reliable approximations. Based on their theoretical foundations,\navailable methods can be grouped as follows: (i) exact or bounds, (ii)\nguarantee-less sampling, and (iii) probably approximately correct (PAC). Group\n(i) is well regarded due to its useful byproducts, but it does not scale in\npractice. Group (ii) scales well and verifies desirable properties, such as the\nbounded relative error, but it lacks error guarantees. Group (iii) is of great\ninterest when precision and scalability are required, as it harbors\ncomputationally feasible approximation schemes with PAC-guarantees. We give a\ncomprehensive review of classical methods before introducing modern techniques\nand our developments. We introduce K-RelNet, an extended counting-based\nestimation method that delivers PAC-guarantees for the K-terminal reliability\nproblem. Then, we test methods' performance using various benchmark systems. We\nhighlight the range of application of algorithms and provide the foundation for\nfuture resilience engineering as it increasingly necessitates methods for\nuncertainty quantification in complex systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:43:31 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 22:36:00 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Paredes", "R.", ""], ["Duenas-Osorio", "L.", ""], ["Meel", "K. S.", ""], ["Vardi", "M. Y.", ""]]}, {"id": "1806.01119", "submitter": "Florian Sikora", "authors": "Riccardo Dondi, Giancarlo Mauri, Florian Sikora, Italo Zoppis", "title": "Covering with Clubs: Complexity and Approximability", "comments": "Accepted in IWOCA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding cohesive subgraphs in a network is a well-known problem in graph\ntheory. Several alternative formulations of cohesive subgraph have been\nproposed, a notable example being $s$-club, which is a subgraph where each\nvertex is at distance at most $s$ to the others. Here we consider the problem\nof covering a given graph with the minimum number of $s$-clubs. We study the\ncomputational and approximation complexity of this problem, when $s$ is equal\nto 2 or 3. First, we show that deciding if there exists a cover of a graph with\nthree $2$-clubs is NP-complete, and that deciding if there exists a cover of a\ngraph with two $3$-clubs is NP-complete. Then, we consider the approximation\ncomplexity of covering a graph with the minimum number of $2$-clubs and\n$3$-clubs. We show that, given a graph $G=(V,E)$ to be covered, covering $G$\nwith the minimum number of $2$-clubs is not approximable within factor\n$O(|V|^{1/2 -\\varepsilon})$, for any $\\varepsilon>0$, and covering $G$ with the\nminimum number of $3$-clubs is not approximable within factor $O(|V|^{1\n-\\varepsilon})$, for any $\\varepsilon>0$. On the positive side, we give an\napproximation algorithm of factor $2|V|^{1/2}\\log^{3/2} |V|$ for covering a\ngraph with the minimum number of $2$-clubs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:51:29 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Dondi", "Riccardo", ""], ["Mauri", "Giancarlo", ""], ["Sikora", "Florian", ""], ["Zoppis", "Italo", ""]]}, {"id": "1806.01217", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Alal Eran, Yuan Luo", "title": "Efficient Genomic Interval Queries Using Augmented Range Trees", "comments": "4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient large-scale annotation of genomic intervals is essential for\npersonal genome interpretation in the realm of precision medicine. There are 13\npossible relations between two intervals according to Allen's interval algebra.\nConventional interval trees are routinely used to identify the genomic\nintervals satisfying a coarse relation with a query interval, but cannot\nsupport efficient query for more refined relations such as all Allen's\nrelations. We design and implement a novel approach to address this unmet need.\nThrough rewriting Allen's interval relations, we transform an interval query to\na range query, then adapt and utilize the range trees for querying. We\nimplement two types of range trees: a basic 2-dimensional range tree (2D-RT)\nand an augmented range tree with fractional cascading (RTFC) and compare them\nwith the conventional interval tree (IT). Theoretical analysis shows that RTFC\ncan achieve the best time complexity for interval queries regarding all Allen's\nrelations among the three trees. We also perform comparative experiments on the\nefficiency of RTFC, 2D-RT and IT in querying noncoding element annotations in a\nlarge collection of personal genomes. Our experimental results show that 2D-RT\nis more efficient than IT for interval queries regarding most of Allen's\nrelations, RTFC is even more efficient than 2D-RT. The results demonstrate that\nRTFC is an efficient data structure for querying large-scale datasets regarding\nAllen's relations between genomic intervals, such as those required by\ninterpreting genome-wide variation in large populations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:59:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Mao", "Chengsheng", ""], ["Eran", "Alal", ""], ["Luo", "Yuan", ""]]}, {"id": "1806.01305", "submitter": "Arman Zaribafiyan", "authors": "Takeshi Yamazaki, Shunji Matsuura, Ali Narimani, Anushervon\n  Saidmuradov, Arman Zaribafiyan", "title": "Towards the Practical Application of Near-Term Quantum Computers in\n  Quantum Chemistry Simulations: A Problem Decomposition Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of establishing a framework to efficiently perform the practical\napplication of quantum chemistry simulation on near-term quantum devices, we\nenvision a hybrid quantum--classical framework for leveraging problem\ndecomposition (PD) techniques in quantum chemistry. Specifically, we use PD\ntechniques to decompose a target molecular system into smaller subsystems\nrequiring fewer computational resources. In our framework, there are two levels\nof hybridization. At the first level, we use a classical algorithm to decompose\na target molecule into subsystems, and utilize a quantum algorithm to simulate\nthe quantum nature of the subsystems. The second level is in the quantum\nalgorithm. We consider the quantum--classical variational algorithm that\niterates between an expectation estimation using a quantum device and a\nparameter optimization using a classical device. We investigate three popular\nPD techniques for our hybrid approach: the fragment molecular-orbital (FMO)\nmethod, the divide-and-conquer (DC) technique, and the density matrix embedding\ntheory (DMET). We examine the efficacy of these techniques in correctly\ndifferentiating conformations of simple alkane molecules. In particular, we\nconsider the ratio between the number of qubits for PD and that of the full\nsystem; the mean absolute deviation; and the Pearson correlation coefficient\nand Spearman's rank correlation coefficient. Sampling error is introduced when\nexpectation values are measured on the quantum device. Therefore, we study how\nthis error affects the predictive performance of PD techniques. The present\nstudy is our first step to opening up the possibility of using quantum\nchemistry simulations at a scale close to the size of molecules relevant to\nindustry on near-term quantum hardware.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:11:59 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Yamazaki", "Takeshi", ""], ["Matsuura", "Shunji", ""], ["Narimani", "Ali", ""], ["Saidmuradov", "Anushervon", ""], ["Zaribafiyan", "Arman", ""]]}, {"id": "1806.01374", "submitter": "Sathish Gopalakrishnan", "authors": "Sathish Gopalakrishnan", "title": "Improving rewards in overloaded real-time systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive analysis of online algorithms has commonly been applied to\nunderstand the behaviour of real-time systems during overload conditions. While\ncompetitive analysis provides insight into the behaviour of certain algorithms,\nit is hard to make inferences about the performance of those algorithms in\npractice. Other approaches to dealing with overload resort to heuristics that\nseem to perform well but are hard to prove as being good. Further, most work on\nhandling overload in real-time systems does not consider using information\nregarding the distribution of arrival rates of jobs and execution times to make\nscheduling decisions. We present an scheduling policy (obtained through\nstochastic approximation, and using information about the workload) to handle\noverload in real-time systems and improve the revenue earned when each\nsuccessful job completion results in revenue accrual. We prove that the policy\nwe outline does lead to increased revenue when compared to a class of\nscheduling policies that make static resource allocations to different service\nclasses. We also use empirical evidence to underscore the fact that this policy\nperforms better than a variety of other scheduling policies. The ideas\npresented can be applied to several soft real-time systems, specifically\nsystems with multiple service classes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:28:01 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gopalakrishnan", "Sathish", ""]]}, {"id": "1806.01469", "submitter": "Andr\\'e Vignatti", "authors": "Santiago Viertel, Andr\\'e Lu\\'is Vignatti", "title": "Labeling Algorithm and Compact Routing Scheme for a Small World Network\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines the toroidal small world labeling problem that asks for a\nlabeling of the vertices of a network such that the labels possess information\nthat allows a compact routing scheme in the network. We consider the problem\nover a small world network model we propose. Both the model and the compact\nrouting scheme have applications in peer-to-peer networks. The proposed model\nis based on the model of Kleinberg (2000), and generates an undirected\ntwo-dimensional torus with one random long-range edge per vertex. These random\nedges create forbidden cycles that mimic the underlying torus topology, and\nthis behavior confuses attempts for extracting the routing information from the\nnetwork. We show that such forbidden cycles happen with small probability,\nallowing us to use a breadth-first search that finds the vertices positions on\nthe torus. The positions are pairs of integer numbers that provides routing\ninformation to a greedy routing algorithm that finds small paths of the\nnetwork. We present a linear time labeling algorithm that detects and removes\nthe random edges, finds the underlying torus and labels almost all vertices\nthrough a breadth-first search. The labeling algorithm is then used by a\ncompact routing scheme for the proposed small world model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 02:44:37 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 21:40:37 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Viertel", "Santiago", ""], ["Vignatti", "Andr\u00e9 Lu\u00eds", ""]]}, {"id": "1806.01504", "submitter": "Rushabh Jitendrakumar Shah", "authors": "Rushabh Jitendrakumar Shah", "title": "Graph Compression Using Pattern Matching Techniques", "comments": "The paper is mainly about compression can be achieved in a adjacency\n  matrix used to store a graph", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs can be used to represent a wide variety of data belonging to different\ndomains. Graphs can capture the relationship among data in an efficient way,\nand have been widely used. In recent times, with the advent of Big Data, there\nhas been a need to store and compute on large data sets efficiently. However,\nconsidering the size of the data sets in question, finding optimal methods to\nstore and process the data has been a challenge. Therefore, in this paper, we\nstudy different graph compression techniques and propose novel algorithms to do\nthe same. Specifically, given a graph G = (V, E), where V is the set of\nvertices and E is the set of edges, and |V| = n, we propose techniques to\ncompress the adjacency matrix representation of the graph. Our algorithms are\nbased on finding patterns within the adjacency matrix data, and replacing the\ncommon patterns with specific markers. All the techniques proposed here are\nlossless compression of graphs. Based on the experimental results, it is\nobserved that our proposed techniques achieve almost 70% compression as\ncompared to adjacency matrix representation. The results show that large graphs\ncan be efficiently stored in smaller memory and exploit the parallel processing\npower of compute nodes as well as efficiently transfer data between resources.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 05:39:26 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Shah", "Rushabh Jitendrakumar", ""]]}, {"id": "1806.01581", "submitter": "Michele Zito", "authors": "Pavan Sangha, Prudence W. H. Wong, Michele Zito", "title": "Dynamic Programming Optimization in Line of Sight Networks", "comments": "18 pages, 6 figures, submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line of Sight (LoS) networks were designed to model wireless communication in\nsettings which may contain obstacles restricting node visibility. For fixed\npositive integer $d$, and positive integer $\\omega$, a graph $G=(V,E)$ is a\n($d$-dimensional) LoS network with range parameter $\\omega$ if it can be\nembedded in a cube of side size $n$ of the $d$-dimensional integer grid so that\neach pair of vertices in $V$ are adjacent if and only if their embedding\ncoordinates differ only in one position and such difference is less than\n$\\omega$.\n  In this paper we investigate a dynamic programming (DP) approach which can be\nused to obtain efficient algorithmic solutions for various combinatorial\nproblems in LoS networks. In particular DP solves the Maximum Independent Set\n(MIS) problem in LoS networks optimally for any $\\omega$ on {\\em narrow} LoS\nnetworks (i.e. networks which can be embedded in a $n \\times k \\times k \\ldots\n\\times k$ region, for some fixed $k$ independent of $n$). In the unrestricted\ncase it has been shown that the MIS problem is NP-hard when $ \\omega > 2$ (the\nhardness proof goes through for any $\\omega=O(n^{1-\\delta})$, for fixed\n$0<\\delta<1$). We describe how DP can be used as a building block in the design\nof good approximation algorithms. In particular we present a 2-approximation\nalgorithm and a fast polynomial time approximation scheme for the MIS problem\nin arbitrary $d$-dimensional LoS networks. Finally we comment on how the\napproach can be adapted to solve a number of important optimization problems in\nLoS networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 09:38:34 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Sangha", "Pavan", ""], ["Wong", "Prudence W. H.", ""], ["Zito", "Michele", ""]]}, {"id": "1806.01667", "submitter": "Johan Van Rooij PhD", "authors": "Johan M. M. van Rooij, Hans L. Bodlaender, Erik Jan van Leeuwen, Peter\n  Rossmanith and Martin Vatshelle", "title": "Fast Dynamic Programming on Graph Decompositions", "comments": "Preliminary parts of this paper have appeared under the title\n  `Dynamic Programming on Tree Decompositions Using Generalised Fast Subset\n  Convolution' on the 17th Annual European Symposium on Algorithms (ESA 2009),\n  and under the title `Faster Algorithms on Branch and Clique Decompositions'\n  on the 35th International Symposium Mathematical Foundations of Computer\n  Science (MFCS 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider tree decompositions, branch decompositions, and\nclique decompositions. We improve the running time of dynamic programming\nalgorithms on these graph decompositions for a large number of problems as a\nfunction of the treewidth, branchwidth, or cliquewidth, respectively.\n  On tree decompositions of width $k$, we improve the running time for\nDominating Set to $O(3^k)$. We generalise this result to\n$[\\rho,\\sigma]$-domination problems with finite or cofinite $\\rho$ and\n$\\sigma$. For these problems, we give $O(s^k)$-time algorithms, where $s$ is\nthe number of `states' a vertex can have in a standard dynamic programming\nalgorithm for such a problems. Furthermore, we give an $O(2^k)$-time algorithm\nfor counting the number of perfect matchings in a graph, and generalise this to\n$O(2^k)$-time algorithms for many clique covering, packing, and partitioning\nproblems. On branch decompositions of width $k$, we give an\n$O(3^{\\frac{\\omega}{2}k})$-time algorithm for Dominating Set, an\n$O(2^{\\frac{\\omega}{2}k})$-time algorithm for counting the number of perfect\nmatchings, and $O(s^{\\frac{\\omega}{2}k})$-time algorithms for\n$[\\rho,\\sigma]$-domination problems involving $s$ states with finite or\ncofinite $\\rho$ and $\\sigma$. Finally, on clique decompositions of width $k$,\nwe give $O(4^k)$-time algorithms for Dominating Set, Independent Dominating\nSet, and Total Dominating Set.\n  The main techniques used in this paper are a generalisation of fast subset\nconvolution, as introduced by Bj\\\"orklund et al., now applied in the setting of\ngraph decompositions and augmented such that multiple states and multiple ranks\ncan be used. Recently, Lokshtanov et al. have shown that some of the algorithms\nobtained in this paper have running times in which the base in the exponents is\noptimal, unless the Strong Exponential-Time Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 13:02:35 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["van Rooij", "Johan M. M.", ""], ["Bodlaender", "Hans L.", ""], ["van Leeuwen", "Erik Jan", ""], ["Rossmanith", "Peter", ""], ["Vatshelle", "Martin", ""]]}, {"id": "1806.01799", "submitter": "Maciej Besta", "authors": "Maciej Besta, Torsten Hoefler", "title": "Survey and Taxonomy of Lossless Graph Compression and Space-Efficient\n  Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various graphs such as web or social networks may contain up to trillions of\nedges. Compressing such datasets can accelerate graph processing by reducing\nthe amount of I/O accesses and the pressure on the memory subsystem. Yet,\nselecting a proper compression method is challenging as there exist a plethora\nof techniques, algorithms, domains, and approaches in compressing graphs. To\nfacilitate this, we present a survey and taxonomy on lossless graph compression\nthat is the first, to the best of our knowledge, to exhaustively analyze this\ndomain. Moreover, our survey does not only categorize existing schemes, but\nalso explains key ideas, discusses formal underpinning in selected works, and\ndescribes the space of the existing compression schemes using three dimensions:\nareas of research (e.g., compressing web graphs), techniques (e.g., gap\nencoding), and features (e.g., whether or not a given scheme targets dynamic\ngraphs). Our survey can be used as a guide to select the best lossless\ncompression scheme in a given setting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:41:07 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 17:00:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1806.01804", "submitter": "Gonzalo Navarro", "authors": "Travis Gagie, Meng He, Gonzalo Navarro", "title": "Tree Path Majority Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first solution to $\\tau$-majorities on tree paths. Given a\ntree of $n$ nodes, each with a label from $[1..\\sigma]$, and a fixed threshold\n$0<\\tau<1$, such a query gives two nodes $u$ and $v$ and asks for all the\nlabels that appear more than $\\tau \\cdot |P_{uv}|$ times in the path $P_{uv}$\nfrom $u$ to $v$, where $|P_{uv}|$ denotes the number of nodes in $P_{uv}$. Note\nthat the answer to any query is of size up to $1/\\tau$. On a $w$-bit RAM, we\nobtain a linear-space data structure with $O((1/\\tau)\\log^* n \\log\\log_w\n\\sigma)$ query time. For any $\\kappa > 1$, we can also build a structure that\nuses $O(n\\log^{[\\kappa]} n)$ space, where $\\log^{[\\kappa]} n$ denotes the\nfunction that applies logarithm $\\kappa$ times to $n$, and answers queries in\ntime $O((1/\\tau)\\log\\log_w \\sigma)$. The construction time of both structures\nis $O(n\\log n)$. We also describe two succinct-space solutions with the same\nquery time of the linear-space structure. One uses $2nH + 4n + o(n)(H+1)$ bits,\nwhere $H \\le \\lg\\sigma$ is the entropy of the label distribution, and can be\nbuilt in $O(n\\log n)$ time. The other uses $nH + O(n) + o(nH)$ bits and is\nbuilt in $O(n\\log n)$ time w.h.p.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 16:53:02 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 20:29:19 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Gagie", "Travis", ""], ["He", "Meng", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1806.02004", "submitter": "Udi Wieder", "authors": "Udi Wieder", "title": "Another Proof of Cuckoo hashing with New Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a new proof for the load of obtained by a Cuckoo Hashing data\nstructure. Our proof is arguably simpler than previous proofs and allows for\nnew generalizations. The proof first appeared in Pinkas et. al. \\cite{PSWW19}\nin the context of a protocol for private set intersection. We present it here\nseparately to improve its readability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 04:39:41 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Wieder", "Udi", ""]]}, {"id": "1806.02112", "submitter": "J. A. Gregor Lagodzinski", "authors": "Benjamin Doerr, Timo K\\\"otzing, J. A. Gregor Lagodzinski and Johannes\n  Lengler", "title": "Bounding Bloat in Genetic Programming", "comments": "An extended abstract has been published at GECCO 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many optimization problems work with a fixed number of decision\nvariables and thus a fixed-length representation of possible solutions, genetic\nprogramming (GP) works on variable-length representations. A naturally\noccurring problem is that of bloat (unnecessary growth of solutions) slowing\ndown optimization. Theoretical analyses could so far not bound bloat and\nrequired explicit assumptions on the magnitude of bloat. In this paper we\nanalyze bloat in mutation-based genetic programming for the two test functions\nORDER and MAJORITY. We overcome previous assumptions on the magnitude of bloat\nand give matching or close-to-matching upper and lower bounds for the expected\noptimization time. In particular, we show that the (1+1) GP takes (i)\n$\\Theta(T_{init} + n \\log n)$ iterations with bloat control on ORDER as well as\nMAJORITY; and (ii) $O(T_{init} \\log T_{init} + n (\\log n)^3)$ and\n$\\Omega(T_{init} + n \\log n)$ (and $\\Omega(T_{init} \\log T_{init})$ for $n=1$)\niterations without bloat control on MAJORITY.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 10:51:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Doerr", "Benjamin", ""], ["K\u00f6tzing", "Timo", ""], ["Lagodzinski", "J. A. Gregor", ""], ["Lengler", "Johannes", ""]]}, {"id": "1806.02207", "submitter": "Xiaowei Wu", "authors": "Zhiyi Huang, Ning Kang, Zhihao Gavin Tang, Xiaowei Wu and Yuhao Zhang", "title": "Online Makespan Minimization: The Power of Restart", "comments": "36 pages, 6 figures, to appear in APPROX 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online makespan minimization problem on identical machines.\nChen and Vestjens (ORL 1997) show that the largest processing time first (LPT)\nalgorithm is 1.5-competitive. For the special case of two machines, Noga and\nSeiden (TCS 2001) introduce the SLEEPY algorithm that achieves a competitive\nratio of $(5 - \\sqrt{5})/2 \\approx 1.382$, matching the lower bound by Chen and\nVestjens (ORL 1997). Furthermore, Noga and Seiden note that in many\napplications one can kill a job and restart it later, and they leave an open\nproblem whether algorithms with restart can obtain better competitive ratios.\n  We resolve this long-standing open problem on the positive end. Our algorithm\nhas a natural rule for killing a processing job: a newly-arrived job replaces\nthe smallest processing job if 1) the new job is larger than other pending\njobs, 2) the new job is much larger than the processing one, and 3) the\nprocessed portion is small relative to the size of the new job. With\nappropriate choice of parameters, we show that our algorithm improves the 1.5\ncompetitive ratio for the general case, and the 1.382 competitive ratio for the\ntwo-machine case.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:19:52 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Huang", "Zhiyi", ""], ["Kang", "Ning", ""], ["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""]]}, {"id": "1806.02326", "submitter": "Brendan Juba", "authors": "Diego Calderon, Brendan Juba, Sirui Li, Zongyi Li, Lisa Ruan", "title": "Conditional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in machine learning and statistics commonly focuses on building models\nthat capture the vast majority of data, possibly ignoring a segment of the\npopulation as outliers. However, there does not often exist a good model on the\nwhole dataset, so we seek to find a small subset where there exists a useful\nmodel. We are interested in finding a linear rule capable of achieving more\naccurate predictions for just a segment of the population. We give an efficient\nalgorithm with theoretical analysis for the conditional linear regression task,\nwhich is the joint task of identifying a significant segment of the population,\ndescribed by a k-DNF, along with its linear regression fit.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:46:30 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 21:25:30 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Calderon", "Diego", ""], ["Juba", "Brendan", ""], ["Li", "Sirui", ""], ["Li", "Zongyi", ""], ["Ruan", "Lisa", ""]]}, {"id": "1806.02329", "submitter": "Aaron Roth", "authors": "Seth Neel, Aaron Roth", "title": "Mitigating Bias in Adaptive Data Gathering via Differential Privacy", "comments": "Conference version appears in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data that is gathered adaptively --- via bandit algorithms, for example ---\nexhibits bias. This is true both when gathering simple numeric valued data ---\nthe empirical means kept track of by stochastic bandit algorithms are biased\ndownwards --- and when gathering more complicated data --- running hypothesis\ntests on complex data gathered via contextual bandit algorithms leads to false\ndiscovery. In this paper, we show that this problem is mitigated if the data\ncollection procedure is differentially private. This lets us both bound the\nbias of simple numeric valued quantities (like the empirical means of\nstochastic bandit algorithms), and correct the p-values of hypothesis tests run\non the adaptively gathered data. Moreover, there exist differentially private\nbandit algorithms with near optimal regret bounds: we apply existing theorems\nin the simple stochastic case, and give a new analysis for linear contextual\nbandits. We complement our theoretical results with experiments validating our\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:54:07 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Neel", "Seth", ""], ["Roth", "Aaron", ""]]}, {"id": "1806.02718", "submitter": "Gabriele Fici", "authors": "Panagiotis Charalampopoulos, Maxime Crochemore, Gabriele Fici, Robert\n  Mercas, Solon P. Pissis", "title": "Alignment-free sequence comparison using absent words", "comments": "Extended version of \"Linear-Time Sequence Comparison Using Minimal\n  Absent Words & Applications\" Proc. LATIN 2016, arxiv:1506.04917", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence comparison is a prerequisite to virtually all comparative genomic\nanalyses. It is often realised by sequence alignment techniques, which are\ncomputationally expensive. This has led to increased research into\nalignment-free techniques, which are based on measures referring to the\ncomposition of sequences in terms of their constituent patterns. These\nmeasures, such as $q$-gram distance, are usually computed in time linear with\nrespect to the length of the sequences. In this paper, we focus on the\ncomplementary idea: how two sequences can be efficiently compared based on\ninformation that does not occur in the sequences. A word is an {\\em absent\nword} of some sequence if it does not occur in the sequence. An absent word is\n{\\em minimal} if all its proper factors occur in the sequence. Here we present\nthe first linear-time and linear-space algorithm to compare two sequences by\nconsidering {\\em all} their minimal absent words. In the process, we present\nresults of combinatorial interest, and also extend the proposed techniques to\ncompare circular sequences. We also present an algorithm that, given a word $x$\nof length $n$, computes the largest integer for which all factors of $x$ of\nthat length occur in some minimal absent word of $x$ in time and space\n$\\cO(n)$. Finally, we show that the known asymptotic upper bound on the number\nof minimal absent words of a word is tight.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:03:00 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Crochemore", "Maxime", ""], ["Fici", "Gabriele", ""], ["Mercas", "Robert", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1806.02771", "submitter": "Quanquan C. Liu", "authors": "Erik D. Demaine, Timothy D. Goodrich, Kyle Kloster, Brian Lavallee,\n  Quanquan C. Liu, Blair D. Sullivan, Ali Vakilian, Andrew van der Poel", "title": "Structural Rounding: Approximation Algorithms for Graphs Near an\n  Algorithmically Tractable Class", "comments": "72 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new framework for generalizing approximation algorithms from the\nstructural graph algorithm literature so that they apply to graphs somewhat\nclose to that class (a scenario we expect is common when working with\nreal-world networks) while still guaranteeing approximation ratios. The idea is\nto $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph\ninto an algorithmically tractable class, apply known approximation algorithms\nfor that class, and then $\\textit{lift}$ the solution to apply to the original\ngraph. We give a general characterization of when an optimization problem is\namenable to this approach, and show that it includes many well-studied graph\nproblems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum\nMaximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge\n($\\ell$-)Dominating Set, and Connected Dominating Set.\n  To enable this framework, we develop new editing algorithms that find the\napproximately-fewest edits required to bring a given graph into one of several\nimportant graph classes (in some cases, also approximating the target parameter\nof the family). For bounded degeneracy, we obtain a bicriteria\n$(4,4)$-approximation which also extends to a smoother bicriteria trade-off.\nFor bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log\nw}))$-approximation, and for bounded pathwidth, we obtain a bicriteria\n$(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth\n$2$ (also related to bounded expansion), we obtain a $4$-approximation. We also\nprove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq\n\\mathrm{NP}$: in particular, these problems are all log-factor inapproximable,\nexcept the last which is not approximable below some constant factor ($2$\nassuming UGC).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 16:39:17 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 19:39:42 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 22:36:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Demaine", "Erik D.", ""], ["Goodrich", "Timothy D.", ""], ["Kloster", "Kyle", ""], ["Lavallee", "Brian", ""], ["Liu", "Quanquan C.", ""], ["Sullivan", "Blair D.", ""], ["Vakilian", "Ali", ""], ["van der Poel", "Andrew", ""]]}, {"id": "1806.02815", "submitter": "Ehsan Kazemi", "authors": "Marko Mitrovic and Ehsan Kazemi and Morteza Zadimoghaddam and Amin\n  Karbasi", "title": "Data Summarization at Scale: A Two-Stage Submodular Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer scale of modern datasets has resulted in a dire need for\nsummarization techniques that identify representative elements in a dataset.\nFortunately, the vast majority of data summarization tasks satisfy an intuitive\ndiminishing returns condition known as submodularity, which allows us to find\nnearly-optimal solutions in linear time. We focus on a two-stage submodular\nframework where the goal is to use some given training functions to reduce the\nground set so that optimizing new functions (drawn from the same distribution)\nover the reduced set provides almost as much value as optimizing them over the\nentire ground set. In this paper, we develop the first streaming and\ndistributed solutions to this problem. In addition to providing strong\ntheoretical guarantees, we demonstrate both the utility and efficiency of our\nalgorithms on real-world tasks including image summarization and ride-share\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:50:24 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Mitrovic", "Marko", ""], ["Kazemi", "Ehsan", ""], ["Zadimoghaddam", "Morteza", ""], ["Karbasi", "Amin", ""]]}, {"id": "1806.02894", "submitter": "Xi Chen", "authors": "Xi Chen, Tengyu Ma, Jiawei Zhang, Yuan Zhou", "title": "Optimal Design of Process Flexibility for General Production Systems", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process flexibility is widely adopted as an effective strategy for responding\nto uncertain demand. Many algorithms for constructing sparse flexibility\ndesigns with good theoretical guarantees have been developed for balanced and\nsymmetrical production systems. These systems assume that the number of plants\nequals the number of products, that supplies have the same capacity, and that\ndemands are independently and identically distributed. In this paper, we relax\nthese assumptions and consider a general class of production systems. We\nconstruct a simple flexibility design to fulfill $(1-\\epsilon)$-fraction of\nexpected demand with high probability (w.h.p.) where the average degree is\n$O(\\ln(1/\\epsilon))$. To motivate our construction, we first consider a natural\nweighted probabilistic construction from Chou et al. (2011) where the degree of\neach node is proportional to its expected capacity. However, this strategy is\nshown to be sub-optimal. To obtain an optimal construction, we develop a simple\nyet effective thresholding scheme. The analysis of our approach extends the\nclassical analysis of expander graphs by overcoming several technical\ndifficulties. Our approach may prove useful in other applications that require\nexpansion properties of graphs with non-uniform degree sequences.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:52:31 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Chen", "Xi", ""], ["Ma", "Tengyu", ""], ["Zhang", "Jiawei", ""], ["Zhou", "Yuan", ""]]}, {"id": "1806.02972", "submitter": "Varun Shankar", "authors": "Varun Shankar, Robert M. Kirby, and Aaron L. Fogelson", "title": "Robust Node Generation for Meshfree Discretizations on Irregular Domains\n  and Surfaces", "comments": "26 pages, 9 figures, accepted SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for the automatic one-shot generation of scattered\nnode sets on irregular 2D and 3D domains using Poisson disk sampling coupled to\nnovel parameter-free, high-order parametric Spherical Radial Basis Function\n(SBF)-based geometric modeling of irregular domain boundaries. Our algorithm\nalso automatically modifies the scattered node sets locally for time-varying\nembedded boundaries in the domain interior. We derive complexity estimates for\nour node generator in 2D and 3D that establish its scalability, and verify\nthese estimates with timing experiments. We explore the influence of Poisson\ndisk sampling parameters on both quasi-uniformity in the node sets and errors\nin an RBF-FD discretization of the heat equation. In all cases, our framework\nrequires only a small number of \"seed\" nodes on domain boundaries. The entire\nframework exhibits O(N) complexity in both 2D and 3D.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 05:12:57 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shankar", "Varun", ""], ["Kirby", "Robert M.", ""], ["Fogelson", "Aaron L.", ""]]}, {"id": "1806.03102", "submitter": "Mikko Berggren Ettienne", "authors": "Philip Bille and Mikko Berggreen Ettienne and Roberto Grossi and Inge\n  Li G{\\o}rtz and Eva Rotenberg", "title": "Compressed Communication Complexity of Longest Common Prefixes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the communication complexity of fundamental longest common prefix\n(Lcp) problems. In the simplest version, two parties, Alice and Bob, each hold\na string, $A$ and $B$, and we want to determine the length of their longest\ncommon prefix $l=\\text{Lcp}(A,B)$ using as few rounds and bits of communication\nas possible. We show that if the longest common prefix of $A$ and $B$ is\ncompressible, then we can significantly reduce the number of rounds compared to\nthe optimal uncompressed protocol, while achieving the same (or fewer) bits of\ncommunication. Namely, if the longest common prefix has an LZ77 parse of $z$\nphrases, only $O(\\lg z)$ rounds and $O(\\lg \\ell)$ total communication is\nnecessary.\n  We extend the result to the natural case when Bob holds a set of strings\n$B_1, \\ldots, B_k$, and the goal is to find the length of the maximal longest\nprefix shared by $A$ and any of $B_1, \\ldots, B_k$. Here, we give a protocol\nwith $O(\\log z)$ rounds and $O(\\lg z \\lg k + \\lg \\ell)$ total communication.\n  We present our result in the public-coin model of computation but by a\nstandard technique our results generalize to the private-coin model.\nFurthermore, if we view the input strings as integers the problems are the\ngreater-than problem and the predecessor problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 11:58:40 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Bille", "Philip", ""], ["Ettienne", "Mikko Berggreen", ""], ["Grossi", "Roberto", ""], ["G\u00f8rtz", "Inge Li", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1806.03190", "submitter": "Yuanzhi Li", "authors": "Yuanzhi Li, Yoram Singer", "title": "The Well Tempered Lasso", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the entire regularization path for least squares\nregression with 1-norm penalty, known as the Lasso. Every regression parameter\nin the Lasso changes linearly as a function of the regularization value. The\nnumber of changes is regarded as the Lasso's complexity. Experimental results\nusing exact path following exhibit polynomial complexity of the Lasso in the\nproblem size. Alas, the path complexity of the Lasso on artificially designed\nregression problems is exponential.\n  We use smoothed analysis as a mechanism for bridging the gap between worst\ncase settings and the de facto low complexity. Our analysis assumes that the\nobserved data has a tiny amount of intrinsic noise. We then prove that the\nLasso's complexity is polynomial in the problem size. While building upon the\nseminal work of Spielman and Teng on smoothed complexity, our analysis is\nmorally different as it is divorced from specific path following algorithms. We\nverify the validity of our analysis in experiments with both worst case\nsettings and real datasets. The empirical results we obtain closely match our\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 14:34:26 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Li", "Yuanzhi", ""], ["Singer", "Yoram", ""]]}, {"id": "1806.03220", "submitter": "Anirudh Subramanyam", "authors": "Anirudh Subramanyam, Akang Wang, Chrysanthos E. Gounaris", "title": "A Scenario Decomposition Algorithm for Strategic Time Window Assignment\n  Vehicle Routing Problems", "comments": "45 pages, 10 figures, 8 tables", "journal-ref": "Transportation Research Part B: Methodological, 117:296-317, 2018", "doi": "10.1016/j.trb.2018.09.008", "report-no": null, "categories": "math.OC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the strategic decision-making problem of assigning time windows to\ncustomers in the context of vehicle routing applications that are affected by\noperational uncertainty. This problem, known as the Time Window Assignment\nVehicle Routing Problem, can be viewed as a two-stage stochastic optimization\nproblem, where time window assignments constitute first-stage decisions,\nvehicle routes adhering to the assigned time windows constitute second-stage\ndecisions, and the objective is to minimize the expected routing costs. We\nprove that a sampled deterministic equivalent of this stochastic model can be\nreduced to a variant of the Consistent Vehicle Routing Problem, and we leverage\nthis result to develop a new scenario decomposition algorithm to solve it. From\na modeling viewpoint, our approach can accommodate both continuous and discrete\nsets of feasible time window assignments as well as general scenario-based\nmodels of uncertainty for several routing-specific parameters, including\ncustomer demands and travel times, among others. From an algorithmic viewpoint,\nour approach can be easily parallelized, can utilize any available vehicle\nrouting solver as a black box, and can be readily modified as a heuristic for\nlarge-scale instances. We perform a comprehensive computational study to\ndemonstrate that our algorithm strongly outperforms all existing solution\nmethods, as well as to quantify the trade-off between computational\ntractability and expected cost savings when considering a larger number of\nfuture scenarios during strategic time window assignment.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:29:32 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 21:43:54 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Subramanyam", "Anirudh", ""], ["Wang", "Akang", ""], ["Gounaris", "Chrysanthos E.", ""]]}, {"id": "1806.03365", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Magn\\'us M. Halld\\'orsson and Calvin Newport", "title": "Distributed Algorithms for Minimum Degree Spanning Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum degree spanning tree (MDST) problem requires the construction of\na spanning tree $T$ for graph $G=(V,E)$ with $n$ vertices, such that the\nmaximum degree $d$ of $T$ is the smallest among all spanning trees of $G$. In\nthis paper, we present two new distributed approximation algorithms for the\nMDST problem. Our first result is a randomized distributed algorithm that\nconstructs a spanning tree of maximum degree $\\hat d = O(d\\log{n})$. It\nrequires $O((D + \\sqrt{n}) \\log^2 n)$ rounds (w.h.p.), where $D$ is the graph\ndiameter, which matches (within log factors) the optimal round complexity for\nthe related minimum spanning tree problem. Our second result refines this\napproximation factor by constructing a tree with maximum degree $\\hat d = O(d +\n\\log{n})$, though at the cost of additional polylogarithmic factors in the\nround complexity. Although efficient approximation algorithms for the MDST\nproblem have been known in the sequential setting since the 1990's, our results\nare first efficient distributed solutions for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 22:21:53 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Dinitz", "Michael", ""], ["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Newport", "Calvin", ""]]}, {"id": "1806.03569", "submitter": "Zhao Yu", "authors": "Ryan O'Donnell and Yu Zhao", "title": "On closeness to k-wise uniformity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probability distribution over {-1, 1}^n is (eps, k)-wise uniform if,\nroughly, it is eps-close to the uniform distribution when restricted to any k\ncoordinates. We consider the problem of how far an (eps, k)-wise uniform\ndistribution can be from any globally k-wise uniform distribution. We show that\nevery (eps, k)-wise uniform distribution is O(n^(k/2) eps)-close to a k-wise\nuniform distribution in total variation distance. In addition, we show that\nthis bound is optimal for all even k: we find an (eps, k)-wise uniform\ndistribution that is Omega(n^(k/2) eps)-far from any k-wise uniform\ndistribution in total variation distance. For k = 1, we get a better upper\nbound of O(eps), which is also optimal.\n  One application of our closeness result is to the sample complexity of\ntesting whether a distribution is k-wise uniform or delta-far from k-wise\nuniform. We give an upper bound of O(n^k/delta^2) (or O(log n/delta^2) when k =\n1) on the required samples. We show an improved upper bound of\nO~(n^(k/2)/delta^2) for the special case of testing fully uniform vs. delta-far\nfrom k-wise uniform. Finally, we complement this with a matching lower bound of\nOmega(n/delta^2) when k = 2.\n  Our results improve upon the best known bounds from [AAK+07], and have\nsimpler proofs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 01:58:35 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Zhao", "Yu", ""]]}, {"id": "1806.03611", "submitter": "Salvatore Pontarelli", "authors": "Pedro Reviriego, Salvatore Pontarelli, Gil Levy", "title": "CuCoTrack: Cuckoo Filter Based Connection Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces CuCoTrack, a cuckoo hash based data structure designed\nto efficiently implement connection tracking. The proposed scheme exploits the\nfact that queries always match one existing connection to compress the 5-tuple\nthat identifies the connection. This reduces significantly the amount of memory\nneeded to store the connections and also the memory bandwidth needed for\nlookups. CuCoTrack uses a dynamic fingerprint to avoid collisions thus ensuring\nthat queries are completed in at most two memory accesses and facilitating a\nhardware implementation. The proposed scheme has been analyzed theoretically\nand validated by simulation. The results show that using 16 bits for the\nfingerprint is enough to avoid collisions in practical configurations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 08:29:30 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Reviriego", "Pedro", ""], ["Pontarelli", "Salvatore", ""], ["Levy", "Gil", ""]]}, {"id": "1806.03701", "submitter": "Shrohan Mohapatra", "authors": "Shrohan Mohapatra", "title": "A new quadratic-time number-theoretic algorithm to solve matrix\n  multiplication problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There have been several algorithms designed to optimise matrix\nmultiplication. From schoolbook method with complexity $O(n^3)$ to advanced\ntensor-based tools with time complexity $O(n^{2.3728639})$ (lowest possible\nbound achieved), a lot of work has been done to reduce the steps used in the\nrecursive version. Some group-theoretic and computer algebraic estimations also\nconjecture the existence of an $O(n^2)$ algorithm. This article discusses a\nquadratic-time number-theoretic approach that converts large vectors in the\noperands to a single large entity and combines them to make the dot-product.\nFor two $n \\times n$ matrices, this dot-product is iteratively used for each\nsuch vector. Preprocessing and computation makes it a quadratic time algorithm\nwith a considerable constant of proportionality. Special strategies for\nintegers, floating point numbers and complex numbers are also discussed, with a\ntheoretical estimation of time and space complexity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 18:24:00 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 05:27:59 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 06:39:11 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 04:28:57 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Mohapatra", "Shrohan", ""]]}, {"id": "1806.03708", "submitter": "Amit Jacob Fanani", "authors": "Yossi Azar, Amit Jacob-Fanani", "title": "Deterministic Min-Cost Matching with Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online Minimum-Cost Perfect Matching with Delays (MPMD)\nproblem introduced by Emek et al. (STOC 2016), in which a general metric space\nis given, and requests are submitted in different times in this space by an\nadversary. The goal is to match requests, while minimizing the sum of distances\nbetween matched pairs in addition to the time intervals passed from the moment\neach request appeared until it is matched.\n  In the online Minimum-Cost Bipartite Perfect Matching with Delays (MBPMD)\nproblem introduced by Ashlagi et al. (APPROX/RANDOM 2017), each request is also\nassociated with one of two classes, and requests can only be matched with\nrequests of the other class.\n  Previous algorithms for the problems mentioned above, include randomized\n$O\\left(\\log n\\right)$-competitive algorithms for known and finite metric\nspaces, $n$ being the size of the metric space, and a deterministic\n$O\\left(m\\right)$-competitive algorithm, $m$ being the number of requests.\n  We introduce\n$O\\left(m^{\\log\\left(\\frac{3}{2}+\\epsilon\\right)}\\right)$-competitive\ndeterministic algorithms for both problems and for any fixed $\\epsilon > 0$. In\nparticular, for a small enough $\\epsilon$ the competitive ratio becomes\n$O\\left(m^{0.59}\\right)$. These are the first deterministic algorithms for the\nmentioned online matching problems, achieving a sub-linear competitive ratio.\nOur algorithms do not need to know the metric space in advance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 19:00:01 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 18:39:43 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Azar", "Yossi", ""], ["Jacob-Fanani", "Amit", ""]]}, {"id": "1806.03814", "submitter": "Sai Sandeep", "authors": "Mordecai J. Golin, Sai Sandeep", "title": "Minmax-Regret $k$-Sink Location on a Dynamic Tree Network with Uniform\n  Capacities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic flow network $G$ with uniform capacity $c$ is a graph in which at\nmost $c$ units of flow can enter an edge in one time unit. If flow enters a\nvertex faster than it can leave, congestion occurs. The evacuation problem is\nto evacuate all flow to sinks assuming that all flow is confluent, i.e., all\nflow passing through a particular vertex must follow the same exit edge. The\n$k$-sink location problem is to place $k$-sinks so as to minimize this\nevacuation time. Although the $k$-sink location problem is NP-Hard on a general\ngraph it can be solved in $\\tilde O(k^2 n)$ time on trees.\n  The concept of minmax-regret arises from robust optimization. For each\nsource, a range of possible flow values is provided and any scenario with flow\nvalues in those ranges might occur. The goal is to find a sink placement that\nminimizes, over all possible scenarios, the difference between the evacuation\ntime to those sinks and the minimal evacuation time of that scenario.\n  The Minmax-Regret $k$-Sink Location on a Dynamic Path Networks with uniform\ncapacities is polynomial solvable in $n$ and $k$. Similarly, the Minmax-Regret\n$k$-center problem on trees is polynomial solvable in $n$ and $k$. Prior to\nthis work, polynomial time solutions to the Minmax-Regret $k$-Sink Location on\nDynamic Tree Networks with uniform capacities were only known for $k=1$. This\npaper solves this problem, for general $k,$ in time $$O\\Bigl( \\max(k^2,\\log\n^2n)\\, k^2 n^2 \\log^5 n\\Bigr).$$\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 05:23:18 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Golin", "Mordecai J.", ""], ["Sandeep", "Sai", ""]]}, {"id": "1806.03936", "submitter": "Muhammad Ahmad", "authors": "Maham Anwar Beg, Muhammad Ahmad, Arif Zaman, Imdadullah Khan", "title": "Scalable Approximation Algorithm for Graph Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive sizes of real-world graphs, such as social networks and web graph,\nimpose serious challenges to process and perform analytics on them. These\nissues can be resolved by working on a small summary of the graph instead . A\nsummary is a compressed version of the graph that removes several details, yet\npreserves it's essential structure. Generally, some predefined quality measure\nof the summary is optimized to bound the approximation error incurred by\nworking on the summary instead of the whole graph. All known summarization\nalgorithms are computationally prohibitive and do not scale to large graphs. In\nthis paper we present an efficient randomized algorithm to compute graph\nsummaries with the goal to minimize reconstruction error. We propose a novel\nweighted sampling scheme to sample vertices for merging that will result in the\nleast reconstruction error. We provide analytical bounds on the running time of\nthe algorithm and prove approximation guarantee for our score computation.\nEfficiency of our algorithm makes it scalable to very large graphs on which\nknown algorithms cannot be applied. We test our algorithm on several real world\ngraphs to empirically demonstrate the quality of summaries produced and compare\nto state of the art algorithms. We use the summaries to answer several\nstructural queries about original graph and report their accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 12:34:48 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Beg", "Maham Anwar", ""], ["Ahmad", "Muhammad", ""], ["Zaman", "Arif", ""], ["Khan", "Imdadullah", ""]]}, {"id": "1806.03988", "submitter": "Celine Scornavacca", "authors": "Riccardo Dondi, Manuel Lafond and Celine Scornavacca", "title": "Reconciling Multiple Genes Trees via Segmental Duplications and Losses", "comments": "23 pages, 7 figures, WABI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconciling gene trees with a species tree is a fundamental problem to\nunderstand the evolution of gene families. Many existing approaches reconcile\neach gene tree independently. However, it is well-known that the evolution of\ngene families is interconnected. In this paper, we extend a previous approach\nto reconcile a set of gene trees with a species tree based on segmental\nmacro-evolutionary events, where segmental duplication events and losses are\nassociated with cost $\\delta$ and $\\lambda$, respectively. We show that the\nproblem is polynomial-time solvable when $\\delta \\leq \\lambda$ (via\nLCA-mapping), while if $\\delta > \\lambda$ the problem is NP-hard, even when\n$\\lambda = 0$ and a single gene tree is given, solving a long standing open\nproblem on the complexity of the reconciliation problem. On the positive side,\nwe give a fixed-parameter algorithm for the problem, where the parameters are\n$\\delta/\\lambda$ and the number $d$ of segmental duplications, of time\ncomplexity $O(\\lceil \\frac{\\delta}{\\lambda} \\rceil^{d} \\cdot n \\cdot\n\\frac{\\delta}{\\lambda})$. Finally, we demonstrate the usefulness of this\nalgorithm on two previously studied real datasets: we first show that our\nmethod can be used to confirm or refute hypothetical segmental duplications on\na set of 16 eukaryotes, then show how we can detect whole genome duplications\nin yeast genomes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 13:57:30 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Dondi", "Riccardo", ""], ["Lafond", "Manuel", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1806.04202", "submitter": "Shalmoli Gupta", "authors": "Chandra Chekuri and Shalmoli Gupta", "title": "Perturbation Resilient Clustering for $k$-Center and Related Problems\n  via LP Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider clustering in the perturbation resilience model that has been\nstudied since the work of Bilu and Linial [ICS, 2010] and Awasthi, Blum and\nSheffet [Inf. Proc. Lett., 2012]. A clustering instance $I$ is said to be\n$\\alpha$-perturbation resilient if the optimal solution does not change when\nthe pairwise distances are modified by a factor of $\\alpha$ and the perturbed\ndistances satisfy the metric property --- this is the metric perturbation\nresilience property introduced in Angelidakis et. al. [STOC, 2010] and a weaker\nrequirement than prior models. We make two high-level contributions.\n  1) We show that the natural LP relaxation of $k$-center and asymmetric\n$k$-center is integral for $2$-perturbation resilient instances. We belive that\ndemonstrating the goodness of standard LP relaxations complements existing\nresults that are based on combinatorial algorithms designed for the\nperturbation model.\n  2) We define a simple new model of perturbation resilience for clustering\nwith \\emph{outliers}. Using this model we show that the unified MST and dynamic\nprogramming based algorithm proposed by Angelidakis et. al. [STOC, 2010]\nexactly solves the clustering with outliers problem for several common center\nbased objectives (like $k$-center, $k$-means, $k$-median) when the instances is\n$2$-perturbation resilient. We further show that a natural LP relxation is\nintegral for $2$-perturbation resilient instances of \\kcenter with outliers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:13:52 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Chekuri", "Chandra", ""], ["Gupta", "Shalmoli", ""]]}, {"id": "1806.04277", "submitter": "J\\'er\\'emy Barbay", "authors": "J\\'er\\'emy Barbay, Andr\\'es Olivares", "title": "Indexed Dynamic Programming to boost Edit Distance and LCSS Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are efficient dynamic programming solutions to the computation of the\nEdit Distance from $S\\in[1..\\sigma]^n$ to $T\\in[1..\\sigma]^m$, for many natural\nsubsets of edit operations, typically in time within $O(nm)$ in the worst-case\nover strings of respective lengths $n$ and $m$ (which is likely to be optimal),\nand in time within $O(n{+}m)$ in some special cases (e.g. disjoint alphabets).\nWe describe how indexing the strings (in linear time), and using such an index\nto refine the recurrence formulas underlying the dynamic programs, yield faster\nalgorithms in a variety of models, on a continuum of classes of instances of\nintermediate difficulty between the worst and the best case, thus refining the\nanalysis beyond the worst case analysis. As a side result, we describe similar\nproperties for the computation of the Longest Common Sub Sequence $LCSS(S,T)$\nbetween $S$ and $T$, since it is a particular case of Edit Distance, and we\ndiscuss the application of similar algorithmic and analysis techniques for\nother dynamic programming solutions. More formally, we propose a parameterized\nanalysis of the computational complexity of the Edit Distance for various set\nof operators and of the Longest Common Sub Sequence in function of the area of\nthe dynamic program matrix relevant to the computation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 00:22:04 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["Olivares", "Andr\u00e9s", ""]]}, {"id": "1806.04307", "submitter": "Kazuhiro Kurita", "authors": "Kazuhiro Kurita, Kunihiro Wasa, Alessio Conte, Hiroki Arimura, Takeaki\n  Uno", "title": "Efficient Enumeration of Subgraphs and Induced Subgraphs with Bounded\n  Girth", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-94667-2_17", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The girth of a graph is the length of its shortest cycle. Due to its\nrelevance in graph theory, network analysis and practical fields such as\ndistributed computing, girth-related problems have been object of attention in\nboth past and recent literature. In this paper, we consider the problem of\nlisting connected subgraphs with bounded girth. As a large girth is index of\nsparsity, this allows to extract sparse structures from the input graph. We\npropose two algorithms, for enumerating respectively vertex induced subgraphs\nand edge induced subgraphs with bounded girth, both running in $O(n)$ amortized\ntime per solution and using $O(n^3)$ space. Furthermore, the algorithms can be\neasily adapted to relax the connectivity requirement and to deal with weighted\ngraphs. As a byproduct, the second algorithm can be used to answer the well\nknown question of finding the densest $n$-vertex graph(s) of girth $k$.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 02:53:18 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Kurita", "Kazuhiro", ""], ["Wasa", "Kunihiro", ""], ["Conte", "Alessio", ""], ["Arimura", "Hiroki", ""], ["Uno", "Takeaki", ""]]}, {"id": "1806.04310", "submitter": "Amirali Aghazadeh", "authors": "Amirali Aghazadeh, Ryan Spring, Daniel LeJeune, Gautam Dasarathy,\n  Anshumali Shrivastava, Richard G. Baraniuk", "title": "MISSION: Ultra Large-Scale Feature Selection using Count-Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 03:03:13 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Aghazadeh", "Amirali", ""], ["Spring", "Ryan", ""], ["LeJeune", "Daniel", ""], ["Dasarathy", "Gautam", ""], ["Shrivastava", "Anshumali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1806.04457", "submitter": "Frank Gurski", "authors": "Frank Gurski, Carolin Rehs", "title": "Computing directed path-width and directed tree-width of recursively\n  defined digraphs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the directed path-width and directed tree-width of\nrecursively defined digraphs. As an important combinatorial tool, we show how\nthe directed path-width and the directed tree-width can be computed for the\ndisjoint union, order composition, directed union, and series composition of\ntwo directed graphs. These results imply the equality of directed path-width\nand directed tree-width for all digraphs which can be defined by these four\noperations. This allows us to show a linear-time solution for computing the\ndirected path-width and directed tree-width of all these digraphs. Since\ndirected co-graphs are precisely those digraphs which can be defined by the\ndisjoint union, order composition, and series composition our results imply the\nequality of directed path-width and directed tree-width for directed co-graphs\nand also a linear-time solution for computing the directed path-width and\ndirected tree-width of directed co-graphs, which generalizes the known results\nfor undirected co-graphs of Bodlaender and Moehring.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:13:47 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gurski", "Frank", ""], ["Rehs", "Carolin", ""]]}, {"id": "1806.04602", "submitter": "Zongchen Chen", "authors": "Antonio Blanca, Zongchen Chen, Eric Vigoda", "title": "Swendsen-Wang Dynamics for General Graphs in the Tree Uniqueness Region", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Swendsen-Wang dynamics is a popular algorithm for sampling from the Gibbs\ndistribution for the ferromagnetic Ising model on a graph $G=(V,E)$. The\ndynamics is a \"global\" Markov chain which is conjectured to converge to\nequilibrium in $O(|V|^{1/4})$ steps for any graph $G$ at any (inverse)\ntemperature $\\beta$. It was recently proved by Guo and Jerrum (2017) that the\nSwendsen-Wang dynamics has polynomial mixing time on any graph at all\ntemperatures, yet there are few results providing $o(|V|)$ upper bounds on its\nconvergence time.\n  We prove fast convergence of the Swendsen-Wang dynamics on general graphs in\nthe tree uniqueness region of the ferromagnetic Ising model. In particular,\nwhen $\\beta < \\beta_c(d)$ where $\\beta_c(d)$ denotes the\nuniqueness/non-uniqueness threshold on infinite $d$-regular trees, we prove\nthat the relaxation time (i.e., the inverse spectral gap) of the Swendsen-Wang\ndynamics is $\\Theta(1)$ on any graph of maximum degree $d \\geq 3$. Our proof\nutilizes a version of the Swendsen-Wang dynamics which only updates isolated\nvertices. We establish that this variant of the Swendsen-Wang dynamics has\nmixing time $O(\\log{|V|})$ and relaxation time $\\Theta(1)$ on any graph of\nmaximum degree $d$ for all $\\beta < \\beta_c(d)$. We believe that this Markov\nchain may be of independent interest, as it is a monotone Swendsen-Wang type\nchain. As part of our proofs, we provide modest extensions of the technology of\nMossel and Sly (2013) for analyzing mixing times and of the censoring result of\nPeres and Winkler (2013). Both of these results are for the Glauber dynamics,\nand we extend them here to general monotone Markov chains. This class of\ndynamics includes for example the heat-bath block dynamics, for which we obtain\nnew tight mixing time bounds.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 15:23:29 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Blanca", "Antonio", ""], ["Chen", "Zongchen", ""], ["Vigoda", "Eric", ""]]}, {"id": "1806.04742", "submitter": "Kshitij Jain", "authors": "Sergio Cabello, Kshitij Jain, Anna Lubiw, Debajyoti Mondal", "title": "Minimum Shared-Power Edge Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a problem called the Minimum Shared-Power Edge Cut (MSPEC). The\ninput to the problem is an undirected edge-weighted graph with distinguished\nvertices s and t, and the goal is to find an s-t cut by assigning \"powers\" at\nthe vertices and removing an edge if the sum of the powers at its endpoints is\nat least its weight. The objective is to minimize the sum of the assigned\npowers.\n  MSPEC is a graph generalization of a barrier coverage problem in a wireless\nsensor network: given a set of unit disks with centers in a rectangle, what is\nthe minimum total amount by which we must shrink the disks to permit an\nintruder to cross the rectangle undetected, i.e. without entering any disc.\nThis is a more sophisticated measure of barrier coverage than the minimum\nnumber of disks whose removal breaks the barrier.\n  We develop a fully polynomial time approximation scheme (FPTAS) for MSPEC. We\ngive polynomial time algorithms for the special cases where the edge weights\nare uniform, or the power values are restricted to a bounded set. Although\nMSPEC is related to network flow and matching problems, its computational\ncomplexity (in P or NP-hard) remains open.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:03:05 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Cabello", "Sergio", ""], ["Jain", "Kshitij", ""], ["Lubiw", "Anna", ""], ["Mondal", "Debajyoti", ""]]}, {"id": "1806.04890", "submitter": "Shunsuke Inenaga", "authors": "Akihiro Nishi, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "$O(n \\log n)$-time text compression by LZ-style longest first\n  substitution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mauer et al. [A Lempel-Ziv-style Compression Method for Repetitive Texts, PSC\n2017] proposed a hybrid text compression method called LZ-LFS which has both\nfeatures of Lempel-Ziv 77 factorization and longest first substitution. They\nshowed that LZ-LFS can achieve better compression ratio for repetitive texts,\ncompared to some state-of-the-art compression algorithms. The drawback of Mauer\net al.'s method is that their LZ-LFS compression algorithm takes $O(n^2)$ time\non an input string of length $n$. In this paper, we show a faster LZ-LFS\ncompression algorithm that works in $O(n \\log n)$ time. We also propose a\nsimpler version of LZ-LFS that can be computed in $O(n)$ time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:30:00 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Nishi", "Akihiro", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1806.05105", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Stability and complexity of mixed discriminants", "comments": "Included an algorithm for computing the sum of powers of principal\n  minors of a matrix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the mixed discriminant of $n$ positive semidefinite $n \\times n$\nreal symmetric matrices can be approximated within a relative error $\\epsilon\n>0$ in quasi-polynomial $n^{O(\\ln n -\\ln \\epsilon)}$ time, provided the\ndistance of each matrix to the identity matrix in the operator norm does not\nexceed some absolute constant $\\gamma_0 >0$. We deduce a similar result for the\nmixed discriminant of doubly stochastic $n$-tuples of matrices from the Marcus\n- Spielman - Srivastava bound on the roots of the mixed characteristic\npolynomial. Finally, we construct a quasi-polynomial algorithm for\napproximating the sum of $m$-th powers of principal minors of a matrix,\nprovided the operator norm of the matrix is strictly less than 1. As is shown\nby Gurvits, for $m=2$ the problem is $\\#P$-hard and covers the problem of\ncomputing the mixed discriminant of positive semidefinite matrices of rank 2.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:17:47 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 22:41:05 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "1806.05523", "submitter": "David Harris", "authors": "Paul Burkhardt, Vance Faber, David G. Harris", "title": "Bounds and algorithms for graph trusses", "comments": null, "journal-ref": "Journal of Graph Algorithms and Applications, 24(3):191-214, 2020", "doi": "10.7155/jgaa.00527", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-truss, introduced by Cohen (2005), is a graph where every edge is\nincident to at least $k$ triangles. This is a relaxation of the clique. It has\nproved to be a useful tool in identifying cohesive subnetworks in a variety of\nreal-world graphs. Despite its simplicity and its utility, the combinatorial\nand algorithmic aspects of trusses have not been thoroughly explored.\n  We provide nearly-tight bounds on the edge counts of $k$-trusses. We also\ngive two improved algorithms for finding trusses in large-scale graphs. First,\nwe present a simplified and faster algorithm, based on approach discussed in\nWang & Cheng (2012). Second, we present a theoretical algorithm based on fast\nmatrix multiplication; this converts a triangle-generation algorithm of\nBjorklund et al. (2014) into a dynamic data structure.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 13:12:31 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 13:28:08 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 13:33:20 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 11:51:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Burkhardt", "Paul", ""], ["Faber", "Vance", ""], ["Harris", "David G.", ""]]}, {"id": "1806.05654", "submitter": "Christoph Rauch", "authors": "Thorsten Wi{\\ss}mann, Ulrich Dorsch, Stefan Milius, Lutz Schr\\\"oder", "title": "Efficient and Modular Coalgebraic Partition Refinement", "comments": "Extended journal version of the conference paper arXiv:1705.08362.\n  Beside reorganization of the material, the introductory section 3 is entirely\n  new and the other new section 7 contains new mathematical results", "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 1 (January\n  31, 2020) lmcs:6064", "doi": "10.23638/LMCS-16(1:8)2020", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a generic partition refinement algorithm that quotients\ncoalgebraic systems by behavioural equivalence, an important task in system\nanalysis and verification. Coalgebraic generality allows us to cover not only\nclassical relational systems but also, e.g. various forms of weighted systems\nand furthermore to flexibly combine existing system types. Under assumptions on\nthe type functor that allow representing its finite coalgebras in terms of\nnodes and edges, our algorithm runs in time $\\mathcal{O}(m\\cdot \\log n)$ where\n$n$ and $m$ are the numbers of nodes and edges, respectively. The generic\ncomplexity result and the possibility of combining system types yields a\ntoolbox for efficient partition refinement algorithms. Instances of our generic\nalgorithm match the run-time of the best known algorithms for unlabelled\ntransition systems, Markov chains, deterministic automata (with fixed\nalphabets), Segala systems, and for color refinement.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:22:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 17:55:31 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 18:00:12 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 08:22:43 GMT"}, {"version": "v5", "created": "Thu, 30 Jan 2020 13:27:49 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Wi\u00dfmann", "Thorsten", ""], ["Dorsch", "Ulrich", ""], ["Milius", "Stefan", ""], ["Schr\u00f6der", "Lutz", ""]]}, {"id": "1806.05701", "submitter": "Anson Kahng", "authors": "Bernhard Haeupler and D Ellis Hershkowitz and Anson Kahng and Ariel D.\n  Procaccia", "title": "Computation-Aware Data Aggregation", "comments": "Changed the introduction and title; this is the ITCS camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data aggregation is a fundamental primitive in distributed computing wherein\na network computes a function of every nodes' input. However, while compute\ntime is non-negligible in modern systems, standard models of distributed\ncomputing do not take compute time into account. Rather, most distributed\nmodels of computation only explicitly consider communication time.\n  In this paper, we introduce a model of distributed computation that considers\n\\emph{both} computation and communication so as to give a theoretical treatment\nof data aggregation. We study both the structure of and how to compute the\nfastest data aggregation schedule in this model. As our first result, we give a\npolynomial-time algorithm that computes the optimal schedule when the input\nnetwork is a complete graph. Moreover, since one may want to aggregate data\nover a pre-existing network, we also study data aggregation scheduling on\narbitrary graphs. We demonstrate that this problem on arbitrary graphs is hard\nto approximate within a multiplicative $1.5$ factor. Finally, we give an\n$O(\\log n \\cdot \\log \\frac{\\mathrm{OPT}}{t_m})$-approximation algorithm for\nthis problem on arbitrary graphs, where $n$ is the number of nodes and\n$\\mathrm{OPT}$ is the length of the optimal schedule.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:40:11 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 17:32:06 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 19:07:41 GMT"}, {"version": "v4", "created": "Tue, 12 Nov 2019 20:09:55 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Kahng", "Anson", ""], ["Procaccia", "Ariel D.", ""]]}, {"id": "1806.05797", "submitter": "Bin Fu", "authors": "Bin Fu, Pengfei Gu, and Yuming Zhao", "title": "Polyhedra Circuits and Their Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce polyhedra circuits. Each polyhedra circuit characterizes a\ngeometric region in $\\mathbb{R}^d$. They can be applied to represent a rich\nclass of geometric objects, which include all polyhedra and the union of a\nfinite number of polyhedra. They can be used to approximate a large class of\n$d$-dimensional manifolds in $\\mathbb{R}^d$. Barvinok developed polynomial time\nalgorithms to compute the volume of a rational polyhedra, and to count the\nnumber of lattice points in a rational polyhedra in a fixed dimensional space\n$\\mathbb{R}^d$ with a fix $d$. Define $T_V(d,\\, n)$ be the polynomial time in\n$n$ to compute the volume of one rational polyhedra, $T_L(d,\\, n)$ be the\npolynomial time in $n$ to count the number of lattice points in one rational\npolyhedra with $d$ be a fixed dimensional number, $T_I(d,\\, n)$ be the\npolynomial time in $n$ to solve integer linear programming time with $d$ be the\nfixed dimensional number, where $n$ is the total number of linear inequalities\nfrom input polyhedra. We develop algorithms to count the number of lattice\npoints in the geometric region determined by a polyhedra circuit in\n$O\\left(nd\\cdot r_d(n)\\cdot T_V(d,\\, n)\\right)$ time and to compute the volume\nof the geometric region determined by a polyhedra circuit in $O\\left(n\\cdot\nr_d(n)\\cdot T_I(d,\\, n)+r_d(n)T_L(d,\\, n)\\right)$ time, where $n$ is the number\nof input linear inequalities, $d$ is number of variables and $r_d(n)$ be the\nmaximal number of regions that $n$ linear inequalities with $d$ variables\npartition $\\mathbb{R}^d$.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:28:34 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Fu", "Bin", ""], ["Gu", "Pengfei", ""], ["Zhao", "Yuming", ""]]}, {"id": "1806.05942", "submitter": "Amanda Clare", "authors": "Amanda Clare and Jacqueline W. Daykin", "title": "Enhanced string factoring from alphabet orderings", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note we consider the concept of alphabet ordering in the context of\nstring factoring. We propose a greedy-type algorithm which produces Lyndon\nfactorizations with small numbers of factors along with a modification for\nlarge numbers of factors. For the technique we introduce the Exponent Parikh\nvector. Applications and research directions derived from circ-UMFFs are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:14:08 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Clare", "Amanda", ""], ["Daykin", "Jacqueline W.", ""]]}, {"id": "1806.06031", "submitter": "Rita Gitik", "authors": "Rita Gitik", "title": "Two Algorithms in Group Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm deciding if the intersection of a quasiconvex\nsubgroup of a negatively curved group with a conjugate is finite. We also give\na short proof of decidability of the membership problem for quasiconvex\nsubgroups of finitely generated groups with decidable word problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 16:05:45 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Gitik", "Rita", ""]]}, {"id": "1806.06055", "submitter": "Huang Lingxiao", "authors": "L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi", "title": "Classification with Fairness Constraints: A Meta-Algorithm with Provable\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing classification algorithms that are fair with respect to sensitive\nattributes of the data has become an important problem due to the growing\ndeployment of classification algorithms in various social contexts. Several\nrecent works have focused on fairness with respect to a specific metric,\nmodeled the corresponding fair classification problem as a constrained\noptimization problem, and developed tailored algorithms to solve them. Despite\nthis, there still remain important metrics for which we do not have fair\nclassifiers and many of the aforementioned algorithms do not come with\ntheoretical guarantees; perhaps because the resulting optimization problem is\nnon-convex. The main contribution of this paper is a new meta-algorithm for\nclassification that takes as input a large class of fairness constraints, with\nrespect to multiple non-disjoint sensitive attributes, and which comes with\nprovable guarantees. This is achieved by first developing a meta-algorithm for\na large family of classification problems with convex constraints, and then\nshowing that classification problems with general types of fairness constraints\ncan be reduced to those in this family. We present empirical results that show\nthat our algorithm can achieve near-perfect fairness with respect to various\nfairness metrics, and that the loss in accuracy due to the imposed fairness\nconstraints is often small. Overall, this work unifies several prior works on\nfair classification, presents a practical algorithm with theoretical\nguarantees, and can handle fairness metrics that were previously not possible.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:45:58 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:53:43 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 06:08:21 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Celis", "L. Elisa", ""], ["Huang", "Lingxiao", ""], ["Keswani", "Vijay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1806.06064", "submitter": "Mark Hallen", "authors": "Mark A. Hallen and Bruce R. Donald", "title": "Protein Design by Algorithm", "comments": null, "journal-ref": "Communications of the ACM, October 2019, Vol. 62 No. 10, Pages\n  76-84", "doi": "10.1145/3338124", "report-no": null, "categories": "cs.CE cs.DS q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review algorithms for protein design in general. Although these algorithms\nhave a rich combinatorial, geometric, and mathematical structure, they are\nalmost never covered in computer science classes. Furthermore, many of these\nalgorithms admit provable guarantees of accuracy, soundness, complexity,\ncompleteness, optimality, and approximation bounds. The algorithms represent a\ndelicate and beautiful balance between discrete and continuous computation and\nmodeling, analogous to that which is seen in robotics, computational geometry,\nand other fields in computational science. Finally, computer scientists may be\nunaware of the almost direct impact of these algorithms for predicting and\nintroducing molecular therapies that have gone in a short time from mathematics\nto algorithms to software to predictions to preclinical testing to clinical\ntrials. Indeed, the overarching goal of these algorithms is to enable the\ndevelopment of new therapeutics that might be impossible or too expensive to\ndiscover using experimental methods. Thus the potential impact of these\nalgorithms on individual, community, and global health has the potential to be\nquite significant.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:49:44 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Hallen", "Mark A.", ""], ["Donald", "Bruce R.", ""]]}, {"id": "1806.06091", "submitter": "Mark Velednitsky", "authors": "Mark Velednitsky", "title": "Solving $(k-1)$-Stable Instances of k-Terminal Cut with Isolating Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-Terminal Cut problem, also known as the Multiway Cut problem, is\ndefined on an edge-weighted graph with $k$ distinct vertices called\n\"terminals.\" The goal is to remove a minimum weight collection of edges from\nthe graph such that there is no path between any pair of terminals. The problem\nis NP-hard.\n  Isolating cuts are minimum cuts that separate one terminal from the rest. The\nunion of all the isolating cuts, except the largest, is a\n$(2-2/k)$-approximation to the optimal k-Terminal Cut. This is the only\ncurrently-known approximation algorithm for k-Terminal Cut which does not\nrequire solving a linear program.\n  An instance of k-Terminal Cut is $\\gamma$-stable if edges in the cut can be\nmultiplied by up to $\\gamma$ without changing the unique optimal solution. In\nthis paper, we show that, in any $(k-1)$-stable instance of k-Terminal Cut, the\nsource sets of the isolating cuts are the source sets of the unique optimal\nsolution of that k-Terminal Cut instance. We conclude that the\n$(2-2/k)$-approximation algorithm returns the optimal solution on\n$(k-1)$-stable instances. Ours is the first result showing that this\n$(2-2/k)$-approximation is an exact optimization algorithm on a special class\nof graphs.\n  We also show that our $(k-1)$-stability result is tight. We construct\n$(k-1-\\epsilon)$-stable instances of the k-Terminal Cut problem which only have\ntrivial isolating cuts: that is, the source set of the isolating cuts for each\nterminal is just the terminal itself. Thus, the $(2-2/k)$-approximation does\nnot return an optimal solution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 18:42:49 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 03:51:48 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 03:52:39 GMT"}, {"version": "v4", "created": "Sat, 12 Jan 2019 21:02:20 GMT"}, {"version": "v5", "created": "Wed, 2 Oct 2019 15:03:47 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Velednitsky", "Mark", ""]]}, {"id": "1806.06100", "submitter": "Jonathan Ullman", "authors": "Kobbi Nissim and Adam Smith and Thomas Steinke and Uri Stemmer and\n  Jonathan Ullman", "title": "The Limits of Post-Selection Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistics and machine learning offers numerous methods for ensuring\ngeneralization, these methods often fail in the presence of adaptivity---the\ncommon practice in which the choice of analysis depends on previous\ninteractions with the same dataset. A recent line of work has introduced\npowerful, general purpose algorithms that ensure post hoc generalization (also\ncalled robust or post-selection generalization), which says that, given the\noutput of the algorithm, it is hard to find any statistic for which the data\ndiffers significantly from the population it came from.\n  In this work we show several limitations on the power of algorithms\nsatisfying post hoc generalization. First, we show a tight lower bound on the\nerror of any algorithm that satisfies post hoc generalization and answers\nadaptively chosen statistical queries, showing a strong barrier to progress in\npost selection data analysis. Second, we show that post hoc generalization is\nnot closed under composition, despite many examples of such algorithms\nexhibiting strong composition properties.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 19:36:35 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Nissim", "Kobbi", ""], ["Smith", "Adam", ""], ["Steinke", "Thomas", ""], ["Stemmer", "Uri", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1806.06126", "submitter": "Shinichi Nakajima", "authors": "Hannah Marienwald, Wikor Pronobis, Klaus-Robert M\\\"uller, Shinichi\n  Nakajima", "title": "Tight Bound of Incremental Cover Trees for Dynamic Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic diversification---finding a set of data points with maximum diversity\nfrom a time-dependent sample pool---is an important task in recommender\nsystems, web search, database search, and notification services, to avoid\nshowing users duplicate or very similar items. The incremental cover tree (ICT)\nwith high computational efficiency and flexibility has been applied to this\ntask, and shown good performance. Specifically, it was empirically observed\nthat ICT typically provides a set with its diversity only marginally ($\\sim 1/\n1.2$ times) worse than the greedy max-min (GMM) algorithm, the state-of-the-art\nmethod for static diversification with its performance bound optimal for any\npolynomial time algorithm. Nevertheless, the known performance bound for ICT is\n4 times worse than this optimal bound. With this paper, we aim to fill this\nvery gap between theory and empirical observations. For achieving this, we\nfirst analyze variants of ICT methods, and derive tighter performance bounds.\nWe then investigate the gap between the obtained bound and empirical\nobservations by using specially designed artificial data for which the optimal\ndiversity is known. Finally, we analyze the tightness of the bound, and show\nthat the bound cannot be further improved, i.e., this paper provides the\ntightest possible bound for ICT methods. In addition, we demonstrate a new use\nof dynamic diversification for generative image samplers, where prototypes are\nincrementally collected from a stream of artificial images generated by an\nimage sampler.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 21:11:56 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Marienwald", "Hannah", ""], ["Pronobis", "Wikor", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Nakajima", "Shinichi", ""]]}, {"id": "1806.06173", "submitter": "Georgina Hall", "authors": "Amir Ali Ahmadi, Georgina Hall", "title": "On the Complexity of Detecting Convexity over a Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that the problem of testing global convexity of\npolynomials of degree four is {strongly} NP-hard, answering an open question of\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\nconvexity is of concern. In a number of applications however, one is interested\nin testing convexity only over a compact region, most commonly a box (i.e.,\nhyper-rectangle). In this paper, we show that this problem is also strongly\nNP-hard, in fact for polynomials of degree as low as three. This result is\nminimal in the degree of the polynomial and in some sense justifies why\nconvexity detection in nonlinear optimization solvers is limited to quadratic\nfunctions or functions with special structure. As a byproduct, our proof shows\nthat the problem of testing whether all matrices in an interval family are\npositive semidefinite is strongly NP-hard. This problem, which was previously\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\ntheory of robust control.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 03:22:50 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 11:45:06 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Hall", "Georgina", ""]]}, {"id": "1806.06182", "submitter": "Golshan Golnari", "authors": "Golshan Golnari, Zhi-Li Zhang", "title": "Fast Distance Sensitivity Oracle for Multiple Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a network is prone to failures, it is very expensive to compute the\nshortest paths every time from the scratch. Distance sensitivity oracle\nprovides this privilege to find the new shortest paths faster and with lower\ncost by once pre-computing an oracle in advance. Although several efficient\nsolutions are proposed in the literature to support the single failure, few\nefforts are done to devise an efficient method regarding the case of multiple\nfailures. In this paper, we present a novel distance sensitivity oracle based\non Markov Tensor Theory \\cite{golnari2017markov} to support replacement path\nqueries $(*,t,\\mathcal{F})$ in general directed and weighted networks facing\nthe set of failures $\\mathcal{F}$. In contrast to the existing work, there is\nno limitation on maximum failure size supported by our oracle and there is no\nneed to know the size of failure for constructing the oracle. The\nspecifications of our oracle are: space size of $O(n^2)$, pre-process time of\n$O(n^{\\omega})$, where $\\omega$ is the exponent of fast matrix multiplication,\nand query time of $O(m)$ for answering to replacement path query of\n$(*,t,\\mathcal{F})$ which computes the replacement (shortest) paths from all\nnodes to target $t$ at once. While the computation time for regular shortest\npath methods, such as Dijkstra's, is $O(m+nlogn)$ for each query after a\nfailure, our algorithm can save a considerable computational time when the size\nof failure set $|\\mathcal{F}|$ is $O(m^{1/\\omega})$ or less and the network is\nsparse $O(m)<O(nlogn)$.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 04:50:48 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Golnari", "Golshan", ""], ["Zhang", "Zhi-Li", ""]]}, {"id": "1806.06223", "submitter": "Denis Pankratov", "authors": "Allan Borodin, Joan Boyar, Kim S. Larsen, Denis Pankratov", "title": "Advice Complexity of Priority Algorithms", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The priority model of \"greedy-like\" algorithms was introduced by Borodin,\nNielsen, and Rackoff in 2002. We augment this model by allowing priority\nalgorithms to have access to advice, i.e., side information precomputed by an\nall-powerful oracle. Obtaining lower bounds in the priority model without\nadvice can be challenging and may involve intricate adversary arguments. Since\nthe priority model with advice is even more powerful, obtaining lower bounds\npresents additional difficulties. We sidestep these difficulties by developing\na general framework of reductions which makes lower bound proofs relatively\nstraightforward and routine. We start by introducing the Pair Matching problem,\nfor which we are able to prove strong lower bounds in the priority model with\nadvice. We develop a template for constructing a reduction from Pair Matching\nto other problems in the priority model with advice -- this part is technically\nchallenging since the reduction needs to define a valid priority function for\nPair Matching while respecting the priority function for the other problem.\nFinally, we apply the template to obtain lower bounds for a number of standard\ndiscrete optimization problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 11:16:07 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 15:20:33 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Borodin", "Allan", ""], ["Boyar", "Joan", ""], ["Larsen", "Kim S.", ""], ["Pankratov", "Denis", ""]]}, {"id": "1806.06237", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah and Aarti Singh", "title": "PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automated assignment of papers to reviewers in\nconference peer review, with a focus on fairness and statistical accuracy. Our\nfairness objective is to maximize the review quality of the most disadvantaged\npaper, in contrast to the commonly used objective of maximizing the total\nquality over all papers. We design an assignment algorithm based on an\nincremental max-flow procedure that we prove is near-optimally fair. Our\nstatistical accuracy objective is to ensure correct recovery of the papers that\nshould be accepted. We provide a sharp minimax analysis of the accuracy of the\npeer-review process for a popular objective-score model as well as for a novel\nsubjective-score model that we propose in the paper. Our analysis proves that\nour proposed assignment algorithm also leads to a near-optimal statistical\naccuracy. Finally, we design a novel experiment that allows for an objective\ncomparison of various assignment algorithms, and overcomes the inherent\ndifficulty posed by the absence of a ground truth in experiments on\npeer-review. The results of this experiment as well as of other experiments on\nsynthetic and real data corroborate the theoretical guarantees of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 12:42:04 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 02:15:04 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""]]}, {"id": "1806.06323", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta and Sergio Pequito and Paul Bogdan", "title": "Approximate Submodular Functions and Performance Guarantees", "comments": "23 pages, 8 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing non-negative non-decreasing set\nfunctions. Although most of the recent work focus on exploiting submodularity,\nit turns out that several objectives we encounter in practice are not\nsubmodular. Nonetheless, often we leverage the greedy algorithms used in\nsubmodular functions to determine a solution to the non-submodular functions.\nHereafter, we propose to address the original problem by \\emph{approximating}\nthe non-submodular function and analyze the incurred error, as well as the\nperformance trade-offs. To quantify the approximation error, we introduce a\nnovel concept of $\\delta$-approximation of a function, which we used to define\nthe space of submodular functions that lie within an approximation error. We\nprovide necessary conditions on the existence of such $\\delta$-approximation\nfunctions, which might not be unique. Consequently, we characterize this\nsubspace which we refer to as \\emph{region of submodularity}. Furthermore,\nsubmodular functions are known to lead to different sub-optimality guarantees,\nso we generalize those dependencies upon a $\\delta$-approximation into the\nnotion of \\emph{greedy curvature}. Finally, we used this latter notion to\nsimplify some of the existing results and efficiently (i.e., linear complexity)\ndetermine tightened bounds on the sub-optimality guarantees using objective\nfunctions commonly used in practical setups and validate them using real data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 01:41:14 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Gupta", "Gaurav", ""], ["Pequito", "Sergio", ""], ["Bogdan", "Paul", ""]]}, {"id": "1806.06373", "submitter": "Nisheeth Vishnoi", "authors": "Nisheeth K. Vishnoi", "title": "Geodesic Convex Optimization: Differentiation on Manifolds, Geodesics,\n  and Convexity", "comments": "This exposition is to supplement the talk by the author at the\n  workshop on Optimization, Complexity and Invariant Theory at the Institute\n  for Advanced Study, Princeton", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex optimization is a vibrant and successful area due to the existence of\na variety of efficient algorithms that leverage the rich structure provided by\nconvexity. Convexity of a smooth set or a function in a Euclidean space is\ndefined by how it interacts with the standard differential structure in this\nspace -- the Hessian of a convex function has to be positive semi-definite\neverywhere. However, in recent years, there is a growing demand to understand\nnon-convexity and develop computational methods to optimize non-convex\nfunctions. Intriguingly, there is a type of non-convexity that disappears once\none introduces a suitable differentiable structure and redefines convexity with\nrespect to the straight lines, or {\\em geodesics}, with respect to this\nstructure. Such convexity is referred to as {\\em geodesic convexity}. Interest\nin studying it arises due to recent reformulations of some non-convex problems\nas geodesically convex optimization problems over geodesically convex sets.\nGeodesics on manifolds have been extensively studied in various branches of\nMathematics and Physics. However, unlike convex optimization, understanding\ngeodesics and geodesic convexity from a computational point of view largely\nremains a mystery. The goal of this exposition is to introduce the first part\nof geodesic convex optimization -- geodesic convexity -- in a self-contained\nmanner. We first present a variety of notions from differential and Riemannian\ngeometry such as differentiation on manifolds, geodesics, and then introduce\ngeodesic convexity. We conclude by showing that certain non-convex optimization\nproblems such as computing the Brascamp-Lieb constant and the operator scaling\nproblem have geodesically convex formulations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 12:29:01 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1806.06421", "submitter": "Christopher Liaw", "authors": "Nicholas J. A. Harvey, Christopher Liaw, Paul Liu", "title": "Greedy and Local Ratio Algorithms in the MapReduce Model", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce has become the de facto standard model for designing distributed\nalgorithms to process big data on a cluster. There has been considerable\nresearch on designing efficient MapReduce algorithms for clustering, graph\noptimization, and submodular optimization problems. We develop new techniques\nfor designing greedy and local ratio algorithms in this setting. Our randomized\nlocal ratio technique gives $2$-approximations for weighted vertex cover and\nweighted matching, and an $f$-approximation for weighted set cover, all in a\nconstant number of MapReduce rounds. Our randomized greedy technique gives\nalgorithms for maximal independent set, maximal clique, and a $(1+\\epsilon)\\ln\n\\Delta$-approximation for weighted set cover. We also give greedy algorithms\nfor vertex colouring with $(1+o(1))\\Delta$ colours and edge colouring with\n$(1+o(1))\\Delta$ colours.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 17:51:20 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Harvey", "Nicholas J. A.", ""], ["Liaw", "Christopher", ""], ["Liu", "Paul", ""]]}, {"id": "1806.06427", "submitter": "Audra McMillan", "authors": "Anna Gilbert and Audra McMillan", "title": "Property Testing for Differential Privacy", "comments": "Allerton, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of property testing for differential privacy: with\nblack-box access to a purportedly private algorithm, can we verify its privacy\nguarantees? In particular, we show that any privacy guarantee that can be\nefficiently verified is also efficiently breakable in the sense that there\nexist two databases between which we can efficiently distinguish. We give lower\nbounds on the query complexity of verifying pure differential privacy,\napproximate differential privacy, random pure differential privacy, and random\napproximate differential privacy. We also give algorithmic upper bounds. The\nlower bounds obtained in the work are infeasible for the scale of parameters\nthat are typically considered reasonable in the differential privacy\nliterature, even when we suppose that the verifier has access to an (untrusted)\ndescription of the algorithm. A central message of this work is that verifying\nprivacy requires compromise by either the verifier or the algorithm owner.\nEither the verifier has to be satisfied with a weak privacy guarantee, or the\nalgorithm owner has to compromise on side information or access to the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 18:49:01 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 07:23:11 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Gilbert", "Anna", ""], ["McMillan", "Audra", ""]]}, {"id": "1806.06429", "submitter": "Sidhanth Mohanty", "authors": "Aditya Krishnan, Sidhanth Mohanty, David P. Woodruff", "title": "On Sketching the $q$ to $p$ norms", "comments": "23 pages. To appear in APPROX 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of data dimensionality reduction, or sketching, for the\n$q\\to p$ norms. Given an $n \\times d$ matrix $A$, the $q\\to p$ norm, denoted\n$\\|A\\|_{q \\to p} = \\sup_{x \\in \\mathbb{R}^d \\backslash \\vec{0}}\n\\frac{\\|Ax\\|_p}{\\|x\\|_q}$, is a natural generalization of several matrix and\nvector norms studied in the data stream and sketching models, with applications\nto datamining, hardness of approximation, and oblivious routing. We say a\ndistribution $S$ on random matrices $L \\in \\mathbb{R}^{nd} \\rightarrow\n\\mathbb{R}^k$ is a $(k,\\alpha)$-sketching family if from $L(A)$, one can\napproximate $\\|A\\|_{q \\to p}$ up to a factor $\\alpha$ with constant\nprobability. We provide upper and lower bounds on the sketching dimension $k$\nfor every $p, q \\in [1, \\infty]$, and in a number of cases our bounds are\ntight. While we mostly focus on constant $\\alpha$, we also consider large\napproximation factors $\\alpha$, as well as other variants of the problem such\nas when $A$ has low rank.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 18:58:29 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Krishnan", "Aditya", ""], ["Mohanty", "Sidhanth", ""], ["Woodruff", "David P.", ""]]}, {"id": "1806.06430", "submitter": "Peilin Zhong", "authors": "Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, Ruiqi Zhong", "title": "Subspace Embedding and Linear Regression with Orlicz Norm", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of the classic linear regression problem to the\ncase when the loss is an Orlicz norm. An Orlicz norm is parameterized by a\nnon-negative convex function $G:\\mathbb{R}_+\\rightarrow\\mathbb{R}_+$ with\n$G(0)=0$: the Orlicz norm of a vector $x\\in\\mathbb{R}^n$ is defined as $\n\\|x\\|_G=\\inf\\left\\{\\alpha>0\\large\\mid\\sum_{i=1}^n G(|x_i|/\\alpha)\\leq\n1\\right\\}. $ We consider the cases where the function $G(\\cdot)$ grows\nsubquadratically. Our main result is based on a new oblivious embedding which\nembeds the column space of a given matrix $A\\in\\mathbb{R}^{n\\times d}$ with\nOrlicz norm into a lower dimensional space with $\\ell_2$ norm. Specifically, we\nshow how to efficiently find an embedding matrix $S\\in\\mathbb{R}^{m\\times\nn},m<n$ such that $\\forall x\\in\\mathbb{R}^{d},\\Omega(1/(d\\log n)) \\cdot\n\\|Ax\\|_G\\leq \\|SAx\\|_2\\leq O(d^2\\log n) \\cdot \\|Ax\\|_G.$ By applying this\nsubspace embedding technique, we show an approximation algorithm for the\nregression problem $\\min_{x\\in\\mathbb{R}^d} \\|Ax-b\\|_G$, up to a $O(d\\log^2 n)$\nfactor. As a further application of our techniques, we show how to also use\nthem to improve on the algorithm for the $\\ell_p$ low rank matrix approximation\nproblem for $1\\leq p<2$.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 19:05:49 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Andoni", "Alexandr", ""], ["Lin", "Chengyu", ""], ["Sheng", "Ying", ""], ["Zhong", "Peilin", ""], ["Zhong", "Ruiqi", ""]]}, {"id": "1806.06580", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Italo Epicoco and Marco Pulimeno", "title": "Mining frequent items in unstructured P2P networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale decentralized systems, such as P2P, sensor or IoT device networks\nare becoming increasingly common, and require robust protocols to address the\nchallenges posed by the distribution of data and the large number of peers\nbelonging to the network. In this paper, we deal with the problem of mining\nfrequent items in unstructured P2P networks. This problem, of practical\nimportance, has many useful applications. We design P2PSS, a fully\ndecentralized, gossip--based protocol for frequent items discovery, leveraging\nthe Space-Saving algorithm. We formally prove the correctness and theoretical\nerror bound. Extensive experimental results clearly show that P2PSS provides\nvery good accuracy and scalability, also in the presence of highly dynamic P2P\nnetworks with churning. To the best of our knowledge, this is the first\ngossip--based distributed algorithm providing strong theoretical guarantees for\nboth the Approximate Frequent Items Problem in Unstructured P2P Networks and\nfor the frequency estimation of discovered frequent items.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 10:13:19 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 10:00:14 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 11:49:32 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 18:44:30 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Cafaro", "Massimo", ""], ["Epicoco", "Italo", ""], ["Pulimeno", "Marco", ""]]}, {"id": "1806.06617", "submitter": "Christiane Spisla", "authors": "Michael J\\\"unger, Petra Mutzel, Christiane Spisla", "title": "A Flow Formulation for Horizontal Coordinate Assignment with Prescribed\n  Width", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the coordinate assignment phase of the well known Sugiyama\nframework for drawing directed graphs in a hierarchical style. The extensive\nliterature in this area has given comparatively little attention to a\nprescribed width of the drawing. We present a minimum cost flow formulation\nthat supports prescribed width and optionally other criteria like lower and\nupper bounds on the distance of neighbouring nodes in a layer or enforced\nvertical edges segments. In our experiments we demonstrate that our approach\ncan compete with state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:08:41 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 14:33:05 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["J\u00fcnger", "Michael", ""], ["Mutzel", "Petra", ""], ["Spisla", "Christiane", ""]]}, {"id": "1806.06704", "submitter": "Marie-Christine Costa", "authors": "C\\'edric Bentz (CEDRIC), Marie-Christine Costa (CEDRIC, OC),\n  Pierre-Louis Poirion (CEDRIC, OC), Thomas Ridremont (CEDRIC, OC)", "title": "Formulations for designing robust networks. An application to wind power\n  collection", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.04696", "journal-ref": "Electronic Notes in Discrete Mathematics, 2018, Inernational\n  network optimization conference 2017., 64, pp.365-374", "doi": "10.1016/j.endm.2018.02.011", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the design of survivable capacitated rooted Steiner\nnetworks. Given a graph G = (V, E), capacity and cost functions on E, a root r,\na subset T of V of terminals and an integer k, we search for a minimum cost\nsubset E $\\subset$ E, covering T and r, such that the network induced by E is\nk-survivable: after the removal of any k edges, there still exists a feasible\nflow from r to T. We also consider the possibility of protecting a given number\nof edges. We propose three different formulations: a cut-set, a flow and a\nbi-level formulation where the second-level is a min-max problem (with an\nattacker and a defender). We propose algorithms for each problem formulation\nand compare their efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:58:20 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Bentz", "C\u00e9dric", "", "CEDRIC"], ["Costa", "Marie-Christine", "", "CEDRIC, OC"], ["Poirion", "Pierre-Louis", "", "CEDRIC, OC"], ["Ridremont", "Thomas", "", "CEDRIC, OC"]]}, {"id": "1806.06726", "submitter": "Caleb Levy", "authors": "Robert E. Tarjan, Caleb C. Levy, and Stephen Timmel", "title": "Zip Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the zip tree, a form of randomized binary search tree that\nintegrates previous ideas into one practical, performant, and\npleasant-to-implement package. A zip tree is a binary search tree in which each\nnode has a numeric rank and the tree is (max)-heap-ordered with respect to\nranks, with rank ties broken in favor of smaller keys. Zip trees are\nessentially treaps (Seidel and Aragon 1996), except that ranks are drawn from a\ngeometric distribution instead of a uniform distribution, and we allow rank\nties. These changes enable us to use fewer random bits per node. We perform\ninsertions and deletions by unmerging and merging paths (\"unzipping\" and\n\"zipping\") rather than by doing rotations, which avoids some pointer changes\nand improves efficiency. The methods of zipping and unzipping take inspiration\nfrom previous top-down approaches to insertion and deletion (Stephenson 1980;\nMart\\'inez and Roura 1998; Sprugnoli 1980). From a theoretical standpoint, this\nwork provides two main results. First, zip trees require only $O(\\log \\log n)$\nbits (with high probability) to represent the largest rank in an $n$-node\nbinary search tree; previous data structures require $O(\\log n)$ bits for the\nlargest rank. Second, zip trees are naturally isomorphic to skip lists (Pugh\n1990), and simplify the mapping of (Dean and Jones 2007) between skip lists and\nbinary search trees.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 14:22:07 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 19:30:12 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 01:12:57 GMT"}, {"version": "v4", "created": "Tue, 12 Nov 2019 02:39:25 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Tarjan", "Robert E.", ""], ["Levy", "Caleb C.", ""], ["Timmel", "Stephen", ""]]}, {"id": "1806.06766", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Forest Yang, Avishek Ghosh, Abhay Parekh, Kannan\n  Ramchandran", "title": "Matching Observations to Distributions: Efficient Estimation via\n  Sparsified Hungarian Algorithm", "comments": "8 pages, 1 figure; to appear in the 57th Annual Allerton Conference\n  on Communication, Control, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are given observations, where each observation is drawn\nindependently from one of $k$ known distributions. The goal is to match each\nobservation to the distribution from which it was drawn. We observe that the\nmaximum likelihood estimator (MLE) for this problem can be computed using\nweighted bipartite matching, even when $n$, the number of observations per\ndistribution, exceeds one. This is achieved by instantiating $n$ duplicates of\neach distribution node. However, in the regime where the number of observations\nper distribution is much larger than the number of distributions, the Hungarian\nmatching algorithm for computing the weighted bipartite matching requires\n$\\mathcal O(n^3)$ time. We introduce a novel randomized matching algorithm that\nreduces the runtime to $\\tilde{\\mathcal O}(n^2)$ by sparsifying the original\ngraph, returning the exact MLE with high probability. Next, we give statistical\njustification for using the MLE by bounding the excess risk of the MLE, where\nthe loss is defined as the negative log-likelihood. We test these bounds for\nthe case of isotropic Gaussians with equal covariances and whose means are\nseparated by a distance $\\eta$, and find (1) that $\\gg \\log k$ separation\nsuffices to drive the proportion of mismatches of the MLE to 0, and (2) that\nthe expected fraction of mismatched observations goes to zero at rate $\\mathcal\nO({(\\log k)}^2/\\eta^2)$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:19:05 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 20:34:58 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chewi", "Sinho", ""], ["Yang", "Forest", ""], ["Ghosh", "Avishek", ""], ["Parekh", "Abhay", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1806.06933", "submitter": "Robert Kleinberg", "authors": "Jon Kleinberg and Robert Kleinberg", "title": "Delegated Search Approximates Efficient Search", "comments": "An extended abstract of this work appears in the Proceedings of the\n  19th ACM Conference on Economics and Computation (EC), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many settings in which a principal performs a task by delegating it\nto an agent, who searches over possible solutions and proposes one to the\nprincipal. This describes many aspects of the workflow within organizations, as\nwell as many of the activities undertaken by regulatory bodies, who often\nobtain relevant information from the parties being regulated through a process\nof delegation. A fundamental tension underlying delegation is the fact that the\nagent's interests will typically differ -- potentially significantly -- from\nthe interests of the principal, and as a result the agent may propose solutions\nbased on their own incentives that are inefficient for the principal. A basic\nproblem, therefore, is to design mechanisms by which the principal can\nconstrain the set of proposals they are willing to accept from the agent, to\nensure a certain level of quality for the principal from the proposed solution.\n  In this work, we investigate how much the principal loses -- quantitatively,\nin terms of the objective they are trying to optimize -- when they delegate to\nan agent. We develop a methodology for bounding this loss of efficiency, and\nshow that in a very general model of delegation, there is a family of\nmechanisms achieving a universal bound on the ratio between the quality of the\nsolution obtained through delegation and the quality the principal could obtain\nin an idealized benchmark where they searched for a solution themself.\nMoreover, it is possible to achieve such bounds through mechanisms with a\nnatural threshold structure, which are thus structurally simpler than the\noptimal mechanisms typically considered in the literature on delegation. At the\nheart of our framework is an unexpected connection between delegation and the\nanalysis of prophet inequalities, which we leverage to provide bounds on the\nbehavior of our delegation mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 20:45:04 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Kleinberg", "Jon", ""], ["Kleinberg", "Robert", ""]]}, {"id": "1806.06996", "submitter": "Georgina Hall", "authors": "Georgina Hall", "title": "Optimization over Nonnegative and Convex Polynomials With and Without\n  Semidefinite Programming", "comments": "PhD Thesis (Department of Operations Research and Financial\n  Engineering, Princeton University)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimizing over the cone of nonnegative polynomials is a\nfundamental problem in computational mathematics, with applications to\npolynomial optimization, control, machine learning, game theory, and\ncombinatorics, among others. A number of breakthrough papers in the early 2000s\nshowed that this problem, long thought to be out of reach, could be tackled by\nusing sum of squares programming. This technique however has proved to be\nexpensive for large-scale problems, as it involves solving large semidefinite\nprograms (SDPs).\n  In the first part of this thesis, we present two methods for approximately\nsolving large-scale sum of squares programs that dispense altogether with\nsemidefinite programming and only involve solving a sequence of linear or\nsecond order cone programs generated in an adaptive fashion. We then focus on\nthe problem of finding tight lower bounds on polynomial optimization problems\n(POPs), a fundamental task in this area that is most commonly handled through\nthe use of SDP-based sum of squares hierarchies (e.g., due to Lasserre and\nParrilo). In contrast to previous approaches, we provide the first theoretical\nframework for constructing converging hierarchies of lower bounds on POPs whose\ncomputation simply requires the ability to multiply certain fixed polynomials\ntogether and to check nonnegativity of the coefficients of their product.\n  In the second part of this thesis, we focus on the theory and applications of\nthe problem of optimizing over convex polynomials, a subcase of the problem of\noptimizing over nonnegative polynomials. (See manuscript for the rest of the\nabstract.)\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 01:14:38 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Hall", "Georgina", ""]]}, {"id": "1806.07241", "submitter": "Alexandru Paler", "authors": "Alexandru Paler, Alwin Zulehner, Robert Wille", "title": "NISQ circuit compilation is the travelling salesman problem on a torus", "comments": "preprint of accepted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.ET cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy, intermediate-scale quantum (NISQ) computers are expected to execute\nquantum circuits of up to a few hundred qubits. The circuits have to conform to\nNISQ architectural constraints regarding qubit allocation and the execution of\nmulti-qubit gates. Quantum circuit compilation (QCC) takes a nonconforming\ncircuit and outputs a compatible circuit. Can classical optimisation methods be\nused for QCC? Compilation is a known combinatorial problem shown to be solvable\nby two types of operations: 1) qubit allocation, and 2) gate scheduling. We\nshow informally that the two operations form a discrete ring. The search\nlandscape of QCC is a two dimensional discrete torus where vertices represent\nconfigurations of how circuit qubits are allocated to NISQ registers. Torus\nedges are weighted by the cost of scheduling circuit gates. The novelty of our\napproach uses the fact that a circuit's gate list is circular: compilation can\nstart from any gate as long as all the gates will be processed, and the\ncompiled circuit has the correct gate order. Our work bridges a theoretical and\npractical gap between classical circuit design automation and the emerging\nfield of quantum circuit optimisation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:58:25 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 03:11:57 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 18:48:56 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Paler", "Alexandru", ""], ["Zulehner", "Alwin", ""], ["Wille", "Robert", ""]]}, {"id": "1806.07356", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan", "title": "Deterministic $O(1)$-Approximation Algorithms to 1-Center Clustering\n  with Outliers", "comments": "16 pages, 1 figure. Preliminary version in APPROX, 2018. Keywords:\n  Deterministic, approximation algorithm, cluster, statistic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 1-center clustering with outliers problem asks about identifying a\nprototypical robust statistic that approximates the location of a cluster of\npoints. Given some constant $0 < \\alpha < 1$ and $n$ points such that $\\alpha\nn$ of them are in some (unknown) ball of radius $r,$ the goal is to compute a\nball of radius $O(r)$ that also contains $\\alpha n$ points. This problem can be\nformulated with the points in a normed vector space such as $\\mathbb{R}^d$ or\nin a general metric space.\n  The problem has a simple randomized solution: a randomly selected point is a\ncorrect solution with constant probability, and its correctness can be verified\nin linear time. However, the deterministic complexity of this problem was not\nknown. In this paper, for any $\\ell_p$ vector space, we show an $O(nd)$-time\nsolution with a ball of radius $O(r)$ for a fixed $\\alpha > \\frac{1}{2},$ and\nfor any normed vector space, we show an $O(nd)$-time solution with a ball of\nradius $O(r)$ when $\\alpha > \\frac{1}{2}$ as well as an $O (nd\n\\log^{(k)}(n))$-time solution with a ball of radius $O(r)$ for all $\\alpha > 0,\nk \\in \\mathbb{N},$ where $\\log^{(k)}(n)$ represents the $k$th iterated\nlogarithm, assuming distance computation and vector space operations take\n$O(d)$ time. For an arbitrary metric space, we show for any $C \\in \\mathbb{N}$\nan $O(n^{1+1/C})$-time solution that finds a ball of radius $2Cr,$ assuming\ndistance computation between any pair of points takes $O(1)$-time. Moreover,\nthis algorithm is optimal for general metric spaces, as we show that for any\nfixed $\\alpha, C,$ there is no $o(n^{1+1/C})$-query and thus no\n$o(n^{1+1/C})$-time solution that deterministically finds a ball of radius\n$2Cr$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 17:39:46 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 00:34:08 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Narayanan", "Shyam", ""]]}, {"id": "1806.07404", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Approximating real-rooted and stable polynomials, with combinatorial\n  applications", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $p(x)=a_0 + a_1 x + \\ldots + a_n x^n$ be a polynomial with all roots real\nand satisfying $x \\leq -\\delta$ for some $0<\\delta <1$. We show that for any $0\n< \\epsilon <1$, the value of $p(1)$ is determined within relative error\n$\\epsilon$ by the coefficients $a_k$ with $k \\leq {c \\over \\sqrt{\\delta}} \\ln\n{n \\over \\epsilon \\sqrt{ \\delta}}$ for some absolute constant $c > 0$.\nConsequently, if $m_k(G)$ is the number of matchings with $k$ edges in a graph\n$G$, then for any $0 < \\epsilon < 1$, the total number $M(G)=m_0(G)+m_1(G) +\n\\ldots $ of matchings is determined within relative error $\\epsilon$ by the\nnumbers $m_k(G)$ with $k \\leq c \\sqrt{\\Delta} \\ln (v /\\epsilon)$, where\n$\\Delta$ is the largest degree of a vertex, $v$ is the number of vertices of\n$G$ and $c >0$ is an absolute constant. We prove a similar result for\npolynomials with complex roots satisfying $\\Re\\thinspace z \\leq -\\delta$ and\napply it to estimate the number of unbranched subgraphs of $G$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:01:23 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "1806.07466", "submitter": "Daniel Wiebking", "authors": "Pascal Schweitzer, Daniel Wiebking", "title": "A unifying method for the design of algorithms canonizing combinatorial\n  objects", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a unified framework for the design of canonization algorithms.\nUsing hereditarily finite sets, we define a general notion of combinatorial\nobjects that includes graphs, hypergraphs, relational structures, codes,\npermutation groups, tree decompositions, and so on.\n  Our approach allows for a systematic transfer of the techniques that have\nbeen developed for isomorphism testing to canonization. We use it to design a\ncanonization algorithm for combinatorial objects in general. This result gives\nnew fastest canonization algorithms with an asymptotic running time matching\nthe best known isomorphism algorithm for the following types of objects:\nhypergraphs, hypergraphs of bounded color class size, permutation groups (up to\npermutational isomorphism) and codes that are explicitly given (up to code\nequivalence).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 21:07:41 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 15:02:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Schweitzer", "Pascal", ""], ["Wiebking", "Daniel", ""]]}, {"id": "1806.07508", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler, Wasim Huleihel", "title": "Reducibility and Computational Lower Bounds for Problems with Planted\n  Sparse Structure", "comments": "116 pages, accepted for presentation at Conference on Learning Theory\n  (COLT) 2018, small typos fixed in latest version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prototypical high-dimensional statistics problem entails finding a\nstructured signal in noise. Many of these problems exhibit an intriguing\nphenomenon: the amount of data needed by all known computationally efficient\nalgorithms far exceeds what is needed for inefficient algorithms that search\nover all possible structures. A line of work initiated by Berthet and Rigollet\nin 2013 has aimed to explain these statistical-computational gaps by reducing\nfrom conjecturally hard average-case problems in computer science. However, the\ndelicate nature of average-case reductions has limited the applicability of\nthis approach. In this work we introduce several new techniques to give a web\nof average-case reductions showing strong computational lower bounds based on\nthe planted clique conjecture using natural problems as intermediates. These\ninclude tight lower bounds for Planted Independent Set, Planted Dense Subgraph,\nSparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block\nModel and a biased variant of Sparse PCA. We also give algorithms matching our\nlower bounds and identify the information-theoretic limits of the models we\nconsider.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:48:25 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 16:17:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""], ["Huleihel", "Wasim", ""]]}, {"id": "1806.07586", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund and Thore Husfeldt", "title": "Counting Shortest Two Disjoint Paths in Cubic Planar Graphs with an NC\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph and two disjoint vertex pairs $s_1,t_1$ and\n$s_2,t_2$, the Shortest two disjoint paths problem (S2DP) asks for the minimum\ntotal length of two vertex disjoint paths connecting $s_1$ with $t_1$, and\n$s_2$ with $t_2$, respectively.\n  We show that for cubic planar graphs there are NC algorithms, uniform\ncircuits of polynomial size and polylogarithmic depth, that compute the S2DP\nand moreover also output the number of such minimum length path pairs.\n  Previously, to the best of our knowledge, no deterministic polynomial time\nalgorithm was known for S2DP in cubic planar graphs with arbitrary placement of\nthe terminals. In contrast, the randomized polynomial time algorithm by\nBj\\\"orklund and Husfeldt, ICALP 2014, for general graphs is much slower, is\nserial in nature, and cannot count the solutions.\n  Our results are built on an approach by Hirai and Namba, Algorithmica 2017,\nfor a generalisation of S2DP, and fast algorithms for counting perfect\nmatchings in planar graphs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:26:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Husfeldt", "Thore", ""]]}, {"id": "1806.07598", "submitter": "Kasper Green Larsen", "authors": "Shunhua Jiang, Kasper Green Larsen", "title": "A Faster External Memory Priority Queue with DecreaseKeys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A priority queue is a fundamental data structure that maintains a dynamic set\nof (key, priority)-pairs and supports Insert, Delete, ExtractMin and\nDecreaseKey operations. In the external memory model, the current best priority\nqueue supports each operation in amortized $O(\\frac{1}{B}\\log \\frac{N}{B})$\nI/Os. If the DecreaseKey operation does not need to be supported, one can\ndesign a more efficient data structure that supports the Insert, Delete and\nExtractMin operations in $O(\\frac{1}{B}\\log \\frac{N}{B}/ \\log \\frac{M}{B})$\nI/Os. A recent result shows that a degradation in performance is inevitable by\nproving a lower bound of $\\Omega(\\frac{1}{B}\\log B/\\log\\log N)$ I/Os for\npriority queues with DecreaseKeys. In this paper we tighten the gap between the\nlower bound and the upper bound by proposing a new priority queue which\nsupports the DecreaseKey operation and has an expected amortized I/O complexity\nof $O(\\frac{1}{B}\\log \\frac{N}{B}/\\log\\log N)$. Our result improves the\nexternal memory priority queue with DecreaseKeys for the first time in over a\ndecade, and also gives the fastest external memory single source shortest path\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:57:10 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Jiang", "Shunhua", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1806.07625", "submitter": "Louxin Zhang", "authors": "Andreas D.M. Gunawan, Hongwei Yan, Louxin Zhang", "title": "The compressions of reticulation-visible networks are tree-child", "comments": "18 pages, 4 figures", "journal-ref": "Journal of Computational Biology. 2019 Jan 9", "doi": "10.1089/cmb.2018.0220", "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rooted phylogenetic networks are rooted acyclic digraphs. They are used to\nmodel complex evolution where hybridization, recombination and other\nreticulation events play important roles. A rigorous definition of network\ncompression is introduced on the basis of the recent studies of the\nrelationships between cluster, tree and rooted phylogenetic network. The\nconcept reveals another interesting connection between the two well-studied\nnetwork classes|tree-child networks and reticulation-visible networks|and\nenables us to define a new class of networks for which the cluster containment\nproblem has a linear-time algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 09:21:08 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Gunawan", "Andreas D. M.", ""], ["Yan", "Hongwei", ""], ["Zhang", "Louxin", ""]]}, {"id": "1806.07626", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda, Akimichi Takemura", "title": "Game-theoretic derivation of upper hedging prices of multivariate\n  contingent claims and submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate upper and lower hedging prices of multivariate contingent\nclaims from the viewpoint of game-theoretic probability and submodularity. By\nconsidering a game between \"Market\" and \"Investor\" in discrete time, the\npricing problem is reduced to a backward induction of an optimization over\nsimplexes. For European options with payoff functions satisfying a\ncombinatorial property called submodularity or supermodularity, this\noptimization is solved in closed form by using the Lov\\'asz extension and the\nupper and lower hedging prices can be calculated efficiently. This class\nincludes the options on the maximum or the minimum of several assets. We also\nstudy the asymptotic behavior as the number of game rounds goes to infinity.\nThe upper and lower hedging prices of European options converge to the\nsolutions of the Black-Scholes-Barenblatt equations. For European options with\nsubmodular or supermodular payoff functions, the Black-Scholes-Barenblatt\nequation is reduced to the linear Black-Scholes equation and it is solved in\nclosed form. Numerical results show the validity of the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 09:21:31 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Matsuda", "Takeru", ""], ["Takemura", "Akimichi", ""]]}, {"id": "1806.08131", "submitter": "Alexandru Popa Dr.", "authors": "Alexandru Popa and Andrei Tanasescu", "title": "An output-sensitive algorithm for the minimization of 2-dimensional\n  String Covers", "comments": null, "journal-ref": "TAMC 2019", "doi": "10.1007/978-3-030-14812-6_33", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String covers are a powerful tool for analyzing the quasi-periodicity of\n1-dimensional data and find applications in automata theory, computational\nbiology, coding and the analysis of transactional data. A \\emph{cover} of a\nstring $T$ is a string $C$ for which every letter of $T$ lies within some\noccurrence of $C$. String covers have been generalized in many ways, leading to\n\\emph{k-covers}, \\emph{$\\lambda$-covers}, \\emph{approximate covers} and were\nstudied in different contexts such as \\emph{indeterminate strings}.\n  In this paper we generalize string covers to the context of 2-dimensional\ndata, such as images. We show how they can be used for the extraction of\ntextures from images and identification of primitive cells in lattice data.\nThis has interesting applications in image compression, procedural terrain\ngeneration and crystallography.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:23:03 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 06:05:06 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Popa", "Alexandru", ""], ["Tanasescu", "Andrei", ""]]}, {"id": "1806.08135", "submitter": "Alexandru Popa Dr.", "authors": "Alexandru Popa and Andrei Tanasescu", "title": "Hardness and algorithmic results for the approximate cover problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In CPM 2017, Amir et al. introduce a problem, named \\emph{approximate string\ncover} (\\textbf{ACP}), motivated by many aplications including coding and\nautomata theory, formal language theory, combinatorics and molecular biology. A\n\\emph{cover} of a string $T$ is a string $C$ for which every letter of $T$ lies\nwithin some occurrence of $C$. The input of the \\textbf{ACP} problem consists\nof a string $T$ and an integer $m$ (less than the length of $T$), and the goal\nis to find a string $C$ of length $m$ that covers a string $T'$ which is as\nclose to $T$ as possible (under some predefined distance). Amir et al. study\nthe problem for the Hamming distance.\n  In this paper we continue the work of Amir et al. and show the following\nresults:\n  We show an approximation algorithm for the \\textbf{ACP} with an approximation\nratio of $\\sqrt{OPT}$, where OPT is the size of the optimal solution.\n  We provide an FPT algorithm with respect to the alphabet size. \\item The\n\\textbf{ACP} problem naturally extends to pseudometrics. Moreover, we show that\nfor some family of pseudometrics, that we term \\emph{homogenous additive\npseudometrics}, the complexity of \\textbf{ACP} remains unchanged.\n  We partially give an answer to an open problem of Amir et al. and show that\nthe Hamming distance over an unbounded alphabet is equivalent to an extended\nmetric over a fixed sized alphabet.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:34:25 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Popa", "Alexandru", ""], ["Tanasescu", "Andrei", ""]]}, {"id": "1806.08182", "submitter": "Frederik Mallmann-Trenn", "authors": "Vincent Cohen-Addad, Frederik Mallmann-Trenn, Claire Mathieu", "title": "Instance-Optimality in the Noisy Value-and Comparison-Model --- Accept,\n  Accept, Strong Accept: Which Papers get in?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by crowdsourced computation, peer-grading, and recommendation\nsystems, Braverman, Mao and Weinberg [STOC'16] studied the \\emph{query} and\n\\emph{round} complexity of fundamental problems such as finding the maximum\n(\\textsc{max}), finding all elements above a certain value\n(\\textsc{threshold-$v$}) or computing the top$-k$ elements (\\textsc{Top}-$k$)\nin a noisy environment.\n  For example, consider the task of selecting papers for a conference. This\ntask is challenging due the crowdsourcing nature of peer reviews: the results\nof reviews are noisy and it is necessary to parallelize the review process as\nmuch as possible. We study the noisy value model and the noisy comparison\nmodel: In the \\emph{noisy value model}, a reviewer is asked to evaluate a\nsingle element: \"What is the value of paper $i$?\" (\\eg accept). In the\n\\emph{noisy comparison model} (introduced in the seminal work of Feige, Peleg,\nRaghavan and Upfal [SICOMP'94]) a reviewer is asked to do a pairwise\ncomparison: \"Is paper $i$ better than paper $j$?\"\n  In this paper, we show optimal worst-case query complexity for the\n\\textsc{max},\\textsc{threshold-$v$} and \\textsc{Top}-$k$ problems. For\n\\textsc{max} and \\textsc{Top}-$k$, we obtain optimal worst-case upper and lower\nbounds on the round vs query complexity in both models. For\n\\textsc{threshold}-$v$, we obtain optimal query complexity and nearly-optimal\nround complexity, where $k$ is the size of the output) for both models.\n  We then go beyond the worst-case and address the question of the importance\nof knowledge of the instance by providing, for a large range of parameters,\ninstance-optimal algorithms with respect to the query complexity. Furthermore,\nwe show that the value model is strictly easier than the comparison model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:50:39 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 17:35:16 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Mallmann-Trenn", "Frederik", ""], ["Mathieu", "Claire", ""]]}, {"id": "1806.08232", "submitter": "Alexandru Popa Dr.", "authors": "Alexandru Popa and Andrei Tanasescu", "title": "A connection between String Covers and Cover Deterministic Finite Tree\n  Automata Minimization", "comments": "15 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression plays a crucial part in the cloud based systems of today.\nOne the fundaments of compression is quasi-periodicity, for which there are\nseveral models. We build upon the most popular quasi-periodicity model for\nstrings, i.e., covers, generalizing it to trees.\n  We introduce a new type of cover automata, which we call\n\\textbf{D}eterministic \\textbf{T}ree \\textbf{A}utomata. Then, we formulate a\ncover problem on these DTA and study its complexity, in both sequential and\nparallel settings. We obtain bounds for the Cover Minimization Problem. Along\nthe way, we uncover an interesting application, the Shortest Common Cover\nProblem, for which we give an optimal solution.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:30:27 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Popa", "Alexandru", ""], ["Tanasescu", "Andrei", ""]]}, {"id": "1806.08283", "submitter": "Tilo Wiedera", "authors": "Markus Chimani and Tilo Wiedera", "title": "Cycles to the Rescue! Novel Constraints to Compute Maximum Planar\n  Subgraphs Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The NP-hard Maximum Planar Subgraph problem asks for a planar subgraph $H$ of\na given graph $G$ such that $H$ has maximum edge cardinality. For more than two\ndecades, the only known non-trivial exact algorithm was based on integer linear\nprogramming and Kuratowski's famous planarity criterion. We build upon this\napproach and present new constraint classes, together with a lifting of the\npolyhedron, to obtain provably stronger LP-relaxations, and in turn faster\nalgorithms in practice. The new constraints take Euler's polyhedron formula as\na starting point and combine it with considering cycles in $G$. This paper\ndiscusses both the theoretical as well as the practical sides of this\nstrengthening.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:02:58 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Chimani", "Markus", ""], ["Wiedera", "Tilo", ""]]}, {"id": "1806.08291", "submitter": "Duc A. Hoang", "authors": "Duc A. Hoang, Amanj Khorramian, Ryuhei Uehara", "title": "Shortest Reconfiguration Sequence for Sliding Tokens on Spiders", "comments": "Accepted for presentation at CIAC 2019", "journal-ref": null, "doi": "10.1007/978-3-030-17402-6_22", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Suppose that two independent sets $I$ and $J$ of a graph with $\\vert I \\vert\n= \\vert J \\vert$ are given, and a token is placed on each vertex in $I$. The\nSliding Token problem is to determine whether there exists a sequence of\nindependent sets which transforms $I$ into $J$ so that each independent set in\nthe sequence results from the previous one by sliding exactly one token along\nan edge in the graph. It is one of the representative reconfiguration problems\nthat attract the attention from the viewpoint of theoretical computer science.\nFor a yes-instance of a reconfiguration problem, finding a shortest\nreconfiguration sequence has a different aspect. In general, even if it is\npolynomial time solvable to decide whether two instances are reconfigured with\neach other, it can be $\\mathsf{NP}$-hard to find a shortest sequence between\nthem. In this paper, we show that the problem for finding a shortest sequence\nbetween two independent sets is polynomial time solvable for spiders (i.e.,\ntrees having exactly one vertex of degree at least three).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 15:32:45 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 06:41:33 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 15:03:58 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Hoang", "Duc A.", ""], ["Khorramian", "Amanj", ""], ["Uehara", "Ryuhei", ""]]}, {"id": "1806.08658", "submitter": "Behrooz Razeghi", "authors": "Behrooz Razeghi, Slava Voloshynovskiy, Sohrab Ferdowsi and Dimche\n  Kostadinov", "title": "Privacy-Preserving Identification via Layered Sparse Code Design:\n  Distributed Servers and Multiple Access Authorization", "comments": "EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB cs.DC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computationally efficient privacy-preserving identification\nframework based on layered sparse coding. The key idea of the proposed\nframework is a sparsifying transform learning with ambiguization, which\nconsists of a trained linear map, a component-wise nonlinearity and a privacy\namplification. We introduce a practical identification framework, which\nconsists of two phases: public and private identification. The public untrusted\nserver provides the fast search service based on the sparse privacy protected\ncodebook stored at its side. The private trusted server or the local client\napplication performs the refined accurate similarity search using the results\nof the public search and the layered sparse codebooks stored at its side. The\nprivate search is performed in the decoded domain and also the accuracy of\nprivate search is chosen based on the authorization level of the client. The\nefficiency of the proposed method is in computational complexity of encoding,\ndecoding, \"encryption\" (ambiguization) and \"decryption\" (purification) as well\nas storage complexity of the codebooks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:06:55 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Razeghi", "Behrooz", ""], ["Voloshynovskiy", "Slava", ""], ["Ferdowsi", "Sohrab", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1806.08692", "submitter": "L\\'aszl\\'o Kozma", "authors": "Dani Dorfman, Haim Kaplan, L\\'aszl\\'o Kozma, Seth Pettie, Uri Zwick", "title": "Improved bounds for multipass pairing heaps and path-balanced binary\n  search trees", "comments": "To be presented at ESA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit multipass pairing heaps and path-balanced binary search trees\n(BSTs), two classical algorithms for data structure maintenance. The pairing\nheap is a simple and efficient \"self-adjusting\" heap, introduced in 1986 by\nFredman, Sedgewick, Sleator, and Tarjan. In the multipass variant (one of the\noriginal pairing heap variants described by Fredman et al.) the minimum item is\nextracted via repeated pairing rounds in which neighboring siblings are linked.\n  Path-balanced BSTs, proposed by Sleator (Subramanian, 1996), are a natural\nalternative to Splay trees (Sleator and Tarjan, 1983). In a path-balanced BST,\nwhenever an item is accessed, the search path leading to that item is\nre-arranged into a balanced tree.\n  Despite their simplicity, both algorithms turned out to be difficult to\nanalyse. Fredman et al. showed that operations in multipass pairing heaps take\namortized $O(\\log{n} \\cdot \\log\\log{n} / \\log\\log\\log{n})$ time. For searching\nin path-balanced BSTs, Balasubramanian and Raman showed in 1995 the same\namortized time bound of $O(\\log{n} \\cdot \\log\\log{n} / \\log\\log\\log{n})$, using\na different argument.\n  In this paper we show an explicit connection between the two algorithms and\nimprove the two bounds to $O\\left(\\log{n} \\cdot 2^{\\log^{\\ast}{n}} \\cdot\n\\log^{\\ast}{n}\\right)$, respectively $O\\left(\\log{n} \\cdot 2^{\\log^{\\ast}{n}}\n\\cdot (\\log^{\\ast}{n})^2 \\right)$, where $\\log^{\\ast}(\\cdot)$ denotes the very\nslowly growing iterated logarithm function. These are the first improvements in\nmore than three, resp. two decades, approaching in both cases the\ninformation-theoretic lower bound of $\\Omega(\\log{n})$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 14:26:39 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Dorfman", "Dani", ""], ["Kaplan", "Haim", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Pettie", "Seth", ""], ["Zwick", "Uri", ""]]}, {"id": "1806.08831", "submitter": "Rushabh Jitendrakumar Shah", "authors": "Rushabh Jitendrakumar Shah", "title": "Efficient Graph Compression Using Huffman Coding Based Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have been extensively used to represent data from various domains. In\nthe era of Big Data, information is being generated at a fast pace, and\nanalyzing the same is a challenge. Various methods have been proposed to speed\nup the analysis of the data and also mining it for information. All of this\noften involves using a massive array of compute nodes, and transmitting the\ndata over the network. Of course, with the huge quantity of data, this poses a\nmajor issue to the task of gathering intelligence from data. Therefore, in\norder to address such issues with Big Data, using data compression techniques\nis a viable option. Since graphs represent most real world data, methods to\ncompress graphs have been in the forefront of such endeavors. In this paper we\npropose techniques to compress graphs by finding specific patterns and\nreplacing those with identifiers that are of variable length, an idea inspired\nby Huffman Coding. Specifically, given a graph G = (V, E), where V is the set\nof vertices and E is the set of edges, and |V| = n, we propose methods to\nreduce the space requirements of the graph by compressing the adjacency\nrepresentation of the same. The proposed methods show up to 80% reduction is\nthe space required to store the graphs as compared to using the adjacency\nmatrix. The methods can also be applied to other representations as well. The\nproposed techniques help solve the issues related to computing on the graphs on\nresources limited compute nodes, as well as reduce the latency for transfer of\ndata over the network in case of distributed computing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 06:28:36 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Shah", "Rushabh Jitendrakumar", ""]]}, {"id": "1806.08865", "submitter": "Charles Argue", "authors": "C.J. Argue and S\\'ebastien Bubeck and Michael B. Cohen and Anupam\n  Gupta and Yin Tat Lee", "title": "A Nearly-Linear Bound for Chasing Nested Convex Bodies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Friedman and Linial introduced the convex body chasing problem to explore the\ninterplay between geometry and competitive ratio in metrical task systems. In\nconvex body chasing, at each time step $t \\in \\mathbb{N}$, the online algorithm\nreceives a request in the form of a convex body $K_t \\subseteq \\mathbb{R}^d$\nand must output a point $x_t \\in K_t$. The goal is to minimize the total\nmovement between consecutive output points, where the distance is measured in\nsome given norm.\n  This problem is still far from being understood, and recently Bansal et al.\ngave an algorithm for the nested version, where each convex body is contained\nwithin the previous one. We propose a different strategy which is $O(d \\log\nd)$-competitive algorithm for this nested convex body chasing problem,\nimproving substantially over previous work. Our algorithm works for any norm.\nThis result is almost tight, given an $\\Omega(d)$ lower bound for the\n$\\ell_{\\infty}$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:41:05 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 17:16:11 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Argue", "C. J.", ""], ["Bubeck", "S\u00e9bastien", ""], ["Cohen", "Michael B.", ""], ["Gupta", "Anupam", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1806.08936", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Approximating some network problems with scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the shortest path and the minimum spanning tree problems in a\ngraph with $n$ nodes and $K$ cost scenarios (objectives) are discussed. In\norder to choose a solution the min-max criterion is applied. The minmax\nversions of both problems are hard to approximate within $O(\\log^{1-\\epsilon}\nK)$ for any $\\epsilon>0$. The best approximation algorithm for the min-max\nshortest path problem, known to date, has approximation ratio of $K$. On the\nother hand, for the min-max spanning tree, there is a randomized algorithm with\napproximation ratio of $O(\\log^2 n)$. In this paper a deterministic\n$O(\\sqrt{n\\log K/\\log\\log K})$-approximation algorithm for min-max shortest\npath is constructed. For min-max spanning tree a deterministic $O(\\log n \\log\nK/\\log\\log K)$-approximation algorithm is proposed, which works for a large\nclass of graphs and a randomized $O(\\log n)$-approximation algorithm, which can\nbe applied to all graphs, is constructed. It is also shown that the\napproximation ratios obtained are close to the integrality gaps of the\ncorresponding LP relaxations.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 09:44:37 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1806.08974", "submitter": "Eli Shamir", "authors": "Eli Shamir", "title": "Almost optimal Boolean matrix multiplication [BMM]-by multi-encoding of\n  rows and columns", "comments": "Proof is erroneous", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Boolean product $R = P \\cdot Q$ of two $\\{ 0, 1\\} \\; m \\times m \\; $\nmatrices is $$R(j,k) = 1 \\; \\mathrm{\\ IF\\ for\\ some\\ } \\; t \\; \\,P(j, t) = Q(t,\nk) = 1\\; \\; \\mathrm{ELSE\\ } \\, R(j, k) = 0. $$ The near-optimal design reduces\nthe complexity of computing $R$ from the standard $m^3$ to $O(m^{(2+e)})$, for\narbitrary small $e > 0$, by a practical algorithm. This renders reduced\ncomplexity to several graph-property tests: Finding triangles and higher-size\ncliques; finding all-pairs shortest paths, and more. Also, parsing a string $w$\nby a context-free grammar is reduced to near quadratic in $w$-size. The design\nuses several distinct 2-digit encodings: $j$ by $(j_1, j_2), \\; k \\, $ by $\\,\n(k_1, k_2)$. Each one gives rise to bunches of short digraphs from sources\n$j$'s to sinks $k$'s via switching nodes, and walks between them. The combined\ninformation, using the Chinese remainder theorem, leads to the correct values\nof $R(j, k)$.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 14:18:15 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 12:15:01 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Shamir", "Eli", ""]]}, {"id": "1806.09108", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Magnus Wahlstr\\\"om and Meirav Zehavi", "title": "On $r$-Simple $k$-Path and Related Problems Parameterized by $k/r$", "comments": "To appear in ACM Trans. on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abasi et al. (2014) and Gabizon et al. (2015) studied the following problems.\nIn the $r$-Simple $k$-Path problem, given a digraph $G$ on $n$ vertices and\nintegers $r,k$, decide whether $G$ has an $r$-simple $k$-path, which is a walk\nwhere every vertex occurs at most $r$ times and the total number of vertex\noccurrences is $k$. In the $(r,k)$-Monomial Detection problem, given an\narithmetic circuit that encodes some polynomial $P$ on $n$ variables and\nintegers $k,r$, decide whether $P$ has a monomial of degree $k$ where the\ndegree of each variable is at most~$r$. In the $p$-Set $(r,q)$-Packing problem,\ngiven a universe $V$, positive integers $p,q,r$, and a collection $\\cal H$ of\nsets of size $p$ whose elements belong to $V$, decide whether there exists a\nsubcollection ${\\cal H}'$ of $\\cal H$ of size $q$ where each element occurs in\nat most $r$ sets of ${\\cal H}'$. Abasi et al. and Gabizon et al. proved that\nthe three problems are single-exponentially fixed-parameter tractable (FPT)\nwhen parameterized by $(k/r)\\log r$, where $k=pq$ for $p$-Set $(r,q)$-Packing\nand asked whether the $\\log r$ factor in the exponent can be avoided.\n  We consider their question from a wider perspective: are the above problems\nFPT when parameterized by $k/r$ only? We resolve the wider question by (a)\nobtaining a $2^{O((k/r)^2\\log(k/r))} (n+\\log k)^{O(1)}$-time algorithm for\n$r$-Simple $k$-Path on digraphs and a $2^{O(k/r)} (n+\\log k)^{O(1)}$-time\nalgorithm for $r$-Simple $k$-Path on undirected graphs (i.e., for undirected\ngraphs we answer the original question in affirmative), (b) showing that\n$p$-Set $(r,q)$-Packing is FPT, and (c) proving that $(r,k)$-Monomial Detection\nis para-NP-hard. For $p$-Set $(r,q)$-Packing, we obtain a polynomial kernel for\nany fixed $p$, which resolves a question posed by Gabizon et al. regarding the\nexistence of polynomial kernels for problems with relaxed disjointness\nconstraints.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 08:48:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 19:34:26 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gutin", "Gregory", ""], ["Wahlstr\u00f6m", "Magnus", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1806.09189", "submitter": "Marvin K\\\"unnemann", "authors": "Marvin K\\\"unnemann", "title": "On Nondeterministic Derandomization of Freivalds' Algorithm:\n  Consequences, Avenues and Algorithmic Progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by studying the power of randomness, certifying algorithms and\nbarriers for fine-grained reductions, we investigate the question whether the\nmultiplication of two $n\\times n$ matrices can be performed in near-optimal\nnondeterministic time $\\tilde{O}(n^2)$. Since a classic algorithm due to\nFreivalds verifies correctness of matrix products probabilistically in time\n$O(n^2)$, our question is a relaxation of the open problem of derandomizing\nFreivalds' algorithm.\n  We discuss consequences of a positive or negative resolution of this problem\nand provide potential avenues towards resolving it. Particularly, we show that\nsufficiently fast deterministic verifiers for 3SUM or univariate polynomial\nidentity testing yield faster deterministic verifiers for matrix\nmultiplication. Furthermore, we present the partial algorithmic progress that\ndistinguishing whether an integer matrix product is correct or contains between\n1 and $n$ erroneous entries can be performed in time $\\tilde{O}(n^2)$ --\ninterestingly, the difficult case of deterministic matrix product verification\nis not a problem of \"finding a needle in the haystack\", but rather cancellation\neffects in the presence of many errors.\n  Our main technical contribution is a deterministic algorithm that corrects an\ninteger matrix product containing at most $t$ errors in time\n$\\tilde{O}(\\sqrt{t} n^2 + t^2)$. To obtain this result, we show how to compute\nan integer matrix product with at most $t$ nonzeroes in the same running time.\nThis improves upon known deterministic output-sensitive integer matrix\nmultiplication algorithms for $t = \\Omega(n^{2/3})$ nonzeroes, which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 18:19:09 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["K\u00fcnnemann", "Marvin", ""]]}, {"id": "1806.09251", "submitter": "Sahil Singla", "authors": "Euiwoong Lee and Sahil Singla", "title": "Optimal Online Contention Resolution Schemes via Ex-Ante Prophet\n  Inequalities", "comments": "Appears in ESA 2018, 16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online contention resolution schemes (OCRSs) were proposed by Feldman,\nSvensson, and Zenklusen as a generic technique to round a fractional solution\nin the matroid polytope in an online fashion. It has found applications in\nseveral stochastic combinatorial problems where there is a commitment\nconstraint: on seeing the value of a stochastic element, the algorithm has to\nimmediately and irrevocably decide whether to select it while always\nmaintaining an independent set in the matroid. Although OCRSs immediately lead\nto prophet inequalities, these prophet inequalities are not optimal. Can we\ninstead use prophet inequalities to design optimal OCRSs?\n  We design the first optimal $1/2$-OCRS for matroids by reducing the problem\nto designing a matroid prophet inequality where we compare to the stronger\nbenchmark of an ex-ante relaxation. We also introduce and design optimal\n$(1-1/e)$-random order CRSs for matroids, which are similar to OCRSs but the\narrival is chosen uniformly at random.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 01:57:38 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Lee", "Euiwoong", ""], ["Singla", "Sahil", ""]]}, {"id": "1806.09285", "submitter": "Michael Haythorpe", "authors": "Pouya Baniasadi, Vladimir Ejov, Michael Haythorpe and Serguei\n  Rossomakhine", "title": "A new benchmark set for Traveling salesman problem and Hamiltonian cycle\n  problem", "comments": "21 pages, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a benchmark set for Traveling salesman problem (TSP) with\ncharacteristics that are different from the existing benchmark sets. In\nparticular, we focus on small instances which prove to be challenging for one\nor more state-of-the-art TSP algorithms. These instances are based on difficult\ninstances of Hamiltonian cycle problem (HCP). This includes instances from\nliterature, specially modified randomly generated instances, and instances\narising from the conversion of other difficult problems to HCP. We demonstrate\nthat such benchmark instances are helpful in understanding the weaknesses and\nstrengths of algorithms. In particular, we conduct a benchmarking exercise for\nthis new benchmark set totalling over five years of CPU time, comparing the TSP\nalgorithms Concorde, Chained Lin-Kernighan, and LKH. We also include the HCP\nheuristic SLH in the benchmarking exercise. A discussion about the benefits of\nspecifically considering outlying instances, and in particular instances which\nare unusually difficult relative to size, is also included.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 04:48:34 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Baniasadi", "Pouya", ""], ["Ejov", "Vladimir", ""], ["Haythorpe", "Michael", ""], ["Rossomakhine", "Serguei", ""]]}, {"id": "1806.09383", "submitter": "Iddo Tzameret", "authors": "Fedor Part, Iddo Tzameret", "title": "Resolution with Counting: Dag-Like Lower Bounds and Different Moduli", "comments": "40 pages. To appear in ITCS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution over linear equations is a natural extension of the popular\nresolution refutation system, augmented with the ability to carry out basic\ncounting. Denoted Res(lin_R), this refutation system operates with disjunctions\nof linear equations with boolean variables over a ring R, to refute\nunsatisfiable sets of such disjunctions. Beginning in the work of [RT08],\nthrough the work of [IS14] which focused on tree-like lower bounds, this\nrefutation system was shown to be fairly strong. Subsequent work (cf.[Kra17,\nIS14, KO18, GK18]) made it evident that establishing lower bounds against\ngeneral Res(lin_R) refutations is a challenging and interesting task since the\nsystem captures a 'minimal' extension of resolution with counting gates for\nwhich no super-polynomial lower bounds are known to date.\n  We provide the first super-polynomial size lower bounds on general (dag-like)\nresolution over linear equations refutations in the large characteristic\nregime. In particular we prove that the subset-sum principle 1+x1+...+2^n xn=0\nrequires refutations of exponential size over Q. Our proof technique is\nnontrivial and novel: roughly speaking, we show that under certain conditions\nevery refutation of a subset-sum instance f=0 must pass through a fat clause\ncontaining an equation f=alpha for each alpha in the image of f under boolean\nassignments. We develop a somewhat different approach to prove exponential\nlower bounds against tree-like refutations of any subset-sum instance that\ndepends on n variables, hence also separating tree-like from dag-like\nrefutations over the rationals. (Abstract continued in the full paper.)\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 11:09:39 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 16:42:15 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Part", "Fedor", ""], ["Tzameret", "Iddo", ""]]}, {"id": "1806.09540", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Till Fluschnik and Oxana Yu. Tsidulko", "title": "Parameterized algorithms and data reduction for the short secluded\n  $s$-$t$-path problem", "comments": null, "journal-ref": "Networks, 75(1):34-63, 2020", "doi": "10.1002/net.21904", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$, two vertices $s,t\\in V$, and two integers $k,\\ell$,\nthe Short Secluded Path problem is to find a simple $s$-$t$-path with at most\n$k$ vertices and $\\ell$ neighbors. We study the parameterized complexity of the\nproblem with respect to four structural graph parameters: the vertex cover\nnumber, treewidth, feedback vertex number, and feedback edge number. In\nparticular, we completely settle the question of the existence of problem\nkernels with size polynomial in these parameters and their combinations with\n$k$ and $\\ell$. We also obtain a $2^{O(w)}\\cdot \\ell^2\\cdot n$-time algorithm\nfor graphs of treewidth $w$, which yields subexponential-time algorithms in\nseveral graph classes.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:53:36 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 12:44:02 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 12:41:30 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Fluschnik", "Till", ""], ["Tsidulko", "Oxana Yu.", ""]]}, {"id": "1806.09549", "submitter": "Ioannis Lamprou", "authors": "Ioannis Lamprou, Russell Martin, Sven Schewe, Ioannis Sigalas,\n  Vassilis Zissimopoulos", "title": "Maximum Rooted Connected Expansion", "comments": "15 pages, 1 figure, accepted at MFCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prefetching constitutes a valuable tool toward efficient Web surfing. As a\nresult, estimating the amount of resources that need to be preloaded during a\nsurfer's browsing becomes an important task. In this regard, prefetching can be\nmodeled as a two-player combinatorial game [Fomin et al., Theoretical Computer\nScience 2014], where a surfer and a marker alternately play on a given graph\n(representing the Web graph). During its turn, the marker chooses a set of $k$\nnodes to mark (prefetch), whereas the surfer, represented as a token resting on\ngraph nodes, moves to a neighboring node (Web resource). The surfer's objective\nis to reach an unmarked node before all nodes become marked and the marker\nwins. Intuitively, since the surfer is step-by-step traversing a subset of\nnodes in the Web graph, a satisfactory prefetching procedure would load in\ncache all resources lying in the neighborhood of this growing subset.\n  Motivated by the above, we consider the following problem to which we refer\nto as the Maximum Rooted Connected Expansion (MRCE) problem. Given a graph $G$\nand a root node $v_0$, we wish to find a subset of vertices $S$ such that $S$\nis connected, $S$ contains $v_0$ and the ratio $|N[S]|/|S|$ is maximized, where\n$N[S]$ denotes the closed neighborhood of $S$, that is, $N[S]$ contains all\nnodes in $S$ and all nodes with at least one neighbor in $S$.\n  We prove that the problem is NP-hard even when the input graph $G$ is\nrestricted to be a split graph. On the positive side, we demonstrate a\npolynomial time approximation scheme for split graphs. Furthermore, we present\na $\\frac{1}{6}(1-\\frac{1}{e})$-approximation algorithm for general graphs based\non techniques for the Budgeted Connected Domination problem [Khuller et al.,\nSODA 2014]. Finally, we provide a polynomial-time algorithm for the special\ncase of interval graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 16:12:36 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Lamprou", "Ioannis", ""], ["Martin", "Russell", ""], ["Schewe", "Sven", ""], ["Sigalas", "Ioannis", ""], ["Zissimopoulos", "Vassilis", ""]]}, {"id": "1806.09646", "submitter": "Tatiana Starikovskaya", "authors": "Pawe{\\l} Gawrychowski, Gad M. Landau, Tatiana Starikovskaya", "title": "Fast entropy-bounded string dictionary look-up with mismatches", "comments": "Full version of a paper accepted to MFCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the fundamental problem of dictionary look-up with mismatches.\nGiven a set (dictionary) of $d$ strings of length $m$ and an integer $k$, we\nmust preprocess it into a data structure to answer the following queries: Given\na query string $Q$ of length $m$, find all strings in the dictionary that are\nat Hamming distance at most $k$ from $Q$. Chan and Lewenstein (CPM 2015) showed\na data structure for $k = 1$ with optimal query time $O(m/w + occ)$, where $w$\nis the size of a machine word and $occ$ is the size of the output. The data\nstructure occupies $O(w d \\log^{1+\\varepsilon} d)$ extra bits of space (beyond\nthe entropy-bounded space required to store the dictionary strings). In this\nwork we give a solution with similar bounds for a much wider range of values\n$k$. Namely, we give a data structure that has $O(m/w + \\log^k d + occ)$ query\ntime and uses $O(w d \\log^k d)$ extra bits of space.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 18:02:05 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Landau", "Gad M.", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1806.09683", "submitter": "Philipp Zschoche", "authors": "Tomohiro Koana and Viatcheslav Korenwein and Andr\\'e Nichterlein and\n  Rolf Niedermeier and Philipp Zschoche", "title": "Data Reduction for Maximum Matching on Real-World Graphs: Theory and\n  Experiments", "comments": "An extended abstract of this work appeared at ESA '18. This version\n  has a new coauthor (Tomohiro Koana) and an extended experimental section\n  including comparison against two further implementations for finding\n  matchings. Moreover, it contains further data reduction rules (theoretical\n  and practical findings) for Maximum-Cardinality Matching and improved\n  theoretical bounds on the kernel sizes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a maximum-cardinality or maximum-weight matching in (edge-weighted)\nundirected graphs is among the most prominent problems of algorithmic graph\ntheory. For $n$-vertex and $m$-edge graphs, the best known algorithms run in\n$\\widetilde{O}(m\\sqrt{n})$ time. We build on recent theoretical work focusing\non linear-time data reduction rules for finding maximum-cardinality matchings\nand complement the theoretical results by presenting and analyzing (thereby\nemploying the kernelization methodology of parameterized complexity analysis)\nnew (near-)linear-time data reduction rules for both the unweighted and the\npositive-integer-weighted case. Moreover, we experimentally demonstrate that\nthese data reduction rules provide significant speedups of the state-of-the art\nimplementations for computing matchings in real-world graphs: the average\nspeedup factor is 4.7 in the unweighted case and 12.72 in the weighted case.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 19:43:29 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:17:15 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 16:42:35 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Koana", "Tomohiro", ""], ["Korenwein", "Viatcheslav", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""], ["Zschoche", "Philipp", ""]]}, {"id": "1806.09806", "submitter": "Ryo Yoshinaka", "authors": "Shintaro Narisada, Diptarama Hendrian, Ryo Yoshinaka, Ayumi Shinohara", "title": "Linear-Time Online Algorithm Inferring the Shortest Path from a Walk", "comments": "31 pages, 7 figures, extended version of the proceeding paper in\n  SPIRE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring an edge-labeled graph from the sequence\nof edge labels seen in a walk of that graph. It has been known that this\nproblem is solvable in $O(n \\log n)$ time when the targets are path or cycle\ngraphs. This paper presents an online algorithm for the problem of this\nrestricted case that runs in $O(n)$ time, based on Manacher's algorithm for\ncomputing all the maximal palindromes in a string.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:15:32 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 02:17:44 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 03:27:57 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Narisada", "Shintaro", ""], ["Hendrian", "Diptarama", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1806.09817", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Beyond Worst-Case Analysis", "comments": "To appear in Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the worst-case analysis of algorithms, the overall performance of an\nalgorithm is summarized by its worst performance on any input. This approach\nhas countless success stories, but there are also important computational\nproblems --- like linear programming, clustering, online caching, and neural\nnetwork training --- where the worst-case analysis framework does not provide\nany helpful advice on how to solve the problem. This article covers a number of\nmodeling methods for going beyond worst-case analysis and articulating which\ninputs are the most relevant.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:15:56 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "1806.09823", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Piotr Indyk, Ilya Razenshteyn", "title": "Approximate Nearest Neighbor Search in High Dimensions", "comments": "27 pages, no figures; to appear in the proceedings of ICM 2018\n  (accompanying the talk by P. Indyk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nearest neighbor problem is defined as follows: Given a set $P$ of $n$\npoints in some metric space $(X,D)$, build a data structure that, given any\npoint $q$, returns a point in $P$ that is closest to $q$ (its \"nearest\nneighbor\" in $P$). The data structure stores additional information about the\nset $P$, which is then used to find the nearest neighbor without computing all\ndistances between $q$ and $P$. The problem has a wide range of applications in\nmachine learning, computer vision, databases and other fields.\n  To reduce the time needed to find nearest neighbors and the amount of memory\nused by the data structure, one can formulate the {\\em approximate} nearest\nneighbor problem, where the the goal is to return any point $p' \\in P$ such\nthat the distance from $q$ to $p'$ is at most $c \\cdot \\min_{p \\in P} D(q,p)$,\nfor some $c \\geq 1$. Over the last two decades, many efficient solutions to\nthis problem were developed. In this article we survey these developments, as\nwell as their connections to questions in geometric functional analysis and\ncombinatorial geometry.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:35:45 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Andoni", "Alexandr", ""], ["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1806.09935", "submitter": "Mohamed El Yafrani", "authors": "Marcella S. R. Martins, Mohamed El Yafrani, Roberto Santana, Myriam\n  Delgado, Ricardo L\\\"uders, Bela\\\"id Ahiod", "title": "On the performance of multi-objective estimation of distribution\n  algorithms for combinatorial problems", "comments": "Accepted in IEEE WCCI/CEC '2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitness landscape analysis investigates features with a high influence on the\nperformance of optimization algorithms, aiming to take advantage of the\naddressed problem characteristics. In this work, a fitness landscape analysis\nusing problem features is performed for a Multi-objective Bayesian Optimization\nAlgorithm (mBOA) on instances of MNK-landscape problem for 2, 3, 5 and 8\nobjectives. We also compare the results of mBOA with those provided by NSGA-III\nthrough the analysis of their estimated runtime necessary to identify an\napproximation of the Pareto front. Moreover, in order to scrutinize the\nprobabilistic graphic model obtained by mBOA, the Pareto front is examined\naccording to a probabilistic view. The fitness landscape study shows that mBOA\nis moderately or loosely influenced by some problem features, according to a\nsimple and a multiple linear regression model, which is being proposed to\npredict the algorithms performance in terms of the estimated runtime. Besides,\nwe conclude that the analysis of the probabilistic graphic model produced at\nthe end of evolution can be useful to understand the convergence and diversity\nperformances of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:15:57 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Martins", "Marcella S. R.", ""], ["Yafrani", "Mohamed El", ""], ["Santana", "Roberto", ""], ["Delgado", "Myriam", ""], ["L\u00fcders", "Ricardo", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1806.09992", "submitter": "Keno Merckx", "authors": "Jean Cardinal, Jean-Paul Doignon, Keno Merckx", "title": "Finding a Maximum-Weight Convex Set in a Chordal Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural combinatorial optimization problem on chordal graphs,\nthe class of graphs with no induced cycle of length four or more. A subset of\nvertices of a chordal graph is (monophonically) convex if it contains the\nvertices of all chordless paths between any two vertices of the set. The\nproblem is to find a maximum-weight convex subset of a given vertex-weighted\nchordal graph. It generalizes previously studied special cases in trees and\nsplit graphs. It also happens to be closely related to the closure problem in\npartially ordered sets and directed graphs. We give the first polynomial-time\nalgorithm for the problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:54:51 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Cardinal", "Jean", ""], ["Doignon", "Jean-Paul", ""], ["Merckx", "Keno", ""]]}, {"id": "1806.10051", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Krzysztof Onak, Baruch Schieber, Shay Solomon", "title": "Fully Dynamic Maximal Independent Set with Sublinear in n Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first fully dynamic algorithm for maintaining a maximal independent set\n(MIS) with update time that is sublinear in the number of edges was presented\nrecently by the authors of this paper [Assadi et.al. STOC'18]. The algorithm is\ndeterministic and its update time is $O(m^{3/4})$, where $m$ is the\n(dynamically changing) number of edges. Subsequently, Gupta and Khan and\nindependently Du and Zhang [arXiv, April 2018] presented deterministic\nalgorithms for dynamic MIS with update times of $O(m^{2/3})$ and $O(m^{2/3}\n\\sqrt{\\log m})$, respectively. Du and Zhang also gave a randomized algorithm\nwith update time $\\widetilde{O}(\\sqrt{m})$. Moreover, they provided some\npartial (conditional) hardness results hinting that update time of\n$m^{1/2-\\epsilon}$, and in particular $n^{1-\\epsilon}$ for $n$-vertex dense\ngraphs, is a natural barrier for this problem for any constant $\\epsilon >0$,\nfor both deterministic and randomized algorithms that satisfy a certain natural\nproperty.\n  In this paper, we break this natural barrier and present the first fully\ndynamic (randomized) algorithm for maintaining an MIS with update time that is\nalways sublinear in the number of vertices, namely, an\n$\\widetilde{O}(\\sqrt{n})$ expected amortized update time algorithm. We also\nshow that a simpler variant of our algorithm can already achieve an\n$\\widetilde{O}(m^{1/3})$ expected amortized update time, which results in an\nimproved performance over our $\\widetilde{O}(\\sqrt{n})$ update time algorithm\nfor sufficiently sparse graphs, and breaks the $m^{1/2}$ barrier of Du and\nZhang for all values of $m$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:07:47 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Assadi", "Sepehr", ""], ["Onak", "Krzysztof", ""], ["Schieber", "Baruch", ""], ["Solomon", "Shay", ""]]}, {"id": "1806.10057", "submitter": "Anindya De", "authors": "Anindya De and Elchanan Mossel and Joe Neeman", "title": "Is your function low-dimensional?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing if a function depends on a small number of\nlinear directions of its input data. We call a function $f$ a linear $k$-junta\nif it is completely determined by some $k$-dimensional subspace of the input\nspace. In this paper, we study the problem of testing whether a given $n$\nvariable function $f : \\mathbb{R}^n \\to \\{0,1\\}$, is a linear $k$-junta or\n$\\epsilon$-far from all linear $k$-juntas, where the closeness is measured with\nrespect to the Gaussian measure on $\\mathbb{R}^n$. Linear $k$-juntas are a\ncommon generalization of two fundamental classes from Boolean function analysis\n(both of which have been studied in property testing) $\\textbf{1.}$ $k$- juntas\nwhich are functions on the Boolean cube which depend on at most k of the\nvariables and $\\textbf{2.}$ intersection of $k$ halfspaces, a fundamental\ngeometric concept class.\n  We show that the class of linear $k$-juntas is not testable, but adding a\nsurface area constraint makes it testable: we give a $\\mathsf{poly}(k \\cdot\ns/\\epsilon)$-query non-adaptive tester for linear $k$-juntas with surface area\nat most $s$. We show that the polynomial dependence on $s$ is necessary.\nMoreover, we show that if the function is a linear $k$-junta with surface area\nat most $s$, we give a $(s \\cdot k)^{O(k)}$-query non-adaptive algorithm to\nlearn the function up to a rotation of the basis. In particular, this implies\nthat we can test the class of intersections of $k$ halfspaces in $\\mathbb{R}^n$\nwith query complexity independent of $n$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:19:02 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 16:51:54 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["De", "Anindya", ""], ["Mossel", "Elchanan", ""], ["Neeman", "Joe", ""]]}, {"id": "1806.10176", "submitter": "Sebastian Berndt", "authors": "Max Bannach and Sebastian Berndt", "title": "Practical Access to Dynamic Programming on Tree Decompositions", "comments": "ESA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity theory has lead to a wide range of algorithmic\nbreakthroughs within the last decades, but the practicability of these methods\nfor real-world problems is still not well understood. We investigate the\npracticability of one of the fundamental approaches of this field: dynamic\nprogramming on tree decompositions. Indisputably, this is a key technique in\nparameterized algorithms and modern algorithm design. Despite the enormous\nimpact of this approach in theory, it still has very little influence on\npractical implementations. The reasons for this phenomenon are manifold. One of\nthem is the simple fact that such an implementation requires a long chain of\nnon-trivial tasks (as computing the decomposition, preparing it,...). We\nprovide an easy way to implement such dynamic programs that only requires the\ndefinition of the update rules. With this interface, dynamic programs for\nvarious problems, such as 3-coloring, can be implemented easily in about 100\nlines of structured Java code.\n  The theoretical foundation of the success of dynamic programming on tree\ndecompositions is well understood due to Courcelle's celebrated theorem, which\nstates that every MSO-definable problem can be efficiently solved if a tree\ndecomposition of small width is given. We seek to provide practical access to\nthis theorem as well, by presenting a lightweight model-checker for a small\nfragment of MSO. This fragment is powerful enough to describe many natural\nproblems, and our model-checker turns out to be very competitive against\nsimilar state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:12:10 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Bannach", "Max", ""], ["Berndt", "Sebastian", ""]]}, {"id": "1806.10210", "submitter": "Anne-Sophie Himmel", "authors": "Matthias Bentert, Anne-Sophie Himmel, Hendrik Molter, Marco Morik,\n  Rolf Niedermeier, Ren\\'e Saitenmacher", "title": "Listing All Maximal $k$-Plexes in Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world networks evolve over time, that is, new contacts appear and\nold contacts may disappear. They can be modeled as temporal graphs where\ninteractions between vertices (which represent people in the case of social\nnetworks) are represented by time-stamped edges. One of the most fundamental\nproblems in (social) network analysis is community detection, and one of the\nmost basic primitives to model a community is a clique. Addressing the problem\nof finding communities in temporal networks, Viard et al. [TCS 2016] introduced\n$\\Delta$-cliques as a natural temporal version of cliques. Himmel et al. [SNAM\n2017] showed how to adapt the well-known Bron-Kerbosch algorithm to enumerate\n$\\Delta$-cliques. We continue this work and improve and extend the algorithm of\nHimmel et al. to enumerate temporal $k$-plexes (notably, cliques are the\nspecial case $k=1$).\n  We define a $\\Delta$-$k$-plex as a set of vertices and a time interval, where\nduring this time interval each vertex has in each consecutive $\\Delta + 1$ time\nsteps at least one edge to all but at most $k-1$ vertices in the chosen set of\nvertices. We develop a recursive algorithm for enumerating all maximal\n$\\Delta$-$k$-plexs and perform experiments on real-world social networks that\ndemonstrate the practical feasibility of our approach. In particular, for the\nspecial case of $\\Delta$-1-plexes (that is, $\\Delta$-cliques), we observe that\nour algorithm is on average significantly faster than the previous algorithms\nby Himmel et al. [SNAM 2017] and Viard et al. [IPL 2018] for enumerating\n$\\Delta$-cliques.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 21:07:44 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 15:43:29 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 14:45:14 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 15:07:35 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Bentert", "Matthias", ""], ["Himmel", "Anne-Sophie", ""], ["Molter", "Hendrik", ""], ["Morik", "Marco", ""], ["Niedermeier", "Rolf", ""], ["Saitenmacher", "Ren\u00e9", ""]]}, {"id": "1806.10222", "submitter": "Brendan Juba", "authors": "John Hainline, Brendan Juba, Hai S.Le, David Woodruff", "title": "Conditional Sparse $\\ell_p$-norm Regression With Optimal Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following conditional linear regression problem: the task is\nto identify both (i) a $k$-DNF condition $c$ and (ii) a linear rule $f$ such\nthat the probability of $c$ is (approximately) at least some given bound $\\mu$,\nand $f$ minimizes the $\\ell_p$ loss of predicting the target $z$ in the\ndistribution of examples conditioned on $c$. Thus, the task is to identify a\nportion of the distribution on which a linear rule can provide a good fit.\nAlgorithms for this task are useful in cases where simple, learnable rules only\naccurately model portions of the distribution. The prior state-of-the-art for\nsuch algorithms could only guarantee finding a condition of probability\n$\\Omega(\\mu/n^k)$ when a condition of probability $\\mu$ exists, and achieved an\n$O(n^k)$-approximation to the target loss, where $n$ is the number of Boolean\nattributes. Here, we give efficient algorithms for solving this task with a\ncondition $c$ that nearly matches the probability of the ideal condition, while\nalso improving the approximation to the target loss. We also give an algorithm\nfor finding a $k$-DNF reference class for prediction at a given query point,\nthat obtains a sparse regression fit that has loss within $O(n^k)$ of optimal\namong all sparse regression parameters and sufficiently large $k$-DNF reference\nclasses containing the query point.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 21:49:37 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Hainline", "John", ""], ["Juba", "Brendan", ""], ["Le", "Hai S.", ""], ["Woodruff", "David", ""]]}, {"id": "1806.10254", "submitter": "Nuno Pregui\\c{c}a", "authors": "Nuno Pregui\\c{c}a", "title": "Conflict-free Replicated Data Types: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-scale distributed systems often replicate data at multiple\ngeographic locations to provide low latency and high availability, despite node\nand network failures. Geo-replicated systems that adopt a weak consistency\nmodel allow replicas to temporarily diverge, requiring a mechanism for merging\nconcurrent updates into a common state. Conflict-free Replicated Data Types\n(CRDT) provide a principled approach to address this problem. This document\npresents an overview of Conflict-free Replicated Data Types research and\npractice, organizing the presentation in the aspects relevant for the\napplication developer, the system developer and the CRDT developer.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 00:09:59 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Pregui\u00e7a", "Nuno", ""]]}, {"id": "1806.10261", "submitter": "Kensuke Kojima", "authors": "Kensuke Kojima", "title": "BDDs Naturally Represent Boolean Functions, and ZDDs Naturally Represent\n  Sets of Sets", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a difference between Binary Decision Diagrams (BDDs) and\nZero-suppressed BDDs (ZDDs) from a conceptual point of view. It is commonly\nunderstood that a BDD is a representation of a Boolean function, whereas a ZDD\nis a representation of a set of sets. However, there is a one-to-one\ncorrespondence between Boolean functions and sets of sets, and therefore we\ncould also regard a BDD as a representation of a set of sets, and similarly for\na ZDD and a Boolean function. The aim of this paper is to give an explanation\nwhy the distinction between BDDs and ZDDs mentioned above is made despite the\nexistence of the one-to-one correspondence. To achieve this, we first observe\nthat Boolean functions and sets of sets are equipped with non-isomorphic\nfunctor structures, and show that these functor structures are reflected in the\ndefinitions of BDDs and ZDDs. This result can be stated formally as naturality\nof certain maps. To the author's knowledge, this is the first formally stated\ntheorem that justifies the commonly accepted distinction between BDDs and ZDDs.\nIn addition, we show that this result extends to sentential decision diagrams\nand their zero-suppressed variant.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 00:58:24 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Kojima", "Kensuke", ""]]}, {"id": "1806.10327", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta and Chris Sholley", "title": "Online Matching in a Ride-Sharing Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formal graph-theoretic model for studying the problem of\nmatching rides online in a ride-sharing platform. Unlike most of the literature\non online matching, our model, that we call {\\em Online Windowed Non-Bipartite\nMatching} ($\\mbox{OWNBM}$), pertains to online matching in {\\em non-bipartite}\ngraphs. We show that the edge-weighted and vertex-weighted versions of our\nmodel arise naturally in ride-sharing platforms. We provide a randomized\n$\\frac{1}{4}$-competitive algorithm for the edge-weighted case using a\nbeautiful result of Lehmann, Lehmann and Nisan (EC 2001) for combinatorial\nauctions. We also provide an $\\frac{1}{2} (1 - \\frac{1}{e})$-competitive\nalgorithm for the vertex-weighted case (with some constraint relaxation) using\ninsights from an elegant randomized primal-dual analysis technique of Devanur,\nJain and Kleinberg (SODA 2013).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 07:52:11 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Dutta", "Chinmoy", ""], ["Sholley", "Chris", ""]]}, {"id": "1806.10370", "submitter": "Yan Gu", "authors": "Yan Gu, Yihan Sun, Guy E. Blelloch", "title": "Algorithmic Building Blocks for Asymmetric Memories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future of main memory appears to lie in the direction of new non-volatile\nmemory technologies that provide strong capacity-to-performance ratios, but\nhave write operations that are much more expensive than reads in terms of\nenergy, bandwidth, and latency. This asymmetry can have a significant effect on\nalgorithm design, and in many cases it is possible to reduce writes at the cost\nof reads. In this paper, we study which algorithmic techniques are useful in\ndesigning practical write-efficient algorithms. We focus on several fundamental\nalgorithmic building blocks including unordered set/map implemented using hash\ntables, ordered set/map implemented using various binary search trees,\ncomparison sort, and graph traversal algorithms including breadth-first search\nand Dijkstra's algorithm. We introduce new algorithms and implementations that\ncan reduce writes, and analyze the performance experimentally using a software\nsimulator. Finally we summarize interesting lessons and directions in designing\nwrite-efficient algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 09:40:04 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Gu", "Yan", ""], ["Sun", "Yihan", ""], ["Blelloch", "Guy E.", ""]]}, {"id": "1806.10460", "submitter": "Andrzej Kaczmarczyk", "authors": "Robert Bredereck and Andrzej Kaczmarczyk and Rolf Niedermeier", "title": "On Coalitional Manipulation for Multiwinner Elections: Shortlisting", "comments": "An extended abstract of this work appeared in Proceedings of the\n  Twenty-Sixth International Joint Conference on Artificial Intelligence\n  (IJCAI-17), pp. 887-893, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortlisting of candidates--selecting a group of \"best\" candidates--is a\nspecial case of multiwinner elections. We provide the first in-depth study of\nthe computational complexity of strategic voting for shortlisting based on the\nperhaps most basic voting rule in this scenario, l-Bloc (every voter approves l\ncandidates). In particular, we investigate the influence of several\ntie-breaking mechanisms (e.g., pessimistic versus optimistic) and group\nevaluation functions (e.g., egalitarian versus utilitarian). Among other\nthings, conclude that in an egalitarian setting strategic voting may indeed be\ncomputationally intractable regardless of the tie-breaking rule. Altogether, we\nprovide a fairly comprehensive picture of the computational complexity\nlandscape so far in the literature of this neglected scenario.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 13:13:09 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 13:59:36 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Bredereck", "Robert", ""], ["Kaczmarczyk", "Andrzej", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1806.10498", "submitter": "Yakov Nekrich", "authors": "Mordecai Golin, John Iacono, Stefan Langerman, J. Ian Munro, Yakov\n  Nekrich", "title": "Dynamic Trees with Almost-Optimal Access Cost", "comments": "Full version of an ESA'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An optimal binary search tree for an access sequence on elements is a static\ntree that minimizes the total search cost. Constructing perfectly optimal\nbinary search trees is expensive so the most efficient algorithms construct\nalmost optimal search trees. There exists a long literature of constructing\nalmost optimal search trees dynamically, i.e., when the access pattern is not\nknown in advance. All of these trees, e.g., splay trees and treaps, provide a\nmultiplicative approximation to the optimal search cost.\n  In this paper we show how to maintain an almost optimal weighted binary\nsearch tree under access operations and insertions of new elements where the\napproximation is an additive constant. More technically, we maintain a tree in\nwhich the depth of the leaf holding an element $e_i$ does not exceed\n$\\min(\\log(W/w_i),\\log n)+O(1)$ where $w_i$ is the number of times $e_i$ was\naccessed and $W$ is the total length of the access sequence.\n  Our techniques can also be used to encode a sequence of $m$ symbols with a\ndynamic alphabetic code in $O(m)$ time so that the encoding length is bounded\nby $m(H+O(1))$, where $H$ is the entropy of the sequence. This is the first\nefficient algorithm for adaptive alphabetic coding that runs in constant time\nper symbol.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 14:27:02 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Golin", "Mordecai", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""], ["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1806.10501", "submitter": "Bart M. P. Jansen", "authors": "Bart M.P. Jansen and Jesper Nederlof", "title": "Computing the Chromatic Number Using Graph Decompositions via Matrix\n  Rank", "comments": "29 pages. An extended abstract appears in the proceedings of the 26th\n  Annual European Symposium on Algorithms, ESA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the smallest number $q$ such that the vertices of a given graph can\nbe properly $q$-colored is one of the oldest and most fundamental problems in\ncombinatorial optimization. The $q$-Coloring problem has been studied\nintensively using the framework of parameterized algorithmics, resulting in a\nvery good understanding of the best-possible algorithms for several\nparameterizations based on the structure of the graph. While there is an\nabundance of work for parameterizations based on decompositions of the graph by\nvertex separators, almost nothing is known about parameterizations based on\nedge separators. We fill this gap by studying $q$-Coloring parameterized by\ncutwidth, and parameterized by pathwidth in bounded-degree graphs. Our research\nuncovers interesting new ways to exploit small edge separators.\n  We present two algorithms for $q$-Coloring parameterized by cutwidth $cutw$:\na deterministic one that runs in time $O^*(2^{\\omega \\cdot cutw})$, where\n$\\omega$ is the matrix multiplication constant, and a randomized one with\nruntime $O^*(2^{cutw})$. In sharp contrast to earlier work, the running time is\nindependent of $q$. The dependence on cutwidth is optimal: we prove that even\n3-Coloring cannot be solved in $O^*((2-\\varepsilon)^{cutw})$ time assuming the\nStrong Exponential Time Hypothesis (SETH). Our algorithms rely on a new rank\nbound for a matrix that describes compatible colorings. Combined with a simple\ncommunication protocol for evaluating a product of two polynomials, this also\nyields an $O^*((\\lfloor d/2\\rfloor+1)^{pw})$ time randomized algorithm for\n$q$-Coloring on graphs of pathwidth $pw$ and maximum degree $d$. Such a runtime\nwas first obtained by Bj\\\"orklund, but only for graphs with few proper\ncolorings. We also prove that this result is optimal in the sense that no\n$O^*((\\lfloor d/2\\rfloor+1-\\varepsilon)^{pw})$-time algorithm exists assuming\nSETH.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 14:32:40 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1806.10513", "submitter": "Bart M. P. Jansen", "authors": "Bas A.M. van Geffen and Bart M.P. Jansen and Arnoud A.W.M. de Kroon\n  and Rolf Morel", "title": "Lower Bounds for Dynamic Programming on Planar Graphs of Bounded\n  Cutwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial problems can be solved in time $O^*(c^{tw})$ on graphs of\ntreewidth $tw$, for a problem-specific constant $c$. In several cases, matching\nupper and lower bounds on $c$ are known based on the Strong Exponential Time\nHypothesis (SETH). In this paper we investigate the complexity of solving\nproblems on graphs of bounded cutwidth, a graph parameter that takes larger\nvalues than treewidth. We strengthen earlier treewidth-based lower bounds to\nshow that, assuming SETH, Independent Set cannot be solved in\n$O^*((2-\\varepsilon)^{cutw})$ time, and Dominating Set cannot be solved in\n$O^*((3-\\varepsilon)^{cutw})$ time. By designing a new crossover gadget, we\nextend these lower bounds even to planar graphs of bounded cutwidth or\ntreewidth. Hence planarity does not help when solving Independent Set or\nDominating Set on graphs of bounded width. This sharply contrasts the fact that\nin many settings, planarity allows problems to be solved much more efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 14:56:15 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["van Geffen", "Bas A. M.", ""], ["Jansen", "Bart M. P.", ""], ["de Kroon", "Arnoud A. W. M.", ""], ["Morel", "Rolf", ""]]}, {"id": "1806.10586", "submitter": "Yu Bai", "authors": "Yu Bai, Tengyu Ma, Andrej Risteski", "title": "Approximability of Discriminators Implies Diversity in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) have empirically produced\nimpressive results on learning complex real-world distributions, recent works\nhave shown that they suffer from lack of diversity or mode collapse. The\ntheoretical work of Arora et al. suggests a dilemma about GANs' statistical\nproperties: powerful discriminators cause overfitting, whereas weak\ndiscriminators cannot detect mode collapse.\n  By contrast, we show in this paper that GANs can in principle learn\ndistributions in Wasserstein distance (or KL-divergence in many cases) with\npolynomial sample complexity, if the discriminator class has strong\ndistinguishing power against the particular generator class (instead of against\nall possible generators). For various generator classes such as mixture of\nGaussians, exponential families, and invertible and injective neural networks\ngenerators, we design corresponding discriminators (which are often neural nets\nof specific architectures) such that the Integral Probability Metric (IPM)\ninduced by the discriminators can provably approximate the Wasserstein distance\nand/or KL-divergence. This implies that if the training is successful, then the\nlearned distribution is close to the true distribution in Wasserstein distance\nor KL divergence, and thus cannot drop modes. Our preliminary experiments show\nthat on synthetic datasets the test IPM is well correlated with KL divergence\nor the Wasserstein distance, indicating that the lack of diversity in GANs may\nbe caused by the sub-optimality in optimization instead of statistical\ninefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 17:33:52 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 06:15:38 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 06:19:39 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 05:59:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bai", "Yu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1806.10626", "submitter": "Amit Levi", "authors": "Amit Levi, Yuichi Yoshida", "title": "Sublinear-Time Quadratic Minimization via Spectral Decomposition of\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a sublinear-time approximation algorithm for quadratic function\nminimization problems with a better error bound than the previous algorithm by\nHayashi and Yoshida (NIPS'16). Our approximation algorithm can be modified to\nhandle the case where the minimization is done over a sphere. The analysis of\nour algorithms is obtained by combining results from graph limit theory, along\nwith a novel spectral decomposition of matrices. Specifically, we prove that a\nmatrix $A$ can be decomposed into a structured part and a pseudorandom part,\nwhere the structured part is a block matrix with a polylogarithmic number of\nblocks, such that in each block all the entries are the same, and the\npseudorandom part has a small spectral norm, achieving better error bound than\nthe existing decomposition theorem of Frieze and Kannan (FOCS'96). As an\nadditional application of the decomposition theorem, we give a sublinear-time\napproximation algorithm for computing the top singular values of a matrix.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 18:05:06 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Levi", "Amit", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1806.10660", "submitter": "Devorah Kletenik", "authors": "Dimitrios Gkenosis, Nathaniel Grammel, Lisa Hellerstein, and Devorah\n  Kletenik", "title": "The Stochastic Score Classification Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following Stochastic Score Classification Problem. A doctor is\nassessing a patient's risk of developing a certain disease, and can perform $n$\ntests on the patient. Each test has a binary outcome, positive or negative. A\npositive test result is an indication of risk, and a patient's score is the\ntotal number of positive test results. The doctor needs to classify the patient\ninto one of $B$ risk classes, depending on the score (e.g., LOW, MEDIUM, and\nHIGH risk). Each of these classes corresponds to a contiguous range of scores.\nTest $i$ has probability $p_i$ of being positive, and it costs $c_i$ to perform\nthe test. To reduce costs, instead of performing all tests, the doctor will\nperform them sequentially and stop testing when it is possible to determine the\nrisk category for the patient. The problem is to determine the order in which\nthe doctor should perform the tests, so as to minimize the expected testing\ncost. We provide approximation algorithms for adaptive and non-adaptive\nversions of this problem, and pose a number of open questions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:59:00 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Gkenosis", "Dimitrios", ""], ["Grammel", "Nathaniel", ""], ["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""]]}, {"id": "1806.10772", "submitter": "Jacob Holm", "authors": "Jacob Holm, Giuseppe F. Italiano, Adam Karczmarz, Jakub {\\L}\\k{a}cki,\n  Eva Rotenberg", "title": "Decremental SPQR-trees for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a decremental data structure for maintaining the SPQR-tree of a\nplanar graph subject to edge contractions and deletions. The update time,\namortized over $\\Omega(n)$ operations, is $O(\\log^2 n)$.\n  Via SPQR-trees, we give a decremental data structure for maintaining\n$3$-vertex connectivity in planar graphs. It answers queries in $O(1)$ time and\nprocesses edge deletions and contractions in $O(\\log^2 n)$ amortized time. This\nis an exponential improvement over the previous best bound of $O(\\sqrt{n}\\,)$\nthat has stood for over 20 years. In addition, the previous data structures\nonly supported edge deletions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 05:17:44 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Holm", "Jacob", ""], ["Italiano", "Giuseppe F.", ""], ["Karczmarz", "Adam", ""], ["\u0141\u0105cki", "Jakub", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1806.10952", "submitter": "Yun Kuen Cheung", "authors": "Yun Kuen Cheung, Richard Cole", "title": "Amortized Analysis of Asynchronous Price Dynamics", "comments": "17 pages. In ESA 2018. arXiv admin note: text overlap with\n  arXiv:1612.09171", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a recently developed framework for analyzing asynchronous\ncoordinate descent algorithms to show that an asynchronous version of\ntatonnement, a fundamental price dynamic widely studied in general equilibrium\ntheory, converges toward a market equilibrium for Fisher markets with CES\nutilities or Leontief utilities, for which tatonnement is equivalent to\ncoordinate descent.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 05:35:03 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Cole", "Richard", ""]]}, {"id": "1806.10964", "submitter": "Tigran Tonoyan", "authors": "Magnus M. Halldorsson, Tigran Tonoyan", "title": "Effective Wireless Scheduling via Hypergraph Sketches", "comments": "38 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An overarching issue in resource management of wireless networks is assessing\ntheir capacity: How much communication can be achieved in a network, utilizing\nall the tools available: power control, scheduling, routing, channel assignment\nand rate adjustment? We propose the first framework for approximation\nalgorithms in the physical model of wireless interference that addresses these\nquestions in full. The approximations obtained are at most doubly logarithmic\nin the link length and rate diversity. Where previous bounds are known, this\ngives an exponential improvement (or better).\n  A key contribution is showing that the complex interference relationship of\nthe physical model can be simplified, at a small cost, into a novel type of\namenable conflict graphs. We also show that the approximation obtained is\nprovably the best possible for any conflict graph formulation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 13:41:07 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "1806.11276", "submitter": "Caitlin Gray", "authors": "Caitlin Gray, Lewis Mitchell, Matthew Roughan", "title": "Generating Connected Random Graphs", "comments": "Added references, Expanded Implementation - same conclusions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling random graphs is essential in many applications, and often\nalgorithms use Markov chain Monte Carlo methods to sample uniformly from the\nspace of graphs. However, often there is a need to sample graphs with some\nproperty that we are unable, or it is too inefficient, to sample using standard\napproaches. In this paper, we are interested in sampling graphs from a\nconditional ensemble of the underlying graph model. We present an algorithm to\ngenerate samples from an ensemble of connected random graphs using a\nMetropolis-Hastings framework. The algorithm extends to a general framework for\nsampling from a known distribution of graphs, conditioned on a desired\nproperty. We demonstrate the method to generate connected spatially embedded\nrandom graphs, specifically the well known Waxman network, and illustrate the\nconvergence and practicalities of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 06:03:20 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 00:54:04 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Gray", "Caitlin", ""], ["Mitchell", "Lewis", ""], ["Roughan", "Matthew", ""]]}, {"id": "1806.11282", "submitter": "Ryan Mann", "authors": "Ryan L. Mann and Michael J. Bremner", "title": "Approximation Algorithms for Complex-Valued Ising Models on Bounded\n  Degree Graphs", "comments": "12 pages, 0 figures, published version", "journal-ref": "Quantum 3, 162 (2019)", "doi": "10.22331/q-2019-07-11-162", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.CC cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of approximating the Ising model partition function with\ncomplex parameters on bounded degree graphs. We establish a deterministic\npolynomial-time approximation scheme for the partition function when the\ninteractions and external fields are absolutely bounded close to zero.\nFurthermore, we prove that for this class of Ising models the partition\nfunction does not vanish. Our algorithm is based on an approach due to Barvinok\nfor approximating evaluations of a polynomial based on the location of the\ncomplex zeros and a technique due to Patel and Regts for efficiently computing\nthe leading coefficients of graph polynomials on bounded degree graphs.\nFinally, we show how our algorithm can be extended to approximate certain\noutput probability amplitudes of quantum circuits.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 07:16:27 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 03:16:47 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Mann", "Ryan L.", ""], ["Bremner", "Michael J.", ""]]}, {"id": "1806.11377", "submitter": "Rune Kok Nielsen", "authors": "Rune Kok Nielsen, Andreas Nugaard Holm, Aasa Feragen", "title": "Learning from graphs with structural variation", "comments": "Presented at the NIPS 2017 workshop \"Learning on Distributions,\n  Functions, Graphs and Groups\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of structural variation in graph data on the predictive\nperformance of graph kernels. To this end, we introduce a novel, noise-robust\nadaptation of the GraphHopper kernel and validate it on benchmark data,\nobtaining modestly improved predictive performance on a range of datasets.\nNext, we investigate the performance of the state-of-the-art Weisfeiler-Lehman\ngraph kernel under increasing synthetic structural errors and find that the\neffect of introducing errors depends strongly on the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:29:12 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Nielsen", "Rune Kok", ""], ["Holm", "Andreas Nugaard", ""], ["Feragen", "Aasa", ""]]}, {"id": "1806.11413", "submitter": "Timothy Randolph", "authors": "Emilio Di Giacomo, William J. Lenhart, Giuseppe Liotta, Timothy W.\n  Randolph and Alessandra Tappini", "title": "(k,p)-Planarity: A Relaxation of Hybrid Planarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for hybrid planarity that relaxes existing hybrid\nrepresentations. A graph $G = (V,E)$ is $(k,p)$-planar if $V$ can be\npartitioned into clusters of size at most $k$ such that $G$ admits a drawing\nwhere: (i) each cluster is associated with a closed, bounded planar region,\ncalled a cluster region; (ii) cluster regions are pairwise disjoint, (iii) each\nvertex $v \\in V$ is identified with at most $p$ distinct points, called\n\\emph{ports}, on the boundary of its cluster region; (iv) each inter-cluster\nedge $(u,v) \\in E$ is identified with a Jordan arc connecting a port of $u$ to\na port of $v$; (v) inter-cluster edges do not cross or intersect cluster\nregions except at their endpoints. We first tightly bound the number of edges\nin a $(k,p)$-planar graph with $p<k$. We then prove that $(4,1)$-planarity\ntesting and $(2,2)$-planarity testing are NP-complete problems. Finally, we\nprove that neither the class of $(2,2)$-planar graphs nor the class of\n$1$-planar graphs contains the other, indicating that the $(k,p)$-planar graphs\nare a large and novel class.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:44:50 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 18:52:49 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Di Giacomo", "Emilio", ""], ["Lenhart", "William J.", ""], ["Liotta", "Giuseppe", ""], ["Randolph", "Timothy W.", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1806.11527", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Oxana Yu. Tsidulko and Philipp Zschoche", "title": "Representative families for matroid intersections, with applications to\n  location, packing, and covering problems", "comments": "Restructuring (focus on representative families instead of facility\n  location), slight running time improvements, algorithms factored out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show algorithms for computing representative families for matroid\nintersections and use them in fixed-parameter algorithms for set packing, set\ncovering, and facility location problems with multiple matroid constraints. We\ncomplement our tractability results by hardness results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 16:45:42 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 08:03:36 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 08:51:55 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 10:00:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Tsidulko", "Oxana Yu.", ""], ["Zschoche", "Philipp", ""]]}, {"id": "1806.11542", "submitter": "Arya Mazumdar", "authors": "Raj Kumar Maity, Arya Mazumdar, Soumyabrata Pal", "title": "High Dimensional Discrete Integration over the Hypergrid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Ermon et al. (2013) pioneered a way to practically compute\napproximations to large scale counting or discrete integration problems by\nusing random hashes. The hashes are used to reduce the counting problem into\nmany separate discrete optimization problems. The optimization problems then\ncan be solved by an NP-oracle such as commercial SAT solvers or integer linear\nprogramming (ILP) solvers. In particular, Ermon et al. showed that if the\ndomain of integration is $\\{0,1\\}^n$ then it is possible to obtain a solution\nwithin a factor of $16$ of the optimal (a 16-approximation) by this technique.\n  In many crucial counting tasks, such as computation of partition function of\nferromagnetic Potts model, the domain of integration is naturally $\\{0,1,\\dots,\nq-1\\}^n, q>2$, the hypergrid. The straightforward extension of Ermon et al.'s\nmethod allows a $q^2$-approximation for this problem. For large values of $q$,\nthis is undesirable. In this paper, we show an improved technique to obtain an\napproximation factor of $4+O(1/q^2)$ to this problem. We are able to achieve\nthis by using an idea of optimization over multiple bins of the hash functions,\nthat can be easily implemented by inequality constraints, or even in\nunconstrained way. Also the burden on the NP-oracle is not increased by our\nmethod (an ILP solver can still be used). We provide experimental simulation\nresults to support the theoretical guarantees of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:14:10 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 20:13:13 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 01:21:43 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Maity", "Raj Kumar", ""], ["Mazumdar", "Arya", ""], ["Pal", "Soumyabrata", ""]]}, {"id": "1806.11548", "submitter": "Will Perkins", "authors": "Tyler Helmuth and Will Perkins and Guus Regts", "title": "Algorithmic Pirogov-Sinai theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math-ph math.CO math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient algorithmic approach for approximate counting and\nsampling in the low-temperature regime of a broad class of statistical physics\nmodels on finite subsets of the lattice $\\mathbb Z^d$ and on the torus\n$(\\mathbb Z/n \\mathbb Z)^d$. Our approach is based on combining contour\nrepresentations from Pirogov-Sinai theory with Barvinok's approach to\napproximate counting using truncated Taylor series. Some consequences of our\nmain results include an FPTAS for approximating the partition function of the\nhard-core model at sufficiently high fugacity on subsets of $\\mathbb Z^d$ with\nappropriate boundary conditions and an efficient sampling algorithm for the\nferromagnetic Potts model on the discrete torus $(\\mathbb Z/n \\mathbb Z)^d$ at\nsufficiently low temperature.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:32:10 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 19:02:04 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Helmuth", "Tyler", ""], ["Perkins", "Will", ""], ["Regts", "Guus", ""]]}]