[{"id": "1705.00055", "submitter": "Klaus-Tycho Foerster", "authors": "Saeed Akhoondian Amiri, Klaus-Tycho Foerster, Riko Jacob, Stefan\n  Schmid", "title": "Charting the Complexity Landscape of Waypoint Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer networks support interesting new routing models in which\ntraffic flows from a source s to a destination t can be flexibly steered\nthrough a sequence of waypoints, such as (hardware) middleboxes or\n(virtualized) network functions, to create innovative network services like\nservice chains or segment routing. While the benefits and technological\nchallenges of providing such routing models have been articulated and studied\nintensively over the last years, much less is known about the underlying\nalgorithmic traffic routing problems. This paper shows that the waypoint\nrouting problem features a deep combinatorial structure, and we establish\ninteresting connections to several classic graph theoretical problems. We find\nthat the difficulty of the waypoint routing problem depends on the specific\nsetting, and chart a comprehensive landscape of the computational complexity.\nIn particular, we derive several NP-hardness results, but we also demonstrate\nthat exact polynomial-time algorithms exist for a wide range of practically\nrelevant scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 19:49:49 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 03:45:46 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Foerster", "Klaus-Tycho", ""], ["Jacob", "Riko", ""], ["Schmid", "Stefan", ""]]}, {"id": "1705.00127", "submitter": "Vaggos Chatziafratis", "authors": "Vaggos Chatziafratis, Tim Roughgarden and Jan Vondrak", "title": "Stability and Recovery for Independence Systems", "comments": "version 3, after some reviews/fixes in pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two genres of heuristics that are frequently reported to perform much better\non \"real-world\" instances than in the worst case are greedy algorithms and\nlocal search algorithms. In this paper, we systematically study these two types\nof algorithms for the problem of maximizing a monotone submodular set function\nsubject to downward-closed feasibility constraints. We consider\nperturbation-stable instances, in the sense of Bilu and Linial, and precisely\nidentify the stability threshold beyond which these algorithms are guaranteed\nto recover the optimal solution. Byproducts of our work include the first\ndefinition of perturbation-stability for non-additive objective functions, and\na resolution of the worst-case approximation guarantee of local search in\np-extendible systems.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 04:37:37 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 05:46:52 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 05:13:53 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Chatziafratis", "Vaggos", ""], ["Roughgarden", "Tim", ""], ["Vondrak", "Jan", ""]]}, {"id": "1705.00145", "submitter": "Venkatesan Chakaravarthy", "authors": "Anshul Aggarwal, Venkatesan T. Chakaravarthy, Neelima Gupta, Yogish\n  Sabharwal, Sachin Sharma, Sonika Thakral", "title": "Replica Placement on Bounded Treewidth Graphs", "comments": "An abridged version of this paper is to appear in the proceedings of\n  WADS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the replica placement problem: given a graph with clients and\nnodes, place replicas on a minimum set of nodes to serve all the clients; each\nclient is associated with a request and maximum distance that it can travel to\nget served and there is a maximum limit (capacity) on the amount of request a\nreplica can serve. The problem falls under the general framework of capacitated\nset covering. It admits an O(\\log n)-approximation and it is NP-hard to\napproximate within a factor of $o(\\log n)$. We study the problem in terms of\nthe treewidth $t$ of the graph and present an O(t)-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 07:35:04 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 08:48:46 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 11:47:19 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Aggarwal", "Anshul", ""], ["Chakaravarthy", "Venkatesan T.", ""], ["Gupta", "Neelima", ""], ["Sabharwal", "Yogish", ""], ["Sharma", "Sachin", ""], ["Thakral", "Sonika", ""]]}, {"id": "1705.00314", "submitter": "Hongfei Fu", "authors": "Krishnendu Chatterjee, Hongfei Fu, Aniket Murhekar", "title": "Automated Recurrence Analysis for Almost-Linear Expected-Runtime Bounds", "comments": "41 pages, Full Version to CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of developing automated techniques for solving\nrecurrence relations to aid the expected-runtime analysis of programs. Several\nclassical textbook algorithms have quite efficient expected-runtime complexity,\nwhereas the corresponding worst-case bounds are either inefficient (e.g.,\nQUICK-SORT), or completely ineffective (e.g., COUPON-COLLECTOR). Since the main\nfocus of expected-runtime analysis is to obtain efficient bounds, we consider\nbounds that are either logarithmic, linear, or almost-linear ($\\mathcal{O}(\\log\nn)$, $\\mathcal{O}(n)$, $\\mathcal{O}(n\\cdot\\log n)$, respectively, where n\nrepresents the input size). Our main contribution is an efficient (simple\nlinear-time algorithm) sound approach for deriving such expected-runtime bounds\nfor the analysis of recurrence relations induced by randomized algorithms. Our\napproach can infer the asymptotically optimal expected-runtime bounds for\nrecurrences of classical randomized algorithms, including RANDOMIZED-SEARCH,\nQUICK-SORT, QUICK-SELECT, COUPONCOLLECTOR, where the worst-case bounds are\neither inefficient (such as linear as compared to logarithmic of\nexpected-runtime, or quadratic as compared to linear or almost-linear of\nexpected-runtime), or ineffective. We have implemented our approach, and the\nexperimental results show that we obtain the bounds efficiently for the\nrecurrences of various classical algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 13:30:01 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Murhekar", "Aniket", ""]]}, {"id": "1705.00327", "submitter": "Shang-En Huang", "authors": "Shang-En Huang, Seth Pettie", "title": "Thorup-Zwick Emulators are Universally Optimal Hopsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $(\\beta,\\epsilon)$-$\\textit{hopset}$ is, informally, a weighted edge set\nthat, when added to a graph, allows one to get from point $a$ to point $b$\nusing a path with at most $\\beta$ edges (\"hops\") and length\n$(1+\\epsilon)\\mathrm{dist}(a,b)$. In this paper we observe that Thorup and\nZwick's $\\textit{sublinear additive}$ emulators are also actually\n$(O(k/\\epsilon)^k,\\epsilon)$-hopsets for every $\\epsilon>0$, and that with a\nsmall change to the Thorup-Zwick construction, the size of the hopset can be\nmade $O(n^{1+\\frac{1}{2^{k+1}-1}})$. As corollaries, we also shave \"$k$\"\nfactors off the size of Thorup and Zwick's sublinear additive emulators and the\nsparsest known $(1+\\epsilon,O(k/\\epsilon)^{k-1})$-spanners, due to Abboud,\nBodwin, and Pettie.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:34:02 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Huang", "Shang-En", ""], ["Pettie", "Seth", ""]]}, {"id": "1705.00382", "submitter": "Jonathan Stokes", "authors": "Jonathan Stokes, Steven Weber", "title": "Common greedy wiring and rewiring heuristics do not guarantee maximum\n  assortative graphs of given degree", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine two greedy heuristics - wiring and rewiring - for constructing\nmaximum assortative graphs over all simple connected graphs with a target\ndegree sequence. Counterexamples show that natural greedy rewiring heuristics\ndo not necessarily return a maximum assortative graph, even though it is known\nthat the meta-graph of all simple connected graphs with given degree is\nconnected under rewiring. Counterexamples show an elegant greedy graph wiring\nheuristic from the literature may fail to achieve the target degree sequence or\nmay fail to wire a maximally assortative graph.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 22:20:17 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 13:29:35 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Stokes", "Jonathan", ""], ["Weber", "Steven", ""]]}, {"id": "1705.00415", "submitter": "Travis Gagie", "authors": "Leo Ferres, Jos\\'e Fuentes-Sep\\'ulveda, Travis Gagie, Meng He and\n  Gonzalo Navarro", "title": "Parallel Construction of Compact Planar Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer sizes of modern datasets are forcing data-structure designers to\nconsider seriously both parallel construction and compactness. To achieve those\ngoals we need to design a parallel algorithm with good scalability and with low\nmemory consumption. An algorithm with good scalability improves its performance\nwhen the number of available cores increases, and an algorithm with low memory\nconsumption uses memory proportional to the space used by the dataset in\nuncompact form. In this work, we discuss the engineering of a parallel\nalgorithm with linear work and logarithmic span for the construction of the\ncompact representation of planar embeddings. We also provide an experimental\nstudy of our implementation and prove experimentally that it has good\nscalability and low memory consumption. Additionally, we describe and test\nexperimentally queries supported by the compact representation.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 03:50:09 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ferres", "Leo", ""], ["Fuentes-Sep\u00falveda", "Jos\u00e9", ""], ["Gagie", "Travis", ""], ["He", "Meng", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1705.00677", "submitter": "Nicholas Moehle", "authors": "Nicholas Moehle and Xinyue Shen and Zhi-Quan Luo and Stephen Boyd", "title": "A Distributed Method for Optimal Capacity Reservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reserving link capacity in a network in such a way\nthat any of a given set of flow scenarios can be supported. In the optimal\ncapacity reservation problem, we choose the reserved link capacities to\nminimize the reservation cost. This problem reduces to a large linear program,\nwith the number of variables and constraints on the order of the number of\nlinks times the number of scenarios. Small and medium size problems are within\nthe capabilities of generic linear program solvers. We develop a more scalable,\ndistributed algorithm for the problem that alternates between solving (in\nparallel) one flow problem per scenario, and coordination steps, which connect\nthe individual flows and the reservation capacities.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:30:58 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Moehle", "Nicholas", ""], ["Shen", "Xinyue", ""], ["Luo", "Zhi-Quan", ""], ["Boyd", "Stephen", ""]]}, {"id": "1705.00694", "submitter": "Stepan Kuznetsov", "authors": "Max Kanovich, Stepan Kuznetsov, Glyn Morrill, Andre Scedrov", "title": "A polynomial time algorithm for the Lambek calculus with brackets of\n  bounded order", "comments": null, "journal-ref": "Proc. FSCD 2017, LIPIcs vol. 84, 22:1-22:17", "doi": "10.4230/LIPIcs.FSCD.2017.22", "report-no": null, "categories": "cs.LO cs.CL cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lambek calculus is a logical foundation of categorial grammar, a linguistic\nparadigm of grammar as logic and parsing as deduction. Pentus (2010) gave a\npolynomial-time algorithm for determ- ining provability of bounded depth\nformulas in the Lambek calculus with empty antecedents allowed. Pentus'\nalgorithm is based on tabularisation of proof nets. Lambek calculus with\nbrackets is a conservative extension of Lambek calculus with bracket\nmodalities, suitable for the modeling of syntactical domains. In this paper we\ngive an algorithm for provability the Lambek calculus with brackets allowing\nempty antecedents. Our algorithm runs in polynomial time when both the formula\ndepth and the bracket nesting depth are bounded. It combines a Pentus-style\ntabularisation of proof nets with an automata-theoretic treatment of\nbracketing.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 20:12:11 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 10:39:22 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kanovich", "Max", ""], ["Kuznetsov", "Stepan", ""], ["Morrill", "Glyn", ""], ["Scedrov", "Andre", ""]]}, {"id": "1705.00774", "submitter": "Matthew Wright", "authors": "Abdel-Rahman Madkour, Phillip Nadolny, Matthew Wright", "title": "Finding Minimum Spanning Forests in a Graph", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a graph partitioning problem motivated by computational topology\nand propose two algorithms that produce approximate solutions. Specifically,\ngiven a weighted, undirected graph $G$ and a positive integer $k$, we desire to\nfind $k$ disjoint trees within $G$ such that each vertex of $G$ is contained in\none of the trees and the weight of the largest tree is as small as possible. We\nare unable to find this problem in the graph partitioning literature, but we\nshow that the problem is NP-complete. We then propose two approximation\nalgorithms, one that uses a spectral clustering approach and another that\nemploys a dynamic programming strategy, which produce near-optimal partitions\non a family of test graphs. We describe these algorithms and analyze their\nempirical performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 02:45:42 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 17:39:36 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Madkour", "Abdel-Rahman", ""], ["Nadolny", "Phillip", ""], ["Wright", "Matthew", ""]]}, {"id": "1705.00849", "submitter": "Junichi Teruyama", "authors": "Kazuo Iwama and Junichi Teruyama", "title": "Improved Average Complexity for Comparison-Based Sorting", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the average complexity on the number of comparisons for\nsorting algorithms. Its information-theoretic lower bound is $n \\lg n - 1.4427n\n+ O(\\log n)$. For many efficient algorithms, the first $n\\lg n$ term is easy to\nachieve and our focus is on the (negative) constant factor of the linear term.\nThe current best value is $-1.3999$ for the MergeInsertion sort. Our new value\nis $-1.4106$, narrowing the gap by some $25\\%$. An important building block of\nour algorithm is \"two-element insertion,\" which inserts two numbers $A$ and\n$B$, $A<B$, into a sorted sequence $T$. This insertion algorithm is still\nsufficiently simple for rigorous mathematical analysis and works well for a\ncertain range of the length of $T$ for which the simple binary insertion does\nnot, thus allowing us to take a complementary approach with the binary\ninsertion.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 08:14:42 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Iwama", "Kazuo", ""], ["Teruyama", "Junichi", ""]]}, {"id": "1705.00892", "submitter": "Javier Escudero", "authors": "Loukianos Spyrou, Javier Escudero", "title": "Weighted network estimation by the use of topological graph metrics", "comments": "Accepted for publication in IEEE Transactions on Network Science and\n  Engineering", "journal-ref": null, "doi": "10.1109/TNSE.2018.2849342", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological metrics of graphs provide a natural way to describe the prominent\nfeatures of various types of networks. Graph metrics describe the structure and\ninterplay of graph edges and have found applications in many scientific fields.\nIn this work, graph metrics are used in network estimation by developing\noptimisation methods that incorporate prior knowledge of a network's topology.\nThe derivatives of graph metrics are used in gradient descent schemes for\nweighted undirected network denoising, network completion, and network\ndecomposition. The successful performance of our methodology is shown in a\nnumber of toy examples and real-world datasets. Most notably, our work\nestablishes a new link between graph theory, network science and optimisation.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:31:33 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 20:19:20 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Spyrou", "Loukianos", ""], ["Escudero", "Javier", ""]]}, {"id": "1705.00985", "submitter": "Richard Peng", "authors": "David Durfee, John Peebles, Richard Peng, Anup B. Rao", "title": "Determinant-Preserving Sparsification of SDDM Matrices with Applications\n  to Counting and Sampling Spanning Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show variants of spectral sparsification routines can preserve the total\nspanning tree counts of graphs, which by Kirchhoff's matrix-tree theorem, is\nequivalent to determinant of a graph Laplacian minor, or equivalently, of any\nSDDM matrix. Our analyses utilizes this combinatorial connection to bridge\nbetween statistical leverage scores / effective resistances and the analysis of\nrandom graphs by [Janson, Combinatorics, Probability and Computing `94]. This\nleads to a routine that in quadratic time, sparsifies a graph down to about\n$n^{1.5}$ edges in ways that preserve both the determinant and the distribution\nof spanning trees (provided the sparsified graph is viewed as a random object).\nExtending this algorithm to work with Schur complements and approximate\nCholeksy factorizations leads to algorithms for counting and sampling spanning\ntrees which are nearly optimal for dense graphs.\n  We give an algorithm that computes a $(1 \\pm \\delta)$ approximation to the\ndeterminant of any SDDM matrix with constant probability in about $n^2\n\\delta^{-2}$ time. This is the first routine for graphs that outperforms\ngeneral-purpose routines for computing determinants of arbitrary matrices. We\nalso give an algorithm that generates in about $n^2 \\delta^{-2}$ time a\nspanning tree of a weighted undirected graph from a distribution with total\nvariation distance of $\\delta$ from the $w$-uniform distribution .\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:11:59 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Durfee", "David", ""], ["Peebles", "John", ""], ["Peng", "Richard", ""], ["Rao", "Anup B.", ""]]}, {"id": "1705.00997", "submitter": "Tobias Maier", "authors": "Tobias Maier and Peter Sanders", "title": "Dynamic Space Efficient Hashing", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider space efficient hash tables that can grow and shrink dynamically\nand are always highly space efficient, i.e., their space consumption is always\nclose to the lower bound even while growing and when taking into account\nstorage that is only needed temporarily. None of the traditionally used hash\ntables have this property. We show how known approaches like linear probing and\nbucket cuckoo hashing can be adapted to this scenario by subdividing them into\nmany subtables or using virtual memory overcommitting. However, these rather\nstraightforward solutions suffer from slow amortized insertion times due to\nfrequent reallocation in small increments.\n  Our main result is DySECT ({\\bf Dy}namic {\\bf S}pace {\\bf E}fficient {\\bf\nC}uckoo {\\bf T}able) which avoids these problems. DySECT consists of many\nsubtables which grow by doubling their size. The resulting inhomogeneity in\nsubtable sizes is equalized by the flexibility available in bucket cuckoo\nhashing where each element can go to several buckets each of which containing\nseveral cells. Experiments indicate that DySECT works well with load factors up\nto 98\\%. With up to 2.7 times better performance than the next best solution.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:29:54 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Maier", "Tobias", ""], ["Sanders", "Peter", ""]]}, {"id": "1705.01167", "submitter": "Corey Walsh", "authors": "Corey Walsh, Sertac Karaman", "title": "CDDT: Fast Approximate 2D Ray Casting for Accelerated Localization", "comments": "8 pages, 14 figures, ICRA version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization is an essential component for autonomous robots. A\nwell-established localization approach combines ray casting with a particle\nfilter, leading to a computationally expensive algorithm that is difficult to\nrun on resource-constrained mobile robots. We present a novel data structure\ncalled the Compressed Directional Distance Transform for accelerating ray\ncasting in two dimensional occupancy grid maps. Our approach allows online map\nupdates, and near constant time ray casting performance for a fixed size map,\nin contrast with other methods which exhibit poor worst case performance. Our\nexperimental results show that the proposed algorithm approximates the\nperformance characteristics of reading from a three dimensional lookup table of\nray cast solutions while requiring two orders of magnitude less memory and\nprecomputation. This results in a particle filter algorithm which can maintain\n2500 particles with 61 ray casts per particle at 40Hz, using a single CPU\nthread onboard a mobile robot.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 20:38:42 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 19:00:34 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Walsh", "Corey", ""], ["Karaman", "Sertac", ""]]}, {"id": "1705.01240", "submitter": "Manuel Lafond", "authors": "Mark Jones, Manuel Lafond, Celine Scornavacca", "title": "Consistency of orthology and paralogy constraints in the presence of\n  gene transfers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthology and paralogy relations are often inferred by methods based on gene\nsimilarity, which usually yield a graph depicting the relationships between\ngene pairs. Such relation graphs are known to frequently contain errors, as\nthey cannot be explained via a gene tree that both contains the depicted\northologs/paralogs, and that is consistent with a species tree $S$. This idea\nof detecting errors through inconsistency with a species tree has mostly been\nstudied in the presence of speciation and duplication events only. In this\nwork, we ask: could the given set of relations be consistent if we allow\nlateral gene transfers in the evolutionary model? We formalize this question\nand provide a variety of algorithmic results regarding the underlying problems.\nNamely, we show that deciding if a relation graph $R$ is consistent with a\ngiven species network $N$ is NP-hard, and that it is W[1]-hard under the\nparameter \"minimum number of transfers\". However, we present an FPT algorithm\nbased on the degree of the $DS$-tree associated with $R$. We also study\nanalogous problems in the case that the transfer highways on a species tree are\nunknown.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 03:00:32 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 03:32:55 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 00:50:01 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 17:42:25 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Jones", "Mark", ""], ["Lafond", "Manuel", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1705.01414", "submitter": "Fahad Panolan", "authors": "Daniel Lokshtanov, Fahad Panolan, Saket Saurabh, Roohani Sharma,\n  Meirav Zehavi", "title": "Covering Small Independent Sets and Separators with Applications to\n  Parameterized Algorithms", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new combinatorial tools for the design of parameterized\nalgorithms. The first is a simple linear time randomized algorithm that given\nas input a $d$-degenerate graph $G$ and an integer $k$, outputs an independent\nset $Y$, such that for every independent set $X$ in $G$ of size at most $k$,\nthe probability that $X$ is a subset of $Y$ is at least $\\left({(d+1)k \\choose\nk} \\cdot k(d+1)\\right)^{-1}$.The second is a new (deterministic) polynomial\ntime graph sparsification procedure that given a graph $G$, a set $T = \\{\\{s_1,\nt_1\\}, \\{s_2, t_2\\}, \\ldots, \\{s_\\ell, t_\\ell\\}\\}$ of terminal pairs and an\ninteger $k$, returns an induced subgraph $G^\\star$ of $G$ that maintains all\nthe inclusion minimal multicuts of $G$ of size at most $k$, and does not\ncontain any $(k+2)$-vertex connected set of size $2^{{\\cal O}(k)}$. In\nparticular, $G^\\star$ excludes a clique of size $2^{{\\cal O}(k)}$ as a\ntopological minor. Put together, our new tools yield new randomized fixed\nparameter tractable (FPT) algorithms for Stable $s$-$t$ Separator, Stable Odd\nCycle Transversal and Stable Multicut on general graphs, and for Stable\nDirected Feedback Vertex Set on $d$-degenerate graphs, resolving two problems\nleft open by Marx et al. [ACM Transactions on Algorithms, 2013]. All of our\nalgorithms can be derandomized at the cost of a small overhead in the running\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:28:51 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Sharma", "Roohani", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1705.01465", "submitter": "S\\'andor Kisfaludi-Bak", "authors": "Mark de Berg, Hans L. Bodlaender, S\\'andor Kisfaludi-Bak", "title": "The Homogeneous Broadcast Problem in Narrow and Wide Strips", "comments": "50 pages, WADS 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a set of nodes in a wireless network, where each node is modeled\nas a point in the plane, and let $s\\in P$ be a given source node. Each node $p$\ncan transmit information to all other nodes within unit distance, provided $p$\nis activated. The (homogeneous) broadcast problem is to activate a minimum\nnumber of nodes such that in the resulting directed communication graph, the\nsource $s$ can reach any other node. We study the complexity of the regular and\nthe hop-bounded version of the problem (in the latter, $s$ must be able to\nreach every node within a specified number of hops), with the restriction that\nall points lie inside a strip of width $w$. We almost completely characterize\nthe complexity of both the regular and the hop-bounded versions as a function\nof the strip width $w$.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 14:48:56 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["de Berg", "Mark", ""], ["Bodlaender", "Hans L.", ""], ["Kisfaludi-Bak", "S\u00e1ndor", ""]]}, {"id": "1705.01497", "submitter": "John Augustine", "authors": "John Augustine, Krishna Palem, and Parishkrati", "title": "Sustaining Moore's Law Through Inexactness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inexact computing aims to compute good solutions that require considerably\nless resource -- typically energy -- compared to computing exact solutions.\nWhile inexactness is motivated by concerns derived from technology scaling and\nMoore's law, there is no formal or foundational framework for reasoning about\nthis novel approach to designing algorithms. In this work, we present a\nfundamental relationship between the quality of computing the value of a\nboolean function and the energy needed to compute it in a mathematically\nrigorous and general setting. On this basis, one can study the tradeoff between\nthe quality of the solution to a problem and the amount of energy that is\nconsumed. We accomplish this by introducing a computational model to classify\nproblems based on notions of symmetry inspired by physics. We show that some\nproblems are symmetric in that every input bit is, in a sense, equally\nimportant, while other problems display a great deal of asymmetry in the\nimportance of input bits. We believe that our model is novel and provides a\nfoundation for inexact Computing. Building on this, we show that asymmetric\nproblems allow us to invest resources favoring the important bits -- a feature\nthat can be leveraged to design efficient inexact algorithms. On the negative\nside and in contrast, we can prove that the best inexact algorithms for\nsymmetric problems are no better than simply reducing the resource investment\nuniformly across all bits. Akin to classical theories concerned with space and\ntime complexity, we believe the ability to classify problems as shown in our\npaper will serve as a basis for formally reasoning about the effectiveness of\ninexactness in the context of a range of computational problems with energy\nbeing the primary resource.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:24:13 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 13:26:34 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Augustine", "John", ""], ["Palem", "Krishna", ""], ["Parishkrati", "", ""]]}, {"id": "1705.01570", "submitter": "Greg Bodwin", "authors": "Greg Bodwin", "title": "Testing Core Membership in Public Goods Economies", "comments": "To appear in ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a recent line of economic theory seeking to understand\npublic goods economies using methods of topological analysis. Our first main\nresult is a very clean characterization of the economy's core (the standard\nsolution concept in public goods). Specifically, we prove that a point is in\nthe core iff it is Pareto efficient, individually rational, and the set of\npoints it dominates is path connected.\n  While this structural theorem has a few interesting implications in economic\ntheory, the main focus of the second part of this paper is on a particular\nalgorithmic application that demonstrates its utility. Since the 1960s,\neconomists have looked for an efficient computational process that decides\nwhether or not a given point is in the core. All known algorithms so far run in\nexponential time (except in some artificially restricted settings). By heavily\nexploiting our new structure, we propose a new algorithm for testing core\nmembership whose computational bottleneck is the solution of $O(n)$ convex\noptimization problems on the utility function governing the economy. It is\nfairly natural to assume that convex optimization should be feasible, as it is\nneeded even for very basic economic computational tasks such as testing Pareto\nefficiency. Nevertheless, even without this assumption, our work implies for\nthe first time that core membership can be efficiently tested on (e.g.) utility\nfunctions that admit \"nice\" analytic expressions, or that appropriately defined\n$\\varepsilon$-approximate versions of the problem are tractable (by using\nmodern black-box $\\varepsilon$-approximate convex optimization algorithms).\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 18:15:48 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Bodwin", "Greg", ""]]}, {"id": "1705.01591", "submitter": "Georgios Kydonakis", "authors": "Steven B. Bradlow, Konstantinos Kapenekakis, Georgios Kydonakis,\n  Xinwei Li and Jiarui Xu", "title": "Demonstrating research subcommunities in mathematical networks", "comments": "4 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for demonstrating sub community structure in scientific\nnetworks of relatively small size from analyzing databases of publications.\nResearch relationships between the network members can be visualized as a graph\nwith vertices corresponding to authors and with edges indicating joint\nauthorship. Using a fast clustering algorithm combined with a graph layout\nalgorithm, we demonstrate how to display these clustering results in an\nattractive and informative way. The small size of the graph allows us to\ndevelop tools that keep track of how these research sub communities evolve in\ntime, as well as to present the research articles that create the links between\nthe network members. These tools are included in a web app, where the visitor\ncan easily identify the various sub communities, providing also valuable\ninformation for administrational purposes. Our method was developed for the\nGEAR mathematical network and it can be applied to other networks.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 19:35:16 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Bradlow", "Steven B.", ""], ["Kapenekakis", "Konstantinos", ""], ["Kydonakis", "Georgios", ""], ["Li", "Xinwei", ""], ["Xu", "Jiarui", ""]]}, {"id": "1705.01595", "submitter": "Holger Dell", "authors": "Radu Curticapean and Holger Dell and D\\'aniel Marx", "title": "Homomorphisms Are a Good Basis for Counting Small Subgraphs", "comments": "An extended abstract of this paper appears at STOC 2017", "journal-ref": null, "doi": "10.1145/3055399.3055502", "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce graph motif parameters, a class of graph parameters that depend\nonly on the frequencies of constant-size induced subgraphs. Classical works by\nLov\\'asz show that many interesting quantities have this form, including, for\nfixed graphs $H$, the number of $H$-copies (induced or not) in an input graph\n$G$, and the number of homomorphisms from $H$ to $G$.\n  Using the framework of graph motif parameters, we obtain faster algorithms\nfor counting subgraph copies of fixed graphs $H$ in host graphs $G$: For graphs\n$H$ on $k$ edges, we show how to count subgraph copies of $H$ in time\n$k^{O(k)}\\cdot n^{0.174k + o(k)}$ by a surprisingly simple algorithm. This\nimproves upon previously known running times, such as $O(n^{0.91k + c})$ time\nfor $k$-edge matchings or $O(n^{0.46k + c})$ time for $k$-cycles.\n  Furthermore, we prove a general complexity dichotomy for evaluating graph\nmotif parameters: Given a class $\\mathcal C$ of such parameters, we consider\nthe problem of evaluating $f\\in \\mathcal C$ on input graphs $G$, parameterized\nby the number of induced subgraphs that $f$ depends upon. For every recursively\nenumerable class $\\mathcal C$, we prove the above problem to be either FPT or\n#W[1]-hard, with an explicit dichotomy criterion. This allows us to recover\nknown dichotomies for counting subgraphs, induced subgraphs, and homomorphisms\nin a uniform and simplified way, together with improved lower bounds.\n  Finally, we extend graph motif parameters to colored subgraphs and prove a\ncomplexity trichotomy: For vertex-colored graphs $H$ and $G$, where $H$ is from\na fixed class $\\mathcal H$, we want to count color-preserving $H$-copies in\n$G$. We show that this problem is either polynomial-time solvable or FPT or\n#W[1]-hard, and that the FPT cases indeed need FPT time under reasonable\nassumptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 19:47:33 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Curticapean", "Radu", ""], ["Dell", "Holger", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1705.01843", "submitter": "Ronald de Wolf", "authors": "Joran van Apeldoorn, Andr\\'as Gily\\'en, Sander Gribling, Ronald de\n  Wolf", "title": "Quantum SDP-Solvers: Better upper and lower bounds", "comments": "v4: 69 pages, small corrections and clarifications. This version will\n  appear in Quantum", "journal-ref": "Quantum 4, 230 (2020)", "doi": "10.22331/q-2020-02-14-230", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brand\\~ao and Svore very recently gave quantum algorithms for approximately\nsolving semidefinite programs, which in some regimes are faster than the\nbest-possible classical algorithms in terms of the dimension $n$ of the problem\nand the number $m$ of constraints, but worse in terms of various other\nparameters. In this paper we improve their algorithms in several ways, getting\nbetter dependence on those other parameters. To this end we develop new\ntechniques for quantum algorithms, for instance a general way to efficiently\nimplement smooth functions of sparse Hamiltonians, and a generalized\nminimum-finding procedure.\n  We also show limits on this approach to quantum SDP-solvers, for instance for\ncombinatorial optimizations problems that have a lot of symmetry. Finally, we\nprove some general lower bounds showing that in the worst case, the complexity\nof every quantum LP-solver (and hence also SDP-solver) has to scale linearly\nwith $mn$ when $m\\approx n$, which is the same as classical.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 13:59:43 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 10:19:03 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 10:57:55 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2020 19:18:46 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["van Apeldoorn", "Joran", ""], ["Gily\u00e9n", "Andr\u00e1s", ""], ["Gribling", "Sander", ""], ["de Wolf", "Ronald", ""]]}, {"id": "1705.01887", "submitter": "Samson Zhou", "authors": "Elena Grigorescu, Erfan Sadeqi Azer, Samson Zhou", "title": "Streaming for Aibohphobes: Longest Palindrome with Mismatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A palindrome is a string that reads the same as its reverse, such as\n\"aibohphobia\" (fear of palindromes). Given an integer $d>0$, a\n$d$-near-palindrome is a string of Hamming distance at most $d$ from its\nreverse. We study the natural problem of identifying a longest\n$d$-near-palindrome in data streams. The problem is relevant to the analysis of\nDNA databases, and to the task of repairing recursive structures in documents\nsuch as XML and JSON. We present an algorithm that returns a\n$d$-near-palindrome whose length is within a multiplicative\n$(1+\\epsilon)$-factor of the longest $d$-near-palindrome. Our algorithm also\nreturns the set of mismatched indices of the $d$-near-palindrome, using\n$\\mathcal{O}\\left(\\frac{d\\log^7 n}{\\epsilon\\log(1+\\epsilon)}\\right)$ bits of\nspace, and $\\mathcal{O}\\left(\\frac{d\\log^6 n}{\\epsilon\\log(1+\\epsilon)}\\right)$\nupdate time per arriving symbol. We show that $\\Omega(d\\log n)$ space is\nnecessary for estimating the length of longest $d$-near-palindromes with high\nprobability. We further obtain an additive-error approximation algorithm and a\ncomparable lower bound, as well as an exact two-pass algorithm that solves the\nlongest $d$-near-palindrome problem using $\\mathcal{O}\\left(d^2\\sqrt{n}\\log^6\nn\\right)$ bits of space.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 15:59:56 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Grigorescu", "Elena", ""], ["Azer", "Erfan Sadeqi", ""], ["Zhou", "Samson", ""]]}, {"id": "1705.01963", "submitter": "Ray Li", "authors": "Venkatesan Guruswami and Ray Li", "title": "Polynomial time decodable codes for the binary deletion channel", "comments": "arXiv admin note: substantial text overlap with arXiv:1612.06335. The\n  published version of this paper incorrectly states the alphabet size in\n  Theorem 3.4. This version states the result correctly", "journal-ref": "IEEE Trans. Information Theory 65(4): 2171 - 2178 (2019)", "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the random deletion channel, each bit is deleted independently with\nprobability $p$. For the random deletion channel, the existence of codes of\nrate $(1-p)/9$, and thus bounded away from $0$ for any $p < 1$, has been known.\nWe give an explicit construction with polynomial time encoding and deletion\ncorrection algorithms with rate $c_0 (1-p)$ for an absolute constant $c_0 > 0$.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 18:18:07 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 17:10:26 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 23:12:58 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Li", "Ray", ""]]}, {"id": "1705.02044", "submitter": "Amgad Madkour", "authors": "Amgad Madkour, Walid G. Aref, Faizan Ur Rehman, Mohamed Abdur Rahman,\n  Saleh Basalamah", "title": "A Survey of Shortest-Path Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A shortest-path algorithm finds a path containing the minimal cost between\ntwo vertices in a graph. A plethora of shortest-path algorithms is studied in\nthe literature that span across multiple disciplines. This paper presents a\nsurvey of shortest-path algorithms based on a taxonomy that is introduced in\nthe paper. One dimension of this taxonomy is the various flavors of the\nshortest-path problem. There is no one general algorithm that is capable of\nsolving all variants of the shortest-path problem due to the space and time\ncomplexities associated with each algorithm. Other important dimensions of the\ntaxonomy include whether the shortest-path algorithm operates over a static or\na dynamic graph, whether the shortest-path algorithm produces exact or\napproximate answers, and whether the objective of the shortest-path algorithm\nis to achieve time-dependence or is to only be goal directed. This survey\nstudies and classifies shortest-path algorithms according to the proposed\ntaxonomy. The survey also presents the challenges and proposed solutions\nassociated with each category in the taxonomy.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 23:11:11 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Madkour", "Amgad", ""], ["Aref", "Walid G.", ""], ["Rehman", "Faizan Ur", ""], ["Rahman", "Mohamed Abdur", ""], ["Basalamah", "Saleh", ""]]}, {"id": "1705.02127", "submitter": "Sebastian Krinninger", "authors": "Karl Bringmann, Sebastian Krinninger", "title": "A Note on Hardness of Diameter Approximation", "comments": "Accepted to Information Processing Letters", "journal-ref": "Information Processing Letters 133: 10-15 (2018)", "doi": "10.1016/j.ipl.2017.12.010", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the hardness of approximating the diameter of a network. In the\nCONGEST model of distributed computing, $ \\tilde \\Omega (n) $ rounds are\nnecessary to compute the diameter [Frischknecht et al. SODA'12], where $ \\tilde\n\\Omega (\\cdot) $ hides polylogarithmic factors. Abboud et al. [DISC 2016]\nextended this result to sparse graphs and, at a more fine-grained level, showed\nthat, for any integer $ 1 \\leq \\ell \\leq \\operatorname{polylog} (n) $,\ndistinguishing between networks of diameter $ 4 \\ell + 2 $ and $ 6 \\ell + 1 $\nrequires $ \\tilde \\Omega (n) $ rounds. We slightly tighten this result by\nshowing that even distinguishing between diameter $ 2 \\ell + 1 $ and $ 3 \\ell +\n1 $ requires $ \\tilde \\Omega (n) $ rounds. The reduction of Abboud et al. is\ninspired by recent conditional lower bounds in the RAM model, where the\northogonal vectors problem plays a pivotal role. In our new lower bound, we\nmake the connection to orthogonal vectors explicit, leading to a conceptually\nmore streamlined exposition.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 08:43:21 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 09:24:01 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bringmann", "Karl", ""], ["Krinninger", "Sebastian", ""]]}, {"id": "1705.02194", "submitter": "Xiangkun Shen", "authors": "Viswanath Nagarajan and Xiangkun Shen", "title": "Online Covering with Sum of $\\ell_q$-Norm Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fractional online covering problems with $\\ell_q$-norm\nobjectives. The problem of interest is of the form $\\min\\{ f(x) \\,:\\, Ax\\ge 1,\nx\\ge 0\\}$ where $f(x)=\\sum_{e} c_e \\|x(S_e)\\|_{q_e} $ is the weighted sum of\n$\\ell_q$-norms and $A$ is a non-negative matrix. The rows of $A$ (i.e. covering\nconstraints) arrive online over time. We provide an online $O(\\log d+\\log\n\\rho)$-competitive algorithm where $\\rho = \\frac{\\max a_{ij}}{\\min a_{ij}}$ and\n$d$ is the maximum of the row sparsity of $A$ and $\\max |S_e|$. This is based\non the online primal-dual framework where we use the dual of the above convex\nprogram. Our result expands the class of convex objectives that admit good\nonline algorithms: prior results required a monotonicity condition on the\nobjective $f$ which is not satisfied here. This result is nearly tight even for\nthe linear special case. As direct applications we obtain (i) improved online\nalgorithms for non-uniform buy-at-bulk network design and (ii) the first online\nalgorithm for throughput maximization under $\\ell_p$-norm edge capacities.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 12:45:46 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 15:31:56 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Nagarajan", "Viswanath", ""], ["Shen", "Xiangkun", ""]]}, {"id": "1705.02280", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna, and Yang Li", "title": "The Stochastic Matching Problem: Beating Half with a Non-Adaptive\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stochastic matching problem, we are given a general (not necessarily\nbipartite) graph $G(V,E)$, where each edge in $E$ is realized with some\nconstant probability $p > 0$ and the goal is to compute a bounded-degree\n(bounded by a function depending only on $p$) subgraph $H$ of $G$ such that the\nexpected maximum matching size in $H$ is close to the expected maximum matching\nsize in $G$. The algorithms in this setting are considered non-adaptive as they\nhave to choose the subgraph $H$ without knowing any information about the set\nof realized edges in $G$. Originally motivated by an application to kidney\nexchange, the stochastic matching problem and its variants have received\nsignificant attention in recent years.\n  The state-of-the-art non-adaptive algorithms for stochastic matching achieve\nan approximation ratio of $\\frac{1}{2}-\\epsilon$ for any $\\epsilon > 0$,\nnaturally raising the question that if $1/2$ is the limit of what can be\nachieved with a non-adaptive algorithm. In this work, we resolve this question\nby presenting the first algorithm for stochastic matching with an approximation\nguarantee that is strictly better than $1/2$: the algorithm computes a subgraph\n$H$ of $G$ with the maximum degree $O(\\frac{\\log{(1/ p)}}{p})$ such that the\nratio of expected size of a maximum matching in realizations of $H$ and $G$ is\nat least $1/2+\\delta_0$ for some absolute constant $\\delta_0 > 0$. The degree\nbound on $H$ achieved by our algorithm is essentially the best possible (up to\nan $O(\\log{(1/p)})$ factor) for any constant factor approximation algorithm,\nsince an $\\Omega(\\frac{1}{p})$ degree in $H$ is necessary for a vertex to\nacquire at least one incident edge in a realization.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 16:00:25 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""]]}, {"id": "1705.02313", "submitter": "John Fearnley", "authors": "John Fearnley", "title": "Efficient Parallel Strategy Improvement for Parity Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study strategy improvement algorithms for solving parity games. While\nthese algorithms are known to solve parity games using a very small number of\niterations, experimental studies have found that a high step complexity causes\nthem to perform poorly in practice. In this paper we seek to address this\nsituation. Every iteration of the algorithm must compute a best response, and\nwhile the standard way of doing this uses the Bellman-Ford algorithm, we give\nexperimental results that show that one-player strategy improvement\nsignificantly outperforms this technique in practice. We then study the best\nway to implement one-player strategy improvement, and we develop an efficient\nparallel algorithm for carrying out this task, by reducing the problem to\ncomputing prefix sums on a linked list. We report experimental results for\nthese algorithms, and we find that a GPU implementation of this algorithm shows\na significant speedup over single-core and multi-core CPU implementations.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:29:25 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Fearnley", "John", ""]]}, {"id": "1705.02390", "submitter": "Petr Kolman", "authors": "Petr Kolman", "title": "On Algorithms for $L$-bounded Cut Problem", "comments": "11 pages + 2 pages of Appendix. The new version (Sep 8, 2017)\n  improves by a logarithmic factor the approximation ratio in Section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ with two distinguished vertices $s,t\\in V$ and an\ninteger parameter $L>0$, an {\\em $L$-bounded cut} is a subset $F$ of edges\n(vertices) such that the every path between $s$ and $t$ in $G\\setminus F$ has\nlength more than $L$. The task is to find an $L$-bounded cut of minimum\ncardinality.\n  Though the problem is very simple to state and has been studied since the\nbeginning of the 70's, it is not much understood yet. The problem is known to\nbe $\\cal{NP}$-hard to approximate within a small constant factor even for\n$L\\geq 4$ (for $L\\geq 5$ for the vertex cuts). On the other hand, the best\nknown approximation algorithm for general graphs has approximation ratio only\n$\\mathcal{O}({n^{2/3}})$ in the edge case, and $\\mathcal{O}({\\sqrt{n}})$ in the\nvertex case, where $n$ denotes the number of vertices.\n  We show that for planar graphs, it is possible to solve both the edge- and\nthe vertex-version of the problem optimally in time $\\mathcal{O}(L^{3L}n)$.\nThat is, the problem is fixed parameter tractable (FPT) with respect to $L$ on\nplanar graphs. Furthermore, we show that the problem remains FPT even for\nbounded genus graphs, a super class of planar graphs.\n  Our second contribution deals with approximations of the vertex version of\nthe problem. We describe an algorithm that for a given a graph $G$, its tree\ndecomposition of treewidth $\\tau$ and vertices $s$ and $t$ computes a\n$\\tau$-approximation of the minimum $L$-bounded $s-t$ vertex cut; if the\ndecomposition is not given, then the approximation ratio is $\\mathcal{O}(\\tau\n\\sqrt{\\log \\tau})$. For graphs with treewidth bounded by\n$\\mathcal{O}(n^{1/2-\\epsilon})$ for any $\\epsilon>0$, but not by a constant,\nthis is the best approximation in terms of~$n$ that we are aware of.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 20:36:51 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 12:32:25 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Kolman", "Petr", ""]]}, {"id": "1705.02613", "submitter": "Shahbaz Khan", "authors": "Surender Baswana, Ayush Goel, Shahbaz Khan", "title": "Incremental DFS algorithms: a theoretical and experimental study", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth First Search (DFS) tree is a fundamental data structure for solving\ngraph problems. The DFS tree of a graph $G$ with $n$ vertices and $m$ edges can\nbe built in $O(m+n)$ time. Till date, only a few algorithms have been designed\nfor maintaining incremental DFS. For undirected graphs, the two algorithms,\nnamely, ADFS1 and ADFS2 [ICALP14] achieve total $O(n^{3/2}\\sqrt{m})$ and\n$O(n^2)$ time respectively. For DAGs, the only non-trivial algorithm, namely,\nFDFS [IPL97] requires total $O(mn)$ time.\n  In this paper, we carry out extensive experimental and theoretical evaluation\nof existing incremental DFS algorithms in random and real graphs, and derive\nthe following results.\n  1- For insertion of uniformly random sequence of $n \\choose 2$ edges, ADFS1,\nADFS2 and FDFS perform equally well and are found to take $\\Theta(n^2)$ time\nexperimentally. This is quite surprising because the worst case bounds of ADFS1\nand FDFS are greater than $\\Theta(n^2)$ by a factor of $\\sqrt{m/n}$ and $m/n$\nrespectively. We complement this result by probabilistic analysis of these\nalgorithms proving $\\tilde{O}(n^2)$ bound on the update time. Here, we derive\nresults about the structure of a DFS tree in random graphs, which are of\nindependent interest.\n  2- These insights led us to design an extremely simple incremental DFS\nalgorithm for both undirected and directed graphs. This algorithm theoretically\nmatches and experimentally outperforms the state-of-the-art in dense random\ngraphs. It can also be used as a single-pass semi-streaming algorithm for\nincremental DFS and strong connectivity in random graphs.\n  3- Even for real graphs, both ADFS1 and FDFS perform much better than their\ntheoretical bounds. Here again, we present two simple algorithms for\nincremental DFS for directed and undirected real graphs. In fact, our algorithm\nfor directed graphs almost always matches the performance of FDFS.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 12:57:40 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Baswana", "Surender", ""], ["Goel", "Ayush", ""], ["Khan", "Shahbaz", ""]]}, {"id": "1705.02703", "submitter": "Heman Shakeri", "authors": "Heman Shakeri, Behnaz Moradi-Jamei, Pietro Poggi-Corradini, Nathan\n  Albin and Caterina Scoglio", "title": "Generalization of Effective Conductance Centrality for Egonetworks", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2018.07.039", "report-no": null, "categories": "physics.data-an cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the popular centrality measure known as effective conductance or in\nsome circles as information centrality. This is an important notion of\ncentrality for undirected networks, with many applications, e.g., for random\nwalks, electrical resistor networks, epidemic spreading, etc. In this paper, we\nfirst reinterpret this measure in terms of modulus (energy) of families of\nwalks on the network. This modulus centrality measure coincides with the\neffective conductance measure on simple undirected networks, and extends it to\nmuch more general situations, e.g., directed networks as well. Secondly, we\nstudy a variation of this modulus approach in the egocentric network paradigm.\nEgonetworks are networks formed around a focal node (ego) with a specific order\nof neighborhoods. We propose efficient analytical and approximate methods for\ncomputing these measures on both undirected and directed networks. Finally, we\ndescribe a simple method inspired by the modulus point-of-view, called shell\ndegree, which proved to be a useful tool for network science.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 21:40:42 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 21:28:29 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Shakeri", "Heman", ""], ["Moradi-Jamei", "Behnaz", ""], ["Poggi-Corradini", "Pietro", ""], ["Albin", "Nathan", ""], ["Scoglio", "Caterina", ""]]}, {"id": "1705.02752", "submitter": "Haitao Wang", "authors": "Haitao Wang and Jingru Zhang", "title": "An $O(n\\log n)$-Time Algorithm for the k-Center Problem in Trees", "comments": "9 figures; 32 pages; a preliminary version to appear in SoCG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a classical k-center problem in trees. Let T be a tree of n\nvertices and every vertex has a nonnegative weight. The problem is to find k\ncenters on the edges of T such that the maximum weighted distance from all\nvertices to their closest centers is minimized. Megiddo and Tamir (SIAM J.\nComput., 1983) gave an algorithm that can solve the problem in $O(n\\log^2 n)$\ntime by using Cole's parametric search. Since then it has been open for over\nthree decades whether the problem can be solved in $O(n\\log n)$ time. In this\npaper, we present an $O(n\\log n)$ time algorithm for the problem and thus\nsettle the open problem affirmatively.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 05:58:07 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 16:47:26 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Wang", "Haitao", ""], ["Zhang", "Jingru", ""]]}, {"id": "1705.02822", "submitter": "Syed Mohammad Meesum", "authors": "Syed Mohammad Meesum, Fahad Panolan, Saket Saurabh, and Meirav Zehavi", "title": "Rank Vertex Cover as a Natural Problem for Algebraic Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The question of the existence of a polynomial kernelization of the Vertex\nCover Above LP problem has been a longstanding, notorious open problem in\nParameterized Complexity. Five years ago, the breakthrough work by Kratsch and\nWahlstrom on representative sets has finally answered this question in the\naffirmative [FOCS 2012]. In this paper, we present an alternative, algebraic\ncompression of the Vertex Cover Above LP problem into the Rank Vertex Cover\nproblem. Here, the input consists of a graph G, a parameter k, and a bijection\nbetween V (G) and the set of columns of a representation of a matriod M, and\nthe objective is to find a vertex cover whose rank is upper bounded by k.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 10:56:28 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 11:32:23 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Meesum", "Syed Mohammad", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1705.02828", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Faster tuple lattice sieving using spherical locality-sensitive filters", "comments": "12 pages + references, 2 figures. Subsumed/merged into Cryptology\n  ePrint Archive 2017/228, available at https://ia.cr/2017/1228", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the large memory requirement of classical lattice sieving\nalgorithms for solving hard lattice problems, Bai-Laarhoven-Stehl\\'{e} [ANTS\n2016] studied tuple lattice sieving, where tuples instead of pairs of lattice\nvectors are combined to form shorter vectors. Herold-Kirshanova [PKC 2017]\nrecently improved upon their results for arbitrary tuple sizes, for example\nshowing that a triple sieve can solve the shortest vector problem (SVP) in\ndimension $d$ in time $2^{0.3717d + o(d)}$, using a technique similar to\nlocality-sensitive hashing for finding nearest neighbors.\n  In this work, we generalize the spherical locality-sensitive filters of\nBecker-Ducas-Gama-Laarhoven [SODA 2016] to obtain space-time tradeoffs for near\nneighbor searching on dense data sets, and we apply these techniques to tuple\nlattice sieving to obtain even better time complexities. For instance, our\ntriple sieve heuristically solves SVP in time $2^{0.3588d + o(d)}$. For\npractical sieves based on Micciancio-Voulgaris' GaussSieve [SODA 2010], this\nshows that a triple sieve uses less space and less time than the current best\nnear-linear space double sieve.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 11:24:10 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 14:13:35 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1705.02936", "submitter": "Manuel Mazzara", "authors": "Andrei Lebedev, JooYoung Lee, Victor Rivera, Manuel Mazzara", "title": "Link Prediction using Top-$k$ Shortest Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply an efficient top-$k$ shortest distance routing\nalgorithm to the link prediction problem and test its efficacy. We compare the\nresults with other base line and state-of-the-art methods as well as with the\nshortest path. Our results show that using top-$k$ distances as a similarity\nmeasure outperforms classical similarity measures such as Jaccard and\nAdamic/Adar.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:24:56 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Lebedev", "Andrei", ""], ["Lee", "JooYoung", ""], ["Rivera", "Victor", ""], ["Mazzara", "Manuel", ""]]}, {"id": "1705.02944", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng, Peng Zhang", "title": "Hardness Results for Structured Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that if the nearly-linear time solvers for Laplacian matrices and\ntheir generalizations can be extended to solve just slightly larger families of\nlinear systems, then they can be used to quickly solve all systems of linear\nequations over the reals. This result can be viewed either positively or\nnegatively: either we will develop nearly-linear time algorithms for solving\nall systems of linear equations over the reals, or progress on the families we\ncan solve in nearly-linear time will soon halt.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 16:11:26 GMT"}, {"version": "v2", "created": "Sun, 24 Sep 2017 18:28:43 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Kyng", "Rasmus", ""], ["Zhang", "Peng", ""]]}, {"id": "1705.02973", "submitter": "Chiheon Kim", "authors": "Chiheon Kim, Afonso S. Bandeira and Michel X. Goemans", "title": "Community Detection in Hypergraphs, Spiked Tensor Models, and\n  Sum-of-Squares", "comments": "In proceedings of 2017 International Conference on Sampling Theory\n  and Applications (SampTA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of community detection in hypergraphs under a stochastic\nblock model. Similarly to how the stochastic block model in graphs suggests\nstudying spiked random matrices, our model motivates investigating statistical\nand computational limits of exact recovery in a certain spiked tensor model. In\ncontrast with the matrix case, the spiked model naturally arising from\ncommunity detection in hypergraphs is different from the one arising in the\nso-called tensor Principal Component Analysis model. We investigate the\neffectiveness of algorithms in the Sum-of-Squares hierarchy on these models.\nInterestingly, our results suggest that these two apparently similar models\nexhibit significantly different computational to statistical gaps.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:07:50 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 22:04:40 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Kim", "Chiheon", ""], ["Bandeira", "Afonso S.", ""], ["Goemans", "Michel X.", ""]]}, {"id": "1705.03283", "submitter": "Daniel Neuen", "authors": "Daniel Neuen and Pascal Schweitzer", "title": "An exponential lower bound for Individualization-Refinement algorithms\n  for Graph Isomorphism", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The individualization-refinement paradigm provides a strong toolbox for\ntesting isomorphism of two graphs and indeed, the currently fastest\nimplementations of isomorphism solvers all follow this approach. While these\nsolvers are fast in practice, from a theoretical point of view, no general\nlower bounds concerning the worst case complexity of these tools are known. In\nfact, it is an open question whether individualization-refinement algorithms\ncan achieve upper bounds on the running time similar to the more theoretical\ntechniques based on a group theoretic approach.\n  In this work we give a negative answer to this question and construct a\nfamily of graphs on which algorithms based on the individualization-refinement\nparadigm require exponential time. Contrary to a previous construction of\nMiyazaki, that only applies to a specific implementation within the\nindividualization-refinement framework, our construction is immune to changing\nthe cell selector, or adding various heuristic invariants to the algorithm.\nFurthermore, our graphs also provide exponential lower bounds in the case when\nthe $k$-dimensional Weisfeiler-Leman algorithm is used to replace the standard\ncolor refinement operator and the arguments even work when the entire\nautomorphism group of the inputs is initially provided to the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 11:41:16 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Neuen", "Daniel", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1705.03385", "submitter": "Panagiotis Charalampopoulos", "authors": "Yannis Almirantis, Panagiotis Charalampopoulos, Jia Gao, Costas S.\n  Iliopoulos, Manal Mohamed, Solon P. Pissis, Dimitris Polychronopoulos", "title": "Optimal Computation of Overabundant Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The observed frequency of the longest proper prefix, the longest proper\nsuffix, and the longest infix of a word $w$ in a given sequence $x$ can be used\nfor classifying $w$ as avoided or overabundant. The definitions used for the\nexpectation and deviation of $w$ in this statistical model were described and\nbiologically justified by Brendel et al. (J Biomol Struct Dyn 1986). We have\nvery recently introduced a time-optimal algorithm for computing all avoided\nwords of a given sequence over an integer alphabet (Algorithms Mol Biol 2017).\nIn this article, we extend this study by presenting an $\\mathcal{O}(n)$-time\nand $\\mathcal{O}(n)$-space algorithm for computing all overabundant words in a\nsequence $x$ of length $n$ over an integer alphabet. Our main result is based\non a new non-trivial combinatorial property of the suffix tree $\\mathcal{T}$ of\n$x$: the number of distinct factors of $x$ whose longest infix is the label of\nan explicit node of $\\mathcal{T}$ is no more than $3n-4$. We further show that\nthe presented algorithm is time-optimal by proving that $\\mathcal{O}(n)$ is a\ntight upper bound for the number of overabundant words. Finally, we present\nexperimental results, using both synthetic and real data, which justify the\neffectiveness and efficiency of our approach in practical terms.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:26:46 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Almirantis", "Yannis", ""], ["Charalampopoulos", "Panagiotis", ""], ["Gao", "Jia", ""], ["Iliopoulos", "Costas S.", ""], ["Mohamed", "Manal", ""], ["Pissis", "Solon P.", ""], ["Polychronopoulos", "Dimitris", ""]]}, {"id": "1705.03414", "submitter": "Nisheeth Vishnoi", "authors": "L. Elisa Celis, Peter M. Krafft, Nisheeth K. Vishnoi", "title": "A Distributed Learning Dynamics in Social Groups", "comments": "To appear in PODC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributed learning process observed in human groups and other\nsocial animals. This learning process appears in settings in which each\nindividual in a group is trying to decide over time, in a distributed manner,\nwhich option to select among a shared set of options. Specifically, we consider\na stochastic dynamics in a group in which every individual selects an option in\nthe following two-step process: (1) select a random individual and observe the\noption that individual chose in the previous time step, and (2) adopt that\noption if its stochastic quality was good at that time step. Various\ninstantiations of such distributed learning appear in nature, and have also\nbeen studied in the social science literature. From the perspective of an\nindividual, an attractive feature of this learning process is that it is a\nsimple heuristic that requires extremely limited computational capacities. But\nwhat does it mean for the group -- could such a simple, distributed and\nessentially memoryless process lead the group as a whole to perform optimally?\nWe show that the answer to this question is yes -- this distributed learning is\nhighly effective at identifying the best option and is close to optimal for the\ngroup overall. Our analysis also gives quantitative bounds that show fast\nconvergence of these stochastic dynamics. Prior to our work the only\ntheoretical work related to such learning dynamics has been either in\ndeterministic special cases or in the asymptotic setting. Finally, we observe\nthat our infinite population dynamics is a stochastic variant of the classic\nmultiplicative weights update (MWU) method. Consequently, we arrive at the\nfollowing interesting converse: the learning dynamics on a finite population\nconsidered here can be viewed as a novel distributed and low-memory\nimplementation of the classic MWU method.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:15:18 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Celis", "L. Elisa", ""], ["Krafft", "Peter M.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1705.03603", "submitter": "Fangming Liu", "authors": "Alex Thomo, Fangming Liu", "title": "Computation of K-Core Decomposition on Giraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graphs are an essential data structure that can represent the structure of\nsocial networks. Many online companies, in order to provide intelligent and\npersonalized services for their users, aim to comprehensively analyze a\nsignificant amount of graph data with different features. One example is k-core\ndecomposition which captures the degree of connectedness in social graphs. The\nmain purpose of this report is to explore a distributed algorithm for k-core\ndecomposition on Apache Giraph. Namely, we would like to determine whether a\ncluster-based, Giraph implementation of k-core decomposition that we provide is\nmore efficient than a single-machine, disk-based implementation on GraphChi for\nlarge networks. In this report, we describe (a) the programming model of Giraph\nand GraphChi, (b) the specific implementation of k-core decomposition with\nGiraph, and (c) the result comparison between Giraph and GraphChi. By analyzing\nthe results, we conclude that Giraph is faster than GraphChi when dealing with\nlarge data. However, since worker nodes need time to communicate with each\nother, Giraph is not very efficient for small data.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 04:08:29 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 23:10:57 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Thomo", "Alex", ""], ["Liu", "Fangming", ""]]}, {"id": "1705.03637", "submitter": "Shahbaz Khan", "authors": "Shahbaz Khan", "title": "Near Optimal Parallel Algorithms for Dynamic DFS in Undirected Graphs", "comments": "Accepted to appear in SPAA'17, 32 Pages, 5 Figures", "journal-ref": null, "doi": "10.1145/3087556.3087576", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth first search (DFS) tree is a fundamental data structure for solving\ngraph problems. The classical algorithm [SiComp74] for building a DFS tree\nrequires $O(m+n)$ time for a given graph $G$ having $n$ vertices and $m$ edges.\nRecently, Baswana et al. [SODA16] presented a simple algorithm for updating DFS\ntree of an undirected graph after an edge/vertex update in $\\tilde{O}(n)$ time.\nHowever, their algorithm is strictly sequential. We present an algorithm\nachieving similar bounds, that can be adopted easily to the parallel\nenvironment.\n  In the parallel model, a DFS tree can be computed from scratch using $m$\nprocessors in expected $\\tilde{O}(1)$ time [SiComp90] on an EREW PRAM, whereas\nthe best deterministic algorithm takes $\\tilde{O}(\\sqrt{n})$ time\n[SiComp90,JAlg93] on a CRCW PRAM. Our algorithm can be used to develop optimal\n(upto polylog n factors deterministic algorithms for maintaining fully dynamic\nDFS and fault tolerant DFS, of an undirected graph.\n  1- Parallel Fully Dynamic DFS:\n  Given an arbitrary online sequence of vertex/edge updates, we can maintain a\nDFS tree of an undirected graph in $\\tilde{O}(1)$ time per update using $m$\nprocessors on an EREW PRAM.\n  2- Parallel Fault tolerant DFS:\n  An undirected graph can be preprocessed to build a data structure of size\nO(m) such that for a set of $k$ updates (where $k$ is constant) in the graph,\nthe updated DFS tree can be computed in $\\tilde{O}(1)$ time using $n$\nprocessors on an EREW PRAM.\n  Moreover, our fully dynamic DFS algorithm provides, in a seamless manner,\nnearly optimal (upto polylog n factors) algorithms for maintaining a DFS tree\nin semi-streaming model and a restricted distributed model. These are the first\nparallel, semi-streaming and distributed algorithms for maintaining a DFS tree\nin the dynamic setting.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 07:35:17 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Khan", "Shahbaz", ""]]}, {"id": "1705.03673", "submitter": "Till Fluschnik", "authors": "Till Fluschnik, Marco Morik, and Manuel Sorge", "title": "The Complexity of Routing with Few Collisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of routing multiple objects through a\nnetwork in such a way that only few collisions occur: Given a graph $G$ with\ntwo distinct terminal vertices and two positive integers $p$ and $k$, the\nquestion is whether one can connect the terminals by at least $p$ routes (e.g.\npaths) such that at most $k$ edges are time-wise shared among them. We study\nthree types of routes: traverse each vertex at most once (paths), each edge at\nmost once (trails), or no such restrictions (walks). We prove that for paths\nand trails the problem is NP-complete on undirected and directed graphs even if\n$k$ is constant or the maximum vertex degree in the input graph is constant.\nFor walks, however, it is solvable in polynomial time on undirected graphs for\narbitrary $k$ and on directed graphs if $k$ is constant. We additionally study\nfor all route types a variant of the problem where the maximum length of a\nroute is restricted by some given upper bound. We prove that this\nlength-restricted variant has the same complexity classification with respect\nto paths and trails, but for walks it becomes NP-complete on undirected graphs.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 09:38:03 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Fluschnik", "Till", ""], ["Morik", "Marco", ""], ["Sorge", "Manuel", ""]]}, {"id": "1705.03686", "submitter": "Daniel Neuen", "authors": "Daniel Neuen and Pascal Schweitzer", "title": "Benchmark Graphs for Practical Graph Isomorphism", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art solvers for the graph isomorphism problem can readily\nsolve generic instances with tens of thousands of vertices. Indeed, experiments\nshow that on inputs without particular combinatorial structure the algorithms\nscale almost linearly. In fact, it is non-trivial to create challenging\ninstances for such solvers and the number of difficult benchmark graphs\navailable is quite limited. We describe a construction to efficiently generate\nsmall instances for the graph isomorphism problem that are difficult or even\ninfeasible for said solvers. Up to this point the only other available\ninstances posing challenges for isomorphism solvers were certain incidence\nstructures of combinatorial objects (such as projective planes, Hadamard\nmatrices, Latin squares, etc.). Experiments show that starting from 1500\nvertices our new instances are several orders of magnitude more difficult on\ncomparable input sizes. More importantly, our method is generic and efficient\nin the sense that one can quickly create many isomorphism instances on a\ndesired number of vertices. In contrast to this, said combinatorial objects are\nrare and difficult to generate and with the new construction it is possible to\ngenerate an abundance of instances of arbitrary size. Our construction hinges\non the multipedes of Gurevich and Shelah and the Cai-F\\\"{u}rer-Immerman gadgets\nthat realize a certain abelian automorphism group and have repeatedly played a\nrole in the context of graph isomorphism. Exploring limits of such\nconstructions, we also explain that there are group theoretic obstructions to\ngeneralizing the construction with non-abelian gadgets.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 10:28:17 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Neuen", "Daniel", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1705.03934", "submitter": "Denis Kleyko", "authors": "Denis Kleyko, Abbas Rahimi, Ross W. Gayler, Evgeny Osipov", "title": "Autoscaling Bloom Filter: Controlling Trade-off Between True and False\n  Positives", "comments": "13 pages, 3 figures", "journal-ref": "Neural Computing and Applications (2019)", "doi": "10.1007/s00521-019-04397-1", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bloom filter is a simple data structure supporting membership queries on a\nset. The standard Bloom filter does not support the delete operation,\ntherefore, many applications use a counting Bloom filter to enable deletion.\nThis paper proposes a generalization of the counting Bloom filter approach,\ncalled \"autoscaling Bloom filters\", which allows adjustment of its capacity\nwith probabilistic bounds on false positives and true positives. In essence,\nthe autoscaling Bloom filter is a binarized counting Bloom filter with an\nadjustable binarization threshold. We present the mathematical analysis of the\nperformance as well as give a procedure for minimization of the false positive\nrate.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 19:45:29 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 13:37:35 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kleyko", "Denis", ""], ["Rahimi", "Abbas", ""], ["Gayler", "Ross W.", ""], ["Osipov", "Evgeny", ""]]}, {"id": "1705.03950", "submitter": "Wouter Kuijper", "authors": "Wouter Kuijper", "title": "Zig-zagging in a Triangulation", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an oblivious walk for point location in 2-dimensional\ntriangulations and a corresponding, strictly monotonically decreasing distance\nmeasure.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 21:01:31 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 14:15:24 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Kuijper", "Wouter", ""]]}, {"id": "1705.04022", "submitter": "Jakub Radoszewski", "authors": "Mai Alzamel, Panagiotis Charalampopoulos, Costas S. Iliopoulos, Solon\n  P. Pissis, Jakub Radoszewski, and Wing-Kin Sung", "title": "Faster algorithms for 1-mappability of a sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-mappability problem, we are given a string x of length n and\nintegers m and k, and we are asked to count, for each length-m factor y of x,\nthe number of other factors of length m of x that are at Hamming distance at\nmost k from y. We focus here on the version of the problem where k = 1. The\nfastest known algorithm for k = 1 requires time O(mn log n/ log log n) and\nspace O(n). We present two algorithms that require worst-case time O(mn) and\nO(n log^2 n), respectively, and space O(n), thus greatly improving the state of\nthe art. Moreover, we present an algorithm that requires average-case time and\nspace O(n) for integer alphabets if m = {\\Omega}(log n/ log {\\sigma}), where\n{\\sigma} is the alphabet size.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 05:52:51 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Alzamel", "Mai", ""], ["Charalampopoulos", "Panagiotis", ""], ["Iliopoulos", "Costas S.", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Sung", "Wing-Kin", ""]]}, {"id": "1705.04033", "submitter": "Orr Fischer", "authors": "Orr Fischer, Tzlil Gonen, Rotem Oshman", "title": "Distributed Property Testing for Subgraph-Freeness Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the subgraph-freeness problem, we are given a constant-size graph $H$, and\nwish to determine whether the network contains $H$ as a subgraph or not. The\n\\emph{property-testing} relaxation of the problem only requires us to\ndistinguish graphs that are $H$-free from graphs that are $\\epsilon$-far from\n$H$-free, meaning an $\\epsilon$-fraction of their edges must be removed to\nobtain an $H$-free graph. Recently, Censor-Hillel et. al. and Fraigniaud et al.\nshowed that in the property-testing regime it is possible to test $H$-freeness\nfor any graph $H$ of size 4 in constant time, $O(1/\\epsilon^2)$ rounds,\nregardless of the network size. However, Fraigniaud et. al. also showed that\ntheir techniques for graphs $H$ of size 4 cannot test $5$-cycle-freeness in\nconstant time.\n  In this paper we revisit the subgraph-freeness problem and show that\n$5$-cycle-freeness, and indeed $H$-freeness for many other graphs $H$\ncomprising more than 4 vertices, can be tested in constant time. We show that\n$C_k$-freeness can be tested in $O(1/\\epsilon)$ rounds for any cycle $C_k$,\nimproving on the running time of $O(1/\\epsilon^2)$ of the previous algorithms\nfor triangle-freeness and $C_4$-freeness. In the special case of triangles, we\nshow that triangle-freeness can be solved in $O(1)$ rounds independently of\n$\\epsilon$, when $\\epsilon$ is not too small with respect to the number of\nnodes and edges. We also show that $T$-freeness for any constant-size tree $T$\ncan be tested in $O(1)$ rounds, even without the property-testing relaxation.\nBuilding on these results, we define a general class of graphs for which we can\ntest subgraph-freeness in $O(1/\\epsilon)$ rounds. This class includes all\ngraphs over 5 vertices except the 5-clique, $K_5$. For cliques $K_s$ over $s\n\\geq 3$ nodes, we show that $K_s$-freeness can be tested in\n$O(m^{1/2-1/(s-2)}/\\epsilon^{1/2+1/(s-2)})$ rounds, where $m$ is the number of\nedges.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 06:34:24 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Fischer", "Orr", ""], ["Gonen", "Tzlil", ""], ["Oshman", "Rotem", ""]]}, {"id": "1705.04249", "submitter": "Li Heng Liou", "authors": "Cheng-Shang Chang, Chia-Tai Chang, Duan-Shin Lee and Li-Heng Liou", "title": "K-sets+: a Linear-time Clustering Algorithm for Data Points with a\n  Sparse Similarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a new iterative algorithm, called the K-sets+\nalgorithm for clustering data points in a semi-metric space, where the distance\nmeasure does not necessarily satisfy the triangular inequality. We show that\nthe K-sets+ algorithm converges in a finite number of iterations and it retains\nthe same performance guarantee as the K-sets algorithm for clustering data\npoints in a metric space. We then extend the applicability of the K-sets+\nalgorithm from data points in a semi-metric space to data points that only have\na symmetric similarity measure. Such an extension leads to great reduction of\ncomputational complexity. In particular, for an n * n similarity matrix with m\nnonzero elements in the matrix, the computational complexity of the K-sets+\nalgorithm is O((Kn + m)I), where I is the number of iterations. The memory\ncomplexity to achieve that computational complexity is O(Kn + m). As such, both\nthe computational complexity and the memory complexity are linear in n when the\nn * n similarity matrix is sparse, i.e., m = O(n). We also conduct various\nexperiments to show the effectiveness of the K-sets+ algorithm by using a\nsynthetic dataset from the stochastic block model and a real network from the\nWonderNetwork website.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 15:39:48 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Chang", "Cheng-Shang", ""], ["Chang", "Chia-Tai", ""], ["Lee", "Duan-Shin", ""], ["Liou", "Li-Heng", ""]]}, {"id": "1705.04395", "submitter": "Farhad Shahrokhi", "authors": "Farhad Shahrokhi", "title": "Unit Incomparability Dimension and Clique Cover Width in Graphs", "comments": null, "journal-ref": "Congressus Numerantium, 213 (2012), 91-98", "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a clique cover $C$ in the undirected graph $G$, the {\\it clique cover\ngraph} of $C$ is the graph obtained by contracting the vertices of each clique\nin $C$ into a single vertex. The {\\it clique cover width} of $G$, denoted by\n$CCW(G)$, is the minimum value of the bandwidth of all clique cover graphs in\n$G$. Any $G$ with $CCW(G)=1$ is known to be an incomparability graph, and hence\nis called, a {\\it unit incomparability graph}. We introduced the {\\it unit\nincomparability dimension of $G$}, denoted by$Udim(G)$, to be the smallest\ninteger $d$ so that there are unit incomparability graphs $H_i$ with\n$V(H_i)=V(G), i=1,2,...,d$, so that $E(G)=\\cap_{i=1}^d E(G_i)$. We prove a\ndecomposition theorem establishing the inequality $Udim(G)\\le CCW(G)$.\nSpecifically, given any $G$, there are unit incomparability graphs\n$H_1,H_2,...,H_{CC(W)}$ with $V(H_i)=V(G)$ so that and $E(G)=\\cap_{i=1}^{CCW}\nE(H_i)$. In addition, $H_i$ is co-bipartite, for $i=1,2,...,CCW(G)-1$.\nFurthermore, we observe that $CCW(G)\\ge s(G)/2-1$, where $s(G)$ is the number\nof leaves in a largest induced star of $G$ , and use Ramsey Theory to give an\nupper bound on $s(G)$, when $G$ is represented as an intersection graph using\nour decomposition theorem. Finally, when $G$ is an incomparability graph we\nprove that $CCW (G)\\le s(G)-1$.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 23:14:57 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Shahrokhi", "Farhad", ""]]}, {"id": "1705.04544", "submitter": "Karl D\\\"aubel", "authors": "Aaron Bernstein, Karl D\\\"aubel, Yann Disser, Max Klimm, Torsten\n  M\\\"utze, Frieder Smolny", "title": "Distance-preserving graph contractions", "comments": "An extended abstract of this work has appeared in the Proceedings of\n  the 9th Innovations in Theoretical Computer Science Conference (ITCS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression and sparsification algorithms are frequently applied in a\npreprocessing step before analyzing or optimizing large networks/graphs. In\nthis paper we propose and study a new framework contracting edges of a graph\n(merging vertices into super-vertices) with the goal of preserving pairwise\ndistances as accurately as possible. Formally, given an edge-weighted graph,\nthe contraction should guarantee that for any two vertices at distance $d$, the\ncorresponding super-vertices remain at distance at least $\\varphi(d)$ in the\ncontracted graph, where $\\varphi$ is a tolerance function bounding the\npermitted distance distortion. We present a comprehensive picture of the\nalgorithmic complexity of the contraction problem for affine tolerance\nfunctions $\\varphi(x)=x/\\alpha-\\beta$, where $\\alpha\\geq 1$ and $\\beta\\geq 0$\nare arbitrary real-valued parameters. Specifically, we present polynomial-time\nalgorithms for trees as well as hardness and inapproximability results for\ndifferent graph classes, precisely separating easy and hard cases. Further we\nanalyze the asymptotic behavior of contractions, and find efficient algorithms\nto compute (non-optimal) contractions despite our hardness results.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 12:52:49 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 17:00:39 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 16:37:42 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 13:51:48 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bernstein", "Aaron", ""], ["D\u00e4ubel", "Karl", ""], ["Disser", "Yann", ""], ["Klimm", "Max", ""], ["M\u00fctze", "Torsten", ""], ["Smolny", "Frieder", ""]]}, {"id": "1705.04589", "submitter": "Solon Pissis", "authors": "Mai Alzamel, Panagiotis Charalampopoulos, Costas S. Iliopoulos, Solon\n  P. Pissis", "title": "How to answer a small batch of RMQs or LCA queries in practice", "comments": "Accepted to IWOCA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Range Minimum Query (RMQ) problem, we are given an array $A$ of $n$\nnumbers and we are asked to answer queries of the following type: for indices\n$i$ and $j$ between $0$ and $n-1$, query $\\text{RMQ}_A(i,j)$ returns the index\nof a minimum element in the subarray $A[i..j]$. Answering a small batch of RMQs\nis a core computational task in many real-world applications, in particular due\nto the connection with the Lowest Common Ancestor (LCA) problem. With small\nbatch, we mean that the number $q$ of queries is $o(n)$ and we have them all at\nhand. It is therefore not relevant to build an $\\Omega(n)$-sized data structure\nor spend $\\Omega(n)$ time to build a more succinct one. It is well-known, among\npractitioners and elsewhere, that these data structures for online querying\ncarry high constants in their pre-processing and querying time. We would thus\nlike to answer this batch efficiently in practice. With efficiently in\npractice, we mean that we (ultimately) want to spend $n + \\mathcal{O}(q)$ time\nand $\\mathcal{O}(q)$ space. We write $n$ to stress that the number of\noperations per entry of $A$ should be a very small constant. Here we show how\nexisting algorithms can be easily modified to satisfy these conditions. The\npresented experimental results highlight the practicality of this new scheme.\nThe most significant improvement obtained is for answering a small batch of LCA\nqueries. A library implementation of the presented algorithms is made\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 14:27:28 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Alzamel", "Mai", ""], ["Charalampopoulos", "Panagiotis", ""], ["Iliopoulos", "Costas S.", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1705.04840", "submitter": "Mohsen Ghaffari", "authors": "Manuela Fischer and Mohsen Ghaffari", "title": "Sublogarithmic Distributed Algorithms for Lov\\'asz Local lemma, and the\n  Complexity Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Checkable Labeling (LCL) problems include essentially all the classic\nproblems of $\\mathsf{LOCAL}$ distributed algorithms. In a recent enlightening\nrevelation, Chang and Pettie [arXiv 1704.06297] showed that any LCL (on bounded\ndegree graphs) that has an $o(\\log n)$-round randomized algorithm can be solved\nin $T_{LLL}(n)$ rounds, which is the randomized complexity of solving (a\nrelaxed variant of) the Lov\\'asz Local Lemma (LLL) on bounded degree $n$-node\ngraphs. Currently, the best known upper bound on $T_{LLL}(n)$ is $O(\\log n)$,\nby Chung, Pettie, and Su [PODC'14], while the best known lower bound is\n$\\Omega(\\log\\log n)$, by Brandt et al. [STOC'16]. Chang and Pettie conjectured\nthat there should be an $O(\\log\\log n)$-round algorithm.\n  Making the first step of progress towards this conjecture, and providing a\nsignificant improvement on the algorithm of Chung et al. [PODC'14], we prove\nthat $T_{LLL}(n)= 2^{O(\\sqrt{\\log\\log n})}$. Thus, any $o(\\log n)$-round\nrandomized distributed algorithm for any LCL problem on bounded degree graphs\ncan be automatically sped up to run in $2^{O(\\sqrt{\\log\\log n})}$ rounds.\n  Using this improvement and a number of other ideas, we also improve the\ncomplexity of a number of graph coloring problems (in arbitrary degree graphs)\nfrom the $O(\\log n)$-round results of Chung, Pettie and Su [PODC'14] to\n$2^{O(\\sqrt{\\log\\log n})}$. These problems include defective coloring, frugal\ncoloring, and list vertex-coloring.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 14:49:23 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 09:08:27 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Fischer", "Manuela", ""], ["Ghaffari", "Mohsen", ""]]}, {"id": "1705.04898", "submitter": "Moti Medina", "authors": "Guy Even and Reut Levi and Moti Medina", "title": "Faster and Simpler Distributed Algorithms for Testing and Correcting\n  Graph Properties in the CONGEST-Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present distributed testing algorithms of graph properties\nin the CONGEST-model [Censor-Hillel et al. 2016]. We present one-sided error\ntesting algorithms in the general graph model.\n  We first describe a general procedure for converting $\\epsilon$-testers with\na number of rounds $f(D)$, where $D$ denotes the diameter of the graph, to\n$O((\\log n)/\\epsilon)+f((\\log n)/\\epsilon)$ rounds, where $n$ is the number of\nprocessors of the network. We then apply this procedure to obtain an optimal\ntester, in terms of $n$, for testing bipartiteness, whose round complexity is\n$O(\\epsilon^{-1}\\log n)$, which improves over the $poly(\\epsilon^{-1} \\log\nn)$-round algorithm by Censor-Hillel et al. (DISC 2016). Moreover, for\ncycle-freeness, we obtain a \\emph{corrector} of the graph that locally corrects\nthe graph so that the corrected graph is acyclic. Note that, unlike a tester, a\ncorrector needs to mend the graph in many places in the case that the graph is\nfar from having the property.\n  In the second part of the paper we design algorithms for testing whether the\nnetwork is $H$-free for any connected $H$ of size up to four with round\ncomplexity of $O(\\epsilon^{-1})$. This improves over the\n$O(\\epsilon^{-2})$-round algorithms for testing triangle freeness by\nCensor-Hillel et al. (DISC 2016) and for testing excluded graphs of size $4$ by\nFraigniaud et al. (DISC 2016).\n  In the last part we generalize the global tester by Iwama and Yoshida (ITCS\n2014) of testing $k$-path freeness to testing the exclusion of any tree of\norder $k$. We then show how to simulate this algorithm in the CONGEST-model in\n$O(k^{k^2+1}\\cdot\\epsilon^{-k})$ rounds.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 23:17:28 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Even", "Guy", ""], ["Levi", "Reut", ""], ["Medina", "Moti", ""]]}, {"id": "1705.04970", "submitter": "Shunji Umetani", "authors": "Shunji Umetani, Masanao Arakawa, Mutsunori Yagiura", "title": "Relaxation heuristics for the set multicover problem with generalized\n  upper bound constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension of the set covering problem (SCP) introducing\n(i)~multicover and (ii)~generalized upper bound (GUB)~constraints. For the\nconventional SCP, the pricing method has been introduced to reduce the size of\ninstances, and several efficient heuristic algorithms based on such reduction\ntechniques have been developed to solve large-scale instances. However, GUB\nconstraints often make the pricing method less effective, because they often\nprevent solutions from containing highly evaluated variables together. To\novercome this problem, we develop heuristic algorithms to reduce the size of\ninstances, in which new evaluation schemes of variables are introduced taking\naccount of GUB constraints. We also develop an efficient implementation of a\n2-flip neighborhood local search algorithm that reduces the number of\ncandidates in the neighborhood without sacrificing the solution quality. In\norder to guide the search to visit a wide variety of good solutions, we also\nintroduce a path relinking method that generates new solutions by combining two\nor more solutions obtained so far. According to computational comparison on\nbenchmark instances, the proposed method succeeds in selecting a small number\nof promising variables properly and performs quite effectively even for\nlarge-scale instances having hard GUB constraints.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 14:35:54 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 03:05:53 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Umetani", "Shunji", ""], ["Arakawa", "Masanao", ""], ["Yagiura", "Mutsunori", ""]]}, {"id": "1705.05105", "submitter": "Mauricio Toro", "authors": "Juan Manuel Ciro Restrepo, Andr\\'es Felipe Zapata Palacio and Mauricio\n  Toro", "title": "Assembling sequences of DNA using an on-line algorithm based on DeBruijn\n  graphs", "comments": "5 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.OT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The problem of assembling DNA fragments starting from imperfect strings given\nby a sequencer, classified as NP hard when trying to get perfect answers, has a\nhuge importance in several fields, because of its relation with the possibility\nof detecting similarities between animals, dangerous pests in crops, and so on.\nSome of the algorithms and data structures that have been created to solve this\nproblem are Needleman Wunsch algorithm, DeBruijn graphs and greedy algorithms\nworking on overlaps graphs; these try to work out the problem from different\napproaches that give place to certain advantages and disadvantages to be\ndiscussed.\n  In this article we first expose a summary of the research done on already\ncreated solutions for the DNA assembly problem, to present later an on-line\nsolution to the same matter, which, despite not considering mutations, would\nhave the capacity of using only the necessary amount of readings to assemble an\nuser specified amount of genes.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:54:06 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 13:37:27 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 19:52:38 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Restrepo", "Juan Manuel Ciro", ""], ["Palacio", "Andr\u00e9s Felipe Zapata", ""], ["Toro", "Mauricio", ""]]}, {"id": "1705.05154", "submitter": "Heng Guo", "authors": "Heng Guo and Kaan Kara and Ce Zhang", "title": "Layerwise Systematic Scan: Deep Boltzmann Machines and Beyond", "comments": "v2: typo fixes and improved presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Markov chain Monte Carlo methods, one of the greatest discrepancies\nbetween theory and system is the scan order - while most theoretical\ndevelopment on the mixing time analysis deals with random updates, real-world\nsystems are implemented with systematic scans. We bridge this gap for models\nthat exhibit a bipartite structure, including, most notably, the\nRestricted/Deep Boltzmann Machine. The de facto implementation for these models\nscans variables in a layerwise fashion. We show that the Gibbs sampler with a\nlayerwise alternating scan order has its relaxation time (in terms of epochs)\nno larger than that of a random-update Gibbs sampler (in terms of variable\nupdates). We also construct examples to show that this bound is asymptotically\ntight. Through standard inequalities, our result also implies a comparison on\nthe mixing times.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 11:00:25 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 11:03:50 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Guo", "Heng", ""], ["Kara", "Kaan", ""], ["Zhang", "Ce", ""]]}, {"id": "1705.05295", "submitter": "Manuel Lafond", "authors": "Cedric Chauve, Mark Jones, Manuel Lafond, C\\'eline Scornavacca,\n  Mathias Weller", "title": "Constructing a Consensus Phylogeny from a Leaf-Removal Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the evolution of a set of genes or species is a fundamental\nproblem in evolutionary biology. The problem we study here takes as input a set\nof trees describing {possibly discordant} evolutionary scenarios for a given\nset of genes or species, and aims at finding a single tree that minimizes the\nleaf-removal distance to the input trees. This problem is a specific instance\nof the general consensus/supertree problem, widely used to combine or summarize\ndiscordant evolutionary trees. The problem we introduce is specifically\ntailored to address the case of discrepancies between the input trees due to\nthe misplacement of individual taxa. Most supertree or consensus tree problems\nare computationally intractable, and we show that the problem we introduce is\nalso NP-hard. We provide tractability results in form of a 2-approximation\nalgorithm. We also introduce a variant that minimizes the maximum number $d$ of\nleaves that are removed from any input tree, and provide a parameterized\nalgorithm for this problem with parameter $d$.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 15:31:15 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 10:52:09 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 02:28:40 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Chauve", "Cedric", ""], ["Jones", "Mark", ""], ["Lafond", "Manuel", ""], ["Scornavacca", "C\u00e9line", ""], ["Weller", "Mathias", ""]]}, {"id": "1705.05646", "submitter": "Seri Khoury", "authors": "Keren Censor-Hillel, Seri Khoury, and Ami Paz", "title": "Quadratic and Near-Quadratic Lower Bounds for the CONGEST Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first super-linear lower bounds for natural graph problems in\nthe CONGEST model, answering a long-standing open question.\n  Specifically, we show that any exact computation of a minimum vertex cover or\na maximum independent set requires $\\Omega(n^2/\\log^2{n})$ rounds in the worst\ncase in the CONGEST model, as well as any algorithm for $\\chi$-coloring a\ngraph, where $\\chi$ is the chromatic number of the graph. We further show that\nsuch strong lower bounds are not limited to NP-hard problems, by showing two\nsimple graph problems in P which require a quadratic and near-quadratic number\nof rounds.\n  Finally, we address the problem of computing an exact solution to weighted\nall-pairs-shortest-paths (APSP), which arguably may be considered as a\ncandidate for having a super-linear lower bound. We show a simple $\\Omega(n)$\nlower bound for this problem, which implies a separation between the weighted\nand unweighted cases, since the latter is known to have a complexity of\n$\\Theta(n/\\log{n})$. We also formally prove that the standard Alice-Bob\nframework is incapable of providing a super-linear lower bound for exact\nweighted APSP, whose complexity remains an intriguing open question.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 11:11:37 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""], ["Paz", "Ami", ""]]}, {"id": "1705.05735", "submitter": "Johan Ugander", "authors": "Jon Kleinberg, Sendhil Mullainathan, Johan Ugander", "title": "Comparison-Based Choices", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A broad range of on-line behaviors are mediated by interfaces in which people\nmake choices among sets of options. A rich and growing line of work in the\nbehavioral sciences indicate that human choices follow not only from the\nutility of alternatives, but also from the choice set in which alternatives are\npresented. In this work we study comparison-based choice functions, a simple\nbut surprisingly rich class of functions capable of exhibiting so-called\nchoice-set effects. Motivated by the challenge of predicting complex choices,\nwe study the query complexity of these functions in a variety of settings. We\nconsider settings that allow for active queries or passive observation of a\nstream of queries, and give analyses both at the granularity of individuals or\npopulations that might exhibit heterogeneous choice behavior. Our main result\nis that any comparison-based choice function in one dimension can be inferred\nas efficiently as a basic maximum or minimum choice function across many query\ncontexts, suggesting that choice-set effects need not entail any fundamental\nalgorithmic barriers to inference. We also introduce a class of choice\nfunctions we call distance-comparison-based functions, and briefly discuss the\nanalysis of such functions. The framework we outline provides intriguing\nconnections between human choice behavior and a range of questions in the\ntheory of sorting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:44:13 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Kleinberg", "Jon", ""], ["Mullainathan", "Sendhil", ""], ["Ugander", "Johan", ""]]}, {"id": "1705.05755", "submitter": "Soheil Ehsani", "authors": "Sina Dehghani, Soheil Ehsani, MohammadTaghi HajiAghayi, Vahid Liaghat,\n  Saeed Seddighin", "title": "Stochastic k-Server: How Should Uber Work?", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a stochastic variant of the celebrated k-server\nproblem. In the k-server problem, we are required to minimize the total\nmovement of k servers that are serving an online sequence of t requests in a\nmetric. In the stochastic setting we are given t independent distributions\n<P_1, P_2,..., P_t> in advance, and at every time step i a request is drawn\nfrom Pi. Designing the optimal online algorithm in such setting is NP-hard,\ntherefore the emphasis of our work is on designing an approximately optimal\nonline algorithm. We first show a structural characterization for a certain\nclass of non-adaptive online algorithms. We prove that in general metrics, the\nbest of such algorithms has a cost of no worse than three times that of the\noptimal online algorithm. Next, we present an integer program that finds the\noptimal algorithm of this class for any arbitrary metric. Finally, by rounding\nthe solution of the linear relaxation of this program, we present an online\nalgorithm for the stochastic k-server problem with the approximation factor of\n3 in the line and circle metrics and O(log n) in a general metric of size n.\nMoreover, we define the Uber problem, in which each demand consists of two\nendpoints, a source and a destination. We show that given an a-approximation\nalgorithm for the k-server problem, we can obtain an (a+2)-approximation\nalgorithm for the Uber problem. Motivated by the fact that demands are usually\nhighly correlated with the time we study the stochastic Uber problem.\nFurthermore, we extend our results to the correlated setting where the\nprobability of a request arriving at a certain point depends not only on the\ntime step but also on the previously arrived requests.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:10:31 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 20:03:03 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Dehghani", "Sina", ""], ["Ehsani", "Soheil", ""], ["HajiAghayi", "MohammadTaghi", ""], ["Liaghat", "Vahid", ""], ["Seddighin", "Saeed", ""]]}, {"id": "1705.05920", "submitter": "Alper Atamturk", "authors": "Alper Atamturk, Birce Tezel, Simge Kucukyavuz", "title": "Path Cover and Path Pack Inequalities for the Capacitated Fixed-Charge\n  Network Flow Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": "BCOL 15.03", "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capacitated fixed-charge network flows are used to model a variety of\nproblems in telecommunication, facility location, production planning and\nsupply chain management. In this paper, we investigate capacitated path\nsubstructures and derive strong and easy-to-compute \\emph{path cover and path\npack inequalities}. These inequalities are based on an explicit\ncharacterization of the submodular inequalities through a fast computation of\nparametric minimum cuts on a path, and they generalize the well-known flow\ncover and flow pack inequalities for the single-node relaxations of\nfixed-charge flow models. We provide necessary and sufficient facet conditions.\nComputational results demonstrate the effectiveness of the inequalities when\nused as cuts in a branch-and-cut algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 20:59:00 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Atamturk", "Alper", ""], ["Tezel", "Birce", ""], ["Kucukyavuz", "Simge", ""]]}, {"id": "1705.06125", "submitter": "Petr Ry\\v{s}av\\'y", "authors": "Petr Ry\\v{s}av\\'y and Filip \\v{Z}elezn\\'y", "title": "Estimating Sequence Similarity from Read Sets for Clustering\n  Next-Generation Sequencing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cluster sequences given only their read-set representations, one may try\nto reconstruct each one from the corresponding read set, and then employ\nconventional (dis)similarity measures such as the edit distance on the\nassembled sequences. This approach is however problematic and we propose\ninstead to estimate the similarities directly from the read sets. Our approach\nis based on an adaptation of the Monge-Elkan similarity known from the field of\ndatabases. It avoids the NP-hard problem of sequence assembly. For low coverage\ndata it results in a better approximation of the true sequence similarities and\nconsequently in better clustering, in comparison to the\nfirst-assemble-then-cluster approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 11:01:47 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Ry\u0161av\u00fd", "Petr", ""], ["\u017delezn\u00fd", "Filip", ""]]}, {"id": "1705.06271", "submitter": "Thomas Dickerson", "authors": "Thomas D. Dickerson", "title": "Fast Snapshottable Concurrent Braun Heaps", "comments": "pre-print, submitted to DISC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new concurrent heap algorithm, based on a stateless\nshape property, which efficiently maintains balance during insert and removeMin\noperations implemented with hand-over-hand locking. It also provides a O(1)\nlinearizable snapshot operation based on lazy copy-on-write semantics. Such\nsnapshots can be used to provide consistent views of the heap during iteration,\nas well as to make speculative updates (which can later be dropped).\n  The simplicity of the algorithm allows it to be easily proven correct, and\nthe choice of shape property provides priority queue performance which is\ncompetitive with highly optimized skiplist implementations (and has stronger\nbounds on worst-case time complexity).\n  A Scala reference implementation is provided.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:43:26 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Dickerson", "Thomas D.", ""]]}, {"id": "1705.06319", "submitter": "Kanthi Sarpatwar", "authors": "Kanthi K. Sarpatwar, Baruch Schieber and Hadas Shachnai", "title": "Constrained Submodular Maximization via Greedy Local Search", "comments": "Title changed from \"Interleaved Algorithms for Constrained Submodular\n  Function Maximization\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple combinatorial $\\frac{1 -e^{-2}}{2}$-approximation\nalgorithm for maximizing a monotone submodular function subject to a knapsack\nand a matroid constraint.\n  This classic problem is known to be hard to approximate within factor better\nthan $1 - 1/e$. We show that the algorithm can be extended to yield a ratio of\n$\\frac{1 - e^{-(k+1)}}{k+1}$ for the problem with a single knapsack and the\nintersection of $k$ matroid constraints, for any fixed $k > 1$.\n  Our algorithms, which combine the greedy algorithm of [Khuller, Moss and\nNaor, 1999] and [Sviridenko, 2004] with local search, show the power of this\nnatural framework in submodular maximization with combined constraints.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 19:21:03 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 01:09:21 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 01:37:51 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Sarpatwar", "Kanthi K.", ""], ["Schieber", "Baruch", ""], ["Shachnai", "Hadas", ""]]}, {"id": "1705.06425", "submitter": "Bhadrachalam Chitturi", "authors": "Bhadrachalam Chitturi", "title": "Layered graphs: a class that admits polynomial time solutions for some\n  hard problems", "comments": "14 pages, 1 figure. A generic algorithm is given. It can be extended\n  to handle a wide range of hard problems. Space complexity was incorrectly\n  given as $O(k^2)$ for MIS (identical for MVC) in the earlier version instead\n  of $O(k 2^k)$. The edges in $LLG$ are more clearly defined. For $ V_{it}$ the\n  only permissible edges are $(V_{it}, V_{jt})$ where $j \\in \\{ i-1, i+1\\}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The independent set on a graph $G=(V,E)$ is a subset of $V$ such that no two\nvertices in the subset have an edge between them. The MIS problem on $G$ seeks\nto identify an independent set with maximum cardinality, i.e. maximum\nindependent set or MIS. $V* \\subseteq V$ is a vertex cover $G=(V,E)$ if every\nedge in the graph is incident upon at least one vertex in $V*$. $V* \\subseteq\nV$ is dominating set of $G=(V,E)$ if forall $v \\in V$ either $v \\in V*$ or\n$\\exists u \\in V*$ and $(u,v) \\in E$. A connected dominating set, CDS, is a\ndominating set that forms a single component in $G$. The MVC problem on $G$\nseeks to identify a vertex cover with minimum cardinality, i.e. minimum vertex\ncover or MVC. Likewise, CVC seeks a connected vertex cover (CVC) with minimum\ncardinality. The problems MDS and CDS seek to identify a dominating set and a\nconnected dominating set respectively of minimum cardinalities. MVC, CVC, MDS,\nand CDS on a general graph are known to be NP-complete. On certain classes of\ngraphs they can be computed in polynomial time. Such algorithms are known for\nbipartite graphs, chordal graphs, cycle graphs, comparability graphs, claw-free\ngraphs, interval graphs and circular arc graphs for some of these problems. In\nthis article we introduce a new class of graphs called a layered graph and show\nthat if the number of vertices in a layer is $O(\\log \\mid V \\mid)$ then MIS,\nMVC, CVC, MDS and CDC can be computed in polynomial time. The restrictions that\nare employed on graph classes that admit polynomial time solutions for hard\nproblems, e.g. lack of cycles, bipartiteness, planarity etc. are not applicable\nfor this class. \\\\ Key words: Independent set, vertex cover, dominating set,\ndynamic programming, complexity, polynomial time algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 06:20:24 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 15:09:20 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Chitturi", "Bhadrachalam", ""]]}, {"id": "1705.06559", "submitter": "Zhaoming Yin", "authors": "Zhaoming Yin, Jijun Tang, Stephen W. Schaeffer, David A. Bader", "title": "Exemplar or Matching: Modeling DCJ Problems with Unequal Content Genome\n  Data", "comments": "17 pages", "journal-ref": "Journal of Combinatorial Optimization, 2016", "doi": "10.1007/s10878-015-9940-4", "report-no": null, "categories": "cs.DS cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance under the DCJ model can be computed in linear time for\ngenomes with equal content or with Indels. But it becomes NP-Hard in the\npresence of duplications, a problem largely unsolved especially when Indels are\nconsidered. In this paper, we compare two mainstream methods to deal with\nduplications and associate them with Indels: one by deletion, namely\nDCJ-Indel-Exemplar distance; versus the other by gene matching, namely\nDCJ-Indel-Matching distance. We design branch-and-bound algorithms with set of\noptimization methods to compute exact distances for both. Furthermore, median\nproblems are discussed in alignment with both of these distance methods, which\nare to find a median genome that minimizes distances between itself and three\ngiven genomes. Lin-Kernighan (LK) heuristic is leveraged and powered up by\nsub-graph decomposition and search space reduction technologies to handle\nmedian computation. A wide range of experiments are conducted on synthetic data\nsets and real data sets to show pros and cons of these two distance metrics per\nse, as well as putting them in the median computation scenario.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 12:53:56 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Yin", "Zhaoming", ""], ["Tang", "Jijun", ""], ["Schaeffer", "Stephen W.", ""], ["Bader", "David A.", ""]]}, {"id": "1705.06730", "submitter": "David Woodruff", "authors": "Flavio Chierichetti, Sreenivas Gollapudi, Ravi Kumar, Silvio Lattanzi,\n  Rina Panigrahy, David P. Woodruff", "title": "Algorithms for $\\ell_p$ Low Rank Approximation", "comments": "To appear in ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating a given matrix by a low-rank matrix\nso as to minimize the entrywise $\\ell_p$-approximation error, for any $p \\geq\n1$; the case $p = 2$ is the classical SVD problem. We obtain the first provably\ngood approximation algorithms for this version of low-rank approximation that\nwork for every value of $p \\geq 1$, including $p = \\infty$. Our algorithms are\nsimple, easy to implement, work well in practice, and illustrate interesting\ntradeoffs between the approximation quality, the running time, and the rank of\nthe approximating matrix.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:01:33 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Chierichetti", "Flavio", ""], ["Gollapudi", "Sreenivas", ""], ["Kumar", "Ravi", ""], ["Lattanzi", "Silvio", ""], ["Panigrahy", "Rina", ""], ["Woodruff", "David P.", ""]]}, {"id": "1705.06855", "submitter": "Michael Haythorpe", "authors": "Vladimir Ejov, Michael Haythorpe, and Serguei Rossomakhine", "title": "Using a Hamiltonian cycle problem algorithm to assist in solving\n  difficult instances of Traveling Salesman Problem", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a hybrid procedure for solving the traveling salesman problem\n(TSP) to provable optimality. We first sparsify the instance, and then use a\nhybrid algorithm that combines a branch-and-cut TSP solver with a Hamiltonian\ncycle problem solver. We demonstrate that this procedure enables us to solve\ndifficult instances to optimality, including one which had remained unsolved\nsince its construction in 2002.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 04:06:13 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ejov", "Vladimir", ""], ["Haythorpe", "Michael", ""], ["Rossomakhine", "Serguei", ""]]}, {"id": "1705.06894", "submitter": "Mingda Qiao", "authors": "Haotian Jiang, Jian Li, Mingda Qiao", "title": "Practical Algorithms for Best-K Identification in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Best-$K$ identification problem (Best-$K$-Arm), we are given $N$\nstochastic bandit arms with unknown reward distributions. Our goal is to\nidentify the $K$ arms with the largest means with high confidence, by drawing\nsamples from the arms adaptively. This problem is motivated by various\npractical applications and has attracted considerable attention in the past\ndecade. In this paper, we propose new practical algorithms for the Best-$K$-Arm\nproblem, which have nearly optimal sample complexity bounds (matching the lower\nbound up to logarithmic factors) and outperform the state-of-the-art algorithms\nfor the Best-$K$-Arm problem (even for $K=1$) in practice.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 08:49:29 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Jiang", "Haotian", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1705.07001", "submitter": "Justin Thaler", "authors": "Daniel Anderson, Pryce Bevan, Kevin Lang, Edo Liberty, Lee Rhodes,\n  Justin Thaler", "title": "A High-Performance Algorithm for Identifying Frequent Items in Data\n  Streams", "comments": "Typo corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating frequencies of items over data streams is a common building block\nin streaming data measurement and analysis. Misra and Gries introduced their\nseminal algorithm for the problem in 1982, and the problem has since been\nrevisited many times due its practicality and applicability. We describe a\nhighly optimized version of Misra and Gries' algorithm that is suitable for\ndeployment in industrial settings. Our code is made public via an open source\nlibrary called DataSketches that is already used by several companies and\nproduction systems.\n  Our algorithm improves on two theoretical and practical aspects of prior\nwork. First, it handles weighted updates in amortized constant time, a common\nrequirement in practice. Second, it uses a simple and fast method for merging\nsummaries that asymptotically improves on prior work even for unweighted\nstreams. We describe experiments confirming that our algorithms are more\nefficient than prior proposals.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:01:53 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 02:16:34 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Anderson", "Daniel", ""], ["Bevan", "Pryce", ""], ["Lang", "Kevin", ""], ["Liberty", "Edo", ""], ["Rhodes", "Lee", ""], ["Thaler", "Justin", ""]]}, {"id": "1705.07018", "submitter": "Pavel Vesel\\'y", "authors": "Martin B\\\"ohm, {\\L}ukasz Je\\.z, Ji\\v{r}\\'i Sgall, Pavel Vesel\\'y", "title": "On Packet Scheduling with Adversarial Jamming and Speedup", "comments": "Appeared in Proc. of the 15th Workshop on Approximation and Online\n  Algorithms (WAOA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Packet Scheduling with Adversarial Jamming packets of arbitrary sizes\narrive over time to be transmitted over a channel in which instantaneous\njamming errors occur at times chosen by the adversary and not known to the\nalgorithm. The transmission taking place at the time of jamming is corrupt, and\nthe algorithm learns this fact immediately. An online algorithm maximizes the\ntotal size of packets it successfully transmits and the goal is to develop an\nalgorithm with the lowest possible asymptotic competitive ratio, where the\nadditive constant may depend on packet sizes.\n  Our main contribution is a universal algorithm that works for any speedup and\npacket sizes and, unlike previous algorithms for the problem, it does not need\nto know these properties in advance. We show that this algorithm guarantees\n1-competitiveness with speedup 4, making it the first known algorithm to\nmaintain 1-competitiveness with a moderate speedup in the general setting of\narbitrary packet sizes. We also prove a lower bound of $\\phi+1\\approx 2.618$ on\nthe speedup of any 1-competitive deterministic algorithm, showing that our\nalgorithm is close to the optimum.\n  Additionally, we formulate a general framework for analyzing our algorithm\nlocally and use it to show upper bounds on its competitive ratio for speedups\nin $[1,4)$ and for several special cases, recovering some previously known\nresults, each of which had a dedicated proof. In particular, our algorithm is\n3-competitive without speedup, matching both the (worst-case) performance of\nthe algorithm by Jurdzinski et al. and the lower bound by Anta et al.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:28:14 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 14:25:38 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 09:33:12 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Je\u017c", "\u0141ukasz", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1705.07069", "submitter": "Kevin Yeo", "authors": "Sarvar Patel, Giuseppe Persiano, Kevin Yeo", "title": "CacheShuffle: An Oblivious Shuffle Algorithm Using Caches", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Oblivious Shuffling and K-Oblivious Shuffling, a refinement\nthereof. We provide efficient algorithms for both and discuss their application\nto the design of Oblivious RAM. The task of K-Oblivious Shuffling is to\nobliviously shuffle N encrypted blocks that have been randomly allocated on the\nserver in such a way that an adversary learns nothing about the new allocation\nof blocks. The security guarantee should hold also with respect to an adversary\nthat has learned the initial position of K touched blocks out of the N blocks.\nThe classical notion of Oblivious Shuffling is obtained for K = N.\n  We present a family of algorithms for Oblivious Shuffling. Our first\nconstruction, CacheShuffleRoot, is tailored for clients with $O(\\sqrt{N})$\nblocks of memory and uses $(4+\\epsilon)N$ blocks of bandwidth, for every\n$\\epsilon > 0$. CacheShuffleRoot is a 4.5x improvement over previous best known\nresults on practical sizes of N. We also present CacheShuffle that obliviously\nshuffles using O(S) blocks of client memory with $O(N\\log_S N)$ blocks of\nbandwidth.\n  We then turn to K-Oblivious Shuffling and give algorithms that require 2N +\nf(K) blocks of bandwidth, for some function f. That is, any extra bandwidth\nabove the 2N lower bound depends solely on K. We present KCacheShuffleBasic\nthat uses O(K) client storage and exactly 2N blocks of bandwidth. For smaller\nclient storage requirements, we show KCacheShuffle, which uses O(S) client\nstorage and requires $2N+(1+\\epsilon)O(K\\log_S K)$ blocks of bandwidth.\n  Finally, we consider the case in which, in addition to the N blocks, the\nserver stores D dummy blocks whose content is is irrelevant but still their\npositions must be hidden by the shuffling. For this case, we design algorithm\nKCacheShuffleDummy that, for N + D blocks and K touched blocks, uses O(K)\nclient storage and $D+(2+\\epsilon)N$ blocks of bandwidth.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:15:08 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 22:45:41 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 20:37:23 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Patel", "Sarvar", ""], ["Persiano", "Giuseppe", ""], ["Yeo", "Kevin", ""]]}, {"id": "1705.07157", "submitter": "Colin White", "authors": "Maria-Florina Balcan, Colin White", "title": "Clustering under Local Stability: Bridging the Gap between Worst-Case\n  and Beyond Worst-Case Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.03924", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been substantial interest in clustering research that\ntakes a beyond worst-case approach to the analysis of algorithms. The typical\nidea is to design a clustering algorithm that outputs a near-optimal solution,\nprovided the data satisfy a natural stability notion. For example, Bilu and\nLinial (2010) and Awasthi et al. (2012) presented algorithms that output\nnear-optimal solutions, assuming the optimal solution is preserved under small\nperturbations to the input distances. A drawback to this approach is that the\nalgorithms are often explicitly built according to the stability assumption and\ngive no guarantees in the worst case; indeed, several recent algorithms output\narbitrarily bad solutions even when just a small section of the data does not\nsatisfy the given stability notion.\n  In this work, we address this concern in two ways. First, we provide\nalgorithms that inherit the worst-case guarantees of clustering approximation\nalgorithms, while simultaneously guaranteeing near-optimal solutions when the\ndata is stable. Our algorithms are natural modifications to existing\nstate-of-the-art approximation algorithms. Second, we initiate the study of\nlocal stability, which is a property of a single optimal cluster rather than an\nentire optimal solution. We show our algorithms output all optimal clusters\nwhich satisfy stability locally. Specifically, we achieve strong positive\nresults in our local framework under recent stability notions including metric\nperturbation resilience (Angelidakis et al. 2017) and robust perturbation\nresilience (Balcan and Liang 2012) for the $k$-median, $k$-means, and\nsymmetric/asymmetric $k$-center objectives.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:30:33 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["White", "Colin", ""]]}, {"id": "1705.07258", "submitter": "Ziqi Yan", "authors": "Ziqi Yan, Jiqiang Liu, Gang Li, Zhen Han, Shuo Qiu", "title": "PrivMin: Differentially Private MinHash for Jaccard Similarity\n  Computation", "comments": "27 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many industrial applications of big data, the Jaccard Similarity\nComputation has been widely used to measure the distance between two profiles\nor sets respectively owned by two users. Yet, one semi-honest user with\nunpredictable knowledge may also deduce the private or sensitive information\n(e.g., the existence of a single element in the original sets) of the other\nuser via the shared similarity. In this paper, we aim at solving the privacy\nissues in Jaccard similarity computation with strict differential privacy\nguarantees. To achieve this, we first define the Conditional $\\epsilon$-DPSO, a\nrelaxed differential privacy definition regarding set operations, and prove\nthat the MinHash-based Jaccard Similarity Computation (MH-JSC) satisfies this\ndefinition. Then for achieving strict differential privacy in MH-JSC, we\npropose the PrivMin algorithm, which consists of two private operations: 1) the\nPrivate MinHash Value Generation that works by introducing the Exponential\nnoise to the generation of MinHash signature. 2) the Randomized MinHashing\nSteps Selection that works by adopting Randomized Response technique to\nprivately select several steps within the MinHashing phase that are deployed\nwith the Exponential mechanism. Experiments on real datasets demonstrate that\nthe proposed PrivMin algorithm can successfully retain the utility of the\ncomputed similarity while preserving privacy.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 04:09:12 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yan", "Ziqi", ""], ["Liu", "Jiqiang", ""], ["Li", "Gang", ""], ["Han", "Zhen", ""], ["Qiu", "Shuo", ""]]}, {"id": "1705.07279", "submitter": "Filip Paveti\\'c", "authors": "Filip Paveti\\'c, Ivan Katani\\'c, Gustav Matula, Goran \\v{Z}u\\v{z}i\\'c\n  and Mile \\v{S}iki\\'c", "title": "Fast and simple algorithms for computing both $LCS_{k}$ and $LCS_{k+}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longest Common Subsequence ($LCS$) deals with the problem of measuring\nsimilarity of two strings. While this problem has been analyzed for decades,\nthe recent interest stems from a practical observation that considering single\ncharacters is often too simplistic. Therefore, recent works introduce the\nvariants of $LCS$ based on shared substrings of length exactly or at least $k$\n($LCS_k$ and $LCS_{k+}$ respectively). The main drawback of the\nstate-of-the-art algorithms for computing $LCS_k$ and $LCS_{k+}$ is that they\nwork well only in a limited setting: they either solve the average case well\nwhile being suboptimal in the pathological situations or they achieve a good\nworst-case performance, but fail to exploit the input data properties to speed\nup the computation. Furthermore, these algorithms are based on non-trivial data\nstructures which is not ideal from a practitioner's point of view. We present a\nsingle algorithm to compute both $LCS_k$ and $LCS_{k+}$ which outperforms the\nstate-of-the art algorithms in terms of runtime complexity and requires only\nbasic data structures. In addition, we implement an algorithm to reconstruct\nthe solution which offers significant improvement in terms of memory\nconsumption. Our empirical validation shows that we save several orders of\nmagnitude of memory on human genome data. The C++ implementation of our\nalgorithms is made available at: https://github.com/google/fast-simple-lcsk\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 08:32:12 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 20:36:54 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 06:10:07 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Paveti\u0107", "Filip", ""], ["Katani\u0107", "Ivan", ""], ["Matula", "Gustav", ""], ["\u017du\u017ei\u0107", "Goran", ""], ["\u0160iki\u0107", "Mile", ""]]}, {"id": "1705.07327", "submitter": "Fabian Kuhn", "authors": "Abdolhamid Ghodselahi and Fabian Kuhn", "title": "Dynamic Analysis of the Arrow Distributed Directory Protocol in General\n  Networks", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Arrow protocol is a simple and elegant protocol to coordinate exclusive\naccess to a shared object in a network. The protocol solves the underlying\ndistributed queueing problem by using path reversal on a pre-computed spanning\ntree (or any other tree topology simulated on top of the given network).\n  It is known that the Arrow protocol solves the problem with a competitive\nratio of O(log D) on trees of diameter D. This implies a distributed queueing\nalgorithm with competitive ratio O(s*log D) for general networks with a\nspanning tree of diameter D and stretch s. In this work we show that when\nrunning the Arrow protocol on top of the well-known probabilistic tree\nembedding of Fakcharoenphol, Rao, and Talwar [STOC 03], we obtain a randomized\ndistributed queueing algorithm with a competitive ratio of O(log n) even on\ngeneral network topologies. The result holds even if the queueing requests\noccur in an arbitrarily dynamic and concurrent fashion and even if\ncommunication is asynchronous. From a technical point of view, the main of the\npaper shows that the competitive ratio of the Arrow protocol is constant on a\nspecial family of tree topologies, known as hierarchically well separated\ntrees.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 16:25:22 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ghodselahi", "Abdolhamid", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1705.07369", "submitter": "Ellis Hershkowitz", "authors": "Keren Censor-Hillel, Bernhard Haeupler, D. Ellis Hershkowitz, Goran\n  Zuzic", "title": "Broadcasting in Noisy Radio Networks", "comments": "Principles of Distributed Computing 2017", "journal-ref": null, "doi": "10.1145/3087801.3087808", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely-studied radio network model [Chlamtac and Kutten, 1985] is a\ngraph-based description that captures the inherent impact of collisions in\nwireless communication. In this model, the strong assumption is made that node\n$v$ receives a message from a neighbor if and only if exactly one of its\nneighbors broadcasts.\n  We relax this assumption by introducing a new noisy radio network model in\nwhich random faults occur at senders or receivers. Specifically, for a constant\nnoise parameter $p \\in [0,1)$, either every sender has probability $p$ of\ntransmitting noise or every receiver of a single transmission in its\nneighborhood has probability $p$ of receiving noise.\n  We first study single-message broadcast algorithms in noisy radio networks\nand show that the Decay algorithm [Bar-Yehuda et al., 1992] remains robust in\nthe noisy model while the diameter-linear algorithm of Gasieniec et al., 2007\ndoes not. We give a modified version of the algorithm of Gasieniec et al., 2007\nthat is robust to sender and receiver faults, and extend both this modified\nalgorithm and the Decay algorithm to robust multi-message broadcast algorithms.\n  We next investigate the extent to which (network) coding improves throughput\nin noisy radio networks. We address the previously perplexing result of Alon et\nal. 2014 that worst case coding throughput is no better than worst case routing\nthroughput up to constants: we show that the worst case throughput performance\nof coding is, in fact, superior to that of routing -- by a $\\Theta(\\log(n))$\ngap -- provided receiver faults are introduced. However, we show that any\ncoding or routing scheme for the noiseless setting can be transformed to be\nrobust to sender faults with only a constant throughput overhead. These\ntransformations imply that the results of Alon et al., 2014 carry over to noisy\nradio networks with sender faults.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 00:04:54 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Haeupler", "Bernhard", ""], ["Hershkowitz", "D. Ellis", ""], ["Zuzic", "Goran", ""]]}, {"id": "1705.07551", "submitter": "Tatsuhiko Hatanaka", "authors": "Tatsuhiko Hatanaka, Takehiro Ito and Xiao Zhou", "title": "Parameterized Complexity of the List Coloring Reconfiguration Problem\n  with Graph Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph such that each vertex has its list of available colors,\nand assume that each list is a subset of the common set consisting of $k$\ncolors. For two given list colorings of $G$, we study the problem of\ntransforming one into the other by changing only one vertex color assignment at\na time, while at all times maintaining a list coloring. This problem is known\nto be PSPACE-complete even for bounded bandwidth graphs and a fixed constant\n$k$. In this paper, we study the fixed-parameter tractability of the problem\nwhen parameterized by several graph parameters. We first give a fixed-parameter\nalgorithm for the problem when parameterized by $k$ and the modular-width of an\ninput graph. We next give a fixed-parameter algorithm for the shortest variant\nwhen parameterized by $k$ and the size of a minimum vertex cover of an input\ngraph. As corollaries, we show that the problem for cographs and the shortest\nvariant for split graphs are fixed-parameter tractable even when only $k$ is\ntaken as a parameter. On the other hand, we prove that the problem is W[1]-hard\nwhen parameterized only by the size of a minimum vertex cover of an input\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 04:22:49 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Hatanaka", "Tatsuhiko", ""], ["Ito", "Takehiro", ""], ["Zhou", "Xiao", ""]]}, {"id": "1705.07728", "submitter": "Svyatoslav Covanov", "authors": "Svyatoslav Covanov (CARAMBA)", "title": "Improved method for finding optimal formulae for bilinear maps in a\n  finite field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework\nto exhaustively search for optimal formulae for evaluating bilinear maps, such\nas Strassen or Karatsuba formulae. The main contribution of this work is a new\ncriterion to aggressively prune useless branches in the exhaustive search, thus\nleading to the computation of new optimal formulae, in particular for the short\nproduct modulo X 5 and the circulant product modulo (X 5 -- 1). Moreover , we\nare able to prove that there is essentially only one optimal decomposition of\nthe product of 3 x 2 by 2 x 3 matrices up to the action of some group of\nautomorphisms.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 06:20:33 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 13:39:12 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 16:10:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Covanov", "Svyatoslav", "", "CARAMBA"]]}, {"id": "1705.07746", "submitter": "Zhaoming Yin", "authors": "Zhaoming Yin, Xuan Shi", "title": "Taming Near Repeat Calculation for Crime Analysis via Cohesive Subgraph\n  Computing", "comments": "The Twelfth International Conference on Advanced Geographic\n  Information Systems, Applications, and Services GEOProcessing 2020, ISBN\n  978-1-61208-762-7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near repeat (NR) is a well known phenomenon in crime analysis assuming that\ncrime events exhibit correlations within a given time and space frame.\nTraditional NR calculation generates 2 event pairs if 2 events happened within\na given space and time limit. When the number of events is large, however, NR\ncalculation is time consuming and how these pairs are organized are not yet\nexplored. In this paper, we designed a new approach to calculate clusters of NR\nevents efficiently. To begin with, R-tree is utilized to index crime events, a\nsingle event is represented by a vertex whereas edges are constructed by range\nquerying the vertex in R-tree, and a graph is formed. Cohesive subgraph\napproaches are applied to identify the event chains. k-clique, k-truss, k-core\nplus DBSCAN algorithms are implemented in sequence with respect to their varied\nrange of ability to find cohesive subgraphs. Real world crime data in Chicago,\nNew York and Washington DC are utilized to conduct experiments. The experiment\nconfirmed that near repeat is a solid effect in real big crime data by\nconducting Mapreduce empowered knox tests. The performance of 4 different\nalgorithms are validated, while the quality of the algorithms are gauged by the\ndistribution of number of cohesive subgraphs and their clustering coefficients.\nThe proposed framework is the first to process the real crime data of million\nrecord scale, and is the first to detect NR events with size of more than 2.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 12:39:09 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 22:18:53 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Yin", "Zhaoming", ""], ["Shi", "Xuan", ""]]}, {"id": "1705.07756", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni and Gianluca Della Vedova and Yuri Pirola and Marco\n  Previtali and Raffaella Rizzi", "title": "Computing the BWT and LCP array of a Set of Strings in External Memory", "comments": "Theoretical Computer Science (2020). arXiv admin note: text overlap\n  with arXiv:1607.08342", "journal-ref": null, "doi": "10.1016/j.tcs.2020.11.041", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing very large collections of strings, such as those produced by the\nwidespread next generation sequencing technologies, heavily relies on\nmultistring generalization of the Burrows-Wheeler Transform (BWT): large\nrequirements of in-memory approaches have stimulated recent developments on\nexternal memory algorithms. The related problem of computing the Longest Common\nPrefix (LCP) array of a set of strings is instrumental to compute the\nsuffix-prefix overlaps among strings, which is an essential step for many\ngenome assembly algorithms. In a previous paper, we presented an in-memory\ndivide-and-conquer method for building the BWT and LCP where we merge partial\nBWTs with a forward approach to sort suffixes. In this paper, we propose an\nalternative backward strategy to develop an external memory method to\nsimultaneously build the BWT and the LCP array on a collection of m strings of\ndifferent lengths. The algorithm over a set of strings having constant length k\nhas O(mkl) time and I/O volume, using O(k + m) main memory, where l is the\nmaximum value in the LCP array.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 13:19:22 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 13:22:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Pirola", "Yuri", ""], ["Previtali", "Marco", ""], ["Rizzi", "Raffaella", ""]]}, {"id": "1705.07861", "submitter": "Shreyas Pai", "authors": "Shreyas Pai, Gopal Pandurangan, Sriram V. Pemmaraju, Talal Riaz, Peter\n  Robinson", "title": "Symmetry Breaking in the Congest Model: Time- and Message-Efficient\n  Algorithms for Ruling Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study local symmetry breaking problems in the CONGEST model, focusing on\nruling set problems, which generalize the fundamental Maximal Independent Set\n(MIS) problem. A $\\beta$-ruling set is an independent set such that every node\nin the graph is at most $\\beta$ hops from a node in the independent set. Our\nwork is motivated by the following central question: can we break the\n$\\Theta(\\log n)$ time complexity barrier and the $\\Theta(m)$ message complexity\nbarrier in the CONGEST model for MIS or closely-related symmetry breaking\nproblems? We present the following results:\n  - Time Complexity: We show that we can break the $O(\\log n)$ \"barrier\" for 2-\nand 3-ruling sets. We compute 3-ruling sets in $O\\left(\\frac{\\log n}{\\log \\log\nn}\\right)$ rounds with high probability (whp). More generally we show that\n2-ruling sets can be computed in $O\\left(\\log \\Delta \\cdot (\\log n)^{1/2 +\n\\varepsilon} + \\frac{\\log n}{\\log\\log n}\\right)$ rounds for any $\\varepsilon >\n0$, which is $o(\\log n)$ for a wide range of $\\Delta$ values (e.g., $\\Delta =\n2^{(\\log n)^{1/2-\\varepsilon}}$). These are the first 2- and 3-ruling set\nalgorithms to improve over the $O(\\log n)$-round complexity of Luby's algorithm\nin the CONGEST model.\n  - Message Complexity: We show an $\\Omega(n^2)$ lower bound on the message\ncomplexity of computing an MIS (i.e., 1-ruling set) which holds also for\nrandomized algorithms and present a contrast to this by showing a randomized\nalgorithm for 2-ruling sets that, whp, uses only $O(n \\log^2 n)$ messages and\nruns in $O(\\Delta \\log n)$ rounds. This is the first message-efficient\nalgorithm known for ruling sets, which has message complexity nearly linear in\n$n$ (which is optimal up to a polylogarithmic factor).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:08:21 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Pai", "Shreyas", ""], ["Pandurangan", "Gopal", ""], ["Pemmaraju", "Sriram V.", ""], ["Riaz", "Talal", ""], ["Robinson", "Peter", ""]]}, {"id": "1705.08174", "submitter": "Hendrik Fichtenberger", "authors": "Hendrik Fichtenberger, Yadu Vasudev", "title": "Distributed Testing of Conductance", "comments": "revised introduction and some fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing conductance in the setting of distributed\ncomputing and give a two-sided tester that takes $\\mathcal{O}(\\log(n) /\n(\\epsilon \\Phi^2))$ rounds to decide if a graph has conductance at least $\\Phi$\nor is $\\epsilon$-far from having conductance at least $\\Phi^2 / 1000$ in the\ndistributed CONGEST model. We also show that $\\Omega(\\log n)$ rounds are\nnecessary for testing conductance even in the LOCAL model. In the case of a\nconnected graph, we show that we can perform the test even when the number of\nvertices in the graph is not known a priori. This is the first two-sided tester\nin the distributed model we are aware of. A key observation is that one can\nperform a polynomial number of random walks from a small set of vertices if it\nis sufficient to track only some small statistics of the walks. This greatly\nreduces the congestion on the edges compared to tracking each walk\nindividually.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 10:50:06 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 13:20:42 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 11:28:26 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Fichtenberger", "Hendrik", ""], ["Vasudev", "Yadu", ""]]}, {"id": "1705.08210", "submitter": "Wayne Joubert", "authors": "Wayne Joubert, James Nance, Deborah Weighill, Daniel Jacobson", "title": "Parallel Accelerated Vector Similarity Calculations for Genomics\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.parco.2018.03.009", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surge in availability of genomic data holds promise for enabling\ndetermination of genetic causes of observed individual traits, with\napplications to problems such as discovery of the genetic roots of phenotypes,\nbe they molecular phenotypes such as gene expression or metabolite\nconcentrations, or complex phenotypes such as diseases. However, the growing\nsizes of these datasets and the quadratic, cubic or higher scaling\ncharacteristics of the relevant algorithms pose a serious computational\nchallenge necessitating use of leadership scale computing. In this paper we\ndescribe a new approach to performing vector similarity metrics calculations,\nsuitable for parallel systems equipped with graphics processing units (GPUs) or\nIntel Xeon Phi processors. Our primary focus is the Proportional Similarity\nmetric applied to Genome Wide Association Studies (GWAS) and Phenome Wide\nAssociation Studies (PheWAS). We describe the implementation of the algorithms\non accelerated processors, methods used for eliminating redundant calculations\ndue to symmetries, and techniques for efficient mapping of the calculations to\nmany-node parallel systems. Results are presented demonstrating high per-node\nperformance and parallel scalability with rates of more than five quadrillion\nelementwise comparisons achieved per second on the ORNL Titan system. In a\ncompanion paper we describe corresponding techniques applied to calculations of\nthe Custom Correlation Coefficient for comparative genomics applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:34:55 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 23:17:55 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 15:47:04 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Joubert", "Wayne", ""], ["Nance", "James", ""], ["Weighill", "Deborah", ""], ["Jacobson", "Daniel", ""]]}, {"id": "1705.08213", "submitter": "Wayne Joubert", "authors": "Wayne Joubert, James Nance, Sharlee Climer, Deborah Weighill, Daniel\n  Jacobson", "title": "Parallel Accelerated Custom Correlation Coefficient Calculations for\n  Genomics Applications", "comments": "arXiv admin note: text overlap with arXiv:1705.08210", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive quantities of genomic data being made available through gene\nsequencing techniques are enabling breakthroughs in genomic science in many\nareas such as medical advances in the diagnosis and treatment of diseases.\nAnalyzing this data, however, is a computational challenge insofar as the\ncomputational costs of the relevant algorithms can grow with quadratic, cubic\nor higher complexity-leading to the need for leadership scale computing. In\nthis paper we describe a new approach to calculations of the Custom Correlation\nCoefficient (CCC) between Single Nucleotide Polymorphisms (SNPs) across a\npopulation, suitable for parallel systems equipped with graphics processing\nunits (GPUs) or Intel Xeon Phi processors. We describe the mapping of the\nalgorithms to accelerated processors, techniques used for eliminating redundant\ncalculations due to symmetries, and strategies for efficient mapping of the\ncalculations to many-node parallel systems. Results are presented demonstrating\nhigh per-node performance and near-ideal parallel scalability with rates of\nmore than nine quadrillion elementwise comparisons achieved per second with the\nlatest optimized code on the ORNL Titan system, this being orders of magnitude\nfaster than rates achieved using other codes and platforms as reported in the\nliterature. Also it is estimated that as many as 90 quadrillion comparisons per\nsecond may be achievable on the upcoming ORNL Summit system, an additional 10X\nperformance increase. In a companion paper we describe corresponding techniques\napplied to calculations of the Proportional Similarity metric for comparative\ngenomics applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:39:05 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 23:14:28 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 20:51:41 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Joubert", "Wayne", ""], ["Nance", "James", ""], ["Climer", "Sharlee", ""], ["Weighill", "Deborah", ""], ["Jacobson", "Daniel", ""]]}, {"id": "1705.08242", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna", "title": "Randomized Composable Coresets for Matching and Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach for designing scalable algorithms for massive data sets is\nto distribute the computation across, say $k$, machines and process the data\nusing limited communication between them. A particularly appealing framework\nhere is the simultaneous communication model whereby each machine constructs a\nsmall representative summary of its own data and one obtains an\napproximate/exact solution from the union of the representative summaries. If\nthe representative summaries needed for a problem are small, then this results\nin a communication-efficient and round-optimal protocol. While many fundamental\ngraph problems admit efficient solutions in this model, two prominent problems\nare notably absent from the list of successes, namely, the maximum matching\nproblem and the minimum vertex cover problem. Indeed, it was shown recently\nthat for both these problems, even achieving a polylog$(n)$ approximation\nrequires essentially sending the entire input graph from each machine.\n  The main insight of our work is that the intractability of matching and\nvertex cover in the simultaneous communication model is inherently connected to\nan adversarial partitioning of the underlying graph across machines. We show\nthat when the underlying graph is randomly partitioned across machines, both\nthese problems admit randomized composable coresets of size $\\widetilde{O}(n)$\nthat yield an $\\widetilde{O}(1)$-approximate solution. This results in an\n$\\widetilde{O}(1)$-approximation simultaneous protocol for these problems with\n$\\widetilde{O}(nk)$ total communication when the input is randomly partitioned\nacross $k$ machines. We further prove the optimality of our results. Finally,\nby a standard application of composable coresets, our results also imply\nMapReduce algorithms with the same approximation guarantee in one or two rounds\nof communication\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 13:33:28 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1705.08277", "submitter": "Pieter Leyman", "authors": "Pieter Leyman and Patrick De Causmaecker", "title": "Optimization in large graphs: Toward a better future?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding groups of connected individuals in large graphs with tens of\nthousands or more nodes has received considerable attention in academic\nresearch. In this paper, we analyze three main issues with respect to the\nrecent influx of papers on community detection in (large) graphs, highlight the\nspecific problems with the current research avenues, and propose a first step\ntowards a better approach.\n  First, in spite of the strong interest in community detection, a strong\nconceptual and theoretical foundation of connectedness in large graphs is\nmissing. Yet, it is crucial to be able to determine the specific feats that we\naim to analyze in large networks, to avoid a purely black-or-white view.\n  Second, in literature commonly employed (meta)heuristic frameworks are\napplied for the large graph problems. Currently, it is, however, unclear\nwhether these techniques are even viable options, and what the added value of\nthe constituting parts is. Additionally, the manner in which different\nalgorithms are compared is also ambiguous.\n  Finally, no analyses of the impact of data parameters on the reported\nclusters is done. Nonetheless, it would be interesting to evaluate which\ncharacteristics lead to which type of communities and what their effect is on\ncomputational difficulty.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:02:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Leyman", "Pieter", ""], ["De Causmaecker", "Patrick", ""]]}, {"id": "1705.08282", "submitter": "Anjeneya Swami Kare Mr.", "authors": "N. R. Aravind, Subrahmanyam Kalyanasundaram, Anjeneya Swami Kare and\n  Juho Lauri", "title": "Algorithms and hardness results for happy coloring problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a vertex-colored graph, an edge is happy if its endpoints have the same\ncolor. Similarly, a vertex is happy if all its incident edges are happy.\nMotivated by the computation of homophily in social networks, we consider the\nalgorithmic aspects of the following Maximum Happy Edges (k-MHE) problem: given\na partially k-colored graph G, find an extended full k-coloring of G maximizing\nthe number of happy edges. When we want to maximize the number of happy\nvertices, the problem is known as Maximum Happy Vertices (k-MHV). We further\nstudy the complexity of the problems and their weighted variants. For instance,\nwe prove that for every k >= 3, both problems are NP-complete for bipartite\ngraphs and k-MHV remains hard for split graphs. In terms of exact algorithms,\nwe show both problems can be solved in time O*(2^n), and give an even faster\nO*(1.89^n)-time algorithm when k = 3. From a parameterized perspective, we give\na linear vertex kernel for Weighted k-MHE, where edges are weighted and the\ngoal is to obtain happy edges of at least a specified total weight. Finally, we\nprove both problems are solvable in polynomial-time when the graph has bounded\ntreewidth or bounded neighborhood diversity.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:04:40 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Aravind", "N. R.", ""], ["Kalyanasundaram", "Subrahmanyam", ""], ["Kare", "Anjeneya Swami", ""], ["Lauri", "Juho", ""]]}, {"id": "1705.08350", "submitter": "Vijaya Ramachandran", "authors": "Richard Cole and Vijaya Ramachandran", "title": "Bounding Cache Miss Costs of Multithreaded Computations Under General\n  Schedulers", "comments": "Extended abstract in Proceedings of ACM Symp. on Parallel Alg. and\n  Architectures (SPAA) 2017, pp. 339-350. This revision has a few small updates\n  including a missing citation and the replacement of some big Oh terms with\n  precise constants", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the caching overhead incurred by a class of multithreaded\nalgorithms when scheduled by an arbitrary scheduler. We obtain bounds that\nmatch or improve upon the well-known $O(Q+S \\cdot (M/B))$ caching cost for the\nrandomized work stealing (RWS) scheduler, where $S$ is the number of steals,\n$Q$ is the sequential caching cost, and $M$ and $B$ are the cache size and\nblock (or cache line) size respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:09:12 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 18:59:17 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Cole", "Richard", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1705.08362", "submitter": "Thorsten Wi{\\ss}mann", "authors": "Ulrich Dorsch, Stefan Milius, Lutz Schr\\\"oder, Thorsten Wi{\\ss}mann", "title": "Efficient Coalgebraic Partition Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic partition refinement algorithm that quotients\ncoalgebraic systems by behavioural equivalence, an important task in reactive\nverification; coalgebraic generality implies in particular that we cover not\nonly classical relational systems but also various forms of weighted systems.\nUnder assumptions on the type functor that allow representing its finite\ncoalgebras in terms of nodes and edges, our algorithm runs in time\n$\\mathcal{O}(m\\cdot \\log n)$ where $n$ and $m$ are the numbers of nodes and\nedges, respectively. Instances of our generic algorithm thus match the runtime\nof the best known algorithms for unlabelled transition systems, Markov chains,\nand deterministic automata (with fixed alphabets), and improve the best known\nalgorithms for Segala systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:31:59 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 09:53:47 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 10:49:21 GMT"}, {"version": "v4", "created": "Mon, 9 Oct 2017 10:19:12 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Dorsch", "Ulrich", ""], ["Milius", "Stefan", ""], ["Schr\u00f6der", "Lutz", ""], ["Wi\u00dfmann", "Thorsten", ""]]}, {"id": "1705.08438", "submitter": "Orr Fischer", "authors": "Orr Fischer, Shay Gershtein, Rotem Oshman", "title": "On The Multiparty Communication Complexity of Testing Triangle-Freeness", "comments": "To Appear in PODC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we initiate the study of property testing in simultaneous and\nnon-simultaneous multi-party communication complexity, focusing on testing\ntriangle-freeness in graphs. We consider the $\\textit{coordinator}$ model,\nwhere we have $k$ players receiving private inputs, and a coordinator who\nreceives no input; the coordinator can communicate with all the players, but\nthe players cannot communicate with each other. In this model, we ask: if an\ninput graph is divided between the players, with each player receiving some of\nthe edges, how many bits do the players and the coordinator need to exchange to\ndetermine if the graph is triangle-free, or $\\textit{far}$ from triangle-free?\n  For general communication protocols, we show that\n$\\tilde{O}(k(nd)^{1/4}+k^2)$ bits are sufficient to test triangle-freeness in\ngraphs of size $n$ with average degree $d$ (the degree need not be known in\nadvance). For $\\textit{simultaneous}$ protocols, where there is only one\ncommunication round, we give a protocol that uses $\\tilde{O}(k \\sqrt{n})$ bits\nwhen $d = O(\\sqrt{n})$ and $\\tilde{O}(k (nd)^{1/3})$ when $d =\n\\Omega(\\sqrt{n})$; here, again, the average degree $d$ does not need to be\nknown in advance. We show that for average degree $d = O(1)$, our simultaneous\nprotocol is asymptotically optimal up to logarithmic factors. For higher\ndegrees, we are not able to give lower bounds on testing triangle-freeness, but\nwe give evidence that the problem is hard by showing that finding an edge that\nparticipates in a triangle is hard, even when promised that at least a constant\nfraction of the edges must be removed in order to make the graph triangle-free.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:48:25 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Fischer", "Orr", ""], ["Gershtein", "Shay", ""], ["Oshman", "Rotem", ""]]}, {"id": "1705.08495", "submitter": "Renatha Capua", "authors": "Renatha Capua, Yuri Frota, Luiz Satoru Ochi, Thibaut Vidal", "title": "A study on exponential-size neighborhoods for the bin packing problem\n  with conflicts", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an iterated local search based on several classes of local and\nlarge neighborhoods for the bin packing problem with conflicts. This problem,\nwhich combines the characteristics of both bin packing and vertex coloring,\narises in various application contexts such as logistics and transportation,\ntimetabling, and resource allocation for cloud computing. We introduce $O(1)$\nevaluation procedures for classical local-search moves, polynomial variants of\nejection chains and assignment neighborhoods, an adaptive set covering-based\nneighborhood, and finally a controlled use of 0-cost moves to further diversify\nthe search. The overall method produces solutions of good quality on the\nclassical benchmark instances and scales very well with an increase of problem\nsize. Extensive computational experiments are conducted to measure the\nrespective contribution of each proposed neighborhood. In particular, the\n0-cost moves and the large neighborhood based on set covering contribute very\nsignificantly to the search. Several research perspectives are open in relation\nto possible hybridizations with other state-of-the-art mathematical programming\nheuristics for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:25:43 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Capua", "Renatha", ""], ["Frota", "Yuri", ""], ["Ochi", "Luiz Satoru", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1705.08640", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Fabio Cunial", "title": "Representing the suffix tree with the CDAWG", "comments": "16 pages, 1 figure. Presented at the 28th Annual Symposium on\n  Combinatorial Pattern Matching (CPM 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $T$, it is known that its suffix tree can be represented using\nthe compact directed acyclic word graph (CDAWG) with $e_T$ arcs, taking overall\n$O(e_T+e_{{\\overline{T}}})$ words of space, where ${\\overline{T}}$ is the\nreverse of $T$, and supporting some key operations in time between $O(1)$ and\n$O(\\log{\\log{n}})$ in the worst case. This representation is especially\nappealing for highly repetitive strings, like collections of similar genomes or\nof version-controlled documents, in which $e_T$ grows sublinearly in the length\nof $T$ in practice. In this paper we augment such representation, supporting a\nnumber of additional queries in worst-case time between $O(1)$ and $O(\\log{n})$\nin the RAM model, without increasing space complexity asymptotically. Our\ntechnique, based on a heavy path decomposition of the suffix tree, enables also\na representation of the suffix array, of the inverse suffix array, and of $T$\nitself, that takes $O(e_T)$ words of space, and that supports random access in\n$O(\\log{n})$ time. Furthermore, we establish a connection between the reversed\nCDAWG of $T$ and a context-free grammar that produces $T$ and only $T$, which\nmight have independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 07:42:36 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Cunial", "Fabio", ""]]}, {"id": "1705.08657", "submitter": "Du\\v{s}an Knop", "authors": "Du\\v{s}an Knop, Martin Kouteck\\'y, and Matthias Mnich", "title": "Combinatorial n-fold Integer Programming and Applications", "comments": "245 pages, preliminary results were presented during ESA 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2017.54", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fundamental NP-hard problems can be formulated as integer linear\nprograms (ILPs). A famous algorithm by Lenstra solves ILPs in time that is\nexponential only in the dimension of the program, and polynomial in the size of\nthe ILP. That algorithm became a ubiquitous tool in the design of\nfixed-parameter algorithms for NP-hard problems, where one wishes to isolate\nthe hardness of a problemby some parameter. However, in many cases using\nLenstra's algorithm has two drawbacks: First, the run time of the resulting\nalgorithms is often doubly-exponential in the parameter, and second, an ILP\nformulation in small dimension cannot easily express problems involving many\ndifferent costs.\n  Inspired by the work of Hemmecke, Onn and Romanchuk [Math. Prog. 2013], we\ndevelop a single-exponential algorithm for so-called combinatorial n-fold\ninteger programs, which are remarkably similar to prior ILP formulations for\nvarious problems, but unlike them, also allow variable dimension. We then apply\nour algorithm to a few representative problems like Closest String, Swap\nBribery, Weighted Set Multicover, and obtain exponential speedups in the\ndependence on the respective parameters, the input size, or both.\n  Unlike Lenstra's algorithm, which is essentially a bounded search tree\nalgorithm, our result uses the technique of augmenting steps. At its heart is a\ndeep result stating that in combinatorial n-fold IPs, existence of an\naugmenting step implies existence of a \\local\" augmenting step, which can be\nfound using dynamic programming. Our results provide an important insight into\nmany problems by showing that they exhibit this phenomenon, and highlights the\nimportance of augmentation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 08:35:57 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:23:15 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""], ["Mnich", "Matthias", ""]]}, {"id": "1705.08743", "submitter": "Michiel de Bondt", "authors": "Michiel de Bondt", "title": "Fast algorithms for anti-distance matrices as a generalization of\n  Boolean matrices", "comments": "10 pages + g++ template header files; minor corrections + friend\n  class declarations added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Boolean matrix multiplication, computed as a sum of products of\ncolumn vectors with row vectors, is essentially the same as Warshall's\nalgorithm for computing the transitive closure matrix of a graph from its\nadjacency matrix.\n  Warshall's algorithm can be generalized to Floyd's algorithm for computing\nthe distance matrix of a graph with weighted edges. We will generalize Boolean\nmatrices in the same way, keeping matrix multiplication essentially equivalent\nto the Floyd-Warshall algorithm. This way, we get matrices over a semiring,\nwhich are similar to the so-called \"funny matrices\".\n  We discuss our implementation of operations on Boolean matrices and on their\ngeneralization, which make use of vector instructions.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:18:14 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 15:38:24 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["de Bondt", "Michiel", ""]]}, {"id": "1705.08754", "submitter": "Alexandru I. Tomescu", "authors": "Anna Kuosmanen, Topi Paavilainen, Travis Gagie, Rayan Chikhi,\n  Alexandru I. Tomescu and Veli M\\\"akinen", "title": "Using Minimum Path Cover to Boost Dynamic Programming on DAGs: Co-Linear\n  Chaining Extended", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning sequencing reads on graph representations of genomes is an important\ningredient of pan-genomics. Such approaches typically find a set of local\nanchors that indicate plausible matches between substrings of a read to\nsubpaths of the graph. These anchor matches are then combined to form a\n(semi-local) alignment of the complete read on a subpath. Co-linear chaining is\nan algorithmically rigorous approach to combine the anchors. It is a well-known\napproach for the case of two sequences as inputs. Here we extend the approach\nso that one of the inputs can be a directed acyclic graph (DAGs), e.g. a\nsplicing graph in transcriptomics or a variant graph in pan-genomics.\n  This extension to DAGs turns out to have a tight connection to the minimum\npath cover problem, asking for a minimum-cardinality set of paths that cover\nall the nodes of a DAG. We study the case when the size $k$ of a minimum path\ncover is small, which is often the case in practice. First, we propose an\nalgorithm for finding a minimum path cover of a DAG $(V,E)$ in $O(k|E|\\log|V|)$\ntime, improving all known time-bounds when $k$ is small and the DAG is not too\ndense. Second, we introduce a general technique for extending dynamic\nprogramming (DP) algorithms from sequences to DAGs. This is enabled by our\nminimum path cover algorithm, and works by mimicking the DP algorithm for\nsequences on each path of the minimum path cover. This technique generally\nproduces algorithms that are slower than their counterparts on sequences only\nby a factor $k$. Our technique can be applied, for example, to the classical\nlongest increasing subsequence and longest common subsequence problems,\nextended to labeled DAGs. Finally, we apply this technique to the co-linear\nchaining problem. We also implemented the new co-linear chaining approach.\nExperiments on splicing graphs show that the new method is efficient also in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:37:44 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 08:56:18 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 11:26:40 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kuosmanen", "Anna", ""], ["Paavilainen", "Topi", ""], ["Gagie", "Travis", ""], ["Chikhi", "Rayan", ""], ["Tomescu", "Alexandru I.", ""], ["M\u00e4kinen", "Veli", ""]]}, {"id": "1705.08773", "submitter": "Jamie Fairbrother", "authors": "Jamie Fairbrother, Adam Letchford, Keith Briggs", "title": "A Two-Level Graph Partitioning Problem Arising in Mobile Wireless\n  Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-partition problem (k-PP), one is given an edge-weighted undirected\ngraph, and one must partition the node set into at most k subsets, in order to\nminimise (or maximise) the total weight of the edges that have their end-nodes\nin the same cluster. Various hierarchical variants of this problem have been\nstudied in the context of data mining. We consider a 'two-level' variant that\narises in mobile wireless communications. We show that an exact algorithm based\non intelligent preprocessing, cutting planes and symmetry-breaking is capable\nof solving small- and medium-size instances to proven optimality, and providing\nstrong lower bounds for larger instances.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:03:19 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Fairbrother", "Jamie", ""], ["Letchford", "Adam", ""], ["Briggs", "Keith", ""]]}, {"id": "1705.08885", "submitter": "Vikram Saraph", "authors": "Archita Agarwal, Zhiyu Liu, Eli Rosenthal, Vikram Saraph", "title": "Linearizable Iterators for Concurrent Sets", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general framework for adding linearizable iterators to\na class of data structures that implement set operations. We introduce a\ncondition on set operations, called local consistency, which informally states\nthat set operations never make elements unreachable to a sequential iterator's\ntraversal. We show that sets with locally consistent operations can be\naugmented with a linearizable iterator via the framework. Our technique is\nbroadly applicable to a variety of data structures, including hash tables and\nbinary search trees. We apply the technique to sets taken from existing\nliterature, prove their operations are locally consistent, and demonstrate that\niterators do not significantly affect the performance of concurrent set\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:58:13 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 16:55:35 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 15:33:22 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Agarwal", "Archita", ""], ["Liu", "Zhiyu", ""], ["Rosenthal", "Eli", ""], ["Saraph", "Vikram", ""]]}, {"id": "1705.08992", "submitter": "Leonid Peshkin", "authors": "Nicholas Harvey, Vahab Mirrokni, David Karger, Virginia Savova, Leonid\n  Peshkin", "title": "Matroids Hitting Sets and Unsupervised Dependency Grammar Induction", "comments": "11 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates a novel problem on graphs: find the minimal subset of\nedges in a fully connected graph, such that the resulting graph contains all\nspanning trees for a set of specifed sub-graphs. This formulation is motivated\nby an un-supervised grammar induction problem from computational linguistics.\nWe present a reduction to some known problems and algorithms from graph theory,\nprovide computational complexity results, and describe an approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:53:56 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 20:24:11 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Harvey", "Nicholas", ""], ["Mirrokni", "Vahab", ""], ["Karger", "David", ""], ["Savova", "Virginia", ""], ["Peshkin", "Leonid", ""]]}, {"id": "1705.09061", "submitter": "Francois Le Gall", "authors": "Taisuke Izumi and Fran\\c{c}ois Le Gall", "title": "Triangle Finding and Listing in CONGEST Networks", "comments": "To appear in PODC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle-free graphs play a central role in graph theory, and triangle\ndetection (or triangle finding) as well as triangle enumeration (triangle\nlisting) play central roles in the field of graph algorithms. In distributed\ncomputing, algorithms with sublinear round complexity for triangle finding and\nlisting have recently been developed in the powerful CONGEST clique model,\nwhere communication is allowed between any two nodes of the network. In this\npaper we present the first algorithms with sublinear complexity for triangle\nfinding and triangle listing in the standard CONGEST model, where the\ncommunication topology is the same as the topology of the network. More\nprecisely, we give randomized algorithms for triangle finding and listing with\nround complexity $O(n^{2/3}(\\log n)^{2/3})$ and $O(n^{3/4}\\log n)$,\nrespectively, where $n$ denotes the number of nodes of the network. We also\nshow a lower bound $\\Omega(n^{1/3}/\\log n)$ on the round complexity of triangle\nlisting, which also holds for the CONGEST clique model.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 06:24:25 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Izumi", "Taisuke", ""], ["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1705.09177", "submitter": "Ignasi Sau", "authors": "Sancrey R. Alves, Konrad K. Dabrowski, Luerbio Faria, Sulamita Klein,\n  Ignasi Sau, U\\'everton S. Souza", "title": "On the (parameterized) complexity of recognizing well-covered\n  (r,l)-graphs", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(r, \\ell)$-partition of a graph $G$ is a partition of its vertex set into\n$r$ independent sets and $\\ell$ cliques. A graph is $(r, \\ell)$ if it admits an\n$(r, \\ell)$-partition. A graph is well-covered if every maximal independent set\nis also maximum. A graph is $(r,\\ell)$-well-covered if it is both $(r,\\ell)$\nand well-covered. In this paper we consider two different decision problems. In\nthe $(r,\\ell)$-Well-Covered Graph problem ($(r,\\ell)$WCG for short), we are\ngiven a graph $G$, and the question is whether $G$ is an\n$(r,\\ell)$-well-covered graph. In the Well-Covered $(r,\\ell)$-Graph problem\n(WC$(r,\\ell)$G for short), we are given an $(r,\\ell)$-graph $G$ together with\nan $(r,\\ell)$-partition of $V(G)$ into $r$ independent sets and $\\ell$ cliques,\nand the question is whether $G$ is well-covered. We classify most of these\nproblems into P, coNP-complete, NP-complete, NP-hard, or coNP-hard. Only the\ncases WC$(r,0)$G for $r\\geq 3$ remain open. In addition, we consider the\nparameterized complexity of these problems for several choices of parameters,\nsuch as the size $\\alpha$ of a maximum independent set of the input graph, its\nneighborhood diversity, its clique-width, or the number $\\ell$ of cliques in an\n$(r, \\ell)$-partition. In particular, we show that the parameterized problem of\ndeciding whether a general graph is well-covered parameterized by $\\alpha$ can\nbe reduced to the WC$(0,\\ell)$G problem parameterized by $\\ell$. In addition,\nwe prove that both problems are coW[2]-hard but can be solved in XP-time.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 13:47:33 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 16:22:13 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Alves", "Sancrey R.", ""], ["Dabrowski", "Konrad K.", ""], ["Faria", "Luerbio", ""], ["Klein", "Sulamita", ""], ["Sau", "Ignasi", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "1705.09271", "submitter": "Maxwell Young", "authors": "William C. Anderton and Maxwell Young", "title": "Is Our Model for Contention Resolution Wrong?", "comments": "Accepted to the 29th ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized binary exponential backoff (BEB) is a popular algorithm for\ncoordinating access to a shared channel. With an operational history exceeding\nfour decades, BEB is currently an important component of several wireless\nstandards. Despite this track record, prior theoretical results indicate that\nunder bursty traffic (1) BEB yields poor makespan and (2) superior algorithms\nare possible. To date, the degree to which these findings manifest in practice\nhas not been resolved.\n  To address this issue, we examine one of the strongest cases against BEB: $n$\npackets that simultaneously begin contending for the wireless channel. Using\nNetwork Simulator 3, we compare against more recent algorithms that are\ninspired by BEB, but whose makespan guarantees are superior. Surprisingly, we\ndiscover that these newer algorithms significantly underperform. Through\nfurther investigation, we identify as the culprit a flawed but common\nabstraction regarding the cost of collisions. Our experimental results are\ncomplemented by analytical arguments that the number of collisions -- and not\nsolely makespan -- is an important metric to optimize. We believe that these\nfindings have implications for the design of contention-resolution algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:37:53 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 16:37:32 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Anderton", "William C.", ""], ["Young", "Maxwell", ""]]}, {"id": "1705.09335", "submitter": "Morteza Zadimoghaddam", "authors": "Maxime C. Cohen, Philipp W. Keller, Vahab Mirrokni and Morteza\n  Zadimoghaddam", "title": "Overcommitment in Cloud Services -- Bin packing with Chance Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a traditional problem of resource allocation, scheduling\njobs on machines. One such recent application is cloud computing, where jobs\narrive in an online fashion with capacity requirements and need to be\nimmediately scheduled on physical machines in data centers. It is often\nobserved that the requested capacities are not fully utilized, hence offering\nan opportunity to employ an overcommitment policy, i.e., selling resources\nbeyond capacity. Setting the right overcommitment level can induce a\nsignificant cost reduction for the cloud provider, while only inducing a very\nlow risk of violating capacity constraints. We introduce and study a model that\nquantifies the value of overcommitment by modeling the problem as a bin packing\nwith chance constraints. We then propose an alternative formulation that\ntransforms each chance constraint into a submodular function. We show that our\nmodel captures the risk pooling effect and can guide scheduling and\novercommitment decisions. We also develop a family of online algorithms that\nare intuitive, easy to implement and provide a constant factor guarantee from\noptimal. Finally, we calibrate our model using realistic workload data, and\ntest our approach in a practical setting. Our analysis and experiments\nillustrate the benefit of overcommitment in cloud services, and suggest a cost\nreduction of 1.5% to 17% depending on the provider's risk tolerance.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 19:21:09 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Cohen", "Maxime C.", ""], ["Keller", "Philipp W.", ""], ["Mirrokni", "Vahab", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1705.09358", "submitter": "Darren Strash", "authors": "Raphael Kimmig and Henning Meyerhenke and Darren Strash", "title": "Shared Memory Parallel Subgraph Enumeration", "comments": "18 pages, 12 figures, To appear at the 7th IEEE Workshop on Parallel\n  / Distributed Computing and Optimization (PDCO 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subgraph enumeration problem asks us to find all subgraphs of a target\ngraph that are isomorphic to a given pattern graph. Determining whether even\none such isomorphic subgraph exists is NP-complete---and therefore finding all\nsuch subgraphs (if they exist) is a time-consuming task. Subgraph enumeration\nhas applications in many fields, including biochemistry and social networks,\nand interestingly the fastest algorithms for solving the problem for\nbiochemical inputs are sequential. Since they depend on depth-first tree\ntraversal, an efficient parallelization is far from trivial. Nevertheless,\nsince important applications produce data sets with increasing difficulty,\nparallelism seems beneficial.\n  We thus present here a shared-memory parallelization of the state-of-the-art\nsubgraph enumeration algorithms RI and RI-DS (a variant of RI for dense graphs)\nby Bonnici et al. [BMC Bioinformatics, 2013]. Our strategy uses work stealing\nand our implementation demonstrates a significant speedup on real-world\nbiochemical data---despite a highly irregular data access pattern. We also\nimprove RI-DS by pruning the search space better; this further improves the\nempirical running times compared to the already highly tuned RI-DS.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:52:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Kimmig", "Raphael", ""], ["Meyerhenke", "Henning", ""], ["Strash", "Darren", ""]]}, {"id": "1705.09438", "submitter": "Diptarama", "authors": "Davaajav Jargalsaikhan and Diptarama and Ryo Yoshinaka and Ayumi\n  Shinohara", "title": "Duel and sweep algorithm for order-preserving pattern matching", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a text $T$ and a pattern $P$ over alphabet $\\Sigma$, the classic exact\nmatching problem searches for all occurrences of pattern $P$ in text $T$.\nUnlike exact matching problem, order-preserving pattern matching (OPPM)\nconsiders the relative order of elements, rather than their real values. In\nthis paper, we propose an efficient algorithm for OPPM problem using the\n\"duel-and-sweep\" paradigm. Our algorithm runs in $O(n + m\\log m)$ time in\ngeneral and $O(n + m)$ time under an assumption that the characters in a string\ncan be sorted in linear time with respect to the string size. We also perform\nexperiments and show that our algorithm is faster that KMP-based algorithm.\nLast, we introduce the two-dimensional order preserved pattern matching and\ngive a duel and sweep algorithm that runs in $O(n^2)$ time for duel stage and\n$O(n^2 m)$ time for sweeping time with $O(m^3)$ preprocessing time.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:40:54 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Jargalsaikhan", "Davaajav", ""], ["Diptarama", "", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1705.09504", "submitter": "Yuki Igarashi", "authors": "Yuki Igarashi and Diptarama and Ryo Yoshinaka and Ayumi Shinohara", "title": "New Variants of Pattern Matching with Constants and Variables", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a text and a pattern over two types of symbols called constants and\nvariables, the parameterized pattern matching problem is to find all\noccurrences of substrings of the text that the pattern matches by substituting\na variable in the text for each variable in the pattern, where the substitution\nshould be injective. The function matching problem is a variant of it that\nlifts the injection constraint. In this paper, we discuss variants of those\nproblems, where one can substitute a constant or a variable for each variable\nof the pattern. We give two kinds of algorithms for both problems, a\nconvolution-based method and an extended KMP-based method, and analyze their\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 09:55:38 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Igarashi", "Yuki", ""], ["Diptarama", "", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1705.09538", "submitter": "Dmitry Kosolobov", "authors": "Golnaz Badkobeh, Travis Gagie, Shunsuke Inenaga, Tomasz Kociumaka,\n  Dmitry Kosolobov, Simon J. Puglisi", "title": "On Two LZ78-style Grammars: Compression Bounds and Compressed-Space\n  Computation", "comments": "12 pages, accepted to SPIRE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two closely related LZ78-based compression schemes: LZMW (an\nold scheme by Miller and Wegman) and LZD (a recent variant by Goto et al.).\nBoth LZD and LZMW naturally produce a grammar for a string of length $n$; we\nshow that the size of this grammar can be larger than the size of the smallest\ngrammar by a factor $\\Omega(n^{\\frac{1}3})$ but is always within a factor\n$O((\\frac{n}{\\log n})^{\\frac{2}{3}})$. In addition, we show that the standard\nalgorithms using $\\Theta(z)$ working space to construct the LZD and LZMW\nparsings, where $z$ is the size of the parsing, work in $\\Omega(n^{\\frac{5}4})$\ntime in the worst case. We then describe a new Las Vegas LZD/LZMW parsing\nalgorithm that uses $O (z \\log n)$ space and $O(n + z \\log^2 n)$ time w.h.p..\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 11:33:05 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 11:15:37 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Badkobeh", "Golnaz", ""], ["Gagie", "Travis", ""], ["Inenaga", "Shunsuke", ""], ["Kociumaka", "Tomasz", ""], ["Kosolobov", "Dmitry", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1705.09600", "submitter": "Shana Moothedath", "authors": "Shana Moothedath, Prasanna Chaporkar and Madhu N. Belur", "title": "Approximating Constrained Minimum Cost Input-Output Selection for\n  Generic Arbitrary Pole Placement in Structured Systems", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about minimum cost constrained selection of inputs and outputs\nfor generic arbitrary pole placement. The input-output set is constrained in\nthe sense that the set of states that each input can influence and the set of\nstates that each output can sense is pre-specified. Our goal is to optimally\nselect an input-output set that the system has no structurally fixed modes.\nPolynomial algorithms do not exist for solving this problem unless P=NP. To\nthis end, we propose an approximation algorithm by splitting the problem in to\nthree sub-problems: a) minimum cost accessibility problem, b) minimum cost\nsensability problem and c) minimum cost disjoint cycle problem. We prove that\nproblems a) and b) are equivalent to a suitably defined weighted set cover\nproblems. We also show that problem c) is equivalent to a minimum cost perfect\nmatching problem. Using these we give an approximation algorithm which solves\nthe minimum cost generic arbitrary pole placement problem. The proposed\nalgorithm incorporates an approximation algorithm to solve the weighted set\ncover problem for solving a) and b) and a minimum cost perfect matching\nalgorithm to solve c). Further, we show that the algorithm is polynomial time\nan gives an order optimal solution to the minimum cost input-output selection\nfor generic arbitrary pole placement problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:41:28 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 04:56:39 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Moothedath", "Shana", ""], ["Chaporkar", "Prasanna", ""], ["Belur", "Madhu N.", ""]]}, {"id": "1705.09609", "submitter": "Calvin Newport", "authors": "Calvin Newport", "title": "Gossip in a Smartphone Peer-to-Peer Network", "comments": "Extended Abstract to Appear in the Proceedings of the ACM Conference\n  on the Principles of Distributed Computing (PODC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the fundamental problem of gossip in the mobile\ntelephone model: a recently introduced variation of the classical telephone\nmodel modified to better describe the local peer-to-peer communication services\nimplemented in many popular smartphone operating systems. In more detail, the\nmobile telephone model differs from the classical telephone model in three\nways: (1) each device can participate in at most one connection per round; (2)\nthe network topology can undergo a parameterized rate of change; and (3)\ndevices can advertise a parameterized number of bits about their state to their\nneighbors in each round before connection attempts are initiated. We begin by\ndescribing and analyzing new randomized gossip algorithms in this model under\nthe harsh assumption of a network topology that can change completely in every\nround. We prove a significant time complexity gap between the case where nodes\ncan advertise $0$ bits to their neighbors in each round, and the case where\nnodes can advertise $1$ bit. For the latter assumption, we present two\nsolutions: the first depends on a shared randomness source, while the second\neliminates this assumption using a pseudorandomness generator we prove to exist\nwith a novel generalization of a classical result from the study of two-party\ncommunication complexity. We then turn our attention to the easier case where\nthe topology graph is stable, and describe and analyze a new gossip algorithm\nthat provides a substantial performance improvement for many parameters. We\nconclude by studying a relaxed version of gossip in which it is only necessary\nfor nodes to each learn a specified fraction of the messages in the system.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 15:10:39 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Newport", "Calvin", ""]]}, {"id": "1705.09617", "submitter": "Sebastian Siebertz", "authors": "Saeed Akhoondian Amiri, Stefan Schmid, Sebastian Siebertz", "title": "Distributed Dominating Set Approximations beyond Planar Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.02991", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Dominating Set (MDS) problem is one of the most fundamental and\nchallenging problems in distributed computing. While it is well-known that\nminimum dominating sets cannot be approximated locally on general graphs, over\nthe last years, there has been much progress on computing local approximations\non sparse graphs, and in particular planar graphs.\n  In this paper we study distributed and deterministic MDS approximation\nalgorithms for graph classes beyond planar graphs. In particular, we show that\nexisting approximation bounds for planar graphs can be lifted to bounded genus\ngraphs, and present (1) a local constant-time, constant-factor MDS\napproximation algorithm and (2) a local $\\mathcal{O}(\\log^*{n})$-time\napproximation scheme. Our main technical contribution is a new analysis of a\nslightly modified variant of an existing algorithm by Lenzen et al.\nInterestingly, unlike existing proofs for planar graphs, our analysis does not\nrely on direct topological arguments.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 15:52:44 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 20:46:39 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Schmid", "Stefan", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1705.09634", "submitter": "Jonathan Weed", "authors": "Jason Altschuler, Jonathan Weed, Philippe Rigollet", "title": "Near-linear time approximation algorithms for optimal transport via\n  Sinkhorn iteration", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017),\n  1961-1971", "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing optimal transport distances such as the earth mover's distance is a\nfundamental problem in machine learning, statistics, and computer vision.\nDespite the recent introduction of several algorithms with good empirical\nperformance, it is unknown whether general optimal transport distances can be\napproximated in near-linear time. This paper demonstrates that this ambitious\ngoal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on\na new analysis of Sinkhorn iteration, which also directly suggests a new greedy\ncoordinate descent algorithm, Greenkhorn, with the same theoretical guarantees.\nNumerical simulations illustrate that Greenkhorn significantly outperforms the\nclassical Sinkhorn algorithm in practice.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:14:38 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 18:55:19 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Altschuler", "Jason", ""], ["Weed", "Jonathan", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1705.09642", "submitter": "Jordi Ros-Giralt", "authors": "Jordi Ros-Giralt, Alan Commike, Peter Cullen, Jeff Lucovsky, Dilip\n  Madathil, Richard Lethin", "title": "Multiresolution Priority Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority queues are container data structures essential to many high\nperformance computing (HPC) applications. In this paper, we introduce\nmultiresolution priority queues, a data structure that improves the performance\nof the standard heap based implementations by trading off a controllable amount\nof resolution in the space of priorities. The new data structure can reduce the\nworst case performance of inserting an element from O(log(n)) to O(log(r)),\nwhere n is the number of elements in the queue and r is the number of\nresolution groups in the priority space. The worst case cost of removing the\ntop element is O(1). When the number of elements in the table is high, the\namortized cost to insert an element becomes O(1).\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:31:59 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 04:17:01 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 17:27:33 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 09:57:08 GMT"}, {"version": "v5", "created": "Mon, 7 Aug 2017 18:41:29 GMT"}, {"version": "v6", "created": "Thu, 10 Aug 2017 17:49:07 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Ros-Giralt", "Jordi", ""], ["Commike", "Alan", ""], ["Cullen", "Peter", ""], ["Lucovsky", "Jeff", ""], ["Madathil", "Dilip", ""], ["Lethin", "Richard", ""]]}, {"id": "1705.09643", "submitter": "Asish Mukhopadhyay", "authors": "Yash P. Aneja, Asish Mukhopadhyay, Md. Zamilur Rahman", "title": "A greedy approximation algorithm for the minimum (2,2)-connected\n  dominating set problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a connected dominating set (CDS) to serve as the virtual backbone of a\nwireless sensor network (WSN) is an effective way to save energy and reduce the\nimpact of broadcasting storms. Since nodes may fail due to accidental damage or\nenergy depletion, it is desirable that the virtual backbone is fault tolerant.\nThis could be modeled as a k-connected, m-fold dominating set ((k,m)-CDS).\nGiven a virtual undirected network G=(V,E), a subset C\\subset V is a (k,m)-CDS\nof G if (i) G[C], the subgraph of G induced by C is k-connected, and (ii) each\nnode in V\\C has at least m neighbors in C. We present a two-phase greedy\nalgorithm for computing a (2,2)-CDS that achieves an asymptotic approximation\nfactor of $(3+\\ln(\\Delta+2))$, where $\\Delta$ is the maximum degree of G. This\nresult improves on the previous best known performance factor of\n$(4+\\ln\\Delta+2\\ln(2+\\ln\\Delta))$ for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:33:45 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Aneja", "Yash P.", ""], ["Mukhopadhyay", "Asish", ""], ["Rahman", "Md. Zamilur", ""]]}, {"id": "1705.09700", "submitter": "Rad Niazadeh", "authors": "S\\'ebastien Bubeck, Nikhil R. Devanur, Zhiyi Huang, Rad Niazadeh", "title": "Multi-scale Online Learning and its Applications to Online Auctions", "comments": "Preliminary conference version In the Proc. of 18th ACM conference on\n  Economics and Computation (EC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider revenue maximization in online auction/pricing problems. A seller\nsells an identical item in each period to a new buyer, or a new set of buyers.\nFor the online posted pricing problem, we show regret bounds that scale with\nthe best fixed price, rather than the range of the values. We also show regret\nbounds that are almost scale free, and match the offline sample complexity,\nwhen comparing to a benchmark that requires a lower bound on the market share.\nThese results are obtained by generalizing the classical learning from experts\nand multi-armed bandit problems to their multi-scale versions. In this version,\nthe reward of each action is in a different range, and the regret w.r.t. a\ngiven action scales with its own range, rather than the maximum range.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:59:50 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 18:59:34 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Devanur", "Nikhil R.", ""], ["Huang", "Zhiyi", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1705.09779", "submitter": "Takuya Takagi", "authors": "Takuya Takagi, Keisuke Goto, Yuta Fujishige, Shunsuke Inenaga, and\n  Hiroki Arimura", "title": "Linear-size CDAWG: new repetition-aware indexing and grammar compression", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to combine \\emph{compact directed\nacyclic word graphs} (CDAWGs) and grammar-based compression. This leads us to\nan efficient self-index, called Linear-size CDAWGs (L-CDAWGs), which can be\nrepresented with $O(\\tilde e_T \\log n)$ bits of space allowing for $O(\\log\nn)$-time random and $O(1)$-time sequential accesses to edge labels, and $O(m\n\\log \\sigma + occ)$-time pattern matching. Here, $\\tilde e_T$ is the number of\nall extensions of maximal repeats in $T$, $n$ and $m$ are respectively the\nlengths of the text $T$ and a given pattern, $\\sigma$ is the alphabet size, and\n$occ$ is the number of occurrences of the pattern in $T$. The repetitiveness\nmeasure $\\tilde e_T$ is known to be much smaller than the text length $n$ for\nhighly repetitive text. For constant alphabets, our L-CDAWGs achieve $O(m +\nocc)$ pattern matching time with $O(e_T^r \\log n)$ bits of space, which\nimproves the pattern matching time of Belazzougui et al.'s run-length\nBWT-CDAWGs by a factor of $\\log \\log n$, with the same space complexity. Here,\n$e_T^r$ is the number of right extensions of maximal repeats in $T$. As a\nbyproduct, our result gives a way of constructing an SLP of size $O(\\tilde\ne_T)$ for a given text $T$ in $O(n + \\tilde e_T \\log \\sigma)$ time.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:24:44 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 07:22:13 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Takagi", "Takuya", ""], ["Goto", "Keisuke", ""], ["Fujishige", "Yuta", ""], ["Inenaga", "Shunsuke", ""], ["Arimura", "Hiroki", ""]]}, {"id": "1705.09789", "submitter": "Rivera Mariano", "authors": "Mariano Rivera (CIMAT)", "title": "Half-quadratic transportation problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a primal--dual memory efficient algorithm for solving a relaxed\nversion of the general transportation problem. Our approach approximates the\noriginal cost function with a differentiable one that is solved as a sequence\nof weighted quadratic transportation problems. The new formulation allows us to\nsolve differentiable, non-- convex transportation problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 09:02:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Rivera", "Mariano", "", "CIMAT"]]}, {"id": "1705.09798", "submitter": "Adrian Kosowski", "authors": "Bartlomiej Dudek, Adrian Kosowski (GANG)", "title": "Universal Protocols for Information Dissemination Using Emergent Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a population of $n$ agents which communicate with each other in a\ndecentralized manner, through random pairwise interactions. One or more agents\nin the population may act as authoritative sources of information, and the\nobjective of the remaining agents is to obtain information from or about these\nsource agents. We study two basic tasks: broadcasting, in which the agents are\nto learn the bit-state of an authoritative source which is present in the\npopulation, and source detection, in which the agents are required to decide if\nat least one source agent is present in the population or not.We focus on\ndesigning protocols which meet two natural conditions: (1) universality, i.e.,\nindependence of population size, and (2) rapid convergence to a correct global\nstate after a reconfiguration, such as a change in the state of a source agent.\nOur main positive result is to show that both of these constraints can be met.\nFor both the broadcasting problem and the source detection problem, we obtain\nsolutions with a convergence time of $O(\\log^2 n)$ rounds, w.h.p., from any\nstarting configuration. The solution to broadcasting is exact, which means that\nall agents reach the state broadcast by the source, while the solution to\nsource detection admits one-sided error on a $\\varepsilon$-fraction of the\npopulation (which is unavoidable for this problem). Both protocols are easy to\nimplement in practice and have a compact formulation.Our protocols exploit the\nproperties of self-organizing oscillatory dynamics. On the hardness side, our\nmain structural insight is to prove that any protocol which meets the\nconstraints of universality and of rapid convergence after reconfiguration must\ndisplay a form of non-stationary behavior (of which oscillatory dynamics are an\nexample). We also observe that the periodicity of the oscillatory behavior of\nthe protocol, when present, must necessarily depend on the number $^\\\\# X$ of\nsource agents present in the population. For instance, our protocols inherently\nrely on the emergence of a signal passing through the population, whose period\nis $\\Theta(\\log \\frac{n}{^\\\\# X})$ rounds for most starting configurations. The\ndesign of clocks with tunable frequency may be of independent interest, notably\nin modeling biological networks.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 09:46:18 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 15:22:48 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 15:04:55 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Dudek", "Bartlomiej", "", "GANG"], ["Kosowski", "Adrian", "", "GANG"]]}, {"id": "1705.10097", "submitter": "Aaron Bernstein", "authors": "Aaron Bernstein", "title": "Deterministic Partially Dynamic Single Source Shortest Paths in Weighted\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the decremental single-source shortest paths (SSSP)\nproblem, where given a graph $G$ and a source node $s$ the goal is to maintain\nshortest distances between $s$ and all other nodes in $G$ under a sequence of\nonline adversarial edge deletions. In their seminal work, Even and Shiloach\n[JACM 1981] presented an exact solution to the problem in unweighted graphs\nwith only $O(mn)$ total update time over all edge deletions. Their classic\nalgorithm was the state of the art for the decremental SSSP problem for three\ndecades, even when approximate shortest paths are allowed.\n  A series of results showed how to improve upon $O(mn)$ if approximation is\nallowed, culminating in a recent breakthrough of Henzinger, Krinninger and\nNanongkai [FOCS 14], who presented a $(1+\\epsilon)$-approximate algorithm for\nundirected weighted graphs whose total update time is near linear:\n$O(m^{1+o(1)}\\log(W))$, where $W$ is the ratio of the heaviest to the lightest\nedge weight in the graph. In this paper they posed as a major open problem the\nquestion of derandomizing their result.\n  Until very recently, all known improvements over the Even-Shiloach algorithm\nwere randomized and required the assumption of a non-adaptive adversary. In\nSTOC 2016, Bernstein and Chechik showed the first \\emph{deterministic}\nalgorithm to go beyond $O(mn)$ total update time: the algorithm is also\n$(1+\\epsilon)$-approximate, and has total update time $\\tilde{O}(n^2)$. In SODA\n2017, the same authors presented an algorithm with total update time\n$\\tilde{O}(mn^{3/4})$. However, both algorithms are restricted to undirected,\nunweighted graphs. We present the \\emph{first} deterministic algorithm for\n\\emph{weighted} undirected graphs to go beyond the $O(mn)$ bound. The total\nupdate time is $\\tilde{O}(n^2 \\log(W))$.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 10:05:29 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Bernstein", "Aaron", ""]]}, {"id": "1705.10195", "submitter": "Janne H. Korhonen", "authors": "Janne H. Korhonen and Joel Rybicki", "title": "Deterministic subgraph detection in broadcast CONGEST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple deterministic algorithms for subgraph finding and\nenumeration in the broadcast CONGEST model of distributed computation:\n  -- For any constant $k$, detecting $k$-paths and trees on $k$ nodes can be\ndone in $O(1)$ rounds.\n  -- For any constant $k$, detecting $k$-cycles and pseudotrees on $k$ nodes\ncan be done in $O(n)$ rounds.\n  -- On $d$-degenerate graphs, cliques and $4$-cycles can be enumerated in $O(d\n+ \\log n)$ rounds, and $5$-cycles in $O(d^2 + \\log n)$ rounds.\n  In many cases, these bounds are tight up to logarithmic factors. Moreover, we\nshow that the algorithms for $d$-degenerate graphs can be improved to optimal\ncomplexity $O(d/\\log n)$ and $O(d^2/\\log n)$, respectively, in the supported\nCONGEST model, which can be seen as an intermediate model between CONGEST and\nthe congested clique.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:05:45 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 12:26:15 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Korhonen", "Janne H.", ""], ["Rybicki", "Joel", ""]]}, {"id": "1705.10277", "submitter": "Clelia De Felice", "authors": "Paola Bonizzoni and Clelia De Felice and Rocco Zaccagnino and Rosalba\n  Zizza", "title": "Inverse Lyndon words and Inverse Lyndon factorizations of words", "comments": null, "journal-ref": "Advances in Applied Mathematics, Vol. 101, pp. 281-319, 2018", "doi": "10.1016/j.aam.2018.08.005", "report-no": null, "categories": "cs.FL cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to string processing, we introduce variants of the\nLyndon factorization called inverse Lyndon factorizations. Their factors, named\ninverse Lyndon words, are in a class that strictly contains anti-Lyndon words,\nthat is Lyndon words with respect to the inverse lexicographic order. The\nLyndon factorization of a nonempty word w is unique but w may have several\ninverse Lyndon factorizations. We prove that any nonempty word w admits a\ncanonical inverse Lyndon factorization, named ICFL(w), that maintains the main\nproperties of the Lyndon factorization of w: it can be computed in linear time,\nit is uniquely determined, it preserves a compatibility property for sorting\nsuffixes. In particular, the compatibility property of ICFL(w) is a consequence\nof another result: any factor in ICFL(w) is a concatenation of consecutive\nfactors of the Lyndon factorization of w with respect to the inverse\nlexicographic order.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 16:24:11 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 20:32:56 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 20:23:31 GMT"}, {"version": "v4", "created": "Sun, 24 Sep 2017 00:21:51 GMT"}, {"version": "v5", "created": "Sun, 19 Nov 2017 20:29:04 GMT"}, {"version": "v6", "created": "Sun, 26 Nov 2017 21:05:49 GMT"}, {"version": "v7", "created": "Sun, 17 Dec 2017 22:53:02 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Bonizzoni", "Paola", ""], ["De Felice", "Clelia", ""], ["Zaccagnino", "Rocco", ""], ["Zizza", "Rosalba", ""]]}, {"id": "1705.10351", "submitter": "Eric Tellez Dr.", "authors": "Eric S. Tellez, Guillermo Ruiz, Edgar Chavez, Mario Graff", "title": "A scalable solution to the nearest neighbor search problem through\n  local-search methods on neighbor graphs", "comments": null, "journal-ref": "Pattern Analysis and Applications 24 763--777 2021", "doi": "10.1007/s10044-020-00946-w", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near neighbor search (NNS) is a powerful abstraction for data access;\nhowever, data indexing is troublesome even for approximate indexes. For\nintrinsically high-dimensional data, high-quality fast searches demand either\nindexes with impractically large memory usage or preprocessing time.\n  In this paper, we introduce an algorithm to solve a nearest-neighbor query\n$q$ by minimizing a kernel function defined by the distance from $q$ to each\nobject in the database. The minimization is performed using metaheuristics to\nsolve the problem rapidly; even when some methods in the literature use this\nstrategy behind the scenes, our approach is the first one using it explicitly.\nWe also provide two approaches to select edges in the graph's construction\nstage that limit memory footprint and reduce the number of free parameters\nsimultaneously.\n  We carry out a thorough experimental comparison with state-of-the-art indexes\nthrough synthetic and real-world datasets; we found out that our contributions\nachieve competitive performances regarding speed, accuracy, and memory in\nalmost any of our benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:32:00 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 11:49:06 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 00:26:50 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 14:46:02 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Tellez", "Eric S.", ""], ["Ruiz", "Guillermo", ""], ["Chavez", "Edgar", ""], ["Graff", "Mario", ""]]}, {"id": "1705.10382", "submitter": "Gonzalo Navarro", "authors": "Travis Gagie, Gonzalo Navarro and Nicola Prezza", "title": "Optimal-Time Text Indexing in BWT-runs Bounded Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing highly repetitive texts --- such as genomic databases, software\nrepositories and versioned text collections --- has become an important problem\nsince the turn of the millennium. A relevant compressibility measure for\nrepetitive texts is $r$, the number of runs in their Burrows-Wheeler Transform\n(BWT). One of the earliest indexes for repetitive collections, the Run-Length\nFM-index, used $O(r)$ space and was able to efficiently count the number of\noccurrences of a pattern of length $m$ in the text (in loglogarithmic time per\npattern symbol, with current techniques). However, it was unable to locate the\npositions of those occurrences efficiently within a space bounded in terms of\n$r$. Since then, a number of other indexes with space bounded by other measures\nof repetitiveness --- the number of phrases in the Lempel-Ziv parse, the size\nof the smallest grammar generating the text, the size of the smallest automaton\nrecognizing the text factors --- have been proposed for efficiently locating,\nbut not directly counting, the occurrences of a pattern. In this paper we close\nthis long-standing problem, showing how to extend the Run-Length FM-index so\nthat it can locate the $occ$ occurrences efficiently within $O(r)$ space (in\nloglogarithmic time each), and reaching optimal time $O(m+occ)$ within\n$O(r\\log(n/r))$ space, on a RAM machine of $w=\\Omega(\\log n)$ bits. Within\n$O(r\\log (n/r))$ space, our index can also count in optimal time $O(m)$.\nRaising the space to $O(r w\\log_\\sigma(n/r))$, we support count and locate in\n$O(m\\log(\\sigma)/w)$ and $O(m\\log(\\sigma)/w+occ)$ time, which is optimal in the\npacked setting and had not been obtained before in compressed space. We also\ndescribe a structure using $O(r\\log(n/r))$ space that replaces the text and\nextracts any text substring of length $\\ell$ in almost-optimal time\n$O(\\log(n/r)+\\ell\\log(\\sigma)/w)$. (...continues...)\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 20:24:07 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 23:09:13 GMT"}, {"version": "v3", "created": "Sun, 9 Jul 2017 18:43:38 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 18:21:03 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""], ["Prezza", "Nicola", ""]]}, {"id": "1705.10387", "submitter": "Maxwell Young", "authors": "Mercy O. Jaiyeola, Kyle Patron, Jared Saia, Maxwell Young, Qian M.\n  Zhou", "title": "Tiny Groups Tackle Byzantine Adversaries", "comments": "This work is supported by the National Science Foundation grant CCF\n  1613772 and a C Spire Research Gift", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular technique for tolerating malicious faults in open distributed\nsystems is to establish small groups of participants, each of which has a\nnon-faulty majority. These groups are used as building blocks to design\nattack-resistant algorithms.\n  Despite over a decade of active research, current constructions require group\nsizes of $O(\\log n)$, where $n$ is the number of participants in the system.\nThis group size is important since communication and state costs scale\npolynomially with this parameter. Given the stubbornness of this logarithmic\nbarrier, a natural question is whether better bounds are possible.\n  Here, we consider an attacker that controls a constant fraction of the total\ncomputational resources in the system. By leveraging proof-of-work (PoW), we\ndemonstrate how to reduce the group size exponentially to $O(\\log\\log n)$ while\nmaintaining strong security guarantees. This reduction in group size yields a\nsignificant improvement in communication and state costs.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 20:31:54 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 19:20:46 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 06:06:37 GMT"}, {"version": "v4", "created": "Fri, 3 Nov 2017 02:52:12 GMT"}, {"version": "v5", "created": "Tue, 9 Jan 2018 04:01:54 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Jaiyeola", "Mercy O.", ""], ["Patron", "Kyle", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""], ["Zhou", "Qian M.", ""]]}, {"id": "1705.10396", "submitter": "Zachary Friggstad", "authors": "Sara Ahmadian and Zachary Friggstad", "title": "Further Approximations for Demand Matching: Matroid Constraints and\n  Minor-Closed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pursue a study of the Generalized Demand Matching problem, a common\ngeneralization of the $b$-Matching and Knapsack problems. Here, we are given a\ngraph with vertex capacities, edge profits, and asymmetric demands on the\nedges. The goal is to find a maximum-profit subset of edges so the demands of\nchosen edges do not violate vertex capacities. This problem is APX-hard and\nconstant-factor approximations are known.\n  Our results fall into two categories. First, using iterated relaxation and\nvarious filtering strategies, we show with an efficient rounding algorithm if\nan additional matroid structure $\\mathcal M$ is given and we further only allow\nsets $F \\subseteq E$ that are independent in $\\mathcal M$, the natural LP\nrelaxation has an integrality gap of at most $\\frac{25}{3} \\approx 8.333$. This\ncan be improved in various special cases, for example we improve over the\n15-approximation for the previously-studied Coupled Placement problem [Korupolu\net al. 2014] by giving a $7$-approximation.\n  Using similar techniques, we show the problem of computing a minimum-cost\nbase in $\\mathcal M$ satisfying vertex capacities admits a $(1,3)$-bicriteria\napproximation. This improves over the previous $(1,4)$-approximation in the\nspecial case that $\\mathcal M$ is the graphic matroid over the given graph\n[Fukanaga and Nagamochi, 2009].\n  Second, we show Demand Matching admits a polynomial-time approximation scheme\nin graphs that exclude a fixed minor. If all demands are polynomially-bounded\nintegers, this is somewhat easy using dynamic programming in bounded-treewidth\ngraphs. Our main technical contribution is a sparsification lemma allowing us\nto scale the demands to be used in a more intricate dynamic programming\nalgorithm, followed by randomized rounding to filter our scaled-demand solution\nto a feasible solution.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 20:55:29 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ahmadian", "Sara", ""], ["Friggstad", "Zachary", ""]]}, {"id": "1705.10449", "submitter": "Yaohang Li", "authors": "Hao Ji, Michael Mascagni, Yaohang Li", "title": "Gaussian Variant of Freivalds' Algorithm for Efficient and Reliable\n  Matrix Product Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the general problem of checking the correctness\nof matrix multiplication. Given three $n \\times n$ matrices $A$, $B$, and $C$,\nthe goal is to verify that $A \\times B=C$ without carrying out the\ncomputationally costly operations of matrix multiplication and comparing the\nproduct $A \\times B$ with $C$, term by term. This is especially important when\nsome or all of these matrices are very large, and when the computing\nenvironment is prone to soft errors. Here we extend Freivalds' algorithm to a\nGaussian Variant of Freivalds' Algorithm (GVFA) by projecting the product $A\n\\times B$ as well as $C$ onto a Gaussian random vector and then comparing the\nresulting vectors. The computational complexity of GVFA is consistent with that\nof Freivalds' algorithm, which is $O(n^{2})$. However, unlike Freivalds'\nalgorithm, whose probability of a false positive is $2^{-k}$, where $k$ is the\nnumber of iterations. Our theoretical analysis shows that when $A \\times B \\neq\nC$, GVFA produces a false positive on set of inputs of measure zero with exact\narithmetic. When we introduce round-off error and floating point arithmetic\ninto our analysis, we can show that the larger this error, the higher the\nprobability that GVFA avoids false positives. Moreover, by iterating GVFA $k$\ntimes, the probability of a false positive decreases as $p^k$, where $p$ is a\nvery small value depending on the nature of the fault on the result matrix and\nthe arithmetic system's floating-point precision. Unlike deterministic\nalgorithms, there do not exist any fault patterns that are completely\nundetectable with GVFA. Thus GVFA can be used to provide efficient fault\ntolerance in numerical linear algebra, and it can be efficiently implemented on\nmodern computing architectures. In particular, GVFA can be very efficiently\nimplemented on architectures with hardware support for fused multiply-add\noperations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:46:29 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ji", "Hao", ""], ["Mascagni", "Michael", ""], ["Li", "Yaohang", ""]]}, {"id": "1705.10548", "submitter": "Oren Weimann", "authors": "Pawe{\\l} Gawrychowski, Gad M. Landau, Wing-Kin Sung, Oren Weimann", "title": "A Faster Construction of Greedy Consensus Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A consensus tree is a phylogenetic tree that captures the similarity between\na set of conflicting phylogenetic trees. The problem of computing a consensus\ntree is a major step in phylogenetic tree reconstruction. It also finds\napplications in predicting a species tree from a set of gene trees. This paper\nfocuses on two of the most well-known and widely used oconsensus tree methods:\nthe greedy consensus tree and the frequency difference consensus tree. Given\n$k$ conflicting trees each with $n$ leaves, the previous fastest algorithms for\nthese problems were $O(k n^2)$ for the greedy consensus tree [J. ACM 2016] and\n$\\tilde O(\\min \\{ k n^2, k^2n\\})$ for the frequency difference consensus tree\n[ACM TCBB 2016]. We improve these running times to $\\tilde O(k n^{1.5})$ and\n$\\tilde O(k n)$ respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:12:33 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 15:22:18 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Landau", "Gad M.", ""], ["Sung", "Wing-Kin", ""], ["Weimann", "Oren", ""]]}, {"id": "1705.10648", "submitter": "Christian Loeffeld", "authors": "Christian L\\\"offeld", "title": "The Logarithmic Funnel Heap: A Statistically Self-Similar Priority Queue", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work contains the design and analysis of a statistically\nself-similar data structure using linear space and supporting the operations,\ninsert, search, remove, increase-key and decrease-key for a deterministic\npriority queue in expected O(1) time. Extract-max runs in O(log N) time. The\ndepth of the data structure is at most log* N. On the highest level, each\nelement acts as the entrance of a discrete, log* N-level funnel with a\nlogarithmically decreasing stem diameter, where the stem diameter denotes a\nmetric for the expected number of items maintained on a given level.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 13:55:07 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["L\u00f6ffeld", "Christian", ""]]}, {"id": "1705.10709", "submitter": "Veronika Loitzenbauer", "authors": "Shiri Chechik, Thomas Dueholm Hansen, Giuseppe F. Italiano, Veronika\n  Loitzenbauer, Nikos Parotsidis", "title": "Faster Algorithms for Computing Maximal 2-Connected Subgraphs in Sparse\n  Directed Graphs", "comments": "Revised version of SODA 2017 paper including details for\n  k-edge-connected subgraphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectivity related concepts are of fundamental interest in graph theory.\nThe area has received extensive attention over four decades, but many problems\nremain unsolved, especially for directed graphs. A directed graph is\n2-edge-connected (resp., 2-vertex-connected) if the removal of any edge (resp.,\nvertex) leaves the graph strongly connected. In this paper we present improved\nalgorithms for computing the maximal 2-edge- and 2-vertex-connected subgraphs\nof a given directed graph. These problems were first studied more than 35 years\nago, with $\\widetilde{O}(mn)$ time algorithms for graphs with m edges and n\nvertices being known since the late 1980s. In contrast, the same problems for\nundirected graphs are known to be solvable in linear time. Henzinger et al.\n[ICALP 2015] recently introduced $O(n^2)$ time algorithms for the directed\ncase, thus improving the running times for dense graphs. Our new algorithms run\nin time $O(m^{3/2})$, which further improves the running times for sparse\ngraphs.\n  The notion of 2-connectivity naturally generalizes to k-connectivity for\n$k>2$. For constant values of k, we extend one of our algorithms to compute the\nmaximal k-edge-connected in time $O(m^{3/2} \\log{n})$, improving again for\nsparse graphs the best known algorithm by Henzinger et al. [ICALP 2015] that\nruns in $O(n^2 \\log n)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:46:20 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Chechik", "Shiri", ""], ["Hansen", "Thomas Dueholm", ""], ["Italiano", "Giuseppe F.", ""], ["Loitzenbauer", "Veronika", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1705.10723", "submitter": "Zhao Song", "authors": "Eric Price, Zhao Song, David P. Woodruff", "title": "Fast Regression with an $\\ell_\\infty$ Guarantee", "comments": "ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching has emerged as a powerful technique for speeding up problems in\nnumerical linear algebra, such as regression. In the overconstrained regression\nproblem, one is given an $n \\times d$ matrix $A$, with $n \\gg d$, as well as an\n$n \\times 1$ vector $b$, and one wants to find a vector $\\hat{x}$ so as to\nminimize the residual error $\\|Ax-b\\|_2$. Using the sketch and solve paradigm,\none first computes $S \\cdot A$ and $S \\cdot b$ for a randomly chosen matrix\n$S$, then outputs $x' = (SA)^{\\dagger} Sb$ so as to minimize $\\|SAx' - Sb\\|_2$.\n  The sketch-and-solve paradigm gives a bound on $\\|x'-x^*\\|_2$ when $A$ is\nwell-conditioned. Our main result is that, when $S$ is the subsampled\nrandomized Fourier/Hadamard transform, the error $x' - x^*$ behaves as if it\nlies in a \"random\" direction within this bound: for any fixed direction $a\\in\n\\mathbb{R}^d$, we have with $1 - d^{-c}$ probability that\n  \\[\n  \\langle a, x'-x^*\\rangle \\lesssim\n\\frac{\\|a\\|_2\\|x'-x^*\\|_2}{d^{\\frac{1}{2}-\\gamma}}, \\quad (1)\n  \\]\n  where $c, \\gamma > 0$ are arbitrary constants.\n  This implies $\\|x'-x^*\\|_{\\infty}$ is a factor $d^{\\frac{1}{2}-\\gamma}$\nsmaller than $\\|x'-x^*\\|_2$. It also gives a better bound on the generalization\nof $x'$ to new examples: if rows of $A$ correspond to examples and columns to\nfeatures, then our result gives a better bound for the error introduced by\nsketch-and-solve when classifying fresh examples. We show that not all\noblivious subspace embeddings $S$ satisfy these properties. In particular, we\ngive counterexamples showing that matrices based on Count-Sketch or leverage\nscore sampling do not satisfy these properties.\n  We also provide lower bounds, both on how small $\\|x'-x^*\\|_2$ can be, and\nfor our new guarantee (1), showing that the subsampled randomized\nFourier/Hadamard transform is nearly optimal.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:20:34 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Price", "Eric", ""], ["Song", "Zhao", ""], ["Woodruff", "David P.", ""]]}, {"id": "1705.10923", "submitter": "Jayesh Choudhari", "authors": "Jayesh Choudhari, Anirban Dasgupta, Neeldhara Misra, and M. S.\n  Ramanujan", "title": "Saving Critical Nodes with Firefighters is FPT", "comments": "21 pages, Accepted at ICALP-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of firefighting to save a critical subset of nodes.\nThe firefighting game is a turn-based game played on a graph, where the fire\nspreads to vertices in a breadth-first manner from a source, and firefighters\ncan be placed on yet unburnt vertices on alternate rounds to block the fire. In\nthis work, we consider the problem of saving a critical subset of nodes from\ncatching fire, given a total budget on the number of firefighters. We show that\nthe problem is para-NP-hard when parameterized by the size of the critical set.\nWe also show that it is fixed-parameter tractable on general graphs when\nparameterized by the number of firefighters. We also demonstrate improved\nrunning times on trees and establish that the problem is unlikely to admit a\npolynomial kernelization (even when restricted to trees). Our work is the first\nto exploit the connection between the firefighting problem and the notions of\nimportant separators and tight separator sequences. Finally, we consider the\nspreading model of the firefighting game, a closely related problem, and show\nthat the problem of saving a critical set parameterized by the number of\nfirefighters is W[2]-hard, which contrasts our FPT result for the non-spreading\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:44:22 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Choudhari", "Jayesh", ""], ["Dasgupta", "Anirban", ""], ["Misra", "Neeldhara", ""], ["Ramanujan", "M. S.", ""]]}, {"id": "1705.10987", "submitter": "Nicola Prezza", "authors": "Philip Bille, Anders Roy Christiansen, Nicola Prezza, Frederik Rye\n  Skjoldjensen", "title": "Succinct Partial Sums and Fenwick Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the well-studied partial sums problem in succint space where one\nis to maintain an array of n k-bit integers subject to updates such that\npartial sums queries can be efficiently answered. We present two succint\nversions of the Fenwick Tree - which is known for its simplicity and\npracticality. Our results hold in the encoding model where one is allowed to\nreuse the space from the input data. Our main result is the first that only\nrequires nk + o(n) bits of space while still supporting sum/update in O(log_b\nn) / O(b log_b n) time where 2 <= b <= log^O(1) n. The second result shows how\noptimal time for sum/update can be achieved while only slightly increasing the\nspace usage to nk + o(nk) bits. Beyond Fenwick Trees, the results are primarily\nbased on bit-packing and sampling - making them very practical - and they also\nallow for simple optimal parallelization.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 08:46:22 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Bille", "Philip", ""], ["Christiansen", "Anders Roy", ""], ["Prezza", "Nicola", ""], ["Skjoldjensen", "Frederik Rye", ""]]}, {"id": "1705.11107", "submitter": "Ankur Moitra", "authors": "Linus Hamilton, Frederic Koehler, Ankur Moitra", "title": "Information Theoretic Properties of Markov Random Fields, and their\n  Algorithmic Applications", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields area popular model for high-dimensional probability\ndistributions. Over the years, many mathematical, statistical and algorithmic\nproblems on them have been studied. Until recently, the only known algorithms\nfor provably learning them relied on exhaustive search, correlation decay or\nvarious incoherence assumptions. Bresler gave an algorithm for learning general\nIsing models on bounded degree graphs. His approach was based on a structural\nresult about mutual information in Ising models.\n  Here we take a more conceptual approach to proving lower bounds on the mutual\ninformation through setting up an appropriate zero-sum game. Our proof\ngeneralizes well beyond Ising models, to arbitrary Markov random fields with\nhigher order interactions. As an application, we obtain algorithms for learning\nMarkov random fields on bounded degree graphs on $n$ nodes with $r$-order\ninteractions in $n^r$ time and $\\log n$ sample complexity. The sample\ncomplexity is information theoretically optimal up to the dependence on the\nmaximum degree. The running time is nearly optimal under standard conjectures\nabout the hardness of learning parity with noise.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:18:20 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Hamilton", "Linus", ""], ["Koehler", "Frederic", ""], ["Moitra", "Ankur", ""]]}, {"id": "1705.11163", "submitter": "Adam Karczmarz", "authors": "Giuseppe F. Italiano, Adam Karczmarz, Jakub {\\L}\\k{a}cki, Piotr\n  Sankowski", "title": "Decremental Single-Source Reachability in Planar Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show a new algorithm for the decremental single-source\nreachability problem in directed planar graphs. It processes any sequence of\nedge deletions in $O(n\\log^2{n}\\log\\log{n})$ total time and explicitly\nmaintains the set of vertices reachable from a fixed source vertex. Hence, if\nall edges are eventually deleted, the amortized time of processing each edge\ndeletion is only $O(\\log^2 n \\log \\log n)$, which improves upon a previously\nknown $O(\\sqrt{n})$ solution. We also show an algorithm for decremental\nmaintenance of strongly connected components in directed planar graphs with the\nsame total update time. These results constitute the first almost optimal (up\nto polylogarithmic factors) algorithms for both problems.\n  To the best of our knowledge, these are the first dynamic algorithms with\npolylogarithmic update times on general directed planar graphs for non-trivial\nreachability-type problems, for which only polynomial bounds are known in\ngeneral graphs.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 16:23:39 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Italiano", "Giuseppe F.", ""], ["Karczmarz", "Adam", ""], ["\u0141\u0105cki", "Jakub", ""], ["Sankowski", "Piotr", ""]]}]