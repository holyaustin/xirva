[{"id": "1404.0078", "submitter": "EPTCS", "authors": "Benedek Nagy, S\\'andor V\\'alyi", "title": "Computing discrete logarithm by interval-valued paradigm", "comments": "In Proceedings DCM 2012, arXiv:1403.7579", "journal-ref": "EPTCS 143, 2014, pp. 76-86", "doi": "10.4204/EPTCS.143.7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval-valued computing is a relatively new computing paradigm. It uses\nfinitely many interval segments over the unit interval in a computation as data\nstructure. The satisfiability of Quantified Boolean formulae and other hard\nproblems, like integer factorization, can be solved in an effective way by its\nmassive parallelism. The discrete logarithm problem plays an important role in\npractice, there are cryptographical methods based on its computational\nhardness. In this paper we show that the discrete logarithm problem is\ncomputable by an interval-valued computing in a polynomial number of steps\n(within this paradigm).\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:37:52 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Nagy", "Benedek", ""], ["V\u00e1lyi", "S\u00e1ndor", ""]]}, {"id": "1404.0261", "submitter": "Victor Alvarez", "authors": "Victor Alvarez, Karl Bringmann, Saurabh Ray, Raimund Seidel", "title": "Counting Triangulations and other Crossing-Free Structures Approximately", "comments": "19 pages, 2 figures. A preliminary version appeared at CCCG 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of counting straight-edge triangulations of a given\nset $P$ of $n$ points in the plane. Until very recently it was not known\nwhether the exact number of triangulations of $P$ can be computed\nasymptotically faster than by enumerating all triangulations. We now know that\nthe number of triangulations of $P$ can be computed in $O^{*}(2^{n})$ time,\nwhich is less than the lower bound of $\\Omega(2.43^{n})$ on the number of\ntriangulations of any point set. In this paper we address the question of\nwhether one can approximately count triangulations in sub-exponential time. We\npresent an algorithm with sub-exponential running time and sub-exponential\napproximation ratio, that is, denoting by $\\Lambda$ the output of our\nalgorithm, and by $c^{n}$ the exact number of triangulations of $P$, for some\npositive constant $c$, we prove that $c^{n}\\leq\\Lambda\\leq c^{n}\\cdot\n2^{o(n)}$. This is the first algorithm that in sub-exponential time computes a\n$(1+o(1))$-approximation of the base of the number of triangulations, more\nprecisely, $c\\leq\\Lambda^{\\frac{1}{n}}\\leq(1 + o(1))c$. Our algorithm can be\nadapted to approximately count other crossing-free structures on $P$, keeping\nthe quality of approximation and running time intact. In this paper we show how\nto do this for matchings and spanning trees.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 14:42:48 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Alvarez", "Victor", ""], ["Bringmann", "Karl", ""], ["Ray", "Saurabh", ""], ["Seidel", "Raimund", ""]]}, {"id": "1404.0281", "submitter": "Chandan K. Dubey", "authors": "Chandan Dubey and Thomas Holenstein", "title": "Sampling a Uniform Random Solution of a Quadratic Equation Modulo $p^k$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.NT math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $n$-ary integral quadratic form is a formal expression\n$Q(x_1,...,x_n)=\\sum_{1\\leq i,j\\leq n}a_{ij}x_ix_j$ in $n$-variables\n$x_1,...,x_n$, where $a_{ij}=a_{ji} \\in \\mathbb{Z}$. We present a poly$(n,k,\n\\log p, \\log t)$ randomized algorithm that given a quadratic form\n$Q(x_1,...,x_n)$, a prime $p$, a positive integer $k$ and an integer $t$,\nsamples a uniform solution of $Q(x_1,...,x_n)\\equiv t \\bmod{p^k}$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 15:39:16 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 12:46:30 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Dubey", "Chandan", ""], ["Holenstein", "Thomas", ""]]}, {"id": "1404.0286", "submitter": "Pawe{\\l} Pszona", "authors": "David Eppstein, Michael T. Goodrich, Michael Mitzenmacher, Pawe{\\l}\n  Pszona", "title": "Wear Minimization for Cuckoo Hashing: How Not to Throw a Lot of Eggs\n  into One Basket", "comments": "13 pages, 1 table, 7 figures; to appear at the 13th Symposium on\n  Experimental Algorithms (SEA 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study wear-leveling techniques for cuckoo hashing, showing that it is\npossible to achieve a memory wear bound of $\\log\\log n+O(1)$ after the\ninsertion of $n$ items into a table of size $Cn$ for a suitable constant $C$\nusing cuckoo hashing. Moreover, we study our cuckoo hashing method empirically,\nshowing that it significantly improves on the memory wear performance for\nclassic cuckoo hashing and linear probing in practice.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 15:54:20 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Mitzenmacher", "Michael", ""], ["Pszona", "Pawe\u0142", ""]]}, {"id": "1404.0321", "submitter": "Bei Yu", "authors": "Bei Yu and David Z. Pan", "title": "Layout Decomposition for Quadruple Patterning Lithography and Beyond", "comments": "DAC'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For next-generation technology nodes, multiple patterning lithography (MPL)\nhas emerged as a key solution, e.g., triple patterning lithography (TPL) for\n14/11nm, and quadruple patterning lithography (QPL) for sub-10nm. In this\npaper, we propose a generic and robust layout decomposition framework for QPL,\nwhich can be further extended to handle any general K-patterning lithography\n(K$>$4). Our framework is based on the semidefinite programming (SDP)\nformulation with novel coloring encoding. Meanwhile, we propose fast yet\neffective coloring assignment and achieve significant speedup. To our best\nknowledge, this is the first work on the general multiple patterning\nlithography layout decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 17:58:40 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Yu", "Bei", ""], ["Pan", "David Z.", ""]]}, {"id": "1404.0337", "submitter": "Paul Bonsma", "authors": "Paul Bonsma and Amer E. Mouawad", "title": "The Complexity of Bounded Length Graph Recoloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following question: Given are two $k$-colorings $\\alpha$ and\n$\\beta$ of a graph $G$ on $n$ vertices, and integer $\\ell$. The question is\nwhether $\\alpha$ can be modified into $\\beta$, by recoloring vertices one at a\ntime, while maintaining a $k$-coloring throughout, and using at most $\\ell$\nsuch recoloring steps. This problem is weakly PSPACE-hard for every constant\n$k\\ge 4$. We show that it is also strongly NP-hard for every constant $k\\ge 4$.\nOn the positive side, we give an $O(f(k,\\ell) n^{O(1)})$ algorithm for the\nproblem, for some computable function $f$. Hence the problem is fixed-parameter\ntractable when parameterized by $k+\\ell$. Finally, we show that the problem is\nW[1]-hard (but in XP) when parameterized only by $\\ell$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 18:38:49 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 11:04:26 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Bonsma", "Paul", ""], ["Mouawad", "Amer E.", ""]]}, {"id": "1404.0390", "submitter": "Sebastiano Vigna", "authors": "Sebastiano Vigna", "title": "Further scramblings of Marsaglia's xorshift generators", "comments": "arXiv admin note: text overlap with arXiv:1402.6246", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  xorshift* generators are a variant of Marsaglia's xorshift generators that\neliminate linear artifacts typical of generators based on $\\mathbf Z/2\\mathbf\nZ$-linear operations using multiplication by a suitable constant. Shortly after\nhigh-dimensional xorshift* generators were introduced, Saito and Matsumoto\nsuggested a different way to eliminate linear artifacts based on addition in\n$\\mathbf Z/2^{32}\\mathbf Z$, leading to the XSadd generator. Starting from the\nobservation that the lower bits of XSadd are very weak, as its reverse fails\nsystematically several statistical tests, we explore xorshift+, a variant of\nXSadd using 64-bit operations, which leads, in small dimension, to extremely\nfast high-quality generators.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 20:14:51 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 09:46:02 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 15:44:18 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Vigna", "Sebastiano", ""]]}, {"id": "1404.0564", "submitter": "Saeid Sahraei", "authors": "Saeid Sahraei and Michael C. Gastpar", "title": "New Shortest Lattice Vector Problems of Polynomial Complexity", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shortest Lattice Vector (SLV) problem is in general hard to solve, except\nfor special cases (such as root lattices and lattices for which an obtuse\nsuperbase is known). In this paper, we present a new class of SLV problems that\ncan be solved efficiently. Specifically, if for an $n$-dimensional lattice, a\nGram matrix is known that can be written as the difference of a diagonal matrix\nand a positive semidefinite matrix of rank $k$ (for some constant $k$), we show\nthat the SLV problem can be reduced to a $k$-dimensional optimization problem\nwith countably many candidate points. Moreover, we show that the number of\ncandidate points is bounded by a polynomial function of the ratio of the\nsmallest diagonal element and the smallest eigenvalue of the Gram matrix.\nHence, as long as this ratio is upper bounded by a polynomial function of $n$,\nthe corresponding SLV problem can be solved in polynomial complexity. Our\ninvestigations are motivated by the emergence of such lattices in the field of\nNetwork Information Theory. Further applications may exist in other areas.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 14:22:27 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Sahraei", "Saeid", ""], ["Gastpar", "Michael C.", ""]]}, {"id": "1404.0605", "submitter": "John Fearnley", "authors": "John Fearnley and Rahul Savani", "title": "The Complexity of the Simplex Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplex method is a well-studied and widely-used pivoting method for\nsolving linear programs. When Dantzig originally formulated the simplex method,\nhe gave a natural pivot rule that pivots into the basis a variable with the\nmost violated reduced cost. In their seminal work, Klee and Minty showed that\nthis pivot rule takes exponential time in the worst case. We prove two main\nresults on the simplex method. Firstly, we show that it is PSPACE-complete to\nfind the solution that is computed by the simplex method using Dantzig's pivot\nrule. Secondly, we prove that deciding whether Dantzig's rule ever chooses a\nspecific variable to enter the basis is PSPACE-complete. We use the known\nconnection between Markov decision processes (MDPs) and linear programming, and\nan equivalence between Dantzig's pivot rule and a natural variant of policy\niteration for average-reward MDPs. We construct MDPs and show\nPSPACE-completeness results for single-switch policy iteration, which in turn\nimply our main results for the simplex method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 16:33:31 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 14:26:37 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Fearnley", "John", ""], ["Savani", "Rahul", ""]]}, {"id": "1404.0614", "submitter": "Shai Vardi", "authors": "Shai Vardi", "title": "The secretary returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online random-arrival model, an algorithm receives a sequence of n\nrequests that arrive in a random order. The algorithm is expected to make an\nirrevocable decision with regard to each request based only on the observed\nhistory. We consider the following natural extension of this model: each\nrequest arrives k times, and the arrival order is a random permutation of the\nkn arrivals; the algorithm is expected to make a decision regarding each\nrequest only upon its last arrival. We focus primarily on the case when k=2,\nwhich can also be interpreted as each request arriving at, and departing from\nthe system, at a random time. We examine the secretary problem: the problem of\nselecting the best secretary when the secretaries are presented online\naccording to a random permutation. We show that when each secretary arrives\ntwice, we can achieve a competitive ratio of ~0.768 (compared to 1/e in the\nclassical secretary problem), and that it is optimal. We also show that without\nany knowledge about the number of secretaries or their arrival times, we can\nstill hire the best secretary with probability at least 2/3, in contrast to the\nimpossibility of achieving a constant success probability in the classical\nsetting. We extend our results to the matroid secretary problem, introduced by\nBabaioff et al., (2007) and show a simple algorithm that achieves a\n2-approximation to the maximal weighted basis in the new model (for k=2). We\nshow that this approximation factor can be improved in special cases of the\nmatroid secretary problem; in particular, we give a 16/9-competitive algorithm\nfor the returning edge-weighted bipartite matching problem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 17:00:12 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 18:05:53 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Vardi", "Shai", ""]]}, {"id": "1404.0703", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Christopher R\\'e, Atri Rudra", "title": "Joins via Geometric Resolutions: Worst-case and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple geometric framework for the relational join. Using this\nframework, we design an algorithm that achieves the fractional hypertree-width\nbound, which generalizes classical and recent worst-case algorithmic results on\ncomputing joins. In addition, we use our framework and the same algorithm to\nshow a series of what are colloquially known as beyond worst-case results. The\nframework allows us to prove results for data stored in Btrees,\nmultidimensional data structures, and even multiple indices per table. A key\nidea in our framework is formalizing the inference one does with an index as a\ntype of geometric resolution; transforming the algorithmic problem of computing\njoins to a geometric problem. Our notion of geometric resolution can be viewed\nas a geometric analog of logical resolution. In addition to the geometry and\nlogic connections, our algorithm can also be thought of as backtracking search\nwith memoization.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 20:50:22 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 16:39:13 GMT"}, {"version": "v3", "created": "Thu, 10 Apr 2014 19:41:03 GMT"}, {"version": "v4", "created": "Mon, 13 Oct 2014 00:06:10 GMT"}, {"version": "v5", "created": "Thu, 5 Feb 2015 03:11:16 GMT"}, {"version": "v6", "created": "Tue, 22 Dec 2015 20:48:18 GMT"}, {"version": "v7", "created": "Fri, 23 Dec 2016 19:05:41 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1404.0718", "submitter": "C. Seshadhri", "authors": "Deeparnab Chakrabarty, Kashyap Dixit, Madhav Jha, C. Seshadhri", "title": "Property Testing on Product Distributions: Optimal Testers for Bounded\n  Derivative Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary problem in property testing is to decide whether a given function\nsatisfies a certain property, or is far from any function satisfying it. This\ncrucially requires a notion of distance between functions. The most prevalent\nnotion is the Hamming distance over the {\\em uniform} distribution on the\ndomain. This restriction to uniformity is more a matter of convenience than of\nnecessity, and it is important to investigate distances induced by more general\ndistributions.\n  In this paper, we make significant strides in this direction. We give simple\nand optimal testers for {\\em bounded derivative properties} over {\\em arbitrary\nproduct distributions}. Bounded derivative properties include fundamental\nproperties such as monotonicity and Lipschitz continuity. Our results subsume\nalmost all known results (upper and lower bounds) on monotonicity and Lipschitz\ntesting.\n  We prove an intimate connection between bounded derivative property testing\nand binary search trees (BSTs). We exhibit a tester whose query complexity is\nthe sum of expected depths of optimal BSTs for each marginal. Furthermore, we\nshow this sum-of-depths is also a lower bound. A fundamental technical\ncontribution of this work is an {\\em optimal dimension reduction theorem} for\nall bounded derivative properties, which relates the distance of a function\nfrom the property to the distance of restrictions of the function to random\nlines. Such a theorem has been elusive even for monotonicity for the past 15\nyears, and our theorem is an exponential improvement to the previous best known\nresult.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 22:04:46 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Dixit", "Kashyap", ""], ["Jha", "Madhav", ""], ["Seshadhri", "C.", ""]]}, {"id": "1404.0753", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Gregory B. Sorkin", "title": "Separate, Measure and Conquer: Faster Algorithms for Max 2-CSP and\n  Counting Dominating Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a method resulting in the improvement of several polynomial-space,\nexponential-time algorithms.\n  An instance of the problem Max (r,2)-CSP, or simply Max 2-CSP, is\nparametrized by the domain size r (often 2), the number of variables n\n(vertices in the constraint graph G), and the number of constraints m (edges in\nG). When G is cubic, and omitting sub-exponential terms here for clarity, we\ngive an algorithm running in time r^((1/5)n) = r^((2/15)m); the previous best\nwas r^((1/4)n) = r^((1/6)m). By known results, this improvement for the cubic\ncase results in an algorithm running in time r^((9/50)m) for general instances;\nthe previous best was r^((19/100)m). We show that the analysis of the earlier\nalgorithm was tight: our improvement is in the algorithm, not just the\nanalysis. The new algorithm, like the old, extends to Polynomial and Ring CSP.\n  We also give faster algorithms for #Dominating Set, counting the dominating\nsets of every cardinality 0,...,n for a graph G of order n. For cubic graphs,\nour algorithm runs in time 3^((1/6)n); the previous best was 2^((1/2)n). For\ngeneral graphs, we give an unrelated algorithm running in time 1.5183^n; the\nprevious best was 1.5673^n.\n  The previous best algorithms for these problems all used local\ntransformations and were analyzed by the \"Measure and Conquer\" method. Our new\nalgorithms capitalize on the existence of small balanced separators for cubic\ngraphs - a non-local property - and the ability to tailor the local algorithms\nalways to \"pivot\" on a vertex in the separator. The new algorithms perform much\nas the old ones until the separator is empty, at which point they gain because\nthe remaining vertices are split into two independent problem instances that\ncan be solved recursively. It is likely that such algorithms can be effective\nfor other problems too, and we present their design and analysis in a general\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 03:19:51 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 03:25:08 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Gaspers", "Serge", ""], ["Sorkin", "Gregory B.", ""]]}, {"id": "1404.0780", "submitter": "Bernhard Haeupler", "authors": "Mohsen Ghaffari, Bernhard Haeupler, Majid Khabbazian", "title": "Randomized Broadcast in Radio Networks with Collision Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized distributed algorithm that in radio networks with\ncollision detection broadcasts a single message in $O(D + \\log^6 n)$ rounds,\nwith high probability. This time complexity is most interesting because of its\noptimal additive dependence on the network diameter $D$. It improves over the\ncurrently best known $O(D\\log\\frac{n}{D}\\,+\\,\\log^2 n)$ algorithms, due to\nCzumaj and Rytter [FOCS 2003], and Kowalski and Pelc [PODC 2003]. These\nalgorithms where designed for the model without collision detection and are\noptimal in that model. However, as explicitly stated by Peleg in his 2007\nsurvey on broadcast in radio networks, it had remained an open question whether\nthe bound can be improved with collision detection.\n  We also study distributed algorithms for broadcasting $k$ messages from a\nsingle source to all nodes. This problem is a natural and important\ngeneralization of the single-message broadcast problem, but is in fact\nconsiderably more challenging and less understood. We show the following\nresults: If the network topology is known to all nodes, then a $k$-message\nbroadcast can be performed in $O(D + k\\log n + \\log^2 n)$ rounds, with high\nprobability. If the topology is not known, but collision detection is\navailable, then a $k$-message broadcast can be performed in $O(D + k\\log n +\n\\log^6 n)$ rounds, with high probability. The first bound is optimal and the\nsecond is optimal modulo the additive $O(\\log^6 n)$ term.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 07:07:56 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""], ["Khabbazian", "Majid", ""]]}, {"id": "1404.0783", "submitter": "Ismail Toroslu", "authors": "Cem Evrendilek, Ismail Hakki Toroslu, Sasan Hashemi", "title": "Task Assignment in Tree-Like Hierarchical Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most large organizations, such as corporations, are hierarchical\norganizations. In hierarchical organizations each entity in the organization,\nexcept the root entity, is a sub-part of another entity. In this paper we study\nthe task assignment problem to the entities of tree-like hierarchical\norganizations. The inherent tree structure introduces an interesting and\nchallenging constraint to the standard assignment problem. When a task is\nassigned to an entity in a hierarchical organization, the whole entity,\nincluding its sub-entities, is responsible from the execution of that\nparticular task. In other words, if an entity has been assigned to a task,\nneither its descendants nor its ancestors can be assigned to a task.\nSub-entities cannot be assigned as they have an ancestor already occupied.\nAncestor entities cannot be assigned since one of their sub-entities has\nalready been employed in an assignment. In the paper, we formally introduce\nthis new version of the assignment problem called Maximum Weight Tree Matching\n($MWTM$), and show its NP-hardness. We also propose an effective heuristic\nsolution based on an iterative LP-relaxation to it.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 07:16:49 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Evrendilek", "Cem", ""], ["Toroslu", "Ismail Hakki", ""], ["Hashemi", "Sasan", ""]]}, {"id": "1404.0799", "submitter": "Seth Pettie", "authors": "Allan Gr{\\o}nlund, Seth Pettie", "title": "Threesomes, Degenerates, and Love Triangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3SUM problem is to decide, given a set of $n$ real numbers, whether any\nthree sum to zero. It is widely conjectured that a trivial $O(n^2)$-time\nalgorithm is optimal and over the years the consequences of this conjecture\nhave been revealed. This 3SUM conjecture implies $\\Omega(n^2)$ lower bounds on\nnumerous problems in computational geometry and a variant of the conjecture\nimplies strong lower bounds on triangle enumeration, dynamic graph algorithms,\nand string matching data structures.\n  In this paper we refute the 3SUM conjecture. We prove that the decision tree\ncomplexity of 3SUM is $O(n^{3/2}\\sqrt{\\log n})$ and give two subquadratic 3SUM\nalgorithms, a deterministic one running in $O(n^2 / (\\log n/\\log\\log n)^{2/3})$\ntime and a randomized one running in $O(n^2 (\\log\\log n)^2 / \\log n)$ time with\nhigh probability. Our results lead directly to improved bounds for $k$-variate\nlinear degeneracy testing for all odd $k\\ge 3$. The problem is to decide, given\na linear function $f(x_1,\\ldots,x_k) = \\alpha_0 + \\sum_{1\\le i\\le k} \\alpha_i\nx_i$ and a set $A \\subset \\mathbb{R}$, whether $0\\in f(A^k)$. We show the\ndecision tree complexity of this problem is $O(n^{k/2}\\sqrt{\\log n})$.\n  Finally, we give a subcubic algorithm for a generalization of the\n$(\\min,+)$-product over real-valued matrices and apply it to the problem of\nfinding zero-weight triangles in weighted graphs. We give a\ndepth-$O(n^{5/2}\\sqrt{\\log n})$ decision tree for this problem, as well as an\nalgorithm running in time $O(n^3 (\\log\\log n)^2/\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 08:30:03 GMT"}, {"version": "v2", "created": "Thu, 29 May 2014 10:02:28 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 19:46:20 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Pettie", "Seth", ""]]}, {"id": "1404.0818", "submitter": "Marcin Pilipczuk", "authors": "Daniel Lokshtanov and Marcin Pilipczuk and Micha{\\l} Pilipczuk and\n  Saket Saurabh", "title": "Fixed-parameter tractable canonization and isomorphism test for graphs\n  of bounded treewidth", "comments": "Full version of a paper presented at FOCS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fixed-parameter tractable algorithm that, given a parameter $k$ and\ntwo graphs $G_1,G_2$, either concludes that one of these graphs has treewidth\nat least $k$, or determines whether $G_1$ and $G_2$ are isomorphic. The running\ntime of the algorithm on an $n$-vertex graph is $2^{O(k^5\\log k)}\\cdot n^5$,\nand this is the first fixed-parameter algorithm for Graph Isomorphism\nparameterized by treewidth.\n  Our algorithm in fact solves the more general canonization problem. We namely\ndesign a procedure working in $2^{O(k^5\\log k)}\\cdot n^5$ time that, for a\ngiven graph $G$ on $n$ vertices, either concludes that the treewidth of $G$ is\nat least $k$, or: * finds in an isomorphic-invariant way a graph\n$\\mathfrak{c}(G)$ that is isomorphic to $G$; * finds an isomorphism-invariant\nconstruction term --- an algebraic expression that encodes $G$ together with a\ntree decomposition of $G$ of width $O(k^4)$.\n  Hence, the isomorphism test reduces to verifying whether the computed\nisomorphic copies or the construction terms for $G_1$ and $G_2$ are equal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 09:49:54 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 11:32:25 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""]]}, {"id": "1404.0948", "submitter": "Peter Schneider-Kamp", "authors": "Michael Codish and Luis Cruz-Filipe and Peter Schneider-Kamp", "title": "The Quest for Optimal Sorting Networks: Efficient Generation of\n  Two-Layer Prefixes", "comments": null, "journal-ref": null, "doi": "10.1109/SYNASC.2014.55", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work identifying depth-optimal $n$-channel sorting networks for\n$9\\leq n \\leq 16$ is based on exploiting symmetries of the first two layers.\nHowever, the naive generate-and-test approach typically applied does not scale.\nThis paper revisits the problem of generating two-layer prefixes modulo\nsymmetries. An improved notion of symmetry is provided and a novel technique\nbased on regular languages and graph isomorphism is shown to generate the set\nof non-symmetric representations. An empirical evaluation demonstrates that the\nnew method outperforms the generate-and-test approach by orders of magnitude\nand easily scales until $n=40$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 15:02:11 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 11:44:04 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Codish", "Michael", ""], ["Cruz-Filipe", "Luis", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1404.0977", "submitter": "Oren Weimann", "authors": "Shay Mozes, Yahav Nussbaum, Oren Weimann", "title": "Faster Shortest Paths in Dense Distance Graphs, with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to combine two techniques for efficiently computing shortest\npaths in directed planar graphs. The first is the linear-time shortest-path\nalgorithm of Henzinger, Klein, Subramanian, and Rao [STOC'94]. The second is\nFakcharoenphol and Rao's algorithm [FOCS'01] for emulating Dijkstra's algorithm\non the dense distance graph (DDG). A DDG is defined for a decomposition of a\nplanar graph $G$ into regions of at most $r$ vertices each, for some parameter\n$r < n$. The vertex set of the DDG is the set of $\\Theta(n/\\sqrt r)$ vertices\nof $G$ that belong to more than one region (boundary vertices). The DDG has\n$\\Theta(n)$ arcs, such that distances in the DDG are equal to the distances in\n$G$. Fakcharoenphol and Rao's implementation of Dijkstra's algorithm on the DDG\n(nicknamed FR-Dijkstra) runs in $O(n\\log(n) r^{-1/2} \\log r)$ time, and is a\nkey component in many state-of-the-art planar graph algorithms for shortest\npaths, minimum cuts, and maximum flows. By combining these two techniques we\nremove the $\\log n$ dependency in the running time of the shortest-path\nalgorithm, making it $O(n r^{-1/2} \\log^2r)$.\n  This work is part of a research agenda that aims to develop new techniques\nthat would lead to faster, possibly linear-time, algorithms for problems such\nas minimum-cut, maximum-flow, and shortest paths with negative arc lengths. As\nimmediate applications, we show how to compute maximum flow in directed\nweighted planar graphs in $O(n \\log p)$ time, where $p$ is the minimum number\nof edges on any path from the source to the sink. We also show how to compute\nany part of the DDG that corresponds to a region with $r$ vertices and $k$\nboundary vertices in $O(r \\log k)$ time, which is faster than has been\npreviously known for small values of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 15:44:54 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Mozes", "Shay", ""], ["Nussbaum", "Yahav", ""], ["Weimann", "Oren", ""]]}, {"id": "1404.1008", "submitter": "Alfred Rossi", "authors": "Tamal K. Dey, Pan Peng, Alfred Rossi, Anastasios Sidiropoulos", "title": "Spectral concentration and greedy k-clustering", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular graph clustering method is to consider the embedding of an input\ngraph into R^k induced by the first k eigenvectors of its Laplacian, and to\npartition the graph via geometric manipulations on the resulting metric space.\nDespite the practical success of this methodology, there is limited\nunderstanding of several heuristics that follow this framework. We provide\ntheoretical justification for one such natural and computationally efficient\nvariant.\n  Our result can be summarized as follows. A partition of a graph is called\nstrong if each cluster has small external conductance, and large internal\nconductance. We present a simple greedy spectral clustering algorithm which\nreturns a partition that is provably close to a suitably strong partition,\nprovided that such a partition exists. A recent result shows that strong\npartitions exist for graphs with a sufficiently large spectral gap between the\nk-th and (k+1)-st eigenvalues. Taking this together with our main theorem gives\na spectral algorithm which finds a partition close to a strong one for graphs\nwith large enough spectral gap. We also show how this simple greedy algorithm\ncan be implemented in near-linear time for any fixed k and error guarantee.\nFinally, we evaluate our algorithm on some real-world and synthetic inputs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 17:05:49 GMT"}, {"version": "v2", "created": "Sat, 12 Jul 2014 02:57:16 GMT"}, {"version": "v3", "created": "Wed, 26 Nov 2014 00:08:19 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2015 21:57:59 GMT"}, {"version": "v5", "created": "Tue, 17 Oct 2017 19:59:17 GMT"}, {"version": "v6", "created": "Wed, 12 Sep 2018 00:48:06 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Dey", "Tamal K.", ""], ["Peng", "Pan", ""], ["Rossi", "Alfred", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1404.1056", "submitter": "Leah Epstein", "authors": "Gyorgy Dosa and Leah Epstein", "title": "Online bin packing with cardinality constraints revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin packing with cardinality constraints is a bin packing problem where an\nupper bound k \\geq 2 on the number of items packed into each bin is given, in\naddition to the standard constraint on the total size of items packed into a\nbin. We study the online scenario where items are presented one by one. We\nanalyze it with respect to the absolute competitive ratio and prove tight\nbounds of 2 for any k \\geq 4. We show that First Fit also has an absolute\ncompetitive ratio of 2 for k=4, but not for larger values of k, and we present\na complete analysis of its asymptotic competitive ratio for all values of k\n\\geq 5. Additionally, we study the case of small $k$ with respect to the\nasymptotic competitive ratio and the absolute competitive ratio.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 19:34:50 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Dosa", "Gyorgy", ""], ["Epstein", "Leah", ""]]}, {"id": "1404.1059", "submitter": "Leah Epstein", "authors": "Leah Epstein and Asaf Levin", "title": "Minimum total weighted completion time: Faster approximation schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classic scheduling problems on uniformly related machines. Efficient\npolynomial time approximation schemes (EPTAS's) are fast and practical\napproximation schemes. New methods and techniques are essential in developing\nsuch improved approximation schemes, and their design is a primary goal of this\nresearch agenda. We present EPTAS's for the scheduling problem of a set of jobs\non uniformly related machines so as to minimize the total weighted completion\ntime, both for the case with release dates and its special case without release\ndates. These problems are NP-hard in the strong sense, and therefore EPTAS's\nare the best possible approximation schemes unless P=NP. Previously, only\nPTAS's were known for these two problems, while an EPTAS was known only for the\nspecial case of identical machines without release dates.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 19:40:00 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "1404.1087", "submitter": "Miguel Mosteiro", "authors": "Martin Farach-Colton and Katia Leal and Miguel A. Mosteiro and\n  Christopher Thraves", "title": "Dynamic Windows Scheduling with Reallocation", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Windows Scheduling problem. The problem is a restricted\nversion of Unit-Fractions Bin Packing, and it is also called Inventory\nReplenishment in the context of Supply Chain. In brief, the problem is to\nschedule the use of communication channels to clients. Each client ci is\ncharacterized by an active cycle and a window wi. During the period of time\nthat any given client ci is active, there must be at least one transmission\nfrom ci scheduled in any wi consecutive time slots, but at most one\ntransmission can be carried out in each channel per time slot. The goal is to\nminimize the number of channels used. We extend previous online models, where\ndecisions are permanent, assuming that clients may be reallocated at some cost.\nWe assume that such cost is a constant amount paid per reallocation. That is,\nwe aim to minimize also the number of reallocations. We present three online\nreallocation algorithms for Windows Scheduling. We evaluate experimentally\nthese protocols showing that, in practice, all three achieve constant amortized\nreallocations with close to optimal channel usage. Our simulations also expose\ninteresting trade-offs between reallocations and channel usage. We introduce a\nnew objective function for WS with reallocations, that can be also applied to\nmodels where reallocations are not possible. We analyze this metric for one of\nthe algorithms which, to the best of our knowledge, is the first online WS\nprotocol with theoretical guarantees that applies to scenarios where clients\nmay leave and the analysis is against current load rather than peak load. Using\nprevious results, we also observe bounds on channel usage for one of the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 20:20:05 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Farach-Colton", "Martin", ""], ["Leal", "Katia", ""], ["Mosteiro", "Miguel A.", ""], ["Thraves", "Christopher", ""]]}, {"id": "1404.1097", "submitter": "Janardhan Kulkarni", "authors": "Sungjin Im, Janardhan Kulkarni, Kamesh Munagala", "title": "Competitive Algorithms from Competitive Equilibria: Non-Clairvoyant\n  Scheduling under Polyhedral Constraints", "comments": "Accepted for publication in STOC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a general scheduling problem that we term the Packing\nScheduling problem. In this problem, jobs can have different arrival times and\nsizes; a scheduler can process job $j$ at rate $x_j$, subject to arbitrary\npacking constraints over the set of rates ($\\vec{x}$) of the outstanding jobs.\nThe PSP framework captures a variety of scheduling problems, including the\nclassical problems of unrelated machines scheduling, broadcast scheduling, and\nscheduling jobs of different parallelizability. It also captures scheduling\nconstraints arising in diverse modern environments ranging from individual\ncomputer architectures to data centers. More concretely, PSP models\nmultidimensional resource requirements and parallelizability, as well as\nnetwork bandwidth requirements found in data center scheduling.\n  In this paper, we design non-clairvoyant online algorithms for PSP and its\nspecial cases -- in this setting, the scheduler is unaware of the sizes of\njobs. Our two main results are, 1) a constant competitive algorithm for\nminimizing total weighted completion time for PSP and 2)a scalable algorithm\nfor minimizing the total flow-time on unrelated machines, which is a special\ncase of PSP.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 20:54:24 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Im", "Sungjin", ""], ["Kulkarni", "Janardhan", ""], ["Munagala", "Kamesh", ""]]}, {"id": "1404.1323", "submitter": "Theresa Migler-VonDollen", "authors": "Glencora Borradaile and Claire Mathieu and Theresa Migler", "title": "Lower bounds for testing digraph connectivity with one-pass streaming\n  algorithms", "comments": "Added some references to previous work, removed the part of the\n  result that was already known before, and changed the label of the result\n  from \"Theorem\" to \"Lemma\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we show that three graph properties - strong connectivity,\nacyclicity, and reachability from a vertex $s$ to all vertices - each require a\nworking memory of $\\Omega (\\epsilon m)$ on a graph with $m$ edges to be\ndetermined correctly with probability greater than $(1+\\epsilon)/2$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 17:50:49 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 18:56:06 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Borradaile", "Glencora", ""], ["Mathieu", "Claire", ""], ["Migler", "Theresa", ""]]}, {"id": "1404.1530", "submitter": "Christos Boutsidis", "authors": "Dimitris Papailiopoulos, Anastasios Kyrillidis, Christos Boutsidis", "title": "Provable Deterministic Leverage Score Sampling", "comments": "20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain theoretically a curious empirical phenomenon: \"Approximating a\nmatrix by deterministically selecting a subset of its columns with the\ncorresponding largest leverage scores results in a good low-rank matrix\nsurrogate\". To obtain provable guarantees, previous work requires randomized\nsampling of the columns with probabilities proportional to their leverage\nscores.\n  In this work, we provide a novel theoretical analysis of deterministic\nleverage score sampling. We show that such deterministic sampling can be\nprovably as accurate as its randomized counterparts, if the leverage scores\nfollow a moderately steep power-law decay. We support this power-law assumption\nby providing empirical evidence that such decay laws are abundant in real-world\ndata sets. We then demonstrate empirically the performance of deterministic\nleverage score sampling, which many times matches or outperforms the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 00:08:54 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 10:19:07 GMT"}, {"version": "v3", "created": "Tue, 3 Jun 2014 01:23:16 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Papailiopoulos", "Dimitris", ""], ["Kyrillidis", "Anastasios", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1404.1560", "submitter": "Vadim Stadnik", "authors": "Vadim Stadnik", "title": "Fast Sequential Summation Algorithms Using Augmented Data Structures", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an introduction to the design of augmented data\nstructures that offer an efficient representation of a mathematical sequence\nand fast sequential summation algorithms, which guarantee both logarithmic\nrunning time and logarithmic computational cost in terms of the total number of\noperations. In parallel summation algorithms, logarithmic running time is\nachieved by high linear computational cost with a linear number of processors.\nThe important practical advantage of the fast sequential summation algorithms\nis that they do not require supercomputers and can run on the cheapest single\nprocessor systems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 10:19:37 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Stadnik", "Vadim", ""]]}, {"id": "1404.1568", "submitter": "Friedrich Eisenbrand", "authors": "Friedrich Eisenbrand and Santosh Vempala", "title": "Geometric Random Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a variant of the random-edge pivoting rule results in a strongly\npolynomial time simplex algorithm for linear programs $\\max\\{c^Tx \\colon Ax\\leq\nb\\}$, whose constraint matrix $A$ satisfies a geometric property introduced by\nBrunsch and R\\\"oglin: The sine of the angle of a row of $A$ to a hyperplane\nspanned by $n-1$ other rows of $A$ is at least $\\delta$. This property is a\ngeometric generalization of $A$ being integral and all sub-determinants of $A$\nbeing bounded by $\\Delta$ in absolute value (since $\\delta \\geq 1/(\\Delta^2\nn)$). In particular, linear programs defined by totally unimodular matrices are\ncaptured in this famework ($\\delta \\geq 1/ n$) for which Dyer and Frieze\npreviously described a strongly polynomial-time randomized algorithm.\n  The number of pivots of the simplex algorithm is polynomial in the dimension\nand $1/\\delta$ and independent of the number of constraints of the linear\nprogram. Our main result can be viewed as an algorithmic realization of the\nproof of small diameter for such polytopes by Bonifas et al., using the ideas\nof Dyer and Frieze.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 11:58:28 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 10:00:56 GMT"}, {"version": "v3", "created": "Sun, 10 Aug 2014 18:30:43 GMT"}, {"version": "v4", "created": "Sun, 31 Aug 2014 13:31:01 GMT"}, {"version": "v5", "created": "Mon, 21 Mar 2016 12:09:32 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Eisenbrand", "Friedrich", ""], ["Vempala", "Santosh", ""]]}, {"id": "1404.1577", "submitter": "Younsun Cho", "authors": "YounSun Cho", "title": "Detecting a Corrupted Area in a 2-Dimensional Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fact that 2-dimensional data have become popularly used in\nmany applications without being much considered its integrity checking. We\nintroduce the problem of detecting a corrupted area in a 2-dimensional space,\nand investigate two possible efficient approaches and show their time and space\ncomplexities. Also, we briefly introduce the idea of an approximation scheme\nusing a hash sieve and suggest a novel \"adaptive tree\" structure revealing\ngranularity of information.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 13:52:16 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Cho", "YounSun", ""]]}, {"id": "1404.1732", "submitter": "Christian Konrad", "authors": "Christian Konrad, L\\'aszl\\'o Kozma", "title": "Streaming Algorithms for Partitioning Integer Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of partitioning integer sequences in the one-pass data\nstreaming model. Given is an input stream of integers $X \\in \\{0, 1, \\dots, m\n\\}^n$ of length $n$ with maximum element $m$, and a parameter $p$. The goal is\nto output the positions of separators splitting the input stream into $p$\ncontiguous blocks such that the maximal weight of a block is minimized. We show\nthat computing an optimal solution requires linear space, and we design space\nefficient $(1+\\epsilon)$-approximation algorithms for this problem following\nthe parametric search framework. We demonstrate that parametric search can be\nsuccessfully applied in the streaming model, and we present more space\nefficient refinements of the basic method. All discussed algorithms require\nspace $O( \\frac{1}{\\epsilon} \\mathrm{polylog} (m,n,\\frac{1}{\\epsilon}))$, and\nwe prove that the linear dependency on $\\frac{1}{\\epsilon}$ is necessary for\nany possibly randomized one-pass streaming algorithm that computes a\n$(1+\\epsilon)$-approximation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 10:37:09 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 12:14:43 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Konrad", "Christian", ""], ["Kozma", "L\u00e1szl\u00f3", ""]]}, {"id": "1404.1810", "submitter": "Lorenzo Pasquini", "authors": "Lorenzo Pasquini", "title": "A class of AM-QFT algorithms for power-of-two FFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a class of power-of-two FFT (Fast Fourier Transform)\nalgorithms, called AM-QFT algorithms, that contains the improved QFT (Quick\nFourier Transform), an algorithm recently published, as a special case. The\nmain idea is to apply the Amplitude Modulation Double Sideband - Suppressed\nCarrier (AM DSB-SC) to convert odd-indices signals into even-indices signals,\nand to insert this elaboration into the improved QFT algorithm, substituting\nthe multiplication by secant function. The 8 variants of this class are\nobtained by re-elaboration of the AM DSB-SC idea, and by means of duality. As a\nresult the 8 variants have both the same computational cost and the same memory\nrequirements than improved QFT. Differently, comparing this class of 8 variants\nof AM-QFT algorithm with the split-radix 3add/3mul (one of the most performing\nFFT approach appeared in the literature), we obtain the same number of\nadditions and multiplications, but employing half of the trigonometric\nconstants. This makes the proposed FFT algorithms interesting and useful for\nfixed-point implementations. Some of these variants show advantages versus the\nimproved QFT. In fact one of this variant slightly enhances the numerical\naccuracy of improved QFT, while other four variants use trigonometric constants\nthat are faster to compute in `on the fly' implementations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 15:06:19 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Pasquini", "Lorenzo", ""]]}, {"id": "1404.1849", "submitter": "Martin N\\\"ollenburg", "authors": "Andreas Gemsa, Martin N\\\"ollenburg, Ignaz Rutter", "title": "Evaluation of Labeling Strategies for Rotating Maps", "comments": "16 pages, extended version of a SEA 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem of labeling points in a dynamic map that\nallows rotation. We are given a set of points in the plane labeled by a set of\nmutually disjoint labels, where each label is an axis-aligned rectangle\nattached with one corner to its respective point. We require that each label\nremains horizontally aligned during the map rotation and our goal is to find a\nset of mutually non-overlapping active labels for every rotation angle $\\alpha\n\\in [0, 2\\pi)$ so that the number of active labels over a full map rotation of\n2$\\pi$ is maximized. We discuss and experimentally evaluate several labeling\nmodels that define additional consistency constraints on label activities in\norder to reduce flickering effects during monotone map rotation. We introduce\nthree heuristic algorithms and compare them experimentally to an existing\napproximation algorithm and exact solutions obtained from an integer linear\nprogram. Our results show that on the one hand low flickering can be achieved\nat the expense of only a small reduction in the objective value, and that on\nthe other hand the proposed heuristics achieve a high labeling quality\nsignificantly faster than the other methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 16:51:46 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Gemsa", "Andreas", ""], ["N\u00f6llenburg", "Martin", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1404.1864", "submitter": "Marco Bressan", "authors": "Marco Bressan, Enoch Peserico, Luca Pretto", "title": "Sublinear algorithms for local graph centrality estimation", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of local graph centrality estimation, with the goal\nof approximating the centrality score of a given target node while exploring\nonly a sublinear number of nodes/arcs of the graph and performing a sublinear\nnumber of elementary operations. We develop a technique, that we apply to the\nPageRank and Heat Kernel centralities, for building a low-variance score\nestimator through a local exploration of the graph. We obtain an algorithm\nthat, given any node in any graph of $m$ arcs, with probability $(1-\\delta)$\ncomputes a multiplicative $(1\\pm\\epsilon)$-approximation of its score by\nexamining only $\\tilde{O}(\\min(m^{2/3} \\Delta^{1/3} d^{-2/3},\\, m^{4/5}\nd^{-3/5}))$ nodes/arcs, where $\\Delta$ and $d$ are respectively the maximum and\naverage outdegree of the graph (omitting for readability\n$\\operatorname{poly}(\\epsilon^{-1})$ and $\\operatorname{polylog}(\\delta^{-1})$\nfactors). A similar bound holds for computational complexity. We also prove a\nlower bound of $\\Omega(\\min(m^{1/2} \\Delta^{1/2} d^{-1/2}, \\, m^{2/3}\nd^{-1/3}))$ for both query complexity and computational complexity. Moreover,\nour technique yields a $\\tilde{O}(n^{2/3})$ query complexity algorithm for the\ngraph access model of [Brautbar et al., 2010], widely used in social network\nmining; we show this algorithm is optimal up to a sublogarithmic factor. These\nare the first algorithms yielding worst-case sublinear bounds for general\ndirected graphs and any choice of the target node.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 17:57:33 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 11:33:17 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 09:58:58 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Bressan", "Marco", ""], ["Peserico", "Enoch", ""], ["Pretto", "Luca", ""]]}, {"id": "1404.1943", "submitter": "Janardhan Kulkarni", "authors": "Sungjin Im, Janardhan Kulkarni, Kamesh Munagala, Kirk Pruhs", "title": "SELFISHMIGRATE: A Scalable Algorithm for Non-clairvoyantly Scheduling\n  Heterogeneous Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of minimizing the total weighted flow-time\nfor unrelated machines in the online \\emph{non-clairvoyant} setting. In this\nproblem, a set of jobs $J$ arrive over time to be scheduled on a set of $M$\nmachines. Each job $j$ has processing length $p_j$, weight $w_j$, and is\nprocessed at a rate of $\\ell_{ij}$ when scheduled on machine $i$. The online\nscheduler knows the values of $w_j$ and $\\ell_{ij}$ upon arrival of the job,\nbut is not aware of the quantity $p_j$. We present the {\\em first} online\nalgorithm that is {\\em scalable} ($(1+\\eps)$-speed\n$O(\\frac{1}{\\epsilon^2})$-competitive for any constant $\\eps > 0$) for the\ntotal weighted flow-time objective. No non-trivial results were known for this\nsetting, except for the most basic case of identical machines. Our result\nresolves a major open problem in online scheduling theory. Moreover, we also\nshow that no job needs more than a logarithmic number of migrations. We further\nextend our result and give a scalable algorithm for the objective of minimizing\ntotal weighted flow-time plus energy cost for the case of unrelated machines\nand obtain a scalable algorithm. The key algorithmic idea is to let jobs\nmigrate selfishly until they converge to an equilibrium. Towards this end, we\ndefine a game where each job's utility which is closely tied to the\ninstantaneous increase in the objective the job is responsible for, and each\nmachine declares a policy that assigns priorities to jobs based on when they\nmigrate to it, and the execution speeds. This has a spirit similar to\ncoordination mechanisms that attempt to achieve near optimum welfare in the\npresence of selfish agents (jobs). To the best our knowledge, this is the first\nwork that demonstrates the usefulness of ideas from coordination mechanisms and\nNash equilibria for designing and analyzing online algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 20:24:08 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Im", "Sungjin", ""], ["Kulkarni", "Janardhan", ""], ["Munagala", "Kamesh", ""], ["Pruhs", "Kirk", ""]]}, {"id": "1404.2019", "submitter": "Sandor P. Fekete", "authors": "Michael A. Bender, Martin Farach-Colton, S\\'andor P. Fekete, Jeremy T.\n  Fineman, Seth Gilbert", "title": "Cost-oblivious storage reallocation", "comments": "20 pages, 3 figures; to appear in Transactions on Algorithms. Full\n  journal version of of previous conference paper in PODS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases need to allocate and free blocks of storage on disk. Freed blocks\nintroduce holes where no data is stored. Allocation systems attempt to reuse\nsuch deallocated regions in order to minimize the footprint on disk. If\npreviously allocated blocks cannot be moved, the problem is called the memory\nallocation problem, which is known to have a logarithmic overhead in the\nfootprint.\n  This paper defines the storage reallocation problem, where previously\nallocated blocks can be moved, or reallocated, but at some cost. The algorithms\npresented here are cost oblivious, in that they work for a broad and reasonable\nclass of cost functions, even when they do not know what the cost function is.\n  The objective is to minimize the storage footprint, that is, the largest\nmemory address containing an allocated object, while simultaneously minimizing\nthe reallocation costs. This paper gives asymptotically optimal algorithms for\nstorage reallocation, in which the storage footprint is at most (1+epsilon)\ntimes optimal, and the reallocation cost is at most (1/epsilon) times the\noriginal allocation cost, which is also optimal. The algorithms are cost\noblivious as long as the allocation/reallocation cost function is subadditive.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 06:37:46 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 18:37:56 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 09:01:16 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Bender", "Michael A.", ""], ["Farach-Colton", "Martin", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Fineman", "Jeremy T.", ""], ["Gilbert", "Seth", ""]]}, {"id": "1404.2396", "submitter": "Ashish Chiplunkar", "authors": "Ashish Chiplunkar, Sundar Vishwanathan", "title": "Approximating the Regular Graphic TSP in near linear time", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized approximation algorithm for computing traveling\nsalesperson tours in undirected regular graphs. Given an $n$-vertex,\n$k$-regular graph, the algorithm computes a tour of length at most\n$\\left(1+\\frac{7}{\\ln k-O(1)}\\right)n$, with high probability, in $O(nk \\log\nk)$ time. This improves upon a recent result by Vishnoi (\\cite{Vishnoi12}, FOCS\n2012) for the same problem, in terms of both approximation factor, and running\ntime. The key ingredient of our algorithm is a technique that uses\nedge-coloring algorithms to sample a cycle cover with $O(n/\\log k)$ cycles with\nhigh probability, in near linear time.\n  Additionally, we also give a deterministic\n$\\frac{3}{2}+O\\left(\\frac{1}{\\sqrt{k}}\\right)$ factor approximation algorithm\nrunning in time $O(nk)$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 08:48:51 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 05:00:56 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Chiplunkar", "Ashish", ""], ["Vishwanathan", "Sundar", ""]]}, {"id": "1404.2465", "submitter": "Alfonso de la Fuente Ruiz MEng MSc MBA", "authors": "Alfonso de la Fuente Ruiz", "title": "Quantum annealing", "comments": "21 pages, several figures from different authors. Seminar given in\n  2013 at SBC (University of Shanghai for Science & Technology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Brief description on the state of the art of some local optimization methods:\nQuantum annealing Quantum annealing (also known as alloy, crystallization or\ntempering) is analogous to simulated annealing but in substitution of thermal\nactivation by quantum tunneling. The class of algorithmic methods for quantum\nannealing (dubbed: 'QA'), sometimes referred by the italian school as Quantum\nStochastic Optimization ('QSO'), is a promising metaheuristic tool for solving\nlocal search problems in multivariable optimization contexts.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 12:46:45 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Ruiz", "Alfonso de la Fuente", ""]]}, {"id": "1404.2677", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro, Sharma V. Thankachan", "title": "Optimal Encodings for Range Majority Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of designing a data structure that reports the positions\nof the distinct $\\tau$-majorities within any range of an array $A[1,n]$,\nwithout storing $A$. A $\\tau$-majority in a range $A[i,j]$, for $0<\\tau< 1$, is\nan element that occurs more than $\\tau(j-i+1)$ times in $A[i,j]$. We show that\n$\\Omega(n\\log(1/\\tau))$ bits are necessary for any data structure able just to\ncount the number of distinct $\\tau$-majorities in any range. Then, we design a\nstructure using $O(n\\log(1/\\tau))$ bits that returns one position of each\n$\\tau$-majority of $A[i,j]$ in $O((1/\\tau)\\log\\log_w(1/\\tau)\\log n)$ time, on a\nRAM machine with word size $w$ (it can output any further position where each\n$\\tau$-majority occurs in $O(1)$ additional time). Finally, we show how to\nremove a $\\log n$ factor from the time by adding $O(n\\log\\log n)$ bits of space\nto the structure.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 02:53:26 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 16:30:24 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 22:11:01 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Thankachan", "Sharma V.", ""]]}, {"id": "1404.2824", "submitter": "Gabriele Fici", "authors": "P\\'eter Burcsi and Gabriele Fici and Zsuzsanna Lipt\\'ak and Frank\n  Ruskey and Joe Sawada", "title": "Normal, Abby Normal, Prefix Normal", "comments": "Accepted at FUN '14", "journal-ref": "LNCS 8496, pages 74-88 (2014)", "doi": "10.1007/978-3-319-07890-8_7", "report-no": null, "categories": "cs.FL cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prefix normal word is a binary word with the property that no substring has\nmore 1s than the prefix of the same length. This class of words is important in\nthe context of binary jumbled pattern matching. In this paper we present\nresults about the number $pnw(n)$ of prefix normal words of length $n$, showing\nthat $pnw(n) =\\Omega\\left(2^{n - c\\sqrt{n\\ln n}}\\right)$ for some $c$ and\n$pnw(n) = O \\left(\\frac{2^n (\\ln n)^2}{n}\\right)$. We introduce efficient\nalgorithms for testing the prefix normal property and a \"mechanical algorithm\"\nfor computing prefix normal forms. We also include games which can be played\nwith prefix normal words. In these games Alice wishes to stay normal but Bob\nwants to drive her \"abnormal\" -- we discuss which parameter settings allow\nAlice to succeed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 07:55:57 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Burcsi", "P\u00e9ter", ""], ["Fici", "Gabriele", ""], ["Lipt\u00e1k", "Zsuzsanna", ""], ["Ruskey", "Frank", ""], ["Sawada", "Joe", ""]]}, {"id": "1404.2827", "submitter": "Nader Bshouty", "authors": "Hasan Abasi and Nader H. Bshouty", "title": "A Simple Algorithm for Hamiltonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algebraic technique that solves the following problem: Given\na black box that contains an arithmetic circuit $f$ over a field of\ncharacteristic $2$ of degree~$d$. Decide whether $f$, expressed as an\nequivalent multivariate polynomial, contains a multilinear monomial of degree\n$d$.\n  This problem was solved by Williams \\cite{W} and Bj\\\"orklund et. al.\n\\cite{BHKK} for a white box (the circuit is given as an input) that contains\narithmetic circuit. We show a simple black box algorithm that solves the\nproblem with the same time complexity.\n  This gives a simple randomized algorithm for the simple $k$-path problem for\ndirected graphs of the same time complexity\\footnote{$O^*(f(k))$ is\n$O(poly(n)\\cdot f(k))$} $O^*(2^k)$ as in \\cite{W} and with reusing the same\nideas from \\cite{BHKK} with the above gives another algorithm (probably not\nsimpler) for undirected graphs of the same time complexity $O^*(1.657^k)$ as in\n\\cite{B10,BHKK}.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 14:32:49 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Abasi", "Hasan", ""], ["Bshouty", "Nader H.", ""]]}, {"id": "1404.2842", "submitter": "Xibo Jin", "authors": "Xibo Jin, Fa Zhang, Lin Wang, Songlin Hu, Biyu Zhou and Zhiyong Liu", "title": "A Joint Optimization of Operational Cost and Performance Interference in\n  Cloud Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machine (VM) scheduling is an important technique to efficiently\noperate the computing resources in a data center. Previous work has mainly\nfocused on consolidating VMs to improve resource utilization and thus to\noptimize energy consumption. However, the interference between collocated VMs\nis usually ignored, which can result in very worse performance degradation to\nthe applications running in those VMs due to the contention of the shared\nresources. Based on this observation, we aim at designing efficient VM\nassignment and scheduling strategies where we consider optimizing both the\noperational cost of the data center and the performance degradation of running\napplications and then, we propose a general model which captures the inherent\ntradeoff between the two contradictory objectives. We present offline and\nonline solutions for this problem by exploiting the spatial and temporal\ninformation of VMs where VM scheduling is done by jointly consider the\ncombinations and the life-cycle overlapping of the VMs. Evaluation results show\nthat the proposed methods can generate efficient schedules for VMs, achieving\nlow operational cost while significantly reducing the performance degradation\nof applications in cloud data centers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 15:10:37 GMT"}, {"version": "v2", "created": "Sat, 19 Apr 2014 06:35:44 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Jin", "Xibo", ""], ["Zhang", "Fa", ""], ["Wang", "Lin", ""], ["Hu", "Songlin", ""], ["Zhou", "Biyu", ""], ["Liu", "Zhiyong", ""]]}, {"id": "1404.2943", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius, Sebastian Lehmann, Ignaz Rutter", "title": "Orthogonal Graph Drawing with Inflexible Edges", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of creating plane orthogonal drawings of 4-planar\ngraphs (planar graphs with maximum degree 4) with constraints on the number of\nbends per edge. More precisely, we have a flexibility function assigning to\neach edge $e$ a natural number $\\mathrm{flex}(e)$, its flexibility. The problem\nFlexDraw asks whether there exists an orthogonal drawing such that each edge\n$e$ has at most $\\mathrm{flex}(e)$ bends. It is known that FlexDraw is NP-hard\nif $\\mathrm{flex}(e) = 0$ for every edge $e$. On the other hand, FlexDraw can\nbe solved efficiently if $\\mathrm{flex}(e) \\ge 1$ and is trivial if\n$\\mathrm{flex}(e) \\ge 2$ for every edge $e$.\n  To close the gap between the NP-hardness for $\\mathrm{flex}(e) = 0$ and the\nefficient algorithm for $\\mathrm{flex}(e) \\ge 1$, we investigate the\ncomputational complexity of FlexDraw in case only few edges are inflexible\n(i.e., have flexibility~$0$). We show that for any $\\varepsilon > 0$ FlexDraw\nis NP-complete for instances with $O(n^\\varepsilon)$ inflexible edges with\npairwise distance $\\Omega(n^{1-\\varepsilon})$ (including the case where they\ninduce a matching). On the other hand, we give an FPT-algorithm with running\ntime $O(2^k\\cdot n \\cdot T_{\\mathrm{flow}}(n))$, where $T_{\\mathrm{flow}}(n)$\nis the time necessary to compute a maximum flow in a planar flow network with\nmultiple sources and sinks, and $k$ is the number of inflexible edges having at\nleast one endpoint of degree 4.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 20:24:06 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 16:03:13 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Lehmann", "Sebastian", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1404.2984", "submitter": "Kuldeep Meel", "authors": "Supratik Chakraborty, Daniel J. Fremont, Kuldeep S. Meel, Sanjit A.\n  Seshia, Moshe Y. Vardi", "title": "Distribution-Aware Sampling and Weighted Model Counting for SAT", "comments": "This is a full version of AAAI 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a CNF formula and a weight for each assignment of values to variables,\ntwo natural problems are weighted model counting and distribution-aware\nsampling of satisfying assignments. Both problems have a wide variety of\nimportant applications. Due to the inherent complexity of the exact versions of\nthe problems, interest has focused on solving them approximately. Prior work in\nthis area scaled only to small problems in practice, or failed to provide\nstrong theoretical guarantees, or employed a computationally-expensive maximum\na posteriori probability (MAP) oracle that assumes prior knowledge of a\nfactored representation of the weight distribution. We present a novel approach\nthat works with a black-box oracle for weights of assignments and requires only\nan {\\NP}-oracle (in practice, a SAT-solver) to solve both the counting and\nsampling problems. Our approach works under mild assumptions on the\ndistribution of weights of satisfying assignments, provides strong theoretical\nguarantees, and scales to problems involving several thousand variables. We\nalso show that the assumptions can be significantly relaxed while improving\ncomputational efficiency if a factored representation of the weights is known.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 02:24:18 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Fremont", "Daniel J.", ""], ["Meel", "Kuldeep S.", ""], ["Seshia", "Sanjit A.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1404.3017", "submitter": "Gergo Barta", "authors": "Gergo Barta", "title": "A Link-based Approach to Entity Resolution in Social Networks", "comments": "10 pages, 5 figures, 2 tables, Second International Conference of\n  Database and Data Mining (DBDM 2014)", "journal-ref": null, "doi": "10.5121/csit.2014.4409", "report-no": null, "categories": "cs.IR cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks initially had been places for people to contact each other,\nfind friends or new acquaintances. As such they ever proved interesting for\nmachine aided analysis. Recent developments, however, pivoted social networks\nto being among the main fields of information exchange, opinion expression and\ndebate. As a result there is growing interest in both analyzing and integrating\nsocial network services. In this environment efficient information retrieval is\nhindered by the vast amount and varying quality of the user-generated content.\nGuiding users to relevant information is a valuable service and also a\ndifficult task, where a crucial part of the process is accurately resolving\nduplicate entities to real-world ones. In this paper we propose a novel\napproach that utilizes the principles of link mining to successfully extend the\nmethodology of entity resolution to multitype problems. The proposed method is\npresented using an illustrative social network-based real-world example and\nvalidated by comprehensive evaluation of the results.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 07:11:31 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Barta", "Gergo", ""]]}, {"id": "1404.3033", "submitter": "Gennaro Cordasco PhD", "authors": "Ferdinando Cicalese, Gennaro Cordasco, Luisa Gargano, Martin Milanic,\n  Joseph Peters, Ugo Vaccaro", "title": "How to go Viral: Cheaply and Quickly", "comments": "An extended abstract of this paper will appear in Proceedings of\n  Seventh International conference on Fun with Algorithms (FUN 2014), Lectures\n  Notes in Computer Science, Springer", "journal-ref": "7th International Conference, FUN 2014, Lipari Island, Sicily,\n  Italy, July 1-3, 2014. Proceedings ISBN 978-3-319-07889-2", "doi": "10.1007/978-3-319-07890-8_9", "report-no": null, "categories": "cs.SI cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a social network represented by a graph $G$, we consider the problem of\nfinding a bounded cardinality set of nodes $S$ with the property that the\ninfluence spreading from $S$ in $G$ is as large as possible. The dynamics that\ngovern the spread of influence is the following: initially only elements in $S$\nare influenced; subsequently at each round, the set of influenced elements is\naugmented by all nodes in the network that have a sufficiently large number of\nalready influenced neighbors. While it is known that the general problem is\nhard to solve --- even in the approximate sense --- we present exact polynomial\ntime algorithms for trees, paths, cycles, and complete graphs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 08:43:46 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 08:53:50 GMT"}, {"version": "v3", "created": "Thu, 17 Apr 2014 18:28:17 GMT"}, {"version": "v4", "created": "Wed, 11 Feb 2015 10:16:11 GMT"}, {"version": "v5", "created": "Sun, 15 Feb 2015 09:11:14 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Milanic", "Martin", ""], ["Peters", "Joseph", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1404.3181", "submitter": "Peter Lofgren", "authors": "Peter Lofgren, Siddhartha Banerjee, Ashish Goel, C. Seshadhri", "title": "FAST-PPR: Scaling Personalized PageRank Estimation for Large Graphs", "comments": "KDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm, FAST-PPR, for estimating personalized PageRank:\ngiven start node $s$ and target node $t$ in a directed graph, and given a\nthreshold $\\delta$, FAST-PPR estimates the Personalized PageRank $\\pi_s(t)$\nfrom $s$ to $t$, guaranteeing a small relative error as long $\\pi_s(t)>\\delta$.\nExisting algorithms for this problem have a running-time of $\\Omega(1/\\delta)$;\nin comparison, FAST-PPR has a provable average running-time guarantee of\n${O}(\\sqrt{d/\\delta})$ (where $d$ is the average in-degree of the graph). This\nis a significant improvement, since $\\delta$ is often $O(1/n)$ (where $n$ is\nthe number of nodes) for applications. We also complement the algorithm with an\n$\\Omega(1/\\sqrt{\\delta})$ lower bound for PageRank estimation, showing that the\ndependence on $\\delta$ cannot be improved.\n  We perform a detailed empirical study on numerous massive graphs, showing\nthat FAST-PPR dramatically outperforms existing algorithms. For example, on the\n2010 Twitter graph with 1.5 billion edges, for target nodes sampled by\npopularity, FAST-PPR has a $20$ factor speedup over the state of the art.\nFurthermore, an enhanced version of FAST-PPR has a $160$ factor speedup on the\nTwitter graph, and is at least $20$ times faster on all our candidate graphs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:41:04 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 03:36:09 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Lofgren", "Peter", ""], ["Banerjee", "Siddhartha", ""], ["Goel", "Ashish", ""], ["Seshadhri", "C.", ""]]}, {"id": "1404.3248", "submitter": "Konstantin Makarychev", "authors": "Konstantin Makarychev and Maxim Sviridenko", "title": "Optimization Problems with Diseconomies of Scale via Decoupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for solving optimization problems with a\ndiseconomy of scale. In such problems, our goal is to minimize the cost of\nresources used to perform a certain task. The cost of resources grows\nsuperlinearly, as $x^q$, $q\\ge 1$, with the amount $x$ of resources used. We\ndefine a novel linear programming relaxation for such problems, and then show\nthat the integrality gap of the relaxation is $A_q$, where $A_q$ is the $q$-th\nmoment of the Poisson random variable with parameter 1. Using our framework, we\nobtain approximation algorithms for the Minimum Energy Efficient Routing,\nMinimum Degree Balanced Spanning Tree, Load Balancing on Unrelated Parallel\nMachines, and Unrelated Parallel Machine Scheduling with Nonlinear Functions of\nCompletion Times problems.\n  Our analysis relies on the decoupling inequality for nonnegative random\nvariables. The inequality states that $$\\big \\|\\sum_{i=1}^n X_i\\big\\|_{q} \\leq\nC_q \\,\\big \\|\\sum_{i=1}^n Y_i\\big\\|_{q},$$ where $X_i$ are independent\nnonnegative random variables, $Y_i$ are possibly dependent nonnegative random\nvariable, and each $Y_i$ has the same distribution as $X_i$. The inequality was\nproved by de la Pe\\~na in 1990. De la Pe\\~na, Ibragimov, and Sharakhmetov\n(2003) showed that $C_q\\leq 2$ for $q\\in (1,2)$ and $C_q\\leq A_q^{1/q}$ for\n$q\\geq 2$. We show that the optimal constant is $C_q=A_q^{1/q}$ for any $q\\geq\n1$. We then prove a more general inequality: For every convex function\n$\\varphi$, $$\\mathbb{E}[\\varphi\\Big(\\sum_{i=1}^n X_i\\Big)]\\leq\n\\mathbb{E}[\\varphi\\Big(P\\sum_{i=1}^n Y_i\\Big)],$$ and, for every concave\nfunction $\\psi$, $$\\mathbb{E}[\\psi\\Big(\\sum_{i=1}^n X_i\\Big)] \\geq\n\\mathbb{E}[\\psi\\Big(P\\sum_{i=1}^n Y_i\\Big)],$$ where $P$ is a Poisson random\nvariable with parameter 1 independent of the random variables $Y_i$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 23:36:55 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 22:23:59 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Sviridenko", "Maxim", ""]]}, {"id": "1404.3318", "submitter": "Michele Scquizzato", "authors": "Gianfranco Bilardi, Andrea Pietracaprina, Geppino Pucci, Michele\n  Scquizzato, and Francesco Silvestri", "title": "Network-Oblivious Algorithms", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is proposed for the design and analysis of\n\\emph{network-oblivious algorithms}, namely, algorithms that can run unchanged,\nyet efficiently, on a variety of machines characterized by different degrees of\nparallelism and communication capabilities. The framework prescribes that a\nnetwork-oblivious algorithm be specified on a parallel model of computation\nwhere the only parameter is the problem's input size, and then evaluated on a\nmodel with two parameters, capturing parallelism granularity and communication\nlatency. It is shown that, for a wide class of network-oblivious algorithms,\noptimality in the latter model implies optimality in the Decomposable BSP\nmodel, which is known to effectively describe a wide and significant class of\nparallel platforms. The proposed framework can be regarded as an attempt to\nport the notion of obliviousness, well established in the context of cache\nhierarchies, to the realm of parallel computation. Its effectiveness is\nillustrated by providing optimal network-oblivious algorithms for a number of\nkey problems. Some limitations of the oblivious approach are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 20:05:06 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Bilardi", "Gianfranco", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Scquizzato", "Michele", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1404.3320", "submitter": "Aviad Rubinstein", "authors": "Ilan Adler, Christos Papadimitriou, Aviad Rubinstein", "title": "On Simplex Pivoting Rules and Complexity Theory", "comments": "To appear in IPCO 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there are simplex pivoting rules for which it is PSPACE-complete\nto tell if a particular basis will appear on the algorithm's path. Such rules\ncannot be the basis of a strongly polynomial algorithm, unless P = PSPACE. We\nconjecture that the same can be shown for most known variants of the simplex\nmethod. However, we also point out that Dantzig's shadow vertex algorithm has a\npolynomial path problem. Finally, we discuss in the same context randomized\npivoting rules.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 21:42:05 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Adler", "Ilan", ""], ["Papadimitriou", "Christos", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1404.3327", "submitter": "Sebastiano Vigna", "authors": "Sebastiano Vigna", "title": "Supremum-Norm Convergence for Step-Asynchronous Successive\n  Overrelaxation on M-matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Step-asynchronous successive overrelaxation updates the values contained in a\nsingle vector using the usual Gau\\ss-Seidel-like weighted rule, but arbitrarily\nmixing old and new values, the only constraint being temporal coherence: you\ncannot use a value before it has been computed. We show that given a\nnonnegative real matrix $A$, a $\\sigma\\geq\\rho(A)$ and a vector $\\boldsymbol\nw>0$ such that $A\\boldsymbol w\\leq\\sigma\\boldsymbol w$, every iteration of\nstep-asynchronous successive overrelaxation for the problem $(sI- A)\\boldsymbol\nx=\\boldsymbol b$, with $s >\\sigma$, reduces geometrically the $\\boldsymbol\nw$-norm of the current error by a factor that we can compute explicitly. Then,\nwe show that given a $\\sigma>\\rho(A)$ it is in principle always possible to\ncompute such a $\\boldsymbol w$. This property makes it possible to estimate the\nsupremum norm of the absolute error at each iteration without any additional\nhypothesis on $A$, even when $A$ is so large that computing the product\n$A\\boldsymbol x$ is feasible, but estimating the supremum norm of $(sI-A)^{-1}$\nis not.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 23:25:52 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Vigna", "Sebastiano", ""]]}, {"id": "1404.3391", "submitter": "Stephen Alstrup", "authors": "Stephen Alstrup and Haim Kaplan and Mikkel Thorup and Uri Zwick", "title": "Adjacency labeling schemes and induced-universal graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a way of assigning labels to the vertices of any undirected graph\non up to $n$ vertices, each composed of $n/2+O(1)$ bits, such that given the\nlabels of two vertices, and no other information regarding the graph, it is\npossible to decide whether or not the vertices are adjacent in the graph. This\nis optimal, up to an additive constant, and constitutes the first improvement\nin almost 50 years of an $n/2+O(\\log n)$ bound of Moon. As a consequence, we\nobtain an induced-universal graph for $n$-vertex graphs containing only\n$O(2^{n/2})$ vertices, which is optimal up to a multiplicative constant,\nsolving an open problem of Vizing from 1968. We obtain similar tight results\nfor directed graphs, tournaments and bipartite graphs.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 15:19:03 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Alstrup", "Stephen", ""], ["Kaplan", "Haim", ""], ["Thorup", "Mikkel", ""], ["Zwick", "Uri", ""]]}, {"id": "1404.3501", "submitter": "Mamadou Moustapha Kant\\'e", "authors": "Mamadou Moustapha Kant\\'e and Vincent Limouzy and Arnaud Mary and\n  Lhouari Nourine and Takeaki Uno", "title": "Polynomial Delay Algorithm for Listing Minimal Edge Dominating sets in\n  Graphs", "comments": "proofs simplified from previous version, 12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transversal problem, i.e, the enumeration of all the minimal transversals\nof a hypergraph in output-polynomial time, i.e, in time polynomial in its size\nand the cumulated size of all its minimal transversals, is a fifty years old\nopen problem, and up to now there are few examples of hypergraph classes where\nthe problem is solved. A minimal dominating set in a graph is a subset of its\nvertex set that has a non empty intersection with the closed neighborhood of\nevery vertex. It is proved in [M. M. Kant\\'e, V. Limouzy, A. Mary, L. Nourine,\nOn the Enumeration of Minimal Dominating Sets and Related Notions, In Revision\n2014] that the enumeration of minimal dominating sets in graphs and the\nenumeration of minimal transversals in hypergraphs are two equivalent problems.\nHoping this equivalence can help to get new insights in the Transversal\nproblem, it is natural to look inside graph classes. It is proved independently\nand with different techniques in [Golovach et al. - ICALP 2013] and [Kant\\'e et\nal. - ISAAC 2012] that minimal edge dominating sets in graphs (i.e, minimal\ndominating sets in line graphs) can be enumerated in incremental\noutput-polynomial time. We provide the first polynomial delay and polynomial\nspace algorithm that lists all the minimal edge dominating sets in graphs,\nanswering an open problem of [Golovach et al. - ICALP 2013]. Besides the\nresult, we hope the used techniques that are a mix of a modification of the\nwell-known Berge's algorithm and a strong use of the structure of line graphs,\nare of great interest and could be used to get new output-polynomial time\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 09:16:39 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 16:05:36 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 12:10:25 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Kant\u00e9", "Mamadou Moustapha", ""], ["Limouzy", "Vincent", ""], ["Mary", "Arnaud", ""], ["Nourine", "Lhouari", ""], ["Uno", "Takeaki", ""]]}, {"id": "1404.3577", "submitter": "Kurt Mehlhorn", "authors": "Tomasz Jurkiewicz and Kurt Mehlhorn and Patrick Nicholson", "title": "Cache-Oblivious VAT-Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VAT-model (virtual address translation model) extends the EM-model\n(external memory model) and takes the cost of address translation in virtual\nmemories into account. In this model, the cost of a single memory access may be\nlogarithmic in the largest address used. We show that the VAT-cost of\ncache-oblivious algorithms is only by a constant factor larger than their\nEM-cost; this requires a somewhat more stringent tall cache assumption as for\nthe EM-model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 13:46:43 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Jurkiewicz", "Tomasz", ""], ["Mehlhorn", "Kurt", ""], ["Nicholson", "Patrick", ""]]}, {"id": "1404.3660", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern, Sepp Hartung, Andr\\'e Nichterlein, Manuel Sorge", "title": "Constant-factor approximations for Capacitated Arc Routing without\n  triangle inequality", "comments": null, "journal-ref": "Operations Research Letters 42(4):290--292, 2014", "doi": "10.1016/j.orl.2014.05.002", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph with edge costs and edge demands, the Capacitated\nArc Routing problem (CARP) asks for minimum-cost routes for equal-capacity\nvehicles so as to satisfy all demands. Constant-factor polynomial-time\napproximation algorithms were proposed for CARP with triangle inequality, while\nCARP was claimed to be NP-hard to approximate within any constant factor in\ngeneral. Correcting this claim, we show that any factor {\\alpha} approximation\nfor CARP with triangle inequality yields a factor {\\alpha} approximation for\nthe general CARP.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 17:28:19 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Hartung", "Sepp", ""], ["Nichterlein", "Andr\u00e9", ""], ["Sorge", "Manuel", ""]]}, {"id": "1404.3768", "submitter": "Udi Wieder", "authors": "Anupam Gupta, Kunal Talwar, Udi Wieder", "title": "Changing Bases: Multistage Optimization for Matroids and Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by the fact that many systems need to be maintained\ncontinually while the underlying costs change over time. The challenge is to\ncontinually maintain near-optimal solutions to the underlying optimization\nproblems, without creating too much churn in the solution itself. We model this\nas a multistage combinatorial optimization problem where the input is a\nsequence of cost functions (one for each time step); while we can change the\nsolution from step to step, we incur an additional cost for every such change.\nWe study the multistage matroid maintenance problem, where we need to maintain\na base of a matroid in each time step under the changing cost functions and\nacquisition costs for adding new elements. The online version of this problem\ngeneralizes online paging. E.g., given a graph, we need to maintain a spanning\ntree $T_t$ at each step: we pay $c_t(T_t)$ for the cost of the tree at time\n$t$, and also $| T_t\\setminus T_{t-1} |$ for the number of edges changed at\nthis step. Our main result is an $O(\\log m \\log r)$-approximation, where $m$ is\nthe number of elements/edges and $r$ is the rank of the matroid. We also give\nan $O(\\log m)$ approximation for the offline version of the problem. These\nbounds hold when the acquisition costs are non-uniform, in which caseboth these\nresults are the best possible unless P=NP.\n  We also study the perfect matching version of the problem, where we must\nmaintain a perfect matching at each step under changing cost functions and\ncosts for adding new elements. Surprisingly, the hardness drastically\nincreases: for any constant $\\epsilon>0$, there is no\n$O(n^{1-\\epsilon})$-approximation to the multistage matching maintenance\nproblem, even in the offline case.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 22:20:41 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Gupta", "Anupam", ""], ["Talwar", "Kunal", ""], ["Wieder", "Udi", ""]]}, {"id": "1404.3801", "submitter": "Amer Mouawad", "authors": "Amer E. Mouawad, Naomi Nishimura, Vinayak Pathak, Venkatesh Raman", "title": "Shortest reconfiguration paths in the solution space of Boolean formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a Boolean formula and a satisfying assignment, a flip is an operation\nthat changes the value of a variable in the assignment so that the resulting\nassignment remains satisfying. We study the problem of computing the shortest\nsequence of flips (if one exists) that transforms a given satisfying assignment\n$s$ to another satisfying assignment $t$ of a Boolean formula. Earlier work\ncharacterized the complexity of finding any (not necessarily the shortest)\nsequence of flips from one satisfying assignment to another using Schaefer's\nframework for classification of Boolean formulas. We build on it to provide a\ntrichotomy for the complexity of finding the shortest sequence of flips and\nshow that it is either in P, NP-complete, or PSPACE-complete.\n  Our result adds to the small set of complexity results known for shortest\nreconfiguration sequence problems by providing an example where the shortest\nsequence can be found in polynomial time even though its length is not equal to\nthe symmetric difference of the values of the variables in $s$ and $t$. This is\nin contrast to all reconfiguration problems studied so far, where polynomial\ntime algorithms for computing the shortest path were known only for cases where\nthe path modified the symmetric difference only.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 02:35:44 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 00:25:27 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""], ["Pathak", "Vinayak", ""], ["Raman", "Venkatesh", ""]]}, {"id": "1404.3875", "submitter": "Pierre Lescanne", "authors": "Pierre Lescanne (LIP)", "title": "Boltzmann samplers for random generation of lambda terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomly generating structured objects is important in testing and optimizing\nfunctional programs, whereas generating random $'l$-terms is more specifically\nneeded for testing and optimizing compilers. For that a tool called QuickCheck\nhas been proposed, but in this tool the control of the random generation is\nleft to the programmer. Ten years ago, a method called Boltzmann samplers has\nbeen proposed to generate combinatorial structures. In this paper, we show how\nBoltzmann samplers can be developed to generate lambda-terms, but also other\ndata structures like trees. These samplers rely on a critical value which\nparameters the main random selector and which is exhibited here with\nexplanations on how it is computed. Haskell programs are proposed to show how\nsamplers are actually implemented.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 11:40:45 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 14:06:56 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Lescanne", "Pierre", "", "LIP"]]}, {"id": "1404.3882", "submitter": "Pedro Montealegre", "authors": "Fedor V. Fomin, Mathieu Liedloff, Pedro Montealegre, Ioan Todinca", "title": "Algorithms parameterized by vertex cover and modular width, through\n  potential maximal cliques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give upper bounds on the number of minimal separators and\npotential maximal cliques of graphs w.r.t. two graph parameters, namely vertex\ncover ($\\operatorname{vc}$) and modular width ($\\operatorname{mw}$). We prove\nthat for any graph, the number of minimal separators is\n$\\mathcal{O}^*(3^{\\operatorname{vc}})$ and\n$\\mathcal{O}^*(1.6181^{\\operatorname{mw}})$, and the number of potential\nmaximal cliques is $\\mathcal{O}^*(4^{\\operatorname{vc}})$ and\n$\\mathcal{O}^*(1.7347^{\\operatorname{mw}})$, and these objects can be listed\nwithin the same running times. (The $\\mathcal{O}^*$ notation suppresses\npolynomial factors in the size of the input.) Combined with known results, we\ndeduce that a large family of problems, e.g., Treewidth, Minimum Fill-in,\nLongest Induced Path, Feedback vertex set and many others, can be solved in\ntime $\\mathcal{O}^*(4^{\\operatorname{vc}})$ or\n$\\mathcal{O}^*(1.7347^{\\operatorname{mw}})$.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 12:01:55 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Liedloff", "Mathieu", ""], ["Montealegre", "Pedro", ""], ["Todinca", "Ioan", ""]]}, {"id": "1404.3918", "submitter": "Van Vu H.", "authors": "Van Vu", "title": "A simple SVD algorithm for finding hidden partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a hidden partition in a random environment is a general and important\nproblem, which contains as subproblems many famous questions, such as finding a\nhidden clique, finding a hidden coloring, finding a hidden bipartition etc.\n  In this paper, we provide a simple SVD algorithm for this purpose, answering\na question of McSherry. This algorithm is very easy to implement and works for\nsparse graphs with optimal density.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 14:07:37 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Vu", "Van", ""]]}, {"id": "1404.3919", "submitter": "Anna Bernasconi", "authors": "Anna Bernasconi, Valentina Ciriani, Lorenzo Lago", "title": "On the Error Resilience of Ordered Binary Decision Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered Binary Decision Diagrams (OBDDs) are a data structure that is used in\nan increasing number of fields of Computer Science (e.g., logic synthesis,\nprogram verification, data mining, bioinformatics, and data protection) for\nrepresenting and manipulating discrete structures and Boolean functions. The\npurpose of this paper is to study the error resilience of OBDDs and to design a\nresilient version of this data structure, i.e., a self-repairing OBDD. In\nparticular, we describe some strategies that make reduced ordered OBDDs\nresilient to errors in the indexes, that are associated to the input variables,\nor in the pointers (i.e., OBDD edges) of the nodes. These strategies exploit\nthe inherent redundancy of the data structure, as well as the redundancy\nintroduced by its efficient implementations. The solutions we propose allow the\nexact restoring of the original OBDD and are suitable to be applied to\nclassical software packages for the manipulation of OBDDs currently in use.\nAnother result of the paper is the definition of a new canonical OBDD model,\ncalled {\\em Index-resilient Reduced OBDD}, which guarantees that a node with a\nfaulty index has a reconstruction cost $O(k)$, where $k$ is the number of nodes\nwith corrupted index.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 14:08:09 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 11:38:44 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Bernasconi", "Anna", ""], ["Ciriani", "Valentina", ""], ["Lago", "Lorenzo", ""]]}, {"id": "1404.3990", "submitter": "Leah Epstein", "authors": "Gyorgy Dosa and Leah Epstein", "title": "Colorful bin packing", "comments": "To appear in SWAT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of online bin packing, called colorful bin packing. In\nthis problem, items that are presented one by one are to be packed into bins of\nsize 1. Each item i has a size s_i \\in [0,1] and a color c_i \\in C, where C is\na set of colors (that is not necessarily known in advance). The total size of\nitems packed into a bin cannot exceed its size, thus an item i can always be\npacked into a new bin, but an item cannot be packed into a non-empty bin if the\nprevious item packed into that bin has the same color, or if the occupied space\nin it is larger than 1-s_i. This problem generalizes standard online bin\npacking and online black and white bin packing (where |C|=2). We prove that\ncolorful bin packing is harder than black and white bin packing in the sense\nthat an online algorithm for zero size items that packs the input into the\nsmallest possible number of bins cannot exist for C \\geq 3, while it is known\nthat such an algorithm exists for |C|=2. We show that natural generalizations\nof classic algorithms for bin packing fail to work for the case |C| \\geq 3 and\nmoreover, algorithms that perform well for black and white bin packing do not\nperform well either, already for the case |C|=3. Our main results are a new\nalgorithm for colorful bin packing that we design and analyze, whose absolute\ncompetitive ratio is 4, and a new lower bound of 2 on the asymptotic\ncompetitive ratio of any algorithm, that is valid even for black and white bin\npacking.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 17:12:38 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Dosa", "Gyorgy", ""], ["Epstein", "Leah", ""]]}, {"id": "1404.4123", "submitter": "Takuro Fukunaga", "authors": "Takuro Fukunaga", "title": "Covering problems in edge- and node-weighted graphs", "comments": "To appear in SWAT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the graph covering problem in which a set of edges in an\nedge- and node-weighted graph is chosen to satisfy some covering constraints\nwhile minimizing the sum of the weights. In this problem, because of the large\nintegrality gap of a natural linear programming (LP) relaxation, LP rounding\nalgorithms based on the relaxation yield poor performance. Here we propose a\nstronger LP relaxation for the graph covering problem. The proposed relaxation\nis applied to designing primal-dual algorithms for two fundamental graph\ncovering problems: the prize-collecting edge dominating set problem and the\nmulticut problem in trees. Our algorithms are an exact polynomial-time\nalgorithm for the former problem, and a 2-approximation algorithm for the\nlatter problem, respectively. These results match the currently known best\nresults for purely edge-weighted graphs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 02:10:00 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Fukunaga", "Takuro", ""]]}, {"id": "1404.4344", "submitter": "Przemyslaw Uznanski", "authors": "Petra Berenbrink (SFU.ca), Ralf Klasing (LaBRI), Adrian Kosowski\n  (INRIA Paris-Rocquencourt, LIAFA), Frederik Mallmann-Trenn (SFU.ca, ENS\n  Paris), Przemyslaw Uznanski", "title": "Improved Analysis of Deterministic Load-Balancing Schemes", "comments": "minor corrections; updated literature overview", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deterministic load balancing of tokens in the\ndiscrete model. A set of $n$ processors is connected into a $d$-regular\nundirected network. In every time step, each processor exchanges some of its\ntokens with each of its neighbors in the network. The goal is to minimize the\ndiscrepancy between the number of tokens on the most-loaded and the\nleast-loaded processor as quickly as possible.\n  Rabani et al. (1998) present a general technique for the analysis of a wide\nclass of discrete load balancing algorithms. Their approach is to characterize\nthe deviation between the actual loads of a discrete balancing algorithm with\nthe distribution generated by a related Markov chain. The Markov chain can also\nbe regarded as the underlying model of a continuous diffusion algorithm. Rabani\net al. showed that after time $T = O(\\log (Kn)/\\mu)$, any algorithm of their\nclass achieves a discrepancy of $O(d\\log n/\\mu)$, where $\\mu$ is the spectral\ngap of the transition matrix of the graph, and $K$ is the initial load\ndiscrepancy in the system.\n  In this work we identify some natural additional conditions on deterministic\nbalancing algorithms, resulting in a class of algorithms reaching a smaller\ndiscrepancy. This class contains well-known algorithms, eg., the Rotor-Router.\nSpecifically, we introduce the notion of cumulatively fair load-balancing\nalgorithms where in any interval of consecutive time steps, the total number of\ntokens sent out over an edge by a node is the same (up to constants) for all\nadjacent edges. We prove that algorithms which are cumulatively fair and where\nevery node retains a sufficient part of its load in each step, achieve a\ndiscrepancy of $O(\\min\\{d\\sqrt{\\log n/\\mu},d\\sqrt{n}\\})$ in time $O(T)$. We\nalso show that in general neither of these assumptions may be omitted without\nincreasing discrepancy. We then show by a combinatorial potential reduction\nargument that any cumulatively fair scheme satisfying some additional\nassumptions achieves a discrepancy of $O(d)$ almost as quickly as the\ncontinuous diffusion process. This positive result applies to some of the\nsimplest and most natural discrete load balancing schemes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 18:50:10 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 15:30:49 GMT"}, {"version": "v3", "created": "Sun, 20 Jul 2014 17:20:24 GMT"}, {"version": "v4", "created": "Sun, 22 Feb 2015 08:03:43 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Berenbrink", "Petra", "", "SFU.ca"], ["Klasing", "Ralf", "", "LaBRI"], ["Kosowski", "Adrian", "", "INRIA Paris-Rocquencourt, LIAFA"], ["Mallmann-Trenn", "Frederik", "", "SFU.ca, ENS\n  Paris"], ["Uznanski", "Przemyslaw", ""]]}, {"id": "1404.4465", "submitter": "Peter Sanders", "authors": "Florian Merz and Peter Sanders", "title": "PReaCH: A Fast Lightweight Reachability Index using Pruning and\n  Contraction Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the data structure PReaCH (for Pruned Reachability Contraction\nHierarchies) which supports reachability queries in a directed graph, i.e., it\nsupports queries that ask whether two nodes in the graph are connected by a\ndirected path. PReaCH adapts the contraction hierarchy speedup techniques for\nshortest path queries to the reachability setting. The resulting approach is\nsurprisingly simple and guarantees linear space and near linear preprocessing\ntime. Orthogonally to that, we improve existing pruning techniques for the\nsearch by gathering more information from a single DFS-traversal of the graph.\nPReaCH-indices significantly outperform previous data structures with\ncomparable preprocessing cost. Methods with faster queries need significantly\nmore preprocessing time in particular for the most difficult instances.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 09:55:59 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Merz", "Florian", ""], ["Sanders", "Peter", ""]]}, {"id": "1404.4473", "submitter": "Moran Feldman", "authors": "Moran Feldman, Ola Svensson and Rico Zenklusen", "title": "A Simple $O(\\log\\log(\\mathrm{rank}))$-Competitive Algorithm for the\n  Matroid Secretary Problem", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only recently progress has been made in obtaining\n$o(\\log(\\mathrm{rank}))$-competitive algorithms for the matroid secretary\nproblem. More precisely, Chakraborty and Lachish (2012) presented a\n$O(\\sqrt{\\log(\\mathrm{rank})})$-competitive procedure, and Lachish (2014) later\npresented a $O(\\log\\log(\\mathrm{rank}))$-competitive algorithm. Both these\nalgorithms and their analyses are very involved, which is also reflected in the\nextremely high constants in their competitive ratios.\n  Using different tools, we present a considerably simpler\n$O(\\log\\log(\\mathrm{rank}))$-competitive algorithm for the matroid secretary\nproblem. Our algorithm can be interpreted as a distribution over a simple type\nof matroid secretary algorithms which are easy to analyze. Due to the\nsimplicity of our procedure, we are also able to vastly improve on the hidden\nconstant in the competitive ratio.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 10:16:28 GMT"}, {"version": "v2", "created": "Fri, 4 Jul 2014 13:17:14 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Feldman", "Moran", ""], ["Svensson", "Ola", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1404.4504", "submitter": "Tom\\'a\\v{s} Valla", "authors": "Ferdinando Cicalese, Bal\\'azs Keszegh, Bernard Lidick\\'y,\n  D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi, Tom\\'a\\v{s} Valla", "title": "On the Tree Search Problem with Non-uniform Costs", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2016.07.019", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching in partially ordered structures has been considered in the context\nof information retrieval and efficient tree-like indexes, as well as in\nhierarchy based knowledge representation. In this paper we focus on tree-like\npartial orders and consider the problem of identifying an initially unknown\nvertex in a tree by asking edge queries: an edge query $e$ returns the\ncomponent of $T-e$ containing the vertex sought for, while incurring some known\ncost $c(e)$.\n  The Tree Search Problem with Non-Uniform Cost is: given a tree $T$ where each\nedge has an associated cost, construct a strategy that minimizes the total cost\nof the identification in the worst case.\n  Finding the strategy guaranteeing the minimum possible cost is an NP-complete\nproblem already for input tree of degree 3 or diameter 6. The best known\napproximation guarantee is the $O(\\log n/\\log \\log \\log n)$-approximation\nalgorithm of [Cicalese et al. TCS 2012].\n  We improve upon the above results both from the algorithmic and the\ncomputational complexity point of view: We provide a novel algorithm that\nprovides an $O(\\frac{\\log n}{\\log \\log n})$-approximation of the cost of the\noptimal strategy. In addition, we show that finding an optimal strategy is\nNP-complete even when the input tree is a spider, i.e., at most one vertex has\ndegree larger than 2.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 12:12:48 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Keszegh", "Bal\u00e1zs", ""], ["Lidick\u00fd", "Bernard", ""], ["P\u00e1lv\u00f6lgyi", "D\u00f6m\u00f6t\u00f6r", ""], ["Valla", "Tom\u00e1\u0161", ""]]}, {"id": "1404.4506", "submitter": "Saket Saurabh", "authors": "Daniel Lokshtanov, Pranabendu Misra, Fahad Panolan and Saket Saurabh", "title": "Deterministic Truncation of Linear Matroids", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M=(E,{\\cal I})$ be a matroid. A {\\em $k$-truncation} of $M$ is a matroid\n{$M'=(E,{\\cal I}')$} such that for any $A\\subseteq E$, $A\\in {\\cal I}'$ if and\nonly if $|A|\\leq k$ and $A\\in {\\cal I}$. Given a linear representation of $M$\nwe consider the problem of finding a linear representation of the\n$k$-truncation of this matroid. This problem can be abstracted out to the\nfollowing problem on matrices. Let $M$ be a $n\\times m$ matrix over a field\n$\\mathbb{F}$. A {\\em rank $k$-truncation} of the matrix $M$ is a $k\\times m$\nmatrix $M_k$ (over $\\mathbb{F}$ or a related field) such that for every subset\n$I\\subseteq \\{1,\\ldots,m\\}$ of size at most $k$, the set of columns\ncorresponding to $I$ in $M$ has rank $|I|$ if and only of the corresponding set\nof columns in $M_k$ has rank $|I|$. Finding rank $k$-truncation of matrices is\na common way to obtain a linear representation of $k$-truncation of linear\nmatroids, which has many algorithmic applications. A common way to compute a\nrank $k$-truncation of a $n \\times m$ matrix is to multiply the matrix with a\nrandom $k\\times n$ matrix (with the entries from a field of an appropriate\nsize), yielding a simple randomized algorithm. So a natural question is whether\nit possible to obtain a rank $k$-truncations of a matrix, {\\em\ndeterministically}. In this paper we settle this question for matrices over any\nfinite field or the field of rationals ($\\mathbb Q$). We show that given a\nmatrix $M$ over a field $\\mathbb{F}$ we can compute a $k$-truncation $M_k$ over\nthe ring $\\mathbb{F}[X]$ in deterministic polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 12:13:39 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Misra", "Pranabendu", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""]]}, {"id": "1404.4526", "submitter": "Shahin Kamali", "authors": "Shahin Kamali and Alejandro L\\'opez-Ortiz", "title": "An All-Around Near-Optimal Solution for the Classic Bin Packing Problem", "comments": "20 pages (including the abstract and references), 3 Figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first algorithm with optimal average-case and\nclose-to-best known worst-case performance for the classic on-line problem of\nbin packing. It has long been observed that known bin packing algorithms with\noptimal average-case performance were not optimal in the worst-case sense. In\nparticular First Fit and Best Fit had optimal average-case ratio of 1 but a\nworst-case competitive ratio of 1.7. The wasted space of First Fit and Best Fit\nfor a uniform random sequence of length $n$ is expected to be $\\Theta(n^{2/3})$\nand $\\Theta(\\sqrt{n} \\log ^{3/4} n)$, respectively. The competitive ratio can\nbe improved to 1.691 using the Harmonic algorithm; further variations of this\nalgorithm can push down the competitive ratio to 1.588. However, Harmonic and\nits variations have poor performance on average; in particular, Harmonic has\naverage-case ratio of around 1.27. In this paper, first we introduce a simple\nalgorithm which we term Harmonic Match. This algorithm performs as well as Best\nFit on average, i.e., it has an average-case ratio of 1 and expected wasted\nspace of $\\Theta(\\sqrt{n} \\log ^{3/4} n)$. Moreover, the competitive ratio of\nthe algorithm is as good as Harmonic, i.e., it converges to $ 1.691$ which is\nan improvement over 1.7 of Best Fit and First Fit. We also introduce a\ndifferent algorithm, termed as Refined Harmonic Match, which achieves an\nimproved competitive ratio of $1.636$ while maintaining the good average-case\nperformance of Harmonic Match and Best Fit. Finally, our extensive experimental\nevaluation of the studied bin packing algorithms shows that our proposed\nalgorithms have comparable average-case performance with Best Fit and First\nFit, and this holds also for sequences that follow distributions other than the\nuniform distribution.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 13:47:49 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Kamali", "Shahin", ""], ["L\u00f3pez-Ortiz", "Alejandro", ""]]}, {"id": "1404.4575", "submitter": "Anand Louis", "authors": "Anand Louis, Yury Makarychev", "title": "Approximation Algorithms for Hypergraph Small Set Expansion and Small\n  Set Vertex Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expansion of a hypergraph, a natural extension of the notion of expansion\nin graphs, is defined as the minimum over all cuts in the hypergraph of the\nratio of the number of the hyperedges cut to the size of the smaller side of\nthe cut. We study the Hypergraph Small Set Expansion problem, which, for a\nparameter $\\delta \\in (0,1/2]$, asks to compute the cut having the least\nexpansion while having at most $\\delta$ fraction of the vertices on the smaller\nside of the cut. We present two algorithms. Our first algorithm gives an\n$\\tilde O(\\delta^{-1} \\sqrt{\\log n})$ approximation. The second algorithm finds\na set with expansion $\\tilde O(\\delta^{-1}(\\sqrt{d_{\\text{max}}r^{-1}\\log r\\,\n\\phi^*} + \\phi^*))$ in a $r$--uniform hypergraph with maximum degree\n$d_{\\text{max}}$ (where $\\phi^*$ is the expansion of the optimal solution).\nUsing these results, we also obtain algorithms for the Small Set Vertex\nExpansion problem: we get an $\\tilde O(\\delta^{-1} \\sqrt{\\log n})$\napproximation algorithm and an algorithm that finds a set with vertex expansion\n$O\\left(\\delta^{-1}\\sqrt{\\phi^V \\log d_{\\text{max}} } + \\delta^{-1}\n\\phi^V\\right)$ (where $\\phi^V$ is the vertex expansion of the optimal\nsolution).\n  For $\\delta=1/2$, Hypergraph Small Set Expansion is equivalent to the\nhypergraph expansion problem. In this case, our approximation factor of\n$O(\\sqrt{\\log n})$ for expansion in hypergraphs matches the corresponding\napproximation factor for expansion in graphs due to ARV.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 16:32:27 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Louis", "Anand", ""], ["Makarychev", "Yury", ""]]}, {"id": "1404.4692", "submitter": "Nishant Doshi mr.", "authors": "Nishant Doshi", "title": "Approximation for the Path Complexity of Binary Search Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of an algorithm is an important parameter to determine its\neffi-ciency. They are of different types viz. Time complexity, Space\ncomplexity, etc. However, none of them consider the execution path as a\ncomplexity measure. Ashok et al, firstly proposed the notion of the Path\nComplexity of a pro-gram/algorithm, which defined based on the number of\nexecution paths as a function of the input size. However, the notion of path\ncomplexity of the pro-gram, cannot apply to the object-oriented environment.\nTherefore, Anupam et al, has extended the notion of path complexity to the\nclass as follows. The notion of the state of the class is defined based on\nstructural representation (aka state) of the class. The class contains data\nmembers and data operations. It considers only those data operations that\nchange the state of the class. The path complexity of the class is defined to\nbe the number of valid input sequences, each of them con-taining valid data\noperations. Anupam et al, had applied this notion to the class Stack. However,\nthe stack is basic and simple data structures. Therefore, in this research we\nhave used a more complex class to understand the path complexity behavior in\nthe object oriented environment. Binary Search Tree (BST) is one of the well\nknown (and more complex too) data structure, which is useful in sorting,\nsearching, Traffic Engineering and many more applications. We have analyzed the\npath complexity of the class BST based on the algorithms for insert and delete\noperations. Additionally, we have modified the delete operation to minimize the\npath complexity for the class BST.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 05:41:46 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Doshi", "Nishant", ""]]}, {"id": "1404.4693", "submitter": "Konstantin Kutzkov", "authors": "Konstantin Kutzkov and Rasmus Pagh", "title": "Consistent Subset Sampling", "comments": "To appear in SWAT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistent sampling is a technique for specifying, in small space, a subset\n$S$ of a potentially large universe $U$ such that the elements in $S$ satisfy a\nsuitably chosen sampling condition. Given a subset $\\mathcal{I}\\subseteq U$ it\nshould be possible to quickly compute $\\mathcal{I}\\cap S$, i.e., the elements\nin $\\mathcal{I}$ satisfying the sampling condition. Consistent sampling has\nimportant applications in similarity estimation, and estimation of the number\nof distinct items in a data stream.\n  In this paper we generalize consistent sampling to the setting where we are\ninterested in sampling size-$k$ subsets occurring in some set in a collection\nof sets of bounded size $b$, where $k$ is a small integer. This can be done by\napplying standard consistent sampling to the $k$-subsets of each set, but that\napproach requires time $\\Theta(b^k)$. Using a carefully designed hash function,\nfor a given sampling probability $p \\in (0,1]$, we show how to improve the time\ncomplexity to $\\Theta(b^{\\lceil k/2\\rceil}\\log \\log b + pb^k)$ in expectation,\nwhile maintaining strong concentration bounds for the sample. The space usage\nof our method is $\\Theta(b^{\\lceil k/4\\rceil})$.\n  We demonstrate the utility of our technique by applying it to several\nwell-studied data mining problems. We show how to efficiently estimate the\nnumber of frequent $k$-itemsets in a stream of transactions and the number of\nbipartite cliques in a graph given as incidence stream. Further, building upon\na recent work by Campagna et al., we show that our approach can be applied to\nfrequent itemset mining in a parallel or distributed setting. We also present\napplications in graph stream mining.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 06:09:42 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Kutzkov", "Konstantin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1404.4696", "submitter": "Konstantin Kutzkov", "authors": "Laurent Bulteau and Vincent Froese and Konstantin Kutzkov and Rasmus\n  Pagh", "title": "Triangle counting in dynamic graph streams", "comments": "New version of a SWAT 2014 paper with improved results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of triangles in graph streams using a limited amount of\nmemory has become a popular topic in the last decade. Different variations of\nthe problem have been studied, depending on whether the graph edges are\nprovided in an arbitrary order or as incidence lists. However, with a few\nexceptions, the algorithms have considered {\\em insert-only} streams. We\npresent a new algorithm estimating the number of triangles in {\\em dynamic}\ngraph streams where edges can be both inserted and deleted. We show that our\nalgorithm achieves better time and space complexity than previous solutions for\nvarious graph classes, for example sparse graphs with a relatively small number\nof triangles. Also, for graphs with constant transitivity coefficient, a common\nsituation in real graphs, this is the first algorithm achieving constant\nprocessing time per edge. The result is achieved by a novel approach combining\nsampling of vertex triples and sparsification of the input graph. In the course\nof the analysis of the algorithm we present a lower bound on the number of\npairwise independent 2-paths in general graphs which might be of independent\ninterest. At the end of the paper we discuss lower bounds on the space\ncomplexity of triangle counting algorithms that make no assumptions on the\nstructure of the graph.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 06:18:07 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 20:50:23 GMT"}, {"version": "v3", "created": "Sun, 27 Apr 2014 07:45:16 GMT"}, {"version": "v4", "created": "Thu, 16 Oct 2014 19:53:15 GMT"}, {"version": "v5", "created": "Tue, 14 Jul 2015 08:51:28 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Bulteau", "Laurent", ""], ["Froese", "Vincent", ""], ["Kutzkov", "Konstantin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1404.4702", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman, Pravesh Kothari and Jan Vondr\\'ak", "title": "Tight Bounds on $\\ell_1$ Approximation and Learning of Self-Bounding\n  Functions", "comments": "Fixed minor mistakes and typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of learning and approximation of self-bounding\nfunctions over the uniform distribution on the Boolean hypercube ${0,1}^n$.\nInformally, a function $f:{0,1}^n \\rightarrow \\mathbb{R}$ is self-bounding if\nfor every $x \\in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginal\ndecreases in the value of the function at $x$. Self-bounding functions include\nsuch well-known classes of functions as submodular and fractionally-subadditive\n(XOS) functions. They were introduced by Boucheron et al. (2000) in the context\nof concentration of measure inequalities. Our main result is a nearly tight\n$\\ell_1$-approximation of self-bounding functions by low-degree juntas.\nSpecifically, all self-bounding functions can be $\\epsilon$-approximated in\n$\\ell_1$ by a polynomial of degree $\\tilde{O}(1/\\epsilon)$ over\n$2^{\\tilde{O}(1/\\epsilon)}$ variables. We show that both the degree and\njunta-size are optimal up to logarithmic terms. Previous techniques considered\nstronger $\\ell_2$ approximation and proved nearly tight bounds of\n$\\Theta(1/\\epsilon^{2})$ on the degree and $2^{\\Theta(1/\\epsilon^2)}$ on the\nnumber of variables. Our bounds rely on the analysis of noise stability of\nself-bounding functions together with a stronger connection between noise\nstability and $\\ell_1$ approximation by low-degree polynomials. This technique\ncan also be used to get tighter bounds on $\\ell_1$ approximation by low-degree\npolynomials and faster learning algorithm for halfspaces.\n  These results lead to improved and in several cases almost tight bounds for\nPAC and agnostic learning of self-bounding functions relative to the uniform\ndistribution. In particular, assuming hardness of learning juntas, we show that\nPAC and agnostic learning of self-bounding functions have complexity of\n$n^{\\tilde{\\Theta}(1/\\epsilon)}$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 06:49:49 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 07:38:04 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 20:01:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Feldman", "Vitaly", ""], ["Kothari", "Pravesh", ""], ["Vondr\u00e1k", "Jan", ""]]}, {"id": "1404.4749", "submitter": "Afonso S. Bandeira", "authors": "Emmanuel Abbe and Afonso S. Bandeira and Annina Bracher and Amit\n  Singer", "title": "Decoding binary node labels from censored edge measurements: Phase\n  transition and efficient recovery", "comments": "will appear in the IEEE Transactions on Network Science and\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering a graph $G$ into two communities by\nobserving a subset of the vertex correlations. Specifically, we consider the\ninverse problem with observed variables $Y=B_G x \\oplus Z$, where $B_G$ is the\nincidence matrix of a graph $G$, $x$ is the vector of unknown vertex variables\n(with a uniform prior) and $Z$ is a noise vector with Bernoulli$(\\varepsilon)$\ni.i.d. entries. All variables and operations are Boolean. This model is\nmotivated by coding, synchronization, and community detection problems. In\nparticular, it corresponds to a stochastic block model or a correlation\nclustering problem with two communities and censored edges. Without noise,\nexact recovery (up to global flip) of $x$ is possible if and only the graph $G$\nis connected, with a sharp threshold at the edge probability $\\log(n)/n$ for\nErd\\H{o}s-R\\'enyi random graphs. The first goal of this paper is to determine\nhow the edge probability $p$ needs to scale to allow exact recovery in the\npresence of noise. Defining the degree (oversampling) rate of the graph by\n$\\alpha =np/\\log(n)$, it is shown that exact recovery is possible if and only\nif $\\alpha >2/(1-2\\varepsilon)^2+ o(1/(1-2\\varepsilon)^2)$. In other words,\n$2/(1-2\\varepsilon)^2$ is the information theoretic threshold for exact\nrecovery at low-SNR. In addition, an efficient recovery algorithm based on\nsemidefinite programming is proposed and shown to succeed in the threshold\nregime up to twice the optimal rate. For a deterministic graph $G$, defining\nthe degree rate as $\\alpha=d/\\log(n)$, where $d$ is the minimum degree of the\ngraph, it is shown that the proposed method achieves the rate $\\alpha>\n4((1+\\lambda)/(1-\\lambda)^2)/(1-2\\varepsilon)^2+ o(1/(1-2\\varepsilon)^2)$,\nwhere $1-\\lambda$ is the spectral gap of the graph $G$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 11:18:56 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 23:48:29 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Bandeira", "Afonso S.", ""], ["Bracher", "Annina", ""], ["Singer", "Amit", ""]]}, {"id": "1404.4766", "submitter": "Frans Schalekamp", "authors": "Esteban Feuerstein, Alberto Marchetti-Spaccamela, Frans Schalekamp,\n  Rene Sitters, Suzanne van der Ster, Leen Stougie and Anke van Zuylen", "title": "Scheduling over Scenarios on Two Machines", "comments": "To appear in COCOON 2014. The final publication is available at\n  link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider scheduling problems over scenarios where the goal is to find a\nsingle assignment of the jobs to the machines which performs well over all\npossible scenarios. Each scenario is a subset of jobs that must be executed in\nthat scenario and all scenarios are given explicitly. The two objectives that\nwe consider are minimizing the maximum makespan over all scenarios and\nminimizing the sum of the makespans of all scenarios. For both versions, we\ngive several approximation algorithms and lower bounds on their\napproximability. With this research into optimization problems over scenarios,\nwe have opened a new and rich field of interesting problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 12:41:45 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Feuerstein", "Esteban", ""], ["Marchetti-Spaccamela", "Alberto", ""], ["Schalekamp", "Frans", ""], ["Sitters", "Rene", ""], ["van der Ster", "Suzanne", ""], ["Stougie", "Leen", ""], ["van Zuylen", "Anke", ""]]}, {"id": "1404.4767", "submitter": "Fabrice Rastello", "authors": "Venmugil Elango (CSE), Fabrice Rastello (INRIA Grenoble\n  Rh\\^one-Alpes), Louis-No\\\"el Pouchet (UCLA-CS), J. Ramanujam (ECE), P.\n  Sadayappan (CSE)", "title": "On Characterizing the Data Movement Complexity of Computational DAGs for\n  Parallel Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-8522", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology trends are making the cost of data movement increasingly dominant,\nboth in terms of energy and time, over the cost of performing arithmetic\noperations in computer systems. The fundamental ratio of aggregate data\nmovement bandwidth to the total computational power (also referred to the\nmachine balance parameter) in parallel computer systems is decreasing. It is\nthere- fore of considerable importance to characterize the inherent data\nmovement requirements of parallel algorithms, so that the minimal architectural\nbalance parameters required to support it on future systems can be well\nunderstood. In this paper, we develop an extension of the well-known red-blue\npebble game to develop lower bounds on the data movement complexity for the\nparallel execution of computational directed acyclic graphs (CDAGs) on parallel\nsystems. We model multi-node multi-core parallel systems, with the total\nphysical memory distributed across the nodes (that are connected through some\ninterconnection network) and in a multi-level shared cache hierarchy for\nprocessors within a node. We also develop new techniques for lower bound\ncharacterization of non-homogeneous CDAGs. We demonstrate the use of the\nmethodology by analyzing the CDAGs of several numerical algorithms, to develop\nlower bounds on data movement for their parallel execution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 12:42:26 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Elango", "Venmugil", "", "CSE"], ["Rastello", "Fabrice", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes"], ["Pouchet", "Louis-No\u00ebl", "", "UCLA-CS"], ["Ramanujam", "J.", "", "ECE"], ["Sadayappan", "P.", "", "CSE"]]}, {"id": "1404.4797", "submitter": "Christian Schulz", "authors": "Henning Meyerhenke, Peter Sanders, Christian Schulz", "title": "Parallel Graph Partitioning for Complex Networks", "comments": "Review article. Parallelization of our previous approach\n  arXiv:1402.3281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NE cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large complex networks like social networks or web graphs has\nrecently attracted considerable interest. In order to do this in parallel, we\nneed to partition them into pieces of about equal size. Unfortunately, previous\nparallel graph partitioners originally developed for more regular mesh-like\nnetworks do not work well for these networks. This paper addresses this problem\nby parallelizing and adapting the label propagation technique originally\ndeveloped for graph clustering. By introducing size constraints, label\npropagation becomes applicable for both the coarsening and the refinement phase\nof multilevel graph partitioning. We obtain very high quality by applying a\nhighly parallel evolutionary algorithm to the coarsened graph. The resulting\nsystem is both more scalable and achieves higher quality than state-of-the-art\nsystems like ParMetis or PT-Scotch. For large complex networks the performance\ndifferences are very big. For example, our algorithm can partition a web graph\nwith 3.3 billion edges in less than sixteen seconds using 512 cores of a high\nperformance cluster while producing a high quality partition -- none of the\ncompeting systems can handle this graph on our system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 14:30:04 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 13:40:34 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 10:07:38 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Meyerhenke", "Henning", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1404.4814", "submitter": "Travis Gagie", "authors": "Djamal Belazzougui, Travis Gagie, Simon Gog, Giovanni Manzini and\n  Jouni Sir\\'en", "title": "Reusing an FM-index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, if two strings $S_1$ and $S_2$ are sufficiently similar and we\nalready have an FM-index for $S_1$ then, by storing a little extra information,\nwe should be able to reuse parts of that index in an FM-index for $S_2$. We\nformalize this intuition and show that it can lead to significant space savings\nin practice, as well as to some interesting theoretical problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 15:13:01 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 10:49:58 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Gagie", "Travis", ""], ["Gog", "Simon", ""], ["Manzini", "Giovanni", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1404.4851", "submitter": "Natan Rubin", "authors": "Pankaj K. Agarwal, Haim Kaplan, Natan Rubin and Micha Sharir", "title": "Kinetic Voronoi Diagrams and Delaunay Triangulations under Polygonal\n  Distance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a set of $n$ points and $Q$ a convex $k$-gon in ${\\mathbb R}^2$.\nWe analyze in detail the topological (or discrete) changes in the structure of\nthe Voronoi diagram and the Delaunay triangulation of $P$, under the convex\ndistance function defined by $Q$, as the points of $P$ move along prespecified\ncontinuous trajectories. Assuming that each point of $P$ moves along an\nalgebraic trajectory of bounded degree, we establish an upper bound of\n$O(k^4n\\lambda_r(n))$ on the number of topological changes experienced by the\ndiagrams throughout the motion; here $\\lambda_r(n)$ is the maximum length of an\n$(n,r)$-Davenport-Schinzel sequence, and $r$ is a constant depending on the\nalgebraic degree of the motion of the points. Finally, we describe an algorithm\nfor efficiently maintaining the above structures, using the kinetic data\nstructure (KDS) framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 18:21:18 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Kaplan", "Haim", ""], ["Rubin", "Natan", ""], ["Sharir", "Micha", ""]]}, {"id": "1404.4887", "submitter": "Christian Schulz", "authors": "Yaroslav Akhremtsev, Peter Sanders, Christian Schulz", "title": "(Semi-)External Algorithms for Graph Partitioning and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop semi-external and external memory algorithms for\ngraph partitioning and clustering problems. Graph partitioning and clustering\nare key tools for processing and analyzing large complex networks. We address\nboth problems in the (semi-)external model by adapting the size-constrained\nlabel propagation technique. Our (semi-)external size-constrained label\npropagation algorithm can be used to compute graph clusterings and is a\nprerequisite for the (semi-)external graph partitioning algorithm. The\nalgorithm is then used for both the coarsening and the refinement phase of a\nmultilevel algorithm to compute graph partitions. Our algorithm is able to\npartition and cluster huge complex networks with billions of edges on cheap\ncommodity machines. Experiments demonstrate that the semi-external graph\npartitioning algorithm is scalable and can compute high quality partitions in\ntime that is comparable to the running time of an efficient internal memory\nimplementation. A parallelization of the algorithm in the semi-external model\nfurther reduces running time.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 20:58:21 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 21:38:12 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Akhremtsev", "Yaroslav", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1404.4895", "submitter": "Raphael Kramer", "authors": "Raphael Kramer, Anand Subramanian, Thibaut Vidal, Luc\\'idio dos Anjos\n  Formiga Cabral", "title": "A matheuristic approach for the Pollution-Routing Problem", "comments": "Working Paper -- UFPB, 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the Pollution-Routing Problem (PRP), a Vehicle Routing\nProblem (VRP) with environmental considerations, recently introduced in the\nliterature by [Bektas and Laporte (2011), Transport. Res. B-Meth. 45 (8),\n1232-1250]. The objective is to minimize operational and environmental costs\nwhile respecting capacity constraints and service time windows. Costs are based\non driver wages and fuel consumption, which depends on many factors, such as\ntravel distance and vehicle load. The vehicle speeds are considered as decision\nvariables. They complement routing decisions, impacting the total cost, the\ntravel time between locations, and thus the set of feasible routes. We propose\na method which combines a local search-based metaheuristic with an integer\nprogramming approach over a set covering formulation and a recursive\nspeed-optimization algorithm. This hybridization enables to integrate more\ntightly route and speed decisions. Moreover, two other \"green\" VRP variants,\nthe Fuel Consumption VRP (FCVRP) and the Energy Minimizing VRP (EMVRP), are\naddressed. The proposed method compares very favorably with previous algorithms\nfrom the literature and many new improved solutions are reported.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 22:17:55 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Kramer", "Raphael", ""], ["Subramanian", "Anand", ""], ["Vidal", "Thibaut", ""], ["Cabral", "Luc\u00eddio dos Anjos Formiga", ""]]}, {"id": "1404.4909", "submitter": "Jouni Sir\\'en", "authors": "Gonzalo Navarro, Simon J. Puglisi, Jouni Sir\\'en", "title": "Document Retrieval on Repetitive Collections", "comments": "Accepted to ESA 2014. Implementation and experiments at\n  http://www.cs.helsinki.fi/group/suds/rlcsa/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document retrieval aims at finding the most important documents where a\npattern appears in a collection of strings. Traditional pattern-matching\ntechniques yield brute-force document retrieval solutions, which has motivated\nthe research on tailored indexes that offer near-optimal performance. However,\nan experimental study establishing which alternatives are actually better than\nbrute force, and which perform best depending on the collection\ncharacteristics, has not been carried out. In this paper we address this\nshortcoming by exploring the relationship between the nature of the underlying\ncollection and the performance of current methods. Via extensive experiments we\nshow that established solutions are often beaten in practice by brute-force\nalternatives. We also design new methods that offer superior time/space\ntrade-offs, particularly on repetitive collections.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 01:09:10 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 21:19:40 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1404.4972", "submitter": "Yasuo Tabei", "authors": "Yoshimasa Takabatake, Yasuo Tabei, Hiroshi Sakamoto", "title": "Improved ESP-index: a practical self-index for highly repetitive texts", "comments": "This is the full version of a proceeding accepted to the 11th\n  International Symposium on Experimental Algorithms (SEA2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several self-indexes for highly repetitive texts exist, developing a\npractical self-index applicable to real world repetitive texts remains a\nchallenge. ESP-index is a grammar-based self-index on the notion of\nedit-sensitive parsing (ESP), an efficient parsing algorithm that guarantees\nupper bounds of parsing discrepancies between different appearances of the same\nsubtexts in a text. Although ESP-index performs efficient top-down searches of\nquery texts, it has a serious issue on binary searches for finding appearances\nof variables for a query text, which resulted in slowing down the query\nsearches. We present an improved ESP-index (ESP-index-I) by leveraging the idea\nbehind succinct data structures for large alphabets. While ESP-index-I keeps\nthe same types of efficiencies as ESP-index about the top-down searches, it\navoid the binary searches using fast rank/select operations. We experimentally\ntest ESP-index-I on the ability to search query texts and extract subtexts from\nreal world repetitive texts on a large-scale, and we show that ESP-index-I\nperforms better that other possible approaches.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 17:08:29 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 03:08:52 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Takabatake", "Yoshimasa", ""], ["Tabei", "Yasuo", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1404.4982", "submitter": "Noy Rotbart", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen and Noy Rotbart", "title": "Dynamic and Multi-functional Labeling Schemes", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate labeling schemes supporting adjacency, ancestry, sibling, and\nconnectivity queries in forests. In the course of more than 20 years, the\nexistence of $\\log n + O(\\log \\log)$ labeling schemes supporting each of these\nfunctions was proven, with the most recent being ancestry [Fraigniaud and\nKorman, STOC '10]. Several multi-functional labeling schemes also enjoy lower\nor upper bounds of $\\log n + \\Omega(\\log \\log n)$ or $\\log n + O(\\log \\log n)$\nrespectively. Notably an upper bound of $\\log n + 5\\log \\log n$ for\nadjacency+siblings and a lower bound of $\\log n + \\log \\log n$ for each of the\nfunctions siblings, ancestry, and connectivity [Alstrup et al., SODA '03]. We\nimprove the constants hidden in the $O$-notation. In particular we show a $\\log\nn + 2\\log \\log n$ lower bound for connectivity+ancestry and\nconnectivity+siblings, as well as an upper bound of $\\log n + 3\\log \\log n +\nO(\\log \\log \\log n)$ for connectivity+adjacency+siblings by altering existing\nmethods.\n  In the context of dynamic labeling schemes it is known that ancestry requires\n$\\Omega(n)$ bits [Cohen, et al. PODS '02]. In contrast, we show upper and lower\nbounds on the label size for adjacency, siblings, and connectivity of $2\\log n$\nbits, and $3 \\log n$ to support all three functions. There exist efficient\nadjacency labeling schemes for planar, bounded treewidth, bounded arboricity\nand interval graphs. In a dynamic setting, we show a lower bound of $\\Omega(n)$\nfor each of those families.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 18:44:57 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Rotbart", "Noy", ""]]}, {"id": "1404.4997", "submitter": "Eric Price", "authors": "Moritz Hardt and Eric Price", "title": "Tight bounds for learning a mixture of two gaussians", "comments": "STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider the problem of identifying the parameters of an unknown mixture\nof two arbitrary $d$-dimensional gaussians from a sequence of independent\nrandom samples. Our main results are upper and lower bounds giving a\ncomputationally efficient moment-based estimator with an optimal convergence\nrate, thus resolving a problem introduced by Pearson (1894). Denoting by\n$\\sigma^2$ the variance of the unknown mixture, we prove that\n$\\Theta(\\sigma^{12})$ samples are necessary and sufficient to estimate each\nparameter up to constant additive error when $d=1.$ Our upper bound extends to\narbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$\nusing a novel---yet simple---dimensionality reduction technique. We further\nidentify several interesting special cases where the sample complexity is\nnotably smaller than our optimal worst-case bound. For instance, if the means\nof the two components are separated by $\\Omega(\\sigma)$ the sample complexity\nreduces to $O(\\sigma^2)$ and this is again optimal.\n  Our results also apply to learning each component of the mixture up to small\nerror in total variation distance, where our algorithm gives strong\nimprovements in sample complexity over previous work. We also extend our lower\nbound to mixtures of $k$ Gaussians, showing that $\\Omega(\\sigma^{6k-2})$\nsamples are necessary to estimate each parameter up to constant additive error.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 23:59:35 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 22:15:35 GMT"}, {"version": "v3", "created": "Sun, 17 May 2015 04:47:58 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Hardt", "Moritz", ""], ["Price", "Eric", ""]]}, {"id": "1404.5002", "submitter": "Iraj Saniee", "authors": "Deepak Ajwani, W. Sean Kennedy, Alessandra Sala, Iraj Saniee", "title": "A Geometric Distance Oracle for Large Real-World Graphs", "comments": "15 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Many graph processing algorithms require determination of shortest-path\ndistances between arbitrary numbers of node pairs. Since computation of exact\ndistances between all node-pairs of a large graph, e.g., 10M nodes and up, is\nprohibitively expensive both in computational time and storage space, distance\napproximation is often used in place of exact computation. In this paper, we\npresent a novel and scalable distance oracle that leverages the hyperbolic core\nof real-world large graphs for fast and scalable distance approximation. We\nshow empirically that the proposed oracle significantly outperforms prior\noracles on a random set of test cases drawn from public domain graph libraries.\nThere are two sets of prior work against which we benchmark our approach. The\nfirst set, which often outperforms other oracles, employs embedding of the\ngraph into low dimensional Euclidean spaces with carefully constructed\nhyperbolic distances, but provides no guarantees on the distance estimation\nerror. The second set leverages Gromov-type tree contraction of the graph with\nthe additive error guaranteed not to exceed $2\\delta\\log{n}$, where $\\delta$ is\nthe hyperbolic constant of the graph. We show that our proposed oracle 1) is\nsignificantly faster than those oracles that use hyperbolic embedding (first\nset) with similar approximation error and, perhaps surprisingly, 2) exhibits\nsubstantially lower average estimation error compared to Gromov-like tree\ncontractions (second set). We substantiate our claims through numerical\ncomputations on a collection of a dozen real world networks and synthetic test\ncases from multiple domains, ranging in size from 10s of thousand to 10s of\nmillions of nodes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 02:05:34 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Ajwani", "Deepak", ""], ["Kennedy", "W. Sean", ""], ["Sala", "Alessandra", ""], ["Saniee", "Iraj", ""]]}, {"id": "1404.5236", "submitter": "Boaz Barak", "authors": "Boaz Barak and David Steurer", "title": "Sum-of-squares proofs and the quest toward optimal algorithms", "comments": "Survey. To appear in proceedings of ICM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to obtain the best-known guarantees, algorithms are traditionally\ntailored to the particular problem we want to solve. Two recent developments,\nthe Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,\nsurprisingly suggest that this tailoring is not necessary and that a single\nefficient algorithm could achieve best possible guarantees for a wide range of\ndifferent problems.\n  The Unique Games Conjecture (UGC) is a tantalizing conjecture in\ncomputational complexity, which, if true, will shed light on the complexity of\na great many problems. In particular this conjecture predicts that a single\nconcrete algorithm provides optimal guarantees among all efficient algorithms\nfor a large class of computational problems.\n  The Sum-of-Squares (SOS) method is a general approach for solving systems of\npolynomial constraints. This approach is studied in several scientific\ndisciplines, including real algebraic geometry, proof complexity, control\ntheory, and mathematical programming, and has found applications in fields as\ndiverse as quantum information theory, formal verification, game theory and\nmany others.\n  We survey some connections that were recently uncovered between the Unique\nGames Conjecture and the Sum-of-Squares method. In particular, we discuss new\ntools to rigorously bound the running time of the SOS method for obtaining\napproximate solutions to hard optimization problems, and how these tools give\nthe potential for the sum-of-squares method to provide new guarantees for many\nproblems of interest, and possibly to even refute the UGC.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 16:24:13 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 17:52:52 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Barak", "Boaz", ""], ["Steurer", "David", ""]]}, {"id": "1404.5244", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov, Mikhail Rubinchik, Arseny M. Shur", "title": "$\\mathrm{Pal}^k$ Is Linear Recognizable Online", "comments": "18 pages, 5 figures, presented in SOFSEM 2015", "journal-ref": "Proc. SOFSEM 2015. Springer, 2015. LNCS Vol. 8939, 289-301", "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a language $L$ that is online recognizable in linear time and space, we\nconstruct a linear time and space online recognition algorithm for the language\n$L\\cdot\\mathrm{Pal}$, where $\\mathrm{Pal}$ is the language of all nonempty\npalindromes. Hence for every fixed positive $k$, $\\mathrm{Pal}^k$ is online\nrecognizable in linear time and space. Thus we solve an open problem posed by\nGalil and Seiferas in 1978.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 17:07:52 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 19:17:25 GMT"}, {"version": "v3", "created": "Wed, 10 Sep 2014 18:36:41 GMT"}, {"version": "v4", "created": "Fri, 30 Jan 2015 14:34:02 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Kosolobov", "Dmitry", ""], ["Rubinchik", "Mikhail", ""], ["Shur", "Arseny M.", ""]]}, {"id": "1404.5398", "submitter": "Shai Vardi", "authors": "Omer Reingold, Shai Vardi", "title": "New Techniques and Tighter Bounds for Local Computation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an input $x$, and a search problem $F$, local computation algorithms\n(LCAs) implement access to specified locations of $y$ in a legal output $y \\in\nF(x)$, using polylogarithmic time and space. Mansour et al., (2012), had\npreviously shown how to convert certain online algorithms to LCAs. In this\nwork, we expand on that line of work and develop new techniques for designing\nLCAs and bounding their space and time complexity. Our contributions are\nfourfold: (1) We significantly improve the running times and space requirements\nof LCAs for previous results, (2) we expand and better define the family of\nonline algorithms which can be converted to LCAs using our techniques, (3) we\nshow that our results apply to a larger family of graphs than that of previous\nresults, and (4) our proofs are simpler and more concise than the previous\nproof methods.\n  For example, we show how to construct LCAs that require\n$O(\\log{n}\\log\\log{n})$ space and $O(\\log^2{n})$ time (and expected time\n$O(\\log\\log{n})$) for problems such as maximal matching on a large family of\ngraphs, as opposed to the henceforth best results that required $O(\\log^3{n})$\nspace and $O(\\log^4{n})$ time, and applied to a smaller family of graphs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 07:15:27 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Reingold", "Omer", ""], ["Vardi", "Shai", ""]]}, {"id": "1404.5424", "submitter": "Damien Prot", "authors": "Odile Bellenguez-Morineau, Marek Chrobak, Christoph D\\\"urr, Damien\n  Prot", "title": "A Note on NP-Hardness of Preemptive Mean Flow-Time Scheduling for\n  Parallel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper \"The complexity of mean flow time scheduling problems with\nrelease times\", by Baptiste, Brucker, Chrobak, D\\\"urr, Kravchenko and Sourd,\nthe authors claimed to prove strong NP-hardness of the scheduling problem\n$P|pmtn,r_j|\\sum C_j$, namely multiprocessor preemptive scheduling where the\nobjective is to minimize the mean flow time. We point out a serious error in\ntheir proof and give a new proof of strong NP-hardness for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 08:54:20 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Bellenguez-Morineau", "Odile", ""], ["Chrobak", "Marek", ""], ["D\u00fcrr", "Christoph", ""], ["Prot", "Damien", ""]]}, {"id": "1404.5428", "submitter": "Bojun Huang", "authors": "Bojun Huang", "title": "Sequential Resource Allocation with Positional Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the total cost to run a sequence of $n$\ntasks in the given order by $k$ agents under the positional cost model. The\ncost to run a task not only depends on the intrinsic cost of the task itself,\nbut also monotonically related to the position this task is in the working list\nof the agent assigned. Such a positional effect can naturally arise from the\nclassic sum-of-completion-time minimization problems, and is also well\nmotivated by the varying efficiency when an agent works in reality (such as due\nto the learning effects or deteriorating effects). Also, it can be seen as a\ndeterministic variant of the classic Baysian sequential decision making\nproblems. This paper presents a simple and practical algorithm that runs in\n$O(k^2 n)$ time and minimizes the total cost of any problem instance consisting\nof two task types. The algorithm works by making greedy decision for each task\nsequentially based on some stopping thresholds in a \"greedy-like\" allocation\nsimulation -- a working style coinciding with Gittins' optimal-stopping based\nalgorithm for the classic Baysian multi-armed bandit problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 09:01:52 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Huang", "Bojun", ""]]}, {"id": "1404.5448", "submitter": "Prashanth Srikanthan", "authors": "Guru Prakash Arumugam, John Augustine, Mordecai J. Golin, Prashanth\n  Srikanthan", "title": "A Polynomial Time Algorithm for Minimax-Regret Evacuation on a Dynamic\n  Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic path network is an undirected path with evacuees situated at each\nvertex. To evacuate the path, evacuees travel towards a designated sink\n(doorway) to exit. Each edge has a capacity, the number of evacuees that can\nenter the edge in unit time. Congestion occurs if an evacuee has to wait at a\nvertex for other evacuees to leave first. The basic problem is to place k sinks\non the line, with an associated evacuation strategy, so as to minimize the\ntotal time needed to evacuate everyone. The minmax-regret version introduces\nuncertainty into the input, with the number of evacuees at vertices only being\nspecified to within a range. The problem is to find a universal solution whose\nregret (difference from optimal for a given input) is minimized over all legal\ninputs. The previously best known algorithms for the minmax regret version\nproblem ran in time exponential in k. In this paper, we derive new prop- erties\nof solutions that yield the first polynomial time algorithms for solving the\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 10:18:29 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Arumugam", "Guru Prakash", ""], ["Augustine", "John", ""], ["Golin", "Mordecai J.", ""], ["Srikanthan", "Prashanth", ""]]}, {"id": "1404.5475", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov and Vladimir Kolmogorov", "title": "Combining pattern-based CRFs and weighted context-free grammars", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two models for the sequence labeling (tagging) problem. The first\none is a {\\em Pattern-Based Conditional Random Field }(\\PB), in which the\nenergy of a string (chain labeling) $x=x_1\\ldots x_n\\in D^n$ is a sum of terms\nover intervals $[i,j]$ where each term is non-zero only if the substring\n$x_i\\ldots x_j$ equals a prespecified word $w\\in \\Lambda$. The second model is\na {\\em Weighted Context-Free Grammar }(\\WCFG) frequently used for natural\nlanguage processing. \\PB and \\WCFG encode local and non-local interactions\nrespectively, and thus can be viewed as complementary.\n  We propose a {\\em Grammatical Pattern-Based CRF model }(\\GPB) that combines\nthe two in a natural way. We argue that it has certain advantages over existing\napproaches such as the {\\em Hybrid model} of Bened{\\'i} and Sanchez that\ncombines {\\em $\\mbox{$N$-grams}$} and \\WCFGs. The focus of this paper is to\nanalyze the complexity of inference tasks in a \\GPB such as computing MAP. We\npresent a polynomial-time algorithm for general \\GPBs and a faster version for\na special case that we call {\\em Interaction Grammars}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 12:44:42 GMT"}, {"version": "v2", "created": "Sat, 1 Nov 2014 13:29:52 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Takhanov", "Rustem", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1404.5510", "submitter": "Abdolhamid Ghodselahi", "authors": "Abdolhamid Ghodselahi and Fabian Kuhn", "title": "Serving Online Requests with Mobile Servers", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online problem in which a set of mobile servers have to be moved\nin order to efficiently serve a set of requests that arrive in an online\nfashion. More formally, there is a set of $n$ nodes and a set of $k$ mobile\nservers that are placed at some of the nodes. Each node can potentially host\nseveral servers and the servers can be moved between the nodes. There are\nrequests $1,2,\\ldots$ that are adversarially issued at nodes one at a time. An\nissued request at time $t$ needs to be served at all times $t' \\geq t$. The\ncost for serving the requests is a function of the number of servers and\nrequests at the different nodes. The requirements on how to serve the requests\nare governed by two parameters $\\alpha\\geq 1$ and $\\beta\\geq 0$. An algorithm\nneeds to guarantee at all times that the total service cost remains within a\nmultiplicative factor of $\\alpha$ and an additive term $\\beta$ of the current\noptimal service cost. We consider online algorithms for two different\nminimization objectives. We first consider the natural problem of minimizing\nthe total number of server movements. We show that in this case for every $k$,\nthe competitive ratio of every deterministic online algorithm needs to be at\nleast $\\Omega(n)$. Given this negative result, we then extend the minimization\nobjective to also include the current service cost. We give almost tight bounds\non the competitive ratio of the online problem where one needs to minimize the\nsum of the total number of movements and the current service cost. In\nparticular, we show that at the cost of an additional additive term which is\nroughly linear in $k$, it is possible to achieve a multiplicative competitive\nratio of $1+\\varepsilon$ for every constant $\\varepsilon>0$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 14:26:49 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 18:04:31 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Ghodselahi", "Abdolhamid", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1404.5545", "submitter": "Kashyap Dixit", "authors": "Kashyap Dixit", "title": "$L_p$-Testers for Bounded Derivative Properties on Product Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of $L_p$-testing of class of bounded derivative\nproperties over hypergrid domain with points distributed according to some\nproduct distribution. This class includes monotonicity, the Lipschitz property,\n$(\\alpha,\\beta)$-generalized Lipschitz and many more properties. Previous\nresults for $L_p$ testing on $[n]^d$ for this class were known for monotonicity\nand $c$-Lipschitz properties over uniformly distributed domains. \\medskip\n  Our results imply testers that give the same upper bound for arbitrary\nproduct distributions as the hitherto known testers, which use uniformly\nrandomly chosen samples from $[n]^d$, for monotonicity and Lipschitz testing.\nAlso, our testers are \\emph{optimal} for a large class of bounded derivative\nproperties, that includes $(\\alpha, \\beta)$-generalized Lipschitz property,\nover uniform distributions. Infact, each edge in $[n]^d$ is allowed to have\nit's own left and right Lipschitz constants. The time complexity is \\emph{same}\nfor arbitrary product distributions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 16:23:20 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 19:55:55 GMT"}, {"version": "v3", "created": "Mon, 28 Apr 2014 00:15:03 GMT"}, {"version": "v4", "created": "Wed, 30 Apr 2014 20:04:33 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Dixit", "Kashyap", ""]]}, {"id": "1404.5548", "submitter": "Martin B\\\"ohm", "authors": "Martin B\\\"ohm, Ji\\v{r}\\'i Sgall and Pavel Vesel\\'y", "title": "Online Colored Bin Packing", "comments": "Added lower bound of 2.5 for at least three colors, expanded some\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Colored Bin Packing problem a sequence of items of sizes up to $1$\narrives to be packed into bins of unit capacity. Each item has one of $c\\geq 2$\ncolors and an additional constraint is that we cannot pack two items of the\nsame color next to each other in the same bin. The objective is to minimize the\nnumber of bins.\n  In the important special case when all items have size zero, we characterize\nthe optimal value to be equal to color discrepancy. As our main result, we give\nan (asymptotically) 1.5-competitive algorithm which is optimal. In fact, the\nalgorithm always uses at most $\\lceil1.5\\cdot OPT\\rceil$ bins and we show a\nmatching lower bound of $\\lceil1.5\\cdot OPT\\rceil$ for any value of $OPT\\geq\n2$. In particular, the absolute ratio of our algorithm is $5/3$ and this is\noptimal.\n  For items of unrestricted sizes we give an asymptotically $3.5$-competitive\nalgorithm. When the items have sizes at most $1/d$ for a real $d \\geq 2$ the\nasymptotic competitive ratio is $1.5+d/(d-1)$. We also show that classical\nalgorithms First Fit, Best Fit and Worst Fit are not constant competitive,\nwhich holds already for three colors and small items.\n  In the case of two colors---the Black and White Bin Packing problem---we\nprove that all Any Fit algorithms have absolute competitive ratio $3$. When the\nitems have sizes at most $1/d$ for a real $d \\geq 2$ we show that the Worst Fit\nalgorithm is absolutely $(1+d/(d-1))$-competitive.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 16:34:50 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 21:09:22 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1404.5566", "submitter": "Akanksha Agrawal", "authors": "Akanksha Agrawal, Sathish Govindarajan, Neeldhara Misra", "title": "Vertex Cover Gets Faster and Harder on Low Degree Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding an optimal vertex cover in a graph is a classic\nNP-complete problem, and is a special case of the hitting set question. On the\nother hand, the hitting set problem, when asked in the context of induced\ngeometric objects, often turns out to be exactly the vertex cover problem on\nrestricted classes of graphs. In this work we explore a particular instance of\nsuch a phenomenon. We consider the problem of hitting all axis-parallel slabs\ninduced by a point set P, and show that it is equivalent to the problem of\nfinding a vertex cover on a graph whose edge set is the union of two\nHamiltonian Paths. We show the latter problem to be NP-complete, and we also\ngive an algorithm to find a vertex cover of size at most k, on graphs of\nmaximum degree four, whose running time is 1.2637^k n^O(1).\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 19:16:51 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 14:39:17 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Govindarajan", "Sathish", ""], ["Misra", "Neeldhara", ""]]}, {"id": "1404.5568", "submitter": "Gilad Tsur", "authors": "Dana Ron and Gilad Tsur", "title": "The Power of an Example: Hidden Set Size Approximation Using Group\n  Queries and Conditional Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a basic problem of approximating the size of an unknown set $S$ in a\nknown universe $U$. We consider two versions of the problem. In both versions\nthe algorithm can specify subsets $T\\subseteq U$. In the first version, which\nwe refer to as the group query or subset query version, the algorithm is told\nwhether $T\\cap S$ is non-empty. In the second version, which we refer to as the\nsubset sampling version, if $T\\cap S$ is non-empty, then the algorithm receives\na uniformly selected element from $T\\cap S$. We study the difference between\nthese two versions under different conditions on the subsets that the algorithm\nmay query/sample, and in both the case that the algorithm is adaptive and the\ncase where it is non-adaptive. In particular we focus on a natural family of\nallowed subsets, which correspond to intervals, as well as variants of this\nfamily.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 07:20:02 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Ron", "Dana", ""], ["Tsur", "Gilad", ""]]}, {"id": "1404.5569", "submitter": "Martin B\\\"ohm", "authors": "Martin B\\\"ohm, Ji\\v{r}\\'i Sgall, Rob van Stee, Pavel Vesel\\'y", "title": "Online Bin Stretching with Three Bins", "comments": "Preprint of a journal version. See version 2 for the conference\n  paper. Conference paper split into two journal submissions; see\n  arXiv:1601.08111", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Bin Stretching is a semi-online variant of bin packing in which the\nalgorithm has to use the same number of bins as an optimal packing, but is\nallowed to slightly overpack the bins. The goal is to minimize the amount of\noverpacking, i.e., the maximum size packed into any bin.\n  We give an algorithm for Online Bin Stretching with a stretching factor of\n$11/8 = 1.375$ for three bins. Additionally, we present a lower bound of $45/33\n= 1.\\overline{36}$ for Online Bin Stretching on three bins and a lower bound of\n$19/14$ for four and five bins that were discovered using a computer search.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 17:40:38 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 01:55:33 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2016 15:52:02 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["van Stee", "Rob", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1404.5660", "submitter": "Richard Cole", "authors": "Richard Cole, Howard Karloff", "title": "Fast Algorithms for Constructing Maximum Entropy Summary Trees", "comments": "17 pages, 4 figures. Extended version of paper appearing in ICALP\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karloff? and Shirley recently proposed summary trees as a new way to\nvisualize large rooted trees (Eurovis 2013) and gave algorithms for generating\na maximum-entropy k-node summary tree of an input n-node rooted tree. However,\nthe algorithm generating optimal summary trees was only pseudo-polynomial (and\nworked only for integral weights); the authors left open existence of a\nolynomial-time algorithm. In addition, the authors provided an additive\napproximation algorithm and a greedy heuristic, both working on real weights.\nThis paper shows how to construct maximum entropy k-node summary trees in time\nO(k^2 n + n log n) for real weights (indeed, as small as the time bound for the\ngreedy heuristic given previously); how to speed up the approximation algorithm\nso that it runs in time O(n + (k^4/eps?) log(k/eps?)), and how to speed up the\ngreedy algorithm so as to run in time O(kn + n log n). Altogether, these\nresults make summary trees a much more practical tool than before.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 21:58:17 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Cole", "Richard", ""], ["Karloff", "Howard", ""]]}, {"id": "1404.5692", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Parikshit Shah, Stephen Wright", "title": "Forward - Backward Greedy Algorithms for Atomic Norm Regularization", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2461515", "report-no": null, "categories": "cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many signal processing applications, the aim is to reconstruct a signal\nthat has a simple representation with respect to a certain basis or frame.\nFundamental elements of the basis known as \"atoms\" allow us to define \"atomic\nnorms\" that can be used to formulate convex regularizations for the\nreconstruction problem. Efficient algorithms are available to solve these\nformulations in certain special cases, but an approach that works well for\ngeneral atomic norms, both in terms of speed and reconstruction accuracy,\nremains to be found. This paper describes an optimization algorithm called\nCoGEnT that produces solutions with succinct atomic representations for\nreconstruction problems, generally formulated with atomic-norm constraints.\nCoGEnT combines a greedy selection scheme based on the conditional gradient\napproach with a backward (or \"truncation\") step that exploits the quadratic\nnature of the objective to reduce the basis size. We establish convergence\nproperties and validate the algorithm via extensive numerical experiments on a\nsuite of signal processing applications. Our algorithm and analysis also allow\nfor inexact forward steps and for occasional enhancements of the current\nrepresentation to be performed. CoGEnT can outperform the basic conditional\ngradient method, and indeed many methods that are tailored to specific\napplications, when the enhancement and truncation steps are defined\nappropriately. We also introduce several novel applications that are enabled by\nthe atomic-norm framework, including tensor completion, moment problems in\nsignal processing, and graph deconvolution.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 03:31:45 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 01:33:16 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Rao", "Nikhil", ""], ["Shah", "Parikshit", ""], ["Wright", "Stephen", ""]]}, {"id": "1404.5743", "submitter": "Yaoyu Wang", "authors": "Yaoyu Wang and Yitong Yin", "title": "Certificates in Data Structures", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study certificates in static data structures. In the cell-probe model,\ncertificates are the cell probes which can uniquely identify the answer to the\nquery. As a natural notion of nondeterministic cell probes, lower bounds for\ncertificates in data structures immediately imply deterministic cell-probe\nlower bounds. In spite of this extra power brought by nondeterminism, we prove\nthat two widely used tools for cell-probe lower bounds: richness lemma of\nMiltersen et al. and direct-sum richness lemma of Patrascu and Thorup, both\nhold for certificates in data structures with even better parameters. Applying\nthese lemmas and adopting existing reductions, we obtain certificate lower\nbounds for a variety of static data structure problems. These certificate lower\nbounds are at least as good as the highest known cell-probe lower bounds for\nthe respective problems. In particular, for approximate near neighbor (ANN)\nproblem in Hamming distance, our lower bound improves the state of the art.\nWhen the space is strictly linear, our lower bound for ANN in d-dimensional\nHamming space becomes t=Omega(d), which along with the recent breakthrough for\npolynomial evaluation of Larsen, are the only two t=Omega(d) lower bounds ever\nproved for any problems in the cell-probe model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 08:42:30 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 11:59:13 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Wang", "Yaoyu", ""], ["Yin", "Yitong", ""]]}, {"id": "1404.5892", "submitter": "Therese Biedl", "authors": "Therese Biedl", "title": "Straightening out planar poly-line drawings", "comments": "The main result turns out to be known (Pach & Toth, J. Graph Theory\n  2004, http://onlinelibrary.wiley.com/doi/10.1002/jgt.10168/pdf )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any $y$-monotone poly-line drawing can be straightened out while\nmaintaining $y$-coordinates and height. The width may increase much, but we\nalso show that on some graphs exponential width is required if we do not want\nto increase the height. Likewise $y$-monotonicity is required: there are\npoly-line drawings (not $y$-monotone) that cannot be straightened out while\nmaintaining the height. We give some applications of our result.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 17:04:37 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 12:11:39 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Biedl", "Therese", ""]]}, {"id": "1404.5996", "submitter": "Lalla Mouatadid", "authors": "Ekkehard K\\\"ohler and Lalla Mouatadid", "title": "Linear Time LexDFS on Cocomparability Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexicographic depth first search (LexDFS) is a graph search protocol which\nhas already proved to be a powerful tool on cocomparability graphs.\nCocomparability graphs have been well studied by investigating their\ncomplements (comparability graphs) and their corresponding posets. Recently\nhowever LexDFS has led to a number of elegant polynomial and near linear time\nalgorithms on cocomparability graphs when used as a preprocessing step [2, 3,\n11]. The nonlinear runtime of some of these results is a consequence of\ncomplexity of this preprocessing step. We present the first linear time\nalgorithm to compute a LexDFS cocomparability ordering, therefore answering a\nproblem raised in [2] and helping achieve the first linear time algorithms for\nthe minimum path cover problem, and thus the Hamilton path problem, the maximum\nindependent set problem and the minimum clique cover for this graph family.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 22:32:11 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["K\u00f6hler", "Ekkehard", ""], ["Mouatadid", "Lalla", ""]]}, {"id": "1404.6003", "submitter": "Aaron Roth", "authors": "Arpita Ghosh and Katrina Ligett and Aaron Roth and Grant Schoenebeck", "title": "Buying Private Data without Verification", "comments": "Appears in EC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing a survey to aggregate non-verifiable\ninformation from a privacy-sensitive population: an analyst wants to compute\nsome aggregate statistic from the private bits held by each member of a\npopulation, but cannot verify the correctness of the bits reported by\nparticipants in his survey. Individuals in the population are strategic agents\nwith a cost for privacy, \\ie, they not only account for the payments they\nexpect to receive from the mechanism, but also their privacy costs from any\ninformation revealed about them by the mechanism's outcome---the computed\nstatistic as well as the payments---to determine their utilities. How can the\nanalyst design payments to obtain an accurate estimate of the population\nstatistic when individuals strategically decide both whether to participate and\nwhether to truthfully report their sensitive information?\n  We design a differentially private peer-prediction mechanism that supports\naccurate estimation of the population statistic as a Bayes-Nash equilibrium in\nsettings where agents have explicit preferences for privacy. The mechanism\nrequires knowledge of the marginal prior distribution on bits $b_i$, but does\nnot need full knowledge of the marginal distribution on the costs $c_i$,\ninstead requiring only an approximate upper bound. Our mechanism guarantees\n$\\epsilon$-differential privacy to each agent $i$ against any adversary who can\nobserve the statistical estimate output by the mechanism, as well as the\npayments made to the $n-1$ other agents $j\\neq i$. Finally, we show that with\nslightly more structured assumptions on the privacy cost functions of each\nagent, the cost of running the survey goes to $0$ as the number of agents\ndiverges.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 00:30:35 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Ghosh", "Arpita", ""], ["Ligett", "Katrina", ""], ["Roth", "Aaron", ""], ["Schoenebeck", "Grant", ""]]}, {"id": "1404.6175", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini and Giordano Da Lozzo", "title": "Deepening the Relationship between SEFE and C-Planarity", "comments": "8 pages, 3 figures, Extended version of 'SEFE = C-Planarity?' (9th\n  International Colloquium on Graph Theory and Combinatorics - ICGT 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deepen the understanding of the connection between two\nlong-standing Graph Drawing open problems, that is, Simultaneous Embedding with\nFixed Edges (SEFE) and Clustered Planarity (C-PLANARITY). In his GD'12 paper\nMarcus Schaefer presented a reduction from C-PLANARITY to SEFE of two planar\ngraphs (SEFE-2). We prove that a reduction exists also in the opposite\ndirection, if we consider instances of SEFE-2 in which the intersection graph\nis connected. We pose as an open question whether the two problems are\npolynomial-time equivalent.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 16:46:56 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""]]}, {"id": "1404.6216", "submitter": "Ping Li", "authors": "Ping Li", "title": "CoRE Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"CoRE kernel\" stands for correlation-resemblance kernel. In many\napplications (e.g., vision), the data are often high-dimensional, sparse, and\nnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binary\nsparse data and demonstrate the effectiveness of the new kernels through a\nclassification experiment. CoRE kernels are simple with no tuning parameters.\nHowever, training nonlinear kernel SVM can be (very) costly in time and memory\nand may not be suitable for truly large-scale industrial applications (e.g.\nsearch). In order to make the proposed CoRE kernels more practical, we develop\nbasic probabilistic hashing algorithms which transform nonlinear kernels into\nlinear kernels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:35:37 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1404.6287", "submitter": "Arman Yousefi", "authors": "Arman Yousefi and Rafail Ostrovsky", "title": "Improved Approximation Algorithms for Earth-Mover Distance in Data\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two multisets $S$ and $T$ of points in $[\\Delta]^2$, such that $|S| =\n|T|= n$, the earth-mover distance (EMD) between $S$ and $T$ is the minimum cost\nof a perfect bipartite matching with edges between points in $S$ and $T$, i.e.,\n$EMD(S,T) = \\min_{\\pi:S\\rightarrow T}\\sum_{a\\in S}||a-\\pi(a)||_1$, where $\\pi$\nranges over all one-to-one mappings. The sketching complexity of approximating\nearth-mover distance in the two-dimensional grid is mentioned as one of the\nopen problems in the literature. We give two algorithms for computing EMD\nbetween two multi-sets when the number of distinct points in one set is a small\nvalue $k=\\log^{O(1)}(\\Delta n)$. Our first algorithm gives a\n$(1+\\epsilon)$-approximation using $O(k\\epsilon^{-2}\\log^{4}n)$ space and works\nonly in the insertion-only model. The second algorithm gives a\n$O(\\min(k^3,\\log\\Delta))$-approximation using\n$O(\\log^{3}\\Delta\\cdot\\log\\log\\Delta\\cdot\\log n)$-space in the turnstile model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 22:55:53 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Yousefi", "Arman", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "1404.6399", "submitter": "Amer Mouawad", "authors": "Faisal N. Abu-Khzam and Karim A. Jahed and Amer E. Mouawad", "title": "A Hybrid Graph Representation for Exact Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many exact search algorithms for NP-hard graph problems adopt the old\nDavis-Putman branch-and-reduce paradigm. The performance of these algorithms\noften suffers from the increasing number of graph modifications, such as\nvertex/edge deletions, that reduce the problem instance and have to be \"taken\nback\" frequently during the search process. We investigate practical\nimplementation-based aspects of exact graph algorithms by providing a simple\nhybrid graph representation that trades space for time to address the said\ntake-back challenge. Experiments on three well studied problems show consistent\nsignificant improvements over classical methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 11:34:55 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Jahed", "Karim A.", ""], ["Mouawad", "Amer E.", ""]]}, {"id": "1404.6451", "submitter": "Krasimir Yordzhev", "authors": "Krasimir Yordzhev", "title": "On an Algorithm for Isomorphism-Free Generations of Combinatorial\n  Objects", "comments": null, "journal-ref": "International Journal of Emerging Trends & Technology in Computer\n  Science (IJETTCS), Vol. 2, No. 6 (2013) 215-220", "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the work are defined the concepts semi-canonical and canonical binary\nmatrix. What is described is an algorithm solving the combinatorial problem for\nfinding the semi-canonical matrices in the set \\Lambda_n^k consisting of all\nn\\times n binary matrices having exactly k 1's in every row and every column\nwithout perambulating all elements. In the described algorithm bitwise\noperations are substantially used. In this way it becomes easier to find the\nsolution to the problem for receiving one representative from every equivalence\nclass regarding the introduced in the article equivalence relation in the set\n\\Lambda_n^k . The last problem is equivalent to the problem for finding all\ncanonical matrices in \\Lambda_n^k .\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 15:14:48 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Yordzhev", "Krasimir", ""]]}, {"id": "1404.6502", "submitter": "Abhinav Srivastav", "authors": "Abhinav Srivastav and Denis Trystram", "title": "Total stretch minimization on single and identical parallel machines", "comments": "Submitted to ISAAC, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of scheduling $n$ jobs with release dates\non both single and identical parallel machines. We measure the quality of\nservice provided to each job by its stretch, which is defined as the ratio of\nits response time to processing time. Our objective is to schedule these jobs\nnon-preemptively so as to minimize sum stretch. So far, there have been very\nfew results for sum stretch minimization especially for the non-preemptive\ncase. For the preemptive version, the Shortest remaining processing time (SRPT)\nalgorithm is known to give $2$-competitive for sum stretch on single machine\nwhile its is $13$-competitive on identical parallel machines. Leonardi and\nKellerer provided the strong lower bound for the more general problem of\n\\textit{sum (weighted) flow time} in single machine and identical parallel\nmachines, respectively . Therefore, we study this problem with some additional\nassumptions and present two new competitive ratio for existing algorithms. We\nshow that the Shortest processing time (SPT) algorithm is $\\Delta -\n\\frac{1}{\\Delta}+1$-competitive for non-preemptive sum stretch minimization on\nsingle machine and it is $\\Delta - \\frac{1}{\\Delta}+ \\frac{3}{2} -\\frac{1}{2m}$\non $m$ identical parallel machines, where $\\Delta$ is the upper bound on the\nratio between the maximum and the minimum processing time of the jobs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 18:58:03 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 09:29:17 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Srivastav", "Abhinav", ""], ["Trystram", "Denis", ""]]}, {"id": "1404.6694", "submitter": "Thibaut Vidal", "authors": "Thibaut Vidal, Patrick Jaillet, Nelson Maculan", "title": "A Decomposition Algorithm for Nested Resource Allocation Problems", "comments": "Working Paper -- MIT, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an exact polynomial algorithm for a resource allocation problem\nwith convex costs and constraints on partial sums of resource consumptions, in\nthe presence of either continuous or integer variables. No assumption of strict\nconvexity or differentiability is needed. The method solves a hierarchy of\nresource allocation subproblems, whose solutions are used to convert\nconstraints on sums of resources into bounds for separate variables at higher\nlevels. The resulting time complexity for the integer problem is $O(n \\log m\n\\log (B/n))$, and the complexity of obtaining an $\\epsilon$-approximate\nsolution for the continuous case is $O(n \\log m \\log (B/\\epsilon))$, $n$ being\nthe number of variables, $m$ the number of ascending constraints (such that $m\n< n$), $\\epsilon$ a desired precision, and $B$ the total resource. This\nalgorithm attains the best-known complexity when $m = n$, and improves it when\n$\\log m = o(\\log n)$. Extensive experimental analyses are conducted with four\nrecent algorithms on various continuous problems issued from theory and\npractice. The proposed method achieves a higher performance than previous\nalgorithms, addressing all problems with up to one million variables in less\nthan one minute on a modern computer.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 23:14:35 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Vidal", "Thibaut", ""], ["Jaillet", "Patrick", ""], ["Maculan", "Nelson", ""]]}, {"id": "1404.6724", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard and Mikkel Thorup", "title": "Approximately Minwise Independence with Twisted Tabulation", "comments": "To appear in Proceedings of SWAT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random hash function $h$ is $\\varepsilon$-minwise if for any set $S$,\n$|S|=n$, and element $x\\in S$, $\\Pr[h(x)=\\min h(S)]=(1\\pm\\varepsilon)/n$.\nMinwise hash functions with low bias $\\varepsilon$ have widespread applications\nwithin similarity estimation.\n  Hashing from a universe $[u]$, the twisted tabulation hashing of\nP\\v{a}tra\\c{s}cu and Thorup [SODA'13] makes $c=O(1)$ lookups in tables of size\n$u^{1/c}$. Twisted tabulation was invented to get good concentration for\nhashing based sampling. Here we show that twisted tabulation yields $\\tilde\nO(1/u^{1/c})$-minwise hashing.\n  In the classic independence paradigm of Wegman and Carter [FOCS'79] $\\tilde\nO(1/u^{1/c})$-minwise hashing requires $\\Omega(\\log u)$-independence [Indyk\nSODA'99]. P\\v{a}tra\\c{s}cu and Thorup [STOC'11] had shown that simple\ntabulation, using same space and lookups yields $\\tilde O(1/n^{1/c})$-minwise\nindependence, which is good for large sets, but useless for small sets. Our\nanalysis uses some of the same methods, but is much cleaner bypassing a\ncomplicated induction argument.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 07:59:38 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 09:12:24 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1404.6727", "submitter": "Sam Chiu-wai Wong", "authors": "MohammadHossein Bateni, Jon Feldman, Vahab Mirrokni, Sam Chiu-wai Wong", "title": "Multiplicative Bidding in Online Advertising", "comments": "25 pages; accepted to EC'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate the study of the multiplicative bidding language\nadopted by major Internet search companies. In multiplicative bidding, the\neffective bid on a particular search auction is the product of a base bid and\nbid adjustments that are dependent on features of the search (for example, the\ngeographic location of the user, or the platform on which the search is\nconducted). We consider the task faced by the advertiser when setting these bid\nadjustments, and establish a foundational optimization problem that captures\nthe core difficulty of bidding under this language. We give matching\nalgorithmic and approximation hardness results for this problem; these results\nare against an information-theoretic bound, and thus have implications on the\npower of the multiplicative bidding language itself. Inspired by empirical\nstudies of search engine price data, we then codify the relevant restrictions\nof the problem, and give further algorithmic and hardness results. Our main\ntechnical contribution is an $O(\\log n)$-approximation for the case of\nmultiplicative prices and monotone values. We also provide empirical\nvalidations of our problem restrictions, and test our algorithms on real data\nagainst natural benchmarks. Our experiments show that they perform favorably\ncompared with the baseline.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 08:25:12 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Bateni", "MohammadHossein", ""], ["Feldman", "Jon", ""], ["Mirrokni", "Vahab", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1404.6763", "submitter": "Yuval Emek", "authors": "Yuval Emek and Adi Rosen", "title": "Semi-Streaming Set Cover", "comments": "Full version of the extended abstract that will appear in Proceedings\n  of ICALP 2014 track A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the set cover problem under the semi-streaming model. The\nunderlying set system is formalized in terms of a hypergraph $G = (V, E)$ whose\nedges arrive one-by-one and the goal is to construct an edge cover $F \\subseteq\nE$ with the objective of minimizing the cardinality (or cost in the weighted\ncase) of $F$. We consider a parameterized relaxation of this problem, where\ngiven some $0 \\leq \\epsilon < 1$, the goal is to construct an edge $(1 -\n\\epsilon)$-cover, namely, a subset of edges incident to all but an\n$\\epsilon$-fraction of the vertices (or their benefit in the weighted case).\nThe key limitation imposed on the algorithm is that its space is limited to\n(poly)logarithmically many bits per vertex.\n  Our main result is an asymptotically tight trade-off between $\\epsilon$ and\nthe approximation ratio: We design a semi-streaming algorithm that on input\ngraph $G$, constructs a succinct data structure $\\mathcal{D}$ such that for\nevery $0 \\leq \\epsilon < 1$, an edge $(1 - \\epsilon)$-cover that approximates\nthe optimal edge \\mbox{($1$-)cover} within a factor of $f(\\epsilon, n)$ can be\nextracted from $\\mathcal{D}$ (efficiently and with no additional space\nrequirements), where \\[ f(\\epsilon, n) = \\left\\{ \\begin{array}{ll} O (1 /\n\\epsilon), & \\text{if } \\epsilon > 1 / \\sqrt{n} \\\\ O (\\sqrt{n}), &\n\\text{otherwise} \\end{array} \\right. \\, . \\] In particular for the traditional\nset cover problem we obtain an $O(\\sqrt{n})$-approximation. This algorithm is\nproved to be best possible by establishing a family (parameterized by\n$\\epsilon$) of matching lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:11:52 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 18:39:32 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Emek", "Yuval", ""], ["Rosen", "Adi", ""]]}, {"id": "1404.6835", "submitter": "Parter Merav", "authors": "Merav Parter", "title": "Bypassing Erd\\H{o}s' Girth Conjecture: Hybrid Stretch and Sourcewise\n  Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(\\alpha,\\beta)$-spanner of an $n$-vertex graph $G=(V,E)$ is a subgraph\n$H$ of $G$ satisfying that $dist(u, v, H) \\leq \\alpha \\cdot dist(u, v,\nG)+\\beta$ for every pair $(u, v)\\in V \\times V$, where $dist(u,v,G')$ denotes\nthe distance between $u$ and $v$ in $G' \\subseteq G$. It is known that for\nevery integer $k \\geq 1$, every graph $G$ has a polynomially constructible\n$(2k-1,0)$-spanner of size $O(n^{1+1/k})$. This size-stretch bound is\nessentially optimal by the girth conjecture. It is therefore intriguing to ask\nif one can \"bypass\" the conjecture by settling for a multiplicative stretch of\n$2k-1$ only for \\emph{neighboring} vertex pairs, while maintaining a strictly\n\\emph{better} multiplicative stretch for the rest of the pairs. We answer this\nquestion in the affirmative and introduce the notion of \\emph{$k$-hybrid\nspanners}, in which non neighboring vertex pairs enjoy a \\emph{multiplicative}\n$k$-stretch and the neighboring vertex pairs enjoy a \\emph{multiplicative}\n$(2k-1)$ stretch (hence, tight by the conjecture). We show that for every\nunweighted $n$-vertex graph $G$ with $m$ edges, there is a (polynomially\nconstructible) $k$-hybrid spanner with $O(k^2 \\cdot n^{1+1/k})$ edges. \\indent\nAn alternative natural approach to bypass the girth conjecture is to allow\nourself to take care only of a subset of pairs $S \\times V$ for a given subset\nof vertices $S \\subseteq V$ referred to here as \\emph{sources}. Spanners in\nwhich the distances in $S \\times V$ are bounded are referred to as\n\\emph{sourcewise spanners}. Several constructions for this variant are provided\n(e.g., multiplicative sourcewise spanners, additive sourcewise spanners and\nmore).\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 21:55:35 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Parter", "Merav", ""]]}, {"id": "1404.6850", "submitter": "Konstantin Makarychev", "authors": "Konstantin Makarychev and Debmalya Panigrahi", "title": "Precedence-constrained Scheduling of Malleable Jobs with Preemption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling jobs with precedence constraints on a set of identical machines to\nminimize the total processing time (makespan) is a fundamental problem in\ncombinatorial optimization. In practical settings such as cloud computing, jobs\nare often malleable, i.e., can be processed on multiple machines\nsimultaneously. The instantaneous processing rate of a job is a non-decreasing\nfunction of the number of machines assigned to it (we call it the processing\nfunction). Previous research has focused on practically relevant concave\nprocessing functions, which obey the law of diminishing utility and generalize\nthe classical (non-malleable) problem. Our main result is a\n$(2+\\epsilon)$-approximation algorithm for concave processing functions (for\nany $\\epsilon > 0$), which is the best possible under complexity theoretic\nassumptions. The approximation ratio improves to $(1 + \\epsilon)$ for the\ninteresting and practically relevant special case of power functions, i.e.,\n$p_j(z) = c_j \\cdot z^{\\gamma}$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 01:12:38 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1404.6890", "submitter": "Joydeep Banerjee", "authors": "Joydeep Banerjee, Arun Das and Arunabha Sen", "title": "Analysis of d-Hop Dominating Set Problem for Directed Graph with\n  Indegree Bounded by One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient communication between nodes in ad-hoc networks can be established\nthrough repeated cluster formations with designated \\textit{cluster-heads}. In\nthis context minimum d-hop dominating set problem was introduced for cluster\nformation in ad-hoc networks and is proved to be NP-complete. Hence, an exact\nsolution to this problem for certain subclass of graphs (representing an ad-hoc\nnetwork) can be beneficial. In this short paper we perform computational\ncomplexity analysis of minimum d-hop dominating set problem for directed graphs\nwith in-degree bounded by $1$. The optimum solution of the problem can be found\npolynomially by exploiting certain properties of the graph under consideration.\nFor a digraph $G_{D}=(V_D,E_D)$ an $\\mathcal{O}(|V_D|^2)$ solution is provided\nto the problem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 07:51:54 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 19:24:15 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Banerjee", "Joydeep", ""], ["Das", "Arun", ""], ["Sen", "Arunabha", ""]]}, {"id": "1404.7006", "submitter": "Erik Jan van Leeuwen", "authors": "Karl Bringmann and Danny Hermelin and Matthias Mnich and Erik Jan van\n  Leeuwen", "title": "Parameterized Complexity Dichotomy for Steiner Multicut", "comments": "As submitted to journal. This version also adds a proof of\n  fixed-parameter tractability for parameter k+t using the technique of\n  randomized contractions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Steiner Multicut problem asks, given an undirected graph G, terminals\nsets T1,...,Tt $\\subseteq$ V(G) of size at most p, and an integer k, whether\nthere is a set S of at most k edges or nodes s.t. of each set Ti at least one\npair of terminals is in different connected components of G \\ S. This problem\ngeneralizes several graph cut problems, in particular the Multicut problem (the\ncase p = 2), which is fixed-parameter tractable for the parameter k [Marx and\nRazgon, Bousquet et al., STOC 2011].\n  We provide a dichotomy of the parameterized complexity of Steiner Multicut.\nThat is, for any combination of k, t, p, and the treewidth tw(G) as constant,\nparameter, or unbounded, and for all versions of the problem (edge deletion and\nnode deletion with and without deletable terminals), we prove either that the\nproblem is fixed-parameter tractable or that the problem is hard (W[1]-hard or\neven (para-)NP-complete). We highlight that:\n  - The edge deletion version of Steiner Multicut is fixed-parameter tractable\nfor the parameter k+t on general graphs (but has no polynomial kernel, even on\ntrees). We present two proofs: one using the randomized contractions technique\nof Chitnis et al, and one relying on new structural lemmas that decompose the\nSteiner cut into important separators and minimal s-t cuts.\n  - In contrast, both node deletion versions of Steiner Multicut are W[1]-hard\nfor the parameter k+t on general graphs.\n  - All versions of Steiner Multicut are W[1]-hard for the parameter k, even\nwhen p=3 and the graph is a tree plus one node. Hence, the results of Marx and\nRazgon, and Bousquet et al. do not generalize to Steiner Multicut.\n  Since we allow k, t, p, and tw(G) to be any constants, our characterization\nincludes a dichotomy for Steiner Multicut on trees (for tw(G) = 1), and a\npolynomial time versus NP-hardness dichotomy (by restricting k,t,p,tw(G) to\nconstant or unbounded).\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 14:41:40 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 14:00:38 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Bringmann", "Karl", ""], ["Hermelin", "Danny", ""], ["Mnich", "Matthias", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1404.7055", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Robert Nowak, and Sebastien Roch", "title": "Data Requirement for Phylogenetic Inference from Multiple Loci: A New\n  Distance Method", "comments": "19 pages, 2 figures. Preliminary version to appear in IEEE ISIT 2014.\n  Added acknowledgements and made the proof of the \"equality\" part of Theorem 3\n  explicit in Appendix C", "journal-ref": null, "doi": "10.1109/TCBB.2014.2361685", "report-no": null, "categories": "q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the evolutionary history of a set of\nspecies (phylogeny or species tree) from several genes. It is known that the\nevolutionary history of individual genes (gene trees) might be topologically\ndistinct from each other and from the underlying species tree, possibly\nconfounding phylogenetic analysis. A further complication in practice is that\none has to estimate gene trees from molecular sequences of finite length. We\nprovide the first full data-requirement analysis of a species tree\nreconstruction method that takes into account estimation errors at the gene\nlevel. Under that criterion, we also devise a novel reconstruction algorithm\nthat provably improves over all previous methods in a regime of interest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 16:54:20 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 09:53:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Nowak", "Robert", ""], ["Roch", "Sebastien", ""]]}, {"id": "1404.7060", "submitter": "Mitsuru Kusumoto", "authors": "Mitsuru Kusumoto and Yuichi Yoshida", "title": "Testing Forest-Isomorphism in the Adjacency List Model", "comments": "ICALP 2014 to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing if two input forests are isomorphic or are\nfar from being so. An algorithm is called an $\\varepsilon$-tester for\nforest-isomorphism if given an oracle access to two forests $G$ and $H$ in the\nadjacency list model, with high probability, accepts if $G$ and $H$ are\nisomorphic and rejects if we must modify at least $\\varepsilon n$ edges to make\n$G$ isomorphic to $H$. We show an $\\varepsilon$-tester for forest-isomorphism\nwith a query complexity $\\mathrm{polylog}(n)$ and a lower bound of\n$\\Omega(\\sqrt{\\log{n}})$. Further, with the aid of the tester, we show that\nevery graph property is testable in the adjacency list model with\n$\\mathrm{polylog}(n)$ queries if the input graph is a forest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 17:10:35 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Kusumoto", "Mitsuru", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1404.7203", "submitter": "Mert Pilanci", "authors": "Mert Pilanci, Martin J. Wainwright", "title": "Randomized Sketches of Convex Programs with Sharp Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projection (RP) is a classical technique for reducing storage and\ncomputational costs. We analyze RP-based approximations of convex programs, in\nwhich the original optimization problem is approximated by the solution of a\nlower-dimensional problem. Such dimensionality reduction is essential in\ncomputation-limited settings, since the complexity of general convex\nprogramming can be quite high (e.g., cubic for quadratic programs, and\nsubstantially higher for semidefinite programs). In addition to computational\nsavings, random projection is also useful for reducing memory usage, and has\nuseful properties for privacy-sensitive optimization. We prove that the\napproximation ratio of this procedure can be bounded in terms of the geometry\nof constraint set. For a broad class of random projections, including those\nbased on various sub-Gaussian distributions as well as randomized Hadamard and\nFourier transforms, the data matrix defining the cost function can be projected\ndown to the statistical dimension of the tangent cone of the constraints at the\noriginal solution, which is often substantially smaller than the original\ndimension. We illustrate consequences of our theory for various cases,\nincluding unconstrained and $\\ell_1$-constrained least squares, support vector\nmachines, low-rank matrix estimation, and discuss implications on\nprivacy-sensitive optimization and some connections with de-noising and\ncompressed sensing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 00:57:59 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1404.7259", "submitter": "Grzegorz Gutowski", "authors": "Grzegorz Gutowski, Jakub Kozik, Piotr Micek and Xuding Zhu", "title": "Lower bounds for on-line graph colorings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-13075-0_40", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two strategies for Presenter in on-line graph coloring games. The\nfirst one constructs bipartite graphs and forces any on-line coloring algorithm\nto use $2\\log_2 n - 10$ colors, where $n$ is the number of vertices in the\nconstructed graph. This is best possible up to an additive constant. The second\nstrategy constructs graphs that contain neither $C_3$ nor $C_5$ as a subgraph\nand forces $\\Omega(\\frac{n}{\\log n}^\\frac{1}{3})$ colors. The best known\non-line coloring algorithm for these graphs uses $O(n^{\\frac{1}{2}})$ colors.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 07:10:15 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 11:19:51 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Gutowski", "Grzegorz", ""], ["Kozik", "Jakub", ""], ["Micek", "Piotr", ""], ["Zhu", "Xuding", ""]]}, {"id": "1404.7307", "submitter": "Keigo Oka", "authors": "Yoichi Iwata and Keigo Oka", "title": "Fast Dynamic Graph Algorithms for Parameterized Problems", "comments": "SWAT 2014 to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully dynamic graph is a data structure that (1) supports edge insertions and\ndeletions and (2) answers problem specific queries. The time complexity of (1)\nand (2) are referred to as the update time and the query time respectively.\nThere are many researches on dynamic graphs whose update time and query time\nare $o(|G|)$, that is, sublinear in the graph size. However, almost all such\nresearches are for problems in P. In this paper, we investigate dynamic graphs\nfor NP-hard problems exploiting the notion of fixed parameter tractability\n(FPT).\n  We give dynamic graphs for Vertex Cover and Cluster Vertex Deletion\nparameterized by the solution size $k$. These dynamic graphs achieve almost the\nbest possible update time $O(\\mathrm{poly}(k)\\log n)$ and the query time\n$O(f(\\mathrm{poly}(k),k))$, where $f(n,k)$ is the time complexity of any static\ngraph algorithm for the problems. We obtain these results by dynamically\nmaintaining an approximate solution which can be used to construct a small\nproblem kernel. Exploiting the dynamic graph for Cluster Vertex Deletion, as a\ncorollary, we obtain a quasilinear-time (polynomial) kernelization algorithm\nfor Cluster Vertex Deletion. Until now, only quadratic time kernelization\nalgorithms are known for this problem.\n  We also give a dynamic graph for Chromatic Number parameterized by the\nsolution size of Cluster Vertex Deletion, and a dynamic graph for\nbounded-degree Feedback Vertex Set parameterized by the solution size. Assuming\nthe parameter is a constant, each dynamic graph can be updated in $O(\\log n)$\ntime and can compute a solution in $O(1)$ time. These results are obtained by\nanother approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 10:49:24 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Iwata", "Yoichi", ""], ["Oka", "Keigo", ""]]}, {"id": "1404.7325", "submitter": "Joan Boyar", "authors": "Joan Boyar and Faith Ellen", "title": "Tight Bounds for Restricted Grid Scheduling", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following online bin packing problem is considered: Items with integer\nsizes are given and variable sized bins arrive online. A bin must be used if\nthere is still an item remaining which fits in it when the bin arrives. The\ngoal is to minimize the total size of all the bins used. Previously, a lower\nbound of 5/4 on the competitive ratio of this problem was achieved using jobs\nof size S and 2S-1. For these item sizes and maximum bin size 4S-3, we obtain\nasymptotically matching upper and lower bounds, which vary depending on the\nratio of the number of small jobs to the number of large jobs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 12:03:39 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 07:10:13 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Boyar", "Joan", ""], ["Ellen", "Faith", ""]]}, {"id": "1404.7479", "submitter": "Robert Els\\\"asser", "authors": "Colin Cooper, Robert Els\\\"asser, Tomasz Radzik", "title": "The Power of Two Choices in Distributed Voting", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed voting is a fundamental topic in distributed computing. In pull\nvoting, in each step every vertex chooses a neighbour uniformly at random, and\nadopts its opinion. The voting is completed when all vertices hold the same\nopinion. On many graph classes including regular graphs, pull voting requires\n$\\Theta(n)$ expected steps to complete, even if initially there are only two\ndistinct opinions.\n  In this paper we consider a related process which we call two-sample voting:\nevery vertex chooses two random neighbours in each step. If the opinions of\nthese neighbours coincide, then the vertex revises its opinion according to the\nchosen sample. Otherwise, it keeps its own opinion. We consider the performance\nof this process in the case where two different opinions reside on vertices of\nsome (arbitrary) sets $A$ and $B$, respectively. Here, $|A| + |B| = n$ is the\nnumber of vertices of the graph.\n  We show that there is a constant $K$ such that if the initial imbalance\nbetween the two opinions is ?$\\nu_0 = (|A| - |B|)/n \\geq K \\sqrt{(1/d) +\n(d/n)}$, then with high probability two sample voting completes in a random $d$\nregular graph in $O(\\log n)$ steps and the initial majority opinion wins. We\nalso show the same performance for any regular graph, if $\\nu_0 \\geq K\n\\lambda_2$ where $\\lambda_2$ is the second largest eigenvalue of the transition\nmatrix. In the graphs we consider, standard pull voting requires $\\Omega(n)$\nsteps, and the minority can still win with probability $|B|/n$.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 19:46:19 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Cooper", "Colin", ""], ["Els\u00e4sser", "Robert", ""], ["Radzik", "Tomasz", ""]]}, {"id": "1404.7559", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari", "title": "Near-Optimal Distributed Approximation of Minimum-Weight Connected\n  Dominating Set", "comments": "An extended abstract version of this result appears in the\n  proceedings of 41st International Colloquium on Automata, Languages, and\n  Programming (ICALP 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a near-optimal distributed approximation algorithm for\nthe minimum-weight connected dominating set (MCDS) problem. The presented\nalgorithm finds an $O(\\log n)$ approximation in $\\tilde{O}(D+\\sqrt{n})$ rounds,\nwhere $D$ is the network diameter and $n$ is the number of nodes.\n  MCDS is a classical NP-hard problem and the achieved approximation factor\n$O(\\log n)$ is known to be optimal up to a constant factor, unless P=NP.\nFurthermore, the $\\tilde{O}(D+\\sqrt{n})$ round complexity is known to be\noptimal modulo logarithmic factors (for any approximation), following [Das\nSarma et al.---STOC'11].\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 00:30:51 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Ghaffari", "Mohsen", ""]]}, {"id": "1404.7569", "submitter": "Zhihan Gao", "authors": "Zhihan Gao", "title": "On the metric s-t path Traveling Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We study the metric $s$-$t$ path Traveling Salesman Problem (TSP). [An,\nKleinberg, and Shmoys, STOC 2012] improved on the long standing\n$\\frac{5}{3}$-approximation factor and presented an algorithm that achieves an\napproximation factor of $\\frac{1+\\sqrt{5}}{2}\\approx1.61803$. Later [Seb\\H{o},\nIPCO 2013] further improved the approximation factor to $\\frac{8}{5}$. We\npresent a simple, self-contained analysis that unifies both results; our main\ncontribution is a \\emph{unified correction vector}. We compare two different\nlinear programming (LP) relaxations of the $s$-$t$ path TSP, namely, the path\nversion of the Held-Karp LP relaxation for TSP and a weaker LP relaxation, and\nwe show that both LPs have the same (fractional) optimal value. Also, we show\nthat the minimum-cost of integral solutions of the two LPs are within a factor\nof $\\frac{3}{2}$ of each other. We prove that a half-integral solution of the\nstronger LP-relaxation of cost $c$ can be rounded to an integral solution of\ncost at most $\\frac{3}{2}c$. Additionally, we give a bad instance that presents\nobstructions to two obvious methods that aim for an approximation factor of\n$\\frac{3}{2}$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 01:24:22 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2015 22:11:51 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gao", "Zhihan", ""]]}, {"id": "1404.7610", "submitter": "Takeaki Uno", "authors": "Takeaki Uno, Hiroko Satoh", "title": "An Efficient Algorithm for Enumerating Chordless Cycles and Chordless\n  Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A chordless cycle (induced cycle) $C$ of a graph is a cycle without any\nchord, meaning that there is no edge outside the cycle connecting two vertices\nof the cycle. A chordless path is defined similarly. In this paper, we consider\nthe problems of enumerating chordless cycles/paths of a given graph $G=(V,E),$\nand propose algorithms taking $O(|E|)$ time for each chordless cycle/path. In\nthe existing studies, the problems had not been deeply studied in the\ntheoretical computer science area, and no output polynomial time algorithm has\nbeen proposed. Our experiments showed that the computation time of our\nalgorithms is constant per chordless cycle/path for non-dense random graphs and\nreal-world graphs. They also show that the number of chordless cycles is much\nsmaller than the number of cycles. We applied the algorithm to prediction of\nNMR (Nuclear Magnetic Resonance) spectra, and increased the accuracy of the\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 06:57:09 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Uno", "Takeaki", ""], ["Satoh", "Hiroko", ""]]}, {"id": "1404.7634", "submitter": "Arnaud Casteigts", "authors": "Matthieu Barjon, Arnaud Casteigts, Serge Chaumette, Colette Johnen,\n  Yessin M. Neggaz", "title": "Testing Temporal Connectivity in Sparse Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of testing whether a given dynamic graph is temporally\nconnected, {\\it i.e} a temporal path (also called a {\\em journey}) exists\nbetween all pairs of vertices. We consider a discrete version of the problem,\nwhere the topology is given as an evolving graph ${\\cal\nG}=\\{G_1,G_2,...,G_{k}\\}$ whose set of vertices is invariant and the set of\n(directed) edges varies over time. Two cases are studied, depending on whether\na single edge or an unlimited number of edges can be crossed in a same $G_i$\n(strict journeys {\\it vs} non-strict journeys).\n  In the case of {\\em strict} journeys, a number of existing algorithms\ndesigned for more general problems can be adapted. We adapt one of them to the\nabove formulation of the problem and characterize its running time complexity.\nThe parameters of interest are the length of the graph sequence $k=|{\\cal G}|$,\nthe maximum {\\em instant} density $\\mu=max(|E_i|)$, and the {\\em cumulated}\ndensity $m=|\\cup E_i|$. Our algorithm has a time complexity of $O(k\\mu n)$,\nwhere $n$ is the number of nodes. This complexity is compared to that of the\nother solutions: one is always more costly (keep in mind that is solves a more\ngeneral problem), the other one is more or less costly depending on the\ninterplay between instant density and cumulated density. The length $k$ of the\nsequence also plays a role. We characterize the key values of $k, \\mu$ and $m$\nfor which either algorithm should be used.\n  In the case of {\\em non-strict} journeys, for which no algorithm is known, we\nshow that some pre-processing of the input graph allows us to re-use the same\nalgorithm than before. By chance, these operations happens to cost again\n$O(k\\mu n)$ time, which implies that the second problem is not more difficult\nthan the first.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 08:53:27 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 14:13:39 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Barjon", "Matthieu", ""], ["Casteigts", "Arnaud", ""], ["Chaumette", "Serge", ""], ["Johnen", "Colette", ""], ["Neggaz", "Yessin M.", ""]]}, {"id": "1404.7638", "submitter": "Srikrishnan Divakaran", "authors": "Srikrishnan Divakaran", "title": "An Optimal Offline Algorithm for List Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the static list update problem, given an ordered list $\\rho_0$ (an\nordering of the list $L$ = \\{ $a_a, a_2, ..., a_l$ \\}), and a sequence $\\sigma\n= (\\sigma_1, \\sigma_2, ..., \\sigma_m)$ of requests for items in $L$, we\ncharacterize the list reorganizations in an optimal offline solution in terms\nof an initial permutation of the list followed by a sequence of $m$ {\\em\nelement transfers}, where an element transfer is a type of list reorganization\nwhere only the requested item can be moved. Then we make use of this\ncharacterization to design an $O(l^{2} (l-1)!m)$ time optimal offline\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 09:02:30 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Divakaran", "Srikrishnan", ""]]}, {"id": "1404.7703", "submitter": "Ofer Neiman", "authors": "Michael Elkin, Ofer Neiman, Shay Solomon", "title": "Light Spanners", "comments": "10 pages, 1 figure, to appear in ICALP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $t$-spanner of a weighted undirected graph $G=(V,E)$, is a subgraph $H$\nsuch that $d_H(u,v)\\le t\\cdot d_G(u,v)$ for all $u,v\\in V$. The sparseness of\nthe spanner can be measured by its size (the number of edges) and weight (the\nsum of all edge weights), both being important measures of the spanner's\nquality -- in this work we focus on the latter.\n  Specifically, it is shown that for any parameters $k\\ge 1$ and $\\epsilon>0$,\nany weighted graph $G$ on $n$ vertices admits a\n$(2k-1)\\cdot(1+\\epsilon)$-stretch spanner of weight at most $w(MST(G))\\cdot\nO_\\epsilon(kn^{1/k}/\\log k)$, where $w(MST(G))$ is the weight of a minimum\nspanning tree of $G$. Our result is obtained via a novel analysis of the\nclassic greedy algorithm, and improves previous work by a factor of $O(\\log\nk)$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 12:46:49 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""], ["Solomon", "Shay", ""]]}, {"id": "1404.7758", "submitter": "Sigve Hortemo S{\\ae}ther", "authors": "Sigve Hortemo S{\\ae}ther, Jan Arne Telle", "title": "Between Treewidth and Clique-width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many hard graph problems can be solved efficiently when restricted to graphs\nof bounded treewidth, and more generally to graphs of bounded clique-width. But\nthere is a price to be paid for this generality, exemplified by the four\nproblems MaxCut, Graph Coloring, Hamiltonian Cycle and Edge Dominating Set that\nare all FPT parameterized by treewidth but none of which can be FPT\nparameterized by clique-width unless FPT = W[1], as shown by Fomin et al [7,\n8]. We therefore seek a structural graph parameter that shares some of the\ngenerality of clique-width without paying this price. Based on splits, branch\ndecompositions and the work of Vatshelle [18] on Maximum Matching-width, we\nconsider the graph parameter sm-width which lies between treewidth and\nclique-width. Some graph classes of unbounded treewidth, like\ndistance-hereditary graphs, have bounded sm-width. We show that MaxCut, Graph\nColoring, Hamiltonian Cycle and Edge Dominating Set are all FPT parameterized\nby sm-width.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 15:13:45 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["S\u00e6ther", "Sigve Hortemo", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1404.7810", "submitter": "Markus Sortland Dregi", "authors": "Markus Sortland Dregi and Daniel Lokshtanov", "title": "Parameterized Complexity of Bandwidth on Trees", "comments": "33 pages, To appear at ICALP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bandwidth of a $n$-vertex graph $G$ is the smallest integer $b$ such that\nthere exists a bijective function $f : V(G) \\rightarrow \\{1,...,n\\}$, called a\nlayout of $G$, such that for every edge $uv \\in E(G)$, $|f(u) - f(v)| \\leq b$.\nIn the {\\sc Bandwidth} problem we are given as input a graph $G$ and integer\n$b$, and asked whether the bandwidth of $G$ is at most $b$. We present two\nresults concerning the parameterized complexity of the {\\sc Bandwidth} problem\non trees.\n  First we show that an algorithm for {\\sc Bandwidth} with running time\n$f(b)n^{o(b)}$ would violate the Exponential Time Hypothesis, even if the input\ngraphs are restricted to be trees of pathwidth at most two. Our lower bound\nshows that the classical $2^{O(b)}n^{b+1}$ time algorithm by Saxe [SIAM Journal\non Algebraic and Discrete Methods, 1980] is essentially optimal.\n  Our second result is a polynomial time algorithm that given a tree $T$ and\ninteger $b$, either correctly concludes that the bandwidth of $T$ is more than\n$b$ or finds a layout of $T$ of bandwidth at most $b^{O(b)}$. This is the first\nparameterized approximation algorithm for the bandwidth of trees.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 17:44:03 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Dregi", "Markus Sortland", ""], ["Lokshtanov", "Daniel", ""]]}]