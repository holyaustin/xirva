[{"id": "2102.00321", "submitter": "Orestis Papadigenopoulos", "authors": "Orestis Papadigenopoulos and Constantine Caramanis", "title": "Recurrent Submodular Welfare and Matroid Blocking Bandits", "comments": "Corrected Remark 3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of research focuses on the study of the stochastic multi-armed\nbandits problem (MAB), in the case where temporal correlations of specific\nstructure are imposed between the player's actions and the reward distributions\nof the arms (Kleinberg and Immorlica [FOCS18], Basu et al. [NeurIPS19]). As\nopposed to the standard MAB setting, where the optimal solution in hindsight\ncan be trivially characterized, these correlations lead to (sub-)optimal\nsolutions that exhibit interesting dynamical patterns -- a phenomenon that\nyields new challenges both from an algorithmic as well as a learning\nperspective. In this work, we extend the above direction to a combinatorial\nbandit setting and study a variant of stochastic MAB, where arms are subject to\nmatroid constraints and each arm becomes unavailable (blocked) for a fixed\nnumber of rounds after each play. A natural common generalization of the\nstate-of-the-art for blocking bandits, and that for matroid bandits, yields a\n$(1-\\frac{1}{e})$-approximation for partition matroids, yet it only guarantees\na $\\frac{1}{2}$-approximation for general matroids. In this paper we develop\nnew algorithmic ideas that allow us to obtain a polynomial-time $(1 -\n\\frac{1}{e})$-approximation algorithm (asymptotically and in expectation) for\nany matroid, and thus to control the $(1-\\frac{1}{e})$-approximate regret. A\nkey ingredient is the technique of correlated (interleaved) scheduling. Along\nthe way, we discover an interesting connection to a variant of Submodular\nWelfare Maximization, for which we provide (asymptotically) matching upper and\nlower approximability bounds.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 21:51:47 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 06:30:35 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 03:34:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Papadigenopoulos", "Orestis", ""], ["Caramanis", "Constantine", ""]]}, {"id": "2102.00338", "submitter": "Pilar Cano", "authors": "Prosenjit Bose, Pilar Cano, Rolf Fagerberg, John Iacono, Riko Jacob,\n  and Stefan Langerman", "title": "Fragile Complexity of Adaptive Algorithms", "comments": "Appears at proceedings of the 12th International Conference on\n  Algorithms and Complexity (CIAC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The fragile complexity of a comparison-based algorithm is $f(n)$ if each\ninput element participates in $O(f(n))$ comparisons. In this paper, we explore\nthe fragile complexity of algorithms adaptive to various restrictions on the\ninput, i.e., algorithms with a fragile complexity parameterized by a quantity\nother than the input size n. We show that searching for the predecessor in a\nsorted array has fragile complexity ${\\Theta}(\\log k)$, where $k$ is the rank\nof the query element, both in a randomized and a deterministic setting. For\npredecessor searches, we also show how to optimally reduce the amortized\nfragile complexity of the elements in the array. We also prove the following\nresults: Selecting the $k$-th smallest element has expected fragile complexity\n$O(\\log \\log k)$ for the element selected. Deterministically finding the\nminimum element has fragile complexity ${\\Theta}(\\log(Inv))$ and\n${\\Theta}(\\log(Runs))$, where $Inv$ is the number of inversions in a sequence\nand $Runs$ is the number of increasing runs in a sequence. Deterministically\nfinding the median has fragile complexity $O(\\log(Runs) + \\log \\log n)$ and\n${\\Theta}(\\log(Inv))$. Deterministic sorting has fragile complexity\n${\\Theta}(\\log(Inv))$ but it has fragile complexity ${\\Theta}(\\log n)$\nregardless of the number of runs.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 23:42:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bose", "Prosenjit", ""], ["Cano", "Pilar", ""], ["Fagerberg", "Rolf", ""], ["Iacono", "John", ""], ["Jacob", "Riko", ""], ["Langerman", "Stefan", ""]]}, {"id": "2102.00551", "submitter": "Siddhartha Srivastava", "authors": "Siddhartha Srivastava and Veera Sundararaghavan", "title": "Bandgap optimization in combinatorial graphs with tailored ground\n  states: Application in Quantum annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO physics.comp-ph quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A mixed-integer linear programming (MILP) formulation is presented for\nparameter estimation of the Potts model. Two algorithms are developed; the\nfirst method estimates the parameters such that the set of ground states\nreplicate the user-prescribed data set; the second method allows the user to\nprescribe the ground states multiplicity. In both instances, the optimization\nprocess ensures that the bandgap is maximized. Consequently, the model\nparameter efficiently describes the user data for a broad range of\ntemperatures. This is useful in the development of energy-based graph models to\nbe simulated on Quantum annealing hardware where the exact simulation\ntemperature is unknown. Computationally, the memory requirement in this method\ngrows exponentially with the graph size. Therefore, this method can only be\npractically applied to small graphs. Such applications include learning of\nsmall generative classifiers and spin-lattice model with energy described by\nIsing hamiltonian. Learning large data sets poses no extra cost to this method;\nhowever, applications involving the learning of high dimensional data are out\nof scope.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 22:11:12 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Srivastava", "Siddhartha", ""], ["Sundararaghavan", "Veera", ""]]}, {"id": "2102.00556", "submitter": "Akash Kumar", "authors": "Akash Kumar, C. Seshadhri, Andrew Stolman", "title": "Random walks and forbidden minors III: poly(d/{\\epsilon})-time partition\n  oracles for minor-free graph classes", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider the family of bounded degree graphs in any minor-closed family (such\nas planar graphs). Let d be the degree bound and n be the number of vertices of\nsuch a graph. Graphs in these classes have hyperfinite decompositions, where,\nfor a sufficiently small \\e > 0, one removes \\edn edges to get connected\ncomponents of size independent of n. An important tool for sublinear algorithms\nand property testing for such classes is the partition oracle, introduced by\nthe seminal work of Hassidim-Kelner-Nguyen-Onak (FOCS 2009). A partition oracle\nis a local procedure that gives consistent access to a hyperfinite\ndecomposition, without any preprocessing. Given a query vertex v, the partition\noracle outputs the component containing v in time independent of n. All the\nanswers are consistent with a single hyperfinite decomposition. The partition\noracle of Hassidim et al. runs in time d^poly(d/\\e) per query. They pose the\nopen problem of whether poly(d/\\e)-time partition oracles exist. Levi-Ron\n(ICALP 2013) give a refinement of the previous approach, to get a partition\noracle that runs in time d^{\\log(d/\\e)-per query. In this paper, we resolve\nthis open problem and give \\poly(d/\\e)-time partition oracles for bounded\ndegree graphs in any minor-closed family. Unlike the previous line of work\nbased on combinatorial methods, we employ techniques from spectral graph\ntheory. We build on a recent spectral graph theoretical toolkit for\nminor-closed graph families, introduced by the authors to develop efficient\nproperty testers. A consequence of our result is a poly(d/\\e)-query tester for\nany monotone and additive property of minor-closed families (such as bipartite\nplanar graphs). Our result also gives poly(d/\\e)-query algorithms for additive\n{\\e}n-approximations for problems such as maximum matching, minimum vertex\ncover, maximum independent set, and minimum dominating set for these graph\nfamilies.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 22:49:18 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 13:50:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kumar", "Akash", ""], ["Seshadhri", "C.", ""], ["Stolman", "Andrew", ""]]}, {"id": "2102.00949", "submitter": "Attila Pereszl\\'enyi", "authors": "B\\'alint Dar\\'oczy, Katalin Friedl, L\\'aszl\\'o Kab\\'odi, Attila\n  Pereszl\\'enyi, D\\'aniel Szab\\'o", "title": "Quantum Inspired Adaptive Boosting", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building on the quantum ensemble based classifier algorithm of Schuld and\nPetruccione [arXiv:1704.02146v1], we devise equivalent classical algorithms\nwhich show that this quantum ensemble method does not have advantage over\nclassical algorithms. Essentially, we simplify their algorithm until it is\nintuitive to come up with an equivalent classical version. One of the classical\nalgorithms is extremely simple and runs in constant time for each input to be\nclassified. We further develop the idea and, as the main contribution of the\npaper, we propose methods inspired by combining the quantum ensemble method\nwith adaptive boosting. The algorithms were tested and found to be comparable\nto the AdaBoost algorithm on publicly available data sets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 16:33:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dar\u00f3czy", "B\u00e1lint", ""], ["Friedl", "Katalin", ""], ["Kab\u00f3di", "L\u00e1szl\u00f3", ""], ["Pereszl\u00e9nyi", "Attila", ""], ["Szab\u00f3", "D\u00e1niel", ""]]}, {"id": "2102.01044", "submitter": "Tadeusz Kobus", "authors": "Tadeusz Kobus, Maciej Kokoci\\'nski, Pawe{\\l} T. Wojciechowski", "title": "Jiffy: A Lock-free Skip List with Batch Updates and Snapshots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Jiffy, the first lock-free, linearizable ordered\nkey-value index that offers both (1) batch updates, which are put and remove\noperations that are executed atomically, and (2) consistent snapshots used by,\ne.g., range scan operations. Jiffy is built as a multiversioned lock-free skip\nlist and relies on CPU's Time Stamp Counter register to generate version\nnumbers at minimal cost. For faster skip list traversals and better utilization\nof the CPU caches, key-value entries are grouped into immutable objects called\nrevisions. Moreover, by changing the size of revisions and thus modifying the\nsynchronization granularity, our index can adapt to varying contentions levels\n(smaller revisions are more suited for write-heavy workloads whereas large\nrevisions benefit read-dominated workloads, especially when they feature many\nrange scan operations). Structure modifications to the index, which result in\nchanging the size of revisions, happen through (lock-free) skip list node split\nand merge operations that are carefully coordinated with the update operations.\nDespite rich semantics, Jiffy offers highly scalable performance, which is\ncomparable or exceeds the performance of the state-of-the-art lock-free ordered\nindices that feature linearizable range scan operations. Compared to its\n(lock-based) rivals that also support batch updates, Jiffy can execute large\nbatch updates up to 7.4x more efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:29:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kobus", "Tadeusz", ""], ["Kokoci\u0144ski", "Maciej", ""], ["Wojciechowski", "Pawe\u0142 T.", ""]]}, {"id": "2102.01124", "submitter": "Vibha Sahlot", "authors": "Sukanya Pandey and Vibha Sahlot", "title": "Role Coloring Bipartite Graphs", "comments": "17 pages including references, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A k-role coloring alpha of a graph G is an assignment of k colors to the\nvertices of G such that if any two vertices are assigned the same color, then\ntheir neighborhood are assigned the same set of colors. That is, if alpha(u) =\nalpha(v) for a pair of vertices u and v, then the set of colors assigned to\nN(u) and N(v) are the same (where N(u) is the set of neighbors of u). By\ndefinition, every graph on n vertices admits an n-role coloring. While for\nevery graph on n vertices, it is trivial to decide if it admits a 1-role\ncoloring, determining whether a graph admits a k-role coloring is a notoriously\nhard problem for k greater than 1. In fact, it is known that k-Role coloring is\nNP-complete for k greater than 1 on arbitrary graphs. There has been extensive\nresearch on the complexity of k-role coloring on various hereditary graph\nclasses. Furthering this direction of research, we show that k-Role coloring is\nNP-complete on bipartite graphs for k greater than 2 (while it is trivial for k\n= 2). We complement the hardness result by characterizing 3-role colorable\nbipartite chain graphs, leading to a polynomial-time algorithm for 3-Role\ncoloring for this class of graphs. We further show that 2-Role coloring is\nNP-complete for graphs that are d vertices or edges away from the class of\nbipartite graphs, even when d = 1.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 19:38:03 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Pandey", "Sukanya", ""], ["Sahlot", "Vibha", ""]]}, {"id": "2102.01149", "submitter": "Devorah Kletenik", "authors": "Lisa Hellerstein, Devorah Kletenik and Srinivasan Parthasarathy", "title": "A Tight Bound for Stochastic Submodular Cover", "comments": "This work extends the result of Srinivasan Parthasarathy in his paper\n  arXiv:1803.07639 from the problem of Stochastic Set Cover to that of\n  Stochastic Submodular Cover", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the Adaptive Greedy algorithm of Golovin and Krause (2011)\nachieves an approximation bound of $(\\ln (Q/\\eta)+1)$ for Stochastic Submodular\nCover: here $Q$ is the \"goal value\" and $\\eta$ is the smallest non-zero\nmarginal increase in utility deliverable by an item. (For integer-valued\nutility functions, we show a bound of $H(Q)$, where $H(Q)$ is the $Q^{th}$\nHarmonic number.) Although this bound was claimed by Golovin and Krause in the\noriginal version of their paper, the proof was later shown to be incorrect by\nNan and Saligrama (2017). The subsequent corrected proof of Golovin and Krause\n(2017) gives a quadratic bound of $(\\ln(Q/\\eta) + 1)^2$. Other previous bounds\nfor the problem are $56(\\ln(Q/\\eta) + 1)$, implied by work of Im et al. (2016)\non a related problem, and $k(\\ln (Q/\\eta)+1)$, due to Deshpande et al. (2016)\nand Hellerstein and Kletenik (2018), where $k$ is the number of states. Our\nbound generalizes the well-known $(\\ln~m + 1)$ approximation bound on the\ngreedy algorithm for the classical Set Cover problem, where $m$ is the size of\nthe ground set.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:37:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "2102.01240", "submitter": "Scott Rodilitz", "authors": "Vahideh Manshadi, Rad Niazadeh, Scott Rodilitz", "title": "Fair Dynamic Rationing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the allocative challenges that governmental and nonprofit\norganizations face when tasked with equitable and efficient rationing of a\nsocial good among agents whose needs (demands) realize sequentially and are\npossibly correlated. To better achieve their dual aims of equity and efficiency\nin such contexts, social planners intend to maximize the minimum fill rate\nacross agents, where each agent's fill rate must be irrevocably decided upon\nits arrival. For an arbitrarily correlated sequence of demands, we establish\nupper bounds on both the expected minimum fill rate (ex-post fairness) and the\nminimum expected fill rate (ex-ante fairness) achievable by any policy. Our\nbounds are parameterized by the number of agents and the expected\ndemand-to-supply ratio, and they shed light on the limits of attaining equity\nin dynamic rationing. Further, we show that for any set of parameters, a simple\nadaptive policy of projected proportional allocation achieves the best possible\nfairness guarantee, ex post as well as ex ante. Our policy is transparent and\neasy to implement; yet despite its simplicity, we demonstrate that this policy\nprovides significant improvement over the class of non-adaptive\ntarget-fill-rate policies. We obtain the performance guarantees of (i) our\nproposed adaptive policy by inductively designing lower-bound functions on its\ncorresponding value-to-go, and (ii) the optimal target-fill-rate policy by\nestablishing an intriguing connection to a monopoly-pricing optimization\nproblem. We complement our theoretical developments with a numerical study\nmotivated by the rationing of COVID-19 medical supplies based on a\nprojected-demand model used by the White House. In such a setting, our simple\nadaptive policy significantly outperforms its theoretical guarantee as well as\nthe optimal target-fill-rate policy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 00:38:59 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 18:03:50 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Manshadi", "Vahideh", ""], ["Niazadeh", "Rad", ""], ["Rodilitz", "Scott", ""]]}, {"id": "2102.01378", "submitter": "Tobias Heuer", "authors": "Tobias Heuer, Nikolai Maas, Sebastian Schlag", "title": "Multilevel Hypergraph Partitioning with Vertex Weights Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balanced hypergraph partitioning problem (HGP) is to partition the vertex\nset of a hypergraph into k disjoint blocks of bounded weight, while minimizing\nan objective function defined on the hyperedges. Whereas real-world\napplications often use vertex and edge weights to accurately model the\nunderlying problem, the HGP research community commonly works with unweighted\ninstances.\n  In this paper, we argue that, in the presence of vertex weights, current\nbalance constraint definitions either yield infeasible partitioning problems or\nallow unnecessarily large imbalances and propose a new definition that\novercomes these problems. We show that state-of-the-art hypergraph partitioners\noften struggle considerably with weighted instances and tight balance\nconstraints (even with our new balance definition). Thus, we present a\nrecursive-bipartitioning technique that is able to reliably compute balanced\n(and hence feasible) solutions. The proposed method balances the partition by\npre-assigning a small subset of the heaviest vertices to the two blocks of each\nbipartition (using an algorithm originally developed for the job scheduling\nproblem) and optimizes the actual partitioning objective on the remaining\nvertices. We integrate our algorithm into the multilevel hypergraph partitioner\nKaHyPar and show that our approach is able to compute balanced partitions of\nhigh quality on a diverse set of benchmark instances.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 08:06:17 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Heuer", "Tobias", ""], ["Maas", "Nikolai", ""], ["Schlag", "Sebastian", ""]]}, {"id": "2102.01540", "submitter": "Sebastian Lamm", "authors": "Demian Hespe, Sebastian Lamm, Christian Schorr", "title": "Targeted Branching for the Maximum Independent Set Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a maximum independent set is a fundamental NP-hard problem that is\nused in many real-world applications. Given an unweighted graph, this problem\nasks for a maximum cardinality set of pairwise non-adjacent vertices. Some of\nthe most successful algorithms for this problem are based on the\nbranch-and-bound or branch-and-reduce paradigms. In particular,\nbranch-and-reduce algorithms, which combine branch-and-bound with reduction\nrules, achieved substantial results, solving many previously infeasible\ninstances. These results were to a large part achieved by developing new, more\npractical reduction rules. However, other components that have been shown to\nhave an impact on the performance of these algorithms have not received as much\nattention. One of these is the branching strategy, which determines what vertex\nis included or excluded in a potential solution. The most commonly used\nstrategy selects vertices based on their degree and does not take into account\nother factors that contribute to the performance. In this work, we develop and\nevaluate several novel branching strategies for both branch-and-bound and\nbranch-and-reduce algorithms. Our strategies are based on one of two\napproaches. They either (1) aim to decompose the graph into two or more\nconnected components which can then be solved independently, or (2) try to\nremove vertices that hinder the application of a reduction rule. Our\nexperiments on a large set of real-world instances indicate that our strategies\nare able to improve the performance of the state-of-the-art branch-and-reduce\nalgorithms. To be more specific, our reduction-based packing branching rule is\nable to outperform the default branching strategy of selecting a vertex of\nhighest degree on 65% of all instances tested. Furthermore, our\ndecomposition-based strategy based on edge cuts is able to achieve a speedup of\n2.29 on sparse networks (1.22 on all instances).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:09:18 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:59:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Hespe", "Demian", ""], ["Lamm", "Sebastian", ""], ["Schorr", "Christian", ""]]}, {"id": "2102.01541", "submitter": "Tatiana Brailovskaya", "authors": "Tatiana Brailovskaya, Mikl\\'os Z. R\\'acz", "title": "Tree trace reconstruction using subtraces", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree trace reconstruction aims to learn the binary node labels of a tree,\ngiven independent samples of the tree passed through an appropriately defined\ndeletion channel. In recent work, Davies, R\\'acz, and Rashtchian used\ncombinatorial methods to show that $\\exp(\\mathcal{O}(k \\log_{k} n))$ samples\nsuffice to reconstruct a complete $k$-ary tree with $n$ nodes with high\nprobability. We provide an alternative proof of this result, which allows us to\ngeneralize it to a broader class of tree topologies and deletion models. In our\nproofs, we introduce the notion of a subtrace, which enables us to connect with\nand generalize recent mean-based complex analytic algorithms for string trace\nreconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:13:04 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Brailovskaya", "Tatiana", ""], ["R\u00e1cz", "Mikl\u00f3s Z.", ""]]}, {"id": "2102.01570", "submitter": "Sitan Chen", "authors": "Sitan Chen, Zhao Song, Runzhou Tao, Ruizhe Zhang", "title": "Symmetric Boolean Factor Analysis with Applications to InstaHide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we examine the security of InstaHide, a recently proposed scheme\nfor distributed learning (Huang et al.). A number of recent works have given\nreconstruction attacks for InstaHide in various regimes by leveraging an\nintriguing connection to the following matrix factorization problem: given the\nGram matrix of a collection of m random k-sparse Boolean vectors in {0,1}^r,\nrecover the vectors (up to the trivial symmetries). Equivalently, this can be\nthought of as a sparse, symmetric variant of the well-studied problem of\nBoolean factor analysis, or as an average-case version of the classic problem\nof recovering a k-uniform hypergraph from its line graph.\n  As previous algorithms either required m to be exponentially large in k or\nonly applied to k = 2, they left open the question of whether InstaHide\npossesses some form of \"fine-grained security\" against reconstruction attacks\nfor moderately large k. In this work, we answer this in the negative by giving\na simple O(m^{\\omega + 1}) time algorithm for the above matrix factorization\nproblem. Our algorithm, based on tensor decomposition, only requires m to be at\nleast quasi-linear in r. We complement this result with a quasipolynomial-time\nalgorithm for a worst-case setting of the problem where the collection of\nk-sparse vectors is chosen arbitrarily.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:52:52 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Sitan", ""], ["Song", "Zhao", ""], ["Tao", "Runzhou", ""], ["Zhang", "Ruizhe", ""]]}, {"id": "2102.01646", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Roi Livni, and Shay Moran", "title": "Online Learning with Simple Predictors and a Combinatorial\n  Characterization of Minimax in 0/1 Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Which classes can be learned properly in the online model? -- that is, by an\nalgorithm that at each round uses a predictor from the concept class. While\nthere are simple and natural cases where improper learning is necessary, it is\nnatural to ask how complex must the improper predictors be in such cases. Can\none always achieve nearly optimal mistake/regret bounds using \"simple\"\npredictors?\n  In this work, we give a complete characterization of when this is possible,\nthus settling an open problem which has been studied since the pioneering works\nof Angluin (1987) and Littlestone (1988). More precisely, given any concept\nclass C and any hypothesis class H, we provide nearly tight bounds (up to a log\nfactor) on the optimal mistake bounds for online learning C using predictors\nfrom H. Our bound yields an exponential improvement over the previously best\nknown bound by Chase and Freitag (2020).\n  As applications, we give constructive proofs showing that (i) in the\nrealizable setting, a near-optimal mistake bound (up to a constant factor) can\nbe attained by a sparse majority-vote of proper predictors, and (ii) in the\nagnostic setting, a near-optimal regret bound (up to a log factor) can be\nattained by a randomized proper algorithm.\n  A technical ingredient of our proof which may be of independent interest is a\ngeneralization of the celebrated Minimax Theorem (von Neumann, 1928) for binary\nzero-sum games. A simple game which fails to satisfy Minimax is \"Guess the\nLarger Number\", where each player picks a number and the larger number wins.\nThe payoff matrix is infinite triangular. We show this is the only obstruction:\nif a game does not contain triangular submatrices of unbounded sizes then the\nMinimax Theorem holds. This generalizes von Neumann's Minimax Theorem by\nremoving requirements of finiteness (or compactness), and captures precisely\nthe games of interest in online learning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:02:01 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hanneke", "Steve", ""], ["Livni", "Roi", ""], ["Moran", "Shay", ""]]}, {"id": "2102.01730", "submitter": "Alexandra Porter", "authors": "Alexandra Porter and Mary Wootters", "title": "On Greedy Approaches to Hierarchical Aggregation", "comments": "Example figures replaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze greedy algorithms for the Hierarchical Aggregation (HAG) problem,\na strategy introduced in [Jia et al., KDD 2020] for speeding up learning on\nGraph Neural Networks (GNNs). The idea of HAG is to identify and remove\nredundancies in computations performed when training GNNs. The associated\noptimization problem is to identify and remove the most redundancies.\n  Previous work introduced a greedy approach for the HAG problem and claimed a\n1-1/e approximation factor. We show by example that this is not correct, and\none cannot hope for better than a 1/2 approximation factor. We prove that this\ngreedy algorithm does satisfy some (weaker) approximation guarantee, by showing\na new connection between the HAG problem and maximum matching problems in\nhypergraphs. We also introduce a second greedy algorithm which can out-perform\nthe first one, and we show how to implement it efficiently in some parameter\nregimes. Finally, we introduce some greedy heuristics that are much faster than\nthe above greedy algorithms, and we demonstrate that they perform well on\nreal-world graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 19:59:10 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 01:34:03 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 21:11:51 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Porter", "Alexandra", ""], ["Wootters", "Mary", ""]]}, {"id": "2102.02171", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Alistair Stewart and Yuxin\n  Sun", "title": "Outlier-Robust Learning of Ising Models Under Dobrushin's Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Ising models satisfying Dobrushin's\ncondition in the outlier-robust setting where a constant fraction of the\nsamples are adversarially corrupted. Our main result is to provide the first\ncomputationally efficient robust learning algorithm for this problem with\nnear-optimal error guarantees. Our algorithm can be seen as a special case of\nan algorithm for robustly learning a distribution from a general exponential\nfamily. To prove its correctness for Ising models, we establish new\nanti-concentration results for degree-$2$ polynomials of Ising models that may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:00:57 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""], ["Sun", "Yuxin", ""]]}, {"id": "2102.02193", "submitter": "Jakub Tetek", "authors": "Kasper Green Larsen, Rasmus Pagh, Jakub T\\v{e}tek", "title": "CountSketches, Feature Hashing and the Median of Three", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the classic CountSketch method, which is a sparse,\nrandom projection that transforms a (high-dimensional) Euclidean vector $v$ to\na vector of dimension $(2t-1) s$, where $t, s > 0$ are integer parameters. It\nis known that even for $t=1$, a CountSketch allows estimating coordinates of\n$v$ with variance bounded by $\\|v\\|_2^2/s$. For $t > 1$, the estimator takes\nthe median of $2t-1$ independent estimates, and the probability that the\nestimate is off by more than $2 \\|v\\|_2/\\sqrt{s}$ is exponentially small in\n$t$. This suggests choosing $t$ to be logarithmic in a desired inverse failure\nprobability. However, implementations of CountSketch often use a small,\nconstant $t$. Previous work only predicts a constant factor improvement in this\nsetting.\n  Our main contribution is a new analysis of Count-Sketch, showing an\nimprovement in variance to $O(\\min\\{\\|v\\|_1^2/s^2,\\|v\\|_2^2/s\\})$ when $t > 1$.\nThat is, the variance decreases proportionally to $s^{-2}$, asymptotically for\nlarge enough $s$. We also study the variance in the setting where an inner\nproduct is to be estimated from two CountSketches. This finding suggests that\nthe Feature Hashing method, which is essentially identical to CountSketch but\ndoes not make use of the median estimator, can be made more reliable at a small\ncost in settings where using a median estimator is possible.\n  We confirm our theoretical findings in experiments and thereby help justify\nwhy a small constant number of estimates often suffice in practice. Our\nimproved variance bounds are based on new general theorems about the variance\nand higher moments of the median of i.i.d. random variables that may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:45:21 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Pagh", "Rasmus", ""], ["T\u011btek", "Jakub", ""]]}, {"id": "2102.02322", "submitter": "Xue Chen", "authors": "Xue Chen, Micha{\\l} Derezi\\'nski", "title": "Query Complexity of Least Absolute Deviation Regression via Robust\n  Uniform Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a regression problem where the learner is given a large collection\nof $d$-dimensional data points, but can only query a small subset of the\nreal-valued labels. How many queries are needed to obtain a $1+\\epsilon$\nrelative error approximation of the optimum? While this problem has been\nextensively studied for least squares regression, little is known for other\nlosses. An important example is least absolute deviation regression ($\\ell_1$\nregression) which enjoys superior robustness to outliers compared to least\nsquares. We develop a new framework for analyzing importance sampling methods\nin regression problems, which enables us to show that the query complexity of\nleast absolute deviation regression is $\\Theta(d/\\epsilon^2)$ up to logarithmic\nfactors. We further extend our techniques to show the first bounds on the query\ncomplexity for any $\\ell_p$ loss with $p\\in(1,2)$. As a key novelty in our\nanalysis, we introduce the notion of robust uniform convergence, which is a new\napproximation guarantee for the empirical loss. While it is inspired by uniform\nconvergence in statistical learning, our approach additionally incorporates a\ncorrection term to avoid unnecessary variance due to outliers. This can be\nviewed as a new connection between statistical learning theory and variance\nreduction techniques in stochastic optimization, which should be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 22:54:27 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 08:24:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Xue", ""], ["Derezi\u0144ski", "Micha\u0142", ""]]}, {"id": "2102.02440", "submitter": "Florin Rusu", "authors": "Yesdaulet Izenov, Asoke Datta, Florin Rusu, Jun Hyung Shin", "title": "Online Sketch-based Query Optimization", "comments": "Extended version of paper \"COMPASS: Online Sketch-based Query\n  Optimization for In-Memory Databases\" accepted at SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-based query optimization remains a critical task in relational databases\neven after decades of research and industrial development. Query optimizers\nrely on a large range of statistical synopses -- including attribute-level\nhistograms and table-level samples -- for accurate cardinality estimation. As\nthe complexity of selection predicates and the number of join predicates\nincrease, two problems arise. First, statistics cannot be incrementally\ncomposed to effectively estimate the cost of the sub-plans generated in plan\nenumeration. Second, small errors are propagated exponentially through join\noperators, which can lead to severely sub-optimal plans. In this paper, we\nintroduce COMPASS, a novel query optimization paradigm for in-memory databases\nbased on a single type of statistics -- Fast-AGMS sketches. In COMPASS, query\noptimization and execution are intertwined. Selection predicates and sketch\nupdates are pushed-down and evaluated online during query optimization. This\nallows Fast-AGMS sketches to be computed only over the relevant tuples -- which\nenhances cardinality estimation accuracy. Plan enumeration is performed over\nthe query join graph by incrementally composing attribute-level sketches -- not\nby building a separate sketch for every sub-plan. We prototype COMPASS in MapD\n-- an open-source parallel database -- and perform extensive experiments over\nthe complete JOB benchmark. The results prove that COMPASS generates better\nexecution plans -- both in terms of cardinality and runtime -- compared to four\nother database systems. Overall, COMPASS achieves a speedup ranging from 1.35X\nto 11.28X in cumulative query execution time over the considered competitors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 06:43:36 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Izenov", "Yesdaulet", ""], ["Datta", "Asoke", ""], ["Rusu", "Florin", ""], ["Shin", "Jun Hyung", ""]]}, {"id": "2102.02484", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Marin Bougeret, Victor A. Campos, Ignasi Sau", "title": "Kernelization of Maximum Minimal Vertex Cover", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Maximum Minimal Vertex Cover (MMVC) problem, we are given a graph $G$\nand a positive integer $k$, and the objective is to decide whether $G$ contains\na minimal vertex cover of size at least $k$. Motivated by the kernelization of\nMMVC with parameter $k$, our main contribution is to introduce a simple general\nframework to obtain lower bounds on the degrees of a certain type of polynomial\nkernels for vertex optimization problems, which we call lop-kernels.\nInformally, this type of kernels is required to preserve large optimal\nsolutions in the reduced instance, and captures the vast majority of existing\nkernels in the literature. As a consequence of this framework, we show that the\ntrivial quadratic kernel for MMVC is essentially optimal, answering a question\nof Boria et al. [Discret. Appl. Math. 2015], and that the known cubic kernel\nfor Maximum Minimal Feedback Vertex Set is also essentially optimal. On the\npositive side, given the (plausible) non-existence of subquadratic kernels for\nMMVC on general graphs, we provide subquadratic kernels on $H$-free graphs for\nseveral graphs $H$, such as the bull, the paw, or the complete graphs, by\nmaking use of the Erd\\H{o}s-Hajnal property in order to find an appropriate\ndecomposition. Finally, we prove that MMVC does not admit polynomial kernels\nparameterized by the size of a minimum vertex cover of the input graph, even on\nbipartite graphs, unless ${\\sf NP} \\subseteq {\\sf coNP} / {\\sf poly}$. This\nindicates that parameters smaller than the solution size are unlike to yield\npolynomial kernels for MMVC.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 08:51:40 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:30:52 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Bougeret", "Marin", ""], ["Campos", "Victor A.", ""], ["Sau", "Ignasi", ""]]}, {"id": "2102.02505", "submitter": "Teresa Anna Steiner", "authors": "Philip Bille, Inge Li G{\\o}rtz, Max Rish{\\o}j Pedersen, Teresa Anna\n  Steiner", "title": "Gapped Indexing for Consecutive Occurrences", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic string indexing problem is to preprocess a string S into a\ncompact data structure that supports efficient pattern matching queries.\nTypical queries include existential queries (decide if the pattern occurs in\nS), reporting queries (return all positions where the pattern occurs), and\ncounting queries (return the number of occurrences of the pattern). In this\npaper we consider a variant of string indexing, where the goal is to compactly\nrepresent the string such that given two patterns P1 and P2 and a gap range\n[\\alpha,\\beta] we can quickly find the consecutive occurrences of P1 and P2\nwith distance in [\\alpha,\\beta], i.e., pairs of occurrences immediately\nfollowing each other and with distance within the range. We present data\nstructures that use \\~O(n) space and query time \\~O(|P1|+|P2|+n^(2/3)) for\nexistence and counting and \\~O(|P1|+|P2|+n^(2/3)*occ^(1/3)) for reporting. We\ncomplement this with a conditional lower bound based on the set intersection\nproblem showing that any solution using \\~O(n) space must use\n\\tilde{\\Omega}}(|P1|+|P2|+\\sqrt{n}) query time. To obtain our results we\ndevelop new techniques and ideas of independent interest including a new suffix\ntree decomposition and hardness of a variant of the set intersection problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:32:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Pedersen", "Max Rish\u00f8j", ""], ["Steiner", "Teresa Anna", ""]]}, {"id": "2102.02597", "submitter": "Floyd Zweydinger", "authors": "Andre Esser, Robert K\\\"ubler, Floyd Zweydinger", "title": "A Faster Algorithm for Finding Closest Pairs in Hamming Metric", "comments": "22, 6 figures code: https://github.com/submission-nn/nn-algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Closest Pair Problem in Hamming metric, which asks to find the\npair with the smallest Hamming distance in a collection of binary vectors. We\ngive a new randomized algorithm for the problem on uniformly random input\noutperforming previous approaches whenever the dimension of input points is\nsmall compared to the dataset size. For moderate to large dimensions, our\nalgorithm matches the time complexity of the previously best-known locality\nsensitive hashing based algorithms. Technically our algorithm follows similar\ndesign principles as Dubiner (IEEE Trans. Inf. Theory 2010) and May-Ozerov\n(Eurocrypt 2015). Besides improving the time complexity in the aforementioned\nareas, we significantly simplify the analysis of these previous works. We give\na modular analysis, which allows us to investigate the performance of the\nalgorithm also on non-uniform input distributions. Furthermore, we give a proof\nof concept implementation of our algorithm which performs well in comparison to\na quadratic search baseline. This is the first step towards answering an open\nquestion raised by May and Ozerov regarding the practicability of algorithms\nfollowing these design principles.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 13:26:39 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Esser", "Andre", ""], ["K\u00fcbler", "Robert", ""], ["Zweydinger", "Floyd", ""]]}, {"id": "2102.02708", "submitter": "Nima Anari", "authors": "Yeganeh Alimohammadi, Nima Anari, Kirankumar Shiragur, Thuy-Duong\n  Vuong", "title": "Fractionally Log-Concave and Sector-Stable Polynomials: Counting Planar\n  Matchings and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show fully polynomial time randomized approximation schemes (FPRAS) for\ncounting matchings of a given size, or more generally sampling/counting\nmonomer-dimer systems in planar, not-necessarily-bipartite, graphs. While\nperfect matchings on planar graphs can be counted exactly in polynomial time,\ncounting non-perfect matchings was shown by [Jer87] to be #P-hard, who also\nraised the question of whether efficient approximate counting is possible. We\nanswer this affirmatively by showing that the multi-site Glauber dynamics on\nthe set of monomers in a monomer-dimer system always mixes rapidly, and that\nthis dynamics can be implemented efficiently on downward-closed families of\ngraphs where counting perfect matchings is tractable. As further applications\nof our results, we show how to sample efficiently using multi-site Glauber\ndynamics from partition-constrained strongly Rayleigh distributions, and\nnonsymmetric determinantal point processes.\n  In order to analyze mixing properties of the multi-site Glauber dynamics, we\nestablish two notions for generating polynomials of discrete set-valued\ndistributions: sector-stability and fractional log-concavity. These notions\ngeneralize well-studied properties like real-stability and log-concavity, but\nunlike them robustly degrade under useful transformations applied to the\ndistribution. We relate these notions to pairwise correlations in the\nunderlying distribution and the notion of spectral independence introduced by\n[ALO20], providing a new tool for establishing spectral independence based on\ngeometry of polynomials. As a byproduct of our techniques, we show that\npolynomials avoiding roots in a sector of the complex plane must satisfy what\nwe call fractional log-concavity; this extends a classic result established by\n[Gar59] who showed homogeneous polynomials that have no roots in a half-plane\nmust be log-concave over the positive orthant.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:07:26 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:36:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Alimohammadi", "Yeganeh", ""], ["Anari", "Nima", ""], ["Shiragur", "Kirankumar", ""], ["Vuong", "Thuy-Duong", ""]]}, {"id": "2102.02765", "submitter": "Tung Mai", "authors": "David Arbour and Drew Dimmery and Tung Mai and Anup Rao", "title": "Online Discrepancy Minimization via Persistent Self-Balancing Walks", "comments": "The proof of Lemma 7 is incorrect. There is a serious issue that we\n  don't know how to fix at the moment. We thank Yang, Nikhil and collaborators\n  for bringing it to our attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online discrepancy minimization problem for vectors in\n$\\mathbb{R}^d$ in the oblivious setting where an adversary is allowed fix the\nvectors $x_1, x_2, \\ldots, x_n$ in arbitrary order ahead of time. We give an\nalgorithm that maintains $O(\\sqrt{\\log(nd/\\delta)})$ discrepancy with\nprobability $1-\\delta$, matching the lower bound given in [Bansal et al. 2020]\nup to an $O(\\sqrt{\\log \\log n})$ factor in the high-probability regime. We also\nprovide results for the weighted and multi-color versions of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:49:12 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 20:00:26 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Arbour", "David", ""], ["Dimmery", "Drew", ""], ["Mai", "Tung", ""], ["Rao", "Anup", ""]]}, {"id": "2102.02873", "submitter": "Shahbaz Khan", "authors": "Shahbaz Khan", "title": "Optimal Construction of Hierarchical Overlap Graphs", "comments": "11 pages, 2 Figures", "journal-ref": "CPM 2021", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genome assembly is a fundamental problem in Bioinformatics, where for a given\nset of overlapping substrings of a genome, the aim is to reconstruct the source\ngenome. The classical approaches to solving this problem use assembly graphs,\nsuch as de Bruijn graphs or overlap graphs, which maintain partial information\nabout such overlaps. For genome assembly algorithms, these graphs present a\ntrade-off between overlap information stored and scalability. Thus,\nHierarchical Overlap Graph (HOG) was proposed to overcome the limitations of\nboth these approaches.\n  For a given set $P$ of $n$ strings, the first algorithm to compute HOG was\ngiven by Cazaux and Rivals [IPL20] requiring $O(||P||+n^2)$ time using\nsuperlinear space, where $||P||$ is the cumulative sum of the lengths of\nstrings in $P$. This was improved by Park et al. [SPIRE20] to $O(||P||\\log n)$\ntime and $O(||P||)$ space using segment trees, and further to\n$O(||P||\\frac{\\log n}{\\log \\log n})$ for the word RAM model. Both these results\ndescribed an open problem to compute HOG in optimal $O(||P||)$ time and space.\nIn this paper, we achieve the desired optimal bounds by presenting a simple\nalgorithm that does not use any complex data structures. At its core, our\nsolution improves the classical result [IPL92] for a special case of the All\nPairs Suffix Prefix (APSP) problem from $O(||P||+n^2)$ time to optimal\n$O(||P||)$ time, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:19:27 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:48:15 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Khan", "Shahbaz", ""]]}, {"id": "2102.02931", "submitter": "Haris Aziz", "authors": "Haris Aziz and Anton Baychkov and Peter Biro", "title": "Cutoff stability under distributional constraints with an application to\n  summer internship matching", "comments": "Extended version of our AAMAS 2020 paper \"Summer Internship Matching\n  with Funding Constraints\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS econ.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new two-sided stable matching problem that describes the\nsummer internship matching practice of an Australian university. The model is a\ncase between two models of Kamada and Kojima on matchings with distributional\nconstraints. We study three solution concepts, the strong and weak stability\nconcepts proposed by Kamada and Kojima, and a new one in between the two,\ncalled cutoff stability. Kamada and Kojima showed that a strongly stable\nmatching may not exist in their most restricted model with disjoint regional\nquotas. Our first result is that checking its existence is NP-hard. We then\nshow that a cutoff stable matching exists not just for the summer internship\nproblem but also for the general matching model with arbitrary heredity\nconstraints. We present an algorithm to compute a cutoff stable matching and\nshow that it runs in polynomial time in our special case of summer internship\nmodel. However, we also show that finding a maximum size cutoff stable matching\nis NP-hard, but we provide a Mixed Integer Linear Program formulation for this\noptimisation problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 00:11:46 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Aziz", "Haris", ""], ["Baychkov", "Anton", ""], ["Biro", "Peter", ""]]}, {"id": "2102.03004", "submitter": "Xusheng Zhang", "authors": "Antonio Blanca, Alistair Sinclair, Xusheng Zhang", "title": "The Critical Mean-field Chayes-Machta Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random-cluster model is a unifying framework for studying random graphs,\nspin systems and electrical networks that plays a fundamental role in designing\nefficient Markov Chain Monte Carlo (MCMC) sampling algorithms for the classical\nferromagnetic Ising and Potts models. In this paper, we study a natural\nnon-local Markov chain known as the Chayes-Machta dynamics for the mean-field\ncase of the random-cluster model, where the underlying graph is the complete\ngraph on $n$ vertices. The random-cluster model is parametrized by an it edge\nprobability $p$ and a cluster weight $q$. Our focus is on the critical regime:\n$p = p_c(q)$ and $q \\in (1,2)$, where $p_c(q)$ is the threshold corresponding\nto the order-disorder phase transition of the model. We show that the mixing\ntime of the Chayes-Machta dynamics is $O(\\log n \\cdot \\log \\log n)$ in this\nparameter regime, which reveals that the dynamics does not undergo an\nexponential slowdown at criticality, a surprising fact that had been predicted\n(but not proved) by statistical physicists. This also provides a nearly optimal\nbound (up to the $\\log\\log n$ factor) for the mixing time of the mean-field\nChayes-Machta dynamics in the only regime of parameters where no non-trivial\nbound was previously known. Our proof consists of a multi-phased coupling\nargument that combines several key ingredients, including a new local limit\ntheorem, a precise bound on the maximum of symmetric random walks with varying\nstep sizes, and tailored estimates for critical random graphs. In addition, we\nderive an improved comparison inequality between the mixing time of the\nChayes-Machta dynamics and that of the local Glauber dynamics on general\ngraphs; this results in better mixing time bounds for the local dynamics in the\nmean-field setting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 05:26:31 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 13:29:04 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Blanca", "Antonio", ""], ["Sinclair", "Alistair", ""], ["Zhang", "Xusheng", ""]]}, {"id": "2102.03117", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Ugo Giocanti, Patrice Ossona de Mendez, Pierre\n  Simon, St\\'ephan Thomass\\'e, Szymon Toru\\'nczyk", "title": "Twin-width IV: ordered graphs and matrices", "comments": "53 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a list of characterizations of bounded twin-width for\nhereditary, totally ordered binary structures. This has several consequences.\nFirst, it allows us to show that a (hereditary) class of matrices over a finite\nalphabet either contains at least $n!$ matrices of size $n \\times n$, or at\nmost $c^n$ for some constant $c$. This generalizes the celebrated Stanley-Wilf\nconjecture/Marcus-Tardos theorem from permutation classes to any matrix class\nover a finite alphabet, answers our small conjecture [SODA '21] in the case of\nordered graphs, and with more work, settles a question first asked by Balogh,\nBollob\\'as, and Morris [Eur. J. Comb. '06] on the growth of hereditary classes\nof ordered graphs. Second, it gives a fixed-parameter approximation algorithm\nfor twin-width on ordered graphs. Third, it yields a full classification of\nfixed-parameter tractable first-order model checking on hereditary classes of\nordered binary structures. Fourth, it provides a model-theoretic\ncharacterization of classes with bounded twin-width.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:43:59 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 11:05:11 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 19:54:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Giocanti", "Ugo", ""], ["de Mendez", "Patrice Ossona", ""], ["Simon", "Pierre", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Toru\u0144czyk", "Szymon", ""]]}, {"id": "2102.03173", "submitter": "Lev Reyzin", "authors": "Thomas Maranzatto, Lev Reyzin", "title": "Reconstructing Arbitrary Trees from Traces in the Tree Edit Distance\n  Model", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of reconstructing trees from traces in\nthe tree edit distance model. Previous work by Davies et al. (2019) analyzed\nspecial cases of reconstructing labeled trees. In this work, we significantly\nexpand our understanding of this problem by giving general results in the case\nof arbitrary trees. Namely, we give: a reduction from the tree trace\nreconstruction problem to the more classical string reconstruction problem when\nthe tree topology is known, a lower bound for learning arbitrary tree\ntopologies, and a general algorithm for learning the topology of any tree using\ntechniques of Nazarov and Peres (2017). We conclude by discussing why arbitrary\ntrees require exponentially many samples under the left propagation model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 13:49:37 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Maranzatto", "Thomas", ""], ["Reyzin", "Lev", ""]]}, {"id": "2102.03277", "submitter": "Llu\\'is Alemany-Puig", "authors": "Llu\\'is Alemany-Puig, Juan Luis Esteban, Ramon Ferrer-i-Cancho", "title": "Minimum projective linearizations of trees in linear time", "comments": "Improved connection with previous Iordanskii's works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run undoubtlessly in\n$O(n)$-time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:35:38 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:20:33 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 14:02:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Alemany-Puig", "Llu\u00eds", ""], ["Esteban", "Juan Luis", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "2102.03304", "submitter": "Sharat Ibrahimpur", "authors": "Sylvia Boyd, Joseph Cheriyan, Arash Haddadan and Sharat Ibrahimpur", "title": "A $2$-Approximation Algorithm for Flexible Graph Connectivity", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $2$-approximation algorithm for the Flexible Graph Connectivity\nproblem [AHM20] via a reduction to the minimum cost $r$-out $2$-arborescence\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:23:06 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Boyd", "Sylvia", ""], ["Cheriyan", "Joseph", ""], ["Haddadan", "Arash", ""], ["Ibrahimpur", "Sharat", ""]]}, {"id": "2102.03311", "submitter": "Shahin Kamali", "authors": "Spyros Angelopoulos and Shahin Kamali and Kimia Shadkami", "title": "Online Bin Packing with Predictions", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin packing is a classic optimization problem with a wide range of\napplications from load balancing in networks to supply chain management. In\nthis work we study the online variant of the problem, in which a sequence of\nitems of various sizes must be placed into a minimum number of bins of uniform\ncapacity. The online algorithm is enhanced with a (potentially erroneous)\nprediction concerning the frequency of item sizes in the sequence. We design\nand analyze online algorithms with efficient tradeoffs between consistency\n(i.e., the competitive ratio assuming no prediction error) and robustness\n(i.e., the competitive ratio under adversarial error), and whose performance\ndegrades gently as a function of the prediction error. This is the first\ntheoretical study of online bin packing in the realistic setting of erroneous\npredictions, as well as the first experimental study in the setting in which\nthe input is generated according to both static and evolving distributions.\nPrevious work on this problem has only addressed the extreme cases with respect\nto the prediction error, has relied on overly powerful and error-free\nprediction oracles, and has focused on experimental evaluation based on static\ninput distributions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:32:52 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:39:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Kamali", "Shahin", ""], ["Shadkami", "Kimia", ""]]}, {"id": "2102.03404", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Marin Bougeret, Victor A. Campos, Ignasi Sau", "title": "Parameterized complexity of computing maximum minimal blocking and\n  hitting sets", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blocking set in a graph $G$ is a subset of vertices that intersects every\nmaximum independent set of $G$. Let ${\\sf mmbs}(G)$ be the size of a maximum\n(inclusion-wise) minimal blocking set of $G$. This parameter has recently\nplayed an important role in the kernelization of Vertex Cover parameterized by\nthe distance to a graph class ${\\cal F}$. Indeed, it turns out that the\nexistence of a polynomial kernel for this problem is closely related to the\nproperty that ${\\sf mmbs}({\\cal F})=\\sup_{G \\in {\\cal F}}{\\sf mmbs}(G)$ is\nbounded by a constant, and thus several recent results focused on determining\n${\\sf mmbs}({\\cal F})$ for different classes ${\\cal F}$. We consider the\nparameterized complexity of computing ${\\sf mmbs}$ under various\nparameterizations, such as the size of a maximum independent set of the input\ngraph and the natural parameter. We provide a panorama of the complexity of\ncomputing both ${\\sf mmbs}$ and ${\\sf mmhs}$, which is the size of a maximum\nminimal hitting set of a hypergraph, a closely related parameter. Finally, we\nconsider the problem of computing ${\\sf mmbs}$ parameterized by treewidth,\nespecially relevant in the context of kernelization. Given the \"counting\"\nnature of ${\\sf mmbs}$, it does not seem to be expressible in monadic\nsecond-order logic, hence its tractability does not follow from Courcelle's\ntheorem. Our main technical contribution is a fixed-parameter tractable\nalgorithm for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 20:00:17 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Bougeret", "Marin", ""], ["Campos", "Victor A.", ""], ["Sau", "Ignasi", ""]]}, {"id": "2102.03434", "submitter": "Aritra Konar", "authors": "Aritra Konar and Nicholas D. Sidiropoulos", "title": "Exploring the Subgraph Density-Size Trade-off via the Lov\\'asz Extension", "comments": "Accepted for publication at ACM WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph, the Densest-k-Subgraph problem (DkS) seeks to find\na subset of k vertices such that the sum of the edge weights in the\ncorresponding subgraph is maximized. The problem is known to be NP-hard, and is\nalso very difficult to approximate, in the worst-case. In this paper, we\npresent a new convex relaxation for the problem. Our key idea is to reformulate\nDkS as minimizing a submodular function subject to a cardinality constraint.\nExploiting the fact that submodular functions possess a convex, continuous\nextension (known as the Lov\\'asz extension), we propose to minimize the\nLov\\'asz extension over the convex hull of the cardinality constraints.\nAlthough the Lov\\'asz extension of a submodular function does not admit an\nanalytical form in general, for DkS we show that it does. We leverage this\nresult to develop a highly scalable algorithm based on the Alternating\nDirection Method of Multipliers (ADMM) for solving the relaxed problem. Coupled\nwith a pair of fortuitously simple rounding schemes, we demonstrate that our\napproach outperforms existing baselines on real-world graphs and can yield high\nquality sub-optimal solutions which typically are a posteriori no worse than\n65-80\\% of the optimal density.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 22:09:21 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Konar", "Aritra", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "2102.03537", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco, Luisa Gargano and Adele Anna Rescigno", "title": "Parameterized Complexity of Immunization in the Threshold Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider the problem of controlling the spread of harmful items in\nnetworks, such as the contagion proliferation of diseases or the diffusion of\nfake news. We assume the linear threshold model of diffusion where each node\nhas a threshold that measures the node resistance to the contagion. We study\nthe parameterized complexity of the problem: Given a network, a set of\ninitially contaminated nodes, and two integers $k$ and $\\ell$, is it possible\nto limit the diffusion to at most $k$ other nodes of the network by immunizing\nat most $\\ell$ nodes? We consider several parameters associated to the input,\nincluding: the bounds $k$ and $\\ell$, the maximum node degree $\\Delta$, the\ntreewidth, and the neighborhood diversity of the network. We first give $W[1]$\nor $W[2]$-hardness results for each of the considered parameters. Then we give\nfixed-parameter algorithms for some parameter combinations.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 08:52:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Rescigno", "Adele Anna", ""]]}, {"id": "2102.03646", "submitter": "Rachel Ward", "authors": "De Huang and Jonathan Niles-Weed and Rachel Ward", "title": "Streaming k-PCA: Efficient guarantees for Oja's algorithm, beyond\n  rank-one updates", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze Oja's algorithm for streaming $k$-PCA and prove that it achieves\nperformance nearly matching that of an optimal offline algorithm. Given access\nto a sequence of i.i.d. $d \\times d$ symmetric matrices, we show that Oja's\nalgorithm can obtain an accurate approximation to the subspace of the top $k$\neigenvectors of their expectation using a number of samples that scales\npolylogarithmically with $d$. Previously, such a result was only known in the\ncase where the updates have rank one. Our analysis is based on recently\ndeveloped matrix concentration tools, which allow us to prove strong bounds on\nthe tails of the random matrices which arise in the course of the algorithm's\nexecution.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 19:21:24 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Huang", "De", ""], ["Niles-Weed", "Jonathan", ""], ["Ward", "Rachel", ""]]}, {"id": "2102.03857", "submitter": "Siddharth Gupta", "authors": "Siddharth Gupta and Meirav Zehavi", "title": "Multivariate Analysis of Scheduling Fair Competitions", "comments": "To appear in the Proceedings of the 20th International Conference on\n  Autonomous Agents and Multiagent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{fair competition}, based on the concept of envy-freeness, is a\nnon-eliminating competition where each contestant (team or individual player)\nmay not play against all other contestants, but the total difficulty for each\ncontestant is the same: the sum of the initial rankings of the opponents for\neach contestant is the same. Similar to other non-eliminating competitions like\nthe Round-robin competition or the Swiss-system competition, the winner of the\nfair competition is the contestant who wins the most games. The {\\sc Fair\nNon-Eliminating Tournament} ({\\sc Fair-NET}) problem can be used to schedule\nfair competitions whose infrastructure is known. In the {\\sc Fair-NET} problem,\nwe are given an infrastructure of a tournament represented by a graph $G$ and\nthe initial rankings of the contestants represented by a multiset of integers\n$S$. The objective is to decide whether $G$ is \\emph{$S$-fair}, i.e., there\nexists an assignment of the contestants to the vertices of $G$ such that the\nsum of the rankings of the neighbors of each contestant in $G$ is the same\nconstant $k\\in\\mathbb{N}$. We initiate a study of the classical and\nparameterized complexity of {\\sc Fair-NET} with respect to several central\nstructural parameters motivated by real world scenarios, thereby presenting a\ncomprehensive picture of it.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 17:35:09 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Gupta", "Siddharth", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2102.03961", "submitter": "Diego D\\'iaz-Dom\\'inguez", "authors": "Diego Diaz-Dominguez annd Gonzalo Navarro", "title": "Efficient construction of the extended BWT from grammar-compressed DNA\n  sequencing reads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an algorithm for building the extended BWT (eBWT) of a string\ncollection from its grammar-compressed representation. Our technique exploits\nthe string repetitions captured by the grammar to boost the computation of the\neBWT. Thus, the more repetitive the collection is, the lower are the resources\nwe use per input symbol. We rely on a new grammar recently proposed at DCC'21\nwhose nonterminals serve as building blocks for inducing the eBWT. A relevant\napplication for this idea is the construction of self-indexes for analyzing\nsequencing reads -- massive and repetitive string collections of raw genomic\ndata. Self-indexes have become increasingly popular in Bioinformatics as they\ncan encode more information in less space. Our efficient eBWT construction\nopens the door to perform accurate bioinformatic analyses on more massive\nsequence datasets, which are not tractable with current eBWT construction\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 02:10:34 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Navarro", "Diego Diaz-Dominguez annd Gonzalo", ""]]}, {"id": "2102.03977", "submitter": "Sainyam Galhotra", "authors": "Sainyam Galhotra, Sandhya Saisubramanian and Shlomo Zilberstein", "title": "Learning to Generate Fair Clusters from Demonstrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fair clustering is the process of grouping similar entities together, while\nsatisfying a mathematically well-defined fairness metric as a constraint. Due\nto the practical challenges in precise model specification, the prescribed\nfairness constraints are often incomplete and act as proxies to the intended\nfairness requirement, leading to biased outcomes when the system is deployed.\nWe examine how to identify the intended fairness constraint for a problem based\non limited demonstrations from an expert. Each demonstration is a clustering\nover a subset of the data.\n  We present an algorithm to identify the fairness metric from demonstrations\nand generate clusters using existing off-the-shelf clustering techniques, and\nanalyze its theoretical properties. To extend our approach to novel fairness\nmetrics for which clustering algorithms do not currently exist, we present a\ngreedy method for clustering. Additionally, we investigate how to generate\ninterpretable solutions using our approach. Empirical evaluation on three\nreal-world datasets demonstrates the effectiveness of our approach in quickly\nidentifying the underlying fairness and interpretability constraints, which are\nthen used to generate fair and interpretable clusters.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:09:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Saisubramanian", "Sandhya", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "2102.04187", "submitter": "Luiz Fernando Afra Brito", "authors": "Luiz F. Afra Brito and Marcelo Albertini and Arnaud Casteigts and\n  Bruno A. N. Traven\\c{c}olo", "title": "A Dynamic Data Structure for Temporal Reachability with Unsorted Contact\n  Insertions", "comments": "16 pages, 3 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal graphs represent interactions between entities over the time. These\ninteractions may be direct (a contact between two nodes at some time instant),\nor indirect, through sequences of contacts called temporal paths (journeys).\nDeciding whether an entity can reach another through a journey is useful for\nvarious applications in communication networks and epidemiology, among other\nfields. In this paper, we present a data structure which maintains temporal\nreachability information under the addition of new contacts (i.e., triplets\n$(u,v,t)$ indicating that node $u$ and node $v$ interacted at time $t$). In\ncontrast to previous works, the contacts can be inserted in arbitrary order --\nin particular, non-chronologically -- which corresponds to systems where the\ninformation is collected a posteriori (e.g. when trying to reconstruct\ncontamination chains among people). The main component of our data structure is\na generalization of transitive closure called timed transitive closure (TTC),\nwhich allows us to maintain reachability information relative to all nested\ntime intervals, without storing all these intervals, nor the journeys\nthemselves. TTCs are of independent interest and we study a number of their\ngeneral properties. Let $n$ be the number of nodes and $\\tau$ be the number of\ntimestamps in the lifetime of the temporal graph. Our data structure answers\nreachability queries regarding the existence of a journey from a given node to\nanother within given time interval in time $O(\\log\\tau)$; it has an amortized\ninsertion time of $O(n^2\\log\\tau)$; and it can reconstruct a valid journey that\nwitnesses reachability in time $O(k\\log\\tau)$, where $k<n$ is the maximum\nnumber of edges of this journey. Finally, the space complexity of our\nreachability data structure is $O(n^2\\tau)$, which remains within the\nworst-case size of the temporal graph itself.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 13:41:36 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:18:27 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Brito", "Luiz F. Afra", ""], ["Albertini", "Marcelo", ""], ["Casteigts", "Arnaud", ""], ["Traven\u00e7olo", "Bruno A. N.", ""]]}, {"id": "2102.04325", "submitter": "Calum MacRury", "authors": "Allan Borodin, Calum MacRury, Akash Rakheja", "title": "Prophet Inequality Matching Meets Probing with Commitment", "comments": "Corrected typos and added details to various proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within the context of stochastic probing with commitment, we consider the\nonline stochastic matching problem for bipartite graphs where edges adjacent to\nan online node must be probed to determine if they exist, based on known edge\nprobabilities. If a probed edge exists, it must be used in the matching (if\npossible). In addition to improving upon existing stochastic bipartite matching\nresults, our results can also be seen as extensions to multi-item prophet\ninequalities. We study this matching problem for given constraints on the\nallowable sequences of probes adjacent to an online node. Our setting\ngeneralizes the patience (or time-out) constraint which limits the number of\nprobes that can be made to edges. The generality of our setting leads to some\nmodelling and computational efficiency issues that are not encountered in\nprevious works. We establish new competitive bounds all of which generalize the\nstandard non-stochastic setting when edges do not need to be probed (i.e.,\nexist with certainty). Specifically, we establish the following competitive\nratio results for a general formulation of edge constraints, arbitrary edge\nweights, and arbitrary edge probabilities:\n  (1) A tight $\\frac{1}{2}$ ratio when the stochastic graph is generated from a\nknown stochastic type graph where the $\\pi(i)^{th}$ online node is drawn\nindependently from a known distribution ${\\cal D}_{\\pi(i)}$ and $\\pi$ is chosen\nadversarially. We refer to this setting as the known i.d. stochastic matching\nproblem with adversarial arrivals.\n  (2) A $1-1/e$ ratio when the stochastic graph is generated from a known\nstochastic type graph where the $\\pi(i)^{th}$ online node is drawn\nindependently from a known distribution ${\\cal D}_{\\pi(i)}$ and $\\pi$ is a\nrandom permutation. This is referred to as the known i.d. stochastic matching\nproblem with random order arrivals.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:29:56 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 17:06:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Borodin", "Allan", ""], ["MacRury", "Calum", ""], ["Rakheja", "Akash", ""]]}, {"id": "2102.04348", "submitter": "Paritosh Garg", "authors": "Paritosh Garg, Linus Jordan, Ola Svensson", "title": "Semi-Streaming Algorithms for Submodular Matroid Intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the basic greedy algorithm gives a semi-streaming algorithm with an\napproximation guarantee of $2$ for the \\emph{unweighted} matching problem, it\nwas only recently that Paz and Schwartzman obtained an analogous result for\nweighted instances. Their approach is based on the versatile local ratio\ntechnique and also applies to generalizations such as weighted hypergraph\nmatchings. However, the framework for the analysis fails for the related\nproblem of weighted matroid intersection and as a result the approximation\nguarantee for weighted instances did not match the factor $2$ achieved by the\ngreedy algorithm for unweighted instances. Our main result closes this gap by\ndeveloping a semi-streaming algorithm with an approximation guarantee of\n$2+\\epsilon$ for \\emph{weighted} matroid intersection, improving upon the\nprevious best guarantee of $4+\\epsilon$. Our techniques also allow us to\ngeneralize recent results by Levin and Wajc on submodular maximization subject\nto matching constraints to that of matroid-intersection constraints.\n  While our algorithm is an adaptation of the local ratio technique used in\nprevious works, the analysis deviates significantly and relies on structural\nproperties of matroid intersection, called kernels. Finally, we also conjecture\nthat our algorithm gives a $(k+\\epsilon)$ approximation for the intersection of\n$k$ matroids but prove that new tools are needed in the analysis as the used\nstructural properties fail for $k\\geq 3$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:50:51 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Garg", "Paritosh", ""], ["Jordan", "Linus", ""], ["Svensson", "Ola", ""]]}, {"id": "2102.04401", "submitter": "Nikos Zarifis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis", "title": "The Optimality of Polynomial Regression for Agnostic Learning under\n  Gaussian Marginals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of agnostic learning under the Gaussian distribution. We\ndevelop a method for finding hard families of examples for a wide class of\nproblems by using LP duality. For Boolean-valued concept classes, we show that\nthe $L^1$-regression algorithm is essentially best possible, and therefore that\nthe computational difficulty of agnostically learning a concept class is\nclosely related to the polynomial degree required to approximate any function\nfrom the class in $L^1$-norm. Using this characterization along with additional\nanalytic tools, we obtain optimal SQ lower bounds for agnostically learning\nlinear threshold functions and the first non-trivial SQ lower bounds for\npolynomial threshold functions and intersections of halfspaces. We also develop\nan analogous theory for agnostically learning real-valued functions, and as an\napplication prove near-optimal SQ lower bounds for agnostically learning ReLUs\nand sigmoids.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:06:32 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Pittas", "Thanasis", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2102.04539", "submitter": "Leon Kellerhals", "authors": "Till Fluschnik and Leon Kellerhals", "title": "Placing Green Bridges Optimally, with a Multivariate Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of placing wildlife crossings, such as green bridges,\nover human-made obstacles to challenge habitat fragmentation. The main task\nherein is, given a graph describing habitats or routes of wildlife animals and\npossibilities of building green bridges, to find a low-cost placement of green\nbridges that connects the habitats. We develop different problem models for\nthis task and study them from a computational complexity and parameterized\nalgorithmics perspective.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:30:33 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Fluschnik", "Till", ""], ["Kellerhals", "Leon", ""]]}, {"id": "2102.04546", "submitter": "Alexandre Nolin", "authors": "Magn\\'us M. Halld\\'orsson, Alexandre Nolin", "title": "Superfast Coloring in CONGEST via Efficient Color Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a procedure for efficiently sampling colors in the {\\congest}\nmodel. It allows nodes whose number of colors exceeds their number of neighbors\nby a constant fraction to sample up to $\\Theta(\\log n)$ semi-random colors\nunused by their neighbors in $O(1)$ rounds, even in the distance-2 setting.\nThis yields algorithms with $O(\\log^* \\Delta)$ complexity for different\nedge-coloring, vertex coloring, and distance-2 coloring problems, matching the\nbest possible. In particular, we obtain an $O(\\log^* \\Delta)$-round CONGEST\nalgorithm for $(1+\\epsilon)\\Delta$-edge coloring when $\\Delta \\ge\n\\log^{1+1/\\log^*n} n$, and a poly($\\log\\log n$)-round algorithm for\n$(2\\Delta-1)$-edge coloring in general. The sampling procedure is inspired by a\nseminal result of Newman in communication complexity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:49:26 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 14:29:37 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Nolin", "Alexandre", ""]]}, {"id": "2102.04633", "submitter": "Daniel Selsam", "authors": "Daniel Selsam and Jesse Michael Han", "title": "$k$-Equivalence Relations and Associated Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lines and circles pose significant scalability challenges in synthetic\ngeometry. A line with $n$ points implies ${n \\choose 3}$ collinearity atoms, or\nalternatively, when lines are represented as functions, equality among ${n\n\\choose 2}$ different lines. Similarly, a circle with $n$ points implies ${n\n\\choose 4}$ cocyclicity atoms or equality among ${n \\choose 3}$ circumcircles.\nWe introduce a new mathematical concept of $k$-equivalence relations, which\ngeneralizes equality ($k=1$) and includes both lines ($k=2$) and circles\n($k=3$), and present an efficient proof-producing procedure to compute the\nclosure of a $k$-equivalence relation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 03:55:09 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Selsam", "Daniel", ""], ["Han", "Jesse Michael", ""]]}, {"id": "2102.04707", "submitter": "Nikolas M\\\"ahlmann", "authors": "Nikolas M\\\"ahlmann, Sebastian Siebertz, Alexandre Vigny", "title": "Recursive Backdoors for SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A strong backdoor in a formula $\\phi$ of propositional logic to a tractable\nclass $\\mathcal{C}$ of formulas is a set $B$ of variables of $\\phi$ such that\nevery assignment of the variables in $B$ results in a formula from\n$\\mathcal{C}$. Strong backdoors of small size or with a good structure, e.g.\nwith small backdoor treewidth, lead to efficient solutions for the\npropositional satisfiability problem SAT. In this paper we propose the new\nnotion of recursive backdoors, which is inspired by the observation that in\norder to solve SAT we can independently recurse into the components that are\ncreated by partial assignments of variables. The quality of a recursive\nbackdoor is measured by its recursive backdoor depth. Similar to the concept of\nbackdoor treewidth, recursive backdoors of bounded depth include backdoors of\nunbounded size that have a certain treelike structure. However, the two\nconcepts are incomparable and our results yield new tractability results for\nSAT.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 08:53:57 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["M\u00e4hlmann", "Nikolas", ""], ["Siebertz", "Sebastian", ""], ["Vigny", "Alexandre", ""]]}, {"id": "2102.04765", "submitter": "Xianghui Zhong", "authors": "Xianghui Zhong", "title": "Lower Bounds on the Integraliy Ratio of the Subtour LP for the Traveling\n  Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate instances with high integrality ratio of the\nsubtour LP. We develop a procedure to generate families of Euclidean TSP\ninstances whose integrality ratios converge to $\\frac{4}{3}$ and may have a\ndifferent structure than the instances currently known from the literature.\nMoreover, we compute the instances maximizing the integrality ratio for\nRectilinear TSP with up to 10 vertices. Based on these instances we give\nfamilies of instances whose integrality ratio converge to $\\frac{4}{3}$ for\nRectilinear, Multidimensional Rectilinear and Euclidean TSP that have similar\nstructures. We show that our instances for Multidimensional Rectilinear TSP and\nthe known instances for Metric TSP maximize the integrality ratio under certain\nassumptions. We also investigate the concept of local optimality with respect\nto integrality ratio and develop several algorithms to find instances with high\nintegrality ratio. Furthermore, we describe a family of instances that are hard\nto solve in practice. The currently fastest TSP solver Concorde needs more than\ntwo days to solve an instance from the family with 52 vertices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:30:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhong", "Xianghui", ""]]}, {"id": "2102.04770", "submitter": "Konstantin Kutzkov", "authors": "Konstantin Kutzkov", "title": "COLOGNE: Coordinated Local Graph Neighborhood Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representation learning for graphs enables the application of standard\nmachine learning algorithms and data analysis tools to graph data. Replacing\ndiscrete unordered objects such as graph nodes by real-valued vectors is at the\nheart of many approaches to learning from graph data. Such vector\nrepresentations, or embeddings, capture the discrete relationships in the\noriginal data by representing nodes as vectors in a high-dimensional space.\n  In most applications graphs model the relationship between real-life objects\nand often nodes contain valuable meta-information about the original objects.\nWhile being a powerful machine learning tool, embeddings are not able to\npreserve such node attributes. We address this shortcoming and consider the\nproblem of learning discrete node embeddings such that the coordinates of the\nnode vector representations are graph nodes. This opens the door to designing\ninterpretable machine learning algorithms for graphs as all attributes\noriginally present in the nodes are preserved.\n  We present a framework for coordinated local graph neighborhood sampling\n(COLOGNE) such that each node is represented by a fixed number of graph nodes,\ntogether with their attributes. Individual samples are coordinated and they\npreserve the similarity between node neighborhoods. We consider different\nnotions of similarity for which we design scalable algorithms. We show\ntheoretical results for all proposed algorithms. Experiments on benchmark\ngraphs evaluate the quality of the designed embeddings and demonstrate how the\nproposed embeddings can be used in training interpretable machine learning\nalgorithms for graph data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:39:06 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kutzkov", "Konstantin", ""]]}, {"id": "2102.04931", "submitter": "Stefan Steinerberger", "authors": "Stefan Steinerberger", "title": "Max-Cut via Kuramoto-type Oscillators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max-Cut problem. Let $G = (V,E)$ be a graph with adjacency\nmatrix $(a_{ij})_{i,j=1}^{n}$. Burer, Monteiro & Zhang proposed to find, for\n$n$ angles $\\left\\{\\theta_1, \\theta_2, \\dots, \\theta_n\\right\\} \\subset [0,\n2\\pi]$, minima of the energy $$ f(\\theta_1, \\dots, \\theta_n) = \\sum_{i,j=1}^{n}\na_{ij} \\cos{(\\theta_i - \\theta_j)}$$ because configurations achieving a global\nminimum leads to a partition of size 0.878 Max-Cut(G). This approach is known\nto be computationally viable and leads to very good results in practice. We\nprove that by replacing $\\cos{(\\theta_i - \\theta_j)}$ with an explicit function\n$g_{\\varepsilon}(\\theta_i - \\theta_j)$ global minima of this new functional\nlead to a $(1-\\varepsilon)$Max-Cut(G). This suggests some interesting\nalgorithms that perform well. It also shows that the problem of finding\napproximate global minima of energy functionals of this type is NP-hard in\ngeneral.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:41:00 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Steinerberger", "Stefan", ""]]}, {"id": "2102.04984", "submitter": "Will Perkins", "authors": "Ewan Davies and Will Perkins", "title": "Approximately counting independent sets of a given size in\n  bounded-degree graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the computational complexity of approximately counting and\nsampling independent sets of a given size in bounded-degree graphs. That is, we\nidentify a critical density $\\alpha_c(\\Delta)$ and provide (i) for $\\alpha <\n\\alpha_c(\\Delta)$ randomized polynomial-time algorithms for approximately\nsampling and counting independent sets of given size at most $\\alpha n$ in\n$n$-vertex graphs of maximum degree $\\Delta$; and (ii) a proof that unless\nNP=RP, no such algorithms exist for $\\alpha>\\alpha_c(\\Delta)$. The critical\ndensity is the occupancy fraction of hard core model on the clique\n$K_{\\Delta+1}$ at the uniqueness threshold on the infinite $\\Delta$-regular\ntree, giving $\\alpha_c(\\Delta)\\sim\\frac{e}{1+e}\\frac{1}{\\Delta}$ as\n$\\Delta\\to\\infty$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:57:26 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Davies", "Ewan", ""], ["Perkins", "Will", ""]]}, {"id": "2102.05028", "submitter": "Cyrus Hettle", "authors": "Cyrus Hettle, Shixiang Zhu, Swati Gupta, Yao Xie", "title": "Balanced Districting on Grid Graphs with Provable Compactness and\n  Contiguity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G = (V,E)$ with vertex weights $w(v)$ and a desired number of\nparts $k$, the goal in graph partitioning problems is to partition the vertex\nset V into parts $V_1,\\ldots,V_k$. Metrics for compactness, contiguity, and\nbalance of the parts $V_i$ are frequent objectives, with much existing\nliterature focusing on compactness and balance. Revisiting an old method known\nas striping, we give the first polynomial-time algorithms with guaranteed\ncontiguity and provable bicriteria approximations for compactness and balance\nfor planar grid graphs. We consider several types of graph partitioning,\nincluding when vertex weights vary smoothly or are stochastic, reflecting\nconcerns in various real-world instances. We show significant improvements in\nexperiments for balancing workloads for the fire department and reducing\nover-policing using 911 call data from South Fulton, GA.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:46:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hettle", "Cyrus", ""], ["Zhu", "Shixiang", ""], ["Gupta", "Swati", ""], ["Xie", "Yao", ""]]}, {"id": "2102.05077", "submitter": "William Kuszmaul", "authors": "William Kuszmaul, Qi Qi", "title": "The Multiplicative Version of Azuma's Inequality, with an Application to\n  Contention Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Azuma's inequality is a tool for proving concentration bounds on random\nvariables. The inequality can be thought of as a natural generalization of\nadditive Chernoff bounds. On the other hand, the analogous generalization of\nmultiplicative Chernoff bounds has, to our knowledge, never been explicitly\nformulated.\n  We formulate a multiplicative-error version of Azuma's inequality. We then\nshow how to apply this new inequality in order to greatly simplify (and\ncorrect) the analysis of contention delays in multithreaded systems managed by\nrandomized work stealing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:06:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kuszmaul", "William", ""], ["Qi", "Qi", ""]]}, {"id": "2102.05168", "submitter": "Ellis Hershkowitz", "authors": "Bernhard Haeupler and D Ellis Hershkowitz and Goran Zuzic", "title": "Deterministic Tree Embeddings with Copies for Algorithms Against\n  Adaptive Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of graphs into distributions of trees that preserve distances in\nexpectation are a cornerstone of many optimization algorithms. Unfortunately,\nonline or dynamic algorithms which use these embeddings seem inherently\nrandomized and ill-suited against adaptive adversaries.\n  In this paper we provide a new tree embedding which addresses these issues by\ndeterministically embedding a graph into a single tree containing $O(\\log n)$\ncopies of each vertex while preserving the connectivity structure of every\nsubgraph and $O(\\log^2 n)$-approximating the cost of every subgraph.\n  Using this embedding we obtain several new algorithmic results: We reduce an\nopen question of Alon et al. [SODA 2004] -- the existence of a deterministic\npoly-log-competitive algorithm for online group Steiner tree on a general graph\n-- to its tree case. We give a poly-log-competitive deterministic algorithm for\na closely related problem -- online partial group Steiner tree -- which,\nroughly, is a bicriteria version of online group Steiner tree. Lastly, we give\nthe first poly-log approximations for demand-robust Steiner forest, group\nSteiner tree and group Steiner forest.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 22:50:34 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Zuzic", "Goran", ""]]}, {"id": "2102.05174", "submitter": "Daniel Liang", "authors": "Aravind Gollakota and Daniel Liang", "title": "On the Hardness of PAC-learning stabilizer States with Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning stabilizer states with noise in the\nProbably Approximately Correct (PAC) framework of Aaronson (2007) for learning\nquantum states. In the noiseless setting, an algorithm for this problem was\nrecently given by Rocchetto (2018), but the noisy case was left open. Motivated\nby approaches to noise tolerance from classical learning theory, we introduce\nthe Statistical Query (SQ) model for PAC-learning quantum states, and prove\nthat algorithms in this model are indeed resilient to common forms of noise,\nincluding classification and depolarizing noise. We prove an exponential lower\nbound on learning stabilizer states in the SQ model. Even outside the SQ model,\nwe prove that learning stabilizer states with noise is in general as hard as\nLearning Parity with Noise (LPN) using classical examples. Our results position\nthe problem of learning stabilizer states as a natural quantum analogue of the\nclassical problem of learning parities: easy in the noiseless setting, but\nseemingly intractable even with simple forms of noise.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 23:06:54 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gollakota", "Aravind", ""], ["Liang", "Daniel", ""]]}, {"id": "2102.05209", "submitter": "Mohsen Heidari", "authors": "Mohsen Heidari and Wojciech Szpankowski", "title": "Quantum State Classification via Quantum Fourier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning from quantum data, in particular quantum state\nclassification which has applications, among others, in classifying the\nseparability of quantum states. In this learning model, there are $n$ quantum\nstates with classical labels as the training samples. Predictors are quantum\nmeasurements that when applied to the next unseen quantum state predict its\nclassical label. By integrating learning theory with quantum information, we\nintroduce a quantum counterpart of the PAC framework for learning with respect\nto classes of measurements. We argue that major challenges arising from the\nquantum nature of the problem are measurement incompatibility and the\nno-cloning principle -- prohibiting sample reuse. Then, after introducing a\nFourier expansion through Pauli's operators, we study learning with respect to\nan infinite class of quantum measurements whose operator's Fourier spectrum is\nconcentrated on low degree terms. We propose a quantum learning algorithm and\nshow that the quantum sample complexity depends on the ``compatibility\nstructure\" of such measurement classes -- the more compatible the class is, the\nlower the quantum sample complexity will be. We further introduce $k$-junta\nmeasurements as a special class of low-depth quantum circuits whose Fourier\nspectrum is concentrated on low degrees.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:20:55 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:38:59 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 22:11:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Heidari", "Mohsen", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "2102.05301", "submitter": "Daniel Anderson", "authors": "Daniel Anderson and Guy E. Blelloch", "title": "Parallel Minimum Cuts in $O(m \\log^2(n))$ Work and Low Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O(m \\log^2(n))$ work, $O(\\text{polylog}(n))$ depth parallel\nalgorithm for minimum cut. This algorithm matches the work of a recent\nsequential algorithm by Gawrychowski, Mozes, and Weimann [ICALP'20, (2020),\n57:1-57:15], and improves on the previously best known parallel algorithm by\nGeissmann and Gianinazzi [SPAA'18, (2018), pp. 1-11] which performs $O(m\n\\log^4(n))$ work in $O(\\text{polylog}(n))$ depth.\n  Our algorithm makes use of three components that might be of independent\ninterest. Firstly, we design a parallel data structure for dynamic trees that\nsolves mixed batches of queries and weight updates in low depth. It generalizes\nand improves the work bounds of a previous data structure of Geissmann and\nGianinazzi and is work efficient with respect to the best sequential algorithm.\nSecondly, we design a parallel algorithm for approximate minimum cut that\nimproves on previous results by Karger and Motwani. We use this algorithm to\ngive a work-efficient procedure to produce a tree packing, as in Karger's\nsequential algorithm for minimum cuts. Lastly, we design a work-efficient\nparallel algorithm for solving the minimum $2$-respecting cut problem.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 07:56:02 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""]]}, {"id": "2102.05347", "submitter": "Thuy Duong Vuong", "authors": "Nima Anari and Thuy-Duong Vuong", "title": "From Sampling to Optimization on Discrete Domains withApplications to\n  Determinant Maximization", "comments": "Replacement, with significant new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a connection between sampling and optimization on discrete domains.\nFor a family of distributions $\\mu$ defined on size $k$ subsets of a ground set\nof elements that is closed under external fields, we show that rapid mixing of\nnatural local random walks implies the existence of simple approximation\nalgorithms to find $\\max \\mu(\\cdot)$. More precisely we show that if\n(multi-step) down-up random walks have spectral gap at least inverse\npolynomially large in $k$, then (multi-step) local search can find $\\max\n\\mu(\\cdot)$ within a factor of $k^{O(k)}$. As the main application of our\nresult, we show a simple nearly-optimal $k^{O(k)}$-factor approximation\nalgorithm for MAP inference on nonsymmetric DPPs. This is the first nontrivial\nmultiplicative approximation for finding the largest size $k$ principal minor\nof a square (not-necessarily-symmetric) matrix $L$ with $L+L^\\intercal\\succeq\n0$.\n  We establish the connection between sampling and optimization by showing that\nan exchange inequality, a concept rooted in discrete convex analysis, can be\nderived from fast mixing of local random walks. We further connect exchange\ninequalities with composable core-sets for optimization, generalizing recent\nresults on composable core-sets for DPP maximization to arbitrary distributions\nthat satisfy either the strongly Rayleigh property or that have a log-concave\ngenerating polynomial.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:34:44 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 21:07:33 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Anari", "Nima", ""], ["Vuong", "Thuy-Duong", ""]]}, {"id": "2102.05548", "submitter": "Joakim Blikstad", "authors": "Joakim Blikstad, Jan van den Brand, Sagnik Mukhopadhyay, Danupon\n  Nanongkai", "title": "Breaking the Quadratic Barrier for Matroid Intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matroid intersection problem is a fundamental problem that has been\nextensively studied for half a century. In the classic version of this problem,\nwe are given two matroids $\\mathcal{M}_1 = (V, \\mathcal{I}_1)$ and\n$\\mathcal{M}_2 = (V, \\mathcal{I}_2)$ on a comment ground set $V$ of $n$\nelements, and then we have to find the largest common independent set $S \\in\n\\mathcal{I}_1 \\cap \\mathcal{I}_2$ by making independence oracle queries of the\nform \"Is $S \\in \\mathcal{I}_1$?\" or \"Is $S \\in \\mathcal{I}_2$?\" for $S\n\\subseteq V$. The goal is to minimize the number of queries.\n  Beating the existing $\\tilde O(n^2)$ bound, known as the quadratic barrier,\nis an open problem that captures the limits of techniques from two lines of\nwork. The first one is the classic Cunningham's algorithm [SICOMP 1986], whose\n$\\tilde O(n^2)$-query implementations were shown by CLS+ [FOCS 2019] and Nguyen\n[2019]. The other one is the general cutting plane method of Lee, Sidford, and\nWong [FOCS 2015]. The only progress towards breaking the quadratic barrier\nrequires either approximation algorithms or a more powerful rank oracle query\n[CLS+ FOCS 2019]. No exact algorithm with $o(n^2)$ independence queries was\nknown.\n  In this work, we break the quadratic barrier with a randomized algorithm\nguaranteeing $\\tilde O(n^{9/5})$ independence queries with high probability,\nand a deterministic algorithm guaranteeing $\\tilde O(n^{11/6})$ independence\nqueries. Our key insight is simple and fast algorithms to solve a graph\nreachability problem that arose in the standard augmenting path framework\n[Edmonds 1968]. Combining this with previous exact and approximation algorithms\nleads to our results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 16:33:14 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 13:13:14 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Blikstad", "Joakim", ""], ["Brand", "Jan van den", ""], ["Mukhopadhyay", "Sagnik", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "2102.05566", "submitter": "Xiaoyuan Liu", "authors": "Xiaoyuan Liu, Anthony Angone, Ruslan Shaydulin, Ilya Safro, Yuri\n  Alexeev, Lukasz Cincio", "title": "Layer VQE: A Variational Approach for Combinatorial Optimization on\n  Noisy Quantum Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-21-20623", "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization on near-term quantum devices is a promising path\nto demonstrating quantum advantage. However, the capabilities of these devices\nare constrained by high noise levels and limited error mitigation. In this\npaper, we propose an iterative Layer VQE (L-VQE) approach, inspired by the\nVariational Quantum Eigensolver (VQE). We present a large-scale numerical\nstudy, simulating circuits with up to 40 qubits and 352 parameters, that\ndemonstrates the potential of the proposed approach. We evaluate quantum\noptimization heuristics on the problem of detecting multiple communities in\nnetworks, for which we introduce a novel qubit-frugal formulation. We\nnumerically compare L-VQE with QAOA and demonstrate that QAOA achieves lower\napproximation ratios while requiring significantly deeper circuits. We show\nthat L-VQE is more robust to sampling noise and has a higher chance of finding\nthe solution as compared with standard VQE approaches. Our simulation results\nshow that L-VQE performs well under realistic hardware noise.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 16:53:22 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Liu", "Xiaoyuan", ""], ["Angone", "Anthony", ""], ["Shaydulin", "Ruslan", ""], ["Safro", "Ilya", ""], ["Alexeev", "Yuri", ""], ["Cincio", "Lukasz", ""]]}, {"id": "2102.05579", "submitter": "Maksim Nikolaev", "authors": "Maksim Nikolaev", "title": "All instantiations of the greedy algorithm for the shortest superstring\n  problem are equivalent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Shortest Common Superstring problem (SCS), one needs to find the\nshortest superstring for a set of strings. While SCS is NP-hard and\nMAX-SNP-hard, the Greedy Algorithm \"choose two strings with the largest\noverlap; merge them; repeat\" achieves a constant factor approximation that is\nknown to be at most 3.5 and conjectured to be equal to 2. The Greedy Algorithm\nis not deterministic, so its instantiations with different tie-breaking rules\nmay have different approximation factors. In this paper, we show that it is not\nthe case: all factors are equal. To prove this, we show how to transform a set\nof strings so that all overlaps are different whereas their ratios stay roughly\nthe same.\n  We also reveal connections between the original version of SCS and the\nfollowing one: find a~superstring minimizing the number of occurrences of a\ngiven symbol. It turns out that the latter problem is equivalent to the\noriginal one.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:21:39 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Nikolaev", "Maksim", ""]]}, {"id": "2102.05629", "submitter": "Nikos Zarifis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Christos Tzamos,\n  Nikos Zarifis", "title": "Agnostic Proper Learning of Halfspaces under Gaussian Marginals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of agnostically learning halfspaces under the Gaussian\ndistribution. Our main result is the {\\em first proper} learning algorithm for\nthis problem whose sample complexity and computational complexity qualitatively\nmatch those of the best known improper agnostic learner. Building on this\nresult, we also obtain the first proper polynomial-time approximation scheme\n(PTAS) for agnostically learning homogeneous halfspaces. Our techniques\nnaturally extend to agnostically learning linear models with respect to other\nnon-linear activations, yielding in particular the first proper agnostic\nalgorithm for ReLU regression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:40:44 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2102.05733", "submitter": "Francesco Betti Sorbelli", "authors": "Francesco Betti Sorbelli, Stefano Carpin, Federico Coro, Sajal K. Das,\n  Alfredo Navarra, Cristina M. Pinotti", "title": "Speeding up Routing Schedules on Aisle-Graphs with Single Access", "comments": "re-submitted revised version to IEEE Transactions on Robotics (T-RO)\n  after a conditionally accepted response", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the Orienteering Aisle-graphs Single-access Problem\n(OASP), a variant of the orienteering problem for a robot moving in a so-called\nsingle-access aisle-graph, i.e., a graph consisting of a set of rows that can\nbe accessed from one side only. Aisle-graphs model, among others, vineyards or\nwarehouses. Each aisle-graph vertex is associated with a reward that a robot\nobtains when visits the vertex itself. As the robot's energy is limited, only a\nsubset of vertices can be visited with a fully charged battery. The objective\nis to maximize the total reward collected by the robot with a battery charge.\nWe first propose an optimal algorithm that solves OASP in O(m^2 n^2) time for\naisle-graphs with a single access consisting of m rows, each with n vertices.\nWith the goal of designing faster solutions, we propose four greedy sub-optimal\nalgorithms that run in at most O(mn (m+n)) time. For two of them, we guarantee\nan approximation ratio of 1/2(1-1/e), where e is the base of the natural\nlogarithm, on the total reward by exploiting the well-known submodularity\nproperty. Experimentally, we show that these algorithms collect more than 80%\nof the optimal reward.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 20:55:08 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Sorbelli", "Francesco Betti", ""], ["Carpin", "Stefano", ""], ["Coro", "Federico", ""], ["Das", "Sajal K.", ""], ["Navarra", "Alfredo", ""], ["Pinotti", "Cristina M.", ""]]}, {"id": "2102.05747", "submitter": "Ruben Hoeksma", "authors": "Ruben Hoeksma and Matthew Maat", "title": "A better lower bound for Lower-Left Anchored Rectangle Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given any set of points $S$ in the unit square that contains the origin, does\na set of axis aligned rectangles, one for each point in $S$, exist, such that\neach of them has a point in $S$ as its lower-left corner, they are pairwise\ninterior disjoint, and the total area that they cover is at least 1/2? This\nquestion is also known as Freedman's conjecture (conjecturing that such a set\nof rectangles does exist) and has been open since Allen Freedman posed it in\n1969. In this paper, we improve the best known lower bound on the total area\nthat can be covered from 0.09121 to 0.1039. Although this step is small, we\nintroduce new insights that push the limits of this analysis.\n  Our lower bound uses a greedy algorithm with a particular order of the points\nin $S$. Therefore, it also implies that this greedy algorithm achieves an\napproximation ratio of 0.1039. We complement the result with an upper bound of\n3/4 on the approximation ratio for a natural class of greedy algorithms that\nincludes the one that achieves the lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 21:42:14 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Hoeksma", "Ruben", ""], ["Maat", "Matthew", ""]]}, {"id": "2102.05778", "submitter": "Yue Xie", "authors": "Yue Xie, Aneta Neumann, Frank Neumann, Andrew M. Sutton", "title": "Runtime Analysis of RLS and the (1+1) EA for the Chance-constrained\n  Knapsack Problem with Correlated Uniform Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing a complex real-world optimization problem is a challenging task.\nThe chance-constrained knapsack problem with correlated uniform weights plays\nan important role in the case where dependent stochastic components are\nconsidered. We perform runtime analysis of a randomized search algorithm (RSA)\nand a basic evolutionary algorithm (EA) for the chance-constrained knapsack\nproblem with correlated uniform weights. We prove bounds for both algorithms\nfor producing a feasible solution. Furthermore, we investigate the behavior of\nthe algorithms and carry out analyses on two settings: uniform profit value and\nthe setting in which every group shares an arbitrary profit profile. We provide\ninsight into the structure of these problems and show how the weight\ncorrelations and the different types of profit profiles influence the runtime\nbehavior of both algorithms in the chance-constrained setting.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 23:40:01 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Xie", "Yue", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""], ["Sutton", "Andrew M.", ""]]}, {"id": "2102.05782", "submitter": "Junyao Zhao", "authors": "Aviad Rubinstein and Junyao Zhao", "title": "Budget-Smoothed Analysis for Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy algorithm for submodular function maximization subject to\ncardinality constraint is guaranteed to approximate the optimal solution to\nwithin a $1-1/e$ factor. For worst-case instances, it is well known that this\nguarantee is essentially tight -- for greedy and in fact any efficient\nalgorithm. Motivated by the question of why greedy performs better in practice,\nwe introduce a new notion of budget smoothed analysis. Our framework requires\nlarger perturbations of budgets than traditional smoothed analysis for e.g.\nlinear programming. Nevertheless, we show that under realistic budget\ndistributions, greedy and related algorithms enjoy provably better\napproximation guarantees, that hold even for worst-case submodular functions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 23:51:36 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 01:54:58 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Rubinstein", "Aviad", ""], ["Zhao", "Junyao", ""]]}, {"id": "2102.05854", "submitter": "Venkata Naga Sreenivasulu Karnati", "authors": "Arindam Khan and Eklavya Sharma and K. V. N. Sreenivas", "title": "Approximation Algorithms for Generalized Multidimensional Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a generalization of the knapsack problem with geometric and vector\nconstraints. The input is a set of rectangular items, each with an associated\nprofit and $d$ nonnegative weights ($d$-dimensional vector), and a square\nknapsack. The goal is to find a non-overlapping axis-parallel packing of a\nsubset of items into the given knapsack such that the vector constraints are\nnot violated, i.e., the sum of weights of all the packed items in any of the\n$d$ dimensions does not exceed one. We consider two variants of the problem:\n$(i)$ the items are not allowed to be rotated, $(ii)$ items can be rotated by\n90 degrees.\n  We give a $(2+\\epsilon)$-approximation algorithm for this problem (both\nversions). In the process, we also study a variant of the maximum generalized\nassignment problem (Max-GAP), called Vector-Max-GAP, and design a PTAS for it.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 05:40:42 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Khan", "Arindam", ""], ["Sharma", "Eklavya", ""], ["Sreenivas", "K. V. N.", ""]]}, {"id": "2102.06043", "submitter": "Minh Ho\\`ang H\\`a", "authors": "Minh Ho\\`ang H\\`a, Dinh Quy Ta, Trung Thanh Nguyen", "title": "Exact Algorithms for Scheduling Problems on Parallel Identical Machines\n  with Conflict Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine scheduling problems involving conflict jobs can be seen as a\nconstrained version of the classical scheduling problem, in which some jobs are\nconflict in the sense that they cannot be proceeded simultaneously on different\nmachines. This conflict constraint naturally arises in several practical\napplications and has recently received considerable attentions in the research\ncommunity. In fact, the problem is typically NP-hard (even for approximation)\nand most of algorithmic results achieved so far have heavily relied on special\nstructures of the underlying graph used to model the conflict-job relation. Our\nfocus is on three objective functions: minimizing the makespan, minimizing the\nweighted summation of the jobs' completion time, and maximizing the total\nweights of completed jobs; the first two of which have been intensively studied\nin the literature. For each objective function considered, we present several\nmixed integer linear programming models and a constraint programming model,\nfrom which we can solve the problems to optimality using dedicated solvers.\nBinary search-based algorithms are also proposed to solve the makespan problem.\nThe results of numerical experiments performed on randomly generated data sets\nwith up to 32 jobs and 6 machines are reported and analysed to verify the\nperformance of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:50:42 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["H\u00e0", "Minh Ho\u00e0ng", ""], ["Ta", "Dinh Quy", ""], ["Nguyen", "Trung Thanh", ""]]}, {"id": "2102.06062", "submitter": "Pasin Manurangsi", "authors": "Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Chiyuan\n  Zhang", "title": "On Deep Learning with Label Differential Privacy", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, the training data can contain highly\nsensitive personal information. Training large-scale deep models that are\nguaranteed not to leak sensitive information while not compromising their\naccuracy has been a significant challenge. In this work, we study the\nmulti-class classification setting where the labels are considered sensitive\nand ought to be protected. We propose a new algorithm for training deep neural\nnetworks with label differential privacy, and run evaluations on several\ndatasets. For Fashion MNIST and CIFAR-10, we demonstrate that our algorithm\nachieves significantly higher accuracy than the state-of-the-art, and in some\nregimes comes close to the non-private baselines. We also provide non-trivial\ntraining results for the the challenging CIFAR-100 dataset. We complement our\nalgorithm with theoretical findings showing that in the setting of convex\nempirical risk minimization, the sample complexity of training with label\ndifferential privacy is dimension-independent, which is in contrast to vanilla\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 15:09:06 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ghazi", "Badih", ""], ["Golowich", "Noah", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""], ["Zhang", "Chiyuan", ""]]}, {"id": "2102.06068", "submitter": "Ajinkya Ramdas Gaikwad", "authors": "Ajinkya Gaikwad and Soumen Maity", "title": "Edge Deletion to Restrict the Size of an Epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a graph $G=(V,E)$, a set $\\mathcal{F}$ of forbidden subgraphs, we study\n$\\mathcal{F}$-Free Edge Deletion, where the goal is to remove minimum number of\nedges such that the resulting graph does not contain any $F\\in \\mathcal{F}$ as\na subgraph. For the parameter treewidth, the question of whether the problem is\nFPT has remained open. Here we give a negative answer by showing that the\nproblem is W[1]-hard when parameterized by the treewidth, which rules out FPT\nalgorithms under common assumption. Thus we give a solution to the conjecture\nposted by Jessica Enright and Kitty Meeks in [Algorithmica 80 (2018)\n1857-1889]. We also prove that the $\\mathcal{F}$-Free Edge Deletion problem is\nW[2]-hard when parameterized by the solution size $k$, feedback vertex set\nnumber or pathwidth of the input graph. A special case of particular interest\nis the situation in which $\\mathcal{F}$ is the set $\\mathcal{T}_{h+1}$ of all\ntrees on $h+1$ vertices, so that we delete edges in order to obtain a graph in\nwhich every component contains at most $h$ vertices. This is desirable from the\npoint of view of restricting the spread of disease in transmission network. We\nprove that the $\\mathcal{T}_{h+1}$-Free Edge Deletion problem is\nfixed-parameter tractable (FPT) when parameterized by the vertex cover number.\nWe also prove that it admits a kernel with $O(hk)$ vertices and $O(h^2k)$\nedges, when parameterized by combined parameters $h$ and the solution size $k$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 15:24:57 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Gaikwad", "Ajinkya", ""], ["Maity", "Soumen", ""]]}, {"id": "2102.06115", "submitter": "Ellis Hershkowitz", "authors": "D Ellis Hershkowitz, Anson Kahng, Dominik Peters, Ariel D. Procaccia", "title": "District-Fair Participatory Budgeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Participatory budgeting is a method used by city governments to select public\nprojects to fund based on residents' votes. Many cities use participatory\nbudgeting at a district level. Typically, a budget is divided among districts\nproportionally to their population, and each district holds an election over\nlocal projects and then uses its budget to fund the projects most preferred by\nits voters. However, district-level participatory budgeting can yield poor\nsocial welfare because it does not necessarily fund projects supported across\nmultiple districts. On the other hand, decision making that only takes global\nsocial welfare into account can be unfair to districts: A\nsocial-welfare-maximizing solution might not fund any of the projects preferred\nby a district, despite the fact that its constituents pay taxes to the city.\nThus, we study how to fairly maximize social welfare in a participatory\nbudgeting setting with a single city-wide election. We propose a notion of\nfairness that guarantees each district at least as much welfare as it would\nhave received in a district-level election. We show that, although optimizing\nsocial welfare subject to this notion of fairness is NP-hard, we can\nefficiently construct a lottery over welfare-optimal outcomes that is fair in\nexpectation. Moreover, we show that, when we are allowed to slightly relax\nfairness, we can efficiently compute a fair solution that is\nwelfare-maximizing, but which may overspend the budget.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 16:55:57 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Hershkowitz", "D Ellis", ""], ["Kahng", "Anson", ""], ["Peters", "Dominik", ""], ["Procaccia", "Ariel D.", ""]]}, {"id": "2102.06137", "submitter": "Antonio Vergari", "authors": "Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, Guy Van den\n  Broeck", "title": "A Compositional Atlas of Tractable Circuit Operations: From Simple\n  Transformations to Complex Information-Theoretic Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Circuit representations are becoming the lingua franca to express and reason\nabout tractable generative and discriminative models. In this paper, we show\nhow complex inference scenarios for these models that commonly arise in machine\nlearning -- from computing the expectations of decision tree ensembles to\ninformation-theoretic divergences of deep mixture models -- can be represented\nin terms of tractable modular operations over circuits. Specifically, we\ncharacterize the tractability of a vocabulary of simple transformations --\nsums, products, quotients, powers, logarithms, and exponentials -- in terms of\nsufficient structural constraints of the circuits they operate on, and present\nnovel hardness results for the cases in which these properties are not\nsatisfied. Building on these operations, we derive a unified framework for\nreasoning about tractable models that generalizes several results in the\nliterature and opens up novel tractable inference scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:26:32 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Vergari", "Antonio", ""], ["Choi", "YooJung", ""], ["Liu", "Anji", ""], ["Teso", "Stefano", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "2102.06181", "submitter": "Yinzhan Xu", "authors": "Timothy M. Chan, Virginia Vassilevska Williams and Yinzhan Xu", "title": "Algorithms, Reductions and Equivalences for Small Weight Variants of\n  All-Pairs Shortest Paths", "comments": "abstract shortened to fit arXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  APSP with small integer weights in undirected graphs [Seidel'95, Galil and\nMargalit'97] has an $\\tilde{O}(n^\\omega)$ time algorithm, where $\\omega<2.373$\nis the matrix multiplication exponent. APSP in directed graphs with small\nweights however, has a much slower running time that would be $\\Omega(n^{2.5})$\neven if $\\omega=2$ [Zwick'02]. To understand this $n^{2.5}$ bottleneck, we\nbuild a web of reductions around directed unweighted APSP. We show that it is\nfine-grained equivalent to computing a rectangular Min-Plus product for\nmatrices with integer entries; the dimensions and entry size of the matrices\ndepend on the value of $\\omega$. As a consequence, we establish an equivalence\nbetween APSP in directed unweighted graphs, APSP in directed graphs with small\n$(\\tilde{O}(1))$ integer weights, All-Pairs Longest Paths in DAGs with small\nweights, approximate APSP with additive error $c$ in directed graphs with small\nweights, for $c\\le \\tilde{O}(1)$ and several other graph problems. We also\nprovide fine-grained reductions from directed unweighted APSP to All-Pairs\nShortest Lightest Paths (APSLP) in undirected graphs with $\\{0,1\\}$ weights and\n$\\#_{\\text{mod}\\ c}$APSP in directed unweighted graphs (computing counts mod\n$c$).\n  We complement our hardness results with new algorithms. We improve the known\nalgorithms for APSLP in directed graphs with small integer weights and for\napproximate APSP with sublinear additive error in directed unweighted graphs.\nOur algorithm for approximate APSP with sublinear additive error is optimal,\nwhen viewed as a reduction to Min-Plus product. We also give new algorithms for\nvariants of #APSP in unweighted graphs, as well as a near-optimal\n$\\tilde{O}(n^3)$-time algorithm for the original #APSP problem in unweighted\ngraphs. Our techniques also lead to a simpler alternative for the original APSP\nproblem in undirected graphs with small integer weights.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:46:48 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chan", "Timothy M.", ""], ["Williams", "Virginia Vassilevska", ""], ["Xu", "Yinzhan", ""]]}, {"id": "2102.06247", "submitter": "Jie Shen", "authors": "Jie Shen", "title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise", "comments": "V2 polished writing; accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study efficient PAC learning of homogeneous halfspaces in $\\mathbb{R}^d$\nin the presence of malicious noise of Valiant~(1985). This is a challenging\nnoise model and only until recently has near-optimal noise tolerance bound been\nestablished under the mild condition that the unlabeled data distribution is\nisotropic log-concave. However, it remains unsettled how to obtain the optimal\nsample complexity simultaneously. In this work, we present a new analysis for\nthe algorithm of Awasthi~et~al.~(2017) and show that it essentially achieves\nthe near-optimal sample complexity bound of $\\tilde{O}(d)$, improving the best\nknown result of $\\tilde{O}(d^2)$. Our main ingredient is a novel incorporation\nof a matrix Chernoff-type inequality to bound the spectrum of an empirical\ncovariance matrix for well-behaved distributions, in conjunction with a careful\nexploration of the localization schemes of Awasthi~et~al.~(2017). We further\nextend the algorithm and analysis to the more general and stronger nasty noise\nmodel of Bshouty~et~al.~(2002), showing that it is still possible to achieve\nnear-optimal noise tolerance and sample complexity in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 20:18:20 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 20:56:20 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shen", "Jie", ""]]}, {"id": "2102.06277", "submitter": "Mohsen Heidari", "authors": "Mohsen Heidari and Wojciech Szpankowski", "title": "On Agnostic PAC Learning using $\\mathcal{L}_2$-polynomial Regression and\n  Fourier-based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework using Hilbert spaces as a proxy to analyze PAC\nlearning problems with structural properties. We consider a joint Hilbert space\nincorporating the relation between the true label and the predictor under a\njoint distribution $D$. We demonstrate that agnostic PAC learning with 0-1 loss\nis equivalent to an optimization in the Hilbert space domain. With our model,\nwe revisit the PAC learning problem using methods based on least-squares such\nas $\\mathcal{L}_2$ polynomial regression and Linial's low-degree algorithm. We\nstudy learning with respect to several hypothesis classes such as half-spaces\nand polynomial-approximated classes (i.e., functions approximated by a\nfixed-degree polynomial). We prove that (under some distributional assumptions)\nsuch methods obtain generalization error up to $2opt$ with $opt$ being the\noptimal error of the class. Hence, we show the tightest bound on generalization\nerror when $opt\\leq 0.2$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 21:28:55 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Heidari", "Mohsen", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "2102.06385", "submitter": "Chunlin Sun", "authors": "Xiaocheng Li, Chunlin Sun, Yinyu Ye", "title": "The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for\n  Bandits with Knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the bandits with knapsacks (BwK) problem and develop\na primal-dual based algorithm that achieves a problem-dependent logarithmic\nregret bound. The BwK problem extends the multi-arm bandit (MAB) problem to\nmodel the resource consumption associated with playing each arm, and the\nexisting BwK literature has been mainly focused on deriving asymptotically\noptimal distribution-free regret bounds. We first study the primal and dual\nlinear programs underlying the BwK problem. From this primal-dual perspective,\nwe discover symmetry between arms and knapsacks, and then propose a new notion\nof sub-optimality measure for the BwK problem. The sub-optimality measure\nhighlights the important role of knapsacks in determining algorithm regret and\ninspires the design of our two-phase algorithm. In the first phase, the\nalgorithm identifies the optimal arms and the binding knapsacks, and in the\nsecond phase, it exhausts the binding knapsacks via playing the optimal arms\nthrough an adaptive procedure. Our regret upper bound involves the proposed\nsub-optimality measure and it has a logarithmic dependence on length of horizon\n$T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the\nnumber of knapsacks). To the best of our knowledge, this is the first\nproblem-dependent logarithmic regret bound for solving the general BwK problem.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 08:14:30 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 00:00:48 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 00:04:07 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Li", "Xiaocheng", ""], ["Sun", "Chunlin", ""], ["Ye", "Yinyu", ""]]}, {"id": "2102.06387", "submitter": "Peter Kairouz", "authors": "Peter Kairouz and Ziyu Liu and Thomas Steinke", "title": "The Distributed Discrete Gaussian Mechanism for Federated Learning with\n  Secure Aggregation", "comments": "Accepted for publication at the 38th International Conference on\n  Machine Learning (ICML 2021).1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider training models on private data that are distributed across user\ndevices. To ensure privacy, we add on-device noise and use secure aggregation\nso that only the noisy sum is revealed to the server. We present a\ncomprehensive end-to-end system, which appropriately discretizes the data and\nadds discrete Gaussian noise before performing secure aggregation. We provide a\nnovel privacy analysis for sums of discrete Gaussians and carefully analyze the\neffects of data quantization and modular summation arithmetic. Our theoretical\nguarantees highlight the complex tension between communication, privacy, and\naccuracy. Our extensive experimental results demonstrate that our solution is\nessentially able to match the accuracy to central differential privacy with\nless than 16 bits of precision per value.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 08:20:18 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 04:24:31 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kairouz", "Peter", ""], ["Liu", "Ziyu", ""], ["Steinke", "Thomas", ""]]}, {"id": "2102.06427", "submitter": "Hung P. Hoang", "authors": "Bernd G\\\"artner, Sebastian Haslebacher, Hung P. Hoang", "title": "A Subexponential Algorithm for ARRIVAL", "comments": "13 pages, 1 figure Added a reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ARRIVAL problem is to decide the fate of a train moving along the edges\nof a directed graph, according to a simple (deterministic) pseudorandom walk.\nThe problem is in $NP \\cap coNP$ but not known to be in $P$. The currently best\nalgorithms have runtime $2^{\\Theta(n)}$ where $n$ is the number of vertices.\nThis is not much better than just performing the pseudorandom walk. We develop\na subexponential algorithm with runtime $2^{O(\\sqrt{n}\\log n)}$. We also give a\npolynomial-time algorithm if the graph is almost acyclic. Both results are\nderived from a new general approach to solve ARRIVAL instances.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 10:14:23 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 10:41:48 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 14:11:24 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["G\u00e4rtner", "Bernd", ""], ["Haslebacher", "Sebastian", ""], ["Hoang", "Hung P.", ""]]}, {"id": "2102.06463", "submitter": "Dimitrios Thilikos", "authors": "Ignasi Sau, Giannos Stamoulis, Dimitrios M. Thilikos", "title": "A more accurate view of the Flat Wall Theorem", "comments": "arXiv admin note: text overlap with arXiv:2004.12692", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a supporting combinatorial framework for the Flat Wall Theorem.\nIn particular, we suggest two variants of the theorem and we introduce a new,\nmore versatile, concept of wall homogeneity as well as the notion of regularity\nin flat walls. All proposed concepts and results aim at facilitating the use of\nthe irrelevant vertex technique in future algorithmic applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 11:52:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sau", "Ignasi", ""], ["Stamoulis", "Giannos", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "2102.06480", "submitter": "Shahbaz Khan", "authors": "Shahbaz Khan and Alexandru I. Tomescu", "title": "Safety of Flow Decompositions in DAGs", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network flows are one of the most studied combinatorial optimization problems\nwith innumerable applications. Any flow on a directed acyclic graph (DAG) $G$\nhaving $n$ vertices and $m$ edges can be decomposed into a set of $O(m)$ paths,\nwith applications from network routing to assembly of biological sequences. In\nsome applications, the flow decomposition corresponds to some particular data\nthat need to be reconstructed from the flow, which require finding paths (or\nsubpaths) appearing in all possible flow decompositions, referred to as safe\npaths.\n  Recently, Ma et al. [WABI 2020] addressed a related problem in a\nprobabilistic framework. Later, they gave a quadratic-time algorithm based on a\nglobal criterion, for a generalized version (AND-Quant) of the corresponding\nproblem, i.e., reporting if a given flow path is safe. Our contributions are as\nfollows:\n  1- A simple characterization for the safety of a given path based on a local\ncriterion, which can be directly adapted to give an optimal linear time\nverification algorithm.\n  2- A simple enumeration algorithm that reports all maximal safe paths on a\nflow network in $O(mn)$ time. The algorithm reports all safe paths using a\ncompact representation of the solution (called ${\\cal P}_c$), which is\n$\\Omega(mn)$ in the worst case, but merely $O(m+n)$ in the best case.\n  3- An improved enumeration algorithm where all safe paths ending at every\nvertex are represented as funnels using $O(n^2+|{\\cal P}_c|)$ space. These can\nbe computed and used to report all maximal safe paths, using time linear in the\ntotal space required by funnels, with an extra logarithmic factor.\n  Overall we present a simple characterization for the problem leading to an\noptimal verification algorithm and a simple enumeration algorithm. The\nenumeration algorithm is improved using the funnel structures for safe paths,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 12:26:49 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Khan", "Shahbaz", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "2102.06486", "submitter": "Vanja Dosko\\v{c}", "authors": "Francesco Quinzan and Vanja Dosko\\v{c} and Andreas G\\\"obel and Tobias\n  Friedrich", "title": "Adaptive Sampling for Fast Constrained Maximization of Submodular\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several large-scale machine learning tasks, such as data summarization, can\nbe approached by maximizing functions that satisfy submodularity. These\noptimization problems often involve complex side constraints, imposed by the\nunderlying application. In this paper, we develop an algorithm with\npoly-logarithmic adaptivity for non-monotone submodular maximization under\ngeneral side constraints. The adaptive complexity of a problem is the minimal\nnumber of sequential rounds required to achieve the objective.\n  Our algorithm is suitable to maximize a non-monotone submodular function\nunder a $p$-system side constraint, and it achieves a $(p +\nO(\\sqrt{p}))$-approximation for this problem, after only poly-logarithmic\nadaptive rounds and polynomial queries to the valuation oracle function.\nFurthermore, our algorithm achieves a $(p + O(1))$-approximation when the given\nside constraint is a $p$-extendible system.\n  This algorithm yields an exponential speed-up, with respect to the\nadaptivity, over any other known constant-factor approximation algorithm for\nthis problem. It also competes with previous known results in terms of the\nquery complexity. We perform various experiments on various real-world\napplications. We find that, in comparison with commonly used heuristics, our\nalgorithm performs better on these instances.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 12:38:03 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Quinzan", "Francesco", ""], ["Dosko\u010d", "Vanja", ""], ["G\u00f6bel", "Andreas", ""], ["Friedrich", "Tobias", ""]]}, {"id": "2102.06543", "submitter": "Matthieu Latapy", "authors": "Fr\\'ed\\'eric Simard and Cl\\'emence Magnien and Matthieu Latapy", "title": "Computing Betweenness Centrality in Link Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Betweeness centrality is one of the most important concepts in graph\nanalysis. It was recently extended to link streams, a graph generalization\nwhere links arrive over time. However, its computation raises non-trivial\nissues, due in particular to the fact that time is considered as continuous. We\nprovide here the first algorithms to compute this generalized betweenness\ncentrality, as well as several companion algorithms that have their own\ninterest. They work in polynomial time and space, we illustrate them on typical\nexamples, and we provide an implementation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:12:15 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Simard", "Fr\u00e9d\u00e9ric", ""], ["Magnien", "Cl\u00e9mence", ""], ["Latapy", "Matthieu", ""]]}, {"id": "2102.06557", "submitter": "Steffen Kl\\\"abe", "authors": "Steffen Kl\\\"abe, Kai-Uwe Sattler, Stephan Baumann", "title": "Updatable Materialization of Approximate Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern big data applications integrate data from various sources. As a\nresult, these datasets may not satisfy perfect constraints, leading to sparse\nschema information and non-optimal query performance. The existing approach of\nPatchIndexes enable the definition of approximate constraints and improve query\nperformance by exploiting the materialized constraint information. As real\nworld data warehouse workloads are often not limited to read-only queries, we\nenhance the PatchIndex structure towards an update-conscious design in this\npaper. Therefore, we present a sharded bitmap as the underlying data structure\nwhich offers efficient update operations, and describe approaches to maintain\napproximate constraints under updates, avoiding index recomputations and full\ntable scans. In our evaluation, we prove that PatchIndexes significantly impact\nquery performance while achieving lightweight update support.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:43:49 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Kl\u00e4be", "Steffen", ""], ["Sattler", "Kai-Uwe", ""], ["Baumann", "Stephan", ""]]}, {"id": "2102.06565", "submitter": "Sagnik Mukhopadhyay", "authors": "Andr\\'es L\\'opez-Mart\\'inez, Sagnik Mukhopadhyay, Danupon Nanongkai", "title": "Work-Optimal Parallel Minimum Cuts for Non-Sparse Graphs", "comments": "Updates on this version: Minor corrections for the previous and our\n  result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first work-optimal polylogarithmic-depth parallel algorithm\nfor the minimum cut problem on non-sparse graphs. For $m\\geq n^{1+\\epsilon}$\nfor any constant $\\epsilon>0$, our algorithm requires $O(m \\log n)$ work and\n$O(\\log^3 n)$ depth and succeeds with high probability. Its work matches the\nbest $O(m \\log n)$ runtime for sequential algorithms [MN STOC 2020, GMW SOSA\n2021]. This improves the previous best work by Geissmann and Gianinazzi [SPAA\n2018] by $O(\\log^3 n)$ factor, while matching the depth of their algorithm. To\ndo this, we design a work-efficient approximation algorithm and parallelize the\nrecent sequential algorithms [MN STOC 2020; GMW SOSA 2021] that exploit a\nconnection between 2-respecting minimum cuts and 2-dimensional orthogonal range\nsearching.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:06:19 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 14:21:33 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["L\u00f3pez-Mart\u00ednez", "Andr\u00e9s", ""], ["Mukhopadhyay", "Sagnik", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "2102.06613", "submitter": "Mong-Jen Kao", "authors": "Mong-Jen Kao", "title": "Improved LP-based Approximation Algorithms for Facility Location with\n  Hard Capacities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LP-based approximation algorithms for the capacitated facility\nlocation problem (CFL), a long-standing problem with intriguing unsettled\napproximability in the literature dated back to the 90s. We present an elegant\niterative rounding scheme for the MFN relaxation that yields an approximation\nguarantee of $\\left(10+\\sqrt{67}\\right)/2 \\approx 9.0927$, a significant\nimprovement upon the previous LP-based ratio of $288$ due to An et al. in~2014.\n  For CFL with cardinality facility cost (CFL-CFC), we present an LP-based\n$4$-approximation algorithm, which not only surpasses the long-standing ratio\nof~$5$ due to Levi et al. that ages up for decades since 2004 but also unties\nthe long-time match to the best approximation for CFL that is obtained via\nlocal search in 2012. Our result considerably deepens the current understanding\nfor the CFL problem and indicates that an LP-based ratio strictly better than\n$5$ in polynomial time for the general problem may still be possible to pursue.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:48:11 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 03:59:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kao", "Mong-Jen", ""]]}, {"id": "2102.06635", "submitter": "Christoph Hertrich", "authors": "Christoph Hertrich and Leon Sering", "title": "ReLU Neural Networks for Exact Maximum Flow Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the great empirical success of artificial neural networks (NNs)\nfrom a theoretical point of view is currently one of the hottest research\ntopics in computer science. In this paper we study the expressive power of NNs\nwith rectified linear units from a combinatorial optimization perspective. In\nparticular, we show that, given a directed graph with $n$ nodes and $m$ arcs,\nthere exists an NN of polynomial size that computes a maximum flow from any\npossible real-valued arc capacities as input. To prove this, we develop the\npseudo-code language Max-Affine Arithmetic Programs (MAAPs) and show\nequivalence between MAAPs and NNs concerning natural complexity measures. We\nthen design a MAAP to exactly solve the Maximum Flow Problem, which translates\nto an NN of size $\\mathcal{O}(m^2 n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 17:23:34 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Hertrich", "Christoph", ""], ["Sering", "Leon", ""]]}, {"id": "2102.06783", "submitter": "George Mertzios", "authors": "George B. Mertzios, Hendrik Molter, Malte Renken, Paul G. Spirakis,\n  Philipp Zschoche", "title": "The Complexity of Transitively Orienting Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a temporal network with discrete time-labels on its edges, entities and\ninformation can only \"flow\" along sequences of edges whose time-labels are\nnon-decreasing (resp. increasing), i.e. along temporal (resp. strict temporal)\npaths. Nevertheless, in the model for temporal networks of [Kempe et al., JCSS,\n2002], the individual time-labeled edges remain undirected: an edge $e=\\{u,v\\}$\nwith time-label $t$ specifies that \"$u$ communicates with $v$ at time $t$\".\nThis is a symmetric relation between $u$ and $v$, and it can be interpreted\nthat the information can flow in either direction. In this paper we make a\nfirst attempt to understand how the direction of information flow on one edge\ncan impact the direction of information flow on other edges. More specifically,\nwe introduce the notion of a temporal transitive orientation and we\nsystematically investigate its algorithmic behavior in various situations. An\norientation of a temporal graph is called temporally transitive if, whenever\n$u$ has a directed edge towards $v$ with time-label $t_1$ and $v$ has a\ndirected edge towards $w$ with time-label $t_2\\geq t_1$, then $u$ also has a\ndirected edge towards $w$ with some time-label $t_3\\geq t_2$. If we just demand\nthat this implication holds whenever $t_2 > t_1$, the orientation is called\nstrictly temporally transitive. Our main result is a conceptually simple, yet\ntechnically quite involved, polynomial-time algorithm for recognizing whether a\ngiven temporal graph $\\mathcal{G}$ is transitively orientable. In wide contrast\nwe prove that, surprisingly, it is NP-hard to recognize whether $\\mathcal{G}$\nis strictly transitively orientable. Additionally we introduce and investigate\nfurther related problems to temporal transitivity, notably among them the\ntemporal transitive completion problem, for which we prove both algorithmic and\nhardness results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 21:39:26 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mertzios", "George B.", ""], ["Molter", "Hendrik", ""], ["Renken", "Malte", ""], ["Spirakis", "Paul G.", ""], ["Zschoche", "Philipp", ""]]}, {"id": "2102.06798", "submitter": "Nicola Cotumaccio", "authors": "Nicola Cotumaccio, Giovanna D'Agostino, Alberto Policriti, Nicola\n  Prezza", "title": "Which Regular Languages can be Efficiently Indexed?", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the present work, we tackle the regular language indexing problem by first\nstudying the hierarchy of $p$-sortable languages: regular languages accepted by\nautomata of width $p$. We show that the hierarchy is strict and does not\ncollapse, and provide (exponential in $p$) upper and lower bounds relating the\nminimum widths of equivalent NFAs and DFAs. Our bounds indicate the importance\nof being able to index NFAs, as they enable indexing regular languages with\nmuch faster and smaller indexes. Our second contribution solves precisely this\nproblem, optimally: we devise a polynomial-time algorithm that indexes any NFA\nwith the optimal value $p$ for its width, without explicitly computing $p$\n(NP-hard to find). In particular, this implies that we can index in polynomial\ntime the well-studied case $p=1$ (Wheeler NFAs). More in general, in polynomial\ntime we can build an index breaking the worst-case conditional lower bound of\n$\\Omega(|P| m)$, whenever the input NFA's width is $p \\in o(\\sqrt{m})$.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 22:25:30 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:51:19 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cotumaccio", "Nicola", ""], ["D'Agostino", "Giovanna", ""], ["Policriti", "Alberto", ""], ["Prezza", "Nicola", ""]]}, {"id": "2102.06805", "submitter": "Seth Pettie", "authors": "Seth Pettie and Longhui Yin", "title": "The Structure of Minimum Vertex Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we continue a long line of work on representing the cut\nstructure of graphs. We classify the types minimum vertex cuts, and the\npossible relationships between multiple minimum vertex cuts.\n  As a consequence of these investigations, we exhibit a simple $O(\\kappa\nn)$-space data structure that can quickly answer pairwise\n$(\\kappa+1)$-connectivity queries in a $\\kappa$-connected graph. We also show\nhow to compute the \"closest\" $\\kappa$-cut to every vertex in near linear\n$\\tilde{O}(m+poly(\\kappa)n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 22:45:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Pettie", "Seth", ""], ["Yin", "Longhui", ""]]}, {"id": "2102.06857", "submitter": "Khang Le", "authors": "Khang Le, Huy Nguyen, Quang Nguyen, Nhat Ho, Tung Pham, Hung Bui", "title": "On Robust Optimal Transport: Computational Complexity, Low-rank\n  Approximation, and Barycenter Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider two robust versions of optimal transport, named $\\textit{Robust\nSemi-constrained Optimal Transport}$ (RSOT) and $\\textit{Robust Unconstrained\nOptimal Transport}$ (ROT), formulated by relaxing the marginal constraints with\nKullback-Leibler divergence. For both problems in the discrete settings, we\npropose Sinkhorn-based algorithms that produce $\\varepsilon$-approximations of\nRSOT and ROT in $\\widetilde{\\mathcal{O}}(\\frac{n^2}{\\varepsilon})$ time, where\n$n$ is the number of supports of the probability distributions. Furthermore, to\nreduce the dependency of the complexity of the Sinkhorn-based algorithms on\n$n$, we apply Nystr\\\"{o}m method to approximate the kernel matrix in both RSOT\nand ROT by a matrix of rank $r$ before passing it to these Sinkhorn-based\nalgorithms. We demonstrate that these new algorithms have\n$\\widetilde{\\mathcal{O}}(n r^2 + \\frac{nr}{\\varepsilon})$ runtime to obtain the\nRSOT and ROT $\\varepsilon$-approximations. Finally, we consider a barycenter\nproblem based on RSOT, named $\\textit{Robust Semi-Constrained Barycenter}$\nproblem (RSBP), and develop a robust iterative Bregman projection algorithm,\ncalled $\\textbf{Normalized-RobustIBP}$ algorithm, to solve the RSBP in the\ndiscrete settings of probability distributions. We show that an\n$\\varepsilon$-approximated solution of the RSBP can be achieved in\n$\\widetilde{\\mathcal{O}}(\\frac{mn^2}{\\varepsilon})$ time using\n$\\textbf{Normalized-RobustIBP}$ algorithm when $m = 2$, which is better than\nthe previous complexity $\\widetilde{\\mathcal{O}}(\\frac{mn^2}{\\varepsilon^2})$\nof IBP algorithm for approximating the Wasserstein barycenter. Extensive\nexperiments confirm our theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 03:55:52 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Le", "Khang", ""], ["Nguyen", "Huy", ""], ["Nguyen", "Quang", ""], ["Ho", "Nhat", ""], ["Pham", "Tung", ""], ["Bui", "Hung", ""]]}, {"id": "2102.06901", "submitter": "Tuukka Korhonen", "authors": "Tuukka Korhonen", "title": "Lower Bounds on Dynamic Programming for Maximum Weight Independent Set", "comments": "14 pages, to appear in ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove lower bounds on pure dynamic programming algorithms for maximum\nweight independent set (MWIS). We model such algorithms as tropical circuits,\ni.e., circuits that compute with $\\max$ and $+$ operations. For a graph $G$, an\nMWIS-circuit of $G$ is a tropical circuit whose inputs correspond to vertices\nof $G$ and which computes the weight of a maximum weight independent set of $G$\nfor any assignment of weights to the inputs. We show that if $G$ has treewidth\n$w$ and maximum degree $d$, then any MWIS-circuit of $G$ has $2^{\\Omega(w/d)}$\ngates and that if $G$ is planar, or more generally $H$-minor-free for any fixed\ngraph $H$, then any MWIS-circuit of $G$ has $2^{\\Omega(w)}$ gates. An\nMWIS-formula is an MWIS-circuit where each gate has fan-out at most one. We\nshow that if $G$ has treedepth $t$ and maximum degree $d$, then any\nMWIS-formula of $G$ has $2^{\\Omega(t/d)}$ gates. It follows that treewidth\ncharacterizes optimal MWIS-circuits up to polynomials for all bounded degree\ngraphs and $H$-minor-free graphs, and treedepth characterizes optimal\nMWIS-formulas up to polynomials for all bounded degree graphs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 11:26:43 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 05:51:10 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Korhonen", "Tuukka", ""]]}, {"id": "2102.06904", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Artur Kraska, Hsiang-Hsuan Liu", "title": "Traveling Repairperson, Unrelated Machines, and Other Stories About\n  Average Completion Times", "comments": "ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for minimizing average completion time for\nmany seemingly disparate online scheduling problems, such as the traveling\nrepairperson problems (TRP), dial-a-ride problems (DARP), and scheduling on\nunrelated machines.\n  We construct a simple algorithm that handles all these scheduling problems,\nby computing and later executing auxiliary schedules, each optimizing a certain\nfunction on already seen prefix of the input. The optimized function resembles\na prize-collecting variant of the original scheduling problem. By a careful\nanalysis of the interplay between these auxiliary schedules, and later\nemploying the resulting inequalities in a factor-revealing linear program, we\nobtain improved bounds on the competitive ratio for all these scheduling\nproblems.\n  In particular, our techniques yield a $4$-competitive deterministic algorithm\nfor all previously studied variants of online TRP and DARP, and a\n$3$-competitive one for the scheduling on unrelated machines (also with\nprecedence constraints). This improves over currently best ratios for these\nproblems that are $5.14$ and $4$, respectively. We also show how to use\nrandomization to further reduce the competitive ratios to $1+2/\\ln 3 < 2.821$\nand $1+1/\\ln 2 < 2.443$, respectively. The randomized bounds also substantially\nimprove the current state of the art. Our upper bound for DARP contradicts the\nlower bound of 3 given by Fink et al. (Inf. Process. Lett. 2009); we pinpoint a\nflaw in their proof.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 11:31:41 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 20:57:39 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Kraska", "Artur", ""], ["Liu", "Hsiang-Hsuan", ""]]}, {"id": "2102.06939", "submitter": "Ge Xia", "authors": "Jianer Chen, Qin Huang, Iyad Kanj, Ge Xia", "title": "Optimal Streaming Algorithms for Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present parameterized streaming algorithms for the graph matching problem\nin both the dynamic and the insert-only models. For the dynamic streaming\nmodel, we present a one-pass algorithm that, with high probability, computes a\nmaximum-weight $k$-matching of a weighted graph in $\\tilde{O}(Wk^2)$ space and\nthat has $\\tilde{O}(1)$ update time, where $W$ is the number of distinct edge\nweights and the notation $\\tilde{O}()$ hides a poly-logarithmic factor in the\ninput size. For the insert-only streaming model, we present a one-pass\nalgorithm that runs in $O(k^2)$ space and has $O(1)$ update time, and that,\nwith high probability, computes a maximum-weight $k$-matching of a weighted\ngraph. The space complexity and the update-time complexity achieved by our\nalgorithms for unweighted $k$-matching in the dynamic model and for weighted\n$k$-matching in the insert-only model are optimal.\n  A notable contribution of this paper is that the presented algorithms {\\it do\nnot} rely on the apriori knowledge/promise that the cardinality of \\emph{every}\nmaximum-weight matching of the input graph is upper bounded by the parameter\n$k$. This promise has been a critical condition in previous works, and lifting\nit required the development of new tools and techniques.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 14:55:50 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 18:31:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chen", "Jianer", ""], ["Huang", "Qin", ""], ["Kanj", "Iyad", ""], ["Xia", "Ge", ""]]}, {"id": "2102.06959", "submitter": "Cassio Neri", "authors": "Cassio Neri and Lorenz Schneider", "title": "Euclidean Affine Functions and Applications to Calendar Algorithms", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study properties of Euclidean affine functions (EAFs), namely those of the\nform $f(r) = (\\alpha\\cdot r + \\beta)/\\delta$, and their closely related\nexpression $\\mathring{f}(r) = (\\alpha\\cdot r + \\beta)\\%\\delta$, where $r$,\n$\\alpha$, $\\beta$ and $\\delta$ are integers, and where $/$ and $\\%$\nrespectively denote the quotient and remainder of Euclidean division. We derive\nalgebraic relations and numerical approximations that are important for the\nefficient evaluation of these expressions in modern CPUs. Since simple division\nand remainder are particular cases of EAFs (when $\\alpha = 1$ and $\\beta = 0$),\nthe optimisations proposed in this paper can also be appplied to them. Such\nexpressions appear in some of the most common tasks in any computer system,\nsuch as printing numbers, times and dates. We use calendar calculations as the\nmain application example because it is richer with respect to the number of\nEAFs employed. Specifically, the main application presented in this article\nrelates to Gregorian calendar algorithms. We will show how they can be\nimplemented substantially more efficiently than is currently the case in widely\nused C, C++, C# and Java open source libraries. Gains in speed of a factor of\ntwo or more are common.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 17:04:59 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Neri", "Cassio", ""], ["Schneider", "Lorenz", ""]]}, {"id": "2102.06977", "submitter": "Deeksha Adil", "authors": "Deeksha Adil, Brian Bullins, Rasmus Kyng, Sushant Sachdeva", "title": "Almost-linear-time Weighted $\\ell_p$-norm Solvers in Slightly Dense\n  Graphs via Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give almost-linear-time algorithms for constructing sparsifiers with $n\\\npoly(\\log n)$ edges that approximately preserve weighted $(\\ell^{2}_2 +\n\\ell^{p}_p)$ flow or voltage objectives on graphs. For flow objectives, this is\nthe first sparsifier construction for such mixed objectives beyond unit\n$\\ell_p$ weights, and is based on expander decompositions. For voltage\nobjectives, we give the first sparsifier construction for these objectives,\nwhich we build using graph spanners and leverage score sampling. Together with\nthe iterative refinement framework of [Adil et al, SODA 2019], and a new\nmultiplicative-weights based constant-approximation algorithm for\nmixed-objective flows or voltages, we show how to find $(1+2^{-\\text{poly}(\\log\nn)})$ approximations for weighted $\\ell_p$-norm minimizing flows or voltages in\n$p(m^{1+o(1)} + n^{4/3 + o(1)})$ time for $p=\\omega(1),$ which is almost-linear\nfor graphs that are slightly dense ($m \\ge n^{4/3 + o(1)}$).\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 18:29:18 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Adil", "Deeksha", ""], ["Bullins", "Brian", ""], ["Kyng", "Rasmus", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "2102.07011", "submitter": "Soheil Behnezhad", "authors": "Sepehr Assadi, Soheil Behnezhad", "title": "Beating Two-Thirds For Random-Order Streaming Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum matching problem in the random-order semi-streaming\nsetting. In this problem, the edges of an arbitrary $n$-vertex graph $G=(V, E)$\narrive in a stream one by one and in a random order. The goal is to have a\nsingle pass over the stream, use $n \\cdot poly(\\log n)$ space, and output a\nlarge matching of $G$.\n  We prove that for an absolute constant $\\epsilon_0 > 0$, one can find a $(2/3\n+ \\epsilon_0)$-approximate maximum matching of $G$ using $O(n \\log n)$ space\nwith high probability. This breaks the natural boundary of $2/3$ for this\nproblem prevalent in the prior work and resolves an open problem of Bernstein\n[ICALP'20] on whether a $(2/3 + \\Omega(1))$-approximation is achievable.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 22:03:58 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 21:53:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Assadi", "Sepehr", ""], ["Behnezhad", "Soheil", ""]]}, {"id": "2102.07089", "submitter": "Jackson Morris", "authors": "Jackson Morris and Fang Song", "title": "Simple vertex coloring algorithms", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a graph $G$ with $n$ vertices and maximum degree $\\Delta$, it is known\nthat $G$ admits a vertex coloring with $\\Delta + 1$ colors such that no edge of\n$G$ is monochromatic. This can be seen constructively by a simple greedy\nalgorithm, which runs in time $O(n\\Delta)$.\n  Very recently, a sequence of results (e.g., [Assadi et. al. SODA'19, Bera et.\nal. ICALP'20, Alon Assadi Approx/Random'20]) show randomized algorithms for\n$(\\epsilon + 1)\\Delta$-coloring in the query model making\n$\\tilde{O}(n\\sqrt{n})$ queries, improving over the greedy strategy on dense\ngraphs. In addition, a lower bound of $\\Omega(n\\sqrt n)$ for any\n$O(\\Delta)$-coloring is established on general graphs.\n  In this work, we give a simple algorithm for $(1 + \\epsilon)\\Delta$-coloring.\nThis algorithm makes $O(\\epsilon^{-1/2}n\\sqrt{n})$ queries, which matches the\nbest existing algorithms as well as the classical lower bound for sufficiently\nlarge $\\epsilon$. Additionally, it can be readily adapted to a quantum query\nalgorithm making $\\tilde{O}(\\epsilon^{-1}n^{4/3})$ queries, bypassing the\nclassical lower bound. Complementary to these algorithmic results, we show a\nquantum lower bound of $\\Omega(n)$ for $O(\\Delta)$-coloring.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 07:27:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Morris", "Jackson", ""], ["Song", "Fang", ""]]}, {"id": "2102.07154", "submitter": "Oren Weimann", "authors": "Aviv Bar-Natan, Panagiotis Charalampopoulos, Pawe{\\l} Gawrychowski,\n  Shay Mozes, Oren Weimann", "title": "Fault-Tolerant Distance Labeling for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In fault-tolerant distance labeling we wish to assign short labels to the\nvertices of a graph $G$ such that from the labels of any three vertices $u,v,f$\nwe can infer the $u$-to-$v$ distance in the graph $G\\setminus \\{f\\}$. We show\nthat any directed weighted planar graph (and in fact any graph in a graph\nfamily with $O(\\sqrt{n})$-size separators, such as minor-free graphs) admits\nfault-tolerant distance labels of size $O(n^{2/3})$. We extend these labels in\na way that allows us to also count the number of shortest paths, and provide\nadditional upper and lower bounds for labels and oracles for counting shortest\npaths.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 13:39:27 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bar-Natan", "Aviv", ""], ["Charalampopoulos", "Panagiotis", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "2102.07528", "submitter": "Kaushik Mondal", "authors": "Anisur Rahaman Molla, Kaushik Mondal, William K. Moses Jr", "title": "Byzantine Dispersion on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of Byzantine dispersion and extends previous\nwork along several parameters. The problem of Byzantine dispersion asks: given\n$n$ robots, up to $f$ of which are Byzantine, initially placed arbitrarily on\nan $n$ node anonymous graph, design a terminating algorithm to be run by the\nrobots such that they eventually reach a configuration where each node has at\nmost one non-Byzantine robot on it. Previous work solved this problem for rings\nand tolerated up to $n-1$ Byzantine robots. In this paper, we investigate the\nproblem on more general graphs.\n  We first develop an algorithm that tolerates up to $n-1$ Byzantine robots and\nworks for a more general class of graphs. We then develop an algorithm that\nworks for any graph but tolerates a lesser number of Byzantine robots. We\nsubsequently turn our focus to the strength of the Byzantine robots. Previous\nwork considers only \"weak\" Byzantine robots that cannot fake their IDs. We\ndevelop an algorithm that solves the problem when Byzantine robots are not weak\nand can fake IDs. Finally, we study the situation where the number of the\nrobots is not $n$ but some $k$. We show that in such a scenario, the number of\nByzantine robots that can be tolerated is severely restricted. Specifically, we\nshow that it is impossible to deterministically solve Byzantine dispersion when\n$\\lceil k/n \\rceil > \\lceil (k-f)/n \\rceil$.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 13:01:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Mondal", "Kaushik", ""], ["Moses", "William K.", "Jr"]]}, {"id": "2102.07587", "submitter": "Simon Apers", "authors": "Florian Adriaens and Simon Apers", "title": "Testing properties of signed graphs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In graph property testing the task is to distinguish whether a graph\nsatisfies a given property or is \"far\" from having that property, preferably\nwith a sublinear query and time complexity. In this work we initiate the study\nof property testing in signed graphs, where every edge has either a positive or\na negative sign. We show that there exist sublinear algorithms for testing\nthree key properties of signed graphs: balance (or 2-clusterability),\nclusterability and signed triangle freeness. We consider both the dense graph\nmodel, where we can query the (signed) adjacency matrix of a signed graph, and\nthe bounded-degree model, where we can query for the neighbors of a node and\nthe sign of the connecting edge. Our algorithms use a variety of tools from\ngraph property testing, as well as reductions from one setting to the other.\nOur main technical contribution is a sublinear algorithm for testing\nclusterability in the bounded-degree model. This contrasts with the property of\nk-clusterability which is not testable with a sublinear number of queries. The\ntester builds on the seminal work of Goldreich and Ron for testing\nbipartiteness.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 14:57:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Adriaens", "Florian", ""], ["Apers", "Simon", ""]]}, {"id": "2102.07684", "submitter": "Konstantina Bairaktari", "authors": "Konstantina Bairaktari, Huy Le Nguyen, Jonathan Ullman", "title": "Fair and Optimal Cohort Selection for Linear Utilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of algorithmic decision-making has created an explosion of research\naround the fairness of those algorithms. While there are many compelling\nnotions of individual fairness, beginning with the work of Dwork et al., these\nnotions typically do not satisfy desirable composition properties. To this end,\nDwork and Ilvento introduced the fair cohort selection problem, which captures\na specific application where a single fair classifier is composed with itself\nto pick a group of candidates of size exactly $k$. In this work we introduce a\nspecific instance of cohort selection where the goal is to choose a cohort\nmaximizing a linear utility function. We give approximately optimal\npolynomial-time algorithms for this problem in both an offline setting where\nthe entire fair classifier is given at once, or an online setting where\ncandidates arrive one at a time and are classified as they arrive.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 17:26:46 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 13:52:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Bairaktari", "Konstantina", ""], ["Nguyen", "Huy Le", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2102.07727", "submitter": "Michael Walter", "authors": "Peter B\\\"urgisser and M. Levent Do\\u{g}an and Visu Makam and Michael\n  Walter and Avi Wigderson", "title": "Polynomial time algorithms in invariant theory for torus actions", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math-ph math.AG math.MP math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An action of a group on a vector space partitions the latter into a set of\norbits. We consider three natural and useful algorithmic \"isomorphism\" or\n\"classification\" problems, namely, orbit equality, orbit closure intersection,\nand orbit closure containment. These capture and relate to a variety of\nproblems within mathematics, physics and computer science, optimization and\nstatistics. These orbit problems extend the more basic null cone problem, whose\nalgorithmic complexity has seen significant progress in recent years.\n  In this paper, we initiate a study of these problems by focusing on the\nactions of commutative groups (namely, tori). We explain how this setting is\nmotivated from questions in algebraic complexity, and is still rich enough to\ncapture interesting combinatorial algorithmic problems. While the structural\ntheory of commutative actions is well understood, no general efficient\nalgorithms were known for the aforementioned problems. Our main results are\npolynomial time algorithms for all three problems. We also show how to\nefficiently find separating invariants for orbits, and how to compute systems\nof generating rational invariants for these actions (in contrast, for\npolynomial invariants the latter is known to be hard). Our techniques are based\non a combination of fundamental results in invariant theory, linear\nprogramming, and algorithmic lattice theory.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:25:18 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Do\u011fan", "M. Levent", ""], ["Makam", "Visu", ""], ["Walter", "Michael", ""], ["Wigderson", "Avi", ""]]}, {"id": "2102.07728", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Louis Jachiet, Charles Paperman", "title": "Dynamic Membership for Regular Languages", "comments": "34 pages. This is the full version with proofs of the ICALP'21\n  article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the dynamic membership problem for regular languages: fix a language\nL, read a word w, build in time O(|w|) a data structure indicating if w is in\nL, and maintain this structure efficiently under letter substitutions on w. We\nconsider this problem on the unit cost RAM model with logarithmic word length,\nwhere the problem always has a solution in O(log |w| / log log |w|) per\noperation.\n  We show that the problem is in O(log log |w|) for languages in an\nalgebraically-defined, decidable class QSG, and that it is in O(1) for another\nsuch class QLZG. We show that languages not in QSG admit a reduction from the\nprefix problem for a cyclic group, so that they require {\\Omega}(log |w| / log\nlog |w|) operations in the worst case; and that QSG languages not in QLZG admit\na reduction from the prefix problem for the multiplicative monoid U 1 = {0, 1},\nwhich we conjecture cannot be maintained in O(1). This yields a conditional\ntrichotomy. We also investigate intermediate cases between O(1) and O(log log\n|w|).\n  Our results are shown via the dynamic word problem for monoids and\nsemigroups, for which we also give a classification. We thus solve open\nproblems of the paper of Skovbjerg Frandsen, Miltersen, and Skyum [30] on the\ndynamic word problem, and additionally cover regular languages.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:25:48 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:34:07 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Amarilli", "Antoine", ""], ["Jachiet", "Louis", ""], ["Paperman", "Charles", ""]]}, {"id": "2102.07740", "submitter": "Amartya Shankha Biswas", "authors": "Amartya Shankha Biswas, Edward Pyne, Ronitt Rubinfeld", "title": "Local Access to Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $G$ on $n$ vertices, naively sampling the position of a random\nwalk of at time $t$ requires work $\\Omega(t)$. We desire local access\nalgorithms supporting $\\text{position}(G,s,t)$ queries, which return the\nposition of a random walk from some start vertex $s$ at time $t$, where the\njoint distribution of returned positions is $1/\\text{poly}(n)$ close to the\nuniform distribution over such walks in $\\ell_1$ distance.\n  We first give an algorithm for local access to walks on undirected regular\ngraphs with $\\widetilde{O}(\\frac{1}{1-\\lambda}\\sqrt{n})$ runtime per query,\nwhere $\\lambda$ is the second-largest eigenvalue in absolute value. Since\nrandom $d$-regular graphs are expanders with high probability, this gives an\n$\\widetilde{O}(\\sqrt{n})$ algorithm for $G(n,d)$, which improves on the naive\nmethod for small numbers of queries.\n  We then prove that no that algorithm with sub-constant error given probe\naccess to random $d$-regular graphs can have runtime better than\n$\\Omega(\\sqrt{n}/\\log(n))$ per query in expectation, obtaining a nearly\nmatching lower bound. We further show an $\\Omega(n^{1/4})$ runtime per query\nlower bound even with an oblivious adversary (i.e. when the query sequence is\nfixed in advance).\n  We then show that for families of graphs with additional group theoretic\nstructure, dramatically better results can be achieved. We give local access to\nwalks on small-degree abelian Cayley graphs, including cycles and hypercubes,\nwith runtime $\\text{polylog}(n)$ per query. This also allows for efficient\nlocal access to walks on $\\text{polylog}$ degree expanders. We extend our\nresults to graphs constructed using the tensor product (giving local access to\nwalks on degree $n^\\epsilon$ graphs for any $\\epsilon \\in (0,1]$) and Cartesian\nproduct.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:37:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Pyne", "Edward", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2102.07839", "submitter": "Panagiotis Kanellopoulos", "authors": "Ioannis Caragiannis, Panagiotis Kanellopoulos, Maria Kyropoulou", "title": "On Interim Envy-Free Allocation Lotteries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With very few exceptions, recent research in fair division has mostly focused\non deterministic allocations. Deviating from this trend, we study the fairness\nnotion of interim envy-freeness (iEF) for lotteries over allocations, which\nserves as a sweet spot between the too stringent notion of ex-post\nenvy-freeness and the very weak notion of ex-ante envy-freeness. iEF is a\nnatural generalization of envy-freeness to random allocations in the sense that\na deterministic envy-free allocation is iEF (when viewed as a degenerate\nlottery). It is also certainly meaningful as it allows for a richer solution\nspace, which includes solutions that are provably better than envy-freeness\naccording to several criteria. Our analysis relates iEF to other fairness\nnotions as well, and reveals tradeoffs between iEF and efficiency. Even though\nseveral of our results apply to general fair division problems, we are\nparticularly interested in instances with equal numbers of agents and items\nwhere allocations are perfect matchings of the items to the agents.\nEnvy-freeness can be trivially decided and (when it can be achieved, it)\nimplies full efficiency in this setting. Although computing iEF allocations in\nmatching allocation instances is considerably more challenging, we show how to\ncompute them in polynomial time, while also maximizing several efficiency\nobjectives. Our algorithms use the ellipsoid method for linear programming and\nefficient solutions to a novel variant of the bipartite matching problem as a\nseparation oracle. We also study the extension of interim envy-freeness notion\nwhen payments to or from the agents are allowed. We present a series of results\non two optimization problems, including a generalization of the classical rent\ndivision problem to random allocations using interim envy-freeness as the\nsolution concept.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:35:55 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 16:45:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Kanellopoulos", "Panagiotis", ""], ["Kyropoulou", "Maria", ""]]}, {"id": "2102.07980", "submitter": "Muhammad Irfan Yousuf Dr.", "authors": "Muhammad Irfan Yousuf, Izza Anwer, Raheel Anwar", "title": "Empirical Characterization of Graph Sampling Algorithms", "comments": "22 Pages, 9 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph sampling allows mining a small representative subgraph from a big\ngraph. Sampling algorithms deploy different strategies to replicate the\nproperties of a given graph in the sampled graph. In this study, we provide a\ncomprehensive empirical characterization of five graph sampling algorithms on\nsix properties of a graph including degree, clustering coefficient, path\nlength, global clustering coefficient, assortativity, and modularity. We\nextract samples from fifteen graphs grouped into five categories including\ncollaboration, social, citation, technological, and synthetic graphs. We\nprovide both qualitative and quantitative results. We find that there is no\nsingle method that extracts true samples from a given graph with respect to the\nproperties tested in this work. Our results show that the sampling algorithm\nthat aggressively explores the neighborhood of a sampled node performs better\nthan the others.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:02:27 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Yousuf", "Muhammad Irfan", ""], ["Anwer", "Izza", ""], ["Anwar", "Raheel", ""]]}, {"id": "2102.08076", "submitter": "Saeed Akhoondian Amiri", "authors": "Saeed Akhoondian Amiri", "title": "Deterministic CONGEST Algorithm for MDS on Bounded Arboricity Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We provide a deterministic CONGEST algorithm to constant factor approximate\nthe minimum dominating set on graphs of bounded arboricity in $O(\\log n)$\nrounds. This improves over the well-known randomized algorithm of Lenzen and\nWattenhofer[DISC2010] by making it a deterministic algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 10:57:03 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 16:02:13 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""]]}, {"id": "2102.08173", "submitter": "Giorgos Stamatelatos", "authors": "Giorgos Stamatelatos and Pavlos S. Efraimidis", "title": "About Weighted Random Sampling in Preferential Attachment Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Barab\\'asi-Albert model is a popular scheme for creating scale-free\ngraphs but has been previously shown to have ambiguities in its definition. In\nthis paper we discuss a new ambiguity in the definition of the BA model by\nidentifying the tight relation between the preferential attachment process and\nunequal probability random sampling. While the probability that each individual\nvertex is selected is set to be proportional to their degree, the model does\nnot specify the joint probabilities that any tuple of $m$ vertices is selected\ntogether for $m>1$. We demonstrate the consequences using analytical,\nexperimental, and empirical analyses and propose a concise definition of the\nmodel that addresses this ambiguity. Using the connection with unequal\nprobability random sampling, we also highlight a confusion about the process\nvia which nodes are selected on each time step, for which -- despite being\nimplicitly indicated in the original paper -- current literature appears\nfragmented.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:22:27 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 13:26:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Stamatelatos", "Giorgos", ""], ["Efraimidis", "Pavlos S.", ""]]}, {"id": "2102.08181", "submitter": "Florian Schneider", "authors": "Christoph Damerius, Dominik Kaaser, Peter Kling, Florian Schneider", "title": "On Greedily Packing Anchored Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set P of points in the unit square U, one of them being the\norigin. For each point p in P you may draw a rectangle in U with its lower-left\ncorner in p. What is the maximum area such rectangles can cover without\noverlapping each other? Freedman [1969] posed this problem in 1969, asking\nwhether one can always cover at least 50% of U. Over 40 years later, Dumitrescu\nand T\\'oth [2011] achieved the first constant coverage of 9.1%; since then, no\nsignificant progress was made. While 9.1% might seem low, the authors could not\nfind any instance where their algorithm covers less than 50%, nourishing the\nhope to eventually prove a 50% bound. While we indeed significantly raise the\nalgorithm's coverage to 39%, we extinguish the hope of reaching 50% by giving\npoints for which the coverage is below 43.3%. Our analysis studies the\nalgorithm's average and worst-case density of so-called tiles, which represent\nthe area where a given point can freely choose its maximum-area rectangle. Our\napproachis comparatively general and may potentially help in analyzing related\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:30:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Damerius", "Christoph", ""], ["Kaaser", "Dominik", ""], ["Kling", "Peter", ""], ["Schneider", "Florian", ""]]}, {"id": "2102.08243", "submitter": "Marius Zimand", "authors": "Marius Zimand", "title": "Online matching in lossless expanders", "comments": "Abstract shortened to meet arxiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bauwens and Zimand [BZ 2019] have shown that lossless expanders have an\ninteresting online matching property. The result appears in an implicit form in\n[BZ 2019]. We present an explicit version of this property which is directly\namenable to typical applications, prove it in a self-contained manner that\nclarifies the role of some parameters, and give two applications.\n  A $(K, \\epsilon)$ lossless expander is a bipartite graph such that any subset\n$S$ of size at most $K$ of nodes on the left side of the bipartition has at\nleast $(1-\\epsilon) D |S|$ neighbors, where $D$ is the left degree.The main\nresult is that any such graph, after a slight modification, admits\n$(1-O(\\epsilon)D, 1)$ online matching up to size $K$. This means that for any\nsequence $S=(x_1, \\ldots, x_K)$ of nodes on the left side of the bipartition,\none can assign in an online manner to each node $x_i$ in $S$ a set $A_i$\nconsisting of $(1-O(\\epsilon))$ fraction of its neighbors so that the sets\n$A_1, \\ldots, A_K$ are pairwise disjoint. \"Online manner\" refers to the fact\nthat, for every $i$, the set of nodes assigned to $x_i$ only depends on the\nnodes assigned to $x_1, \\ldots, x_{i-1}$.\n  The first application concerns storage schemes for representing a set $S$, so\nthat a membership query \"Is $x \\in S$?\" can be answered probabilistically by\nreading a single bit. All the previous one-probe storage schemes were for a\nstatic set $S$. We show that a lossless expander can be used to construct a\none-probe storage scheme for dynamic sets, i.e., sets in which elements can be\ninserted and deleted without affecting the representation of other elements.\nThe second application is about non-blocking networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:01:57 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zimand", "Marius", ""]]}, {"id": "2102.08327", "submitter": "Federico Fusco", "authors": "Georgios Amanatidis, Federico Fusco, Philip Lazos, Stefano Leonardi,\n  Alberto Marchetti Spaccamela, Rebecca Reiffenh\\\"auser", "title": "Submodular Maximization subject to a Knapsack Constraint: Combinatorial\n  Algorithms with Near-optimal Adaptive Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need to deal with massive instances motivates the design of\nalgorithms balancing the quality of the solution with applicability. For the\nlatter, an important measure is the \\emph{adaptive complexity}, capturing the\nnumber of sequential rounds of parallel computation needed. In this work we\nobtain the first \\emph{constant factor} approximation algorithm for\nnon-monotone submodular maximization subject to a knapsack constraint with\n\\emph{near-optimal} $O(\\log n)$ adaptive complexity. Low adaptivity by itself,\nhowever, is not enough: one needs to account for the total number of function\nevaluations (or value queries) as well. Our algorithm asks $\\tilde{O}(n^2)$\nvalue queries, but can be modified to run with only $\\tilde{O}(n)$ instead,\nwhile retaining a low adaptive complexity of $O(\\log^2n)$. Besides the above\nimprovement in adaptivity, this is also the first \\emph{combinatorial} approach\nwith sublinear adaptive complexity for the problem and yields algorithms\ncomparable to the state-of-the-art even for the special cases of cardinality\nconstraints or monotone objectives. Finally, we showcase our algorithms'\napplicability on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:15:51 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Amanatidis", "Georgios", ""], ["Fusco", "Federico", ""], ["Lazos", "Philip", ""], ["Leonardi", "Stefano", ""], ["Spaccamela", "Alberto Marchetti", ""], ["Reiffenh\u00e4user", "Rebecca", ""]]}, {"id": "2102.08341", "submitter": "Cameron Musco", "authors": "Arturs Backurs and Piotr Indyk and Cameron Musco and Tal Wagner", "title": "Faster Kernel Matrix Algebra via Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study fast algorithms for computing fundamental properties of a positive\nsemidefinite kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ corresponding to $n$\npoints $x_1,\\ldots,x_n \\in \\mathbb{R}^d$. In particular, we consider estimating\nthe sum of kernel matrix entries, along with its top eigenvalue and\neigenvector.\n  We show that the sum of matrix entries can be estimated to $1+\\epsilon$\nrelative error in time $sublinear$ in $n$ and linear in $d$ for many popular\nkernels, including the Gaussian, exponential, and rational quadratic kernels.\nFor these kernels, we also show that the top eigenvalue (and an approximate\neigenvector) can be approximated to $1+\\epsilon$ relative error in time\n$subquadratic$ in $n$ and linear in $d$.\n  Our algorithms represent significant advances in the best known runtimes for\nthese problems. They leverage the positive definiteness of the kernel matrix,\nalong with a recent line of work on efficient kernel density estimation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:25:47 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 18:18:57 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""], ["Musco", "Cameron", ""], ["Wagner", "Tal", ""]]}, {"id": "2102.08342", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Huy Tuan Pham, Thuy-Duong Vuong", "title": "On the sampling Lov\\'asz Local Lemma for atomic constraint satisfaction\n  problems", "comments": "35 pages; comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of sampling an approximately uniformly random satisfying\nassignment for atomic constraint satisfaction problems i.e. where each\nconstraint is violated by only one assignment to its variables. Let $p$ denote\nthe maximum probability of violation of any constraint and let $\\Delta$ denote\nthe maximum degree of the line graph of the constraints.\n  Our main result is a nearly-linear (in the number of variables) time\nalgorithm for this problem, which is valid in a Lov\\'asz local lemma type\nregime that is considerably less restrictive compared to previous works. In\nparticular, we provide sampling algorithms for the uniform distribution on:\n  (1) $q$-colorings of $k$-uniform hypergraphs with $\\Delta \\lesssim q^{(k-4)/3\n+ o_{q}(1)}.$\n  The exponent $1/3$ improves the previously best-known $1/7$ in the case $q,\n\\Delta = O(1)$ [Jain, Pham, Vuong; arXiv, 2020] and $1/9$ in the general case\n[Feng, He, Yin; STOC 2021].\n  (2) Satisfying assignments of Boolean $k$-CNF formulas with $\\Delta \\lesssim\n2^{k/5.741}.$\n  The constant $5.741$ in the exponent improves the previously best-known $7$\nin the case $k = O(1)$ [Jain, Pham, Vuong; arXiv, 2020] and $13$ in the general\ncase [Feng, He, Yin; STOC 2021].\n  (3) Satisfying assignments of general atomic constraint satisfaction problems\nwith $p\\cdot \\Delta^{7.043} \\lesssim 1.$\n  The constant $7.043$ improves upon the previously best-known constant of\n$350$ [Feng, He, Yin; STOC 2021].\n  At the heart of our analysis is a novel information-percolation type argument\nfor showing the rapid mixing of the Glauber dynamics for a carefully\nconstructed projection of the uniform distribution on satisfying assignments.\nNotably, there is no natural partial order on the space, and we believe that\nthe techniques developed for the analysis may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:26:34 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Jain", "Vishesh", ""], ["Pham", "Huy Tuan", ""], ["Vuong", "Thuy-Duong", ""]]}, {"id": "2102.08349", "submitter": "Feodor Dragan F", "authors": "Feodor F. Dragan and Guillaume Ducoffe and Heather M. Guarnera", "title": "Fast deterministic algorithms for computing all eccentricities in\n  (hyperbolic) Helly graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A graph is Helly if every family of pairwise intersecting balls has a\nnonempty common intersection. The class of Helly graphs is the discrete\nanalogue of the class of hyperconvex metric spaces. It is also known that every\ngraph isometrically embeds into a Helly graph, making the latter an important\nclass of graphs in Metric Graph Theory. We study diameter, radius and all\neccentricity computations within the Helly graphs. Under plausible complexity\nassumptions, neither the diameter nor the radius can be computed in truly\nsubquadratic time on general graphs. In contrast to these negative results, it\nwas recently shown that the radius and the diameter of an $n$-vertex $m$-edge\nHelly graph $G$ can be computed with high probability in $\\tilde{\\mathcal\nO}(m\\sqrt{n})$ time (i.e., subquadratic in $n+m$). In this paper, we improve\nthat result by presenting a deterministic ${\\mathcal O}(m\\sqrt{n})$ time\nalgorithm which computes not only the radius and the diameter but also all\nvertex eccentricities in a Helly graph. Furthermore, we give a parameterized\nlinear-time algorithm for this problem on Helly graphs, with the parameter\nbeing the Gromov hyperbolicity $\\delta$. More specifically, we show that the\nradius and a central vertex of an $m$-edge $\\delta$-hyperbolic Helly graph $G$\ncan be computed in $\\mathcal O(\\delta m)$ time and that all vertex\neccentricities in $G$ can be computed in $\\mathcal O(\\delta^2 m)$ time. To show\nthis more general result, we heavily use our new structural properties obtained\nfor Helly graphs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:38:47 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Dragan", "Feodor F.", ""], ["Ducoffe", "Guillaume", ""], ["Guarnera", "Heather M.", ""]]}, {"id": "2102.08446", "submitter": "Abhishek Shetty", "authors": "Nika Haghtalab, Tim Roughgarden, Abhishek Shetty", "title": "Smoothed Analysis with Adaptive Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove novel algorithmic guarantees for several online problems in the\nsmoothed analysis model. In this model, at each time an adversary chooses an\ninput distribution with density function bounded above by $\\tfrac{1}{\\sigma}$\ntimes that of the uniform distribution; nature then samples an input from this\ndistribution. Crucially, our results hold for {\\em adaptive} adversaries that\ncan choose an input distribution based on the decisions of the algorithm and\nthe realizations of the inputs in the previous time steps.\n  This paper presents a general technique for proving smoothed algorithmic\nguarantees against adaptive adversaries, in effect reducing the setting of\nadaptive adversaries to the simpler case of oblivious adversaries. We apply\nthis technique to prove strong smoothed guarantees for three problems:\n  -Online learning: We consider the online prediction problem, where instances\nare generated from an adaptive sequence of $\\sigma$-smooth distributions and\nthe hypothesis class has VC dimension $d$. We bound the regret by\n$\\tilde{O}\\big(\\sqrt{T d\\ln(1/\\sigma)} + d\\sqrt{\\ln(T/\\sigma)}\\big)$. This\nanswers open questions of [RST11,Hag18].\n  -Online discrepancy minimization: We consider the online Koml\\'os problem,\nwhere the input is generated from an adaptive sequence of $\\sigma$-smooth and\nisotropic distributions on the $\\ell_2$ unit ball. We bound the $\\ell_\\infty$\nnorm of the discrepancy vector by $\\tilde{O}\\big(\\ln^2\\!\\big(\n\\frac{nT}{\\sigma}\\big) \\big)$.\n  -Dispersion in online optimization: We consider online optimization of\npiecewise Lipschitz functions where functions with $\\ell$ discontinuities are\nchosen by a smoothed adaptive adversary and show that the resulting sequence is\n$\\big( {\\sigma}/{\\sqrt{T\\ell}}, \\tilde O\\big(\\sqrt{T\\ell}\n\\big)\\big)$-dispersed. This matches the parameters of [BDV18] for oblivious\nadversaries, up to log factors.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 20:54:49 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Haghtalab", "Nika", ""], ["Roughgarden", "Tim", ""], ["Shetty", "Abhishek", ""]]}, {"id": "2102.08454", "submitter": "Emily Diana", "authors": "Emily Diana, Wesley Gill, Ira Globus-Harris, Michael Kearns, Aaron\n  Roth and Saeed Sharifi-Malvajerdi", "title": "Lexicographically Fair Learning: Algorithms and Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of minimax fairness in supervised learning problems to\nits natural conclusion: lexicographic minimax fairness (or lexifairness for\nshort). Informally, given a collection of demographic groups of interest,\nminimax fairness asks that the error of the group with the highest error be\nminimized. Lexifairness goes further and asks that amongst all minimax fair\nsolutions, the error of the group with the second highest error should be\nminimized, and amongst all of those solutions, the error of the group with the\nthird highest error should be minimized, and so on. Despite its naturalness,\ncorrectly defining lexifairness is considerably more subtle than minimax\nfairness, because of inherent sensitivity to approximation error. We give a\nnotion of approximate lexifairness that avoids this issue, and then derive\noracle-efficient algorithms for finding approximately lexifair solutions in a\nvery general setting. When the underlying empirical risk minimization problem\nabsent fairness constraints is convex (as it is, for example, with linear and\nlogistic regression), our algorithms are provably efficient even in the worst\ncase. Finally, we show generalization bounds -- approximate lexifairness on the\ntraining sample implies approximate lexifairness on the true distribution with\nhigh probability. Our ability to prove generalization bounds depends on our\nchoosing definitions that avoid the instability of naive definitions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:15:42 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Diana", "Emily", ""], ["Gill", "Wesley", ""], ["Globus-Harris", "Ira", ""], ["Kearns", "Michael", ""], ["Roth", "Aaron", ""], ["Sharifi-Malvajerdi", "Saeed", ""]]}, {"id": "2102.08476", "submitter": "Hoa Vu", "authors": "Andrew McGregor, David Tench, Hoa T. Vu", "title": "Maximum Coverage in the Data Stream Model: Parameterized and Generalized", "comments": "Conference version to appear at ICDT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present algorithms for the Max-Cover and Max-Unique-Cover problems in the\ndata stream model. The input to both problems are $m$ subsets of a universe of\nsize $n$ and a value $k\\in [m]$. In Max-Cover, the problem is to find a\ncollection of at most $k$ sets such that the number of elements covered by at\nleast one set is maximized. In Max-Unique-Cover, the problem is to find a\ncollection of at most $k$ sets such that the number of elements covered by\nexactly one set is maximized. Our goal is to design single-pass algorithms that\nuse space that is sublinear in the input size. Our main algorithmic results\nare:\n  If the sets have size at most $d$, there exist single-pass algorithms using\n$\\tilde{O}(d^{d+1} k^d)$ space that solve both problems exactly. This is\noptimal up to polylogarithmic factors for constant $d$.\n  If each element appears in at most $r$ sets, we present single pass\nalgorithms using $\\tilde{O}(k^2 r/\\epsilon^3)$ space that return a $1+\\epsilon$\napproximation in the case of Max-Cover. We also present a single-pass algorithm\nusing slightly more memory, i.e., $\\tilde{O}(k^3 r/\\epsilon^{4})$ space, that\n$1+\\epsilon$ approximates Max-Unique-Cover.\n  In contrast to the above results, when $d$ and $r$ are arbitrary, any\nconstant pass $1+\\epsilon$ approximation algorithm for either problem requires\n$\\Omega(\\epsilon^{-2}m)$ space but a single pass $O(\\epsilon^{-2}mk)$ space\nalgorithm exists. In fact any constant-pass algorithm with an approximation\nbetter than $e/(e-1)$ and $e^{1-1/k}$ for Max-Cover and Max-Unique-Cover\nrespectively requires $\\Omega(m/k^2)$ space when $d$ and $r$ are unrestricted.\nEn route, we also obtain an algorithm for a parameterized version of the\nstreaming Set-Cover problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 22:30:45 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["McGregor", "Andrew", ""], ["Tench", "David", ""], ["Vu", "Hoa T.", ""]]}, {"id": "2102.08529", "submitter": "Muhammad Farhan", "authors": "Muhammad Farhan and Qing Wang", "title": "Efficient Maintenance of Distance Labelling for Incremental Updates in\n  Large Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the shortest path distance between an arbitrary pair of vertices is a\nfundamental problem in graph theory. A tremendous amount of research has been\nsuccessfully attempted on this problem, most of which is limited to static\ngraphs. Due to the dynamic nature of real-world networks, there is a pressing\nneed to address this problem for dynamic networks undergoing changes. In this\npaper, we propose an \\emph{online incremental} method to efficiently answer\ndistance queries over very large dynamic graphs. Our proposed method\nincorporates incremental update operations, i.e. edge and vertex additions,\ninto a highly scalable framework of answering distance queries. We\ntheoretically prove the correctness of our method and the preservation of\nlabelling minimality. We have also conducted extensive experiments on 12 large\nreal-world networks to empirically verify the efficiency, scalability, and\nrobustness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 01:57:52 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Farhan", "Muhammad", ""], ["Wang", "Qing", ""]]}, {"id": "2102.08569", "submitter": "Hanlin Ren", "authors": "Yong Gu and Hanlin Ren", "title": "Constructing a Distance Sensitivity Oracle in $O(n^{2.5794}M)$ Time", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue the study of distance sensitivity oracles (DSOs). Given a\ndirected graph $G$ with $n$ vertices and edge weights in $\\{1, 2, \\dots, M\\}$,\nwe want to build a data structure such that given any source vertex $u$, any\ntarget vertex $v$, and any failure $f$ (which is either a vertex or an edge),\nit outputs the length of the shortest path from $u$ to $v$ not going through\n$f$. Our main result is a DSO with preprocessing time $O(n^{2.5794}M)$ and\nconstant query time. Previously, the best preprocessing time of DSOs for\ndirected graphs is $O(n^{2.7233}M)$, and even in the easier case of undirected\ngraphs, the best preprocessing time is $O(n^{2.6865}M)$ [Ren, ESA 2020]. One\ndrawback of our DSOs, though, is that it only supports distance queries but not\npath queries.\n  Our main technical ingredient is an algorithm that computes the inverse of a\ndegree-$d$ polynomial matrix (i.e. a matrix whose entries are degree-$d$\nunivariate polynomials) modulo $x^r$. The algorithm is adapted from [Zhou,\nLabahn and Storjohann, Journal of Complexity, 2015], and we replace some of its\nintermediate steps with faster rectangular matrix multiplication algorithms.\n  We also show how to compute unique shortest paths in a directed graph with\nedge weights in $\\{1, 2, \\dots, M\\}$, in $O(n^{2.5286}M)$ time. This algorithm\nis crucial in the preprocessing algorithm of our DSO. Our solution improves the\n$O(n^{2.6865}M)$ time bound in [Ren, ESA 2020], and matches the current best\ntime bound for computing all-pairs shortest paths.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 04:28:04 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Gu", "Yong", ""], ["Ren", "Hanlin", ""]]}, {"id": "2102.08598", "submitter": "Terrance Liu", "authors": "Terrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, Zhiwei\n  Steven Wu", "title": "Leveraging Public Data for Practical Private Query Release", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical problems, incorporating priors can significantly improve\nperformance. However, the use of prior knowledge in differentially private\nquery release has remained underexplored, despite such priors commonly being\navailable in the form of public datasets, such as previous US Census releases.\nWith the goal of releasing statistics about a private dataset, we present\nPMW^Pub, which -- unlike existing baselines -- leverages public data drawn from\na related distribution as prior information. We provide a theoretical analysis\nand an empirical evaluation on the American Community Survey (ACS) and ADULT\ndatasets, which shows that our method outperforms state-of-the-art methods.\nFurthermore, PMW^Pub scales well to high-dimensional data domains, where\nrunning many existing methods would be computationally infeasible.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 06:19:34 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 03:13:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liu", "Terrance", ""], ["Vietri", "Giuseppe", ""], ["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2102.08670", "submitter": "Jonas Ellert", "authors": "Jonas Ellert, Johannes Fischer", "title": "Linear Time Runs over General Ordered Alphabets", "comments": "This work has been submitted to ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A run in a string is a maximal periodic substring. For example, the string\n$\\texttt{bananatree}$ contains the runs $\\texttt{anana} = (\\texttt{an})^{3/2}$\nand $\\texttt{ee} = \\texttt{e}^2$. There are less than $n$ runs in any\nlength-$n$ string, and computing all runs for a string over a linearly-sortable\nalphabet takes $\\mathcal{O}(n)$ time (Bannai et al., SODA 2015). Kosolobov\nconjectured that there also exists a linear time runs algorithm for general\nordered alphabets (Inf. Process. Lett. 2016). The conjecture was almost proven\nby Crochemore et al., who presented an $\\mathcal{O}(n\\alpha(n))$ time algorithm\n(where $\\alpha(n)$ is the extremely slowly growing inverse Ackermann function).\nWe show how to achieve $\\mathcal{O}(n)$ time by exploiting combinatorial\nproperties of the Lyndon array, thus proving Kosolobov's conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 10:25:06 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ellert", "Jonas", ""], ["Fischer", "Johannes", ""]]}, {"id": "2102.08703", "submitter": "Darya Melnyk", "authors": "Alkida Balliu, Juho Hirvonen, Darya Melnyk, Dennis Olivetti, Joel\n  Rybicki, and Jukka Suomela", "title": "Local Mending", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce the graph-theoretic notion of mendability: for each\nlocally checkable graph problem we can define its mending radius, which\ncaptures the idea of how far one needs to modify a partial solution in order to\n\"patch a hole.\"\n  We explore how mendability is connected to the existence of efficient\nalgorithms, especially in distributed, parallel, and fault-tolerant settings.\nIt is easy to see that $O(1)$-mendable problems are also solvable in $O(\\log^*\nn)$ rounds in the LOCAL model of distributed computing. One of the surprises is\nthat in paths and cycles, a converse also holds in the following sense: if a\nproblem $\\Pi$ can be solved in $O(\\log^* n)$, there is always a restriction\n$\\Pi' \\subseteq \\Pi$ that is still efficiently solvable but that is also\n$O(1)$-mendable.\n  We also explore the structure of the landscape of mendability. For example,\nwe show that in trees, the mending radius of any locally checkable problem is\n$O(1)$, $\\Theta(\\log n)$, or $\\Theta(n)$, while in general graphs the structure\nis much more diverse.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 11:18:10 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 11:07:13 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 10:47:21 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Hirvonen", "Juho", ""], ["Melnyk", "Darya", ""], ["Olivetti", "Dennis", ""], ["Rybicki", "Joel", ""], ["Suomela", "Jukka", ""]]}, {"id": "2102.08765", "submitter": "Davide Ferrari", "authors": "Davide Ferrari, Ivano Tavernelli, Michele Amoretti", "title": "Deterministic Algorithms for Compiling Quantum Circuits with Recurrent\n  Patterns", "comments": null, "journal-ref": "Quantum Inf Process 20, 213 (2021)", "doi": "10.1007/s11128-021-03150-9", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current quantum processors are noisy, have limited coherence and imperfect\ngate implementations. On such hardware, only algorithms that are shorter than\nthe overall coherence time can be implemented and executed successfully. A good\nquantum compiler must translate an input program into the most efficient\nequivalent of itself, getting the most out of the available hardware. In this\nwork, we present novel deterministic algorithms for compiling recurrent quantum\ncircuit patterns in polynomial time. In particular, such patterns appear in\nquantum circuits that are used to compute the ground state properties of\nmolecular systems using the variational quantum eigensolver (VQE) method\ntogether with the RyRz heuristic wavefunction Ansatz. We show that our\npattern-oriented compiling algorithms, combined with an efficient swapping\nstrategy, produces - in general - output programs that are comparable to those\nobtained with state-of-art compilers, in terms of CNOT count and CNOT depth. In\nparticular, our solution produces unmatched results on RyRz circuits.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 13:59:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ferrari", "Davide", ""], ["Tavernelli", "Ivano", ""], ["Amoretti", "Michele", ""]]}, {"id": "2102.08778", "submitter": "Giacomo Da Col", "authors": "Giacomo Da Col and Erich Teppan", "title": "Large-Scale Benchmarks for the Job Shop Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report contains the description of two novel job shop scheduling\nbenchmarks that resemble instances of real scheduling problem as they appear in\nindustry. In particular, the aim was to provide large-scale benchmarks (up to 1\nmillion operations) to test the state-of-the-art scheduling solutions on\nproblems that are closer to what occurs in a real industrial context. The first\nbenchmark is an extension of the well known Taillard benchmark (1992), while\nthe second is a collection of scheduling instances with a known-optimum\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:18:48 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 15:07:02 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Da Col", "Giacomo", ""], ["Teppan", "Erich", ""]]}, {"id": "2102.08808", "submitter": "Joel Rybicki", "authors": "Dan Alistarh and Rati Gelashvili and Joel Rybicki", "title": "Fast Graphical Population Protocols", "comments": "42 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph on $n$ nodes. In the stochastic population protocol model,\na collection of $n$ indistinguishable, resource-limited nodes collectively\nsolve tasks via pairwise interactions. In each interaction, two randomly chosen\nneighbors first read each other's states, and then update their local states. A\nrich line of research has established tight upper and lower bounds on the\ncomplexity of fundamental tasks, such as majority and leader election, in this\nmodel, when $G$ is a clique. Specifically, in the clique, these tasks can be\nsolved fast, i.e., in $n \\operatorname{polylog} n$ pairwise interactions, with\nhigh probability, using at most $\\operatorname{polylog} n$ states per node.\n  In this work, we consider the more general setting where $G$ is an arbitrary\ngraph, and present a technique for simulating protocols designed for\nfully-connected networks in any connected regular graph. Our main result is a\nsimulation that is efficient on many interesting graph families: roughly, the\nsimulation overhead is polylogarithmic in the number of nodes, and quadratic in\nthe conductance of the graph. As a sample application, we show that, in any\nregular graph with conductance $\\phi$, both leader election and exact majority\ncan be solved in $\\phi^{-2} \\cdot n \\operatorname{polylog} n$ pairwise\ninteractions, with high probability, using at most $\\phi^{-2} \\cdot\n\\operatorname{polylog} n$ states per node. This shows that there are fast and\nspace-efficient population protocols for leader election and exact majority on\ngraphs with good expansion properties. We believe our results will prove\ngenerally useful, as they allow efficient technology transfer between the\nwell-mixed (clique) case, and the under-explored spatial setting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 15:13:25 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 18:51:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Alistarh", "Dan", ""], ["Gelashvili", "Rati", ""], ["Rybicki", "Joel", ""]]}, {"id": "2102.08885", "submitter": "Marek Eli\\'a\\v{s}", "authors": "Mark Bun, Marek Eli\\'a\\v{s}, Janardhan Kulkarni", "title": "Differentially Private Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Correlation clustering is a widely used technique in unsupervised machine\nlearning. Motivated by applications where individual privacy is a concern, we\ninitiate the study of differentially private correlation clustering. We propose\nan algorithm that achieves subquadratic additive error compared to the optimal\ncost. In contrast, straightforward adaptations of existing non-private\nalgorithms all lead to a trivial quadratic error. Finally, we give a lower\nbound showing that any pure differentially private algorithm for correlation\nclustering requires additive error of $\\Omega(n)$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:27:48 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Bun", "Mark", ""], ["Eli\u00e1\u0161", "Marek", ""], ["Kulkarni", "Janardhan", ""]]}, {"id": "2102.08905", "submitter": "Tomohiro Koana", "authors": "Matthias Bentert, Tomohiro Koana, Rolf Niedermeier", "title": "The Complexity of Gerrymandering Over Graphs: Paths and Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roughly speaking, gerrymandering is the systematic manipulation of the\nboundaries of electoral districts to make a specific (political) party win as\nmany districts as possible. While typically studied from a geographical point\nof view, addressing social network structures, the investigation of\ngerrymandering over graphs was recently initiated by Cohen-Zemach et al. [AAMAS\n2018]. Settling three open questions of Ito et al. [AAMAS 2019], we classify\nthe computational complexity of the NP-hard problem Gerrymandering over Graphs\nwhen restricted to paths and trees. Our results, which are mostly of negative\nnature (that is, worst-case hardness), in particular yield two complexity\ndichotomies for trees. For instance, the problem is polynomial-time solvable\nfor two parties but becomes weakly NP-hard for three. Moreover, we show that\nthe problem remains NP-hard even when the input graph is a path.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:54:03 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Bentert", "Matthias", ""], ["Koana", "Tomohiro", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2102.09002", "submitter": "Ioannis Caragiannis", "authors": "Ioannis Caragiannis, George Christodoulou, Nicos Protopapas", "title": "Impartial selection with prior information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of {\\em impartial selection}, a topic that lies at the\nintersection of computational social choice and mechanism design. The goal is\nto select the most popular individual among a set of community members. The\ninput can be modeled as a directed graph, where each node represents an\nindividual, and a directed edge indicates nomination or approval of a community\nmember to another. An {\\em impartial mechanism} is robust to potential selfish\nbehavior of the individuals and provides appropriate incentives to voters to\nreport their true preferences by ensuring that the chance of a node to become a\nwinner does not depend on its outgoing edges. The goal is to design impartial\nmechanisms that select a node with an in-degree that is as close as possible to\nthe highest in-degree. We measure the efficiency of such a mechanism by the\ndifference of these in-degrees, known as its {\\em additive} approximation.\n  In particular, we study the extent to which prior information on voters'\npreferences could be useful in the design of efficient deterministic impartial\nselection mechanisms with good additive approximation guarantees. We consider\nthree models of prior information, which we call the {\\em opinion poll}, the\n{\\em a prior popularity}, and the {\\em uniform} model. We analyze the\nperformance of a natural selection mechanism that we call {\\em approval voting\nwith default} (AVD) and show that it achieves a $O(\\sqrt{n\\ln{n}})$ additive\nguarantee for opinion poll and a $O(\\ln^2n)$ for a priori popularity inputs,\nwhere $n$ is the number of individuals. We consider this polylogarithmic bound\nas our main technical contribution. We complement this last result by showing\nthat our analysis is close to tight, showing an $\\Omega(\\ln{n})$ lower bound.\nThis holds in the uniform model, which is the simplest among the three models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:49:37 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Christodoulou", "George", ""], ["Protopapas", "Nicos", ""]]}, {"id": "2102.09032", "submitter": "Karl B\\\"ackstr\\\"om", "authors": "Karl B\\\"ackstr\\\"om, Ivan Walulya, Marina Papatriantafilou, Philippas\n  Tsigas", "title": "Consistent Lock-free Parallel Stochastic Gradient Descent for Fast and\n  Stable Convergence", "comments": "13 pages, 10 figures. Accepted in the 35th IEEE International\n  Parallel & Distributed Processing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient descent (SGD) is an essential element in Machine Learning\n(ML) algorithms. Asynchronous parallel shared-memory SGD (AsyncSGD), including\nsynchronization-free algorithms, e.g. HOGWILD!, have received interest in\ncertain contexts, due to reduced overhead compared to synchronous\nparallelization. Despite that they induce staleness and inconsistency, they\nhave shown speedup for problems satisfying smooth, strongly convex targets, and\ngradient sparsity. Recent works take important steps towards understanding the\npotential of parallel SGD for problems not conforming to these strong\nassumptions, in particular for deep learning (DL). There is however a gap in\ncurrent literature in understanding when AsyncSGD algorithms are useful in\npractice, and in particular how mechanisms for synchronization and consistency\nplay a role. We focus on the impact of consistency-preserving non-blocking\nsynchronization in SGD convergence, and in sensitivity to hyper-parameter\ntuning. We propose Leashed-SGD, an extensible algorithmic framework of\nconsistency-preserving implementations of AsyncSGD, employing lock-free\nsynchronization, effectively balancing throughput and latency. We argue\nanalytically about the dynamics of the algorithms, memory consumption, the\nthreads' progress over time, and the expected contention. We provide a\ncomprehensive empirical evaluation, validating the analytical claims,\nbenchmarking the proposed Leashed-SGD framework, and comparing to baselines for\ntraining multilayer perceptrons (MLP) and convolutional neural networks (CNN).\nWe observe the crucial impact of contention, staleness and consistency and show\nhow Leashed-SGD provides significant improvements in stability as well as\nwall-clock time to convergence (from 20-80% up to 4x improvements) compared to\nthe standard lock-based AsyncSGD algorithm and HOGWILD!, while reducing the\noverall memory footprint.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 21:24:44 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["B\u00e4ckstr\u00f6m", "Karl", ""], ["Walulya", "Ivan", ""], ["Papatriantafilou", "Marina", ""], ["Tsigas", "Philippas", ""]]}, {"id": "2102.09101", "submitter": "Robi Bhattacharjee", "authors": "Robi Bhattacharjee and Jacob Imola", "title": "No-Substitution $k$-means Clustering with Low Center Complexity and\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering is a fundamental task in machine learning. Given a dataset $X =\n\\{x_1, \\ldots x_n\\}$, the goal of $k$-means clustering is to pick $k$ \"centers\"\nfrom $X$ in a way that minimizes the sum of squared distances from each point\nto its nearest center. We consider $k$-means clustering in the online, no\nsubstitution setting, where one must decide whether to take $x_t$ as a center\nimmediately upon streaming it and cannot remove centers once taken.\n  The online, no substitution setting is challenging for clustering--one can\nshow that there exist datasets $X$ for which any $O(1)$-approximation $k$-means\nalgorithm must have center complexity $\\Omega(n)$, meaning that it takes\n$\\Omega(n)$ centers in expectation. Bhattacharjee and Moshkovitz (2020) refined\nthis bound by defining a complexity measure called $Lower_{\\alpha, k}(X)$, and\nproving that any $\\alpha$-approximation algorithm must have center complexity\n$\\Omega(Lower_{\\alpha, k}(X))$. They then complemented their lower bound by\ngiving a $O(k^3)$-approximation algorithm with center complexity\n$\\tilde{O}(k^2Lower_{k^3, k}(X))$, thus showing that their parameter is a tight\nmeasure of required center complexity. However, a major drawback of their\nalgorithm is its memory requirement, which is $O(n)$. This makes the algorithm\nimpractical for very large datasets.\n  In this work, we strictly improve upon their algorithm on all three fronts;\nwe develop a $36$-approximation algorithm with center complexity\n$\\tilde{O}(kLower_{36, k}(X))$ that uses only $O(k)$ additional memory. In\naddition to having nearly optimal memory, this algorithm is the first known\nalgorithm with center complexity bounded by $Lower_{36, k}(X)$ that is a true\n$O(1)$-approximation with its approximation factor being independent of $k$ or\n$n$.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:20:03 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Bhattacharjee", "Robi", ""], ["Imola", "Jacob", ""]]}, {"id": "2102.09127", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen and Matei Zaharia and James Zou", "title": "FrugalMCT: Efficient Online ML API Selection for Multi-Label\n  Classification Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification tasks such as OCR and multi-object recognition are\na major focus of the growing machine learning as a service industry. While many\nmulti-label prediction APIs are available, it is challenging for users to\ndecide which API to use for their own data and budget, due to the heterogeneity\nin those APIs' price and performance. Recent work shows how to select from\nsingle-label prediction APIs. However the computation complexity of the\nprevious approach is exponential in the number of labels and hence is not\nsuitable for settings like OCR. In this work, we propose FrugalMCT, a\nprincipled framework that adaptively selects the APIs to use for different data\nin an online fashion while respecting user's budget. The API selection problem\nis cast as an integer linear program, which we show has a special structure\nthat we leverage to develop an efficient online API selector with strong\nperformance guarantees. We conduct systematic experiments using ML APIs from\nGoogle, Microsoft, Amazon, IBM, Tencent and other providers for tasks including\nmulti-label image classification, scene text recognition and named entity\nrecognition. Across diverse tasks, FrugalMCT can achieve over 90% cost\nreduction while matching the accuracy of the best single API, or up to 8%\nbetter accuracy while matching the best API's cost.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 02:59:58 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Chen", "Lingjiao", ""], ["Zaharia", "Matei", ""], ["Zou", "James", ""]]}, {"id": "2102.09299", "submitter": "Pavel Vesel\\'y", "authors": "Graham Cormode, Abhinav Mishra, Joseph Ross, Pavel Vesel\\'y", "title": "Theory meets Practice at the Median: a worst case comparison of relative\n  error quantile algorithms", "comments": "Updated experiments, improved presentation. To appear in KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the distribution and quantiles of data is a foundational task in\ndata mining and data science. We study algorithms which provide accurate\nresults for extreme quantile queries using a small amount of space, thus\nhelping to understand the tails of the input distribution. Namely, we focus on\ntwo recent state-of-the-art solutions: $t$-digest and ReqSketch. While\n$t$-digest is a popular compact summary which works well in a variety of\nsettings, ReqSketch comes with formal accuracy guarantees at the cost of its\nsize growing as new observations are inserted. In this work, we provide insight\ninto which conditions make one preferable to the other. Namely, we show how to\nconstruct inputs for $t$-digest that induce an almost arbitrarily large error\nand demonstrate that it fails to provide accurate results even on i.i.d.\nsamples from a highly non-uniform distribution. We propose practical\nimprovements to ReqSketch, making it faster than $t$-digest, while its error\nstays bounded on any instance. Still, our results confirm that $t$-digest\nremains more accurate on the ``non-adversarial'' data encountered in practice.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:18:44 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:13:02 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Cormode", "Graham", ""], ["Mishra", "Abhinav", ""], ["Ross", "Joseph", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "2102.09384", "submitter": "Marcelo Fonseca Faraj", "authors": "Marcelo Fonseca Faraj, Christian Schulz", "title": "Buffered Streaming Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partitioning graphs into blocks of roughly equal size is a widely used tool\nwhen processing large graphs. Currently there is a gap in the space of\navailable partitioning algorithms. On the one hand, there are streaming\nalgorithms that have been adopted to partition massive graph data on small\nmachines. In the streaming model, vertices arrive one at a time including their\nneighborhood and then have to be assigned directly to a block. These algorithms\ncan partition huge graphs quickly with little memory, but they produce\npartitions with low quality. On the other hand, there are offline\n(shared-memory) multilevel algorithms that produce partitions with high quality\nbut also need a machine with enough memory to partition a network. In this\nwork, we make a first step to close this gap by presenting an algorithm that\ncomputes high-quality partitions of huge graphs using a single machine with\nlittle memory. First, we extend the streaming model to a more reasonable\napproach in practice: the buffered streaming model. In this model, a PE can\nstore a batch of nodes (including their neighborhood) before making assignment\ndecisions. When our algorithm receives a batch of nodes, we build a model graph\nthat represents the nodes of the batch and the already present partition\nstructure. This model enables us to apply multilevel algorithms and in turn\ncompute high-quality solutions of huge graphs on cheap machines. To partition\nthe model, we develop a multilevel algorithm that optimizes an objective\nfunction that has previously shown to be effective for the streaming setting.\nSurprisingly, this also removes the dependency on the number of blocks from the\nrunning time. Overall, our algorithm computes on average 55% better solutions\nthan Fennel using a very small batch size. In addition, our algorithm is\nsignificantly faster than one of the main one-pass partitioning algorithms for\nlarger amounts of blocks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:31:34 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Faraj", "Marcelo Fonseca", ""], ["Schulz", "Christian", ""]]}, {"id": "2102.09413", "submitter": "Joel Rybicki", "authors": "Maciej Pacut, Mahmoud Parham, Joel Rybicki, Stefan Schmid, Jukka\n  Suomela and Aleksandr Tereshchenko", "title": "Locality in Online Algorithms", "comments": "46 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online algorithms make decisions based on past inputs. In general, the\ndecision may depend on the entire history of inputs. If many computers run the\nsame online algorithm with the same input stream but are started at different\ntimes, they do not necessarily make consistent decisions.\n  In this work we introduce time-local online algorithms. These are online\nalgorithms where the output at a given time only depends on $T = O(1)$ latest\ninputs. The use of (deterministic) time-local algorithms in a distributed\nsetting automatically leads to globally consistent decisions.\n  Our key observation is that time-local online algorithms (in which the output\nat a given time only depends on local inputs in the temporal dimension) are\nclosely connected to local distributed graph algorithms (in which the output of\na given node only depends on local inputs in the spatial dimension). This makes\nit possible to interpret prior work on distributed graph algorithms from the\nperspective of online algorithms.\n  We describe an algorithm synthesis method that one can use to design optimal\ntime-local online algorithms for small values of $T$. We demonstrate the power\nof the technique in the context of a variant of the online file migration\nproblem, and show that e.g. for two nodes and unit migration costs there exists\na $3$-competitive time-local algorithm with horizon $T=4$, while no\ndeterministic online algorithm (in the classic sense) can do better. We also\nderive upper and lower bounds for a more general version of the problem; we\nshow that there is a $6$-competitive deterministic time-local algorithm and a\n$2.62$-competitive randomized time-local algorithm for any migration cost\n$\\alpha \\ge 1$.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:02:22 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Pacut", "Maciej", ""], ["Parham", "Mahmoud", ""], ["Rybicki", "Joel", ""], ["Schmid", "Stefan", ""], ["Suomela", "Jukka", ""], ["Tereshchenko", "Aleksandr", ""]]}, {"id": "2102.09432", "submitter": "Marilena Leichter", "authors": "Alexander Eckl, Anja Kirschbaum, Marilena Leichter and Kevin Schewior", "title": "A Stronger Impossibility for Fully Online Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the fully online matching model (Huang et al., J.\\ ACM, 2020), an\nextension of the classic online matching model due to Karp, Vazirani, and\nVazirani (STOC 1990), which has recently received a lot of attention (Huang et\nal., SODA 2019 and FOCS 2020), partly due to applications in ride-sharing\nplatforms. It has been shown that the fully online version is harder than the\nclassic version for which the achievable competitive ratio is at most $0.6317$,\nrather than precisely $1-\\frac{1}{e}\\approx 0.6321$. We introduce two new ideas\nto the construction. By optimizing the parameters of the modified construction\nnumerically, we obtain an improved impossibility result of $0.6297$. Like the\nprevious bound, the new bound even holds for fractional (rather than\nrandomized) algorithms on bipartite graphs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:46:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Eckl", "Alexander", ""], ["Kirschbaum", "Anja", ""], ["Leichter", "Marilena", ""], ["Schewior", "Kevin", ""]]}, {"id": "2102.09463", "submitter": "Lu\\'is M. S. Russo", "authors": "Lu\\'is M. S. Russo", "title": "Range Minimum Queries in Minimal Space", "comments": "29 pages, 3 figures, 3 tables, 6 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a sequence of range minimum queries. We\nassume a sequence of commands that contains values and queries. Our goal is to\nquickly determine the minimum value that exists between the current position\nand a previous position $i$. Range minimum queries are used as a sub-routine of\nseveral algorithms, namely related to string processing. We propose a data\nstructure that can process these commands sequences. We obtain efficient\nresults for several variations of the problem, in particular we obtain $O(1)$\ntime per command for the offline version and $O(\\alpha(n))$ amortized time for\nthe online version, where $\\alpha(n)$ is the inverse Ackermann function and $n$\nthe number of values in the sequence. This data structure also has very small\nspace requirements, namely $O(\\ell)$ where $\\ell$ is the maximum number active\n$i$ positions. We implemented our data structure and show that it is\ncompetitive against existing alternatives. We obtain comparable command\nprocessing time, in the nano second range, and much smaller space requirements.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:25:13 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Russo", "Lu\u00eds M. S.", ""]]}, {"id": "2102.09510", "submitter": "Federico Ricci-Tersenghi", "authors": "M. Bernaschi, M. Bisson, M. Fatica, E. Marinari, V. Martin-Mayor, G.\n  Parisi and F. Ricci-Tersenghi", "title": "How we are leading a 3-XORSAT challenge: from the energy landscape to\n  the algorithm and its efficient implementation on GPUs", "comments": "7 pages, 7 figure, EPL format + SM (2 pages)", "journal-ref": "EPL, 133 (2021) 60005", "doi": "10.1209/0295-5075/133/60005", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DC cs.DS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent 3-XORSAT challenge required to minimize a very complex and rough\nenergy function, typical of glassy models with a random first order transition\nand a golf course like energy landscape. We present the ideas beyond the\nquasi-greedy algorithm and its very efficient implementation on GPUs that are\nallowing us to rank first in such a competition. We suggest a better protocol\nto compare algorithmic performances and we also provide analytical predictions\nabout the exponential growth of the times to find the solution in terms of\nfree-energy barriers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 17:52:32 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 10:58:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bernaschi", "M.", ""], ["Bisson", "M.", ""], ["Fatica", "M.", ""], ["Marinari", "E.", ""], ["Martin-Mayor", "V.", ""], ["Parisi", "G.", ""], ["Ricci-Tersenghi", "F.", ""]]}, {"id": "2102.09544", "submitter": "Christopher Morris", "authors": "Quentin Cappart, Didier Ch\\'etelat, Elias Khalil, Andrea Lodi,\n  Christopher Morris, Petar Veli\\v{c}kovi\\'c", "title": "Combinatorial optimization and reasoning with graph neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization is a well-established area in operations research\nand computer science. Until recently, its methods have focused on solving\nproblem instances in isolation, ignoring the fact that they often stem from\nrelated data distributions in practice. However, recent years have seen a surge\nof interest in using machine learning, especially graph neural networks (GNNs),\nas a key building block for combinatorial tasks, either directly as solvers or\nby enhancing exact solvers. The inductive bias of GNNs effectively encodes\ncombinatorial and relational input due to their invariance to permutations and\nawareness of input sparsity. This paper presents a conceptual review of recent\nkey advancements in this emerging field, aiming at researchers in both\noptimization and machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:47:20 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:47:38 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Cappart", "Quentin", ""], ["Ch\u00e9telat", "Didier", ""], ["Khalil", "Elias", ""], ["Lodi", "Andrea", ""], ["Morris", "Christopher", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2102.09644", "submitter": "Theophile Thiery", "authors": "Theophile Thiery, Justin Ward", "title": "Two-Sided Weak Submodularity for Matroid Constrained Optimization and\n  Regression", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of weak submodularity and the related submodularity ratio\nconsiders the behavior of a set function as elements are added to some current\nsolution. By considering the submodularity ratio, strong guarantees have been\nobtained for maximizing various non-submodular objectives subject to a\ncardinality constraint via the standard greedy algorithm. Here, we give a\nnatural complement to the notion of weak submodularity by considering how a\nfunction changes as elements are removed from the solution. We show that a\ncombination of these two notions can be used to obtain strong guarantees for\nmaximizing non-submodular objectives subject to an arbitrary matroid constraint\nvia both standard and distorted local search algorithms. Our guarantees improve\non the state of the art whenever $\\gamma$ is moderately large, and agree with\nknown guarantees for submodular objectives when $\\gamma = 1$. As motivation, we\nconsider both the subset selection problem, and the Bayesian A-optimal design\nproblem for linear regression, both of which were previously studied in the\ncontext of weak submodularity. We show that these problems satisfy our\ncomplementary notion of approximate submodularity, as well, allowing us to\napply our new guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 22:21:46 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Thiery", "Theophile", ""], ["Ward", "Justin", ""]]}, {"id": "2102.09679", "submitter": "Theophile Thiery", "authors": "Chien-Chung Huang, Theophile Thiery, Justin Ward", "title": "Improved Multi-Pass Streaming Algorithms for Submodular Maximization\n  with Matroid Constraints", "comments": "Accepted at APPROX 2020, 25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give improved multi-pass streaming algorithms for the problem of\nmaximizing a monotone or arbitrary non-negative submodular function subject to\na general $p$-matchoid constraint in the model in which elements of the ground\nset arrive one at a time in a stream. The family of constraints we consider\ngeneralizes both the intersection of $p$ arbitrary matroid constraints and\n$p$-uniform hypergraph matching. For monotone submodular functions, our\nalgorithm attains a guarantee of $p+1+\\varepsilon$ using\n$O(p/\\varepsilon)$-passes and requires storing only $O(k)$ elements, where $k$\nis the maximum size of feasible solution. This immediately gives an\n$O(1/\\varepsilon)$-pass $(2+\\varepsilon)$-approximation algorithms for monotone\nsubmodular maximization in a matroid and $(3+\\varepsilon)$-approximation for\nmonotone submodular matching. Our algorithm is oblivious to the choice\n$\\varepsilon$ and can be stopped after any number of passes, delivering the\nappropriate guarantee. We extend our techniques to obtain the first multi-pass\nstreaming algorithm for general, non-negative submodular functions subject to a\n$p$-matchoid constraint with a number of passes independent of the size of the\nground set and $k$. We show that a randomized $O(p/\\varepsilon)$-pass algorithm\nstoring $O(p^3k\\log(k)/\\varepsilon^3)$ elements gives a\n$(p+1+\\bar{\\gamma}+O(\\varepsilon))$-approximation, where $\\bar{gamma}$ is the\nguarantee of the best-known offline algorithm for the same problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:54:23 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Thiery", "Theophile", ""], ["Ward", "Justin", ""]]}, {"id": "2102.09791", "submitter": "Marcin Pilipczuk", "authors": "Shaohua Li and Marcin Pilipczuk", "title": "Hardness of Metric Dimension in Graphs of Constant Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metric Dimension problem asks for a minimum-sized resolving set in a\ngiven (unweighted, undirected) graph $G$. Here, a set $S \\subseteq V(G)$ is\nresolving if no two distinct vertices of $G$ have the same distance vector to\n$S$. The complexity of Metric Dimension in graphs of bounded treewidth remained\nelusive in the past years. Recently, Bonnet and Purohit [IPEC 2019] showed that\nthe problem is W[1]-hard under treewidth parameterization. In this work, we\nstrengthen their lower bound to show that Metric Dimension is NP-hard in graphs\nof treewidth 24.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:15:43 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Li", "Shaohua", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "2102.09798", "submitter": "Tillmann Miltzow", "authors": "Mikkel Abrahamsen, Linda Kleist, Tillmann Miltzow", "title": "Training Neural Networks is ER-complete", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DS cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a neural network, training data, and a threshold, it was known that it\nis NP-hard to find weights for the neural network such that the total error is\nbelow the threshold. We determine the algorithmic complexity of this\nfundamental problem precisely, by showing that it is ER-complete. This means\nthat the problem is equivalent, up to polynomial-time reductions, to deciding\nwhether a system of polynomial equations and inequalities with integer\ncoefficients and real unknowns has a solution. If, as widely expected, ER is\nstrictly larger than NP, our work implies that the problem of training neural\nnetworks is not even in NP.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:28:37 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Abrahamsen", "Mikkel", ""], ["Kleist", "Linda", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "2102.09820", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang and Mohsen Ghaffari", "title": "Strong-Diameter Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network decomposition is a central concept in the study of distributed graph\nalgorithms. We present the first polylogarithmic-round deterministic\ndistributed algorithm with small messages that constructs a strong-diameter\nnetwork decomposition with polylogarithmic parameters.\n  Concretely, a ($C$, $D$) strong-diameter network decomposition is a\npartitioning of the nodes of the graph into disjoint clusters, colored with $C$\ncolors, such that neighboring clusters have different colors and the subgraph\ninduced by each cluster has a diameter at most $D$. In the weak-diameter\nvariant, the requirement is relaxed by measuring the diameter of each cluster\nin the original graph, instead of the subgraph induced by the cluster.\n  A recent breakthrough of Rozho\\v{n} and Ghaffari [STOC 2020] presented the\nfirst $\\text{poly}(\\log n)$-round deterministic algorithm for constructing a\nweak-diameter network decomposition where $C$ and $D$ are both in\n$\\text{poly}(\\log n)$. Their algorithm uses small $O(\\log n)$-bit messages. One\ncan transform their algorithm to a strong-diameter network decomposition\nalgorithm with similar parameters. However, that comes at the expense of\nrequiring unbounded messages. The key remaining qualitative question in the\nstudy of network decompositions was whether one can achieve a similar result\nfor strong-diameter network decompositions using small messages. We resolve\nthis question by presenting a novel technique that can transform any black-box\nweak-diameter network decomposition algorithm to a strong-diameter one, using\nsmall messages and with only moderate loss in the parameters.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 09:16:28 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 21:10:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Ghaffari", "Mohsen", ""]]}, {"id": "2102.09889", "submitter": "Sanjukta Roy", "authors": "Sushmita Gupta, Pallavi Jain, Fahad Panolan, Sanjukta Roy, Saket\n  Saurabh", "title": "Gerrymandering on graphs: Computational complexity and parameterized\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partitioning a region into districts to favor a particular candidate or a\nparty is commonly known as gerrymandering. In this paper, we investigate the\ngerrymandering problem in graph theoretic setting as proposed by Cohen-Zemach\net al. [AAMAS 2018]. Our contributions in this article are two-fold, conceptual\nand computational. We first resolve the open question posed by Ito et al.\n[AAMAS 2019] about the computational complexity of the problem when the input\ngraph is a path. Next, we propose a generalization of their model, where the\ninput consists of a graph on $n$ vertices representing the set of voters, a set\nof $m$ candidates $\\mathcal{C}$, a weight function $w_v: \\mathcal{C}\\rightarrow\n{\\mathbb Z}^+$ for each voter $v\\in V(G)$ representing the preference of the\nvoter over the candidates, a distinguished candidate $p\\in \\mathcal{C}$, and a\npositive integer $k$. The objective is to decide if one can partition the\nvertex set into $k$ pairwise disjoint connected sets (districts) s.t $p$ wins\nmore districts than any other candidate. The problem is known to be NPC even if\n$k=2$, $m=2$, and $G$ is either a complete bipartite graph (in fact $K_{2,n}$)\nor a complete graph. This means that in search for FPT algorithms we need to\neither focus on the parameter $n$, or subclasses of forest. Circumventing these\nintractable results, we give a deterministic and a randomized algorithms for\nthe problem on paths running in times $2.619^{k}(n+m)^{O(1)}$ and\n$2^{k}(n+m)^{O(1)}$, respectively. Additionally, we prove that the problem on\ngeneral graphs is solvable in time $2^n (n+m)^{O(1)}$. Our algorithmic results\nuse sophisticated technical tools such as representative set family and Fast\nFourier transform based polynomial multiplication, and their (possibly first)\napplication to problems arising in social choice theory and/or game theory may\nbe of independent interest to the community.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 12:14:09 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 15:08:21 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gupta", "Sushmita", ""], ["Jain", "Pallavi", ""], ["Panolan", "Fahad", ""], ["Roy", "Sanjukta", ""], ["Saurabh", "Saket", ""]]}, {"id": "2102.10077", "submitter": "Nicole Wein", "authors": "Adir Morgan, Shay Solomon, Nicole Wein", "title": "Algorithms for the Minimum Dominating Set Problem in Bounded Arboricity\n  Graphs: Simpler, Faster, and Combinatorial", "comments": "abstract shortened to meet arxiv requirement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the minimum dominating set problem on graphs with arboricity\n$\\alpha$. In the (standard) centralized setting, Bansal and Umboh [BU17] gave\nan $O(\\alpha)$-approximation LP rounding algorithm, which translates into a\nnear-linear time algorithm using general-purpose approximation results for\nexplicit covering LPs [KY14, You14, AZO19, Qua20]. Moreover, [BU17] showed that\nit is NP-hard to achieve an asymptotic improvement for the approximation\nfactor. On the other hand, the previous two non-LP-based algorithms, by Lenzen\nand Wattenhofer [LW10] and Jones et al. [JLR+13], achieve an approximation\nfactor of $O(\\alpha^2)$ in linear time.\n  There is a similar situation in the distributed setting: While there is an\n$O(\\log^2 n)$-round LP-based $O(\\alpha)$-approximation algorithms [KMW06,\nDKM19], the best non-LP-based algorithm by Lenzen and Wattenhofer [LW10] is an\nimplementation of their centralized algorithm, providing an\n$O(\\alpha^2)$-approximation within $O(\\log n)$ rounds with high probability.\n  We address the questions of whether one can achieve an\n$O(\\alpha)$-approximation algorithm that is elementary, i.e., not based on any\nLP-based methods, either in the centralized setting or in the distributed\nsetting. We resolve both questions in the affirmative, and en route achieve\nalgorithms that are faster than the state-of-the-art LP-based algorithms:\n  1. In the centralized setting, we provide a surprisingly simple combinatorial\nalgorithm that is asymptotically optimal in terms of both approximation factor\nand runtime: an $O(\\alpha)$-approximation in linear time. The previous best\n$O(\\alpha)$-approximation algorithms are LP-based and have super-linear running\ntime.\n  2. Based on our centralized algorithm, we design a distributed combinatorial\n$O(\\alpha)$-approximation algorithm in the $\\mathsf{CONGEST}$ model that runs\nin $O(\\alpha\\log n)$ rounds with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:16:57 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:44:02 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Morgan", "Adir", ""], ["Solomon", "Shay", ""], ["Wein", "Nicole", ""]]}, {"id": "2102.10169", "submitter": "Yu Zhu", "authors": "Yu Zhu, Boning Li, Santiago Segarra", "title": "Co-clustering Vertices and Hyperedges via Spectral Hypergraph\n  Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to co-cluster the vertices and hyperedges of\nhypergraphs with edge-dependent vertex weights (EDVWs). In this hypergraph\nmodel, the contribution of every vertex to each of its incident hyperedges is\nrepresented through an edge-dependent weight, conferring the model higher\nexpressivity than the classical hypergraph. In our method, we leverage random\nwalks with EDVWs to construct a hypergraph Laplacian and use its spectral\nproperties to embed vertices and hyperedges in a common space. We then cluster\nthese embeddings to obtain our proposed co-clustering method, of particular\nrelevance in applications requiring the simultaneous clustering of data\nentities and features. Numerical experiments using real-world data demonstrate\nthe effectiveness of our proposed approach in comparison with state-of-the-art\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:47:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhu", "Yu", ""], ["Li", "Boning", ""], ["Segarra", "Santiago", ""]]}, {"id": "2102.10174", "submitter": "Greg Bodwin", "authors": "Greg Bodwin, Merav Parter", "title": "Restorable Shortest Path Tiebreaking for Edge-Faulty Graphs", "comments": "PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restoration lemma by Afek, Bremler-Barr, Kaplan, Cohen, and Merritt\n[Dist.\\ Comp.\\ '02] proves that, in an undirected unweighted graph, any\nreplacement shortest path avoiding a failing edge can be expressed as the\nconcatenation of two original shortest paths. However, the lemma is\ntiebreaking-sensitive: if one selects a particular canonical shortest path for\neach node pair, it is no longer guaranteed that one can build replacement paths\nby concatenating two selected shortest paths. They left as an open problem\nwhether a method of shortest path tiebreaking with this desirable property is\ngenerally possible.\n  We settle this question affirmatively with the first general construction of\nrestorable tiebreaking schemes. We then show applications to various problems\nin fault-tolerant network design. These include a faster algorithm for subset\nreplacement paths, more efficient fault-tolerant (exact) distance labeling\nschemes, fault-tolerant subset distance preservers and $+4$ additive spanners\nwith improved sparsity, and fast distributed algorithms that construct these\nobjects. For example, an almost immediate corollary of our restorable\ntiebreaking scheme is the first nontrivial distributed construction of sparse\nfault-tolerant distance preservers resilient to three faults.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:57:19 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 17:23:26 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bodwin", "Greg", ""], ["Parter", "Merav", ""]]}, {"id": "2102.10196", "submitter": "Romain Cosson", "authors": "Romain Cosson, Devavrat Shah", "title": "Approximating the Log-Partition Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approximation, such as mean-field (MF) and tree-reweighted (TRW),\nprovide a computationally efficient approximation of the log-partition function\nfor a generic graphical model. TRW provably provides an upper bound, but the\napproximation ratio is generally not quantified.\n  As the primary contribution of this work, we provide an approach to quantify\nthe approximation ratio through the property of the underlying graph structure.\nSpecifically, we argue that (a variant of) TRW produces an estimate that is\nwithin factor $\\frac{1}{\\sqrt{\\kappa(G)}}$ of the true log-partition function\nfor any discrete pairwise graphical model over graph $G$, where $\\kappa(G) \\in\n(0,1]$ captures how far $G$ is from tree structure with $\\kappa(G) = 1$ for\ntrees and $2/N$ for the complete graph over $N$ vertices. As a consequence, the\napproximation ratio is $1$ for trees, $\\sqrt{(d+1)/2}$ for any graph with\nmaximum average degree $d$, and $\\stackrel{\\beta\\to\\infty}{\\approx}\n1+1/(2\\beta)$ for graphs with girth (shortest cycle) at least $\\beta \\log N$.\nIn general, $\\kappa(G)$ is the solution of a max-min problem associated with\n$G$ that can be evaluated in polynomial time for any graph.\n  Using samples from the uniform distribution over the spanning trees of G, we\nprovide a near linear-time variant that achieves an approximation ratio equal\nto the inverse of square-root of minimal (across edges) effective resistance of\nthe graph. We connect our results to the graph partition-based approximation\nmethod and thus provide a unified perspective.\n  Keywords: variational inference, log-partition function, spanning tree\npolytope, minimum effective resistance, min-max spanning tree, local inference\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:57:32 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cosson", "Romain", ""], ["Shah", "Devavrat", ""]]}, {"id": "2102.10245", "submitter": "Ahmed E. Helal", "authors": "Ahmed E. Helal, Jan Laukemann, Fabio Checconi, Jesmin Jahan Tithi,\n  Teresa Ranadive, Fabrizio Petrini, Jeewhan Choi", "title": "ALTO: Adaptive Linearized Storage of Sparse Tensors", "comments": "Accepted to ICS 2021", "journal-ref": null, "doi": "10.1145/3447818.3461703", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of high-dimensional sparse data is becoming increasingly popular\nin many important domains. However, real-world sparse tensors are challenging\nto process due to their irregular shapes and data distributions. We propose the\nAdaptive Linearized Tensor Order (ALTO) format, a novel mode-agnostic (general)\nrepresentation that keeps neighboring nonzero elements in the multi-dimensional\nspace close to each other in memory. To generate the indexing metadata, ALTO\nuses an adaptive bit encoding scheme that trades off index computations for\nlower memory usage and more effective use of memory bandwidth. Moreover, by\ndecoupling its sparse representation from the irregular spatial distribution of\nnonzero elements, ALTO eliminates the workload imbalance and greatly reduces\nthe synchronization overhead of tensor computations. As a result, the parallel\nperformance of ALTO-based tensor operations becomes a function of their\ninherent data reuse. On a gamut of tensor datasets, ALTO outperforms an oracle\nthat selects the best state-of-the-art format for each dataset, when used in\nkey tensor decomposition operations. Specifically, ALTO achieves a geometric\nmean speedup of 8X over the best mode-agnostic (coordinate and hierarchical\ncoordinate) formats, while delivering a geometric mean compression ratio of\n4.3X relative to the best mode-specific (compressed sparse fiber) formats.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:32:08 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 07:36:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Helal", "Ahmed E.", ""], ["Laukemann", "Jan", ""], ["Checconi", "Fabio", ""], ["Tithi", "Jesmin Jahan", ""], ["Ranadive", "Teresa", ""], ["Petrini", "Fabrizio", ""], ["Choi", "Jeewhan", ""]]}, {"id": "2102.10261", "submitter": "David Wajc", "authors": "Christos Papadimitriou, Tristan Pollner, Amin Saberi, David Wajc", "title": "Online Stochastic Max-Weight Bipartite Matching: Beyond Prophet\n  Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rich literature on online Bayesian selection problems has long focused on\nso-called prophet inequalities, which compare the gain of an online algorithm\nto that of a \"prophet\" who knows the future. An equally-natural, though\nsignificantly less well-studied benchmark is the optimum online algorithm,\nwhich may be omnipotent (i.e., computationally-unbounded), but not omniscient.\nWhat is the computational complexity of the optimum online? How well can a\npolynomial-time algorithm approximate it?\n  Motivated by applications in ride hailing, we study the above questions for\nthe online stochastic maximum-weight matching problem under vertex arrivals.\nThis problem was recently introduced by Ezra, Feldman, Gravin and Tang (EC'20),\nwho gave a $1/2$-competitive algorithm for it. This is the best possible ratio,\nas this problem is a generalization of the original single-item prophet\ninequality.\n  We present a polynomial-time algorithm which approximates optimal online\nwithin a factor of $0.51$, beating the best-possible prophet inequality. At the\ncore of our result are a new linear program formulation, an algorithm that\ntries to match the arriving vertices in two attempts, and an analysis that\nbounds the correlation resulting from the second attempts. In contrast, we show\nthat it is PSPACE-hard to approximate this problem within some constant $\\alpha\n< 1$.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 05:27:28 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 18:22:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Papadimitriou", "Christos", ""], ["Pollner", "Tristan", ""], ["Saberi", "Amin", ""], ["Wajc", "David", ""]]}, {"id": "2102.10474", "submitter": "Christian Coester", "authors": "Christian Coester, Elias Koutsoupias", "title": "Towards the k-server conjecture: A unifying potential, pushing the\n  frontier to the circle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-server conjecture, first posed by Manasse, McGeoch and Sleator in\n1988, states that a $k$-competitive deterministic algorithm for the $k$-server\nproblem exists. It is conjectured that the work function algorithm (WFA)\nachieves this guarantee, a multi-purpose algorithm with applications to various\nonline problems. This has been shown for several special cases: $k=2$,\n$(k+1)$-point metrics, $(k+2)$-point metrics, the line metric, weighted star\nmetrics, and $k=3$ in the Manhattan plane.\n  The known proofs of these results are based on potential functions tied to\neach particular special case, thus requiring six different potential functions\nfor the six cases. We present a single potential function proving\n$k$-competitiveness of WFA for all these cases. We also use this potential to\nshow $k$-competitiveness of WFA on multiray spaces and for $k=3$ on trees.\nWhile the DoubleCoverage algorithm was known to be $k$-competitive for these\nlatter cases, it has been open for WFA. Our potential captures a type of lazy\nadversary and thus shows that in all settled cases, the worst-case adversary is\nlazy. Chrobak and Larmore conjectured in 1992 that a potential capturing the\nlazy adversary would resolve the $k$-server conjecture.\n  To our major surprise, this is not the case, as we show (using connections to\nthe $k$-taxi problem) that our potential fails for three servers on the circle.\nThus, our potential highlights laziness of the adversary as a fundamental\nproperty that is shared by all settled cases but violated in general. On the\none hand, this weakens our confidence in the validity of the $k$-server\nconjecture. On the other hand, if the $k$-server conjecture holds, then we\nbelieve it can be proved by a variant of our potential.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 23:29:31 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Coester", "Christian", ""], ["Koutsoupias", "Elias", ""]]}, {"id": "2102.10689", "submitter": "Ana Ozaki", "authors": "Ricardo Guimar\\~aes, Ana Ozaki, Cosimo Persia, Baris Sertkaya", "title": "Mining EL Bases with Adaptable Role Depth", "comments": "AAAI 2021 (Main Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Formal Concept Analysis, a base for a finite structure is a set of\nimplications that characterizes all valid implications of the structure. This\nnotion can be adapted to the context of Description Logic, where the base\nconsists of a set of concept inclusions instead of implications. In this\nsetting, concept expressions can be arbitrarily large. Thus, it is not clear\nwhether a finite base exists and, if so, how large concept expressions may need\nto be. We first revisit results in the literature for mining EL bases from\nfinite interpretations. Those mainly focus on finding a finite base or on\nfixing the role depth but potentially losing some of the valid concept\ninclusions with higher role depth. We then present a new strategy for mining EL\nbases which is adaptable in the sense that it can bound the role depth of\nconcepts depending on the local structure of the interpretation. Our strategy\nguarantees to capture all EL concept inclusions holding in the interpretation,\nnot only the ones up to a fixed role depth.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 21:33:49 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 10:13:35 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Guimar\u00e3es", "Ricardo", ""], ["Ozaki", "Ana", ""], ["Persia", "Cosimo", ""], ["Sertkaya", "Baris", ""]]}, {"id": "2102.10814", "submitter": "Philipp Zschoche", "authors": "Hendrik Molter, Malte Renken and Philipp Zschoche", "title": "Temporal Reachability Minimization: Delaying vs. Deleting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study spreading processes in temporal graphs, i. e., graphs whose\nconnections change over time. These processes naturally model real-world\nphenomena such as infectious diseases or information flows. More precisely, we\ninvestigate how such a spreading process, emerging from a given set of sources,\ncan be contained to a small part of the graph. To this end we consider two ways\nof modifying the graph, which are (1) deleting connections and (2) delaying\nconnections. We show a close relationship between the two associated problems\nand give a polynomial time algorithm when the graph has tree structure. For the\ngeneral version, we consider parameterization by the number of vertices to\nwhich the spread is contained. Surprisingly, we prove W[1]-hardness for the\ndeletion variant but fixed-parameter tractability for the delaying variant.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 07:50:36 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 08:06:53 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Molter", "Hendrik", ""], ["Renken", "Malte", ""], ["Zschoche", "Philipp", ""]]}, {"id": "2102.10892", "submitter": "Lorenzo Balzotti", "authors": "Lorenzo Balzotti and Paolo G. Franciosa", "title": "Multi-Terminal Shortest Paths in Unit-Weight Planar Graphs in Linear\n  Time", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of terminal pairs on the external face of a planar graph with\nunit edge weights, we give a linear-time algorithm to compute a set of\nnon-crossing shortest paths joining each terminal pair, if it exists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 10:57:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Balzotti", "Lorenzo", ""], ["Franciosa", "Paolo G.", ""]]}, {"id": "2102.10939", "submitter": "Liang Chen", "authors": "Liang Chen", "title": "A High-dimensional Sparse Fourier Transform in the Continuous Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NA math.IT math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we theoretically propose a new hashing scheme to establish the\nsparse Fourier transform in high-dimensional space. The estimation of the\nalgorithm complexity shows that this sparse Fourier transform can overcome the\ncurse of dimensionality. To the best of our knowledge, this is the first\npolynomial-time algorithm to recover the high-dimensional continuous\nfrequencies.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:14:26 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 10:23:00 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 03:09:41 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 12:30:00 GMT"}, {"version": "v5", "created": "Sun, 7 Mar 2021 11:56:42 GMT"}, {"version": "v6", "created": "Wed, 9 Jun 2021 07:25:44 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chen", "Liang", ""]]}, {"id": "2102.11050", "submitter": "Fransisca Susan", "authors": "Rad Niazadeh (1), Negin Golrezaei (2), Joshua Wang (3), Fransisca\n  Susan (2), Ashwinkumar Badanidiyuru (3) ((1) Chicago Booth School of\n  Business, Operations Management, (2) MIT Sloan School of Management,\n  Operations Management, (3) Google Research Mountain View)", "title": "Online Learning via Offline Greedy Algorithms: Applications in Market\n  Design and Optimization", "comments": "70 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online decision-making in time-varying combinatorial\nenvironments, we study the problem of transforming offline algorithms to their\nonline counterparts. We focus on offline combinatorial problems that are\namenable to a constant factor approximation using a greedy algorithm that is\nrobust to local errors. For such problems, we provide a general framework that\nefficiently transforms offline robust greedy algorithms to online ones using\nBlackwell approachability. We show that the resulting online algorithms have\n$O(\\sqrt{T})$ (approximate) regret under the full information setting. We\nfurther introduce a bandit extension of Blackwell approachability that we call\nBandit Blackwell approachability. We leverage this notion to transform greedy\nrobust offline algorithms into a $O(T^{2/3})$ (approximate) regret in the\nbandit setting. Demonstrating the flexibility of our framework, we apply our\noffline-to-online transformation to several problems at the intersection of\nrevenue management, market design, and online optimization, including product\nranking optimization in online platforms, reserve price optimization in\nauctions, and submodular maximization. We show that our transformation, when\napplied to these applications, leads to new regret bounds or improves the\ncurrent known bounds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 19:05:26 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Niazadeh", "Rad", ""], ["Golrezaei", "Negin", ""], ["Wang", "Joshua", ""], ["Susan", "Fransisca", ""], ["Badanidiyuru", "Ashwinkumar", ""]]}, {"id": "2102.11119", "submitter": "Nikhil Ayyadevara", "authors": "Nikhil Ayyadevara, Ashish Chiplunkar", "title": "The Randomized Competitive Ratio of Weighted $k$-server is at least\n  Exponential", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The weighted $k$-server problem is a natural generalization of the $k$-server\nproblem in which the cost incurred in moving a server is the distance traveled\ntimes the weight of the server. Even after almost three decades since the\nseminal work of Fiat and Ricklin (1994), the competitive ratio of this problem\nremains poorly understood, even on the simplest class of metric spaces -- the\nuniform metric spaces. In particular, in the case of randomized algorithms\nagainst the oblivious adversary, neither a better upper bound that the doubly\nexponential deterministic upper bound, nor a better lower bound than the\nlogarithmic lower bound of unweighted $k$-server, is known. In this article, we\nmake significant progress towards understanding the randomized competitive\nratio of weighted $k$-server on uniform metrics. We cut down the triply\nexponential gap between the upper and lower bound to a singly exponential gap\nby proving that the competitive ratio is at least exponential in $k$,\nsubstantially improving on the previously known lower bound of about $\\ln k$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:50:09 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 11:37:52 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 17:13:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ayyadevara", "Nikhil", ""], ["Chiplunkar", "Ashish", ""]]}, {"id": "2102.11169", "submitter": "Christian Schulz", "authors": "Kathrin Hanauer, Monika Henzinger, Christian Schulz", "title": "Recent Advances in Fully Dynamic Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, significant advances have been made in the design and\nanalysis of fully dynamic algorithms. However, these theoretical results have\nreceived very little attention from the practical perspective. Few of the\nalgorithms are implemented and tested on real datasets, and their practical\npotential is far from understood. Here, we present a quick reference guide to\nrecent engineering and theory results in the area of fully dynamic graph\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:47:37 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 17:31:48 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 16:43:20 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 10:16:24 GMT"}, {"version": "v5", "created": "Thu, 1 Jul 2021 14:38:12 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hanauer", "Kathrin", ""], ["Henzinger", "Monika", ""], ["Schulz", "Christian", ""]]}, {"id": "2102.11251", "submitter": "Lijie Chen", "authors": "Lijie Chen, Gillat Kol, Dmitry Paramonov, Raghuvansh Saxena, Zhao\n  Song, Huacheng Yu", "title": "Near-Optimal Two-Pass Streaming Algorithm for Sampling Random Walks over\n  Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a directed graph $G$ with $n$ vertices and a start vertex $u_{\\sf\nstart}$, we wish to (approximately) sample an $L$-step random walk over $G$\nstarting from $u_{\\sf start}$ with minimum space using an algorithm that only\nmakes few passes over the edges of the graph. This problem found many\napplications, for instance, in approximating the PageRank of a webpage. If only\na single pass is allowed, the space complexity of this problem was shown to be\n$\\tilde{\\Theta}(n \\cdot L)$. Prior to our work, a better space complexity was\nonly known with $\\tilde{O}(\\sqrt{L})$ passes.\n  We settle the space complexity of this random walk simulation problem for\ntwo-pass streaming algorithms, showing that it is $\\tilde{\\Theta}(n \\cdot\n\\sqrt{L})$, by giving almost matching upper and lower bounds. Our lower bound\nargument extends to every constant number of passes $p$, and shows that any\n$p$-pass algorithm for this problem uses $\\tilde{\\Omega}(n \\cdot L^{1/p})$\nspace. In addition, we show a similar $\\tilde{\\Theta}(n \\cdot \\sqrt{L})$ bound\non the space complexity of any algorithm (with any number of passes) for the\nrelated problem of sampling an $L$-step random walk from every vertex in the\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:33:19 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Lijie", ""], ["Kol", "Gillat", ""], ["Paramonov", "Dmitry", ""], ["Saxena", "Raghuvansh", ""], ["Song", "Zhao", ""], ["Yu", "Huacheng", ""]]}, {"id": "2102.11349", "submitter": "Shih-Han Hung", "authors": "Andrew M. Childs, Shih-Han Hung, Tongyang Li", "title": "Quantum query complexity with matrix-vector products", "comments": "19 pages. Added discussion of related prior work", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study quantum algorithms that learn properties of a matrix using queries\nthat return its action on an input vector. We show that for various problems,\nincluding computing the trace, determinant, or rank of a matrix or solving a\nlinear system that it specifies, quantum computers do not provide an asymptotic\nspeedup over classical computation. On the other hand, we show that for some\nproblems, such as computing the parities of rows or columns or deciding if\nthere are two identical rows or columns, quantum computers provide exponential\nspeedup. We demonstrate this by showing equivalence between models that provide\nmatrix-vector products, vector-matrix products, and vector-matrix-vector\nproducts, whereas the power of these models can vary significantly for\nclassical computation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:42:17 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 18:13:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Childs", "Andrew M.", ""], ["Hung", "Shih-Han", ""], ["Li", "Tongyang", ""]]}, {"id": "2102.11360", "submitter": "Greg Bodwin", "authors": "Greg Bodwin, Michael Dinitz, Caleb Robelle", "title": "Partially Optimal Edge Fault-Tolerant Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has established that, for every positive integer $k$, every\n$n$-node graph has a $(2k-1)$-spanner on $O(f^{1-1/k} n^{1+1/k})$ edges that is\nresilient to $f$ edge or vertex faults. For vertex faults, this bound is tight.\nHowever, the case of edge faults is not as well understood: the best known\nlower bound for general $k$ is $\\Omega(f^{\\frac12 - \\frac{1}{2k}} n^{1+1/k}\n+fn)$. Our main result is to nearly close this gap with an improved upper\nbound, thus separating the cases of edge and vertex faults. For odd $k$, our\nnew upper bound is $O_k(f^{\\frac12 - \\frac{1}{2k}} n^{1+1/k} + fn)$, which is\ntight up to hidden $poly(k)$ factors. For even $k$, our new upper bound is\n$O_k(f^{1/2} n^{1+1/k} +fn)$, which leaves a gap of $poly(k) f^{1/(2k)}$. Our\nproof is an analysis of the fault-tolerant greedy algorithm, which requires\nexponential time, but we also show that there is a polynomial-time algorithm\nwhich creates edge fault tolerant spanners that are larger only by factors of\n$k$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 21:04:57 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Bodwin", "Greg", ""], ["Dinitz", "Michael", ""], ["Robelle", "Caleb", ""]]}, {"id": "2102.11435", "submitter": "Maryam Negahbani", "authors": "Deeparnab Chakrabarty and Maryam Negahbani", "title": "Robust $k$-Center with Two Types of Radii", "comments": "To appear in IPCO '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the non-uniform $k$-center problem, the objective is to cover points in a\nmetric space with specified number of balls of different radii. Chakrabarty,\nGoyal, and Krishnaswamy [ICALP 2016, Trans. on Algs. 2020] (CGK, henceforth)\ngive a constant factor approximation when there are two types of radii. In this\npaper, we give a constant factor approximation for the two radii case in the\npresence of outliers. To achieve this, we need to bypass the technical barrier\nof bad integrality gaps in the CGK approach. We do so using \"the ellipsoid\nmethod inside the ellipsoid method\": use an outer layer of the ellipsoid method\nto reduce to stylized instances and use an inner layer of the ellipsoid method\nto solve these specialized instances. This idea is of independent interest and\ncould be applicable to other problems.\n  Keywords: Approximation, Clustering, Outliers, and Round-or-Cut.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 00:45:44 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Negahbani", "Maryam", ""]]}, {"id": "2102.11489", "submitter": "Elaine Shi", "authors": "Wei-Kai Lin, Elaine Shi", "title": "Optimal Sorting Circuits for Short Keys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing open question in the algorithms and complexity literature is\nwhether there exist sorting circuits of size $o(n \\log n)$. A recent work by\nAsharov, Lin, and Shi (SODA'21) showed that if the elements to be sorted have\nshort keys whose length $k = o(\\log n)$, then one can indeed overcome the\n$n\\log n$ barrier for sorting circuits, by leveraging non-comparison-based\ntechniques. More specifically, Asharov et al.~showed that there exist $O(n)\n\\cdot \\min(k, \\log n)$-sized sorting circuits for $k$-bit keys, ignoring\n$poly\\log^*$ factors. Interestingly, the recent works by Farhadi et al.\n(STOC'19) and Asharov et al. (SODA'21) also showed that the above result is\nessentially optimal for every key length $k$, assuming that the famous Li-Li\nnetwork coding conjecture holds. Note also that proving any {\\it unconditional}\nsuper-linear circuit lower bound for a wide class of problems is beyond the\nreach of current techniques.\n  Unfortunately, the approach taken by Asharov et al.~to achieve optimality in\nsize somewhat crucially relies on sacrificing the depth: specifically, their\ncircuit is super-{\\it poly}logarithmic in depth even for 1-bit keys. Asharov et\nal.~phrase it as an open question how to achieve optimality both in size and\ndepth. In this paper, we close this important gap in our understanding. We\nconstruct a sorting circuit of size $O(n) \\cdot \\min(k, \\log n)$ (ignoring\n$poly\\log^*$ terms) and depth $O(\\log n)$. To achieve this, our approach\ndeparts significantly from the prior works. Our result can be viewed as a\ngeneralization of the landmark result by Ajtai, Koml\\'os, and Szemer\\'edi\n(STOC'83), simultaneously in terms of size and depth. Specifically, for $k =\no(\\log n)$, we achieve asymptotical improvements in size over the AKS sorting\ncircuit, while preserving optimality in depth.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 04:50:48 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Lin", "Wei-Kai", ""], ["Shi", "Elaine", ""]]}, {"id": "2102.11548", "submitter": "Vaggos Chatziafratis", "authors": "Vaggos Chatziafratis, Mohammad Mahdian, Sara Ahmadian", "title": "Maximizing Agreements for Ranking, Clustering and Hierarchical\n  Clustering via MAX-CUT", "comments": "AISTATS 2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a number of well-known combinatorial optimization\nproblems that fit in the following paradigm: the input is a collection of\n(potentially inconsistent) local relationships between the elements of a ground\nset (e.g., pairwise comparisons, similar/dissimilar pairs, or ancestry\nstructure of triples of points), and the goal is to aggregate this information\ninto a global structure (e.g., a ranking, a clustering, or a hierarchical\nclustering) in a way that maximizes agreement with the input. Well-studied\nproblems such as rank aggregation, correlation clustering, and hierarchical\nclustering with triplet constraints fall in this class of problems.\n  We study these problems on stochastic instances with a hidden embedded ground\ntruth solution. Our main algorithmic contribution is a unified technique that\nuses the maximum cut problem in graphs to approximately solve these problems.\nUsing this technique, we can often get approximation guarantees in the\nstochastic setting that are better than the known worst case inapproximability\nbounds for the corresponding problem. On the negative side, we improve the\nworst case inapproximability bound on several hierarchical clustering\nformulations through a reduction to related ranking problems.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 08:39:28 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chatziafratis", "Vaggos", ""], ["Mahdian", "Mohammad", ""], ["Ahmadian", "Sara", ""]]}, {"id": "2102.11660", "submitter": "Davin Choo", "authors": "M\\'elanie Cambus, Davin Choo, Havu Miikonen, Jara Uitto", "title": "Massively Parallel Correlation Clustering in Bounded Arboricity Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying clusters of similar elements in a set is a common task in data\nanalysis. With the immense growth of data and physical limitations on single\nprocessor speed, it is necessary to find efficient parallel algorithms for\nclustering tasks. In this paper, we study the problem of correlation clustering\nin bounded arboricity graphs with respect to the Massively Parallel Computation\n(MPC) model. More specifically, we are given a complete graph where the edges\nare either positive or negative, indicating whether pairs of vertices are\nsimilar or dissimilar. The task is to partition the vertices into clusters with\nas few disagreements as possible. That is, we want to minimize the number of\npositive inter-cluster edges and negative intra-cluster edges.\n  Consider an input graph $G$ on $n$ vertices such that the positive edges\ninduce a $\\lambda$-arboric graph. Our main result is a 3-approximation\n($\\textit{in expectation}$) algorithm that runs in $\\mathcal{O}(\\log \\lambda\n\\cdot \\textrm{poly}(\\log \\log n))$ MPC rounds in the $\\textit{strongly\nsublinear memory regime}$. This is obtained by combining structural properties\nof correlation clustering on bounded arboricity graphs with the insights of\nFischer and Noever (SODA '18) on randomized greedy MIS and the $\\texttt{PIVOT}$\nalgorithm of Ailon, Charikar, and Newman (STOC '05). Combined with known graph\nmatching algorithms, our structural property also implies an exact algorithm\nand algorithms with $\\textit{worst case}$ $(1+\\epsilon)$-approximation\nguarantees in the special case of forests, where $\\lambda=1$.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 12:26:52 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 14:36:15 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cambus", "M\u00e9lanie", ""], ["Choo", "Davin", ""], ["Miikonen", "Havu", ""], ["Uitto", "Jara", ""]]}, {"id": "2102.11728", "submitter": "Reut Levi", "authors": "Reut Levi and Nadav Shoshan", "title": "Testing Hamiltonicity (and other problems) in Minor-Free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide sub-linear algorithms for several fundamental\nproblems in the setting in which the input graph excludes a fixed minor, i.e.,\nis a minor-free graph. In particular, we provide the following algorithms for\nminor-free unbounded degree graphs. (1) A tester for Hamiltonicity with\ntwo-sided error with $poly(1/\\epsilon)$-query complexity, where $\\epsilon$ is\nthe proximity parameter. (2) A local algorithm, as defined by Rubinfeld et al.\n(ICS 2011), for constructing a spanning subgraph with almost minimum weight,\nspecifically, at most a factor $(1+\\epsilon)$ of the optimum, with\n$poly(1/\\epsilon)$-query complexity. Both our algorithms use partition oracles,\na tool introduced by Hassidim et al. (FOCS 2009), which are oracles that\nprovide access to a partition of the graph such that the number of cut-edges is\nsmall and each part of the partition is small. The polynomial dependence in\n$1/\\epsilon$ of our algorithms is achieved by combining the recent\n$poly(d/\\epsilon)$-query partition oracle of Kumar-Seshadhri-Stolman (ECCC\n2021) for minor-free graphs with degree bounded by $d$.\n  For bounded degree minor-free graphs we introduce the notion of covering\npartition oracles which is a relaxed version of partition oracles and design a\n$poly(d/\\epsilon)$-time covering partition oracle. Using our covering partition\noracle we provide the same results as above (except that the tester for\nHamiltonicity has one-sided error) for minor-free bounded degree graphs, as\nwell as showing that any property which is monotone and additive (e.g.\nbipartiteness) can be tested in minor-free graphs by making\n$poly(d/\\epsilon)$-queries.\n  The benefit of using the covering partition oracle rather than the partition\noracle in our algorithms is its simplicity and an improved polynomial\ndependence in $1/\\epsilon$ in the obtained query complexity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 14:42:17 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:38:33 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Levi", "Reut", ""], ["Shoshan", "Nadav", ""]]}, {"id": "2102.11797", "submitter": "Wojciech Janczewski", "authors": "Pawe{\\l} Gawrychowski and Wojciech Janczewski", "title": "Conditional Lower Bounds for Variants of Dynamic LIS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we consider the complexity of maintaining the longest\nincreasing subsequence (LIS) of an array under (i) inserting an element, and\n(ii) deleting an element of an array. We show that no algorithm can support\nqueries and updates in time $\\mathcal{O}(n^{1/2-\\epsilon})$ and\n$\\mathcal{O}(n^{1/3-\\epsilon})$ for the dynamic LIS problem, for any constant\n$\\epsilon>0$, when the elements are weighted or the algorithm supports\n1D-queries (on subarrays), respectively, assuming the All-Pairs Shortest Paths\n(APSP) conjecture or the Online Boolean Matrix-Vector Multiplication (OMv)\nconjecture. The main idea in our construction comes from the work of Abboud and\nDahlgaard [FOCS 2016], who proved conditional lower bounds for dynamic planar\ngraph algorithm. However, this needs to be appropriately adjusted and\ntranslated to obtain an instance of the dynamic LIS problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:04:25 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Janczewski", "Wojciech", ""]]}, {"id": "2102.11911", "submitter": "Sharon Qian", "authors": "Eric Balkanski, Sharon Qian, Yaron Singer", "title": "Instance Specific Approximations for Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many optimization problems in machine learning, finding an optimal\nsolution is computationally intractable and we seek algorithms that perform\nwell in practice. Since computational intractability often results from\npathological instances, we look for methods to benchmark the performance of\nalgorithms against optimal solutions on real-world instances. The main\nchallenge is that an optimal solution cannot be efficiently computed for\nintractable problems, and we therefore often do not know how far a solution is\nfrom being optimal. A major question is therefore how to measure the\nperformance of an algorithm in comparison to an optimal solution on instances\nwe encounter in practice.\n  In this paper, we address this question in the context of submodular\noptimization problems. For the canonical problem of submodular maximization\nunder a cardinality constraint, it is intractable to compute a solution that is\nbetter than a $1-1/e \\approx 0.63$ fraction of the optimum. Algorithms like the\ncelebrated greedy algorithm are guaranteed to achieve this $1-1/e$ bound on any\ninstance and are used in practice.\n  Our main contribution is not a new algorithm for submodular maximization but\nan analytical method that measures how close an algorithm for submodular\nmaximization is to optimal on a given problem instance. We use this method to\nshow that on a wide variety of real-world datasets and objectives, the\napproximation of the solution found by greedy goes well beyond $1-1/e$ and is\noften at least 0.95. We develop this method using a novel technique that lower\nbounds the objective of a dual minimization problem to obtain an upper bound on\nthe value of an optimal solution to the primal maximization problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:39:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Balkanski", "Eric", ""], ["Qian", "Sharon", ""], ["Singer", "Yaron", ""]]}, {"id": "2102.11992", "submitter": "Josh Alman", "authors": "Josh Alman", "title": "Kronecker Products, Low-Depth Circuits, and Matrix Rigidity", "comments": "40 pages, to appear in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a matrix $M$ and a positive integer $r$, the rank $r$ rigidity of $M$ is\nthe smallest number of entries of $M$ which one must change to make its rank at\nmost $r$. There are many known applications of rigidity lower bounds to a\nvariety of areas in complexity theory, but fewer known applications of rigidity\nupper bounds. In this paper, we use rigidity upper bounds to prove new upper\nbounds in a few different models of computation. Our results include:\n  $\\bullet$ For any $d> 1$, and over any field $\\mathbb{F}$, the $N \\times N$\nWalsh-Hadamard transform has a depth-$d$ linear circuit of size $O(d \\cdot N^{1\n+ 0.96/d})$. This circumvents a known lower bound of $\\Omega(d \\cdot N^{1 +\n1/d})$ for circuits with bounded coefficients over $\\mathbb{C}$ by Pudl\\'ak\n(2000), by using coefficients of magnitude polynomial in $N$. Our construction\nalso generalizes to linear transformations given by a Kronecker power of any\nfixed $2 \\times 2$ matrix.\n  $\\bullet$ The $N \\times N$ Walsh-Hadamard transform has a linear circuit of\nsize $\\leq (1.81 + o(1)) N \\log_2 N$, improving on the bound of $\\approx 1.88 N\n\\log_2 N$ which one obtains from the standard fast Walsh-Hadamard transform.\n  $\\bullet$ A new rigidity upper bound, showing that the following classes of\nmatrices are not rigid enough to prove circuit lower bounds using Valiant's\napproach:\n  $-$ for any field $\\mathbb{F}$ and any function $f : \\{0,1\\}^n \\to\n\\mathbb{F}$, the matrix $V_f \\in \\mathbb{F}^{2^n \\times 2^n}$ given by, for any\n$x,y \\in \\{0,1\\}^n$, $V_f[x,y] = f(x \\wedge y)$, and\n  $-$ for any field $\\mathbb{F}$ and any fixed-size matrices $M_1, \\ldots, M_n\n\\in \\mathbb{F}^{q \\times q}$, the Kronecker product $M_1 \\otimes M_2 \\otimes\n\\cdots \\otimes M_n$.\n  This generalizes recent results on non-rigidity, using a simpler approach\nwhich avoids needing the polynomial method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 00:25:47 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Alman", "Josh", ""]]}, {"id": "2102.12058", "submitter": "Wei Yao", "authors": "Wei Yao, Junyi Ye, Renita Murimi, and Guiling Wang", "title": "A Survey on Consortium Blockchain Consensus Mechanisms", "comments": "under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain is a distributed ledger that is decentralized, immutable, and\ntransparent, which maintains a continuously growing list of transaction records\nordered into blocks. As the core of blockchain, the consensus algorithm is an\nagreement to validate the correctness of blockchain transactions. For example,\nBitcoin is a public blockchain where each node in Bitcoin uses the Proof of\nWork (PoW) algorithm to reach a consensus by competing to solve a puzzle.\nUnlike a public blockchain, a consortium blockchain is an enterprise-level\nblockchain that does not contend with the issues of creating a resource-saving\nglobal consensus protocol. This paper highilights several state-of-the art\nsolutions in consensus algorithms for enterprise blockchain. For example, the\nHyperLedger by Linux Foundation includes implementing Practical Byzantine Fault\nTolerance (PBFT) as the consensus algorithm. PBFT can tolerate a range of\nmalicious nodes and reach consensus with quadratic complexity. Another\nconsensus algorithm, HotStuff, implemented by Facebook Libra project, has\nachieved linear complexity of the authenticator. This paper presents the\noperational mechanisms of these and other consensus protocols, and analyzes and\ncompares their advantages and drawbacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 04:19:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Yao", "Wei", ""], ["Ye", "Junyi", ""], ["Murimi", "Renita", ""], ["Wang", "Guiling", ""]]}, {"id": "2102.12099", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Kunal Talwar", "title": "Lossless Compression of Efficient Private Local Randomizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Differentially Private (LDP) Reports are commonly used for collection\nof statistics and machine learning in the federated setting. In many cases the\nbest known LDP algorithms require sending prohibitively large messages from the\nclient device to the server (such as when constructing histograms over large\ndomain or learning a high-dimensional model). This has led to significant\nefforts on reducing the communication cost of LDP algorithms.\n  At the same time LDP reports are known to have relatively little information\nabout the user's data due to randomization. Several schemes are known that\nexploit this fact to design low-communication versions of LDP algorithm but all\nof them do so at the expense of a significant loss in utility. Here we\ndemonstrate a general approach that, under standard cryptographic assumptions,\ncompresses every efficient LDP algorithm with negligible loss in privacy and\nutility guarantees. The practical implication of our result is that in typical\napplications the message can be compressed to the size of the server's\npseudo-random generator seed. More generally, we relate the properties of an\nLDP randomizer to the power of a pseudo-random generator that suffices for\ncompressing the LDP randomizer. From this general approach we derive\nlow-communication algorithms for the problems of frequency estimation and\nhigh-dimensional mean estimation. Our algorithms are simpler and more accurate\nthan existing low-communication LDP algorithms for these well-studied problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 07:04:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Feldman", "Vitaly", ""], ["Talwar", "Kunal", ""]]}, {"id": "2102.12301", "submitter": "Aditya Desai", "authors": "Aditya Desai, Benjamin Coleman, Anshumali Shrivastava", "title": "Density Sketches for Sampling and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Density sketches (DS): a succinct online summary of the data\ndistribution. DS can accurately estimate point wise probability density.\nInterestingly, DS also provides a capability to sample unseen novel data from\nthe underlying data distribution. Thus, analogous to popular generative models,\nDS allows us to succinctly replace the real-data in almost all machine learning\npipelines with synthetic examples drawn from the same distribution as the\noriginal data. However, unlike generative models, which do not have any\nstatistical guarantees, DS leads to theoretically sound asymptotically\nconverging consistent estimators of the underlying density function. Density\nsketches also have many appealing properties making them ideal for large-scale\ndistributed applications. DS construction is an online algorithm. The sketches\nare additive, i.e., the sum of two sketches is the sketch of the combined data.\nThese properties allow data to be collected from distributed sources,\ncompressed into a density sketch, efficiently transmitted in the sketch form to\na central server, merged, and re-sampled into a synthetic database for modeling\napplications. Thus, density sketches can potentially revolutionize how we\nstore, communicate, and distribute data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:30:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Desai", "Aditya", ""], ["Coleman", "Benjamin", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2102.12317", "submitter": "Yi Li", "authors": "Yi Li, Honghao Lin, David P. Woodruff", "title": "Learning-Augmented Sketches for Hessians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching is a dimensionality reduction technique where one compresses a\nmatrix by linear combinations that are typically chosen at random. A line of\nwork has shown how to sketch the Hessian to speed up each iteration in a second\norder method, but such sketches usually depend only on the matrix at hand, and\nin a number of cases are even oblivious to the input matrix. One could instead\nhope to learn a distribution on sketching matrices that is optimized for the\nspecific distribution of input matrices. We show how to design learned sketches\nfor the Hessian in the context of second order methods, where we learn\npotentially different sketches for the different iterations of an optimization\nprocedure. We show empirically that learned sketches, compared with their\n\"non-learned\" counterparts, improve the approximation accuracy for important\nproblems, including LASSO, SVM, and matrix estimation with nuclear norm\nconstraints. Several of our schemes can be proven to perform no worse than\ntheir unlearned counterparts. Additionally, we show that a smaller sketching\ndimension of the column space of a tall matrix is possible, assuming an oracle\nfor predicting rows which have a large leverage score.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:50:59 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Li", "Yi", ""], ["Lin", "Honghao", ""], ["Woodruff", "David P.", ""]]}, {"id": "2102.12351", "submitter": "Alexander Golovnev", "authors": "Chi-Ning Chou, Alexander Golovnev, Madhu Sudan, and Santhoshini\n  Velusamy", "title": "Approximability of all Boolean CSPs in the dynamic streaming setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Boolean constraint satisfaction problem (CSP), Max-CSP$(f)$, is a\nmaximization problem specified by a constraint $f:\\{-1,1\\}^k\\to\\{0,1\\}$. An\ninstance of the problem consists of $m$ constraint applications on $n$ Boolean\nvariables, where each constraint application applies the constraint to $k$\nliterals chosen from the $n$ variables and their negations. The goal is to\ncompute the maximum number of constraints that can be satisfied by a Boolean\nassignment to the $n$~variables. In the $(\\gamma,\\beta)$-approximation version\nof the problem for parameters $\\gamma \\geq \\beta \\in [0,1]$, the goal is to\ndistinguish instances where at least $\\gamma$ fraction of the constraints can\nbe satisfied from instances where at most $\\beta$ fraction of the constraints\ncan be satisfied.\n  In this work we consider the approximability of Max-CSP$(f)$ in the (dynamic)\nstreaming setting, where constraints are inserted (and may also be deleted in\nthe dynamic setting) one at a time. We completely characterize the\napproximability of all Boolean CSPs in the dynamic streaming setting.\nSpecifically, given $f$, $\\gamma$ and $\\beta$ we show that either (1) the\n$(\\gamma,\\beta)$-approximation version of Max-CSP$(f)$ has a probabilistic\ndynamic streaming algorithm using $O(\\log n)$ space, or (2) for every\n$\\varepsilon > 0$ the $(\\gamma-\\varepsilon,\\beta+\\varepsilon)$-approximation\nversion of Max-CSP$(f)$ requires $\\Omega(\\sqrt{n})$ space for probabilistic\ndynamic streaming algorithms. We also extend previously known results in the\ninsertion-only setting to a wide variety of cases, and in particular the case\nof $k=2$ where we get a dichotomy and the case when the satisfying assignments\nof $f$ support a distribution on $\\{-1,1\\}^k$ with uniform marginals.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:36:22 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 21:16:28 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 16:24:55 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 18:33:26 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chou", "Chi-Ning", ""], ["Golovnev", "Alexander", ""], ["Sudan", "Madhu", ""], ["Velusamy", "Santhoshini", ""]]}, {"id": "2102.12531", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Gil Einziger, Michael Mitzenmacher, Shay Vargaftik", "title": "SALSA: Self-Adjusting Lean Streaming Analytics", "comments": "An extended version of the conference paper that will appear in IEEE\n  ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counters are the fundamental building block of many data sketching schemes,\nwhich hash items to a small number of counters and account for collisions to\nprovide good approximations for frequencies and other measures. Most existing\nmethods rely on fixed-size counters, which may be wasteful in terms of space,\nas counters must be large enough to eliminate any risk of overflow. Instead,\nsome solutions use small, fixed-size counters that may overflow into secondary\nstructures.\n  This paper takes a different approach. We propose a simple and general method\ncalled SALSA for dynamic re-sizing of counters and show its effectiveness.\nSALSA starts with small counters, and overflowing counters simply merge with\ntheir neighbors. SALSA can thereby allow more counters for a given space,\nexpanding them as necessary to represent large numbers. Our evaluation\ndemonstrates that, at the cost of a small overhead for its merging logic, SALSA\nsignificantly improves the accuracy of popular schemes (such as Count-Min\nSketch and Count Sketch) over a variety of tasks. Our code is released as\nopen-source [1].\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:51:24 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Mitzenmacher", "Michael", ""], ["Vargaftik", "Shay", ""]]}, {"id": "2102.12589", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "A New Algorithm for Euclidean Shortest Paths in the Plane", "comments": "To appear in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of pairwise disjoint polygonal obstacles in the plane, finding an\nobstacle-avoiding Euclidean shortest path between two points is a classical\nproblem in computational geometry and has been studied extensively. Previously,\nHershberger and Suri [SIAM J. Comput. 1999] gave an algorithm of $O(n\\log n)$\ntime and $O(n\\log n)$ space, where $n$ is the total number of vertices of all\nobstacles. Recently, by modifying Hershberger and Suri's algorithm, Wang [SODA\n2021] reduced the space to $O(n)$ while the runtime of the algorithm is still\n$O(n\\log n)$. In this paper, we present a new algorithm of $O(n+h\\log h)$ time\nand $O(n)$ space, provided that a triangulation of the free space is given,\nwhere $h$ is the number of obstacles. The algorithm, which improves the\nprevious work when $h=o(n)$, is optimal in both time and space as\n$\\Omega(n+h\\log h)$ is a lower bound on the runtime. Our algorithm builds a\nshortest path map for a source point $s$, so that given any query point $t$,\nthe shortest path length from $s$ to $t$ can be computed in $O(\\log n)$ time\nand a shortest $s$-$t$ path can be produced in additional time linear in the\nnumber of edges of the path.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 22:35:38 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "2102.12604", "submitter": "Katherine Van Koevering", "authors": "Katherine Van Koevering, Austin R. Benson, Jon Kleinberg", "title": "Random Graphs with Prescribed $K$-Core Sequences: A New Null Model for\n  Network Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3450001", "report-no": null, "categories": "cs.SI cs.DS math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of large-scale network data, a fundamental operation is the\ncomparison of observed phenomena to the predictions provided by null models:\nwhen we find an interesting structure in a family of real networks, it is\nimportant to ask whether this structure is also likely to arise in random\nnetworks with similar characteristics to the real ones. A long-standing\nchallenge in network analysis has been the relative scarcity of reasonable null\nmodels for networks; arguably the most common such model has been the\nconfiguration model, which starts with a graph $G$ and produces a random graph\nwith the same node degrees as $G$. This leads to a very weak form of null\nmodel, since fixing the node degrees does not preserve many of the crucial\nproperties of the network, including the structure of its subgraphs.\n  Guided by this challenge, we propose a new family of network null models that\noperate on the $k$-core decomposition. For a graph $G$, the $k$-core is its\nmaximal subgraph of minimum degree $k$; and the core number of a node $v$ in\n$G$ is the largest $k$ such that $v$ belongs to the $k$-core of $G$. We provide\nthe first efficient sampling algorithm to solve the following basic\ncombinatorial problem: given a graph $G$, produce a random graph sampled nearly\nuniformly from among all graphs with the same sequence of core numbers as $G$.\nThis opens the opportunity to compare observed networks $G$ with random graphs\nthat exhibit the same core numbers, a comparison that preserves aspects of the\nstructure of $G$ that are not captured by more local measures like the degree\nsequence. We illustrate the power of this core-based null model on some\nfundamental tasks in network analysis, including the enumeration of networks\nmotifs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 23:41:11 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Van Koevering", "Katherine", ""], ["Benson", "Austin R.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2102.12610", "submitter": "Alvaro Garcia-Recuero", "authors": "Alvaro Garcia-Recuero", "title": "Approximate Privacy-Preserving Neighbourhood Estimations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anonymous social networks present a number of new and challenging problems\nfor existing Social Network Analysis techniques. Traditionally, existing\nmethods for analysing graph structure, such as community detection, required\nglobal knowledge of the graph structure. That implies that a centralised entity\nmust be given access to the edge list of each node in the graph. This is\nimpossible for anonymous social networks and other settings where privacy is\nvalued by its participants. In addition, using their graph structure inputs for\nlearning tasks defeats the purpose of anonymity. In this work, we hypothesise\nthat one can re-purpose the use of the HyperANF a.k.a HyperBall algorithm --\nintended for approximate diameter estimation -- to the task of\nprivacy-preserving community detection for friend recommending systems that\nlearn from an anonymous representation of the social network graph structure\nwith limited privacy impact. This is possible because the core data structure\nmaintained by HyperBall is a HyperLogLog with a counter of the number of\nreachable neighbours from a given node. Exchanging this data structure in\nfuture decentralised learning deployments gives away no information about the\nneighbours of the node and therefore does preserve the privacy of the graph\nstructure.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 00:45:33 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 09:10:14 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 12:47:07 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 13:33:39 GMT"}, {"version": "v5", "created": "Sat, 19 Jun 2021 10:41:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Garcia-Recuero", "Alvaro", ""]]}, {"id": "2102.12646", "submitter": "Naoto Ohsaka", "authors": "Tatsuya Matsuoka and Naoto Ohsaka", "title": "Spanning Tree Constrained Determinantal Point Processes are Hard to\n  (Approximately) Evaluate", "comments": null, "journal-ref": null, "doi": "10.1016/j.orl.2021.02.004", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider determinantal point processes (DPPs) constrained by spanning\ntrees. Given a graph $G=(V,E)$ and a positive semi-definite matrix $\\mathbf{A}$\nindexed by $E$, a spanning-tree DPP defines a distribution such that we draw\n$S\\subseteq E$ with probability proportional to $\\det(\\mathbf{A}_S)$ only if\n$S$ induces a spanning tree. We prove $\\sharp\\textsf{P}$-hardness of computing\nthe normalizing constant for spanning-tree DPPs and provide an\napproximation-preserving reduction from the mixed discriminant, for which FPRAS\nis not known. We show similar results for DPPs constrained by forests.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 02:45:10 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Matsuoka", "Tatsuya", ""], ["Ohsaka", "Naoto", ""]]}, {"id": "2102.12822", "submitter": "Veli M\\\"akinen", "authors": "Massimo Equi, Tuukka Norri, Jarno Alanko, Bastien Cazaux, Alexandru I.\n  Tomescu and Veli M\\\"akinen", "title": "Algorithms and Complexity on Indexing Founder Graphs", "comments": "This is an extended version of WABI 2020 paper\n  (https://doi.org/10.4230/LIPIcs.WABI.2020.7), whose preprint is in\n  arXiv:2005.09342", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of matching a string in a labeled graph. Previous\nresearch has shown that unless the Orthogonal Vectors Hypothesis (OVH) is\nfalse, one cannot solve this problem in strongly sub-quadratic time, nor index\nthe graph in polynomial time to answer queries efficiently (Equi et al. ICALP\n2019, SOFSEM 2021). These conditional lower-bounds cover even deterministic\ngraphs with binary alphabet, but there naturally exist also graph classes that\nare easy to index: E.g. Wheeler graphs (Gagie et al. Theor. Comp. Sci. 2017)\ncover graphs admitting a Burrows-Wheeler transform -based indexing scheme.\nHowever, it is NP-complete to recognize if a graph is a Wheeler graph (Gibney,\nThankachan, ESA 2019).\n  We propose an approach to alleviate the construction bottleneck of Wheeler\ngraphs. Rather than starting from an arbitrary graph, we study graphs induced\nfrom multiple sequence alignments (MSAs). Elastic degenerate strings (Bernadini\net al. SPIRE 2017, ICALP 2019) can be seen as such graphs, and we introduce\nhere their generalization: elastic founder graphs. We first prove that even\nsuch induced graphs are hard to index under OVH. Then we introduce two\nsubclasses, repeat-free and semi-repeat-free graphs, that are easy to index. We\ngive a linear time algorithm to construct a repeat-free non-elastic founder\ngraph from a gapless MSA, and (parameterized) near-linear time algorithms to\nconstruct semi-repeat-free (repeat-free, respectively) elastic founder graphs\nfrom general MSAs. Finally, we show that repeat-free elastic founder graphs\nadmit a reduction to Wheeler graphs in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 12:47:13 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:33:24 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 18:39:58 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 11:19:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Equi", "Massimo", ""], ["Norri", "Tuukka", ""], ["Alanko", "Jarno", ""], ["Cazaux", "Bastien", ""], ["Tomescu", "Alexandru I.", ""], ["M\u00e4kinen", "Veli", ""]]}, {"id": "2102.12824", "submitter": "Sung Gwan Park", "authors": "Sangsoo Park, Sung Gwan Park, Bastien Cazaux, Kunsoo Park and Eric\n  Rivals", "title": "A Linear Time Algorithm for Constructing Hierarchical Overlap Graphs", "comments": "8 pages, 2 figures, submitted to CPM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hierarchical overlap graph (HOG) is a graph that encodes overlaps from a\ngiven set P of n strings, as the overlap graph does. A best known algorithm\nconstructs HOG in O(||P|| log n) time and O(||P||) space, where ||P|| is the\nsum of lengths of the strings in P. In this paper we present a new algorithm to\nconstruct HOG in O(||P||) time and space. Hence, the construction time and\nspace of HOG are better than those of the overlap graph, which are O(||P|| +\nn^2).\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 12:51:33 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 07:28:28 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Park", "Sangsoo", ""], ["Park", "Sung Gwan", ""], ["Cazaux", "Bastien", ""], ["Park", "Kunsoo", ""], ["Rivals", "Eric", ""]]}, {"id": "2102.12842", "submitter": "Hans-Peter Deifel", "authors": "Hans-Peter Deifel, Stefan Milius, Thorsten Wi{\\ss}mann", "title": "Coalgebra Encoding for Efficient Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, we have developed an efficient generic partition refinement\nalgorithm, which computes behavioural equivalence on a state-based system given\nas an encoded coalgebra, and implemented it in the tool CoPaR. Here we extend\nthis to a fully fledged minimization algorithm and tool by integrating two new\naspects: (1) the computation of the transition structure on the minimized state\nset, and (2) the computation of the reachable part of the given system. In our\ngeneric coalgebraic setting these two aspects turn out to be surprisingly\nnon-trivial requiring us to extend the previous theory. In particular, we\nidentify a sufficient condition on encodings of coalgebras, and we show how to\naugment the existing interface, which encapsulates computations that are\nspecific for the coalgebraic type functor, to make the above extensions\npossible. Both extensions have linear run time.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:29:04 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 17:36:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Deifel", "Hans-Peter", ""], ["Milius", "Stefan", ""], ["Wi\u00dfmann", "Thorsten", ""]]}, {"id": "2102.12879", "submitter": "Ariel Kulik", "authors": "Ariel Kulik and Roy Schwartz and Hadas Shachnai", "title": "A Refined Analysis of Submodular Greedy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for maximizing a monotone submodular function subject to a\nknapsack constraint rely on the natural greedy heuristic. We present a novel\nrefined analysis of this greedy heuristic which enables us to: $(1)$ reduce the\nenumeration in the tight $(1-e^{-1})$-approximation of [Sviridenko 04] from\nsubsets of size three to two; $(2)$ present an improved upper bound of\n$0.42945$ for the classic algorithm which returns the better between a single\nelement and the output of the greedy heuristic.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 14:27:04 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 10:45:31 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kulik", "Ariel", ""], ["Schwartz", "Roy", ""], ["Shachnai", "Hadas", ""]]}, {"id": "2102.12886", "submitter": "Prerona Chatterjee", "authors": "Prerona Chatterjee, Kshitij Gajjar, Jaikumar Radhakrishnan, Girish\n  Varma", "title": "Generalized Parametric Path Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Parametric path problems arise independently in diverse domains, ranging from\ntransportation to finance, where they are studied under various assumptions. We\nformulate a general path problem with relaxed assumptions, and describe how\nthis formulation is applicable in these domains.\n  We study the complexity of the general problem, and a variant of it where\npreprocessing is allowed. We show that when the parametric weights are linear\nfunctions, algorithms remain tractable even under our relaxed assumptions.\nFurthermore, we show that if the weights are allowed to be non-linear, the\nproblem becomes NP-hard. We also study the mutli-dimensional version of the\nproblem where the weight functions are parameterized by multiple parameters. We\nshow that even with two parameters, the problem is NP-hard.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 14:42:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 12:19:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chatterjee", "Prerona", ""], ["Gajjar", "Kshitij", ""], ["Radhakrishnan", "Jaikumar", ""], ["Varma", "Girish", ""]]}, {"id": "2102.12975", "submitter": "Liren Yu", "authors": "Liren Yu, Jiaming Xu, Xiaojun Lin", "title": "The Power of $D$-hops in Matching Power-Law Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies seeded graph matching for power-law graphs. Assume that\ntwo edge-correlated graphs are independently edge-sampled from a common parent\ngraph with a power-law degree distribution. A set of correctly matched\nvertex-pairs is chosen at random and revealed as initial seeds. Our goal is to\nuse the seeds to recover the remaining latent vertex correspondence between the\ntwo graphs. Departing from the existing approaches that focus on the use of\nhigh-degree seeds in $1$-hop neighborhoods, we develop an efficient algorithm\nthat exploits the low-degree seeds in suitably-defined $D$-hop neighborhoods.\nSpecifically, we first match a set of vertex-pairs with appropriate degrees\n(which we refer to as the first slice) based on the number of low-degree seeds\nin their $D$-hop neighborhoods. This significantly reduces the number of\ninitial seeds needed to trigger a cascading process to match the rest of the\ngraphs. Under the Chung-Lu random graph model with $n$ vertices, max degree\n$\\Theta(\\sqrt{n})$, and the power-law exponent $2<\\beta<3$, we show that as\nsoon as $D> \\frac{4-\\beta}{3-\\beta}$, by optimally choosing the first slice,\nwith high probability our algorithm can correctly match a constant fraction of\nthe true pairs without any error, provided with only $\\Omega((\\log\nn)^{4-\\beta})$ initial seeds. Our result achieves an exponential reduction in\nthe seed size requirement, as the best previously known result requires\n$n^{1/2+\\epsilon}$ seeds (for any small constant $\\epsilon>0$). Performance\nevaluation with synthetic and real data further corroborates the improved\nperformance of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 05:36:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yu", "Liren", ""], ["Xu", "Jiaming", ""], ["Lin", "Xiaojun", ""]]}, {"id": "2102.13098", "submitter": "Sitan Chen", "authors": "Sitan Chen, Jerry Li, Ryan O'Donnell", "title": "Toward Instance-Optimal State Certification With Incoherent Measurements", "comments": "47 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the basic problem of quantum state certification: given copies of\nunknown mixed state $\\rho\\in\\mathbb{C}^{d\\times d}$ and the description of a\nmixed state $\\sigma$, decide whether $\\sigma = \\rho$ or $\\|\\sigma -\n\\rho\\|_{\\mathsf{tr}} \\ge \\epsilon$. When $\\sigma$ is maximally mixed, this is\nmixedness testing, and it is known that $\\Omega(d^{\\Theta(1)}/\\epsilon^2)$\ncopies are necessary, where the exact exponent depends on the type of\nmeasurements the learner can make [OW15, BCL20], and in many of these settings\nthere is a matching upper bound [OW15, BOW19, BCL20].\n  Can one avoid this $d^{\\Theta(1)}$ dependence for certain kinds of mixed\nstates $\\sigma$, e.g. ones which are approximately low rank? More ambitiously,\ndoes there exist a simple functional $f:\\mathbb{C}^{d\\times\nd}\\to\\mathbb{R}_{\\ge 0}$ for which one can show that\n$\\Theta(f(\\sigma)/\\epsilon^2)$ copies are necessary and sufficient for state\ncertification with respect to any $\\sigma$? Such instance-optimal bounds are\nknown in the context of classical distribution testing, e.g. [VV17].\n  Here we give the first bounds of this nature for the quantum setting, showing\n(up to log factors) that the copy complexity for state certification using\nnonadaptive incoherent measurements is essentially given by the copy complexity\nfor mixedness testing times the fidelity between $\\sigma$ and the maximally\nmixed state. Surprisingly, our bound differs substantially from instance\noptimal bounds for the classical problem, demonstrating a qualitative\ndifference between the two settings.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 18:59:11 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chen", "Sitan", ""], ["Li", "Jerry", ""], ["O'Donnell", "Ryan", ""]]}, {"id": "2102.13220", "submitter": "Chenyang Yuan", "authors": "Chenyang Yuan and Pablo A. Parrilo", "title": "Semidefinite Relaxations of Products of Nonnegative Forms on the Sphere", "comments": "26 pages, 3 figures. New Section 2.4 and fixed typos involving Fact\n  4.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing the geometric mean of $d$ low-degree\nnon-negative forms on the real or complex sphere in $n$ variables. We show that\nthis highly non-convex problem is NP-hard even when the forms are quadratic and\nis equivalent to optimizing a homogeneous polynomial of degree $O(d)$ on the\nsphere. The standard Sum-of-Squares based convex relaxation for this polynomial\noptimization problem requires solving a semidefinite program (SDP) of size\n$n^{O(d)}$, with multiplicative approximation guarantees of\n$\\Omega(\\frac{1}{n})$. We exploit the compact representation of this polynomial\nto introduce a SDP relaxation of size polynomial in $n$ and $d$, and prove that\nit achieves a constant factor multiplicative approximation when maximizing the\ngeometric mean of non-negative quadratic forms. We also show that this analysis\nis asymptotically tight, with a sequence of instances where the gap between the\nrelaxation and true optimum approaches this constant factor as $d \\rightarrow\n\\infty$. Next we propose a series of intermediate relaxations of increasing\ncomplexity that interpolate to the full Sum-of-Squares relaxation, as well as a\nrounding algorithm that finds an approximate solution from the solution of any\nintermediate relaxation. Finally we show that this approach can be generalized\nfor relaxations of products of non-negative forms of any degree.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 23:06:32 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 06:50:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yuan", "Chenyang", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "2102.13242", "submitter": "Xing Hu", "authors": "Vassos Hadzilacos, Xing Hu, Sam Toueg", "title": "On Register Linearizability and Termination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal work, Golab et al. showed that a randomized algorithm that works\nwith atomic objects may lose some of its properties if we replace the atomic\nobjects that it uses with linearizable objects. It was not known whether the\nproperties that can be lost include the important property of termination (with\nprobability 1). In this paper, we first show that, for randomized algorithms,\ntermination can indeed be lost.\n  Golab et al. also introduced strong linearizability, and proved that strongly\nlinearizable objects can be used as if they were atomic objects, even for\nrandomized algorithms: they preserve the algorithm's correctness properties,\nincluding termination. Unfortunately, there are important cases where strong\nlinearizability is impossible to achieve. In particular, Helmi et al. MWMR\nregisters do not have strongly linearizable implementations from SWMR\nregisters.\n  So we propose a new type of register linearizability, called write\nstrong-linearizability, that is strictly stronger than linearizability but\nstrictly weaker than strong linearizability. We prove that some randomized\nalgorithms that fail to terminate with linearizable registers, work with write\nstrongly-linearizable ones. In other words, there are cases where\nlinearizability is not sufficient but write strong-linearizability is. In\ncontrast to the impossibility result mentioned above, we prove that write\nstrongly-linearizable MWMR registers are implementable from SWMR registers.\nAchieving write strong-linearizability, however, is harder than achieving just\nlinearizability: we give a simple implementation of MWMR registers from SWMR\nregisters and we prove that this implementation is linearizable but not write\nstrongly-linearizable. Finally, we prove that any linearizable implementation\nof SWMR registers is necessarily write strongly-linearizable; this holds for\nshared-memory, message-passing, and hybrid systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 00:16:39 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Hadzilacos", "Vassos", ""], ["Hu", "Xing", ""], ["Toueg", "Sam", ""]]}, {"id": "2102.13409", "submitter": "Dimitrios Thilikos", "authors": "Fedor V. Fomin and Petr A. Golovach and Dimitrios M. Thilikos", "title": "Can Romeo and Juliet Meet? Or Rendezvous Games with Adversaries on\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the rendezvous game with adversaries. In this game, two players,\n{\\sl Facilitator} and {\\sl Disruptor}, play against each other on a graph.\nFacilitator has two agents, and Disruptor has a team of $k$ agents located in\nsome vertices of the graph. They take turns in moving their agents to adjacent\nvertices (or staying). Facilitator wins if his agents meet in some vertex of\nthe graph. The goal of Disruptor is to prevent the rendezvous of Facilitator's\nagents. Our interest is to decide whether Facilitator can win. It appears that,\nin general, the problem is PSPACE-hard and, when parameterized by $k$,\nco-W[2]-hard. Moreover, even the game's variant where we ask whether\nFacilitator can ensure the meeting of his agents within $\\tau$ steps is\nco-NP-complete already for $\\tau=2$. On the other hand, for chordal and\n$P_5$-free graphs, we prove that the problem is solvable in polynomial time.\nThese algorithms exploit an interesting relation of the game and minimum vertex\ncuts in certain graph classes. Finally, we show that the problem is\nfixed-parameter tractable parameterized by both the graph's neighborhood\ndiversity and $\\tau$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:38:16 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 11:49:30 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Thilikos", "Dimitrios M.", ""]]}]