[{"id": "1208.0108", "submitter": "Den Brechka", "authors": "Denis Brechka", "title": "Analysis of access in the Take-Grant model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article discribe methods of verifing the conditions of access in computer\nsystems based on Take-Grant protection model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 06:12:20 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Brechka", "Denis", ""]]}, {"id": "1208.0142", "submitter": "Stefan Kratsch", "authors": "Stefan Kratsch and Pascal Schweitzer", "title": "Graph Isomorphism for Graph Classes Characterized by two Forbidden\n  Induced Subgraphs", "comments": "22 pages, 4 figures. To appear in the proceedings of WG 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Graph Isomorphism problem on graph classes\nthat are characterized by a finite number of forbidden induced subgraphs,\nfocusing mostly on the case of two forbidden subgraphs. We show hardness\nresults and develop techniques for the structural analysis of such graph\nclasses, which applied to the case of two forbidden subgraphs give the\nfollowing results: A dichotomy into isomorphism complete and polynomial-time\nsolvable graph classes for all but finitely many cases, whenever neither of the\nforbidden graphs is a clique, a pan, or a complement of these graphs. Further\nreducing the remaining open cases we show that (with respect to graph\nisomorphism) forbidding a pan is equivalent to forbidding a clique of size\nthree.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 09:02:57 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2012 06:51:17 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Kratsch", "Stefan", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1208.0144", "submitter": "He Huang", "authors": "He Huang, Yu-e Sun, Xiang-yang Li, Hongli Xu, Yousong Zhou and\n  Liusheng Huang", "title": "Truthful Auction Mechanism for Heterogeneous Spectrum Allocation in\n  Wireless Networks", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secondary spectrum auction is widely applied in wireless networks for\nmitigating the spectrum scarcity. In a realistic spectrum trading market, the\nrequests from secondary users often specify the usage of a fixed spectrum\nfrequency band in a certain geographical region and require a duration time in\na fixed available time interval. Considering the selfish behaviors of secondary\nusers, it is imperative to design a truthful auction which matches the\navailable spectrums and requests of secondary users optimally. Unfortunately,\nexisting designs either do not consider spectrum heterogeneity or ignore the\ndifferences of required time among secondary users. In this paper, we address\nthis problem by investigating how to use auction mechanisms to allocate and\nprice spectrum resources so that the social efficiency can be maximized. We\nbegin by classifying the spectrums and requests from secondary users into\ndifferent local markets which ensures there is no interference between local\nmarkets, and then we can focus on the auction in a single local market. We\nfirst design an optimal auction based on the Vickrey-Clarke-Groves (VCG)\nmechanism to maximize the social efficiency while enforcing truthfulness. To\nreduce the computational complexity, we further propose a truthful sub-optimal\nauction with polynomial time complexity, which yields an approximation factor\n6+4\\surd2. Our extensive simulation results using real spectrum availability\ndata show that the social efficiency ratio of the sub-optimal auction is always\nabove 70% compared with the optimal auction.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 09:14:26 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Huang", "He", ""], ["Sun", "Yu-e", ""], ["Li", "Xiang-yang", ""], ["Xu", "Hongli", ""], ["Zhou", "Yousong", ""], ["Huang", "Liusheng", ""]]}, {"id": "1208.0202", "submitter": "Sandor P. Fekete", "authors": "S\\'andor P. Fekete", "title": "The Complexity of MaxMin Length Triangulation", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1991, Edelsbrunner and Tan gave an O(n^2) algorithm for finding the MinMax\nLength triangulation of a set of points in the plane. In this paper we resolve\none of the open problems stated in that paper, by showing that finding a MaxMin\nLength triangulation is an NP-complete problem. The proof implies that (unless\nP=NP), there is no polynomial-time approximation algorithm that can approximate\nthe problem within any polynomial factor.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 13:20:32 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""]]}, {"id": "1208.0257", "submitter": "Neal E. Young", "authors": "Daniel Sheldon, Neal E. Young", "title": "Hamming Approximation of NP Witnesses", "comments": null, "journal-ref": "Theory of Computing 9(22), 2013, pp. 685-702", "doi": "10.4086/toc.2013.v009a022", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a satisfiable 3-SAT formula, how hard is it to find an assignment to\nthe variables that has Hamming distance at most n/2 to a satisfying assignment?\nMore generally, consider any polynomial-time verifier for any NP-complete\nlanguage. A d(n)-Hamming-approximation algorithm for the verifier is one that,\ngiven any member x of the language, outputs in polynomial time a string a with\nHamming distance at most d(n) to some witness w, where (x,w) is accepted by the\nverifier. Previous results have shown that, if P != NP, then every NP-complete\nlanguage has a verifier for which there is no\n(n/2-n^(2/3+d))-Hamming-approximation algorithm, for various constants d > 0.\n  Our main result is that, if P != NP, then every paddable NP-complete language\nhas a verifier that admits no (n/2+O(sqrt(n log n)))-Hamming-approximation\nalgorithm. That is, one cannot get even half the bits right. We also consider\nnatural verifiers for various well-known NP-complete problems. They do have\nn/2-Hamming-approximation algorithms, but, if P != NP, have no\n(n/2-n^epsilon)-Hamming-approximation algorithms for any constant epsilon > 0.\n  We show similar results for randomized algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 15:48:57 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 17:44:32 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Sheldon", "Daniel", ""], ["Young", "Neal E.", ""]]}, {"id": "1208.0312", "submitter": "Jessica Chang", "authors": "Jessica Chang, Harold N. Gabow and Samir Khuller", "title": "A Model for Minimizing Active Processor Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the following elementary scheduling problem. We are given a\ncollection of n jobs, where each job has an integer length as well as a set Ti\nof time intervals in which it can be feasibly scheduled. Given a parameter B,\nthe processor can schedule up to B jobs at a timeslot t so long as it is\n\"active\" at t. The goal is to schedule all the jobs in the fewest number of\nactive timeslots. The machine consumes a fixed amount of energy per active\ntimeslot, regardless of the number of jobs scheduled in that slot (as long as\nthe number of jobs is non-zero). In other words, subject to all units of each\njob being scheduled in its feasible region and at each slot at most B jobs\nbeing scheduled, we are interested in minimizing the total time during which\nthe machine is active. We present a linear time algorithm for the case where\njobs are unit length and each Ti is a single interval. For general Ti, we show\nthat the problem is NP-complete even for B = 3. However when B = 2, we show\nthat it can be efficiently solved. In addition, we consider a version of the\nproblem where jobs have arbitrary lengths and can be preempted at any point in\ntime. For general B, the problem can be solved by linear programming. For B =\n2, the problem amounts to finding a triangle-free 2-matching on a special\ngraph. We extend the algorithm of Babenko et. al. to handle our variant, and\nalso to handle non-unit length jobs. This yields an O(sqrt(L)m) time algorithm\nto solve the preemptive scheduling problem for B = 2, where L is the sum of the\njob lengths. We also show that for B = 2 and unit length jobs, the optimal\nnon-preemptive schedule has at most 4/3 times the active time of the optimal\npreemptive schedule; this bound extends to several versions of the problem when\njobs have arbitrary length.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 18:34:36 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Chang", "Jessica", ""], ["Gabow", "Harold N.", ""], ["Khuller", "Samir", ""]]}, {"id": "1208.0370", "submitter": "Zoltan Toroczkai", "authors": "Maria Ercsey-Ravasz and Zoltan Toroczkai", "title": "The Chaos Within Sudoku", "comments": "9 pages, 4 color figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.DS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical structure of the widely popular Sudoku puzzles is akin to\ntypical hard constraint satisfaction problems that lie at the heart of many\napplications, including protein folding and the general problem of finding the\nground state of a glassy spin system. Via an exact mapping of Sudoku into a\ndeterministic, continuous-time dynamical system, here we show that the\ndifficulty of Sudoku translates into transient chaotic behavior exhibited by\nthe dynamical system. In particular, we show that the escape rate $\\kappa$, an\ninvariant characteristic of transient chaos, provides a single scalar measure\nof the puzzle's hardness, which correlates well with human difficulty level\nratings. Accordingly, $\\eta = -\\log_{10}{\\kappa}$ can be used to define a\n\"Richter\"-type scale for puzzle hardness, with easy puzzles falling in the\nrange $0 < \\eta \\leq 1$, medium ones within $1 < \\eta \\leq 2$, hard in $2 <\n\\eta \\leq 3$ and ultra-hard with $\\eta > 3$. To our best knowledge, there are\nno known puzzles with $\\eta > 4$.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 21:41:09 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Ercsey-Ravasz", "Maria", ""], ["Toroczkai", "Zoltan", ""]]}, {"id": "1208.0378", "submitter": "Charless Fowlkes", "authors": "Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes", "title": "Fast Planar Correlation Clustering for Image Segmentation", "comments": "This is the extended version of a paper to appear at the 12th\n  European Conference on Computer Vision (ECCV 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new optimization scheme for finding high-quality correlation\nclusterings in planar graphs that uses weighted perfect matching as a\nsubroutine. Our method provides lower-bounds on the energy of the optimal\ncorrelation clustering that are typically fast to compute and tight in\npractice. We demonstrate our algorithm on the problem of image segmentation\nwhere this approach outperforms existing global optimization techniques in\nminimizing the objective and is competitive with the state of the art in\nproducing high-quality segmentations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 00:54:02 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Yarkony", "Julian", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1208.0396", "submitter": "Andy Nguyen", "authors": "Andy Nguyen", "title": "Solving Cyclic Longest Common Subsequence in Quadratic Time", "comments": "Updated references; an O(n^2) solution already exists, though it is\n  terribly unwieldy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical algorithm for the cyclic longest common subsequence\n(CLCS) problem that runs in O(mn) time, where m and n are the lengths of the\ntwo input strings. While this is not necessarily an asymptotic improvement over\nthe existing record, it is far simpler to understand and to implement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 04:23:12 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2012 08:07:46 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2012 00:28:29 GMT"}], "update_date": "2012-08-17", "authors_parsed": [["Nguyen", "Andy", ""]]}, {"id": "1208.0460", "submitter": "Patrick Prosser", "authors": "Alice Miller and Patrick Prosser", "title": "Diamond-free Degree Sequences", "comments": "8 pages, 2 figures, 2 algorithms, 2 models, 1 table", "journal-ref": "Acta Univ. Sapientiae, Informatica, 4(2): 189-200, 2012", "doi": null, "report-no": "TR-2010-318", "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new problem, CSPLib problem number 50, to generate all degree\nsequences that have a corresponding diamond-free graph with secondary\nproperties. This problem arises naturally from a problem in mathematics to do\nwith balanced incomplete block designs; we devote a section of this paper to\nthis. The problem itself is challenging with respect to computational effort\narising from the large number of symmetries within the models. We introduce two\nmodels for this problem. The second model is an improvement on the first, and\nthis improvement largely consists of breaking the problem into two stages, the\nfirst stage producing graphical degree sequences that satisfy arithmetic\nconstraints and the second part testing that there exists a graph with that\ndegree sequence that is diamond-free.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 12:11:25 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 10:38:35 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2012 15:32:06 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Miller", "Alice", ""], ["Prosser", "Patrick", ""]]}, {"id": "1208.0542", "submitter": "Wenqi Duan Dr.", "authors": "Wen-Qi Duan", "title": "A Constructive Algorithm to Prove P=NP", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After reducing the undirected Hamiltonian cycle problem into the TSP problem\nwith cost 0 or 1, we developed an effective algorithm to compute the optimal\ntour of the transformed TSP. Our algorithm is described as a growth process:\ninitially, constructing 4-vertexes optimal tour; next, one new vertex being\nadded into the optimal tour in such a way to obtain the new optimal tour; then,\nrepeating the previous step until all vertexes are included into the optimal\ntour. This paper has shown that our constructive algorithm can solve the\nundirected Hamiltonian cycle problem in polynomial time. According to\nCook-Levin theorem, we argue that we have provided a constructive proof of\nP=NP.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2012 07:10:37 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Duan", "Wen-Qi", ""]]}, {"id": "1208.0554", "submitter": "Janne H. Korhonen", "authors": "Petteri Kaski, Mikko Koivisto, Janne H. Korhonen", "title": "Fast Monotone Summation over Disjoint Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing an ensemble of multiple sums where the\nsummands in each sum are indexed by subsets of size $p$ of an $n$-element\nground set. More precisely, the task is to compute, for each subset of size $q$\nof the ground set, the sum over the values of all subsets of size $p$ that are\ndisjoint from the subset of size $q$. We present an arithmetic circuit that,\nwithout subtraction, solves the problem using $O((n^p+n^q)\\log n)$ arithmetic\ngates, all monotone; for constant $p$, $q$ this is within the factor $\\log n$\nof the optimal. The circuit design is based on viewing the summation as a \"set\nnucleation\" task and using a tree-projection approach to implement the\nnucleation. Applications include improved algorithms for counting heaviest\n$k$-paths in a weighted graph, computing permanents of rectangular matrices,\nand dynamic feature selection in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 17:38:41 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Kaski", "Petteri", ""], ["Koivisto", "Mikko", ""], ["Korhonen", "Janne H.", ""]]}, {"id": "1208.0798", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher, George Varghese", "title": "Biff (Bloom Filter) Codes : Fast Error Correction for Large Data Sets", "comments": "5 pages, Corrected typos from ISIT 2012 conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large data sets are increasingly common in cloud and virtualized\nenvironments. For example, transfers of multiple gigabytes are commonplace, as\nare replicated blocks of such sizes. There is a need for fast error-correction\nor data reconciliation in such settings even when the expected number of errors\nis small.\n  Motivated by such cloud reconciliation problems, we consider error-correction\nschemes designed for large data, after explaining why previous approaches\nappear unsuitable. We introduce Biff codes, which are based on Bloom filters\nand are designed for large data. For Biff codes with a message of length $L$\nand $E$ errors, the encoding time is $O(L)$, decoding time is $O(L + E)$ and\nthe space overhead is $O(E)$. Biff codes are low-density parity-check codes;\nthey are similar to Tornado codes, but are designed for errors instead of\nerasures. Further, Biff codes are designed to be very simple, removing any\nexplicit graph structures and based entirely on hash tables. We derive Biff\ncodes by a simple reduction from a set reconciliation algorithm for a recently\ndeveloped data structure, invertible Bloom lookup tables. While the underlying\ntheory is extremely simple, what makes this code especially attractive is the\nease with which it can be implemented and the speed of decoding. We present\nresults from a prototype implementation that decodes messages of 1 million\nwords with thousands of errors in well under a second.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 17:15:53 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Varghese", "George", ""]]}, {"id": "1208.0811", "submitter": "Guanhong Pei", "authors": "Guanhong Pei and Anil Kumar S. Vullikanti", "title": "Efficient Algorithms for Maximum Link Scheduling in Distributed\n  Computing Models with SINR Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in wireless networks is the maximum link scheduling\nproblem: given a set $L$ of links, compute the largest possible subset\n$L'\\subseteq L$ of links that can be scheduled simultaneously without\ninterference. This problem is particularly challenging in the physical\ninterference model based on SINR constraints (referred to as the SINR model),\nwhich has gained a lot of interest in recent years. Constant factor\napproximation algorithms have been developed for this problem, but low\ncomplexity distributed algorithms that give the same approximation guarantee in\nthe SINR model are not known. Distributed algorithms are especially challenging\nin this model, because of its non-locality.\n  In this paper, we develop a set of fast distributed algorithms in the SINR\nmodel, providing constant approximation for the maximum link scheduling problem\nunder uniform power assignment. We find that different aspects of available\ntechnology, such as full/half-duplex communication, and non-adaptive/adaptive\npower control, have a significant impact on the performance of the algorithm;\nthese issues have not been explored in the context of distributed algorithms in\nthe SINR model before. Our algorithms' running time is $O(g(L) \\log^c m)$,\nwhere $c=1,2,3$ for different problem instances, and $g(L)$ is the \"link\ndiversity\" determined by the logarithmic scale of a communication link length.\nSince $g(L)$ is small and remains in a constant range in most cases, our\nalgorithms serve as the first set of \"sublinear\" time distributed solution. The\nalgorithms are randomized and crucially use physical carrier sensing in\ndistributed communication steps.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 18:26:06 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2012 15:46:26 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Pei", "Guanhong", ""], ["Vullikanti", "Anil Kumar S.", ""]]}, {"id": "1208.1248", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Marcin Pilipczuk", "title": "On fixed-parameter algorithms for Split Vertex Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Split Vertex Deletion problem, given a graph G and an integer k, we\nask whether one can delete k vertices from the graph G to obtain a split graph\n(i.e., a graph, whose vertex set can be partitioned into two sets: one inducing\na clique and the second one inducing an independent set). In this paper we\nstudy fixed-parameter algorithms for Split Vertex Deletion parameterized by k:\nwe show that, up to a factor quasipolynomial in k and polynomial in n, the\nSplit Vertex Deletion problem can be solved in the same time as the\nwell-studied Vertex Cover problem. Plugging the currently best fixed-parameter\nalgorithm for Vertex Cover due to Chen et al. [TCS 2010], we obtain an\nalgorithm that solves Split Vertex Deletion in time O(1.2738^k * k^O(log k) +\nn^O(1)).\n  To achieve our goal, we prove the following structural result that may be of\nindependent interest: for any graph G we may compute a family P of size n^O(log\nn) containing partitions of V(G) into two parts, such for any two disjoint\nsubsets X_C, X_I of V(G) where G[X_C] is a clique and G[X_I] is an independent\nset, there is a partition in P which contains all vertices of X_C on one side\nand all vertices of X_I on the other.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 19:23:26 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1208.1272", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy and Shi Li", "title": "A Polylogarithimic Approximation Algorithm for Edge-Disjoint Paths with\n  Congestion 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Edge-Disjoint Paths with Congestion problem (EDPwC), we are given an\nundirected n-vertex graph G, a collection M={(s_1,t_1),...,(s_k,t_k)} of demand\npairs and an integer c. The goal is to connect the maximum possible number of\nthe demand pairs by paths, so that the maximum edge congestion - the number of\npaths sharing any edge - is bounded by c. When the maximum allowed congestion\nis c=1, this is the classical Edge-Disjoint Paths problem (EDP).\n  The best current approximation algorithm for EDP achieves an $O(\\sqrt\nn)$-approximation, by rounding the standard multi-commodity flow relaxation of\nthe problem. This matches the $\\Omega(\\sqrt n)$ lower bound on the integrality\ngap of this relaxation. We show an $O(poly log k)$-approximation algorithm for\nEDPwC with congestion c=2, by rounding the same multi-commodity flow\nrelaxation. This gives the best possible congestion for a sub-polynomial\napproximation of EDPwC via this relaxation. Our results are also close to\noptimal in terms of the number of pairs routed, since EDPwC is known to be hard\nto approximate to within a factor of $\\tilde{\\Omega}((\\log n)^{1/(c+1)})$ for\nany constant congestion c. Prior to our work, the best approximation factor for\nEDPwC with congestion 2 was $\\tilde O(n^{3/7})$, and the best algorithm\nachieving a polylogarithmic approximation required congestion 14.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 20:28:20 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Li", "Shi", ""]]}, {"id": "1208.1454", "submitter": "Danupon Nanongkai", "authors": "Atish Das Sarma, Ashwin Lall, Danupon Nanongkai, Amitabh Trehan", "title": "Dense Subgraphs on Dynamic Networks", "comments": "To appear in the 26th International Symposium on Distributed\n  Computing (DISC 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed networks, it is often useful for the nodes to be aware of\ndense subgraphs, e.g., such a dense subgraph could reveal dense subtructures in\notherwise sparse graphs (e.g. the World Wide Web or social networks); these\nmight reveal community clusters or dense regions for possibly maintaining good\ncommunication infrastructure. In this work, we address the problem of\nself-awareness of nodes in a dynamic network with regards to graph density,\ni.e., we give distributed algorithms for maintaining dense subgraphs that the\nmember nodes are aware of. The only knowledge that the nodes need is that of\nthe dynamic diameter $D$, i.e., the maximum number of rounds it takes for a\nmessage to traverse the dynamic network. For our work, we consider a model\nwhere the number of nodes are fixed, but a powerful adversary can add or remove\na limited number of edges from the network at each time step. The communication\nis by broadcast only and follows the CONGEST model. Our algorithms are\ncontinuously executed on the network, and at any time (after some\ninitialization) each node will be aware if it is part (or not) of a particular\ndense subgraph. We give algorithms that ($2 + \\epsilon$)-approximate the\ndensest subgraph and ($3 + \\epsilon$)-approximate the at-least-$k$-densest\nsubgraph (for a given parameter $k$). Our algorithms work for a wide range of\nparameter values and run in $O(D\\log_{1+\\epsilon} n)$ time. Further, a special\ncase of our results also gives the first fully decentralized approximation\nalgorithms for densest and at-least-$k$-densest subgraph problems for static\ndistributed graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2012 15:54:28 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Sarma", "Atish Das", ""], ["Lall", "Ashwin", ""], ["Nanongkai", "Danupon", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1208.1532", "submitter": "Daniel Denton", "authors": "Daniel Denton", "title": "Methods of computing deque sortable permutations given complete and\n  incomplete information", "comments": "dartmouth senior honors thesis advised by Peter Doyle and Scot\n  Drysdale 45 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "Dartmouth Computer Science Technical Report TR2012-719", "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of determining which permutations can be sorted using certain\nswitchyard networks dates back to Knuth in 1968. In this work, we are\ninterested in permutations which are sortable on a double-ended queue (called a\ndeque), or on two parallel stacks. In 1982, Rosenstiehl and Tarjan presented an\nO(n) algorithm for testing whether a given permutation was sortable on parallel\nstacks. In the same paper, they also presented a modification giving O(n) test\nfor sortability on a deque. We demonstrate a slight error in the version of\ntheir algorithm for testing deque sortability, and present a fix for this\nproblem.\n  The general enumeration problem for both of these classes of permutations\nremains unsolved. What is known is that the growth rate of both classes is\napproximately Theta(8^n), so computing the number of sortable permutations of\nlength n, even for small values of n, is difficult to do using any method that\nmust evaluate each sortable permutation individually. As far as we know, the\nnumber of deque sortable permutations was known only up to n=14. This was\ncomputed using algorithms which effectively generate all sortable permutations.\nBy using the symmetries inherent in the execution of Tarjan's algorithm, we\nhave developed a new dynamic programming algorithm which can count the number\nof sortable permutations in both classes in O(n^5 2^n) time, allowing the\ncalculation of the number of deque and parallel stack sortable permutation for\nmuch higher values of n than was previously possible.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2012 21:39:37 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Denton", "Daniel", ""]]}, {"id": "1208.1565", "submitter": "Robert Schweller", "authors": "Robert Schweller, Michael Sherman", "title": "Fuel Efficient Computation in Passive Self-Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that passive self-assembly in the context of the tile\nself-assembly model is capable of performing fuel efficient, universal\ncomputation. The tile self-assembly model is a premiere model of self-assembly\nin which particles are modeled by four-sided squares with glue types assigned\nto each tile edge. The assembly process is driven by positive and negative\nforce interactions between glue types, allowing for tile assemblies floating in\nthe plane to combine and break apart over time. We refer to this type of\nassembly model as passive in that the constituent parts remain unchanged\nthroughout the assembly process regardless of their interactions. A\ncomputationally universal system is said to be fuel efficient if the number of\ntiles used up per computation step is bounded by a constant. Work within this\nmodel has shown how fuel guzzling tile systems can perform universal\ncomputation with only positive strength glue interactions. Recent work has\nintroduced space-efficient, fuel-guzzling universal computation with the\naddition of negative glue interactions and the use of a powerful non-diagonal\nclass of glue interactions. Other recent work has shown how to achieve fuel\nefficient computation within active tile self-assembly. In this paper we\nutilize negative interactions in the tile self-assembly model to achieve the\nfirst computationally universal passive tile self-assembly system that is both\nspace and fuel-efficient. In addition, we achieve this result using a limited\ndiagonal class of glue interactions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 02:33:30 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Schweller", "Robert", ""], ["Sherman", "Michael", ""]]}, {"id": "1208.1688", "submitter": "Sebastian Ordyniak", "authors": "Serge Gaspers, Eun Jung Kim, Sebastian Ordyniak, Saket Saurabh, Stefan\n  Szeider", "title": "Don't Be Strict in Local Search!", "comments": "(author's self-archived copy)", "journal-ref": "Proc. AAAI'12, pp. 486-492 (AAAI Press 2012)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Search is one of the fundamental approaches to combinatorial\noptimization and it is used throughout AI. Several local search algorithms are\nbased on searching the k-exchange neighborhood. This is the set of solutions\nthat can be obtained from the current solution by exchanging at most k\nelements. As a rule of thumb, the larger k is, the better are the chances of\nfinding an improved solution. However, for inputs of size n, a na\\\"ive\nbrute-force search of the k-exchange neighborhood requires n to the power of\nO(k) time, which is not practical even for very small values of k.\n  Fellows et al. (IJCAI 2009) studied whether this brute-force search is\navoidable and gave positive and negative answers for several combinatorial\nproblems. They used the notion of local search in a strict sense. That is, an\nimproved solution needs to be found in the k-exchange neighborhood even if a\nglobal optimum can be found efficiently.\n  In this paper we consider a natural relaxation of local search, called\npermissive local search (Marx and Schlotter, IWPEC 2009) and investigate\nwhether it enhances the domain of tractable inputs. We exemplify this approach\non a fundamental combinatorial problem, Vertex Cover. More precisely, we show\nthat for a class of inputs, finding an optimum is hard, strict local search is\nhard, but permissive local search is tractable.\n  We carry out this investigation in the framework of parameterized complexity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 15:21:50 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2012 14:03:21 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2012 07:57:06 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Gaspers", "Serge", ""], ["Kim", "Eun Jung", ""], ["Ordyniak", "Sebastian", ""], ["Saurabh", "Saket", ""], ["Szeider", "Stefan", ""]]}, {"id": "1208.1692", "submitter": "Sebastian Ordyniak", "authors": "Serge Gaspers, Mikko Koivisto, Mathieu Liedloff, Sebastian Ordyniak,\n  Stefan Szeider", "title": "On Finding Optimal Polytrees", "comments": "(author's self-archived copy)", "journal-ref": "Proc. AAAI'12, pp. 750-756 (AAAI Press 2012)", "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring probabilistic networks from data is a notoriously difficult task.\nUnder various goodness-of-fit measures, finding an optimal network is NP-hard,\neven if restricted to polytrees of bounded in-degree. Polynomial-time\nalgorithms are known only for rare special cases, perhaps most notably for\nbranchings, that is, polytrees in which the in-degree of every node is at most\none. Here, we study the complexity of finding an optimal polytree that can be\nturned into a branching by deleting some number of arcs or nodes, treated as a\nparameter.\n  We show that the problem can be solved via a matroid intersection formulation\nin polynomial time if the number of deleted arcs is bounded by a constant. The\norder of the polynomial time bound depends on this constant, hence the\nalgorithm does not establish fixed-parameter tractability when parameterized by\nthe number of deleted arcs. We show that a restricted version of the problem\nallows fixed-parameter tractability and hence scales well with the parameter.\nWe contrast this positive result by showing that if we parameterize by the\nnumber of deleted nodes, a somewhat more powerful parameter, the problem is not\nfixed-parameter tractable, subject to a complexity-theoretic assumption.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 15:32:42 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2012 13:36:15 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Gaspers", "Serge", ""], ["Koivisto", "Mikko", ""], ["Liedloff", "Mathieu", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1208.1723", "submitter": "Philipp Woelfel", "authors": "Abhijeet Pareek and Philipp Woelfel", "title": "RMR-Efficient Randomized Abortable Mutual Exclusion", "comments": "Extended abstract will appear at DISC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on mutual exclusion for shared-memory systems has focused on\n\"local spin\" algorithms. Performance is measured using the \"remote memory\nreferences\" (RMRs) metric. As common in recent literature, we consider a\nstandard asynchronous shared memory model with N processes, which allows atomic\nread, write and compare-and-swap (short: CAS) operations.\n  In such a model, the asymptotically tight upper and lower bound on the number\nof RMRs per passage through the Critical Section is Theta(log N) for the\noptimal deterministic algorithms (see Yang and Anderson,1995, and Attiya,\nHendler and Woelfel, 2008). Recently, several randomized algorithms have been\ndevised that break the Omega(log N) barrier and need only o(log N) RMRs per\npassage in expectation (see Hendler and Woelfel, 2010, Hendler and Woelfel,\n2011, and Bender and Gilbert, 2011). In this paper we present the first\nrandomized \"abortable\" mutual exclusion algorithm that achieves a\nsub-logarithmic expected RMR complexity. More precisely, against a weak\nadversary (which can make scheduling decisions based on the entire past\nhistory, but not the latest coin-flips of each process) every process needs an\nexpected number of O(log N/ log log N) RMRs to enter end exit the critical\nsection. If a process receives an abort-signal, it can abort an attempt to\nenter the critical section within a finite number of its own steps and by\nincurring O(log N/ log log N) RMRs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 17:51:24 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Pareek", "Abhijeet", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1208.1819", "submitter": "Peter Sarlin", "authors": "Peter Sarlin", "title": "Self-Organizing Time Map: An Abstraction of Temporal Multivariate\n  Patterns", "comments": null, "journal-ref": "Neurocomputing 99(1) (2013), pp. 496-508", "doi": "10.1016/j.neucom.2012.07.011", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adopts and adapts Kohonen's standard Self-Organizing Map (SOM) for\nexploratory temporal structure analysis. The Self-Organizing Time Map (SOTM)\nimplements SOM-type learning to one-dimensional arrays for individual time\nunits, preserves the orientation with short-term memory and arranges the arrays\nin an ascending order of time. The two-dimensional representation of the SOTM\nattempts thus twofold topology preservation, where the horizontal direction\npreserves time topology and the vertical direction data topology. This enables\ndiscovering the occurrence and exploring the properties of temporal structural\nchanges in data. For representing qualities and properties of SOTMs, we adapt\nmeasures and visualizations from the standard SOM paradigm, as well as\nintroduce a measure of temporal structural changes. The functioning of the\nSOTM, and its visualizations and quality and property measures, are illustrated\non artificial toy data. The usefulness of the SOTM in a real-world setting is\nshown on poverty, welfare and development indicators.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 06:14:19 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Sarlin", "Peter", ""]]}, {"id": "1208.2159", "submitter": "Karsten Wolf", "authors": "Karsten Wolf (Universit\\\"at Rostock, Institut f\\\"ur Informatik), Harro\n  Wimmel (Universit\\\"at Rostock, Institut f\\\"ur Informatik)", "title": "Applying CEGAR to the Petri Net State Equation", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 3 (September\n  29, 2012) lmcs:1036", "doi": "10.2168/LMCS-8(3:27)2012", "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reachability verification technique that combines the Petri net\nstate equation (a linear algebraic overapproximation of the set of reachable\nstates) with the concept of counterexample guided abstraction refinement. In\nessence, we replace the search through the set of reachable states by a search\nthrough the space of solutions of the state equation. We demonstrate the\nexcellent performance of the technique on several real-world examples. The\ntechnique is particularly useful in those cases where the reachability query\nyields a negative result: While state space based techniques need to fully\nexpand the state space in this case, our technique often terminates promptly.\nIn addition, we can derive some diagnostic information in case of\nunreachability while state space methods can only provide witness paths in the\ncase of reachability.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 12:51:32 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 12:34:02 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Wolf", "Karsten", "", "Universit\u00e4t Rostock, Institut f\u00fcr Informatik"], ["Wimmel", "Harro", "", "Universit\u00e4t Rostock, Institut f\u00fcr Informatik"]]}, {"id": "1208.2223", "submitter": "Shay Mozes", "authors": "Philip N. Klein, Shay Mozes and Christian Sommer", "title": "Structured Recursive Separator Decompositions for Planar Graphs in\n  Linear Time", "comments": "30 pages, 5 figures", "journal-ref": "STOC 2013", "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a planar graph G on n vertices and an integer parameter r<n, an\nr-division of G with few holes is a decomposition of G into O(n/r) regions of\nsize at most r such that each region contains at most a constant number of\nfaces that are not faces of G (also called holes), and such that, for each\nregion, the total number of vertices on these faces is O(sqrt r).\n  We provide a linear-time algorithm for computing r-divisions with few holes.\nIn fact, our algorithm computes a structure, called decomposition tree, which\nrepresents a recursive decomposition of G that includes r-divisions for\nessentially all values of r. In particular, given an exponentially increasing\nsequence r = (r_1,r_2,...), our algorithm can produce a recursive r-division\nwith few holes in linear time.\n  r-divisions with few holes have been used in efficient algorithms to compute\nshortest paths, minimum cuts, and maximum flows. Our linear-time algorithm\nimproves upon the decomposition algorithm used in the state-of-the-art\nalgorithm for minimum st-cut (Italiano, Nussbaum, Sankowski, and Wulff-Nilsen,\nSTOC 2011), removing one of the bottlenecks in the overall running time of\ntheir algorithm (analogously for minimum cut in planar and bounded-genus\ngraphs).\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 17:16:53 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 20:08:33 GMT"}, {"version": "v3", "created": "Fri, 17 May 2013 19:54:05 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Klein", "Philip N.", ""], ["Mozes", "Shay", ""], ["Sommer", "Christian", ""]]}, {"id": "1208.2294", "submitter": "Grigory Yaroslavtsev", "authors": "Sofya Raskhodnikova and Grigory Yaroslavtsev", "title": "Learning pseudo-Boolean k-DNF and Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any submodular function f: {0,1}^n -> {0,1,...,k} can be\nrepresented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are a\nnatural generalization of DNF representation for functions with integer range.\nEach term in such a formula has an associated integral constant. We show that\nan analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if all\nconstants associated with the terms of the formula are bounded.\n  This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs to\npseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membership\nqueries under the uniform distribution for submodular functions of the form\nf:{0,1}^n -> {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k\n\\log k / \\epsilon)}, 1/\\epsilon and log(1/\\delta) and works even in the\nagnostic setting. The line of previous work on learning submodular functions\n[Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi,\nKlivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity for\nlearning submodular functions in this setting, for fixed epsilon and delta.\n  Our learning algorithm implies a property tester for submodularity of\nfunctions f:{0,1}^n -> {0, ..., k} with query complexity polynomial in n for\nk=O((\\log n/ \\loglog n)^{1/2}) and constant proximity parameter \\epsilon.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 22:22:14 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Raskhodnikova", "Sofya", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1208.2318", "submitter": "Markus Wagner", "authors": "Olaf Mersmann, Bernd Bischl, Heike Trautmann, Markus Wagner, Frank\n  Neumann", "title": "A Novel Feature-Based Approach to Characterize Algorithm Performance for\n  the Traveling Salesman Problem", "comments": "33 pages, 17 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-heuristics are frequently used to tackle NP-hard combinatorial\noptimization problems. With this paper we contribute to the understanding of\nthe success of 2-opt based local search algorithms for solving the traveling\nsalesman problem (TSP). Although 2-opt is widely used in practice, it is hard\nto understand its success from a theoretical perspective. We take a statistical\napproach and examine the features of TSP instances that make the problem either\nhard or easy to solve. As a measure of problem difficulty for 2-opt we use the\napproximation ratio that it achieves on a given instance. Our investigations\npoint out important features that make TSP instances hard or easy to be\napproximated by 2-opt.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2012 07:31:50 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Mersmann", "Olaf", ""], ["Bischl", "Bernd", ""], ["Trautmann", "Heike", ""], ["Wagner", "Markus", ""], ["Neumann", "Frank", ""]]}, {"id": "1208.2329", "submitter": "Taisuke Izumi", "authors": "Taisuke Izumi, Tadashi Wadayama", "title": "A New Direction for Counting Perfect Matchings", "comments": "The 53rd Annual Symposium on Foundations of Computer Science (FOCS\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new exact algorithm for counting perfect\nmatchings, which relies on neither inclusion-exclusion principle nor\ntree-decompositions. For any bipartite graph of $2n$ nodes and $\\Delta n$ edges\nsuch that $\\Delta \\geq 3$, our algorithm runs with $O^{\\ast}(2^{(1 - 1/O(\\Delta\n\\log \\Delta))n})$ time and exponential space. Compared to the previous\nalgorithms, it achieves a better time bound in the sense that the performance\ndegradation to the increase of $\\Delta$ is quite slower. The main idea of our\nalgorithm is a new reduction to the problem of computing the cut-weight\ndistribution of the input graph. The primary ingredient of this reduction is\nMacWilliams Identity derived from elementary coding theory. The whole of our\nalgorithm is designed by combining that reduction with a non-trivial fast\nalgorithm computing the cut-weight distribution. To the best of our knowledge,\nthe approach posed in this paper is new and may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2012 09:04:59 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Izumi", "Taisuke", ""], ["Wadayama", "Tadashi", ""]]}, {"id": "1208.2447", "submitter": "Rishi Gupta", "authors": "Rishi Gupta, Piotr Indyk, Eric Price, and Yaron Rachlin", "title": "Compressive Sensing with Local Geometric Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a framework for compressive sensing of images with local\ndistinguishable objects, such as stars, and apply it to solve a problem in\ncelestial navigation. Specifically, let x be an N-pixel real-valued image,\nconsisting of a small number of local distinguishable objects plus noise. Our\ngoal is to design an m-by-N measurement matrix A with m << N, such that we can\nrecover an approximation to x from the measurements Ax.\n  We construct a matrix A and recovery algorithm with the following properties:\n(i) if there are k objects, the number of measurements m is O((k log N)/(log\nk)), undercutting the best known bound of O(k log(N/k)) (ii) the matrix A is\nvery sparse, which is important for hardware implementations of compressive\nsensing algorithms, and (iii) the recovery algorithm is empirically fast and\nruns in time polynomial in k and log(N).\n  We also present a comprehensive study of the application of our algorithm to\nattitude determination, or finding one's orientation in space. Spacecraft\ntypically use cameras to acquire an image of the sky, and then identify stars\nin the image to compute their orientation. Taking pictures is very expensive\nfor small spacecraft, since camera sensors use a lot of power. Our algorithm\noptically compresses the image before it reaches the camera's array of pixels,\nreducing the number of sensors that are required.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2012 18:12:58 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Gupta", "Rishi", ""], ["Indyk", "Piotr", ""], ["Price", "Eric", ""], ["Rachlin", "Yaron", ""]]}, {"id": "1208.2543", "submitter": "Dennis Luxen", "authors": "Dennis Luxen and Dennis Schieferdecker", "title": "Doing More for Less -- Cache-Aware Parallel Contraction Hierarchies\n  Preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contraction Hierarchies is a successful speedup-technique to Dijkstra's\nseminal shortest path algorithm that has a convenient trade-off between\npreprocessing and query times. We investigate a shared-memory parallel\nimplementation that uses $O(n+m)$ space for storing the graph and O(1) space\nfor each core during preprocessing. The presented data structures and\nalgorithms consequently exploits cache locality and thus exhibit competitive\npreprocessing times. The presented implementation is especially suitable for\npreprocessing graphs of planet-wide scale in practice. Also, our experiments\nshow that optimal data structures in the PRAM model can be beaten in practice\nby exploiting memory cache hierarchies.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 10:48:39 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Luxen", "Dennis", ""], ["Schieferdecker", "Dennis", ""]]}, {"id": "1208.2596", "submitter": "Patrick Jaillet", "authors": "Patrick Jaillet, Xin Lu", "title": "Near-Optimal Online Algorithms for Dynamic Resource Allocation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a general online linear programming problem whose\nformulation encompasses many practical dynamic resource allocation problems,\nincluding internet advertising display applications, revenue management,\nvarious routing, packing, and auction problems. We propose a model, which under\nmild assumptions, allows us to design near-optimal learning-based online\nalgorithms that do not require the a priori knowledge about the total number of\nonline requests to come, a first of its kind. We then consider two variants of\nthe problem that relax the initial assumptions imposed on the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 14:22:13 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Jaillet", "Patrick", ""], ["Lu", "Xin", ""]]}, {"id": "1208.2724", "submitter": "Monik  Khare", "authors": "Monik Khare, Neal E. Young", "title": "Caching with rental cost and zapping", "comments": "Caching with rental cost, caching with zapping", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{file caching} problem is defined as follows. Given a cache of size\n$k$ (a positive integer), the goal is to minimize the total retrieval cost for\nthe given sequence of requests to files. A file $f$ has size $size(f)$ (a\npositive integer) and retrieval cost $cost(f)$ (a non-negative number) for\nbringing the file into the cache. A \\emph{miss} or \\emph{fault} occurs when the\nrequested file is not in the cache and the file has to be retrieved into the\ncache by paying the retrieval cost, and some other file may have to be removed\n(\\emph{evicted}) from the cache so that the total size of the files in the\ncache does not exceed $k$.\n  We study the following variants of the online file caching problem.\n\\textbf{\\emph{Caching with Rental Cost} (or \\emph{Rental Caching})}: There is a\nrental cost $\\lambda$ (a positive number) for each file in the cache at each\ntime unit. The goal is to minimize the sum of the retrieval costs and the\nrental costs. \\textbf{\\emph{Caching with Zapping}}: A file can be \\emph{zapped}\nby paying a zapping cost $N \\ge 1$. Once a file is zapped, all future requests\nof the file don't incur any cost. The goal is to minimize the sum of the\nretrieval costs and the zapping costs.\n  We study these two variants and also the variant which combines these two\n(rental caching with zapping). We present deterministic lower and upper bounds\nin the competitive-analysis framework. We study and extend the online covering\nalgorithm from \\citep{young02online} to give deterministic online algorithms.\nWe also present randomized lower and upper bounds for some of these problems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 22:49:00 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 05:07:27 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2012 18:26:43 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2012 19:27:13 GMT"}, {"version": "v5", "created": "Thu, 18 Oct 2012 19:05:01 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Khare", "Monik", ""], ["Young", "Neal E.", ""]]}, {"id": "1208.2761", "submitter": "EPTCS", "authors": "Hiroshi Umeo (Univ. of Osaka Electro-Communication, Japan), Kinuo\n  Nishide (Univ. of Osaka Electro-Communication, Japan), Keisuke Kubo (Univ. of\n  Osaka Electro-Communication, Japan)", "title": "A Simple Optimum-Time FSSP Algorithm for Multi-Dimensional Cellular\n  Automata", "comments": "In Proceedings AUTOMATA&JAC 2012, arXiv:1208.2498", "journal-ref": "EPTCS 90, 2012, pp. 151-165", "doi": "10.4204/EPTCS.90.13", "report-no": null, "categories": "cs.FL cs.DM cs.DS nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The firing squad synchronization problem (FSSP) on cellular automata has been\nstudied extensively for more than forty years, and a rich variety of\nsynchronization algorithms have been proposed for not only one-dimensional\narrays but two-dimensional arrays. In the present paper, we propose a simple\nrecursive-halving based optimum-time synchronization algorithm that can\nsynchronize any rectangle arrays of size m*n with a general at one corner in\nm+n+max(m, n)-3 steps. The algorithm is a natural expansion of the well-known\nFSSP algorithm proposed by Balzer [1967], Gerken [1987], and Waksman [1966] and\nit can be easily expanded to three-dimensional arrays, even to\nmulti-dimensional arrays with a general at any position of the array.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 01:55:26 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Umeo", "Hiroshi", "", "Univ. of Osaka Electro-Communication, Japan"], ["Nishide", "Kinuo", "", "Univ. of Osaka Electro-Communication, Japan"], ["Kubo", "Keisuke", "", "Univ. of\n  Osaka Electro-Communication, Japan"]]}, {"id": "1208.2832", "submitter": "Sergey Yakhontov V", "authors": "Sergey V. Yakhontov", "title": "Time- and space-efficient evaluation of the complex exponential function\n  using series expansion", "comments": null, "journal-ref": "Vestnik St.Peterburg University. Ser. 10. 2011. Issue 4. P.\n  105-118", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for the evaluation of the complex exponential function is\nproposed which is quasi-linear in time and linear in space. This algorithm is\nbased on a modified binary splitting method for the hypergeometric series and a\nmodified Karatsuba method for the fast evaluation of the exponential function.\nThe time complexity of this algorithm is equal to that of the ordinary\nalgorithm for the evaluation of the exponential function based on the series\nexpansion: O(M(n)log(n)^2).\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 11:31:16 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Yakhontov", "Sergey V.", ""]]}, {"id": "1208.2846", "submitter": "Kasper Green Larsen", "authors": "Joshua Brody and Kasper Green Larsen", "title": "Adapt or Die: Polynomial Lower Bounds for Non-Adaptive Dynamic Data\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the role non-adaptivity plays in maintaining dynamic\ndata structures. Roughly speaking, a data structure is non-adaptive if the\nmemory locations it reads and/or writes when processing a query or update\ndepend only on the query or update and not on the contents of previously read\ncells. We study such non-adaptive data structures in the cell probe model. This\nmodel is one of the least restrictive lower bound models and in particular,\ncell probe lower bounds apply to data structures developed in the popular\nword-RAM model. Unfortunately, this generality comes at a high cost: the\nhighest lower bound proved for any data structure problem is only\npolylogarithmic. Our main result is to demonstrate that one can in fact obtain\npolynomial cell probe lower bounds for non-adaptive data structures.\n  To shed more light on the seemingly inherent polylogarithmic lower bound\nbarrier, we study several different notions of non-adaptivity and identify key\nproperties that must be dealt with if we are to prove polynomial lower bounds\nwithout restrictions on the data structures.\n  Finally, our results also unveil an interesting connection between data\nstructures and depth-2 circuits. This allows us to translate conjectured hard\ndata structure problems into good candidates for high circuit lower bounds; in\nparticular, in the area of linear circuits for linear operators. Building on\nlower bound proofs for data structures in slightly more restrictive models, we\nalso present a number of properties of linear operators which we believe are\nworth investigating in the realm of circuit lower bounds.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 12:34:25 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 06:17:17 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Brody", "Joshua", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1208.2936", "submitter": "Vitaly Skachek", "authors": "Vincent Gripon, Vitaly Skachek, and Michael Rabbat", "title": "Forwarding Without Repeating: Efficient Rumor Spreading in\n  Bounded-Degree Graphs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a gossip protocol called forwarding without repeating (FWR). The\nobjective is to spread multiple rumors over a graph as efficiently as possible.\nFWR accomplishes this by having nodes record which messages they have forwarded\nto each neighbor, so that each message is forwarded at most once to each\nneighbor. We prove that FWR spreads a rumor over a strongly connected digraph,\nwith high probability, in time which is within a constant factor of optimal for\ndigraphs with bounded out-degree. Moreover, on digraphs with bounded out-degree\nand bounded number of rumors, the number of transmissions required by FWR is\narbitrarily better than that of existing approaches. Specifically, FWR requires\nO(n) messages on bounded-degree graphs with n nodes, whereas classical\nforwarding and an approach based on network coding both require {\\omega}(n)\nmessages. Our results are obtained using combinatorial and probabilistic\narguments. Notably, they do not depend on expansion properties of the\nunderlying graph, and consequently the message complexity of FWR is arbitrarily\nbetter than classical forwarding even on constant-degree expander graphs, as n\n\\rightarrow \\infty. In resource-constrained applications, where each\ntransmission consumes battery power and bandwidth, our results suggest that\nusing a small amount of memory at each node leads to a significant savings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 18:16:36 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Gripon", "Vincent", ""], ["Skachek", "Vitaly", ""], ["Rabbat", "Michael", ""]]}, {"id": "1208.2956", "submitter": "Alan Guo", "authors": "Andrea Campagna, Alan Guo, Ronitt Rubinfeld", "title": "Local reconstructors and tolerant testers for connectivity and diameter", "comments": "21 pages, updated abstract, improved exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A local property reconstructor for a graph property is an algorithm which,\ngiven oracle access to the adjacency list of a graph that is \"close\" to having\nthe property, provides oracle access to the adjacency matrix of a \"correction\"\nof the graph, i.e. a graph which has the property and is close to the given\ngraph. For this model, we achieve local property reconstructors for the\nproperties of connectivity and $k$-connectivity in undirected graphs, and the\nproperty of strong connectivity in directed graphs. Along the way, we present a\nmethod of transforming a local reconstructor (which acts as a \"adjacency matrix\noracle\" for the corrected graph) into an \"adjacency list oracle\". This allows\nus to recursively use our local reconstructor for $(k-1)$-connectivity to\nobtain a local reconstructor for $k$-connectivity.\n  We also extend this notion of local property reconstruction to parametrized\ngraph properties (for instance, having diameter at most $D$ for some parameter\n$D$) and require that the corrected graph has the property with parameter close\nto the original. We obtain a local reconstructor for the low diameter property,\nwhere if the original graph is close to having diameter $D$, then the corrected\ngraph has diameter roughly 2D.\n  We also exploit a connection between local property reconstruction and\nproperty testing, observed by Brakerski, to obtain new tolerant property\ntesters for all of the aforementioned properties. Except for the one for\nconnectivity, these are the first tolerant property testers for these\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 19:46:41 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2013 14:36:32 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Campagna", "Andrea", ""], ["Guo", "Alan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1208.3054", "submitter": "Marek Cygan", "authors": "Marek Cygan, MohammadTaghi Hajiaghayi, Samir Khuller", "title": "LP Rounding for k-Centers with Non-uniform Hard Capacities", "comments": "To appear in FOCS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a generalization of the classical k-center problem\nwith capacities. Our goal is to select k centers in a graph, and assign each\nnode to a nearby center, so that we respect the capacity constraints on\ncenters. The objective is to minimize the maximum distance a node has to travel\nto get to its assigned center. This problem is NP-hard, even when centers have\nno capacity restrictions and optimal factor 2 approximation algorithms are\nknown. With capacities, when all centers have identical capacities, a 6\napproximation is known with no better lower bounds than for the infinite\ncapacity version.\n  While many generalizations and variations of this problem have been studied\nextensively, no progress was made on the capacitated version for a general\ncapacity function. We develop the first constant factor approximation algorithm\nfor this problem. Our algorithm uses an LP rounding approach to solve this\nproblem, and works for the case of non-uniform hard capacities, when multiple\ncopies of a node may not be chosen and can be extended to the case when there\nis a hard bound on the number of copies of a node that may be selected. In\naddition we establish a lower bound on the integrality gap of 7(5) for\nnon-uniform (uniform) hard capacities. In addition we prove that if there is a\n(3-eps)-factor approximation for this problem then P=NP.\n  Finally, for non-uniform soft capacities we present a much simpler\n11-approximation algorithm, which we find as one more evidence that hard\ncapacities are much harder to deal with.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 08:20:10 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Cygan", "Marek", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Khuller", "Samir", ""]]}, {"id": "1208.3071", "submitter": "Anisur Molla Rahaman", "authors": "Atish Das Sarma, Anisur Rahaman Molla, Gopal Pandurangan, Eli Upfal", "title": "Fast Distributed PageRank Computation", "comments": "14 pages", "journal-ref": "Theoretical Computer Science, Volume 561, Pages 113-121, 2015", "doi": "10.1016/j.tcs.2014.04.003", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, PageRank has gained importance in a wide range of\napplications and domains, ever since it first proved to be effective in\ndetermining node importance in large graphs (and was a pioneering idea behind\nGoogle's search engine). In distributed computing alone, PageRank vector, or\nmore generally random walk based quantities have been used for several\ndifferent applications ranging from determining important nodes, load\nbalancing, search, and identifying connectivity structures. Surprisingly,\nhowever, there has been little work towards designing provably efficient\nfully-distributed algorithms for computing PageRank. The difficulty is that\ntraditional matrix-vector multiplication style iterative methods may not always\nadapt well to the distributed setting owing to communication bandwidth\nrestrictions and convergence rates.\n  In this paper, we present fast random walk-based distributed algorithms for\ncomputing PageRanks in general graphs and prove strong bounds on the round\ncomplexity. We first present a distributed algorithm that takes $O\\big(\\log\nn/\\eps \\big)$ rounds with high probability on any graph (directed or\nundirected), where $n$ is the network size and $\\eps$ is the reset probability\nused in the PageRank computation (typically $\\eps$ is a fixed constant). We\nthen present a faster algorithm that takes $O\\big(\\sqrt{\\log n}/\\eps \\big)$\nrounds in undirected graphs. Both of the above algorithms are scalable, as each\nnode sends only small ($\\polylog n$) number of bits over each edge per round.\nTo the best of our knowledge, these are the first fully distributed algorithms\nfor computing PageRank vector with provably efficient running time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 09:36:02 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 10:55:10 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Sarma", "Atish Das", ""], ["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""], ["Upfal", "Eli", ""]]}, {"id": "1208.3313", "submitter": "Tomasz Kociumaka", "authors": "Maxime Crochemore, Costas Iliopoulos, Tomasz Kociumaka, Marcin Kubica,\n  Jakub Pachocki, Jakub Radoszewski, Wojciech Rytter, Wojciech Tyczy\\'nski,\n  Tomasz Wale\\'n", "title": "A Note on Efficient Computation of All Abelian Periods in a String", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a simple efficient algorithm for Abelian periods knowing all\nAbelian squares in a string. An efficient algorithm for the latter problem was\ngiven by Cummings and Smyth in 1997. By the way we show an alternative\nalgorithm for Abelian squares. We also obtain a linear time algorithm finding\nall `long' Abelian periods. The aim of the paper is a (new) reduction of the\nproblem of all Abelian periods to that of (already solved) all Abelian squares\nwhich provides new insight into both connected problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 08:33:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas", ""], ["Kociumaka", "Tomasz", ""], ["Kubica", "Marcin", ""], ["Pachocki", "Jakub", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Tyczy\u0144ski", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1208.3384", "submitter": "Pankaj Agarwal", "authors": "Pankaj K. Agarwal, Jiri Matousek, Micha Sharir", "title": "On Range Searching with Semialgebraic Sets II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a set of $n$ points in $\\R^d$. We present a linear-size data\nstructure for answering range queries on $P$ with constant-complexity\nsemialgebraic sets as ranges, in time close to $O(n^{1-1/d})$. It essentially\nmatches the performance of similar structures for simplex range searching, and,\nfor $d\\ge 5$, significantly improves earlier solutions by the first two authors\nobtained in~1994. This almost settles a long-standing open problem in range\nsearching.\n  The data structure is based on the polynomial-partitioning technique of Guth\nand Katz [arXiv:1011.4105], which shows that for a parameter $r$, $1 < r \\le\nn$, there exists a $d$-variate polynomial $f$ of degree $O(r^{1/d})$ such that\neach connected component of $\\R^d\\setminus Z(f)$ contains at most $n/r$ points\nof $P$, where $Z(f)$ is the zero set of $f$. We present an efficient randomized\nalgorithm for computing such a polynomial partition, which is of independent\ninterest and is likely to have additional applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 14:42:38 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 03:01:47 GMT"}, {"version": "v3", "created": "Thu, 30 May 2013 18:35:17 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Matousek", "Jiri", ""], ["Sharir", "Micha", ""]]}, {"id": "1208.3629", "submitter": "Hang Zhou", "authors": "Marc Lelarge and Hang Zhou", "title": "Sublinear-Time Algorithms for Monomer-Dimer Systems on Bounded Degree\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $G$, let $Z(G,\\lambda)$ be the partition function of the\nmonomer-dimer system defined by $\\sum_k m_k(G)\\lambda^k$, where $m_k(G)$ is the\nnumber of matchings of size $k$ in $G$. We consider graphs of bounded degree\nand develop a sublinear-time algorithm for estimating $\\log Z(G,\\lambda)$ at an\narbitrary value $\\lambda>0$ within additive error $\\epsilon n$ with high\nprobability. The query complexity of our algorithm does not depend on the size\nof $G$ and is polynomial in $1/\\epsilon$, and we also provide a lower bound\nquadratic in $1/\\epsilon$ for this problem. This is the first analysis of a\nsublinear-time approximation algorithm for a $# P$-complete problem. Our\napproach is based on the correlation decay of the Gibbs distribution associated\nwith $Z(G,\\lambda)$. We show that our algorithm approximates the probability\nfor a vertex to be covered by a matching, sampled according to this Gibbs\ndistribution, in a near-optimal sublinear time. We extend our results to\napproximate the average size and the entropy of such a matching within an\nadditive error with high probability, where again the query complexity is\npolynomial in $1/\\epsilon$ and the lower bound is quadratic in $1/\\epsilon$.\nOur algorithms are simple to implement and of practical use when dealing with\nmassive datasets. Our results extend to other systems where the correlation\ndecay is known to hold as for the independent set problem up to the critical\nactivity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 16:11:27 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2012 21:07:44 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2013 21:01:49 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2013 10:58:30 GMT"}, {"version": "v5", "created": "Wed, 4 Sep 2013 07:49:39 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Lelarge", "Marc", ""], ["Zhou", "Hang", ""]]}, {"id": "1208.3663", "submitter": "Matias Korman", "authors": "Luis Barba, Matias Korman, Stefan Langerman, Kunikiko Sadakane and\n  Rodrigo Silveira", "title": "Space-Time Trade-offs for Stack-Based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In memory-constrained algorithms we have read-only access to the input, and\nthe number of additional variables is limited. In this paper we introduce the\ncompressed stack technique, a method that allows to transform algorithms whose\nspace bottleneck is a stack into memory-constrained algorithms. Given an\nalgorithm \\alg\\ that runs in O(n) time using $\\Theta(n)$ variables, we can\nmodify it so that it runs in $O(n^2/s)$ time using a workspace of O(s)\nvariables (for any $s\\in o(\\log n)$) or $O(n\\log n/\\log p)$ time using $O(p\\log\nn/\\log p)$ variables (for any $2\\leq p\\leq n$). We also show how the technique\ncan be applied to solve various geometric problems, namely computing the convex\nhull of a simple polygon, a triangulation of a monotone polygon, the shortest\npath between two points inside a monotone polygon, 1-dimensional pyramid\napproximation of a 1-dimensional vector, and the visibility profile of a point\ninside a simple polygon. Our approach exceeds or matches the best-known results\nfor these problems in constant-workspace models (when they exist), and gives\nthe first trade-off between the size of the workspace and running time. To the\nbest of our knowledge, this is the first general framework for obtaining\nmemory-constrained algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 19:39:34 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2012 17:20:35 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2013 13:11:26 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2013 08:04:14 GMT"}, {"version": "v5", "created": "Wed, 25 Jun 2014 14:08:38 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Barba", "Luis", ""], ["Korman", "Matias", ""], ["Langerman", "Stefan", ""], ["Sadakane", "Kunikiko", ""], ["Silveira", "Rodrigo", ""]]}, {"id": "1208.3747", "submitter": "Pavlos Efraimidis", "authors": "Pavlos S. Efraimidis, Remous-Aris Koutsiamanis", "title": "On Money as a Means of Coordination between Network Packets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we apply a common economic tool, namely money, to coordinate\nnetwork packets. In particular, we present a network economy, called\nPacketEconomy, where each flow is modeled as a population of rational network\npackets, and these packets can self-regulate their access to network resources\nby mutually trading their positions in router queues. Every packet of the\neconomy has its price, and this price determines if and when the packet will\nagree to buy or sell a better position. We consider a corresponding Markov\nmodel of trade and show that there are Nash equilibria (NE) where queue\npositions and money are exchanged directly between the network packets. This\nsimple approach, interestingly, delivers improvements even when fiat money is\nused. We present theoretical arguments and experimental results to support our\nclaims.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 14:11:17 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Efraimidis", "Pavlos S.", ""], ["Koutsiamanis", "Remous-Aris", ""]]}, {"id": "1208.3798", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz", "title": "On-line Indexing for General Alphabets via Predecessor Queries on\n  Subsets of an Ordered List", "comments": "Accepted to FOCS 2012, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Text Indexing is a fundamental algorithmic problem in which\none wishes to preprocess a text in order to quickly locate pattern queries\nwithin the text. In the ever evolving world of dynamic and on-line data, there\nis also a need for developing solutions to index texts which arrive on-line,\ni.e. a character at a time, and still be able to quickly locate said patterns.\nIn this paper, a new solution for on-line indexing is presented by providing an\non-line suffix tree construction in $O(\\log \\log n + \\log\\log |\\Sigma|)$\nworst-case expected time per character, where $n$ is the size of the string,\nand $\\Sigma$ is the alphabet. This improves upon all previously known on-line\nsuffix tree constructions for general alphabets, at the cost of having the run\ntime in expectation.\n  The main idea is to reduce the problem of constructing a suffix tree on-line\nto an interesting variant of the order maintenance problem, which may be of\nindependent interest. In the famous order maintenance problem, one wishes to\nmaintain a dynamic list $L$ of size $n$ under insertions, deletions, and order\nqueries. In an order query, one is given two nodes from $L$ and must determine\nwhich node precedes the other in $L$. In the Predecessor search on Dynamic\nSubsets of an Ordered Dynamic List problem (POLP) it is also necessary to\nmaintain dynamic subsets of $L$ such that given some $u\\in L$ it will be\npossible to quickly locate the predecessor of $u$ in any subset. This paper\nprovides an efficient data structure capable of solving the POLP with\nworst-case expected bounds that match the currently best known bounds for\npredecessor search in the RAM model, improving over a solution which may be\nimplicitly obtained from Dietz [Die89].\n  Furthermore, this paper improves or simplifies bounds for several additional\napplications, including fully-persistent arrays and the Order-Maintenance\nProblem.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 01:26:33 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Kopelowitz", "Tsvi", ""]]}, {"id": "1208.3835", "submitter": "Kewen Liao", "authors": "Kewen Liao, Hong Shen and Longkun Guo", "title": "Constrained Fault-Tolerant Resource Allocation", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Constrained Fault-Tolerant Resource Allocation (FTRA) problem, we are\ngiven a set of sites containing facilities as resources, and a set of clients\naccessing these resources. Specifically, each site i is allowed to open at most\nR_i facilities with cost f_i for each opened facility. Each client j requires\nan allocation of r_j open facilities and connecting j to any facility at site i\nincurs a connection cost c_ij. The goal is to minimize the total cost of this\nresource allocation scenario.\n  FTRA generalizes the Unconstrained Fault-Tolerant Resource Allocation\n(FTRA_{\\infty}) [18] and the classical Fault-Tolerant Facility Location (FTFL)\n[13] problems: for every site i, FTRA_{\\infty} does not have the constraint\nR_i, whereas FTFL sets R_i=1. These problems are said to be uniform if all\nr_j's are the same, and general otherwise.\n  For the general metric FTRA, we first give an LP-rounding algorithm achieving\nthe approximation ratio of 4. Then we show the problem reduces to FTFL,\nimplying the ratio of 1.7245 from [3]. For the uniform FTRA, we provide a\n1.52-approximation primal-dual algorithm in O(n^4) time, where n is the total\nnumber of sites and clients. We also consider the Constrained Fault-Tolerant\nk-Resource Allocation (k-FTRA) problem where additionally the total number of\nfacilities can be opened across all sites is bounded by k. For the uniform\nk-FTRA, we give the first constant-factor approximation algorithm with a factor\nof 4. Note that the above results carry over to FTRA_{\\infty} and\nk-FTRA_{\\infty}.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 13:46:18 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 10:19:53 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2013 05:55:33 GMT"}, {"version": "v4", "created": "Thu, 2 May 2013 09:02:23 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Liao", "Kewen", ""], ["Shen", "Hong", ""], ["Guo", "Longkun", ""]]}, {"id": "1208.3874", "submitter": "Igor Sergeev", "authors": "Igor S. Sergeev", "title": "Upper bounds for the formula size of the majority function", "comments": "12 pages, in English; 13 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that the counting function of n Boolean variables can be\nimplemented with the formulae of size O(n^3.06) over the basis of all 2-input\nBoolean functions and of size O(n^4.54) over the standard basis. The same\nbounds follow for the complexity of any threshold symmetric function of n\nvariables and particularly for the majority function. Any bit of the product of\nbinary numbers of length n can be computed by formulae of size O(n^4.06) or\nO(n^5.54) depending on basis. Incidentally the bounds O(n^3.23) and O(n^4.82)\non the formula size of any symmetric function of n variables with respect to\nthe basis are obtained.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 18:14:55 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1208.4225", "submitter": "Jesper Nederlof", "authors": "Jesper Nederlof, Erik Jan van Leeuwen, Ruben van der Zwaan", "title": "Reducing a Target Interval to a Few Exact Queries", "comments": "10 pages, to appear at MFCS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial problems involving weights can be formulated as a\nso-called ranged problem. That is, their input consists of a universe $U$, a\n(succinctly-represented) set family $\\mathcal{F} \\subseteq 2^{U}$, a weight\nfunction $\\omega:U \\rightarrow \\{1,...,N\\}$, and integers $0 \\leq l \\leq u \\leq\n\\infty$. Then the problem is to decide whether there is an $X \\in \\mathcal{F}$\nsuch that $l \\leq \\sum_{e \\in X}\\omega(e) \\leq u$. Well-known examples of such\nproblems include Knapsack, Subset Sum, Maximum Matching, and Traveling\nSalesman. In this paper, we develop a generic method to transform a ranged\nproblem into an exact problem (i.e. a ranged problem for which $l=u$). We show\nthat our method has several intriguing applications in exact exponential\nalgorithms and parameterized complexity, namely:\n  - In exact exponential algorithms, we present new insight into whether Subset\nSum and Knapsack have efficient algorithms in both time and space. In\nparticular, we show that the time and space complexity of Subset Sum and\nKnapsack are equivalent up to a small polynomial factor in the input size. We\nalso give an algorithm that solves sparse instances of Knapsack efficiently in\nterms of space and time. - In parameterized complexity, we present the first\nkernelization results on weighted variants of several well-known problems. In\nparticular, we show that weighted variants of Vertex Cover, Dominating Set,\nTraveling Salesman and Knapsack all admit polynomial randomized Turing kernels\nwhen parameterized by $|U|$.\n  Curiously, our method relies on a technique more commonly found in\napproximation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 09:28:50 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Nederlof", "Jesper", ""], ["van Leeuwen", "Erik Jan", ""], ["van der Zwaan", "Ruben", ""]]}, {"id": "1208.4238", "submitter": "Enrico Siragusa", "authors": "Enrico Siragusa, David Weese, Knut Reinert", "title": "Fast and sensitive read mapping with approximate seeds and multiple\n  backtracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Masai, a read mapper representing the state of the art in terms of\nspeed and sensitivity. Our tool is an order of magnitude faster than RazerS 3\nand mrFAST, 2--3 times faster and more accurate than Bowtie 2 and BWA. The\nnovelties of our read mapper are filtration with approximate seeds and a method\nfor multiple backtracking. Approximate seeds, compared to exact seeds, increase\nfiltration specificity while preserving sensitivity. Multiple backtracking\namortizes the cost of searching a large set of seeds by taking advantage of the\nrepetitiveness of next-generation sequencing data. Combined together, these two\nmethods significantly speed up approximate search on genomic datasets. Masai is\nimplemented in C++ using the SeqAn library. The source code is distributed\nunder the BSD license and binaries for Linux, Mac OS X and Windows can be\nfreely downloaded from http://www.seqan.de/projects/masai.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 11:08:06 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Siragusa", "Enrico", ""], ["Weese", "David", ""], ["Reinert", "Knut", ""]]}, {"id": "1208.4449", "submitter": "Micha{\\l} Pilipczuk", "authors": "Marcin Pilipczuk, Micha{\\l} Pilipczuk", "title": "Finding a maximum induced degenerate subgraph faster than 2^n", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of finding a maximum induced d-degenerate\nsubgraph in a given n-vertex graph from the point of view of exact algorithms.\nWe show that for any fixed d one can find a maximum induced d-degenerate\nsubgraph in randomized (2-eps_d)^n n^O(1) time, for some constant eps_d>0\ndepending only on d. Moreover, our algorithm can be used to sample\ninclusion-wise maximal induced d-degenerate subgraphs in such a manner that\nevery such subgraph is output with probability at least (2-eps_d)^-n; hence, we\nprove that their number is bounded by (2-eps_d)^n.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 09:07:12 GMT"}], "update_date": "2012-08-23", "authors_parsed": [["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1208.4511", "submitter": "Yufei Tao", "authors": "Yufei Tao, Jeonghun Yoon", "title": "Optimal Planar Range Skyline Reporting with Linear Space in External\n  Memory", "comments": "This article, after a merge with an article by Casper\n  Kejlberg-Rasmussen, Konstantinos Tsakalidis, and Kostas Tsichlas, has appeard\n  in PODS'13. The merged article can be found at http://arxiv.org/abs/1306.2815", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let P be a set of n points in R^2. Given a rectangle Q = [\\alpha_1, \\alpha_2]\nx [\\beta_1, \\beta_2], a range skyline query returns the maxima of the points in\nP \\cap Q. An important variant is the so-called top-open queries, where Q is a\n3-sided rectangle whose upper edge is grounded at y = \\infty (that is, \\beta_2\n= \\infty). These queries are crucial in numerous database applications. In\ninternal memory, extensive research has been devoted to designing data\nstructures that can answer such queries efficiently. In contrast, currently\nthere is no clear understanding about their exact complexities in external\nmemory.\n  This paper presents several structures of linear size for answering the above\nqueries with the optimal I/O cost. We show that a top-open query can be solved\nin O(log_B(n) + k/B) I/Os, where B is the block size and k is the number of\npoints in the query result. The query cost can be made O(log log_B(U) + k/B)\nwhen the data points lie in a U x U grid for some integer U >= n, and further\nlowered to O(1 + k/B) if U = O(n). The same efficiency also applies to 3-sided\nqueries where Q is a right-open rectangle. However, the hardness of the problem\nincreases if Q is a left- or bottom-open 3-sided rectangle. We prove that any\nlinear-size structure must perform \\Omega((n/B)^\\eps + k/B) I/Os to solve such\na query in the worst case, where \\eps > 0 can be an arbitrarily small constant.\nIn fact, left- and right-open queries are just as difficult as general\n(4-sided) queries, for which we give a linear-size structure with query time\nO((n/B)^\\eps + k/B). Interestingly, this indicates that 4-sided range skyline\nqueries have exactly the same hardness as 4-sided range reporting (where the\ngoal is to report simply the whole P \\cap Q). That is, the skyline requirement\ndoes not alter the problem difficulty at all.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 14:35:06 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2013 01:31:13 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Tao", "Yufei", ""], ["Yoon", "Jeonghun", ""]]}, {"id": "1208.4516", "submitter": "Yufei Tao", "authors": "Yufei Tao", "title": "A Dynamic I/O-Efficient Structure for One-Dimensional Top-k Range\n  Reporting", "comments": "In PODS'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a structure in external memory for \"top-k range reporting\", which\nuses linear space, answers a query in O(lg_B n + k/B) I/Os, and supports an\nupdate in O(lg_B n) amortized I/Os, where n is the input size, and B is the\nblock size. This improves the state of the art which incurs O(lg^2_B n)\namortized I/Os per update.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 14:50:34 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 07:28:55 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Tao", "Yufei", ""]]}, {"id": "1208.5083", "submitter": "Ian Post", "authors": "Ian Post and Yinyu Ye", "title": "The simplex method is strongly polynomial for deterministic Markov\n  decision processes", "comments": "Minor typo fixes and improvements over version 1. Appeared in SODA\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the simplex method with the highest gain/most-negative-reduced\ncost pivoting rule converges in strongly polynomial time for deterministic\nMarkov decision processes (MDPs) regardless of the discount factor. For a\ndeterministic MDP with n states and m actions, we prove the simplex method runs\nin O(n^3m^2log^2 n) iterations if the discount factor is uniform and\nO(n^5m^3log^2 n) iterations if each action has a distinct discount factor.\nPreviously the simplex method was known to run in polynomial time only for\ndiscounted MDPs where the discount was bounded away from 1 [Ye11].\n  Unlike in the discounted case, the algorithm does not greedily converge to\nthe optimum, and we require a more complex measure of progress. We identify a\nset of layers in which the values of primal variables must lie and show that\nthe simplex method always makes progress optimizing one layer, and when the\nupper layer is updated the algorithm makes a substantial amount of progress. In\nthe case of nonuniform discounts, we define a polynomial number of \"milestone\"\npolicies and we prove that, while the objective function may not improve\nsubstantially overall, the value of at least one dual variable is always making\nprogress towards some milestone, and the algorithm will reach the next\nmilestone in a polynomial number of steps.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2012 00:18:36 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2013 06:42:52 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Post", "Ian", ""], ["Ye", "Yinyu", ""]]}, {"id": "1208.5211", "submitter": "Dan Garber", "authors": "Dan Garber, Elad Hazan", "title": "Almost Optimal Sublinear Time Algorithm for Semidefinite Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for approximating semidefinite programs with running\ntime that is sublinear in the number of entries in the semidefinite instance.\nWe also present lower bounds that show our algorithm to have a nearly optimal\nrunning time.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2012 10:20:28 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Garber", "Dan", ""], ["Hazan", "Elad", ""]]}, {"id": "1208.5247", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz and Robert Krauthgamer", "title": "Faster Clustering via Preprocessing", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the efficiency of clustering a set of points, when the\nencompassing metric space may be preprocessed in advance. In computational\nproblems of this genre, there is a first stage of preprocessing, whose input is\na collection of points $M$; the next stage receives as input a query set\n$Q\\subset M$, and should report a clustering of $Q$ according to some\nobjective, such as 1-median, in which case the answer is a point $a\\in M$\nminimizing $\\sum_{q\\in Q} d_M(a,q)$.\n  We design fast algorithms that approximately solve such problems under\nstandard clustering objectives like $p$-center and $p$-median, when the metric\n$M$ has low doubling dimension. By leveraging the preprocessing stage, our\nalgorithms achieve query time that is near-linear in the query size $n=|Q|$,\nand is (almost) independent of the total number of points $m=|M|$.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2012 20:03:04 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1208.5345", "submitter": "Yngve Villanger", "authors": "Petr A. Golovach and Pinar Heggernes and Dieter Kratsch and Yngve\n  Villanger", "title": "Generating All Minimal Edge Dominating Sets with Incremental-Polynomial\n  Delay", "comments": "The incremental-polynomial delay for enumerating minimal dominating\n  sets in line graphs have been improved from O(m^4 |L|^2) to O(m^5 |L|)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an arbitrary undirected simple graph G with m edges, we give an algorithm\nwith running time O(m^4 |L|^2) to generate the set L of all minimal edge\ndominating sets of G. For bipartite graphs we obtain a better result; we show\nthat their minimal edge dominating sets can be enumerated in time O(m^4 |L|).\nIn fact our results are stronger; both algorithms generate the next minimal\nedge dominating set with incremental-polynomial delay O(m^5 |L|) and O(m^4 |L|)\nrespectively, when L is the set of already generated minimal edge dominating\nsets. Our algorithms are tailored for and solve the equivalent problems of\nenumerating minimal (vertex) dominating sets of line graphs and line graphs of\nbipartite graphs, with incremental-polynomial delay, and consequently in\noutput-polynomial time. Enumeration of minimal dominating sets in graphs has\nvery recently been shown to be equivalent to enumeration of minimal\ntransversals in hypergraphs. The question whether the minimal transversals of a\nhypergraph can be enumerated in output-polynomial time is a fundamental and\nchallenging question in Output-Sensitive Enumeration; it has been open for\nseveral decades and has triggered extensive research in the field.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2012 09:38:56 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 08:35:27 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Kratsch", "Dieter", ""], ["Villanger", "Yngve", ""]]}, {"id": "1208.5542", "submitter": "Huiwei Lv", "authors": "Huiwei Lv, Guangming Tan, Mingyu Chen, Ninghui Sun", "title": "Compression and Sieve: Reducing Communication in Parallel Breadth First\n  Search on Distributed Memory Systems", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For parallel breadth first search (BFS) algorithm on large-scale distributed\nmemory systems, communication often costs significantly more than arithmetic\nand limits the scalability of the algorithm. In this paper we sufficiently\nreduce the communication cost in distributed BFS by compressing and sieving the\nmessages. First, we leverage a bitmap compression algorithm to reduce the size\nof messages before communication. Second, we propose a novel distributed\ndirectory algorithm, cross directory, to sieve the redundant data in messages.\nExperiments on a 6,144-core SMP cluster show our algorithm outperforms the\nbaseline implementation in Graph500 by 2.2 times, reduces its communication\ntime by 79.0%, and achieves a performance rate of 12.1 GTEPS (billion edge\nvisits per second)\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 02:59:38 GMT"}], "update_date": "2012-08-29", "authors_parsed": [["Lv", "Huiwei", ""], ["Tan", "Guangming", ""], ["Chen", "Mingyu", ""], ["Sun", "Ninghui", ""]]}, {"id": "1208.5589", "submitter": "Gwena\\\"el Joret", "authors": "Jean Cardinal and Gwena\\\"el Joret", "title": "Hitting all Maximal Independent Sets of a Bipartite Graph", "comments": "v3: minor change", "journal-ref": "Algorithmica, 72/2:359--368, 2015", "doi": "10.1007/s00453-013-9847-3", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that given a bipartite graph G with vertex set V and an integer k,\ndeciding whether there exists a subset of V of size k hitting all maximal\nindependent sets of G is complete for the class Sigma_2^P.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 08:52:10 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 22:48:24 GMT"}, {"version": "v3", "created": "Fri, 27 Dec 2013 15:57:53 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Cardinal", "Jean", ""], ["Joret", "Gwena\u00ebl", ""]]}, {"id": "1208.5639", "submitter": "Shmuel Onn", "authors": "Shmuel Onn and Michal Rozenblit", "title": "Convex Integer Optimization by Constantly Many Linear Counterparts", "comments": null, "journal-ref": "Linear Algebra and its Applications 447 (2014) 88-109", "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study convex integer maximization problems with composite\nobjective functions of the form $f(Wx)$, where $f$ is a convex function on\n$\\R^d$ and $W$ is a $d\\times n$ matrix with small or binary entries, over\nfinite sets $S\\subset \\Z^n$ of integer points presented by an oracle or by\nlinear inequalities.\n  Continuing the line of research advanced by Uri Rothblum and his colleagues\non edge-directions, we introduce here the notion of {\\em edge complexity} of\n$S$, and use it to establish polynomial and constant upper bounds on the number\nof vertices of the projection $\\conv(WS)$ and on the number of linear\noptimization counterparts needed to solve the above convex problem.\n  Two typical consequences are the following. First, for any $d$, there is a\nconstant $m(d)$ such that the maximum number of vertices of the projection of\nany matroid $S\\subset\\{0,1\\}^n$ by any binary $d\\times n$ matrix $W$ is $m(d)$\nregardless of $n$ and $S$; and the convex matroid problem reduces to $m(d)$\ngreedily solvable linear counterparts. In particular, $m(2)=8$. Second, for any\n$d,l,m$, there is a constant $t(d;l,m)$ such that the maximum number of\nvertices of the projection of any three-index $l\\times m\\times n$\ntransportation polytope for any $n$ by any binary $d\\times(l\\times m\\times n)$\nmatrix $W$ is $t(d;l,m)$; and the convex three-index transportation problem\nreduces to $t(d;l,m)$ linear counterparts solvable in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 12:33:51 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Onn", "Shmuel", ""], ["Rozenblit", "Michal", ""]]}, {"id": "1208.5713", "submitter": "Sandeep Hosangadi", "authors": "Sandeep Hosangadi", "title": "Distance Measures for Sequences", "comments": "16 PAGES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of sequences, the distance between pairs of them helps us to find\ntheir similarity and derive structural relationship amongst them. For genomic\nsequences such measures make it possible to construct the evolution tree of\norganisms. In this paper we compare several distance measures and examine a\nmethod that involves circular shifting one sequence against the other for\nfinding good alignment to minimize Hamming distance. We also use run-length\nencoding together with LZ77 to characterize information in a binary sequence.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 16:49:59 GMT"}], "update_date": "2012-08-29", "authors_parsed": [["Hosangadi", "Sandeep", ""]]}, {"id": "1208.5738", "submitter": "Xiaohua Xu", "authors": "Xiaohua Xu, Xiang-Yang Li", "title": "Efficient Construction of Dominating Set in Wireless Networks", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Considering a communication topology of a wireless network modeled by a graph\nwhere an edge exists between two nodes if they are within each other's\ncommunication range. A subset $U$ of nodes is a dominating set if each node is\neither in $U$ or adjacent to some node in $U$. Assume each node has a disparate\ncommunication range and is associated with a positive weight, we present a\nrandomized algorithm to find a min-weight dominating set. Considering any\norientation of the graph where an arc $\\overrightarrow{uv}$ exists if the node\n$v$ lies in $u$'s communication range. A subset $U$ of nodes is a strongly\ndominating set if every node except $U$ has both in-neighbor(s) and\nout-neighbor(s) in $U$. We present a polynomial-time algorithm to find a\nstrongly dominating set of size at most $(2+\\epsilon)$ times of the optimum. We\nalso investigate another related problem called $K$-Coverage. Given are a set\n${\\cal D}$ of disks with positive weight and a set ${\\cal P}$ of nodes. Assume\nall input nodes lie below a horizontal line $l$ and all input disks lie above\nthis line $l$ in the plane. The objective is to find a min-weight subset ${\\cal\nD}'\\subseteq {\\cal D}$ of disks such that each node is covered at least $K$\ndisks in ${\\cal D}'$. We propose a novel two-approximation algorithm for this\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 22:44:27 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Xu", "Xiaohua", ""], ["Li", "Xiang-Yang", ""]]}, {"id": "1208.5907", "submitter": "Etienne Birmel\\'e", "authors": "Etienne Birmel\\'e", "title": "Choose Outsiders First: a mean 2-approximation random algorithm for\n  covering problems", "comments": "8 pages The paper has been withdrawn due to an error in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high number of discrete optimization problems, including Vertex Cover, Set\nCover or Feedback Vertex Set, can be unified into the class of covering\nproblems. Several of them were shown to be inapproximable by deterministic\nalgorithms. This article proposes a new random approach, called Choose\nOutsiders First, which consists in selecting randomly ele- ments which are\nexcluded from the cover. We show that this approach leads to random outputs\nwhich mean size is at most twice the optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2012 13:16:40 GMT"}, {"version": "v2", "created": "Mon, 13 May 2013 14:26:59 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Birmel\u00e9", "Etienne", ""]]}, {"id": "1208.5956", "submitter": "Peter G. Doyle", "authors": "Vaughan R. Pratt", "title": "A combinatorial analysis of the average time for open-address hash\n  coding insertion", "comments": "December 1973, revised 1974", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analysing a well-known hash-coding method, Knuth gave an exact expression\nfor the average number of rejections encountered by players of a variant of\nmusical chairs. We study a variant more closely related to musical chairs\nitself and deduce the same expression by a purely combinatorial approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2012 16:11:49 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Pratt", "Vaughan R.", ""]]}, {"id": "1208.6051", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Fabian Kuhn", "title": "Lower Bounds on Information Dissemination in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study lower bounds on information dissemination in adversarial dynamic\nnetworks. Initially, k pieces of information (henceforth called tokens) are\ndistributed among n nodes. The tokens need to be broadcast to all nodes through\na synchronous network in which the topology can change arbitrarily from round\nto round provided that some connectivity requirements are satisfied.\n  If the network is guaranteed to be connected in every round and each node can\nbroadcast a single token per round to its neighbors, there is a simple token\ndissemination algorithm that manages to deliver all k tokens to all the nodes\nin O(nk) rounds. Interestingly, in a recent paper, Dutta et al. proved an\nalmost matching Omega(n + nk/log n) lower bound for deterministic\ntoken-forwarding algorithms that are not allowed to combine, split, or change\ntokens in any way. In the present paper, we extend this bound in different\nways.\n  If nodes are allowed to forward b < k tokens instead of only one token in\nevery round, a straight-forward extension of the O(nk) algorithm disseminates\nall k tokens in time O(nk/b). We show that for any randomized token-forwarding\nalgorithm, Omega(n + nk/(b^2 log n log log n)) rounds are necessary. If nodes\ncan only send a single token per round, but we are guaranteed that the network\ngraph is c-vertex connected in every round, we show a lower bound of\nOmega(nk/(c log^{3/2} n)), which almost matches the currently best O(nk/c)\nupper bound. Further, if the network is T-interval connected, a notion that\ncaptures connection stability over time, we prove that Omega(n + nk/(T^2 log\nn)) rounds are needed. The best known upper bound in this case manages to solve\nthe problem in O(n + nk/T) rounds. Finally, we show that even if each node only\nneeds to obtain a delta-fraction of all the tokens for some delta in [0,1],\nOmega(nk delta^3 log n) are still required.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2012 23:12:20 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1208.6125", "submitter": "Bernhard Haeupler", "authors": "Keren Censor-Hillel, Bernhard Haeupler, Nancy Lynch, Muriel M\\'edard", "title": "Bounded-Contention Coding for Wireless Networks in the High SNR Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient communication in wireless networks is typically challenged by the\npossibility of interference among several transmitting nodes. Much important\nresearch has been invested in decreasing the number of collisions in order to\nobtain faster algorithms for communication in such networks.\n  This paper proposes a novel approach for wireless communication, which\nembraces collisions rather than avoiding them, over an additive channel. It\nintroduces a coding technique called Bounded-Contention Coding (BCC) that\nallows collisions to be successfully decoded by the receiving nodes into the\noriginal transmissions and whose complexity depends on a bound on the\ncontention among the transmitters.\n  BCC enables deterministic local broadcast in a network with n nodes and at\nmost a transmitters with information of l bits each within O(a log n + al) bits\nof communication with full-duplex radios, and O((a log n + al)(log n)) bits,\nwith high probability, with half-duplex radios. When combined with random\nlinear network coding, BCC gives global broadcast within O((D + a + log n)(a\nlog n + l)) bits, with high probability. This also holds in dynamic networks\nthat can change arbitrarily over time by a worst-case adversary. When no bound\non the contention is given, it is shown how to probabilistically estimate it\nand obtain global broadcast that is adaptive to the true contention in the\nnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 10:08:29 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Haeupler", "Bernhard", ""], ["Lynch", "Nancy", ""], ["M\u00e9dard", "Muriel", ""]]}, {"id": "1208.6269", "submitter": "Hadi  Katebi", "authors": "Hadi Katebi and Karem A. Sakallah and Igor L. Markov", "title": "Conflict Anticipation in the Search for Graph Automorphisms", "comments": "15 pages, 9 Figures, 1 Table, Int'l Conf. on Logic for Programming,\n  Artificial Intelligence and Reasoning (LPAR)", "journal-ref": "H. Katebi, K. A. Sakallah and I. L. Markov, \"Conflict Anticipation\n  in the Search for Graph Automorphisms\" in Proc. Int'l Conf. on Logic for\n  Programming, Artificial Intelligence and Reasoning (LPAR), pp. 243-257,\n  Merida, Venezuela, 2012", "doi": null, "report-no": null, "categories": "cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective search for graph automorphisms allows identifying symmetries in\nmany discrete structures, ranging from chemical molecules to microprocessor\ncircuits. Using this type of structure can enhance visualization as well as\nspeed up computational optimization and verification. Competitive algorithms\nfor the graph automorphism problem are based on efficient partition refinement\naugmented with group-theoretic pruning techniques. In this paper, we improve\nprior algorithms for the graph automorphism problem by introducing simultaneous\nrefinement of multiple partitions, which enables the anticipation of future\nconflicts in search and leads to significant pruning, reducing overall\nruntimes. Empirically, we observe an exponential speedup for the family of\nMiyazaki graphs, which have been shown to impede leading graph-automorphism\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 19:10:11 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Katebi", "Hadi", ""], ["Sakallah", "Karem A.", ""], ["Markov", "Igor L.", ""]]}, {"id": "1208.6271", "submitter": "Hadi  Katebi", "authors": "Hadi Katebi and Karem A. Sakallah and Igor L. Markov", "title": "Graph Symmetry Detection and Canonical Labeling: Differences and\n  Synergies", "comments": "15 pages, 10 figures, 1 table, Turing-100", "journal-ref": "H. Katebi, K. A. Sakallah and I. L. Markov, \"Graph Symmetry\n  Detection and Canonical Labeling: Differences and Synergies'' in Proc.\n  Turing-100, EPIC vol. 10, pp. 181-195, Manchester, UK, 2012", "doi": null, "report-no": null, "categories": "cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetries of combinatorial objects are known to complicate search\nalgorithms, but such obstacles can often be removed by detecting symmetries\nearly and discarding symmetric subproblems. Canonical labeling of combinatorial\nobjects facilitates easy equivalence checking through quick matching. All\nexisting canonical labeling software also finds symmetries, but the fastest\nsymmetry-finding software does not perform canonical labeling. In this work, we\ncontrast the two problems and dissect typical algorithms to identify their\nsimilarities and differences. We then develop a novel approach to canonical\nlabeling where symmetries are found first and then used to speed up the\ncanonical labeling algorithms. Empirical results show that this approach\noutperforms state-of-the-art canonical labelers.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 19:18:49 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Katebi", "Hadi", ""], ["Sakallah", "Karem A.", ""], ["Markov", "Igor L.", ""]]}, {"id": "1208.6322", "submitter": "Fabio D'Andreagiovanni", "authors": "Christina B\\\"using and Fabio D'Andreagiovanni", "title": "New results about multi-band uncertainty in Robust Optimization", "comments": "15 pages. The present paper is a revised version of the one appeared\n  in the Proceedings of SEA 2012", "journal-ref": "Proc. of the 11th Symposium on Experimental Algorithms - SEA 2012,\n  LNCS 7276 (Springer, Heidelberg, 2012) pp. 63-74", "doi": "10.1007/978-3-642-30850-5_7", "report-no": null, "categories": "math.OC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"The Price of Robustness\" by Bertsimas and Sim represented a breakthrough in\nthe development of a tractable robust counterpart of Linear Programming\nProblems. However, the central modeling assumption that the deviation band of\neach uncertain parameter is single may be too limitative in practice:\nexperience indeed suggests that the deviations distribute also internally to\nthe single band, so that getting a higher resolution by partitioning the band\ninto multiple sub-bands seems advisable. The critical aim of our work is to\nclose the knowledge gap about the adoption of a multi-band uncertainty set in\nRobust Optimization: a general definition and intensive theoretical study of a\nmulti-band model are actually still missing. Our new developments have been\nalso strongly inspired and encouraged by our industrial partners, which have\nbeen interested in getting a better modeling of arbitrary distributions, built\non historical data of the uncertainty affecting the considered real-world\nproblems. In this paper, we study the robust counterpart of a Linear\nProgramming Problem with uncertain coefficient matrix, when a multi-band\nuncertainty set is considered. We first show that the robust counterpart\ncorresponds to a compact LP formulation. Then we investigate the problem of\nseparating cuts imposing robustness and we show that the separation can be\nefficiently operated by solving a min-cost flow problem. Finally, we test the\nperformance of our new approach to Robust Optimization on realistic instances\nof a Wireless Network Design Problem subject to uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 21:53:12 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["B\u00fcsing", "Christina", ""], ["D'Andreagiovanni", "Fabio", ""]]}, {"id": "1208.6589", "submitter": "Kristan Temme", "authors": "Kristan Temme and Pawel Wocjan", "title": "Efficient Computation of the Permanent of Block Factorizable Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for computing the permanent for matrices of\nsize N that can written as a product of L block diagonal matrices with blocks\nof size at most 2. For fixed L, the time and space resources scale linearly in\nN, with a prefactor that scales exponentially in L. This class of matrices\ncontains banded matrices with banded inverse. We show that such a factorization\ninto a product of block diagonal matrices gives rise to a circuit acting on a\nHilbert space with a tensor product structure and that the permanent is equal\nto the transition amplitude of this circuit and a product basis state. In this\ncorrespondence, a block diagonal matrix gives rise to one layer of the circuit,\nwhere each block to a gate acting either on a single tensor component or on two\nadjacent tensor components. This observation allows us to adopt matrix product\nstates, a computational method from condensed matter physics and quantum\ninformation theory used to simulate quantum systems, to evaluate the transition\namplitude.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 19:43:43 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Temme", "Kristan", ""], ["Wocjan", "Pawel", ""]]}]