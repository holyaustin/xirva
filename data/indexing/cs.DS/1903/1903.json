[{"id": "1903.00092", "submitter": "Rohan Kodialam", "authors": "Rohan Kodialam", "title": "Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the classic Ski Rental online algorithm with\napplications to machine learning. In our variant, we allow the skier access to\na black-box machine-learning algorithm that provides an estimate of the\nprobability that there will be at most a threshold number of ski-days. We\nderive a class of optimal randomized algorithms to determine the strategy that\nminimizes the worst-case expected competitive ratio for the skier given a\nprediction from the machine learning algorithm,and analyze the performance and\nrobustness of these algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 22:34:46 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 20:29:57 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Kodialam", "Rohan", ""]]}, {"id": "1903.00227", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders", "title": "Parallel Weighted Random Sampling", "comments": "Preliminary versions of this paper were presented at ESA 2019 and\n  SPAA 2020. Includes material from arXiv:1910.11069", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures for efficient sampling from a set of weighted items are an\nimportant building block of many applications. However, few parallel solutions\nare known. We close many of these gaps both for shared-memory and\ndistributed-memory machines. We give efficient, fast, and practicable parallel\nalgorithms for building data structures that support sampling single items\n(alias tables, compressed data structures). This also yields a simplified and\nmore space-efficient sequential algorithm for alias table construction. Our\napproaches to sampling $k$ out of $n$ items with/without replacement and to\nsubset (Poisson) sampling are output-sensitive, i.e., the sampling algorithms\nuse work linear in the number of different samples. This is also interesting in\nthe sequential case. Weighted random permutation can be done by sorting\nappropriate random deviates. We show that this is possible with linear work\nusing a nonlinear transformation of these deviates. Finally, we give a\ncommunication-efficient, highly scalable approach to (weighted and unweighted)\nreservoir sampling. This algorithm is based on a fully distributed model of\nstreaming algorithms that might be of independent interest. Experiments for\nalias tables and sampling with replacement show near linear speedups both for\nconstruction and queries using up to 158 threads of shared-memory machines. An\nexperimental evaluation of distributed weighted reservoir sampling on up to 256\nnodes (5120 cores) also shows good speedups.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 09:52:11 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 15:23:15 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 15:01:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1903.00436", "submitter": "Amaury Van Bemten", "authors": "Amaury Van Bemten, Jochen W. Guck, Carmen Mas Machuca, Wolfgang\n  Kellerer", "title": "Bounded Dijkstra (BD): Search Space Reduction for Expediting Shortest\n  Path Subroutines", "comments": "9 pages -- accompanying web page: https://lora.lkn.ei.tum.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortest path (SP) and shortest paths tree (SPT) problems arise both as\ndirect applications and as subroutines of overlay algorithms solving more\ncomplex problems such as the constrained shortest path (CSP) or the constrained\nminimum Steiner tree (CMST) problems. Often, such algorithms do not use the\nresult of an SP subroutine if its total cost is greater than a given bound. For\nexample, for delay-constrained problems, paths resulting from a least-delay SP\nrun and whose delay is greater than the delay constraint of the original\nproblem are not used by the overlay algorithm to construct its solution. As a\nresult of the existence of these bounds, and because the Dijkstra SP algorithm\ndiscovers paths in increasing order of cost, we can terminate the SP search\nearlier, i.e., once it is known that paths with a greater total cost will not\nbe considered by the overlay algorithm. This early termination allows to reduce\nthe runtime of the SP subroutine, thereby reducing the runtime of the overlay\nalgorithm without impacting its final result. We refer to this adaptation of\nDijkstra for centralized implementations as bounded Dijkstra (BD). On the\nexample of CSP algorithms, we confirm the usefulness of BD by showing that it\ncan reduce the runtime of some algorithms by 75% on average.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:52:51 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 11:59:22 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Van Bemten", "Amaury", ""], ["Guck", "Jochen W.", ""], ["Machuca", "Carmen Mas", ""], ["Kellerer", "Wolfgang", ""]]}, {"id": "1903.00507", "submitter": "Giorgio Vinciguerra", "authors": "Giorgio Vinciguerra, Paolo Ferragina, Michele Miccinesi", "title": "Superseding traditional indexes by orchestrating learning and geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design the first learned index that solves the dictionary problem with\ntime and space complexity provably better than classic data structures for\nhierarchical memories, such as B-trees, and modern learned indexes. We call our\nsolution the Piecewise Geometric Model index (PGM-index) because it turns the\nindexing of a sequence of keys into the coverage of a sequence of 2D-points via\nlinear models (i.e. segments) suitably learned to trade query time vs space\nefficiency. This idea comes from some known heuristic results which we\nstrengthen by showing that the minimal number of such segments can be computed\nvia known and optimal streaming algorithms. Our index is then obtained by\nrecursively applying this geometric idea that guarantees a smoothed adaptation\nto the \"geometric complexity\" of the input data. Finally, we propose a variant\nof the index that adapts not only to the distribution of the dictionary keys\nbut also to their access frequencies, thus obtaining the first\ndistribution-aware learned index.\n  The second main contribution of this paper is the proposal and study of the\nconcept of Multicriteria Data Structure, namely one that asks a data structure\nto adapt in an automatic way to the constraints imposed by the application of\nuse. We show that our index is a multicriteria data structure because its\nsignificant flexibility in storage and query time can be exploited by a\nproperly designed optimisation algorithm that efficiently finds its best design\nsetting in order to match the input constraints.\n  A thorough experimental analysis shows that our index and its multicriteria\nvariant improve uniformly, over both time and space, classic and learned\nindexes up to several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 19:34:48 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 20:17:52 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 11:20:16 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Vinciguerra", "Giorgio", ""], ["Ferragina", "Paolo", ""], ["Miccinesi", "Michele", ""]]}, {"id": "1903.00547", "submitter": "Zhihao Jiang", "authors": "Zhihao Jiang, Haoyu Zhao", "title": "An FPTAS for Stochastic Unbounded Min-Knapsack Problem", "comments": "24 pages", "journal-ref": "In Chen Y., Deng X., Lu M. (eds) Frontiers in Algorithmics. FAW\n  2019. Lecture Notes in Computer Science, vol 11458. Springer, Cham", "doi": "10.1007/978-3-030-18126-0_11", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic unbounded min-knapsack problem\n($\\textbf{Min-SUKP}$). The ordinary unbounded min-knapsack problem states that:\nThere are $n$ types of items, and there is an infinite number of items of each\ntype. The items of the same type have the same cost and weight. We want to\nchoose a set of items such that the total weight is at least $W$ and the total\ncost is minimized. The \\prob~generalizes the ordinary unbounded min-knapsack\nproblem to the stochastic setting, where the weight of each item is a random\nvariable following a known distribution and the items of the same type follow\nthe same weight distribution. In \\prob, different types of items may have\ndifferent cost and weight distributions. In this paper, we provide an FPTAS for\n$\\textbf{Min-SUKP}$, i.e., the approximate value our algorithm computes is at\nmost $(1+\\epsilon)$ times the optimum, and our algorithm runs in\n$poly(1/\\epsilon,n,\\log W)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 21:40:08 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Jiang", "Zhihao", ""], ["Zhao", "Haoyu", ""]]}, {"id": "1903.00581", "submitter": "Klaus-Tycho Foerster", "authors": "Sebastian Brandt and Klaus-Tycho Foerster and Jonathan Maurer and\n  Roger Wattenhofer", "title": "Online Graph Exploration on a Restricted Graph Class: Optimal Solutions\n  for Tadpole Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online graph exploration on undirected graphs, where\na searcher has to visit every vertex and return to the origin. Once a new\nvertex is visited, the searcher learns of all neighboring vertices and the\nconnecting edge weights. The goal such an exploration is to minimize its total\ncost, where each edge traversal incurs a cost of the corresponding edge weight.\nWe investigate the problem on tadpole graphs (also known as dragons, kites),\nwhich consist of a cycle with an attached path. Miyazaki et al. (The online\ngraph exploration problem on restricted graphs, IEICE Transactions 92-D (9),\n2009) showed that every online algorithm on these graphs must have a\ncompetitive ratio of 2-epsilon, but did not provide upper bounds for non-unit\nedge weights. We show via amortized analysis that a greedy approach yields a\nmatching competitive ratio of 2 on tadpole graphs, for arbitrary non-negative\nedge weights.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 23:51:01 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 13:48:06 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 19:33:12 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Brandt", "Sebastian", ""], ["Foerster", "Klaus-Tycho", ""], ["Maurer", "Jonathan", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1903.00729", "submitter": "Fatih Ta\\c{s}yaran", "authors": "Fatih Ta\\c{s}yaran, Kerem Y{\\i}ld{\\i}r{\\i}r, Kamer Kaya, Mustafa Kemal\n  Ta\\c{s}", "title": "One Table to Count Them All: Parallel Frequency Estimation on\n  Single-Board Computers", "comments": "12 pages, 4 figures, 3 algorithms, 1 table, submitted to EuroPar'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketches are probabilistic data structures that can provide approximate\nresults within mathematically proven error bounds while using orders of\nmagnitude less memory than traditional approaches. They are tailored for\nstreaming data analysis on architectures even with limited memory such as\nsingle-board computers that are widely exploited for IoT and edge computing.\nSince these devices offer multiple cores, with efficient parallel sketching\nschemes, they are able to manage high volumes of data streams. However, since\ntheir caches are relatively small, a careful parallelization is required. In\nthis work, we focus on the frequency estimation problem and evaluate the\nperformance of a high-end server, a 4-core Raspberry Pi and an 8-core Odroid.\nAs a sketch, we employed the widely used Count-Min Sketch. To hash the stream\nin parallel and in a cache-friendly way, we applied a novel tabulation approach\nand rearranged the auxiliary tables into a single one. To parallelize the\nprocess with performance, we modified the workflow and applied a form of\nbuffering between hash computations and sketch updates. Today, many\nsingle-board computers have heterogeneous processors in which slow and fast\ncores are equipped together. To utilize all these cores to their full\npotential, we proposed a dynamic load-balancing mechanism which significantly\nincreased the performance of frequency estimation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 16:18:00 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Ta\u015fyaran", "Fatih", ""], ["Y\u0131ld\u0131r\u0131r", "Kerem", ""], ["Kaya", "Kamer", ""], ["Ta\u015f", "Mustafa Kemal", ""]]}, {"id": "1903.00750", "submitter": "Sainyam Galhotra Mr", "authors": "Sainyam Galhotra, Sandhya Saisubramanian and Shlomo Zilberstein", "title": "Lexicographically Ordered Multi-Objective Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a rich model for multi-objective clustering with lexicographic\nordering over objectives and a slack. The slack denotes the allowed\nmultiplicative deviation from the optimal objective value of the higher\npriority objective to facilitate improvement in lower-priority objectives. We\nthen propose an algorithm called Zeus to solve this class of problems, which is\ncharacterized by a makeshift function. The makeshift fine tunes the clusters\nformed by the processed objectives so as to improve the clustering with respect\nto the unprocessed objectives, given the slack. We present makeshift for\nsolving three different classes of objectives and analyze their solution\nguarantees. Finally, we empirically demonstrate the effectiveness of our\napproach on three applications using real-world data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 19:32:00 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Saisubramanian", "Sandhya", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1903.00995", "submitter": "Vasileios Nakos", "authors": "Yi Li, Vasileios Nakos", "title": "Deterministic Sparse Fourier Transform with an ell_infty Guarantee", "comments": "ICALP 2020--presentation improved according to reviewers' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the deterministic version of the Sparse Fourier\nTransform problem, which asks to read only a few entries of $x \\in\n\\mathbb{C}^n$ and design a recovery algorithm such that the output of the\nalgorithm approximates $\\hat x$, the Discrete Fourier Transform (DFT) of $x$.\nThe randomized case has been well-understood, while the main work in the\ndeterministic case is that of Merhi et al.\\@ (J Fourier Anal Appl 2018), which\nobtains $O(k^2 \\log^{-1}k \\cdot \\log^{5.5}n)$ samples and a similar runtime\nwith the $\\ell_2/\\ell_1$ guarantee. We focus on the stronger\n$\\ell_{\\infty}/\\ell_1$ guarantee and the closely related problem of incoherent\nmatrices. We list our contributions as follows.\n  1. We find a deterministic collection of $O(k^2 \\log n)$ samples for the\n$\\ell_\\infty/\\ell_1$ recovery in time $O(nk \\log^2 n)$, and a deterministic\ncollection of $O(k^2 \\log^2 n)$ samples for the $\\ell_\\infty/\\ell_1$ sparse\nrecovery in time $O(k^2 \\log^3n)$.\n  2. We give new deterministic constructions of incoherent matrices that are\nrow-sampled submatrices of the DFT matrix, via a derandomization of Bernstein's\ninequality and bounds on exponential sums considered in analytic number theory.\nOur first construction matches a previous randomized construction of Nelson,\nNguyen and Woodruff (RANDOM'12), where there was no constraint on the form of\nthe incoherent matrix.\n  Our algorithms are nearly sample-optimal, since a lower bound of $\\Omega(k^2\n+ k \\log n)$ is known, even for the case where the sensing matrix can be\narbitrarily designed. A similar lower bound of $\\Omega(k^2 \\log n/ \\log k)$ is\nknown for incoherent matrices.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 21:33:05 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 21:01:40 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 11:58:08 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Li", "Yi", ""], ["Nakos", "Vasileios", ""]]}, {"id": "1903.01047", "submitter": "Yusuke Kobayashi", "authors": "Yusuke Kobayashi", "title": "An FPT Algorithm for Minimum Additive Spanner Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a positive integer $t$ and a graph $G$, an additive $t$-spanner of $G$ is\na spanning subgraph in which the distance between every pair of vertices is at\nmost the original distance plus $t$. Minimum Additive $t$-Spanner Problem is to\nfind an additive $t$-spanner with the minimum number of edges in a given graph,\nwhich is known to be NP-hard. Since we need to care about global properties of\ngraphs when we deal with additive $t$-spanners, Minimum Additive $t$-Spanner\nProblem is hard to handle, and hence only few results are known for it. In this\npaper, we study Minimum Additive $t$-Spanner Problem from the viewpoint of\nparameterized complexity. We formulate a parameterized version of the problem\nin which the number of removed edges is regarded as a parameter, and give a\nfixed-parameter algorithm for it. We also extend our result to $(\\alpha,\n\\beta)$-spanners.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 02:42:11 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Kobayashi", "Yusuke", ""]]}, {"id": "1903.01291", "submitter": "Fahad Panolan", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh,\n  Meirav Zehavi", "title": "Decomposition of Map Graphs with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidimensionality is the most common technique to design subexponential-time\nparameterized algorithms on special classes of graphs, particularly planar\ngraphs. The core engine behind it is a combinatorial lemma of Robertson,\nSeymour and Thomas that states that every planar graph either has a\n$\\sqrt{k}\\times \\sqrt{k}$-grid as a minor, or its treewidth is $O(\\sqrt{k})$.\nHowever, bidimensionality theory cannot be extended directly to several\nwell-known classes of geometric graphs. Nevertheless, a relaxation of this\nlemma has been proven useful for unit disk graphs. Inspired by this, we prove a\nnew decomposition lemma for map graphs. Informally, our lemma states the\nfollowing. For any map graph $G$, there exists a collection $(U_1,\\ldots,U_t)$\nof cliques of $G$ with the following property: $G$ either contains a\n$\\sqrt{k}\\times \\sqrt{k}$-grid as a minor, or it admits a tree decomposition\nwhere every bag is the union of $O(\\sqrt{k})$ of the cliques in the above\ncollection. The new lemma appears to be a handy tool in the design of\nsubexponential parameterized algorithms on map graphs. We demonstrate its\nusability by designing algorithms on map graphs with running time\n$2^{O({\\sqrt{k}\\log{k}})} \\cdot n^{O(1)}$ for the Connected Planar $\\cal\nF$-Deletion problem (that encompasses problems such as Feedback Vertex Set and\nVertex Cover). Obtaining subexponential algorithms for Longest Cycle/Path and\nCycle Packing is more challenging. We have to construct tree decompositions\nwith more powerful properties and to prove sublinear bounds on the number of\nways an optimum solution could \"cross\" bags in these decompositions.\n  For Longest Cycle/Path, these are the first subexponential-time parameterized\nalgorithms on map graphs. For Feedback Vertex Set and Cycle Packing, we improve\nupon known $2^{O({k^{0.75}\\log{k}})} \\cdot n^{O(1)}$-time algorithms on map\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:55:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1903.01387", "submitter": "Gaurav Menghani", "authors": "Gaurav Menghani, Dhruv Matani", "title": "A Simple Solution to the Level-Ancestor Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Level Ancestory query LA($u$, $d$) asks for the the ancestor of the node\n$u$ at a depth $d$. We present a simple solution, which pre-processes the tree\nin $O(n)$ time with $O(n)$ extra space, and answers the queries in $O(\\log\\\n{n})$ time. Though other optimal algorithms exist, this is a simple enough\nsolution that could be taught and implemented easily.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 17:30:51 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Menghani", "Gaurav", ""], ["Matani", "Dhruv", ""]]}, {"id": "1903.01417", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "A Divide-and-Conquer Algorithm for Two-Point $L_1$ Shortest Path Queries\n  in Polygonal Domains", "comments": "A preliminary version to appear in SoCG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{P}$ be a polygonal domain of $h$ holes and $n$ vertices. We\nstudy the problem of constructing a data structure that can compute a shortest\npath between $s$ and $t$ in $\\mathcal{P}$ under the $L_1$ metric for any two\nquery points $s$ and $t$. To do so, a standard approach is to first find a set\nof $n_s$ \"gateways\" for $s$ and a set of $n_t$ \"gateways\" for $t$ such that\nthere exist a shortest $s$-$t$ path containing a gateway of $s$ and a gateway\nof $t$, and then compute a shortest $s$-$t$ path using these gateways. Previous\nalgorithms all take quadratic $O(n_s\\cdot n_t)$ time to solve this problem. In\nthis paper, we propose a divide-and-conquer technique that solves the problem\nin $O(n_s + n_t \\log n_s)$ time. As a consequence, we construct a data\nstructure of $O(n+(h^2\\log^3 h/\\log\\log h))$ size in $O(n+(h^2\\log^4 h/\\log\\log\nh))$ time such that each query can be answered in $O(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 18:17:51 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1903.01465", "submitter": "Giovanni Manzini", "authors": "Lavinia Egidi, Giovanni Manzini", "title": "Lightweight merging of compressed indices based on BWT variants", "comments": "23 pages. A preliminary version appeared in Proc. SPIRE 2017,\n  Springer Verlag LNCS 10508. arXiv admin note: text overlap with\n  arXiv:1609.04618", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a flexible and lightweight technique for merging\ncompressed indices based on variants of Burrows-Wheeler transform (BWT), thus\naddressing the need for algorithms that compute compressed indices over large\ncollections using a limited amount of working memory. Merge procedures make it\npossible to use an incremental strategy for building large indices based on\nmerging indices for progressively larger subcollections.\n  Starting with a known lightweight algorithm for merging BWTs [Holt and\nMcMillan, Bionformatics 2014], we show how to modify it in order to merge, or\ncompute from scratch, also the Longest Common Prefix (LCP) array. We then\nexpand our technique for merging compressed tries and circular/permuterm\ncompressed indices, two compressed data structures for which there were\nhitherto no known merging algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 18:34:01 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Egidi", "Lavinia", ""], ["Manzini", "Giovanni", ""]]}, {"id": "1903.01538", "submitter": "Andrew van der Poel", "authors": "Blair D. Sullivan, Andrew van der Poel, Trey Woodlief", "title": "Faster Biclique Mining in Near-Bipartite Graphs", "comments": "16 pages + 15 page appendix (additional figures and supporting\n  proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying dense bipartite subgraphs is a common graph data mining task.\nMany applications focus on the enumeration of all maximal bicliques (MBs),\nthough sometimes the stricter variant of maximal induced bicliques (MIBs) is of\ninterest. Recent work of Kloster et al. introduced a MIB-enumeration approach\ndesigned for \"near-bipartite\" graphs, where the runtime is parameterized by the\nsize k of an odd cycle transversal (OCT), a vertex set whose deletion results\nin a bipartite graph. Their algorithm was shown to outperform the previously\nbest known algorithm even when k was logarithmic in |V|. In this paper, we\nintroduce two new algorithms optimized for near-bipartite graphs - one which\nenumerates MIBs in time O(M_I |V||E| k), and another based on the approach of\nAlexe et al. which enumerates MBs in time O(M_B |V||E| k), where M_I and M_B\ndenote the number of MIBs and MBs in the graph, respectively. We implement all\nof our algorithms in open-source C++ code and experimentally verify that the\nOCT-based approaches are faster in practice than the previously existing\nalgorithms on graphs with a wide variety of sizes, densities, and OCT\ndecompositions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 21:07:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Sullivan", "Blair D.", ""], ["van der Poel", "Andrew", ""], ["Woodlief", "Trey", ""]]}, {"id": "1903.01682", "submitter": "Sinan Aksoy", "authors": "Sinan G. Aksoy, Kathleen E. Nowak, Stephen J. Young", "title": "A linear-time algorithm and analysis of graph Relative Hausdorff\n  distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity metrics serve far-ranging purposes across many domains in\ndata science. As graph datasets grow in size, scientists need comparative tools\nthat capture meaningful differences, yet are lightweight and scalable. Graph\nRelative Hausdorff (RH) distance is a promising, recently proposed measure for\nquantifying degree distribution similarity. In spite of recent interest in RH\ndistance, little is known about its properties. Here, we conduct an algorithmic\nand analytic study of RH distance. In particular, we provide the first\nlinear-time algorithm for computing RH distance, analyze examples of RH\ndistance between pairs of real-world networks as well as structured families of\ngraphs, and prove several analytic results concerning the range, density, and\nextremal behavior of RH distance values.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 05:50:32 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 17:54:13 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Aksoy", "Sinan G.", ""], ["Nowak", "Kathleen E.", ""], ["Young", "Stephen J.", ""]]}, {"id": "1903.01756", "submitter": "Sanjiang Li", "authors": "Sanjiang Li and Yongming Li", "title": "Semi-dynamic shortest-path tree algorithms for directed graphs with\n  arbitrary weights", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G$ with arbitrary real-valued weights, the single\nsource shortest-path problem (SSSP) asks for, given a source $s$ in $G$,\nfinding a shortest path from $s$ to each vertex $v$ in $G$. A classical SSSP\nalgorithm detects a negative cycle of $G$ or constructs a shortest-path tree\n(SPT) rooted at $s$ in $O(mn)$ time, where $m,n$ are the numbers of edges and\nvertices in $G$ respectively. In many practical applications, new constraints\ncome from time to time and we need to update the SPT frequently. Given an SPT\n$T$ of $G$, suppose the weight on a certain edge is modified. We show by\nrigorous proof that the well-known {\\sf Ball-String} algorithm for positively\nweighted graphs can be adapted to solve the dynamic SPT problem for directed\ngraphs with arbitrary weights. Let $n_0$ be the number of vertices that are\naffected (i.e., vertices that have different distances from $s$ or different\nparents in the input and output SPTs) and $m_0$ the number of edges incident to\nan affected vertex. The adapted algorithms terminate in $O(m_0+n_0 \\log n_0)$\ntime, either detecting a negative cycle (only in the decremental case) or\nconstructing a new SPT $T'$ for the updated graph. We show by an example that\nthe output SPT $T'$ may have more than necessary edge changes to $T$. To remedy\nthis, we give a general method for transforming $T'$ into an SPT with minimal\nedge changes in time $O(n_0)$ provided that $G$ has no cycles with zero length.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 10:23:07 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Li", "Sanjiang", ""], ["Li", "Yongming", ""]]}, {"id": "1903.01859", "submitter": "Swati Padmanabhan", "authors": "Yin Tat Lee and Swati Padmanabhan", "title": "An $\\widetilde{\\mathcal{O}}(m/\\varepsilon^{3.5})$-Cost Algorithm for\n  Semidefinite Programs with Diagonal Constraints", "comments": "Main body: 12 pages; With Appendix: 50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study semidefinite programs with diagonal constraints. This problem class\nappears in combinatorial optimization and has a wide range of engineering\napplications such as in circuit design, channel assignment in wireless\nnetworks, phase recovery, covariance matrix estimation, and low-order\ncontroller design.\n  In this paper, we give an algorithm to solve this problem to\n$\\varepsilon$-accuracy, with a run time of\n$\\widetilde{\\mathcal{O}}(m/\\varepsilon^{3.5})$, where $m$ is the number of\nnon-zero entries in the cost matrix. We improve upon the previous best run time\nof $\\widetilde{\\mathcal{O}}(m/\\varepsilon^{4.5})$ by Arora and Kale. As a\ncorollary of our result, given an instance of the Max-Cut problem with $n$\nvertices and $m \\gg n$ edges, our algorithm when applied to the standard SDP\nrelaxation of Max-Cut returns a $(1 - \\varepsilon)\\alpha_{GW}$ cut in time\n$\\widetilde{\\mathcal{O}}(m/\\varepsilon^{3.5})$, where $\\alpha_{GW} \\approx\n0.878567$ is the Goemans-Williamson approximation ratio. We obtain this run\ntime via the stochastic variance reduction framework applied to the Arora-Kale\nalgorithm, by constructing a constant-accuracy estimator to maintain the primal\niterates.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:36:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Lee", "Yin Tat", ""], ["Padmanabhan", "Swati", ""]]}, {"id": "1903.01909", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov, Daniel Valenzuela, Gonzalo Navarro, Simon J. Puglisi", "title": "Lempel-Ziv-like Parsing in Small Space", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lempel-Ziv (LZ77 or, briefly, LZ) is one of the most effective and\nwidely-used compressors for repetitive texts. However, the existing efficient\nmethods computing the exact LZ parsing have to use linear or close to linear\nspace to index the input text during the construction of the parsing, which is\nprohibitive for long inputs. An alternative is Relative Lempel-Ziv (RLZ), which\nindexes only a fixed reference sequence, whose size can be controlled. Deriving\nthe reference sequence by sampling the text yields reasonable compression\nratios for RLZ, but performance is not always competitive with that of LZ and\ndepends heavily on the similarity of the reference to the text. In this paper\nwe introduce ReLZ, a technique that uses RLZ as a preprocessor to approximate\nthe LZ parsing using little memory. RLZ is first used to produce a sequence of\nphrases, and these are regarded as metasymbols that are input to LZ for a\nsecond-level parsing on a (most often) drastically shorter sequence. This\nparsing is finally translated into one on the original sequence.\n  We analyze the new scheme and prove that, like LZ, it achieves the $k$th\norder empirical entropy compression $n H_k + o(n\\log\\sigma)$ with $k =\no(\\log_\\sigma n)$, where $n$ is the input length and $\\sigma$ is the alphabet\nsize. In fact, we prove this entropy bound not only for ReLZ but for a wide\nclass of LZ-like encodings. Then, we establish a lower bound on ReLZ\napproximation ratio showing that the number of phrases in it can be\n$\\Omega(\\log n)$ times larger than the number of phrases in LZ. Our experiments\nshow that ReLZ is faster than existing alternatives to compute the (exact or\napproximate) LZ parsing, at the reasonable price of an approximation factor\nbelow $2.0$ in all tested scenarios, and sometimes below $1.05$, to the size of\nLZ.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 15:50:07 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 17:41:48 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kosolobov", "Dmitry", ""], ["Valenzuela", "Daniel", ""], ["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1903.02161", "submitter": "Kazuhiro Kurita", "authors": "Kazuhiro Kurita and Kunihiro Wasa and Hiroki Arimura and Takeaki Uno", "title": "An Efficient Algorithm for Enumerating Chordal Bipartite Induced\n  Subgraphs in Sparse Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-25005-8_28", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a characterization of chordal bipartite graphs and\nan efficient enumeration algorithm for chordal bipartite induced subgraphs. A\nchordal bipartite graph is a bipartite graph without induced cycles with length\nsix or more. It is known that the incident graph of a hypergraph is chordal\nbipartite graph if and only if the hypergraph is $\\beta$-acyclic. As the main\nresult of our paper, we show that a graph $G$ is chordal bipartite if and only\nif there is a special vertex elimination ordering for $G$, called CBEO.\nMoreover, we propose an algorithm ECB which enumerates all chordal bipartite\ninduced subgraphs in $O(kt\\Delta^2)$ time per solution on average, where $k$ is\nthe degeneracy, $t$ is the maximum size of $K_{t,t}$ as an induced subgraph,\nand $\\Delta$ is the degree. ECB achieves constant amortized time enumeration\nfor bounded degree graphs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 03:48:52 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kurita", "Kazuhiro", ""], ["Wasa", "Kunihiro", ""], ["Arimura", "Hiroki", ""], ["Uno", "Takeaki", ""]]}, {"id": "1903.02185", "submitter": "Suthee Ruangwises", "authors": "Suthee Ruangwises, Toshiya Itoh", "title": "Stable Noncrossing Matchings", "comments": "This paper has appeared at IWOCA 2019", "journal-ref": null, "doi": "10.1007/978-3-030-25005-8_33", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ men represented by $n$ points lying on a line, and $n$\nwomen represented by $n$ points lying on another parallel line, with each\nperson having a list that ranks some people of opposite gender as his/her\nacceptable partners in strict order of preference. In this problem, we want to\nmatch people of opposite genders to satisfy people's preferences as well as\nmaking the edges not crossing one another geometrically. A noncrossing blocking\npair w.r.t. a matching $M$ is a pair $(m,w)$ of a man and a woman such that\nthey are not matched with each other but prefer each other to their own\npartners in $M$, and the segment $(m,w)$ does not cross any edge in $M$. A\nweakly stable noncrossing matching (WSNM) is a noncrossing matching that does\nnot admit any noncrossing blocking pair. In this paper, we prove the existence\nof a WSNM in any instance by developing an $O(n^2)$ algorithm to find one in a\ngiven instance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 05:50:40 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 09:31:00 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 07:31:08 GMT"}, {"version": "v4", "created": "Fri, 25 Oct 2019 08:54:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ruangwises", "Suthee", ""], ["Itoh", "Toshiya", ""]]}, {"id": "1903.02195", "submitter": "Vahid Roostapour", "authors": "Mojgan Pourhassan, Vahid Roostapour, and Frank Neumann", "title": "Runtime Analysis of RLS and (1+1) EA for the Dynamic Weighted Vertex\n  Cover Problem", "comments": "The paper has been accepted for Theoretical Computer Science journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform theoretical analyses on the behaviour of an\nevolutionary algorithm and a randomised search algorithm for the dynamic vertex\ncover problem based on its dual formulation. The dynamic vertex cover problem\nhas already been theoretically investigated to some extent and it has been\nshown that using its dual formulation to represent possible solutions can lead\nto a better approximation behaviour. We improve some of the existing results,\ni.e. we find a linear expected re-optimization time for a (1+1) EA to\nre-discover a 2-approximation when edges are dynamically deleted from the\ngraph. Furthermore, we investigate a different setting for applying the\ndynamism to the problem, in which a dynamic change happens at each step with a\nprobability $P_D$. We also expand these analyses to the weighted vertex cover\nproblem, in which weights are assigned to vertices and the goal is to find a\ncover set with minimum total weight. Similar to the classical case, the dynamic\nchanges that we consider on the weighted vertex cover problem are adding and\nremoving edges to and from the graph. We aim at finding a maximal solution for\nthe dual problem, which gives a 2-approximate solution for the vertex cover\nproblem. This is equivalent to the maximal matching problem for the classical\nvertex cover problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 06:35:06 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Pourhassan", "Mojgan", ""], ["Roostapour", "Vahid", ""], ["Neumann", "Frank", ""]]}, {"id": "1903.02388", "submitter": "Jiadong Han", "authors": "Han Jiadong", "title": "An Adaptive Grid Algorithm for Computing the Homology Group of\n  Semialgebraic Set", "comments": "arXiv admin note: text overlap with arXiv:1706.07473 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking for an efficient algorithm for the computation of the homology groups\nof an algebraic set or even a semi-algebraic set is an important problem in the\neffective real algebraic geometry. Recently, Peter Burgisser, Felipe Cucker and\nPierre Lairez wrote a paper [1], which made a step forward by giving an\nalgorithm of weak exponential time. However, the algorithm is not not\npractical. In my thesis, I will introduce my improvement of this algorithm\nusing an adaptive grid algorithm on the unit sphere.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:23:11 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Jiadong", "Han", ""]]}, {"id": "1903.02445", "submitter": "Siddharth Gupta", "authors": "Siddharth Gupta, Guy Sa'ar, Meirav Zehavi", "title": "The Parameterized Complexity of Motion Planning for Snake-Like Robots", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of a variant of the classic video game\nSnake that models real-world problems of motion planning. Given a snake-like\nrobot with an initial position and a final position in an environment (modeled\nby a graph), our objective is to determine whether the robot can reach the\nfinal position from the initial position without intersecting itself.\nNaturally, this problem models a wide-variety of scenarios, ranging from the\ntransportation of linked wagons towed by a locomotor at an airport or a\nsupermarket to the movement of a group of agents that travel in an `ant-like'\nfashion and the construction of trains in amusement parks. Unfortunately,\nalready on grid graphs, this problem is PSPACE-complete [Biasi and Ophelders,\n2016]. Nevertheless, we prove that even on general graphs, the problem is\nsolvable in time $k^{\\mathcal{O}(k)}|I|^{\\mathcal{O}(1)}$ where $k$ is the size\nof the snake, and $|I|$ is the input size. In particular, this shows that the\nproblem is fixed-parameter tractable (FPT). Towards this, we show how to employ\ncolor-coding to sparsify the configuration graph of the problem to have size\n$k^{\\mathcal{O}(k)}|I|^{\\mathcal{O}(1)}$ rather than $|I|^{\\mathcal{O}(k)}$. We\nbelieve that our approach will find other applications in motion planning.\nAdditionally, we show that the problem is unlikely to admit a polynomial kernel\neven on grid graphs, but it admits a treewidth-reduction procedure. To the best\nof our knowledge, the study of the parameterized complexity of motion planning\nproblems (where the intermediate configurations of the motion are of\nimportance) has so far been largely overlooked. Thus, our work is pioneering in\nthis regard.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 15:20:14 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Gupta", "Siddharth", ""], ["Sa'ar", "Guy", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1903.02533", "submitter": "Sebastian Wild", "authors": "J. Ian Munro and Sebastian Wild", "title": "Entropy Trees and Range-Minimum Queries In Optimal Average-Case Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The range-minimum query (RMQ) problem is a fundamental data structuring task\nwith numerous applications. Despite the fact that succinct solutions with\nworst-case optimal $2n+o(n)$ bits of space and constant query time are known,\nit has been unknown whether such a data structure can be made adaptive to the\nreduced entropy of random inputs (Davoodi et al. 2014). We construct a succinct\ndata structure with the optimal $1.736n+o(n)$ bits of space on average for\nrandom RMQ instances, settling this open problem.\n  Our solution relies on a compressed data structure for binary trees that is\nof independent interest. It can store a (static) binary search tree generated\nby random insertions in asymptotically optimal expected space and supports many\nqueries in constant time. Using an instance-optimal encoding of subtrees, we\nfurthermore obtain a \"hyper-succinct\" data structure for binary trees that\nimproves upon the ultra-succinct representation of Jansson, Sadakane and Sung\n(2012).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:13:33 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Munro", "J. Ian", ""], ["Wild", "Sebastian", ""]]}, {"id": "1903.02645", "submitter": "Aur\\'elien Ooms", "authors": "Sergio Cabello, Jean Cardinal, John Iacono, Stefan Langerman, Pat\n  Morin, Aur\\'elien Ooms", "title": "Encoding 3SUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: given three sets of real numbers, output a\nword-RAM data structure from which we can efficiently recover the sign of the\nsum of any triple of numbers, one in each set. This is similar to a previous\nwork by some of the authors to encode the order type of a finite set of points.\nWhile this previous work showed that it was possible to achieve slightly\nsubquadratic space and logarithmic query time, we show here that for the\nsimpler 3SUM problem, one can achieve an encoding that takes\n$\\tilde{O}(N^{\\frac 32})$ space for inputs sets of size $N$ and allows constant\ntime queries in the word-RAM.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 22:56:24 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Cabello", "Sergio", ""], ["Cardinal", "Jean", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""], ["Morin", "Pat", ""], ["Ooms", "Aur\u00e9lien", ""]]}, {"id": "1903.02675", "submitter": "Yair Carmon", "authors": "Yair Carmon, John C. Duchi, Aaron Sidford and Kevin Tian", "title": "A Rank-1 Sketch for Matrix Multiplicative Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple randomized sketch of the matrix multiplicative weight\n(MMW) update enjoys (in expectation) the same regret bounds as MMW, up to a\nsmall constant factor. Unlike MMW, where every step requires full matrix\nexponentiation, our steps require only a single product of the form $e^A b$,\nwhich the Lanczos method approximates efficiently. Our key technique is to view\nthe sketch as a $\\textit{randomized mirror projection}$, and perform mirror\ndescent analysis on the $\\textit{expected projection}$. Our sketch solves the\nonline eigenvector problem, improving the best known complexity bounds by\n$\\Omega(\\log^5 n)$. We also apply this sketch to semidefinite programming in\nsaddle-point form, yielding a simple primal-dual scheme with guarantees\nmatching the best in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 01:05:27 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 03:16:37 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Carmon", "Yair", ""], ["Duchi", "John C.", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "1903.02742", "submitter": "Zhao Song", "authors": "Vasileios Nakos, Zhao Song", "title": "Stronger L2/L2 Compressed Sensing; Without Iterating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the extensively studied problem of $\\ell_2/\\ell_2$ compressed\nsensing. The main contribution of our work is an improvement over [Gilbert, Li,\nPorat and Strauss, STOC 2010] with faster decoding time and significantly\nsmaller column sparsity, answering two open questions of the aforementioned\nwork.\n  Previous work on sublinear-time compressed sensing employed an iterative\nprocedure, recovering the heavy coordinates in phases. We completely depart\nfrom that framework, and give the first sublinear-time $\\ell_2/\\ell_2$ scheme\nwhich achieves the optimal number of measurements without iterating; this new\napproach is the key step to our progress. Towards that, we satisfy the\n$\\ell_2/\\ell_2$ guarantee by exploiting the heaviness of coordinates in a way\nthat was not exploited in previous work. Via our techniques we obtain improved\nresults for various sparse recovery tasks, and indicate possible further\napplications to problems in the field, to which the aforementioned iterative\nprocedure creates significant obstructions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 06:29:48 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Nakos", "Vasileios", ""], ["Song", "Zhao", ""]]}, {"id": "1903.02758", "submitter": "Arnold Filtser", "authors": "Arnold Filtser", "title": "A face cover perspective to $\\ell_1$ embeddings of planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was conjectured by Gupta et al. [Combinatorica04] that every planar graph\ncan be embedded into $\\ell_1$ with constant distortion. However, given an\n$n$-vertex weighted planar graph, the best upper bound on the distortion is\nonly $O(\\sqrt{\\log n})$, by Rao [SoCG99]. In this paper we study the case where\nthere is a set $K$ of terminals, and the goal is to embed only the terminals\ninto $\\ell_1$ with low distortion. In a seminal paper, Okamura and Seymour\n[J.Comb.Theory81] showed that if all the terminals lie on a single face, they\ncan be embedded isometrically into $\\ell_1$. The more general case, where the\nset of terminals can be covered by $\\gamma$ faces, was studied by Lee and\nSidiropoulos [STOC09] and Chekuri et al. [J.Comb.Theory13]. The state of the\nart is an upper bound of $O(\\log \\gamma)$ by Krauthgamer, Lee and Rika\n[SODA19]. Our contribution is a further improvement on the upper bound to\n$O(\\sqrt{\\log\\gamma})$. Since every planar graph has at most $O(n)$ faces, any\nfurther improvement on this result, will be a major breakthrough, directly\nimproving upon Rao's long standing upper bound. Moreover, it is well known that\nthe flow-cut gap equals to the distortion of the best embedding into $\\ell_1$.\nTherefore, our result provides a polynomial time $O(\\sqrt{\\log\n\\gamma})$-approximation to the sparsest cut problem on planar graphs, for the\ncase where all the demand pairs can be covered by $\\gamma$ faces.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 07:36:00 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 08:02:08 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Filtser", "Arnold", ""]]}, {"id": "1903.02810", "submitter": "Dorothee Henke", "authors": "Christoph Buchheim and Dorothee Henke", "title": "The robust bilevel continuous knapsack problem with uncertain follower's\n  objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a bilevel continuous knapsack problem where the leader controls\nthe capacity of the knapsack and the follower chooses an optimal packing\naccording to his own profits, which may differ from those of the leader. To\nthis bilevel problem, we add uncertainty in a natural way, assuming that the\nleader does not have full knowledge about the follower's problem. More\nprecisely, adopting the robust optimization approach and assuming that the\nfollower's profits belong to a given uncertainty set, our aim is to compute a\nsolution that optimizes the worst-case follower's reaction from the leader's\nperspective. By investigating the complexity of this problem with respect to\ndifferent types of uncertainty sets, we make first steps towards better\nunderstanding the combination of bilevel optimization and robust combinatorial\noptimization. We show that the problem can be solved in polynomial time for\nboth discrete and interval uncertainty, but that the same problem becomes\nNP-hard when each coefficient can independently assume only a finite number of\nvalues. In particular, this demonstrates that replacing uncertainty sets by\ntheir convex hulls may change the problem significantly, in contrast to the\nsituation in classical single-level robust optimization. For general polytopal\nuncertainty, the problem again turns out to be NP-hard, and the same is true\nfor ellipsoidal uncertainty even in the uncorrelated case. All presented\nhardness results already apply to the evaluation of the leader's objective\nfunction.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 10:15:54 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 16:09:41 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 12:34:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Buchheim", "Christoph", ""], ["Henke", "Dorothee", ""]]}, {"id": "1903.02858", "submitter": "Daniel Tenbrinck", "authors": "Daniel Tenbrinck, Fjedor Gaede, Martin Burger", "title": "Variational Graph Methods for Efficient Point Cloud Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DM cs.DS cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years new application areas have emerged in which one aims to\ncapture the geometry of objects by means of three-dimensional point clouds.\nOften the obtained data consist of a dense sampling of the object's surface,\ncontaining many redundant 3D points. These unnecessary data samples lead to\nhigh computational effort in subsequent processing steps. Thus, point cloud\nsparsification or compression is often applied as a preprocessing step. The two\nstandard methods to compress dense 3D point clouds are random subsampling and\napproximation schemes based on hierarchical tree structures, e.g., octree\nrepresentations. However, both approaches give little flexibility for adjusting\npoint cloud compression based on a-priori knowledge on the geometry of the\nscanned object. Furthermore, these methods lead to suboptimal approximations if\nthe 3D point cloud data is prone to noise. In this paper we propose a\nvariational method defined on finite weighted graphs, which allows to sparsify\na given 3D point cloud while giving the flexibility to control the appearance\nof the resulting approximation based on the chosen regularization functional.\nThe main contribution in this paper is a novel coarse-to-fine optimization\nscheme for point cloud sparsification, inspired by the efficiency of the\nrecently proposed Cut Pursuit algorithm for total variation denoising. This\nstrategy gives a substantial speed up in computing sparse point clouds compared\nto a direct application on all points as done in previous works and renders\nvariational methods now applicable for this task. We compare different settings\nfor our point cloud sparsification method both on unperturbed as well as noisy\n3D point cloud data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 12:03:44 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 12:14:59 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 10:01:43 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Tenbrinck", "Daniel", ""], ["Gaede", "Fjedor", ""], ["Burger", "Martin", ""]]}, {"id": "1903.02904", "submitter": "Andrzej Kapanowski", "authors": "A. Kapanowski and A. Krawczyk", "title": "Halin graphs are 3-vertex-colorable except even wheels", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Halin graph is a graph obtained by embedding a tree having no nodes of\ndegree two in the plane, and then adding a cycle to join the leaves of the tree\nin such a way that the resulting graph is planar. According to the four color\ntheorem, Halin graphs are 4-vertex-colorable. On the other hand, they are not\n2-vertex-colorable because they have triangles. We show that all Halin graphs\nare 3-vertex-colorable except even wheels. We also show how to find the perfect\nelimination ordering of a chordal completion for a given Halin graph. The\nalgorithms are implemented in Python using the graphtheory package. Generators\nof random Halin graphs (general or cubic) are included. The source code is\navailable from the public GitHub repository.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 13:41:07 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kapanowski", "A.", ""], ["Krawczyk", "A.", ""]]}, {"id": "1903.02999", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Tiers for peers: a practical algorithm for discovering hierarchy in\n  weighted networks", "comments": "Journal version. The previous version is the conference version of\n  the paper", "journal-ref": null, "doi": "10.1007/s10618-016-0485-7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions in many real-world phenomena can be explained by a strong\nhierarchical structure. Typically, this structure or ranking is not known;\ninstead we only have observed outcomes of the interactions, and the goal is to\ninfer the hierarchy from these observations. Discovering a hierarchy in the\ncontext of directed networks can be formulated as follows: given a graph,\npartition vertices into levels such that, ideally, there are only edges from\nupper levels to lower levels. The ideal case can only happen if the graph is\nacyclic. Consequently, in practice we have to introduce a penalty function that\npenalizes edges violating the hierarchy. A practical variant for such penalty\nis agony, where each violating edge is penalized based on the severity of the\nviolation. Hierarchy minimizing agony can be discovered in $O(m^2)$ time, and\nmuch faster in practice. In this paper we introduce several extensions to\nagony. We extend the definition for weighted graphs and allow a cardinality\nconstraint that limits the number of levels. While, these are conceptually\ntrivial extensions, current algorithms cannot handle them, nor they can be\neasily extended. We solve the problem by showing the connection to the\ncapacitated circulation problem, and we demonstrate that we can compute the\nexact solution fast in practice for large datasets. We also introduce a\nprovably fast heuristic algorithm that produces rankings with competitive\nscores. In addition, we show that we can compute agony in polynomial time for\nany convex penalty, and, to complete the picture, we show that minimizing\nhierarchy with any concave penalty is an NP-hard problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:52:45 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 17:26:24 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1903.03003", "submitter": "Vincent Froese", "authors": "Vincent Froese and Brijnesh Jain and Maciej Rymar and Mathias Weller", "title": "Fast Exact Dynamic Time Warping on Run-Length Encoded Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Time Warping (DTW) is a well-known similarity measure for time\nseries. The standard dynamic programming approach to compute the DTW distance\nof two length-$n$ time series, however, requires~$O(n^2)$ time, which is often\ntoo slow for real-world applications. Therefore, many heuristics have been\nproposed to speed up the DTW computation. These are often based on lower\nbounding techniques, approximating the DTW distance, or considering special\ninput data such as binary or piecewise constant time series. In this paper, we\npresent a first exact algorithm to compute the DTW distance of two run-length\nencoded time series whose running time only depends on the encoding lengths of\nthe inputs. The worst-case running time is cubic in the encoding length. In\nexperiments we show that our algorithm is indeed fast for time series with\nshort encoding lengths.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:46:20 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 08:48:45 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 11:40:13 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 14:31:53 GMT"}, {"version": "v5", "created": "Sat, 18 Apr 2020 10:13:59 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Froese", "Vincent", ""], ["Jain", "Brijnesh", ""], ["Rymar", "Maciej", ""], ["Weller", "Mathias", ""]]}, {"id": "1903.03070", "submitter": "Thomas Powell", "authors": "Thomas Powell, Peter M Schuster, Franziskus Wiesnet", "title": "An algorithmic approach to the existence of ideal objects in commutative\n  algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS math.AC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of ideal objects, such as maximal ideals in nonzero rings,\nplays a crucial role in commutative algebra. These are typically justified\nusing Zorn's lemma, and thus pose a challenge from a computational point of\nview. Giving a constructive meaning to ideal objects is a problem which dates\nback to Hilbert's program, and today is still a central theme in the area of\ndynamical algebra, which focuses on the elimination of ideal objects via\nsyntactic methods. In this paper, we take an alternative approach based on\nKreisel's no counterexample interpretation and sequential algorithms. We first\ngive a computational interpretation to an abstract maximality principle in the\ncountable setting via an intuitive, state based algorithm. We then carry out a\nconcrete case study, in which we give an algorithmic account of the result that\nin any commutative ring, the intersection of all prime ideals is contained in\nits nilradical.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 17:50:28 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Powell", "Thomas", ""], ["Schuster", "Peter M", ""], ["Wiesnet", "Franziskus", ""]]}, {"id": "1903.03147", "submitter": "Konstantinos Tsakalidis", "authors": "John Iacono, Riko Jacob and Konstantinos Tsakalidis", "title": "Cache-Oblivious Priority Queues with Decrease-Key and Applications to\n  Graph Algorithms", "comments": "A preliminary version of this work is published in ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present priority queues in the cache-oblivious external memory model with\nblock size $B$ and main memory size $M$ that support on $N$ elements, operation\n\\textsc{UPDATE} (combination of \\textsc{INSERT} and \\textsc{DECREASEKEY}) in $O\n  \\left(\\frac{1}{B}\\log_{\\frac{\\lambda}{B}} \\frac{N}{B}\\right)$ amortized I/Os\nand operations \\textsc{EXTRACT-MIN} and \\textsc{DELETE} in $O\n  \\left(\\lceil \\frac{\\lambda^{\\varepsilon}}{B} \\log_{\\frac{\\lambda}{B}}\n\\frac{N}{B} \\rceil \\log_{\\frac{\\lambda}{B}} \\frac{N}{B}\\right)$ amortized I/Os,\nusing $O\n  \\left(\\frac{N}{B}\\log_{\\frac{\\lambda}{B}} \\frac{N}{B}\\right)$ blocks, for a\nuser-defined parameter $\\lambda \\in [2, N ]$ and any real $\\varepsilon \\in\n(0,1)$. Our result improves upon previous I/O-efficient cache-oblivious and\ncache-aware priority queues [Chowdhury and Ramachandran, TALG 2018], [Brodal et\nal., SWAT 2004], [Kumar and Schwabe, SPDP 1996], [Arge et al., SICOMP 2007],\n[Fadel et al., TCS 1999].\n  We also present buffered repository trees that support on a multi-set of $N$\nelements, operation \\textsc{INSERT} in $O\n  \\left(\\frac{1}{B}\\log_{\\frac{\\lambda}{B}} \\frac{N}{B}\\right)$ I/Os and\noperation \\textsc{EXTRACT} on $K$ extracted elements in $O\n  \\left(\\frac{\\lambda^{\\varepsilon}}{B} \\log_{\\frac{\\lambda}{B}} \\frac{N}{B} +\n\\frac{K}{B}\\right)$ amortized I/Os, using $O\n  \\left(\\frac{N}{B}\\right)$ blocks, improving previous cache-aware and\ncache-oblivious results [Arge et al., SICOMP '07], [Buchsbaum et al., SODA\n'00].\n  In the cache-oblivious model, for $\\lambda = O\n  \\left(E/V\\right)$, we achieve $O\n  \\left(\\frac{E}{B}\\log_{\\frac{E}{V B}} \\frac{E}{B}\\right)$ I/Os for\nsingle-source shortest paths, depth-first search and breadth-first search\nalgorithms on massive directed dense graphs $(V,E)$. Our algorithms are\nI/O-optimal for $E/V = \\Omega (M)$ (and in the cache-aware setting for $\\lambda\n= O(M)$).\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 19:45:37 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 11:10:57 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 10:26:25 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Iacono", "John", ""], ["Jacob", "Riko", ""], ["Tsakalidis", "Konstantinos", ""]]}, {"id": "1903.03404", "submitter": "Zeke Wang Dr.", "authors": "Zeke Wang, Kaan Kara, Hantian Zhang, Gustavo Alonso, Onur Mutlu, Ce\n  Zhang", "title": "Accelerating Generalized Linear Models with MLWeaving: A\n  One-Size-Fits-All System for Any-precision Learning (Technical Report)", "comments": "18 pages", "journal-ref": "PVLDB, 2019", "doi": "10.14778/3317315.3317322", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from the data stored in a database is an important function\nincreasingly available in relational engines. Methods using lower precision\ninput data are of special interest given their overall higher efficiency but,\nin databases, these methods have a hidden cost: the quantization of the real\nvalue into a smaller number is an expensive step. To address the issue, in this\npaper we present MLWeaving, a data structure and hardware acceleration\ntechnique intended to speed up learning of generalized linear models in\ndatabases. ML-Weaving provides a compact, in-memory representation enabling the\nretrieval of data at any level of precision. MLWeaving also takes advantage of\nthe increasing availability of FPGA-based accelerators to provide a highly\nefficient implementation of stochastic gradient descent. The solution adopted\nin MLWeaving is more efficient than existing designs in terms of space (since\nit can process any resolution on the same design) and resources (via the use of\nbit-serial multipliers). MLWeaving also enables the runtime tuning of\nprecision, instead of a fixed precision level during the training. We\nillustrate this using a simple, dynamic precision schedule. Experimental\nresults show MLWeaving achieves up to16 performance improvement over\nlow-precision CPU implementations of first-order methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 13:03:11 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 08:25:41 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Wang", "Zeke", ""], ["Kara", "Kaan", ""], ["Zhang", "Hantian", ""], ["Alonso", "Gustavo", ""], ["Mutlu", "Onur", ""], ["Zhang", "Ce", ""]]}, {"id": "1903.03479", "submitter": "Ovidiu Vaduvescu", "authors": "Dorian Gorgan, Ovidiu Vaduvescu, Teodor Stefanut, Victor Bacu, Adrian\n  Sabou, Denisa Copandean Balazs, Constantin Nandra, Costin Boldea, Afrodita\n  Boldea, Marian Predatu, Viktoria Pinter and Adrian Stanica", "title": "NEARBY Platform for Automatic Asteroids Detection and EURONEAR Surveys", "comments": "ESA NEO and Debris Detection Conference, ESA/ESOC, Darmstadt,\n  Germany, 22-24 Jan 2019", "journal-ref": "Published online by the ESA Space Safety Programme Office, 2019", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The survey of the nearby space and continuous monitoring of the Near Earth\nObjects (NEOs) and especially Near Earth Asteroids (NEAs) are essential for the\nfuture of our planet and should represent a priority for our solar system\nresearch and nearby space exploration. More computing power and sophisticated\ndigital tracking algorithms are needed to cope with the larger astronomy\nimaging cameras dedicated for survey telescopes. The paper presents the NEARBY\nplatform that aims to experiment new algorithms for automatic image reduction,\ndetection and validation of moving objects in astronomical surveys,\nspecifically NEAs. The NEARBY platform has been developed and experimented\nthrough a collaborative research work between the Technical University of\nCluj-Napoca (UTCN) and the University of Craiova, Romania, using observing\ninfrastructure of the Instituto de Astrofisica de Canarias (IAC) and Isaac\nNewton Group (ING), La Palma, Spain. The NEARBY platform has been developed and\ndeployed on the UTCN's cloud infrastructure and the acquired images are\nprocessed remotely by the astronomers who transfer it from ING through the web\ninterface of the NEARBY platform. The paper analyzes and highlights the main\naspects of the NEARBY platform development, and the results and conclusions on\nthe EURONEAR surveys.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 14:56:01 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Gorgan", "Dorian", ""], ["Vaduvescu", "Ovidiu", ""], ["Stefanut", "Teodor", ""], ["Bacu", "Victor", ""], ["Sabou", "Adrian", ""], ["Balazs", "Denisa Copandean", ""], ["Nandra", "Constantin", ""], ["Boldea", "Costin", ""], ["Boldea", "Afrodita", ""], ["Predatu", "Marian", ""], ["Pinter", "Viktoria", ""], ["Stanica", "Adrian", ""]]}, {"id": "1903.03487", "submitter": "Carla Ferreira", "authors": "Andr\\'e Rijo and Carla Ferreira and Nuno Pregui\\c{c}a", "title": "Set CRDT com M\\'ultiplas Pol\\'iticas de Resolu\\c{c}\\~ao de Conflitos", "comments": "Article in Portuguese, accepted in the national informatics\n  conference INForum 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Um CRDT \\'e um tipo de dados que pode ser replicado e modificado\nconcorrentemente sem coordena\\c{c}\\~ao, garantindo-se a converg\\^encia das\nr\\'eplicas atrav\\'es da resolu\\c{c}\\~ao autom\\'atica de conflitos. Cada CRDT\nimplementa uma pol\\'itica espec\\'ifica para resolver conflitos. Por exemplo, um\nconjunto CRDT add-wins d\\'a prioridade ao \"add\" aquando da execu\\c{c}\\~ao\nconcorrente de um \"add\" e \"rem\" do mesmo elemento. Em algumas aplica\\c{c}\\~oes\npode ser necess\\'ario usar diferentes pol\\'iticas para diferentes\nexecu\\c{c}\\~oes de uma opera\\c{c}\\~ao -- por exemplo, uma aplica\\c{c}\\~ao que\nutilize um conjunto CRDT add-wins pode querer que alguns \"removes\" ganhem sobre\n\"adds\" concorrentes. Neste artigo \\'e apresentado e avaliado o desenho dum\nconjunto CRDT que implementa as sem\\^anticas referidas.\n  ---\n  Conflict-Free Replicated Data Types (CRDTs) allow objects to be replicated\nand concurrently modified without coordination. CRDTs solve conflicts\nautomatically and provide eventual consistency. Typically each CRDT uses a\nspecific policy for solving conflicts. For example, in an add-wins set CRDT,\nwhen an element is concurrently add and removed in different replicas, priority\nis given to add, i.e., the element stays in the set. Unfortunately, this may be\ninadequate for some applications - it may be desired to overrule the default\npolicy for some operation executions. For example, an application using an\nadd-wins set may want some removes to win over concurrent adds. This paper\npresent the design of a set CRDT that implements such semantics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:10:37 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Rijo", "Andr\u00e9", ""], ["Ferreira", "Carla", ""], ["Pregui\u00e7a", "Nuno", ""]]}, {"id": "1903.03520", "submitter": "William Kuszmaul", "authors": "Vladimir Braverman, Moses Charikar, William Kuszmaul, David P.\n  Woodruff, Lin F. Yang", "title": "The One-Way Communication Complexity of Dynamic Time Warping Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve the randomized one-way communication complexity of Dynamic Time\nWarping (DTW) distance. We show that there is an efficient one-way\ncommunication protocol using $\\widetilde{O}(n/\\alpha)$ bits for the problem of\ncomputing an $\\alpha$-approximation for DTW between strings $x$ and $y$ of\nlength $n$, and we prove a lower bound of $\\Omega(n / \\alpha)$ bits for the\nsame problem. Our communication protocol works for strings over an arbitrary\nmetric of polynomial size and aspect ratio, and we optimize the logarithmic\nfactors depending on properties of the underlying metric, such as when the\npoints are low-dimensional integer vectors equipped with various metrics or\nhave bounded doubling dimension. We also consider linear sketches of DTW,\nshowing that such sketches must have size $\\Omega(n)$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:47:51 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Braverman", "Vladimir", ""], ["Charikar", "Moses", ""], ["Kuszmaul", "William", ""], ["Woodruff", "David P.", ""], ["Yang", "Lin F.", ""]]}, {"id": "1903.03560", "submitter": "Grigorios Koumoutsos", "authors": "Erik D.Demaine, John Iacono, Grigorios Koumoutsos and Stefan Langerman", "title": "Belga B-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit self-adjusting external memory tree data structures, which combine\nthe optimal (and practical) worst-case I/O performances of B-trees, while\nadapting to the online distribution of queries. Our approach is analogous to\nundergoing efforts in the BST model, where Tango Trees (Demaine et al. 2007)\nwere shown to be $O(\\log\\log N)$-competitive with the runtime of the best\noffline binary search tree on every sequence of searches. Here we formalize the\nB-Tree model as a natural generalization of the BST model. We prove lower\nbounds for the B-Tree model, and introduce a B-Tree model data structure, the\nBelga B-tree, that executes any sequence of searches within a $O(\\log \\log N)$\nfactor of the best offline B-tree model algorithm, provided $B=\\log^{O(1)}N$.\nWe also show how to transform any static BST into a static B-tree which is\nfaster by a $\\Theta(\\log B)$ factor; the transformation is randomized and we\nshow that randomization is necessary to obtain any significant speedup.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 17:16:26 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Demaine", "Erik D.", ""], ["Iacono", "John", ""], ["Koumoutsos", "Grigorios", ""], ["Langerman", "Stefan", ""]]}, {"id": "1903.03605", "submitter": "Meena Jagadeesan", "authors": "Meena Jagadeesan", "title": "Understanding Sparse JL for Feature Hashing", "comments": "Appeared at NeurIPS 2019; this is the full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature hashing and other random projection schemes are commonly used to\nreduce the dimensionality of feature vectors. The goal is to efficiently\nproject a high-dimensional feature vector living in $\\mathbb{R}^n$ into a much\nlower-dimensional space $\\mathbb{R}^m$, while approximately preserving\nEuclidean norm. These schemes can be constructed using sparse random\nprojections, for example using a sparse Johnson-Lindenstrauss (JL) transform. A\nline of work introduced by Weinberger et. al (ICML '09) analyzes the accuracy\nof sparse JL with sparsity 1 on feature vectors with small\n$\\ell_\\infty$-to-$\\ell_2$ norm ratio. Recently, Freksen, Kamma, and Larsen\n(NeurIPS '18) closed this line of work by proving a tight tradeoff between\n$\\ell_\\infty$-to-$\\ell_2$ norm ratio and accuracy for sparse JL with sparsity\n$1$.\n  In this paper, we demonstrate the benefits of using sparsity $s$ greater than\n$1$ in sparse JL on feature vectors. Our main result is a tight tradeoff\nbetween $\\ell_\\infty$-to-$\\ell_2$ norm ratio and accuracy for a general\nsparsity $s$, that significantly generalizes the result of Freksen et. al. Our\nresult theoretically demonstrates that sparse JL with $s > 1$ can have\nsignificantly better norm-preservation properties on feature vectors than\nsparse JL with $s = 1$; we also empirically demonstrate this finding.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 18:50:42 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 17:51:43 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Jagadeesan", "Meena", ""]]}, {"id": "1903.03636", "submitter": "George Mertzios", "authors": "Eleni C. Akrida, George B. Mertzios, Sotiris Nikoletseas, Christoforos\n  Raptopoulos, Paul G. Spirakis, and Viktor Zamaraev", "title": "How fast can we reach a target vertex in stochastic temporal graphs?", "comments": "22 pages, 2 figures, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs are used to abstractly model real-life networks that are\ninherently dynamic in nature. Given a static underlying graph $G=(V,E)$, a\ntemporal graph on $G$ is a sequence of snapshots $G_t$, one for each time step\n$t\\geq 1$. In this paper we study stochastic temporal graphs, i.e. stochastic\nprocesses $\\mathcal{G}$ whose random variables are the snapshots of a temporal\ngraph on $G$. A natural feature observed in various real-life scenarios is a\nmemory effect in the appearance probabilities of particular edges; i.e. the\nprobability an edge $e\\in E$ appears at time step $t$ depends on its appearance\n(or absence) at the previous $k$ steps. In this paper we study the hierarchy of\nmodels memory-$k$, addressing this memory effect in an edge-centric network\nevolution: every edge of $G$ has its own independent probability distribution\nfor its appearance over time. Clearly, for every $k\\geq 1$, memory-$(k-1)$ is a\nspecial case of memory-$k$. We make a clear distinction between the values\n$k=0$ (\"no memory\") and $k\\geq 1$ (\"some memory\"), as in some cases these\nmodels exhibit a fundamentally different computational behavior, as our results\nindicate. For every $k\\geq 0$ we investigate the complexity of two naturally\nrelated, but fundamentally different, temporal path (journey) problems: MINIMUM\nARRIVAL and BEST POLICY. In the first problem we are looking for the expected\narrival time of a foremost journey between two designated vertices $s,y$. In\nthe second one we are looking for the arrival time of the best policy for\nactually choosing a particular $s$-$y$ journey. We present a detailed\ninvestigation of the computational landscape of both problems for the different\nvalues of memory $k$. Among other results we prove that, surprisingly, MINIMUM\nARRIVAL is strictly harder than BEST POLICY; in fact, for $k=0$, MINIMUM\nARRIVAL is #P-hard while BEST POLICY is solvable in $O(n^2)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 19:22:56 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Akrida", "Eleni C.", ""], ["Mertzios", "George B.", ""], ["Nikoletseas", "Sotiris", ""], ["Raptopoulos", "Christoforos", ""], ["Spirakis", "Paul G.", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "1903.03697", "submitter": "Kiril Solovey", "authors": "Kiril Solovey and Mauro Salazar and Marco Pavone", "title": "Scalable and Congestion-aware Routing for Autonomous Mobility-on-Demand\n  via Frank-Wolfe Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of vehicle routing for Autonomous Mobility-on-Demand\n(AMoD) systems, wherein a fleet of self-driving vehicles provides on-demand\nmobility in a given environment. Specifically, the task it to compute routes\nfor the vehicles (both customer-carrying and empty travelling) so that travel\ndemand is fulfilled and operational cost is minimized. The routing process must\naccount for congestion effects affecting travel times, as modeled via a\nvolume-delay function (VDF). Route planning with VDF constraints is notoriously\nchallenging, as such constraints compound the combinatorial complexity of the\nrouting optimization process. Thus, current solutions for AMoD routing resort\nto relaxations of the congestion constraints, thereby trading optimality with\ncomputational efficiency. In this paper, we present the first\ncomputationally-efficient approach for AMoD routing where VDF constraints are\nexplicitly accounted for. We demonstrate that our approach is faster by at\nleast one order of magnitude with respect to the state of the art, while\nproviding higher quality solutions. From a methodological standpoint, the key\ntechnical insight is to establish a mathematical reduction of the AMoD routing\nproblem to the classical traffic assignment problem (a related vehicle-routing\nproblem where empty traveling vehicles are not present). Such a reduction\nallows us to extend powerful algorithmic tools for traffic assignment, which\ncombine the classic Frank-Wolfe algorithm with modern techniques for\npathfinding, to the AMoD routing problem. We provide strong theoretical\nguarantees for our approach in terms of near-optimality of the returned\nsolution.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 23:26:46 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Solovey", "Kiril", ""], ["Salazar", "Mauro", ""], ["Pavone", "Marco", ""]]}, {"id": "1903.03944", "submitter": "Balasubramanian Sivan", "authors": "Nikhil R. Devanur, Kamal Jain, Balasubramanian Sivan and Christopher\n  A. Wilkens", "title": "Near Optimal Online Algorithms and Fast Approximation Algorithms for\n  Resource Allocation Problems", "comments": "Appeared in the Journal of the ACM, Volume 66, Issue 1, Pages\n  7:1--7:41, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present prior robust algorithms for a large class of resource allocation\nproblems where requests arrive one-by-one (online), drawn independently from an\nunknown distribution at every step. We design a single algorithm that, for\nevery possible underlying distribution, obtains a $1-\\epsilon$ fraction of the\nprofit obtained by an algorithm that knows the entire request sequence ahead of\ntime. The factor $\\epsilon$ approaches $0$ when no single request\nconsumes/contributes a significant fraction of the global\nconsumption/contribution by all requests together. We show that the tradeoff we\nobtain here that determines how fast $\\epsilon$ approaches $0$, is near\noptimal: we give a nearly matching lower bound showing that the tradeoff cannot\nbe improved much beyond what we obtain.\n  Going beyond the model of a static underlying distribution, we introduce the\nadversarial stochastic input model, where an adversary, possibly in an adaptive\nmanner, controls the distributions from which the requests are drawn at each\nstep. Placing no restriction on the adversary, we design an algorithm that\nobtains a $1-\\epsilon$ fraction of the optimal profit obtainable w.r.t. the\nworst distribution in the adversarial sequence.\n  In the offline setting we give a fast algorithm to solve very large LPs with\nboth packing and covering constraints. We give algorithms to approximately\nsolve (within a factor of $1+\\epsilon$) the mixed packing-covering problem with\n$O(\\frac{\\gamma m \\log (n/\\delta)}{\\epsilon^2})$ oracle calls where the\nconstraint matrix of this LP has dimension $n\\times m$, the success probability\nof the algorithm is $1-\\delta$, and $\\gamma$ quantifies how significant a\nsingle request is when compared to the sum total of all requests.\n  We discuss implications of our results to several special cases including\nonline combinatorial auctions, network routing and the adwords problem.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 07:45:34 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Devanur", "Nikhil R.", ""], ["Jain", "Kamal", ""], ["Sivan", "Balasubramanian", ""], ["Wilkens", "Christopher A.", ""]]}, {"id": "1903.04054", "submitter": "Samuel Zbarsky", "authors": "Samuel Zbarsky", "title": "Asymptotically faster algorithm for counting self-avoiding walks and\n  self-avoiding polygons", "comments": "Added citations and rewrote some places for clarity", "journal-ref": "Journal of Physics A: Mathematical and Theoretical, 52(50):505001,\n  nov 2019", "doi": "10.1088/1751-8121/ab52b0", "report-no": null, "categories": "cs.DS cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for counting self-avoiding walks or self-avoiding\npolygons that runs in time $\\exp(C\\sqrt{n\\log n})$ on 2-dimensional lattices\nand time $\\exp(C_dn^{(d-1)/d}\\log n)$ on $d$-dimensional lattices for $d>2$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 20:22:33 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 02:20:33 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 04:12:31 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Zbarsky", "Samuel", ""]]}, {"id": "1903.04097", "submitter": "Zhonghua Han", "authors": "Zhonghua Han, Jingyuan Zhang, Xiaoting Dong and Yuanwei Qi", "title": "Bus Manufacturing Workshop Scheduling Method with Routing Buffer", "comments": "The paper has been accepted by IJSPM, and this is the authors'\n  version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at solving the problem that the moving route is complicated and the\nscheduling is difficult in the routing buffer of the bus in the manufacturing\nworkshop, a routing buffer mathematical programming model for bus manufacturing\nworkshop is proposed. We design a moving approach for minimizing the total\nsetup cost for moving in routing buffer. The framework and the solution ofthe\noptimization problem of such a bus manufacturing workshop scheduling with\nrouting buffer arepresented. The evaluation results show that, comparing with\nthe irregularly guided moving method, the proposed method can better guide the\nbus movement in routing buffer by reducing the total setup time of all buses\nprocessed at the next stage, and obtaining a better scheduling optimization\nsolution with minimize maximum total completion time.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 01:44:32 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 19:11:43 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Han", "Zhonghua", ""], ["Zhang", "Jingyuan", ""], ["Dong", "Xiaoting", ""], ["Qi", "Yuanwei", ""]]}, {"id": "1903.04300", "submitter": "Arthur Queffelec", "authors": "Tristan Charrier, Arthur Queffelec, Ocan Sankur and Fran\\c{c}ois\n  Schwarzentruber", "title": "Reachability and Coverage Planning for Connected Agents: Extended\n  Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing appeal of robots in information-gathering\nmissions, we study multi-agent path planning problems in which the agents must\nremain interconnected. We model an area by a topological graph specifying the\nmovement and the connectivity constraints of the agents. We study the\ntheoretical complexity of the reachability and the coverage problems of a fleet\nof connected agents on various classes of topological graphs. We establish the\ncomplexity of these problems on known classes, and introduce a new class called\nsight-moveable graphs which admit efficient algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 13:52:57 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Charrier", "Tristan", ""], ["Queffelec", "Arthur", ""], ["Sankur", "Ocan", ""], ["Schwarzentruber", "Fran\u00e7ois", ""]]}, {"id": "1903.04351", "submitter": "Shaofeng Jiang", "authors": "Vladimir Braverman and Shaofeng H.-C. Jiang and Robert Krauthgamer and\n  Xuan Wu", "title": "Coresets for Ordered Weighted Clustering", "comments": "23 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design coresets for Ordered k-Median, a generalization of classical\nclustering problems such as k-Median and k-Center, that offers a more flexible\ndata analysis, like easily combining multiple objectives (e.g., to increase\nfairness or for Pareto optimization). Its objective function is defined via the\nOrdered Weighted Averaging (OWA) paradigm of Yager (1988), where data points\nare weighted according to a predefined weight vector, but in order of their\ncontribution to the objective (distance from the centers).\n  A powerful data-reduction technique, called a coreset, is to summarize a\npoint set $X$ in $\\mathbb{R}^d$ into a small (weighted) point set $X'$, such\nthat for every set of $k$ potential centers, the objective value of the coreset\n$X'$ approximates that of $X$ within factor $1\\pm \\epsilon$. When there are\nmultiple objectives (weights), the above standard coreset might have limited\nusefulness, whereas in a \\emph{simultaneous} coreset, which was introduced\nrecently by Bachem and Lucic and Lattanzi (2018), the above approximation holds\nfor all weights (in addition to all centers). Our main result is a construction\nof a simultaneous coreset of size $O_{\\epsilon, d}(k^2 \\log^2 |X|)$ for Ordered\nk-Median.\n  To validate the efficacy of our coreset construction we ran experiments on a\nreal geographical data set. We find that our algorithm produces a small\ncoreset, which translates to a massive speedup of clustering computations,\nwhile maintaining high accuracy for a range of weights.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 15:07:37 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Braverman", "Vladimir", ""], ["Jiang", "Shaofeng H. -C.", ""], ["Krauthgamer", "Robert", ""], ["Wu", "Xuan", ""]]}, {"id": "1903.04650", "submitter": "Yihan Sun", "authors": "Guy E. Blelloch and Jeremy T. Fineman and Yan Gu and Yihan Sun", "title": "Optimal (Randomized) Parallel Algorithms in the Binary-Forking Model", "comments": null, "journal-ref": null, "doi": "10.1145/3350755.3400227", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop optimal algorithms in the binary-forking model for a\nvariety of fundamental problems, including sorting, semisorting, list ranking,\ntree contraction, range minima, and ordered set union, intersection and\ndifference. In the binary-forking model, tasks can only fork into two child\ntasks, but can do so recursively and asynchronously. The tasks share memory,\nsupporting reads, writes and test-and-sets. Costs are measured in terms of work\n(total number of instructions), and span (longest dependence chain).\n  The binary-forking model is meant to capture both algorithm performance and\nalgorithm-design considerations on many existing multithreaded languages, which\nare also asynchronous and rely on binary forks either explicitly or under the\ncovers. In contrast to the widely studied PRAM model, it does not assume\narbitrary-way forks nor synchronous operations, both of which are hard to\nimplement in modern hardware. While optimal PRAM algorithms are known for the\nproblems studied herein, it turns out that arbitrary-way forking and strict\nsynchronization are powerful, if unrealistic, capabilities. Natural simulations\nof these PRAM algorithms in the binary-forking model (i.e., implementations in\nexisting parallel languages) incur an $\\Omega(\\log n)$ overhead in span. This\npaper explores techniques for designing optimal algorithms when limited to\nbinary forking and assuming asynchrony. All algorithms described in this paper\nare the first algorithms with optimal work and span in the binary-forking\nmodel. Most of the algorithms are simple. Many are randomized.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 23:19:43 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 02:14:44 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 21:00:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Fineman", "Jeremy T.", ""], ["Gu", "Yan", ""], ["Sun", "Yihan", ""]]}, {"id": "1903.04761", "submitter": "Marcin Pilipczuk", "authors": "Maria Chudnovsky, Marcin Pilipczuk, Micha{\\l} Pilipczuk, and St\\'ephan\n  Thomass\\'e", "title": "On the Maximum Weight Independent Set Problem in graphs without induced\n  cycles of length at least five", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hole in a graph is an induced cycle of length at least $4$, and an antihole\nis the complement of an induced cycle of length at least $4$. A hole or\nantihole is long if its length is at least $5$. For an integer $k$, the\n$k$-prism is the graph consisting of two cliques of size $k$ joined by a\nmatching. The complexity of Maximum (Weight) Independent Set (MWIS) in\nlong-hole-free graphs remains an important open problem. In this paper we give\na polynomial time algorithm to solve MWIS in long-hole-free graphs with no\n$k$-prism (for any fixed integer $k$), and a subexponential algorithm for MWIS\nin long-hole-free graphs in general. As a special case this gives a polynomial\ntime algorithm to find a maximum weight clique in perfect graphs with no long\nantihole, and no hole of length $6$. The algorithms use the framework of\nminimal chordal completions and potential maximal cliques.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 07:37:40 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 08:49:20 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Chudnovsky", "Maria", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Thomass\u00e9", "St\u00e9phan", ""]]}, {"id": "1903.04925", "submitter": "Angana Chakraborty", "authors": "Angana Chakraborty and Sanghamitra Bandyopadhyay", "title": "conLSH: Context based Locality Sensitive Hashing for Mapping of noisy\n  SMRT Reads", "comments": "arXiv admin note: text overlap with arXiv:1705.03933", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Molecule Real-Time (SMRT) sequencing is a recent advancement of Next\nGen technology developed by Pacific Bio (PacBio). It comes with an explosion of\nlong and noisy reads demanding cutting edge research to get most out of it. To\ndeal with the high error probability of SMRT data, a novel contextual Locality\nSensitive Hashing (conLSH) based algorithm is proposed in this article, which\ncan effectively align the noisy SMRT reads to the reference genome. Here,\nsequences are hashed together based not only on their closeness, but also on\nsimilarity of context. The algorithm has $\\mathcal{O}(n^{\\rho+1})$ space\nrequirement, where $n$ is the number of sequences in the corpus and $\\rho$ is a\nconstant. The indexing time and querying time are bounded by $\\mathcal{O}(\n\\frac{n^{\\rho+1} \\cdot \\ln n}{\\ln \\frac{1}{P_2}})$ and $\\mathcal{O}(n^\\rho)$\nrespectively, where $P_2 > 0$, is a probability value. This algorithm is\nparticularly useful for retrieving similar sequences, a widely used task in\nbiology. The proposed conLSH based aligner is compared with rHAT, popularly\nused for aligning SMRT reads, and is found to comprehensively beat it in speed\nas well as in memory requirements. In particular, it takes approximately\n$24.2\\%$ less processing time, while saving about $70.3\\%$ in peak memory\nrequirement for H.sapiens PacBio dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:49:01 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Chakraborty", "Angana", ""], ["Bandyopadhyay", "Sanghamitra", ""]]}, {"id": "1903.04936", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki", "title": "The k-d tree data structure and a proof for neighborhood computation in\n  expected logarithmic time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For practical applications, any neighborhood concept imposed on a finite\npoint set P is not of any use if it cannot be computed efficiently. Thus, in\nthis paper, we give an introduction to the data structure of k-d trees, first\npresented by Friedman, Bentley, and Finkel in 1977. After a short introduction\nto the data structure (Section 1), we turn to the proof of efficiency by\nFriedman and his colleagues (Section 2). The main contribution of this paper is\nthe translation of the proof of Freedman, Bentley, and Finkel into modern terms\nand the elaboration of the proof.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:05:07 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Skrodzki", "Martin", ""]]}, {"id": "1903.04996", "submitter": "Adam Kurpisz", "authors": "Adam Kurpisz, Timo de Wolff", "title": "New Dependencies of Hierarchies in Polynomial Optimization", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.AG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare four key hierarchies for solving Constrained Polynomial\nOptimization Problems (CPOP): Sum of Squares (SOS), Sum of Diagonally Dominant\nPolynomials (SDSOS), Sum of Nonnegative Circuits (SONC), and the Sherali Adams\n(SA) hierarchies. We prove a collection of dependencies among these hierarchies\nboth for general CPOPs and for optimization problems on the Boolean hypercube.\nKey results include for the general case that the SONC and SOS hierarchy are\npolynomially incomparable, while SDSOS is contained in SONC. A direct\nconsequence is the non-existence of a Putinar-like Positivstellensatz for\nSDSOS. On the Boolean hypercube, we show as a main result that Schm\\\"udgen-like\nversions of the hierarchies SDSOS*, SONC*, and SA* are polynomially equivalent.\nMoreover, we show that SA* is contained in any Schm\\\"udgen-like hierarchy that\nprovides a O(n) degree bound.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 15:29:15 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Kurpisz", "Adam", ""], ["de Wolff", "Timo", ""]]}, {"id": "1903.05239", "submitter": "Babak Hosseini", "authors": "Babak Hosseini, Barbara Hammer", "title": "Non-Negative Local Sparse Coding for Subspace Clustering", "comments": "15 pages, IDA 2018 conference", "journal-ref": null, "doi": "10.1007/978-3-030-01768-2_12", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace sparse coding (SSC) algorithms have proven to be beneficial to\nclustering problems. They provide an alternative data representation in which\nthe underlying structure of the clusters can be better captured. However, most\nof the research in this area is mainly focused on enhancing the sparse coding\npart of the problem. In contrast, we introduce a novel objective term in our\nproposed SSC framework which focuses on the separability of data points in the\ncoding space. We also provide mathematical insights into how this\nlocal-separability term improves the clustering result of the SSC framework.\nOur proposed non-linear local SSC algorithm (NLSSC) also benefits from the\nefficient choice of its sparsity terms and constraints. The NLSSC algorithm is\nalso formulated in the kernel-based framework (NLKSSC) which can represent the\nnonlinear structure of data. In addition, we address the possibility of having\nredundancies in sparse coding results and its negative effect on graph-based\nclustering problems. We introduce the link-restore post-processing step to\nimprove the representation graph of non-negative SSC algorithms such as ours.\nEmpirical evaluations on well-known clustering benchmarks show that our\nproposed NLSSC framework results in better clusterings compared to the\nstate-of-the-art baselines and demonstrate the effectiveness of the\nlink-restore post-processing in improving the clustering accuracy via\ncorrecting the broken links of the representation graph.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 22:20:10 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Hosseini", "Babak", ""], ["Hammer", "Barbara", ""]]}, {"id": "1903.05255", "submitter": "Jie Xue", "authors": "Haitao Wang, Jie Xue", "title": "Near-Optimal Algorithms for Shortest Paths in Weighted Unit-Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a classical graph-theoretic problem, the \\textit{single-source\nshortest-path} (SSSP) problem, in weighted unit-disk graphs. We first propose\nan exact (and deterministic) algorithm which solves the problem in $O(n \\log^2\nn)$ time using linear space, where $n$ is the number of the vertices of the\ngraph. This significantly improves the previous deterministic algorithm by\nCabello and Jej\\v{c}i\\v{c} [CGTA'15] which uses $O(n^{1+\\delta})$ time and\n$O(n^{1+\\delta})$ space (for any small constant $\\delta>0$) and the previous\nrandomized algorithm by Kaplan et al. [SODA'17] which uses $O(n \\log^{12+o(1)}\nn)$ expected time and $O(n \\log^3 n)$ space. More specifically, we show that if\nthe 2D offline insertion-only (additively-)weighted nearest-neighbor problem\nwith $k$ operations (i.e., insertions and queries) can be solved in $f(k)$\ntime, then the SSSP problem in weighted unit-disk graphs can be solved in $O(n\n\\log n+f(n))$ time. Using the same framework with some new ideas, we also\nobtain a $(1+\\varepsilon)$-approximate algorithm for the problem, using $O(n\n\\log n + n \\log^2(1/\\varepsilon))$ time and linear space. This improves the\nprevious $(1+\\varepsilon)$-approximate algorithm by Chan and Skrepetos\n[SoCG'18] which uses $O((1/\\varepsilon)^2 n \\log n)$ time and\n$O((1/\\varepsilon)^2 n)$ space. More specifically, we show that if the 2D\noffline insertion-only weighted nearest-neighbor problem with $k_1$ operations\nin which at most $k_2$ operations are insertions can be solved in $f(k_1,k_2)$\ntime, then the $(1+\\varepsilon)$-approximate SSSP problem in weighted unit-disk\ngraphs can be solved in $O(n \\log n+f(n,O(\\varepsilon^{-2})))$ time. Because of\nthe $\\Omega(n \\log n)$-time lower bound of the problem (even when approximation\nis allowed), both of our algorithms are almost optimal.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 23:11:02 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Wang", "Haitao", ""], ["Xue", "Jie", ""]]}, {"id": "1903.05377", "submitter": "Lavanya Selvaganesh  PhD", "authors": "Sivakumar Karunakaran, Lavanya Selvaganesh", "title": "Efficient Shortest Path Algorithm Using An Unique And Novel Graph Matrix\n  Representation", "comments": "Some of the results proved here were found to be with mistakes and\n  requires major revision. It should therefore be removed/withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neighbourhood matrix, $\\mathcal{NM}(G)$, a novel representation of graphs\nproposed in \\cite {ALPaper} is defined using the neighbourhood sets of the\nvertices. The matrix also exhibits a bijection between the product of two\nwell-known graph matrices, namely the adjacency matrix and the Laplacian\nmatrix. In this article, we extend this work and introduce the sequence of\npowers of $\\mathcal{NM}(G)$ and denote it by $ \\mathcal{NM}^{\\{l\\}}, 1\\leq l\n\\leq k(G) $ where $ k(G) $ is called the \\textbf{iteration number}, $\nk(G)=\\lceil{\\log_{2} diameter(G)}\\rceil$. The sequence of matrices captures the\ndistance between the vertices in a profound fashion and is found to be useful\nin various applications. One of the interesting results of this article is that\nwhenever $ \\eta_{ij}^{\\{l\\}}=-1$, for $ 1\\leq l \\leq k(G) $, then\n$d_{G}(i,j)=2^{l}$ , where $d_{G}(i,j)$ is the shortest path distance between $\ni $ and $ j $. Further, we characterize the entries of the matrices $\n\\mathcal{NM}^{\\{l\\}}$, for every $l, 1\\leq l \\leq k(G)$. Using this concept of\nthe sequence of powers of neighbourhood matrix and with the aid of some of its\nproperties, we propose an algorithm to find the shortest path between any pair\nof vertices in a given undirected unweighted simple graph. The proposed\nalgorithm and the claims therein are formally validated through simulations on\nsynthetic data and the real network data from Facebook where sampling-based\ncomputations are performed for large collection of graphs containing\nlarge-sized graph. The empirical results are quite promising with our algorithm\nhaving the best running time among all the existing well known shortest path\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 09:37:33 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 11:07:07 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Karunakaran", "Sivakumar", ""], ["Selvaganesh", "Lavanya", ""]]}, {"id": "1903.05617", "submitter": "Nikolai Karpov", "authors": "Sepehr Assadi, Nikolai Karpov, Qin Zhang", "title": "Distributed and Streaming Linear Programming in Low Dimensions", "comments": "To appear in PODS'19; 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study linear programming and general LP-type problems in several big data\n(streaming and distributed) models. We mainly focus on low dimensional problems\nin which the number of constraints is much larger than the number of variables.\nLow dimensional LP-type problems appear frequently in various machine learning\ntasks such as robust regression, support vector machines, and core vector\nmachines. As supporting large-scale machine learning queries in database\nsystems has become an important direction for database research, obtaining\nefficient algorithms for low dimensional LP-type problems on massive datasets\nis of great value. In this paper we give both upper and lower bounds for\nLP-type problems in distributed and streaming models. Our bounds are almost\ntight when the dimensionality of the problem is a fixed constant.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 17:31:34 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Assadi", "Sepehr", ""], ["Karpov", "Nikolai", ""], ["Zhang", "Qin", ""]]}, {"id": "1903.05832", "submitter": "Palash Dey", "authors": "Palash Dey, Sourav Medya", "title": "Covert Networks: How Hard is It to Hide?", "comments": "Accepted as a full paper in AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covert networks are social networks that often consist of harmful users.\nSocial Network Analysis (SNA) has played an important role in reducing criminal\nactivities (e.g., counter terrorism) via detecting the influential users in\nsuch networks. There are various popular measures to quantify how influential\nor central any vertex is in a network. As expected, strategic and influential\nmiscreants in covert networks would try to hide herself and her partners\n(called {\\em leaders}) from being detected via these measures by introducing\nnew edges. Waniek et al. show that the corresponding computational problem,\ncalled Hiding Leader, is NP-Complete for the degree and closeness centrality\nmeasures. We study the popular core centrality measure and show that the\nproblem is NP-Complete even when the core centrality of every leader is only\n$3$. On the contrary, we prove that the problem becomes polynomial time\nsolvable for the degree centrality measure if the degree of every leader is\nbounded above by any constant. We then focus on the optimization version of the\nproblem and show that the Hiding Leader problem admits a $2$ factor\napproximation algorithm for the degree centrality measure. We complement it by\nproving that one cannot hope to have any $(2-\\varepsilon)$ factor approximation\nalgorithm for any constant $\\varepsilon>0$ unless there is a $\\varepsilon/2$\nfactor polynomial time algorithm for the Densest $k$-Subgraph problem which\nwould be considered a significant breakthrough.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 06:42:21 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Dey", "Palash", ""], ["Medya", "Sourav", ""]]}, {"id": "1903.05948", "submitter": "Junping Zhou", "authors": "Luzhi Wang, Chu-Min Li, Junping Zhou, Bo Jin, Minghao Yin", "title": "An Exact Algorithm for Minimum Weight Vertex Cover Problem in Large\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel branch-and-bound(BMWVC) algorithm to exactly\nsolve the minimum weight vertex cover problem (MWVC) in large graphs. The\noriginal contribution is several new graph reduction rules, allowing to reduce\na graph G and the time needed to find a minimum weight vertex cover in G.\nExperiments on large graphs from real-world applications show that the\nreduction rules are effective and the resulting BMWVC algorithm outperforms\nrelevant exact and heuristic MWVC algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:44:55 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Wang", "Luzhi", ""], ["Li", "Chu-Min", ""], ["Zhou", "Junping", ""], ["Jin", "Bo", ""], ["Yin", "Minghao", ""]]}, {"id": "1903.05956", "submitter": "Janne H. Korhonen", "authors": "Keren Censor-Hillel, Michal Dory, Janne H. Korhonen, Dean Leitersdorf", "title": "Fast Approximate Shortest Paths in the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design fast deterministic algorithms for distance computation in the\ncongested clique model. Our key contributions include:\n  -- A $(2+\\epsilon)$-approximation for all-pairs shortest paths in\n$O(\\log^2{n} / \\epsilon)$ rounds on unweighted undirected graphs. With a small\nadditional additive factor, this also applies for weighted graphs. This is the\nfirst sub-polynomial constant-factor approximation for APSP in this model.\n  -- A $(1+\\epsilon)$-approximation for multi-source shortest paths from\n$O(\\sqrt{n})$ sources in $O(\\log^2{n} / \\epsilon)$ rounds on weighted\nundirected graphs. This is the first sub-polynomial algorithm obtaining this\napproximation for a set of sources of polynomial size.\n  Our main techniques are new distance tools that are obtained via improved\nalgorithms for sparse matrix multiplication, which we leverage to construct\nefficient hopsets and shortest paths. Furthermore, our techniques extend to\nadditional distance problems for which we improve upon the state-of-the-art,\nincluding diameter approximation, and an exact single-source shortest paths\nalgorithm for weighted undirected graphs in $\\tilde{O}(n^{1/6})$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:54:11 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 16:18:59 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""], ["Korhonen", "Janne H.", ""], ["Leitersdorf", "Dean", ""]]}, {"id": "1903.06061", "submitter": "Christine Dahn", "authors": "Markus Chimani, Christine Dahn, Martina Juhnke-Kubitzke, Nils M.\n  Kriege, Petra Mutzel, Alexander Nover", "title": "Maximum Cut Parameterized by Crossing Number", "comments": null, "journal-ref": "J. Graph Algorithms Appl. 24(3): 155-170 (2020)", "doi": "10.7155/jgaa.00523", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an edge-weighted graph $G$ on $n$ nodes, the NP-hard Max-Cut problem\nasks for a node bipartition such that the sum of edge weights joining the\ndifferent partitions is maximized. We propose a fixed-parameter tractable\nalgorithm parameterized by the number $k$ of crossings in a given drawing of\n$G$. Our algorithm achieves a running time of $O(2^k \\cdot p(n + k))$, where\n$p$ is the polynomial running time for planar Max-Cut. The only previously\nknown similar algorithm [8] is restricted to 1-planar graphs (i.e., at most one\ncrossing per edge) and its dependency on $k$ is of order $3^k$ . A direct\nconsequence of our result is that Max-Cut is fixed-parameter tractable w.r.t.\nthe crossing number, even without a given drawing. Moreover, the results\nnaturally carry over to the minor crossing number.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 15:05:47 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 08:33:51 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 12:21:47 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Chimani", "Markus", ""], ["Dahn", "Christine", ""], ["Juhnke-Kubitzke", "Martina", ""], ["Kriege", "Nils M.", ""], ["Mutzel", "Petra", ""], ["Nover", "Alexander", ""]]}, {"id": "1903.06081", "submitter": "Heng Guo", "authors": "Mary Cryan, Heng Guo, Giorgos Mousa", "title": "Modified log-Sobolev inequalities for strongly log-concave distributions", "comments": "accepted to Annals of Probability. Simplified proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the modified log-Sobolev constant for a natural Markov chain\nwhich converges to an $r$-homogeneous strongly log-concave distribution is at\nleast $1/r$. Applications include a sharp mixing time bound for the\nbases-exchange walk for matroids, and a concentration bound for Lipschitz\nfunctions over these distributions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 15:37:33 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 21:54:19 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 15:50:39 GMT"}, {"version": "v4", "created": "Sat, 8 Aug 2020 18:59:01 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cryan", "Mary", ""], ["Guo", "Heng", ""], ["Mousa", "Giorgos", ""]]}, {"id": "1903.06289", "submitter": "Shunsuke Inenaga", "authors": "Noriki Fujisato, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "The Parameterized Position Heap of a Trie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\Sigma$ and $\\Pi$ be disjoint alphabets of respective size $\\sigma$ and\n$\\pi$. Two strings over $\\Sigma \\cup \\Pi$ of equal length are said to\nparameterized match (p-match) if there is a bijection $f:\\Sigma \\cup \\Pi\n\\rightarrow \\Sigma \\cup \\Pi$ such that (1) $f$ is identity on $\\Sigma$ and (2)\n$f$ maps the characters of one string to those of the other string so that the\ntwo strings become identical. We consider the p-matching problem on a\n(reversed) trie $\\mathcal{T}$ and a string pattern $P$ such that every path\nthat p-matches $P$ has to be reported. Let $N$ be the size of the given trie\n$\\mathcal{T}$. In this paper, we propose the parameterized position heap for\n$\\mathcal{T}$ that occupies $O(N)$ space and supports p-matching queries in\n$O(m \\log (\\sigma + \\pi) + m \\pi + \\mathit{pocc}))$ time, where $m$ is the\nlength of a query pattern $P$ and $\\mathit{pocc}$ is the number of paths in\n$\\mathcal{T}$ to report. We also present an algorithm which constructs the\nparameterized position heap for a given trie $\\mathcal{T}$ in $O(N (\\sigma +\n\\pi))$ time and working space.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 22:45:34 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Fujisato", "Noriki", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1903.06290", "submitter": "Shunsuke Inenaga", "authors": "Kiichi Watanabe, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "Fast Algorithms for the Shortest Unique Palindromic Substring Problem on\n  Run-Length Encoded Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a string $S$, a palindromic substring $S[i..j]$ is said to be a\n\\emph{shortest unique palindromic substring} ($\\mathit{SUPS}$) for an interval\n$[s, t]$ in $S$, if $S[i..j]$ occurs exactly once in $S$, the interval $[i, j]$\ncontains $[s, t]$, and every palindromic substring containing $[s, t]$ which is\nshorter than $S[i..j]$ occurs at least twice in $S$. In this paper, we study\nthe problem of answering $\\mathit{SUPS}$ queries on run-length encoded strings.\nWe show how to preprocess a given run-length encoded string $\\mathit{RLE}_{S}$\nof size $m$ in $O(m)$ space and $O(m \\log \\sigma_{\\mathit{RLE}_{S}} + m\n\\sqrt{\\log m / \\log\\log m})$ time so that all $\\mathit{SUPSs}$ for any\nsubsequent query interval can be answered in $O(\\sqrt{\\log m / \\log\\log m} +\n\\alpha)$ time, where $\\alpha$ is the number of outputs, and\n$\\sigma_{\\mathit{RLE}_{S}}$ is the number of distinct runs of\n$\\mathit{RLE}_{S}$. Additionaly, we consider a variant of the SUPS problem\nwhere a query interval is also given in a run-length encoded form. For this\nvariant of the problem, we present two alternative algorithms with faster\nqueries. The first one answers queries in $O(\\sqrt{\\log\\log m /\\log\\log\\log m}\n+ \\alpha)$ time and can be built in $O(m \\log \\sigma_{\\mathit{RLE}_{S}} + m\n\\sqrt{\\log m / \\log\\log m})$ time, and the second one answers queries in\n$O(\\log \\log m + \\alpha)$ time and can be built in $O(m \\log\n\\sigma_{\\mathit{RLE}_{S}})$ time. Both of these data structures require $O(m)$\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 22:46:55 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 02:30:35 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Watanabe", "Kiichi", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1903.06350", "submitter": "Jiaxin Xie", "authors": "Jiaxin Xie and Zhiqiang Xu", "title": "Subset Selection for Matrices with Fixed Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection for matrices is the task of extracting a column sub-matrix\nfrom a given matrix $B\\in\\mathbb{R}^{n\\times m}$ with $m>n$ such that the\npseudoinverse of the sampled matrix has as small Frobenius or spectral norm as\npossible. In this paper, we consider a more general problem of subset selection\nfor matrices that allows a block to be fixed at the beginning. Under this\nsetting, we provide a deterministic method for selecting a column sub-matrix\nfrom $B$. We also present a bound for both the Frobenius and spectral norms of\nthe pseudoinverse of the sampled matrix, showing that the bound is\nasymptotically optimal. The main technology for proving this result is the\ninterlacing families of polynomials developed by Marcus, Spielman, and\nSrivastava. This idea also results in a deterministic greedy selection\nalgorithm that produces the sub-matrix promised by our result.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 04:03:25 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 06:49:21 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Xie", "Jiaxin", ""], ["Xu", "Zhiqiang", ""]]}, {"id": "1903.06361", "submitter": "Jack Murtagh", "authors": "Jack Murtagh, Omer Reingold, Aaron Sidford, Salil Vadhan", "title": "Deterministic Approximation of Random Walks in Small Space", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic, nearly logarithmic-space algorithm that given an\nundirected graph $G$, a positive integer $r$, and a set $S$ of vertices,\napproximates the conductance of $S$ in the $r$-step random walk on $G$ to\nwithin a factor of $1+\\epsilon$, where $\\epsilon>0$ is an arbitrarily small\nconstant. More generally, our algorithm computes an $\\epsilon$-spectral\napproximation to the normalized Laplacian of the $r$-step walk.\n  Our algorithm combines the derandomized square graph operation (Rozenman and\nVadhan, 2005), which we recently used for solving Laplacian systems in nearly\nlogarithmic space (Murtagh, Reingold, Sidford, and Vadhan, 2017), with ideas\nfrom (Cheng, Cheng, Liu, Peng, and Teng, 2015), which gave an algorithm that is\ntime-efficient (while ours is space-efficient) and randomized (while ours is\ndeterministic) for the case of even $r$ (while ours works for all $r$). Along\nthe way, we provide some new results that generalize technical machinery and\nyield improvements over previous work. First, we obtain a nearly linear-time\nrandomized algorithm for computing a spectral approximation to the normalized\nLaplacian for odd $r$. Second, we define and analyze a generalization of the\nderandomized square for irregular graphs and for sparsifying the product of two\ndistinct graphs. As part of this generalization, we also give a strongly\nexplicit construction of expander graphs of every size.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 04:58:45 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 02:52:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Murtagh", "Jack", ""], ["Reingold", "Omer", ""], ["Sidford", "Aaron", ""], ["Vadhan", "Salil", ""]]}, {"id": "1903.06412", "submitter": "Mikito Nanashima", "authors": "Mikito Nanashima", "title": "A Faster Algorithm Enumerating Relevant Features over Finite Fields", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of enumerating relevant features hidden in other\nirrelevant information for multi-labeled data, which is formalized as learning\njuntas.\n  A $k$-junta function is a function which depends on only $k$ coordinates of\nthe input. For relatively small $k$ w.r.t. the input size $n$, learning\n$k$-junta functions is one of fundamental problems both theoretically and\npractically in machine learning. For the last two decades, much effort has been\nmade to design efficient learning algorithms for Boolean junta functions, and\nsome novel techniques have been developed. However, in real world,\nmulti-labeled data seem to be obtained in much more often than binary-labeled\none. Thus, it is a natural question whether these techniques can be applied to\nmore general cases about the alphabet size.\n  In this paper, we expand the Fourier detection techniques for the binary\nalphabet to any finite field $\\mathbb{F}_q$, and give, roughly speaking, an\n$O(n^{0.8k})$-time learning algorithm for $k$-juntas over $\\mathbb{F}_q$. Note\nthat our algorithm is the first non-trivial (i.e., non-brute force) algorithm\nfor such a class even in the case where $q=3$ and we give an affirmative answer\nto the question posed by Mossel et al.\n  Our algorithm consists of two reductions: (1) from learning juntas to LDME\nwhich is a variant of the learning with errors (LWE) problems introduced by\nRegev, and (2) from LDME to the light bulb problem (LBP) introduced by\nL.Valiant. Since the reduced problem (i.e., LBP) is a kind of binary problem\nregardless of the alphabet size of the original problem (i.e., learning\njuntas), we can directly apply the techniques for the binary case in the\nprevious work.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 08:51:51 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 14:38:10 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Nanashima", "Mikito", ""]]}, {"id": "1903.06570", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Sabuzima Nayak and Samir Kumar Borgohain", "title": "scaleBF: A High Scalable Membership Filter using 3D Bloom Filter", "comments": "6 Pages, 3 Figures, 1 Table", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 9 Issue 12, 2018", "doi": "10.14569/IJACSA.2018.091277", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom Filter is extensively deployed data structure in various applications\nand research domain since its inception. Bloom Filter is able to reduce the\nspace consumption in an order of magnitude. Thus, Bloom Filter is used to keep\ninformation of a very large scale data. There are numerous variants of Bloom\nFilters available, however, scalability is a serious dilemma of Bloom Filter\nfor years. To solve this dilemma, there are also diverse variants of Bloom\nFilter. However, the time complexity and space complexity become the key issue\nagain. In this paper, we present a novel Bloom Filter to address the\nscalability issue without compromising the performance, called scaleBF. scaleBF\ndeploys many 3D Bloom Filter to filter the set of items. In this paper, we\ntheoretically compare the contemporary Bloom Filter for scalability and scaleBF\noutperforms in terms of time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:26:59 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Patgiri", "Ripon", ""], ["Nayak", "Sabuzima", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "1903.06579", "submitter": "Cl\\'ement Dallard", "authors": "Cristina Bazgan, Janka Chleb\\'ikov\\'a, Cl\\'ement Dallard, Thomas\n  Pontoizeau", "title": "Proportionally dense subgraph of maximum size: complexity and\n  approximation", "comments": null, "journal-ref": "Discrete Applied Mathematics, Volume 270, 2019, Pages 25-36", "doi": "10.1016/j.dam.2019.07.010", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a proportionally dense subgraph (PDS) as an induced subgraph of a\ngraph with the property that each vertex in the PDS is adjacent to\nproportionally as many vertices in the subgraph as in the graph. We prove that\nthe problem of finding a PDS of maximum size is APX-hard on split graphs, and\nNP-hard on bipartite graphs. We also show that deciding if a PDS is\ninclusion-wise maximal is co-NP-complete on bipartite graphs. Nevertheless, we\npresent a simple polynomial-time $(2-\\frac{2}{\\Delta+1})$-approximation\nalgorithm for the problem, where $\\Delta$ is the maximum degree of the graph.\nFinally, we show that all Hamiltonian cubic graphs with $n$ vertices (except\ntwo) have a PDS of size $\\lfloor \\frac{2n+1}{3} \\rfloor$, which we prove to be\nan upper bound on the size of a PDS in cubic graphs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:42:10 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 14:22:32 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 10:28:25 GMT"}, {"version": "v4", "created": "Wed, 3 Jul 2019 11:11:51 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bazgan", "Cristina", ""], ["Chleb\u00edkov\u00e1", "Janka", ""], ["Dallard", "Cl\u00e9ment", ""], ["Pontoizeau", "Thomas", ""]]}, {"id": "1903.06601", "submitter": "Yakov Nekrich", "authors": "J. Ian Munro, Yakov Nekrich", "title": "Dynamic Planar Point Location in External Memory", "comments": "extended version of a SoCG'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a fully-dynamic data structure for the planar point\nlocation problem in the external memory model. Our data structure supports\nqueries in $O(\\log_B n(\\log\\log_B n)^3))$ I/Os and updates in $O(\\log_B\nn(\\log\\log_B n)^2))$ amortized I/Os, where $n$ is the number of segments in the\nsubdivision and $B$ is the block size. This is the first dynamic data structure\nwith almost-optimal query cost. For comparison all previously known results for\nthis problem require $O(\\log_B^2 n)$ I/Os to answer queries. Our result almost\nmatches the best known upper bound in the internal-memory model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:31:47 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1903.06898", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal, Joel H. Spencer", "title": "On-Line Balancing of Random Inputs", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online vector balancing game where vectors $v_t$, chosen\nuniformly at random in $\\{-1,+1\\}^n$, arrive over time and a sign $x_t \\in\n\\{-1,+1\\}$ must be picked immediately upon the arrival of $v_t$. The goal is to\nminimize the $L^\\infty$ norm of the signed sum $\\sum_t x_t v_t$. We give an\nonline strategy for picking the signs $x_t$ that has value $O(n^{1/2})$ with\nhigh probability. Up to constants, this is the best possible even when the\nvectors are given in advance.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 08:54:29 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:59:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Bansal", "Nikhil", ""], ["Spencer", "Joel H.", ""]]}, {"id": "1903.06904", "submitter": "Yair Marom", "authors": "Yair Marom and Dan Feldman", "title": "k-Means Clustering of Lines for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the $k$-median for lines problem is a set $L$ of $n$ lines in\n$\\mathbb{R}^d$, and the goal is to compute a set of $k$ centers (points) in\n$\\mathbb{R}^d$ that minimizes the sum of squared distances over every line in\n$L$ and its nearest center. This is a straightforward generalization of the\n$k$-median problem where the input is a set of $n$ points instead of lines.\n  We suggest the first PTAS that computes a $(1+\\epsilon)$-approximation to\nthis problem in time $O(n \\log n)$ for any constant approximation error\n$\\epsilon \\in (0, 1)$, and constant integers $k, d \\geq 1$. This is by proving\nthat there is always a weighted subset (called coreset) of $dk^{O(k)}\\log\n(n)/\\epsilon^2$ lines in $L$ that approximates the sum of squared distances\nfrom $L$ to any given set of $k$ points.\n  Using traditional merge-and-reduce technique, this coreset implies results\nfor a streaming set (possibly infinite) of lines to $M$ machines in one pass\n(e.g. cloud) using memory, update time and communication that is\nnear-logarithmic in $n$, as well as deletion of any line but using linear\nspace. These results generalized for other distance functions such as\n$k$-median (sum of distances) or ignoring farthest $m$ lines from the given\ncenters to handle outliers.\n  Experimental results on 10 machines on Amazon EC2 cloud show that the\nalgorithm performs well in practice. Open source code for all the algorithms\nand experiments is also provided.\n  This thesis is an extension of the following accepted paper: \"$k$-Means\nClustering of Lines for Big Data\", by Yair Marom & Dan Feldman, Proceedings of\nNeurIPS 2019 conference, to appear on December 2019.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 10:34:52 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 16:53:54 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 13:35:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Marom", "Yair", ""], ["Feldman", "Dan", ""]]}, {"id": "1903.06981", "submitter": "Debajyoti Mondal", "authors": "Ahmad Biniaz, Kshitij Jain, Anna Lubiw, Zuzana Mas\\'arov\\'a, Tillmann\n  Miltzow, Debajyoti Mondal, Anurag Murty Naredla, Josef Tkadlec, Alexi\n  Turcotte", "title": "Token Swapping on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the token swapping problem is a graph with vertices $v_1, v_2,\n\\ldots, v_n$, and $n$ tokens with labels $1, 2, \\ldots, n$, one on each vertex.\nThe goal is to get token $i$ to vertex $v_i$ for all $i= 1, \\ldots, n$ using a\nminimum number of \\emph{swaps}, where a swap exchanges the tokens on the\nendpoints of an edge.\n  Token swapping on a tree, also known as \"sorting with a transposition tree\",\nis not known to be in P nor NP-complete. We present some partial results:\n  1. An optimum swap sequence may need to perform a swap on a leaf vertex that\nhas the correct token (a \"happy leaf\"), disproving a conjecture of Vaughan.\n  2. Any algorithm that fixes happy leaves---as all known approximation\nalgorithms for the problem do---has approximation factor at least $4/3$.\nFurthermore, the two best-known 2-approximation algorithms have approximation\nfactor exactly 2.\n  3. A generalized problem---weighted coloured token swapping---is NP-complete\non trees, but solvable in polynomial time on paths and stars. In this version,\ntokens and vertices have colours, and colours have weights. The goal is to get\nevery token to a vertex of the same colour, and the cost of a swap is the sum\nof the weights of the two tokens involved.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 20:30:47 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 19:07:05 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Biniaz", "Ahmad", ""], ["Jain", "Kshitij", ""], ["Lubiw", "Anna", ""], ["Mas\u00e1rov\u00e1", "Zuzana", ""], ["Miltzow", "Tillmann", ""], ["Mondal", "Debajyoti", ""], ["Naredla", "Anurag Murty", ""], ["Tkadlec", "Josef", ""], ["Turcotte", "Alexi", ""]]}, {"id": "1903.07410", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Julien Baste, Michael R. Fellows, Lars Jaffke, Tom\\'a\\v{s}\n  Masa\\v{r}\\'ik, Mateus de Oliveira Oliveira, Geevarghese Philip, Frances A.\n  Rosamond", "title": "Diversity of Solutions: An Exploration Through the Lens of\n  Fixed-Parameter Tractability Theory", "comments": "Accepted to Twenty-Ninth International Joint Conference on Artificial\n  Intelligence, {IJCAI} 2020, 16 pages", "journal-ref": null, "doi": "10.24963/ijcai.2020/156", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling an application of practical relevance as an instance of a\ncombinatorial problem X, we are often interested not merely in finding one\noptimal solution for that instance, but in finding a sufficiently diverse\ncollection of good solutions. In this work we initiate a systematic study of\ndiversity from the point of view of fixed-parameter tractability theory. First,\nwe consider an intuitive notion of diversity of a collection of solutions which\nsuits a large variety of combinatorial problems of practical interest. We then\npresent an algorithmic framework which --automatically-- converts a\ntree-decomposition-based dynamic programming algorithm for a given\ncombinatorial problem X into a dynamic programming algorithm for the diverse\nversion of X. Surprisingly, our algorithm has a polynomial dependence on the\ndiversity parameter.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 13:05:44 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 01:38:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Baste", "Julien", ""], ["Fellows", "Michael R.", ""], ["Jaffke", "Lars", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Oliveira", "Mateus de Oliveira", ""], ["Philip", "Geevarghese", ""], ["Rosamond", "Frances A.", ""]]}, {"id": "1903.07418", "submitter": "Eden Chlamt\\'a\\v{c}", "authors": "Eden Chlamt\\'a\\v{c}, Michael Dinitz, Thomas Robinson", "title": "The Norms of Graph Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $t$-spanner of a graph $G$ is a subgraph $H$ in which all distances are\npreserved up to a multiplicative $t$ factor. A classical result of Alth\\\"ofer\net al. is that for every integer $k$ and every graph $G$, there is a\n$(2k-1)$-spanner of $G$ with at most $O(n^{1+1/k})$ edges. But for some\nsettings the more interesting notion is not the number of edges, but the\ndegrees of the nodes. This spurred interest in and study of spanners with small\nmaximum degree. However, this is not necessarily a robust enough objective: we\nwould like spanners that not only have small maximum degree, but also have\n\"few\" nodes of \"large\" degree. To interpolate between these two extremes, in\nthis paper we initiate the study of graph spanners with respect to the\n$\\ell_p$-norm of their degree vector, thus simultaneously modeling the number\nof edges (the $\\ell_1$-norm) and the maximum degree (the $\\ell_{\\infty}$-norm).\nWe give precise upper bounds for all ranges of $p$ and stretch $t$: we prove\nthat the greedy $(2k-1)$-spanner has $\\ell_p$ norm of at most $\\max(O(n),\nO(n^{(k+p)/(kp)}))$, and that this bound is tight (assuming the Erd\\H{o}s girth\nconjecture). We also study universal lower bounds, allowing us to give\n\"generic\" guarantees on the approximation ratio of the greedy algorithm which\ngeneralize and interpolate between the known approximations for the $\\ell_1$\nand $\\ell_{\\infty}$ norm. Finally, we show that at least in some situations,\nthe $\\ell_p$ norm behaves fundamentally differently from $\\ell_1$ or\n$\\ell_{\\infty}$: there are regimes ($p=2$ and stretch $3$ in particular) where\nthe greedy spanner has a provably superior approximation to the generic\nguarantee.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 13:13:47 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chlamt\u00e1\u010d", "Eden", ""], ["Dinitz", "Michael", ""], ["Robinson", "Thomas", ""]]}, {"id": "1903.07493", "submitter": "Andr\\'as Gily\\'en", "authors": "Andris Ambainis and Andr\\'as Gily\\'en and Stacey Jeffery and Martins\n  Kokainis", "title": "Quadratic speedup for finding marked vertices by quantum walks", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum walk algorithm can detect the presence of a marked vertex on a\ngraph quadratically faster than the corresponding random walk algorithm\n(Szegedy, FOCS 2004). However, quantum algorithms that actually find a marked\nelement quadratically faster than a classical random walk were only known for\nthe special case when the marked set consists of just a single vertex, or in\nthe case of some specific graphs. We present a new quantum algorithm for\nfinding a marked vertex in any graph, with any set of marked vertices, that is\n(up to a log factor) quadratically faster than the corresponding classical\nrandom walk.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 14:59:57 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ambainis", "Andris", ""], ["Gily\u00e9n", "Andr\u00e1s", ""], ["Jeffery", "Stacey", ""], ["Kokainis", "Martins", ""]]}, {"id": "1903.07531", "submitter": "Chao Liao", "authors": "Chao Liao, Jiabao Lin, Pinyan Lu, Zhenyu Mao", "title": "Counting independent sets and colorings on random regular bipartite\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fully polynomial-time approximation scheme (FPTAS) to count the\nnumber of independent sets on almost every $\\Delta$-regular bipartite graph if\n$\\Delta\\ge 53$. In the weighted case, for all sufficiently large integers\n$\\Delta$ and weight parameters\n$\\lambda=\\tilde\\Omega\\left(\\frac{1}{\\Delta}\\right)$, we also obtain an FPTAS on\nalmost every $\\Delta$-regular bipartite graph. Our technique is based on the\nrecent work of Jenssen, Keevash and Perkins (SODA, 2019) and we also apply it\nto confirm an open question raised there: For all $q\\ge 3$ and sufficiently\nlarge integers $\\Delta=\\Delta(q)$, there is an FPTAS to count the number of\n$q$-colorings on almost every $\\Delta$-regular bipartite graph.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:17:36 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Liao", "Chao", ""], ["Lin", "Jiabao", ""], ["Lu", "Pinyan", ""], ["Mao", "Zhenyu", ""]]}, {"id": "1903.07595", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini, Steven Chaplick, Sabine Cornelsen, Giordano Da\n  Lozzo, and Vincenzo Roselli", "title": "Morphing Contact Representations of Graphs", "comments": "Extended version of \"Morphing Contact Representations of Graphs\", to\n  appear in Proceedings of the 35th International Symposium on Computational\n  Geometry (SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of morphing between contact representations of a\nplane graph. In an $\\mathcal F$-contact representation of a plane graph $G$,\nvertices are realized by internally disjoint elements from a family $\\mathcal\nF$ of connected geometric objects. Two such elements touch if and only if their\ncorresponding vertices are adjacent. These touchings also induce the same\nembedding as in $G$. In a morph between two $\\mathcal F$-contact\nrepresentations we insist that at each time step (continuously throughout the\nmorph) we have an $\\mathcal F$-contact representation.\n  We focus on the case when $\\mathcal{F}$ is the family of triangles in\n$\\mathbb{R}^2$ that are the lower-right half of axis-parallel rectangles. Such\nRT-representations exist for every plane graph and right triangles are one of\nthe simplest families of shapes supporting this property. Thus, they provide a\nnatural case to study regarding morphs of contact representations of plane\ngraphs.\n  We study piecewise linear morphs, where each step is a linear morph moving\nthe endpoints of each triangle at constant speed along straight-line\ntrajectories. We provide a polynomial-time algorithm that decides whether there\nis a piecewise linear morph between two RT-representations of an $n$-vertex\nplane triangulation, and, if so, computes a morph with $\\mathcal O(n^2)$ linear\nmorphs. As a direct consequence, we obtain that for $4$-connected plane\ntriangulations there is a morph between every pair of RT-representations where\nthe ``top-most'' triangle in both representations corresponds to the same\nvertex. This shows that the realization space of such RT-representations of any\n$4$-connected plane triangulation forms a connected set.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:42:52 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Angelini", "Patrizio", ""], ["Chaplick", "Steven", ""], ["Cornelsen", "Sabine", ""], ["Da Lozzo", "Giordano", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1903.07614", "submitter": "Laurent Duval", "authors": "Jean-Luc Peyrot and Laurent Duval and Fr\\'ed\\'eric Payan and Lauriane\n  Bouard and L\\'ena\\\"ic Chizat and S\\'ebastien Schneider and Marc Antonini", "title": "HexaShrink, an exact scalable framework for hexahedral meshes with\n  attributes and discontinuities: multiresolution rendering and storage of\n  geoscience models", "comments": null, "journal-ref": null, "doi": "10.1007/s10596-019-9816-2", "report-no": null, "categories": "cs.GR cs.CV cs.DS physics.data-an physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With huge data acquisition progresses realized in the past decades and\nacquisition systems now able to produce high resolution grids and point clouds,\nthe digitization of physical terrains becomes increasingly more precise. Such\nextreme quantities of generated and modeled data greatly impact computational\nperformances on many levels of high-performance computing (HPC): storage media,\nmemory requirements, transfer capability, and finally simulation interactivity,\nnecessary to exploit this instance of big data. Efficient representations and\nstorage are thus becoming \"enabling technologies'' in HPC experimental and\nsimulation science. We propose HexaShrink, an original decomposition scheme for\nstructured hexahedral volume meshes. The latter are used for instance in\nbiomedical engineering, materials science, or geosciences. HexaShrink provides\na comprehensive framework allowing efficient mesh visualization and storage.\nIts exactly reversible multiresolution decomposition yields a hierarchy of\nmeshes of increasing levels of details, in terms of either geometry, continuous\nor categorical properties of cells. Starting with an overview of volume meshes\ncompression techniques, our contribution blends coherently different\nmultiresolution wavelet schemes in different dimensions. It results in a global\nframework preserving discontinuities (faults) across scales, implemented as a\nfully reversible upscaling at different resolutions. Experimental results are\nprovided on meshes of varying size and complexity. They emphasize the\nconsistency of the proposed representation, in terms of visualization,\nattribute downsampling and distribution at different resolutions. Finally,\nHexaShrink yields gains in storage space when combined to lossless compression\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 10:24:22 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 13:33:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Peyrot", "Jean-Luc", ""], ["Duval", "Laurent", ""], ["Payan", "Fr\u00e9d\u00e9ric", ""], ["Bouard", "Lauriane", ""], ["Chizat", "L\u00e9na\u00efc", ""], ["Schneider", "S\u00e9bastien", ""], ["Antonini", "Marc", ""]]}, {"id": "1903.07775", "submitter": "James Allen Fill", "authors": "James Allen Fill, Wei-Chun Hung", "title": "QuickSort: Improved right-tail asymptotics for the limiting\n  distribution, and large deviations", "comments": "15 pages; submitted for publication in January, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We substantially refine asymptotic logarithmic upper bounds produced by\nSvante Janson (2015) on the right tail of the limiting QuickSort distribution\nfunction $F$ and by Fill and Hung (2018) on the right tails of the\ncorresponding density $f$ and of the absolute derivatives of $f$ of each order.\nFor example, we establish an upper bound on $\\log[1 - F(x)]$ that matches\nconjectured asymptotics of Knessl and Szpankowski (1999) through terms of order\n$(\\log x)^2$; the corresponding order for the Janson (2015) bound is the lead\norder, $x \\log x$.\n  Using the refined asymptotic bounds on $F$, we derive right-tail large\ndeviation (LD) results for the distribution of the number of comparisons\nrequired by QuickSort that substantially sharpen the two-sided LD results of\nMcDiarmid and Hayward (1996).\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 00:04:32 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Fill", "James Allen", ""], ["Hung", "Wei-Chun", ""]]}, {"id": "1903.07797", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Richard Cole, Vasilis Gkatzelis, Jason D. Hartline", "title": "A Truthful Cardinal Mechanism for One-Sided Matching", "comments": "Appears in SODA 2020", "journal-ref": "Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\n  Discrete Algorithms (pp. 2096-2113). Society for Industrial and Applied\n  Mathematics (SIAM)", "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the well-studied problem of designing mechanisms for one-sided\nmatching markets, where a set of $n$ agents needs to be matched to a set of $n$\nheterogeneous items. Each agent $i$ has a value $v_{i,j}$ for each item $j$,\nand these values are private information that the agents may misreport if doing\nso leads to a preferred outcome. Ensuring that the agents have no incentive to\nmisreport requires a careful design of the matching mechanism, and mechanisms\nproposed in the literature mitigate this issue by eliciting only the\n\\emph{ordinal} preferences of the agents, i.e., their ranking of the items from\nmost to least preferred. However, the efficiency guarantees of these mechanisms\nare based only on weak measures that are oblivious to the underlying values. In\nthis paper we achieve stronger performance guarantees by introducing a\nmechanism that truthfully elicits the full \\emph{cardinal} preferences of the\nagents, i.e., all of the $v_{i,j}$ values. We evaluate the performance of this\nmechanism using the much more demanding Nash bargaining solution as a\nbenchmark, and we prove that our mechanism significantly outperforms all\nordinal mechanisms (even non-truthful ones). To prove our approximation bounds,\nwe also study the population monotonicity of the Nash bargaining solution in\nthe context of matching markets, providing both upper and lower bounds which\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 02:25:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 21:45:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Abebe", "Rediet", ""], ["Cole", "Richard", ""], ["Gkatzelis", "Vasilis", ""], ["Hartline", "Jason D.", ""]]}, {"id": "1903.07870", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins and Jerry Li", "title": "How Hard Is Robust Mean Estimation?", "comments": "Conference on Learning Theory (COLT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust mean estimation is the problem of estimating the mean $\\mu \\in\n\\mathbb{R}^d$ of a $d$-dimensional distribution $D$ from a list of independent\nsamples, an $\\epsilon$-fraction of which have been arbitrarily corrupted by a\nmalicious adversary. Recent algorithmic progress has resulted in the first\npolynomial-time algorithms which achieve \\emph{dimension-independent} rates of\nerror: for instance, if $D$ has covariance $I$, in polynomial-time one may find\n$\\hat{\\mu}$ with $\\|\\mu - \\hat{\\mu}\\| \\leq O(\\sqrt{\\epsilon})$. However, error\nrates achieved by current polynomial-time algorithms, while\ndimension-independent, are sub-optimal in many natural settings, such as when\n$D$ is sub-Gaussian, or has bounded $4$-th moments.\n  In this work we give worst-case complexity-theoretic evidence that improving\non the error rates of current polynomial-time algorithms for robust mean\nestimation may be computationally intractable in natural settings. We show that\nseveral natural approaches to improving error rates of current polynomial-time\nrobust mean estimation algorithms would imply efficient algorithms for the\nsmall-set expansion problem, refuting Raghavendra and Steurer's small-set\nexpansion hypothesis (so long as $P \\neq NP$). We also give the first direct\nreduction to the robust mean estimation problem, starting from a plausible but\nnonstandard variant of the small-set expansion problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 07:48:33 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 23:17:58 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""]]}, {"id": "1903.07966", "submitter": "Giordano Da Lozzo", "authors": "Carla Binucci, Giordano Da Lozzo, Emilio Di Giacomo, Walter Didimo,\n  Tamara Mchedlidze, and Maurizio Patrignani", "title": "Upward Book Embeddings of st-Graphs", "comments": "Extended version of \"Upward Book Embeddings of st-Graphs\", to appear\n  in Proceedings of the 35th International Symposium on Computational Geometry\n  (SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-page upward book embeddings ($k$UBEs) of $st$-graphs, that is,\nbook embeddings of single-source single-sink directed acyclic graphs on $k$\npages with the additional requirement that the vertices of the graph appear in\na topological ordering along the spine of the book. We show that testing\nwhether a graph admits a $k$UBE is NP-complete for $k\\geq 3$. A hardness result\nfor this problem was previously known only for $k = 6$ [Heath and Pemmaraju,\n1999]. Motivated by this negative result, we focus our attention on $k=2$. On\nthe algorithmic side, we present polynomial-time algorithms for testing the\nexistence of $2$UBEs of planar $st$-graphs with branchwidth $\\beta$ and of\nplane $st$-graphs whose faces have a special structure. These algorithms run in\n$O(f(\\beta)\\cdot n+n^3)$ time and $O(n)$ time, respectively, where $f$ is a\nsingly-exponential function on $\\beta$. Moreover, on the combinatorial side, we\npresent two notable families of plane $st$-graphs that always admit an\nembedding-preserving $2$UBE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 12:45:04 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Binucci", "Carla", ""], ["Da Lozzo", "Giordano", ""], ["Di Giacomo", "Emilio", ""], ["Didimo", "Walter", ""], ["Mchedlidze", "Tamara", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1903.07967", "submitter": "Peyman Afshani", "authors": "Peyman Afshani", "title": "A New Lower Bound for Semigroup Orthogonal Range Searching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the first improvement in the space-time trade-off of lower bounds\nfor the orthogonal range searching problem in the semigroup model, since\nChazelle's result from 1990. This is one of the very fundamental problems in\nrange searching with a long history. Previously, Andrew Yao's influential\nresult had shown that the problem is already non-trivial in one\ndimension~\\cite{Yao-1Dlb}: using $m$ units of space, the query time $Q(n)$ must\nbe $\\Omega( \\alpha(m,n) + \\frac{n}{m-n+1})$ where $\\alpha(\\cdot,\\cdot)$ is the\ninverse Ackermann's function, a very slowly growing function.\n  In $d$ dimensions, Bernard Chazelle~\\cite{Chazelle.LB.II} proved that the\nquery time must be $Q(n) = \\Omega( (\\log_\\beta n)^{d-1})$ where $\\beta = 2m/n$.\nChazelle's lower bound is known to be tight for when space consumption is\n`high' i.e., $m = \\Omega(n \\log^{d+\\varepsilon}n)$. We have two main results.\nThe first is a lower bound that shows Chazelle's lower bound was not tight for\n`low space': we prove that we must have $m (n) = \\Omega(n (\\log n \\log\\log\nn)^{d-1})$. Our lower bound does not close the gap to the existing data\nstructures, however, our second result is that our analysis is tight. Thus, we\nbelieve the gap is in fact natural since lower bounds are proven for idempotent\nsemigroups while the data structures are built for general semigroups and thus\nthey cannot assume (and use) the properties of an idempotent semigroup. As a\nresult, we believe to close the gap one must study lower bounds for\nnon-idempotent semigroups or building data structures for idempotent\nsemigroups. We develope significantly new ideas for both of our results that\ncould be useful in pursuing either of these directions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 12:45:58 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Afshani", "Peyman", ""]]}, {"id": "1903.08014", "submitter": "Peyman Afshani", "authors": "Peyman Afshani and Jeff M. Phillips", "title": "Independent Range Sampling, Revisited Again", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the range sampling problem: the input is a set of points where\neach point is associated with a real-valued weight. The goal is to store them\nin a structure such that given a query range and an integer $k$, we can extract\n$k$ independent random samples from the points inside the query range, where\nthe probability of sampling a point is proportional to its weight.\n  This line of work was initiated in 2014 by Hu, Qiao, and Tao and it was later\nfollowed up by Afshani and Wei. The first line of work mostly studied\nunweighted but dynamic version of the problem in one dimension whereas the\nsecond result considered the static weighted problem in one dimension as well\nas the unweighted problem in 3D for halfspace queries.\n  We offer three main results and some interesting insights that were missed by\nthe previous work: We show that it is possible to build efficient data\nstructures for range sampling queries if we allow the query time to hold in\nexpectation (the first result), or obtain efficient worst-case query bounds by\nallowing the sampling probability to be approximately proportional to the\nweight (the second result). The third result is a conditional lower bound that\nshows essentially one of the previous two concessions is needed. For instance,\nfor the 3D range sampling queries, the first two results give efficient data\nstructures with near-linear space and polylogarithmic query time whereas the\nlower bound shows with near-linear space the worst-case query time must be\nclose to $n^{2/3}$, ignoring polylogarithmic factors. Up to our knowledge, this\nis the first such major gap between the expected and worst-case query time of a\nrange searching problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:21:43 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Afshani", "Peyman", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1903.08247", "submitter": "Enric Boix-Adser\\`a", "authors": "Enric Boix-Adser\\`a, Matthew Brennan, and Guy Bresler", "title": "The Average-Case Complexity of Counting Cliques in Erdos-Renyi\n  Hypergraphs", "comments": "44 pages, 2 figures, appeared in FOCS'19, accepted to SICOMP special\n  edition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of counting $k$-cliques in $s$-uniform Erdos-Renyi\nhypergraphs $G(n,c,s)$ with edge density $c$, and show that its fine-grained\naverage-case complexity can be based on its worst-case complexity. We prove the\nfollowing:\n  1. Dense Erdos-Renyi graphs and hypergraphs: Counting $k$-cliques on\n$G(n,c,s)$ with $k$ and $c$ constant matches its worst-case time complexity up\nto a $\\mathrm{polylog}(n)$ factor. Assuming randomized ETH, it takes\n$n^{\\Omega(k)}$ time to count $k$-cliques in $G(n,c,s)$ if $k$ and $c$ are\nconstant.\n  2. Sparse Erdos-Renyi graphs and hypergraphs: When $c = \\Theta(n^{-\\alpha})$,\nwe give several algorithms exploiting the sparsity of $G(n, c, s)$ that are\nfaster than the best known worst-case algorithms. Complementing this, based on\na fine-grained worst-case assumption, our results imply a different\naverage-case phase diagram for each fixed $\\alpha$ depicting a tradeoff between\na runtime lower bound and $k$. Surprisingly, in the hypergraph case ($s \\ge\n3$), these lower bounds are tight against our algorithms exactly when $c$ is\nabove the Erd\\H{o}s-R\\'{e}nyi $k$-clique percolation threshold.\n  This is the first worst-case-to-average-case hardness reduction for a problem\non Erd\\H{o}s-R\\'{e}nyi hypergraphs that we are aware of. We also give a variant\nof our result for computing the parity of the $k$-clique count that tolerates\nhigher error probability.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 20:23:42 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 21:15:19 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 18:50:19 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 12:23:16 GMT"}, {"version": "v5", "created": "Wed, 14 Oct 2020 15:30:14 GMT"}, {"version": "v6", "created": "Fri, 11 Jun 2021 01:07:04 GMT"}, {"version": "v7", "created": "Thu, 22 Jul 2021 01:59:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Boix-Adser\u00e0", "Enric", ""], ["Brennan", "Matthew", ""], ["Bresler", "Guy", ""]]}, {"id": "1903.08263", "submitter": "Allen Xiao", "authors": "Pankaj K. Agarwal, Kyle Fox, Debmalya Panigrahi, Kasturi R.\n  Varadarajan, Allen Xiao", "title": "Faster Algorithms for the Geometric Transportation Problem", "comments": "33 pages, 6 figures, full version of a paper that appeared in SoCG\n  2017", "journal-ref": "Symposium on Computational Geometry 2017: 7:1-7:16", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $R$ and $B$ be two point sets in $\\mathbb{R}^d$, with $|R|+ |B| = n$ and\nwhere $d$ is a constant. Next, let $\\lambda : R \\cup B \\to \\mathbb{N}$ such\nthat $\\sum_{r \\in R } \\lambda(r) = \\sum_{b \\in B} \\lambda(b)$ be demand\nfunctions over $R$ and $B$. Let $\\|\\cdot\\|$ be a suitable distance function\nsuch as the $L_p$ distance. The transportation problem asks to find a map $\\tau\n: R \\times B \\to \\mathbb{N}$ such that $\\sum_{b \\in B}\\tau(r,b) = \\lambda(r)$,\n$\\sum_{r \\in R}\\tau(r,b) = \\lambda(b)$, and $\\sum_{r \\in R, b \\in B} \\tau(r,b)\n\\|r-b\\|$ is minimized. We present three new results for the transportation\nproblem when $\\|r-b\\|$ is any $L_p$ metric:\n  - For any constant $\\varepsilon > 0$, an $O(n^{1+\\varepsilon})$ expected time\nrandomized algorithm that returns a transportation map with expected cost\n$O(\\log^2(1/\\varepsilon))$ times the optimal cost.\n  - For any $\\varepsilon > 0$, a $(1+\\varepsilon)$-approximation in\n$O(n^{3/2}\\varepsilon^{-d} \\operatorname{polylog}(U)\n\\operatorname{polylog}(n))$ time, where $U = \\max_{p\\in R\\cup B} \\lambda(p)$.\n  - An exact strongly polynomial $O(n^2 \\operatorname{polylog}n)$ time\nalgorithm, for $d = 2$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 21:37:18 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Fox", "Kyle", ""], ["Panigrahi", "Debmalya", ""], ["Varadarajan", "Kasturi R.", ""], ["Xiao", "Allen", ""]]}, {"id": "1903.08364", "submitter": "Hossein Falsafain Ph.D.", "authors": "Hossein Falsafain and Mohammad Tamannaei", "title": "A Novel Dynamic Programming Approach to the Train Marshalling Problem", "comments": "Accepted Paper - IEEE Transactions on Intelligent Transportation\n  Systems, 2019", "journal-ref": null, "doi": "10.1109/TITS.2019.2898476", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Train marshalling is the process of reordering the railcars of a train in\nsuch a way that the railcars with the same destination appear consecutively in\nthe final, reassembled train. The process takes place in the shunting yard by\nmeans of a number of classification tracks. In the Train Marshalling Problem\n(TMP), the objective is to perform this rearrangement of the railcars with the\nuse of as few classification tracks as possible. The problem has been shown to\nbe NP-hard, and several exact and approximation algorithms have been developed\nfor it. In this paper, we propose a novel exact dynamic programming (DP)\nalgorithm for the TMP. The worst-case time complexity of this algorithm (which\nis exponential in the number of destinations and linear in the number of\nrailcars) is lower than that of the best presently available algorithm for the\nproblem, which is an inclusion-exclusion-based DP algorithm. In practice, the\nproposed algorithm can provide a substantially improved performance compared to\nits inclusion-exclusion-based counterpart, as demonstrated by the experimental\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 07:25:30 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Falsafain", "Hossein", ""], ["Tamannaei", "Mohammad", ""]]}, {"id": "1903.08568", "submitter": "Andre Wibisono", "authors": "Santosh S. Vempala and Andre Wibisono", "title": "Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry\n  Suffices", "comments": "v2: Added analysis of R\\'enyi divergence and Poincar\\'e assumption \\\\\n  v3: Simplified analysis of R\\'enyi divergence, improved exposition, and added\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Unadjusted Langevin Algorithm (ULA) for sampling from a\nprobability distribution $\\nu = e^{-f}$ on $\\mathbb{R}^n$. We prove a\nconvergence guarantee in Kullback-Leibler (KL) divergence assuming $\\nu$\nsatisfies a log-Sobolev inequality and the Hessian of $f$ is bounded. Notably,\nwe do not assume convexity or bounds on higher derivatives. We also prove\nconvergence guarantees in R\\'enyi divergence of order $q > 1$ assuming the\nlimit of ULA satisfies either the log-Sobolev or Poincar\\'e inequality.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:49:10 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 16:52:02 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 16:27:16 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Vempala", "Santosh S.", ""], ["Wibisono", "Andre", ""]]}, {"id": "1903.08603", "submitter": "Arnaud de Mesmay", "authors": "Vincent Cohen-Addad, \\'Eric Colin de Verdi\\`ere, Daniel Marx and\n  Arnaud de Mesmay", "title": "Almost Tight Lower Bounds for Hard Cutting Problems in Embedded Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove essentially tight lower bounds, conditionally to the Exponential\nTime Hypothesis, for two fundamental but seemingly very different cutting\nproblems on surface-embedded graphs: the Shortest Cut Graph problem and the\nMultiway Cut problem. A cut graph of a graph $G$ embedded on a surface $S$ is a\nsubgraph of $G$ whose removal from $S$ leaves a disk. We consider the problem\nof deciding whether an unweighted graph embedded on a surface of genus $g$ has\na cut graph of length at most a given value. We prove a time lower bound for\nthis problem of $n^{\\Omega(g/\\log g)}$ conditionally to ETH. In other words,\nthe first $n^{O(g)}$-time algorithm by Erickson and Har-Peled [SoCG 2002,\nDiscr.\\ Comput.\\ Geom.\\ 2004] is essentially optimal. We also prove that the\nproblem is W[1]-hard when parameterized by the genus, answering a 17-year old\nquestion of these authors. A multiway cut of an undirected graph $G$ with $t$\ndistinguished vertices, called terminals, is a set of edges whose removal\ndisconnects all pairs of terminals. We consider the problem of deciding whether\nan unweighted graph $G$ has a multiway cut of weight at most a given value. We\nprove a time lower bound for this problem of $n^{\\Omega(\\sqrt{gt +\ng^2+t}/\\log(g+t))}$, conditionally to ETH, for any choice of the genus $g\\ge0$\nof the graph and the number of terminals $t\\ge4$. In other words, the algorithm\nby the second author [Algorithmica 2017] (for the more general multicut\nproblem) is essentially optimal; this extends the lower bound by the third\nauthor [ICALP 2012] (for the planar case). Reductions to planar problems\nusually involve a grid-like structure. The main novel idea for our results is\nto understand what structures instead of grids are needed if we want to exploit\noptimally a certain value $g$ of the genus.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:36:08 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:56:29 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 13:06:12 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["de Verdi\u00e8re", "\u00c9ric Colin", ""], ["Marx", "Daniel", ""], ["de Mesmay", "Arnaud", ""]]}, {"id": "1903.08794", "submitter": "Daniel Anderson", "authors": "Umut A. Acar, Daniel Anderson, Guy E. Blelloch, Laxman Dhulipala", "title": "Parallel Batch-Dynamic Graph Connectivity", "comments": "This is the full version of the paper appearing in the ACM Symposium\n  on Parallelism in Algorithms and Architectures (SPAA), 2019", "journal-ref": "Proceedings of The 31st ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '19) (2019) 381-392", "doi": "10.1145/3323165.3323196", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study batch parallel algorithms for the dynamic\nconnectivity problem, a fundamental problem that has received considerable\nattention in the sequential setting. The most well known sequential algorithm\nfor dynamic connectivity is the elegant level-set algorithm of Holm, de\nLichtenberg and Thorup (HDT), which achieves $O(\\log^2 n)$ amortized time per\nedge insertion or deletion, and $O(\\log n / \\log\\log n)$ time per query. We\ndesign a parallel batch-dynamic connectivity algorithm that is work-efficient\nwith respect to the HDT algorithm for small batch sizes, and is asymptotically\nfaster when the average batch size is sufficiently large. Given a sequence of\nbatched updates, where $\\Delta$ is the average batch size of all deletions, our\nalgorithm achieves $O(\\log n \\log(1 + n / \\Delta))$ expected amortized work per\nedge insertion and deletion and $O(\\log^3 n)$ depth w.h.p. Our algorithm\nanswers a batch of $k$ connectivity queries in $O(k \\log(1 + n/k))$ expected\nwork and $O(\\log n)$ depth w.h.p. To the best of our knowledge, our algorithm\nis the first parallel batch-dynamic algorithm for connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 01:39:11 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 23:35:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Acar", "Umut A.", ""], ["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""], ["Dhulipala", "Laxman", ""]]}, {"id": "1903.08804", "submitter": "Michelle Blom", "authors": "Michelle Blom, Peter J. Stuckey, Vanessa Teague", "title": "RAIRE: Risk-Limiting Audits for IRV Elections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk-limiting post election audits guarantee a high probability of correcting\nincorrect election results, independent of why the result was incorrect.\nBallot-polling audits select ballots at random and interpret those ballots as\nevidence for and against the reported result, continuing this process until\neither they support the recorded result, or they fall back to a full manual\nrecount. For elections with digitised scanning and counting of ballots, a\ncomparison audit compares randomly selected digital ballots with their paper\nversions. Discrepancies are referred to as errors, and are used to build\nevidence against or in support of the recorded result. Risk-limiting audits for\nfirst-past-the-post elections are well understood, and used in some US\nelections. We define a number of approaches to ballot-polling and comparison\nrisk-limiting audits for Instant Runoff Voting (IRV) elections. We show that\nfor almost all real elections we found, we can perform a risk-limiting audit by\nlooking at only a small fraction of the total ballots (assuming no errors were\nmade in the tallying and distribution of votes).\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 06:19:10 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 07:04:27 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Blom", "Michelle", ""], ["Stuckey", "Peter J.", ""], ["Teague", "Vanessa", ""]]}, {"id": "1903.08936", "submitter": "Henrique Becker", "authors": "Henrique Becker, Luciana S. Buriol", "title": "An empirical analysis of exact algorithms for the unbounded knapsack\n  problem", "comments": "42 pages, 7 tables (one in conclusions, the rest in appendix), 2\n  diagrams, 1 algorithm, 8 plots", "journal-ref": null, "doi": "10.1016/j.ejor.2019.02.011", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an empirical analysis of exact algorithms for the\nunbounded knapsack problem, which includes seven algorithms from the\nliterature, two commercial solvers, and more than ten thousand instances. The\nterminating step-off, a dynamic programming algorithm from 1966, presented the\nlowest mean time to solve the most recent benchmark from the literature. The\nthreshold and collective dominances are properties of the unbounded knapsack\nproblem first discussed in 1998, and exploited by the current state-of-the-art\nalgorithms. The terminating step-off algorithm did not exploit such dominances,\nbut has an alternative mechanism for dealing with dominances which does not\nexplicitly exploits collective and threshold dominances. Also, the pricing\nsubproblems found when solving hard cutting stock problems with column\ngeneration can cause branch-and-bound algorithms to display worst-case times.\nThe authors present a new class of instances which favors the branch-and-bound\napproach over the dynamic programming approach but do not have high amounts of\nsimple, multiple and collective dominated items. This behaviour illustrates how\nthe definition of hard instances changes among algorithm approachs. The codes\nused for solving the unbounded knapsack problem and for instance generation are\nall available online.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 11:55:05 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Becker", "Henrique", ""], ["Buriol", "Luciana S.", ""]]}, {"id": "1903.09358", "submitter": "Hsien-Chih Chang", "authors": "Pankaj K. Agarwal, Hsien-Chih Chang, Allen Xiao", "title": "Efficient Algorithms for Geometric Partial Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $A$ and $B$ be two point sets in the plane of sizes $r$ and $n$\nrespectively (assume $r \\leq n$), and let $k$ be a parameter. A matching\nbetween $A$ and $B$ is a family of pairs in $A \\times B$ so that any point of\n$A \\cup B$ appears in at most one pair. Given two positive integers $p$ and\n$q$, we define the cost of matching $M$ to be $c(M) = \\sum_{(a, b) \\in\nM}\\|{a-b}\\|_p^q$ where $\\|{\\cdot}\\|_p$ is the $L_p$-norm. The geometric partial\nmatching problem asks to find the minimum-cost size-$k$ matching between $A$\nand $B$.\n  We present efficient algorithms for geometric partial matching problem that\nwork for any powers of $L_p$-norm matching objective: An exact algorithm that\nruns in $O((n + k^2) {\\mathop{\\mathrm{polylog}}} n)$ time, and a $(1 +\n\\varepsilon)$-approximation algorithm that runs in $O((n + k\\sqrt{k})\n{\\mathop{\\mathrm{polylog}}} n \\cdot \\log\\varepsilon^{-1})$ time. Both\nalgorithms are based on the primal-dual flow augmentation scheme; the main\nimprovements involve using dynamic data structures to achieve efficient flow\naugmentations. With similar techniques, we give an exact algorithm for the\nplanar transportation problem running in $O(\\min\\{n^2, rn^{3/2}\\}\n{\\mathop{\\mathrm{polylog}}} n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:03:14 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Chang", "Hsien-Chih", ""], ["Xiao", "Allen", ""]]}, {"id": "1903.09625", "submitter": "Rodrigo Lambert", "authors": "Adriana Coutinho, Rodrigo Lambert, J\\'er\\^ome Rousseau", "title": "Matching strings in encoded sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the longest common substring problem for encoded sequences and\nits asymptotic behaviour. The main result is a strong law of large numbers for\na re-scaled version of this quantity, which presents an explicit relation with\nthe R\\'enyi entropy of the source. We apply this result to the zero-inflated\ncontamination model and the stochastic scrabble. In the case of dynamical\nsystems, this problem is equivalent to the shortest distance between two\nobserved orbits and its limiting relationship with the correlation dimension of\nthe pushforward measure. An extension to the shortest distance between orbits\nfor random dynamical systems is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 17:46:15 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 17:58:58 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Coutinho", "Adriana", ""], ["Lambert", "Rodrigo", ""], ["Rousseau", "J\u00e9r\u00f4me", ""]]}, {"id": "1903.10081", "submitter": "Ortho Flint", "authors": "Ortho Flint, Asanka Wickramasinghe, Jason Brasse, Christopher Fowler", "title": "Determining satisfiability of 3-SAT in polynomial time", "comments": "The ninth version is the version submitted to peer review. ArXiv does\n  not allow antiquated versions to be removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a deterministic polynomial time algorithm that\ndetermines satisfiability of 3-SAT. The complexity analysis for the algorithm\ntakes into account no efficiency and yet provides a low enough bound, that\nefficient versions are practical with respect to today's hardware. We accompany\nthis paper with a serial version of the algorithm without non-trivial\nefficiencies (link: polynomial3sat.org).\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 23:38:27 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 19:11:40 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 15:34:37 GMT"}, {"version": "v4", "created": "Thu, 18 Apr 2019 02:52:42 GMT"}, {"version": "v5", "created": "Sun, 28 Apr 2019 13:28:25 GMT"}, {"version": "v6", "created": "Thu, 6 Jun 2019 14:07:02 GMT"}, {"version": "v7", "created": "Mon, 8 Jul 2019 12:33:07 GMT"}, {"version": "v8", "created": "Sat, 5 Oct 2019 16:47:14 GMT"}, {"version": "v9", "created": "Wed, 1 Jul 2020 16:06:02 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Flint", "Ortho", ""], ["Wickramasinghe", "Asanka", ""], ["Brasse", "Jason", ""], ["Fowler", "Christopher", ""]]}, {"id": "1903.10326", "submitter": "Abdelmoneim Amer Desouki", "authors": "Abdelmoneim Amer Desouki, Michael R\\\"oder, Axel-Cyrille Ngonga Ngomo", "title": "topFiberM: Scalable and Efficient Boolean Matrix Factorization", "comments": "9 pages, 1 Figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matrix Factorization has many applications such as clustering. When the\nmatrix is Boolean it is favorable to have Boolean factors too. This will save\nthe efforts of quantizing the reconstructed data back, which usually is done\nusing arbitrary thresholds. Here we introduce topFiberM a Boolean matrix\nfactorization algorithm. topFiberM chooses in a greedy way the fibers (rows or\ncolumns) to represent the entire matrix. Fibers are extended to rectangles\naccording to a threshold on precision. The search for these \"top fibers\" can\ncontinue beyond the required rank and according to an optional parameter that\ndefines the limit for this search. A factor with a better gain replaces the\nfactor with minimum gain in \"top fibers\". We compared topFiberM to the\nstate-of-the-art methods, it achieved better quality for the set of datasets\nusually used in literature. We also applied our algorithm to linked-data to\nshow its scalability. topFiberM was in average 128 times faster than the well\nknown Asso method when applied to a set of matrices representing a real\nmultigraph although Asso is implemented in C and topFiberM is implemented in R\nwhich is generally slower than C. topFiberM is publicly available from Github\n(https://github.com/dice-group/BMF).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 02:41:09 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Desouki", "Abdelmoneim Amer", ""], ["R\u00f6der", "Michael", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1903.10402", "submitter": "Jordi Bataller Mascarell", "authors": "Jordi Bataller Mascarell", "title": "Two Mutual Exclusion Algorithms for Shared Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce two algorithms that solve the mutual exclusion\nproblem for concurrent processes that communicate through shared variables,\n[2]. Our algorithms guarantee that any process trying to enter the critical\nsection, eventually, does enter it. They are formally proven to be correct. The\nfirst algorithm uses a special coordinator process in order to ensure equal\nchances to processes waiting for the critical section. In the second algorithm,\nwith no coordinator, the process exiting the critical section is in charge to\nfairly elect the following one. In the case that no process is waiting, the\nturn is marked free and will be determined by future waiting processes. The\ntype of shared variables used are a turn variable, readable and writable by all\nprocesses; and a flag array, readable by all with flag[i] writable only by\nprocess i. There is a version of the first algorithm where no writable by all\nvariable is used. The bibliography reviewed for this paper is [4] and [3], all\nthe rest is original work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:36:47 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mascarell", "Jordi Bataller", ""]]}, {"id": "1903.10445", "submitter": "Sharath Raghvendra", "authors": "Nathaniel Lahn and Sharath Raghvendra", "title": "A Weighted Approach to the Maximum Cardinality Bipartite Matching\n  Problem with Applications in Geometric Settings", "comments": "Appears in SoCG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weighted approach to compute a maximum cardinality matching in\nan arbitrary bipartite graph. Our main result is a new algorithm that takes as\ninput a weighted bipartite graph $G(A\\cup B,E)$ with edge weights of $0$ or\n$1$. Let $w \\leq n$ be an upper bound on the weight of any matching in $G$.\nConsider the subgraph induced by all the edges of $G$ with a weight $0$.\nSuppose every connected component in this subgraph has $\\mathcal{O}(r)$\nvertices and $\\mathcal{O}(mr/n)$ edges. We present an algorithm to compute a\nmaximum cardinality matching in $G$ in $\\tilde{\\mathcal{O}}( m(\\sqrt{w}+\n\\sqrt{r}+\\frac{wr}{n}))$ time.\n  When all the edge weights are $1$ (symmetrically when all weights are $0$),\nour algorithm will be identical to the well-known Hopcroft-Karp (HK) algorithm,\nwhich runs in $\\mathcal{O}(m\\sqrt{n})$ time. However, if we can carefully\nassign weights of $0$ and $1$ on its edges such that both $w$ and $r$ are\nsub-linear in $n$ and $wr=\\mathcal{O}(n^{\\gamma})$ for $\\gamma < 3/2$, then we\ncan compute maximum cardinality matching in $G$ in $o(m\\sqrt{n})$ time. Using\nour algorithm, we obtain a new $\\tilde{\\mathcal{O}}(n^{4/3}/\\varepsilon^4)$\ntime algorithm to compute an $\\varepsilon$-approximate bottleneck matching of\n$A,B\\subset\\mathbb{R}^2$ and an\n$\\frac{1}{\\varepsilon^{\\mathcal{O}(d)}}n^{1+\\frac{d-1}{2d-1}}\\mathrm{poly}\\log\nn$ time algorithm for computing $\\varepsilon$-approximate bottleneck matching\nin $d$-dimensions. All previous algorithms take $\\Omega(n^{3/2})$ time. Given\nany graph $G(A \\cup B,E)$ that has an easily computable balanced vertex\nseparator for every subgraph $G'(V',E')$ of size $|V'|^{\\delta}$, for\n$\\delta\\in [1/2,1)$, we can apply our algorithm to compute a maximum matching\nin $\\tilde{\\mathcal{O}}(mn^{\\frac{\\delta}{1+\\delta}})$ time improving upon the\n$\\mathcal{O}(m\\sqrt{n})$ time taken by the HK-Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:27:04 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Lahn", "Nathaniel", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "1903.10583", "submitter": "Felipe A. Louza", "authors": "Felipe A. Louza, Guilherme P. Telles, Simon Gog, Liang Zhao", "title": "Algorithms to compute the Burrows-Wheeler Similarity Distribution", "comments": "Accepted to TCS", "journal-ref": null, "doi": "10.1016/j.tcs.2019.03.012", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler transform (BWT) is a well studied text transformation\nwidely used in data compression and text indexing. The BWT of two strings can\nalso provide similarity measures between them, based on the observation that\nthe more their symbols are intermixed in the transformation, the more the\nstrings are similar. In this article we present two new algorithms to compute\nsimilarity measures based on the BWT for string collections. In particular, we\npresent practical and theoretical improvements to the computation of the\nBurrows-Wheeler similarity distribution for all pairs of strings in a\ncollection. Our algorithms take advantage of the BWT computed for the\nconcatenation of all strings, and use compressed data structures that allow\nreducing the running time with a small memory footprint, as shown by a set of\nexperiments with real and artificial datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 20:21:17 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Louza", "Felipe A.", ""], ["Telles", "Guilherme P.", ""], ["Gog", "Simon", ""], ["Zhao", "Liang", ""]]}, {"id": "1903.10700", "submitter": "Dharmarajan R Dr.", "authors": "R. Dharmarajan, and D. Ramachandran", "title": "On the tractability of the maximum clique problem", "comments": "15 (fifteen) pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum clique problem is a classical NP-complete problem in graph theory\nand has important applications in many domains. In this paper we show, in a\npartially non-constructive way, the existence of an exact polynomial-time\nalgorithm for this problem. We outline the algorithm in pseudo-code style. Then\nwe prove its exactness and efficiency by analysis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 06:23:04 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 05:47:35 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Dharmarajan", "R.", ""], ["Ramachandran", "D.", ""]]}, {"id": "1903.10701", "submitter": "Wiktor Zuba", "authors": "Wojciech Rytter and Wiktor Zuba", "title": "Syntactic View of Sigma-Tau Generation of Permutations", "comments": "accepted on LATA2019", "journal-ref": null, "doi": "10.1007/978-3-030-13435-8_33", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a syntactic view of the Sawada-Williams $(\\sigma,\\tau)$-generation of\npermutations. The corresponding sequence of $\\sigma-\\tau$-operations, of length\n$n!-1$ is shown to be highly compressible: it has $O(n^2\\log n)$ bit\ndescription. Using this compact description we design fast algorithms for\nranking and unranking permutations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 06:31:35 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Rytter", "Wojciech", ""], ["Zuba", "Wiktor", ""]]}, {"id": "1903.10983", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "A Tight Runtime Analysis for the cGA on Jump Functions---EDAs Can Cross\n  Fitness Valleys at No Extra Cost", "comments": "25 pages, full version of a paper to appear at GECCO 2019", "journal-ref": null, "doi": "10.1145/3321707.3321747", "report-no": null, "categories": "cs.NE cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the compact genetic algorithm (cGA) with hypothetical\npopulation size $\\mu = \\Omega(\\sqrt n \\log n) \\cap \\text{poly}(n)$ with high\nprobability finds the optimum of any $n$-dimensional jump function with jump\nsize $k < \\frac 1 {20} \\ln n$ in $O(\\mu \\sqrt n)$ iterations. Since it is known\nthat the cGA with high probability needs at least $\\Omega(\\mu \\sqrt n + n \\log\nn)$ iterations to optimize the unimodal OneMax function, our result shows that\nthe cGA in contrast to most classic evolutionary algorithms here is able to\ncross moderate-sized valleys of low fitness at no extra cost.\n  Our runtime guarantee improves over the recent upper bound $O(\\mu n^{1.5}\n\\log n)$ valid for $\\mu = \\Omega(n^{3.5+\\varepsilon})$ of Hasen\\\"ohrl and\nSutton (GECCO 2018). For the best choice of the hypothetical population size,\nthis result gives a runtime guarantee of $O(n^{5+\\varepsilon})$, whereas ours\ngives $O(n \\log n)$.\n  We also provide a simple general method based on parallel runs that, under\nmild conditions, (i)~overcomes the need to specify a suitable population size,\nbut gives a performance close to the one stemming from the best-possible\npopulation size, and (ii)~transforms EDAs with high-probability performance\nguarantees into EDAs with similar bounds on the expected runtime.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:12:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1903.11016", "submitter": "Orestis Papadigenopoulos", "authors": "Dimitris Fotakis, Jannik Matuschke, Orestis Papadigenopoulos", "title": "Malleable scheduling beyond identical machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In malleable job scheduling, jobs can be executed simultaneously on multiple\nmachines with the processing time depending on the number of allocated\nmachines. In this setting, jobs are required to be executed non-preemptively\nand in unison, in the sense that they occupy, during their execution, the same\ntime interval over all the machines of the allocated set. In this work, we\nstudy generalizations of malleable job scheduling inspired by standard\nscheduling on unrelated machines. Specifically, we introduce a general model of\nmalleable job scheduling, where each machine has a (possibly different) speed\nfor each job, and the processing time of a job $j$ on a set of allocated\nmachines $S$ depends on the total speed of $S$ with respect to $j$. For\nmachines with unrelated speeds, we show that the optimal makespan cannot be\napproximated within a factor less than $\\frac{e}{e-1}$, unless $P = NP$. On the\npositive side, we present polynomial-time algorithms with approximation ratios\n$\\frac{2e}{e-1}$ for machines with unrelated speeds, $3$ for machines with\nuniform speeds, and $7/3$ for restricted assignments on identical machines. Our\nalgorithms are based on deterministic LP rounding. They result in sparse\nschedules, in the sense that each machine shares at most one job with other\nmachines. We also prove lower bounds on the integrality gap of $1+\\varphi$ for\nunrelated speeds ($\\varphi$ is the golden ratio) and $2$ for uniform speeds and\nrestricted assignments. To indicate the generality of our approach, we show\nthat it also yields constant factor approximation algorithms for a variant\nwhere we determine the effective speed of a set of allocated machines based on\nthe $L_p$ norm of their speeds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:53:35 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 15:26:20 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 07:24:11 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Matuschke", "Jannik", ""], ["Papadigenopoulos", "Orestis", ""]]}, {"id": "1903.11062", "submitter": "Peter Zeman", "authors": "Roman Nedela, Ilia Ponomarenko, Peter Zeman", "title": "Testing isomorphism of circular-arc graphs in polynomial time", "comments": "Dear reader, recently we have found a problem in the proof of the\n  main theorem. It seems that we will be able to fill in the gap in the\n  arguments, and we are doing our best to do this as soon as possible. We\n  appologize for publishing an unfinished work. The authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is said to be circular-arc if the vertices can be associated with\narcs of a circle so that two vertices are adjacent if and only if the\ncorresponding arcs overlap. It is proved that the isomorphism of circular-arc\ngraphs can be tested by the Weisfeiler-Leman algorithm after individualization\nof two vertices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 10:03:19 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 06:57:01 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Nedela", "Roman", ""], ["Ponomarenko", "Ilia", ""], ["Zeman", "Peter", ""]]}, {"id": "1903.11980", "submitter": "Stefan Klootwijk", "authors": "Stefan Klootwijk, Bodo Manthey", "title": "Probabilistic Analysis of Facility Location on Random Shortest Path\n  Metrics", "comments": "A preliminary version accepted to CiE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The facility location problem is an NP-hard optimization problem. Therefore,\napproximation algorithms are often used to solve large instances. Such\nalgorithms often perform much better than worst-case analysis suggests.\nTherefore, probabilistic analysis is a widely used tool to analyze such\nalgorithms. Most research on probabilistic analysis of NP-hard optimization\nproblems involving metric spaces, such as the facility location problem, has\nbeen focused on Euclidean instances, and also instances with independent\n(random) edge lengths, which are non-metric, have been researched. We would\nlike to extend this knowledge to other, more general, metrics.\n  We investigate the facility location problem using random shortest path\nmetrics. We analyze some probabilistic properties for a simple greedy heuristic\nwhich gives a solution to the facility location problem: opening the $\\kappa$\ncheapest facilities (with $\\kappa$ only depending on the facility opening\ncosts). If the facility opening costs are such that $\\kappa$ is not too large,\nthen we show that this heuristic is asymptotically optimal. On the other hand,\nfor large values of $\\kappa$, the analysis becomes more difficult, and we\nprovide a closed-form expression as upper bound for the expected approximation\nratio. In the special case where all facility opening costs are equal this\nclosed-form expression reduces to $O(\\sqrt[4]{\\ln(n)})$ or $O(1)$ or even\n$1+o(1)$ if the opening costs are sufficiently small.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:01:44 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Klootwijk", "Stefan", ""], ["Manthey", "Bodo", ""]]}, {"id": "1903.12050", "submitter": "Miklos Z. Racz", "authors": "Mikl\\'os Z. R\\'acz, Benjamin Schiffer", "title": "Finding a planted clique by adaptive probing", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the planted clique problem where we are allowed\nunbounded computational time but can only investigate a small part of the graph\nby adaptive edge queries. We determine (up to logarithmic factors) the number\nof queries necessary both for detecting the presence of a planted clique and\nfor finding the planted clique.\n  Specifically, let $G \\sim G(n,1/2,k)$ be a random graph on $n$ vertices with\na planted clique of size $k$. We show that no algorithm that makes at most $q =\no(n^2 / k^2 + n)$ adaptive queries to the adjacency matrix of $G$ is likely to\nfind the planted clique. On the other hand, when $k \\geq (2+\\epsilon) \\log_2 n$\nthere exists a simple algorithm (with unbounded computational power) that finds\nthe planted clique with high probability by making $q = O( (n^2 / k^2) \\log^2 n\n+ n \\log n)$ adaptive queries. For detection, the additive $n$ term is not\nnecessary: the number of queries needed to detect the presence of a planted\nclique is $n^2 / k^2$ (up to logarithmic factors).\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:28:43 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 03:33:28 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["R\u00e1cz", "Mikl\u00f3s Z.", ""], ["Schiffer", "Benjamin", ""]]}, {"id": "1903.12135", "submitter": "Kyle Luh", "authors": "Jaros{\\l}aw B{\\l}asiok, Patrick Lopatto, Kyle Luh, Jake Marcinek and\n  Shravas Rao", "title": "An Improved Lower Bound for Sparse Reconstruction from Subsampled\n  Hadamard Matrices", "comments": "Improved exposition and added an author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a short argument that yields a new lower bound on the number of\nsubsampled rows from a bounded, orthonormal matrix necessary to form a matrix\nwith the restricted isometry property. We show that a matrix formed by\nuniformly subsampling rows of an $N \\times N$ Hadamard matrix contains a\n$K$-sparse vector in the kernel, unless the number of subsampled rows is\n$\\Omega(K \\log K \\log (N/K))$ --- our lower bound applies whenever $\\min(K,\nN/K) > \\log^C N$. Containing a sparse vector in the kernel precludes not only\nthe restricted isometry property, but more generally the application of those\nmatrices for uniform sparse recovery.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:21:51 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 13:41:05 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Lopatto", "Patrick", ""], ["Luh", "Kyle", ""], ["Marcinek", "Jake", ""], ["Rao", "Shravas", ""]]}, {"id": "1903.12146", "submitter": "Shravas Rao", "authors": "Shravas Rao", "title": "Improved Lower Bounds for the Restricted Isometry Property of Subsampled\n  Fourier Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A$ be an $N \\times N$ Fourier matrix over\n$\\mathbb{F}_p^{\\log{N}/\\log{p}}$ for some prime $p$. We improve upon known\nlower bounds for the number of rows of $A$ that must be sampled so that the\nresulting matrix $M$ satisfies the restricted isometry property for $k$-sparse\nvectors. This property states that $\\|Mv\\|_2^2$ is approximately $\\|v\\|_2^2$\nfor all $k$-sparse vectors $v$. In particular, if $k = \\Omega( \\log^2{N})$, we\nshow that $\\Omega(k\\log{k}\\log{N}/\\log{p})$ rows must be sampled to satisfy the\nrestricted isometry property with constant probability.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:34:10 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Rao", "Shravas", ""]]}, {"id": "1903.12150", "submitter": "Navid Nouri", "authors": "Michael Kapralov and Navid Nouri and Aaron Sidford and Jakab Tardos", "title": "Dynamic Streaming Spectral Sparsification in Nearly Linear Time and\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of computing spectral approximations to\ngraphs in the single pass dynamic streaming model. We provide a linear\nsketching based solution that given a stream of edge insertions and deletions\nto a $n$-node undirected graph, uses $\\tilde O(n)$ space, processes each update\nin $\\tilde O(1)$ time, and with high probability recovers a spectral sparsifier\nin $\\tilde O(n)$ time. Prior to our work, state of the art results either used\nnear optimal $\\tilde O(n)$ space complexity, but brute-force $\\Omega(n^2)$\nrecovery time [Kapralov et al.'14], or with subquadratic runtime, but\npolynomially suboptimal space complexity [Ahn et al.'14, Kapralov et al.'19].\n  Our main technical contribution is a novel method for `bucketing' vertices of\nthe input graph into clusters that allows fast recovery of edges of\nsufficiently large effective resistance. Our algorithm first buckets vertices\nof the graph by performing ball-carving using (an approximation to) its\neffective resistance metric, and then recovers the high effective resistance\nedges from a sketched version of an electrical flow between vertices in a\nbucket, taking nearly linear time in the number of vertices overall. This\nprocess is performed at different geometric scales to recover a sample of edges\nwith probabilities proportional to effective resistances and obtain an actual\nsparsifier of the input graph.\n  This work provides both the first efficient $\\ell_2$-sparse recovery\nalgorithm for graphs and new primitives for manipulating the effective\nresistance embedding of a graph, both of which we hope have further\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:38:45 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kapralov", "Michael", ""], ["Nouri", "Navid", ""], ["Sidford", "Aaron", ""], ["Tardos", "Jakab", ""]]}, {"id": "1903.12165", "submitter": "Navid Nouri", "authors": "Michael Kapralov and Aida Mousavifar and Cameron Musco and Christopher\n  Musco and Navid Nouri", "title": "Faster Spectral Sparsification in Dynamic Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sketching has emerged as a powerful technique for processing massive\ngraphs that change over time (i.e., are presented as a dynamic stream of edge\nupdates) over the past few years, starting with the work of Ahn, Guha and\nMcGregor (SODA'12) on graph connectivity via sketching. In this paper we\nconsider the problem of designing spectral approximations to graphs, or\nspectral sparsifiers, using a small number of linear measurements, with the\nadditional constraint that the sketches admit an efficient recovery scheme.\n  Prior to our work, sketching algorithms were known with near optimal $\\tilde\nO(n)$ space complexity, but $\\Omega(n^2)$ time decoding (brute-force over all\npotential edges of the input graph), or with subquadratic time, but rather\nlarge $\\Omega(n^{5/3})$ space complexity (due to their reliance on a rather\nweak relation between connectivity and effective resistances). In this paper we\nfirst show how a simple relation between effective resistances and edge\nconnectivity leads to an improved $\\widetilde O(n^{3/2})$ space and time\nalgorithm, which we show is a natural barrier for connectivity based\napproaches. Our main result then gives the first algorithm that achieves\nsubquadratic recovery time, i.e. avoids brute-force decoding, and at the same\ntime nontrivially uses the effective resistance metric, achieving\n$n^{1.4+o(1)}$ space and recovery time.\n  Our main technical contribution is a novel method for `bucketing' vertices of\nthe input graph into clusters that allows fast recovery of edges of high\neffective resistance: the buckets are formed by performing ball-carving on the\ninput graph using (an approximation to) its effective resistance metric. We\nfeel that this technique is likely to be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:50:51 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kapralov", "Michael", ""], ["Mousavifar", "Aida", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Nouri", "Navid", ""]]}, {"id": "1903.12312", "submitter": "Paul Medvedev", "authors": "Rayan Chikhi, Jan Holub, and Paul Medvedev", "title": "Data structures to represent a set of k-long DNA sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of biological sequencing data has been one of the biggest\napplications of string algorithms. The approaches used in many such\napplications are based on the analysis of k-mers, which are short fixed-length\nstrings present in a dataset. While these approaches are rather diverse,\nstoring and querying a k-mer set has emerged as a shared underlying component.\nA set of k-mers has unique features and applications that, over the last ten\nyears, have resulted in many specialized approaches for its representation. In\nthis survey, we give a unified presentation and comparison of the data\nstructures that have been proposed to store and query a k-mer set. We hope this\nsurvey will serve as a resource for researchers in the field as well as make\nthe area more accessible to researchers outside the field.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 01:09:50 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 19:26:46 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 16:20:46 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 20:10:54 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Chikhi", "Rayan", ""], ["Holub", "Jan", ""], ["Medvedev", "Paul", ""]]}, {"id": "1903.12400", "submitter": "Efstratios Rappos", "authors": "Efstratios Rappos and Stephan Robert and Philippe Cudr\\'e-Mauroux", "title": "A Force-Directed Approach for Offline GPS Trajectory Map Matching", "comments": "10 pages, 12 figures, accepted version of article submitted to ACM\n  SIGSPATIAL 2018, Seattle, USA", "journal-ref": "26th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (SIGSPATIAL '18), November 6-9, 2018, Seattle,\n  WA, USA", "doi": "10.1145/3274895.3274919", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm to match GPS trajectories onto maps offline (in\nbatch mode) using techniques borrowed from the field of force-directed graph\ndrawing. We consider a simulated physical system where each GPS trajectory is\nattracted or repelled by the underlying road network via electrical-like\nforces. We let the system evolve under the action of these physical forces such\nthat individual trajectories are attracted towards candidate roads to obtain a\nmap matching path. Our approach has several advantages compared to traditional,\nrouting-based, algorithms for map matching, including the ability to account\nfor noise and to avoid large detours due to outliers in the data whilst taking\ninto account the underlying topological restrictions (such as one-way roads).\nOur empirical evaluation using real GPS traces shows that our method produces\nbetter map matching results compared to alternative offline map matching\nalgorithms on average, especially for routes in dense, urban areas.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 08:58:41 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Rappos", "Efstratios", ""], ["Robert", "Stephan", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1903.12449", "submitter": "Igor Nesiolovskiy", "authors": "Igor Nesiolovskiy, Artem Nesiolovskiy", "title": "Multiplication method for factoring natural numbers", "comments": "10 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer multiplication method for factoring big natural numbers which\nextends the group of the Fermat's and Lehman's factorization algorithms and has\nrun-time complexity $O(n^{1/3})$. This paper is argued the finiteness of\nproposed algorithm depending on the value of the factorizable number n. We\nprovide here comparative tests results of related algorithms on a large amount\nof computational checks. We describe identified advantages of the proposed\nalgorithm over others. The possibilities of algorithm optimization for reducing\nthe complexity of factorization are also shown here.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 11:05:51 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Nesiolovskiy", "Igor", ""], ["Nesiolovskiy", "Artem", ""]]}, {"id": "1903.12525", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Sabuzima Nayak and Samir Kumar Borgohain", "title": "Shed More Light on Bloom Filter's Variants", "comments": "8 pages, 5 Figures, 1 Table, Proceedings of the 2018 International\n  Conference on Information and Knowledge Engineering (IKE'18), pp. 14-21", "journal-ref": "Proceedings of the 2018 International Conference on Information\n  and Knowledge Engineering (IKE'18), pp. 14-21, 2018", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom Filter is a probabilistic membership data structure and it is\nexcessively used data structure for membership query. Bloom Filter becomes the\npredominant data structure in approximate membership filtering. Bloom Filter\nextremely enhances the query response time, and the response time is very fast.\nBloom filter (BF) is used to detect whether an element belongs to a given set\nor not. The Bloom Filter returns True Positive (TP), False Positive (FP), or\nTrue Negative (TN). The Bloom Filter is widely adapted in numerous areas to\nenhance the performance of a system. In this paper, we present a) in-depth\ninsight on the Bloom Filter,and b) the prominent variants of the Bloom Filters.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 20:19:23 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Patgiri", "Ripon", ""], ["Nayak", "Sabuzima", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "1903.12641", "submitter": "Brahim Chaourar", "authors": "Brahim Chaourar", "title": "Connected max cut is polynomial for graphs without $K_5\\backslash e$ as\n  a minor", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V, E)$, a connected cut $\\delta (U)$ is the set of edges of\nE linking all vertices of U to all vertices of $V\\backslash U$ such that the\ninduced subgraphs $G[U]$ and $G[V\\backslash U]$ are connected. Given a positive\nweight function $w$ defined on $E$, the connected maximum cut problem (CMAX\nCUT) is to find a connected cut $\\Omega$ such that $w(\\Omega)$ is maximum among\nall connected cuts. CMAX CUT is NP-hard even for planar graphs. In this paper,\nwe prove that CMAX CUT is polynomial for graphs without $K_5\\backslash e$ as a\nminor. We deduce a quadratic time algorithm for the minimum cut problem in the\nsame class of graphs without computing the maximum flow.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 17:42:37 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 21:23:11 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chaourar", "Brahim", ""]]}]