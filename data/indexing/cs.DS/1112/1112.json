[{"id": "1112.0031", "submitter": "C. Seshadhri", "authors": "David Gleich and C. Seshadhri", "title": "Neighborhoods are good communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The communities of a social network are sets of vertices with more\nconnections inside the set than outside. We theoretically demonstrate that two\ncommonly observed properties of social networks, heavy-tailed degree\ndistributions and large clustering coefficients, imply the existence of vertex\nneighborhoods (also known as egonets) that are themselves good communities. We\nevaluate these neighborhood communities on a range of graphs. What we find is\nthat the neighborhood communities often exhibit conductance scores that are as\ngood as the Fiedler cut. Also, the conductance of neighborhood communities\nshows similar behavior as the network community profile computed with a\npersonalized PageRank community detection method. The latter requires sweeping\nover a great many starting vertices, which can be expensive. By using a small\nand easy-to-compute set of neighborhood communities as seeds for these PageRank\ncommunities, however, we find communities that precisely capture the behavior\nof the network community profile when seeded everywhere in the graph, and at a\nsignificant reduction in total work.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 21:12:11 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Gleich", "David", ""], ["Seshadhri", "C.", ""]]}, {"id": "1112.0184", "submitter": "Christian Konrad", "authors": "Christian Konrad and Fr\\'ed\\'eric Magniez and Claire Mathieu", "title": "Maximum Matching in Semi-Streaming with Few Passes", "comments": "Algorithms for general graphs have been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the semi-streaming model, an algorithm receives a stream of edges of a\ngraph in arbitrary order and uses a memory of size $O(n \\mbox{ polylog } n)$,\nwhere $n$ is the number of vertices of a graph. In this work, we present\nsemi-streaming algorithms that perform one or two passes over the input stream\nfor maximum matching with no restrictions on the input graph, and for the\nimportant special case of bipartite graphs that we refer to as maximum\nbipartite matching (MBM). The Greedy matching algorithm performs one pass over\nthe input and outputs a $1/2$ approximation. Whether there is a better one-pass\nalgorithm has been an open question since the appearance of the first paper on\nstreaming algorithms for matching problems in 2005 [Feigenbaum et al., SODA\n2005]. We make the following progress on this problem:\n  In the one-pass setting, we show that there is a deterministic semi-streaming\nalgorithm for MBM with expected approximation factor $1/2+0.005$, assuming that\nedges arrive one by one in (uniform) random order. We extend this algorithm to\ngeneral graphs, and we obtain a $1/2+0.003$ approximation.\n  In the two-pass setting, we do not require the random arrival order\nassumption (the edge stream is in arbitrary order). We present a simple\nrandomized two-pass semi-streaming algorithm for MBM with expected\napproximation factor $1/2 + 0.019$. Furthermore, we discuss a more involved\ndeterministic two-pass semi-streaming algorithm for MBM with approximation\nfactor $1/2 + 0.019$ and a generalization of this algorithm to general graphs\nwith approximation factor $1/2 + 0.0071$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 14:13:45 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 07:23:42 GMT"}, {"version": "v3", "created": "Thu, 10 Apr 2014 14:33:40 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Konrad", "Christian", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""], ["Mathieu", "Claire", ""]]}, {"id": "1112.0245", "submitter": "Ignaz Rutter", "authors": "Thomas Bl\\\"asius and Ignaz Rutter", "title": "Simultaneous PQ-Ordering with Applications to Constrained Embedding\n  Problems", "comments": "46 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define and study the new problem Simultaneous PQ-Ordering.\nIts input consists of a set of PQ-trees, which represent sets of circular\norders of their leaves, together with a set of child-parent relations between\nthese PQ-trees, such that the leaves of the child form a subset of the leaves\nof the parent. Simultaneous PQ-Ordering asks whether orders of the leaves of\neach of the trees can be chosen simultaneously, that is, for every child-parent\nrelation the order chosen for the parent is an extension of the order chosen\nfor the child. We show that Simultaneous PQ-Ordering is NP-complete in general\nand that it is efficiently solvable for a special subset of instances, the\n2-fixed instances. We then show that several constrained embedding problems can\nbe formulated as such 2-fixed instances.\n  In particular, we obtain a linear-time algorithm for Partially PQ-Constrained\nPlanarity for biconnected graphs, a common generalization of two recently\nconsidered embedding problems, and a quadratic-time algorithm for Simultaneous\nEmbedding with Fixed Edges for biconnected graphs with a connected\nintersection; formerly only the much more restricted case that the intersection\nis biconnected was known to be efficiently solvable. Both results can be\nextended to the case where the input graphs are not necessarily biconnected but\nhave the property that each cutvertex is contained in at most two non-trivial\nblocks. This includes for example the case where both graphs have maximum\ndegree 5. Moreover, we give an optimal linear-time algorithm for recognition of\nsimultaneous interval graphs, improving upon a recent O(n^2 log n)-time\nalgorithm due to Jampani and Lubiw and show that this can be used to also solve\nthe problem of extending partial interval representations of graphs with n\nvertices and m edges in time O(n + m), improving a recent result of Klav\\'ik et\nal.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 17:13:40 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1112.0278", "submitter": "Peng Zhang", "authors": "Tian-Ming Bu, Chen Yuan, Peng Zhang", "title": "Computing on Binary Strings", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in Computer Science can be abstracted to the following\nquestion: given a set of objects and rules respectively, which new objects can\nbe produced? In the paper, we consider a succinct version of the question:\ngiven a set of binary strings and several operations like conjunction and\ndisjunction, which new binary strings can be generated? Although it is a\nfundamental problem, to the best of our knowledge, the problem hasn't been\nstudied yet. In this paper, an O(m^2n) algorithm is presented to determine\nwhether a string s is representable by a set W, where n is the number of\nstrings in W and each string has the same length m. However, looking for the\nminimum subset from a set to represent a given string is shown to be NP-hard.\nAlso, finding the smallest subset from a set to represent each string in the\noriginal set is NP-hard. We establishes inapproximability results and\napproximation algorithms for them. In addition, we prove that counting the\nnumber of strings representable is #P-complete. We then explore how the\nproblems change when the operator negation is available. For example, if the\noperator negation can be used, the number is some power of 2. This difference\nmaybe help us understand the problem more profoundly.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 19:24:16 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2012 11:18:29 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Bu", "Tian-Ming", ""], ["Yuan", "Chen", ""], ["Zhang", "Peng", ""]]}, {"id": "1112.0520", "submitter": "Bin Fu", "authors": "Bin Fu", "title": "On the Complexity of Approximate Sum of Sorted List", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity for computing the approximate sum\n$a_1+a_2+...+a_n$ of a sorted list of numbers $a_1\\le a_2\\le ...\\le a_n$. We\nshow an algorithm that computes an $(1+\\epsilon)$-approximation for the sum of\na sorted list of nonnegative numbers in an $O({1\\over \\epsilon}\\min(\\log n,\n{\\log ({x_{max}\\over x_{min}})})\\cdot (\\log {1\\over \\epsilon}+\\log\\log n))$\ntime, where $x_{max}$ and $x_{min}$ are the largest and the least positive\nelements of the input list, respectively. We prove a lower bound\n$\\Omega(\\min(\\log n,\\log ({x_{max}\\over x_{min}}))$ time for every\nO(1)-approximation algorithm for the sum of a sorted list of nonnegative\nelements. We also show that there is no sublinear time approximation algorithm\nfor the sum of a sorted list that contains at least one negative number.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 17:50:10 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2011 20:22:51 GMT"}, {"version": "v3", "created": "Mon, 16 Jan 2012 21:08:54 GMT"}, {"version": "v4", "created": "Sat, 21 Jan 2012 21:58:00 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Fu", "Bin", ""]]}, {"id": "1112.0534", "submitter": "Christoph Durr", "authors": "Christoph D\\\"urr, Maurice Queyranne, Frits C.R. Spieksma, Fabrice\n  Talla Nobibon and Gerhard J. Woeginger", "title": "The interval ordering problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given set of intervals on the real line, we consider the problem of\nordering the intervals with the goal of minimizing an objective function that\ndepends on the exposed interval pieces (that is, the pieces that are not\ncovered by earlier intervals in the ordering). This problem is motivated by an\napplication in molecular biology that concerns the determination of the\nstructure of the backbone of a protein.\n  We present polynomial-time algorithms for several natural special cases of\nthe problem that cover the situation where the interval boundaries are\nagreeably ordered and the situation where the interval set is laminar. Also the\nbottleneck variant of the problem is shown to be solvable in polynomial time.\nFinally we prove that the general problem is NP-hard, and that the existence of\na constant-factor-approximation algorithm is unlikely.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 18:35:06 GMT"}], "update_date": "2011-12-05", "authors_parsed": [["D\u00fcrr", "Christoph", ""], ["Queyranne", "Maurice", ""], ["Spieksma", "Frits C. R.", ""], ["Nobibon", "Fabrice Talla", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1112.0689", "submitter": "Ashwinkumar B. V.", "authors": "Ashwinkumar Badanidiyuru, Robert Kleinberg, Hooyeon Lee", "title": "Approximating Low-Dimensional Coverage Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the maximum coverage problem, restricted to set\nsystems of bounded VC-dimension. Our main result is a fixed-parameter tractable\napproximation scheme: an algorithm that outputs a $(1-\\eps)$-approximation to\nthe maximum-cardinality union of $k$ sets, in running time $O(f(\\eps,k,d)\\cdot\npoly(n))$ where $n$ is the problem size, $d$ is the VC-dimension of the set\nsystem, and $f(\\eps,k,d)$ is exponential in $(kd/\\eps)^c$ for some constant\n$c$. We complement this positive result by showing that the function\n$f(\\eps,k,d)$ in the running-time bound cannot be replaced by a function\ndepending only on $(\\eps,d)$ or on $(k,d)$, under standard complexity\nassumptions.\n  We also present an improved upper bound on the approximation ratio of the\ngreedy algorithm in special cases of the problem, including when the sets have\nbounded cardinality and when they are two-dimensional halfspaces. Complementing\nthese positive results, we show that when the sets are four-dimensional\nhalfspaces neither the greedy algorithm nor local search is capable of\nimproving the worst-case approximation ratio of $1-1/e$ that the greedy\nalgorithm achieves on arbitrary instances of maximum coverage.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2011 20:52:31 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Kleinberg", "Robert", ""], ["Lee", "Hooyeon", ""]]}, {"id": "1112.0699", "submitter": "Lee-Ad Gottlieb", "authors": "Yair Bartal, Lee-Ad Gottlieb, Robert Krauthgamer", "title": "The Traveling Salesman Problem: Low-Dimensionality Implies a Polynomial\n  Time Approximation Scheme", "comments": null, "journal-ref": null, "doi": "10.1137/130913328", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Traveling Salesman Problem (TSP) is among the most famous NP-hard\noptimization problems. We design for this problem a randomized polynomial-time\nalgorithm that computes a (1+eps)-approximation to the optimal tour, for any\nfixed eps>0, in TSP instances that form an arbitrary metric space with bounded\nintrinsic dimension.\n  The celebrated results of Arora (A-98) and Mitchell (M-99) prove that the\nabove result holds in the special case of TSP in a fixed-dimensional Euclidean\nspace. Thus, our algorithm demonstrates that the algorithmic tractability of\nmetric TSP depends on the dimensionality of the space and not on its specific\ngeometry. This result resolves a problem that has been open since the\nquasi-polynomial time algorithm of Talwar (T-04).\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2011 22:58:13 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 12:56:30 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Bartal", "Yair", ""], ["Gottlieb", "Lee-Ad", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1112.0784", "submitter": "Jeremy Fineman", "authors": "Michael A. Bender, Jeremy T. Fineman, Seth Gilbert, and Robert E.\n  Tarjan", "title": "A New Approach to Incremental Cycle Detection and Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a cycle in a directed graph that grows\nby arc insertions, and the related problems of maintaining a topological order\nand the strong components of such a graph. For these problems, we give two\nalgorithms, one suited to sparse graphs, and the other to dense graphs. The\nformer takes the minimum of O(m^{3/2}) and O(mn^{2/3}) time to insert m arcs\ninto an n-vertex graph; the latter takes O(n^2 log(n)) time. Our sparse\nalgorithm is considerably simpler than a previous O(m^{3/2})-time algorithm; it\nis also faster on graphs of sufficient density. The time bound of our dense\nalgorithm beats the previously best time bound of O(n^{5/2}) for dense graphs.\nOur algorithms rely for their efficiency on topologically ordered vertex\nnumberings; bounds on the size of the numbers give bound on running times.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2011 18:12:38 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Bender", "Michael A.", ""], ["Fineman", "Jeremy T.", ""], ["Gilbert", "Seth", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1112.0790", "submitter": "Seth Pettie", "authors": "Ran Duan and Seth Pettie and Hsin-Hao Su", "title": "Scaling algorithms for approximate and exact maximum weight matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em maximum cardinality} and {\\em maximum weight matching} problems can\nbe solved in time $\\tilde{O}(m\\sqrt{n})$, a bound that has resisted improvement\ndespite decades of research. (Here $m$ and $n$ are the number of edges and\nvertices.) In this article we demonstrate that this \"$m\\sqrt{n}$ barrier\" is\nextremely fragile, in the following sense. For any $\\epsilon>0$, we give an\nalgorithm that computes a $(1-\\epsilon)$-approximate maximum weight matching in\n$O(m\\epsilon^{-1}\\log\\epsilon^{-1})$ time, that is, optimal {\\em linear time}\nfor any fixed $\\epsilon$. Our algorithm is dramatically simpler than the best\nexact maximum weight matching algorithms on general graphs and should be\nappealing in all applications that can tolerate a negligible relative error.\n  Our second contribution is a new {\\em exact} maximum weight matching\nalgorithm for integer-weighted bipartite graphs that runs in time\n$O(m\\sqrt{n}\\log N)$. This improves on the $O(Nm\\sqrt{n})$-time and\n$O(m\\sqrt{n}\\log(nN))$-time algorithms known since the mid 1980s, for $1\\ll\n\\log N \\ll \\log n$. Here $N$ is the maximum integer edge weight.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2011 20:05:24 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Duan", "Ran", ""], ["Pettie", "Seth", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1112.0826", "submitter": "Yingyu Liang", "authors": "Maria Florina Balcan, Yingyu Liang", "title": "Clustering under Perturbation Resilience", "comments": "54 pages. Appears in SIAM Journal on Computing (SICOMP), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fact that distances between data points in many real-world\nclustering instances are often based on heuristic measures, Bilu and\nLinial~\\cite{BL} proposed analyzing objective based clustering problems under\nthe assumption that the optimum clustering to the objective is preserved under\nsmall multiplicative perturbations to distances between points. The hope is\nthat by exploiting the structure in such instances, one can overcome worst case\nhardness results.\n  In this paper, we provide several results within this framework. For\ncenter-based objectives, we present an algorithm that can optimally cluster\ninstances resilient to perturbations of factor $(1 + \\sqrt{2})$, solving an\nopen problem of Awasthi et al.~\\cite{ABS10}. For $k$-median, a center-based\nobjective of special interest, we additionally give algorithms for a more\nrelaxed assumption in which we allow the optimal solution to change in a small\n$\\epsilon$ fraction of the points after perturbation. We give the first bounds\nknown for $k$-median under this more realistic and more general assumption. We\nalso provide positive results for min-sum clustering which is typically a\nharder objective than center-based objectives from approximability standpoint.\nOur algorithms are based on new linkage criteria that may be of independent\ninterest.\n  Additionally, we give sublinear-time algorithms, showing algorithms that can\nreturn an implicit clustering from only access to a small random sample.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 03:42:07 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2011 19:49:32 GMT"}, {"version": "v3", "created": "Fri, 30 Dec 2011 03:37:24 GMT"}, {"version": "v4", "created": "Fri, 8 Aug 2014 02:27:56 GMT"}, {"version": "v5", "created": "Sun, 11 Dec 2016 21:41:33 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Liang", "Yingyu", ""]]}, {"id": "1112.0857", "submitter": "Jelle Hellings", "authors": "Jelle Hellings, George H. L. Fletcher, and Herman Haverkort", "title": "I/O efficient bisimulation partitioning on very large directed acyclic\n  graphs", "comments": null, "journal-ref": null, "doi": "10.1145/2213836.2213899", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the first efficient external-memory algorithm to\ncompute the bisimilarity equivalence classes of a directed acyclic graph (DAG).\nDAGs are commonly used to model data in a wide variety of practical\napplications, ranging from XML documents and data provenance models, to web\ntaxonomies and scientific workflows. In the study of efficient reasoning over\nmassive graphs, the notion of node bisimilarity plays a central role. For\nexample, grouping together bisimilar nodes in an XML data set is the first step\nin many sophisticated approaches to building indexing data structures for\nefficient XPath query evaluation. To date, however, only internal-memory\nbisimulation algorithms have been investigated. As the size of real-world DAG\ndata sets often exceeds available main memory, storage in external memory\nbecomes necessary. Hence, there is a practical need for an efficient approach\nto computing bisimulation in external memory.\n  Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)),\nwhere |N| and |E| are the numbers of nodes and edges, resp., in the data graph\nand Sort(n) is the number of accesses to external memory needed to sort an\ninput of size n. We also study specializations of this algorithm to common\nvariations of bisimulation for tree-structured XML data sets. We empirically\nverify efficient performance of the algorithms on graphs and XML documents\nhaving billions of nodes and edges, and find that the algorithms can process\nsuch graphs efficiently even when very limited internal memory is available.\nThe proposed algorithms are simple enough for practical implementation and use,\nand open the door for further study of external-memory bisimulation algorithms.\nTo this end, the full open-source C++ implementation has been made freely\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 08:30:55 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Hellings", "Jelle", ""], ["Fletcher", "George H. L.", ""], ["Haverkort", "Herman", ""]]}, {"id": "1112.0993", "submitter": "Amr Elmasry", "authors": "Amr Elmasry and Jyrki Katajainen", "title": "Worst-Case Optimal Priority Queues via Extended Regular Counters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of representing a collection of priority\nqueues under the operations \\Findmin{}, \\Insert{}, \\Decrease{}, \\Meld{},\n\\Delete{}, and \\Deletemin{}. In the comparison-based model, if the first four\noperations are to be supported in constant time, the last two operations must\ntake at least logarithmic time. Brodal showed that his worst-case efficient\npriority queues achieve these worst-case bounds. Unfortunately, this data\nstructure is involved and the time bounds hide large constants. We describe a\nnew variant of the worst-case efficient priority queues that relies on extended\nregular counters and provides the same asymptotic time and space bounds as the\noriginal. Due to the conceptual separation of the operations on regular\ncounters and all other operations, our data structure is simpler and easier to\ndescribe and understand. Also, the constants in the time and space bounds are\nsmaller. In addition, we give an implementation of our structure on a pointer\nmachine. For our pointer-machine implementation, \\Decrease{} and \\Meld{} are\nasymptotically slower and require $O(\\lg\\lg{n})$ worst-case time, where $n$\ndenotes the number of elements stored in the resulting priority queue.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 16:55:27 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Elmasry", "Amr", ""], ["Katajainen", "Jyrki", ""]]}, {"id": "1112.1116", "submitter": "Oren Weimann", "authors": "Oren Weimann and Raphael Yuster", "title": "Approximating the Diameter of Planar Graphs in Near Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $(1+\\epsilon)$-approximation algorithm running in\n$O(f(\\epsilon)\\cdot n \\log^4 n)$ time for finding the diameter of an undirected\nplanar graph with non-negative edge lengths.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 22:46:04 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 10:02:06 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2012 10:09:35 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2012 10:24:01 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2013 13:08:37 GMT"}, {"version": "v6", "created": "Sat, 20 Apr 2013 17:24:33 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Weimann", "Oren", ""], ["Yuster", "Raphael", ""]]}, {"id": "1112.1136", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Seeun Umboh, Shuchi Chawla and David Malec", "title": "Secretary Problems with Convex Costs", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online resource allocation problems where given a set of requests\nour goal is to select a subset that maximizes a value minus cost type of\nobjective function. Requests are presented online in random order, and each\nrequest possesses an adversarial value and an adversarial size. The online\nalgorithm must make an irrevocable accept/reject decision as soon as it sees\neach request. The \"profit\" of a set of accepted requests is its total value\nminus a convex cost function of its total size. This problem falls within the\nframework of secretary problems. Unlike previous work in that area, one of the\nmain challenges we face is that the objective function can be positive or\nnegative and we must guard against accepting requests that look good early on\nbut cause the solution to have an arbitrarily large cost as more requests are\naccepted. This requires designing new techniques.\n  We study this problem under various feasibility constraints and present\nonline algorithms with competitive ratios only a constant factor worse than\nthose known in the absence of costs for the same feasibility constraints. We\nalso consider a multi-dimensional version of the problem that generalizes\nmulti-dimensional knapsack within a secretary framework. In the absence of any\nfeasibility constraints, we present an O(l) competitive algorithm where l is\nthe number of dimensions; this matches within constant factors the best known\nratio for multi-dimensional knapsack secretary.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 01:01:44 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Barman", "Siddharth", ""], ["Umboh", "Seeun", ""], ["Chawla", "Shuchi", ""], ["Malec", "David", ""]]}, {"id": "1112.1139", "submitter": "Mark Heiligman", "authors": "Mark Heiligman", "title": "Quantum Verification of Minimum Spanning Tree", "comments": "5 papges", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Previous studies has shown that for a weighted undirected graph having $n$\nvertices and $m$ edges, a minimal weight spanning tree can be found with\n$O^*(\\sqrt{mn})$ calls to the weight oracle. The present note shows that a\ngiven spanning tree can be verified to be a minimal weight spanning tree with\nonly $O(n\\bigr)$ calls to the weight oracle and $O(n+\\sqrt{m}\\log n)$ total\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 15:26:51 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Heiligman", "Mark", ""]]}, {"id": "1112.1141", "submitter": "Fabiano Botelho Dr.", "authors": "Nitin Garg and Ed Zhu and Fabiano C. Botelho", "title": "Highly-Concurrent Doubly-Linked Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As file systems are increasingly being deployed on ever larger systems with\nmany cores and multi-gigabytes of memory, scaling the internal data structures\nof file systems has taken greater importance and urgency. A doubly-linked list\nis a simple and very commonly used data structure in file systems but it is not\nvery friendly to multi-threaded use. While special cases of lists, such as\nqueues and stacks, have lock-free versions that scale reasonably well, the\ngeneral form of a doubly-linked list offers no such solution. Using a mutex to\nserialize all operations remains the de-facto method of maintaining a doubly\nlinked list. This severely limits the scalability of the list and developers\nmust resort to ad-hoc workarounds that involve using multiple smaller lists\n(with individual locks) and deal with the resulting complexity of the system.\nIn this paper, we present an approach to building highly concurrent data\nstructures, with special focus on the implementation of highly concurrent\ndoubly-linked lists. Dubbed \"advanced doubly-linked list\" or \"adlist\" for\nshort, our list allows iteration in any direction, and insert/delete operations\nover non-overlapping nodes to execute in parallel. Operations with common nodes\nget serialized so as to always present a locally consistent view to the\ncallers. An adlist node needs an additional 8 bytes of space for keeping\nsynchronization information. The Data Domain File System makes extensive use of\nadlists which has allowed for significant scaling of the system without\nsacrificing simplicity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 01:37:51 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Garg", "Nitin", ""], ["Zhu", "Ed", ""], ["Botelho", "Fabiano C.", ""]]}, {"id": "1112.1178", "submitter": "Hassan Halabian", "authors": "Hassan Halabian, Ioannis Lambadaris, Yannis Viniotis, Chung-Horng Lung", "title": "Optimal Server Assignment in Multi-Server Queueing Systems with Random\n  Connectivities", "comments": "41 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of assigning $K$ identical servers to a set of $N$\nparallel queues in a time-slotted queueing system. The connectivity of each\nqueue to each server is randomly changing with time; each server can serve at\nmost one queue and each queue can be served by at most one server during each\ntime slot. Such a queueing model has been used in addressing resource\nallocation problems in wireless networks. It has been previously proven that\nMaximum Weighted Matching (MWM) is a throughput-optimal server assignment\npolicy for such a queueing system. In this paper, we prove that for a system\nwith i.i.d. Bernoulli packet arrivals and connectivities, MWM minimizes, in\nstochastic ordering sense, a broad range of cost functions of the queue lengths\nsuch as total queue occupancy (which implies minimization of average queueing\ndelays). Then, we extend the model by considering imperfect services where it\nis assumed that the service of a scheduled packet fails randomly with a certain\nprobability. We prove that the same policy is still optimal for the extended\nmodel. We finally show that the results are still valid for more general\nconnectivity and arrival processes which follow conditional permutation\ninvariant distributions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 07:28:08 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2013 20:07:47 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Halabian", "Hassan", ""], ["Lambadaris", "Ioannis", ""], ["Viniotis", "Yannis", ""], ["Lung", "Chung-Horng", ""]]}, {"id": "1112.1210", "submitter": "Michael Dinitz", "authors": "Atish Das Sarma, Michael Dinitz and Gopal Pandurangan", "title": "Efficient Computation of Distance Sketches in Distributed Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance computation is one of the most fundamental primitives used in\ncommunication networks. The cost of effectively and accurately computing\npairwise network distances can become prohibitive in large-scale networks such\nas the Internet and Peer-to-Peer (P2P) networks. To negotiate the rising need\nfor very efficient distance computation, approximation techniques for numerous\nvariants of this question have recently received significant attention in the\nliterature. The goal is to preprocess the graph and store a small amount of\ninformation such that whenever a query for any pairwise distance is issued, the\ndistance can be well approximated (i.e., with small stretch) very quickly in an\nonline fashion. Specifically, the pre-processing (usually) involves storing a\nsmall sketch with each node, such that at query time only the sketches of the\nconcerned nodes need to be looked up to compute the approximate distance. In\nthis paper, we present the first theoretical study of distance sketches derived\nfrom distance oracles in a distributed network. We first present a fast\ndistributed algorithm for computing approximate distance sketches, based on a\ndistributed implementation of the distance oracle scheme of [Thorup-Zwick, JACM\n2005]. We also show how to modify this basic construction to achieve different\ntradeoffs between the number of pairs for which the distance estimate is\naccurate and other parameters. These tradeoffs can then be combined to give an\nefficient construction of small sketches with provable average-case as well as\nworst-case performance. Our algorithms use only small-sized messages and hence\nare suitable for bandwidth-constrained networks, and can be used in various\nnetworking applications such as topology discovery and construction, token\nmanagement, load balancing, monitoring overlays, and several other problems in\ndistributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 10:01:28 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Sarma", "Atish Das", ""], ["Dinitz", "Michael", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1112.1229", "submitter": "Fabio Iannello", "authors": "Fabio Iannello, Osvaldo Simeone and Umberto Spagnolini", "title": "On the Optimal Scheduling of Independent, Symmetric and Time-Sensitive\n  Tasks", "comments": "Submitted for possible publication to IEEE Transactions on Automatic\n  Control, Jul. 2012", "journal-ref": null, "doi": "10.1109/TAC.2013.2258791", "report-no": null, "categories": "math.OC cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a discrete-time system in which a centralized controller (CC) is\ntasked with assigning at each time interval (or slot) K resources (or servers)\nto K out of M>=K nodes. When assigned a server, a node can execute a task. The\ntasks are independently generated at each node by stochastically symmetric and\nmemoryless random processes and stored in a finite-capacity task queue.\nMoreover, they are time-sensitive in the sense that within each slot there is a\nnon-zero probability that a task expires before being scheduled. The scheduling\nproblem is tackled with the aim of maximizing the number of tasks completed\nover time (or the task-throughput) under the assumption that the CC has no\ndirect access to the state of the task queues. The scheduling decisions at the\nCC are based on the outcomes of previous scheduling commands, and on the known\nstatistical properties of the task generation and expiration processes. Based\non a Markovian modeling of the task generation and expiration processes, the CC\nscheduling problem is formulated as a partially observable Markov decision\nprocess (POMDP) that can be cast into the framework of restless multi-armed\nbandit (RMAB) problems. When the task queues are of capacity one, the\noptimality of a myopic (or greedy) policy is proved. It is also demonstrated\nthat the MP coincides with the Whittle index policy. For task queues of\narbitrary capacity instead, the myopic policy is generally suboptimal, and its\nperformance is compared with an upper bound obtained through a relaxation of\nthe original problem. Overall, the settings in this paper provide a rare\nexample where a RMAB problem can be explicitly solved, and in which the Whittle\nindex policy is proved to be optimal.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 10:52:13 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2012 13:19:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Iannello", "Fabio", ""], ["Simeone", "Osvaldo", ""], ["Spagnolini", "Umberto", ""]]}, {"id": "1112.1313", "submitter": "Hong-Gwa Yeh", "authors": "Chun-Ying Chiang, Liang-Hao Huang, Wei-Ting Huang, Hong-Gwa Yeh", "title": "The Target Set Selection Problem on Cycle Permutation Graphs,\n  Generalized Petersen Graphs and Torus Cordalis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a fundamental problem in the area of viral\nmarketing, called T{\\scriptsize ARGET} S{\\scriptsize ET} S{\\scriptsize\nELECTION} problem.\n  In a a viral marketing setting, social networks are modeled by graphs with\npotential customers of a new product as vertices and friend relationships as\nedges, where each vertex $v$ is assigned a threshold value $\\theta(v)$. The\nthresholds represent the different latent tendencies of customers (vertices) to\nbuy the new product when their friend (neighbors) do.\n  Consider a repetitive process on social network $(G,\\theta)$ where each\nvertex $v$ is associated with two states, active and inactive, which indicate\nwhether $v$ is persuaded into buying the new product. Suppose we are given a\ntarget set $S\\subseteq V(G)$. Initially, all vertices in $G$ are inactive. At\ntime step 0, we choose all vertices in $S$ to become active.\n  Then, at every time step $t>0$, all vertices that were active in time step\n$t-1$ remain active, and we activate any vertex $v$ if at least $\\theta(v)$ of\nits neighbors were active at time step $t-1$. The activation process terminates\nwhen no more vertices can get activated. We are interested in the following\noptimization problem, called T{\\scriptsize ARGET} S{\\scriptsize ET}\nS{\\scriptsize ELECTION}: Finding a target set $S$ of smallest possible size\nthat activates all vertices of $G$. There is an important and well-studied\nthreshold called strict majority threshold, where for every vertex $v$ in $G$\nwe have $\\theta(v)=\\lceil{(d(v) +1)/2}\\rceil$ and $d(v)$ is the degree of $v$\nin $G$. In this paper, we consider the T{\\scriptsize ARGET} S{\\scriptsize ET}\nS{\\scriptsize ELECTION} problem under strict majority thresholds and focus on\nthree popular regular network structures: cycle permutation graphs, generalized\nPetersen graphs and torus cordalis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 15:25:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chiang", "Chun-Ying", ""], ["Huang", "Liang-Hao", ""], ["Huang", "Wei-Ting", ""], ["Yeh", "Hong-Gwa", ""]]}, {"id": "1112.1444", "submitter": "Xavier Allamigeon", "authors": "Xavier Allamigeon", "title": "On the complexity of strongly connected components in directed\n  hypergraphs", "comments": "v1: 32 pages, 7 figures; v2: revised version, 34 pages, 7 figures", "journal-ref": "Algorithmica, Volume 69, Issue 2, June 2014, Pages 335-369", "doi": "10.1007/s00453-012-9729-0", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of some algorithmic problems on directed hypergraphs\nand their strongly connected components (SCCs). The main contribution is an\nalmost linear time algorithm computing the terminal strongly connected\ncomponents (i.e. SCCs which do not reach any components but themselves).\n\"Almost linear\" here means that the complexity of the algorithm is linear in\nthe size of the hypergraph up to a factor alpha(n), where alpha is the inverse\nof Ackermann function, and n is the number of vertices. Our motivation to study\nthis problem arises from a recent application of directed hypergraphs to\ncomputational tropical geometry.\n  We also discuss the problem of computing all SCCs. We establish a superlinear\nlower bound on the size of the transitive reduction of the reachability\nrelation in directed hypergraphs, showing that it is combinatorially more\ncomplex than in directed graphs. Besides, we prove a linear time reduction from\nthe well-studied problem of finding all minimal sets among a given family to\nthe problem of computing the SCCs. Only subquadratic time algorithms are known\nfor the former problem. These results strongly suggest that the problem of\ncomputing the SCCs is harder in directed hypergraphs than in directed graphs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 23:26:05 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2013 17:31:15 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Allamigeon", "Xavier", ""]]}, {"id": "1112.1538", "submitter": "Micha{\\l} Pilipczuk", "authors": "Fedor V. Fomin and Micha{\\l} Pilipczuk", "title": "Jungles, bundles, and fixed parameter tractability", "comments": "The new version contains simplified proofs providing better running\n  times of the algorithms, as well as a wider discussion of the context", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fixed-parameter tractable (FPT) approximation algorithm computing\nthe path-width of a tournament, and more generally, of a semi-complete digraph.\nBased on this result, we prove that topological containment and rooted\nimmersion problems are FPT on semi-complete digraphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 12:09:30 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2012 11:43:30 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1112.1828", "submitter": "L\\'aszl\\'o Kozma", "authors": "Laszlo Kozma", "title": "Minimum Average Distance Triangulations", "comments": "ESA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a triangulation T of a planar point set S\nsuch as to minimize the expected distance between two points x and y chosen\nuniformly at random from S. By distance we mean the length of the shortest path\nbetween x and y along edges of T. The length of a path is the sum of the\nweights of its edges. Edge weights are assumed to be given as part of the\nproblem for every pair of distinct points (x,y) in S^2.\n  In a different variant of the problem, the points are vertices of a simple\npolygon and we look for a triangulation of the interior of the polygon that is\noptimal in the same sense.\n  We prove that a general formulation of the problem in which the weights are\narbitrary positive numbers is strongly NP-complete. For the case when all the\nweights are equal we give polynomial-time algorithms. In the end we mention\nseveral open problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 13:19:24 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2012 16:35:46 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2012 13:18:20 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Kozma", "Laszlo", ""]]}, {"id": "1112.1831", "submitter": "Grant Schoenebeck", "authors": "Sanjeev Arora and Rong Ge and Sushant Sachdeva and Grant Schoenebeck", "title": "Finding Overlapping Communities in Social Networks: Toward a Rigorous\n  Approach", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \"community\" in a social network is usually understood to be a group of\nnodes more densely connected with each other than with the rest of the network.\nThis is an important concept in most domains where networks arise: social,\ntechnological, biological, etc. For many years algorithms for finding\ncommunities implicitly assumed communities are nonoverlapping (leading to use\nof clustering-based approaches) but there is increasing interest in finding\noverlapping communities. A barrier to finding communities is that the solution\nconcept is often defined in terms of an NP-complete problem such as Clique or\nHierarchical Clustering.\n  This paper seeks to initiate a rigorous approach to the problem of finding\noverlapping communities, where \"rigorous\" means that we clearly state the\nfollowing: (a) the object sought by our algorithm (b) the assumptions about the\nunderlying network (c) the (worst-case) running time.\n  Our assumptions about the network lie between worst-case and average-case. An\naverage case analysis would require a precise probabilistic model of the\nnetwork, on which there is currently no consensus. However, some plausible\nassumptions about network parameters can be gleaned from a long body of work in\nthe sociology community spanning five decades focusing on the study of\nindividual communities and ego-centric networks. Thus our assumptions are\nsomewhat \"local\" in nature. Nevertheless they suffice to permit a rigorous\nanalysis of running time of algorithms that recover global structure.\n  Our algorithms use random sampling similar to that in property testing and\nalgorithms for dense graphs. However, our networks are not necessarily dense\ngraphs, not even in local neighborhoods.\n  Our algorithms explore a local-global relationship between ego-centric and\nsocio-centric networks that we hope will provide a fruitful framework for\nfuture work both in computer science and sociology.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 13:28:04 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Sachdeva", "Sushant", ""], ["Schoenebeck", "Grant", ""]]}, {"id": "1112.1945", "submitter": "Suman Kalyan Bera", "authors": "Suman Kalyan Bera (1), Shalmoli Gupta (2), Amit Kumar (2), Sambuddha\n  Roy (1) ((2) Indian Institute of Technology Delhi, (1) IBM Research - India\n  New Delhi)", "title": "Approximation Algorithms for Edge Partitioned Vertex Cover Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural generalization of the Partial Vertex Cover problem.\nHere an instance consists of a graph G = (V,E), a positive cost function c: V->\nZ^{+}, a partition $P_1,..., P_r$ of the edge set $E$, and a parameter $k_i$\nfor each partition $P_i$. The goal is to find a minimum cost set of vertices\nwhich cover at least $k_i$ edges from the partition $P_i$. We call this the\nPartition Vertex Cover problem. In this paper, we give matching upper and lower\nbound on the approximability of this problem. Our algorithm is based on a novel\nLP relaxation for this problem. This LP relaxation is obtained by adding\nknapsack cover inequalities to a natural LP relaxation of the problem. We show\nthat this LP has integrality gap of $O(log r)$, where $r$ is the number of sets\nin the partition of the edge set. We also extend our result to more general\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 20:56:07 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2012 17:03:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bera", "Suman Kalyan", ""], ["Gupta", "Shalmoli", ""], ["Kumar", "Amit", ""], ["Roy", "Sambuddha", ""]]}, {"id": "1112.1994", "submitter": "Elena Grigorescu", "authors": "Elena Grigorescu, Chris Peikert", "title": "List Decoding Barnes-Wall Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of list decoding error-correcting codes over finite fields\n(under the Hamming metric) has been widely studied in recent years. Motivated\nby the similar discrete structure of linear codes and point lattices in R^N,\nand their many shared applications across complexity theory, cryptography, and\ncoding theory, we initiate the study of list decoding for lattices. Namely: for\na lattice L in R^N, given a target vector r in R^N and a distance parameter d,\noutput the set of all lattice points w in L that are within distance d of r.\n  In this work we focus on combinatorial and algorithmic questions related to\nlist decoding for the well-studied family of Barnes-Wall lattices. Our main\ncontributions are twofold:\n  1) We give tight (up to polynomials) combinatorial bounds on the worst-case\nlist size, showing it to be polynomial in the lattice dimension for any error\nradius bounded away from the lattice's minimum distance (in the Euclidean\nnorm).\n  2) Building on the unique decoding algorithm of Micciancio and Nicolosi (ISIT\n'08), we give a list-decoding algorithm that runs in time polynomial in the\nlattice dimension and worst-case list size, for any error radius. Moreover, our\nalgorithm is highly parallelizable, and with sufficiently many processors can\nrun in parallel time only poly-logarithmic in the lattice dimension.\n  In particular, our results imply a polynomial-time list-decoding algorithm\nfor any error radius bounded away from the minimum distance, thus beating a\ntypical barrier for error-correcting codes posed by the Johnson radius.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 01:12:47 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2012 03:01:11 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Grigorescu", "Elena", ""], ["Peikert", "Chris", ""]]}, {"id": "1112.2012", "submitter": "Joshua Grochow", "authors": "Joshua A. Grochow", "title": "Lie algebra conjugacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.SC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of matrix Lie algebra conjugacy. Lie algebras arise\ncentrally in areas as diverse as differential equations, particle physics,\ngroup theory, and the Mulmuley--Sohoni Geometric Complexity Theory program. A\nmatrix Lie algebra is a set L of matrices such that $A, B\\in L$ implies $AB -\nBA \\in L$. Two matrix Lie algebras are conjugate if there is an invertible\nmatrix $M$ such that $L_1 = M L_2 M^{-1}$.\n  We show that certain cases of Lie algebra conjugacy are equivalent to graph\nisomorphism. On the other hand, we give polynomial-time algorithms for other\ncases of Lie algebra conjugacy, which allow us to essentially derandomize a\nrecent result of Kayal on affine equivalence of polynomials. Affine equivalence\nis related to many complexity problems such as factoring integers, graph\nisomorphism, matrix multiplication, and permanent versus determinant.\n  Specifically, we show:\n  Abelian Lie algebra conjugacy is equivalent to the code equivalence problem,\nand hence is as hard as graph isomorphism.\n  Abelian Lie algebra conjugacy of $n \\times n$ matrices can be solved in\npoly(n) time when the Lie algebras have dimension O(1).\n  Semisimple Lie algebra conjugacy is equivalent to graph isomorphism. A Lie\nalgebra is semisimple if it is a direct sum of simple Lie algebras.\n  Semisimple Lie algebra conjugacy of $n \\times n$ matrices can be solved in\npolynomial time when the Lie algebras consist of only $O(\\log n)$ simple direct\nsummands.\n  Conjugacy of completely reducible Lie algebras---that is, a direct sum of an\nabelian and a semisimple Lie algebra---can be solved in polynomial time when\nthe abelian part has dimension O(1) and the semisimple part has $O(\\log n)$\nsimple direct summands.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 03:24:19 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Grochow", "Joshua A.", ""]]}, {"id": "1112.2143", "submitter": "Andrea Schumm", "authors": "Robert G\\\"orke, Andrea Schumm, Dorothea Wagner", "title": "Experiments on Density-Constrained Graph Clustering", "comments": "Expanded version of a paper appearing in the proceedings of the\n  Meeting on Algorithm Engineering and Experiments, ALENEX 2012. 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering a graph means identifying internally dense subgraphs which are\nonly sparsely interconnected. Formalizations of this notion lead to measures\nthat quantify the quality of a clustering and to algorithms that actually find\nclusterings. Since, most generally, corresponding optimization problems are\nhard, heuristic clustering algorithms are used in practice, or other approaches\nwhich are not based on an objective function. In this work we conduct a\ncomprehensive experimental evaluation of the qualitative behavior of greedy\nbottom-up heuristics driven by cut-based objectives and constrained by\nintracluster density, using both real-world data and artificial instances. Our\nstudy documents that a greedy strategy based on local movement is superior to\none based on merging. We further reveal that the former approach generally\noutperforms alternative setups and reference algorithms from the literature in\nterms of its own objective, while a modularity-based algorithm competes\nsurprisingly well. Finally, we exhibit which combinations of cut-based inter-\nand intracluster measures are suitable for identifying a hidden reference\nclustering in synthetic random graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 16:43:59 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["G\u00f6rke", "Robert", ""], ["Schumm", "Andrea", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1112.2273", "submitter": "Marek Cygan", "authors": "Marek Cygan, Guy Kortsarz, Zeev Nutov", "title": "Steiner Forest Orientation Problems", "comments": "full version of ESA 2012 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider connectivity problems with orientation constraints. Given a\ndirected graph $D$ and a collection of ordered node pairs $P$ let $P[D]=\\{(u,v)\n\\in P: D {contains a} uv{-path}}$. In the {\\sf Steiner Forest Orientation}\nproblem we are given an undirected graph $G=(V,E)$ with edge-costs and a set $P\n\\subseteq V \\times V$ of ordered node pairs. The goal is to find a minimum-cost\nsubgraph $H$ of $G$ and an orientation $D$ of $H$ such that $P[D]=P$. We give a\n4-approximation algorithm for this problem.\n  In the {\\sf Maximum Pairs Orientation} problem we are given a graph $G$ and a\nmulti-collection of ordered node pairs $P$ on $V$. The goal is to find an\norientation $D$ of $G$ such that $|P[D]|$ is maximum. Generalizing the result\nof Arkin and Hassin [DAM'02] for $|P|=2$, we will show that for a mixed graph\n$G$ (that may have both directed and undirected edges), one can decide in\n$n^{O(|P|)}$ time whether $G$ has an orientation $D$ with $P[D]=P$ (for\nundirected graphs this problem admits a polynomial time algorithm for any $P$,\nbut it is NP-complete on mixed graphs). For undirected graphs, we will show\nthat one can decide whether $G$ admits an orientation $D$ with $|P[D]| \\geq k$\nin $O(n+m)+2^{O(k\\cdot \\log \\log k)}$ time; hence this decision problem is\nfixed-parameter tractable, which answers an open question from Dorn et al.\n[AMB'11]. We also show that {\\sf Maximum Pairs Orientation} admits ratio\n$O(\\log |P|/\\log\\log |P|)$, which is better than the ratio $O(\\log n/\\log\\log\nn)$ of Gamzu et al. [WABI'10] when $|P|<n$.\n  Finally, we show that the following node-connectivity problem can be solved\nin polynomial time: given a graph $G=(V,E)$ with edge-costs, $s,t \\in V$, and\nan integer $\\ell$, find a min-cost subgraph $H$ of $G$ with an orientation $D$\nsuch that $D$ contains $\\ell$ internally-disjoint $st$-paths, and $\\ell$\ninternally-disjoint $ts$-paths.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 12:24:13 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2012 09:40:16 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Cygan", "Marek", ""], ["Kortsarz", "Guy", ""], ["Nutov", "Zeev", ""]]}, {"id": "1112.2275", "submitter": "Saket Saurabh", "authors": "Marek Cygan and Holger Dell and Daniel Lokshtanov and Daniel Marx and\n  Jesper Nederlof and Yoshio Okamoto and Ramamohan Paturi and Saket Saurabh and\n  Magnus Wahlstrom", "title": "On Problems as Hard as CNFSAT", "comments": "25 pages, 1 figure", "journal-ref": "ACM Trans. Algorithms 12(3): 41:1-41:24 (2016)", "doi": "10.1145/2925416 10.1109/CCC.2012.36", "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of exact exponential time algorithms for NP-hard problems has\nthrived over the last decade. While exhaustive search remains asymptotically\nthe fastest known algorithm for some basic problems, difficult and non-trivial\nexponential time algorithms have been found for a myriad of problems, including\nGraph Coloring, Hamiltonian Path, Dominating Set and 3-CNF-Sat. In some\ninstances, improving these algorithms further seems to be out of reach. The\nCNF-Sat problem is the canonical example of a problem for which the trivial\nexhaustive search algorithm runs in time O(2^n), where n is the number of\nvariables in the input formula. While there exist non-trivial algorithms for\nCNF-Sat that run in time o(2^n), no algorithm was able to improve the growth\nrate 2 to a smaller constant, and hence it is natural to conjecture that 2 is\nthe optimal growth rate. The strong exponential time hypothesis (SETH) by\nImpagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that,\nfor every epsilon<1, there is a (large) integer k such that that k-CNF-Sat\ncannot be computed in time 2^{epsilon n}.\n  In this paper, we show that, for every epsilon < 1, the problems Hitting Set,\nSet Splitting, and NAE-Sat cannot be computed in time O(2^{epsilon n}) unless\nSETH fails. Here n is the number of elements or variables in the input. For\nthese problems, we actually get an equivalence to SETH in a certain sense. We\nconjecture that SETH implies a similar statement for Set Cover, and prove that,\nunder this assumption, the fastest known algorithms for Steinter Tree,\nConnected Vertex Cover, Set Partitioning, and the pseudo-polynomial time\nalgorithm for Subset Sum cannot be significantly improved. Finally, we justify\nour assumption about the hardness of Set Cover by showing that the parity of\nthe number of set covers cannot be computed in time O(2^{epsilon n}) for any\nepsilon<1 unless SETH fails.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 13:19:33 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 08:57:25 GMT"}, {"version": "v3", "created": "Thu, 27 Mar 2014 01:34:30 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Cygan", "Marek", ""], ["Dell", "Holger", ""], ["Lokshtanov", "Daniel", ""], ["Marx", "Daniel", ""], ["Nederlof", "Jesper", ""], ["Okamoto", "Yoshio", ""], ["Paturi", "Ramamohan", ""], ["Saurabh", "Saket", ""], ["Wahlstrom", "Magnus", ""]]}, {"id": "1112.2310", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern", "title": "Towards Optimal and Expressive Kernelization for d-Hitting Set", "comments": "This version gives corrected experimental results, adds additional\n  figures, and more formally defines \"expressive kernelization\"", "journal-ref": "Algorithmica 70(1):129-147, 2014", "doi": "10.1007/s00453-013-9774-3", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  d-Hitting Set is the NP-hard problem of selecting at most k vertices of a\nhypergraph so that each hyperedge, all of which have cardinality at most d,\ncontains at least one selected vertex. The applications of d-Hitting Set are,\nfor example, fault diagnosis, automatic program verification, and the\nnoise-minimizing assignment of frequencies to radio transmitters.\n  We show a linear-time algorithm that transforms an instance of d-Hitting Set\ninto an equivalent instance comprising at most O(k^d) hyperedges and vertices.\nIn terms of parameterized complexity, this is a problem kernel. Our\nkernelization algorithm is based on speeding up the well-known approach of\nfinding and shrinking sunflowers in hypergraphs, which yields problem kernels\nwith structural properties that we condense into the concept of expressive\nkernelization.\n  We conduct experiments to show that our kernelization algorithm can kernelize\ninstances with more than 10^7 hyperedges in less than five minutes.\n  Finally, we show that the number of vertices in the problem kernel can be\nfurther reduced to O(k^{d-1}) with additional O(k^{1.5 d}) processing time by\nnontrivially combining the sunflower technique with d-Hitting Set problem\nkernels due to Abu-Khzam and Moser.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 23:40:52 GMT"}, {"version": "v2", "created": "Fri, 11 May 2012 15:18:44 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2013 09:54:25 GMT"}, {"version": "v4", "created": "Tue, 15 Jul 2014 13:24:01 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["van Bevern", "Ren\u00e9", ""]]}, {"id": "1112.2930", "submitter": "Zachary Friggstad", "authors": "Zachary Friggstad", "title": "Multiple Traveling Salesmen in Asymmetric Metrics", "comments": "19 Pages, 3 Figures. First revision fixes a broken reference and adds\n  to the discussion for General 2-ATSPP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider some generalizations of the Asymmetric Traveling Salesman Path\nproblem. Suppose we have an asymmetric metric G = (V,A) with two distinguished\nnodes s,t. We are also given a positive integer k. The goal is to find k paths\nof minimum total cost from s to t whose union spans all nodes. We call this the\nk-Person Asymmetric Traveling Salesmen Path problem (k-ATSPP). Our main result\nfor k-ATSPP is a bicriteria approximation that, for some parameter b >= 1 we\nmay choose, finds between k and k + k/b paths of total length O(b log |V|)\ntimes the optimum value of an LP relaxation based on the Held-Karp relaxation\nfor the Traveling Salesman problem. On one extreme this is an O(log\n|V|)-approximation that uses up to 2k paths and on the other it is an O(k log\n|V|)-approximation that uses exactly k paths.\n  Next, we consider the case where we have k pairs of nodes (s_1,t_1), ...,\n(s_k,t_k). The goal is to find an s_i-t_i path for every pair such that each\nnode of G lies on at least one of these paths. Simple approximation algorithms\nare presented for the special cases where the metric is symmetric or where s_i\n= t_i for each i. We also show that the problem can be approximated within a\nfactor O(log n) when k=2. On the other hand, we demonstrate that the general\nproblem cannot be approximated within any bounded ratio unless P = NP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 15:59:58 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2011 14:35:31 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Friggstad", "Zachary", ""]]}, {"id": "1112.3244", "submitter": "Serge Gaspers", "authors": "Fedor V. Fomin, Serge Gaspers, Petr Golovach, Karol Suchan, Stefan\n  Szeider, Erik Jan van Leeuwen, Martin Vatshelle, Yngve Villanger", "title": "k-Gap Interval Graphs", "comments": "LATIN 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of a new parameterization of graph problems. In a\nmultiple interval representation of a graph, each vertex is associated to at\nleast one interval of the real line, with an edge between two vertices if and\nonly if an interval associated to one vertex has a nonempty intersection with\nan interval associated to the other vertex. A graph on n vertices is a k-gap\ninterval graph if it has a multiple interval representation with at most n+k\nintervals in total. In order to scale up the nice algorithmic properties of\ninterval graphs (where k=0), we parameterize graph problems by k, and find FPT\nalgorithms for several problems, including Feedback Vertex Set, Dominating Set,\nIndependent Set, Clique, Clique Cover, and Multiple Interval Transversal. The\nColoring problem turns out to be W[1]-hard and we design an XP algorithm for\nthe recognition problem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 15:06:52 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2011 14:08:29 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Gaspers", "Serge", ""], ["Golovach", "Petr", ""], ["Suchan", "Karol", ""], ["Szeider", "Stefan", ""], ["van Leeuwen", "Erik Jan", ""], ["Vatshelle", "Martin", ""], ["Villanger", "Yngve", ""]]}, {"id": "1112.3323", "submitter": "Toryn Klassen", "authors": "Toryn Qwyllyn Klassen and Philipp Woelfel", "title": "Independence of Tabulation-Based Hash Classes", "comments": "12 pages with 2 page appendix showing experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tabulation-based hash function maps a key into d derived characters\nindexing random values in tables that are then combined with bitwise xor\noperations to give the hash. Thorup and Zhang (2004) presented d-wise\nindependent tabulation-based hash classes that use linear maps over finite\nfields to map a key, considered as a vector (a,b), to derived characters. We\nshow that a variant where the derived characters are a+b*i for i=0,..., q-1\n(using integer arithmetic) yielding (2d-1)-wise independence. Our analysis is\nbased on an algebraic property that characterizes k-wise independence of\ntabulation-based hashing schemes, and combines this characterization with a\ngeometric argument. We also prove a non-trivial lower bound on the number of\nderived characters necessary for k-wise independence with our and related hash\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 20:17:50 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Klassen", "Toryn Qwyllyn", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1112.3337", "submitter": "Andris Ambainis", "authors": "Andris Ambainis, Arturs Backurs, Nikolajs Nahimovs, Raitis Ozols,\n  Alexander Rivosh", "title": "Search by quantum walks on two-dimensional grid without amplitude\n  amplification", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study search by quantum walk on a finite two dimensional grid. The\nalgorithm of Ambainis, Kempe, Rivosh (quant-ph/0402107) takes O(\\sqrt{N log N})\nsteps and finds a marked location with probability O(1/log N) for grid of size\n\\sqrt{N} * \\sqrt{N}. This probability is small, thus amplitude amplification is\nneeded to achieve \\Theta(1) success probability. The amplitude amplification\nadds an additional O(\\sqrt{log N}) factor to the number of steps, making it\nO(\\sqrt{N} log N).\n  In this paper, we show that despite a small probability to find a marked\nlocation, the probability to be within an O(\\sqrt{N}) neighbourhood (at an\nO(\\sqrt[4]{N}) distance) of the marked location is \\Theta(1). This allows to\nskip amplitude amplification step and leads to an O(\\sqrt{log N}) speed-up.\n  We describe the results of numerical experiments supporting this idea, and we\nprove this fact analytically.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 20:55:56 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Ambainis", "Andris", ""], ["Backurs", "Arturs", ""], ["Nahimovs", "Nikolajs", ""], ["Ozols", "Raitis", ""], ["Rivosh", "Alexander", ""]]}, {"id": "1112.3506", "submitter": "Mark Jones Mr", "authors": "Robert Crowston, Mark Jones, Matthias Mnich", "title": "Max-Cut Parameterized Above the Edwards-Erd\\H{o}s Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the boundary of tractability for the Max-Cut problem in graphs. Our\nmain result shows that Max-Cut above the Edwards-Erd\\H{o}s bound is\nfixed-parameter tractable: we give an algorithm that for any connected graph\nwith n vertices and m edges finds a cut of size m/2 + (n-1)/4 + k in time\n2^O(k)n^4, or decides that no such cut exists. This answers a long-standing\nopen question from parameterized complexity that has been posed several times\nover the past 15 years. Our algorithm is asymptotically optimal, under the\nExponential Time Hypothesis, and is strengthened by a polynomial-time\ncomputable kernel of polynomial size.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 13:38:50 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2012 09:17:40 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2013 16:02:31 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Crowston", "Robert", ""], ["Jones", "Mark", ""], ["Mnich", "Matthias", ""]]}, {"id": "1112.3611", "submitter": "Aravindan Vijayaraghavan", "authors": "Julia Chuzhoy, Yury Makarychev, Aravindan Vijayaraghavan and Yuan Zhou", "title": "Approximation Algorithms and Hardness of the k-Route Cut Problem", "comments": "To appear in the Symposium on Discrete Algorithms (SODA) 2012. 44\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the k-route cut problem: given an undirected edge-weighted graph\nG=(V,E), a collection {(s_1,t_1),(s_2,t_2),...,(s_r,t_r)} of source-sink pairs,\nand an integer connectivity requirement k, the goal is to find a minimum-weight\nsubset E' of edges to remove, such that the connectivity of every pair (s_i,\nt_i) falls below k. Specifically, in the edge-connectivity version, EC-kRC, the\nrequirement is that there are at most (k-1) edge-disjoint paths connecting s_i\nto t_i in G \\ E', while in the vertex-connectivity version, NC-kRC, the same\nrequirement is for vertex-disjoint paths. Prior to our work, poly-logarithmic\napproximation algorithms have been known for the special case where k >= 3, but\nno non-trivial approximation algorithms were known for any value k>3, except in\nthe single-source setting. We show an O(k log^{3/2}r)-approximation algorithm\nfor EC-kRC with uniform edge weights, and several polylogarithmic bi-criteria\napproximation algorithms for EC-kRC and NC-kRC, where the connectivity\nrequirement k is violated by a constant factor. We complement these upper\nbounds by proving that NC-kRC is hard to approximate to within a factor of\nk^{eps} for some fixed eps>0.\n  We then turn to study a simpler version of NC-kRC, where only one source-sink\npair is present. We give a simple bi-criteria approximation algorithm for this\ncase, and show evidence that even this restricted version of the problem may be\nhard to approximate. For example, we prove that the single source-sink pair\nversion of NC-kRC has no constant-factor approximation, assuming Feige's Random\nk-AND assumption.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 19:11:56 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2011 02:24:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""], ["Zhou", "Yuan", ""]]}, {"id": "1112.3692", "submitter": "Mark Huber", "authors": "Mark Huber and Sarah Schott", "title": "Random construction of interpolating sets for high dimensional\n  integration", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional integrals can be reduced to the problem of finding the\nrelative measures of two sets. Often one set will be exponentially larger than\nthe other, making it difficult to compare the sizes. A standard method of\ndealing with this problem is to interpolate between the sets with a sequence of\nnested sets where neighboring sets have relative measures bounded above by a\nconstant. Choosing such a well balanced sequence can be very difficult in\npractice. Here a new approach that automatically creates such sets is\npresented. These well balanced sets allow for faster approximation algorithms\nfor integrals and sums, and better tempering and annealing Markov chains for\ngenerating random samples. Applications such as finding the partition function\nof the Ising model and normalizing constants for posterior distributions in\nBayesian methods are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 00:32:45 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Huber", "Mark", ""], ["Schott", "Sarah", ""]]}, {"id": "1112.3925", "submitter": "Emil Je\\v{r}\\'abek", "authors": "Emil Je\\v{r}\\'abek", "title": "Root finding with threshold circuits", "comments": "19 pages, 1 figure", "journal-ref": "Theoretical Computer Science 462 (2012), pp. 59--69", "doi": "10.1016/j.tcs.2012.09.001", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for any constant d, complex roots of degree d univariate\nrational (or Gaussian rational) polynomials---given by a list of coefficients\nin binary---can be computed to a given accuracy by a uniform TC^0 algorithm (a\nuniform family of constant-depth polynomial-size threshold circuits). The basic\nidea is to compute the inverse function of the polynomial by a power series. We\nalso discuss an application to the theory VTC^0 of bounded arithmetic.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 19:20:03 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 14:56:21 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Je\u0159\u00e1bek", "Emil", ""]]}, {"id": "1112.4105", "submitter": "Jeff M Phillips", "authors": "Jeff M. Phillips", "title": "epsilon-Samples of Kernels", "comments": "13 pages, 2 figures. Cleaned up writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the worst case error of kernel density estimates via subset\napproximation. A kernel density estimate of a distribution is the convolution\nof that distribution with a fixed kernel (e.g. Gaussian kernel). Given a subset\n(i.e. a point set) of the input distribution, we can compare the kernel density\nestimates of the input distribution with that of the subset and bound the worst\ncase error. If the maximum error is eps, then this subset can be thought of as\nan eps-sample (aka an eps-approximation) of the range space defined with the\ninput distribution as the ground set and the fixed kernel representing the\nfamily of ranges. Interestingly, in this case the ranges are not binary, but\nhave a continuous range (for simplicity we focus on kernels with range of\n[0,1]); these allow for smoother notions of range spaces.\n  It turns out, the use of this smoother family of range spaces has an added\nbenefit of greatly decreasing the size required for eps-samples. For instance,\nin the plane the size is O((1/eps^{4/3}) log^{2/3}(1/eps)) for disks (based on\nVC-dimension arguments) but is only O((1/eps) sqrt{log (1/eps)}) for Gaussian\nkernels and for kernels with bounded slope that only affect a bounded domain.\nThese bounds are accomplished by studying the discrepancy of these \"kernel\"\nrange spaces, and here the improvement in bounds are even more pronounced. In\nthe plane, we show the discrepancy is O(sqrt{log n}) for these kernels, whereas\nfor balls there is a lower bound of Omega(n^{1/4}).\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 01:19:25 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2012 05:35:25 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2012 22:46:53 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Phillips", "Jeff M.", ""]]}, {"id": "1112.4109", "submitter": "Ali Sinop", "authors": "Venkatesan Guruswami and Ali Kemal Sinop", "title": "Approximating Non-Uniform Sparsest Cut via Generalized Spectra", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an approximation algorithm for non-uniform sparsest cut with the\nfollowing guarantee: For any $\\epsilon,\\delta \\in (0,1)$, given cost and demand\ngraphs with edge weights $C, D$ respectively, we can find a set $T\\subseteq V$\nwith $\\frac{C(T,V\\setminus T)}{D(T,V\\setminus T)}$ at most\n$\\frac{1+\\epsilon}{\\delta}$ times the optimal non-uniform sparsest cut value,\nin time $2^{r/(\\delta\\epsilon)}\\poly(n)$ provided $\\lambda_r \\ge\n\\Phi^*/(1-\\delta)$. Here $\\lambda_r$ is the $r$'th smallest generalized\neigenvalue of the Laplacian matrices of cost and demand graphs; $C(T,V\\setminus\nT)$ (resp. $D(T,V\\setminus T)$) is the weight of edges crossing the\n$(T,V\\setminus T)$ cut in cost (resp. demand) graph and $\\Phi^*$ is the\nsparsity of the optimal cut. In words, we show that the non-uniform sparsest\ncut problem is easy when the generalized spectrum grows moderately fast. To the\nbest of our knowledge, there were no results based on higher order spectra for\nnon-uniform sparsest cut prior to this work.\n  Even for uniform sparsest cut, the quantitative aspects of our result are\nsomewhat stronger than previous methods. Similar results hold for other\nexpansion measures like edge expansion, normalized cut, and conductance, with\nthe $r$'th smallest eigenvalue of the normalized Laplacian playing the role of\n$\\lambda_r$ in the latter two cases.\n  Our proof is based on an l1-embedding of vectors from a semi-definite program\nfrom the Lasserre hierarchy. The embedded vectors are then rounded to a cut\nusing standard threshold rounding. We hope that the ideas connecting\n$\\ell_1$-embeddings to Lasserre SDPs will find other applications. Another\naspect of the analysis is the adaptation of the column selection paradigm from\nour earlier work on rounding Lasserre SDPs [GS11] to pick a set of edges rather\nthan vertices. This feature is important in order to extend the algorithms to\nnon-uniform sparsest cut.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 02:41:12 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 21:21:32 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2012 05:09:22 GMT"}, {"version": "v4", "created": "Mon, 17 Dec 2012 22:33:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Sinop", "Ali Kemal", ""]]}, {"id": "1112.4131", "submitter": "Frederic Paccaut", "authors": "Peggy C\\'enac (IMB), Brigitte Chauvin (LM-Versailles), Fr\\'ed\\'eric\n  Paccaut (LAMFA), Nicolas Pouyanne (LM-Versailles)", "title": "Uncommon Suffix Tries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common assumptions on the source producing the words inserted in a suffix\ntrie with $n$ leaves lead to a $\\log n$ height and saturation level. We provide\nan example of a suffix trie whose height increases faster than a power of $n$\nand another one whose saturation level is negligible with respect to $\\log n$.\nBoth are built from VLMC (Variable Length Markov Chain) probabilistic sources;\nthey are easily extended to families of sources having the same properties. The\nfirst example corresponds to a \"logarithmic infinite comb\" and enjoys a non\nuniform polynomial mixing. The second one corresponds to a \"factorial infinite\ncomb\" for which mixing is uniform and exponential.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 08:01:08 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2011 10:20:37 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["C\u00e9nac", "Peggy", "", "IMB"], ["Chauvin", "Brigitte", "", "LM-Versailles"], ["Paccaut", "Fr\u00e9d\u00e9ric", "", "LAMFA"], ["Pouyanne", "Nicolas", "", "LM-Versailles"]]}, {"id": "1112.4419", "submitter": "Micha{\\l} Pilipczuk", "authors": "Fedor V. Fomin and Stefan Kratsch and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk and Yngve Villanger", "title": "Subexponential fixed-parameter tractability of cluster editing", "comments": "The new version contains results accepted for publication on the 30th\n  Symposium on Theoretical Aspects of Computer Science (STACS 2013) under title\n  'Tight bounds for Parameterized Complexity of Cluster Editing'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Correlation Clustering, also known as Cluster Editing, we are given an\nundirected n-vertex graph G and a positive integer k. The task is to decide if\nG can be transformed into a cluster graph, i.e., a disjoint union of cliques,\nby changing at most k adjacencies, i.e. by adding/deleting at most k edges. We\ngive a subexponential algorithm that, in time 2^O(sqrt(pk)) + n^O(1) decides\nwhether G can be transformed into a cluster graph with p cliques by changing at\nmost k adjacencies. We complement our algorithmic findings by the following\ntight lower bounds on the asymptotic behaviour of our algorithm. We show that,\nunless ETH fails, for any constant 0 < s <= 1, there is p = Theta(k^s) such\nthat there is no algorithm deciding in time 2^o(sqrt(pk)) n^O(1) whether G can\nbe transformed into a cluster graph with p cliques by changing at most k\nadjacencies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 17:43:36 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2011 10:31:34 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2012 12:55:42 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2013 17:54:14 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Kratsch", "Stefan", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Villanger", "Yngve", ""]]}, {"id": "1112.4438", "submitter": "Stefano Lonardi", "authors": "Stefano Lonardi, Denisa Duma, Matthew Alpert, Francesca Cordero, Marco\n  Beccuti, Prasanna R. Bhat, Yonghui Wu, Gianfranco Ciardo, Burair Alsaihati,\n  Yaqin Ma, Steve Wanamaker, Josh Resnik, Timothy J. Close", "title": "Barcoding-free BAC Pooling Enables Combinatorial Selective Sequencing of\n  the Barley Gene Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sequencing protocol that combines recent advances in\ncombinatorial pooling design and second-generation sequencing technology to\nefficiently approach de novo selective genome sequencing. We show that\ncombinatorial pooling is a cost-effective and practical alternative to\nexhaustive DNA barcoding when dealing with hundreds or thousands of DNA\nsamples, such as genome-tiling gene-rich BAC clones. The novelty of the\nprotocol hinges on the computational ability to efficiently compare hundreds of\nmillion of short reads and assign them to the correct BAC clones so that the\nassembly can be carried out clone-by-clone. Experimental results on simulated\ndata for the rice genome show that the deconvolution is extremely accurate\n(99.57% of the deconvoluted reads are assigned to the correct BAC), and the\nresulting BAC assemblies have very high quality (BACs are covered by contigs\nover about 77% of their length, on average). Experimental results on real data\nfor a gene-rich subset of the barley genome confirm that the deconvolution is\naccurate (almost 70% of left/right pairs in paired-end reads are assigned to\nthe same BAC, despite being processed independently) and the BAC assemblies\nhave good quality (the average sum of all assembled contigs is about 88% of the\nestimated BAC length).\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 19:06:57 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Lonardi", "Stefano", ""], ["Duma", "Denisa", ""], ["Alpert", "Matthew", ""], ["Cordero", "Francesca", ""], ["Beccuti", "Marco", ""], ["Bhat", "Prasanna R.", ""], ["Wu", "Yonghui", ""], ["Ciardo", "Gianfranco", ""], ["Alsaihati", "Burair", ""], ["Ma", "Yaqin", ""], ["Wanamaker", "Steve", ""], ["Resnik", "Josh", ""], ["Close", "Timothy J.", ""]]}, {"id": "1112.4523", "submitter": "Bjarke Hammersholt Roune", "authors": "Bjarke Hammersholt Roune, Eduardo S\\'aenz de Cabez\\'on", "title": "Complexity and Algorithms for Euler Characteristic of Simplicial\n  Complexes", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS cs.MS cs.SC math.AC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the Euler characteristic of an abstract\nsimplicial complex given by its vertices and facets. We show that this problem\nis #P-complete and present two new practical algorithms for computing Euler\ncharacteristic. The two new algorithms are derived using combinatorial\ncommutative algebra and we also give a second description of them that requires\nno algebra. We present experiments showing that the two new algorithms can be\nimplemented to be faster than previous Euler characteristic implementations by\na large margin.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 22:51:21 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["Roune", "Bjarke Hammersholt", ""], ["de Cabez\u00f3n", "Eduardo S\u00e1enz", ""]]}, {"id": "1112.4578", "submitter": "Sebastian Kreft", "authors": "Sebastian Kreft and Gonzalo Navarro (advisor)", "title": "Self-Index based on LZ77 (thesis)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domains like bioinformatics, version control systems, collaborative editing\nsystems (wiki), and others, are producing huge data collections that are very\nrepetitive. That is, there are few differences between the elements of the\ncollection. This fact makes the compressibility of the collection extremely\nhigh. For example, a collection with all different versions of a Wikipedia\narticle can be compressed up to the 0.1% of its original space, using the\nLempel-Ziv 1977 (LZ77) compression scheme.\n  Many of these repetitive collections handle huge amounts of text data. For\nthat reason, we require a method to store them efficiently, while providing the\nability to operate on them. The most common operations are the extraction of\nrandom portions of the collection and the search for all the occurrences of a\ngiven pattern inside the whole collection.\n  A self-index is a data structure that stores a text in compressed form and\nallows to find the occurrences of a pattern efficiently. On the other hand,\nself-indexes can extract any substring of the collection, hence they are able\nto replace the original text. One of the main goals when using these indexes is\nto store them within main memory.\n  In this thesis we present a scheme for random text extraction from text\ncompressed with a Lempel-Ziv parsing. Additionally, we present a variant of\nLZ77, called LZ-End, that efficiently extracts text using space close to that\nof LZ77.\n  The main contribution of this thesis is the first self-index based on\nLZ77/LZ-End and oriented to repetitive texts, which outperforms the state of\nthe art (the RLCSA self-index) in many aspects. Finally, we present a corpus of\nrepetitive texts, coming from several application domains. We aim at providing\na standard set of texts for research and experimentation, hence this corpus is\npublicly available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 06:09:35 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["Kreft", "Sebastian", "", "advisor"], ["Navarro", "Gonzalo", "", "advisor"]]}, {"id": "1112.4671", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Finding D-optimal designs by randomised decomposition and switching", "comments": "18 pages, 3 figures, 5 tables (figures corrected in v4). v5 added a\n  reference and made minor improvements. Presented at the International\n  Workshop on Hadamard Matrices held in honour of Kathy Horadam's 60th\n  birthday, Melbourne, Nov. 2011. Data files are available at\n  http://maths.anu.edu.au/~brent/maxdet/", "journal-ref": "Australasian Journal of Combinatorics 55 (2013), 15-30. Erratum\n  http://maths-people.anu.edu.au/~brent/pub/pub245_errata.html", "doi": null, "report-no": null, "categories": "math.CO cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hadamard maximal determinant (maxdet) problem is to find the maximum\ndeterminant D(n) of a square {+1, -1} matrix of given order n. Such a matrix\nwith maximum determinant is called a saturated D-optimal design. We consider\nsome cases where n > 2 is not divisible by 4, so the Hadamard bound is not\nattainable, but bounds due to Barba or Ehlich and Wojtas may be attainable. If\nR is a matrix with maximal (or conjectured maximal) determinant, then G = RR^T\nis the corresponding Gram matrix. For the cases that we consider, maximal or\nconjectured maximal Gram matrices are known. We show how to generate many\nHadamard equivalence classes of solutions from a given Gram matrix G, using a\nrandomised decomposition algorithm and row/column switching. In particular, we\nconsider orders 26, 27 and 33, and obtain new saturated D-optimal designs (for\norder 26) and new conjectured saturated D-optimal designs (for orders 27 and\n33).\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 12:41:50 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2011 01:27:17 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2012 05:00:40 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2012 13:59:04 GMT"}, {"version": "v5", "created": "Sat, 11 Aug 2012 06:37:39 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1112.5071", "submitter": "Philippe Duchon", "authors": "Philippe Duchon (INRIA Bordeaux - Sud-Ouest, LaBRI)", "title": "Random generation of combinatorial structures: Boltzmann samplers and\n  beyond", "comments": "Winter Simulation Conference (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Boltzmann model for the random generation of \"decomposable\" combinatorial\nstructures is a set of techniques that allows for efficient random sampling\nalgorithms for a large class of families of discrete objects. The usual\nrequirement of sampling uniformly from the set of objects of a given size is\nsomehow relaxed, though uniformity among objects of each size is still ensured.\nGenerating functions, rather than the enumeration sequences they are based on,\nare the crucial ingredient. We give a brief description of the general theory,\nas well as a number of newer developments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 16:00:22 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Duchon", "Philippe", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"]]}, {"id": "1112.5153", "submitter": "David Woodruff", "authors": "David P. Woodruff and Qin Zhang", "title": "Tight Bounds for Distributed Functional Monitoring", "comments": "Added a formal embedding argument in Section 3.2.2. This embedding\n  argument required some other changes in Section 3, causing us to relax the\n  definition of k-GAP-MAJ to k-APPROX-SUM, which is a similar problem. We still\n  use the original k-GAP_MAJ in Section 6.1. Section 4 also now has missing\n  details regarding the predictor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve several fundamental questions in the area of distributed\nfunctional monitoring, initiated by Cormode, Muthukrishnan, and Yi (SODA,\n2008). In this model there are $k$ sites each tracking their input and\ncommunicating with a central coordinator that continuously maintain an\napproximate output to a function $f$ computed over the union of the inputs. The\ngoal is to minimize the communication.\n  We show the randomized communication complexity of estimating the number of\ndistinct elements up to a $1+\\eps$ factor is $\\tilde{\\Omega}(k/\\eps^2)$,\nimproving the previous $\\Omega(k + 1/\\eps^2)$ bound and matching known upper\nbounds up to a logarithmic factor. For the $p$-th frequency moment $F_p$, $p >\n1$, we improve the previous $\\Omega(k + 1/\\eps^2)$ communication bound to\n$\\tilde{\\Omega}(k^{p-1}/\\eps^2)$. We obtain similar improvements for heavy\nhitters, empirical entropy, and other problems. We also show that we can\nestimate $F_p$, for any $p > 1$, using $\\tilde{O}(k^{p-1}\\poly(\\eps^{-1}))$\ncommunication. This greatly improves upon the previous\n$\\tilde{O}(k^{2p+1}N^{1-2/p} \\poly(\\eps^{-1}))$ bound of Cormode,\nMuthukrishnan, and Yi for general $p$, and their $\\tilde{O}(k^2/\\eps +\nk^{1.5}/\\eps^3)$ bound for $p = 2$. For $p = 2$, our bound resolves their main\nopen question.\n  Our lower bounds are based on new direct sum theorems for approximate\nmajority, and yield significant improvements to problems in the data stream\nmodel, improving the bound for estimating $F_p, p > 2,$ in $t$ passes from\n$\\tilde{\\Omega}(n^{1-2/p}/(\\eps^{2/p} t))$ to\n$\\tilde{\\Omega}(n^{1-2/p}/(\\eps^{4/p} t))$, giving the first bound for\nestimating $F_0$ in $t$ passes of $\\Omega(1/(\\eps^2 t))$ bits of space that\ndoes not use the gap-hamming problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 20:56:38 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2012 23:49:13 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2013 05:24:18 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1112.5215", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou and Dacheng Tao", "title": "Bilateral Random Projections", "comments": "17 pages, 3 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Low-rank structure have been profoundly studied in data mining and machine\nlearning. In this paper, we show a dense matrix $X$'s low-rank approximation\ncan be rapidly built from its left and right random projections $Y_1=XA_1$ and\n$Y_2=X^TA_2$, or bilateral random projection (BRP). We then show power scheme\ncan further improve the precision. The deterministic, average and deviation\nbounds of the proposed method and its power scheme modification are proved\ntheoretically. The effectiveness and the efficiency of BRP based low-rank\napproximation is empirically verified on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 01:16:20 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1112.5359", "submitter": "Steven Kelk", "authors": "Steven Kelk, Leo van Iersel, Nela Lekic, Simone Linz, Celine\n  Scornavacca, Leen Stougie", "title": "Cycle killer... qu'est-ce que c'est? On the comparative approximability\n  of hybridization number and directed feedback vertex set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of computing the hybridization number of two rooted\nbinary phylogenetic trees on the same set of taxa X has a constant factor\npolynomial-time approximation if and only if the problem of computing a\nminimum-size feedback vertex set in a directed graph (DFVS) has a constant\nfactor polynomial-time approximation. The latter problem, which asks for a\nminimum number of vertices to be removed from a directed graph to transform it\ninto a directed acyclic graph, is one of the problems in Karp's seminal 1972\nlist of 21 NP-complete problems. However, despite considerable attention from\nthe combinatorial optimization community it remains to this day unknown whether\na constant factor polynomial-time approximation exists for DFVS. Our result\nthus places the (in)approximability of hybridization number in a much broader\ncomplexity context, and as a consequence we obtain that hybridization number\ninherits inapproximability results from the problem Vertex Cover. On the\npositive side, we use results from the DFVS literature to give an O(log r log\nlog r) approximation for hybridization number, where r is the value of an\noptimal solution to the hybridization number problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 15:58:01 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Kelk", "Steven", ""], ["van Iersel", "Leo", ""], ["Lekic", "Nela", ""], ["Linz", "Simone", ""], ["Scornavacca", "Celine", ""], ["Stougie", "Leen", ""]]}, {"id": "1112.5396", "submitter": "Vahid Liaghat", "authors": "Saeed Alaei, Mohammad T. Hajiaghayi, Vahid Liaghat, Dan Pei, Barna\n  Saha", "title": "AdCell: Ad Allocation in Cellular Networks", "comments": null, "journal-ref": "ESA 2011: 311-322", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more than four billion usage of cellular phones worldwide, mobile\nadvertising has become an attractive alternative to online advertisements. In\nthis paper, we propose a new targeted advertising policy for Wireless Service\nProviders (WSPs) via SMS or MMS- namely {\\em AdCell}. In our model, a WSP\ncharges the advertisers for showing their ads. Each advertiser has a valuation\nfor specific types of customers in various times and locations and has a limit\non the maximum available budget. Each query is in the form of time and location\nand is associated with one individual customer. In order to achieve a\nnon-intrusive delivery, only a limited number of ads can be sent to each\ncustomer. Recently, new services have been introduced that offer location-based\nadvertising over cellular network that fit in our model (e.g., ShopAlerts by\nAT&T) .\n  We consider both online and offline version of the AdCell problem and develop\napproximation algorithms with constant competitive ratio. For the online\nversion, we assume that the appearances of the queries follow a stochastic\ndistribution and thus consider a Bayesian setting. Furthermore, queries may\ncome from different distributions on different times. This model generalizes\nseveral previous advertising models such as online secretary problem\n\\cite{HKP04}, online bipartite matching \\cite{KVV90,FMMM09} and AdWords\n\\cite{saberi05}. ...\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 17:44:55 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2012 02:51:38 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Alaei", "Saeed", ""], ["Hajiaghayi", "Mohammad T.", ""], ["Liaghat", "Vahid", ""], ["Pei", "Dan", ""], ["Saha", "Barna", ""]]}, {"id": "1112.5472", "submitter": "Casper Kejlberg-Rasmussen", "authors": "Gerth St{\\o}lting Brodal and Casper Kejlberg-Rasmussen", "title": "Cache-Oblivious Implicit Predecessor Dictionaries with the Working Set\n  Property", "comments": "An extended abstract is accepted at STACS 2012, this is the full\n  version of that paper with the same name \"Cache-Oblivious Implicit\n  Predecessor Dictionaries with the Working-Set Property\", Symposium on\n  Theoretical Aspects of Computer Science 2012", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2012.112", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we present an implicit dynamic dictionary with the working-set\nproperty, supporting insert(e) and delete(e) in O(log n) time, predecessor(e)\nin O(log l_{p(e)}) time, successor(e) in O(log l_{s(e)}) time and search(e) in\nO(log min(l_{p(e)},l_{e}, l_{s(e)})) time, where n is the number of elements\nstored in the dictionary, l_{e} is the number of distinct elements searched for\nsince element e was last searched for and p(e) and s(e) are the predecessor and\nsuccessor of e, respectively. The time-bounds are all worst-case. The\ndictionary stores the elements in an array of size n using no additional space.\nIn the cache-oblivious model the log is base B and the cache-obliviousness is\ndue to our black box use of an existing cache-oblivious implicit dictionary.\nThis is the first implicit dictionary supporting predecessor and successor\nsearches in the working-set bound. Previous implicit structures required O(log\nn) time.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 21:45:16 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2012 15:43:49 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2012 10:59:40 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Brodal", "Gerth St\u00f8lting", ""], ["Kejlberg-Rasmussen", "Casper", ""]]}, {"id": "1112.5507", "submitter": "Joshua Vogelstein", "authors": "Joshua T. Vogelstein, John M. Conroy, Vince Lyzinski, Louis J.\n  Podrazik, Steven G. Kratzer, Eric T. Harley, Donniell E. Fishkind, R. Jacob\n  Vogelstein, Carey E. Priebe", "title": "Fast Approximate Quadratic Programming for Large (Brain) Graph Matching", "comments": "17 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic assignment problems (QAPs) arise in a wide variety of domains,\nranging from operations research to graph theory to computer vision to\nneuroscience. In the age of big data, graph valued data is becoming more\nprominent, and with it, a desire to run algorithms on ever larger graphs.\nBecause QAP is NP-hard, exact algorithms are intractable. Approximate\nalgorithms necessarily employ an accuracy/efficiency trade-off. We developed a\nfast approximate quadratic assignment algorithm (FAQ). FAQ finds a local optima\nin (worst case) time cubic in the number of vertices, similar to other\napproximate QAP algorithms. We demonstrate empirically that our algorithm is\nfaster and achieves a lower objective value on over 80% of the suite of QAP\nbenchmarks, compared with the previous state-of-the-art. Applying the\nalgorithms to our motivating example, matching C. elegans connectomes\n(brain-graphs), we find that FAQ achieves the optimal performance in record\ntime, whereas none of the others even find the optimum.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 02:56:26 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2012 03:44:51 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 04:56:59 GMT"}, {"version": "v4", "created": "Fri, 10 May 2013 01:58:09 GMT"}, {"version": "v5", "created": "Sat, 13 Sep 2014 15:08:28 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Vogelstein", "Joshua T.", ""], ["Conroy", "John M.", ""], ["Lyzinski", "Vince", ""], ["Podrazik", "Louis J.", ""], ["Kratzer", "Steven G.", ""], ["Harley", "Eric T.", ""], ["Fishkind", "Donniell E.", ""], ["Vogelstein", "R. Jacob", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1112.5636", "submitter": "Jan Bul\\'anek", "authors": "Jan Bul\\'anek, Michal Kouck\\'y, Michael Saks", "title": "Tight lower bounds for online labeling problem", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the file maintenance problem (also called the online labeling\nproblem) in which n integer items from the set {1,...,r} are to be stored in an\narray of size m >= n. The items are presented sequentially in an arbitrary\norder, and must be stored in the array in sorted order (but not necessarily in\nconsecutive locations in the array). Each new item must be stored in the array\nbefore the next item is received. If r<=m then we can simply store item j in\nlocation j but if r>m then we may have to shift the location of stored items to\nmake space for a newly arrived item. The algorithm is charged each time an item\nis stored in the array, or moved to a new location. The goal is to minimize the\ntotal number of such moves done by the algorithm. This problem is non-trivial\nwhen n=<m<r.\n  In the case that m=Cn for some C>1, algorithms for this problem with cost\nO(log(n)^2) per item have been given [IKR81, Wil92, BCD+02]. When m=n,\nalgorithms with cost O(log(n)^3) per item were given [Zha93, BS07]. In this\npaper we prove lower bounds that show that these algorithms are optimal, up to\nconstant factors. Previously, the only lower bound known for this range of\nparameters was a lower bound of \\Omega(log(n)^2) for the restricted class of\nsmooth algorithms [DSZ05a, Zha93].\n  We also provide an algorithm for the sparse case: If the number of items is\npolylogarithmic in the array size then the problem can be solved in amortized\nconstant time per item.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 19:02:01 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bul\u00e1nek", "Jan", ""], ["Kouck\u00fd", "Michal", ""], ["Saks", "Michael", ""]]}, {"id": "1112.5659", "submitter": "Ilias Diakonikolas", "authors": "Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio,\n  Gregory Valiant, Paul Valiant", "title": "Testing $k$-Modal Distributions: Optimal Algorithms via Reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give highly efficient algorithms, and almost matching lower bounds, for a\nrange of basic statistical problems that involve testing and estimating the L_1\ndistance between two k-modal distributions $p$ and $q$ over the discrete domain\n$\\{1,\\dots,n\\}$. More precisely, we consider the following four problems: given\nsample access to an unknown k-modal distribution $p$,\n  Testing identity to a known or unknown distribution:\n  1. Determine whether $p = q$ (for an explicitly given k-modal distribution\n$q$) versus $p$ is $\\eps$-far from $q$;\n  2. Determine whether $p=q$ (where $q$ is available via sample access) versus\n$p$ is $\\eps$-far from $q$;\n  Estimating $L_1$ distance (\"tolerant testing'') against a known or unknown\ndistribution:\n  3. Approximate $d_{TV}(p,q)$ to within additive $\\eps$ where $q$ is an\nexplicitly given k-modal distribution $q$;\n  4. Approximate $d_{TV}(p,q)$ to within additive $\\eps$ where $q$ is available\nvia sample access.\n  For each of these four problems we give sub-logarithmic sample algorithms,\nthat we show are tight up to additive $\\poly(k)$ and multiplicative\n$\\polylog\\log n+\\polylog k$ factors. Thus our bounds significantly improve the\nprevious results of \\cite{BKR:04}, which were for testing identity of\ndistributions (items (1) and (2) above) in the special cases k=0 (monotone\ndistributions) and k=1 (unimodal distributions) and required $O((\\log n)^3)$\nsamples.\n  As our main conceptual contribution, we introduce a new reduction-based\napproach for distribution-testing problems that lets us obtain all the above\nresults in a unified way. Roughly speaking, this approach enables us to\ntransform various distribution testing problems for k-modal distributions over\n$\\{1,\\dots,n\\}$ to the corresponding distribution testing problems for\nunrestricted distributions over a much smaller domain $\\{1,\\dots,\\ell\\}$ where\n$\\ell = O(k \\log n).$\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 20:50:05 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "1112.5741", "submitter": "Amit Weinstein", "authors": "Eric Blais, Amit Weinstein and Yuichi Yoshida", "title": "Partially Symmetric Functions are Efficiently Isomorphism-Testable", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a function f: {0,1}^n \\to {0,1}, the f-isomorphism testing problem\nrequires a randomized algorithm to distinguish functions that are identical to\nf up to relabeling of the input variables from functions that are far from\nbeing so. An important open question in property testing is to determine for\nwhich functions f we can test f-isomorphism with a constant number of queries.\nDespite much recent attention to this question, essentially only two classes of\nfunctions were known to be efficiently isomorphism testable: symmetric\nfunctions and juntas.\n  We unify and extend these results by showing that all partially symmetric\nfunctions---functions invariant to the reordering of all but a constant number\nof their variables---are efficiently isomorphism-testable. This class of\nfunctions, first introduced by Shannon, includes symmetric functions, juntas,\nand many other functions as well. We conjecture that these functions are\nessentially the only functions efficiently isomorphism-testable.\n  To prove our main result, we also show that partial symmetry is efficiently\ntestable. In turn, to prove this result we had to revisit the junta testing\nproblem. We provide a new proof of correctness of the nearly-optimal junta\ntester. Our new proof replaces the Fourier machinery of the original proof with\na purely combinatorial argument that exploits the connection between sets of\nvariables with low influence and intersecting families.\n  Another important ingredient in our proofs is a new notion of symmetric\ninfluence. We use this measure of influence to prove that partial symmetry is\nefficiently testable and also to construct an efficient sample extractor for\npartially symmetric functions. We then combine the sample extractor with the\ntesting-by-implicit-learning approach to complete the proof that partially\nsymmetric functions are efficiently isomorphism-testable.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2011 17:23:14 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Blais", "Eric", ""], ["Weinstein", "Amit", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1112.6255", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Marcin Pilipczuk and Micha{\\l} Pilipczuk", "title": "On group feedback vertex set parameterized by the size of the cutset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of a robust generalization of the\nclassical Feedback Vertex Set problem, namely the Group Feedback Vertex Set\nproblem; we are given a graph G with edges labeled with group elements, and the\ngoal is to compute the smallest set of vertices that hits all cycles of G that\nevaluate to a non-null element of the group. This problem generalizes not only\nFeedback Vertex Set, but also Subset Feedback Vertex Set, Multiway Cut and Odd\nCycle Transversal. Completing the results of Guillemot [Discr. Opt. 2011], we\nprovide a fixed-parameter algorithm for the parameterization by the size of the\ncutset only. Our algorithm works even if the group is given as a\npolynomial-time oracle.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 09:02:35 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1112.6256", "submitter": "Li Ning Mr.", "authors": "Mingfei Li, Christoffer Ma and Li Ning", "title": "(1+epsilon)-Distance Oracle for Planar Labeled Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex-labeled graph, each vertex $v$ is attached with a label from a\nset of labels. The vertex-label query desires the length of the shortest path\nfrom the given vertex to the set of vertices with the given label. We show how\nto construct an oracle if the given graph is planar, such that\n$O(\\frac{1}{\\epsilon}n\\log n)$ storing space is needed, and any vertex-label\nquery could be answered in $O(\\frac{1}{\\epsilon}\\log n\\log \\rho)$ time with\nstretch $1+\\epsilon$. $\\rho$ is the radius of the given graph, which is half of\nthe diameter. For the case that $\\rho = O(\\log n)$, we construct an oracle that\nachieves $O(\\log n)$ query time, without changing the order of storing space.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 09:23:49 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2012 13:17:01 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Li", "Mingfei", ""], ["Ma", "Christoffer", ""], ["Ning", "Li", ""]]}]