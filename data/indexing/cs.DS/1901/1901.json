[{"id": "1901.00156", "submitter": "Faisal Abu-Khzam", "authors": "Faisal N. Abu-Khzam, Judith Egan, Serge Gaspers, Alexis Shaw and Peter\n  Shaw", "title": "On the Parameterized Cluster Editing with Vertex Splitting Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Cluster Editing problem, a given graph is to be transformed into a\ndisjoint union of cliques via a minimum number of edge editing operations. In\nthis paper we introduce a new variant of Cluster Editing whereby a vertex can\nbe divided, or split, into two or more vertices thus allowing a single vertex\nto belong to multiple clusters. This new problem, Cluster Editing with Vertex\nSplitting, has applications in finding correlation clusters in discrete data,\nincluding graphs obtained from Biological Network analysis. We initiate the\nstudy of this new problem and show that it is fixed-parameter tractable when\nparameterized by the total number of vertex splitting and edge editing\noperations. In particular we obtain a 4k(k + 1) vertex kernel for the problem\nand an $\\Oh^*(2^{\\Oh(k^2)})$ search algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 14:28:31 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Egan", "Judith", ""], ["Gaspers", "Serge", ""], ["Shaw", "Alexis", ""], ["Shaw", "Peter", ""]]}, {"id": "1901.00335", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski, Matthew Johnson, Dani\\\"el Paulusma", "title": "Clique-Width for Hereditary Graph Classes", "comments": "61 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique-width is a well-studied graph parameter owing to its use in\nunderstanding algorithmic tractability: if the clique-width of a graph class\n${\\cal G}$ is bounded by a constant, a wide range of problems that are\nNP-complete in general can be shown to be polynomial-time solvable on ${\\cal\nG}$. For this reason, the boundedness or unboundedness of clique-width has been\ninvestigated and determined for many graph classes. We survey these results for\nhereditary graph classes, which are the graph classes closed under taking\ninduced subgraphs. We then discuss the algorithmic consequences of these\nresults, in particular for the Colouring and Graph Isomorphism problems. We\nalso explain a possible strong connection between results on boundedness of\nclique-width and on well-quasi-orderability by the induced subgraph relation\nfor hereditary graph classes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 12:16:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 18:51:57 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Johnson", "Matthew", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1901.00479", "submitter": "Hoa Vu", "authors": "Hsin-Hao Su and Hoa T. Vu", "title": "Towards the Locality of Vizing's Theorem", "comments": "Extended abstract to appear at STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vizing showed that it suffices to color the edges of a simple graph using\n$\\Delta + 1$ colors, where $\\Delta$ is the maximum degree of the graph.\nHowever, up to this date, no efficient distributed edge-coloring algorithms are\nknown for obtaining such a coloring, even for constant degree graphs. The\ncurrent algorithms that get closest to this number of colors are the randomized\n$(\\Delta + \\tilde{\\Theta}(\\sqrt{\\Delta}))$-edge-coloring algorithm that runs in\n$\\text{polylog}(n)$ rounds by Chang et al. (SODA '18) and the deterministic\n$(\\Delta + \\text{polylog}(n))$-edge-coloring algorithm that runs in\n$\\text{poly}(\\Delta, \\log n)$ rounds by Ghaffari et al. (STOC '18).\n  We present two distributed edge-coloring algorithms that run in\n$\\text{poly}(\\Delta,\\log n)$ rounds. The first algorithm, with randomization,\nuses only $\\Delta+2$ colors. The second algorithm is a deterministic algorithm\nthat uses $\\Delta+ O(\\log n/ \\log \\log n)$ colors. Our approach is to reduce\nthe distributed edge-coloring problem into an online, restricted version of\nballs-into-bins problem. If $\\ell$ is the maximum load of the bins, our\nalgorithm uses $\\Delta + 2\\ell - 1$ colors. We show how to achieve $\\ell = 1$\nwith randomization and $\\ell = O(\\log n / \\log \\log n)$ without randomization.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 18:30:14 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 04:27:08 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1901.00512", "submitter": "Alex Frid", "authors": "Eitan Netzer, Alex Frid, Dan Feldman", "title": "Real-Time EEG Classification via Coresets for BCI Applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.engappai.2019.103455", "report-no": null, "categories": "cs.DS cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-computer interface (BCI) based on the motor imagery (MI) paradigm\ntranslates one's motor intention into a control signal by classifying the\nElectroencephalogram (EEG) signal of different tasks. However, most existing\nsystems either (i) use a high-quality algorithm to train the data off-line and\nrun only classification in real-time, since the off-line algorithm is too slow,\nor (ii) use low-quality heuristics that are sufficiently fast for real-time\ntraining but introduces relatively large classification error. In this work, we\npropose a novel processing pipeline that allows real-time and parallel learning\nof EEG signals using high-quality but possibly inefficient algorithms. This is\ndone by forging a link between BCI and core-sets, a technique that originated\nin computational geometry for handling streaming data via data summarization.\n  We suggest an algorithm that maintains the representation such coreset\ntailored to handle the EEG signal which enables: (i) real time and continuous\ncomputation of the Common Spatial Pattern (CSP) feature extraction method on a\ncoreset representation of the signal (instead on the signal itself) , (ii)\nimprovement of the CSP algorithm efficiency with provable guarantees by\napplying CSP algorithm on the coreset, and (iii) real time addition of the data\ntrials (EEG data windows) to the coreset.\n  For simplicity, we focus on the CSP algorithm, which is a classic algorithm.\nNevertheless, we expect that our coreset will be extended to other algorithms\nin future papers. In the experimental results we show that our system can\nindeed learn EEG signals in real-time for example a 64 channels setup with\nhundreds of time samples per second. Full open source is provided to reproduce\nthe experiment and in the hope that it will be used and extended to more\ncoresets and BCI applications in the future.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 19:24:38 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Netzer", "Eitan", ""], ["Frid", "Alex", ""], ["Feldman", "Dan", ""]]}, {"id": "1901.00622", "submitter": "I-Ting Lee", "authors": "Robert Utterback, Kunal Agrawal, Jeremy Fineman, I-Ting Angelina Lee", "title": "Efficient Race Detection with Futures", "comments": null, "journal-ref": null, "doi": "10.1145/3293883.3295732", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of provably efficient and practically good\non-the-fly determinacy race detection in task parallel programs that use\nfutures. Prior works determinacy race detection have mostly focused on either\ntask parallel programs that follow a series-parallel dependence structure or\nones with unrestricted use of futures that generate arbitrary dependences. In\nthis work, we consider a restricted use of futures and show that it can be race\ndetected more efficiently than general use of futures.\n  Specifically, we present two algorithms: MultiBags and MultiBags+. MultiBags\ntargets programs that use futures in a restricted fashion and runs in time\n$O(T_1 \\alpha(m,n))$, where $T_1$ is the sequential running time of the\nprogram, $\\alpha$ is the inverse Ackermann's function, $m$ is the total number\nof memory accesses, $n$ is the dynamic count of places at which parallelism is\ncreated. Since $\\alpha$ is a very slowly growing function (upper bounded by $4$\nfor all practical purposes), it can be treated as a close-to-constant overhead.\nMultiBags+ an extension of MultiBags that target programs with general use of\nfutures. It runs in time $O((T_1+k^2)\\alpha(m,n))$ where $T_1$, $\\alpha$, $m$\nand $n$ are defined as before, and $k$ is the number of future operations in\nthe computation. We implemented both algorithms and empirically demonstrate\ntheir efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 06:10:51 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Utterback", "Robert", ""], ["Agrawal", "Kunal", ""], ["Fineman", "Jeremy", ""], ["Lee", "I-Ting Angelina", ""]]}, {"id": "1901.00626", "submitter": "Dharmarajan R Dr.", "authors": "R. Dharmarajan and D. Ramachandran", "title": "A modified greedy algorithm to improve bounds for the vertex cover\n  number", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any attempt at designing an efficient algorithm for the minimum vertex\ncover problem, obtaining good upper and lower bounds for the vertex cover\nnumber could be crucial. In this article we present a modified greedy algorithm\nof worst-case time complexity O(n3) to obtain bounds for the vertex cover\nnumber of an input graph of order n. Using simple facts, the proposed algorithm\ncomputes a lower bound for the vertex cover number. Then using this lower bound\nit outputs a minimal vertex cover and hence gives an upper bound. The algorithm\nensures the output vertex cover is always minimal, which feature is an\nimprovement upon the existing greedy algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 06:27:43 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Dharmarajan", "R.", ""], ["Ramachandran", "D.", ""]]}, {"id": "1901.00650", "submitter": "Peng Jia", "authors": "Peng Jia, Pinghui Wang, Jing Tao, Xiaohong Guan", "title": "A Fast Sketch Method for Mining User Similarities over Fully Dynamic\n  Graph Streams", "comments": "Accepted in ICDE 2019 (4-page short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world networks such as Twitter and YouTube are given as fully\ndynamic graph streams represented as sequences of edge insertions and\ndeletions. (e.g., users can subscribe and unsubscribe to channels on YouTube).\nExisting similarity estimation methods such as MinHash and OPH are customized\nto static graphs. We observe that they are indeed sampling methods and exhibit\na sampling bias when applied to fully dynamic graph streams, which results in\nlarge estimation errors. To solve this challenge, we develop a fast and\naccurate sketch method VOS. VOS processes each edge in the graph stream of\ninterest with small time complexity O(1) and uses small memory space to build a\ncompact sketch of the dynamic graph stream over time. Based on the sketch built\non-the-fly, we develop a method to estimate user similarities over time. We\nconduct extensive experiments and the experimental results demonstrate the\nefficiency and efficacy of our method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 08:30:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 07:29:41 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Jia", "Peng", ""], ["Wang", "Pinghui", ""], ["Tao", "Jing", ""], ["Guan", "Xiaohong", ""]]}, {"id": "1901.00695", "submitter": "Clemens Thielen", "authors": "Ulrich Pferschy, Joachim Schauer, Clemens Thielen", "title": "The Product Knapsack Problem: Approximation and Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the product knapsack problem, which is the variant of the\nclassical 0-1 knapsack problem where the objective consists of maximizing the\nproduct of the profits of the selected items. These profits are allowed to be\npositive or negative. We show that this recently introduced variant of the\nknapsack problem is weakly NP-hard and present a fully polynomial-time\napproximation scheme (FPTAS) for the problem. Moreover, we analyze the\napproximation quality achieved by a natural extension of the classical greedy\nprocedure to the product knapsack problem.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 12:25:59 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 07:37:39 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 09:04:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Pferschy", "Ulrich", ""], ["Schauer", "Joachim", ""], ["Thielen", "Clemens", ""]]}, {"id": "1901.00717", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty", "title": "Almost Optimal Distribution-free Junta Testing", "comments": "arXiv admin note: text overlap with arXiv:1802.04859 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing whether an unknown $n$-variable Boolean\nfunction is a $k$-junta in the distribution-free property testing model, where\nthe distance between function is measured with respect to an arbitrary and\nunknown probability distribution over $\\{0,1\\}^n$. Chen, Liu, Servedio, Sheng\nand Xie showed that the distribution-free $k$-junta testing can be performed,\nwith one-sided error, by an adaptive algorithm that makes $\\tilde\nO(k^2)/\\epsilon$ queries. In this paper, we give a simple two-sided error\nadaptive algorithm that makes $\\tilde O(k/\\epsilon)$ queries.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 17:00:55 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 11:44:23 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 06:29:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bshouty", "Nader H.", ""]]}, {"id": "1901.00718", "submitter": "Inge Li G{\\o}rtz", "authors": "Philip Bille, Mikko Berggren Etienne, Inge Li G{\\o}rtz", "title": "Mergeable Dictionaries With Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the mergeable dictionaries with shift problem, where the goal is\nto maintain a family of sets subject to search, split, merge, make-set, and\nshift operations. The search, split, and make-set operations are the usual\nwell-known textbook operations. The merge operation merges two sets and the\nshift operation adds or subtracts an integer from all elements in a set. Note\nthat unlike the join operation on standard balanced search tree structures,\nsuch as AVL trees or 2-4 trees, the merge operation has no restriction on the\nkey space of the input sets and supports merging arbitrarily interleaved sets.\nThis problem is a key component in searching Lempel-Ziv compressed texts, in\nthe mergeable trees problem, and in the union-split-find problem.\n  We present the first solution achieving O(log U) amortized time for all\noperations, where {1, 2, ..., U} is the universe of the sets. This bound is\noptimal when the size of the universe is polynomially bounded by the sum of the\nsizes of the sets. Our solution is simple and based on a novel extension of\nbiased search trees.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 13:58:15 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Bille", "Philip", ""], ["Etienne", "Mikko Berggren", ""], ["G\u00f8rtz", "Inge Li", ""]]}, {"id": "1901.00754", "submitter": "Stanislav Zivny", "authors": "Silvia Butti and Stanislav Zivny", "title": "Sparsification of Binary CSPs", "comments": "Full version of a STACS'19 paper", "journal-ref": "SIAM Journal on Discrete Mathematics 34(1) (2020) 825-842", "doi": "10.1137/19M1242446", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cut $\\varepsilon$-sparsifier of a weighted graph $G$ is a re-weighted\nsubgraph of $G$ of (quasi)linear size that preserves the size of all cuts up to\na multiplicative factor of $\\varepsilon$. Since their introduction by Bencz\\'ur\nand Karger [STOC'96], cut sparsifiers have proved extremely influential and\nfound various applications. Going beyond cut sparsifiers, Filtser and\nKrauthgamer [SIDMA'17] gave a precise classification of which binary Boolean\nCSPs are sparsifiable. In this paper, we extend their result to binary CSPs on\narbitrary finite domains.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 14:15:10 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 16:19:38 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Butti", "Silvia", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1901.00984", "submitter": "Penghui Yao", "authors": "Janet Leahy, Dave Touchette, Penghui Yao", "title": "Quantum Insertion-Deletion Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model of quantum insertion-deletion (insdel) channels. Insdel\nchannels are meant to represent, for example, synchronization errors arising in\ndata transmission. In the classical setting, they represent a strict\ngeneralization of the better-understood corruption error channels, and until\nrecently, had mostly resisted effort toward a similar understanding as their\ncorruption counterparts. They have received considerable attention in recent\nyears. Very recently, Haeupler and Shahrasbi developed a framework, using what\nthey call synchronisation strings, that allows one to turn insdel-type errors\ninto corruption-type errors. These can then be handled by the use of standard\nerror-correcting codes. We show that their framework can be extended to the\nquantum setting, providing a way to turn quantum insdel errors into quantum\ncorruption errors, which can be handled with standard quantum error-correcting\ncodes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 05:27:10 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Leahy", "Janet", ""], ["Touchette", "Dave", ""], ["Yao", "Penghui", ""]]}, {"id": "1901.01135", "submitter": "Kim-Manuel Klein", "authors": "Kim-Manuel Klein", "title": "About the Complexity of Two-Stage Stochastic IPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider so called $2$-stage stochastic integer programs (IPs) and their\ngeneralized form of multi-stage stochastic IPs. A $2$-stage stochastic IP is an\ninteger program of the form $\\max \\{ c^T x \\mid Ax = b, l \\leq x \\leq u, x \\in\n\\mathbb{Z}^{nt + s} \\}$ where the constraint matrix $A \\in \\mathbb{Z}^{r \\times\ns}$ consists roughly of $n$ repetition of a block matrix $A$ on the vertical\nline and $n$ repetitions of a matrix $B \\in \\mathbb{Z}^{r \\times t}$ on the\ndiagonal. In this paper we improve upon an algorithmic result by Hemmecke and\nSchultz form 2003 to solve $2$-stage stochastic IPs. The algorithm is based on\nthe Graver augmentation framework where our main contribution is to give an\nexplicit doubly exponential bound on the size of the augmenting steps. The\nprevious bound for the size of the augmenting steps relied on non-constructive\nfiniteness arguments from commutative algebra and therefore only an implicit\nbound was known that depends on parameters $r,s,t$ and $\\Delta$, where $\\Delta$\nis the largest entry of the constraint matrix. Our new improved bound however\nis obtained by a novel theorem which argues about the intersection of paths in\na vector space. As a result of our new bound we obtain an algorithm to solve\n$2$-stage stochastic IPs in time $poly(n,t) \\cdot f(r,s,\\Delta)$, where $f$ is\na doubly exponential function. To complement our result, we also prove a doubly\nexponential lower bound for the size of the augmenting steps.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 14:35:05 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 18:58:17 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Klein", "Kim-Manuel", ""]]}, {"id": "1901.01172", "submitter": "Diego Seco", "authors": "Rodrigo Rivera, Andrea Rodr\\'iguez, Diego Seco", "title": "Faster and Smaller Two-Level Index for Network-based Trajectories", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proceedings of the 25th International Symposium on String\n  Processing and Information Retrieval (SPIRE 2018)", "doi": "10.1007/978-3-030-00479-8_28", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-level indexes have been widely used to handle trajectories of moving\nobjects that are constrained to a network. The top-level of these indexes\nhandles the spatial dimension, whereas the bottom level handles the temporal\ndimension. The latter turns out to be an instance of the interval-intersection\nproblem, but it has been tackled by non-specialized spatial indexes. In this\nwork, we propose the use of a compact data structure on the bottom level of\nthese indexes. Our experimental evaluation shows that our approach is both\nfaster and smaller than existing solutions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 15:25:16 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Rivera", "Rodrigo", ""], ["Rodr\u00edguez", "Andrea", ""], ["Seco", "Diego", ""]]}, {"id": "1901.01341", "submitter": "Linas Vepstas PhD", "authors": "Linas Vepstas", "title": "Sheaves: A Topological Approach to Big Data", "comments": "49 pages, 24 figures", "journal-ref": "Bulletin of Novosibirsk Computing Center,Computer Science, No. 41,\n  2017", "doi": "10.31144/bncc.cs.2542-1972.2017.n41.p55-89", "report-no": null, "categories": "cs.LG cs.DS cs.SC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This document develops general concepts useful for extracting knowledge\nembedded in large graphs or datasets that have pair-wise relationships, such as\ncause-effect-type relations. Almost no underlying assumptions are made, other\nthan that the data can be presented in terms of pair-wise relationships between\nobjects/events. This assumption is used to mine for patterns in the dataset,\ndefining a reduced graph or dataset that boils-down or concentrates information\ninto a more compact form. The resulting extracted structure or set of patterns\nare manifestly symbolic in nature, as they capture and encode the graph\nstructure of the dataset in terms of a (generative) grammar. This structure is\nidentified as having the formal mathematical structure of a sheaf. In essence,\nthis paper introduces the basic concepts of sheaf theory into the domain of\ngraphical datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 23:44:47 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Vepstas", "Linas", ""]]}, {"id": "1901.01412", "submitter": "Ohad Trabelsi", "authors": "Amir Abboud, Robert Krauthgamer, Ohad Trabelsi", "title": "New Algorithms and Lower Bounds for All-Pairs Max-Flow in Undirected\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the time-complexity of the All-Pairs Max-Flow problem: Given a\ngraph with $n$ nodes and $m$ edges, compute for all pairs of nodes the\nmaximum-flow value between them. If Max-Flow (the version with a given\nsource-sink pair $s,t$) can be solved in time $T(m)$, then an $O(n^2) \\cdot\nT(m)$ is a trivial upper bound. But can we do better?\n  For directed graphs, recent results in fine-grained complexity suggest that\nthis time bound is essentially optimal. In contrast, for undirected graphs with\nedge capacities, a seminal algorithm of Gomory and Hu (1961) runs in much\nfaster time $O(n)\\cdot T(m)$. Under the plausible assumption that Max-Flow can\nbe solved in near-linear time $m^{1+o(1)}$, this half-century old algorithm\nyields an $nm^{1+o(1)}$ bound. Several other algorithms have been designed\nthrough the years, including $\\tilde{O}(mn)$ time for unit-capacity edges\n(unconditionally), but none of them break the $O(mn)$ barrier. Meanwhile, no\nsuper-linear lower bound was shown for undirected graphs.\n  We design the first hardness reductions for All-Pairs Max-Flow in undirected\ngraphs, giving an essentially optimal lower bound for the\n$\\textit{node-capacities}$ setting. For edge capacities, our efforts to prove\nsimilar lower bounds have failed, but we have discovered a surprising new\nalgorithm that breaks the $O(mn)$ barrier for graphs with unit-capacity edges!\nAssuming $T(m)=m^{1+o(1)}$, our algorithm runs in time $m^{3/2 +o(1)}$ and\noutputs a cut-equivalent tree (similarly to the Gomory-Hu algorithm). Even with\ncurrent Max-Flow algorithms we improve state-of-the-art as long as\n$m=O(n^{5/3-\\varepsilon})$. Finally, we explain the lack of lower bounds by\nproving a $\\textit{non-reducibility}$ result. This result is based on a new\nquasi-linear time $\\tilde{O}(m)$ $\\textit{non-deterministic}$ algorithm for\nconstructing a cut-equivalent tree and may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 13:20:46 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 17:09:58 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 15:07:04 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 01:54:52 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Abboud", "Amir", ""], ["Krauthgamer", "Robert", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "1901.01630", "submitter": "Ami Paz", "authors": "Amir Abboud and Keren Censor-Hillel and Seri Khoury and Ami Paz", "title": "Smaller Cuts, Higher Lower Bounds", "comments": "This is work is a merger of arXiv:1605.05109 and arXiv:1705.05646", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves strong lower bounds for distributed computing in the\nCONGEST model, by presenting the bit-gadget: a new technique for constructing\ngraphs with small cuts.\n  The contribution of bit-gadgets is twofold. First, developing careful sparse\ngraph constructions with small cuts extends known techniques to show a\nnear-linear lower bound for computing the diameter, a result previously known\nonly for dense graphs. Moreover, the sparseness of the construction plays a\ncrucial role in applying it to approximations of various distance computation\nproblems, drastically improving over what can be obtained when using dense\ngraphs.\n  Second, small cuts are essential for proving super-linear lower bounds, none\nof which were known prior to this work. In fact, they allow us to show\nnear-quadratic lower bounds for several problems, such as exact minimum vertex\ncover or maximum independent set, as well as for coloring a graph with its\nchromatic number. Such strong lower bounds are not limited to NP-hard problems,\nas given by two simple graph problems in P which are shown to require a\nquadratic and near-quadratic number of rounds. All of the above are optimal up\nto logarithmic factors. In addition, in this context, the complexity of the\nall-pairs-shortest-paths problem is discussed.\n  Finally, it is shown that graph constructions for CONGEST lower bounds\ntranslate to lower bounds for the semi-streaming model, despite being very\ndifferent in its nature.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 00:09:33 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 09:40:21 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Abboud", "Amir", ""], ["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""], ["Paz", "Ami", ""]]}, {"id": "1901.01665", "submitter": "Nina Holden", "authors": "Giulia Fanti, Nina Holden, Yuval Peres, and Gireeja Ranade", "title": "Communication cost of consensus for nodes with limited memory", "comments": "62 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in blockchains and sensor networks, we consider a\nmodel of $n$ nodes trying to reach consensus on their majority bit. Each node\n$i$ is assigned a bit at time zero, and is a finite automaton with $m$ bits of\nmemory (i.e., $2^m$ states) and a Poisson clock. When the clock of $i$ rings,\n$i$ can choose to communicate, and is then matched to a uniformly chosen node\n$j$. The nodes $j$ and $i$ may update their states based on the state of the\nother node. Previous work has focused on minimizing the time to consensus and\nthe probability of error, while our goal is minimizing the number of\ncommunications. We show that when $m>3 \\log\\log\\log(n)$, consensus can be\nreached at linear communication cost, but this is impossible if\n$m<\\log\\log\\log(n)$. We also study a synchronous variant of the model, where\nour upper and lower bounds on $m$ for achieving linear communication cost are\n$2\\log\\log\\log(n)$ and $\\log\\log\\log(n)$, respectively. A key step is to\ndistinguish when nodes can become aware of knowing the majority bit and stop\ncommunicating. We show that this is impossible if their memory is too low.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 04:48:34 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Fanti", "Giulia", ""], ["Holden", "Nina", ""], ["Peres", "Yuval", ""], ["Ranade", "Gireeja", ""]]}, {"id": "1901.01710", "submitter": "Yoshitaka Yamamoto", "authors": "Yoshitaka Yamamoto, Yasuo Tabei, and Koji Iwanuma", "title": "Approximate-Closed-Itemset Mining for Streaming Data Under Resource\n  Constraint", "comments": "14 pages, 16 figures, submitted to VLDB2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present a novel algorithm for frequent itemset mining for streaming\ndata (FIM-SD). For the past decade, various FIM-SD methods in one-pass\napproximation settings have been developed to approximate the frequency of each\nitemset. These approaches can be categorized into two approximation types:\nparameter-constrained (PC) mining and resource-constrained (RC) mining. PC\nmethods control the maximum error that can be included in the frequency based\non a pre-defined parameter. In contrast, RC methods limit the maximum memory\nconsumption based on resource constraints. However, the existing PC methods can\nexponentially increase the memory consumption, while the existing RC methods\ncan rapidly increase the maximum error. In this study, we address this problem\nby introducing the notion of a condensed representation, called a\n$\\Delta$-covered set, to the RC approximation. This notion is regarded as an\nextension of the closedness compression and when $\\Delta = 0$, the solution\ncorresponds to an ordinary closed itemset. The algorithm searches for such\napproximate closed itemsets that can restore the frequent itemsets and their\nfrequencies under resource constraint while the maximum error is bounded by an\ninteger, $\\Delta$. We first propose a one-pass approximation algorithm to find\nthe condensed solution. Then, we improve the basic algorithm by introducing a\nunified PC-RC approximation approach. Finally, we empirically demonstrate that\nthe proposed algorithm significantly outperforms the state-of-the-art PC and RC\nmethods for FIM-SD.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 08:58:59 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Yamamoto", "Yoshitaka", ""], ["Tabei", "Yasuo", ""], ["Iwanuma", "Koji", ""]]}, {"id": "1901.01825", "submitter": "Mohammad Ashraful Hoque", "authors": "Francesco Concas, Pengfei Xu, Mohammad A. Hoque, Jiaheng Lu, and Sasu\n  Tarkoma", "title": "Multiple Set Matching and Pre-Filtering with Bloom Multifilters", "comments": "12 pages, 11 figures, Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bloom filter is a space-efficient probabilistic data structure for checking\nelements' membership in a set. Given multiple sets, however, a standard Bloom\nfilter is not sufficient when looking for the items to which an element or a\nset of input elements belong to. In this article, we solve multiple set\nmatching problem by proposing two efficient Bloom Multifilters called Bloom\nMatrix and Bloom Vector. Both of them are space efficient and answer queries\nwith a set of identifiers for multiple set matching problems. We show that the\nspace efficiency can be optimized further according to the distribution of\nlabels among multiple sets: Uniform and Zipf. While both of them are space\nefficient, Bloom Vector can efficiently exploit Zipf distribution of data for\nfurther space reduction. Our results also highlight that basic ADD and LOOKUP\noperations on Bloom Matrix are faster than on Bloom Vector. However, Bloom\nMatrix does not meet the theoretical false positive rate of less than $10^{-2}$\nfor LOOKUP operations if the represented data or the labels are not uniformly\ndistributed among the multiple sets. Consequently, we introduce \\textit{Bloom\nTest} which uses Bloom Matrix as the pre-filter structure to determine which\nstructure is suitable for improved performance with an arbitrary input dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 14:21:01 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 08:26:58 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Concas", "Francesco", ""], ["Xu", "Pengfei", ""], ["Hoque", "Mohammad A.", ""], ["Lu", "Jiaheng", ""], ["Tarkoma", "Sasu", ""]]}, {"id": "1901.01861", "submitter": "Esther Galby", "authors": "Esther Galby, Paloma T. Lima, Dani\\\"el Paulusma and Bernard Ries", "title": "On the Parameterized Complexity of $k$-Edge Colouring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every fixed integer $k \\geq 1$, we prove that $k$-Edge Colouring is\nfixed-parameter-tractable when parameterized by the number of vertices of\nmaximum degree.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 15:02:07 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 17:22:48 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 13:25:57 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2019 09:44:22 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Galby", "Esther", ""], ["Lima", "Paloma T.", ""], ["Paulusma", "Dani\u00ebl", ""], ["Ries", "Bernard", ""]]}, {"id": "1901.01926", "submitter": "Grzegorz Gu\\'spiel", "authors": "Grzegorz Gu\\'spiel", "title": "An in-place, subquadratic algorithm for permutation inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume the permutation $\\pi$ is given by an $n$-element array in which the\n$i$-th element denotes the value $\\pi(i)$. Constructing its inverse in-place\n(i.e. using $O(\\log{n})$ bits of additional memory) can be achieved in linear\ntime with a simple algorithm. Limiting the numbers that can be stored in our\narray to the range $[1...n]$ still allows a straightforward $O(n^2)$ time\nsolution. The time complexity can be improved using randomization, but this\nonly improves the expected, not the pessimistic running time. We present a\ndeterministic algorithm that runs in $O(n^{3/2})$ time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:14:25 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 10:55:07 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Gu\u015bpiel", "Grzegorz", ""]]}, {"id": "1901.01944", "submitter": "Diego Seco", "authors": "Nataly Cruces, Diego Seco, Gilberto Guti\\'errez", "title": "A Compact Representation of Raster Time Series", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proceedings of the Data Compression Conference (DCC 2019)", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raster model is widely used in Geographic Information Systems to\nrepresent data that vary continuously in space, such as temperatures,\nprecipitations, elevation, among other spatial attributes. In applications like\nweather forecast systems, not just a single raster, but a sequence of rasters\ncovering the same region at different timestamps, known as a raster time\nseries, needs to be stored and queried. Compact data structures have proven\nsuccessful to provide space-efficient representations of rasters with query\ncapabilities. Hence, a naive approach to save space is to use such a\nrepresentation for each raster in a time series. However, in this paper we show\nthat it is possible to take advantage of the temporal locality that exists in a\nraster time series to reduce the space necessary to store it while keeping\ncompetitive query times for several types of queries.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:52:23 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Cruces", "Nataly", ""], ["Seco", "Diego", ""], ["Guti\u00e9rrez", "Gilberto", ""]]}, {"id": "1901.02166", "submitter": "Sourav Medya", "authors": "Sourav Medya, Tiyani Ma, Arlei Silva, Ambuj Singh", "title": "K-Core Minimization: A Game Theoretic Approach", "comments": "To appear as an extended abstract in AAMAS 2020 and as a full paper\n  in IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-cores are maximal induced subgraphs where all vertices have degree at least\nk. These dense patterns have applications in community detection, network\nvisualization and protein function prediction. However, k-cores can be quite\nunstable to network modifications, which motivates the question: How resilient\nis the k-core structure of a network, such as the Web or Facebook, to edge\ndeletions? We investigate this question from an algorithmic perspective. More\nspecifically, we study the problem of computing a small set of edges for which\nthe removal minimizes the $k$-core structure of a network. This paper provides\na comprehensive characterization of the hardness of the k-core minimization\nproblem (KCM), including innaproximability and fixed-parameter intractability.\nMotivated by such a challenge in terms of algorithm design, we propose a novel\nalgorithm inspired by Shapley value -- a cooperative game-theoretic concept --\nthat is able to leverage the strong interdependencies in the effects of edge\nremovals in the search space. As computing Shapley values is also NP-hard, we\nefficiently approximate them using a randomized algorithm with probabilistic\nguarantees. Our experiments, using several real datasets, show that the\nproposed algorithm outperforms competing solutions in terms of k-core\nminimization while being able to handle large graphs. Moreover, we illustrate\nhow KCM can be applied in the analysis of the k-core resilience of networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 05:33:56 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 20:52:11 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Medya", "Sourav", ""], ["Ma", "Tiyani", ""], ["Silva", "Arlei", ""], ["Singh", "Ambuj", ""]]}, {"id": "1901.02209", "submitter": "Prafullkumar Tale Mr", "authors": "Geevarghese Philip and Varun Rajan and Saket Saurabh and Prafullkumar\n  Tale", "title": "Subset Feedback Vertex Set in Chordal and Split Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the \\textsc{Subset Feedback Vertex Set (Subset-FVS)} problem the input is\na graph $G$, a subset \\(T\\) of vertices of \\(G\\) called the `terminal'\nvertices, and an integer $k$. The task is to determine whether there exists a\nsubset of vertices of cardinality at most $k$ which together intersect all\ncycles which pass through the terminals. \\textsc{Subset-FVS} generalizes\nseveral well studied problems including \\textsc{Feedback Vertex Set} and\n\\textsc{Multiway Cut}. This problem is known to be \\NP-Complete even in split\ngraphs. Cygan et al. proved that \\textsc{Subset-FVS} is fixed parameter\ntractable (\\FPT) in general graphs when parameterized by $k$ [SIAM J. Discrete\nMath (2013)]. In split graphs a simple observation reduces the problem to an\nequivalent instance of the $3$-\\textsc{Hitting Set} problem with same solution\nsize. This directly implies, for \\textsc{Subset-FVS} \\emph{restricted to split\ngraphs}, (i) an \\FPT algorithm which solves the problem in $\\OhStar(2.076^k)$\ntime \\footnote{The \\(\\OhStar()\\) notation hides polynomial factors.}% for\n\\textsc{Subset-FVS} in Chordal % Graphs [Wahlstr\\\"om, Ph.D. Thesis], and (ii) a\nkernel of size $\\mathcal{O}(k^3)$. We improve both these results for\n\\textsc{Subset-FVS} on split graphs; we derive (i) a kernel of size\n$\\mathcal{O}(k^2)$ which is the best possible unless $\\NP \\subseteq \\coNP/{\\sf\npoly}$, and (ii) an algorithm which solves the problem in time\n$\\mathcal{O}^*(2^k)$. Our algorithm, in fact, solves \\textsc{Subset-FVS} on the\nmore general class of \\emph{chordal graphs}, also in $\\mathcal{O}^*(2^k)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 08:50:02 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Philip", "Geevarghese", ""], ["Rajan", "Varun", ""], ["Saurabh", "Saket", ""], ["Tale", "Prafullkumar", ""]]}, {"id": "1901.02393", "submitter": "Maryam Negahbani", "authors": "Suman K. Bera, Deeparnab Chakrabarty, Nicolas J. Flores, Maryam\n  Negahbani", "title": "Fair Algorithms for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding low-cost Fair Clusterings in data where each\ndata point may belong to many protected groups. Our work significantly\ngeneralizes the seminal work of Chierichetti et.al. (NIPS 2017) as follows.\n  - We allow the user to specify the parameters that define fair\nrepresentation. More precisely, these parameters define the maximum over- and\nminimum under-representation of any group in any cluster.\n  - Our clustering algorithm works on any $\\ell_p$-norm objective (e.g.\n$k$-means, $k$-median, and $k$-center). Indeed, our algorithm transforms any\nvanilla clustering solution into a fair one incurring only a slight loss in\nquality.\n  - Our algorithm also allows individuals to lie in multiple protected groups.\nIn other words, we do not need the protected groups to partition the data and\nwe can maintain fairness across different groups simultaneously.\n  Our experiments show that on established data sets, our algorithm performs\nmuch better in practice than what our theoretical results suggest.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 16:39:16 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 14:01:05 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Bera", "Suman K.", ""], ["Chakrabarty", "Deeparnab", ""], ["Flores", "Nicolas J.", ""], ["Negahbani", "Maryam", ""]]}, {"id": "1901.02491", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Faster parameterized algorithm for pumpkin vertex deletion set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph $G$ is called a pumpkin if $G$ is a union of induced paths\nwith a common start vertex $s$ and a common end vertex $t$, and the internal\nvertices of every two paths are disjoint. We give an algorithm that given a\ndirected graph $G$ and an integer $k$, decides whether a pumpkin can be\nobtained from $G$ by deleting at most $k$ vertices. The algorithm runs in\n$O^*(2^k)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:00:07 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1901.02510", "submitter": "Sawsen Ben Nasr", "authors": "Sawsen Ben Nasr", "title": "New approach for a stable multi-criteria ridesharing system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The witnessed boom in mobility results in many problems such as urbanization,\ncostly construction of many highways and air pollution. In an attempt to\naddress these problems, in this master, we are interested in the implementation\nof a ridesharing system. Ridesharing is recognized as a highly effective means\nof transport to solve energy consumption, environmental pollution and traffic\ncongestion issues. Indeed, ridesharing can reduce the number of vehicles on the\nroads to avoid traffic jams and thus it contributes to a reduction in\ngreenhouse gas emissions. Its main thrust resides in sharing transport\nexpenses, meeting different people and making traveling more enjoyable. In this\nrespect, we introduce in this dissertation an effective ridesharing system,\ncalled the Stable Multi-Criteria Rideshare Matching (SMRM) system, that (i)\nconsiders users' personal preferences when sharing a private space with others\nand (ii) enables a stable matching between driver and passenger sets. The\nperformed experiments show that the introduced system outperforms its\ncompetitors in terms of stability quality and cost.\n  Keywords: Smart cities, Social sustainability, Ridesharing , Social\npreferences , TOPSIS , Stable marriage .\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:44:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 18:16:21 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Nasr", "Sawsen Ben", ""]]}, {"id": "1901.02536", "submitter": "Christopher Umans", "authors": "Chris Umans", "title": "Fast generalized DFTs for all finite groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any finite group $G$, we give an arithmetic algorithm to compute\ngeneralized Discrete Fourier Transforms (DFTs) with respect to $G$, using\n$O(|G|^{\\omega/2 + \\epsilon})$ operations, for any $\\epsilon > 0$. Here,\n$\\omega$ is the exponent of matrix multiplication.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:02:50 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Umans", "Chris", ""]]}, {"id": "1901.02560", "submitter": "Arash Atashpendar", "authors": "Peter B. R{\\o}nne and Arash Atashpendar and Kristian Gj{\\o}steen and\n  Peter Y. A. Ryan", "title": "Coercion-Resistant Voting in Linear Time via Fully Homomorphic\n  Encryption: Towards a Quantum-Safe Scheme", "comments": "9 pages; added acknowledgments, revised the first paragraph in the\n  section on security remarks, revised a few sentences throughout; to appear in\n  the proceedings of Financial Cryptography and Data Security 2019, published\n  by Springer", "journal-ref": null, "doi": "10.1007/978-3-030-43725-1_20", "report-no": null, "categories": "cs.CR cs.CC cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for performing the tallying work in the\ncoercion-resistant JCJ voting protocol, introduced by Juels, Catalano, and\nJakobsson, in linear time using fully homomorphic encryption (FHE). The\nsuggested enhancement also paves the path towards making JCJ quantum-resistant,\nwhile leaving the underlying structure of JCJ intact. The exhaustive,\ncomparison-based approach of JCJ using plaintext equivalence tests leads to a\nquadratic blow-up in the number of votes, which makes the tallying process\nrather impractical in realistic settings with a large number of voters. We show\nhow the removal of invalid votes can be done in linear time via a solution\nbased on recent advances in various FHE primitives such as hashing,\nzero-knowledge proofs of correct decryption, verifiable shuffles and threshold\nFHE. We conclude by touching upon some of the advantages and challenges of such\nan approach, followed by a discussion of further security and post-quantum\nconsiderations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 23:39:20 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 16:52:36 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["R\u00f8nne", "Peter B.", ""], ["Atashpendar", "Arash", ""], ["Gj\u00f8steen", "Kristian", ""], ["Ryan", "Peter Y. A.", ""]]}, {"id": "1901.02771", "submitter": "Christian Truden", "authors": "Christoph Hertrich, Philipp Hungerl\\\"ander, Christian Truden", "title": "Sweep Algorithms for the Capacitated Vehicle Routing Problem with\n  Structured Time Window", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-18500-8_17", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacitated Vehicle Routing Problem with structured Time Windows\n(cVRPsTW) is concerned with finding optimal tours for vehicles with given\ncapacity constraints to deliver goods to customers within assigned time\nwindows. In our problem variant these time windows have a special structure,\nnamely they are non-overlapping and each time window holds several customers.\nThis is a reasonable assumption for Attended Home Delivery services. Sweep\nalgorithms are known as simple, yet effective heuristics for the classical\ncapacitated Vehicle Routing Problem. We propose variants of the sweep algorithm\nthat are not only able to deal with time windows, but also exploit the\nadditional structure of the time windows in a cVRPsTW. Afterwards we suggest\nlocal improvement heuristics to decrease our objective function even further. A\ncarefully constructed benchmark set that resembles real-world data is used to\nprove the efficacy of our algorithms in a computational study.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 15:00:46 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Hertrich", "Christoph", ""], ["Hungerl\u00e4nder", "Philipp", ""], ["Truden", "Christian", ""]]}, {"id": "1901.02857", "submitter": "Manuel Penschuck", "authors": "Peyman Afshani, Rolf Fagerberg, David Hammer, Riko Jacob, Irina\n  Kostitsyna, Ulrich Meyer, Manuel Penschuck, Nodari Sitchinava", "title": "Fragile Complexity of Comparison-Based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a study of algorithms with a focus on the computational\ncomplexity of individual elements, and introduce the fragile complexity of\ncomparison-based algorithms as the maximal number of comparisons any individual\nelement takes part in. We give a number of upper and lower bounds on the\nfragile complexity for fundamental problems, including Minimum, Selection,\nSorting and Heap Construction. The results include both deterministic and\nrandomized upper and lower bounds, and demonstrate a separation between the two\nsettings for a number of problems. The depth of a comparator network is a\nstraight-forward upper bound on the worst case fragile complexity of the\ncorresponding fragile algorithm. We prove that fragile complexity is a\ndifferent and strictly easier property than the depth of comparator networks,\nin the sense that for some problems a fragile complexity equal to the best\nnetwork depth can be achieved with less total work and that with randomization,\neven a lower fragile complexity is possible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 18:21:01 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 07:58:49 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Afshani", "Peyman", ""], ["Fagerberg", "Rolf", ""], ["Hammer", "David", ""], ["Jacob", "Riko", ""], ["Kostitsyna", "Irina", ""], ["Meyer", "Ulrich", ""], ["Penschuck", "Manuel", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1901.02871", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, David Simchi-Levi, Xinshang Wang", "title": "The Lingering of Gradients: Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 18:45:10 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 11:44:06 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Simchi-Levi", "David", ""], ["Wang", "Xinshang", ""]]}, {"id": "1901.03155", "submitter": "Markus Lohrey", "authors": "Danny Hucke, Markus Lohrey, and Louisa Seelbach Benkner", "title": "Entropy Bounds for Grammar-Based Tree Compressors", "comments": "A short version of this paper appeared in the IEEE Proceedings of\n  ISIT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The definition of $k^{th}$-order empirical entropy of strings is extended to\nnode labelled binary trees. A suitable binary encoding of tree straight-line\nprograms (that have been used for grammar-based tree compression before) is\nshown to yield binary tree encodings of size bounded by the $k^{th}$-order\nempirical entropy plus some lower order terms. This generalizes recent results\nfor grammar-based string compression to grammar-based tree compression.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 13:41:19 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 05:22:50 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 13:34:44 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Hucke", "Danny", ""], ["Lohrey", "Markus", ""], ["Benkner", "Louisa Seelbach", ""]]}, {"id": "1901.03254", "submitter": "Tongyang Li", "authors": "Nai-Hui Chia, Tongyang Li, Han-Hsuan Lin, Chunhao Wang", "title": "Quantum-inspired sublinear algorithm for solving low-rank semidefinite\n  programming", "comments": "37 pages, 1 figure. To appear in the Proceedings of the 45th\n  International Symposium on Mathematical Foundations of Computer Science (MFCS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite programming (SDP) is a central topic in mathematical\noptimization with extensive studies on its efficient solvers. In this paper, we\npresent a proof-of-principle sublinear-time algorithm for solving SDPs with\nlow-rank constraints; specifically, given an SDP with $m$ constraint matrices,\neach of dimension $n$ and rank $r$, our algorithm can compute any entry and\nefficient descriptions of the spectral decomposition of the solution matrix.\nThe algorithm runs in time $O(m\\cdot\\mathrm{poly}(\\log n,r,1/\\varepsilon))$\ngiven access to a sampling-based low-overhead data structure for the constraint\nmatrices, where $\\varepsilon$ is the precision of the solution. In addition, we\napply our algorithm to a quantum state learning task as an application.\n  Technically, our approach aligns with 1) SDP solvers based on the matrix\nmultiplicative weight (MMW) framework by Arora and Kale [TOC '12]; 2)\nsampling-based dequantizing framework pioneered by Tang [STOC '19]. In order to\ncompute the matrix exponential required in the MMW framework, we introduce two\nnew techniques that may be of independent interest:\n  $\\bullet$ Weighted sampling: assuming sampling access to each individual\nconstraint matrix $A_{1},\\ldots,A_{\\tau}$, we propose a procedure that gives a\ngood approximation of $A=A_{1}+\\cdots+A_{\\tau}$.\n  $\\bullet$ Symmetric approximation: we propose a sampling procedure that gives\nthe \\emph{spectral decomposition} of a low-rank Hermitian matrix $A$. To the\nbest of our knowledge, this is the first sampling-based algorithm for spectral\ndecomposition, as previous works only give singular values and vectors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 16:26:15 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 19:38:50 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Chia", "Nai-Hui", ""], ["Li", "Tongyang", ""], ["Lin", "Han-Hsuan", ""], ["Wang", "Chunhao", ""]]}, {"id": "1901.03270", "submitter": "Luiz Fernando Bittencourt", "authors": "Luiz F. Bittencourt, Alfredo Goldman, Edmundo R. M. Madeira, Nelson L.\n  S. da Fonseca, Rizos Sakellariou", "title": "Scheduling in distributed systems: A cloud computing perspective", "comments": null, "journal-ref": "Computer Science Review, Volume 30, 2018, Pages 31-54", "doi": "10.1016/j.cosrev.2018.08.002", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling is essentially a decision-making process that enables resource\nsharing among a number of activities by determining their execution order on\nthe set of available resources. The emergence of distributed systems brought\nnew challenges on scheduling in computer systems, including clusters, grids,\nand more recently clouds. On the other hand, the plethora of research makes it\nhard for both newcomers researchers to understand the relationship among\ndifferent scheduling problems and strategies proposed in the literature, which\nhampers the identification of new and relevant research avenues. In this paper\nwe introduce a classification of the scheduling problem in distributed systems\nby presenting a taxonomy that incorporates recent developments, especially\nthose in cloud computing. We review the scheduling literature to corroborate\nthe taxonomy and analyze the interest in different branches of the proposed\ntaxonomy. Finally, we identify relevant future directions in scheduling for\ndistributed systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 16:56:53 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Bittencourt", "Luiz F.", ""], ["Goldman", "Alfredo", ""], ["Madeira", "Edmundo R. M.", ""], ["da Fonseca", "Nelson L. S.", ""], ["Sakellariou", "Rizos", ""]]}, {"id": "1901.03582", "submitter": "Eva-Maria Hols", "authors": "Eva-Maria C. Hols, Stefan Kratsch", "title": "On Kernelization for Edge Dominating Set under Structural Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the NP-hard Edge Dominating Set problem (EDS) we are given a graph\n$G=(V,E)$ and an integer $k$, and need to determine whether there is a set\n$F\\subseteq E$ of at most $k$ edges that are incident with all (other) edges of\n$G$. It is known that this problem is fixed-parameter tractable and admits a\npolynomial kernel when parameterized by $k$. A caveat for this parameter is\nthat it needs to be large, i.e., at least equal to half the size of a maximum\nmatching of $G$, for instances not to be trivially negative. Motivated by this,\nwe study the existence of polynomial kernels for EDS when parameterized by\nstructural parameters that may be much smaller than $k$.\n  Unfortunately, at first glance this looks rather hopeless: Even when\nparameterized by the deletion distance to a disjoint union of paths $P_3$ of\nlength two there is no polynomial kernelization (under standard assumptions),\nruling out polynomial kernels for many smaller parameters like the feedback\nvertex set size. In contrast, somewhat surprisingly, there is a polynomial\nkernelization for deletion distance to a disjoint union of paths $P_5$ of\nlength four. As our main result, we fully classify for all finite sets\n$\\mathcal{H}$ of graphs, whether a kernel size polynomial in $|X|$ is possible\nwhen given $X$ such that each connected component of $G-X$ is isomorphic to a\ngraph in $\\mathcal{H}$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 13:57:33 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Hols", "Eva-Maria C.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1901.03615", "submitter": "Maximilian Probst", "authors": "Aaron Bernstein, Maximilian Probst, Christian Wulff-Nilsen", "title": "Decremental Strongly-Connected Components and Single-Source Reachability\n  in Near-Linear Time", "comments": "Accepted to STOC 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the Strongly-Connected Components (SCCs) in a graph $G=(V,E)$ is\nknown to take only $O(m + n)$ time using an algorithm by Tarjan from\n1972[SICOMP 72] where $m = |E|$, $n=|V|$. For fully-dynamic graphs, conditional\nlower bounds provide evidence that the update time cannot be improved by\npolynomial factors over recomputing the SCCs from scratch after every update.\nNevertheless, substantial progress has been made to find algorithms with fast\nupdate time for \\emph{decremental} graphs, i.e. graphs that undergo edge\ndeletions.\n  In this paper, we present the first algorithm for general decremental graphs\nthat maintains the SCCs in total update time $\\tilde{O}(m)$, thus only a\npolylogarithmic factor from the optimal running time. Previously such a result\nwas only known for the special case of planar graphs [Italiano et al, STOC\n2017]. Our result should be compared to the formerly best algorithm for general\ngraphs achieving $\\tilde{O}(m\\sqrt{n})$ total update time by Chechik et.al.\n[FOCS 16] which improved upon a breakthrough result leading to $O(mn^{0.9 +\no(1)})$ total update time by Henzinger, Krinninger and Nanongkai [STOC 14,\nICALP 15]; these results in turn improved upon the longstanding bound of\n$O(mn)$ by Roditty and Zwick [STOC 04].\n  All of the above results also apply to the decremental Single-Source\nReachability (SSR) problem, which can be reduced to decrementally maintaining\nSCCs. A bound of $O(mn)$ total update time for decremental SSR was established\nalready in 1981 by Even and Shiloach [JACM 1981].\n  Using a well known reduction, we can maintain the reachability of pairs $S\n\\times V$, $S \\subseteq V$ in fully-dynamic graphs with update time\n$\\tilde{O}(\\frac{|S|m}{t})$ and query time $O(t)$ for all $t \\in [1,|S|]$; this\ngeneralizes an earlier All-Pairs Reachability where $S = V$ [{\\L}\\k{a}cki, TALG\n2013].\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 15:37:37 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 00:40:04 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Bernstein", "Aaron", ""], ["Probst", "Maximilian", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1901.03627", "submitter": "Niels Gr\\\"uttemeier", "authors": "Niels Gr\\\"uttemeier, Christian Komusiewicz, Jannik Schestag, Frank\n  Sommer", "title": "Destroying Bicolored $P_3$s by Deleting Few Edges", "comments": "25 pages", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 23 no.\n  1, Graph Theory (June 8, 2021) dmtcs:7553", "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and study the Bicolored $P_3$ Deletion problem defined as\nfollows. The input is a graph $G=(V,E)$ where the edge set $E$ is partitioned\ninto a set $E_r$ of red edges and a set $E_b$ of blue edges. The question is\nwhether we can delete at most $k$ edges such that $G$ does not contain a\nbicolored $P_3$ as an induced subgraph. Here, a bicolored $P_3$ is a path on\nthree vertices with one blue and one red edge. We show that Bicolored $P_3$\nDeletion is NP-hard and cannot be solved in $2^{o(|V|+|E|)}$ time on\nbounded-degree graphs if the ETH is true. Then, we show that Bicolored $P_3$\nDeletion is polynomial-time solvable when $G$ does not contain a bicolored\n$K_3$, that is, a triangle with edges of both colors. Moreover, we provide a\npolynomial-time algorithm for the case that $G$ contains no blue $P_3$, red\n$P_3$, blue $K_3$, and red $K_3$. Finally, we show that Bicolored $P_3$\nDeletion can be solved in $ O(1.84^k\\cdot |V| \\cdot |E|)$ time and that it\nadmits a kernel with $ O(k\\Delta\\min(k,\\Delta))$ vertices, where $\\Delta$ is\nthe maximum degree of $G$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 16:05:23 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 14:14:29 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 13:20:54 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 11:35:39 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""], ["Schestag", "Jannik", ""], ["Sommer", "Frank", ""]]}, {"id": "1901.03689", "submitter": "Shahbaz Khan", "authors": "Shahbaz Khan and Shashank K. Mehta", "title": "Depth First Search in the Semi-streaming Model", "comments": "25 pages, 6 Figures, STACS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth first search (DFS) tree is a fundamental data structure for solving\nvarious graph problems. The classical DFS algorithm requires $O(m+n)$ time for\na graph having $n$ vertices and $m$ edges. In the streaming model, an algorithm\nis allowed several passes (preferably single) over the input graph having a\nrestriction on the size of local space used.\n  Trivially, a DFS tree can be computed using a single pass using $O(m)$ space.\nIn the semi-streaming model allowing $O(n)$ space, it can be computed in $O(n)$\npasses, where each pass adds one vertex to the DFS tree. However, it remains an\nopen problem to compute a DFS tree using $o(n)$ passes using $o(m)$ space even\nin any relaxed streaming environment.\n  We present the first semi-streaming algorithms that compute a DFS tree of an\nundirected graph in $o(n)$ passes using $o(m)$ space. We first describe an\nextremely simple algorithm that requires at most $\\lceil n/k\\rceil$ passes\nusing $O(nk)$ space, where $k$ is any positive integer. We then improve this\nalgorithm by using more involved techniques to reduce the number of passes to\n$\\lceil h/k\\rceil$ under similar space constraints, where $h$ is the height of\nthe computed DFS tree. In particular, this algorithm improves the bounds for\nthe case where the computed DFS tree is shallow (having $o(n)$ height).\nMoreover, this algorithm is presented as a framework that allows the\nflexibility of using any algorithm to maintain a DFS tree of a stored sparser\nsubgraph as a black box, which may be of independent interest. Both these\nalgorithms essentially demonstrate the existence of a trade-off between the\nspace and number of passes required for computing a DFS tree. Furthermore, we\nevaluate these algorithms experimentally which reveals their exceptional\nperformance in practice. For both random and real graphs, they require merely a\nfew passes even when allowed just $O(n)$ space.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 18:50:21 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Khan", "Shahbaz", ""], ["Mehta", "Shashank K.", ""]]}, {"id": "1901.03744", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, MohammadTaghi Hajiaghayi, David G. Harris", "title": "Exponentially Faster Massively Parallel Maximal Matching", "comments": "A preliminary version of this paper is to appear in the proceedings\n  of The 60th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of approximate matching in the Massively Parallel Computations\n(MPC) model has recently seen a burst of breakthroughs. Despite this progress,\nhowever, we still have a far more limited understanding of maximal matching\nwhich is one of the central problems of parallel and distributed computing. All\nknown MPC algorithms for maximal matching either take polylogarithmic time\nwhich is considered inefficient, or require a strictly super-linear space of\n$n^{1+\\Omega(1)}$ per machine.\n  In this work, we close this gap by providing a novel analysis of an extremely\nsimple algorithm a variant of which was conjectured to work by Czumaj et al.\n[STOC'18]. The algorithm edge-samples the graph, randomly partitions the\nvertices, and finds a random greedy maximal matching within each partition. We\nshow that this algorithm drastically reduces the vertex degrees. This, among\nsome other results, leads to an $O(\\log \\log \\Delta)$ round algorithm for\nmaximal matching with $O(n)$ space (or even mildly sublinear in $n$ using\nstandard techniques).\n  As an immediate corollary, we get a $2$ approximate minimum vertex cover in\nessentially the same rounds and space. This is the best possible approximation\nfactor under standard assumptions, culminating a long line of research. It also\nleads to an improved $O(\\log\\log \\Delta)$ round algorithm for $1 + \\varepsilon$\napproximate matching. All these results can also be implemented in the\ncongested clique model within the same number of rounds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 20:50:30 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 02:37:22 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 22:24:31 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Harris", "David G.", ""]]}, {"id": "1901.03783", "submitter": "Neal E. Young", "authors": "Marek Chrobak, Mordecai Golin, J. Ian Munro, Neal E. Young", "title": "On Huang and Wong's Algorithm for Generalized Binary Split Trees", "comments": "v3 has updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huang and Wong [1984] proposed a polynomial-time dynamic-programming\nalgorithm for computing optimal generalized binary split trees. We show that\ntheir algorithm is incorrect. Thus, it remains open whether such trees can be\ncomputed in polynomial time.\n  Spuler [1994] proposed modifying Huang and Wong's algorithm to obtain an\nalgorithm for a different problem: computing optimal two-way-comparison search\ntrees. We show that the dynamic program underlying Spuler's algorithm is not\nvalid, in that it does not satisfy the necessary optimal-substructure property\nand its proposed recurrence relation is incorrect. It remains unknown whether\nthe algorithm is guaranteed to compute a correct overall solution.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 02:36:21 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 21:49:35 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 17:02:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chrobak", "Marek", ""], ["Golin", "Mordecai", ""], ["Munro", "J. Ian", ""], ["Young", "Neal E.", ""]]}, {"id": "1901.03931", "submitter": "Gamal Sallam", "authors": "Gamal Sallam, Bo Ji", "title": "Joint Placement and Allocation of VNF Nodes with Budget and Capacity\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Network Function Virtualization (NFV), network services\nthat traditionally run on proprietary dedicated hardware can now be realized\nusing Virtual Network Functions (VNFs) that are hosted on general-purpose\ncommodity hardware. This new network paradigm offers a great flexibility to\nInternet service providers (ISPs) for efficiently operating their networks\n(collecting network statistics, enforcing management policies, etc.). However,\nintroducing NFV requires an investment to deploy VNFs at certain network nodes\n(called VNF-nodes), which has to account for practical constraints such as the\ndeployment budget and the VNF-node capacity. To that end, it is important to\ndesign a joint VNF-nodes placement and capacity allocation algorithm that can\nmaximize the total amount of network flows that are fully processed by the\nVNF-nodes while respecting such practical constraints. In contrast to most\nprior work that often neglects either the budget constraint or the capacity\nconstraint, we explicitly consider both of them. We prove that accounting for\nthese constraints introduces several new challenges. Specifically, we prove\nthat the studied problem is not only NP-hard but also non-submodular. To\naddress these challenges, we introduce a novel relaxation method such that the\nobjective function of the relaxed placement subproblem becomes submodular.\nLeveraging this useful submodular property, we propose two algorithms that\nachieve an approximation ratio of $\\frac{1}{2}(1-1/e)$ and $\\frac{1}{3}(1-1/e)$\nfor the original non-relaxed problem, respectively. Finally, we corroborate the\neffectiveness of the proposed algorithms through extensive evaluations using\ntrace-driven simulations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 04:11:47 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 12:23:01 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 05:33:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sallam", "Gamal", ""], ["Ji", "Bo", ""]]}, {"id": "1901.04008", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel, Neta Dafni, Victor I. Kolobov, Ami Paz, Gregory\n  Schwartzman", "title": "Fast Deterministic Algorithms for Highly-Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an algorithmic framework for obtaining fast distributed\nalgorithms for a highly-dynamic setting, in which *arbitrarily many* edge\nchanges may occur in each round. Our algorithm significantly improves upon\nprior work in its combination of (1) having an $O(1)$ amortized time\ncomplexity, (2) using only $O(\\log{n})$-bit messages, (3) not posing any\nrestrictions on the dynamic behavior of the environment, (4) being\ndeterministic, (5) having strong guarantees for intermediate solutions, and (6)\nbeing applicable for a wide family of tasks.\n  The tasks for which we deduce such an algorithm are maximal matching,\n$(degree+1)$-coloring, 2-approximation for minimum weight vertex cover, and\nmaximal independent set (which is the most subtle case). For some of these\ntasks, node insertions can also be among the allowed topology changes, and for\nsome of them also abrupt node deletions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 16:11:22 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 20:09:12 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 16:16:11 GMT"}, {"version": "v4", "created": "Sun, 23 Feb 2020 21:46:40 GMT"}, {"version": "v5", "created": "Sun, 11 Oct 2020 14:00:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dafni", "Neta", ""], ["Kolobov", "Victor I.", ""], ["Paz", "Ami", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1901.04068", "submitter": "Evangelos Kipouridis", "authors": "Evangelos Kipouridis, and Kostas Tsichlas", "title": "Longest Common Subsequence on Weighted Sequences", "comments": "This is an updated version of the paper accepted at CPM 2020 (Best\n  Paper Award). Includes all proofs that did not fit the page limit, and\n  corrections to some typos", "journal-ref": null, "doi": "10.4230/LIPIcs.CPM.2020.19", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of the Longest Common Subsequence (LCS) on\nweighted sequences. Weighted sequences are an extension of classical strings,\nwhere in each position every letter of the alphabet may occur with some\nprobability. Previous results presented a PTAS and noticed that no FPTAS is\npossible unless P=NP. In this paper we essentially close the gap between upper\nand lower bounds by improving both. First of all, we provide an EPTAS for\nbounded alphabets (which is the most natural case), and prove that there does\nnot exist any EPTAS for unbounded alphabets unless FPT=W[1]. Furthermore, under\nthe Exponential Time Hypothesis, we provide a lower bound which shows that no\nsignificantly better PTAS can exist for unbounded alphabets. As a side note, we\nprove that it is sufficient to work with only one threshold in the general\nvariant of the problem.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 21:28:35 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:42:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kipouridis", "Evangelos", ""], ["Tsichlas", "Kostas", ""]]}, {"id": "1901.04272", "submitter": "Alexander Birx", "authors": "Alexander Birx and Yann Disser", "title": "Tight Analysis of the Smartstart Algorithm for Online Dial-a-Ride on the\n  Line", "comments": "STACS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online Dial-a-Ride problem is a fundamental online problem in a metric\nspace, where transportation requests appear over time and may be served in any\norder by a single server with unit speed. Restricted to the real line, online\nDial-a-Ride captures natural problems like controlling a personal elevator.\nTight results in terms of competitive ratios are known for the general setting\nand for online TSP on the line (where source and target of each request\ncoincide). In contrast, online Dial-a-Ride on the line has resisted tight\nanalysis so far, even though it is a very natural online problem. We conduct a\ntight competitive analysis of the Smartstart algorithm that gave the best known\nresults for the general, metric case. In particular, our analysis yields a new\nupper bound of 2.94 for open, non-preemptive online Dial-a-Ride on the line,\nwhich improves the previous bound of 3.41 [Krumke'00]. The best known lower\nbound remains 2.04 [SODA'17]. We also show that the known upper bound of 2\n[STACS'00] regarding Smartstart's competitive ratio for closed, non-preemptive\nonline Dial-a-Ride is tight on the line.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 12:58:51 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Birx", "Alexander", ""], ["Disser", "Yann", ""]]}, {"id": "1901.04358", "submitter": "Marius Lombard-Platet", "authors": "R\\'emi G\\'eraud, Marius Lombard-Platet, David Naccache", "title": "Quotient Hash Tables - Efficiently Detecting Duplicates in Streaming\n  Data", "comments": "Shorter version was accepted at SIGAPP SAC '19", "journal-ref": null, "doi": "10.1145/3297280.3297335", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the Quotient Hash Table (QHT) a new data structure for\nduplicate detection in unbounded streams. QHTs stem from a corrected analysis\nof streaming quotient filters (SQFs), resulting in a 33\\% reduction in memory\nusage for equal performance. We provide a new and thorough analysis of both\nalgorithms, with results of interest to other existing constructions.\n  We also introduce an optimised version of our new data structure dubbed\nQueued QHT with Duplicates (QQHTD).\n  Finally we discuss the effect of adversarial inputs for hash-based duplicate\nfilters similar to QHT.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 15:08:16 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["G\u00e9raud", "R\u00e9mi", ""], ["Lombard-Platet", "Marius", ""], ["Naccache", "David", ""]]}, {"id": "1901.04372", "submitter": "Mohammad Hajiesmaili", "authors": "Lin Yang, Mohammad H. Hajiesmaili, Ramesh Sitaraman, Enrique Mallada,\n  Wing S. Wong, Adam Wierman", "title": "Online Inventory Management with Application to Energy Procurement in\n  Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the application of energy storage management in electricity\nmarkets, this paper considers the problem of online linear programming with\ninventory management constraints. Specifically, a decision maker should satisfy\nsome units of an asset as her demand, either form a market with time-varying\nprice or from her own inventory. The decision maker is presented a price in\nslot-by-slot manner, and must immediately decide the purchased amount with the\ncurrent price to cover the demand or to store in inventory for covering the\nfuture demand. The inventory has a limited capacity and its critical role is to\nbuy and store assets at low price and use the stored assets to cover the demand\nat high price. The ultimate goal of the decision maker is to cover the demands\nwhile minimizing the cost of buying assets from the market. We propose BatMan,\nan online algorithm for simple inventory models, and BatManRate, an extended\nversion for the case with rate constraints. Both BatMan and BatManRate achieve\noptimal competitive ratios, meaning that no other online algorithm can achieve\na better theoretical guarantee. To illustrate the results, we use the proposed\nalgorithms to design and evaluate energy procurement and storage management\nstrategies for data centers with a portfolio of energy sources including the\nelectric grid, local renewable generation, and energy storage systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:08:05 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Yang", "Lin", ""], ["Hajiesmaili", "Mohammad H.", ""], ["Sitaraman", "Ramesh", ""], ["Mallada", "Enrique", ""], ["Wong", "Wing S.", ""], ["Wierman", "Adam", ""]]}, {"id": "1901.04583", "submitter": "Rik Timmerman", "authors": "R. W. Timmerman and M. A. A. Boon", "title": "Platoon Forming Algorithms for Intelligent Street Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study intersection access control for autonomous vehicles. Platoon forming\nalgorithms, which aim to organize individual vehicles in platoons, are very\npromising. To create those platoons, we slow down vehicles before the actual\narrival at the intersection in such a way that each vehicle can traverse the\nintersection at high speed. This increases the capacity of the intersection\nsignificantly, offering huge potential savings with respect to travel time\ncompared to nowadays traffic.\n  We propose several new platoon forming algorithms and provide an approximate\nmean delay analysis for our algorithms. A comparison between the current day\npractice at intersections (through a case study in SUMO) and our proposed\nalgorithms is provided. Simulation results for fairness are obtained as well,\nshowing that platoon forming algorithms with a low mean delay sometimes are\nrelatively unfair, indicating a potential need for balancing mean delay and\nfairness.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 07:08:56 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 14:32:50 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Timmerman", "R. W.", ""], ["Boon", "M. A. A.", ""]]}, {"id": "1901.04628", "submitter": "Yicheng Xu", "authors": "Yicheng Xu, Rolf H. M\\\"ohring, Dachuan Xu, Yong Zhang, Yifei Zou", "title": "A constant FPT approximation algorithm for hard-capacitated k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hard-capacitated $k$-means (HCKM) is one of the fundamental problems\nremaining open in combinatorial optimization and data mining areas. In this\nproblem, one is required to partition a given $n$-point set into $k$ disjoint\nclusters with known capacity so as to minimize the sum of within-cluster\nvariances. It is known to be at least APX-hard and for which most of the work\nis from a meta heuristic perspective. To the best our knowledge, no constant\napproximation algorithm or existence proof of such an algorithm is known. As\nour main contribution, we propose an FPT($k$) algorithm with performance\nguarantee of $69+\\epsilon$ for any HCKM instances in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 01:30:19 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 01:16:08 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 01:26:25 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Xu", "Yicheng", ""], ["M\u00f6hring", "Rolf H.", ""], ["Xu", "Dachuan", ""], ["Zhang", "Yong", ""], ["Zou", "Yifei", ""]]}, {"id": "1901.04911", "submitter": "Yuri Gurevich", "authors": "Yuri Gurevich", "title": "Unconstrained Church-Turing thesis cannot possibly be true", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL cs.DS math.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Church-Turing thesis asserts that if a partial strings-to-strings\nfunction is effectively computable then it is computable by a Turing machine.\n  In the 1930s, when Church and Turing worked on their versions of the thesis,\nthere was a robust notion of algorithm. These traditional algorithms are known\nalso as classical or sequential. In the original thesis, effectively computable\nmeant computable by an effective classical algorithm. Based on an earlier\naxiomatization of classical algorithms, the original thesis was proven in 2008.\n  Since the 1930s, the notion of algorithm has changed dramatically. New\nspecies of algorithms have been and are being introduced. We argue that the\ngeneralization of the original thesis, where effectively computable means\ncomputable by an effective algorithm of any species, cannot possibly be true.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 16:24:07 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Gurevich", "Yuri", ""]]}, {"id": "1901.04934", "submitter": "George Bissias", "authors": "George Bissias", "title": "An Algorithm for Bounding the Probability of r-core Formation in\n  k-uniform Random Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for bounding the probability of r-core formation in\nk-uniform hypergraphs. Understanding the probability of core formation is\nuseful in numerous applications including bounds on the failure rate of\nInvertible Bloom Lookup Tables (IBLTs) and the probability that a boolean\nformula is satisfiable.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 16:57:17 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Bissias", "George", ""]]}, {"id": "1901.05028", "submitter": "Alberto Vera", "authors": "Alberto Vera and Siddhartha Banerjee", "title": "The Bayesian Prophet: A Low-Regret Framework for Online Decision Making", "comments": "Extended abstract appeared in SIGMETRICS 2019. Accepted by Management\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new framework for designing online policies given access to an\noracle providing statistical information about an offline benchmark. Having\naccess to such prediction oracles enables simple and natural Bayesian selection\npolicies, and raises the question as to how these policies perform in different\nsettings.\n  Our work makes two important contributions towards this question: First, we\ndevelop a general technique we call *compensated coupling* which can be used to\nderive bounds on the expected regret (i.e., additive loss with respect to a\nbenchmark) for any online policy and offline benchmark. Second, using this\ntechnique, we show that a natural greedy policy, which we call *the Bayes\nSelector*, has constant expected regret (i.e., independent of the number of\narrivals and resource levels) for a large class of problems we refer to as\nOnline Allocation with finite types, which includes widely-studied Online\nPacking and Online Matching problems. Our results generalize and simplify\nseveral existing results for Online Packing and Online Matching, and suggest a\npromising pathway for obtaining oracle-driven policies for other online\ndecision-making settings.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 19:45:09 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 20:38:34 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Vera", "Alberto", ""], ["Banerjee", "Siddhartha", ""]]}, {"id": "1901.05149", "submitter": "Guangmo Tong", "authors": "Gunagmo Tong, Ding-Zhu Du", "title": "Beyond Uniform Reverse Sampling: A Hybrid Sampling Technique for\n  Misinformation Prevention", "comments": "New parameter estimation methods have been proposed to fix an error\n  in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online misinformation has been considered as one of the top global risks as\nit may cause serious consequences such as economic damages and public panic.\nThe misinformation prevention problem aims at generating a positive cascade\nwith appropriate seed nodes in order to compete against the misinformation. In\nthis paper, we study the misinformation prevention problem under the prominent\nindependent cascade model. Due to the #P-hardness in computing influence, the\ncore problem is to design effective sampling methods to estimate the function\nvalue. The main contribution of this paper is a novel sampling method.\nDifferent from the classic reverse sampling technique which treats all nodes\nequally and samples the node uniformly, the proposed method proceeds with a\nhybrid sampling process which is able to attach high weights to the users who\nare prone to be affected by the misinformation. Consequently, the new sampling\nmethod is more powerful in generating effective samples used for computing seed\nnodes for the positive cascade. Based on the new hybrid sample technique, we\ndesign an algorithm offering a $(1-1/e-\\epsilon)$-approximation. We\nexperimentally evaluate the proposed method on extensive datasets and show that\nit significantly outperforms the state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 06:24:37 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 20:56:38 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tong", "Gunagmo", ""], ["Du", "Ding-Zhu", ""]]}, {"id": "1901.05226", "submitter": "Nicola Prezza", "authors": "Nicola Prezza and Giovanna Rosone", "title": "Space-Efficient Computation of the LCP Array from the Burrows-Wheeler\n  Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Longest Common Prefix Array of a text collection of total\nsize n on alphabet [1, {\\sigma}] can be computed from the Burrows-Wheeler\ntransformed collection in O(n log {\\sigma}) time using o(n log {\\sigma}) bits\nof working space on top of the input and output. Our result improves (on small\nalphabets) and generalizes (to string collections) the previous solution from\nBeller et al., which required O(n) bits of extra working space. We also show\nhow to merge the BWTs of two collections of total size n within the same time\nand space bounds. The procedure at the core of our algorithms can be used to\nenumerate suffix tree intervals in succinct space from the BWT, which is of\nindependent interest. An engineered implementation of our first algorithm on\nDNA alphabet induces the LCP of a large (16 GiB) collection of short (100\nbases) reads at a rate of 2.92 megabases per second using in total 1.5 Bytes\nper base in RAM. Our second algorithm merges the BWTs of two short-reads\ncollections of 8 GiB each at a rate of 1.7 megabases per second and uses 0.625\nBytes per base in RAM. An extension of this algorithm that computes also the\nLCP array of the merged collection processes the data at a rate of 1.48\nmegabases per second and uses 1.625 Bytes per base in RAM.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 10:53:33 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 08:38:35 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 17:00:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Prezza", "Nicola", ""], ["Rosone", "Giovanna", ""]]}, {"id": "1901.05264", "submitter": "Massimo Equi", "authors": "Massimo Equi, Roberto Grossi, Veli M\\\"akinen", "title": "On the Complexity of Exact Pattern Matching in Graphs: Binary Strings\n  and Bounded Degree", "comments": "Using Lemma 12 and Lemma 13 might to be enough to prove Lemma 14.\n  However, the proof of Lemma 14 is correct if you assume that the graph used\n  in the reduction is a DAG. Hence, since the problem is already quadratic for\n  a DAG and a binary alphabet, it has to be quadratic also for a general graph\n  and a binary alphabet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact pattern matching in labeled graphs is the problem of searching paths of\na graph $G=(V,E)$ that spell the same string as the pattern $P[1..m]$. This\nbasic problem can be found at the heart of more complex operations on variation\ngraphs in computational biology, of query operations in graph databases, and of\nanalysis operations in heterogeneous networks, where the nodes of some paths\nmust match a sequence of labels or types. We describe a simple conditional\nlower bound that, for any constant $\\epsilon>0$, an $O(|E|^{1 - \\epsilon} \\,\nm)$-time or an $O(|E| \\, m^{1 - \\epsilon})$-time algorithm for exact pattern\nmatching on graphs, with node labels and patterns drawn from a binary alphabet,\ncannot be achieved unless the Strong Exponential Time Hypothesis (SETH) is\nfalse. The result holds even if restricted to undirected graphs of maximum\ndegree three or directed acyclic graphs of maximum sum of indegree and\noutdegree three. Although a conditional lower bound of this kind can be somehow\nderived from previous results (Backurs and Indyk, FOCS'16), we give a direct\nreduction from SETH for dissemination purposes, as the result might interest\nresearchers from several areas, such as computational biology, graph database,\nand graph mining, as mentioned before. Indeed, as approximate pattern matching\non graphs can be solved in $O(|E|\\,m)$ time, exact and approximate matching are\nthus equally hard (quadratic time) on graphs under the SETH assumption. In\ncomparison, the same problems restricted to strings have linear time vs\nquadratic time solutions, respectively, where the latter ones have a matching\nSETH lower bound on computing the edit distance of two strings (Backurs and\nIndyk, STOC'15).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 12:59:07 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 16:33:56 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 07:21:57 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Equi", "Massimo", ""], ["Grossi", "Roberto", ""], ["M\u00e4kinen", "Veli", ""]]}, {"id": "1901.05316", "submitter": "Yann Strozecki", "authors": "David Auger, Pierre Coucheney, Yann Strozecki", "title": "Solving Simple Stochastic Games with few Random Nodes faster using\n  Bland's Rule", "comments": "Article accepted at STACS 2019, include the proofs in the annexe", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best algorithm so far for solving Simple Stochastic Games is Ludwig's\nrandomized algorithm which works in expected $2^{O(\\sqrt{n})}$ time. We first\ngive a simpler iterative variant of this algorithm, using Bland's rule from the\nsimplex algorithm, which uses exponentially less random bits than Ludwig's\nversion. Then, we show how to adapt this method to the algorithm of Gimbert and\nHorn whose worst case complexity is $O(k!)$, where $k$ is the number of random\nnodes. Our algorithm has an expected running time of $2^{O(k)}$, and works for\ngeneral random nodes with arbitrary outdegree and probability distribution on\noutgoing arcs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 14:39:51 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Auger", "David", ""], ["Coucheney", "Pierre", ""], ["Strozecki", "Yann", ""]]}, {"id": "1901.05549", "submitter": "Bernardo Lopo Tavares", "authors": "Bernardo Lopo Tavares", "title": "An analysis of the Geodesic Distance and other comparative metrics for\n  tree-like structures", "comments": "62 pages, 16 figures. Author's MSc Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphs are interesting structures: extremely useful to depict real-life\nproblems, extremely easy to understand given a sketch, extremely complicated to\nrepresent formally, extremely complicated to compare. Phylogeny is the study of\nthe relations between biological entities. From it, the interest in comparing\ntree graphs grew more than in other fields of science. Since there is no\ndefinitive way to compare them, multiple distances were formalized over the\nyears since the early sixties, when the first effective numerical method to\ncompare dendrograms was described. This work consists of formalizing,\ncompleting (with original work) and give a universal notation to analyze and\ncompare the discriminatory power and time complexity of computing the thirteen\nhere formalized metrics. We also present a new way to represent tree graphs,\nreach deeper in the details of the Geodesic Distance and discuss its worst-case\ntime complexity in a suggested implementation. Our contribution ends up as a\nclean, valuable resource for anyone looking for an introduction to comparative\nmetrics for tree graphs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 22:26:19 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Tavares", "Bernardo Lopo", ""]]}, {"id": "1901.05620", "submitter": "James Allen Fill", "authors": "James Allen Fill, Daniel Q. Naiman", "title": "The Pareto Record Frontier", "comments": "3 figures; small change to abstract, and proof of Theorem 1.4\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For iid $d$-dimensional observations $X^{(1)}, X^{(2)}, \\ldots$ with\nindependent Exponential$(1)$ coordinates, consider the boundary (relative to\nthe closed positive orthant), or \"frontier\", $F_n$ of the closed Pareto\nrecord-setting (RS) region \\[ \\mbox{RS}_n := \\{0 \\leq x \\in {\\mathbb R}^d: x\n\\not\\prec X^{(i)}\\ \\mbox{for all $1 \\leq i \\leq n$}\\} \\] at time $n$, where $0\n\\leq x$ means that $0 \\leq x_j$ for $1 \\leq j \\leq d$ and $x \\prec y$ means\nthat $x_j < y_j$ for $1 \\leq j \\leq d$. With $x_+ := \\sum_{j = 1}^d x_j$, let\n\\[ F_n^- := \\min\\{x_+: x \\in F_n\\} \\quad \\mbox{and} \\quad F_n^+ := \\max\\{x_+: x\n\\in F_n\\}, \\] and define the width of $F_n$ as \\[ W_n := F_n^+ - F_n^-. \\] We\ndescribe typical and almost sure behavior of the processes $F^+$, $F^-$, and\n$W$. In particular, we show that $F^+_n \\sim \\ln n \\sim F^-_n$ almost surely\nand that $W_n / \\ln \\ln n$ converges in probability to $d - 1$; and for $d \\geq\n2$ we show that, almost surely, the set of limit points of the sequence $W_n /\n\\ln \\ln n$ is the interval $[d - 1, d]$.\n  We also obtain modifications of our results that are important in connection\nwith efficient simulation of Pareto records. Let $T_m$ denote the time that the\n$m$th record is set. We show that $F^+_{T_m} \\sim (d! m)^{1/d} \\sim F^-_{T_m}$\nalmost surely and that $W_{T_m} / \\ln m$ converges in probability to $1 -\nd^{-1}$; and for $d \\geq 2$ we show that, almost surely, the sequence $W_{T_m}\n/ \\ln m$ has $\\liminf$ equal to $1 - d^{-1}$ and $\\limsup$ equal to $1$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 05:03:26 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 07:25:58 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Fill", "James Allen", ""], ["Naiman", "Daniel Q.", ""]]}, {"id": "1901.05621", "submitter": "James Allen Fill", "authors": "James Allen Fill, Daniel Q. Naiman", "title": "Generating Pareto records", "comments": "3 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present, (partially) analyze, and apply an efficient algorithm for the\nsimulation of multivariate Pareto records. A key role is played by minima of\nthe record-setting region (we call these generators) each time a new record is\ngenerated, and two highlights of our work are (i) efficient dynamic maintenance\nof the set of generators and (ii) asymptotic analysis of the expected number of\ngenerators at each time.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 05:18:56 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Fill", "James Allen", ""], ["Naiman", "Daniel Q.", ""]]}, {"id": "1901.05797", "submitter": "Pauli Miettinen", "authors": "Nikolaj Tatti and Pauli Miettinen", "title": "Boolean matrix factorization meets consecutive ones property", "comments": "13 pages, 6 figures. To appear in 2019 SIAM International Conference\n  on Data Mining (SDM19). For the associated source code, see\n  https://cs.uef.fi/~pauli/bmf/ordered_bmf/", "journal-ref": "Proc. 2019 SIAM International Conference on Data Mining (SDM19),\n  2019, 729-737", "doi": "10.1137/1.9781611975673.82", "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean matrix factorization is a natural and a popular technique for\nsummarizing binary matrices. In this paper, we study a problem of Boolean\nmatrix factorization where we additionally require that the factor matrices\nhave consecutive ones property (OBMF). A major application of this optimization\nproblem comes from graph visualization: standard techniques for visualizing\ngraphs are circular or linear layout, where nodes are ordered in circle or on a\nline. A common problem with visualizing graphs is clutter due to too many\nedges. The standard approach to deal with this is to bundle edges together and\nrepresent them as ribbon. We also show that we can use OBMF for edge bundling\ncombined with circular or linear layout techniques.\n  We demonstrate that not only this problem is NP-hard but we cannot have a\npolynomial-time algorithm that yields a multiplicative approximation guarantee\n(unless P = NP). On the positive side, we develop a greedy algorithm where at\neach step we look for the best 1-rank factorization. Since even obtaining\n1-rank factorization is NP-hard, we propose an iterative algorithm where we fix\none side and and find the other, reverse the roles, and repeat. We show that\nthis step can be done in linear time using pq-trees. We also extend the problem\nto cyclic ones property and symmetric factorizations. Our experiments show that\nour algorithms find high-quality factorizations and scale well.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:16:55 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Miettinen", "Pauli", ""]]}, {"id": "1901.05917", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan", "title": "Tight Bounds on the Minimum Size of a Dynamic Monopoly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that you are given a graph $G=(V,E)$ with an initial coloring, where\neach node is black or white. Then, in discrete-time rounds all nodes\nsimultaneously update their color following a predefined deterministic rule.\nThis process is called two-way $r$-bootstrap percolation, for some integer $r$,\nif a node with at least $r$ black neighbors gets black and white otherwise.\nSimilarly, in two-way $\\alpha$-bootstrap percolation, for some $0<\\alpha<1$, a\nnode gets black if at least $\\alpha$ fraction of its neighbors are black, and\nwhite otherwise. The two aforementioned processes are called respectively\n$r$-bootstrap and $\\alpha$-bootstrap percolation if we require that a black\nnode stays black forever. For each of these processes, we say a node set $D$ is\na dynamic monopoly whenever the following holds: If all nodes in $D$ are black\nthen the graph gets fully black eventually. We provide tight upper and lower\nbounds on the minimum size of a dynamic monopoly.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 08:57:22 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Zehmakan", "Ahad N.", ""]]}, {"id": "1901.05925", "submitter": "Kasra Khosoussi", "authors": "Yulun Tian, Kasra Khosoussi, Jonathan P. How", "title": "Resource-Aware Algorithms for Distributed Loop Closure Detection with\n  Provable Performance Guarantees", "comments": "International Workshop on the Algorithmic Foundations of Robotics\n  (WAFR) 2018 (Extended Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-robot loop closure detection, e.g., for collaborative simultaneous\nlocalization and mapping (CSLAM), is a fundamental capability for many\nmultirobot applications in GPS-denied regimes. In real-world scenarios, this is\na resource-intensive process that involves exchanging observations and\nverifying potential matches. This poses severe challenges especially for\nsmall-size and low-cost robots with various operational and resource\nconstraints that limit, e.g., energy consumption, communication bandwidth, and\ncomputation capacity. This paper presents resource-aware algorithms for\ndistributed inter-robot loop closure detection. In particular, we seek to\nselect a subset of potential inter-robot loop closures that maximizes a\nmonotone submodular performance metric without exceeding computation and\ncommunication budgets. We demonstrate that this problem is in general NP-hard,\nand present efficient approximation algorithms with provable performance\nguarantees. A convex relaxation scheme is used to certify near-optimal\nperformance of the proposed framework in real and synthetic SLAM benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 17:44:10 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Tian", "Yulun", ""], ["Khosoussi", "Kasra", ""], ["How", "Jonathan P.", ""]]}, {"id": "1901.06080", "submitter": "Yuan Guo", "authors": "Yuan Guo, Jennifer Dy, Deniz Erdogmus, Jayashree Kalpathy-Cramer,\n  Susan Ostmo, J. Peter Campbell, Michael F. Chiang, Stratis Ioannidis", "title": "Accelerated Experimental Design for Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison labels are more informative and less variable than class\nlabels, but generating them poses a challenge: their number grows quadratically\nin the dataset size. We study a natural experimental design objective, namely,\nD-optimality, that can be used to identify which $K$ pairwise comparisons to\ngenerate. This objective is known to perform well in practice, and is\nsubmodular, making the selection approximable via the greedy algorithm. A\nna\\\"ive greedy implementation has $O(N^2d^2K)$ complexity, where $N$ is the\ndataset size, $d$ is the feature space dimension, and $K$ is the number of\ngenerated comparisons. We show that, by exploiting the inherent geometry of the\ndataset--namely, that it consists of pairwise comparisons--the greedy\nalgorithm's complexity can be reduced to $O(N^2(K+d)+N(dK+d^2) +d^2K).$ We\napply the same acceleration also to the so-called lazy greedy algorithm. When\ncombined, the above improvements lead to an execution time of less than 1 hour\nfor a dataset with $10^8$ comparisons; the na\\\"ive greedy algorithm on the same\ndataset would require more than 10 days to terminate.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 04:17:25 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Guo", "Yuan", ""], ["Dy", "Jennifer", ""], ["Erdogmus", "Deniz", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Ostmo", "Susan", ""], ["Campbell", "J. Peter", ""], ["Chiang", "Michael F.", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "1901.06482", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Nhat Ho, Michael I. Jordan", "title": "On Efficient Optimal Transport: An Analysis of Greedy and Accelerated\n  Mirror Descent Algorithms", "comments": "Derive the explicit dual objective function for APDAMD (Remark 4.2)\n  which satisfies Lemma~4.1; Accepted by ICML 2019; The longer version is\n  available here: arXiv:1906.01437", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical analyses for two algorithms that solve the regularized\noptimal transport (OT) problem between two discrete probability measures with\nat most $n$ atoms. We show that a greedy variant of the classical Sinkhorn\nalgorithm, known as the \\emph{Greenkhorn algorithm}, can be improved to\n$\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$, improving on the best known\ncomplexity bound of $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-3})$. Notably,\nthis matches the best known complexity bound for the Sinkhorn algorithm and\nhelps explain why the Greenkhorn algorithm can outperform the Sinkhorn\nalgorithm in practice. Our proof technique, which is based on a primal-dual\nformulation and a novel upper bound for the dual solution, also leads to a new\nclass of algorithms that we refer to as \\emph{adaptive primal-dual accelerated\nmirror descent} (APDAMD) algorithms. We prove that the complexity of these\nalgorithms is $\\widetilde{\\mathcal{O}}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$,\nwhere $\\delta > 0$ refers to the inverse of the strong convexity module of\nBregman divergence with respect to $\\|\\cdot\\|_\\infty$. This implies that the\nAPDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms\nof $\\varepsilon$. Experimental results on synthetic and real datasets\ndemonstrate the favorable performance of the Greenkhorn and APDAMD algorithms\nin practice.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 08:31:29 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 08:33:22 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 04:20:21 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 06:37:14 GMT"}, {"version": "v5", "created": "Sat, 17 Jul 2021 06:06:58 GMT"}, {"version": "v6", "created": "Mon, 26 Jul 2021 15:20:29 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lin", "Tianyi", ""], ["Ho", "Nhat", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1901.06493", "submitter": "Amitabha Bagchi", "authors": "Sidharth Negi, Ameya Dubey, Amitabha Bagchi, Manish Yadav, Nishant\n  Yadav, Jeetu Raj", "title": "Dynamic Partition Bloom Filters: A Bounded False Positive Solution For\n  Dynamic Set Membership (Extended Abstract)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Bloom filters (DBF) were proposed by Guo et. al. in 2010 to tackle\nthe situation where the size of the set to be stored compactly is not known in\nadvance or can change during the course of the application. We propose a novel\ncompetitor to DBF with the following important property that DBF is not able to\nachieve: our structure is able to maintain a bound on the false positive rate\nfor the set membership query across all possible sizes of sets that are stored\nin it. The new data structure we propose is a dynamic structure that we call\nDynamic Partition Bloom filter (DPBF). DPBF is based on our novel concept of a\nBloom partition tree which is a tree structure with standard Bloom filters at\nthe leaves. DPBF is superior to standard Bloom filters because it can\nefficiently handle a large number of unions and intersections of sets of\ndifferent sizes while controlling the false positive rate. This makes DPBF the\nfirst structure to do so to the best of our knowledge. We provide theoretical\nbounds comparing the false positive probability of DPBF to DBF.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 09:53:29 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Negi", "Sidharth", ""], ["Dubey", "Ameya", ""], ["Bagchi", "Amitabha", ""], ["Yadav", "Manish", ""], ["Yadav", "Nishant", ""], ["Raj", "Jeetu", ""]]}, {"id": "1901.06581", "submitter": "Fatemeh Navidi", "authors": "Inge Li G{\\o}rtz, Viswanath Nagarajan and Fatemeh Navidi", "title": "Approximation Algorithms for the A Priori TravelingRepairman", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the a priori traveling repairman problem, which is a stochastic\nversion of the classic traveling repairman problem (also called the traveling\ndeliveryman or minimum latency problem). Given a metric $(V,d)$ with a root\n$r\\in V$, the traveling repairman problem (TRP) involves finding a tour\noriginating from $r$ that minimizes the sum of arrival-times at all vertices.\nIn its a priori version, we are also given independent probabilities of each\nvertex being active. We want to find a master tour $\\tau$ originating from $r$\nand visiting all vertices. The objective is to minimize the expected sum of\narrival-times at all active vertices, when $\\tau$ is shortcut over the inactive\nvertices. We obtain the first constant-factor approximation algorithm for a\npriori TRP under non-uniform probabilities. Previously, such a result was only\nknown for uniform probabilities.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 19:53:52 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["G\u00f8rtz", "Inge Li", ""], ["Nagarajan", "Viswanath", ""], ["Navidi", "Fatemeh", ""]]}, {"id": "1901.06628", "submitter": "Ashish Dwivedi", "authors": "Ashish Dwivedi, Rajat Mittal and Nitin Saxena", "title": "Efficiently factoring polynomials modulo $p^4$", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.SC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial factoring has famous practical algorithms over fields-- finite,\nrational \\& $p$-adic. However, modulo prime powers it gets hard as there is\nnon-unique factorization and a combinatorial blowup ensues. For example, $x^2+p\n\\bmod p^2$ is irreducible, but $x^2+px \\bmod p^2$ has exponentially many\nfactors! We present the first randomized poly(deg $f, \\log p$) time algorithm\nto factor a given univariate integral $f(x)$ modulo $p^k$, for a prime $p$ and\n$k \\leq 4$. Thus, we solve the open question of factoring modulo $p^3$ posed in\n(Sircana, ISSAC'17).\n  Our method reduces the general problem of factoring $f(x) \\bmod p^k$ to that\nof {\\em root finding} in a related polynomial $E(y) \\bmod\\langle p^k,\n\\varphi(x)^\\ell \\rangle$ for some irreducible $\\varphi \\bmod p$. We could\nefficiently solve the latter for $k\\le4$, by incrementally transforming $E(y)$.\nMoreover, we discover an efficient and strong generalization of Hensel lifting\nto lift factors of $f(x) \\bmod p$ to those $\\bmod\\ p^4$ (if possible). This was\npreviously unknown, as the case of repeated factors of $f(x) \\bmod p$ forbids\nclassical Hensel lifting.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 06:40:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Dwivedi", "Ashish", ""], ["Mittal", "Rajat", ""], ["Saxena", "Nitin", ""]]}, {"id": "1901.06653", "submitter": "James Stewart", "authors": "Zongchen Chen, Andreas Galanis, Leslie Ann Goldberg, Will Perkins,\n  James Stewart and Eric Vigoda", "title": "Fast algorithms at low temperatures via Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a discrete-time Markov chain for abstract polymer models and show\nthat under sufficient decay of the polymer weights, this chain mixes rapidly.\nWe apply this Markov chain to polymer models derived from the hard-core and\nferromagnetic Potts models on bounded-degree (bipartite) expander graphs. In\nthis setting, Jenssen, Keevash and Perkins (2019) recently gave an FPTAS and an\nefficient sampling algorithm at sufficiently high fugacity and low temperature\nrespectively. Their method is based on using the cluster expansion to obtain a\ncomplex zero-free region for the partition function of a polymer model, and\nthen approximating this partition function using the polynomial interpolation\nmethod of Barvinok.\n  Our approach via the polymer model Markov chain circumvents the zero-free\nanalysis and the generalization to complex parameters, and leads to a sampling\nalgorithm with a fast running time of $O(n \\log n)$ for the Potts model and\n$O(n^2 \\log n)$ for the hard-core model, in contrast to typical running times\nof $n^{O(\\log \\Delta)}$ for algorithms based on Barvinok's polynomial\ninterpolation method on graphs of maximum degree $\\Delta$. We finally combine\nour results for the hard-core and ferromagnetic Potts models with standard\nMarkov chain comparison tools to obtain polynomial mixing time for the usual\nspin Glauber dynamics restricted to even and odd or `red' dominant portions of\nthe respective state spaces.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 10:11:51 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 19:22:41 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 15:34:34 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 15:38:46 GMT"}, {"version": "v5", "created": "Thu, 28 May 2020 10:56:21 GMT"}, {"version": "v6", "created": "Tue, 13 Apr 2021 15:04:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chen", "Zongchen", ""], ["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Perkins", "Will", ""], ["Stewart", "James", ""], ["Vigoda", "Eric", ""]]}, {"id": "1901.06764", "submitter": "Rasmus J Kyng", "authors": "Deeksha Adil, Rasmus Kyng, Richard Peng, Sushant Sachdeva", "title": "Iterative Refinement for $\\ell_p$-norm Regression", "comments": "Published in SODA 2019. Was initially submitted to SODA on July 12,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give improved algorithms for the $\\ell_{p}$-regression problem, $\\min_{x}\n\\|x\\|_{p}$ such that $A x=b,$ for all $p \\in (1,2) \\cup (2,\\infty).$ Our\nalgorithms obtain a high accuracy solution in $\\tilde{O}_{p}(m^{\\frac{|p-2|}{2p\n+ |p-2|}}) \\le \\tilde{O}_{p}(m^{\\frac{1}{3}})$ iterations, where each iteration\nrequires solving an $m \\times m$ linear system, $m$ being the dimension of the\nambient space.\n  By maintaining an approximate inverse of the linear systems that we solve in\neach iteration, we give algorithms for solving $\\ell_{p}$-regression to $1 /\n\\text{poly}(n)$ accuracy that run in time $\\tilde{O}_p(m^{\\max\\{\\omega,\n7/3\\}}),$ where $\\omega$ is the matrix multiplication constant. For the current\nbest value of $\\omega > 2.37$, we can thus solve $\\ell_{p}$ regression as fast\nas $\\ell_{2}$ regression, for all constant $p$ bounded away from $1.$\n  Our algorithms can be combined with fast graph Laplacian linear equation\nsolvers to give minimum $\\ell_{p}$-norm flow / voltage solutions to $1 /\n\\text{poly}(n)$ accuracy on an undirected graph with $m$ edges in\n$\\tilde{O}_{p}(m^{1 + \\frac{|p-2|}{2p + |p-2|}}) \\le\n\\tilde{O}_{p}(m^{\\frac{4}{3}})$ time.\n  For sparse graphs and for matrices with similar dimensions, our iteration\ncounts and running times improve on the $p$-norm regression algorithm by\n[Bubeck-Cohen-Lee-Li STOC`18] and general-purpose convex optimization\nalgorithms. At the core of our algorithms is an iterative refinement scheme for\n$\\ell_{p}$-norms, using the smoothed $\\ell_{p}$-norms introduced in the work of\nBubeck et al. Given an initial solution, we construct a problem that seeks to\nminimize a quadratically-smoothed $\\ell_{p}$ norm over a subspace, such that a\ncrude solution to this problem allows us to improve the initial solution by a\nconstant factor, leading to algorithms with fast convergence.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 01:42:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Adil", "Deeksha", ""], ["Kyng", "Rasmus", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1901.06880", "submitter": "Anne-Elisabeth Falq", "authors": "Anne-Elisabeth Falq, Pierre Fouilhoux, Safia Kedad-Sidhoum", "title": "Mixed integer formulations using natural variables for single machine\n  scheduling around a common due date", "comments": "32 pages, 10 figures", "journal-ref": "Discrete Applied Mathematics 290 (2021) 36-59", "doi": "10.1016/j.dam.2020.08.033", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While almost all existing works which optimally solve just-in-time scheduling\nproblems propose dedicated algorithmic approaches, we propose in this work\nmixed integer formulations. We consider a single machine scheduling problem\nthat aims at minimizing the weighted sum of earliness tardiness penalties\naround a common due-date. Using natural variables, we provide one compact\nformulation for the unrestrictive case and, for the general case, a non-compact\nformulation based on non-overlapping inequalities. We show that the separation\nproblem related to the latter formulation is solved polynomially. In this\nformulation, solutions are only encoded by extreme points. We establish a\ntheoretical framework to show the validity of such a formulation using\nnon-overlapping inequalities, which could be used for other scheduling\nproblems. A Branch-and-Cut algorithm together with an experimental analysis are\nproposed to assess the practical relevance of this mixed integer programming\nbased methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 11:17:10 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 16:11:14 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Falq", "Anne-Elisabeth", ""], ["Fouilhoux", "Pierre", ""], ["Kedad-Sidhoum", "Safia", ""]]}, {"id": "1901.07032", "submitter": "Philip Klein", "authors": "Amariah Becker and Philip N. Klein and Aaron Schild", "title": "A PTAS for Bounded-Capacity Vehicle Routing in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Capacitated Vehicle Routing problem is to find a minimum-cost set of\ntours that collectively cover clients in a graph, such that each tour starts\nand ends at a specified depot and is subject to a capacity bound on the number\nof clients it can serve. In this paper, we present a polynomial-time\napproximation scheme (PTAS) for instances in which the input graph is planar\nand the capacity is bounded. Previously, only a quasipolynomial-time\napproximation scheme was known for these instances. To obtain this result, we\nshow how to embed planar graphs into bounded-treewidth graphs while preserving,\nin expectation, the client-to-client distances up to a small additive error\nproportional to client distances to the depot.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 18:45:29 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Becker", "Amariah", ""], ["Klein", "Philip N.", ""], ["Schild", "Aaron", ""]]}, {"id": "1901.07070", "submitter": "William Long", "authors": "Thomas Lively, William Long, Artidoro Pagnoni", "title": "Analyzing Branch-and-Bound Algorithms for the Multiprocessor Scheduling\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multiprocessor Scheduling Problem (MSP) is an NP-Complete problem with\nsignificant applications in computer and operations systems. We provide a\nsurvey of the wide array of polynomial-time approximation, heuristic, and\nmeta-heuristic based algorithms that exist for solving MSP. We also implement\nFujita's state-of-the-art Branch-and-Bound algorithm and evaluate the benefit\nof using Fujita's binary search bounding method instead of the Fernandez bound.\nWe find that in fact Fujita's method does not offer any improvement over the\nFernandez bound on our data set.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:34:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Lively", "Thomas", ""], ["Long", "William", ""], ["Pagnoni", "Artidoro", ""]]}, {"id": "1901.07107", "submitter": "Stanislav Zivny", "authors": "Gregor Matl and Stanislav Zivny", "title": "Using a min-cut generalisation to go beyond Boolean surjective VCSPs", "comments": "Full version of a STACS'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we first study a natural generalisation of the Min-Cut problem,\nwhere a graph is augmented by a superadditive set function defined on its\nvertex subsets. The goal is to select a vertex subset such that the weight of\nthe induced cut plus the set function value are minimised. In addition, a lower\nand upper bound is imposed on the solution size. We present a polynomial-time\nalgorithm for enumerating all near-optimal solutions of this Bounded\nGeneralised Min-Cut problem.\n  Second, we apply this novel algorithm to surjective general-valued constraint\nsatisfaction problems (VCSPs), i.e., VCSPs in which each label has to be used\nat least once. On the Boolean domain, Fulla, Uppman, and Zivny [ACM ToCT'18]\nhave recently established a complete classification of surjective VCSPs based\non an unbounded version of the Generalised Min-Cut problem. Their result\nfeatures the discovery of a new non-trivial tractable case called EDS that does\nnot appear in the non-surjective setting.\n  As our main result, we extend the class EDS to arbitrary finite domains and\nprovide a conditional complexity classification for surjective VCSPs of this\ntype based on a reduction to smaller domains. On three-element domains, this\nleads to a complete classification of such VCSPs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 22:53:11 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 15:55:51 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Matl", "Gregor", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1901.07118", "submitter": "Mahdi Belbasi", "authors": "Mahdi Belbasi, Martin F\\\"urer", "title": "A Space-efficient Parameterized Algorithm for the Hamiltonian Cycle\n  Problem by Dynamic Algebraziation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An NP-hard graph problem may be intractable for general graphs but it could\nbe efficiently solvable using dynamic programming for graphs with bounded width\n(or depth or some other structural parameter). Dynamic programming is a\nwell-known approach used for finding exact solutions for NP-hard graph problems\nbased on tree decompositions. It has been shown that there exist algorithms\nusing linear time in the number of vertices and single exponential time in the\nwidth (depth or other parameters) of a given tree decomposition for many\nconnectivity problems. Employing dynamic programming on a tree decomposition\nusually uses exponential space. In 2010, Lokshtanov and Nederlof introduced an\nelegant framework to avoid exponential space by algebraization. Later, F\\\"urer\nand Yu modified the framework in a way that even works when the underlying set\nis dynamic, thus applying it to tree decompositions. In this work, we design\nspace-efficient algorithms to solve the Hamiltonian Cycle and the Traveling\nSalesman problems, using polynomial space while the time complexity is only\nslightly increased. This might be inevitable since we are reducing the space\nusage from an exponential amount (in dynamic programming solution) to\npolynomial. We give an algorithm to solve Hamiltonian cycle in time\n$\\mathcal{O}((4w)^d\\, nM(n\\log{n}))$ using $\\mathcal{O}(dn\\log{n})$ space,\nwhere $M(r)$ is the time complexity to multiply two integers, each of which\nbeing represented by at most $r$ bits. Then, we solve the more general\nTraveling Salesman problem in time $\\mathcal{O}((4w)^d poly(n))$ using space\n$\\mathcal{O}(\\mathcal{W}dn\\log{n})$, where $w$ and $d$ are the width and the\ndepth of the given tree decomposition and $\\mathcal{W}$ is the sum of weights.\nFurthermore, this algorithm counts the number of Hamiltonian Cycles.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 23:22:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Belbasi", "Mahdi", ""], ["F\u00fcrer", "Martin", ""]]}, {"id": "1901.07231", "submitter": "Kurt Mehlhorn", "authors": "Andreas Karrenbauer and Pavel Kolev and Kurt Mehlhorn", "title": "Convergence of the Non-Uniform Physarum Dynamics", "comments": "to appear in Theoretical Computer Science C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $c \\in \\mathbb{Z}^m_{> 0}$, $A \\in \\mathbb{Z}^{n\\times m}$, and $b \\in\n\\mathbb{Z}^n$. We show under fairly general conditions that the non-uniform\nPhysarum dynamics \\[ \\dot{x}_e = a_e(x,t) \\left(|q_e| - x_e\\right) \\] converges\nto the optimum solution $x^*$ of the weighted basis pursuit problem minimize\n$c^T x$ subject to $A f = b$ and $|f| \\le x$. Here, $f$ and $x$ are $m$-vectors\nof real variables, $q$ minimizes the energy $\\sum_e (c_e/x_e) q_e^2$ subject to\nthe constraints $A q = b$ and $\\mathrm{supp}(q) \\subseteq \\mathrm{supp}(x)$,\nand $a_e(x,t) > 0$ is the reactivity of edge $e$ to the difference $|q_e| -\nx_e$ at time $t$ and in state $x$. Previously convergence was only shown for\nthe uniform case $a_e(x,t) = 1$ for all $e$, $x$, and $t$. We also show\nconvergence for the dynamics \\[ \\dot{x}_e = x_e \\cdot \\left( g_e\n\\left(\\frac{|q_e|}{x_e}\\right) - 1\\right),\\] where $g_e$ is an increasing\ndifferentiable function with $g_e(1) = 1$. Previously convergence was only\nshown for the special case of the shortest path problem on a graph consisting\nof two nodes connected by parallel edges.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 09:50:17 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 08:58:08 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Karrenbauer", "Andreas", ""], ["Kolev", "Pavel", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1901.07246", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "A $(4+\\epsilon)$-approximation for $k$-connected subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain approximation ratio $2(2+\\frac{1}{\\ell})$ for the (undirected)\n$k$-Connected Subgraph problem, where $\\ell \\approx \\frac{1}{2} (\\log_k n-1)$\nis the largest integer such that $2^{\\ell-1} k^{2\\ell+1} \\leq n$. For large\nvalues of $n$ this improves the $6$-approximation of Cheriyan and V\\'egh when\n$n =\\Omega(k^3)$, which is the case $\\ell=1$. For $k$ bounded by a constant we\nobtain ratio $4+\\epsilon$. For large values of $n$ our ratio matches the best\nknown ratio $4$ for the augmentation version of the problem, as well as the\nbest known ratios for $k=6,7$. Similar results are shown for the problem of\ncovering an arbitrary crossing supermodular biset function.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 10:34:12 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "1901.07361", "submitter": "Antonio Blanca", "authors": "Ivona Bezakova, Antonio Blanca, Zongchen Chen, Daniel\n  \\v{S}tefankovi\\v{c} and Eric Vigoda", "title": "Lower bounds for testing graphical models: colorings and\n  antiferromagnetic Ising models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the identity testing problem in the context of spin systems or\nundirected graphical models, where it takes the following form: given the\nparameter specification of the model $M$ and a sampling oracle for the\ndistribution $\\mu_{\\hat{M}}$ of an unknown model $\\hat{M}$, can we efficiently\ndetermine if the two models $M$ and $\\hat{M}$ are the same? We consider\nidentity testing for both soft-constraint and hard-constraint systems. In\nparticular, we prove hardness results in two prototypical cases, the Ising\nmodel and proper colorings, and explore whether identity testing is any easier\nthan structure learning.\n  For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018)\npresented a polynomial time algorithm for identity testing. We prove hardness\nresults in the antiferromagnetic (repulsive) setting in the same regime of\nparameters where structure learning is known to require a super-polynomial\nnumber of samples. In particular, for $n$-vertex graphs of maximum degree $d$,\nwe prove that if $|\\beta| d = \\omega(\\log{n})$ (where $\\beta$ is the inverse\ntemperature parameter), then there is no polynomial running time identity\ntesting algorithm unless $RP=NP$. We also establish computational lower bounds\nfor a broader set of parameters under the (randomized) exponential time\nhypothesis. Our proofs utilize insights into the design of gadgets using random\ngraphs in recent works concerning the hardness of approximate counting by Sly\n(2010). In the hard-constraint setting, we present hardness results for\nidentity testing for proper colorings. Our results are based on the presumed\nhardness of #BIS, the problem of (approximately) counting independent sets in\nbipartite graphs. In particular, we prove that identity testing is hard in the\nsame range of parameters where structure learning is known to be hard.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:50:30 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 16:49:12 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Bezakova", "Ivona", ""], ["Blanca", "Antonio", ""], ["Chen", "Zongchen", ""], ["\u0160tefankovi\u010d", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "1901.07463", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "Solve For Shortest Paths Problem Within Logarithm Runtime", "comments": "8 page; 6014 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shortest Paths Problem (SPP) is no longer unresolved. Just for a large\nscalar of instance on this problem, even we cannot know if an algorithm\nachieves the computing. Those cutting-edge methods are still in the low\nperformance. If we go to a strategy the best-first-search to deal with\ncomputing, it is awkward that the technical barrier from another field: the\ndatabase, which with the capable of Online Oriented. In this paper, we will\nintroduce such a synthesis to solve for SPP which comprises various modules\ntherein including such database leads to finish the task in a logarithm\nruntime.\n  Through experiments taken on three typical instances on mega-scalar data for\ntransaction in a common laptop, we show off a totally robust, tractable and\npractical applicability for other projects.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 16:58:12 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1901.07503", "submitter": "Oscar Defrain", "authors": "Oscar Defrain and Lhouari Nourine", "title": "Dualization in lattices given by implicational bases", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently proved that the dualization in lattices given by\nimplicational bases is impossible in output-polynomial time unless P=NP. In\nthis paper, we~show that this result holds even when the premises in the\nimplicational base are of size at most two. Then we show using hypergraph\ndualization that the problem can be solved in output quasi-polynomial time\nwhenever the implicational base has bounded independent-width, defined as the\nsize of a maximum set of implications having independent conclusions. Lattices\nthat share this property include distributive lattices coded by the ideals of\nan interval order, when both the independent-width and the size of the premises\nequal one.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:26:39 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 12:52:59 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 10:34:36 GMT"}, {"version": "v4", "created": "Sun, 29 Sep 2019 17:01:45 GMT"}, {"version": "v5", "created": "Fri, 31 Jan 2020 09:11:24 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Defrain", "Oscar", ""], ["Nourine", "Lhouari", ""]]}, {"id": "1901.07530", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese and Luisa Gargano and Ugo Vaccaro", "title": "Minimum--Entropy Couplings and their Applications", "comments": "This paper has been accepted for publication in IEEE Transactions on\n  Information Theory. arXiv admin note: text overlap with arXiv:1701.05243", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two discrete random variables $X$ and $Y,$ with probability\ndistributions ${\\bf p}=(p_1, \\ldots , p_n)$ and ${\\bf q}=(q_1, \\ldots , q_m)$,\nrespectively, denote by ${\\cal C}({\\bf p}, {\\bf q})$ the set of all couplings\nof ${\\bf p}$ and ${\\bf q}$, that is, the set of all bivariate probability\ndistributions that have ${\\bf p}$ and ${\\bf q}$ as marginals. In this paper, we\nstudy the problem of finding a joint probability distribution in ${\\cal C}({\\bf\np}, {\\bf q})$ of \\emph{minimum entropy} (equivalently, a coupling that\n\\emph{maximizes} the mutual information between $X$ and $Y$), and we discuss\nseveral situations where the need for this kind of optimization naturally\narises. Since the optimization problem is known to be NP-hard, we give an\nefficient algorithm to find a joint probability distribution in ${\\cal C}({\\bf\np}, {\\bf q})$ with entropy exceeding the minimum possible at most by {1 bit},\nthus providing an approximation algorithm with an additive gap of at most 1\nbit. Leveraging on this algorithm, we extend our result to the problem of\nfinding a minimum--entropy joint distribution of arbitrary $k\\geq 2$ discrete\nrandom variables $X_1, \\ldots , X_k$, consistent with the known $k$ marginal\ndistributions of the individual random variables $X_1, \\ldots , X_k$. In this\ncase, our algorithm has an { additive gap of at most $\\log k$ from optimum.}\n  We also discuss several related applications of our findings and {extensions\nof our results to entropies different from the Shannon entropy.}\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 10:57:21 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Gargano", "Luisa", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1901.07609", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Faster parameterized algorithm for Cluster Vertex Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Cluster Vertex Deletion problem the input is a graph $G$ and an\ninteger $k$. The goal is to decide whether there is a set of vertices $S$ of\nsize at most $k$ such that the deletion of the vertices of $S$ from $G$ results\na graph in which every connected component is a clique. We give an algorithm\nfor Cluster Vertex Deletion whose running time is $O^*(1.811^k)$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 20:33:34 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1901.07809", "submitter": "Uriel Feige", "authors": "Uriel Feige", "title": "A randomized strategy in the mirror game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alice and Bob take turns (with Alice playing first) in declaring numbers from\nthe set $[1,2N]$. If a player declares a number that was previously declared,\nthat player looses and the other player wins. If all numbers are declared\nwithout repetition, the outcome is a tie. If both players have unbounded memory\nand play optimally, then the game will be tied. Garg and Schneider [ITCS 2019]\nshowed that if Alice has unbounded memory, then Bob can secure a tie with $\\log\nN$ memory, whereas if Bob has unbounded memory, then Alice needs memory linear\nin $N$ in order to secure a tie.\n  Garg and Schneider also considered an {\\em auxiliary matching} model in which\nAlice gets as an additional input a random matching $M$ over the numbers\n$[1,2N]$, and storing this input does not count towards the memory used by\nAlice. They showed that is this model there is a strategy for Alice that ties\nwith probability at least $1 - \\frac{1}{N}$, and uses only $O(\\sqrt{N} (\\log\nN)^2)$ memory. We show how to modify Alice's strategy so that it uses only\n$O((\\log N)^3)$ space.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 10:42:25 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Feige", "Uriel", ""]]}, {"id": "1901.07904", "submitter": "Arnaud Lazare", "authors": "Sourour Elloumi (CEDRIC), Am\\'elie Lambert (CEDRIC), Arnaud Lazare\n  (CEDRIC)", "title": "Solving unconstrained 0-1 polynomial programs through quadratic convex\n  reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a solution approach for the problem (P) of minimizing an\nunconstrained binary polynomial optimization problem. We call this method PQCR\n(Polynomial Quadratic Convex Reformulation). The resolution is based on a\n3-phase method. The first phase consists in reformulating (P) into a quadratic\nprogram (QP). For this, we recursively reduce the degree of (P) to two, by use\nof the standard substitution of the product of two variables by a new one. We\nthen obtain a linearly constrained binary program. In the second phase, we\nrewrite the quadratic objective function into an equivalent and parametrized\nquadratic function using the equality x 2 i = x i and new valid quadratic\nequalities. Then, we focus on finding the best parameters to get a quadratic\nconvex program which continuous relaxation's optimal value is maximized. For\nthis, we build a semidefinite relaxation (SDP) of (QP). Then, we prove that the\nstandard linearization inequalities, used for the quadratization step, are\nredundant in (SDP) in presence of the new quadratic equalities. Next, we deduce\nour optimal parameters from the dual optimal solution of (SDP). The third phase\nconsists in solving (QP *), the optimal reformulated problem, with a standard\nsolver. In particular, at each node of the branch-and-bound, the solver\ncomputes the optimal value of a continuous quadratic convex program. We present\ncomputational results on instances of the image restoration problem and of the\nlow autocorrelation binary sequence problem. We compare PQCR with other\nconvexification methods, and with the general solver Baron 17.4.1 [39]. We\nobserve that most of the considered instances can be solved with our approach\ncombined with the use of Cplex [24].\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 09:10:21 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Elloumi", "Sourour", "", "CEDRIC"], ["Lambert", "Am\u00e9lie", "", "CEDRIC"], ["Lazare", "Arnaud", "", "CEDRIC"]]}, {"id": "1901.07906", "submitter": "Zeren Tan", "authors": "Zeren Tan", "title": "How Many Passengers Can We Serve with Ride-sharing?", "comments": "11 figures, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ride-sharing can reduce traffic congestion and thus reduce gas emissions and\nsave travel time. However, transportation system with ride-sharing is currently\ninefficient due to low occupancy rate, high travel demand and some other\nfactors. Existing literature did not consider ride-sharing with multi-request\ngrouped in one trip. In our paper, we firstly proposed a graph-based algorithm\nthat can obtain an approximation solution in polynomial time and then proposed\nan exact algorithm to solve this problem with maximizing the number of\npassenegers served in $O(1.2312^{|\\mathcal{E}|})$ time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 08:48:30 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Tan", "Zeren", ""]]}, {"id": "1901.07928", "submitter": "Phuc Thai", "authors": "Hung Nguyen, Phuc Thai, My Thai, Tam Vu, Thang Dinh", "title": "Approximate k-Cover in Hypergraphs: Efficient Algorithms, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted hypergraph $\\mathcal{H}(V, \\mathcal{E} \\subseteq 2^V, w)$,\nthe approximate $k$-cover problem seeks for a size-$k$ subset of $V$ that has\nthe maximum weighted coverage by \\emph{sampling only a few hyperedges} in\n$\\mathcal{E}$. The problem has emerged from several network analysis\napplications including viral marketing, centrality maximization, and landmark\nselection. Despite many efforts, even the best approaches require $O(k n \\log\nn)$ space complexities, thus, cannot scale to, nowadays, humongous networks\nwithout sacrificing formal guarantees. In this paper, we propose BCA, a family\nof algorithms for approximate $k$-cover that can find $(1-\\frac{1}{e}\n-\\epsilon)$-approximation solutions within an \\emph{$O(\\epsilon^{-2}n \\log n)$\nspace}. That is a factor $k$ reduction on space comparing to the\nstate-of-the-art approaches with the same guarantee. We further make BCA more\nefficient and robust on real-world instances by introducing a novel adaptive\nsampling scheme, termed DTA.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 14:49:36 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nguyen", "Hung", ""], ["Thai", "Phuc", ""], ["Thai", "My", ""], ["Vu", "Tam", ""], ["Dinh", "Thang", ""]]}, {"id": "1901.08210", "submitter": "Rei Mizuta", "authors": "Rei Mizuta", "title": "Pseudo-Polynomial Time Algorithm for Computing Moments of Polynomials in\n  Free Semicircular Elements", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OA cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider about calculating $M$th moments of a given polynomial in free\nindependent semicircular elements in free probability theory. By a naive\napproach, this calculation requires exponential time with respect to $M$. We\nexplicitly give an algorithm for calculating them in polynomial time by\nrearranging Sch\\\"utzenberger's algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 03:14:38 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 07:48:52 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Mizuta", "Rei", ""]]}, {"id": "1901.08235", "submitter": "Tingran Gao", "authors": "Tingran Gao, Zhizhen Zhao", "title": "Multi-Frequency Phase Synchronization", "comments": "12 pages, 9 figures. ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel formulation for phase synchronization -- the statistical\nproblem of jointly estimating alignment angles from noisy pairwise comparisons\n-- as a nonconvex optimization problem that enforces consistency among the\npairwise comparisons in multiple frequency channels. Inspired by harmonic\nretrieval in signal processing, we develop a simple yet efficient two-stage\nalgorithm that leverages the multi-frequency information. We demonstrate in\ntheory and practice that the proposed algorithm significantly outperforms\nstate-of-the-art phase synchronization algorithms, at a mild computational\ncosts incurred by using the extra frequency channels. We also extend our\nalgorithmic framework to general synchronization problems over compact Lie\ngroups.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 04:52:20 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 15:30:27 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 03:28:59 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Gao", "Tingran", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "1901.08525", "submitter": "Adrien Chan-Hon-Tong", "authors": "Adrien Chan-Hon-Tong", "title": "Self concordant Perceptron: Solving linear feasibility in linear number\n  of Newton steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers a new algorithm for linear feasibility. This algorithm is\npolynomial time (despite an arithmetic time complexity slightly higher than\nstate of art, the algorithm has excellent binary property). This algorithm\nencodes the original Perceptron idea into a self concordant function, and, has\nthe best complexity in this family of algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 20:51:40 GMT"}, {"version": "v10", "created": "Sat, 23 Nov 2019 21:50:25 GMT"}, {"version": "v11", "created": "Tue, 26 Nov 2019 08:39:05 GMT"}, {"version": "v12", "created": "Tue, 3 Dec 2019 11:56:37 GMT"}, {"version": "v13", "created": "Thu, 2 Jan 2020 14:30:31 GMT"}, {"version": "v14", "created": "Mon, 17 Feb 2020 12:31:51 GMT"}, {"version": "v15", "created": "Tue, 24 Mar 2020 14:01:45 GMT"}, {"version": "v16", "created": "Tue, 9 Jun 2020 20:10:11 GMT"}, {"version": "v17", "created": "Mon, 12 Oct 2020 08:47:46 GMT"}, {"version": "v18", "created": "Tue, 5 Jan 2021 20:40:24 GMT"}, {"version": "v19", "created": "Sun, 2 May 2021 19:59:16 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 14:47:59 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 10:46:29 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 18:52:22 GMT"}, {"version": "v5", "created": "Tue, 9 Apr 2019 11:25:02 GMT"}, {"version": "v6", "created": "Tue, 7 May 2019 12:31:18 GMT"}, {"version": "v7", "created": "Wed, 29 May 2019 20:39:55 GMT"}, {"version": "v8", "created": "Mon, 21 Oct 2019 08:02:21 GMT"}, {"version": "v9", "created": "Thu, 21 Nov 2019 15:51:17 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chan-Hon-Tong", "Adrien", ""]]}, {"id": "1901.08544", "submitter": "Tal Wagner", "authors": "Yihe Dong and Piotr Indyk and Ilya Razenshteyn and Tal Wagner", "title": "Learning Space Partitions for Nearest Neighbor Search", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space partitions of $\\mathbb{R}^d$ underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:07:59 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 04:48:38 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 19:22:44 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 02:50:54 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Dong", "Yihe", ""], ["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""], ["Wagner", "Tal", ""]]}, {"id": "1901.08564", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Cordelia Avery, Joseph Bergeron, Erik D. Demaine,\n  Justin Kopinsky, Jason Ku", "title": "Infinite All-Layers Simple Foldability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deciding whether a crease pattern can be folded by\nsimple folds (folding along one line at a time) under the infinite all-layers\nmodel introduced by [Akitaya et al., 2017], in which each simple fold is\ndefined by an infinite line and must fold all layers of paper that intersect\nthis line. This model is motivated by folding in manufacturing such as\nsheet-metal bending. We improve on [Arkin et al., 2004] by giving a\ndeterministic $O(n)$-time algorithm to decide simple foldability of 1D crease\npatterns in the all-layers model. Then we extend this 1D result to 2D, showing\nthat simple foldability in this model can be decided in linear time for\nunassigned axis-aligned orthogonal crease patterns on axis-aligned 2D\northogonal paper. On the other hand, we show that simple foldability is\nstrongly NP-complete if a subset of the creases have a mountain-valley\nassignment, even for an axis-aligned rectangle of paper.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:31:28 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Avery", "Cordelia", ""], ["Bergeron", "Joseph", ""], ["Demaine", "Erik D.", ""], ["Kopinsky", "Justin", ""], ["Ku", "Jason", ""]]}, {"id": "1901.08575", "submitter": "J\\'er\\^ome Durand-Lose", "authors": "J\\'er\\^ome Durand-Lose, Hendrik Jan Hoogeboom, Nata\\v{s}a Jonoska", "title": "Deterministic 2-Dimensional Temperature-1 Tile Assembly Systems Cannot\n  Compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider non cooperative binding in so called `temperature 1', in\ndeterministic (here called {\\it confluent}) tile self-assembly systems (1-TAS)\nand prove the standing conjecture that such systems do not have universal\ncomputational power. We call a TAS whose maximal assemblies contain at least\none ultimately periodic assembly path {\\it para-periodic}. We observe that a\nconfluent 1-TAS has at most one maximal producible assembly, $\\alpha_{max}$,\nthat can be considered a union of path assemblies, and we show that such a\nsystem is always para-periodic. This result is obtained through a superposition\nand a combination of two paths that produce a new path with desired properties,\na technique that we call \\emph{co-grow} of two paths. Moreover we provide a\ncharacterization of an $\\alpha_{max}$ of a confluent 1-TAS as one of two\npossible cases, so called, a grid or a disjoint union of combs. To a given\n$\\alpha_{max}$ we can associate a finite labeled graph, called \\emph{quipu},\nsuch that the union of all labels of paths in the quipu equals $\\alpha_{max}$,\ntherefore giving a finite description for $\\alpha_{max}$. This finite\ndescription implies that $\\alpha_{max}$ is a union of semi-affine subsets of\n$\\mathbb{Z}^2$ and since such a finite description can be algorithmicly\ngenerated from any 1-TAS, 1-TAS cannot have universal computational power.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:47:33 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Durand-Lose", "J\u00e9r\u00f4me", ""], ["Hoogeboom", "Hendrik Jan", ""], ["Jonoska", "Nata\u0161a", ""]]}, {"id": "1901.08628", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner, Pranjal Awasthi, Jamie Morgenstern", "title": "Fair k-Center Clustering for Data Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data summarization we want to choose $k$ prototypes in order to summarize\na data set. We study a setting where the data set comprises several demographic\ngroups and we are restricted to choose $k_i$ prototypes belonging to group $i$.\nA common approach to the problem without the fairness constraint is to optimize\na centroid-based clustering objective such as $k$-center. A natural extension\nthen is to incorporate the fairness constraint into the clustering problem.\nExisting algorithms for doing so run in time super-quadratic in the size of the\ndata set, which is in contrast to the standard $k$-center problem being\napproximable in linear time. In this paper, we resolve this gap by providing a\nsimple approximation algorithm for the $k$-center problem under the fairness\nconstraint with running time linear in the size of the data set and $k$. If the\nnumber of demographic groups is small, the approximation guarantee of our\nalgorithm only incurs a constant-factor overhead.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 20:05:57 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 19:29:06 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["Awasthi", "Pranjal", ""], ["Morgenstern", "Jamie", ""]]}, {"id": "1901.08639", "submitter": "Fan Zhang", "authors": "Fan Zhang, Lei Zou, Li Zeng, Xiangyang Gou", "title": "Dolha - an Efficient and Exact Data Structure for Streaming Graphs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A streaming graph is a graph formed by a sequence of incoming edges with time\nstamps. Unlike static graphs, the streaming graph is highly dynamic and time\nrelated. In the real world, the high volume and velocity streaming graphs such\nas internet traffic data, social network communication data and financial\ntransfer data are bringing challenges to the classic graph data structures. We\npresent a new data structure: double orthogonal list in hash table (Dolha)\nwhich is a high speed and high memory efficiency graph structure applicable to\nstreaming graph. Dolha has constant time cost for single edge and near linear\nspace cost that we can contain billions of edges information in memory size and\nprocess an incoming edge in nanoseconds. Dolha also has linear time cost for\nneighborhood queries, which allow it to support most algorithms in graphs\nwithout extra cost. We also present a persistent structure based on Dolha that\nhas the ability to handle the sliding window update and time related queries.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 20:41:32 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Zhang", "Fan", ""], ["Zou", "Lei", ""], ["Zeng", "Li", ""], ["Gou", "Xiangyang", ""]]}, {"id": "1901.08668", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner, Samira Samadi, Pranjal Awasthi, Jamie\n  Morgenstern", "title": "Guarantees for Spectral Clustering with Fairness Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the widespread popularity of spectral clustering (SC) for partitioning\ngraph data, we study a version of constrained SC in which we try to incorporate\nthe fairness notion proposed by Chierichetti et al. (2017). According to this\nnotion, a clustering is fair if every demographic group is approximately\nproportionally represented in each cluster. To this end, we develop variants of\nboth normalized and unnormalized constrained SC and show that they help find\nfairer clusterings on both synthetic and real data. We also provide a rigorous\ntheoretical analysis of our algorithms on a natural variant of the stochastic\nblock model, where $h$ groups have strong inter-group connectivity, but also\nexhibit a \"natural\" clustering structure which is fair. We prove that our\nalgorithms can recover this fair clustering with high probability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 22:27:46 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 19:42:37 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["Samadi", "Samira", ""], ["Awasthi", "Pranjal", ""], ["Morgenstern", "Jamie", ""]]}, {"id": "1901.08686", "submitter": "Pavel Dvurechensky", "authors": "Alexey Kroshnin, Darina Dvinskikh, Pavel Dvurechensky, Alexander\n  Gasnikov, Nazarii Tupitsa, Cesar Uribe", "title": "On the Complexity of Approximating Wasserstein Barycenter", "comments": "Corrected misprints. Added a reference to accelerated Iterative\n  Bregman Projections introduced in arXiv:1906.03622", "journal-ref": "ICML 2019, in PMLR 97:3530-3540.\n  http://proceedings.mlr.press/v97/kroshnin19a.html", "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating Wassertein barycenter of $m$\ndiscrete measures, or histograms of size $n$ by contrasting two alternative\napproaches, both using entropic regularization. The first approach is based on\nthe Iterative Bregman Projections (IBP) algorithm for which our novel analysis\ngives a complexity bound proportional to $\\frac{mn^2}{\\varepsilon^2}$ to\napproximate the original non-regularized barycenter. Using an alternative\naccelerated-gradient-descent-based approach, we obtain a complexity\nproportional to $\\frac{mn^{2.5}}{\\varepsilon} $. As a byproduct, we show that\nthe regularization parameter in both approaches has to be proportional to\n$\\varepsilon$, which causes instability of both algorithms when the desired\naccuracy is high. To overcome this issue, we propose a novel proximal-IBP\nalgorithm, which can be seen as a proximal gradient method, which uses IBP on\neach iteration to make a proximal step. We also consider the question of\nscalability of these algorithms using approaches from distributed optimization\nand show that the first algorithm can be implemented in a centralized\ndistributed setting (master/slave), while the second one is amenable to a more\ngeneral decentralized distributed setting with an arbitrary network topology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 23:28:14 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 17:18:07 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 14:24:27 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Kroshnin", "Alexey", ""], ["Dvinskikh", "Darina", ""], ["Dvurechensky", "Pavel", ""], ["Gasnikov", "Alexander", ""], ["Tupitsa", "Nazarii", ""], ["Uribe", "Cesar", ""]]}, {"id": "1901.08708", "submitter": "Harsh Gupta", "authors": "Harsh Gupta, Seo Taek Kong, R. Srikant, Weina Wang", "title": "Almost Boltzmann Exploration", "comments": "12 pages, 14 figures. Fixed a figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boltzmann exploration is widely used in reinforcement learning to provide a\ntrade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et\nal., 2017) it has been shown that pure Boltzmann exploration does not perform\nwell from a regret perspective, even in the simplest setting of stochastic\nmulti-armed bandit (MAB) problems. In this paper, we show that a simple\nmodification to Boltzmann exploration, motivated by a variation of the standard\ndoubling trick, achieves $O(K\\log^{1+\\alpha} T)$ regret for a stochastic MAB\nproblem with $K$ arms, where $\\alpha>0$ is a parameter of the algorithm. This\nimproves on the result in (Cesa-Bianchi et al., 2017), where an algorithm\ninspired by the Gumbel-softmax trick achieves $O(K\\log^2 T)$ regret. We also\nshow that our algorithm achieves $O(\\beta(G) \\log^{1+\\alpha} T)$ regret in\nstochastic MAB problems with graph-structured feedback, without knowledge of\nthe graph structure, where $\\beta(G)$ is the independence number of the\nfeedback graph. Additionally, we present extensive experimental results on real\ndatasets and applications for multi-armed bandits with both traditional bandit\nfeedback and graph-structured feedback. In all cases, our algorithm performs as\nwell or better than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 01:29:24 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 20:45:15 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Gupta", "Harsh", ""], ["Kong", "Seo Taek", ""], ["Srikant", "R.", ""], ["Wang", "Weina", ""]]}, {"id": "1901.08711", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Local Distance Constrained Bribery in Voting", "comments": "Published in Theoretical Computer Science journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying complexity of various bribery problems has been one of the main\nresearch focus in computational social choice. In all the models of bribery\nstudied so far, the briber has to pay every voter some amount of money\ndepending on what the briber wants the voter to report and the briber has some\nbudget at her disposal. Although these models successfully capture many real\nworld applications, in many other scenarios, the voters may be unwilling to\ndeviate too much from their true preferences. In this paper, we study the\ncomputational complexity of the problem of finding a preference profile which\nis as close to the true preference profile as possible and still achieves the\nbriber's goal subject to budget constraints. We call this problem Optimal\nBribery. We consider three important measures of distances, namely, swap\ndistance, footrule distance, and maximum displacement distance, and resolve the\ncomplexity of the optimal bribery problem for many common voting rules. We show\nthat the problem is polynomial time solvable for the plurality and veto voting\nrules for all the three measures of distance. On the other hand, we prove that\nthe problem is NP-complete for a class of scoring rules which includes the\nBorda voting rule, maximin, Copeland$^\\alpha$ for any $\\alpha\\in[0,1]$, and\nBucklin voting rules for all the three measures of distance even when the\ndistance allowed per voter is $1$ for the swap and maximum displacement\ndistances and $2$ for the footrule distance even without the budget constraints\n(which corresponds to having an infinite budget). For the $k$-approval voting\nrule for any constant $k>1$ and the simplified Bucklin voting rule, we show\nthat the problem is NP-complete for the swap distance even when the distance\nallowed is $2$ and for the footrule distance even when the distance allowed is\n$4$ even without the budget constraints.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 02:04:21 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 12:00:32 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 12:15:11 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1901.08836", "submitter": "Vincenzo Bonifaci", "authors": "Vincenzo Bonifaci", "title": "A Laplacian Approach to $\\ell_1$-Norm Minimization", "comments": null, "journal-ref": "Computational Optimization and Applications, 2021", "doi": "10.1007/s10589-021-00270-x", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel differentiable reformulation of the linearly-constrained\n$\\ell_1$ minimization problem, also known as the basis pursuit problem. The\nreformulation is inspired by the Laplacian paradigm of network theory and leads\nto a new family of gradient-based methods for the solution of $\\ell_1$\nminimization problems. We analyze the iteration complexity of a natural\nsolution approach to the reformulation, based on a multiplicative weights\nupdate scheme, as well as the iteration complexity of an accelerated gradient\nscheme. The results can be seen as bounds on the complexity of iteratively\nreweighted least squares (IRLS) type methods of basis pursuit.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 11:29:25 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 10:24:19 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 10:55:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bonifaci", "Vincenzo", ""]]}, {"id": "1901.09017", "submitter": "Adrian Dumitrescu", "authors": "Adrian Dumitrescu", "title": "Finding a Mediocre Player", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a totally ordered set $S$ of $n$ elements; as an example, a set of\ntennis players and their rankings. Further assume that their ranking is a total\norder and thus satisfies transitivity and anti-symmetry. Following Frances Yao\n(1974), an element (player) is said to be $(i,j)$-\\emph{mediocre} if it is\nneither among the top $i$ nor among the bottom $j$ elements of $S$. Finding a\nmediocre element is closely related to finding the median element. More than\n$40$ years ago, Yao suggested a very simple and elegant algorithm for finding\nan $(i,j)$-mediocre element: Pick $i+j+1$ elements arbitrarily and select the\n$(i+1)$-th largest among them. She also asked: \"Is this the best algorithm?\" No\none seems to have found a better algorithm ever since. We first provide a\ndeterministic algorithm that beats the worst-case comparison bound in Yao's\nalgorithm for a large range of values of $i$ (and corresponding suitable\n$j=j(i)$) even if the current best selection algorithm is used. We then repeat\nthe exercise for randomized algorithms; the average number of comparisons of\nour algorithm beats the average comparison bound in Yao's algorithm for another\nlarge range of values of $i$ (and corresponding suitable $j=j(i)$) even if the\nbest selection algorithm is used; the improvement is most notable in the\nsymmetric case $i=j$. Moreover, the tight bound obtained in the analysis of\nYao's algorithm allows us to give a definite answer for this class of\nalgorithms. In summary, we answer Yao's question as follows: (i)~\"Presently\nnot\" for deterministic algorithms and (ii)~\"Definitely not\" for randomized\nalgorithms. (In fairness, it should be said however that Yao posed the question\nin the context of deterministic algorithms.)\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 18:37:11 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 04:04:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dumitrescu", "Adrian", ""]]}, {"id": "1901.09154", "submitter": "Guido Tagliavini Ponce", "authors": "Diego Delle Donne, Guido Tagliavini", "title": "Star Routing: Between Vehicle Routing and Vertex Cover", "comments": "Accepted to the 12th Annual International Conference on Combinatorial\n  Optimization and Applications (COCOA'18)", "journal-ref": null, "doi": "10.1007/978-3-030-04651-4_35", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an optimization problem posed by an actual newspaper company,\nwhich consists of computing a minimum length route for a delivery truck, such\nthat the driver only stops at street crossings, each time delivering copies to\nall customers adjacent to the crossing. This can be modeled as an abstract\nproblem that takes an unweighted simple graph $G = (V, E)$ and a subset of\nedges $X$ and asks for a shortest cycle, not necessarily simple, such that\nevery edge of $X$ has an endpoint in the cycle.\n  We show that the decision version of the problem is strongly NP-complete,\neven if $G$ is a grid graph. Regarding approximate solutions, we show that the\ngeneral case of the problem is APX-hard, and thus no PTAS is possible unless P\n$=$ NP. Despite the hardness of approximation, we show that given any\n$\\alpha$-approximation algorithm for metric TSP, we can build a\n$3\\alpha$-approximation algorithm for our optimization problem, yielding a\nconcrete $9/2$-approximation algorithm.\n  The grid case is of particular importance, because it models a city map or\nsome part of it. A usual scenario is having some neighborhood full of\ncustomers, which translates as an instance of the abstract problem where almost\nevery edge of $G$ is in $X$. We model this property as $|E - X| = o(|E|)$, and\nfor these instances we give a $(3/2 + \\varepsilon)$-approximation algorithm,\nfor any $\\varepsilon > 0$, provided that the grid is sufficiently big.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 03:52:03 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Donne", "Diego Delle", ""], ["Tagliavini", "Guido", ""]]}, {"id": "1901.09161", "submitter": "Minghua Chen", "authors": "Qiulin Lin and Hanling Yi and John Pang and Minghua Chen and Adam\n  Wierman and Michael Honig and Yuanzhang Xiao", "title": "Competitive Online Optimization under Inventory Constraints", "comments": "The first two authors contribute to the work equally. Manuscript\n  submitted October 22, 2018; accepted December 17, 2018; to appear in ACM\n  SIGMETRICS 2019", "journal-ref": "Proceedings of the ACM on Measurement and Analysis of Computing\n  Systems (for publishing papers of ACM SIGMETRICS), 2019", "doi": null, "report-no": null, "categories": "cs.PF cs.DS cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies online optimization under inventory (budget) constraints.\nWhile online optimization is a well-studied topic, versions with inventory\nconstraints have proven difficult. We consider a formulation of\ninventory-constrained optimization that is a generalization of the classic\none-way trading problem and has a wide range of applications. We present a new\nalgorithmic framework, \\textsf{CR-Pursuit}, and prove that it achieves the\nminimal competitive ratio among all deterministic algorithms (up to a\nproblem-dependent constant factor) for inventory-constrained online\noptimization. Our algorithm and its analysis not only simplify and unify the\nstate-of-the-art results for the standard one-way trading problem, but they\nalso establish novel bounds for generalizations including concave revenue\nfunctions. For example, for one-way trading with price elasticity, the\n\\textsf{CR-Pursuit} algorithm achieves a competitive ratio that is within a\nsmall additive constant (i.e., 1/3) to the lower bound of $\\ln \\theta+1$, where\n$\\theta$ is the ratio between the maximum and minimum base prices.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 04:45:01 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Lin", "Qiulin", ""], ["Yi", "Hanling", ""], ["Pang", "John", ""], ["Chen", "Minghua", ""], ["Wierman", "Adam", ""], ["Honig", "Michael", ""], ["Xiao", "Yuanzhang", ""]]}, {"id": "1901.09349", "submitter": "Rachit Nimavat", "authors": "Julia Chuzhoy and Rachit Nimavat", "title": "Large Minors in Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study expander graphs and their minors. Specifically, we\nattempt to answer the following question: what is the largest function\n$f(n,\\alpha,d)$, such that every $n$-vertex $\\alpha$-expander with maximum\nvertex degree at most $d$ contains {\\bf every} graph $H$ with at most\n$f(n,\\alpha,d)$ edges and vertices as a minor? Our main result is that there is\nsome universal constant $c$, such that $f(n,\\alpha,d)\\geq \\frac{n}{c\\log\nn}\\cdot \\left(\\frac{\\alpha}{d}\\right )^c$. This bound achieves a tight\ndependence on $n$: it is well known that there are bounded-degree $n$-vertex\nexpanders, that do not contain any grid with $\\Omega(n/\\log n)$ vertices and\nedges as a minor. The best previous result showed that $f(n,\\alpha,d) \\geq\n\\Omega(n/\\log^{\\kappa}n)$, where $\\kappa$ depends on both $\\alpha$ and $d$.\nAdditionally, we provide a randomized algorithm, that, given an $n$-vertex\n$\\alpha$-expander with maximum vertex degree at most $d$, and another graph $H$\ncontaining at most $\\frac{n}{c\\log n}\\cdot \\left(\\frac{\\alpha}{d}\\right )^c$\nvertices and edges, with high probability finds a model of $H$ in $G$, in time\npoly$(n)\\cdot (d/\\alpha)^{O\\left( \\log(d/\\alpha) \\right)}$.\n  We note that similar but stronger results were independently obtained by\nKrivelevich and Nenadov: they show that $f(n,\\alpha,d)=\\Omega\n\\left(\\frac{n\\alpha^2}{d^2\\log n} \\right)$, and provide an efficient algorithm,\nthat, given an $n$-vertex $\\alpha$-expander of maximum vertex degree at most\n$d$, and a graph $H$ with $O\\left( \\frac{n\\alpha^2}{d^2\\log n} \\right)$\nvertices and edges, finds a model of $H$ in $G$.\n  Finally, we observe that expanders are the `most minor-rich' family of graphs\nin the following sense: for every $n$-vertex and $m$-edge graph $G$, there\nexists a graph $H$ with $O \\left( \\frac{n+m}{\\log n} \\right)$ vertices and\nedges, such that $H$ is not a minor of $G$.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 10:47:49 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 09:12:51 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Nimavat", "Rachit", ""]]}, {"id": "1901.09355", "submitter": "Vasileios Nakos", "authors": "Vasileios Nakos", "title": "Nearly Optimal Sparse Polynomial Multiplication", "comments": "Accepted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sparse polynomial multiplication problem, one is asked to multiply two\nsparse polynomials f and g in time that is proportional to the size of the\ninput plus the size of the output. The polynomials are given via lists of their\ncoefficients F and G, respectively. Cole and Hariharan (STOC 02) have given a\nnearly optimal algorithm when the coefficients are positive, and Arnold and\nRoche (ISSAC 15) devised an algorithm running in time proportional to the\n\"structural sparsity\" of the product, i.e. the set supp(F)+supp(G). The latter\nalgorithm is particularly efficient when there not \"too many cancellations\" of\ncoefficients in the product. In this work we give a clean, nearly optimal\nalgorithm for the sparse polynomial multiplication problem.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 11:43:59 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 16:46:41 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 20:51:24 GMT"}, {"version": "v4", "created": "Sat, 7 Dec 2019 17:42:50 GMT"}, {"version": "v5", "created": "Tue, 4 Feb 2020 21:22:28 GMT"}, {"version": "v6", "created": "Mon, 20 Apr 2020 21:01:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Nakos", "Vasileios", ""]]}, {"id": "1901.09505", "submitter": "Ibragim Junussov", "authors": "I.A. Junussov", "title": "Note on distance matrix hashing", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing algorithm of dynamical set of distances is described. Proposed\nhashing function is residual. Data structure which implementation accelerates\ncomputations is presented\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 11:19:43 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 03:49:47 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Junussov", "I. A.", ""]]}, {"id": "1901.09515", "submitter": "Lin Chen", "authors": "Lin Chen, Mingrui Zhang, Hamed Hassani, Amin Karbasi", "title": "Black Box Submodular Maximization: Discrete and Continuous Settings", "comments": "Accepted to AISTATS 2020. First two authors contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of black box continuous submodular\nmaximization where we only have access to the function values and no\ninformation about the derivatives is provided. For a monotone and continuous\nDR-submodular function, and subject to a bounded convex body constraint, we\npropose Black-box Continuous Greedy, a derivative-free algorithm that provably\nachieves the tight $[(1-1/e)OPT-\\epsilon]$ approximation guarantee with\n$O(d/\\epsilon^3)$ function evaluations. We then extend our result to the\nstochastic setting where function values are subject to stochastic zero-mean\nnoise. It is through this stochastic generalization that we revisit the\ndiscrete submodular maximization problem and use the multi-linear extension as\na bridge between discrete and continuous settings. Finally, we extensively\nevaluate the performance of our algorithm on continuous and discrete submodular\nobjective functions using both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 04:53:53 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 20:57:49 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Lin", ""], ["Zhang", "Mingrui", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1901.09527", "submitter": "Erel Segal-Halevi", "authors": "Elad Aigner-Horev and Erel Segal-Halevi", "title": "Envy-free Matchings in Bipartite Graphs and their Applications to Fair\n  Division", "comments": "This version presents the Lone Divider algorithm in a generic way,\n  and shows that it can be used to allocate both goods and chores. For goods,\n  it yields 1-out-of-(2n-2) MMS; for chores, 1-out-of-(2n/3) MMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matching in a bipartite graph $G:=(X + Y,E)$ is said to be envy-free if no\nunmatched vertex in $X$ is adjacent to a mathced vertex in $Y$. Every perfect\nmatching is envy-free, but envy-free matchings may exist even when perfect\nmatchings do not.\n  We provide a polynomial-time algorithm for finding an envy-free matching of\nmaximum cardinality. For edge-weighted bipartite graphs, we provide a\npolynomial-time algorithm for finding a maximum-cardinality envy-free matching\nof minimum weight.\n  We show how envy-free matchings can be used in various fair division problems\nwith either continuous resources (\"cakes\") or discrete ones. In particular, we\nshow a symmetric algorithm for proportional cake-cutting, an algorithm for\n$1$-out-of-$(2n-2)$ maximin-share allocation of discrete goods, and an\nalgorithm for $1$-out-of-$\\lfloor 2n/3\\rfloor$ maximin-share allocation of\ndiscrete bads (chores) among $n$ agents.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 06:03:25 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 10:56:16 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 19:31:33 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 09:15:57 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Aigner-Horev", "Elad", ""], ["Segal-Halevi", "Erel", ""]]}, {"id": "1901.09858", "submitter": "Jasjeet Dhaliwal", "authors": "Jasjeet Dhaliwal, Geoffrey So, Aleatha Parker-Wood, Melanie Beck", "title": "Utility Preserving Secure Private Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy mechanisms that also make reconstruction of the data\nimpossible come at a cost - a decrease in utility. In this paper, we tackle\nthis problem by designing a private data release mechanism that makes\nreconstruction of the original data impossible and also preserves utility for a\nwide range of machine learning algorithms. We do so by combining the\nJohnson-Lindenstrauss (JL) transform with noise generated from a Laplace\ndistribution. While the JL transform can itself provide privacy guarantees\n\\cite{blocki2012johnson} and make reconstruction impossible, we do not rely on\nits differential privacy properties and only utilize its ability to make\nreconstruction impossible. We present novel proofs to show that our mechanism\nis differentially private under single element changes as well as single row\nchanges to any database. In order to show utility, we prove that our mechanism\nmaintains pairwise distances between points in expectation and also show that\nits variance is proportional to the dimensionality of the subspace we project\nthe data into. Finally, we experimentally show the utility of our mechanism by\ndeploying it on the task of clustering.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 17:56:24 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 16:06:32 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 02:14:52 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dhaliwal", "Jasjeet", ""], ["So", "Geoffrey", ""], ["Parker-Wood", "Aleatha", ""], ["Beck", "Melanie", ""]]}, {"id": "1901.09863", "submitter": "Govind Ramnarayan", "authors": "Ran Gelles, Yael T. Kalai, Govind Ramnarayan", "title": "Efficient Multiparty Interactive Coding for Insertions, Deletions and\n  Substitutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of interactive coding, two or more parties wish to carry out a\ndistributed computation over a communication network that may be noisy. The\nultimate goal is to develop efficient coding schemes that can tolerate a high\nlevel of noise while increasing the communication by only a constant factor\n(i.e., constant rate).\n  In this work we consider synchronous communication networks over an arbitrary\ntopology, in the powerful adversarial insertion-deletion noise model. Namely,\nthe noisy channel may adversarially alter the content of any transmitted\nsymbol, as well as completely remove a transmitted symbol or inject a new\nsymbol into the channel. We provide efficient, constant rate schemes that\nsuccessfully conduct any computation with high probability as long as the\nadversary corrupts at most $\\varepsilon /m$ fraction of the total\ncommunication, where $m$ is the number of links in the network and\n$\\varepsilon$ is a small constant. This scheme assumes the parties share a\nrandom string to which the adversarial noise is oblivious. We can remove this\nassumption at the price of being resilient to $\\varepsilon / (m\\log m)$\nadversarial error.\n  While previous work considered the insertion-deletion noise model in the\ntwo-party setting, to the best of our knowledge, our scheme is the first\nmultiparty scheme that is resilient to insertions and deletions. Furthermore,\nour scheme is the first computationally efficient scheme in the multiparty\nsetting that is resilient to adversarial noise.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:09:07 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gelles", "Ran", ""], ["Kalai", "Yael T.", ""], ["Ramnarayan", "Govind", ""]]}, {"id": "1901.09877", "submitter": "David Saulpic", "authors": "Niklas Hjuler, Giuseppe F. Italiano, Nikos Parotsidis, David Saulpic", "title": "Dominating Sets and Connected Dominating Sets in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the dynamic versions of two basic graph problems:\nMinimum Dominating Set and its variant Minimum Connected Dominating Set. For\nthose two problems, we present algorithms that maintain a solution under edge\ninsertions and edge deletions in time $O(\\Delta\\cdot \\text{polylog}~n)$ per\nupdate, where $\\Delta$ is the maximum vertex degree in the graph. In both\ncases, we achieve an approximation ratio of $O(\\log n)$, which is optimal up to\na constant factor (under the assumption that $P \\ne NP$). Although those two\nproblems have been widely studied in the static and in the distributed\nsettings, to the best of our knowledge we are the first to present efficient\nalgorithms in the dynamic setting.\n  As a further application of our approach, we also present an algorithm that\nmaintains a Minimal Dominating Set in $O(min(\\Delta, \\sqrt{m}))$ per update.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:47:22 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Hjuler", "Niklas", ""], ["Italiano", "Giuseppe F.", ""], ["Parotsidis", "Nikos", ""], ["Saulpic", "David", ""]]}, {"id": "1901.10026", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Nesreen K. Ahmed, Aldo Carranza, David Arbour, Anup\n  Rao, Sungchul Kim, and Eunyee Koh", "title": "Heterogeneous Network Motifs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications give rise to large heterogeneous networks where\nnodes and edges can be of any arbitrary type (e.g., user, web page, location).\nSpecial cases of such heterogeneous graphs include homogeneous graphs,\nbipartite, k-partite, signed, labeled graphs, among many others. In this work,\nwe generalize the notion of network motifs to heterogeneous networks. In\nparticular, small induced typed subgraphs called typed graphlets (heterogeneous\nnetwork motifs) are introduced and shown to be the fundamental building blocks\nof complex heterogeneous networks. Typed graphlets are a powerful\ngeneralization of the notion of graphlet (network motif) to heterogeneous\nnetworks as they capture both the induced subgraph of interest and the types\nassociated with the nodes in the induced subgraph. To address this problem, we\npropose a fast, parallel, and space-efficient framework for counting typed\ngraphlets in large networks. We discover the existence of non-trivial\ncombinatorial relationships between lower-order ($k-1$)-node typed graphlets\nand leverage them for deriving many of the $k$-node typed graphlets in $o(1)$\nconstant time. Thus, we avoid explicit enumeration of those typed graphlets.\nNotably, the time complexity matches the best untyped graphlet counting\nalgorithm. The experiments demonstrate the effectiveness of the proposed\nframework in terms of runtime, space-efficiency, parallel speedup, and\nscalability as it is able to handle large-scale networks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 22:49:42 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 18:32:30 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 02:10:43 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Carranza", "Aldo", ""], ["Arbour", "David", ""], ["Rao", "Anup", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""]]}, {"id": "1901.10045", "submitter": "Diptarama Hendrian", "authors": "Diptarama Hendrian, Takuya Takagi, Shunsuke Inenaga", "title": "Online Algorithms for Constructing Linear-size Suffix Trie", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix trees are fundamental data structures for various kinds of string\nprocessing. The suffix tree of a string $T$ of length $n$ has $O(n)$ nodes and\nedges, and the string label of each edge is encoded by a pair of positions in\n$T$. Thus, even after the tree is built, the input text $T$ needs to be kept\nstored and random access to $T$ is still needed. The linear-size suffix tries\n(LSTs), proposed by Crochemore et al. [Linear-size suffix tries, TCS\n638:171-178, 2016], are a `stand-alone' alternative to the suffix trees.\nNamely, the LST of a string $T$ of length $n$ occupies $O(n)$ total space, and\nsupports pattern matching and other tasks in the same efficiency as the suffix\ntree without the need to store the input text $T$. Crochemore et al. proposed\nan offline algorithm which transforms the suffix tree of $T$ into the LST of\n$T$ in $O(n \\log \\sigma)$ time and $O(n)$ space, where $\\sigma$ is the alphabet\nsize. In this paper, we present two types of online algorithms which `directly'\nconstruct the LST, from right to left, and from left to right, without\nconstructing the suffix tree as an intermediate structure. Both algorithms\nconstruct the LST incrementally when a new symbol is read, and do not access to\nthe previously read symbols. The right-to-left construction algorithm works in\n$O(n \\log \\sigma)$ time and $O(n)$ space and the left-to-right construction\nalgorithm works in $O(n (\\log \\sigma + \\log n / \\log \\log n))$ time and $O(n)$\nspace. The main feature of our algorithms is that the input text does not need\nto be stored.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 00:14:32 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 02:15:13 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 08:45:20 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Hendrian", "Diptarama", ""], ["Takagi", "Takuya", ""], ["Inenaga", "Shunsuke", ""]]}, {"id": "1901.10084", "submitter": "Nate Veldt", "authors": "Cameron Ruggles and Nate Veldt and David F. Gleich", "title": "A Parallel Projection Method for Metric Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering applications in machine learning and data mining rely on\nsolving metric-constrained optimization problems. These problems are\ncharacterized by $O(n^3)$ constraints that enforce triangle inequalities on\ndistance variables associated with $n$ objects in a large dataset. Despite its\nusefulness, metric-constrained optimization is challenging in practice due to\nthe cubic number of constraints and the high-memory requirements of standard\noptimization software. Recent work has shown that iterative projection methods\nare able to solve metric-constrained optimization problems on a much larger\nscale than was previously possible, thanks to their comparatively low memory\nrequirement. However, the major limitation of projection methods is their slow\nconvergence rate. In this paper we present a parallel projection method for\nmetric-constrained optimization which allows us to speed up the convergence\nrate in practice. The key to our approach is a new parallel execution schedule\nthat allows us to perform projections at multiple metric constraints\nsimultaneously without any conflicts or locking of variables. We illustrate the\neffectiveness of this execution schedule by implementing and testing a parallel\nprojection method for solving the metric-constrained linear programming\nrelaxation of correlation clustering. We show numerous experimental results on\nproblems involving up to 2.9 trillion constraints.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 03:14:24 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ruggles", "Cameron", ""], ["Veldt", "Nate", ""], ["Gleich", "David F.", ""]]}, {"id": "1901.10165", "submitter": "Fabio Cunial", "authors": "Fabio Cunial and Djamal Belazzougui", "title": "Fully-functional bidirectional Burrows-Wheeler indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $T$ on an alphabet of size $\\sigma$, we describe a\nbidirectional Burrows-Wheeler index that takes $O(|T|\\log{\\sigma})$ bits of\nspace, and that supports the addition \\emph{and removal} of one character, on\nthe left or right side of any substring of $T$, in constant time. Previously\nknown data structures that used the same space allowed constant-time addition\nto any substring of $T$, but they could support removal only from specific\nsubstrings of $T$. We also describe an index that supports bidirectional\naddition and removal in $O(\\log{\\log{|T|}})$ time, and that occupies a number\nof words proportional to the number of left and right extensions of the maximal\nrepeats of $T$. We use such fully-functional indexes to implement\nbidirectional, frequency-aware, variable-order de Bruijn graphs in small space,\nwith no upper bound on their order, and supporting natural criteria for\nincreasing and decreasing the order during traversal.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 08:33:09 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 14:08:49 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Cunial", "Fabio", ""], ["Belazzougui", "Djamal", ""]]}, {"id": "1901.10317", "submitter": "Jin-San Cheng", "authors": "Kai Jin, Jin-San Cheng", "title": "On the Complexity of Computing the Topology of Real Algebraic Space\n  Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deterministic algorithm to find a strong generic\nposition for an algebraic space curve. We modify our existing algorithm for\ncomputing the topology of an algebraic space curve and analyze the bit\ncomplexity of the algorithm. It is $\\tilde{\\mathcal {O}} (N^{20})$, where\n$N=\\max\\{d,\\tau\\}$, $d, \\tau$ are the degree bound and the bit size bound of\nthe coefficients of the defining polynomials of the algebraic space curve. To\nour knowledge, this is the best bound among the existing work. It gains the\nexisting results at least $N^2$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 10:38:46 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Jin", "Kai", ""], ["Cheng", "Jin-San", ""]]}, {"id": "1901.10330", "submitter": "Daniel Neuen", "authors": "Martin Grohe, Daniel Neuen", "title": "Canonisation and Definability for Graphs of Bounded Rank Width", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the combinatorial Weisfeiler-Leman algorithm of dimension\n$(3k+4)$ is a complete isomorphism test for the class of all graphs of rank\nwidth at most $k$. Rank width is a graph invariant that, similarly to tree\nwidth, measures the width of a certain style of hierarchical decomposition of\ngraphs; it is equivalent to clique width. It was known that isomorphism of\ngraphs of rank width $k$ is decidable in polynomial time (Grohe and Schweitzer,\nFOCS 2015), but the best previously known algorithm has a running time\n$n^{f(k)}$ for a non-elementary function $f$. Our result yields an isomorphism\ntest for graphs of rank width $k$ running in time $n^{O(k)}$. Another\nconsequence of our result is the first polynomial time canonisation algorithm\nfor graphs of bounded rank width. Our second main result is that fixed-point\nlogic with counting captures polynomial time on all graph classes of bounded\nrank width.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:06:58 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Grohe", "Martin", ""], ["Neuen", "Daniel", ""]]}, {"id": "1901.10387", "submitter": "Nima Anari", "authors": "Nima Anari and Vijay V. Vazirani", "title": "Matching is as Easy as the Decision Problem, in the NC Model", "comments": "Appeared in ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is matching in NC, i.e., is there a deterministic fast parallel algorithm for\nit? This has been an outstanding open question in TCS for over three decades,\never since the discovery of randomized NC matching algorithms [KUW85, MVV87].\nOver the last five years, the theoretical computer science community has\nlaunched a relentless attack on this question, leading to the discovery of\nseveral powerful ideas. We give what appears to be the culmination of this line\nof work: An NC algorithm for finding a minimum-weight perfect matching in a\ngeneral graph with polynomially bounded edge weights, provided it is given an\noracle for the decision problem. Consequently, for settling the main open\nproblem, it suffices to obtain an NC algorithm for the decision problem. We\nbelieve this new fact has qualitatively changed the nature of this open\nproblem.\n  All known efficient matching algorithms for general graphs follow one of two\napproaches: given by Edmonds [Edm65] and Lov\\'asz [Lov79]. Our oracle-based\nalgorithm follows a new approach and uses many of the ideas discovered in the\nlast five years.\n  The difficulty of obtaining an NC perfect matching algorithm led researchers\nto study matching vis-a-vis clever relaxations of the class NC. In this vein,\nrecently Goldwasser and Grossman [GG15] gave a pseudo-deterministic RNC\nalgorithm for finding a perfect matching in a bipartite graph, i.e., an RNC\nalgorithm with the additional requirement that on the same graph, it should\nreturn the same (i.e., unique) perfect matching for almost all choices of\nrandom bits. A corollary of our reduction is an analogous algorithm for general\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 16:53:57 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 07:15:56 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 17:33:14 GMT"}, {"version": "v4", "created": "Fri, 23 Aug 2019 12:48:35 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2020 00:36:43 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Anari", "Nima", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1901.10453", "submitter": "Diego D\\'iaz-Dom\\'inguez", "authors": "Diego D\\'iaz-Dom\\'inguez, Travis Gagie, Gonzalo Navarro", "title": "Simulating the DNA String Graph in Succinct Space", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Converting a set of sequencing reads into a lossless compact data structure\nthat encodes all the relevant biological information is a major challenge. The\nclassical approaches are to build the string graph or the de Bruijn graph. Each\nhas advantages over the other depending on the application. Still, the ideal\nsetting would be to have an index of the reads that is easy to build and can be\nadapted to any type of biological analysis. In this paper, we propose a new\ndata structure we call rBOSS, which gets close to that ideal. Our rBOSS is a de\nBruijn graph in practice, but it simulates any length up to k and can compute\noverlaps of size at least m between the labels of the nodes, with k and m being\nparameters. If we choose the parameter k equal to the size of the reads, then\nwe can simulate a complete string graph. As most BWT-based structures, rBOSS is\nunidirectional, but it exploits the property of the DNA reverse complements to\nsimulate bi-directionality with some time-space trade-offs. We implemented a\ngenome assembler on top of rBOSS to demonstrate its usefulness. Our\nexperimental results show that using k = 100, rBOSS can assemble 185 MB of\nreads in less than 15 minutes and using 110 MB in total. It produces contigs of\nmean sizes over 10,000, which is twice the size obtained by using a pure de\nBruijn graph of fixed length k.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:57:00 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 13:15:51 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["D\u00edaz-Dom\u00ednguez", "Diego", ""], ["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1901.10475", "submitter": "Stanley Bak", "authors": "Stanley Bak and Kerianne Hobbs", "title": "Efficient n-to-n Collision Detection for Space Debris using 4D AABB\n  Trees (Extended Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision detection algorithms are used in aerospace, swarm robotics,\nautomotive, video gaming, dynamics simulation and other domains. As many\napplications of collision detection run online, timing requirements are imposed\non the algorithm runtime: algorithms must, at a minimum, keep up with the\npassage of time. In practice, this places a limit on the number of objects, n,\nthat can be tracked at the same time. In this paper, we improve the scalability\nof collision detection, effectively raising the limit n for online object\ntracking.\n  The key to our approach is the use of a four-dimensional axis-aligned\nbounding box (AABB) tree, which stores each object's three-dimensional\noccupancy region in space during a one-dimensional interval of time. This\nimproves efficiency by permitting per-object variable times steps. Further, we\ndescribe partitioning strategies that can decompose the 4D AABB tree search\ninto several smaller-dimensional problems that can be solved in parallel. We\nformalize the collision detection problem and prove our algorithm's\ncorrectness. We demonstrate the feasibility of online collision detection for\nan orbital space debris application, using publicly available data on the full\ncatalog of n=16848 objects provided by www.space-track.org.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 21:28:03 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Bak", "Stanley", ""], ["Hobbs", "Kerianne", ""]]}, {"id": "1901.10633", "submitter": "Hideo Bannai", "authors": "Ryo Sugahara, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai, Masayuki\n  Takeda", "title": "Efficiently computing runs on a trie", "comments": "an updated version of CPM 2019 paper (10.4230/LIPIcs.CPM.2019.23),\n  submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximal repetition, or run, in a string, is a maximal periodic substring\nwhose smallest period is at most half the length of the substring. In this\npaper, we consider runs that correspond to a path on a trie, or in other words,\non a rooted edge-labeled tree where the endpoints of the path must be a\ndescendant/ancestor of the other. For a trie with $n$ edges, we show that the\nnumber of runs is less than $n$. We also show an asymptotic lower bound on the\nmaximum density of runs in tries:\n$\\lim_{n\\rightarrow\\infty}\\rho_\\mathcal{T}(n)/n \\geq 0.993238$ where\n$\\rho_{\\mathcal{T}}(n)$ is the maximum number of runs in a trie with $n$ edges.\nFurthermore, we also show an $O(n\\log \\log n)$ time and $O(n)$ space algorithm\nfor finding all runs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 01:26:07 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 05:05:45 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 07:37:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sugahara", "Ryo", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1901.10698", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, MohammadTaghi Hajiaghayi, Brendan Lucier, Michael\n  Mitzenmacher", "title": "Online Pandora's Boxes and Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online variations of the Pandora's box problem (Weitzman. 1979),\na standard model for understanding issues related to the cost of acquiring\ninformation for decision-making. Our problem generalizes both the classic\nPandora's box problem and the prophet inequality framework. Boxes are presented\nonline, each with a random value and cost drew jointly from some known\ndistribution. Pandora chooses online whether to open each box given its cost,\nand then chooses irrevocably whether to keep the revealed prize or pass on it.\nWe aim for approximation algorithms against adversaries that can choose the\nlargest prize over any opened box, and use optimal offline policies to decide\nwhich boxes to open (without knowledge of the value inside). We consider\nvariations where Pandora can collect multiple prizes subject to feasibility\nconstraints, such as cardinality, matroid, or knapsack constraints. We also\nconsider variations related to classic multi-armed bandit problems from\nreinforcement learning. Our results use a reduction-based framework where we\nseparate the issues of the cost of acquiring information from the online\ndecision process of which prizes to keep. Our work shows that in many\nscenarios, Pandora can achieve a good approximation to the best possible\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 07:52:39 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Lucier", "Brendan", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1901.10722", "submitter": "Mitsuru Funakoshi", "authors": "Mitsuru Funakoshi, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai and\n  Masayuki Takeda", "title": "Computing longest palindromic substring after single-character or\n  block-wise edits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Palindromes are important objects in strings which have been extensively\nstudied from combinatorial, algorithmic, and bioinformatics points of views. It\nis known that the length of the longest palindromic substrings (LPSs) of a\ngiven string T of length n can be computed in O(n) time by Manacher's algorithm\n[J. ACM '75]. In this paper, we consider the problem of finding the LPS after\nthe string is edited. We present an algorithm that uses O(n) time and space for\npreprocessing, and answers the length of the LPSs in O(\\log (\\min \\{\\sigma,\n\\log n\\})) time after a single character substitution, insertion, or deletion,\nwhere \\sigma denotes the number of distinct characters appearing in T. We also\npropose an algorithm that uses O(n) time and space for preprocessing, and\nanswers the length of the LPSs in O(\\ell + \\log \\log n) time, after an existing\nsubstring in T is replaced by a string of arbitrary length \\ell.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 09:31:31 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 11:03:22 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Funakoshi", "Mitsuru", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1901.10789", "submitter": "Kasper Green Larsen", "authors": "Allan Gr{\\o}nlund, Kasper Green Larsen, Alexander Mathiasen", "title": "Optimal Minimal Margin Maximization with Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting algorithms produce a classifier by iteratively combining base\nhypotheses. It has been observed experimentally that the generalization error\nkeeps improving even after achieving zero training error. One popular\nexplanation attributes this to improvements in margins. A common goal in a long\nline of research, is to maximize the smallest margin using as few base\nhypotheses as possible, culminating with the AdaBoostV algorithm by (R{\\\"a}tsch\nand Warmuth [JMLR'04]). The AdaBoostV algorithm was later conjectured to yield\nan optimal trade-off between number of hypotheses trained and the minimal\nmargin over all training points (Nie et al. [JMLR'13]). Our main contribution\nis a new algorithm refuting this conjecture. Furthermore, we prove a lower\nbound which implies that our new algorithm is optimal.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 12:52:45 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""], ["Mathiasen", "Alexander", ""]]}, {"id": "1901.10848", "submitter": "Matthias Bentert", "authors": "Matthias Bentert and Piotr Skowron", "title": "Comparing Election Methods Where Each Voter Ranks Only Few Candidates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Election rules are formal processes that aggregate voters preferences,\ntypically to select a single candidate, called the winner. Most of the election\nrules studied in the literature require the voters to rank the candidates from\nthe most to the least preferred one. This method of eliciting preferences is\nimpractical when the number of candidates to be ranked is large. We ask how\nwell certain election rules (focusing on positional scoring rules and the\nMinimax rule) can be approximated from partial preferences collected through\none of the following procedures: (i) randomized-we ask each voter to rank a\nrandom subset of $\\ell$ candidates, and (ii) deterministic-we ask each voter to\nprovide a ranking of her $\\ell$ most preferred candidates (the $\\ell$-truncated\nballot). We establish theoretical bounds on the approximation ratios and we\ncomplement our theoretical analysis with computer simulations. We find that\nmostly (apart from the cases when the preferences have no or very little\nstructure) it is better to use the randomized approach. While we obtain fairly\ngood approximation guarantees for the Borda rule already for $\\ell = 2$, for\napproximating the Minimax rule one needs to ask each voter to compare a larger\nset of candidates in order to obtain good guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 14:29:34 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Bentert", "Matthias", ""], ["Skowron", "Piotr", ""]]}, {"id": "1901.11260", "submitter": "Bruno Escoffier", "authors": "Evripidis Bampis and Bruno Escoffier and Alexandre Teiller", "title": "Multistage Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems have to be maintained while the underlying constraints, costs\nand/or profits change over time. Although the state of a system may evolve\nduring time, a non-negligible transition cost is incured for transitioning from\none state to another. In order to model such situations, Gupta et al. (ICALP\n2014) and Eisenstat et al. (ICALP 2014) introduced a multistage model where the\ninput is a sequence of instances (one for each time step), and the goal is to\nfind a sequence of solutions (one for each time step) that are both (i) near\noptimal for each time step and (ii) as stable as possible. We focus on the\nmultistage version of the Knapsack problem where we are given a time horizon\nt=1,2,...,T, and a sequence of knapsack instances I_1,I_2,...,I_T, one for each\ntime step, defined on a set of n objects. In every time step t we have to\nchoose a feasible knapsack S_t of I_t, which gives a knapsack profit. To\nmeasure the stability/similarity of two consecutive solutions S_t and S_{t+1},\nwe identify the objects for which the decision, to be picked or not, remains\nthe same in S_t and S_{t+1}, giving a transition profit. We are asked to\nproduce a sequence of solutions S_1,S_2,...,S_T so that the total knapsack\nprofit plus the overall transition profit is maximized.\n  We propose a PTAS for the Multistage Knapsack problem. Then, we prove that\nthere is no FPTAS for the problem even in the case where T=2, unless P=NP.\nFurthermore, we give a pseudopolynomial time algorithm for the case where the\nnumber of steps is bounded by a fixed constant and we show that otherwise the\nproblem remains NP-hard even in the case where all the weights, profits and\ncapacities are 0 or 1.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 08:21:54 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Bampis", "Evripidis", ""], ["Escoffier", "Bruno", ""], ["Teiller", "Alexandre", ""]]}, {"id": "1901.11305", "submitter": "Wiktor Zuba", "authors": "Mai Alzamel, Maxime Crochemore, Costas S. Iliopoulos, Tomasz\n  Kociumaka, Jakub Radoszewski, Wojciech Rytter, Juliusz Straszy\\'nski, Tomasz\n  Wale\\'n and Wiktor Zuba", "title": "Quasi-Linear-Time Algorithm for Longest Common Circular Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Longest Common Circular Factor (LCCF) problem in which,\ngiven strings $S$ and $T$ of length $n$, we are to compute the longest factor\nof $S$ whose cyclic shift occurs as a factor of $T$. It is a new similarity\nmeasure, an extension of the classic Longest Common Factor. We show how to\nsolve the LCCF problem in $O(n \\log^5 n)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 11:15:04 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Alzamel", "Mai", ""], ["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "1901.11453", "submitter": "J\\\"org Bachmann", "authors": "J\\\"org P. Bachmann", "title": "The SuperM-Tree: Indexing metric spaces with sized objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common approach to implementing similarity search applications is the usage\nof distance functions, where small distances indicate high similarity. In the\ncase of metric distance functions, metric index structures can be used to\naccelerate nearest neighbor queries. On the other hand, many applications ask\nfor approximate subsequences or subsets, e.g. searching for a similar partial\nsequence of a gene, for a similar scene in a movie, or for a similar object in\na picture which is represented by a set of multidimensional features. Metric\nindex structures such as the M-Tree cannot be utilized for these tasks because\nof the symmetry of the metric distance functions. In this work, we propose the\nSuperM-Tree as an extension of the M-Tree where approximate subsequence and\nsubset queries become nearest neighbor queries. In order to do this, we\nintroduce metric subset spaces as a generalized concept of metric spaces.\nVarious metric distance functions can be extended to metric subset distance\nfunctions, e.g. the Euclidean distance (on windows), the Hausdorff distance (on\nsubsets), the Edit distance and the Dog-Keeper distance (on subsequences). We\nshow that these examples subsume the applications mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:26:25 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 09:33:29 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Bachmann", "J\u00f6rg P.", ""]]}, {"id": "1901.11477", "submitter": "Amarnath R", "authors": "Amarnath R and P Nagabhushan", "title": "Text line Segmentation in Compressed Representation of Handwritten\n  Document using Tunneling Algorithm", "comments": "Compressed Representation, Handwritten Document Image, Text-Line\n  Terminal Point, Text-Line Segmentation, Search Space, Grid", "journal-ref": "International Journal of Intelligent Systems and Applications in\n  Engineering, Vol 6, No 4 (2018)", "doi": "10.18201/ijisae.2018448451", "report-no": null, "categories": "cs.CV cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research work, we perform text line segmentation directly in\ncompressed representation of an unconstrained handwritten document image. In\nthis relation, we make use of text line terminal points which is the current\nstate-of-the-art. The terminal points spotted along both margins (left and\nright) of a document image for every text line are considered as source and\ntarget respectively. The tunneling algorithm uses a single agent (or robot) to\nidentify the coordinate positions in the compressed representation to perform\ntext-line segmentation of the document. The agent starts at a source point and\nprogressively tunnels a path routing in between two adjacent text lines and\nreaches the probable target. The agent's navigation path from source to the\ntarget bypassing obstacles, if any, results in segregating the two adjacent\ntext lines. However, the target point would be known only when the agent\nreaches the destination; this is applicable for all source points and\nhenceforth we could analyze the correspondence between source and target nodes.\nArtificial Intelligence in Expert systems, dynamic programming and greedy\nstrategies are employed for every search space while tunneling. An exhaustive\nexperimentation is carried out on various benchmark datasets including ICDAR13\nand the performances are reported.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 05:19:38 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["R", "Amarnath", ""], ["Nagabhushan", "P", ""]]}]