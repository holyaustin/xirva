[{"id": "1507.00056", "submitter": "Or Sheffet", "authors": "Or Sheffet", "title": "Private Approximations of the 2nd-Moment Matrix Using Existing\n  Techniques in Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce three differentially-private algorithms that approximates the\n2nd-moment matrix of the data. These algorithm, which in contrast to existing\nalgorithms output positive-definite matrices, correspond to existing techniques\nin linear regression literature. Specifically, we discuss the following three\ntechniques. (i) For Ridge Regression, we propose setting the regularization\ncoefficient so that by approximating the solution using Johnson-Lindenstrauss\ntransform we preserve privacy. (ii) We show that adding a small batch of random\nsamples to our data preserves differential privacy. (iii) We show that sampling\nthe 2nd-moment matrix from a Bayesian posterior inverse-Wishart distribution is\ndifferentially private provided the prior is set correctly. We also evaluate\nour techniques experimentally and compare them to the existing \"Analyze Gauss\"\nalgorithm of Dwork et al.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 22:46:35 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 00:22:08 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Sheffet", "Or", ""]]}, {"id": "1507.00447", "submitter": "Shmuel Onn", "authors": "Asaf Levin, Shmuel Onn", "title": "Shifted Matroid Optimization", "comments": null, "journal-ref": "Operations Research Letters, 44:535-539, 2016", "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that finding lexicographically minimal $n$ bases in a matroid can be\ndone in polynomial time in the oracle model. This follows from a more general\nresult that the shifted problem over a matroid can be solved in polynomial time\nas well.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 07:19:30 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 09:20:41 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Levin", "Asaf", ""], ["Onn", "Shmuel", ""]]}, {"id": "1507.00505", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o, Fabrizio Grandoni, Luciano Gual\\`a, Stefano Leucci,\n  Guido Proietti", "title": "Improved Purely Additive Fault-Tolerant Spanners", "comments": "17 pages, 4 figures, ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be an unweighted $n$-node undirected graph. A \\emph{$\\beta$-additive\nspanner} of $G$ is a spanning subgraph $H$ of $G$ such that distances in $H$\nare stretched at most by an additive term $\\beta$ w.r.t. the corresponding\ndistances in $G$. A natural research goal related with spanners is that of\ndesigning \\emph{sparse} spanners with \\emph{low} stretch.\n  In this paper, we focus on \\emph{fault-tolerant} additive spanners, namely\nadditive spanners which are able to preserve their additive stretch even when\none edge fails. We are able to improve all known such spanners, in terms of\neither sparsity or stretch. In particular, we consider the sparsest known\nspanners with stretch $6$, $28$, and $38$, and reduce the stretch to $4$, $10$,\nand $14$, respectively (while keeping the same sparsity).\n  Our results are based on two different constructions. On one hand, we show\nhow to augment (by adding a \\emph{small} number of edges) a fault-tolerant\nadditive \\emph{sourcewise spanner} (that approximately preserves distances only\nfrom a given set of source nodes) into one such spanner that preserves all\npairwise distances. On the other hand, we show how to augment some known\nfault-tolerant additive spanners, based on clustering techniques. This way we\ndecrease the additive stretch without any asymptotic increase in their size. We\nalso obtain improved fault-tolerant additive spanners for the case of one\nvertex failure, and for the case of $f$ edge failures.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:15:53 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Grandoni", "Fabrizio", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1507.00552", "submitter": "Ninh Pham", "authors": "Rasmus Pagh and Ninh Pham and Francesco Silvestri and Morten St\\\"ockel", "title": "I/O-Efficient Similarity Join", "comments": "20 pages in Proceedings of the 23rd Annual European Symposium on\n  Algorithms 2015. The full version appeared in Algorithmica 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an I/O-efficient algorithm for computing similarity joins based on\nlocality-sensitive hashing (LSH). In contrast to the filtering methods commonly\nsuggested our method has provable sub-quadratic dependency on the data size.\nFurther, in contrast to straightforward implementations of known LSH-based\nalgorithms on external memory, our approach is able to take significant\nadvantage of the available internal memory: Whereas the time complexity of\nclassical algorithms includes a factor of $N^\\rho$, where $\\rho$ is a parameter\nof the LSH used, the I/O complexity of our algorithm merely includes a factor\n$(N/M)^\\rho$, where $N$ is the data size and $M$ is the size of internal\nmemory. Our algorithm is randomized and outputs the correct result with high\nprobability. It is a simple, recursive, cache-oblivious procedure, and we\nbelieve that it will be useful also in other computational settings such as\nparallel computation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 12:45:43 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 13:06:04 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Pagh", "Rasmus", ""], ["Pham", "Ninh", ""], ["Silvestri", "Francesco", ""], ["St\u00f6ckel", "Morten", ""]]}, {"id": "1507.00648", "submitter": "Manish Purohit", "authors": "MohammadTaghi Hajiaghayi, Guy Kortsarz, Robert MacDavid, Manish\n  Purohit, and Kanthi Sarpatwar", "title": "Approximation Algorithms for Connected Maximum Cut and Related Problems", "comments": "17 pages, Conference version to appear in ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of the Connected Maximum Cut problem consists of an undirected\ngraph G = (V, E) and the goal is to find a subset of vertices S $\\subseteq$ V\nthat maximizes the number of edges in the cut \\delta(S) such that the induced\ngraph G[S] is connected. We present the first non-trivial \\Omega(1/log n)\napproximation algorithm for the connected maximum cut problem in general graphs\nusing novel techniques. We then extend our algorithm to an edge weighted case\nand obtain a poly-logarithmic approximation algorithm. Interestingly, in stark\ncontrast to the classical max-cut problem, we show that the connected maximum\ncut problem remains NP-hard even on unweighted, planar graphs. On the positive\nside, we obtain a polynomial time approximation scheme for the connected\nmaximum cut problem on planar graphs and more generally on graphs with bounded\ngenus.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 16:20:13 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Hajiaghayi", "MohammadTaghi", ""], ["Kortsarz", "Guy", ""], ["MacDavid", "Robert", ""], ["Purohit", "Manish", ""], ["Sarpatwar", "Kanthi", ""]]}, {"id": "1507.00662", "submitter": "Manish Purohit", "authors": "Sreyash Kenkre, Vinayaka Pandit, Manish Purohit, and Rishi Saket", "title": "On the Approximability of Digraph Ordering", "comments": "21 pages, Conference version to appear in ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an n-vertex digraph D = (V, A) the Max-k-Ordering problem is to compute\na labeling $\\ell : V \\to [k]$ maximizing the number of forward edges, i.e.\nedges (u,v) such that $\\ell$(u) < $\\ell$(v). For different values of k, this\nreduces to Maximum Acyclic Subgraph (k=n), and Max-Dicut (k=2). This work\nstudies the approximability of Max-k-Ordering and its generalizations,\nmotivated by their applications to job scheduling with soft precedence\nconstraints. We give an LP rounding based 2-approximation algorithm for\nMax-k-Ordering for any k={2,..., n}, improving on the known\n2k/(k-1)-approximation obtained via random assignment. The tightness of this\nrounding is shown by proving that for any k={2,..., n} and constant\n$\\varepsilon > 0$, Max-k-Ordering has an LP integrality gap of 2 -\n$\\varepsilon$ for $n^{\\Omega\\left(1/\\log\\log k\\right)}$ rounds of the\nSherali-Adams hierarchy.\n  A further generalization of Max-k-Ordering is the restricted maximum acyclic\nsubgraph problem or RMAS, where each vertex v has a finite set of allowable\nlabels $S_v \\subseteq \\mathbb{Z}^+$. We prove an LP rounding based\n$4\\sqrt{2}/(\\sqrt{2}+1) \\approx 2.344$ approximation for it, improving on the\n$2\\sqrt{2} \\approx 2.828$ approximation recently given by Grandoni et al.\n(Information Processing Letters, Vol. 115(2), Pages 182-185, 2015). In fact,\nour approximation algorithm also works for a general version where the\nobjective counts the edges which go forward by at least a positive offset\nspecific to each edge.\n  The minimization formulation of digraph ordering is DAG edge deletion or\nDED(k), which requires deleting the minimum number of edges from an n-vertex\ndirected acyclic graph (DAG) to remove all paths of length k. We show that\nboth, the LP relaxation and a local ratio approach for DED(k) yield\nk-approximation for any $k\\in [n]$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 17:17:06 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Kenkre", "Sreyash", ""], ["Pandit", "Vinayaka", ""], ["Purohit", "Manish", ""], ["Saket", "Rishi", ""]]}, {"id": "1507.00710", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng and Anup Rao and Sushant Sachdeva", "title": "Fast, Provable Algorithms for Isotonic Regression in all\n  $\\ell_{p}$-norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices,\nthe Isotonic Regression of $y$ is a vector $x$ that respects the partial order\ndescribed by $G,$ and minimizes $||x-y||,$ for a specified norm. This paper\ngives improved algorithms for computing the Isotonic Regression for all\nweighted $\\ell_{p}$-norms with rigorous performance guarantees. Our algorithms\nare quite practical, and their variants can be implemented to run fast in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 19:42:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:14:21 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Kyng", "Rasmus", ""], ["Rao", "Anup", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1507.00748", "submitter": "Jochen Koenemann", "authors": "Hossein Efsandiari, MohammadTaghi Hajiaghyi, Jochen Koenemann, Hamid\n  Mahini, David Malec, Laura Sanita", "title": "Approximate Deadline-Scheduling with Precedence Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of scheduling a set of n jobs\nnon-preemptively on a single machine. Each job j has non-negative processing\ntime, weight, and deadline, and a feasible schedule needs to be consistent with\nchain-like precedence constraints. The goal is to compute a feasible schedule\nthat minimizes the sum of penalties of late jobs. Lenstra and Rinnoy Kan\n[Annals of Disc. Math., 1977] in their seminal work introduced this problem and\nshowed that it is strongly NP-hard, even when all processing times and weights\nare 1. We study the approximability of the problem and our main result is an\nO(log k)-approximation algorithm for instances with k distinct job deadlines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 20:22:22 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Efsandiari", "Hossein", ""], ["Hajiaghyi", "MohammadTaghi", ""], ["Koenemann", "Jochen", ""], ["Mahini", "Hamid", ""], ["Malec", "David", ""], ["Sanita", "Laura", ""]]}, {"id": "1507.00773", "submitter": "Jonathan Yaniv", "authors": "Yossi Azar, Inna Kalp-Shaltiel, Brendan Lucier, Ishai Menache, Joseph\n  (Seffi) Naor and Jonathan Yaniv", "title": "Truthful Online Scheduling with Commitments", "comments": null, "journal-ref": null, "doi": "10.1145/2764468.2764535", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online mechanisms for preemptive scheduling with deadlines, with the\ngoal of maximizing the total value of completed jobs. This problem is\nfundamental to deadline-aware cloud scheduling, but there are strong lower\nbounds even for the algorithmic problem without incentive constraints. However,\nthese lower bounds can be circumvented under the natural assumption of deadline\nslackness, i.e., that there is a guaranteed lower bound $s > 1$ on the ratio\nbetween a job's size and the time window in which it can be executed.\n  In this paper, we construct a truthful scheduling mechanism with a constant\ncompetitive ratio, given slackness $s > 1$. Furthermore, we show that if $s$ is\nlarge enough then we can construct a mechanism that also satisfies a commitment\nproperty: it can be determined whether or not a job will finish, and the\nrequisite payment if so, well in advance of each job's deadline. This is\nnotable because, in practice, users with strict deadlines may find it\nunacceptable to discover only very close to their deadline that their job has\nbeen rejected.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 21:41:12 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Azar", "Yossi", "", "Seffi"], ["Kalp-Shaltiel", "Inna", "", "Seffi"], ["Lucier", "Brendan", "", "Seffi"], ["Menache", "Ishai", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""], ["Yaniv", "Jonathan", ""]]}, {"id": "1507.00805", "submitter": "Yasuo Tabei", "authors": "Yoshimasa Takabatake, Yasuo Tabei, Hiroshi Sakamoto", "title": "Online Self-Indexed Grammar Compression", "comments": "To appear in the Proceedings of the 22nd edition of the International\n  Symposium on String Processing and Information Retrieval (SPIRE2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several grammar-based self-indexes have been proposed thus far,\ntheir applicability is limited to offline settings where whole input texts are\nprepared, thus requiring to rebuild index structures for given additional\ninputs, which is often the case in the big data era. In this paper, we present\nthe first online self-indexed grammar compression named OESP-index that can\ngradually build the index structure by reading input characters one-by-one.\nSuch a property is another advantage which enables saving a working space for\nconstruction, because we do not need to store input texts in memory. We\nexperimentally test OESP-index on the ability to build index structures and\nsearch query texts, and we show OESP-index's efficiency, especially\nspace-efficiency for building index structures.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 03:15:01 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 11:50:02 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Takabatake", "Yoshimasa", ""], ["Tabei", "Yasuo", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1507.00843", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Optimal linear Bernoulli factories for small mean problems", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 07:56:55 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 20:56:56 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1507.00990", "submitter": "Leo Liberti", "authors": "Ky Vu, Pierre-Louis Poirion, Leo Liberti", "title": "Using the Johnson-Lindenstrauss lemma in linear and integer programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Johnson-Lindenstrauss lemma allows dimension reduction on real vectors\nwith low distortion on their pairwise Euclidean distances. This result is often\nused in algorithms such as $k$-means or $k$ nearest neighbours since they only\nuse Euclidean distances, and has sometimes been used in optimization algorithms\ninvolving the minimization of Euclidean distances. In this paper we introduce a\nfirst attempt at using this lemma in the context of feasibility problems in\nlinear and integer programming, which cannot be expressed only in function of\nEuclidean distances.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 19:30:20 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Vu", "Ky", ""], ["Poirion", "Pierre-Louis", ""], ["Liberti", "Leo", ""]]}, {"id": "1507.01026", "submitter": "Dimitri Bertsekas", "authors": "Dimitri P. Bertsekas", "title": "Value and Policy Iteration in Optimal Control and Adaptive Dynamic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report LIDS-P-3174, Laboratory for Information and Decision Systems,\n  M.I.T., Cambridge, Mass", "categories": "cs.SY cs.DS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider discrete-time infinite horizon problems of optimal\ncontrol to a terminal set of states. These are the problems that are often\ntaken as the starting point for adaptive dynamic programming. Under very\ngeneral assumptions, we establish the uniqueness of solution of Bellman's\nequation, and we provide convergence results for value and policy iteration.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 20:41:40 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 20:29:17 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Bertsekas", "Dimitri P.", ""]]}, {"id": "1507.01029", "submitter": "Dimitri Bertsekas", "authors": "Dimitri P. Bertsekas", "title": "Lambda-Policy Iteration: A Review and a New Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report LIDS - 2874, Laboratory for Information and Decision Systems,\n  MIT, Cambridge, Mass., Feb. 2012", "categories": "cs.SY cs.DS math.NA math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we discuss $\\l$-policy iteration, a method for exact and\napproximate dynamic programming. It is intermediate between the classical value\niteration (VI) and policy iteration (PI) methods, and it is closely related to\noptimistic (also known as modified) PI, whereby each policy evaluation is done\napproximately, using a finite number of VI. We review the theory of the method\nand associated questions of bias and exploration arising in simulation-based\ncost function approximation. We then discuss various implementations, which\noffer advantages over well-established PI methods that use LSPE($\\l$),\nLSTD($\\l$), or TD($\\l$) for policy evaluation with cost function approximation.\nOne of these implementations is based on a new simulation scheme, called\ngeometric sampling, which uses multiple short trajectories rather than a single\ninfinitely long trajectory.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 21:09:29 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Bertsekas", "Dimitri P.", ""]]}, {"id": "1507.01030", "submitter": "Dimitri Bertsekas", "authors": "Dimitri P. Bertsekas", "title": "Incremental Gradient, Subgradient, and Proximal Methods for Convex\n  Optimization: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report LIDS - 2848, Laboratory for Information and Decision Systems,\n  MIT, Cambridge, Mass., Dec., 2010", "categories": "cs.SY cs.DS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey incremental methods for minimizing a sum $\\sum_{i=1}^mf_i(x)$\nconsisting of a large number of convex component functions $f_i$. Our methods\nconsist of iterations applied to single components, and have proved very\neffective in practice. We introduce a unified algorithmic framework for a\nvariety of such methods, some involving gradient and subgradient iterations,\nwhich are known, and some involving combinations of subgradient and proximal\nmethods, which are new and offer greater flexibility in exploiting the special\nstructure of $f_i$. We provide an analysis of the convergence and rate of\nconvergence properties of these methods, including the advantages offered by\nrandomization in the selection of components. We also survey applications in\ninference/machine learning, signal processing, and large-scale and distributed\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 21:20:51 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 20:11:29 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Bertsekas", "Dimitri P.", ""]]}, {"id": "1507.01159", "submitter": "Euiwoong Lee", "authors": "Euiwoong Lee", "title": "APX-Hardness of Maximizing Nash Social Welfare with Indivisible Items", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of allocating a set of indivisible items to agents with\nadditive utilities to maximize the Nash social welfare. Cole and Gkatzelis\nrecently proved that this problem admits a constant factor approximation. We\ncomplement their result by showing that this problem is APX-hard.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 02:14:46 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Lee", "Euiwoong", ""]]}, {"id": "1507.01196", "submitter": "Michael Dinitz", "authors": "Michael Dinitz, Michael Schapira, Asaf Valadarsky", "title": "Explicit Expanding Expanders", "comments": "Extended abstract appears in ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic constructions of expander graphs have been an important topic\nof research in computer science and mathematics, with many well-studied\nconstructions of infinite families of expanders. In some applications, though,\nan infinite family is not enough: we need expanders which are \"close\" to each\nother. We study the following question: Construct an an infinite sequence of\nexpanders $G_0,G_1,\\dots$, such that for every two consecutive graphs $G_i$ and\n$G_{i+1}$, $G_{i+1}$ can be obtained from $G_i$ by adding a single vertex and\ninserting/removing a small number of edges, which we call the expansion cost of\ntransitioning from $G_i$ to $G_{i+1}$. This question is very natural, e.g., in\nthe context of datacenter networks, where the vertices represent racks of\nservers, and the expansion cost captures the amount of rewiring needed when\nadding another rack to the network. We present an explicit construction of\n$d$-regular expanders with expansion cost at most $5d/2$, for any $d\\geq 6$.\nOur construction leverages the notion of a \"2-lift\" of a graph. This operation\nwas first analyzed by Bilu and Linial, who repeatedly applied 2-lifts to\nconstruct an infinite family of expanders which double in size from one\nexpander to the next. Our construction can be viewed as a way to \"interpolate\"\nbetween Bilu-Linial expanders with low expansion cost while preserving good\nedge expansion throughout.\n  While our main motivation is centralized (datacenter networks), we also get\nthe best-known distributed expander construction in the \"self-healing\" model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 11:22:45 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Dinitz", "Michael", ""], ["Schapira", "Michael", ""], ["Valadarsky", "Asaf", ""]]}, {"id": "1507.01231", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Computing Runs on a General Alphabet", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a RAM algorithm computing all runs (maximal repetitions) of a\ngiven string of length $n$ over a general ordered alphabet in\n$O(n\\log^{\\frac{2}3} n)$ time and linear space. Our algorithm outperforms all\nknown solutions working in $\\Theta(n\\log\\sigma)$ time provided $\\sigma =\nn^{\\Omega(1)}$, where $\\sigma$ is the alphabet size. We conjecture that there\nexists a linear time RAM algorithm finding all runs.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 15:42:02 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 19:57:52 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1507.01309", "submitter": "Joseph Cheriyan", "authors": "Joe Cheriyan and Zhihan Gao", "title": "Approximating (Unweighted) Tree Augmentation via Lift-and-Project, Part\n  II", "comments": "36 pages, 13 figures. Minor revisions to first draft of July 5. Added\n  reference [10], revised Introduction (paragraphs 3,4), added some content at\n  the end (last 2 pages, Section 8) including Corollary 8.10, Theorem 8.11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part II, we study the unweighted Tree Augmentation Problem (TAP) via the\nLasserre (Sum~of~Squares) system. We prove that the integrality ratio of an SDP\nrelaxation (the Lasserre tightening of an LP relaxation) is $\\leq\n\\frac{3}{2}+\\epsilon$, where $\\epsilon>0$ can be any small constant. We obtain\nthis result by designing a polynomial-time algorithm for TAP that achieves an\napproximation guarantee of ($\\frac32+\\epsilon$) relative to the SDP relaxation.\nThe algorithm is combinatorial and does not solve the SDP relaxation, but our\nanalysis relies on the SDP relaxation.\n  We generalize the combinatorial analysis of integral solutions from the\nprevious literature to fractional solutions by identifying some properties of\nfractional solutions of the Lasserre system via the decomposition result of\nKarlin, Mathieu and Nguyen (IPCO 2011).\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 00:20:32 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 20:25:52 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Cheriyan", "Joe", ""], ["Gao", "Zhihan", ""]]}, {"id": "1507.01345", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng", "title": "DiffNodesets: An Efficient Structure for Fast Mining Frequent Itemsets", "comments": "22 pages, 13 figures", "journal-ref": "Applied Soft Computing. 41 (2016) 214-223", "doi": "10.1016/j.asoc.2016.01.010", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemsets is an essential problem in data mining and plays an\nimportant role in many data mining applications. In recent years, some itemset\nrepresentations based on node sets have been proposed, which have shown to be\nvery efficient for mining frequent itemsets. In this paper, we propose\nDiffNodeset, a novel and more efficient itemset representation, for mining\nfrequent itemsets. Based on the DiffNodeset structure, we present an efficient\nalgorithm, named dFIN, to mining frequent itemsets. To achieve high efficiency,\ndFIN finds frequent itemsets using a set-enumeration tree with a hybrid search\nstrategy and directly enumerates frequent itemsets without candidate generation\nunder some case. For evaluating the performance of dFIN, we have conduct\nextensive experiments to compare it against with existing leading algorithms on\na variety of real and synthetic datasets. The experimental results show that\ndFIN is significantly faster than these leading algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 08:04:25 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Deng", "Zhi-Hong", ""]]}, {"id": "1507.01391", "submitter": "Nodari Sitchinava", "authors": "Peyman Afshani and Nodari Sitchinava", "title": "Sorting and Permuting without Bank Conflicts on GPUs", "comments": "12 pages, 2 figures, 23rd European Symposium on Algorithms (ESA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we look at the complexity of designing algorithms without any\nbank conflicts in the shared memory of Graphical Processing Units (GPUs). Given\ninput of size $n$, $w$ processors and $w$ memory banks, we study three\nfundamental problems: sorting, permuting and $w$-way partitioning (defined as\nsorting an input containing exactly $n/w$ copies of every integer in $[w]$).\n  We solve sorting in optimal $O(\\frac{n}{w} \\log n)$ time. When $n \\ge w^2$,\nwe solve the partitioning problem optimally in $O(n/w)$ time. We also present a\ngeneral solution for the partitioning problem which takes $O(\\frac{n}{w}\n\\log^3_{n/w} w)$ time. Finally, we solve the permutation problem using a\nrandomized algorithm in $O(\\frac{n}{w} \\log\\log\\log_{n/w} n)$ time. Our results\nshow evidence that when working with banked memory architectures, there is a\nseparation between these problems and the permutation and partitioning problems\nare not as easy as simple parallel scanning.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 11:20:34 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Afshani", "Peyman", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1507.01428", "submitter": "Peter Schneider-Kamp", "authors": "Michael Codish and Lu\\'is Cruz-Filipe and Thorsten Ehlers and Mike\n  M\\\"uller and Peter Schneider-Kamp", "title": "Sorting Networks: to the End and Back Again", "comments": "IMADA-preprint-cs. arXiv admin note: text overlap with\n  arXiv:1411.6408", "journal-ref": null, "doi": "10.1016/j.jcss.2016.04.004", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies new properties of the front and back ends of a sorting\nnetwork, and illustrates the utility of these in the search for new bounds on\noptimal sorting networks. Search focuses first on the \"outsides\" of the network\nand then on the inner part. All previous works focus only on properties of the\nfront end of networks and on how to apply these to break symmetries in the\nsearch. The new, out-side-in, properties help shed understanding on how sorting\nnetworks sort, and facilitate the computation of new bounds on optimal sorting\nnetworks. We present new parallel sorting networks for 17 to 20 inputs. For 17,\n19, and 20 inputs these networks are faster than the previously known best\nnetworks. For 17 inputs, the new sorting network is shown optimal in the sense\nthat no sorting network using less layers exists.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 12:57:38 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Codish", "Michael", ""], ["Cruz-Filipe", "Lu\u00eds", ""], ["Ehlers", "Thorsten", ""], ["M\u00fcller", "Mike", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1507.01490", "submitter": "Michele Borassi", "authors": "Michele Borassi, Pierluigi Crescenzi, Andrea Marino", "title": "Fast and Simple Computation of Top-k Closeness Centralities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closeness is an important centrality measure widely used in the analysis of\nreal-world complex networks. In particular, the problem of selecting the k most\ncentral nodes with respect to this measure has been deeply analyzed in the last\ndecade. However, even for not very large networks, this problem is\ncomputationally intractable in practice: indeed, Abboud et al have recently\nshown that its complexity is strictly related to the complexity of the\nAll-Pairs Shortest Path (in short, APSP) problem, for which no subcubic\n\"combinatorial\" algorithm is known. In this paper, we propose a new algorithm\nfor selecting the k most closeness central nodes in a graph. In practice, this\nalgorithm significantly improves over the APSP approach, even though its\nworst-case time complexity is the same. For example, the algorithm is able to\ncompute the top k nodes in few dozens of seconds even when applied to\nreal-world networks with millions of nodes and edges. We will also\nexperimentally prove that our algorithm drastically outperforms the most\nrecently designed algorithm, proposed by Olsen et al. Finally, we apply the new\nalgorithm to the computation of the most central actors in the IMDB\ncollaboration network, where two actors are linked if they played together in a\nmovie.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 14:58:24 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Borassi", "Michele", ""], ["Crescenzi", "Pierluigi", ""], ["Marino", "Andrea", ""]]}, {"id": "1507.01512", "submitter": "Irena Rusu Ph.D.", "authors": "Irena Rusu", "title": "Log-Lists and Their Applications to Sorting by Transpositions, Reversals\n  and Block-Interchanges", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link-cut trees have been introduced by D.D. Sleator and R.E. Tarjan (Journal\nof Computer and System Sciences, 1983) with the aim of efficiently maintaining\na forest of vertex-disjoint dynamic rooted trees under cut and link operations.\nThese operations respectively disconnect a subtree from a tree, and join two\ntrees by an edge. Additionally, link-cut trees allow to change the root of a\ntree and to perform a number of updates and queries on cost values defined on\nthe arcs of the trees. All these operations are performed in $O(\\log\\, n)$\namortized or worst-case time, depending on the implementation, where $n$ is the\ntotal size of the forest.\n  In this paper, we show that a list of elements implemented using link-cut\ntrees (we call it a $\\log$-list) allows us to obtain a common running time of\n$O(\\log\\, n)$ for the classical operations on lists, but also for some other\nessential operations that usually take linear time on lists. Such operations\nrequire to find the minimum/maximum element in a sublist defined by its\nendpoints, the position of a given element in the list or the element placed at\na given position in the list; or they require to add a value $a$, or to\nmultiply by $-1$, all the elements in a sublist.\n  Furthermore, we use $\\log$-lists to implement several existing algorithms for\nsorting permutations by transpositions and/or reversals and/or\nblock-interchanges, and obtain $O(n\\,\\log\\, n)$ running time for all of them.\nIn this way, the running time of several algorithms is improved, whereas in\nother cases our algorithms perform as well as the best existing\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 15:58:14 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 10:12:15 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Rusu", "Irena", ""]]}, {"id": "1507.01688", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad and Arnaud de Mesmay", "title": "A Fixed Parameter Tractable Approximation Scheme for the Optimal Cut\n  Graph of a Surface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ cellularly embedded on a surface $\\Sigma$ of genus $g$, a\ncut graph is a subgraph of $G$ such that cutting $\\Sigma$ along $G$ yields a\ntopological disk. We provide a fixed parameter tractable approximation scheme\nfor the problem of computing the shortest cut graph, that is, for any\n$\\varepsilon >0$, we show how to compute a $(1+ \\varepsilon)$ approximation of\nthe shortest cut graph in time $f(\\varepsilon, g)n^3$.\n  Our techniques first rely on the computation of a spanner for the problem\nusing the technique of brick decompositions, to reduce the problem to the case\nof bounded tree-width. Then, to solve the bounded tree-width case, we introduce\na variant of the surface-cut decomposition of Ru\\'e, Sau and Thilikos, which\nmay be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 07:12:29 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["de Mesmay", "Arnaud", ""]]}, {"id": "1507.01695", "submitter": "Stefano Leucci", "authors": "Annalisa D'Andrea, Mattia D'Emidio, Daniele Frigioni, Stefano Leucci,\n  Guido Proietti", "title": "Path-Fault-Tolerant Approximate Shortest-Path Trees", "comments": "21 pages, 3 figures, SIROCCO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an $n$-nodes non-negatively real-weighted undirected graph.\nIn this paper we show how to enrich a {\\em single-source shortest-path tree}\n(SPT) of $G$ with a \\emph{sparse} set of \\emph{auxiliary} edges selected from\n$E$, in order to create a structure which tolerates effectively a \\emph{path\nfailure} in the SPT. This consists of a simultaneous fault of a set $F$ of at\nmost $f$ adjacent edges along a shortest path emanating from the source, and it\nis recognized as one of the most frequent disruption in an SPT. We show that,\nfor any integer parameter $k \\geq 1$, it is possible to provide a very sparse\n(i.e., of size $O(kn\\cdot f^{1+1/k})$) auxiliary structure that carefully\napproximates (i.e., within a stretch factor of $(2k-1)(2|F|+1)$) the true\nshortest paths from the source during the lifetime of the failure. Moreover, we\nshow that our construction can be further refined to get a stretch factor of\n$3$ and a size of $O(n \\log n)$ for the special case $f=2$, and that it can be\nconverted into a very efficient \\emph{approximate-distance sensitivity oracle},\nthat allows to quickly (even in optimal time, if $k=1$) reconstruct the\nshortest paths (w.r.t. our structure) from the source after a path failure,\nthus permitting to perform promptly the needed rerouting operations. Our\nstructure compares favorably with previous known solutions, as we discuss in\nthe paper, and moreover it is also very effective in practice, as we assess\nthrough a large set of experiments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 07:59:40 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["D'Andrea", "Annalisa", ""], ["D'Emidio", "Mattia", ""], ["Frigioni", "Daniele", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1507.01732", "submitter": "Amos Fiat", "authors": "Amos Fiat, Ilia Gorelik, Haim Kaplan, Slava Novgorodov", "title": "The Temp Secretary Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of the secretary problem where contracts are\ntemporary, and for a fixed duration. This models online hiring of temporary\nemployees, or online auctions for re-usable resources. The problem is related\nto the question of Finding a large independent set in a random unit interval\ngraph.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 10:10:42 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 12:14:33 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Fiat", "Amos", ""], ["Gorelik", "Ilia", ""], ["Kaplan", "Haim", ""], ["Novgorodov", "Slava", ""]]}, {"id": "1507.01767", "submitter": "Frank Kammer", "authors": "Amr Elmasry and Frank Kammer", "title": "Space-Efficient Plane-Sweep Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce space-efficient plane-sweep algorithms for basic planar\ngeometric problems. It is assumed that the input is in a read-only array of $n$\nitems and that the available workspace is $\\Theta(s)$ bits, where $\\lg n \\leq s\n\\leq n \\cdot \\lg n$. Three techniques that can be used as general tools in\ndifferent space-efficient algorithms are introduced and employed within our\nalgorithms. In particular, we give an almost-optimal algorithm for finding the\nclosest pair among a set of $n$ points that runs in $O(n^2/s + n \\cdot \\lg s)$\ntime. We also give a simple algorithm to enumerate the intersections of $n$\nline segments that runs in $O((n^2/s^{2/3}) \\cdot \\lg s + k)$ time, where $k$\nis the number of intersections. The counting version can be solved in\n$O((n^2/s^{2/3}) \\cdot \\lg s)$~time. When the segments are axis-parallel, we\ngive an $O((n^2/s) \\cdot \\lg^{4/3} s + n^{4/3} \\cdot \\lg^{1/3} n)$-time\nalgorithm for counting the intersections, and an algorithm for enumerating the\nintersections that runs in $O((n^2/s) \\cdot \\lg s \\cdot \\lg \\lg s + n \\cdot \\lg\ns + k)$ time, where $k$ is the number of intersections. We finally present an\nalgorithm that runs in $O((n^2/s + n \\cdot \\lg s) \\cdot \\sqrt{(n/s) \\cdot \\lg\nn})$ time to calculate Klee's measure of axis-parallel rectangles.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 11:53:49 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 19:47:12 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Elmasry", "Amr", ""], ["Kammer", "Frank", ""]]}, {"id": "1507.01768", "submitter": "Ishay Haviv", "authors": "Ishay Haviv and Oded Regev", "title": "The Restricted Isometry Property of Subsampled Fourier Matrices", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matrix $A \\in \\mathbb{C}^{q \\times N}$ satisfies the restricted isometry\nproperty of order $k$ with constant $\\varepsilon$ if it preserves the $\\ell_2$\nnorm of all $k$-sparse vectors up to a factor of $1\\pm \\varepsilon$. We prove\nthat a matrix $A$ obtained by randomly sampling $q = O(k \\cdot \\log^2 k \\cdot\n\\log N)$ rows from an $N \\times N$ Fourier matrix satisfies the restricted\nisometry property of order $k$ with a fixed $\\varepsilon$ with high\nprobability. This improves on Rudelson and Vershynin (Comm. Pure Appl. Math.,\n2008), its subsequent improvements, and Bourgain (GAFA Seminar Notes, 2014).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 11:59:52 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 12:49:34 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Haviv", "Ishay", ""], ["Regev", "Oded", ""]]}, {"id": "1507.01917", "submitter": "Youming Qiao", "authors": "Joshua A. Grochow and Youming Qiao", "title": "Polynomial-time isomorphism test of groups that are tame extensions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.GR math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new polynomial-time algorithms for testing isomorphism of a class of\ngroups given by multiplication tables (GpI). Two results (Cannon & Holt, J.\nSymb. Comput. 2003; Babai, Codenotti & Qiao, ICALP 2012) imply that GpI reduces\nto the following: given groups G, H with characteristic subgroups of the same\ntype and isomorphic to $\\mathbb{Z}_p^d$, and given the coset of isomorphisms\n$Iso(G/\\mathbb{Z}_p^d, H/\\mathbb{Z}_p^d)$, compute Iso(G, H) in time poly(|G|).\nBabai & Qiao (STACS 2012) solved this problem when a Sylow p-subgroup of\n$G/\\mathbb{Z}_p^d$ is trivial. In this paper, we solve the preceding problem in\nthe so-called \"tame\" case, i.e., when a Sylow p-subgroup of $G/\\mathbb{Z}_p^d$\nis cyclic, dihedral, semi-dihedral, or generalized quaternion. These cases\ncorrespond exactly to the group algebra\n$\\overline{\\mathbb{F}}_p[G/\\mathbb{Z}_p^d]$ being of tame type, as in the\ncelebrated tame-wild dichotomy in representation theory. We then solve new\ncases of GpI in polynomial time.\n  Our result relies crucially on the divide-and-conquer strategy proposed\nearlier by the authors (CCC 2014), which splits GpI into two problems, one on\ngroup actions (representations), and one on group cohomology. Based on this\nstrategy, we combine permutation group and representation algorithms with new\nmathematical results, including bounds on the number of indecomposable\nrepresentations of groups in the tame case, and on the size of their cohomology\ngroups.\n  Finally, we note that when a group extension is not tame, the preceding\nbounds do not hold. This suggests a precise sense in which the tame-wild\ndichotomy from representation theory may also be a dividing line between the\n(currently) easy and hard instances of GpI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 18:46:49 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 02:20:15 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Qiao", "Youming", ""]]}, {"id": "1507.01926", "submitter": "Niklas Baumstark", "authors": "Niklas Baumstark, Guy Blelloch, Julian Shun", "title": "Efficient Implementation of a Synchronous Parallel Push-Relabel\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the observation that FIFO-based push-relabel algorithms are able\nto outperform highest label-based variants on modern, large maximum flow\nproblem instances, we introduce an efficient implementation of the algorithm\nthat uses coarse-grained parallelism to avoid the problems of existing parallel\napproaches. We demonstrate good relative and absolute speedups of our algorithm\non a set of large graph instances taken from real-world applications. On a\nmodern 40-core machine, our parallel implementation outperforms existing\nsequential implementations by up to a factor of 12 and other parallel\nimplementations by factors of up to 3.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 19:11:12 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 20:02:31 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Baumstark", "Niklas", ""], ["Blelloch", "Guy", ""], ["Shun", "Julian", ""]]}, {"id": "1507.01934", "submitter": "Hisao Tamaki", "authors": "Kenta Kitsunai, Yasuaki Kobayashi, and Hisao Tamaki", "title": "On the pathwidth of almost semicomplete digraphs", "comments": "33pages, a shorter version to appear in ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We call a digraph {\\em $h$-semicomplete} if each vertex of the digraph has at\nmost $h$ non-neighbors, where a non-neighbor of a vertex $v$ is a vertex $u\n\\neq v$ such that there is no edge between $u$ and $v$ in either direction.\nThis notion generalizes that of semicomplete digraphs which are\n$0$-semicomplete and tournaments which are semicomplete and have no\nanti-parallel pairs of edges. Our results in this paper are as follows. (1) We\ngive an algorithm which, given an $h$-semicomplete digraph $G$ on $n$ vertices\nand a positive integer $k$, in $(h + 2k + 1)^{2k} n^{O(1)}$ time either\nconstructs a path-decomposition of $G$ of width at most $k$ or concludes\ncorrectly that the pathwidth of $G$ is larger than $k$. (2) We show that there\nis a function $f(k, h)$ such that every $h$-semicomplete digraph of pathwidth\nat least $f(k, h)$ has a semicomplete subgraph of pathwidth at least $k$.\n  One consequence of these results is that the problem of deciding if a fixed\ndigraph $H$ is topologically contained in a given $h$-semicomplete digraph $G$\nadmits a polynomial-time algorithm for fixed $h$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 19:55:36 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Kitsunai", "Kenta", ""], ["Kobayashi", "Yasuaki", ""], ["Tamaki", "Hisao", ""]]}, {"id": "1507.01981", "submitter": "Wei Quan Lim", "authors": "Wei Quan Lim, Seth Gilbert, Wei Zhong Lim", "title": "Dynamic Reallocation Problems in Scheduling", "comments": "29 pages; updated references and other minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we look at the problem of scheduling tasks on a\nsingle-processor system, where each task requires unit time and must be\nscheduled within a certain time window, and each task can be added to or\nremoved from the system at any time. On each operation, the system is allowed\nto reschedule any tasks, but the goal is to minimize the number of rescheduled\ntasks. Our main result is an allocator that maintains a valid schedule for all\ntasks in the system if their time windows have constant size and reschedules\nO(1/{\\epsilon}*log(1/{\\epsilon})) tasks on each insertion as {\\epsilon}->0,\nwhere {\\epsilon} is a certain measure of the schedule flexibility of the\nsystem. We also show that it is optimal for any allocator that works on\narbitrary instances. We also briefly mention a few variants of the problem,\nsuch as if the tasks have time windows of difference sizes, for which we have\nan allocator that we conjecture reschedules only 1 task on each insertion if\nthe schedule flexibility remains above a certain threshold.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 22:28:39 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 18:07:17 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lim", "Wei Quan", ""], ["Gilbert", "Seth", ""], ["Lim", "Wei Zhong", ""]]}, {"id": "1507.02069", "submitter": "Lap Chi Lau", "authors": "Siu On Chan, Tsz Chiu Kwok, Lap Chi Lau", "title": "Random Walks and Evolving Sets: Faster Convergences and Limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the mixing time of random walks is a well-studied problem with\napplications in random sampling and more recently in graph partitioning. In\nthis work, we present new analysis of random walks and evolving sets using more\ncombinatorial graph structures, and show some implications in approximating\nsmall-set expansion. On the other hand, we provide examples showing the\nlimitations of using random walks and evolving sets in disproving the small-set\nexpansion hypothesis.\n  - We define a combinatorial analog of the spectral gap, and use it to prove\nthe convergence of non-lazy random walks. A corollary is a tight lower bound on\nthe small-set expansion of graph powers for any graph.\n  - We prove that random walks converge faster when the robust vertex expansion\nof the graph is larger. This provides an improved analysis of the local graph\npartitioning algorithm using the evolving set process.\n  - We give an example showing that the evolving set process fails to disprove\nthe small-set expansion hypothesis. This refutes a conjecture of Oveis Gharan\nand shows the limitations of local graph partitioning algorithms in\napproximating small-set expansion.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 08:41:05 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Chan", "Siu On", ""], ["Kwok", "Tsz Chiu", ""], ["Lau", "Lap Chi", ""]]}, {"id": "1507.02089", "submitter": "Guus Regts", "authors": "Guus Regts", "title": "Zero-free regions of partition functions with applications to algorithms\n  and graph limits", "comments": "Based on comments of the referees some changes have been made to\n  make. 21 pages. To appear in Combinatorica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a technique of Barvinok and Barvinok and Sober\\'on we identify a\nclass of edge-coloring models whose partition functions do not evaluate to zero\non bounded degree graphs. Subsequently we give a quasi-polynomial time\napproximation scheme for computing these partition functions. As another\napplication we show that the normalised partition functions of these models are\ncontinuous with respect the Benjamini-Schramm topology on bounded degree\ngraphs. We moreover give quasi-polynomial time approximation schemes for\nevaluating a large class of graph polynomials, including the Tutte polynomial,\non bounded degree graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 10:20:35 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 11:44:12 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 15:07:05 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Regts", "Guus", ""]]}, {"id": "1507.02119", "submitter": "Louxin Zhang", "authors": "Andreas D.M. Gunawan, Bhaskar DasGupta, Louxin Zhang", "title": "Locating a Tree in a Reticulation-Visible Network in Cubic Time", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we answer an open problem in the study of phylogenetic\nnetworks. Phylogenetic trees are rooted binary trees in which all edges are\ndirected away from the root, whereas phylogenetic networks are rooted acyclic\ndigraphs. For the purpose of evolutionary model validation, biologists often\nwant to know whether or not a phylogenetic tree is contained in a phylogenetic\nnetwork. The tree containment problem is NP-complete even for very restricted\nclasses of networks such as tree-sibling phylogenetic networks. We prove that\nthis problem is solvable in cubic time for stable phylogenetic networks. A\nlinear time algorithm is also presented for the cluster containment problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 12:18:03 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 04:29:05 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Gunawan", "Andreas D. M.", ""], ["DasGupta", "Bhaskar", ""], ["Zhang", "Louxin", ""]]}, {"id": "1507.02163", "submitter": "Marcin Pilipczuk", "authors": "Daniel Lokshtanov and Marcin Pilipczuk and Erik Jan van Leeuwen", "title": "Independence and Efficient Domination on $P_6$-free Graphs", "comments": "v2: added reference to independent work arXiv:1508.07733", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Independent set problem, the input is a graph $G$, every vertex has a\nnon-negative integer weight, and the task is to find a set $S$ of pairwise\nnon-adjacent vertices, maximizing the total weight of the vertices in $S$. We\ngive an $n^{O (\\log^2 n)}$ time algorithm for this problem on graphs excluding\nthe path $P_6$ on $6$ vertices as an induced subgraph. Currently, there is no\nconstant $k$ known for which Independent Set on $P_{k}$-free graphs becomes\nNP-complete, and our result implies that if such a $k$ exists, then $k > 6$\nunless all problems in NP can be decided in (quasi)polynomial time.\n  Using the combinatorial tools that we develop for the above algorithm, we\nalso give a polynomial-time algorithm for Efficient Dominating Set on\n$P_6$-free graphs. In this problem, the input is a graph $G$, every vertex has\nan integer weight, and the objective is to find a set $S$ of maximum weight\nsuch that every vertex in $G$ has exactly one vertex in $S$ in its closed\nneighborhood, or to determine that no such set exists. Prior to our work, the\nclass of $P_6$-free graphs was the only class of graphs defined by a single\nforbidden induced subgraph on which the computational complexity of Efficient\nDominating Set was unknown.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:12:58 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 15:36:07 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Marcin", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1507.02168", "submitter": "Marcin Pilipczuk", "authors": "Marcin Pilipczuk and Micha{\\l} Pilipczuk and Marcin Wrochna", "title": "Edge Bipartization faster than $2^k$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Edge Bipartization problem one is given an undirected graph $G$ and an\ninteger $k$, and the question is whether $k$ edges can be deleted from $G$ so\nthat it becomes bipartite. In 2006, Guo et al. [J. Comput. Syst. Sci.,\n72(8):1386-1396, 2006] proposed an algorithm solving this problem in time\n$O(2^k m^2)$; today, this algorithm is a textbook example of an application of\nthe iterative compression technique. Despite extensive progress in the\nunderstanding of the parameterized complexity of graph separation problems in\nthe recent years, no significant improvement upon this result has been yet\nreported.\n  We present an algorithm for Edge Bipartization that works in time $O(1.977^k\nnm)$, which is the first algorithm with the running time dependence on the\nparameter better than $2^k$. To this end, we combine the general iterative\ncompression strategy of Guo et al. [J. Comput. Syst. Sci., 72(8):1386-1396,\n2006], the technique proposed by Wahlstrom [SODA 2014, 1762-1781] of using a\npolynomial-time solvable relaxation in the form of a Valued Constraint\nSatisfaction Problem to guide a bounded-depth branching algorithm, and an\ninvolved Measure & Conquer analysis of the recursion tree.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:21:59 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2015 22:24:21 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 17:42:03 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1507.02178", "submitter": "Marcin Pilipczuk", "authors": "Marcin Pilipczuk and Magnus Wahlstr\\\"om", "title": "Directed multicut is W[1]-hard, even for four terminal pairs", "comments": "v2: Added almost tight ETH lower bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that Multicut in directed graphs, parameterized by the size of the\ncutset, is W[1]-hard and hence unlikely to be fixed-parameter tractable even if\nrestricted to instances with only four terminal pairs. This negative result\nalmost completely resolves one of the central open problems in the area of\nparameterized complexity of graph separation problems, posted originally by\nMarx and Razgon [SIAM J. Comput. 43(2):355-388 (2014)], leaving only the case\nof three terminal pairs open.\n  Our gadget methodology allows us also to prove W[1]-hardness of the Steiner\nOrientation problem parameterized by the number of terminal pairs, resolving an\nopen problem of Cygan, Kortsarz, and Nutov [SIAM J. Discrete Math.\n27(3):1503-1513 (2013)].\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:38:17 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 08:19:04 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 10:05:14 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Pilipczuk", "Marcin", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1507.02184", "submitter": "Sang-il Oum", "authors": "Jisu Jeong and Eun Jung Kim and Sang-il Oum", "title": "The \"art of trellis decoding\" is fixed-parameter tractable", "comments": "50 pages. Accepted to SODA 2016 under the title \"constructive\n  algorithms for path-width of matroids\". We added several figures to improve\n  its presentation. We found a mistake in the proof of Lemma 3.24 of the\n  previous version. In order to fix it, we changed some definitions in Section\n  3 and were able to recover our theorem", "journal-ref": "IEEE Trans. Inform. Theory, 63(11)(November 2017), pp. 7178-7205", "doi": "10.1109/TIT.2017.2740283", "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n subspaces of a finite-dimensional vector space over a fixed finite\nfield $\\mathbb F$, we wish to find a linear layout $V_1,V_2,\\ldots,V_n$ of the\nsubspaces such that $\\dim((V_1+V_2+\\cdots+V_i) \\cap (V_{i+1}+\\cdots+V_n))\\le k$\nfor all i, such a linear layout is said to have width at most k. When\nrestricted to 1-dimensional subspaces, this problem is equivalent to computing\nthe trellis-width (or minimum trellis state-complexity) of a linear code in\ncoding theory and computing the path-width of an $\\mathbb F$-represented\nmatroid in matroid theory.\n  We present a fixed-parameter tractable algorithm to construct a linear layout\nof width at most k, if it exists, for input subspaces of a finite-dimensional\nvector space over $\\mathbb F$. As corollaries, we obtain a fixed-parameter\ntractable algorithm to produce a path-decomposition of width at most k for an\ninput $\\mathbb F$-represented matroid of path-width at most k, and a\nfixed-parameter tractable algorithm to find a linear rank-decomposition of\nwidth at most k for an input graph of linear rank-width at most k. In both\ncorollaries, no such algorithms were known previously.\n  It was previously known that a fixed-parameter tractable algorithm exists for\nthe decision version of the problem for matroid path-width, a theorem by\nGeelen, Gerards, and Whittle~(2002) implies that for each fixed finite field\n$\\mathbb F$, there are finitely many forbidden $\\mathbb F$-representable minors\nfor the class of matroids of path-width at most k. An algorithm by\nHlin\\v{e}n\\'y (2006) can detect a minor in an input $\\mathbb F$-represented\nmatroid of bounded branch-width. However, this indirect approach would not\nproduce an actual path-decomposition. Our algorithm is the first one to\nconstruct such a path-decomposition and does not depend on the finiteness of\nforbidden minors.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:57:05 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 19:54:46 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 07:31:59 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 08:40:45 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Jeong", "Jisu", ""], ["Kim", "Eun Jung", ""], ["Oum", "Sang-il", ""]]}, {"id": "1507.02222", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Kasturi Varadarajan", "title": "Approximate Clustering via Metric Partitioning", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two metric covering/clustering problems -\n\\textit{Minimum Cost Covering Problem} (MCC) and $k$-clustering. In the MCC\nproblem, we are given two point sets $X$ (clients) and $Y$ (servers), and a\nmetric on $X \\cup Y$. We would like to cover the clients by balls centered at\nthe servers. The objective function to minimize is the sum of the $\\alpha$-th\npower of the radii of the balls. Here $\\alpha \\geq 1$ is a parameter of the\nproblem (but not of a problem instance). MCC is closely related to the\n$k$-clustering problem. The main difference between $k$-clustering and MCC is\nthat in $k$-clustering one needs to select $k$ balls to cover the clients.\n  For any $\\eps > 0$, we describe quasi-polynomial time $(1 + \\eps)$\napproximation algorithms for both of the problems. However, in case of\n$k$-clustering the algorithm uses $(1 + \\eps)k$ balls. Prior to our work, a\n$3^{\\alpha}$ and a ${c}^{\\alpha}$ approximation were achieved by\npolynomial-time algorithms for MCC and $k$-clustering, respectively, where $c >\n1$ is an absolute constant. These two problems are thus interesting examples of\nmetric covering/clustering problems that admit $(1 + \\eps)$-approximation\n(using $(1+\\eps)k$ balls in case of $k$-clustering), if one is willing to\nsettle for quasi-polynomial time. In contrast, for the variant of MCC where\n$\\alpha$ is part of the input, we show under standard assumptions that no\npolynomial time algorithm can achieve an approximation factor better than\n$O(\\log |X|)$ for $\\alpha \\geq \\log |X|$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 17:08:13 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 19:24:39 GMT"}, {"version": "v3", "created": "Mon, 3 Oct 2016 20:40:28 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Varadarajan", "Kasturi", ""]]}, {"id": "1507.02226", "submitter": "Quentin Stout", "authors": "Quentin F. Stout", "title": "L infinity Isotonic Regression for Linear, Multidimensional, and Tree\n  Orders", "comments": "updated references, minor modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are given for determining $L_\\infty$ isotonic regression of\nweighted data. For a linear order, grid in multidimensional space, or tree, of\n$n$ vertices, optimal algorithms are given, taking $\\Theta(n)$ time. These\nimprove upon previous algorithms by a factor of $\\Omega(\\log n)$. For vertices\nat arbitrary positions in $d$-dimensional space a $\\Theta(n \\log^{d-1} n)$\nalgorithm employs iterative sorting to yield the functionality of a\nmultidimensional structure while using only $\\Theta(n)$ space. The algorithms\nutilize a new non-constructive feasibility test on a rendezvous graph, with\nbounded error envelopes at each vertex.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 17:16:28 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 22:06:37 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Stout", "Quentin F.", ""]]}, {"id": "1507.02259", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yin Tat Lee, Lorenzo Orecchia", "title": "Using Optimization to Obtain a Width-Independent, Parallel, Simpler, and\n  Faster Positive SDP Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.NA math.OC math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of polylogarithmic depth algorithms for approximately\nsolving packing and covering semidefinite programs (or positive SDPs for\nshort). This is a natural SDP generalization of the well-studied positive LP\nproblem.\n  Although positive LPs can be solved in polylogarithmic depth while using only\n$\\tilde{O}(\\log^{2} n/\\varepsilon^2)$ parallelizable iterations, the best known\npositive SDP solvers due to Jain and Yao require $O(\\log^{14} n\n/\\varepsilon^{13})$ parallelizable iterations. Several alternative solvers have\nbeen proposed to reduce the exponents in the number of iterations. However, the\ncorrectness of the convergence analyses in these works has been called into\nquestion, as they both rely on algebraic monotonicity properties that do not\ngeneralize to matrix algebra.\n  In this paper, we propose a very simple algorithm based on the optimization\nframework proposed for LP solvers. Our algorithm only needs $\\tilde{O}(\\log^2 n\n/ \\varepsilon^2)$ iterations, matching that of the best LP solver. To surmount\nthe obstacles encountered by previous approaches, our analysis requires a new\nmatrix inequality that extends Lieb-Thirring's inequality, and a\nsign-consistent, randomized variant of the gradient truncation technique\nproposed in.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 19:09:33 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 20:34:43 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Lee", "Yin Tat", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1507.02268", "submitter": "Jelani Nelson", "authors": "Michael B. Cohen, Jelani Nelson, David P. Woodruff", "title": "Optimal approximate matrix product in terms of stable rank", "comments": "v3: minor edits; v2: fixed one step in proof of Theorem 9 which was\n  wrong by a constant factor (see the new Lemma 5 and its use; final theorem\n  unaffected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10].\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 19:45:21 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 22:33:26 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 12:58:32 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Cohen", "Michael B.", ""], ["Nelson", "Jelani", ""], ["Woodruff", "David P.", ""]]}, {"id": "1507.02314", "submitter": "Stefan Kiefer", "authors": "Stefan Kiefer and A. Prasad Sistla", "title": "Distinguishing Hidden Markov Chains", "comments": "This is the full version of a LICS'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Chains (HMCs) are commonly used mathematical models of\nprobabilistic systems. They are employed in various fields such as speech\nrecognition, signal processing, and biological sequence analysis. We consider\nthe problem of distinguishing two given HMCs based on an observation sequence\nthat one of the HMCs generates. More precisely, given two HMCs and an\nobservation sequence, a distinguishing algorithm is expected to identify the\nHMC that generates the observation sequence. Two HMCs are called\ndistinguishable if for every $\\varepsilon > 0$ there is a distinguishing\nalgorithm whose error probability is less than $\\varepsilon$. We show that one\ncan decide in polynomial time whether two HMCs are distinguishable. Further, we\npresent and analyze two distinguishing algorithms for distinguishable HMCs. The\nfirst algorithm makes a decision after processing a fixed number of\nobservations, and it exhibits two-sided error. The second algorithm processes\nan unbounded number of observations, but the algorithm has only one-sided\nerror. The error probability, for both algorithms, decays exponentially with\nthe number of processed observations. We also provide an algorithm for\ndistinguishing multiple HMCs. Finally, we discuss an application in stochastic\nruntime verification.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 21:14:14 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 11:43:06 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Kiefer", "Stefan", ""], ["Sistla", "A. Prasad", ""]]}, {"id": "1507.02318", "submitter": "Chao Xu", "authors": "Konstantinos Koiliaris, Chao Xu", "title": "A Faster Pseudopolynomial Time Algorithm for Subset Sum", "comments": "Fixed Lemma 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a multiset $S$ of $n$ positive integers and a target integer $t$, the\nsubset sum problem is to decide if there is a subset of $S$ that sums up to\n$t$. We present a new divide-and-conquer algorithm that computes all the\nrealizable subset sums up to an integer $u$ in\n$\\widetilde{O}\\!\\left(\\min\\{\\sqrt{n}u,u^{4/3},\\sigma\\}\\right)$, where $\\sigma$\nis the sum of all elements in $S$ and $\\widetilde{O}$ hides polylogarithmic\nfactors. This result improves upon the standard dynamic programming algorithm\nthat runs in $O(nu)$ time. To the best of our knowledge, the new algorithm is\nthe fastest general algorithm for this problem. We also present a modified\nalgorithm for cyclic groups, which computes all the realizable subset sums\nwithin the group in $\\widetilde{O}\\!\\left(\\min\\{\\sqrt{n}m,m^{5/4}\\}\\right)$\ntime, where $m$ is the order of the group.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 21:38:20 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 17:47:01 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 05:34:40 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Koiliaris", "Konstantinos", ""], ["Xu", "Chao", ""]]}, {"id": "1507.02323", "submitter": "Naman Agarwal", "authors": "Naman Agarwal and Afonso S. Bandeira and Konstantinos Koiliaris and\n  Alexandra Kolla", "title": "Multisection in the Stochastic Block Model using Semidefinite\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying underlying community-like structures\nin graphs. Towards this end we study the Stochastic Block Model (SBM) on\n$k$-clusters: a random model on $n=km$ vertices, partitioned in $k$ equal sized\nclusters, with edges sampled independently across clusters with probability $q$\nand within clusters with probability $p$, $p>q$. The goal is to recover the\ninitial \"hidden\" partition of $[n]$. We study semidefinite programming (SDP)\nbased algorithms in this context. In the regime $p = \\frac{\\alpha \\log(m)}{m}$\nand $q = \\frac{\\beta \\log(m)}{m}$ we show that a certain natural SDP based\nalgorithm solves the problem of {\\em exact recovery} in the $k$-community SBM,\nwith high probability, whenever $\\sqrt{\\alpha} - \\sqrt{\\beta} > \\sqrt{1}$, as\nlong as $k=o(\\log n)$. This threshold is known to be the information\ntheoretically optimal. We also study the case when $k=\\theta(\\log(n))$. In this\ncase however we achieve recovery guarantees that no longer match the optimal\ncondition $\\sqrt{\\alpha} - \\sqrt{\\beta} > \\sqrt{1}$, thus leaving achieving\noptimality for this range an open question.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 22:02:45 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Agarwal", "Naman", ""], ["Bandeira", "Afonso S.", ""], ["Koiliaris", "Konstantinos", ""], ["Kolla", "Alexandra", ""]]}, {"id": "1507.02331", "submitter": "Khodakhast Bibak", "authors": "Khodakhast Bibak, Bruce M. Kapron, Venkatesh Srinivasan, L\\'aszl\\'o\n  T\\'oth", "title": "On an almost-universal hash function family with applications to\n  authentication and secrecy codes", "comments": "International Journal of Foundations of Computer Science, to appear", "journal-ref": "International Journal of Foundations of Computer Science 29\n  (2018), 357-375", "doi": "10.1142/S0129054118500089", "report-no": null, "categories": "cs.CR cs.DS math.CO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal hashing, discovered by Carter and Wegman in 1979, has many\nimportant applications in computer science. MMH$^*$, which was shown to be\n$\\Delta$-universal by Halevi and Krawczyk in 1997, is a well-known universal\nhash function family. We introduce a variant of MMH$^*$, that we call GRDH,\nwhere we use an arbitrary integer $n>1$ instead of prime $p$ and let the keys\n$\\mathbf{x}=\\langle x_1, \\ldots, x_k \\rangle \\in \\mathbb{Z}_n^k$ satisfy the\nconditions $\\gcd(x_i,n)=t_i$ ($1\\leq i\\leq k$), where $t_1,\\ldots,t_k$ are\ngiven positive divisors of $n$. Then via connecting the universal hashing\nproblem to the number of solutions of restricted linear congruences, we prove\nthat the family GRDH is an $\\varepsilon$-almost-$\\Delta$-universal family of\nhash functions for some $\\varepsilon<1$ if and only if $n$ is odd and\n$\\gcd(x_i,n)=t_i=1$ $(1\\leq i\\leq k)$. Furthermore, if these conditions are\nsatisfied then GRDH is $\\frac{1}{p-1}$-almost-$\\Delta$-universal, where $p$ is\nthe smallest prime divisor of $n$. Finally, as an application of our results,\nwe propose an authentication code with secrecy scheme which strongly\ngeneralizes the scheme studied by Alomair et al. [{\\it J. Math. Cryptol.} {\\bf\n4} (2010), 121--148], and [{\\it J.UCS} {\\bf 15} (2009), 2937--2956].\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 22:53:41 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 11:03:23 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 07:50:44 GMT"}, {"version": "v4", "created": "Fri, 21 Apr 2017 05:21:42 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bibak", "Khodakhast", ""], ["Kapron", "Bruce M.", ""], ["Srinivasan", "Venkatesh", ""], ["T\u00f3th", "L\u00e1szl\u00f3", ""]]}, {"id": "1507.02351", "submitter": "Lior Seeman", "authors": "Ashwinkumar Badanidiyuru, Christos Papadimitriou, Aviad Rubinstein,\n  Lior Seeman, Yaron Singer", "title": "Locally Adaptive Optimization: Adaptive Seeding for Monotone Submodular\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adaptive Seeding problem is an algorithmic challenge motivated by\ninfluence maximization in social networks: One seeks to select among certain\naccessible nodes in a network, and then select, adaptively, among neighbors of\nthose nodes as they become accessible in order to maximize a global objective\nfunction. More generally, adaptive seeding is a stochastic optimization\nframework where the choices in the first stage affect the realizations in the\nsecond stage, over which we aim to optimize.\n  Our main result is a $(1-1/e)^2$-approximation for the adaptive seeding\nproblem for any monotone submodular function. While adaptive policies are often\napproximated via non-adaptive policies, our algorithm is based on a novel\nmethod we call \\emph{locally-adaptive} policies. These policies combine a\nnon-adaptive global structure, with local adaptive optimizations. This method\nenables the $(1-1/e)^2$-approximation for general monotone submodular functions\nand circumvents some of the impossibilities associated with non-adaptive\npolicies.\n  We also introduce a fundamental problem in submodular optimization that may\nbe of independent interest: given a ground set of elements where every element\nappears with some small probability, find a set of expected size at most $k$\nthat has the highest expected value over the realization of the elements. We\nshow a surprising result: there are classes of monotone submodular functions\n(including coverage) that can be approximated almost optimally as the\nprobability vanishes. For general monotone submodular functions we show via a\nreduction from \\textsc{Planted-Clique} that approximations for this problem are\nnot likely to be obtainable. This optimization problem is an important tool for\nadaptive seeding via non-adaptive policies, and its hardness motivates the\nintroduction of \\emph{locally-adaptive} policies we use in the main result.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:31:20 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Papadimitriou", "Christos", ""], ["Rubinstein", "Aviad", ""], ["Seeman", "Lior", ""], ["Singer", "Yaron", ""]]}, {"id": "1507.02378", "submitter": "Jiri Sgall", "authors": "Marcin Bienkowski, Martin B\\\"ohm, Jaroslaw Byrka, Marek Chrobak,\n  Christoph D\\\"urr, Luk\\'a\\v{s} Folwarczn\\'y, {\\L}ukasz Je\\.z, Ji\\v{r}\\'i\n  Sgall, Nguyen Kim Thang, Pavel Vesel\\'y", "title": "Online Algorithms for Multi-Level Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Multi-Level Aggregation Problem (MLAP), requests arrive at the nodes\nof an edge-weighted tree T, and have to be served eventually. A service is\ndefined as a subtree X of T that contains its root. This subtree X serves all\nrequests that are pending in the nodes of X, and the cost of this service is\nequal to the total weight of X. Each request also incurs waiting cost between\nits arrival and service times. The objective is to minimize the total waiting\ncost of all requests plus the total cost of all service subtrees. MLAP is a\ngeneralization of some well-studied optimization problems; for example, for\ntrees of depth 1, MLAP is equivalent to the TCP Acknowledgment Problem, while\nfor trees of depth 2, it is equivalent to the Joint Replenishment Problem.\nAggregation problem for trees of arbitrary depth arise in multicasting, sensor\nnetworks, communication in organization hierarchies, and in supply-chain\nmanagement. The instances of MLAP associated with these applications are\nnaturally online, in the sense that aggregation decisions need to be made\nwithout information about future requests.\n  Constant-competitive online algorithms are known for MLAP with one or two\nlevels. However, it has been open whether there exist constant competitive\nonline algorithms for trees of depth more than 2. Addressing this open problem,\nwe give the first constant competitive online algorithm for networks of\narbitrary (fixed) number of levels. The competitive ratio is O(D^4 2^D), where\nD is the depth of T. The algorithm works for arbitrary waiting cost functions,\nincluding the variant with deadlines.\n  We also show several additional lower and upper bound results for some\nspecial cases of MLAP, including the Single-Phase variant and the case when the\ntree is a path.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 05:01:37 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 21:22:42 GMT"}, {"version": "v3", "created": "Wed, 28 Dec 2016 14:59:21 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Bienkowski", "Marcin", ""], ["B\u00f6hm", "Martin", ""], ["Byrka", "Jaroslaw", ""], ["Chrobak", "Marek", ""], ["D\u00fcrr", "Christoph", ""], ["Folwarczn\u00fd", "Luk\u00e1\u0161", ""], ["Je\u017c", "\u0141ukasz", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["Thang", "Nguyen Kim", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1507.02384", "submitter": "Jisu Jeong", "authors": "Jisu Jeong, Sigve Hortemo S{\\ae}ther, Jan Arne Telle", "title": "Maximum matching width: new characterizations and a fast algorithm for\n  dominating set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give alternative definitions for maximum matching width, e.g. a graph $G$\nhas $\\operatorname{mmw}(G) \\leq k$ if and only if it is a subgraph of a chordal\ngraph $H$ and for every maximal clique $X$ of $H$ there exists $A,B,C \\subseteq\nX$ with $A \\cup B \\cup C=X$ and $|A|,|B|,|C| \\leq k$ such that any subset of\n$X$ that is a minimal separator of $H$ is a subset of either $A, B$ or $C$.\nTreewidth and branchwidth have alternative definitions through intersections of\nsubtrees, where treewidth focuses on nodes and branchwidth focuses on edges. We\nshow that mm-width combines both aspects, focusing on nodes and on edges. Based\non this we prove that given a graph $G$ and a branch decomposition of mm-width\n$k$ we can solve Dominating Set in time $O^*({8^k})$, thereby beating\n$O^*(3^{\\operatorname{tw}(G)})$ whenever $\\operatorname{tw}(G) > \\log_3{8}\n\\times k \\approx 1.893 k$. Note that $\\operatorname{mmw}(G) \\leq\n\\operatorname{tw}(G)+1 \\leq 3 \\operatorname{mmw}(G)$ and these inequalities are\ntight. Given only the graph $G$ and using the best known algorithms to find\ndecompositions, maximum matching width will be better for solving Dominating\nSet whenever $\\operatorname{tw}(G) > 1.549 \\times \\operatorname{mmw}(G)$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 05:58:47 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Jeong", "Jisu", ""], ["S\u00e6ther", "Sigve Hortemo", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1507.02407", "submitter": "Julian Yarkony", "authors": "Julian Yarkony, Charless C. Fowlkes", "title": "Planar Ultrametric Rounding for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of hierarchical clustering on planar graphs. We\nformulate this in terms of an LP relaxation of ultrametric rounding. To solve\nthis LP efficiently we introduce a dual cutting plane scheme that uses minimum\ncost perfect matching as a subroutine in order to efficiently explore the space\nof planar partitions. We apply our algorithm to the problem of hierarchical\nimage segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 08:07:34 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 21:28:24 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2015 03:32:55 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Yarkony", "Julian", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1507.02414", "submitter": "Angelo Fanelli", "authors": "Angelo Fanelli and Gianluigi Greco", "title": "Ride Sharing with a Vehicle of Unlimited Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A ride sharing problem is considered where we are given a graph, whose edges\nare equipped with a travel cost, plus a set of objects, each associated with a\ntransportation request given by a pair of origin and destination nodes. A\nvehicle travels through the graph, carrying each object from its origin to its\ndestination without any bound on the number of objects that can be\nsimultaneously transported. The vehicle starts and terminates its ride at given\nnodes, and the goal is to compute a minimum-cost ride satisfying all requests.\nThis ride sharing problem is shown to be tractable on paths by designing a $O(h\n\\log h+n)$ algorithm, with $h$ being the number of distinct requests and with\n$n$ being the number of nodes in the path. The algorithm is then used as a\nsubroutine to efficiently solve instances defined over cycles, hence covering\nall graphs with maximum degree $2$. This traces the frontier of tractability,\nsince $\\bf NP$-hard instances are exhibited over trees whose maximum degree is\n$3$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 08:31:35 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 15:05:00 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 10:55:38 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Fanelli", "Angelo", ""], ["Greco", "Gianluigi", ""]]}, {"id": "1507.02426", "submitter": "Jakub {\\L}\\k{a}cki", "authors": "Pawe{\\l} Brach, Marek Cygan, Jakub {\\L}\\k{a}cki, Piotr Sankowski", "title": "Algorithmic Complexity of Power Law Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was experimentally observed that the majority of real-world networks\nfollow power law degree distribution. The aim of this paper is to study the\nalgorithmic complexity of such \"typical\" networks. The contribution of this\nwork is twofold.\n  First, we define a deterministic condition for checking whether a graph has a\npower law degree distribution and experimentally validate it on real-world\nnetworks. This definition allows us to derive interesting properties of power\nlaw networks. We observe that for exponents of the degree distribution in the\nrange $[1,2]$ such networks exhibit double power law phenomenon that was\nobserved for several real-world networks. Our observation indicates that this\nphenomenon could be explained by just pure graph theoretical properties.\n  The second aim of our work is to give a novel theoretical explanation why\nmany algorithms run faster on real-world data than what is predicted by\nalgorithmic worst-case analysis. We show how to exploit the power law degree\ndistribution to design faster algorithms for a number of classical P-time\nproblems including transitive closure, maximum matching, determinant, PageRank\nand matrix inverse. Moreover, we deal with the problems of counting triangles\nand finding maximum clique. Previously, it has been only shown that these\nproblems can be solved very efficiently on power law graphs when these graphs\nare random, e.g., drawn at random from some distribution. However, it is\nunclear how to relate such a theoretical analysis to real-world graphs, which\nare fixed. Instead of that, we show that the randomness assumption can be\nreplaced with a simple condition on the degrees of adjacent vertices, which can\nbe used to obtain similar results. As a result, in some range of power law\nexponents, we are able to solve the maximum clique problem in polynomial time,\nalthough in general power law networks the problem is NP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 09:11:26 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Brach", "Pawe\u0142", ""], ["Cygan", "Marek", ""], ["\u0141\u0105cki", "Jakub", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1507.02479", "submitter": "Ramanujan M. S.", "authors": "Robert Ganian, M. S. Ramanujan, Stefan Szeider", "title": "Discovering Archipelagos of Tractability for Constraint Satisfaction and\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Constraint Satisfaction Problem (CSP) is a central and generic\ncomputational problem which provides a common framework for many theoretical\nand practical applications. A central line of research is concerned with the\nidentification of classes of instances for which CSP can be solved in\npolynomial time; such classes are often called \"islands of tractability.\" A\nprominent way of defining islands of tractability for CSP is to restrict the\nrelations that may occur in the constraints to a fixed set, called a constraint\nlanguage, whereas a constraint language is conservative if it contains all\nunary relations. This paper addresses the general limit of the mentioned\ntractability results for CSP and #CSP, that they only apply to instances where\nall constraints belong to a single tractable language (in general, the union of\ntwo tractable languages isn't tractable). We show that we can overcome this\nlimitation as long as we keep some control of how constraints over the various\nconsidered tractable languages interact with each other. For this purpose we\nutilize the notion of a \\emph{strong backdoor} of a CSP instance, as introduced\nby Williams et al. (IJCAI 2003), which is a set of variables that when\ninstantiated moves the instance to an island of tractability, i.e., to a\ntractable class of instances. In this paper, we consider strong backdoors into\n\\emph{scattered classes}, consisting of CSP instances where each connected\ncomponent belongs entirely to some class from a list of tractable classes. Our\nmain result is an algorithm that, given a CSP instance with $n$ variables,\nfinds in time $f(k)n^{O(1)}$ a strong backdoor into a scattered class\n(associated with a list of finite conservative constraint languages) of size\n$k$ or correctly decides that there isn't such a backdoor.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 12:25:49 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 12:10:45 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Ganian", "Robert", ""], ["Ramanujan", "M. S.", ""], ["Szeider", "Stefan", ""]]}, {"id": "1507.02482", "submitter": "Or Sheffet", "authors": "Or Sheffet", "title": "Differentially Private Ordinary Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is one of the most prevalent techniques in machine\nlearning, however, it is also common to use linear regression for its\n\\emph{explanatory} capabilities rather than label prediction. Ordinary Least\nSquares (OLS) is often used in statistics to establish a correlation between an\nattribute (e.g. gender) and a label (e.g. income) in the presence of other\n(potentially correlated) features. OLS assumes a particular model that randomly\ngenerates the data, and derives \\emph{$t$-values} --- representing the\nlikelihood of each real value to be the true correlation. Using $t$-values, OLS\ncan release a \\emph{confidence interval}, which is an interval on the reals\nthat is likely to contain the true correlation, and when this interval does not\nintersect the origin, we can \\emph{reject the null hypothesis} as it is likely\nthat the true correlation is non-zero. Our work aims at achieving similar\nguarantees on data under differentially private estimators. First, we show that\nfor well-spread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives\na very good approximation of $t$-values, secondly, when JLT approximates Ridge\nregression (linear regression with $l_2$-regularization) we derive, under\ncertain conditions, confidence intervals using the projected data, lastly, we\nderive, under different conditions, confidence intervals for the \"Analyze\nGauss\" algorithm (Dwork et al, STOC 2014).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 12:32:19 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 02:19:03 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 00:24:42 GMT"}, {"version": "v4", "created": "Mon, 21 Aug 2017 21:30:27 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Sheffet", "Or", ""]]}, {"id": "1507.02525", "submitter": "Aleksandr Cariow", "authors": "Bartosz Andreatto, Aleksandr Cariow", "title": "An algorithm for fast computation of the multiresolution discrete\n  Fourier transform", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a computationally effective algorithm for calculating\nthe multiresolution discrete Fourier transform (MrDFT). The algorithm is based\non the idea of reducing the computational complexity which was introduced by\nWen and Sandler [10] and utilizes the vectorization of calculating process at\neach stage of the considered transformation. This allows for the use of a\ncomputational process parallelization and results in a reduction of computation\ntime. In the description of the computational procedure, which describes the\nalgorithm, we use the matrix notation. This notation enables to represent\nadequately the space-time structures of the implemented computational process\nand directly map these structures into the constructions of a high-level\nprogramming language or into a hardware realization space.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 14:23:59 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Andreatto", "Bartosz", ""], ["Cariow", "Aleksandr", ""]]}, {"id": "1507.02564", "submitter": "Ronen Eldan", "authors": "S\\'ebastien Bubeck, Ronen Eldan, Joseph Lehec", "title": "Sampling from a log-concave distribution with Projected Langevin Monte\n  Carlo", "comments": "Preliminary version; 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Langevin Monte Carlo (LMC) algorithm to compactly supported\nmeasures via a projection step, akin to projected Stochastic Gradient Descent\n(SGD). We show that (projected) LMC allows to sample in polynomial time from a\nlog-concave distribution with smooth potential. This gives a new Markov chain\nto sample from a log-concave distribution. Our main result shows in particular\nthat when the target distribution is uniform, LMC mixes in $\\tilde{O}(n^7)$\nsteps (where $n$ is the dimension). We also provide preliminary experimental\nevidence that LMC performs at least as well as hit-and-run, for which a better\nmixing time of $\\tilde{O}(n^4)$ was proved by Lov{\\'a}sz and Vempala.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 15:44:57 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""], ["Lehec", "Joseph", ""]]}, {"id": "1507.02615", "submitter": "Rad Niazadeh", "authors": "Saeed Alaei, Jason Hartline, Rad Niazadeh, Emmanouil Pountourakis,\n  Yang Yuan", "title": "Optimal Auctions vs. Anonymous Pricing", "comments": "19 pages, 6 figures, To appear in 56th Annual IEEE Symposium on\n  Foundations of Computer Science (FOCS 2015)", "journal-ref": null, "doi": "10.1109/FOCS.2015.92", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For selling a single item to agents with independent but non-identically\ndistributed values, the revenue optimal auction is complex. With respect to it,\nHartline and Roughgarden (2009) showed that the approximation factor of the\nsecond-price auction with an anonymous reserve is between two and four. We\nconsider the more demanding problem of approximating the revenue of the ex ante\nrelaxation of the auction problem by posting an anonymous price (while supplies\nlast) and prove that their worst-case ratio is e. As a corollary, the\nupper-bound of anonymous pricing or anonymous reserves versus the optimal\nauction improves from four to $e$. We conclude that, up to an $e$ factor,\ndiscrimination and simultaneity are unimportant for driving revenue in\nsingle-item auctions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 17:42:37 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Alaei", "Saeed", ""], ["Hartline", "Jason", ""], ["Niazadeh", "Rad", ""], ["Pountourakis", "Emmanouil", ""], ["Yuan", "Yang", ""]]}, {"id": "1507.02618", "submitter": "S{\\o}ren Dahlgaard", "authors": "Stephen Alstrup and S{\\o}ren Dahlgaard and Mathias B{\\ae}k Tejs\n  Knudsen and Ely Porat", "title": "Sublinear Distance Labeling", "comments": "A preliminary version of this paper appeared at ESA'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distance labeling scheme labels the $n$ nodes of a graph with binary\nstrings such that, given the labels of any two nodes, one can determine the\ndistance in the graph between the two nodes by looking only at the labels. A\n$D$-preserving distance labeling scheme only returns precise distances between\npairs of nodes that are at distance at least $D$ from each other. In this paper\nwe consider distance labeling schemes for the classical case of unweighted\ngraphs with both directed and undirected edges.\n  We present a $O(\\frac{n}{D}\\log^2 D)$ bit $D$-preserving distance labeling\nscheme, improving the previous bound by Bollob\\'as et. al. [SIAM J. Discrete\nMath. 2005]. We also give an almost matching lower bound of\n$\\Omega(\\frac{n}{D})$. With our $D$-preserving distance labeling scheme as a\nbuilding block, we additionally achieve the following results:\n  1. We present the first distance labeling scheme of size $o(n)$ for sparse\ngraphs (and hence bounded degree graphs). This addresses an open problem by\nGavoille et. al. [J. Algo. 2004], hereby separating the complexity from\ndistance labeling in general graphs which require $\\Omega(n)$ bits, Moon [Proc.\nof Glasgow Math. Association 1965].\n  2. For approximate $r$-additive labeling schemes, that return distances\nwithin an additive error of $r$ we show a scheme of size $O\\left ( \\frac{n}{r}\n\\cdot\\frac{\\operatorname{polylog} (r\\log n)}{\\log n} \\right )$ for $r \\ge 2$.\nThis improves on the current best bound of $O\\left(\\frac{n}{r}\\right)$ by\nAlstrup et. al. [SODA 2016] for sub-polynomial $r$, and is a generalization of\na result by Gawrychowski et al. [arXiv preprint 2015] who showed this for\n$r=2$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 17:50:21 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 11:13:56 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Alstrup", "Stephen", ""], ["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Porat", "Ely", ""]]}, {"id": "1507.02731", "submitter": "Monirehalsadat Mahmoudi", "authors": "Monirehalsadat Mahmoudi, Xuesong Zhou", "title": "Finding optimal solutions for vehicle routing problem with pickup and\n  delivery services with time windows: A dynamic programming approach based on\n  state-space-time network representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of on-demand transportation systems and ride-sharing services\ninvolves solving a class of complex vehicle routing problems with pickup and\ndelivery with time windows (VRPPDTW). This paper first proposes a new\ntime-discretized multi-commodity network flow model for the VRPPDTW based on\nthe integration of vehicles carrying states within space-time transportation\nnetworks, so as to allow a joint optimization of passenger-to-vehicle\nassignment and turn-by-turn routing in congested transportation networks. Our\nthree-dimensional state-space-time network construct is able to comprehensively\nenumerate possible transportation states at any given time along vehicle\nspace-time paths, and further allows a forward dynamic programming solution\nalgorithm to solve the single vehicle VRPPDTW problem. By utilizing a\nLagrangian relaxation approach, the primal multi-vehicle routing problem is\ndecomposed to a sequence of single vehicle routing sub-problems, with\nLagrangian multipliers for individual passengers requests being updated by\nsub-gradient-based algorithms. We further discuss a number of search space\nreduction strategies and test our algorithms, implemented through a specialized\nprogram in C++, on medium-scale and large-scale transportation networks, namely\nthe Chicago sketch and Phoenix regional networks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 22:29:19 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 03:21:30 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Mahmoudi", "Monirehalsadat", ""], ["Zhou", "Xuesong", ""]]}, {"id": "1507.02799", "submitter": "Zeev Nutov", "authors": "Guy Kortsarz and Zeev Nutov", "title": "A simplified 1.5-approximation algorithm for augmenting\n  edge-connectivity of a graph from 1 to 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tree Augmentation Problem (TAP) is: given a connected graph $G=(V,{\\cal\nE})$ and an edge set $E$ on $V$ find a minimum size subset of edges $F\n\\subseteq E$ such that $(V,{\\cal E} \\cup F)$ is $2$-edge-connected. In the\nconference version \\cite{EFKN-APPROX} was sketched a $1.5$-approximation\nalgorithm for the problem. Since a full proof was very complex and long, the\njournal version was cut into two parts. In the first part \\cite{EFKN-TALG} was\nonly proved ratio $1.8$. An attempt to simplify the second part produced an\nerror in \\cite{EKN-IPL}. Here we give a correct, different, and self contained\nproof of the ratio $1.5$, that is also substantially simpler and shorter than\nthe previous proofs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 08:10:40 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Kortsarz", "Guy", ""], ["Nutov", "Zeev", ""]]}, {"id": "1507.02805", "submitter": "Moritz M\\\"uhlenthaler", "authors": "Moritz M\\\"uhlenthaler and Rolf Wanka", "title": "On the Connectedness of Clash-free Timetables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the connectedness of clash-free timetables with respect to the\nKempe-exchange operation. This investigation is related to the connectedness of\nthe search space of timetabling problem instances, which is a desirable\nproperty, for example for two-step algorithms using the Kempe-exchange during\nthe optimization step. The theoretical framework for our investigations is\nbased on the study of reconfiguration graphs, which model the search space of\ntimetabling problems. We contribute to this framework by including timeslot\navailability requirements in the analysis and we derive improved conditions for\nthe connectedness of clash-free timetables in this setting. We apply the\ntheoretical insights to establish the connectedness of clash-free timetables\nfor a number of benchmark instances.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 08:26:53 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["M\u00fchlenthaler", "Moritz", ""], ["Wanka", "Rolf", ""]]}, {"id": "1507.02808", "submitter": "Vincent Chau", "authors": "Eric Angel, Evripidis Bampis, Vincent Chau, Vassilis Zissimopoulos", "title": "Calibrations Scheduling Problem with Arbitrary Lengths and Activation\n  Length", "comments": "9 pages, 4 figures. A preliminary version appeared at FAW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bender et al. (SPAA 2013) have proposed a theoretical framework for testing\nin contexts where safety mistakes must be avoided. Testing in such a context is\nmade by machines that need to be often calibrated. Given that calibration\ncosts, it is important to study policies minimizing the calibration cost while\nperforming all the necessary tests. We focus on the single-machine setting and\nwe extend the model proposed by Bender et al. by considering that the jobs have\narbitrary processing times and that the preemption of jobs is allowed. For this\ncase, we propose an optimal polynomial time algorithm. Then, we study the case\nwhere there are several types of calibrations with different lengths and costs.\nWe first prove that the problem becomes NP-hard for arbitrary processing times\neven when the preemption of the jobs is allowed. Finally, we focus on the case\nof unit-time jobs and we show that a more general problem, where the\nrecalibration of the machine is not instantaneous but takes time, can be solved\nin polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 08:56:29 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 15:03:58 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Angel", "Eric", ""], ["Bampis", "Evripidis", ""], ["Chau", "Vincent", ""], ["Zissimopoulos", "Vassilis", ""]]}, {"id": "1507.02853", "submitter": "Patrick Hagge Cording", "authors": "Philip Bille, Anders Roy Christiansen, Patrick Hagge Cording, Inge Li\n  G{\\o}rtz", "title": "Finger Search in Grammar-Compressed Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammar-based compression, where one replaces a long string by a small\ncontext-free grammar that generates the string, is a simple and powerful\nparadigm that captures many popular compression schemes. Given a grammar, the\nrandom access problem is to compactly represent the grammar while supporting\nrandom access, that is, given a position in the original uncompressed string\nreport the character at that position. In this paper we study the random access\nproblem with the finger search property, that is, the time for a random access\nquery should depend on the distance between a specified index $f$, called the\n\\emph{finger}, and the query index $i$. We consider both a static variant,\nwhere we first place a finger and subsequently access indices near the finger\nefficiently, and a dynamic variant where also moving the finger such that the\ntime depends on the distance moved is supported.\n  Let $n$ be the size the grammar, and let $N$ be the size of the string. For\nthe static variant we give a linear space representation that supports placing\nthe finger in $O(\\log N)$ time and subsequently accessing in $O(\\log D)$ time,\nwhere $D$ is the distance between the finger and the accessed index. For the\ndynamic variant we give a linear space representation that supports placing the\nfinger in $O(\\log N)$ time and accessing and moving the finger in $O(\\log D +\n\\log \\log N)$ time. Compared to the best linear space solution to random\naccess, we improve a $O(\\log N)$ query bound to $O(\\log D)$ for the static\nvariant and to $O(\\log D + \\log \\log N)$ for the dynamic variant, while\nmaintaining linear space. As an application of our results we obtain an\nimproved solution to the longest common extension problem in grammar compressed\nstrings. To obtain our results, we introduce several new techniques of\nindependent interest, including a novel van Emde Boas style decomposition of\ngrammars.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 11:17:32 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 12:48:20 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 08:58:48 GMT"}, {"version": "v4", "created": "Wed, 16 Nov 2016 12:29:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bille", "Philip", ""], ["Christiansen", "Anders Roy", ""], ["Cording", "Patrick Hagge", ""], ["G\u00f8rtz", "Inge Li", ""]]}, {"id": "1507.02929", "submitter": "Jenna Birch", "authors": "Jenna Birch, Athanasios A. Pantelous and Konstantin Zuev", "title": "The Maximum Number of 3- and 4-Cliques within a Planar Maximally\n  Filtered Graph", "comments": "12 pages, 11 figures", "journal-ref": "Physica A 417 (2015) 221-229", "doi": "10.1016/j.physa.2014.09.011", "report-no": null, "categories": "math-ph cs.DS math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planar Maximally Filtered Graphs (PMFG) are an important tool for filtering\nthe most relevant information from correlation based networks such as stock\nmarket networks. One of the main characteristics of a PMFG is the number of its\n3- and 4-cliques. Recently in a few high impact papers it was stated that,\nbased on heuristic evidence, the maximum number of 3- and 4-cliques that can\nexist in a PMFG with n vertices is 3n - 8 and n - 4 respectively. In this\npaper, we prove that this is indeed the case.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 15:04:33 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Birch", "Jenna", ""], ["Pantelous", "Athanasios A.", ""], ["Zuev", "Konstantin", ""]]}, {"id": "1507.02989", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski and Robert Susik and Marcin Raniszewski", "title": "A Bloom filter based semi-index on $q$-grams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple $q$-gram based semi-index, which allows to look for a\npattern typically only in a small fraction of text blocks. Several space-time\ntradeoffs are presented. Experiments on Pizza & Chili datasets show that our\nsolution is up to three orders of magnitude faster than the Claude et al.\n\\cite{CNPSTjda10} semi-index at a comparable space usage.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 19:03:00 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Grabowski", "Szymon", ""], ["Susik", "Robert", ""], ["Raniszewski", "Marcin", ""]]}, {"id": "1507.03009", "submitter": "Zeev Nutov", "authors": "Guy Kortsarz and Zeev Nutov", "title": "A $1.75$ LP approximation for the Tree Augmentation Problem", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.02799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Tree Augmentation Problem (TAP) the goal is to augment a tree $T$ by a\nminimum size edge set $F$ from a given edge set $E$ such that $T \\cup F$ is\n$2$-edge-connected. The best approximation ratio known for TAP is $1.5$. In the\nmore general Weighted TAP problem, $F$ should be of minimum weight. Weighted\nTAP admits several $2$-approximation algorithms w.r.t. to the standard cut\nLP-relaxation, but for all of them the performance ratio of $2$ is tight even\nfor TAP. The problem is equivalent to the problem of covering a laminar set\nfamily. Laminar set families play an important role in the design of\napproximation algorithms for connectivity network design problems. In fact,\nWeighted TAP is the simplest connectivity network design problem for which a\nratio better than $2$ is not known. Improving this \"natural\" ratio is a major\nopen problem, which may have implications on many other network design\nproblems. It seems that achieving this goal requires finding an LP-relaxation\nwith integrality gap better than $2$, which is a long time open problem even\nfor TAP. In this paper we introduce such an LP-relaxation and give an algorithm\nthat computes a feasible solution for TAP of size at most $1.75$ times the\noptimal LP value. This gives some hope to break the ratio $2$ for the weighted\ncase. Our algorithm computes some initial edge set by solving a partial system\nof constraints that form the integral edge-cover polytope, and then applies\nlocal search on $3$-leaf subtrees to exchange some of the edges and to add\nadditional edges. Thus we do not need to solve the LP, and the algorithm runs\nroughly in time required to find a minimum weight edge-cover in a general\ngraph.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 08:41:13 GMT"}], "update_date": "2015-07-19", "authors_parsed": [["Kortsarz", "Guy", ""], ["Nutov", "Zeev", ""]]}, {"id": "1507.03046", "submitter": "Diego Cifuentes", "authors": "Diego Cifuentes and Pablo A. Parrilo", "title": "An efficient tree decomposition method for permanents and mixed\n  discriminants", "comments": "32 pages, 4 figures", "journal-ref": "Linear Algebra and its Applications, Volume 493, 15 March 2016,\n  pages 45-81", "doi": "10.1016/j.laa.2015.12.004", "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm to compute permanents, mixed discriminants\nand hyperdeterminants of structured matrices and multidimensional arrays\n(tensors). We describe the sparsity structure of an array in terms of a graph,\nand we assume that its treewidth, denoted as $\\omega$, is small. Our algorithm\nrequires $O(n 2^\\omega)$ arithmetic operations to compute permanents, and\n$O(n^2 + n 3^\\omega)$ for mixed discriminants and hyperdeterminants. We finally\nshow that mixed volume computation continues to be hard under bounded treewidth\nassumptions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 23:26:53 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Cifuentes", "Diego", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "1507.03067", "submitter": "Takeaki Uno", "authors": "Takeaki Uno, Hiroki Maegawa, Takanobu Nakahara, Yukinobu Hamuro, Ryo\n  Yoshinaka, Makoto Tatsuta", "title": "Micro-Clustering: Finding Small Clusters in Large Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of un-supervised soft-clustering called\nmicro-clustering. The aim of the problem is to enumerate all groups composed of\nrecords strongly related to each other, while standard clustering methods\nseparate records at sparse parts. The problem formulation of micro-clustering\nis non-trivial. Clique mining in a similarity graph is a typical approach, but\nit results in a huge number of cliques that are of many similar cliques. We\npropose a new concept data polishing. The cause of huge solutions can be\nconsidered that the groups are not clear in the data, that is, the boundaries\nof the groups are not clear, because of noise, uncertainty, lie, lack, etc.\nData polishing clarifies the groups by perturbating the data. Specifically,\ndense subgraphs that would correspond to clusters are replaced by cliques. The\nclusters are clarified as maximal cliques, thus the number of maximal cliques\nwill be drastically reduced. We also propose an efficient algorithm applicable\neven for large scale data. Computational experiments showed the efficiency of\nour algorithm, i.e., the number of solutions is small, (e.g., 1,000), the\nmembers of each group are deeply related, and the computation time is short.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 06:21:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 12:41:49 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Uno", "Takeaki", ""], ["Maegawa", "Hiroki", ""], ["Nakahara", "Takanobu", ""], ["Hamuro", "Yukinobu", ""], ["Yoshinaka", "Ryo", ""], ["Tatsuta", "Makoto", ""]]}, {"id": "1507.03225", "submitter": "Rasmus Pagh", "authors": "Rasmus Pagh", "title": "CoveringLSH: Locality-sensitive Hashing without False Negatives", "comments": "Short version appears in Proceedings of SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new construction of locality-sensitive hash functions for\nHamming space that is \\emph{covering} in the sense that is it guaranteed to\nproduce a collision for every pair of vectors within a given radius $r$. The\nconstruction is \\emph{efficient} in the sense that the expected number of hash\ncollisions between vectors at distance~$cr$, for a given $c>1$, comes close to\nthat of the best possible data independent LSH without the covering guarantee,\nnamely, the seminal LSH construction of Indyk and Motwani (STOC '98). The\nefficiency of the new construction essentially \\emph{matches} their bound when\nthe search radius is not too large --- e.g., when $cr = o(\\log(n)/\\log\\log n)$,\nwhere $n$ is the number of points in the data set, and when $cr = \\log(n)/k$\nwhere $k$ is an integer constant. In general, it differs by at most a factor\n$\\ln(4)$ in the exponent of the time bounds. As a consequence, LSH-based\nsimilarity search in Hamming space can avoid the problem of false negatives at\nlittle or no cost in efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 12:36:53 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 22:29:31 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 17:10:38 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Pagh", "Rasmus", ""]]}, {"id": "1507.03269", "submitter": "David Steurer", "authors": "Samuel B. Hopkins and Jonathan Shi and David Steurer", "title": "Tensor principal component analysis via sum-of-squares proofs", "comments": "published in Conference on Learning Theory (COLT) 2015 (submitted\n  February 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 20:30:09 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1507.03338", "submitter": "Mark Saroufim", "authors": "Mark Saroufim", "title": "Aren't we all nearest neighbors: Spatial trees, high dimensional\n  reductions and batch nearest neighbor search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start with a review of the pervasiveness of the nearest neighbor search\nproblem and techniques used to solve it along with some experimental results.\nIn the second chapter, we show reductions between two different classes of geo-\nmetric proximity problems: the nearest neighbor problems to solve the Euclidean\nminimum spanning tree problem and the farthest neighbor problems to solve the\nk-centers problem. In the third chapter, we unify spatial partitioning trees\nun- der one framework the meta-tree. Finally, we propose a dual tree algorithm\nfor Bichromatic Closest Pair and measure the complexity of batch nearest\nneighbor search.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 07:07:09 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Saroufim", "Mark", ""]]}, {"id": "1507.03348", "submitter": "Maurice Chandoo", "authors": "Maurice Chandoo", "title": "Deciding Circular-Arc Graph Isomorphism in Parameterized Logspace", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2016.26", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We compute a canonical circular-arc representation for a given circular-arc\n(CA) graph which implies solving the isomorphism and recognition problem for\nthis class. To accomplish this we split the class of CA graphs into uniform and\nnon-uniform ones and employ a generalized version of the argument given by\nK\\\"obler et al (2013) that has been used to show that the subclass of Helly CA\ngraphs can be canonized in logspace. For uniform CA graphs our approach works\nin logspace and in addition to that Helly CA graphs are a strict subset of\nuniform CA graphs. Thus our result is a generalization of the canonization\nresult for Helly CA graphs. In the non-uniform case a specific set of ambiguous\nvertices arises. By choosing the parameter to be the cardinality of this set\nthe obstacle can be solved by brute force. This leads to an O(k + log n) space\nalgorithm to compute a canonical representation for non-uniform and therefore\nall CA graphs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 08:01:13 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 11:41:56 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2015 10:16:40 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Chandoo", "Maurice", ""]]}, {"id": "1507.03403", "submitter": "Wolfgang Mulzer", "authors": "Matias Korman, Wolfgang Mulzer, Andre van Renssen, Marcel Roeloffzen,\n  Paul Seiferth, Yannik Stein", "title": "Time-Space Trade-offs for Triangulations and Voronoi Diagrams", "comments": "17 pages, 4 figures, a preliminary version appeared in WADS 2015", "journal-ref": "Computational Geometry: Theory and Applications (CGTA), 73, 2018,\n  pp. 35-45", "doi": "10.1016/j.comgeo.2017.01.001", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $S$ be a planar $n$-point set. A triangulation for $S$ is a maximal plane\nstraight-line graph with vertex set $S$. The Voronoi diagram for $S$ is the\nsubdivision of the plane into cells such that all points in a cell have the\nsame nearest neighbor in $S$. Classically, both structures can be computed in\n$O(n \\log n)$ time and $O(n)$ space. We study the situation when the available\nworkspace is limited: given a parameter $s \\in \\{1, \\dots, n\\}$, an\n$s$-workspace algorithm has read-only access to an input array with the points\nfrom $S$ in arbitrary order, and it may use only $O(s)$ additional words of\n$\\Theta(\\log n)$ bits for reading and writing intermediate data. The output\nshould then be written to a write-only structure. We describe a deterministic\n$s$-workspace algorithm for computing an arbitrary triangulation of $S$ in time\n$O(n^2/s + n \\log n \\log s )$ and a randomized $s$-workspace algorithm for\nfinding the Voronoi diagram of $S$ in expected time $O((n^2/s) \\log s + n \\log\ns \\log^*s)$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 11:36:37 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 20:03:44 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 10:45:45 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Korman", "Matias", ""], ["Mulzer", "Wolfgang", ""], ["van Renssen", "Andre", ""], ["Roeloffzen", "Marcel", ""], ["Seiferth", "Paul", ""], ["Stein", "Yannik", ""]]}, {"id": "1507.03549", "submitter": "Frank Vallentin", "authors": "Etienne de Klerk, Frank Vallentin", "title": "On the Turing model complexity of interior point methods for\n  semidefinite programming", "comments": "(v2) some comments added, 16 pages", "journal-ref": "SIAM J. Optim., 26(3), 1944-1961, 2016", "doi": "10.1137/15M103114X", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that one can solve semidefinite programs to within fixed accuracy\nin polynomial time using the ellipsoid method (under some assumptions). In this\npaper it is shown that the same holds true when one uses the short-step, primal\ninterior point method. The main idea of the proof is to employ Diophantine\napproximation at each iteration to bound the intermediate bit-sizes of\niterates.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 18:56:35 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 22:49:21 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["de Klerk", "Etienne", ""], ["Vallentin", "Frank", ""]]}, {"id": "1507.03558", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt\n  Rubinfeld", "title": "Testing Shape Restrictions of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of testing structured properties (classes) of discrete\ndistributions. Specifically, given sample access to an arbitrary distribution\n$D$ over $[n]$ and a property $\\mathcal{P}$, the goal is to distinguish between\n$D\\in\\mathcal{P}$ and $\\ell_1(D,\\mathcal{P})>\\varepsilon$. We develop a general\nalgorithm for this question, which applies to a large range of\n\"shape-constrained\" properties, including monotone, log-concave, $t$-modal,\npiecewise-polynomial, and Poisson Binomial distributions. Moreover, for all\ncases considered, our algorithm has near-optimal sample complexity with regard\nto the domain size and is computationally efficient. For most of these classes,\nwe provide the first non-trivial tester in the literature. In addition, we also\ndescribe a generic method to prove lower bounds for this problem, and use it to\nshow our upper bounds are nearly tight. Finally, we extend some of our\ntechniques to tolerant testing, deriving nearly-tight upper and lower bounds\nfor the corresponding questions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 19:22:41 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 16:28:19 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 19:56:27 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1507.03648", "submitter": "Cengis Hasan", "authors": "Cengis Hasan and Zygmunt J. Haas", "title": "Deadline-aware Power Management in Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamic power optimization problem in data centers. We formulate\nand solve the following offline problem: in which slot which server has to be\nassigned to which job; and in which slot which server has to be switched ON or\nOFF so that the total power is optimal for some time horizon. We show that the\noffline problem is a new version of generalized assignment problem including\nnew constraints issuing from deadline characteristics of jobs and difference of\nactivation energy of servers. We propose an online algorithm that solves the\nproblem heuristically and compare it to randomized routing.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 22:54:20 GMT"}], "update_date": "2015-07-19", "authors_parsed": [["Hasan", "Cengis", ""], ["Haas", "Zygmunt J.", ""]]}, {"id": "1507.03719", "submitter": "Huy Nguyen", "authors": "Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward", "title": "A New Framework for Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of problems in machine learning, including exemplar\nclustering, document summarization, and sensor placement, can be cast as\nconstrained submodular maximization problems. A lot of recent effort has been\ndevoted to developing distributed algorithms for these problems. However, these\nresults suffer from high number of rounds, suboptimal approximation ratios, or\nboth. We develop a framework for bringing existing algorithms in the sequential\nsetting to the distributed setting, achieving near optimal approximation ratios\nfor many settings in only a constant number of MapReduce rounds. Our techniques\nalso give a fast sequential algorithm for non-monotone maximization subject to\na matroid constraint.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 04:46:01 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 21:20:02 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Barbosa", "Rafael da Ponte", ""], ["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Ward", "Justin", ""]]}, {"id": "1507.03738", "submitter": "Alexander Golovnev", "authors": "Fedor V. Fomin, Alexander Golovnev, Alexander S. Kulikov, and Ivan\n  Mihajlin", "title": "Tight Bounds for Subgraph Isomorphism and Graph Homomorphism", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that unless Exponential Time Hypothesis (ETH) fails, deciding if\nthere is a homomorphism from graph $G$ to graph $H$ cannot be done in time\n$|V(H)|^{o(|V(G)|)}$. Combined with the reduction of Cygan, Pachocki, and\nSoca{\\l}a, our result rules out (subject to ETH) a possibility of\n$|V(G)|^{o(|V(G)|)}$-time algorithm deciding if graph $H$ is a subgraph of $G$.\nFor both problems our lower bounds asymptotically match the running time of\nbrute-force algorithms trying all possible mappings of one graph into another.\nThus, our work closes the gap in the known complexity of these fundamental\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 06:51:29 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovnev", "Alexander", ""], ["Kulikov", "Alexander S.", ""], ["Mihajlin", "Ivan", ""]]}, {"id": "1507.03823", "submitter": "Carsten Grimm", "authors": "Carsten Grimm", "title": "A Lower Bound on Supporting Predecessor Search in $k$ sorted Arrays", "comments": "This work was presented at the Young Researcher Workshop on Automata,\n  Languages and Programming (YR-ICALP 2015), July 5th, 2015 in Kyoto, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to perform efficient queries for the predecessor among $n$ values\nstored in $k$ sorted arrays. Evading the $\\Omega(n \\log k)$ lower bound from\nmerging $k$ arrays, we support predecessor queries in $O(\\log n)$ time after\n$O(n \\log(\\frac{k}{\\log n}))$ construction time. By applying Ben-Or's\ntechnique, we establish that this is optimal for strict predecessor queries,\ni.e., every data structure supporting $O(\\log n)$-time strict predecessor\nqueries requires $\\Omega(n \\log(\\frac{k}{\\log n}))$ construction time. Our\napproach generalizes as a template for deriving similar lower bounds on the\nconstruction time of data structures with some desired query time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 12:36:15 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Grimm", "Carsten", ""]]}, {"id": "1507.04046", "submitter": "Stephen Alstrup", "authors": "Stephen Alstrup and Inge Li G{\\o}rtz and Esben Bistrup Halvorsen and\n  Ely Porat", "title": "Distance labeling schemes for trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distance labeling schemes for trees: given a tree with $n$ nodes,\nlabel the nodes with binary strings such that, given the labels of any two\nnodes, one can determine, by looking only at the labels, the distance in the\ntree between the two nodes.\n  A lower bound by Gavoille et. al. (J. Alg. 2004) and an upper bound by Peleg\n(J. Graph Theory 2000) establish that labels must use $\\Theta(\\log^2 n)$\nbits\\footnote{Throughout this paper we use $\\log$ for $\\log_2$.}. Gavoille et.\nal. (ESA 2001) show that for very small approximate stretch, labels use\n$\\Theta(\\log n \\log \\log n)$ bits. Several other papers investigate various\nvariants such as, for example, small distances in trees (Alstrup et. al.,\nSODA'03).\n  We improve the known upper and lower bounds of exact distance labeling by\nshowing that $\\frac{1}{4} \\log^2 n$ bits are needed and that $\\frac{1}{2}\n\\log^2 n$ bits are sufficient. We also give ($1+\\epsilon$)-stretch labeling\nschemes using $\\Theta(\\log n)$ bits for constant $\\epsilon>0$.\n($1+\\epsilon$)-stretch labeling schemes with polylogarithmic label size have\npreviously been established for doubling dimension graphs by Talwar (STOC\n2004).\n  In addition, we present matching upper and lower bounds for distance labeling\nfor caterpillars, showing that labels must have size $2\\log n - \\Theta(\\log\\log\nn)$. For simple paths with $k$ nodes and edge weights in $[1,n]$, we show that\nlabels must have size $\\frac{k-1}{k}\\log n+\\Theta(\\log k)$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 23:18:17 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Alstrup", "Stephen", ""], ["G\u00f8rtz", "Inge Li", ""], ["Halvorsen", "Esben Bistrup", ""], ["Porat", "Ely", ""]]}, {"id": "1507.04188", "submitter": "Justin Thaler", "authors": "Justin Thaler", "title": "Stream Verification", "comments": "A significantly abridged version of this article is to appear in the\n  Springer Encyclopedia of Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey models and algorithms for stream verification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 12:31:13 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Thaler", "Justin", ""]]}, {"id": "1507.04220", "submitter": "Guido Hartmann", "authors": "Guido Hartmann", "title": "A numerical analysis of Quicksort: How many cases are bad cases?", "comments": "29 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present numerical results for the probability of bad cases for Quicksort,\ni.e. cases of input data for which the sorting cost considerably exceeds that\nof the average. Dynamic programming was used to compute solutions of the\nrecurrence for the frequency distributions of comparisons. From these\nsolutions, probabilities of numbers of comparisons above certain thresholds\nrelative to the average were extracted. Computations were done for array sizes\nup to n = 500 elements and for several methods to select the partitioning\nelement, from a simple random selection to what we call \"recursive median of\nthree medians.\" We found that the probability strongly depends on the selection\nmethod: for n = 500 and a theshold 25% above the average number of comparisons\nit ranges from 2.2*10^(-3) to 3.0*10^(-23). A version of Quicksort based on the\nrecursive median of medians approach is proposed, for which our data suggest a\nworst case time complexity of O(n^1.37).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:52:08 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Hartmann", "Guido", ""]]}, {"id": "1507.04227", "submitter": "Justin Ward", "authors": "Konstantin Makarychev, Yury Makarychev, Maxim Sviridenko, Justin Ward", "title": "A bi-criteria approximation algorithm for $k$ Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical $k$-means clustering problem in the setting\nbi-criteria approximation, in which an algoithm is allowed to output $\\beta k >\nk$ clusters, and must produce a clustering with cost at most $\\alpha$ times the\nto the cost of the optimal set of $k$ clusters. We argue that this approach is\nnatural in many settings, for which the exact number of clusters is a priori\nunknown, or unimportant up to a constant factor. We give new bi-criteria\napproximation algorithms, based on linear programming and local search,\nrespectively, which attain a guarantee $\\alpha(\\beta)$ depending on the number\n$\\beta k$ of clusters that may be opened. Our gurantee $\\alpha(\\beta)$ is\nalways at most $9 + \\epsilon$ and improves rapidly with $\\beta$ (for example:\n$\\alpha(2)<2.59$, and $\\alpha(3) < 1.4$). Moreover, our algorithms have only\npolynomial dependence on the dimension of the input data, and so are applicable\nin high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 14:15:01 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 15:55:09 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Sviridenko", "Maxim", ""], ["Ward", "Justin", ""]]}, {"id": "1507.04299", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Ilya Razenshteyn", "title": "Tight Lower Bounds for Data-Dependent Locality-Sensitive Hashing", "comments": "16 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a tight lower bound for the exponent $\\rho$ for data-dependent\nLocality-Sensitive Hashing schemes, recently used to design efficient solutions\nfor the $c$-approximate nearest neighbor search. In particular, our lower bound\nmatches the bound of $\\rho\\le \\frac{1}{2c-1}+o(1)$ for the $\\ell_1$ space,\nobtained via the recent algorithm from [Andoni-Razenshteyn, STOC'15].\n  In recent years it emerged that data-dependent hashing is strictly superior\nto the classical Locality-Sensitive Hashing, when the hash function is\ndata-independent. In the latter setting, the best exponent has been already\nknown: for the $\\ell_1$ space, the tight bound is $\\rho=1/c$, with the upper\nbound from [Indyk-Motwani, STOC'98] and the matching lower bound from\n[O'Donnell-Wu-Zhou, ITCS'11].\n  We prove that, even if the hashing is data-dependent, it must hold that\n$\\rho\\ge \\frac{1}{2c-1}-o(1)$. To prove the result, we need to formalize the\nexact notion of data-dependent hashing that also captures the complexity of the\nhash functions (in addition to their collision properties). Without restricting\nsuch complexity, we would allow for obviously infeasible solutions such as the\nVoronoi diagram of a dataset. To preclude such solutions, we require our hash\nfunctions to be succinct. This condition is satisfied by all the known\nalgorithmic results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 17:02:20 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Andoni", "Alexandr", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1507.04438", "submitter": "Akbar Rafiey", "authors": "Binay Bhattacharya, Ante \\'Custi\\'c, Akbar Rafiey, Arash Rafiey,\n  Vladyslav Sokol", "title": "Approximation Algorithms for Generalized MST and TSP in Grid Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a special case of the generalized minimum spanning tree problem\n(GMST) and the generalized travelling salesman problem (GTSP) where we are\ngiven a set of points inside the integer grid (in Euclidean plane) where each\ngrid cell is $1 \\times 1$. In the MST version of the problem, the goal is to\nfind a minimum tree that contains exactly one point from each non-empty grid\ncell (cluster). Similarly, in the TSP version of the problem, the goal is to\nfind a minimum weight cycle containing one point from each non-empty grid cell.\nWe give a $(1+4\\sqrt{2}+\\epsilon)$ and $(1.5+8\\sqrt{2}+\\epsilon)$-approximation\nalgorithm for these two problems in the described setting, respectively.\n  Our motivation is based on the problem posed in [7] for a constant\napproximation algorithm. The authors designed a PTAS for the more special case\nof the GMST where non-empty cells are connected end dense enough. However,\ntheir algorithm heavily relies on this connectivity restriction and is\nunpractical. Our results develop the topic further.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 03:00:41 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Bhattacharya", "Binay", ""], ["\u0106usti\u0107", "Ante", ""], ["Rafiey", "Akbar", ""], ["Rafiey", "Arash", ""], ["Sokol", "Vladyslav", ""]]}, {"id": "1507.04499", "submitter": "Francesco Ald\\`a", "authors": "Francesco Ald\\`a, Benjamin I. P. Rubinstein", "title": "The Bernstein Mechanism: Function Release under Differential Privacy", "comments": "10 pages; 2 figures; abridged version to appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of general function release under differential\nprivacy, by developing a functional mechanism that applies under the weak\nassumptions of oracle access to target function evaluation and sensitivity.\nThese conditions permit treatment of functions described explicitly or\nimplicitly as algorithmic black boxes. We achieve this result by leveraging the\niterated Bernstein operator for polynomial approximation of the target\nfunction, and polynomial coefficient perturbation. Under weak regularity\nconditions, we establish fast rates on utility measured by high-probability\nuniform approximation. We provide a lower bound on the utility achievable for\nany functional mechanism that is $\\varepsilon$-differentially private. The\ngenerality of our mechanism is demonstrated by the analysis of a number of\nexample learners, including naive Bayes, non-parametric estimators and\nregularized empirical risk minimization. Competitive rates are demonstrated for\nkernel density estimation; and $\\varepsilon$-differential privacy is achieved\nfor a broader class of support vector machines than known previously.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 09:25:06 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 16:14:56 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 14:53:05 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Ald\u00e0", "Francesco", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1507.04500", "submitter": "Ale\\v{s} Bizjak", "authors": "John Fearnley and Rahul Savani", "title": "The Complexity of All-switches Strategy Improvement", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 4 (October\n  31, 2018) lmcs:4940", "doi": "10.23638/LMCS-14(4:9)2018", "report-no": null, "categories": "cs.DS cs.CC cs.GT cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strategy improvement is a widely-used and well-studied class of algorithms\nfor solving graph-based infinite games. These algorithms are parameterized by a\nswitching rule, and one of the most natural rules is \"all switches\" which\nswitches as many edges as possible in each iteration. Continuing a recent line\nof work, we study all-switches strategy improvement from the perspective of\ncomputational complexity. We consider two natural decision problems, both of\nwhich have as input a game $G$, a starting strategy $s$, and an edge $e$. The\nproblems are: 1.) The edge switch problem, namely, is the edge $e$ ever\nswitched by all-switches strategy improvement when it is started from $s$ on\ngame $G$? 2.) The optimal strategy problem, namely, is the edge $e$ used in the\nfinal strategy that is found by strategy improvement when it is started from\n$s$ on game $G$? We show $\\mathtt{PSPACE}$-completeness of the edge switch\nproblem and optimal strategy problem for the following settings: Parity games\nwith the discrete strategy improvement algorithm of V\\\"oge and Jurdzi\\'nski;\nmean-payoff games with the gain-bias algorithm [14,37]; and discounted-payoff\ngames and simple stochastic games with their standard strategy improvement\nalgorithms. We also show $\\mathtt{PSPACE}$-completeness of an analogous problem\nto edge switch for the bottom-antipodal algorithm for finding the sink of an\nAcyclic Unique Sink Orientation on a cube.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 09:27:04 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 17:57:49 GMT"}, {"version": "v3", "created": "Sat, 23 Jun 2018 09:09:30 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 09:15:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Fearnley", "John", ""], ["Savani", "Rahul", ""]]}, {"id": "1507.04645", "submitter": "Amit Chakrabarti", "authors": "Amit Chakrabarti and Anthony Wirth", "title": "Incidence Geometries and the Pass Complexity of Semi-Streaming Set Cover", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set cover, over a universe of size $n$, may be modelled as a data-streaming\nproblem, where the $m$ sets that comprise the instance are to be read one by\none. A semi-streaming algorithm is allowed only $O(n\\, \\mathrm{poly}\\{\\log n,\n\\log m\\})$ space to process this stream. For each $p \\ge 1$, we give a very\nsimple deterministic algorithm that makes $p$ passes over the input stream and\nreturns an appropriately certified $(p+1)n^{1/(p+1)}$-approximation to the\noptimum set cover. More importantly, we proceed to show that this approximation\nfactor is essentially tight, by showing that a factor better than\n$0.99\\,n^{1/(p+1)}/(p+1)^2$ is unachievable for a $p$-pass semi-streaming\nalgorithm, even allowing randomisation. In particular, this implies that\nachieving a $\\Theta(\\log n)$-approximation requires $\\Omega(\\log n/\\log\\log n)$\npasses, which is tight up to the $\\log\\log n$ factor. These results extend to a\nrelaxation of the set cover problem where we are allowed to leave an\n$\\varepsilon$ fraction of the universe uncovered: the tight bounds on the best\napproximation factor achievable in $p$ passes turn out to be\n$\\Theta_p(\\min\\{n^{1/(p+1)}, \\varepsilon^{-1/p}\\})$. Our lower bounds are based\non a construction of a family of high-rank incidence geometries, which may be\nthought of as vast generalisations of affine planes. This construction, based\non algebraic techniques, appears flexible enough to find other applications and\nis therefore interesting in its own right.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:42:09 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Wirth", "Anthony", ""]]}, {"id": "1507.04674", "submitter": "Vivek Madan", "authors": "Chandra Chekuri and Vivek Madan", "title": "Simple and Fast Rounding Algorithms for Directed and Node-weighted\n  Multiway Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Directed Multiway Cut(Dir-MC) the input is an edge-weighted directed graph\n$G=(V,E)$ and a set of $k$ terminal nodes $\\{s_1,s_2,\\ldots,s_k\\} \\subseteq V$;\nthe goal is to find a min-weight subset of edges whose removal ensures that\nthere is no path from $s_i$ to $s_j$ for any $i \\neq j$. In Node-weighted\nMultiway Cut(Node-MC) the input is a node-weighted undirected graph $G$ and a\nset of $k$ terminal nodes $\\{s_1,s_2,\\ldots,s_k\\} \\subseteq V$; the goal is to\nremove a min-weight subset of nodes to disconnect each pair of terminals.\nDir-MC admits a $2$-approximation [Naor, Zosin '97] and Node-MC admits a\n$2(1-\\frac{1}{k})$-approximation [Garg, Vazirani, Yannakakis '94], both via\nrounding of LP relaxations. Previous rounding algorithms for these problems,\nfrom nearly twenty years ago, are based on careful rounding of an \"optimum\"\nsolution to an LP relaxation. This is particularly true for Dir-MC for which\nthe rounding relies on a custom LP formulation instead of the natural distance\nbased LP relaxation [Naor, Zosin '97].\n  In this paper we describe extremely simple and near linear-time rounding\nalgorithms for Dir-MC and Node-MC via a natural distance based LP relaxation.\nThe dual of this relaxation is a special case of the maximum multicommodity\nflow problem. Our algorithms achieve the same bounds as before but have the\nsignificant advantage in that they can work with \"any feasible\" solution to the\nrelaxation. Consequently, in addition to obtaining \"book\" proofs of LP rounding\nfor these two basic problems, we also obtain significantly faster approximation\nalgorithms by taking advantage of known algorithms for computing near-optimal\nsolutions for maximum multicommodity flow problems. We also investigate lower\nbounds for Dir-MC when $k=2$ and in particular prove that the integrality gap\nof the LP relaxation is $2$ even in directed planar graphs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 18:01:52 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Chekuri", "Chandra", ""], ["Madan", "Vivek", ""]]}, {"id": "1507.04774", "submitter": "Andrew King", "authors": "Kelly Boothby, Andrew D. King and Aidan Roy", "title": "Fast clique minor generation in Chimera qubit connectivity graphs", "comments": "14 pages v2 corrected first author's name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current generation of D-Wave quantum annealing processor is designed to\nminimize the energy of an Ising spin configuration whose pairwise interactions\nlie on the edges of a {\\em Chimera} graph $\\mathcal C_{M,N,L}$. In order to\nsolve an Ising spin problem with arbitrary pairwise interaction structure, the\ncorresponding graph must be minor-embedded into a Chimera graph. We define a\ncombinatorial class of {\\em native clique minors} in Chimera graphs with vertex\nimages of uniform, near minimal size, and provide a polynomial-time algorithm\nthat finds a maximum native clique minor in a given induced subgraph of a\nChimera graph. These minors allow improvement over recent work and have\nimmediate practical applications in the field of quantum annealing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 21:18:44 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:40:05 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Boothby", "Kelly", ""], ["King", "Andrew D.", ""], ["Roy", "Aidan", ""]]}, {"id": "1507.04907", "submitter": "Martin Kouteck\\'y", "authors": "Petr Kolman, Martin Kouteck\\'y, Hans Raj Tiwary", "title": "Extension Complexity, MSO Logic, and Treewidth", "comments": "Final version accepted by DMTCS", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 22 no.\n  4, Discrete Algorithms (October 1, 2020) dmtcs:6811", "doi": "10.23638/DMTCS-22-4-8", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the convex hull $P_{\\varphi}(G)$ of all satisfying assignments of\na given MSO formula $\\varphi$ on a given graph $G$. We show that there exists\nan extended formulation of the polytope $P_{\\varphi}(G)$ that can be described\nby $f(|\\varphi|,\\tau)\\cdot n$ inequalities, where $n$ is the number of vertices\nin $G$, $\\tau$ is the treewidth of $G$ and $f$ is a computable function\ndepending only on $\\varphi$ and $\\tau.$\n  In other words, we prove that the extension complexity of $P_{\\varphi}(G)$ is\nlinear in the size of the graph $G$, with a constant depending on the treewidth\nof $G$ and the formula $\\varphi$. This provides a very general yet very simple\nmeta-theorem about the extension complexity of polytopes related to a wide\nclass of problems and graphs. As a corollary of our main result, we obtain an\nanalogous result % for the weaker MSO$_1$ logic on the wider class of graphs of\nbounded cliquewidth.\n  Furthermore, we study our main geometric tool which we term the glued product\nof polytopes. While the glued product of polytopes has been known since the\n'90s, we are the first to show that it preserves decomposability and\nboundedness of treewidth of the constraint matrix. This implies that our\nextension of $P_\\varphi(G)$ is decomposable and has a constraint matrix of\nbounded treewidth; so far only few classes of polytopes are known to be\ndecomposable. These properties make our extension useful in the construction of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:31:53 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 19:24:43 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 02:30:32 GMT"}, {"version": "v4", "created": "Sun, 27 Nov 2016 17:08:24 GMT"}, {"version": "v5", "created": "Tue, 28 Feb 2017 11:25:46 GMT"}, {"version": "v6", "created": "Thu, 11 May 2017 20:12:44 GMT"}, {"version": "v7", "created": "Mon, 17 Jun 2019 13:45:25 GMT"}, {"version": "v8", "created": "Mon, 29 Jun 2020 18:02:14 GMT"}, {"version": "v9", "created": "Tue, 29 Sep 2020 15:08:52 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kolman", "Petr", ""], ["Kouteck\u00fd", "Martin", ""], ["Tiwary", "Hans Raj", ""]]}, {"id": "1507.04925", "submitter": "Martin Hoefer", "authors": "Xiaohui Bei, Jugal Garg, Martin Hoefer", "title": "Ascending-Price Algorithms for Unknown Markets", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a simple ascending-price algorithm to compute a\n$(1+\\varepsilon)$-approximate equilibrium in Arrow-Debreu exchange markets with\nweak gross substitute (WGS) property, which runs in time polynomial in market\nparameters and $\\log 1/\\varepsilon$. This is the first polynomial-time\nalgorithm for most of the known tractable classes of Arrow-Debreu markets,\nwhich is easy to implement and avoids heavy machinery such as the ellipsoid\nmethod. In addition, our algorithm can be applied in unknown market setting\nwithout exact knowledge about the number of agents, their individual utilities\nand endowments. Instead, our algorithm only relies on queries to a global\ndemand oracle by posting prices and receiving aggregate demand for goods as\nfeedback. When demands are real-valued functions of prices, the oracles can\nonly return values of bounded precision based on real utility functions. Due to\nthis more realistic assumption, precision and representation of prices and\ndemands become a major technical challenge, and we develop new tools and\ninsights that may be of independent interest. Furthermore, our approach also\ngives the first polynomial-time algorithm to compute an exact equilibrium for\nmarkets with spending constraint utilities, a piecewise linear concave\ngeneralization of linear utilities. This resolves an open problem posed by Duan\nand Mehlhorn (2015).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 11:15:59 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 19:53:16 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Bei", "Xiaohui", ""], ["Garg", "Jugal", ""], ["Hoefer", "Martin", ""]]}, {"id": "1507.05061", "submitter": "Ahmed Younes Dr.", "authors": "Ahmed Younes and Jonathan E. Rowe", "title": "A Polynomial Time Bounded-error Quantum Algorithm for Boolean\n  Satisfiability", "comments": "15 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1505.06284", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to answer a long-standing open problem on the\nrelationship between NP and BQP. The paper shows that BQP contains NP by\nproposing a BQP quantum algorithm for the MAX-E3-SAT problem which is a\nfundamental NP-hard problem. Given an E3-CNF Boolean formula, the aim of the\nMAX-E3-SAT problem is to find the variable assignment that maximizes the number\nof satisfied clauses. The proposed algorithm runs in $O(m^2)$ for an E3-CNF\nBoolean formula with $m$ clauses and in the worst case runs in $O(n^6)$ for an\nE3-CNF Boolean formula with $n$ inputs. The proposed algorithm maximizes the\nset of satisfied clauses using a novel iterative partial negation and partial\nmeasurement technique. The algorithm is shown to achieve an arbitrary high\nprobability of success of $1-\\epsilon$ for small $\\epsilon>0$ using a\npolynomial resources. In addition to solving the MAX-E3-SAT problem, the\nproposed algorithm can also be used to decide if an E3-CNF Boolean formula is\nsatisfiable or not, which is an NP-complete problem, based on the maximum\nnumber of satisfied clauses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 10:22:11 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 01:24:50 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Younes", "Ahmed", ""], ["Rowe", "Jonathan E.", ""]]}, {"id": "1507.05086", "submitter": "Dimitris S. Papailiopoulos", "authors": "Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht,\n  Kannan Ramchandran, Michael I. Jordan", "title": "Parallel Correlation Clustering on Big Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a similarity graph between items, correlation clustering (CC) groups\nsimilar items together and dissimilar ones apart. One of the most popular CC\nalgorithms is KwikCluster: an algorithm that serially clusters neighborhoods of\nvertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster in\npractice requires a large number of clustering rounds, a potential bottleneck\nfor large graphs.\n  We present C4 and ClusterWild!, two algorithms for parallel correlation\nclustering that run in a polylogarithmic number of rounds and achieve nearly\nlinear speedups, provably. C4 uses concurrency control to enforce\nserializability of a parallel clustering process, and guarantees a\n3-approximation ratio. ClusterWild! is a coordination free algorithm that\nabandons consistency for the benefit of better scaling; this leads to a\nprovably small loss in the 3-approximation ratio.\n  We provide extensive experimental results for both algorithms, where we\noutperform the state of the art, both in terms of clustering accuracy and\nrunning time. We show that our algorithms can cluster billion-edge graphs in\nunder 5 seconds on 32 cores, while achieving a 15x speedup.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:48:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 18:36:02 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Pan", "Xinghao", ""], ["Papailiopoulos", "Dimitris", ""], ["Oymak", "Samet", ""], ["Recht", "Benjamin", ""], ["Ramchandran", "Kannan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1507.05106", "submitter": "Joshua Alman", "authors": "Josh Alman, Ryan Williams", "title": "Probabilistic Polynomials and Hamming Nearest Neighbors", "comments": "16 pages. To appear in 56th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2015)", "journal-ref": null, "doi": "10.1109/FOCS.2015.18", "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute any symmetric Boolean function on $n$ variables over\nany field (as well as the integers) with a probabilistic polynomial of degree\n$O(\\sqrt{n \\log(1/\\epsilon)})$ and error at most $\\epsilon$. The degree\ndependence on $n$ and $\\epsilon$ is optimal, matching a lower bound of Razborov\n(1987) and Smolensky (1987) for the MAJORITY function. The proof is\nconstructive: a low-degree polynomial can be efficiently sampled from the\ndistribution.\n  This polynomial construction is combined with other algebraic ideas to give\nthe first subquadratic time algorithm for computing a (worst-case) batch of\nHamming distances in superlogarithmic dimensions, exactly. To illustrate, let\n$c(n) : \\mathbb{N} \\rightarrow \\mathbb{N}$. Suppose we are given a database $D$\nof $n$ vectors in $\\{0,1\\}^{c(n) \\log n}$ and a collection of $n$ query vectors\n$Q$ in the same dimension. For all $u \\in Q$, we wish to compute a $v \\in D$\nwith minimum Hamming distance from $u$. We solve this problem in $n^{2-1/O(c(n)\n\\log^2 c(n))}$ randomized time. Hence, the problem is in \"truly subquadratic\"\ntime for $O(\\log n)$ dimensions, and in subquadratic time for $d = o((\\log^2\nn)/(\\log \\log n)^2)$. We apply the algorithm to computing pairs with maximum\ninner product, closest pair in $\\ell_1$ for vectors with bounded integer\nentries, and pairs with maximum Jaccard coefficients.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 20:26:56 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Alman", "Josh", ""], ["Williams", "Ryan", ""]]}, {"id": "1507.05136", "submitter": "Tselil Schramm", "authors": "Prasad Raghavendra and Tselil Schramm", "title": "Tight Lower Bounds for Planted Clique in the Degree-4 SOS Program", "comments": "This paper appeared in SODA 2016, in a merged manuscript with the\n  paper of Hopkins, Kothari and Potechin: http://arxiv.org/abs/1507.05230", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a lower bound of $\\tilde{\\Omega}(\\sqrt{n})$ for the degree-4\nSum-of-Squares SDP relaxation for the planted clique problem. Specifically, we\nshow that on an Erd\\\"os-R\\'enyi graph $G(n,\\tfrac{1}{2})$, with high\nprobability there is a feasible point for the degree-4 SOS relaxation of the\nclique problem with an objective value of $\\tilde{\\Omega}(\\sqrt{n})$, so that\nthe program cannot distinguish between a random graph and a random graph with a\nplanted clique of size $\\tilde{O}(\\sqrt{n})$. This bound is tight.\n  We build on the works of Deshpande and Montanari and Meka et al., who give\nlower bounds of $\\tilde{\\Omega}(n^{1/3})$ and $\\tilde{\\Omega}(n^{1/4})$\nrespectively. We improve on their results by making a perturbation to the SDP\nsolution proposed in their work, then showing that this perturbation remains\nPSD as the objective value approaches $\\tilde{\\Omega}(n^{1/2})$.\n  In an independent work, Hopkins, Kothari and Potechin [HKP15] have obtained a\nsimilar lower bound for the degree-$4$ SOS relaxation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 00:32:28 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 06:59:53 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 23:40:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Schramm", "Tselil", ""]]}, {"id": "1507.05387", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow and Dorota Majorkowska-Mech", "title": "An algorithm for discrete fractional Hadamard transform", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for calculating the discrete fractional Hadamard\ntransform for data vectors whose size N is a power of two. A direct method for\ncalculation of the discrete fractional Hadamard transform requires $N^2$\nmultiplications, while in proposed algorithm the number of real multiplications\nis reduced to $N$log$_2N$.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 05:23:52 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Majorkowska-Mech", "Dorota", ""]]}, {"id": "1507.05463", "submitter": "Robert Ganian", "authors": "Eduard Eiben and Robert Ganian and Stefan Szeider", "title": "Solving Problems on Graphs of High Rank-Width", "comments": "Accepted at WADS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modulator of a graph G to a specified graph class H is a set of vertices\nwhose deletion puts G into H. The cardinality of a modulator to various\ntractable graph classes has long been used as a structural parameter which can\nbe exploited to obtain FPT algorithms for a range of hard problems. Here we\ninvestigate what happens when a graph contains a modulator which is large but\n\"well-structured\" (in the sense of having bounded rank-width). Can such\nmodulators still be exploited to obtain efficient algorithms? And is it even\npossible to find such modulators efficiently?\n  We first show that the parameters derived from such well-structured\nmodulators are strictly more general than the cardinality of modulators and\nrank-width itself. Then, we develop an FPT algorithm for finding such\nwell-structured modulators to any graph class which can be characterized by a\nfinite set of forbidden induced subgraphs. We proceed by showing how\nwell-structured modulators can be used to obtain efficient parameterized\nalgorithms for Minimum Vertex Cover and Maximum Clique. Finally, we use\nwell-structured modulators to develop an algorithmic meta-theorem for deciding\nproblems expressible in Monadic Second Order (MSO) logic, and prove that this\nresult is tight in the sense that it cannot be generalized to LinEMSO problems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 12:17:26 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Szeider", "Stefan", ""]]}, {"id": "1507.05544", "submitter": "Robert Ganian", "authors": "Eduard Eiben and Robert Ganian and Stefan Szeider", "title": "Meta-Kernelization using Well-Structured Modulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization investigates exact preprocessing algorithms with performance\nguarantees. The most prevalent type of parameters used in kernelization is the\nsolution size for optimization problems; however, also structural parameters\nhave been successfully used to obtain polynomial kernels for a wide range of\nproblems. Many of these parameters can be defined as the size of a smallest\nmodulator of the given graph into a fixed graph class (i.e., a set of vertices\nwhose deletion puts the graph into the graph class). Such parameters admit the\nconstruction of polynomial kernels even when the solution size is large or not\napplicable. This work follows up on the research on meta-kernelization\nframeworks in terms of structural parameters.\n  We develop a class of parameters which are based on a more general view on\nmodulators: instead of size, the parameters employ a combination of rank-width\nand split decompositions to measure structure inside the modulator. This allows\nus to lift kernelization results from modulator-size to more general\nparameters, hence providing smaller kernels. We show (i) how such large but\nwell-structured modulators can be efficiently approximated, (ii) how they can\nbe used to obtain polynomial kernels for any graph problem expressible in\nMonadic Second Order logic, and (iii) how they allow the extension of previous\nresults in the area of structural meta-kernelization.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 16:04:14 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Szeider", "Stefan", ""]]}, {"id": "1507.05605", "submitter": "Alexander Wein", "authors": "Amelia Perry and Alexander S. Wein", "title": "A semidefinite program for unbalanced multisection in the stochastic\n  block model", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semidefinite programming (SDP) algorithm for community detection\nin the stochastic block model, a popular model for networks with latent\ncommunity structure. We prove that our algorithm achieves exact recovery of the\nlatent communities, up to the information-theoretic limits determined by Abbe\nand Sandon (2015). Our result extends prior SDP approaches by allowing for many\ncommunities of different sizes. By virtue of a semidefinite approach, our\nalgorithms succeed against a semirandom variant of the stochastic block model,\nguaranteeing a form of robustness and generalization. We further explore how\nsemirandom models can lend insight into both the strengths and limitations of\nSDPs in this setting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 19:58:52 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 17:59:55 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1507.05854", "submitter": "Chi Jin", "authors": "Prateek Jain, Chi Jin, Sham M. Kakade and Praneeth Netrapalli", "title": "Global Convergence of Non-Convex Gradient Descent for Computing Matrix\n  Squareroot", "comments": "Appear in AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been a significant amount of work studying gradient descent\ntechniques for non-convex optimization problems over the last few years, all\nexisting results establish either local convergence with good rates or global\nconvergence with highly suboptimal rates, for many problems of interest. In\nthis paper, we take the first step in getting the best of both worlds --\nestablishing global convergence and obtaining a good rate of convergence for\nthe problem of computing squareroot of a positive definite (PD) matrix, which\nis a widely studied problem in numerical linear algebra with applications in\nmachine learning and statistics among others. Given a PD matrix $M$ and a PD\nstarting point $U_0$, we show that gradient descent with appropriately chosen\nstep-size finds an $\\epsilon$-accurate squareroot of $M$ in $O(\\alpha \\log\n(\\|M-U_0^2\\|_F /\\epsilon))$ iterations, where $\\alpha =\n(\\max\\{\\|U_0\\|_2^2,\\|M\\|_2\\} / \\min \\{\\sigma_{\\min}^2(U_0),\\sigma_{\\min}(M) \\}\n)^{3/2}$. Our result is the first to establish global convergence for this\nproblem and that it is robust to errors in each iteration. A key contribution\nof our work is the general proof technique which we believe should further\nexcite research in understanding deterministic and stochastic variants of\nsimple non-convex gradient descent algorithms with good global convergence\nrates for other problems in machine learning and numerical linear algebra.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 14:42:45 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 17:46:28 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Jain", "Prateek", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1507.05890", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen", "title": "On Structural Parameterizations of Hitting Set: Hitting Paths in Graphs\n  Using 2-SAT", "comments": "Presented at the 41st International Workshop on Graph-Theoretic\n  Concepts in Computer Science, WG 2015. (The statement of Lemma 4 was\n  corrected in this update.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hitting Set is a classic problem in combinatorial optimization. Its input\nconsists of a set system F over a finite universe U and an integer t; the\nquestion is whether there is a set of t elements that intersects every set in\nF. The Hitting Set problem parameterized by the size of the solution is a\nwell-known W[2]-complete problem in parameterized complexity theory. In this\npaper we investigate the complexity of Hitting Set under various structural\nparameterizations of the input. Our starting point is the folklore result that\nHitting Set is polynomial-time solvable if there is a tree T on vertex set U\nsuch that the sets in F induce connected subtrees of T. We consider the case\nthat there is a treelike graph with vertex set U such that the sets in F induce\nconnected subgraphs; the parameter of the problem is a measure of how treelike\nthe graph is. Our main positive result is an algorithm that, given a graph G\nwith cyclomatic number k, a collection P of simple paths in G, and an integer\nt, determines in time 2^{5k} (|G| +|P|)^O(1) whether there is a vertex set of\nsize t that hits all paths in P. It is based on a connection to the 2-SAT\nproblem in multiple valued logic. For other parameterizations we derive\nW[1]-hardness and para-NP-completeness results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 16:04:51 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 13:55:37 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Jansen", "Bart M. P.", ""]]}, {"id": "1507.05944", "submitter": "Seth Pettie", "authors": "Casper Kejlberg-Rasmussen, Tsvi Kopelowitz, Seth Pettie, and Mikkel\n  Thorup", "title": "Faster Worst Case Deterministic Dynamic Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic dynamic connectivity data structure for undirected\ngraphs with worst case update time $O\\left(\\sqrt{\\frac{n(\\log\\log n)^2}{\\log\nn}}\\right)$ and constant query time. This improves on the previous best\ndeterministic worst case algorithm of Frederickson (STOC 1983) and Eppstein\nGalil, Italiano, and Nissenzweig (J. ACM 1997), which had update time\n$O(\\sqrt{n})$. All other algorithms for dynamic connectivity are either\nrandomized (Monte Carlo) or have only amortized performance guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 18:57:39 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 22:19:57 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Kejlberg-Rasmussen", "Casper", ""], ["Kopelowitz", "Tsvi", ""], ["Pettie", "Seth", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1507.05950", "submitter": "Dimitris S. Papailiopoulos", "authors": "Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein", "title": "On the Worst-Case Approximability of Sparse PCA", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Sparse PCA (Sparse Principal Component Analysis) is\nNP-hard to solve exactly on worst-case instances. What is the complexity of\nsolving Sparse PCA approximately? Our contributions include: 1) a simple and\nefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness\nof approximation to within $(1-\\varepsilon)$, for some small constant\n$\\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant\nfactor; and 4) an $\\exp\\exp\\left(\\Omega\\left(\\sqrt{\\log \\log n}\\right)\\right)$\n(\"quasi-quasi-polynomial\") gap for the standard semidefinite program.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:34:32 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Chan", "Siu On", ""], ["Papailiopoulos", "Dimitris", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1507.05952", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Constantinos Daskalakis, Gautam Kamath", "title": "Optimal Testing for Properties of Distributions", "comments": "31 pages, extended abstract appeared as a spotlight in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from an unknown distribution $p$, is it possible to distinguish\nwhether $p$ belongs to some class of distributions $\\mathcal{C}$ versus $p$\nbeing far from every distribution in $\\mathcal{C}$? This fundamental question\nhas received tremendous attention in statistics, focusing primarily on\nasymptotic analysis, and more recently in information theory and theoretical\ncomputer science, where the emphasis has been on small sample size and\ncomputational complexity. Nevertheless, even for basic properties of\ndistributions such as monotonicity, log-concavity, unimodality, independence,\nand monotone-hazard rate, the optimal sample complexity is unknown.\n  We provide a general approach via which we obtain sample-optimal and\ncomputationally efficient testers for all these distribution families. At the\ncore of our approach is an algorithm which solves the following problem: Given\nsamples from an unknown distribution $p$, and a known distribution $q$, are $p$\nand $q$ close in $\\chi^2$-distance, or far in total variation distance?\n  The optimality of our testers is established by providing matching lower\nbounds with respect to both $n$ and $\\varepsilon$. Finally, a necessary\nbuilding block for our testers and an important byproduct of our work are the\nfirst known computationally efficient proper learners for discrete log-concave\nand monotone hazard rate distributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:52:56 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 14:12:33 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 20:00:11 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Acharya", "Jayadev", ""], ["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""]]}, {"id": "1507.05955", "submitter": "Richard Johnson", "authors": "Richard A. B. Johnson, Gabor Meszaros", "title": "Sorting using non-binary comparisons", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of sorting a set of $n$ coins, each\nwith distinct but unknown weights, using an unusual scale. The classical\nversion of this problem, which has been well-studied, gives the user a binary\nscale, enabling them to determine which is the lighter/heavier of any two\nobjects. We generalise this, considering a scale that accepts $k$ coins as\ninput and returns the $t^{\\text{th}}$ lightest, for a fixed $k$ and $t$. We\nconsider this in both an on-line and off-line setting, and exhibit algorithms\nin both settings that are best-possible in terms of the order of the number of\nqueries required.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:59:57 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Johnson", "Richard A. B.", ""], ["Meszaros", "Gabor", ""]]}, {"id": "1507.05980", "submitter": "Farzad Zafarani", "authors": "Glencora Borradaile, Amir Nayyeri, Farzad Zafarani", "title": "Towards single face shortest vertex-disjoint paths in undirected planar\n  graphs", "comments": "This is a preliminary version of a paper that will appear in\n  Proceedings of the 23rd European Symposium on Algorithms (ESA 2015), Patras,\n  Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $k$ pairs of terminals $\\{(s_{1}, t_{1}), \\ldots, (s_{k}, t_{k})\\}$ in\na graph $G$, the min-sum $k$ vertex-disjoint paths problem is to find a\ncollection $\\{Q_{1}, Q_{2}, \\ldots, Q_{k}\\}$ of vertex-disjoint paths with\nminimum total length, where $Q_{i}$ is an $s_i$-to-$t_i$ path between $s_i$ and\n$t_i$. We consider the problem in planar graphs, where little is known about\ncomputational tractability, even in restricted cases. Kobayashi and Sommer\npropose a polynomial-time algorithm for $k \\le 3$ in undirected planar graphs\nassuming all terminals are adjacent to at most two faces. Colin de Verdiere and\nSchrijver give a polynomial-time algorithm when all the sources are on the\nboundary of one face and all the sinks are on the boundary of another face and\nask about the existence of a polynomial-time algorithm provided all terminals\nare on a common face. We make progress toward Colin de Verdiere and Schrijver's\nopen question by giving an $O(kn^5)$ time algorithm for undirected planar\ngraphs when $\\{(s_{1}, t_{1}), \\ldots, (s_{k}, t_{k})\\}$ are in\ncounter-clockwise order on a common face.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 20:20:10 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Borradaile", "Glencora", ""], ["Nayyeri", "Amir", ""], ["Zafarani", "Farzad", ""]]}, {"id": "1507.05998", "submitter": "Peter Lofgren", "authors": "Siddhartha Banerjee, Peter Lofgren", "title": "Fast Bidirectional Probability Estimation in Markov Models", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new bidirectional algorithm for estimating Markov chain\nmulti-step transition probabilities: given a Markov chain, we want to estimate\nthe probability of hitting a given target state in $\\ell$ steps after starting\nfrom a given source distribution. Given the target state $t$, we use a\n(reverse) local power iteration to construct an `expanded target distribution',\nwhich has the same mean as the quantity we want to estimate, but a smaller\nvariance -- this can then be sampled efficiently by a Monte Carlo algorithm.\nOur method extends to any Markov chain on a discrete (finite or countable)\nstate-space, and can be extended to compute functions of multi-step transition\nprobabilities such as PageRank, graph diffusions, hitting/return times, etc.\nOur main result is that in `sparse' Markov Chains -- wherein the number of\ntransitions between states is comparable to the number of states -- the running\ntime of our algorithm for a uniform-random target node is order-wise smaller\nthan Monte Carlo and power iteration based algorithms; in particular, our\nmethod can estimate a probability $p$ using only $O(1/\\sqrt{p})$ running time.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 21:48:48 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 00:12:00 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Banerjee", "Siddhartha", ""], ["Lofgren", "Peter", ""]]}, {"id": "1507.05999", "submitter": "Peter Lofgren", "authors": "Peter Lofgren, Siddhartha Banerjee, Ashish Goel", "title": "Personalized PageRank Estimation and Search: A Bidirectional Approach", "comments": "WSDM 2016", "journal-ref": null, "doi": "10.1145/2835776.2835823", "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for Personalized PageRank estimation and\nPersonalized PageRank search. First, for the problem of estimating Personalized\nPageRank (PPR) from a source distribution to a target node, we present a new\nbidirectional estimator with simple yet strong guarantees on correctness and\nperformance, and 3x to 8x speedup over existing estimators in experiments on a\ndiverse set of networks. Moreover, it has a clean algebraic structure which\nenables it to be used as a primitive for the Personalized PageRank Search\nproblem: Given a network like Facebook, a query like \"people named John\", and a\nsearching user, return the top nodes in the network ranked by PPR from the\nperspective of the searching user. Previous solutions either score all nodes or\nscore candidate nodes one at a time, which is prohibitively slow for large\ncandidate sets. We develop a new algorithm based on our bidirectional PPR\nestimator which identifies the most relevant results by sampling candidates\nbased on their PPR; this is the first solution to PPR search that can find the\nbest results without iterating through the set of all candidate results.\nFinally, by combining PPR sampling with sequential PPR estimation and Monte\nCarlo, we develop practical algorithms for PPR search, and we show via\nexperiments that our algorithms are efficient on networks with billions of\nedges.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 21:50:08 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 22:54:15 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 02:07:04 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lofgren", "Peter", ""], ["Banerjee", "Siddhartha", ""], ["Goel", "Ashish", ""]]}, {"id": "1507.06056", "submitter": "Subir  Ghosh", "authors": "Subir Kumar Ghosh and Sudebkumar Prasant Pal", "title": "A National Effort for Motivating Indian Students and Teachers towards\n  Algorithmic Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During 2008-2015, twenty-two introductory workshops on graph and geometric\nalgorithms were organized for teachers and students (undergraduate,\npost-graduate and doctoral) of engineering colleges and universities at\ndifferent states and union territories of India. The lectures were meant to\nprovide exposure to the field of graph and geometric algorithms and to motivate\nthe participants towards research. Fifty-eight professors from TIFR, IITs,\nIISc, IMSc, CMI, ISI Kolkata, and other institutes and universities delivered\ninvited lectures on different topics in the design and analysis of algorithms,\ndiscrete applied mathematics, computer graphics, computer vision, and robotics.\nThe first four workshops were funded by TIFR, BRNS and IIT Kharagpur, and the\nremaining workshops were funded by the NBHM. In this paper, we present the\nsalient features of these workshops, and state our observations on the national\nimpact of these workshops.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 04:49:01 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 12:49:37 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 02:18:34 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Ghosh", "Subir Kumar", ""], ["Pal", "Sudebkumar Prasant", ""]]}, {"id": "1507.06175", "submitter": "Venkatesan Guruswami", "authors": "Joshua Brakensiek, Venkatesan Guruswami, Samuel Zbarsky", "title": "Efficient Low-Redundancy Codes for Correcting Multiple Deletions", "comments": "The published version of this paper claimed in an appendix a rate\n  limitation of linear deletion codes. This claim is false and has been\n  retracted in this version", "journal-ref": "IEEE Trans. Information Theory 64(5): 3403-3410 (2018)", "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing binary codes to recover from $k$-bit\ndeletions with efficient encoding/decoding, for a fixed $k$. The single\ndeletion case is well understood, with the Varshamov-Tenengolts-Levenshtein\ncode from 1965 giving an asymptotically optimal construction with $\\approx\n2^n/n$ codewords of length $n$, i.e., at most $\\log n$ bits of redundancy.\nHowever, even for the case of two deletions, there was no known explicit\nconstruction with redundancy less than $n^{\\Omega(1)}$.\n  For any fixed $k$, we construct a binary code with $c_k \\log n$ redundancy\nthat can be decoded from $k$ deletions in $O_k(n \\log^4 n)$ time. The\ncoefficient $c_k$ can be taken to be $O(k^2 \\log k)$, which is only\nquadratically worse than the optimal, non-constructive bound of $O(k)$. We also\nindicate how to modify this code to allow for a combination of up to $k$\ninsertions and deletions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 13:20:28 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 02:49:39 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Guruswami", "Venkatesan", ""], ["Zbarsky", "Samuel", ""]]}, {"id": "1507.06199", "submitter": "Rani Izsak", "authors": "Moran Feldman and Rani Izsak", "title": "Building a Good Team: Secretary Problems and the Supermodular Degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Secretary Problem, one has to hire the best among n candidates. The\ncandidates are interviewed, one at a time, at a random order, and one has to\ndecide on the spot, whether to hire a candidate or continue interviewing. It is\nwell known that the best candidate can be hired with a probability of 1/e\n(Dynkin, 1963). Recent works extend this problem to settings in which multiple\ncandidates can be hired, subject to some constraint. Here, one wishes to hire a\nset of candidates maximizing a given set function.\n  Almost all extensions considered in the literature assume the objective set\nfunction is either linear or submodular. Unfortunately, real world functions\nmight not have either of these properties. Consider, for example, a scenario\nwhere one hires researchers for a project. Indeed, it can be that some\nresearchers can substitute others for that matter. However, it can also be that\nsome combinations of researchers result in synergy (see, e.g, Woolley et al.,\nScience 2010, for a research about collective intelligence). The first\nphenomenon can be modeled by a submoudlar set function, while the latter\ncannot.\n  In this work, we study the secretary problem with an arbitrary non-negative\nmonotone function, subject to a general matroid constraint. It is not difficult\nto prove that, generally, only very poor results can be obtained for this class\nof objective functions. We tackle this hardness by combining the following:\n1.Parametrizing our algorithms by the supermodular degree of the objective\nfunction (defined by Feige and Izsak, ITCS 2013), which, roughly speaking,\nmeasures the distance of a function from being submodular. 2.Suggesting an\n(arguably) natural model that permits approximation guarantees that are\npolynomial in the supermodular degree (as opposed to the standard model which\nallows only exponential guarantees).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 14:15:10 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Feldman", "Moran", ""], ["Izsak", "Rani", ""]]}, {"id": "1507.06240", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Pawe{\\l} Gawrychowski, Adrian Kosowski, Przemys{\\l}aw Uzna\\'nski", "title": "Sublinear-Space Distance Labeling using Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distance labeling scheme is an assignment of bit-labels to the vertices of\nan undirected, unweighted graph such that the distance between any pair of\nvertices can be decoded solely from their labels. We propose a series of new\nlabeling schemes within the framework of so-called hub labeling (HL, also known\nas landmark labeling or 2-hop-cover labeling), in which each node $u$ stores\nits distance to all nodes from an appropriately chosen set of hubs $S(u)\n\\subseteq V$. For a queried pair of nodes $(u,v)$, the length of a shortest\n$u-v$-path passing through a hub node from $S(u)\\cap S(v)$ is then used as an\nupper bound on the distance between $u$ and $v$.\n  We present a hub labeling which allows us to decode exact distances in sparse\ngraphs using labels of size sublinear in the number of nodes. For graphs with\nat most $n$ nodes and average degree $\\Delta$, the tradeoff between label bit\nsize $L$ and query decoding time $T$ for our approach is given by $L = O(n \\log\n\\log_\\Delta T / \\log_\\Delta T)$, for any $T \\leq n$. Our simple approach is\nthus the first sublinear-space distance labeling for sparse graphs that\nsimultaneously admits small decoding time (for constant $\\Delta$, we can\nachieve any $T=\\omega(1)$ while maintaining $L=o(n)$), and it also provides an\nimprovement in terms of label size with respect to previous slower approaches.\n  By using similar techniques, we then present a $2$-additive labeling scheme\nfor general graphs, i.e., one in which the decoder provides a\n2-additive-approximation of the distance between any pair of nodes. We achieve\nalmost the same label size-time tradeoff $L = O(n \\log^2 \\log T / \\log T)$, for\nany $T \\leq n$. To our knowledge, this is the first additive scheme with\nconstant absolute error to use labels of sublinear size. The corresponding\ndecoding time is then small (any $T=\\omega(1)$ is sufficient).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 16:24:02 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 23:14:43 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 11:48:16 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1507.06341", "submitter": "R.B. Sandeep", "authors": "N. R. Aravind, R. B. Sandeep, Naveen Sivadasan", "title": "Parameterized lower bound and NP-completeness of some $H$-free Edge\n  Deletion problems", "comments": "15 pages, COCOA 15 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $H$, the $H$-free Edge Deletion problem asks whether there exist\nat most $k$ edges whose deletion from the input graph $G$ results in a graph\nwithout any induced copy of $H$. We prove that $H$-free Edge Deletion is\nNP-complete if $H$ is a graph with at least two edges and $H$ has a component\nwith maximum number of vertices which is a tree or a regular graph.\nFurthermore, we obtain that these NP-complete problems cannot be solved in\nparameterized subexponential time, i.e., in time $2^{o(k)}\\cdot |G|^{O(1)}$,\nunless Exponential Time Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 21:05:26 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 13:28:28 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Aravind", "N. R.", ""], ["Sandeep", "R. B.", ""], ["Sivadasan", "Naveen", ""]]}, {"id": "1507.06365", "submitter": "Scott Summers", "authors": "David Furcy and Scott M. Summers", "title": "Optimal self-assembly of finite shapes at temperature 1 in 3D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working in a three-dimensional variant of Winfree's abstract Tile Assembly\nModel, we show that, for an arbitrary finite, connected shape $X \\subset\n\\mathbb{Z}^2$, there is a tile set that uniquely self-assembles into a 3D\nrepresentation of a scaled-up version of $X$ at temperature 1 in 3D with\noptimal program-size complexity (the \"program-size complexity\", also known as\n\"tile complexity\", of a shape is the minimum number of tile types required to\nuniquely self-assemble it). Moreover, our construction is \"just barely\" 3D in\nthe sense that it only places tiles in the $z = 0$ and $z = 1$ planes. Our\nresult is essentially a just-barely 3D temperature 1 simulation of a similar 2D\ntemperature 2 result by Soloveichik and Winfree (SICOMP 2007).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:01:10 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Furcy", "David", ""], ["Summers", "Scott M.", ""]]}, {"id": "1507.06495", "submitter": "Yang Cai", "authors": "Nicolas Bousquet, Yang Cai, Christoph Hunkenschr\\\"oder, Adrian Vetta", "title": "On the Economic Efficiency of the Combinatorial Clock Auction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the 1990s spectrum auctions have been implemented world-wide. This has\nprovided for a practical examination of an assortment of auction mechanisms\nand, amongst these, two simultaneous ascending price auctions have proved to be\nextremely successful. These are the simultaneous multiround ascending auction\n(SMRA) and the combinatorial clock auction (CCA). It has long been known that,\nfor certain classes of valuation functions, the SMRA provides good theoretical\nguarantees on social welfare. However, no such guarantees were known for the\nCCA.\n  In this paper, we show that CCA does provide strong guarantees on social\nwelfare provided the price increment and stopping rule are well-chosen. This is\nvery surprising in that the choice of price increment has been used primarily\nto adjust auction duration and the stopping rule has attracted little\nattention. The main result is a polylogarithmic approximation guarantee for\nsocial welfare when the maximum number of items demanded $\\mathcal{C}$ by a\nbidder is fixed. Specifically, we show that either the revenue of the CCA is at\nleast an $\\Omega\\Big(\\frac{1}{\\mathcal{C}^{2}\\log n\\log^2m}\\Big)$-fraction of\nthe optimal welfare or the welfare of the CCA is at least an\n$\\Omega\\Big(\\frac{1}{\\log n}\\Big)$-fraction of the optimal welfare, where $n$\nis the number of bidders and $m$ is the number of items. As a corollary, the\nwelfare ratio -- the worst case ratio between the social welfare of the optimum\nallocation and the social welfare of the CCA allocation -- is at most\n$O(\\mathcal{C}^2 \\cdot \\log n \\cdot \\log^2 m)$. We emphasize that this latter\nresult requires no assumption on bidders valuation functions. Finally, we prove\nthat such a dependence on $\\mathcal{C}$ is necessary. In particular, we show\nthat the welfare ratio of the CCA is at least $\\Omega \\Big(\\mathcal{C} \\cdot\n\\frac{\\log m}{\\log \\log m}\\Big)$.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 14:00:20 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Cai", "Yang", ""], ["Hunkenschr\u00f6der", "Christoph", ""], ["Vetta", "Adrian", ""]]}, {"id": "1507.06616", "submitter": "Rajan Udwani", "authors": "James B. Orlin, Andreas S. Schulz, Rajan Udwani", "title": "Robust Monotone Submodular Function Maximization", "comments": "Preliminary version in IPCO 2016", "journal-ref": "Mathematical Programming, 172(1), 505-537, 2018", "doi": "10.1007/s10107-018-1320-2", "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a robust formulation, introduced by Krause et al. (2008), of the\nclassical cardinality constrained monotone submodular function maximization\nproblem, and give the first constant factor approximation results. The\nrobustness considered is w.r.t. adversarial removal of up to $\\tau$ elements\nfrom the chosen set. For the fundamental case of $\\tau=1$, we give a\ndeterministic $(1-1/e)-1/\\Theta(m)$ approximation algorithm, where $m$ is an\ninput parameter and number of queries scale as $O(n^{m+1})$. In the process, we\ndevelop a deterministic $(1-1/e)-1/\\Theta(m)$ approximate greedy algorithm for\nbi-objective maximization of (two) monotone submodular functions. Generalizing\nthe ideas and using a result from Chekuri et al. (2010), we show a randomized\n$(1-1/e)-\\epsilon$ approximation for constant $\\tau$ and $\\epsilon\\leq\n\\frac{1}{\\tilde{\\Omega}(\\tau)}$, making $O(n^{1/\\epsilon^3})$ queries. Further,\nfor $\\tau\\ll \\sqrt{k}$, we give a fast and practical 0.387 algorithm. Finally,\nwe also give a black box result result for the much more general setting of\nrobust maximization subject to an Independence System.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 19:07:55 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 20:50:40 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 14:48:57 GMT"}, {"version": "v4", "created": "Wed, 15 Nov 2017 22:01:09 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Orlin", "James B.", ""], ["Schulz", "Andreas S.", ""], ["Udwani", "Rajan", ""]]}, {"id": "1507.06677", "submitter": "Krishnendra Shekhawat", "authors": "Krishnendra Shekhawat", "title": "Connectivity Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, for the given adjacency matrix of a graph, we present an\nalgorithm which checks the connectivity of a graph and computes all of its\nconnected components. Also, it is mathematically proved that the algorithm\npresents all the desired results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 13:30:28 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Shekhawat", "Krishnendra", ""]]}, {"id": "1507.06702", "submitter": "Jesun Sahariar Firoz", "authors": "Jesun Sahariar Firoz, Thejaka Amila Kanewala, Marcin Zalewski, Martina\n  Barnas, Andrew Lumsdaine", "title": "The Anatomy of Large-Scale Distributed Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of the software/hardware stack of modern\nsupercomputers results in explosion of parameters. The performance analysis\nbecomes a truly experimental science, even more challenging in the presence of\nmassive irregularity and data dependency. We analyze how the existing body of\nresearch handles the experimental aspect in the context of distributed graph\nalgorithms (DGAs). We distinguish algorithm-level contributions, often\nprioritized by authors, from runtime-level concerns that are harder to place.\nWe show that the runtime is such an integral part of DGAs that experimental\nresults are difficult to interpret and extrapolate without understanding the\nproperties of the runtime used. We argue that in order to gain understanding\nabout the impact of runtimes, more information needs to be gathered. To begin\nthis process, we provide an initial set of recommendations for describing DGA\nresults based on our analysis of the current state of the field.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 23:42:34 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Firoz", "Jesun Sahariar", ""], ["Kanewala", "Thejaka Amila", ""], ["Zalewski", "Marcin", ""], ["Barnas", "Martina", ""], ["Lumsdaine", "Andrew", ""]]}, {"id": "1507.06866", "submitter": "Yakov Nekrich", "authors": "J. Ian Munro and Yakov Nekrich", "title": "Compressed Data Structures for Dynamic Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of storing a dynamic string $S$ over an alphabet\n$\\Sigma=\\{\\,1,\\ldots,\\sigma\\,\\}$ in compressed form. Our representation\nsupports insertions and deletions of symbols and answers three fundamental\nqueries: $\\mathrm{access}(i,S)$ returns the $i$-th symbol in $S$,\n$\\mathrm{rank}_a(i,S)$ counts how many times a symbol $a$ occurs among the\nfirst $i$ positions in $S$, and $\\mathrm{select}_a(i,S)$ finds the position\nwhere a symbol $a$ occurs for the $i$-th time. We present the first\nfully-dynamic data structure for arbitrarily large alphabets that achieves\noptimal query times for all three operations and supports updates with\nworst-case time guarantees. Ours is also the first fully-dynamic data structure\nthat needs only $nH_k+o(n\\log\\sigma)$ bits, where $H_k$ is the $k$-th order\nentropy and $n$ is the string length. Moreover our representation supports\nextraction of a substring $S[i..i+\\ell]$ in optimal $O(\\log n/\\log\\log n +\n\\ell/\\log_{\\sigma}n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 14:57:18 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1507.06878", "submitter": "Shogo Nakajima", "authors": "Fran\\c{c}ois Le Gall and Shogo Nakajima", "title": "Quantum Algorithm for Triangle Finding in Sparse Graphs", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a quantum algorithm for triangle finding over sparse\ngraphs that improves over the previous best quantum algorithm for this task by\nBuhrman et al. [SIAM Journal on Computing, 2005]. Our algorithm is based on the\nrecent $\\tilde O(n^{5/4})$-query algorithm given by Le Gall [FOCS 2014] for\ntriangle finding over dense graphs (here $n$ denotes the number of vertices in\nthe graph). We show in particular that triangle finding can be solved with\n$O(n^{5/4-\\epsilon})$ queries for some constant $\\epsilon>0$ whenever the graph\nhas at most $O(n^{2-c})$ edges for some constant $c>0$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 15:16:31 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Nakajima", "Shogo", ""]]}, {"id": "1507.06953", "submitter": "L\\'aszl\\'o Kozma", "authors": "Parinya Chalermsook, Mayank Goswami, Laszlo Kozma, Kurt Mehlhorn,\n  Thatchaphol Saranurak", "title": "Pattern-avoiding access in binary search trees", "comments": "To be presented at FOCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic optimality conjecture is perhaps the most fundamental open\nquestion about binary search trees (BST). It postulates the existence of an\nasymptotically optimal online BST, i.e. one that is constant factor competitive\nwith any BST on any input access sequence. The two main candidates for dynamic\noptimality in the literature are splay trees [Sleator and Tarjan, 1985], and\nGreedy [Lucas, 1988; Munro, 2000; Demaine et al. 2009] [..]\n  Dynamic optimality is trivial for almost all sequences: the optimum access\ncost of most length-n sequences is Theta(n log n), achievable by any balanced\nBST. Thus, the obvious missing step towards the conjecture is an understanding\nof the \"easy\" access sequences. [..] The difficulty of proving dynamic\noptimality is witnessed by highly restricted special cases that remain\nunresolved; one prominent example is the traversal conjecture [Sleator and\nTarjan, 1985], which states that preorder sequences (whose optimum is linear)\nare linear-time accessed by splay trees; no online BST is known to satisfy this\nconjecture.\n  In this paper, we prove two different relaxations of the traversal conjecture\nfor Greedy: (i) Greedy is almost linear for preorder traversal, (ii) if a\nlinear-time preprocessing is allowed, Greedy is in fact linear. These\nstatements are corollaries of our more general results that express the\ncomplexity of access sequences in terms of a pattern avoidance parameter k.\n[..] To our knowledge, these are the first upper bounds for Greedy that are not\nknown to hold for any other online BST. To obtain these results we identify an\ninput-revealing property of Greedy. Informally, this means that the execution\nlog partially reveals the structure of the access sequence. This property\nfacilitates the use of rich technical tools from forbidden submatrix theory.\n  [Abridged]\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 18:43:59 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Goswami", "Mayank", ""], ["Kozma", "Laszlo", ""], ["Mehlhorn", "Kurt", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1507.06970", "submitter": "Horia Mania", "authors": "Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht,\n  Kannan Ramchandran, Michael I. Jordan", "title": "Perturbed Iterate Analysis for Asynchronous Stochastic Optimization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze stochastic optimization methods where the input to\neach gradient update is perturbed by bounded noise. We show that this framework\nforms the basis of a unified approach to analyze asynchronous implementations\nof stochastic optimization algorithms.In this framework, asynchronous\nstochastic optimization algorithms can be thought of as serial methods\noperating on noisy inputs. Using our perturbed iterate framework, we provide\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\ndescent, that are simpler than earlier analyses, remove many assumptions of\nprevious models, and in some cases yield improved upper bounds on the\nconvergence rates. We proceed to apply our framework to develop and analyze\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\nsparse and parallel version of SVRG is in some cases more than four orders of\nmagnitude faster than the standard SVRG algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 19:36:13 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 20:00:45 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Mania", "Horia", ""], ["Pan", "Xinghao", ""], ["Papailiopoulos", "Dimitris", ""], ["Recht", "Benjamin", ""], ["Ramchandran", "Kannan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1507.07038", "submitter": "Ali Alatabbi", "authors": "Ali Alatabbi, Jacqueline W. Daykin, M. Sohel Rahman and W. F. Smyth", "title": "String Comparison in $V$-Order: New Lexicographic Properties & On-line\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $V$-order is a global order on strings related to Unique Maximal\nFactorization Families (UMFFs), which are themselves generalizations of Lyndon\nwords. $V$-order has recently been proposed as an alternative to\nlexicographical order in the computation of suffix arrays and in the\nsuffix-sorting induced by the Burrows-Wheeler transform. Efficient $V$-ordering\nof strings thus becomes a matter of considerable interest. In this paper we\npresent new and surprising results on $V$-order in strings, then go on to\nexplore the algorithmic consequences.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 22:40:14 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Alatabbi", "Ali", ""], ["Daykin", "Jacqueline W.", ""], ["Rahman", "M. Sohel", ""], ["Smyth", "W. F.", ""]]}, {"id": "1507.07080", "submitter": "Simon Puglisi", "authors": "Djamal Belazzougui and Simon J. Puglisi", "title": "Range Predecessor and Lempel-Ziv Parsing", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lempel-Ziv parsing of a string (LZ77 for short) is one of the most\nimportant and widely-used algorithmic tools in data compression and string\nprocessing. We show that the Lempel-Ziv parsing of a string of length $n$ on an\nalphabet of size $\\sigma$ can be computed in $O(n\\log\\log\\sigma)$ time ($O(n)$\ntime if we allow randomization) using $O(n\\log\\sigma)$ bits of working space;\nthat is, using space proportional to that of the input string in bits. The\nprevious fastest algorithm using $O(n\\log\\sigma)$ space takes\n$O(n(\\log\\sigma+\\log\\log n))$ time. We also consider the important rightmost\nvariant of the problem, where the goal is to associate with each phrase of the\nparsing its most recent occurrence in the input string. We solve this problem\nin $O(n(1 + (\\log\\sigma/\\sqrt{\\log n}))$ time, using the same working space as\nabove. The previous best solution for rightmost parsing uses\n$O(n(1+\\log\\sigma/\\log\\log n))$ time and $O(n\\log n)$ space. As a bonus, in our\nsolution for rightmost parsing we provide a faster construction method for\nefficient 2D orthogonal range reporting, which is of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 08:42:56 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1507.07121", "submitter": "Anke Van Zuylen", "authors": "Anke van Zuylen", "title": "Improved Approximations for Cubic and Cubic Bipartite TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show improved approximation guarantees for the traveling salesman problem\non cubic graphs, and cubic bipartite graphs. For cubic bipartite graphs with n\nnodes, we improve on recent results of Karp and Ravi (2014) by giving a simple\n\"local improvement\" algorithm that finds a tour of length at most 5/4 n - 2.\nFor 2-connected cubic graphs, we show that the techniques of Moemke and\nSvensson (2011) can be combined with the techniques of Correa, Larre and Soto\n(2012), to obtain a tour of length at most (4/3-1/8754)n.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 17:05:06 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 08:30:48 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 15:28:39 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["van Zuylen", "Anke", ""]]}, {"id": "1507.07225", "submitter": "Chihao Zhang", "authors": "Yitong Yin, Chihao Zhang", "title": "Spatial mixing and approximate counting for Potts model on graphs with\n  bounded average degree", "comments": "The result of this paper generalizes the result of our previous work\n  arXiv:1503.03351", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a notion of contraction function for a family of graphs and\nestablish its connection to the strong spatial mixing for spin systems. More\nspecifically, we show that for anti-ferromagnetic Potts model on families of\ngraphs characterized by a specific contraction function, the model exhibits\nstrong spatial mixing, and if further the graphs exhibit certain local sparsity\nwhich are very natural and easy to satisfy by typical sparse graphs, then we\nalso have FPTAS for computing the partition function.\n  This new characterization of strong spatial mixing of multi-spin system does\nnot require maximum degree of the graphs to be bounded, but instead it relates\nthe decay of correlation of the model to a notion of effective average degree\nmeasured by the contraction of a function on the family of graphs. It also\ngeneralizes other notion of effective average degree which may determine the\nstrong spatial mixing, such as the connective constant, whose connection to\nstrong spatial mixing is only known for very simple models and is not\nextendable to general spin systems.\n  As direct consequences: (1) we obtain FPTAS for the partition function of\n$q$-state anti-ferromagnetic Potts model with activity $0\\le\\beta<1$ on graphs\nof maximum degree bounded by $d$ when $q> 3(1-\\beta)d+1$, improving the\nprevious best bound $\\beta> 3(1-\\beta)d$ and asymptotically approaching the\ninapproximability threshold $q=(1-\\beta)d$, and (2) we obtain an efficient\nsampler (in the same sense of fully polynomial-time almost uniform sampler,\nFPAUS) for the Potts model on Erd\\H{o}s-R\\'enyi random graph\n$\\mathcal{G}(n,d/n)$ with sufficiently large constant $d$, provided that $q>\n3(1-\\beta)d+4$. In particular when $\\beta=0$, the sampler becomes an FPAUS for\nfor proper $q$-coloring in $\\mathcal{G}(n,d/n)$ with $q> 3d+4$, improving the\ncurrent best bound $q> 5.5d$ for FPAUS for $q$-coloring in\n$\\mathcal{G}(n,d/n)$.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 17:24:44 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Yin", "Yitong", ""], ["Zhang", "Chihao", ""]]}, {"id": "1507.07237", "submitter": "Shahar Dobzinski", "authors": "Shahar Dobzinski and Ami Mor", "title": "A Deterministic Algorithm for Maximizing Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing a non-negative submodular function was introduced\nby Feige, Mirrokni, and Vondrak [FOCS'07] who provided a deterministic\nlocal-search based algorithm that guarantees an approximation ratio of $\\frac 1\n3$, as well as a randomized $\\frac 2 5$-approximation algorithm. An extensive\nline of research followed and various algorithms with improving approximation\nratios were developed, all of them are randomized. Finally, Buchbinder et al.\n[FOCS'12] presented a randomized $\\frac 1 2$-approximation algorithm, which is\nthe best possible.\n  This paper gives the first deterministic algorithm for maximizing a\nnon-negative submodular function that achieves an approximation ratio better\nthan $\\frac 1 3$. The approximation ratio of our algorithm is $\\frac 2 5$. Our\nalgorithm is based on recursive composition of solutions obtained by the local\nsearch algorithm of Feige et al. We show that the $\\frac 2 5$ approximation\nratio can be guaranteed when the recursion depth is $2$, and leave open the\nquestion of whether the approximation ratio improves as the recursion depth\nincreases.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 19:40:24 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Dobzinski", "Shahar", ""], ["Mor", "Ami", ""]]}, {"id": "1507.07330", "submitter": "Kenji Kume", "authors": "Kenji Kume and Naoko Nose-Togawa", "title": "Spectral structure of singular spectrum decomposition for time series", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular spectrum analysis (SSA) is a nonparametric and adaptive spectral\ndecomposition of a time series. The singular value decomposition of the\ntrajectory matrix and the anti-diagonal averaging leads to a time-series\ndecomposition. In this algorithm, a single free parameter, window length $K$,\nis involved which is the FIR filter length for the time series. There are no\ngenerally accepted criterion for the proper choice of the window length $K$.\nMoreover, the proper window length depends on the specific problem which we are\ninterested in. Thus, it is important to monitor the spectral structure of the\nSSA decomposition and its window length dependence in detail for the practical\napplication. In this paper, based on the filtering interpretation of SSA, it is\nshown that the decomposition of the power spectrum for the original time series\nis possible with the filters constructed from the eigenvectors of the\nlagged-covariance matrix. With this, we can obtain insights into the spectral\nstructure of the SSA decomposition and it helps us for the proper choice of the\nwindow length in the practical application of SSA.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 08:46:25 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Kume", "Kenji", ""], ["Nose-Togawa", "Naoko", ""]]}, {"id": "1507.07368", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty and Ariel Gabizon", "title": "Almost Optimal Cover-Free Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roughly speaking, an $(n,(r,s))$-Cover Free Family (CFF) is a small set of\n$n$-bit strings such that: \"in any $d:=r+s$ indices we see all patterns of\nweight $r$\". CFFs have been of interest for a long time both in discrete\nmathematics as part of block design theory, and in theoretical computer science\nwhere they have found a variety of applications, for example, in parametrized\nalgorithms where they were introduced in the recent breakthrough work of Fomin,\nLokshtanov and Saurabh under the name `lopsided universal sets'.\n  In this paper we give the first explicit construction of cover-free families\nof optimal size up to lower order multiplicative terms, {for any $r$ and $s$}.\nIn fact, our construction time is almost linear in the size of the family.\nBefore our work, such a result existed only for $r=d^{o(1)}$. and $r=\n\\omega(d/(\\log\\log d\\log\\log\\log d))$. As a sample application, we improve the\nrunning times of parameterized algorithms from the recent work of Gabizon,\nLokshtanov and Pilipczuk.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 11:24:08 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Gabizon", "Ariel", ""]]}, {"id": "1507.07396", "submitter": "Sebastian Ott", "authors": "Chien-Chung Huang and Sebastian Ott", "title": "A Combinatorial Approximation Algorithm for Graph Balancing with Light\n  Hyper Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makespan minimization in restricted assignment $(R|p_{ij}\\in \\{p_j,\n\\infty\\}|C_{\\max})$ is a classical problem in the field of machine scheduling.\nIn a landmark paper in 1990 [8], Lenstra, Shmoys, and Tardos gave a\n2-approximation algorithm and proved that the problem cannot be approximated\nwithin 1.5 unless P=NP. The upper and lower bounds of the problem have been\nessentially unimproved in the intervening 25 years, despite several remarkable\nsuccessful attempts in some special cases of the problem [2,4,12] recently.\n  In this paper, we consider a special case called graph-balancing with light\nhyper edges, where heavy jobs can be assigned to at most two machines while\nlight jobs can be assigned to any number of machines. For this case, we present\nalgorithms with approximation ratios strictly better than 2. Specifically,\n  Two job sizes: Suppose that light jobs have weight $w$ and heavy jobs have\nweight $W$, and $w < W$. We give a $1.5$-approximation algorithm (note that the\ncurrent 1.5 lower bound is established in an even more restrictive setting\n[1,3]). Indeed, depending on the specific values of $w$ and $W$, sometimes our\nalgorithm guarantees sub-1.5 approximation ratios.\n  Arbitrary job sizes: Suppose that $W$ is the largest given weight, heavy jobs\nhave weights in the range of $(\\beta W, W]$, where $4/7\\leq \\beta < 1$, and\nlight jobs have weights in the range of $(0,\\beta W]$. We present a\n$(5/3+\\beta/3)$-approximation algorithm.\n  Our algorithms are purely combinatorial, without the need of solving a linear\nprogram as required in most other known approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 13:21:26 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 10:45:46 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2015 07:34:56 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Ott", "Sebastian", ""]]}, {"id": "1507.07402", "submitter": "David Harris", "authors": "Antares Chen, David G. Harris, Aravind Srinivasan", "title": "Partial resampling to approximate covering integer programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider column-sparse covering integer programs, a generalization of set\ncover, which have a long line of research of (randomized) approximation\nalgorithms. We develop a new rounding scheme based on the Partial Resampling\nvariant of the Lov\\'{a}sz Local Lemma developed by Harris & Srinivasan (2019).\n  This achieves an approximation ratio of $1 + \\frac{\\ln\n(\\Delta_1+1)}{a_{\\min}} + O\\Big( \\log(1 + \\sqrt{ \\frac{\\log\n(\\Delta_1+1)}{a_{\\min}}} \\Big)$, where $a_{\\min}$ is the minimum covering\nconstraint and $\\Delta_1$ is the maximum $\\ell_1$-norm of any column of the\ncovering matrix (whose entries are scaled to lie in $[0,1]$). When there are\nadditional constraints on the variable sizes, we show an approximation ratio of\n$\\ln \\Delta_0 + O(\\log \\log \\Delta_0)$ (where $\\Delta_0$ is the maximum number\nof non-zero entries in any column of the covering matrix). These results\nimprove asymptotically, in several different ways, over results of Srinivasan\n(2006) and Kolliopoulos & Young (2005).\n  We show nearly-matching inapproximability and integrality-gap lower bounds.\nWe also show that the rounding process leads to negative correlation among the\nvariables, which allows us to handle multi-criteria programs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 13:39:10 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 14:47:42 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 17:22:02 GMT"}, {"version": "v4", "created": "Fri, 9 Dec 2016 18:59:06 GMT"}, {"version": "v5", "created": "Fri, 7 Apr 2017 18:26:30 GMT"}, {"version": "v6", "created": "Sat, 14 Apr 2018 15:16:30 GMT"}, {"version": "v7", "created": "Thu, 20 Sep 2018 21:57:05 GMT"}, {"version": "v8", "created": "Thu, 2 Jul 2020 22:08:03 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Chen", "Antares", ""], ["Harris", "David G.", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1507.07495", "submitter": "Asif Shakeel", "authors": "David A. Meyer and Asif Shakeel", "title": "Estimating an Activity Driven Hidden Markov Model", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a Hidden Markov Model (HMM) in which each hidden state has\ntime-dependent $\\textit{activity levels}$ that drive transitions and emissions,\nand show how to estimate its parameters. Our construction is motivated by the\nproblem of inferring human mobility on sub-daily time scales from, for example,\nmobile phone records.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 17:37:27 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Meyer", "David A.", ""], ["Shakeel", "Asif", ""]]}, {"id": "1507.07497", "submitter": "Pavel Kolev", "authors": "Gorav Jindal and Pavel Kolev", "title": "An Efficient Parallel Algorithm for Spectral Sparsification of Laplacian\n  and SDDM Matrix Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For \"large\" class $\\mathcal{C}$ of continuous probability density functions\n(p.d.f.), we demonstrate that for every $w\\in\\mathcal{C}$ there is mixture of\ndiscrete Binomial distributions (MDBD) with $T\\geq N\\sqrt{\\phi_{w}/\\delta}$\ndistinct Binomial distributions $B(\\cdot,N)$ that $\\delta$-approximates a\ndiscretized p.d.f. $\\widehat{w}(i/N)\\triangleq\nw(i/N)/[\\sum_{\\ell=0}^{N}w(\\ell/N)]$ for all $i\\in[3:N-3]$, where\n$\\phi_{w}\\geq\\max_{x\\in[0,1]}|w(x)|$. Also, we give two efficient parallel\nalgorithms to find such MDBD.\n  Moreover, we propose a sequential algorithm that on input MDBD with $N=2^k$\nfor $k\\in\\mathbb{N}_{+}$ that induces a discretized p.d.f. $\\beta$, $B=D-M$\nthat is either Laplacian or SDDM matrix and parameter $\\epsilon\\in(0,1)$,\noutputs in $\\widehat{O}(\\epsilon^{-2}m + \\epsilon^{-4}nT)$ time a spectral\nsparsifier $D-\\widehat{M}_{N} \\approx_{\\epsilon}\nD-D\\sum_{i=0}^{N}\\beta_{i}(D^{-1} M)^i$ of a matrix-polynomial, where\n$\\widehat{O}(\\cdot)$ notation hides $\\mathrm{poly}(\\log n,\\log N)$ factors.\nThis improves the Cheng et al.'s [CCLPT15] algorithm whose run time is\n$\\widehat{O}(\\epsilon^{-2} m N^2 + NT)$.\n  Furthermore, our algorithm is parallelizable and runs in work\n$\\widehat{O}(\\epsilon^{-2}m + \\epsilon^{-4}nT)$ and depth $O(\\log\nN\\cdot\\mathrm{poly}(\\log n)+\\log T)$. Our main algorithmic contribution is to\npropose the first efficient parallel algorithm that on input continuous p.d.f.\n$w\\in\\mathcal{C}$, matrix $B=D-M$ as above, outputs a spectral sparsifier of\nmatrix-polynomial whose coefficients approximate component-wise the discretized\np.d.f. $\\widehat{w}$.\n  Our results yield the first efficient and parallel algorithm that runs in\nnearly linear work and poly-logarithmic depth and analyzes the long term\nbehaviour of Markov chains in non-trivial settings. In addition, we strengthen\nthe Spielman and Peng's [PS14] parallel SDD solver.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 17:39:21 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 13:27:46 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 11:11:45 GMT"}, {"version": "v4", "created": "Thu, 21 Apr 2016 10:29:39 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Jindal", "Gorav", ""], ["Kolev", "Pavel", ""]]}, {"id": "1507.07581", "submitter": "Shant Boodaghians", "authors": "Shant Boodaghians and Adrian Vetta", "title": "Testing Consumer Rationality using Perfect Graphs and Oriented Discs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a consumer data-set, the axioms of revealed preference proffer a binary\ntest for rational behaviour. A natural (non-binary) measure of the degree of\nrationality exhibited by the consumer is the minimum number of data points\nwhose removal induces a rationalisable data-set.We study the computational\ncomplexity of the resultant consumer rationality problem in this paper. This\nproblem is, in the worst case, equivalent (in terms of approximation) to the\ndirected feedback vertex set problem. Our main result is to obtain an exact\nthreshold on the number of commodities that separates easy cases and hard\ncases. Specifically, for two-commodity markets the consumer rationality problem\nis polynomial time solvable; we prove this via a reduction to the vertex cover\nproblem on perfect graphs. For three-commodity markets, however, the problem is\nNP-complete; we prove thisusing a reduction from planar 3-SAT that is based\nupon oriented-disc drawings.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 20:31:51 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 23:02:29 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Boodaghians", "Shant", ""], ["Vetta", "Adrian", ""]]}, {"id": "1507.07598", "submitter": "Kevin Keys", "authors": "Kenneth Lange and Kevin L. Keys", "title": "The proximal distance algorithm", "comments": "22 pages, 0 figures, 8 tables, modified from conference publication", "journal-ref": "In Proceedings of the 2014 International Congress of\n  Mathematicians. Seoul: Kyung Moon, 4:95-116", "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MM principle is a device for creating optimization algorithms satisfying\nthe ascent or descent property. The current survey emphasizes the role of the\nMM principle in nonlinear programming. For smooth functions, one can construct\nan adaptive interior point method based on scaled Bregmann barriers. This\nalgorithm does not follow the central path. For convex programming subject to\nnonsmooth constraints, one can combine an exact penalty method with distance\nmajorization to create versatile algorithms that are effective even in discrete\noptimization. These proximal distance algorithms are highly modular and reduce\nto set projections and proximal mappings, both very well-understood techniques\nin optimization. We illustrate the possibilities in linear programming, binary\npiecewise-linear programming, nonnegative quadratic programming, $\\ell_0$\nregression, matrix completion, and inverse sparse covariance estimation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 22:53:01 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Lange", "Kenneth", ""], ["Keys", "Kevin L.", ""]]}, {"id": "1507.07622", "submitter": "Takuya Takagi", "authors": "Takuya Takagi, Shunsuke Inenaga, Hiroki Arimura, Dany Breslauer, and\n  Diptarama Hendrian", "title": "Fully-Online Suffix Tree and Directed Acyclic Word Graph Construction\n  for Multiple Texts", "comments": "28 pages, 6 figures, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider construction of the suffix tree and the directed acyclic word\ngraph (DAWG) indexing data structures for a collection $\\mathcal{T}$ of texts,\nwhere a new symbol may be appended to any text in $\\mathcal{T} = \\{T_1, \\ldots,\nT_K\\}$, at any time. This fully-online scenario, which arises in dynamically\nindexing multi-sensor data, is a natural generalization of the long solved\nsemi-online text indexing problem, where texts $T_1, \\ldots, T_{k}$ are\npermanently fixed before the next text $T_{k+1}$ is processed for each $1 \\leq\nk < K$. We present fully-online algorithms that construct the suffix tree and\nthe DAWG for $\\mathcal{T}$ in $O(N \\log \\sigma)$ time and $O(N)$ space, where\n$N$ is the total lengths of the strings in $\\mathcal{T}$ and $\\sigma$ is their\nalphabet size. The standard explicit representation of the suffix tree leaf\nedges and some DAWG edges must be relaxed in our fully-online scenario, since\ntoo many updates on these edges are required in the worst case. Instead, we\nprovide access to the updated suffix tree leaf edge labels and the DAWG edges\nto be redirected via auxiliary data structures, in $O(\\log \\sigma)$ time per\nadded character.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 02:15:02 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 08:57:15 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2016 08:18:14 GMT"}, {"version": "v4", "created": "Tue, 23 Jan 2018 19:39:17 GMT"}, {"version": "v5", "created": "Thu, 12 Jul 2018 14:35:38 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Takagi", "Takuya", ""], ["Inenaga", "Shunsuke", ""], ["Arimura", "Hiroki", ""], ["Breslauer", "Dany", ""], ["Hendrian", "Diptarama", ""]]}, {"id": "1507.07727", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Mat\\'u\\v{s} Mihal\\'ak, Przemys{\\l}aw Uzna\\'nski, Pencho Yordanov", "title": "Prime Factorization of the Kirchhoff Polynomial: Compact Enumeration of\n  Arborescences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of enumerating all rooted directed spanning trees\n(arborescences) of a directed graph (digraph) $G=(V,E)$ of $n$ vertices. An\narborescence $A$ consisting of edges $e_1,\\ldots,e_{n-1}$ can be represented as\na monomial $e_1\\cdot e_2 \\cdots e_{n-1}$ in variables $e \\in E$. All\narborescences $\\mathsf{arb}(G)$ of a digraph then define the Kirchhoff\npolynomial $\\sum_{A \\in \\mathsf{arb}(G)} \\prod_{e\\in A} e$. We show how to\ncompute a compact representation of the Kirchhoff polynomial -- its prime\nfactorization, and how it relates to combinatorial properties of digraphs such\nas strong connectivity and vertex domination. In particular, we provide digraph\ndecomposition rules that correspond to factorization steps of the polynomial,\nand also give necessary and sufficient primality conditions of the resulting\nfactors expressed by connectivity properties of the corresponding decomposed\ncomponents. Thereby, we obtain a linear time algorithm for decomposing a\ndigraph into components corresponding to factors of the initial polynomial, and\na guarantee that no finer factorization is possible. The decomposition serves\nas a starting point for a recursive deletion-contraction algorithm, and also as\na preprocessing phase for iterative enumeration algorithms. Both approaches\nproduce a compressed output and retain some structural properties in the\nresulting polynomial. This proves advantageous in practical applications such\nas calculating steady states on digraphs governed by Laplacian dynamics, or\ncomputing the greatest common divisor of Kirchhoff polynomials. Finally, we\ninitiate the study of a class of digraphs which allow for a practical\nenumeration of arborescences. Using our decomposition rules we observe that\nvarious digraphs from real-world applications fall into this class or are\nstructurally similar to it.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 11:15:09 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Mihal\u00e1k", "Mat\u00fa\u0161", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""], ["Yordanov", "Pencho", ""]]}, {"id": "1507.07789", "submitter": "Eric Friedman", "authors": "Eric J. Friedman and Adam S. Landsberg", "title": "Duality and Nonlinear Graph Laplacians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an iterative algorithm for solving a class of \\\\nonlinear\nLaplacian system of equations in $\\tilde{O}(k^2m \\log(kn/\\epsilon))$\niterations, where $k$ is a measure of nonlinearity, $n$ is the number of\nvariables, $m$ is the number of nonzero entries in the graph Laplacian $L$,\n$\\epsilon$ is the solution accuracy and $\\tilde{O}()$ neglects (non-leading)\nlogarithmic terms. This algorithm is a natural nonlinear extension of the one\nby of Kelner et. al., which solves a linear Laplacian system of equations in\nnearly linear time. Unlike the linear case, in the nonlinear case each\niteration takes $\\tilde{O}(n)$ time so the total running time is\n$\\tilde{O}(k^2mn \\log(kn/\\epsilon))$. For sparse graphs where $m = O(n)$ and\nfixed $k$ this nonlinear algorithm is $\\tilde{O}(n^2 \\log(n/\\epsilon))$ which\nis slightly faster than standard methods for solving linear equations, which\nrequire approximately $O(n^{2.38})$ time. Our analysis relies on the\nconstruction of a nonlinear \"energy function\" and a nonlinear extension of the\nduality analysis of Kelner et. al to the nonlinear case without any explicit\nreferences to spectral analysis or electrical flows. These new insights and\nresults provide tools for more general extensions to spectral theory and\nnonlinear applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 14:46:48 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Friedman", "Eric J.", ""], ["Landsberg", "Adam S.", ""]]}, {"id": "1507.08080", "submitter": "Wolfgang Mulzer", "authors": "Oswin Aichholzer, Vincent Kusters, Wolfgang Mulzer, Alexander Pilz,\n  Manuel Wettstein", "title": "An Optimal Algorithm for Reconstructing Point Set Order Types from\n  Radial Orderings", "comments": "23 pages, 11 figures; a preliminary version appeared at ISAAC 2015", "journal-ref": "International Journal of Computational Geometry and Applications\n  (IJCGA), 27(1n2), 2017, pp. 57-83", "doi": "10.1142/S0218195917600044", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a set of $n$ labeled points in the plane. The radial system of $P$\ndescribes, for each $p\\in P$, the order in which a ray that rotates around $p$\nencounters the points in $P \\setminus \\{p\\}$. This notion is related to the\norder type of $P$, which describes the orientation (clockwise or\ncounterclockwise) of every ordered triple in $P$. Given only the order type,\nthe radial system is uniquely determined and can easily be obtained. The\nconverse, however, is not true. Indeed, let $R$ be the radial system of $P$,\nand let $T(R)$ be the set of all order types with radial system $R$ (we define\n$T(R) = \\emptyset$ for the case that $R$ is not a valid radial system).\nAichholzer et al. (Reconstructing Point Set Order Types from Radial Orderings,\nin ISAAC 2014) show that $T(R)$ may contain up to $n-1$ order types. They also\nprovide polynomial-time algorithms to compute $T(R)$ when only $R$ is given.\n  We describe a new algorithm for finding $T(R)$. The algorithm constructs the\nconvex hulls of all possible point sets with the radial system $R$. After that,\norientation queries on point triples can be answered in constant time. A\nrepresentation of this set of convex hulls can be found in $O(n)$ queries to\nthe radial system, using $O(n)$ additional processing time. This is optimal.\nOur results also generalize to abstract order types.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:50:06 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 18:20:24 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Aichholzer", "Oswin", ""], ["Kusters", "Vincent", ""], ["Mulzer", "Wolfgang", ""], ["Pilz", "Alexander", ""], ["Wettstein", "Manuel", ""]]}, {"id": "1507.08139", "submitter": "James Payor", "authors": "Donggu Kang and James Payor", "title": "Flow Rounding", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider flow rounding: finding an integral flow from a fractional flow.\nCosted flow rounding asks that we find an integral flow with no worse cost.\nRandomized flow rounding requires we randomly find an integral flow such that\nthe expected flow along each edge matches the fractional flow. Both problems\nare reduced to cycle canceling, for which we develop an $O(m \\log(n^2/m))$\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 13:29:14 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Kang", "Donggu", ""], ["Payor", "James", ""]]}, {"id": "1507.08158", "submitter": "P{\\aa}l Gr{\\o}n{\\aa}s Drange", "authors": "P{\\aa}l Gr{\\o}n{\\aa}s Drange and Felix Reidl and Fernando S\\'anchez\n  Villaamil and Somnath Sikdar", "title": "Fast Biclustering by Dual Parameterization", "comments": "Accepted for presentation at IPEC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study two clustering problems, Starforest Editing, the problem of adding\nand deleting edges to obtain a disjoint union of stars, and the generalization\nBicluster Editing. We show that, in addition to being NP-hard, none of the\nproblems can be solved in subexponential time unless the exponential time\nhypothesis fails.\n  Misra, Panolan, and Saurabh (MFCS 2013) argue that introducing a bound on the\nnumber of connected components in the solution should not make the problem\neasier: In particular, they argue that the subexponential time algorithm for\nediting to a fixed number of clusters (p-Cluster Editing) by Fomin et al. (J.\nComput. Syst. Sci., 80(7) 2014) is an exception rather than the rule. Here, p\nis a secondary parameter, bounding the number of components in the solution.\n  However, upon bounding the number of stars or bicliques in the solution, we\nobtain algorithms which run in time $2^{5 \\sqrt{pk}} + O(n+m)$ for p-Starforest\nEditing and $2^{O(p \\sqrt{k} \\log(pk))} + O(n+m)$ for p-Bicluster Editing. We\nobtain a similar result for the more general case of t-Partite p-Cluster\nEditing. This is subexponential in k for fixed number of clusters, since p is\nthen considered a constant.\n  Our results even out the number of multivariate subexponential time\nalgorithms and give reasons to believe that this area warrants further study.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 14:31:41 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Drange", "P\u00e5l Gr\u00f8n\u00e5s", ""], ["Reidl", "Felix", ""], ["Villaamil", "Fernando S\u00e1nchez", ""], ["Sikdar", "Somnath", ""]]}, {"id": "1507.08348", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi and Dana Moshkovitz", "title": "Approximating Dense Max 2-CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a polynomial-time algorithm that approximates\nsufficiently high-value Max 2-CSPs on sufficiently dense graphs to within\n$O(N^{\\varepsilon})$ approximation ratio for any constant $\\varepsilon > 0$.\nUsing this algorithm, we also achieve similar results for free games,\nprojection games on sufficiently dense random graphs, and the Densest\n$k$-Subgraph problem with sufficiently dense optimal solution. Note, however,\nthat algorithms with similar guarantees to the last algorithm were in fact\ndiscovered prior to our work by Feige et al. and Suzuki and Tokuyama.\n  In addition, our idea for the above algorithms yields the following\nby-product: a quasi-polynomial time approximation scheme (QPTAS) for\nsatisfiable dense Max 2-CSPs with better running time than the known\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 00:02:49 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Manurangsi", "Pasin", ""], ["Moshkovitz", "Dana", ""]]}, {"id": "1507.08384", "submitter": "Rico Zenklusen", "authors": "Moran Feldman and Rico Zenklusen", "title": "The Submodular Secretary Problem Goes Linear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade, the matroid secretary problem (MSP) became one of the\nmost prominent classes of online selection problems. Partially linked to its\nnumerous applications in mechanism design, substantial interest arose also in\nthe study of nonlinear versions of MSP, with a focus on the submodular matroid\nsecretary problem (SMSP). So far, O(1)-competitive algorithms have been\nobtained for SMSP over some basic matroid classes. This created some hope that,\nanalogously to the matroid secretary conjecture, one may even obtain\nO(1)-competitive algorithms for SMSP over any matroid. However, up to now, most\nquestions related to SMSP remained open, including whether SMSP may be\nsubstantially more difficult than MSP; and more generally, to what extend MSP\nand SMSP are related.\n  Our goal is to address these points by presenting general black-box\nreductions from SMSP to MSP. In particular, we show that any O(1)-competitive\nalgorithm for MSP, even restricted to a particular matroid class, can be\ntransformed in a black-box way to an O(1)-competitive algorithm for SMSP over\nthe same matroid class. This implies that the matroid secretary conjecture is\nequivalent to the same conjecture for SMSP. Hence, in this sense SMSP is not\nharder than MSP. Also, to find O(1)-competitive algorithms for SMSP over a\nparticular matroid class, it suffices to consider MSP over the same matroid\nclass. Using our reductions we obtain many first and improved O(1)-competitive\nalgorithms for SMSP over various matroid classes by leveraging known algorithms\nfor MSP. Moreover, our reductions imply an O(loglog(rank))-competitive\nalgorithm for SMSP, thus, matching the currently best asymptotic algorithm for\nMSP, and substantially improving on the previously best\nO(log(rank))-competitive algorithm for SMSP.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:28:46 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Feldman", "Moran", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1507.08501", "submitter": "Dhiraj Madan", "authors": "Dhiraj Madan and Sandeep Sen", "title": "Randomised Rounding with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new techniques for rounding packing integer programs using\niterative randomized rounding. It is based on a novel application of\nmultidimensional Brownian motion in $\\mathbb{R}^n$. Let $\\overset{\\sim}{x} \\in\n{[0,1]}^n$ be a fractional feasible solution of a packing constraint $A x \\leq\n1,\\ \\ $ $A \\in {\\{0,1 \\}}^{m\\times n}$ that maximizes a linear objective\nfunction. The independent randomized rounding method of Raghavan-Thompson\nrounds each variable $x_i$ to 1 with probability $\\overset{\\sim}{x_i}$ and 0\notherwise. The expected value of the rounded objective function matches the\nfractional optimum and no constraint is violated by more than $O(\\frac{\\log m}\n{\\log\\log m})$.In contrast, our algorithm iteratively transforms\n$\\overset{\\sim}{x}$ to $\\hat{x} \\in {\\{ 0,1\\}}^{n}$ using a random walk, such\nthat the expected values of $\\hat{x}_i$'s are consistent with the\nRaghavan-Thompson rounding. In addition, it gives us intermediate values $x'$\nwhich can then be used to bias the rounding towards a superior solution.The\nreduced dependencies between the constraints of the sparser system can be\nexploited using {\\it Lovasz Local Lemma}. For $m$ randomly chosen packing\nconstraints in $n$ variables, with $k$ variables in each inequality, the\nconstraints are satisfied within $O(\\frac{\\log (mkp\\log m/n) }{\\log\\log\n(mkp\\log m/n)})$ with high probability where $p$ is the ratio between the\nmaximum and minimum coefficients of the linear objective function. Further, we\nexplore trade-offs between approximation factors and error, and present\napplications to well-known problems like circuit-switching, maximum independent\nset of rectangles and hypergraph $b$-matching. Our methods apply to the\nweighted instances of the problems and are likely to lead to better insights\nfor even dependent rounding.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 13:47:09 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Madan", "Dhiraj", ""], ["Sen", "Sandeep", ""]]}, {"id": "1507.08705", "submitter": "Peter Lofgren", "authors": "Peter Lofgren, Siddhartha Banerjee, Ashish Goel", "title": "Bidirectional PageRank Estimation: From Average-Case to Worst-Case", "comments": "Workshop on Algorithms and Models for the Web-Graph (WAW) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for estimating the Personalized PageRank (PPR)\nbetween a source and target node on undirected graphs, with sublinear\nrunning-time guarantees over the worst-case choice of source and target nodes.\nOur work builds on a recent line of work on bidirectional estimators for PPR,\nwhich obtained sublinear running-time guarantees but in an average-case sense,\nfor a uniformly random choice of target node. Crucially, we show how the\nreversibility of random walks on undirected networks can be exploited to\nconvert average-case to worst-case guarantees. While past bidirectional methods\ncombine forward random walks with reverse local pushes, our algorithm combines\nforward local pushes with reverse random walks. We also discuss how to modify\nour methods to estimate random-walk probabilities for any length distribution,\nthereby obtaining fast algorithms for estimating general graph diffusions,\nincluding the heat kernel, on undirected networks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 23:18:32 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 23:47:58 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 02:18:58 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lofgren", "Peter", ""], ["Banerjee", "Siddhartha", ""], ["Goel", "Ashish", ""]]}, {"id": "1507.08792", "submitter": "R.B. Sandeep", "authors": "R. B. Sandeep, Naveen Sivadasan", "title": "A cubic vertex kernel for Diamond-free Edge Deletion and more", "comments": "A preliminary version of this paper has appeared in the proceedings\n  of IPEC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A diamond is a graph obtained by removing an edge from a complete graph on\nfour vertices. A graph is diamond-free if it does not contain an induced\ndiamond. The Diamond-free Edge Deletion problem asks whether there exist at\nmost $k$ edges in the input graph $G$ whose deletion results in a diamond-free\ngraph. For this problem, a polynomial kernel of $O(k^4$) vertices was found by\nFellows et. al. (Discrete Optimization, 2011).\n  In this paper, we give an improved kernel of $O(k^3)$ vertices for\nDiamond-free Edge Deletion. Further, we give an $O(k^2)$ vertex kernel for a\nrelated problem {Diamond,K_t}-free Edge Deletion, where $t\\geq 4$ is any fixed\ninteger. To complement our results, we prove that these problems are\nNP-complete even for $K_4$-free graphs and can be solved neither in\nsubexponential time (i.e., $2^{o(|G|)}$) nor in parameterized subexponential\ntime (i.e., $2^{o(k)}\\cdot |G|^{O(1)}$), unless Exponential Time Hypothesis\nfails. Our reduction implies the hardness and lower bound for a general class\nof problems, where these problems come as a special case.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 08:06:38 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 17:02:10 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Sandeep", "R. B.", ""], ["Sivadasan", "Naveen", ""]]}, {"id": "1507.08838", "submitter": "Ertem Esiner", "authors": "Ertem Esiner and Anwitaman Datta", "title": "Auditable Versioned Data Storage Outsourcing", "comments": "31 Pages, 12 Figures, 2 Tables, 4 Pseudocodes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auditability is crucial for data outsourcing, facilitating accountability and\nidentifying data loss or corruption incidents in a timely manner, reducing in\nturn the risks from such losses. In recent years, in synch with the growing\ntrend of outsourcing, a lot of progress has been made in designing\nprobabilistic (for efficiency) provable data possession (PDP) schemes. However,\neven the recent and advanced PDP solutions that do deal with dynamic data, do\nso in a limited manner, and for only the latest version of the data. A naive\nsolution treating different versions in isolation would work, but leads to\ntremendous overheads, and is undesirable. In this paper, we present algorithms\nto achieve full persistence (all intermediate configurations are preserved and\nare modifiable) for an optimized skip list (known as FlexList) so that\nversioned data can be audited. The proposed scheme provides deduplication at\nthe level of logical, variable sized blocks, such that only the altered parts\nof the different versions are kept, while the persistent data-structure\nfacilitates access (read) of any arbitrary version with the same storage and\nprocess efficiency that state-of-the-art dynamic PDP solutions provide for only\nthe current version, while commit (write) operations incur around 5% additional\ntime. Furthermore, the time overhead for auditing arbitrary versions in\naddition to the latest version is imperceptible even on a low-end server...\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 11:39:23 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Esiner", "Ertem", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1507.08923", "submitter": "Rafal Kapelko", "authors": "Rafa{\\l} Kapelko and Evangelos Kranakis", "title": "On the Displacement for Covering a Unit Interval with Randomly Placed\n  Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $n$ mobile sensors placed independently at random with the uniform\ndistribution on a barrier represented as the unit line segment $[0,1]$. The\nsensors have identical sensing radius, say $r$. When a sensor is displaced on\nthe line a distance equal to $d$ it consumes energy (in movement) which is\nproportional to some (fixed) power $a > 0$ of the distance $d$ traveled. The\nenergy consumption of a system of $n$ sensors thus displaced is defined as the\nsum of the energy consumptions for the displacement of the individual sensors.\n  We focus on the problem of energy efficient displacement of the sensors so\nthat in their final placement the sensor system ensures coverage of the barrier\nand the energy consumed for the displacement of the sensors to these final\npositions is minimized in expectation. In particular, we analyze the problem of\ndisplacing the sensors from their initial positions so as to attain coverage of\nthe unit interval and derive trade-offs for this displacement as a function of\nthe sensor range. We obtain several tight bounds in this setting thus\ngeneralizing several of the results of [10] to any power $a >0$.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 15:52:54 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 09:03:49 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 17:36:44 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Kapelko", "Rafa\u0142", ""], ["Kranakis", "Evangelos", ""]]}]