[{"id": "1104.0733", "submitter": "Wei Ren", "authors": "Wei Ren, Qing Zhao", "title": "A Note on: `Algorithms for Connected Set Cover Problem and\n  Fault-Tolerant Connected Set Cover Problem'", "comments": "6 pages, 1 figure, submitted to Theoretical Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flaw in the greedy approximation algorithm proposed by Zhang et al. for\nminimum connected set cover problem is corrected, and a stronger result on the\napproximation ratio of the modified greedy algorithm is established. The\nresults are now consistent with the existing results on connected dominating\nset problem which is a special case of the minimum connected set cover problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 04:57:44 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ren", "Wei", ""], ["Zhao", "Qing", ""]]}, {"id": "1104.0739", "submitter": "Ran Gelles", "authors": "Ran Gelles and Amit Sahai", "title": "Potent Tree Codes and their applications: Coding for Interactive\n  Communication, revisited", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of reliable interactive communication over a\nnoisy channel. In a breakthrough sequence of papers published in 1992 and 1993,\nSchulman gave non-constructive proofs of the existence of general methods to\nemulate any two-party interactive protocol such that: (1) the emulation\nprotocol takes a constant-factor longer than the original protocol, and (2) if\nthe emulation protocol is executed over a noisy channel, then the probability\nthat the emulation protocol fails is exponentially small in the total length of\nthe protocol. Unfortunately, Schulman's emulation procedures either only work\nin a model with a large amount of shared randomness, or are non-constructive in\nthat they rely on the existence of good tree codes. The only known proofs of\nthe existence of good tree codes are non-constructive, and finding an explicit\nconstruction remains an important open problem. Indeed, randomly generated tree\ncodes are not good tree codes with overwhelming probability.\n  In this work, we revisit the problem of reliable interactive communication,\nand obtain the following results: We introduce a new notion of goodness for a\ntree code, and define the notion of a potent tree code. We believe that this\nnotion is of independent interest. We prove the correctness of an explicit\nemulation procedure based on any potent tree code. We show that a randomly\ngenerated tree code (with suitable constant alphabet size) is a potent tree\ncode with overwhelming probability. Furthermore we are able to partially\nderandomize this result using only O(n) random bits, where $n$ is the depth of\nthe tree.\n  These results allow us to obtain the first fully explicit emulation procedure\nfor reliable interactive communication over noisy channels with a constant\ncommunication overhead, and exponentially small failure probability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 06:10:01 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Gelles", "Ran", ""], ["Sahai", "Amit", ""]]}, {"id": "1104.0848", "submitter": "Girish Varma", "authors": "Ajesh Babu, Nutan Limaye, Jaikumar Radhakrishnan, Girish Varma", "title": "Streaming algorithms for language recognition problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the following problems in the streaming model.\n  Membership testing for \\DLIN We show that every language in \\DLIN\\ can be\nrecognised by a randomized one-pass $O(\\log n)$ space algorithm with inverse\npolynomial one-sided error, and by a deterministic p-pass $O(n/p)$ space\nalgorithm. We show that these algorithms are optimal.\n  Membership testing for \\LL$(k)$ For languages generated by \\LL$(k)$ grammars\nwith a bound of $r$ on the number of nonterminals at any stage in the left-most\nderivation, we show that membership can be tested by a randomized one-pass\n$O(r\\log n)$ space algorithm with inverse polynomial (in $n$) one-sided error.\n  Membership testing for \\DCFL We show that randomized algorithms as efficient\nas the ones described above for \\DLIN\\ and $\\LL(k)$ (which are subclasses of\n\\DCFL) cannot exist for all of \\DCFL: there is a language in \\VPL\\ (a subclass\nof \\DCFL) for which any randomized p-pass algorithm with error bounded by\n$\\epsilon < 1/2$ must use $\\Omega(n/p)$ space.\n  Degree sequence problem We study the problem of determining, given a sequence\n$d_1, d_2,..., d_n$ and a graph $G$, whether the degree sequence of $G$ is\nprecisely $d_1, d_2,..., d_n$. We give a randomized one-pass $O(\\log n)$ space\nalgorithm with inverse polynomial one-sided error probability. We show that our\nalgorithms are optimal.\n  Our randomized algorithms are based on the recent work of Magniez et al.\n\\cite{MMN09}; our lower bounds are obtained by considering related\ncommunication complexity problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 13:47:02 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Babu", "Ajesh", ""], ["Limaye", "Nutan", ""], ["Radhakrishnan", "Jaikumar", ""], ["Varma", "Girish", ""]]}, {"id": "1104.0867", "submitter": "Dan Olteanu", "authors": "Dan Olteanu and Jakub Zavodny", "title": "Factorised Representations of Query Results", "comments": "44 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query tractability has been traditionally defined as a function of input\ndatabase and query sizes, or of both input and output sizes, where the query\nresult is represented as a bag of tuples. In this report, we introduce a\nframework that allows to investigate tractability beyond this setting. The key\ninsight is that, although the cardinality of a query result can be exponential,\nits structure can be very regular and thus factorisable into a nested\nrepresentation whose size is only polynomial in the size of both the input\ndatabase and query.\n  For a given query result, there may be several equivalent representations,\nand we quantify the regularity of the result by its readability, which is the\nminimum over all its representations of the maximum number of occurrences of\nany tuple in that representation. We give a characterisation of\nselect-project-join queries based on the bounds on readability of their results\nfor any input database. We complement it with an algorithm that can find\nasymptotically optimal upper bounds and corresponding factorised\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 15:09:09 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Olteanu", "Dan", ""], ["Zavodny", "Jakub", ""]]}, {"id": "1104.0882", "submitter": "Amitabh Trehan", "authors": "Gopal Pandurangan and Amitabh Trehan", "title": "Xheal: Localized Self-healing using Expanders", "comments": "A shorter version of this to be presented at PODC, 2011, San Jose, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of self-healing in reconfigurable networks (e.g.\npeer-to-peer and wireless mesh networks) that are under repeated attack by an\nomniscient adversary and propose a fully distributed algorithm, Xheal that\nmaintains good expansion and spectral properties of the network, also keeping\nthe network connected. Moreover, Xheal does this while allowing only low\nstretch and degree increase per node. Thus, the algorithm heals global\nproperties while only doing local changes and using only local information.\n  Our work improves over the self-healing algorithms 'Forgiving tree'[PODC\n2008] and 'Forgiving graph'[PODC 2009] (using a similar model) in that we are\nable to give guarantees on degree and stretch, while at the same time\npreserving the expansion and spectral properties of the network. These repairs\npreserve the invariants in the following sense. At any point in the algorithm,\nthe expansion of the graph will be either `better' than the expansion of the\ngraph formed by considering only the adversarial insertions (not the\nadversarial deletions) or the expansion will be, at least, a constant. Also,\nthe stretch i.e. the distance between any pair of nodes in the healed graph is\nno more than a $O(\\log n)$ factor. Similarly, at any point, a node $v$ whose\ndegree would have been $d$ in the graph with adversarial insertions only, will\nhave degree at most $O(\\kappa d)$ in the actual graph, for a small parameter\n$\\kappa$. We also provide bounds on the second smallest eigenvalue of the\nLaplacian which captures key properties such as mixing time, conductance,\ncongestion in routing etc. Our distributed data structure has low amortized\nlatency and bandwidth requirements.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 16:24:16 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Pandurangan", "Gopal", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1104.0919", "submitter": "Benjamin Burton", "authors": "Benjamin A. Burton and Mathias Hiron", "title": "Locating regions in a sequence under density constraints", "comments": "17 pages, 8 figures; v2: minor revisions, additional explanations; to\n  appear in SIAM Journal on Computing", "journal-ref": "SIAM Journal on Computing 42 (2013), no. 3, 1201-1215", "doi": "10.1137/110830605", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several biological problems require the identification of regions in a\nsequence where some feature occurs within a target density range: examples\nincluding the location of GC-rich regions, identification of CpG islands, and\nsequence matching. Mathematically, this corresponds to searching a string of 0s\nand 1s for a substring whose relative proportion of 1s lies between given lower\nand upper bounds. We consider the algorithmic problem of locating the longest\nsuch substring, as well as other related problems (such as finding the shortest\nsubstring or a maximal set of disjoint substrings). For locating the longest\nsuch substring, we develop an algorithm that runs in O(n) time, improving upon\nthe previous best-known O(n log n) result. For the related problems we develop\nO(n log log n) algorithms, again improving upon the best-known O(n log n)\nresults. Practical testing verifies that our new algorithms enjoy significantly\nsmaller time and memory footprints, and can process sequences that are orders\nof magnitude longer as a result.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 19:42:00 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 21:14:45 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Burton", "Benjamin A.", ""], ["Hiron", "Mathias", ""]]}, {"id": "1104.1044", "submitter": "Ming Lam Leung", "authors": "Ming Lam Leung", "title": "Fixed Parameter Tractable Algorithm for Firefighting Problem", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The firefighter problem is defined as below. A fire initially breaks out at a\nvertex r on a graph G. In each step, a firefighter chooses to protect one\nvertex, which is not yet burnt. And the fire spreads out to its unprotected\nneighboring vertices afterwards. The objective of the problem is to choose a\nsequence of vertices to protect, in order to save maximum number of vertices\nfrom the fire.\n  In this paper, we will introduce a parameter k into the firefighter problem\nand give several FPT algorithms using a random separation technique of Cai,\nChan and Chan. We will prove firefighter problem is FPT on general graph if we\ntake total number of vertices burnt to be a parameter. If we parameterize the\nnumber of protected vertices, we discover several FPT algorithms of the\nfirefighter problem on degree bounded graph and unicyclic graph. Furthermore,\nwe also study the firefighter problem on weighted and valued graph, and the\nproblem with multiple fire sources on degree-bounded graph.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 09:33:42 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2011 10:51:21 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Leung", "Ming Lam", ""]]}, {"id": "1104.1080", "submitter": "Gwena\\\"el Joret", "authors": "Samuel Fiorini and Gwena\\\"el Joret", "title": "Approximating the Balanced Minimum Evolution Problem", "comments": null, "journal-ref": "Operations Research Letters, 40/1:31--35, 2012", "doi": "10.1016/j.orl.2011.10.003", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a strong inapproximability result for the Balanced Minimum Evolution\nProblem. Our proof also implies that the problem remains NP-hard even when\nrestricted to metric instances. Furthermore, we give a MST-based\n2-approximation algorithm for the problem for such instances.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 12:42:55 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Fiorini", "Samuel", ""], ["Joret", "Gwena\u00ebl", ""]]}, {"id": "1104.1135", "submitter": "Gregory Gutin", "authors": "R. Crowston, M. Fellows, G. Gutin, M. Jones, F. Rosamond, S. Thomasse\n  and A. Yeo", "title": "Simultaneously Satisfying Linear Equations Over $\\mathbb{F}_2$: MaxLin2\n  and Max-$r$-Lin2 Parameterized Above Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the parameterized problem \\textsc{MaxLin2-AA}[$k$], we are given a system\nwith variables $x_1,...,x_n$ consisting of equations of the form $\\prod_{i \\in\nI}x_i = b$, where $x_i,b \\in \\{-1, 1\\}$ and $I\\subseteq [n],$ each equation has\na positive integral weight, and we are to decide whether it is possible to\nsimultaneously satisfy equations of total weight at least $W/2+k$, where $W$ is\nthe total weight of all equations and $k$ is the parameter (if $k=0$, the\npossibility is assured). We show that \\textsc{MaxLin2-AA}[$k$] has a kernel\nwith at most $O(k^2\\log k)$ variables and can be solved in time $2^{O(k\\log\nk)}(nm)^{O(1)}$. This solves an open problem of Mahajan et al. (2006).\n  The problem \\textsc{Max-$r$-Lin2-AA}[$k,r$] is the same as\n\\textsc{MaxLin2-AA}[$k$] with two differences: each equation has at most $r$\nvariables and $r$ is the second parameter. We prove a theorem on\n\\textsc{Max-$r$-Lin2-AA}[$k,r$] which implies that\n\\textsc{Max-$r$-Lin2-AA}[$k,r$] has a kernel with at most $(2k-1)r$ variables\nimproving a number of results including one by Kim and Williams (2010). The\ntheorem also implies a lower bound on the maximum of a function $f:\\ \\{-1,1\\}^n\n\\rightarrow \\mathbb{R}$ of degree $r$. We show applicability of the lower bound\nby giving a new proof of the Edwards-Erd{\\H o}s bound (each connected graph on\n$n$ vertices and $m$ edges has a bipartite subgraph with at least $m/2 +\n(n-1)/4$ edges) and obtaining a generalization.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 16:11:03 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2011 14:53:43 GMT"}, {"version": "v3", "created": "Sun, 15 May 2011 17:11:47 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Crowston", "R.", ""], ["Fellows", "M.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Rosamond", "F.", ""], ["Thomasse", "S.", ""], ["Yeo", "A.", ""]]}, {"id": "1104.1204", "submitter": "Charless Fowlkes", "authors": "Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes", "title": "Planar Cycle Covering Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new variational lower-bound on the minimum energy configuration\nof a planar binary Markov Random Field (MRF). Our method is based on adding\nauxiliary nodes to every face of a planar embedding of the graph in order to\ncapture the effect of unary potentials. A ground state of the resulting\napproximation can be computed efficiently by reduction to minimum-weight\nperfect matching. We show that optimization of variational parameters achieves\nthe same lower-bound as dual-decomposition into the set of all cycles of the\noriginal graph. We demonstrate that our variational optimization converges\nquickly and provides high-quality solutions to hard combinatorial problems\n10-100x faster than competing algorithms that optimize the same bound.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 22:12:14 GMT"}], "update_date": "2011-04-08", "authors_parsed": [["Yarkony", "Julian", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1104.1330", "submitter": "Moti Medina", "authors": "Guy Even, Yakov Matsri, Moti Medina", "title": "Multi-Hop Routing and Scheduling in Wireless Networks in the SINR model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for multi-hop routing and scheduling of requests in\nwireless networks in the \\sinr\\ model. The goal of our algorithm is to maximize\nthe throughput or maximize the minimum ratio between the flow and the demand.\n  Our algorithm partitions the links into buckets. Every bucket consists of a\nset of links that have nearly equivalent reception powers. We denote the number\nof nonempty buckets by $\\sigdiv$. Our algorithm obtains an approximation ratio\nof $O(\\sigdiv \\cdot \\log n)$, where $n$ denotes the number of nodes. For the\ncase of linear powers $\\sigdiv =1$, hence the approximation ratio of the\nalgorithm is $O(\\log n)$. This is the first practical approximation algorithm\nfor linear powers with an approximation ratio that depends only on $n$ (and not\non the max-to-min distance ratio).\n  If the transmission power of each link is part of the input (and arbitrary),\nthen $\\sigdiv = O(\\log\\Gamma + \\log \\Delta)$, where $\\Gamma$ denotes the ratio\nof the max-to-min power, and $\\Delta$ denotes the ratio of the max-to-min\ndistance. Hence, the approximation ratio is $O(\\log n \\cdot (\\log\\Gamma + \\log\n\\Delta))$.\n  Finally, we consider the case that the algorithm needs to assign powers to\neach link in a range $[\\pmin,\\pmax]$. An extension of the algorithm to this\ncase achieves an approximation ratio of $O[(\\log n + \\log \\log \\Gamma) \\cdot\n(\\log\\Gamma + \\log \\Delta)]$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 13:44:46 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 16:48:26 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2011 19:23:20 GMT"}, {"version": "v4", "created": "Mon, 4 Jul 2011 08:04:06 GMT"}, {"version": "v5", "created": "Wed, 31 Aug 2011 20:15:45 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Even", "Guy", ""], ["Matsri", "Yakov", ""], ["Medina", "Moti", ""]]}, {"id": "1104.1355", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet and Andrew Simmons", "title": "Recursive Shortest Path Algorithm with Application to\n  Density-integration of Weighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.DS q-bio.NC stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Graph theory is increasingly commonly utilised in genetics, proteomics and\nneuroimaging. In such fields, the data of interest generally constitute\nweighted graphs. Analysis of such weighted graphs often require the integration\nof topological metrics with respect to the density of the graph. Here, density\nrefers to the proportion of the number of edges present in that graph. When\ntopological metrics based on shortest paths are of interest, such\ndensity-integration usually necessitates the iterative application of\nDijkstra's algorithm in order to compute the shortest path matrix at each\ndensity level. In this short note, we describe a recursive shortest path\nalgorithm based on single edge updating, which replaces the need for the\niterative use of Dijkstra's algorithm. Our proposed procedure is based on pairs\nof breadth-first searches around each of the vertices incident to the edge\nadded at each recursion. An algorithmic analysis of the proposed technique is\nprovided. When the graph of interest is coded as an adjacency list, our\nalgorithm can be shown to be more efficient than an iterative use of Dijkstra's\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 15:23:24 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Simmons", "Andrew", ""]]}, {"id": "1104.1377", "submitter": "Ning Xie", "authors": "Ronitt Rubinfeld and Gil Tamir and Shai Vardi and Ning Xie", "title": "Fast Local Computation Algorithms", "comments": "A preliminary version of this paper appeared in ICS 2011, pp. 223-238", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For input $x$, let $F(x)$ denote the set of outputs that are the \"legal\"\nanswers for a computational problem $F$. Suppose $x$ and members of $F(x)$ are\nso large that there is not time to read them in their entirety. We propose a\nmodel of {\\em local computation algorithms} which for a given input $x$,\nsupport queries by a user to values of specified locations $y_i$ in a legal\noutput $y \\in F(x)$. When more than one legal output $y$ exists for a given\n$x$, the local computation algorithm should output in a way that is consistent\nwith at least one such $y$. Local computation algorithms are intended to\ndistill the common features of several concepts that have appeared in various\nalgorithmic subfields, including local distributed computation, local\nalgorithms, locally decodable codes, and local reconstruction.\n  We develop a technique, based on known constructions of small sample spaces\nof $k$-wise independent random variables and Beck's analysis in his algorithmic\napproach to the Lov{\\'{a}}sz Local Lemma, which under certain conditions can be\napplied to construct local computation algorithms that run in {\\em\npolylogarithmic} time and space. We apply this technique to maximal independent\nset computations, scheduling radio network broadcasts, hypergraph coloring and\nsatisfying $k$-SAT formulas.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 16:55:23 GMT"}], "update_date": "2011-04-08", "authors_parsed": [["Rubinfeld", "Ronitt", ""], ["Tamir", "Gil", ""], ["Vardi", "Shai", ""], ["Xie", "Ning", ""]]}, {"id": "1104.1479", "submitter": "Amir Aavani", "authors": "Amir Aavani", "title": "A Family of Encodings for Translating Pseudo-Boolean Constraints into\n  SAT", "comments": "Used as the reference for SAT-2013 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Pseudo-Boolean (PB) constraint is a linear arithmetic constraint over\nBoolean variables. PB constraints are convenient and widely used in expressing\nNP-complete problems.\n  We introduce a new, two step, method for transforming PB constraints to\npropositional CNF formulas. The first step involves re-writing each PB\nconstraint as a conjunction of PB-Mod constraints. The advantage is that PB-Mod\nconstraints are easier to transform to CNF. In the second step, we translate\neach PB-Mod constraints, obtained in the previous step, into CNF. The resulting\nCNF formulas are small, and unit propagation can derive facts that it cannot\nderive using in the CNF formulas obtained by other commonly-used\ntransformations.\n  We also characterize the constraints for which one can expect the SAT solvers\nto perform well on the produced CNF. We show that there are many constraints\nfor which the proposed encoding has a good performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 04:40:28 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2012 01:45:44 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2013 22:27:07 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Aavani", "Amir", ""]]}, {"id": "1104.1601", "submitter": "Gregory Kucherov", "authors": "Gregory Kucherov", "title": "On-line construction of position heaps", "comments": "to appear in Journal of Discrete Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple linear-time on-line algorithm for constructing a position\nheap for a string [Ehrenfeucht et al, 2011]. Our definition of position heap\ndiffers slightly from the one proposed in [Ehrenfeucht et al, 2011] in that it\nconsiders the suffixes ordered from left to right. Our construction is based on\nclassic suffix pointers and resembles the Ukkonen's algorithm for suffix trees\n[Ukkonen, 1995]. Using suffix pointers, the position heap can be extended into\nthe augmented position heap that allows for a linear-time string matching\nalgorithm [Ehrenfeucht et al, 2011].\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 15:46:44 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2011 08:17:27 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2011 17:57:09 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2011 16:35:53 GMT"}, {"version": "v5", "created": "Thu, 4 Oct 2012 10:25:19 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kucherov", "Gregory", ""]]}, {"id": "1104.1690", "submitter": "Milan Tasic", "authors": "Petkovi\\'c, M.D. and Stanimirovi\\'c, P.S. and Tasi\\'c, M.B", "title": "Effective partitioning method for computing weighted Moore-Penrose\n  inverse", "comments": null, "journal-ref": "Computers & Mathematics with Applications, Volume 55, Issue 8,\n  April 2008, Pages 1720-1734", "doi": "10.1016/j.camwa.2007.07.014", "report-no": null, "categories": "cs.SC cs.DS math.FA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce a method and an algorithm for computing the weighted\nMoore-Penrose inverse of multiple-variable polynomial matrix and the related\nalgorithm which is appropriated for sparse polynomial matrices. These methods\nand algorithms are generalizations of algorithms developed in [M.B. Tasic, P.S.\nStanimirovic, M.D. Petkovic, Symbolic computation of weighted Moore-Penrose\ninverse using partitioning method, Appl. Math. Comput. 189 (2007) 615-640] to\nmultiple-variable rational and polynomial matrices and improvements of these\nalgorithms on sparse matrices. Also, these methods are generalizations of the\npartitioning method for computing the Moore-Penrose inverse of rational and\npolynomial matrices introduced in [P.S. Stanimirovic, M.B. Tasic, Partitioning\nmethod for rational and polynomial matrices, Appl. Math. Comput. 155 (2004)\n137-163; M.D. Petkovic, P.S. Stanimirovic, Symbolic computation of the\nMoore-Penrose inverse using partitioning method, Internat. J. Comput. Math. 82\n(2005) 355-367] to the case of weighted Moore-Penrose inverse. Algorithms are\nimplemented in the symbolic computational package MATHEMATICA.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 11:31:06 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Petkovi\u0107", "", ""], ["D.", "M.", ""], ["Stanimirovi\u0107", "", ""], ["S.", "P.", ""], ["Tasi\u0107", "", ""], ["B", "M.", ""]]}, {"id": "1104.1696", "submitter": "Milan Tasic", "authors": "Tasi\\'c, M.B. and Stanimirovi\\'c, P.S. and Petkovi\\'c, M.D", "title": "Symbolic computation of weighted Moore-Penrose inverse using\n  partitioning method", "comments": null, "journal-ref": "Applied Mathematics and Computation, Volume 189, Issue 1, 1 June\n  2007, Pages 615-640", "doi": "10.1016/j.amc.2006.11.114", "report-no": null, "categories": "cs.SC cs.DS cs.MS math.FA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a method and algorithm for computing the weighted Moore-Penrose\ninverse of one-variable rational matrices. Continuing this idea, we develop an\nalgorithm for computing the weighted Moore-Penrose inverse of one-variable\npolynomial matrix. These methods and algorithms are generalizations of the\nmethod for computing the weighted Moore-Penrose inverse for constant matrices,\noriginated in Wang and Chen [G.R. Wang, Y.L. Chen, A recursive algorithm for\ncomputing the weighted Moore-Penrose inverse AMN, J. Comput. Math. 4 (1986)\n74-85], and the partitioning method for computing the Moore-Penrose inverse of\nrational and polynomial matrices introduced in Stanimirovic and Tasic [P.S.\nStanimirovic, M.B. Tasic, Partitioning method for rational and polynomial\nmatrices, Appl. Math. Comput. 155 (2004) 137-163]. Algorithms are implemented\nin the symbolic computational package MATHEMATICA.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 11:54:31 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Tasi\u0107", "", ""], ["B.", "M.", ""], ["Stanimirovi\u0107", "", ""], ["S.", "P.", ""], ["Petkovi\u0107", "", ""], ["D", "M.", ""]]}, {"id": "1104.1697", "submitter": "Milan Tasic", "authors": "Stanimirovi\\'c, P. S. and Tasi\\'c, M. B", "title": "Computing generalized inverses using LU factorization of matrix product", "comments": null, "journal-ref": "International Journal Of Computer Mathematics, Volume 85, Issue\n  12, 2008, Pages 1865 - 1878", "doi": "10.1080/00207160701582077", "report-no": null, "categories": "cs.SC cs.DS math.FA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  An algorithm for computing {2, 3}, {2, 4}, {1, 2, 3}, {1, 2, 4} -inverses and\nthe Moore-Penrose inverse of a given rational matrix A is established. Classes\nA(2, 3)s and A(2, 4)s are characterized in terms of matrix products (R*A)+R*\nand T*(AT*)+, where R and T are rational matrices with appropriate dimensions\nand corresponding rank. The proposed algorithm is based on these general\nrepresentations and the Cholesky factorization of symmetric positive matrices.\nThe algorithm is implemented in programming languages MATHEMATICA and DELPHI,\nand illustrated via examples. Numerical results of the algorithm, corresponding\nto the Moore-Penrose inverse, are compared with corresponding results obtained\nby several known methods for computing the Moore-Penrose inverse.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 12:03:07 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Stanimirovi\u0107", "", ""], ["S.", "P.", ""], ["Tasi\u0107", "", ""], ["B", "M.", ""]]}, {"id": "1104.1698", "submitter": "Milan Tasic", "authors": "Milan B. Tasi\\'ic, Predrag S. Stanimirovi\\'c, Selver H. Pep\\'i", "title": "About the generalized LM-inverse and the weighted Moore-Penrose inverse", "comments": null, "journal-ref": "Applied Mathematics and Computation, Volume 216, Issue 1, 1 March\n  2010, Pages 114-124", "doi": "10.1016/j.amc.2010.01.019", "report-no": null, "categories": "cs.SC cs.DS cs.MS cs.NA math.FA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The recursive method for computing the generalized LM-inverse of a constant\nrectangular matrix augmented by a column vector is proposed in Udwadia and\nPhohomsiri (2007) [16] and [17]. The corresponding algorithm for the sequential\ndetermination of the generalized LM-inverse is established in the present\npaper. We prove that the introduced algorithm for computing the generalized\nLM-inverse and the algorithm for the computation of the weighted Moore-Penrose\ninverse developed by Wang and Chen (1986) in [23] are equivalent algorithms.\nBoth of the algorithms are implemented in the present paper using the package\nMATHEMATICA. Several rational test matrices and randomly generated constant\nmatrices are tested and the CPU time is compared and discussed.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 12:09:08 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Tasi\u00edc", "Milan B.", ""], ["Stanimirovi\u0107", "Predrag S.", ""], ["Pep\u00ed", "Selver H.", ""]]}, {"id": "1104.1732", "submitter": "Ali Sinop", "authors": "Venkatesan Guruswami and Ali Kemal Sinop", "title": "Optimal Column-Based Low-Rank Matrix Reconstruction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that for any real-valued matrix $X \\in \\R^{m \\times n}$, and\npositive integers $r \\ge k$, there is a subset of $r$ columns of $X$ such that\nprojecting $X$ onto their span gives a $\\sqrt{\\frac{r+1}{r-k+1}}$-approximation\nto best rank-$k$ approximation of $X$ in Frobenius norm. We show that the\ntrade-off we achieve between the number of columns and the approximation ratio\nis optimal up to lower order terms. Furthermore, there is a deterministic\nalgorithm to find such a subset of columns that runs in $O(r n m^{\\omega} \\log\nm)$ arithmetic operations where $\\omega$ is the exponent of matrix\nmultiplication. We also give a faster randomized algorithm that runs in $O(r n\nm^2)$ arithmetic operations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 18:40:25 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 22:16:28 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2011 05:10:03 GMT"}, {"version": "v4", "created": "Wed, 4 Jan 2012 23:05:00 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Sinop", "Ali Kemal", ""]]}, {"id": "1104.1822", "submitter": "Eduardo Hwang", "authors": "Eduardo Hwang", "title": "Dimensionality Decrease Heuristics for NP Complete Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of scientific community believes that P!=NP, with countless\nsupporting arguments. The number of people who believe otherwise probably\namounts to as few as those opposing the 2nd Law of Thermodynamics. But isn't\nnature elegant enough, not to resource to brute-force search? In this article,\na novel concept of dimensionality is presented, which may lead to a more\nefficient class of heuristic implementations to solve NP complete problems.\nThus, broadening the universe of man-machine tractable problems.\nDimensionality, as defined here, will be a closer analog of strain energy in\nnature.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 01:04:05 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Hwang", "Eduardo", ""]]}, {"id": "1104.1852", "submitter": "Yujie Wan", "authors": "Tony T. Lee and Yujie Wan and Hao Guan", "title": "Randomized $\\Delta$-Edge-Coloring via Quaternion of Complex Colors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper explores the application of a new algebraic method of color\nexchanges to the edge coloring of simple graphs. Vizing's theorem states that\nthe edge coloring of a simple graph $G$ requires either $\\Delta$ or $\\Delta+1$\ncolors, where $\\Delta$ is the maximum vertex degree of $G$. Holyer proved that\nit is {\\bf NP}-complete to decide whether $G$ is $\\Delta$-edge-colorable even\nfor cubic graphs. By introducing the concept of complex colors, we show that\nthe color-exchange operation follows the same multiplication rules as\nquaternion. An initially $\\Delta$-edge-colored graph $G$ allows\nvariable-colored edges, which can be eliminated by color exchanges in a manner\nsimilar to variable eliminations in solving systems of linear equations. The\nproblem is solved if all variables are eliminated and a properly\n$\\Delta$-edge-colored graph is reached. For a randomly generated graph $G$, we\nprove that our algorithm returns a proper $\\Delta$-edge-coloring with a\nprobability of at least 1/2 in $O(\\Delta|V||E|^5)$ time if $G$ is\n$\\Delta$-edge-colorable. Otherwise, the algorithm halts in polynomial time and\nsignals the impossibility of a solution, meaning that the chromatic index of\n$G$ probably equals $\\Delta+1$. Animations of the edge-coloring algorithms\nproposed in this paper are posted at YouTube\nhttp://www.youtube.com/watch?v=KMnj4UMYl7k.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 06:25:40 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Lee", "Tony T.", ""], ["Wan", "Yujie", ""], ["Guan", "Hao", ""]]}, {"id": "1104.2076", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail", "title": "A Note On Estimating the Spectral Norm of A Matrix Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm which can obtain a relative error\napproximation to the spectral norm of a matrix, combining the power iteration\nmethod with some techniques from matrix reconstruction which use random\nsampling.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 22:06:19 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Magdon-Ismail", "Malik", ""]]}, {"id": "1104.2230", "submitter": "Yngve Villanger", "authors": "Fedor V. Fomin and Yngve Villanger", "title": "Subexponential Parameterized Algorithm for Minimum Fill-in", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Fill-in problem is to decide if a graph can be triangulated by\nadding at most k edges. Kaplan, Shamir, and Tarjan [FOCS 1994] have shown that\nthe problem is solvable in time O(2^(O(k)) + k2 * nm) on graphs with n vertices\nand m edges and thus is fixed parameter tractable. Here, we give the first\nsubexponential parameterized algorithm solving Minimum Fill-in in time\nO(2^(O(\\sqrt{k} log k)) + k2 * nm). This substantially lower the complexity of\nthe problem. Techniques developed for Minimum Fill-in can be used to obtain\nsubexponential parameterized algorithms for several related problems including\nMinimum Chain Completion, Chordal Graph Sandwich, and Triangulating Colored\nGraph.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 14:39:13 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Villanger", "Yngve", ""]]}, {"id": "1104.2275", "submitter": "Frank Kammer", "authors": "Frank Kammer and Torsten Tholey", "title": "Approximate Tree Decompositions of Planar Graphs in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms have been developed for NP-hard problems on graphs with small\ntreewidth $k$. For example, all problems that are expressable in linear\nextended monadic second order can be solved in linear time on graphs of bounded\ntreewidth. It turns out that the bottleneck of many algorithms for NP-hard\nproblems is the computation of a tree decomposition of width $O(k)$. In\nparticular, by the bidimensional theory, there are many linear extended monadic\nsecond order problems that can be solved on $n$-vertex planar graphs with\ntreewidth $k$ in a time linear in $n$ and subexponential in $k$ if a tree\ndecomposition of width $O(k)$ can be found in such a time.\n  We present the first algorithm that, on $n$-vertex planar graphs with\ntreewidth $k$, finds a tree decomposition of width $O(k)$ in such a time. In\nmore detail, our algorithm has a running time of $O(n k^2 \\log k)$. We show the\nresult as a special case of a result concerning so-called weighted treewidth of\nweighted graphs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 17:12:46 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 16:32:47 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 12:27:34 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2015 15:29:58 GMT"}, {"version": "v5", "created": "Sat, 14 May 2016 20:49:11 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Kammer", "Frank", ""], ["Tholey", "Torsten", ""]]}, {"id": "1104.2315", "submitter": "Kook Jin Ahn", "authors": "Kook Jin Ahn and Sudipto Guha", "title": "Linear Programming in the Semi-streaming Model with Application to the\n  Maximum Matching Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study linear programming based approaches to the maximum\nmatching problem in the semi-streaming model. The semi-streaming model has\ngained attention as a model for processing massive graphs as the importance of\nsuch graphs has increased. This is a model where edges are streamed-in in an\nadversarial order and we are allowed a space proportional to the number of\nvertices in a graph.\n  In recent years, there has been several new results in this semi-streaming\nmodel. However broad techniques such as linear programming have not been\nadapted to this model. We present several techniques to adapt and optimize\nlinear programming based approaches in the semi-streaming model with an\napplication to the maximum matching problem. As a consequence, we improve\n(almost) all previous results on this problem, and also prove new results on\ninteresting variants.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 19:51:47 GMT"}, {"version": "v2", "created": "Tue, 3 May 2011 19:20:33 GMT"}, {"version": "v3", "created": "Tue, 13 Dec 2011 21:03:24 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ahn", "Kook Jin", ""], ["Guha", "Sudipto", ""]]}, {"id": "1104.2486", "submitter": "Dimitrios Thilikos", "authors": "Juanjo Ru\\'e, Ignasi Sau, and Dimitrios M. Thilikos", "title": "Dynamic Programming for Graphs on Surfaces", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a framework for the design and analysis of dynamic programming\nalgorithms for surface-embedded graphs on n vertices and branchwidth at most k.\nOur technique applies to general families of problems where standard dynamic\nprogramming runs in 2^{O(k log k)} n steps. Our approach combines tools from\ntopological graph theory and analytic combinatorics. In particular, we\nintroduce a new type of branch decomposition called \"surface cut\ndecomposition\", generalizing sphere cut decompositions of planar graphs\nintroduced by Seymour and Thomas, which has nice combinatorial properties.\nNamely, the number of partial solutions that can be arranged on a surface cut\ndecomposition can be upper-bounded by the number of non-crossing partitions on\nsurfaces with boundary. It follows that partial solutions can be represented by\na single-exponential (in the branchwidth k) number of configurations. This\nproves that, when applied on surface cut decompositions, dynamic programming\nruns in 2^{O(k)} n steps. That way, we considerably extend the class of\nproblems that can be solved in running times with a single-exponential\ndependence on branchwidth and unify/improve most previous results in this\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 13:32:08 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2011 09:41:52 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2011 20:16:42 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2011 16:03:53 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ru\u00e9", "Juanjo", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1104.2527", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, David Karger", "title": "Faster Information Dissemination in Dynamic Networks via Network Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use network coding to improve the speed of distributed computation in the\ndynamic network model of Kuhn, Lynch and Oshman [STOC '10]. In this model an\nadversary adaptively chooses a new network topology in every round, making even\nbasic distributed computations challenging.\n  Kuhn et al. show that n nodes, each starting with a d-bit token, can\nbroadcast them to all nodes in time O(n^2) using b-bit messages, where b > d +\nlog n. Their algorithms take the natural approach of {token forwarding}: in\nevery round each node broadcasts some particular token it knows. They prove\nmatching Omega(n^2) lower bounds for a natural class of token forwarding\nalgorithms and an Omega(n log n) lower bound that applies to all\ntoken-forwarding algorithms.\n  We use network coding, transmitting random linear combinations of tokens, to\nbreak both lower bounds. Our algorithm's performance is quadratic in the\nmessage size b, broadcasting the n tokens in roughly d/b^2 * n^2 rounds. For b\n= d = O(log n) our algorithms use O(n^2/log n) rounds, breaking the first lower\nbound, while for larger message sizes we obtain linear-time algorithms. We also\nconsider networks that change only every T rounds, and achieve an additional\nfactor T^2 speedup. This contrasts with related lower and upper bounds of Kuhn\net al. implying that for natural token-forwarding algorithms a speedup of T,\nbut not more, can be obtained. Lastly, we give a general way to derandomize\nrandom linear network coding, that also leads to new deterministic information\ndissemination algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 15:16:16 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Karger", "David", ""]]}, {"id": "1104.2541", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Kernels for Global Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bessiere et al. (AAAI'08) showed that several intractable global constraints\ncan be efficiently propagated when certain natural problem parameters are\nsmall. In particular, the complete propagation of a global constraint is\nfixed-parameter tractable in k - the number of holes in domains - whenever\nbound consistency can be enforced in polynomial time; this applies to the\nglobal constraints AtMost-NValue and Extended Global Cardinality (EGC).\n  In this paper we extend this line of research and introduce the concept of\nreduction to a problem kernel, a key concept of parameterized complexity, to\nthe field of global constraints. In particular, we show that the consistency\nproblem for AtMost-NValue constraints admits a linear time reduction to an\nequivalent instance on O(k^2) variables and domain values. This small kernel\ncan be used to speed up the complete propagation of NValue constraints. We\ncontrast this result by showing that the consistency problem for EGC\nconstraints does not admit a reduction to a polynomial problem kernel unless\nthe polynomial hierarchy collapses.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 16:27:20 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1104.2562", "submitter": "Mareike Fischer", "authors": "Mareike Fischer", "title": "Mathematical aspects of phylogenetic groves", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference of new information on the relatedness of species by\nphylogenetic trees based on DNA data is one of the main challenges of modern\nbiology. But despite all technological advances, DNA sequencing is still a\ntime-consuming and costly process. Therefore, decision criteria would be\ndesirable to decide a priori which data might contribute new information to the\nsupertree which is not explicitly displayed by any input tree. A new concept,\nso-called groves, to identify taxon sets with the potential to construct such\ninformative supertrees was suggested by An\\'e et al. in 2009. But the important\nconjecture that maximal groves can easily be identified in a database remained\nunproved and was published on the Isaac Newton Institute's list of open\nphylogenetic problems. In this paper, we show that the conjecture does not\ngenerally hold, but also introduce a new concept, namely 2-overlap groves,\nwhich overcomes this problem.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 17:58:47 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fischer", "Mareike", ""]]}, {"id": "1104.2732", "submitter": "Gleb Beliakov", "authors": "Gleb Beliakov", "title": "Parallel calculation of the median and order statistics on GPUs with\n  application to robust regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and compare various approaches to a classical selection problem on\nGraphics Processing Units (GPUs). The selection problem consists in selecting\nthe $k$-th smallest element from an array of size $n$, called $k$-th order\nstatistic. We focus on calculating the median of a sample, the $n/2$-th order\nstatistic. We introduce a new method based on minimization of a convex\nfunction, and show its numerical superiority when calculating the order\nstatistics of very large arrays on GPUs. We outline an application of this\napproach to efficient estimation of model parameters in high breakdown robust\nregression.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 11:44:39 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Beliakov", "Gleb", ""]]}, {"id": "1104.2762", "submitter": "Tamas Szantai", "authors": "Tamas Szantai, Edith Kovacs", "title": "Discovering a junction tree behind a Markov network by a greedy\n  algorithm", "comments": "The paper was presented at VOCAL 2010 in Veszprem, Hungary", "journal-ref": "Optimization and Engineering, Vol. 14, Issue 4, 2013", "doi": "10.1007/s11081-013-9232-8", "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an earlier paper we introduced a special kind of k-width junction tree,\ncalled k-th order t-cherry junction tree in order to approximate a joint\nprobability distribution. The approximation is the best if the Kullback-Leibler\ndivergence between the true joint probability distribution and the\napproximating one is minimal. Finding the best approximating k-width junction\ntree is NP-complete if k>2. In our earlier paper we also proved that the best\napproximating k-width junction tree can be embedded into a k-th order t-cherry\njunction tree. We introduce a greedy algorithm resulting very good\napproximations in reasonable computing time.\n  In this paper we prove that if the Markov network underlying fullfills some\nrequirements then our greedy algorithm is able to find the true probability\ndistribution or its best approximation in the family of the k-th order t-cherry\ntree probability distributions. Our algorithm uses just the k-th order marginal\nprobability distributions as input.\n  We compare the results of the greedy algorithm proposed in this paper with\nthe greedy algorithm proposed by Malvestuto in 1991.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 13:51:21 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 05:03:53 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2011 10:34:30 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Szantai", "Tamas", ""], ["Kovacs", "Edith", ""]]}, {"id": "1104.2799", "submitter": "John Iacono", "authors": "John Iacono and Mihai P\\v{a}tra\\c{s}cu", "title": "Using Hashing to Solve the Dictionary Problem (In External Memory)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dictionary problem in external memory and improve the update\ntime of the well-known buffer tree by roughly a logarithmic factor. For any\n\\lambda >= max {lg lg n, log_{M/B} (n/B)}, we can support updates in time\nO(\\lambda / B) and queries in sublogarithmic time, O(log_\\lambda n). We also\npresent a lower bound in the cell-probe model showing that our data structure\nis optimal.\n  In the RAM, hash tables have been used to solve the dictionary problem faster\nthan binary search for more than half a century. By contrast, our data\nstructure is the first to beat the comparison barrier in external memory. Ours\nis also the first data structure to depart convincingly from the indivisibility\nparadigm.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 15:16:32 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Iacono", "John", ""], ["P\u01cetra\u015fcu", "Mihai", ""]]}, {"id": "1104.2809", "submitter": "Matthew Patitz", "authors": "Bin Fu and Matthew J. Patitz and Robert T. Schweller and Bobby Sheline", "title": "Self-Assembly with Geometric Tiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a generalization of Winfree's abstract Tile Assembly\nModel (aTAM) in which tile types are assigned rigid shapes, or geometries,\nalong each tile face. We examine the number of distinct tile types needed to\nassemble shapes within this model, the temperature required for efficient\nassembly, and the problem of designing compact geometric faces to meet given\ncompatibility specifications. Our results show a dramatic decrease in the\nnumber of tile types needed to assemble $n \\times n$ squares to\n$\\Theta(\\sqrt{\\log n})$ at temperature 1 for the most simple model which meets\na lower bound from Kolmogorov complexity, and $O(\\log\\log n)$ in a model in\nwhich tile aggregates must move together through obstacle free paths within the\nplane. This stands in contrast to the $\\Theta(\\log n / \\log\\log n)$ tile types\nat temperature 2 needed in the basic aTAM. We also provide a general method for\nsimulating a large and computationally universal class of temperature 2 aTAM\nsystems with geometric tiles at temperature 1. Finally, we consider the problem\nof computing a set of compact geometric faces for a tile system to implement a\ngiven set of compatibility specifications. We show a number of bounds on the\ncomplexity of geometry size needed for various classes of compatibility\nspecifications, many of which we directly apply to our tile assembly results to\nachieve non-trivial reductions in geometry size.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 15:46:11 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fu", "Bin", ""], ["Patitz", "Matthew J.", ""], ["Schweller", "Robert T.", ""], ["Sheline", "Bobby", ""]]}, {"id": "1104.2818", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Mark Jones, Dominik Scheder, Anders Yeo", "title": "A New Bound for 3-Satisfiable MaxSat and its Algorithmic Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let F be a CNF formula with n variables and m clauses. F is 3-satisfiable if\nfor any 3 clauses in F, there is a truth assignment which satisfies all of\nthem. Lieberherr and Specker (1982) and, later, Yannakakis (1994) proved that\nin each 3-satisfiable CNF formula at least 2/3 of its clauses can be satisfied\nby a truth assignment. We improve this result by showing that every\n3-satisfiable CNF formula F contains a subset of variables U, such that some\ntruth assignment $\\tau$ will satisfy at least $2m/3+ m_U/3+\\rho n'$ clauses,\nwhere m is the number of clauses of F, m_U is the number of clauses of F\ncontaining a variable from U, n' is the total number of variables in clauses\nnot containing a variable in U, and \\rho is a positive absolute constant. Both\nU and $\\tau$ can be found in polynomial time. We use our result to show that\nthe following parameterized problem is fixed-parameter tractable and, moreover,\nhas a kernel with a linear number of variables. In 3-S-MAXSAT-AE, we are given\na 3-satisfiable CNF formula F with m clauses and asked to determine whether\nthere is an assignment which satisfies at least 2m/3 + k clauses, where k is\nthe parameter.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 16:13:43 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2011 13:50:22 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2012 08:46:17 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Scheder", "Dominik", ""], ["Yeo", "Anders", ""]]}, {"id": "1104.2824", "submitter": "L.T. Handoko", "authors": "Z. Akbar, L.T. Handoko", "title": "Pattern discovery for semi-structured web pages using bar-tree\n  representation", "comments": "9 pages", "journal-ref": "Int. J. Comput. Theor. Eng. 3 (2011) 261-269", "doi": "10.7763/IJCTE.2011.V3.314", "report-no": "FISIKALIPI-10040", "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many websites with an underlying database containing structured data provide\nthe richest and most dense source of information relevant for topical data\nintegration. The real data integration requires sustainable and reliable\npattern discovery to enable accurate content retrieval and to recognize pattern\nchanges from time to time; yet, extracting the structured data from web\ndocuments is still lacking from its accuracy. This paper proposes the bar-tree\nrepresentation to describe the whole pattern of web pages in an efficient way\nbased on the reverse algorithm. While previous algorithms always trace the\npattern and extract the region of interest from \\textit{top root}, the reverse\nalgorithm recognizes the pattern from the region of interest to both top and\nbottom roots simultaneously. The attributes are then extracted and labeled\nreversely from the region of interest of targeted contents. Since using\nconventional representations for the algorithm should require more\ncomputational power, the bar-tree method is developed to represent the\ngenerated patterns using bar graphs characterized by the depths and widths from\nthe document roots. We show that this representation is suitable for extracting\nthe data from the semi-structured web sources, and for detecting the template\nchanges of targeted pages. The experimental results show perfect recognition\nrate for template changes in several web targets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 16:27:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Akbar", "Z.", ""], ["Handoko", "L. T.", ""]]}, {"id": "1104.2882", "submitter": "Virginia Vassilevska Williams", "authors": "Liam Roditty and Virginia Vassilevska Williams", "title": "Minimum Weight Cycles and Triangles: Equivalences and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental algorithmic problem of finding a cycle of minimum\nweight in a weighted graph. In particular, we show that the minimum weight\ncycle problem in an undirected n-node graph with edge weights in {1,...,M} or\nin a directed n-node graph with edge weights in {-M,..., M} and no negative\ncycles can be efficiently reduced to finding a minimum weight triangle in an\nTheta(n)-node undirected graph with weights in {1,...,O(M)}. Roughly speaking,\nour reductions imply the following surprising phenomenon: a minimum cycle with\nan arbitrary number of weighted edges can be \"encoded\" using only three edges\nwithin roughly the same weight interval! This resolves a longstanding open\nproblem posed by Itai and Rodeh [SIAM J. Computing 1978 and STOC'77].\n  A direct consequence of our efficient reductions are O (Mn^{omega})-time\nalgorithms using fast matrix multiplication (FMM) for finding a minimum weight\ncycle in both undirected graphs with integral weights from the interval [1,M]\nand directed graphs with integral weights from the interval [-M,M]. The latter\nseems to reveal a strong separation between the all pairs shortest paths (APSP)\nproblem and the minimum weight cycle problem in directed graphs as the fastest\nknown APSP algorithm has a running time of O(M^{0.681}n^{2.575}) by Zwick [J.\nACM 2002].\n  In contrast, when only combinatorial algorithms are allowed (that is, without\nFMM) the only known solution to minimum weight cycle is by computing APSP.\nInterestingly, any separation between the two problems in this case would be an\namazing breakthrough as by a recent paper by Vassilevska W. and Williams\n[FOCS'10], any O(n^{3-eps})-time algorithm (eps>0) for minimum weight cycle\nimmediately implies a O(n^{3-delta})-time algorithm (delta>0) for APSP.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 19:32:10 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Roditty", "Liam", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1104.3045", "submitter": "Sebastien Collette", "authors": "Sebastien Collette and John Iacono and Stefan Langerman", "title": "Confluent Persistence Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown how to enhance any data structure in the pointer model to make it\nconfluently persistent, with efficient query and update times and limited space\noverhead. Updates are performed in $O(\\log n)$ amortized time, and following a\npointer takes $O(\\log c \\log n)$ time where $c$ is the in-degree of a node in\nthe data structure. In particular, this proves that confluent persistence can\nbe achieved at a logarithmic cost in the bounded in-degree model used widely in\nprevious work. This is a $O(n/\\log n)$-factor improvement over the previous\nknown transform to make a data structure confluently persistent.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 13:13:05 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Collette", "Sebastien", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""]]}, {"id": "1104.3057", "submitter": "Micha{\\l} Pilipczuk", "authors": "Micha{\\l} Pilipczuk", "title": "Problems parameterized by treewidth tractable in single exponential\n  time: a logical approach", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of modal logic, dubbed EXISTENTIAL COUNTING MODAL\nLOGIC (ECML), which captures a vast majority of problems known to be tractable\nin single exponential time when parameterized by treewidth. It appears that all\nthese results can be subsumed by the theorem that model checking of ECML admits\nan algorithm with such complexity. We extend ECML by adding connectivity\nrequirements and, using the Cut&Count technique introduced by Cygan et al. [4],\nprove that problems expressible in the extension are also tractable in single\nexponential time when parameterized by treewidth; however, using randomization.\nThe need for navigationality of the introduced logic is justified by a negative\nresult that two expository problems involving non-acyclic conditions, C_l\nVERTEX DELETION and GIRTH>l VERTEX DELETION for l>=5, do not admit such a\nrobust algorithm unless Exponential Time Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 13:47:38 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1104.3084", "submitter": "Rasmus Pagh", "authors": "Kasper Green Larsen and Rasmus Pagh", "title": "I/O-Efficient Data Structures for Colored Range and Prefix Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by information retrieval applications, we consider the\none-dimensional colored range reporting problem in rank space. The goal is to\nbuild a static data structure for sets C_1,...,C_m \\subseteq {1,...,sigma} that\nsupports queries of the kind: Given indices a,b, report the set Union_{a <= i\n<= b} C_i.\n  We study the problem in the I/O model, and show that there exists an optimal\nlinear-space data structure that answers queries in O(1+k/B) I/Os, where k\ndenotes the output size and B the disk block size in words. In fact, we obtain\nthe same bound for the harder problem of three-sided orthogonal range\nreporting. In this problem, we are to preprocess a set of n two-dimensional\npoints in rank space, such that all points inside a query rectangle of the form\n[x_1,x_2] x (-infinity,y] can be reported. The best previous bounds for this\nproblem is either O(n lg^2_B n) space and O(1+k/B) query I/Os, or O(n) space\nand O(lg^(h)_B n +k/B) query I/Os, where lg^(h)_B n is the base B logarithm\niterated h times, for any constant integer h. The previous bounds are both\nachieved under the indivisibility assumption, while our solution exploits the\nfull capabilities of the underlying machine. Breaking the indivisibility\nassumption thus provides us with cleaner and optimal bounds.\n  Our results also imply an optimal solution to the following colored prefix\nreporting problem. Given a set S of strings, each O(1) disk blocks in length,\nand a function c: S -> 2^{1,...,sigma}, support queries of the kind: Given a\nstring p, report the set Union_{x in S intersection p*} c(x), where p* denotes\nthe set of strings with prefix p. Finally, we consider the possibility of top-k\nextensions of this result, and present a simple solution in a model that allows\nnon-blocked I/O.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 15:15:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1104.3090", "submitter": "Ola Svensson", "authors": "Tobias M\\\"omke and Ola Svensson", "title": "Approximating Graphic TSP by Matchings", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for approximating the metric TSP based on a novel use\nof matchings. Traditionally, matchings have been used to add edges in order to\nmake a given graph Eulerian, whereas our approach also allows for the removal\nof certain edges leading to a decreased cost.\n  For the TSP on graphic metrics (graph-TSP), the approach yields a\n1.461-approximation algorithm with respect to the Held-Karp lower bound. For\ngraph-TSP restricted to a class of graphs that contains degree three bounded\nand claw-free graphs, we show that the integrality gap of the Held-Karp\nrelaxation matches the conjectured ratio 4/3. The framework allows for\ngeneralizations in a natural way and also leads to a 1.586-approximation\nalgorithm for the traveling salesman path problem on graphic metrics where the\nstart and end vertices are prespecified.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 15:40:56 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["M\u00f6mke", "Tobias", ""], ["Svensson", "Ola", ""]]}, {"id": "1104.3100", "submitter": "Stefan Kiefer", "authors": "Stefan Kiefer and Andrzej Murawski and Jo\\\"el Ouaknine and James\n  Worrell and Lijun Zhang", "title": "On Stabilization in Herman's Algorithm", "comments": "Technical report accompanying an ICALP'11 paper with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herman's algorithm is a synchronous randomized protocol for achieving\nself-stabilization in a token ring consisting of N processes. The interaction\nof tokens makes the dynamics of the protocol very difficult to analyze. In this\npaper we study the expected time to stabilization in terms of the initial\nconfiguration. It is straightforward that the algorithm achieves stabilization\nalmost surely from any initial configuration, and it is known that the\nworst-case expected time to stabilization (with respect to the initial\nconfiguration) is Theta(N^2). Our first contribution is to give an upper bound\nof 0.64 N^2 on the expected stabilization time, improving on previous upper\nbounds and reducing the gap with the best existing lower bound. We also\nintroduce an asynchronous version of the protocol, showing a similar O(N^2)\nconvergence bound in this case. Assuming that errors arise from the corruption\nof some number k of bits, where k is fixed independently of the size of the\nring, we show that the expected time to stabilization is O(N). This reveals a\nhitherto unknown and highly desirable property of Herman's algorithm: it\nrecovers quickly from bounded errors. We also show that if the initial\nconfiguration arises by resetting each bit independently and uniformly at\nrandom, then stabilization is significantly faster than in the worst case.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 16:08:59 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Kiefer", "Stefan", ""], ["Murawski", "Andrzej", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Worrell", "James", ""], ["Zhang", "Lijun", ""]]}, {"id": "1104.3119", "submitter": "Alfons Laarman", "authors": "Alfons Laarman, Jaco van de Pol, Michael Weber", "title": "Parallel Recursive State Compression for Free", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on reducing memory usage in enumerative model checking,\nwhile maintaining the multi-core scalability obtained in earlier work. We\npresent a tree-based multi-core compression method, which works by leveraging\nsharing among sub-vectors of state vectors.\n  An algorithmic analysis of both worst-case and optimal compression ratios\nshows the potential to compress even large states to a small constant on\naverage (8 bytes). Our experiments demonstrate that this holds up in practice:\nthe median compression ratio of 279 measured experiments is within 17% of the\noptimum for tree compression, and five times better than the median compression\nratio of SPIN's COLLAPSE compression.\n  Our algorithms are implemented in the LTSmin tool, and our experiments show\nthat for model checking, multi-core tree compression pays its own way: it comes\nvirtually without overhead compared to the fastest hash table-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 17:59:12 GMT"}, {"version": "v2", "created": "Sat, 14 May 2011 10:09:18 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Laarman", "Alfons", ""], ["van de Pol", "Jaco", ""], ["Weber", "Michael", ""]]}, {"id": "1104.3128", "submitter": "Chaitanya Swamy", "authors": "Sara Ahmadian, Chaitanya Swamy", "title": "Improved Approximation Guarantees for Lower-Bounded Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the {\\em lower-bounded facility location} (\\lbfl) problem (also\nsometimes called {\\em load-balanced facility location}), which is a\ngeneralization of {\\em uncapacitated facility location} (\\ufl), where each open\nfacility is required to serve a certain {\\em minimum} amount of demand. More\nformally, an instance $\\I$ of \\lbfl is specified by a set $\\F$ of facilities\nwith facility-opening costs $\\{f_i\\}$, a set $\\D$ of clients, and connection\ncosts $\\{c_{ij}\\}$ specifying the cost of assigning a client $j$ to a facility\n$i$, where the $c_{ij}$s form a metric. A feasible solution specifies a subset\n$F$ of facilities to open, and assigns each client $j$ to an open facility\n$i(j)\\in F$ so that each open facility serves {\\em at least $M$ clients}, where\n$M$ is an input parameter. The cost of such a solution is $\\sum_{i\\in\nF}f_i+\\sum_j c_{i(j)j}$, and the goal is to find a feasible solution of minimum\ncost. The current best approximation ratio for \\lbfl is 448 \\cite{Svitkina08}.\nWe substantially advance the state-of-the-art for \\lbfl by devising an\napproximation algorithm for \\lbfl that achieves a significantly-improved\napproximation guarantee of 82.6. Our improvement comes from a variety of ideas\nin algorithm design and analysis, which also yield new insights into \\lbfl.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 19:15:21 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 21:00:18 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Ahmadian", "Sara", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1104.3153", "submitter": "Jakub Radoszewski", "authors": "Michalis Christou, Maxime Crochemore, Costas S. Iliopoulos, Marcin\n  Kubica, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter, Bartosz Szreder,\n  Tomasz Walen", "title": "Efficient Seeds Computation Revisited", "comments": "14 pages, accepted to CPM 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of the cover is a generalization of a period of a string, and\nthere are linear time algorithms for finding the shortest cover. The seed is a\nmore complicated generalization of periodicity, it is a cover of a superstring\nof a given string, and the shortest seed problem is of much higher algorithmic\ndifficulty. The problem is not well understood, no linear time algorithm is\nknown. In the paper we give linear time algorithms for some of its versions ---\ncomputing shortest left-seed array, longest left-seed array and checking for\nseeds of a given length. The algorithm for the last problem is used to compute\nthe seed array of a string (i.e., the shortest seeds for all the prefixes of\nthe string) in $O(n^2)$ time. We describe also a simpler alternative algorithm\ncomputing efficiently the shortest seeds. As a by-product we obtain an\n$O(n\\log{(n/m)})$ time algorithm checking if the shortest seed has length at\nleast $m$ and finding the corresponding seed. We also correct some important\ndetails missing in the previously known shortest-seed algorithm (Iliopoulos et\nal., 1996).\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 20:29:30 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Christou", "Michalis", ""], ["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kubica", "Marcin", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Szreder", "Bartosz", ""], ["Walen", "Tomasz", ""]]}, {"id": "1104.3212", "submitter": "Hongrae Lee", "authors": "Hongrae Lee (University of British Columbia), Raymond T. Ng\n  (University of British Columbia), Kyuseok Shim (Seoul National University)", "title": "Similarity Join Size Estimation using Locality Sensitive Hashing", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 6, pp.\n  338-349 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity joins are important operations with a broad range of applications.\nIn this paper, we study the problem of vector similarity join size estimation\n(VSJ). It is a generalization of the previously studied set similarity join\nsize estimation (SSJ) problem and can handle more interesting cases such as\nTF-IDF vectors. One of the key challenges in similarity join size estimation is\nthat the join size can change dramatically depending on the input similarity\nthreshold.\n  We propose a sampling based algorithm that uses the\nLocality-Sensitive-Hashing (LSH) scheme. The proposed algorithm LSH-SS uses an\nLSH index to enable effective sampling even at high thresholds. We compare the\nproposed technique with random sampling and the state-of-the-art technique for\nSSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates at\nboth high and low similarity thresholds and small variance using real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2011 08:44:29 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Lee", "Hongrae", "", "University of British Columbia"], ["Ng", "Raymond T.", "", "University of British Columbia"], ["Shim", "Kyuseok", "", "Seoul National University"]]}, {"id": "1104.3283", "submitter": "Christophe Paul", "authors": "Emeric Gioan, Christophe Paul, Marc Tedder and Derek Corneil", "title": "Practical and Efficient Split Decomposition via Graph-Labelled Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split decomposition of graphs was introduced by Cunningham (under the name\njoin decomposition) as a generalization of the modular decomposition. This\npaper undertakes an investigation into the algorithmic properties of split\ndecomposition. We do so in the context of graph-labelled trees (GLTs), a new\ncombinatorial object designed to simplify its consideration. GLTs are used to\nderive an incremental characterization of split decomposition, with a simple\ncombinatorial description, and to explore its properties with respect to\nLexicographic Breadth-First Search (LBFS). Applying the incremental\ncharacterization to an LBFS ordering results in a split decomposition algorithm\nthat runs in time $O(n+m)\\alpha(n+m)$, where $\\alpha$ is the inverse Ackermann\nfunction, whose value is smaller than 4 for any practical graph. Compared to\nDahlhaus' linear-time split decomposition algorithm [Dahlhaus'00], which does\nnot rely on an incremental construction, our algorithm is just as fast in all\nbut the asymptotic sense and full implementation details are given in this\npaper. Also, our algorithm extends to circle graph recognition, whereas no such\nextension is known for Dahlhaus' algorithm. The companion paper [Gioan et al.]\nuses our algorithm to derive the first sub-quadratic circle graph recognition\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2011 07:09:41 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 09:11:03 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2012 06:43:08 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Gioan", "Emeric", ""], ["Paul", "Christophe", ""], ["Tedder", "Marc", ""], ["Corneil", "Derek", ""]]}, {"id": "1104.3284", "submitter": "Christophe Paul", "authors": "Emeric Gioan, Christophe Paul, Marc Tedder and Derek Corneil", "title": "Practical and Efficient Circle Graph Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circle graphs are the intersection graphs of chords in a circle. This paper\npresents the first sub-quadratic recognition algorithm for the class of circle\ngraphs. Our algorithm is O(n + m) times the inverse Ackermann function,\n{\\alpha}(n + m), whose value is smaller than 4 for any practical graph. The\nalgorithm is based on a new incremental Lexicographic Breadth-First Search\ncharacterization of circle graphs, and a new efficient data-structure for\ncircle graphs, both developed in the paper. The algorithm is an extension of a\nSplit Decomposition algorithm with the same running time developed by the\nauthors in a companion paper.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2011 07:22:30 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 09:09:03 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2012 06:50:16 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Gioan", "Emeric", ""], ["Paul", "Christophe", ""], ["Tedder", "Marc", ""], ["Corneil", "Derek", ""]]}, {"id": "1104.3463", "submitter": "Vijayakumar S", "authors": "M. A. Shalu and S. Vijayakumar", "title": "The Two Bicliques Problem is in NP intersection coNP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of deciding whether the vertex set of a graph can be\ncovered with at most two bicliques is in NP$\\cap$coNP. We thus almost determine\nthe computational complexity of a problem whose status has remained open for\nquite some time. Our result implies that a polynomial time algorithm for the\nproblem is more likely than it being NP-complete unless P = NP.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2011 12:43:24 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 13:36:36 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2011 11:55:32 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2011 17:48:40 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Shalu", "M. A.", ""], ["Vijayakumar", "S.", ""]]}, {"id": "1104.3677", "submitter": "Pim van 't Hof", "authors": "Pinar Heggernes, Pim van 't Hof, Benjamin L\\'ev\\^eque, Daniel\n  Lokshtanov, Christophe Paul", "title": "Contracting Graphs to Paths and Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex deletion and edge deletion problems play a central role in\nParameterized Complexity. Examples include classical problems like Feedback\nVertex Set, Odd Cycle Transversal, and Chordal Deletion. Interestingly, the\nstudy of edge contraction problems of this type from a parameterized\nperspective has so far been left largely unexplored. We consider two basic edge\ncontraction problems, which we call Path-Contractibility and\nTree-Contractibility. Both problems take an undirected graph $G$ and an integer\n$k$ as input, and the task is to determine whether we can obtain a path or an\nacyclic graph, respectively, by contracting at most $k$ edges of $G$. Our main\ncontribution is an algorithm with running time $4^{k+O(\\log^2 k)} + n^{O(1)}$\nfor Path-Contractibility and an algorithm with running time $4.88^k n^{O(1)}$\nfor Tree-Contractibility, based on a novel application of the color coding\ntechnique of Alon, Yuster and Zwick. Furthermore, we show that\nPath-Contractibility has a kernel with at most $5k+3$ vertices, while\nTree-Contractibility does not have a polynomial kernel unless coNP $\\subseteq$\nNP/poly. We find the latter result surprising, because of the strong connection\nbetween Tree-Contractibility and Feedback Vertex Set, which is known to have a\nvertex kernel with size $O(k^2)$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 08:49:35 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Heggernes", "Pinar", ""], ["Hof", "Pim van 't", ""], ["L\u00e9v\u00eaque", "Benjamin", ""], ["Lokshtanov", "Daniel", ""], ["Paul", "Christophe", ""]]}, {"id": "1104.3720", "submitter": "Stefanie Naewe", "authors": "Johannes Bl\\\"omer and Stefanie Naewe", "title": "Solving the Closest Vector Problem with respect to l_p Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deterministic algorithm for the closest vector\nproblem for all l_p-norms, 1 < p < \\infty, and all polyhedral norms, especially\nfor the l_1-norm and the l_{\\infty}-norm. We achieve our results by introducing\na new lattice problem, the lattice membership problem. We describe a\ndeterministic algorithm for the lattice membership problem, which is a\ngeneralization of Lenstra's algorithm for integer programming. We also describe\na polynomial time reduction from the closest vector problem to the lattice\nmembership problem. This approach leads to a deterministic algorithm that\nsolves the closest vector problem for all l_p-norms, 1 < p < \\infty, in time p\nlog_2 (r)^{O (1)} n^{(5/2+o(1))n} and for all polyhedral norms in time (s log_2\n(r))^{O (1)} n^{(2+o(1))n}, where s is the number of constraints defining the\npolytope and r is an upper bound on the coefficients used to describe the\nconvex body.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 12:03:46 GMT"}, {"version": "v2", "created": "Mon, 16 May 2011 12:57:28 GMT"}, {"version": "v3", "created": "Tue, 12 Jul 2011 17:25:13 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2011 11:35:15 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Naewe", "Stefanie", ""]]}, {"id": "1104.3806", "submitter": "Konstantin Makarychev", "authors": "Alexandra Kolla, Konstantin Makarychev, Yury Makarychev", "title": "How to Play Unique Games against a Semi-Random Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the average case complexity of the Unique Games\nproblem. We propose a natural semi-random model, in which a unique game\ninstance is generated in several steps. First an adversary selects a completely\nsatisfiable instance of Unique Games, then she chooses an epsilon-fraction of\nall edges, and finally replaces (\"corrupts\") the constraints corresponding to\nthese edges with new constraints. If all steps are adversarial, the adversary\ncan obtain any (1-epsilon) satisfiable instance, so then the problem is as hard\nas in the worst case. In our semi-random model, one of the steps is random, and\nall other steps are adversarial. We show that known algorithms for unique games\n(in particular, all algorithms that use the standard SDP relaxation) fail to\nsolve semi-random instances of Unique Games.\n  We present an algorithm that with high probability finds a solution\nsatisfying a (1-delta) fraction of all constraints in semi-random instances (we\nrequire that the average degree of the graph is Omega(log k). To this end, we\nconsider a new non-standard SDP program for Unique Games, which is not a\nrelaxation for the problem, and show how to analyze it. We present a new\nrounding scheme that simultaneously uses SDP and LP solutions, which we believe\nis of independent interest.\n  Our result holds only for epsilon less than some absolute constant. We prove\nthat if epsilon > 1/2, then the problem is hard in one of the models, the\nresult assumes the 2-to-2 conjecture.\n  Finally, we study semi-random instances of Unique Games that are at most\n(1-epsilon) satisfiable. We present an algorithm that with high probability,\ndistinguishes between the case when the instance is a semi-random instance and\nthe case when the instance is an (arbitrary) (1-delta) satisfiable instance if\nepsilon > c delta.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 17:16:59 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Kolla", "Alexandra", ""], ["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""]]}, {"id": "1104.3808", "submitter": "Siamak Tazari", "authors": "Stephan Kreutzer and Siamak Tazari", "title": "Directed Nowhere Dense Classes of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of shallow directed minors and based on this a new\nclassification of classes of directed graphs which is diametric to existing\ndirected graph decompositions and width measures proposed in the literature.\n  We then study in depth one type of classes of directed graphs which we call\nnowhere crownful. The classes are very general as they include, on one hand,\nall classes of directed graphs whose underlying undirected class is nowhere\ndense, such as planar, bounded-genus, and $H$-minor-free graphs; and on the\nother hand, also contain classes of high edge density whose underlying class is\nnot nowhere dense. Yet we are able to show that problems such as directed\ndominating set and many others become fixed-parameter tractable on nowhere\ncrownful classes of directed graphs. This is of particular interest as these\nproblems are not tractable on any existing digraph measure for sparse classes.\n  The algorithmic results are established via proving a structural equivalence\nof nowhere crownful classes and classes of graphs which are directed uniformly\nquasi-wide. This rather surprising result is inspired by Nesetril and Ossana de\nMendez (2008) and yet a different and more delicate proof is needed, which is a\nsignificant part of our contribution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 17:24:20 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Kreutzer", "Stephan", ""], ["Tazari", "Siamak", ""]]}, {"id": "1104.3810", "submitter": "Juha K\\\"arkk\\\"ainen", "authors": "Juha K\\\"arkk\\\"ainen and Simon J. Puglisi", "title": "Fixed Block Compression Boosting in FM-Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compressed full-text self-index occupies space close to that of the\ncompressed text and simultaneously allows fast pattern matching and random\naccess to the underlying text. Among the best compressed self-indexes, in\ntheory and in practice, are several members of the FM-index family. In this\npaper, we describe new FM-index variants that combine nice theoretical\nproperties, simple implementation and improved practical performance. Our main\nresult is a new technique called fixed block compression boosting, which is a\nsimpler and faster alternative to optimal compression boosting and implicit\ncompression boosting used in previous FM-indexes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 17:26:46 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Juha", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1104.3905", "submitter": "Alexander Langer", "authors": "Joachim Kneis, Alexander Langer, Peter Rossmanith", "title": "Courcelle's Theorem - A Game-Theoretic Approach", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Courcelle's Theorem states that every problem definable in Monadic\nSecond-Order logic can be solved in linear time on structures of bounded\ntreewidth, for example, by constructing a tree automaton that recognizes or\nrejects a tree decomposition of the structure. Existing, optimized software\nlike the MONA tool can be used to build the corresponding tree automata, which\nfor bounded treewidth are of constant size. Unfortunately, the constants\ninvolved can become extremely large - every quantifier alternation requires a\npower set construction for the automaton. Here, the required space can become a\nproblem in practical applications.\n  In this paper, we present a novel, direct approach based on model checking\ngames, which avoids the expensive power set construction. Experiments with an\nimplementation are promising, and we can solve problems on graphs where the\nautomata-theoretic approach fails in practice.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 23:33:15 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["Kneis", "Joachim", ""], ["Langer", "Alexander", ""], ["Rossmanith", "Peter", ""]]}, {"id": "1104.3917", "submitter": "Ton Kloks Dr", "authors": "Ling-Ju Hung, Ton Kloks, Fernando Villaamil", "title": "Black-and-white threshold graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let k be a natural number. We introduce k-threshold graphs. We show that\nthere exists an O(n^3) algorithm for the recognition of k-threshold graphs for\neach natural number k. k-Threshold graphs are characterized by a finite\ncollection of forbidden induced subgraphs. For the case k=2 we characterize the\npartitioned 2-threshold graphs by forbidden induced subgraphs. We introduce\nrestricted -, and special 2-threshold graphs. We characterize both classes by\nforbidden induced subgraphs. The restricted 2-threshold graphs coincide with\nthe switching class of threshold graphs. This provides a decomposition theorem\nfor the switching class of threshold graphs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 01:56:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hung", "Ling-Ju", ""], ["Kloks", "Ton", ""], ["Villaamil", "Fernando", ""]]}, {"id": "1104.3923", "submitter": "Bundit Laekhanukit", "authors": "Bundit Laekhanukit", "title": "An improved approximation algorithm for the minimum-cost subset\n  k-connected subgraph problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum-cost subset $k$-connected subgraph problem is a cornerstone\nproblem in the area of network design with vertex connectivity requirements. In\nthis problem, we are given a graph $G=(V,E)$ with costs on edges and a set of\nterminals $T$. The goal is to find a minimum cost subgraph such that every pair\nof terminals are connected by $k$ openly (vertex) disjoint paths. In this\npaper, we present an approximation algorithm for the subset $k$-connected\nsubgraph problem which improves on the previous best approximation guarantee of\n$O(k^2\\log{k})$ by Nutov (FOCS 2009). Our approximation guarantee,\n$\\alpha(|T|)$, depends upon the number of terminals: [\\alpha(|T|) \\ \\ =\\ \\\nO(|T|^2) & if |T| < 2k O(k \\log^2 k) & if 2k\\le |T| < k^2 O(k \\log k) & if |T|\n\\ge k^2]\n  So, when the number of terminals is {\\em large enough}, the approximation\nguarantee improves significantly. Moreover, we show that, given an\napproximation algorithm for $|T|=k$, we can obtain almost the same\napproximation guarantee for any instances with $|T|> k$. This suggests that the\nhardest instances of the problem are when $|T|\\approx k$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 02:14:05 GMT"}, {"version": "v2", "created": "Fri, 20 May 2011 14:04:05 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2011 20:26:47 GMT"}, {"version": "v4", "created": "Fri, 18 Jan 2013 02:01:17 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Laekhanukit", "Bundit", ""]]}, {"id": "1104.4024", "submitter": "Marco Pretti", "authors": "Alessandro Pelizzola, Marco Pretti, and Jort van Mourik", "title": "Palette-colouring: a belief-propagation approach", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": "10.1088/1742-5468/2011/05/P05010", "report-no": null, "categories": "cond-mat.stat-mech cs.AI cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variation of the prototype combinatorial-optimisation problem\nknown as graph-colouring. Our optimisation goal is to colour the vertices of a\ngraph with a fixed number of colours, in a way to maximise the number of\ndifferent colours present in the set of nearest neighbours of each given\nvertex. This problem, which we pictorially call \"palette-colouring\", has been\nrecently addressed as a basic example of problem arising in the context of\ndistributed data storage. Even though it has not been proved to be NP complete,\nrandom search algorithms find the problem hard to solve. Heuristics based on a\nnaive belief propagation algorithm are observed to work quite well in certain\nconditions. In this paper, we build upon the mentioned result, working out the\ncorrect belief propagation algorithm, which needs to take into account the\nmany-body nature of the constraints present in this problem. This method\nimproves the naive belief propagation approach, at the cost of increased\ncomputational effort. We also investigate the emergence of a satisfiable to\nunsatisfiable \"phase transition\" as a function of the vertex mean degree, for\ndifferent ensembles of sparse random graphs in the large size (\"thermodynamic\")\nlimit.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 13:48:32 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Pelizzola", "Alessandro", ""], ["Pretti", "Marco", ""], ["van Mourik", "Jort", ""]]}, {"id": "1104.4058", "submitter": "Kook Jin Ahn", "authors": "Kook Jin Ahn and Sudipto Guha", "title": "Laminar Families and Metric Embeddings: Non-bipartite Maximum Matching\n  Problem in the Semi-Streaming Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the non-bipartite maximum matching problem in the\nsemi-streaming model. The maximum matching problem in the semi-streaming model\nhas received a significant amount of attention lately. While the problem has\nbeen somewhat well solved for bipartite graphs, the known algorithms for\nnon-bipartite graphs use $2^{\\frac1\\epsilon}$ passes or $n^{\\frac1\\epsilon}$\ntime to compute a $(1-\\epsilon)$ approximation. In this paper we provide the\nfirst FPTAS (polynomial in $n,\\frac1\\epsilon$) for the problem which is\nefficient in both the running time and the number of passes. We also show that\nwe can estimate the size of the matching in $O(\\frac1\\epsilon)$ passes using\nslightly superlinear space.\n  To achieve both results, we use the structural properties of the matching\npolytope such as the laminarity of the tight sets and total dual integrality.\nThe algorithms are iterative, and are based on the fractional packing and\ncovering framework. However the formulations herein require exponentially many\nvariables or constraints. We use laminarity, metric embeddings and graph\nsparsification to reduce the space required by the algorithms in between and\nacross the iterations. This is the first use of these ideas in the\nsemi-streaming model to solve a combinatorial optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 15:39:28 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ahn", "Kook Jin", ""], ["Guha", "Sudipto", ""]]}, {"id": "1104.4081", "submitter": "Shayan Oveis Gharan", "authors": "Shayan Oveis Gharan, Jan Vondr\\'ak", "title": "On Variants of the Matroid Secretary Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a number of positive and negative results for variants of the\nmatroid secretary problem. Most notably, we design a constant-factor\ncompetitive algorithm for the \"random assignment\" model where the weights are\nassigned randomly to the elements of a matroid, and then the elements arrive\non-line in an adversarial order (extending a result of Soto \\cite{Soto11}).\nThis is under the assumption that the matroid is known in advance. If the\nmatroid is unknown in advance, we present an $O(\\log r \\log n)$-approximation,\nand prove that a better than $O(\\log n / \\log \\log n)$ approximation is\nimpossible. This resolves an open question posed by Babaioff et al.\n\\cite{BIK07}.\n  As a natural special case, we also consider the classical secretary problem\nwhere the number of candidates $n$ is unknown in advance. If $n$ is chosen by\nan adversary from $\\{1,...,N\\}$, we provide a nearly tight answer, by providing\nan algorithm that chooses the best candidate with probability at least\n$1/(H_{N-1}+1)$ and prove that a probability better than $1/H_N$ cannot be\nachieved (where $H_N$ is the $N$-th harmonic number).\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 17:38:01 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 20:16:51 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Vondr\u00e1k", "Jan", ""]]}, {"id": "1104.4203", "submitter": "Pawel Gawrychowski", "authors": "Pawel Gawrychowski", "title": "Pattern matching in Lempel-Ziv compressed strings: fast, simple, and\n  deterministic", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Countless variants of the Lempel-Ziv compression are widely used in many\nreal-life applications. This paper is concerned with a natural modification of\nthe classical pattern matching problem inspired by the popularity of such\ncompression methods: given an uncompressed pattern s[1..m] and a Lempel-Ziv\nrepresentation of a string t[1..N], does s occur in t? Farach and Thorup gave a\nrandomized O(nlog^2(N/n)+m) time solution for this problem, where n is the size\nof the compressed representation of t. We improve their result by developing a\nfaster and fully deterministic O(nlog(N/n)+m) time algorithm with the same\nspace complexity. Note that for highly compressible texts, log(N/n) might be of\norder n, so for such inputs the improvement is very significant. A (tiny)\nfragment of our method can be used to give an asymptotically optimal solution\nfor the substring hashing problem considered by Farach and Muthukrishnan.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 08:39:37 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Gawrychowski", "Pawel", ""]]}, {"id": "1104.4217", "submitter": "Bart M. P. Jansen", "authors": "Hans L. Bodlaender and Bart M. P. Jansen and Stefan Kratsch", "title": "Preprocessing for Treewidth: A Combinatorial Analysis through\n  Kernelization", "comments": "An extended abstract of this paper appeared in the proceedings of\n  ICALP 2011. This is the full version containing all proofs, along with some\n  improvements to the results of the extended abstract. This paper will appear\n  in the SIAM Journal on Discrete Mathematics. The SIAM version will contain\n  slight improvements to this arXiv version; for example, revised figures and\n  typesetting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of treewidth plays an important role in theoretical and practical\nstudies of graph problems. It has been recognized that, especially in practical\nenvironments, when computing the treewidth of a graph it is invaluable to first\napply an array of preprocessing rules that simplify and shrink it. This work\nseeks to prove rigorous performance guarantees for such preprocessing rules,\nboth known and new ones, by studying them in the framework of kernelization\nfrom parameterized complexity.\n  It is known that the NP-complete problem of determining whether a given graph\nG has treewidth at most k admits no polynomial-time preprocessing algorithm\nthat reduces any input instance to size polynomial in k, unless NP is in\ncoNP/poly and the polynomial hierarchy collapses to its third level. In this\npaper we therefore consider structural graph measures larger than treewidth,\nand determine whether efficient preprocessing can shrink the instance size to a\npolynomial in such a parameter value.\n  We prove that given an instance (G,k) of treewidth we can efficiently reduce\nits size to O(fvs(G)^4) vertices, where fvs(G) is the size of a minimum\nfeedback vertex set in G. We can also prove a size reduction to O(vc(G)^3)\nvertices, where vc(G) is the size of a minimum vertex cover. Phrased in the\nlanguage of parameterized complexity, we show that Treewidth has a polynomial\nkernel when parameterized by the size of a given feedback vertex set, and also\nby the size of a vertex cover. In contrast we show that Treewidth parameterized\nby the vertex-deletion distance to a single clique, and Weighted Treewidth\nparameterized by the size of a vertex cover, do not admit polynomial\nkernelizations unless NP is in coNP/poly.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 09:52:16 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2013 12:06:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Jansen", "Bart M. P.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1104.4229", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen and Stefan Kratsch", "title": "Data Reduction for Graph Coloring Problems", "comments": "Author-accepted manuscript of the article that will appear in the FCT\n  2011 special issue of Information & Computation", "journal-ref": null, "doi": "10.1016/j.ic.2013.08.005", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the kernelization complexity of graph coloring problems\nwith respect to certain structural parameterizations of the input instances. We\nare interested in how well polynomial-time data reduction can provably shrink\ninstances of coloring problems, in terms of the chosen parameter. It is well\nknown that deciding 3-colorability is already NP-complete, hence parameterizing\nby the requested number of colors is not fruitful. Instead, we pick up on a\nresearch thread initiated by Cai (DAM, 2003) who studied coloring problems\nparameterized by the modification distance of the input graph to a graph class\non which coloring is polynomial-time solvable; for example parameterizing by\nthe number k of vertex-deletions needed to make the graph chordal. We obtain\nvarious upper and lower bounds for kernels of such parameterizations of\nq-Coloring, complementing Cai's study of the time complexity with respect to\nthese parameters.\n  Our results show that the existence of polynomial kernels for q-Coloring\nparameterized by the vertex-deletion distance to a graph class F is strongly\nrelated to the existence of a function f(q) which bounds the number of vertices\nwhich are needed to preserve the NO-answer to an instance of q-List-Coloring on\nF.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 10:55:45 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 10:46:50 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1104.4279", "submitter": "Stefan Szeider", "authors": "Sebastian Ordyniak, Daniel Paulusma, Stefan Szeider", "title": "Satisfiability of Acyclic and Almost Acyclic CNF Formulas", "comments": "Extended abstracts appeared in the Proceedings of FSTTCS 2010 and SAT\n  2011. The latter corresponds to revision 1 of this arXiv paper\n  (arXiv:1104.4279v1)", "journal-ref": "Theoretical Computer Science, vol. 481, pp. 85-99, 2013", "doi": "10.1016/j.tcs.2012.12.039", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Satisfiability (SAT) problem for CNF formulas with\n{\\beta}-acyclic hypergraphs can be solved in polynomial time by using a special\ntype of Davis-Putnam resolution in which each resolvent is a subset of a parent\nclause. We extend this class to CNF formulas for which this type of\nDavis-Putnam resolution still applies and show that testing membership in this\nclass is NP-complete. We compare the class of {\\beta}-acyclic formulas and this\nsuperclass with a number of known polynomial formula classes. We then study the\nparameterized complexity of SAT for \"almost\" {\\beta}-acyclic instances, using\nas parameter the formula's distance from being {\\beta}-acyclic. As distance we\nuse the size of a smallest strong backdoor set and the {\\beta}-hypertree width.\nAs a by-product we obtain the W[1]-hardness of SAT parameterized by the\n(undirected) clique-width of the incidence graph, which disproves a conjecture\nby Fischer, Makowsky, and Ravve.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 15:01:02 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 12:14:49 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Paulusma", "Daniel", ""], ["Szeider", "Stefan", ""]]}, {"id": "1104.4353", "submitter": "Alexis Kaporis C.", "authors": "D. Belazzougui, A. C. Kaporis and P. G. Spirakis", "title": "Random input helps searching predecessors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the dynamic Predecessor Problem with high probability (whp) in\nconstant time, using only $n^{1+\\delta}$ bits of memory, for any constant\n$\\delta > 0$. The input keys are random wrt a wider class of the well studied\nand practically important class of $(f_1, f_2)$-smooth distributions introduced\nin \\cite{and:mat}. It achieves O(1) whp amortized time. Its worst-case time is\n$O(\\sqrt{\\frac{\\log n}{\\log \\log n}})$. Also, we prove whp $O(\\log \\log \\log\nn)$ time using only $n^{1+ \\frac{1}{\\log \\log n}}= n^{1+o(1)}$ bits. Finally,\nwe show whp $O(\\log \\log n)$ time using O(n) space.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 20:25:03 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Belazzougui", "D.", ""], ["Kaporis", "A. C.", ""], ["Spirakis", "P. G.", ""]]}, {"id": "1104.4370", "submitter": "Bang Ye Wu", "authors": "Bang Ye Wu", "title": "The maximum disjoint paths problem on multi-relations social networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.disopt.2012.01.002", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to social network analysis (SNA), we study the\nproblem of finding the maximum number of disjoint uni-color paths in an\nedge-colored graph. We show the NP-hardness and the approximability of the\nproblem, and both approximation and exact algorithms are proposed. Since short\npaths are much more significant in SNA, we also study the length-bounded\nversion of the problem, in which the lengths of paths are required to be upper\nbounded by a fixed integer $l$. It is shown that the problem can be solved in\npolynomial time for $l=3$ and is NP-hard for $l\\geq 4$. We also show that the\nproblem can be approximated with ratio $(l-1)/2+\\epsilon$ in polynomial time\nfor any $\\epsilon >0$. Particularly, for $l=4$, we develop an efficient\n2-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 01:23:06 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Wu", "Bang Ye", ""]]}, {"id": "1104.4471", "submitter": "Sebastian B\\\"ocker", "authors": "Sebastian B\\\"ocker", "title": "Towards a Data Reduction for the Minimum Flip Supertree Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational phylogenetics, the problem of constructing a supertree of a\ngiven set of rooted input trees can be formalized in different ways, to cope\nwith contradictory information in the input. We consider the Minimum Flip\nSupertree problem, where the input trees are transformed into a 0/1/?-matrix,\nsuch that each row represents a taxon, and each column represents an inner node\nof one of the input trees. Our goal is to find a perfect phylogeny for the\ninput matrix requiring a minimum number of 0/1-flips, that is, corrections of\n0/1-entries in the matrix. The problem is known to be NP-complete. Here, we\npresent a parameterized data reduction with polynomial running time. The data\nreduction guarantees that the reduced instance has a solution if and only if\nthe original instance has a solution. We then make our data reduction\nparameter-independent by using upper bounds. This allows us to preprocess an\ninstance, and to solve the reduced instance with an arbitrary method. Different\nfrom an existing data reduction for the consensus tree problem, our reduction\nallows us to draw conclusions about certain entries in the matrix. We have\nimplemented and evaluated our data reduction. Unfortunately, we find that the\nMinimum Flip Supertree problem is also hard in practice: The amount of\ninformation that can be derived during data reduction diminishes as instances\nget more \"complicated\", and running times for \"complicated\" instances quickly\nbecome prohibitive. Still, our method offers another route of attack for this\nrelevant phylogenetic problem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 15:48:15 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["B\u00f6cker", "Sebastian", ""]]}, {"id": "1104.4506", "submitter": "Pawel Rzazewski", "authors": "Konstanty Junosza-Szaniawski and Pawe{\\l} Rz\\k{a}\\zewski", "title": "Determining L(2,1)-Span in Polynomial Space", "comments": null, "journal-ref": null, "doi": "10.1016/j.dam.2013.03.027", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-L(2,1)-labeling of a graph is a function from its vertex set into the\nset $\\{0,...,k\\}$, such that the labels assigned to adjacent vertices differ by\nat least 2, and labels assigned to vertices of distance 2 are different. It is\nknown that finding the smallest $k$ admitting the existence of a\n$k$-L(2,1)-labeling of any given graph is NP-Complete.\n  In this paper we present an algorithm for this problem, which works in time\n$O(\\complexity ^n)$ and polynomial memory, where $\\eps$ is an arbitrarily small\npositive constant. This is the first exact algorithm for L(2,1)-labeling\nproblem with time complexity $O(c^n)$ for some constant $c$ and polynomial\nspace complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 20:32:21 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Junosza-Szaniawski", "Konstanty", ""], ["Rz\u0105\\zewski", "Pawe\u0142", ""]]}, {"id": "1104.4552", "submitter": "Sumit Ganguly", "authors": "Sumit Ganguly", "title": "Polynomial Estimators for High Frequency Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for computing $F_p$, the $p$th moment of an\n$n$-dimensional frequency vector of a data stream, for $2 < p < \\log (n) $, to\nwithin $1\\pm \\epsilon$ factors, $\\epsilon \\in [n^{-1/p},1]$ with high constant\nprobability. Let $m$ be the number of stream records and $M$ be the largest\nmagnitude of a stream update.\n  The algorithm uses space in bits $$ O(p^2\\epsilon^{-2}n^{1-2/p}E(p,n) \\log\n(n) \\log (nmM)/\\min(\\log (n),\\epsilon^{4/p-2}))$$ where, $E(p,n) =\n(1-2/p)^{-1}(1-n^{-4(1-2/p})$. Here $E(p,n)$ is $ O(1)$ for $p = 2+\\Omega(1)$\nand $ O(\\log n)$ for $p = 2 + O(1/\\log (n)$. This improves upon the space\nrequired by current algorithms\n\\cite{iw:stoc05,bgks:soda06,ako:arxiv10,bo:arxiv10} by a factor of at least\n$\\Omega(\\epsilon^{-4/p} \\min(\\log (n), \\epsilon^{4/p-2}))$. The update time is\n$O(\\log (n))$. We use a new technique for designing estimators for functions of\nthe form $\\psi(\\expect{X})$, where, $X$ is a random variable and $\\psi$ is a\nsmooth function, based on a low-degree Taylor polynomial expansion of\n$\\psi(\\expect{X})$ around an estimate of $\\expect{X}$.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2011 12:32:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ganguly", "Sumit", ""]]}, {"id": "1104.4597", "submitter": "Thomas Rothvoss", "authors": "Thomas Rothvoss", "title": "The Entropy Rounding Method in Approximation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let A be a matrix, c be any linear objective function and x be a fractional\nvector, say an LP solution to some discrete optimization problem. Then a\nrecurring task in theoretical computer science (and in approximation algorithms\nin particular) is to obtain an integral vector y such that Ax is roughly Ay and\nc*y exceeds c*x by only a moderate factor.\n  We give a new randomized rounding procedure for this task, provided that A\nhas bounded Delta-approximate entropy. This property means that for uniformly\nchosen random signs chi(j) in {-1,+1} on any subset of the columns, the outcome\nA*chi can be approximately described using a sub-linear number of bits in\nexpectation.\n  To achieve this result, we modify well-known techniques from the field of\ndiscrepancy theory, especially we rely on Beck's entropy method, which to the\nbest of our knowledge has never been used before in the context of\napproximation algorithms. Our result can be made constructive using the Bansal\nframework based on semidefinite programming.\n  We demonstrate the versatility of our procedure by rounding fractional\nsolutions to column-based linear programs for some generalizations of Bin\nPacking. For example we obtain a polynomial time OPT + O(log^2 OPT)\napproximation for Bin Packing With Rejection and the first AFPTAS for the Train\nDelivery problem.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2011 00:48:36 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Rothvoss", "Thomas", ""]]}, {"id": "1104.4617", "submitter": "Michael Codish", "authors": "Amit Metodi and Michael Codish and Vitaly Lagoon and Peter J. Stuckey", "title": "Boolean Equi-propagation for Optimized SAT Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to propagation based solving, Boolean\nequi-propagation, where constraints are modelled as propagators of information\nabout equalities between Boolean literals. Propagation based solving applies\nthis information as a form of partial evaluation resulting in optimized SAT\nencodings. We demonstrate for a variety of benchmarks that our approach results\nin smaller CNF encodings and leads to speed-ups in solving times.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2011 10:39:07 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Metodi", "Amit", ""], ["Codish", "Michael", ""], ["Lagoon", "Vitaly", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "1104.4618", "submitter": "Panos Giannopoulos", "authors": "Helmut Alt, Sergio Cabello, Panos Giannopoulos and Christian Knauer", "title": "Minimum cell connection and separation in line segment arrangements", "comments": "21 pages, 9 figures. About half of the results have appeared in the\n  abstracts of EuroCG'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the following cell connection and separation\nproblems in segment arrangements. Given a set of straight-line segments in the\nplane and two points $a$ and $b$ in different cells of the induced arrangement:\n  (i) compute the minimum number of segments one needs to remove so that there\nis a path connecting $a$ to $b$ that does not intersect any of the remaining\nsegments; (ii) compute the minimum number of segments one needs to remove so\nthat the arrangement induced by the remaining segments has a single cell; (iii)\ncompute the minimum number of segments one needs to retain so that any path\nconnecting $a$ to $b$ intersects some of the retained segments.\n  We show that problems (i) and (ii) are NP-hard and discuss some special,\ntractable cases. Most notably, we provide a linear-time algorithm for a variant\nof problem (i) where the path connecting $a$ to $b$ must stay inside a given\npolygon $P$ with a constant number of holes, the segments are contained in $P$,\nand the endpoints of the segments are on the boundary of $P$. For problem (iii)\nwe provide a cubic-time algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2011 10:51:01 GMT"}, {"version": "v2", "created": "Sun, 19 Jun 2011 15:08:09 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Alt", "Helmut", ""], ["Cabello", "Sergio", ""], ["Giannopoulos", "Panos", ""], ["Knauer", "Christian", ""]]}, {"id": "1104.4669", "submitter": "Hao-Hsiang Hung", "authors": "Michelangelo Grigni and Hao-Hsiang Hung", "title": "Finding Light Spanners in Bounded Pathwidth Graphs", "comments": "10 pages, 3 figures; 37th International Symposium on Mathematical\n  Foundations of Computer Science (MFCS 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Given an edge-weighted graph $G$ and $\\epsilon>0$, a $(1+\\epsilon)$-spanner\nis a spanning subgraph $G'$ whose shortest path distances approximate those of\n$G$ within a $(1+\\epsilon)$ factor. If $G$ is from certain minor-closed graph\nfamilies (at least bounded genus graphs and apex graphs), then we know that\nlight spanners exist. That is, we can compute a $(1+\\epsilon)$-spanner $G'$\nwith total edge weight at most a constant times the weight of a minimum\nspanning tree. This constant may depend on $\\epsilon$ and the graph family, but\nnot on the particular graph $G$ nor on its edge weighting. For weighted graphs\nfrom several minor-closed graph families, the existence of light spanners has\nbeen essential in the design of approximation schemes for the metric TSP (the\ntraveling salesman problem) and some similar problems. In this paper we make\nsome progress towards the conjecture that light spanners exist for every\nminor-closed graph family. In particular, we show that they exist for graphs\nwith bounded pathwidth. We do this via the construction of light enough\nmonotone spanning trees in such graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 00:59:46 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2012 01:39:55 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Grigni", "Michelangelo", ""], ["Hung", "Hao-Hsiang", ""]]}, {"id": "1104.4674", "submitter": "Eric Price", "authors": "Piotr Indyk and Eric Price", "title": "K-Median Clustering, Model-Based Compressive Sensing, and Sparse\n  Recovery for Earth Mover Distance", "comments": "21 pages. Appeared in STOC 2011. This version corrects a bug in the\n  proof of Theorem B.5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We initiate the study of sparse recovery problems under the Earth-Mover\nDistance (EMD). Specifically, we design a distribution over m x n matrices A\nsuch that for any x, given Ax, we can recover a k-sparse approximation to x\nunder the EMD distance. One construction yields m = O(k log(n/k)) and a 1 +\nepsilon approximation factor, which matches the best achievable bound for other\nerror measures, such as the L_1 norm. Our algorithms are obtained by exploiting\nnovel connections to other problems and areas, such as streaming algorithms for\nk-median clustering and model-based compressive sensing. We also provide novel\nalgorithms and results for the latter problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 03:49:54 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 18:43:39 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Indyk", "Piotr", ""], ["Price", "Eric", ""]]}, {"id": "1104.4680", "submitter": "Prasad Raghavendra", "authors": "Boaz Barak, Prasad Raghavendra, David Steurer", "title": "Rounding Semidefinite Programming Hierarchies via Global Correlation", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a new way to round vector solutions of semidefinite programming (SDP)\nhierarchies into integral solutions, based on a connection between these\nhierarchies and the spectrum of the input graph. We demonstrate the utility of\nour method by providing a new SDP-hierarchy based algorithm for constraint\nsatisfaction problems with 2-variable constraints (2-CSP's).\n  More concretely, we show for every 2-CSP instance I a rounding algorithm for\nr rounds of the Lasserre SDP hierarchy for I that obtains an integral solution\nthat is at most \\eps worse than the relaxation's value (normalized to lie in\n[0,1]), as long as r > k\\cdot\\rank_{\\geq \\theta}(\\Ins)/\\poly(\\e) \\;, where k is\nthe alphabet size of I, $\\theta=\\poly(\\e/k)$, and $\\rank_{\\geq \\theta}(\\Ins)$\ndenotes the number of eigenvalues larger than $\\theta$ in the normalized\nadjacency matrix of the constraint graph of $\\Ins$.\n  In the case that $\\Ins$ is a \\uniquegames instance, the threshold $\\theta$ is\nonly a polynomial in $\\e$, and is independent of the alphabet size. Also in\nthis case, we can give a non-trivial bound on the number of rounds for\n\\emph{every} instance. In particular our result yields an SDP-hierarchy based\nalgorithm that matches the performance of the recent subexponential algorithm\nof Arora, Barak and Steurer (FOCS 2010) in the worst case, but runs faster on a\nnatural family of instances, thus further restricting the set of possible hard\ninstances for Khot's Unique Games Conjecture.\n  Our algorithm actually requires less than the $n^{O(r)}$ constraints\nspecified by the $r^{th}$ level of the Lasserre hierarchy, and in some cases\n$r$ rounds of our program can be evaluated in time $2^{O(r)}\\poly(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 04:58:50 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Barak", "Boaz", ""], ["Raghavendra", "Prasad", ""], ["Steurer", "David", ""]]}, {"id": "1104.4728", "submitter": "Shay Mozes", "authors": "Philip N. Klein and Shay Mozes", "title": "Multiple-Source Single-Sink Maximum Flow in Directed Planar Graphs in\n  O(diameter*n*log(n)) Time", "comments": "proofs included. preliminary version to appear in WADS 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new technique for computing maximum flow in directed planar\ngraphs with multiple sources and a single sink that significantly deviates from\npreviously known techniques for flow problems. This gives rise to an\nO(diameter*n*log(n)) algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 14:44:26 GMT"}, {"version": "v2", "created": "Tue, 10 May 2011 15:24:12 GMT"}], "update_date": "2011-05-11", "authors_parsed": [["Klein", "Philip N.", ""], ["Mozes", "Shay", ""]]}, {"id": "1104.4746", "submitter": "Ali Sinop", "authors": "Venkatesan Guruswami and Ali Kemal Sinop", "title": "Lasserre Hierarchy, Higher Eigenvalues, and Approximation Schemes for\n  Quadratic Integer Programming with PSD Objectives", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximation scheme for optimizing certain Quadratic Integer\nProgramming problems with positive semidefinite objective functions and global\nlinear constraints. This framework includes well known graph problems such as\nMinimum graph bisection, Edge expansion, Uniform sparsest cut, and Small Set\nexpansion, as well as the Unique Games problem. These problems are notorious\nfor the existence of huge gaps between the known algorithmic results and\nNP-hardness results. Our algorithm is based on rounding semidefinite programs\nfrom the Lasserre hierarchy, and the analysis uses bounds for low-rank\napproximations of a matrix in Frobenius norm using columns of the matrix.\n  For all the above graph problems, we give an algorithm running in time\n$n^{O(r/\\epsilon^2)}$ with approximation ratio\n$\\frac{1+\\epsilon}{\\min\\{1,\\lambda_r\\}}$, where $\\lambda_r$ is the $r$'th\nsmallest eigenvalue of the normalized graph Laplacian $\\mathcal{L}$. In the\ncase of graph bisection and small set expansion, the number of vertices in the\ncut is within lower-order terms of the stipulated bound. Our results imply\n$(1+O(\\epsilon))$ factor approximation in time $n^{O(r^\\ast/\\epsilon^2)}$ where\n$r^\\ast$ is the number of eigenvalues of $\\mathcal{L}$ smaller than\n$1-\\epsilon$.\n  For Unique Games, we give a factor $(1+\\frac{2+\\epsilon}{\\lambda_r})$\napproximation for minimizing the number of unsatisfied constraints in\n$n^{O(r/\\epsilon)}$ time. This improves an earlier bound for solving Unique\nGames on expanders, and also shows that Lasserre SDPs are powerful enough to\nsolve well-known integrality gap instances for the basic SDP.\n  We also give an algorithm for independent sets in graphs that performs well\nwhen the Laplacian does not have too many eigenvalues bigger than $1+o(1)$.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 16:32:51 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2011 17:18:28 GMT"}, {"version": "v3", "created": "Wed, 18 May 2011 01:08:50 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Sinop", "Ali Kemal", ""]]}, {"id": "1104.4890", "submitter": "Jakub {\\L}{\\ka}cki", "authors": "Jakub \\L\\k{a}cki and Piotr Sankowski", "title": "Min-cuts and Shortest Cycles in Planar Graphs in O(n log log n) Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic O(n log log n) time algorithm for finding shortest\ncycles and minimum cuts in planar graphs. The algorithm improves the previously\nknown fastest algorithm by Italiano et al. in STOC'11 by a factor of log n.\nThis speedup is obtained through the use of dense distance graphs combined with\na divide-and-conquer approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 11:29:22 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1104.4892", "submitter": "Hsien-Chih Chang", "authors": "Hsien-Chih Chang and Hsueh-I Lu", "title": "Computing the Girth of a Planar Graph in Linear Time", "comments": "20 pages, 7 figures, accepted to SIAM Journal on Computing", "journal-ref": "SIAM Journal on Computing 42(3): 1077-1094 (2013)", "doi": "10.1137/110832033", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The girth of a graph is the minimum weight of all simple cycles of the graph.\nWe study the problem of determining the girth of an n-node unweighted\nundirected planar graph. The first non-trivial algorithm for the problem, given\nby Djidjev, runs in O(n^{5/4} log n) time. Chalermsook, Fakcharoenphol, and\nNanongkai reduced the running time to O(n log^2 n). Weimann and Yuster further\nreduced the running time to O(n log n). In this paper, we solve the problem in\nO(n) time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 11:44:19 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 10:32:57 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Chang", "Hsien-Chih", ""], ["Lu", "Hsueh-I", ""]]}, {"id": "1104.4954", "submitter": "Michael Sagraloff", "authors": "Pavel Emeliyanenko, Michael Sagraloff", "title": "On the Complexity of Solving a Bivariate Polynomial System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of computing the real solutions of a bivariate\npolynomial system using the recently proposed algorithm BISOLVE. BISOLVE is a\nclassical elimination method which first projects the solutions of a system\nonto the $x$- and $y$-axes and, then, selects the actual solutions from the so\ninduced candidate set. However, unlike similar algorithms, BISOLVE requires no\ngenericity assumption on the input nor it needs any change of the coordinate\nsystem. Furthermore, extensive benchmarks from \\cite{bes-bisolve-2011} confirm\nthat the algorithm outperforms state of the art approaches by a large factor.\nIn this work, we show that, for two polynomials $f,g\\in\\mathbb{Z}[x,y]$ of\ntotal degree at most $n$ with integer coefficients bounded by $2^\\tau$, BISOLVE\ncomputes isolating boxes for all real solutions of the system $f=g=0$ using\n$\\Otilde(n^8\\tau^{2})$ bit operations, thereby improving the previous record\nbound by a factor of at least $n^{2}$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 15:57:43 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Emeliyanenko", "Pavel", ""], ["Sagraloff", "Michael", ""]]}, {"id": "1104.5111", "submitter": "Martin Dietzfelbinger", "authors": "Martin Dietzfelbinger, Michael Mitzenmacher, Michael Rink", "title": "Cuckoo Hashing with Pages", "comments": "18 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cuckoo hashing has significant applications in both theoretical and\npractical settings, a relevant downside is that it requires lookups to multiple\nlocations. In many settings, where lookups are expensive, cuckoo hashing\nbecomes a less compelling alternative. One such standard setting is when memory\nis arranged in large pages, and a major cost is the number of page accesses. We\npropose the study of cuckoo hashing with pages, advocating approaches where\neach key has several possible locations, or cells, on a single page, and\nadditional choices on a second backup page. We show experimentally that with k\ncell choices on one page and a single backup cell choice, one can achieve\nnearly the same loads as when each key has k+1 random cells to choose from,\nwith most lookups requiring just one page access, even when keys are placed\nonline using a simple algorithm. While our results are currently experimental,\nthey suggest several interesting new open theoretical questions for cuckoo\nhashing with pages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 10:32:34 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Mitzenmacher", "Michael", ""], ["Rink", "Michael", ""]]}, {"id": "1104.5200", "submitter": "Magnus M. Halldorsson", "authors": "Magnus M. Halldorsson and Pradipta Mitra", "title": "Nearly Optimal Bounds for Distributed Wireless Scheduling in the SINR\n  Model", "comments": "Expanded and improved version of ICALP 2011 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the wireless scheduling problem in the SINR model. More\nspecifically, given a set of $n$ links, each a sender-receiver pair, we wish to\npartition (or \\emph{schedule}) the links into the minimum number of slots, each\nsatisfying interference constraints allowing simultaneous transmission. In the\nbasic problem, all senders transmit with the same uniform power.\n  We give a distributed $O(\\log n)$-approximation algorithm for the scheduling\nproblem, matching the best ratio known for centralized algorithms. It holds in\narbitrary metric space and for every length-monotone and sublinear power\nassignment. It is based on an algorithm of Kesselheim and V\\\"ocking, whose\nanalysis we improve by a logarithmic factor. We show that every distributed\nalgorithm uses $\\Omega(\\log n)$ slots to schedule certain instances that\nrequire only two slots, which implies that the best possible absolute\nperformance guarantee is logarithmic.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 17:48:07 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 20:13:02 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Mitra", "Pradipta", ""]]}, {"id": "1104.5214", "submitter": "Christian Sommer", "authors": "Ken-ichi Kawarabayashi, Philip N. Klein, and Christian Sommer", "title": "Linear-Space Approximate Distance Oracles for Planar, Bounded-Genus, and\n  Minor-Free Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-22006-7_12", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A (1 + eps)-approximate distance oracle for a graph is a data structure that\nsupports approximate point-to-point shortest-path-distance queries. The most\nrelevant measures for a distance-oracle construction are: space, query time,\nand preprocessing time. There are strong distance-oracle constructions known\nfor planar graphs (Thorup, JACM'04) and, subsequently, minor-excluded graphs\n(Abraham and Gavoille, PODC'06). However, these require Omega(eps^{-1} n lg n)\nspace for n-node graphs. We argue that a very low space requirement is\nessential. Since modern computer architectures involve hierarchical memory\n(caches, primary memory, secondary memory), a high memory requirement in effect\nmay greatly increase the actual running time. Moreover, we would like data\nstructures that can be deployed on small mobile devices, such as handhelds,\nwhich have relatively small primary memory. In this paper, for planar graphs,\nbounded-genus graphs, and minor-excluded graphs we give distance-oracle\nconstructions that require only O(n) space. The big O hides only a fixed\nconstant, independent of \\epsilon and independent of genus or size of an\nexcluded minor. The preprocessing times for our distance oracle are also faster\nthan those for the previously known constructions. For planar graphs, the\npreprocessing time is O(n lg^2 n). However, our constructions have slower query\ntimes. For planar graphs, the query time is O(eps^{-2} lg^2 n). For our\nlinear-space results, we can in fact ensure, for any delta > 0, that the space\nrequired is only 1 + delta times the space required just to represent the graph\nitself.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 18:57:20 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Klein", "Philip N.", ""], ["Sommer", "Christian", ""]]}, {"id": "1104.5226", "submitter": "David Doty", "authors": "Ho-Lin Chen and David Doty", "title": "Parallelism and Time in Hierarchical Self-Assembly", "comments": "accepted to appear in SIAM Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role that parallelism plays in time complexity of Winfree's\nabstract Tile Assembly Model (aTAM), a model of molecular algorithmic\nself-assembly. In the \"hierarchical\" aTAM, two assemblies, both consisting of\nmultiple tiles, are allowed to aggregate together, whereas in the \"seeded\"\naTAM, tiles attach one at a time to a growing assembly. Adleman, Cheng, Goel,\nand Huang (\"Running Time and Program Size for Self-Assembled Squares\", STOC\n2001) showed how to assemble an n x n square in O(n) time in the seeded aTAM\nusing O(log n / log log n) unique tile types, where both of these parameters\nare optimal. They asked whether the hierarchical aTAM could allow a tile system\nto use the ability to form large assemblies in parallel before they attach to\nbreak the Omega(n) lower bound for assembly time. We show that there is a tile\nsystem with the optimal O(log n / log log n) tile types that assembles an n x n\nsquare using O(log^2 n) parallel \"stages\", which is close to the optimal\nOmega(log n) stages, forming the final n x n square from four n/2 x n/2\nsquares, which are themselves recursively formed from n/4 x n/4 squares, etc.\nHowever, despite this nearly maximal parallelism, the system requires\nsuperlinear time to assemble the square. We extend the definition of *partial\norder tile systems* studied by Adleman et al. in a natural way to hierarchical\nassembly and show that no hierarchical partial order tile system can build any\nshape with diameter N in less than time Omega(N), demonstrating that in this\ncase the hierarchical model affords no speedup whatsoever over the seeded\nmodel. We strengthen the Omega(N) time lower bound for deterministic seeded\nsystems of Adleman et al. to nondeterministic seeded systems. Finally, we show\nthat for infinitely many n, a tile system can assemble an n x n' rectangle,\nwith n > n', in time O(n^{4/5} log n), breaking the linear-time lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 19:56:01 GMT"}, {"version": "v2", "created": "Sun, 12 Feb 2017 18:35:40 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Chen", "Ho-Lin", ""], ["Doty", "David", ""]]}, {"id": "1104.5400", "submitter": "Bar Shalem", "authors": "Ely Porat and Bar Shalem", "title": "A cuckoo hashing variant with improved memory utilization and insertion\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo hashing [4] is a multiple choice hashing scheme in which each item can\nbe placed in multiple locations, and collisions are resolved by moving items to\ntheir alternative locations. In the classical implementation of two-way cuckoo\nhashing, the memory is partitioned into contiguous disjoint fixed-size buckets.\nEach item is hashed to two buckets, and may be stored in any of the positions\nwithin those buckets. Ref. [2] analyzed a variation in which the buckets are\ncontiguous and overlap. However, many systems retrieve data from secondary\nstorage in same-size blocks called pages. Fetching a page is a relatively\nexpensive process; but once a page is fetched, its contents can be accessed\norders of magnitude faster. We utilize this property of memory retrieval,\npresenting a variant of cuckoo hashing incorporating the following constraint:\neach bucket must be fully contained in a single page, but buckets are not\nnecessarily contiguous. Empirical results show that this modification increases\nmemory utilization and decreases the number of iterations required to insert an\nitem. If each item is hashed to two buckets of capacity two, the page size is\n8, and each bucket is fully contained in a single page, the memory utilization\nequals 89.71% in the classical contiguous disjoint bucket variant, 93.78% in\nthe contiguous overlapping bucket variant, and increases to 97.46% in our new\nnon-contiguous bucket variant. When the memory utilization is 92% and we use\nbreadth first search to look for a vacant position, the number of iterations\nrequired to insert a new item is dramatically reduced from 545 in the\ncontiguous overlapping buckets variant to 52 in our new non-contiguous bucket\nvariant. In addition to the empirical results, we present a theoretical lower\nbound on the memory utilization of our variation as a function of the page\nsize.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 14:18:26 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2011 17:47:06 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2011 01:41:44 GMT"}], "update_date": "2011-11-16", "authors_parsed": [["Porat", "Ely", ""], ["Shalem", "Bar", ""]]}, {"id": "1104.5510", "submitter": "Fahad Saeed", "authors": "Fahad Saeed, Trairak Pisitkun, Mark A. Knepper and Jason D. Hoffert", "title": "Mining Temporal Patterns from iTRAQ Mass Spectrometry(LC-MS/MS) Data", "comments": "12 pages, 10 figures, The Proceedings of the ISCA 3rd International\n  Conference on Bioinformatics and Computational Biology (BiCoB), pp 152-159\n  New Orleans, Louisiana, USA, March 23-25, 2011 (ISBN: 978-1-880843-81-9)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DB cs.DS q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale proteomic analysis is emerging as a powerful technique in biology\nand relies heavily on data acquired by state-of-the-art mass spectrometers. As\nwith any other field in Systems Biology, computational tools are required to\ndeal with this ocean of data. iTRAQ (isobaric Tags for Relative and Absolute\nquantification) is a technique that allows simultaneous quantification of\nproteins from multiple samples. Although iTRAQ data gives useful insights to\nthe biologist, it is more complex to perform analysis and draw biological\nconclusions because of its multi-plexed design. One such problem is to find\nproteins that behave in a similar way (i.e. change in abundance) among various\ntime points since the temporal variations in the proteomics data reveal\nimportant biological information. Distance based methods such as Euclidian\ndistance or Pearson coefficient, and clustering techniques such as k-mean etc,\nare not able to take into account the temporal information of the series. In\nthis paper, we present an linear-time algorithm for clustering similar patterns\namong various iTRAQ time course data irrespective of their absolute values. The\nalgorithm, referred to as Temporal Pattern Mining(TPM), maps the data from a\nCartesian plane to a discrete binary plane. After the mapping a dynamic\nprogramming technique allows mining of similar data elements that are\ntemporally closer to each other. The proposed algorithm accurately clusters\niTRAQ data that are temporally closer to each other with more than 99%\naccuracy. Experimental results for different problem sizes are analyzed in\nterms of quality of clusters, execution time and scalability for large data\nsets. An example from our proteomics data is provided at the end to demonstrate\nthe performance of the algorithm and its ability to cluster temporal series\nirrespective of their distance from each other.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 20:41:43 GMT"}], "update_date": "2011-05-02", "authors_parsed": [["Saeed", "Fahad", ""], ["Pisitkun", "Trairak", ""], ["Knepper", "Mark A.", ""], ["Hoffert", "Jason D.", ""]]}, {"id": "1104.5517", "submitter": "Patrick Nicholson", "authors": "Amr Elmasry, Meng He, J. Ian Munro, and Patrick K. Nicholson", "title": "Dynamic Range Majority Data Structures", "comments": "16 pages, Preliminary version appeared in ISAAC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of coloured points on the real line, we study the problem of\nanswering range $\\alpha$-majority (or \"heavy hitter\") queries on $P$. More\nspecifically, for a query range $Q$, we want to return each colour that is\nassigned to more than an $\\alpha$-fraction of the points contained in $Q$. We\npresent a new data structure for answering range $\\alpha$-majority queries on a\ndynamic set of points, where $\\alpha \\in (0,1)$. Our data structure uses O(n)\nspace, supports queries in $O((\\lg n) / \\alpha)$ time, and updates in $O((\\lg\nn) / \\alpha)$ amortized time. If the coordinates of the points are integers,\nthen the query time can be improved to $O(\\lg n / (\\alpha \\lg \\lg n) +\n(\\lg(1/\\alpha))/\\alpha))$. For constant values of $\\alpha$, this improved query\ntime matches an existing lower bound, for any data structure with\npolylogarithmic update time. We also generalize our data structure to handle\nsets of points in d-dimensions, for $d \\ge 2$, as well as dynamic arrays, in\nwhich each entry is a colour.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 21:35:40 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 16:40:45 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Elmasry", "Amr", ""], ["He", "Meng", ""], ["Munro", "J. Ian", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1104.5533", "submitter": "Justin Thaler", "authors": "Elaine Angelino, Michael T. Goodrich, Michael Mitzenmacher, Justin\n  Thaler", "title": "External-Memory Multimaps", "comments": "Accepted to ISAAC 2011. 22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data structures support dictionaries, also known as maps or associative\narrays, which store and manage a set of key-value pairs. A \\emph{multimap} is\ngeneralization that allows multiple values to be associated with the same key.\nFor example, the inverted file data structure that is used prevalently in the\ninfrastructure supporting search engines is a type of multimap, where words are\nused as keys and document pointers are used as values. We study the multimap\nabstract data type and how it can be implemented efficiently online in external\nmemory frameworks, with constant expected I/O performance. The key technique\nused to achieve our results is a combination of cuckoo hashing using buckets\nthat hold multiple items with a multiqueue implementation to cope with varying\nnumbers of values per key. Our external-memory results are for the standard\ntwo-level memory model.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 01:55:00 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2011 15:44:29 GMT"}], "update_date": "2011-09-19", "authors_parsed": [["Angelino", "Elaine", ""], ["Goodrich", "Michael T.", ""], ["Mitzenmacher", "Michael", ""], ["Thaler", "Justin", ""]]}, {"id": "1104.5557", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Randomized algorithms for matrices and data", "comments": "Review article, 54 pages, 198 references. Version appearing as a\n  monograph in Now Publishers' \"Foundations and Trends in Machine Learning\"\n  series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms for very large matrix problems have received a great\ndeal of attention in recent years. Much of this work was motivated by problems\nin large-scale data analysis, and this work was performed by individuals from\nmany different research communities. This monograph will provide a detailed\noverview of recent work on the theory of randomized matrix algorithms as well\nas the application of those ideas to the solution of practical problems in\nlarge-scale data analysis. An emphasis will be placed on a few simple core\nideas that underlie not only recent theoretical advances but also the\nusefulness of these tools in large-scale data applications. Crucial in this\ncontext is the connection with the concept of statistical leverage. This\nconcept has long been used in statistical regression diagnostics to identify\noutliers; and it has recently proved crucial in the development of improved\nworst-case matrix algorithms that are also amenable to high-quality numerical\nimplementation and that are useful to domain scientists. Randomized methods\nsolve problems such as the linear least-squares problem and the low-rank matrix\napproximation problem by constructing and operating on a randomized sketch of\nthe input matrix. Depending on the specifics of the situation, when compared\nwith the best previously-existing deterministic algorithms, the resulting\nrandomized algorithms have worst-case running time that is asymptotically\nfaster; their numerical implementations are faster in terms of clock-time; or\nthey can be implemented in parallel computing environments where existing\nnumerical algorithms fail to run at all. Numerous examples illustrating these\nobservations will be described in detail.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 06:41:53 GMT"}, {"version": "v2", "created": "Mon, 2 May 2011 16:50:00 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2011 08:24:46 GMT"}], "update_date": "2011-11-16", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1104.5597", "submitter": "Wolfgang Mulzer", "authors": "John Iacono, Wolfgang Mulzer", "title": "A Static Optimality Transformation with Applications to Planar Point\n  Location", "comments": "13 pages, 1 figure, a preliminary version appeared at SoCG 2011", "journal-ref": "International Journal of Computational Geometry and Applications\n  (IJCGA), 22(4), 2012, pp. 327-340", "doi": "10.1142/S0218195912600084", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, there have been several data structures that, given a\nplanar subdivision and a probability distribution over the plane, provide a way\nfor answering point location queries that is fine-tuned for the distribution.\nAll these methods suffer from the requirement that the query distribution must\nbe known in advance.\n  We present a new data structure for point location queries in planar\ntriangulations. Our structure is asymptotically as fast as the optimal\nstructures, but it requires no prior information about the queries. This is a\n2D analogue of the jump from Knuth's optimum binary search trees (discovered in\n1971) to the splay trees of Sleator and Tarjan in 1985. While the former need\nto know the query distribution, the latter are statically optimal. This means\nthat we can adapt to the query sequence and achieve the same asymptotic\nperformance as an optimum static structure, without needing any additional\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 10:55:50 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2011 12:59:03 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2012 09:04:20 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Iacono", "John", ""], ["Mulzer", "Wolfgang", ""]]}, {"id": "1104.5646", "submitter": "Tamal Dey", "authors": "Dan Burghelea and Tamal K. Dey", "title": "Persistence for Circle Valued Maps", "comments": "A complete algorithm to compute barcodes and Jordan cells is provided\n  in this version. The paper is accepted in in the journal Discrete &\n  Computational Geometry. arXiv admin note: text overlap with arXiv:1210.3092\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study circle valued maps and consider the persistence of the homology of\ntheir fibers. The outcome is a finite collection of computable invariants which\nanswer the basic questions on persistence and in addition encode the topology\nof the source space and its relevant subspaces. Unlike persistence of real\nvalued maps, circle valued maps enjoy a different class of invariants called\nJordan cells in addition to bar codes. We establish a relation between the\nhomology of the source space and of its relevant subspaces with these\ninvariants and provide a new algorithm to compute these invariants from an\ninput matrix that encodes a circle valued map on an input simplicial complex.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 15:02:12 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2011 19:23:28 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2011 18:49:58 GMT"}, {"version": "v4", "created": "Fri, 9 Mar 2012 15:30:13 GMT"}, {"version": "v5", "created": "Tue, 19 Feb 2013 23:24:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Burghelea", "Dan", ""], ["Dey", "Tamal K.", ""]]}]