[{"id": "2003.00094", "submitter": "Mohit Daga", "authors": "Mohit Daga", "title": "Improved Algorithm for Min-Cuts in Distributed Networks", "comments": "This is a copy of thesis which was submitted to IIT Madras for MS\n  Degree in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this thesis, we present fast deterministic algorithm to find small cuts in\ndistributed networks. Finding small min-cuts for a network is essential for\nensuring the quality of service and reliability. Throughout this thesis, we use\nthe CONGEST model which is a typical message passing model used to design and\nanalyze algorithms in distributed networks. We survey various algorithmic\ntechniques in the CONGEST model and give an overview of the recent results to\nfind cuts. We also describe elegant graph theoretic ideas like cut spaces and\ncycle spaces that provide useful intuition upon which our work is built.\n  Our contribution is a novel fast algorith to find small cuts. Our algorithm\nrelies on a new characterization of trees and cuts introduced in this thesis.\nOur algorithm is built upon several new algorithmic ideas that, when coupled\nwith our characterization of trees and cuts, help us to find the required\nmin-cuts. Our novel techniques include a tree restricted semigroup function\n(TRSF), a novel sketching technique, and a layered algorithm. TRSF is defined\nwith respect to a spanning tree and is based on a commutative semigroup. This\nsimple yet powerful technique helps us to deterministically find min-cuts of\nsize one (bridges) and min-cuts of size two optimally. Our sketching technique\nsamples a small but relevant vertex set which is enough to find small min-cuts\nin certain cases. Our layered algorithm finds min-cuts in smaller sub-graphs\npivoted by nodes at different levels in a spanning tree and uses them to make\nthe decision about the min-cuts in the complete graph. This is interesting\nbecause it enables us to show that even for a global property like finding\nmin-cuts, local information can be exploited in a coordinated manner.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 22:25:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Daga", "Mohit", ""]]}, {"id": "2003.00103", "submitter": "Nikos Batsaras", "authors": "Nikos Batsaras, Giorgos Saloustros, Anastasios Papagiannis, Panagiota\n  Fatourou, and Angelos Bilas", "title": "VAT: Asymptotic Cost Analysis for Multi-Level Key-Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, there has been an increasing number of key-value (KV)\nstore designs, each optimizing for a different set of requirements.\nFurthermore, with the advancements of storage technology the design space of KV\nstores has become even more complex. More recent KV-store designs target fast\nstorage devices, such as SSDs and NVM. Most of these designs aim to reduce\namplification during data reorganization by taking advantage of device\ncharacteristics. However, until today most analysis of KV-store designs is\nexperimental and limited to specific design points. This makes it difficult to\ncompare tradeoffs across different designs, find optimal configurations and\nguide future KV-store design. In this paper, we introduce the Variable\nAmplification- Throughput analysis (VAT) to calculate insert-path amplification\nand its impact on multi-level KV-store performance.We use VAT to express the\nbehavior of several existing design points and to explore tradeoffs that are\nnot possible or easy to measure experimentally. VAT indicates that by inserting\nrandomness in the insert-path, KV stores can reduce amplification by more than\n10x for fast storage devices. Techniques, such as key-value separation and\ntiering compaction, reduce amplification by 10x and 5x, respectively.\nAdditionally, VAT predicts that the advancements in device technology towards\nNVM, reduces the benefits from both using key-value separation and tiering.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 22:54:18 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Batsaras", "Nikos", ""], ["Saloustros", "Giorgos", ""], ["Papagiannis", "Anastasios", ""], ["Fatourou", "Panagiota", ""], ["Bilas", "Angelos", ""]]}, {"id": "2003.00119", "submitter": "Grace Dinh", "authors": "Grace Dinh and James Demmel", "title": "Communication-Optimal Tilings for Projective Nested Loops with Arbitrary\n  Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing communication - either between levels of a memory hierarchy or\nbetween processors over a network - is a key component of performance\noptimization (in both time and energy) for many problems, including dense\nlinear algebra, particle interactions, and machine learning. For these\nproblems, which can be represented as nested-loop computations, previous tiling\nbased approaches have been used to find both lower bounds on the communication\nrequired to execute them and optimal rearrangements, or blockings, to attain\nsuch lower bounds. However, such general approaches have typically assumed the\nproblem sizes are large, an assumption that is often not met in practice. For\ninstance, the classical $(\\text{# arithmetic operations})/(\\text{cache\nsize})^{1/2}$ lower bound for matrix multiplication is not tight for\nmatrix-vector multiplications, which must read in at least $O(\\text{#\narithmetic operations})$ words of memory; similar issues occur for almost all\nconvolutions in machine learning applications, which use extremely small filter\nsizes (and therefore, loop bounds).\n  In this paper, we provide an efficient way to both find and obtain, via an\nappropriate, efficiently constructible blocking, communication lower bounds and\nmatching tilings which attain these lower bounds for nested loop programs with\narbitrary loop bounds that operate on multidimensional arrays in the projective\ncase, where the array indices are subsets of the loop indices. Our approach\nworks on all such problems, regardless of dimensionality, size, memory access\npatterns, or number of arrays, and directly applies to (among other examples)\nmatrix multiplication and similar dense linear algebra operations, tensor\ncontractions, n-body pairwise interactions, pointwise convolutions, and fully\nconnected layers.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 23:38:15 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Dinh", "Grace", ""], ["Demmel", "James", ""]]}, {"id": "2003.00269", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Bin Li, Scott A. Sisson", "title": "Online Binary Space Partitioning Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Binary Space Partitioning-Tree~(BSP-Tree) process was recently proposed\nas an efficient strategy for space partitioning tasks. Because it uses more\nthan one dimension to partition the space, the BSP-Tree Process is more\nefficient and flexible than conventional axis-aligned cutting strategies.\nHowever, due to its batch learning setting, it is not well suited to\nlarge-scale classification and regression problems. In this paper, we develop\nan online BSP-Forest framework to address this limitation. With the arrival of\nnew data, the resulting online algorithm can simultaneously expand the space\ncoverage and refine the partition structure, with guaranteed universal\nconsistency for both classification and regression problems. The effectiveness\nand competitive performance of the online BSP-Forest is verified via\nsimulations on real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 14:35:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fan", "Xuhui", ""], ["Li", "Bin", ""], ["Sisson", "Scott A.", ""]]}, {"id": "2003.00468", "submitter": "Moti Medina", "authors": "Reut Levi and Moti Medina", "title": "Distributed Testing of Graph Isomorphism in the CONGEST model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of testing graph isomorphism (GI) in the\nCONGEST distributed model. In this setting we test whether the distributive\nnetwork, $G_U$, is isomorphic to $G_K$ which is given as an input to all the\nnodes in the network, or alternatively, only to a single node.\n  We first consider the decision variant of the problem in which the algorithm\ndistinguishes $G_U$ and $G_K$ which are isomorphic from $G_U$ and $G_K$ which\nare not isomorphic. We provide a randomized algorithm with $O(n)$ rounds for\nthe setting in which $G_K$ is given only to a single node. We prove that for\nthis setting the number of rounds of any deterministic algorithm is\n$\\tilde{\\Omega}(n^2)$ rounds, where $n$ denotes the number of nodes, which\nimplies a separation between the randomized and the deterministic complexities\nof deciding GI.\n  We then consider the \\emph{property testing} variant of the problem, where\nthe algorithm is only required to distinguish the case that $G_U$ and $G_K$ are\nisomorphic from the case that $G_U$ and $G_K$ are \\emph{far} from being\nisomorphic (according to some predetermined distance measure). We show that\nevery algorithm requires $\\Omega(D)$ rounds, where $D$ denotes the diameter of\nthe network. This lower bound holds even if all the nodes are given $G_K$ as an\ninput, and even if the message size is unbounded. We provide a randomized\nalgorithm with an almost matching round complexity of $O(D+(\\epsilon^{-1}\\log\nn)^2)$ rounds that is suitable for dense graphs.\n  We also show that with the same number of rounds it is possible that each\nnode outputs its mapping according to a bijection which is an\n\\emph{approximated} isomorphism.\n  We conclude with simple simulation arguments that allow us to obtain\nessentially tight algorithms with round complexity $\\tilde{O}(D)$ for special\nfamilies of sparse graphs.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 12:03:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Levi", "Reut", ""], ["Medina", "Moti", ""]]}, {"id": "2003.00488", "submitter": "Pongsaphol Pongsawakul", "authors": "Pongsaphol Pongsawakul", "title": "An Algorithm for Consensus Trees", "comments": "erroneous claim by JF removed, reference to a faster algorithm by\n  Jansson, Shen, and Sung [JACM'16] added, also reference to Sung [WALCOM'19]\n  added (thanks to Pawel Gawrychowski and Oren Weiman for comments and\n  references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the tree consensus problem, an important problem in\nbioinformatics. Given a rooted tree $t$ and another tree $T$, one would like to\nincorporate compatible information from $T$ to $t$. This problem is a\nsubproblem in the tree refinement problem called the RF-Optimal Tree Refinement\nProblem defined by in Christensen, Molloy, Vachaspati and Warnow [WABI'19] who\nemploy the greedy algorithm by Gawrychowski, Landau, Sung, and Weimann\n[ICALP'18] that runs in time $O(n^{1.5}\\log n)$. We give a faster algorithm for\nthis problem that runs in time $O(n\\log n)$. Our key ingredient is a\nbipartition compatibility criteria based on amortized-time leaf counters. While\nthis is an improvement, the fastest solution is an algorithm by Jansson, Shen,\nand Sung [JACM'16] which runs in time $O(n)$.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 13:35:03 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 12:27:11 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Pongsawakul", "Pongsaphol", ""]]}, {"id": "2003.00556", "submitter": "Fabrizio Frati", "authors": "Giordano Da Lozzo, Anthony D'Angelo, and Fabrizio Frati", "title": "On the Area Requirements of Planar Greedy Drawings of Triconnected\n  Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the area requirements of planar greedy drawings of\ntriconnected planar graphs. Cao, Strelzoff, and Sun exhibited a family $\\cal H$\nof subdivisions of triconnected plane graphs and claimed that every planar\ngreedy drawing of the graphs in $\\mathcal H$ respecting the prescribed plane\nembedding requires exponential area. However, we show that every $n$-vertex\ngraph in $\\cal H$ actually has a planar greedy drawing respecting the\nprescribed plane embedding on an $O(n)\\times O(n)$ grid. This reopens the\nquestion whether triconnected planar graphs admit planar greedy drawings on a\npolynomial-size grid. Further, we provide evidence for a positive answer to the\nabove question by proving that every $n$-vertex Halin graph admits a planar\ngreedy drawing on an $O(n)\\times O(n)$ grid. Both such results are obtained by\nactually constructing drawings that are convex and angle-monotone. Finally, we\nconsider $\\alpha$-Schnyder drawings, which are angle-monotone and hence greedy\nif $\\alpha\\leq 30^\\circ$, and show that there exist planar triangulations for\nwhich every $\\alpha$-Schnyder drawing with a fixed $\\alpha<60^\\circ$ requires\nexponential area for any resolution rule.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 19:04:27 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 16:29:39 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["D'Angelo", "Anthony", ""], ["Frati", "Fabrizio", ""]]}, {"id": "2003.00614", "submitter": "Sixue Liu", "authors": "S. Cliff Liu, Robert E. Tarjan, Peilin Zhong", "title": "Connected Components on a PRAM in Log Diameter Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O(\\log d + \\log\\log_{m/n} n)$-time randomized PRAM algorithm\nfor computing the connected components of an $n$-vertex, $m$-edge undirected\ngraph with maximum component diameter $d$. The algorithm runs on an ARBITRARY\nCRCW (concurrent-read, concurrent-write with arbitrary write resolution) PRAM\nusing $O(m)$ processors. The time bound holds with good probability.\n  Our algorithm is based on the breakthrough results of Andoni et al. [FOCS'18]\nand Behnezhad et al. [FOCS'19]. Their algorithms run on the more powerful MPC\nmodel and rely on sorting and computing prefix sums in $O(1)$ time, tasks that\ntake $\\Omega(\\log n / \\log\\log n)$ time on a CRCW PRAM with $\\text{poly}(n)$\nprocessors. Our simpler algorithm uses limited-collision hashing and does not\nsort or do prefix sums. It matches the time and space bounds of the algorithm\nof Behnezhad et al., who improved the time bound of Andoni et al.\n  It is widely believed that the larger private memory per processor and\nunbounded local computation of the MPC model admit algorithms faster than that\non a PRAM. Our result suggests that such additional power might not be\nnecessary, at least for fundamental graph problems like connected components\nand spanning forest.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 00:00:11 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 22:56:31 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 13:03:59 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "S. Cliff", ""], ["Tarjan", "Robert E.", ""], ["Zhong", "Peilin", ""]]}, {"id": "2003.00649", "submitter": "Hsien-Chih Chang", "authors": "Hsien-Chih Chang and Arnaud de Mesmay", "title": "Tightening Curves on Surfaces Monotonically with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the first polynomial bound on the number of monotonic homotopy moves\nrequired to tighten a collection of closed curves on any compact orientable\nsurface, where the number of crossings in the curve is not allowed to increase\nat any time during the process. The best known upper bound before was\nexponential, which can be obtained by combining the algorithm of de Graaf and\nSchrijver [J. Comb. Theory Ser. B, 1997] together with an exponential upper\nbound on the number of possible surface maps. To obtain the new upper bound we\napply tools from hyperbolic geometry, as well as operations in graph drawing\nalgorithms---the cluster and pipe expansions---to the study of curves on\nsurfaces.\n  As corollaries, we present two efficient algorithms for curves and graphs on\nsurfaces. First, we provide a polynomial-time algorithm to convert any given\nmulticurve on a surface into minimal position. Such an algorithm only existed\nfor single closed curves, and it is known that previous techniques do not\ngeneralize to the multicurve case. Second, we provide a polynomial-time\nalgorithm to reduce any $k$-terminal plane graph (and more generally, surface\ngraph) using degree-1 reductions, series-parallel reductions, and $\\Delta\nY$-transformations for arbitrary integer $k$. Previous algorithms only existed\nin the planar setting when $k \\le 4$, and all of them rely on extensive\ncase-by-case analysis based on different values of $k$. Our algorithm makes use\nof the connection between electrical transformations and homotopy moves, and\nthus solves the problem in a unified fashion.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 04:09:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chang", "Hsien-Chih", ""], ["de Mesmay", "Arnaud", ""]]}, {"id": "2003.00736", "submitter": "Manuel Penschuck", "authors": "Manuel Penschuck and Ulrik Brandes and Michael Hamann and Sebastian\n  Lamm and Ulrich Meyer and Ilya Safro and Peter Sanders and Christian Schulz", "title": "Recent Advances in Scalable Network Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph models are frequently used as a controllable and versatile data\nsource for experimental campaigns in various research fields. Generating such\ndata-sets at scale is a non-trivial task as it requires design decisions\ntypically spanning multiple areas of expertise. Challenges begin with the\nidentification of relevant domain-specific network features, continue with the\nquestion of how to compile such features into a tractable model, and culminate\nin algorithmic details arising while implementing the pertaining model.\n  In the present survey, we explore crucial aspects of random graph models with\nknown scalable generators. We begin by briefly introducing network features\nconsidered by such models, and then discuss random graphs alongside with\ngeneration algorithms. Our focus lies on modelling techniques and algorithmic\nprimitives that have proven successful in obtaining massive graphs. We consider\nconcepts and graph models for various domains (such as social network,\ninfrastructure, ecology, and numerical simulations), and discuss generators for\ndifferent models of computation (including shared-memory parallelism,\nmassive-parallel GPUs, and distributed systems).\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 09:51:56 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Penschuck", "Manuel", ""], ["Brandes", "Ulrik", ""], ["Hamann", "Michael", ""], ["Lamm", "Sebastian", ""], ["Meyer", "Ulrich", ""], ["Safro", "Ilya", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "2003.00844", "submitter": "Kirankumar Shiragur", "authors": "Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "A General Framework for Symmetric Property Estimation", "comments": "Published in Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a general framework for estimating symmetric\nproperties of distributions from i.i.d. samples. For a broad class of symmetric\nproperties we identify the easy region where empirical estimation works and the\ndifficult region where more complex estimators are required. We show that by\napproximately computing the profile maximum likelihood (PML) distribution\n\\cite{ADOS16} in this difficult region we obtain a symmetric property\nestimation framework that is sample complexity optimal for many properties in a\nbroader parameter regime than previous universal estimation approaches based on\nPML. The resulting algorithms based on these pseudo PML distributions are also\nmore practical.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:00:04 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "2003.00938", "submitter": "Meirav Zehavi", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh,\n  Meirav Zehavi", "title": "ETH-Tight Algorithms for Long Path and Cycle on Unit Disk Graphs", "comments": "Extended version to appear in SoCG'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for the extensively studied Long Path and Long Cycle\nproblems on unit disk graphs that runs in time $2^{O(\\sqrt{k})}(n+m)$. Under\nthe Exponential Time Hypothesis, Long Path and Long Cycle on unit disk graphs\ncannot be solved in time $2^{o(\\sqrt{k})}(n+m)^{O(1)}$ [de Berg et al., STOC\n2018], hence our algorithm is optimal. Besides the\n$2^{O(\\sqrt{k})}(n+m)^{O(1)}$-time algorithm for the (arguably) much simpler\nVertex Cover problem by de Berg et al. [STOC 2018] (which easily follows from\nthe existence of a $2k$-vertex kernel for the problem), this is the only known\nETH-optimal fixed-parameter tractable algorithm on UDGs. Previously, Long Path\nand Long Cycle on unit disk graphs were only known to be solvable in time\n$2^{O(\\sqrt{k}\\log k)}(n+m)$. This algorithm involved the introduction of a new\ntype of a tree decomposition, entailing the design of a very tedious dynamic\nprogramming procedure. Our algorithm is substantially simpler: we completely\navoid the use of this new type of tree decomposition. Instead, we use a marking\nprocedure to reduce the problem to (a weighted version of) itself on a standard\ntree decomposition of width $O(\\sqrt{k})$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:34:53 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2003.00973", "submitter": "Debabrota Basu", "authors": "Ashish Dandekar, Debabrota Basu, Stephane Bressan", "title": "Differential Privacy at Risk: Bridging Randomness and Privacy Budget", "comments": "Presented in Workshop on Privacy Preserving AI (PPAI) at AAAI, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The calibration of noise for a privacy-preserving mechanism depends on the\nsensitivity of the query and the prescribed privacy level. A data steward must\nmake the non-trivial choice of a privacy level that balances the requirements\nof users and the monetary constraints of the business entity. We analyse roles\nof the sources of randomness, namely the explicit randomness induced by the\nnoise distribution and the implicit randomness induced by the data-generation\ndistribution, that are involved in the design of a privacy-preserving\nmechanism. The finer analysis enables us to provide stronger privacy guarantees\nwith quantifiable risks. Thus, we propose privacy at risk that is a\nprobabilistic calibration of privacy-preserving mechanisms. We provide a\ncomposition theorem that leverages privacy at risk. We instantiate the\nprobabilistic calibration for the Laplace mechanism by providing analytical\nresults. We also propose a cost model that bridges the gap between the privacy\nlevel and the compensation budget estimated by a GDPR compliant business\nentity. The convexity of the proposed cost model leads to a unique fine-tuning\nof privacy level that minimises the compensation budget. We show its\neffectiveness by illustrating a realistic scenario that avoids overestimation\nof the compensation budget by using privacy at risk for the Laplace mechanism.\nWe quantitatively show that composition using the cost optimal privacy at risk\nprovides stronger privacy guarantee than the classical advanced composition.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:44:14 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 23:14:27 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Dandekar", "Ashish", ""], ["Basu", "Debabrota", ""], ["Bressan", "Stephane", ""]]}, {"id": "2003.01154", "submitter": "Charles Carlson", "authors": "Charles Carlson, Ewan Davies, Alexandra Kolla", "title": "Efficient algorithms for the Potts model on small-set expanders", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approximation algorithm for the partition function of the\nferromagnetic Potts model on graphs with a small-set expansion condition, and\nas a step in the argument we give a graph partitioning algorithm with expansion\nand minimum degree conditions on the subgraphs induced by each part. These\nresults extend previous work of Jenssen, Keevash, and Perkins (2019) on the\nPotts model and related problems in expander graphs, and of Oveis Gharan and\nTrevisan (2014) on partitioning into expanders.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 19:30:49 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Carlson", "Charles", ""], ["Davies", "Ewan", ""], ["Kolla", "Alexandra", ""]]}, {"id": "2003.01203", "submitter": "Siddhartha Jayanti", "authors": "Siddhartha V. Jayanti and Robert E. Tarjan", "title": "Concurrent Disjoint Set Union", "comments": "40 pages, combines ideas in two previous PODC papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze concurrent algorithms for the disjoint set union\n(union-find) problem in the shared memory, asynchronous multiprocessor model of\ncomputation, with CAS (compare and swap) or DCAS (double compare and swap) as\nthe synchronization primitive. We give a deterministic bounded wait-free\nalgorithm that uses DCAS and has a total work bound of $O(m \\cdot (\\log(np/m +\n1) + \\alpha(n, m/(np)))$ for a problem with $n$ elements and $m$ operations\nsolved by $p$ processes, where $\\alpha$ is a functional inverse of Ackermann's\nfunction. We give two randomized algorithms that use only CAS and have the same\nwork bound in expectation. The analysis of the second randomized algorithm is\nvalid even if the scheduler is adversarial. Our DCAS and randomized algorithms\ntake $O(\\log n)$ steps per operation, worst-case for the DCAS algorithm,\nhigh-probability for the randomized algorithms. Our work and step bounds grow\nonly logarithmically with $p$, making our algorithms truly scalable. We prove\nthat for a class of symmetric algorithms that includes ours, no better step or\nwork bound is possible.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 21:43:46 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Jayanti", "Siddhartha V.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "2003.01254", "submitter": "Yasamin Nazari", "authors": "Amartya Shankha Biswas, Michal Dory, Mohsen Ghaffari, Slobodan\n  Mitrovi\\'c, Yasamin Nazari", "title": "Massively Parallel Algorithms for Distance Approximation and Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, there has been increasing interest in\ndistributed/parallel algorithms for processing large-scale graphs. By now, we\nhave quite fast algorithms -- usually sublogarithmic-time and often\n$poly(\\log\\log n)$-time, or even faster -- for a number of fundamental graph\nproblems in the massively parallel computation (MPC) model. This model is a\nwidely-adopted theoretical abstraction of MapReduce style settings, where a\nnumber of machines communicate in an all-to-all manner to process large-scale\ndata. Contributing to this line of work on MPC graph algorithms, we present\n$poly(\\log k) \\in poly(\\log\\log n)$ round MPC algorithms for computing\n$O(k^{1+{o(1)}})$-spanners in the strongly sublinear regime of local memory. To\nthe best of our knowledge, these are the first sublogarithmic-time MPC\nalgorithms for spanner construction. As primary applications of our spanners,\nwe get two important implications, as follows:\n  -For the MPC setting, we get an $O(\\log^2\\log n)$-round algorithm for\n$O(\\log^{1+o(1)} n)$ approximation of all pairs shortest paths (APSP) in the\nnear-linear regime of local memory. To the best of our knowledge, this is the\nfirst sublogarithmic-time MPC algorithm for distance approximations.\n  -Our result above also extends to the Congested Clique model of distributed\ncomputing, with the same round complexity and approximation guarantee. This\ngives the first sub-logarithmic algorithm for approximating APSP in weighted\ngraphs in the Congested Clique model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 23:52:06 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 22:16:35 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 16:51:13 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 18:13:31 GMT"}, {"version": "v5", "created": "Mon, 1 Feb 2021 01:52:33 GMT"}, {"version": "v6", "created": "Mon, 28 Jun 2021 20:08:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Dory", "Michal", ""], ["Ghaffari", "Mohsen", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Nazari", "Yasamin", ""]]}, {"id": "2003.01430", "submitter": "Federico Altieri", "authors": "Federico Altieri, Andrea Pietracaprina, Geppino Pucci, Fabio Vandin", "title": "Scalable Distributed Approximation of Internal Measures for Clustering\n  Evaluation", "comments": "16 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used internal measure for clustering evaluation is the\nsilhouette coefficient, whose naive computation requires a quadratic number of\ndistance calculations, which is clearly unfeasible for massive datasets.\nSurprisingly, there are no known general methods to efficiently approximate the\nsilhouette coefficient of a clustering with rigorously provable high accuracy.\nIn this paper, we present the first scalable algorithm to compute such a\nrigorous approximation for the evaluation of clusterings based on any metric\ndistances. Our algorithm hinges on a Probability Proportional to Size (PPS)\nsampling scheme, and, for any fixed $\\varepsilon, \\delta \\in (0,1)$, it\napproximates the silhouette coefficient within a mere additive error\n$O(\\varepsilon)$ with probability $1-\\delta$, using a very small number of\ndistance calculations. We also prove that the algorithm can be adapted to\nobtain rigorous approximations of other internal measures of clustering\nquality, such as cohesion and separation. Importantly, we provide a distributed\nimplementation of the algorithm using the MapReduce model, which runs in\nconstant rounds and requires only sublinear local space at each worker, which\nmakes our estimation approach applicable to big data scenarios. We perform an\nextensive experimental evaluation of our silhouette approximation algorithm,\ncomparing its performance to a number of baseline heuristics on real and\nsynthetic datasets. The experiments provide evidence that, unlike other\nheuristics, our estimation strategy not only provides tight theoretical\nguarantees but is also able to return highly accurate estimations while running\nin a fraction of the time required by the exact computation, and that its\ndistributed implementation is highly scalable, thus enabling the computation of\ninternal measures for very large datasets for which the exact computation is\nprohibitive.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 10:28:14 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 15:56:46 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 09:34:20 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Altieri", "Federico", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Vandin", "Fabio", ""]]}, {"id": "2003.01703", "submitter": "Jonathan Schneider", "authors": "Allen Liu, Renato Paes Leme, Jon Schneider", "title": "Optimal Contextual Pricing and Extensions", "comments": "Added note on optimality of results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the contextual pricing problem a seller repeatedly obtains products\ndescribed by an adversarially chosen feature vector in $\\mathbb{R}^d$ and only\nobserves the purchasing decisions of a buyer with a fixed but unknown linear\nvaluation over the products. The regret measures the difference between the\nrevenue the seller could have obtained knowing the buyer valuation and what can\nbe obtained by the learning algorithm.\n  We give a poly-time algorithm for contextual pricing with $O(d \\log \\log T +\nd \\log d)$ regret which matches the $\\Omega(d \\log \\log T)$ lower bound up to\nthe $d \\log d$ additive factor. If we replace pricing loss by the symmetric\nloss, we obtain an algorithm with nearly optimal regret of $O(d \\log d)$\nmatching the $\\Omega(d)$ lower bound up to $\\log d$. These algorithms are based\non a novel technique of bounding the value of the Steiner polynomial of a\nconvex region at various scales. The Steiner polynomial is a degree $d$\npolynomial with intrinsic volumes as the coefficients.\n  We also study a generalized version of contextual search where the hidden\nlinear function over the Euclidean space is replaced by a hidden function $f :\n\\mathcal{X} \\rightarrow \\mathcal{Y}$ in a certain hypothesis class\n$\\mathcal{H}$. We provide a generic algorithm with $O(d^2)$ regret where $d$ is\nthe covering dimension of this class. This leads in particular to a\n$\\tilde{O}(s^2)$ regret algorithm for linear contextual search if the linear\nfunction is guaranteed to be $s$-sparse. Finally we also extend our results to\nthe noisy feedback model, where each round our feedback is flipped with a fixed\nprobability $p < 1/2$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 18:46:29 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 18:49:34 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 23:33:43 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Liu", "Allen", ""], ["Leme", "Renato Paes", ""], ["Schneider", "Jon", ""]]}, {"id": "2003.01853", "submitter": "Geon Lee", "authors": "Geon Lee, Jihoon Ko, Kijung Shin", "title": "Hypergraph Motifs: Concepts, Algorithms, and Discoveries", "comments": "to be published in the 46th International Conference on Very Large\n  Data Bases (VLDB '20)", "journal-ref": null, "doi": "10.14778/3407790.3407823", "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs naturally represent group interactions, which are omnipresent in\nmany domains: collaborations of researchers, co-purchases of items, joint\ninteractions of proteins, to name a few. In this work, we propose tools for\nanswering the following questions in a systematic manner: (Q1) what are\nstructural design principles of real-world hypergraphs? (Q2) how can we compare\nlocal structures of hypergraphs of different sizes? (Q3) how can we identify\ndomains which hypergraphs are from? We first define hypergraph motifs\n(h-motifs), which describe the connectivity patterns of three connected\nhyperedges. Then, we define the significance of each h-motif in a hypergraph as\nits occurrences relative to those in properly randomized hypergraphs. Lastly,\nwe define the characteristic profile (CP) as the vector of the normalized\nsignificance of every h-motif. Regarding Q1, we find that h-motifs' occurrences\nin 11 real-world hypergraphs from 5 domains are clearly distinguished from\nthose of randomized hypergraphs. In addition, we demonstrate that CPs capture\nlocal structural patterns unique to each domain, and thus comparing CPs of\nhypergraphs addresses Q2 and Q3. Our algorithmic contribution is to propose\nMoCHy, a family of parallel algorithms for counting h-motifs' occurrences in a\nhypergraph. We theoretically analyze their speed and accuracy, and we show\nempirically that the advanced approximate version MoCHy-A+ is up to 25X more\naccurate and 32X faster than the basic approximate and exact versions,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 01:40:20 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:27:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lee", "Geon", ""], ["Ko", "Jihoon", ""], ["Shin", "Kijung", ""]]}, {"id": "2003.01902", "submitter": "James Aspnes", "authors": "James Aspnes", "title": "Notes on Randomized Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Lecture notes for the Yale Computer Science course CPSC 469/569 Randomized\nAlgorithms. Suitable for use as a supplementary text for an introductory\ngraduate or advanced undergraduate course on randomized algorithms. Discusses\ntools from probability theory, including random variables and expectations,\nunion bound arguments, concentration bounds, applications of martingales and\nMarkov chains, and the Lov\\'asz Local Lemma. Algorithmic topics include\nanalysis of classic randomized algorithms such as Quicksort and Hoare's FIND,\nrandomized tree data structures, hashing, Markov chain Monte Carlo sampling,\nrandomized approximate counting, derandomization, quantum computing, and some\nexamples of randomized distributed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 05:41:34 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Aspnes", "James", ""]]}, {"id": "2003.01937", "submitter": "Alexander Prolubnikov", "authors": "Alexander Prolubnikov", "title": "The interval greedy algorithm for discrete optimization problems with\n  interval objective function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wide class of the discrete optimization problems with interval\nobjective function. We give a generalization of the greedy algorithm for the\nproblems. Using the algorithm, we obtain the set of all possible greedy\nsolutions and the set of all possible values of the objective function for the\nsolutions. For a given probability distribution on intervals of objective\nfunction' coefficients, we compute probabilities of the solutions, compute\nexpected values of the objective function for them and other probabilistic\ncharacteristics of the problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:04:53 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 17:41:29 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 06:47:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Prolubnikov", "Alexander", ""]]}, {"id": "2003.02016", "submitter": "Stav Ben-Nun", "authors": "Stav Ben-Nun, Shay Golan, Tomasz Kociumaka, Matan Kraus", "title": "Time-Space Tradeoffs for Finding a Long Common Substring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding, given two documents of total length $n$,\na longest string occurring as a substring of both documents. This problem,\nknown as the Longest Common Substring (LCS) problem, has a classic $O(n)$-time\nsolution dating back to the discovery of suffix trees (Weiner, 1973) and their\nefficient construction for integer alphabets (Farach-Colton, 1997). However,\nthese solutions require $\\Theta(n)$ space, which is prohibitive in many\napplications. To address this issue, Starikovskaya and Vildh{\\o}j (CPM 2013)\nshowed that for $n^{2/3} \\le s \\le n^{1-o(1)}$, the LCS problem can be solved\nin $O(s)$ space and $O(\\frac{n^2}{s})$ time. Kociumaka et al. (ESA 2014)\ngeneralized this tradeoff to $1 \\leq s \\leq n$, thus providing a smooth\ntime-space tradeoff from constant to linear space. In this paper, we obtain a\nsignificant speed-up for instances where the length $L$ of the sought LCS is\nlarge. For $1 \\leq s \\leq n$, we show that the LCS problem can be solved in\n$O(s)$ space and $\\tilde{O}(\\frac{n^2}{L\\cdot s}+n)$ time. The result is based\non techniques originating from the LCS with Mismatches problem (Flouri et al.,\n2015; Charalampopoulos et al., CPM 2018), on space-efficient locally consistent\nparsing (Birenzwige et al., SODA 2020), and on the structure of maximal\nrepetitions (runs) in the input documents.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 11:48:05 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:16:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ben-Nun", "Stav", ""], ["Golan", "Shay", ""], ["Kociumaka", "Tomasz", ""], ["Kraus", "Matan", ""]]}, {"id": "2003.02144", "submitter": "Adam Polak", "authors": "Antonios Antoniadis and Christian Coester and Marek Elias and Adam\n  Polak and Bertrand Simon", "title": "Online metric algorithms with untrusted predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learned predictors, although achieving very good results for inputs\nresembling training data, cannot possibly provide perfect predictions in all\nsituations. Still, decision-making systems that are based on such predictors\nneed not only to benefit from good predictions but also to achieve a decent\nperformance when the predictions are inadequate. In this paper, we propose a\nprediction setup for arbitrary metrical task systems (MTS) (e.g., caching,\nk-server and convex body chasing) and online matching on the line. We utilize\nresults from the theory of online algorithms to show how to make the setup\nrobust. Specifically for caching, we present an algorithm whose performance, as\na function of the prediction error, is exponentially better than what is\nachievable for general MTS. Finally, we present an empirical evaluation of our\nmethods on real world datasets, which suggests practicality.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 15:48:37 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 13:10:29 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Coester", "Christian", ""], ["Elias", "Marek", ""], ["Polak", "Adam", ""], ["Simon", "Bertrand", ""]]}, {"id": "2003.02161", "submitter": "Grigorios Koumoutsos", "authors": "Dimitris Fotakis, Loukas Kavouras, Grigorios Koumoutsos, Stratis\n  Skoulakis and Manolis Vardas", "title": "The Online Min-Sum Set Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online Min-Sum Set Cover (MSSC), a natural and intriguing\ngeneralization of the classical list update problem. In Online MSSC, the\nalgorithm maintains a permutation on $n$ elements based on subsets $S_1, S_2,\n\\ldots$ arriving online. The algorithm serves each set $S_t$ upon arrival,\nusing its current permutation $\\pi_{t}$, incurring an access cost equal to the\nposition of the first element of $S_t$ in $\\pi_{t}$. Then, the algorithm may\nupdate its permutation to $\\pi_{t+1}$, incurring a moving cost equal to the\nKendall tau distance of $\\pi_{t}$ to $\\pi_{t+1}$. The objective is to minimize\nthe total access and moving cost for serving the entire sequence. We consider\nthe $r$-uniform version, where each $S_t$ has cardinality $r$. List update is\nthe special case where $r = 1$.\n  We obtain tight bounds on the competitive ratio of deterministic online\nalgorithms for MSSC against a static adversary, that serves the entire sequence\nby a single permutation. First, we show a lower bound of\n$(r+1)(1-\\frac{r}{n+1})$ on the competitive ratio. Then, we consider several\nnatural generalizations of successful list update algorithms and show that they\nfail to achieve any interesting competitive guarantee. On the positive side, we\nobtain a $O(r)$-competitive deterministic algorithm using ideas from online\nlearning and the multiplicative weight updates (MWU) algorithm.\n  Furthermore, we consider efficient algorithms. We propose a memoryless online\nalgorithm, called Move-All-Equally, which is inspired by the Double Coverage\nalgorithm for the $k$-server problem. We show that its competitive ratio is\n$\\Omega(r^2)$ and $2^{O(\\sqrt{\\log n \\cdot \\log r})}$, and conjecture that it\nis $f(r)$-competitive. We also compare Move-All-Equally against the dynamic\noptimal solution and obtain (almost) tight bounds by showing that it is\n$\\Omega(r \\sqrt{n})$ and $O(r^{3/2} \\sqrt{n})$-competitive.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 16:16:07 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kavouras", "Loukas", ""], ["Koumoutsos", "Grigorios", ""], ["Skoulakis", "Stratis", ""], ["Vardas", "Manolis", ""]]}, {"id": "2003.02169", "submitter": "Pedro Mirabal", "authors": "Pedro Mirabal, Jos\\'e Abreu, Oscar Pedreira", "title": "Pivot Selection for Median String Problem", "comments": null, "journal-ref": null, "doi": "10.1109/SCCC49216.2019.8966441", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Median String Problem is W[1]-Hard under the Levenshtein distance, thus,\napproximation heuristics are used. Perturbation-based heuristics have been\nproved to be very competitive as regards the ratio approximation\naccuracy/convergence speed. However, the computational burden increase with the\nsize of the set. In this paper, we explore the idea of reducing the size of the\nproblem by selecting a subset of representative elements, i.e. pivots, that are\nused to compute the approximate median instead of the whole set. We aim to\nreduce the computation time through a reduction of the problem size while\nachieving similar approximation accuracy. We explain how we find those pivots\nand how to compute the median string from them. Results on commonly used test\ndata suggest that our approach can reduce the computational requirements\n(measured in computed edit distances) by $8$\\% with approximation accuracy as\ngood as the state of the art heuristic.\n  This work has been supported in part by CONICYT-PCHA/Doctorado\nNacional/$2014-63140074$ through a Ph.D. Scholarship; Universidad Cat\\'{o}lica\nde la Sant\\'{i}sima Concepci\\'{o}n through the research project DIN-01/2016;\nEuropean Union's Horizon 2020 under the Marie Sk\\l odowska-Curie grant\nagreement $690941$; Millennium Institute for Foundational Research on Data\n(IMFD); FONDECYT-CONICYT grant number $1170497$; and for O. Pedreira, Xunta de\nGalicia/FEDER-UE refs. CSI ED431G/01 and GRC: ED431C 2017/58.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 16:25:05 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Mirabal", "Pedro", ""], ["Abreu", "Jos\u00e9", ""], ["Pedreira", "Oscar", ""]]}, {"id": "2003.02187", "submitter": "Du\\v{s}an Knop", "authors": "Du\\v{s}an Knop, Martin Kouteck\\'y", "title": "Scheduling Kernels via Configuration LP", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makespan minimization (on parallel identical or unrelated machines) is\narguably the most natural and studied scheduling problem. A common approach in\npractical algorithm design is to reduce the size of a given instance by a fast\npreprocessing step while being able to recover key information even after this\nreduction. This notion is formally studied as kernelization (or simply, kernel)\n-- a polynomial time procedure which yields an equivalent instance whose size\nis bounded in terms of some given parameter. It follows from known results that\nmakespan minimization parameterized by the longest job processing time\n$p_{\\max}$ has a kernelization yielding a reduced instance whose size is\nexponential in $p_{\\max}$. Can this be reduced to polynomial in $p_{\\max}$?\n  We answer this affirmatively not only for makespan minimization, but also for\nthe (more complicated) objective of minimizing the weighted sum of completion\ntimes, also in the setting of unrelated machines when the number of machine\nkinds is a parameter. Our algorithm first solves the Configuration LP and based\non its solution constructs a solution of an intermediate problem, called huge\n$N$-fold integer programming. This solution is further reduced in size by a\nseries of steps, until its encoding length is polynomial in the parameters.\nThen, we show that huge $N$-fold IP is in NP, which implies that there is a\npolynomial reduction back to our scheduling problem, yielding a kernel.\n  Our technique is highly novel in the context of kernelization, and our\nstructural theorem about the Configuration LP is of independent interest.\nMoreover, we show a polynomial kernel for huge $N$-fold IP conditional on\nwhether the so-called separation subproblem can be solved in polynomial time.\nConsidering that integer programming does not admit polynomial kernels except\nfor quite restricted cases, our \"conditional kernel\" provides new insight.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:01:16 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""]]}, {"id": "2003.02200", "submitter": "Andres Jesus Sanchez Fernandez", "authors": "A. J. Sanchez-Fernandez, L. F. Romero, G. Bandera and S. Tabik", "title": "A data relocation approach for terrain surface analysis on multi-GPU\n  systems: a case study on the total viewshed problem", "comments": null, "journal-ref": null, "doi": "10.1080/13658816.2020.1844207", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Elevation Models (DEMs) are important datasets for modelling the line\nof sight, such as radio signals, sound waves and human vision. These are\ncommonly analyzed using rotational sweep algorithms. However, such algorithms\nrequire large numbers of memory accesses to 2D arrays which, despite being\nregular, result in poor data locality in memory. Here, we propose a new\nmethodology called skewed Digital Elevation Model (sDEM), which substantially\nimproves the locality of memory accesses and increases the inherent parallelism\ninvolved in the computation of rotational sweep-based algorithms. In\nparticular, sDEM applies a data restructuring technique before accessing the\nmemory and performing the computation. To demonstrate the high efficiency of\nsDEM, we use the problem of total viewshed computation as a case study\nconsidering different implementations for single-core, multi-core, single-GPU\nand multi-GPU platforms. We conducted two experiments to compare sDEM with (i)\nthe most commonly used geographic information systems (GIS) software and (ii)\nthe state-of-the-art algorithm. In the first experiment, sDEM is on average\n8.8x faster than current GIS software despite being able to consider only few\npoints because of their limitations. In the second experiment, sDEM is 827.3x\nfaster than the state-of-the-art algorithm in the best case.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:13:17 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 14:21:11 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Sanchez-Fernandez", "A. J.", ""], ["Romero", "L. F.", ""], ["Bandera", "G.", ""], ["Tabik", "S.", ""]]}, {"id": "2003.02291", "submitter": "Bryan Ford", "authors": "Bryan Ford, Philipp Jovanovic, Ewa Syta", "title": "Que Sera Consensus: Simple Asynchronous Agreement with Private Coins and\n  Threshold Logical Clocks", "comments": "6 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly held that asynchronous consensus is much more complex,\ndifficult, and costly than partially-synchronous algorithms, especially without\nusing common coins. This paper challenges that conventional wisdom with que\nsera consensus QSC, an approach to consensus that cleanly decomposes the\nagreement problem from that of network asynchrony. QSC uses only private coins\nand reaches consensus in $O(1)$ expected communication rounds. It relies on\n\"lock-step\" synchronous broadcast, but can run atop a threshold logical clock\n(TLC) algorithm to time and pace partially-reliable communication atop an\nunderlying asynchronous network. This combination is arguably simpler than\npartially-synchronous consensus approaches like (Multi-)Paxos or Raft with\nleader election, and is more robust to slow leaders or targeted network\ndenial-of-service attacks. The simplest formulations of QSC atop TLC incur\nexpected $O(n^2)$ messages and $O(n^4)$ bits per agreement, or $O(n^3)$ bits\nwith straightforward optimizations. An on-demand implementation, in which\nclients act as \"natural leaders\" to execute the protocol atop stateful servers\nthat merely implement passive key-value stores, can achieve $O(n^2)$ expected\ncommunication bits per client-driven agreement.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 19:06:58 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ford", "Bryan", ""], ["Jovanovic", "Philipp", ""], ["Syta", "Ewa", ""]]}, {"id": "2003.02302", "submitter": "Dennis Luxen", "authors": "Dennis Luxen", "title": "An Inverse Olympic Medal Tally Transformation for Optimal Lane-level\n  Road Network Path Traversal", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane-level traversal of (almost) arbitrary input paths is a common problem in\nthe mapping industry. This paper considers the problem of generating\n\\emph{feasible} and maximally convenient lane-level path traversals. The\npresented approach exploits a graph transformation of an input path which is\nsubsequentially explored by a multi-criteria search algorithm. This approach is\nable to yield paths traversals that are guaranteed to obey lane crossing rules\nwhenever possible, and minimize the number of legal, yet inconvenient lane\ncrossings along any given input path.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 19:32:13 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Luxen", "Dennis", ""]]}, {"id": "2003.02336", "submitter": "Lu\\'is M. S. Russo", "authors": "Lu\\'is M. S. Russo, Ana D. Correia, Gonzalo Navarro, Alexandre P.\n  Francisco", "title": "Approximating Optimal Bidirectional Macro Schemes", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lempel-Ziv is an easy-to-compute member of a wide family of so-called macro\nschemes; it restricts pointers to go in one direction only. Optimal\nbidirectional macro schemes are NP-complete to find, but they may provide much\nbetter compression on highly repetitive sequences. We consider the problem of\napproximating optimal bidirectional macro schemes. We describe a simulated\nannealing algorithm that usually converges quickly. Moreover, in some cases, we\nobtain bidirectional macro schemes that are provably a 2-approximation of the\noptimal. We test our algorithm on a number of artificial repetitive texts and\nverify that it is efficient in practice and outperforms Lempel-Ziv, sometimes\nby a wide margin.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 21:31:42 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Russo", "Lu\u00eds M. S.", ""], ["Correia", "Ana D.", ""], ["Navarro", "Gonzalo", ""], ["Francisco", "Alexandre P.", ""]]}, {"id": "2003.02433", "submitter": "Rudy Zhou", "authors": "Sungjin Im, Mahshid Montazer Qaem, Benjamin Moseley, Xiaorui Sun, Rudy\n  Zhou", "title": "Fast Noise Removal for $k$-Means Clustering", "comments": "Published in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers $k$-means clustering in the presence of noise. It is\nknown that $k$-means clustering is highly sensitive to noise, and thus noise\nshould be removed to obtain a quality solution. A popular formulation of this\nproblem is called $k$-means clustering with outliers. The goal of $k$-means\nclustering with outliers is to discard up to a specified number $z$ of points\nas noise/outliers and then find a $k$-means solution on the remaining data. The\nproblem has received significant attention, yet current algorithms with\ntheoretical guarantees suffer from either high running time or inherent loss in\nthe solution quality. The main contribution of this paper is two-fold. Firstly,\nwe develop a simple greedy algorithm that has provably strong worst case\nguarantees. The greedy algorithm adds a simple preprocessing step to remove\nnoise, which can be combined with any $k$-means clustering algorithm. This\nalgorithm gives the first pseudo-approximation-preserving reduction from\n$k$-means with outliers to $k$-means without outliers. Secondly, we show how to\nconstruct a coreset of size $O(k \\log n)$. When combined with our greedy\nalgorithm, we obtain a scalable, near linear time algorithm. The theoretical\ncontributions are verified experimentally by demonstrating that the algorithm\nquickly removes noise and obtains a high-quality clustering.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 05:04:10 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 04:26:07 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Im", "Sungjin", ""], ["Qaem", "Mahshid Montazer", ""], ["Moseley", "Benjamin", ""], ["Sun", "Xiaorui", ""], ["Zhou", "Rudy", ""]]}, {"id": "2003.02475", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Stefan Kratsch and Tom\\'a\\v{s} Masa\\v{r}\\'ik and Irene Muzi and Marcin\n  Pilipczuk and Manuel Sorge", "title": "Optimal Discretization is Fixed-parameter Tractable", "comments": "Accepted to ACM-SIAM Symposium on Discrete Algorithms (SODA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two disjoint sets $W_1$ and $W_2$ of points in the plane, the Optimal\nDiscretization problem asks for the minimum size of a family of horizontal and\nvertical lines that separate $W_1$ from $W_2$, that is, in every region into\nwhich the lines partition the plane there are either only points of $W_1$, or\nonly points of $W_2$, or the region is empty. Equivalently, Optimal\nDiscretization can be phrased as a task of discretizing continuous variables:\nwe would like to discretize the range of $x$-coordinates and the range of\n$y$-coordinates into as few segments as possible, maintaining that no pair of\npoints from $W_1 \\times W_2$ are projected onto the same pair of segments under\nthis discretization.\n  We provide a fixed-parameter algorithm for the problem, parameterized by the\nnumber of lines in the solution. Our algorithm works in time $2^{O(k^2 \\log k)}\nn^{O(1)}$, where $k$ is the bound on the number of lines to find and $n$ is the\nnumber of points in the input.\n  Our result answers in positive a question of Bonnet, Giannopolous, and Lampis\n[IPEC 2017] and of Froese (PhD thesis, 2018) and is in contrast with the known\nintractability of two closely related generalizations: the Rectangle Stabbing\nproblem and the generalization in which the selected lines are not required to\nbe axis-parallel.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:09:16 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 06:15:02 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Kratsch", "Stefan", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Muzi", "Irene", ""], ["Pilipczuk", "Marcin", ""], ["Sorge", "Manuel", ""]]}, {"id": "2003.02483", "submitter": "Matthias Mnich", "authors": "Alexander G\\\"oke, D\\'aniel Marx, Matthias Mnich", "title": "Parameterized Algorithms for Generalizations of Directed Feedback Vertex\n  Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Directed Feedback Vertex Set (DFVS) problem takes as input a directed\ngraph~$G$ and seeks a smallest vertex set~$S$ that hits all cycles in $G$. This\nis one of Karp's 21 $\\mathsf{NP}$-complete problems. Resolving the\nparameterized complexity status of DFVS was a long-standing open problem until\nChen et al. [STOC 2008, J. ACM 2008] showed its fixed-parameter tractability\nvia a $4^kk! n^{\\mathcal{O}(1)}$-time algorithm, where $k = |S|$.\n  Here we show fixed-parameter tractability of two generalizations of DFVS:\n  - Find a smallest vertex set $S$ such that every strong component of $G - S$\nhas size at most~$s$: we give an algorithm solving this problem in time\n$4^k(ks+k+s)!\\cdot n^{\\mathcal{O}(1)}$. This generalizes an algorithm by Xiao\n[JCSS 2017] for the undirected version of the problem.\n  - Find a smallest vertex set $S$ such that every non-trivial strong component\nof $G - S$ is 1-out-regular: we give an algorithm solving this problem in time\n$2^{\\mathcal{O}(k^3)}\\cdot n^{\\mathcal{O}(1)}$.\n  We also solve the corresponding arc versions of these problems by\nfixed-parameter algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:47:13 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["G\u00f6ke", "Alexander", ""], ["Marx", "D\u00e1niel", ""], ["Mnich", "Matthias", ""]]}, {"id": "2003.02513", "submitter": "Xiaocheng Li", "authors": "Xiaocheng Li, Chunlin Sun, Yinyu Ye", "title": "Simple and Fast Algorithm for Binary Integer and Online Linear\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a simple and fast online algorithm for solving a\nclass of binary integer linear programs (LPs) arisen in general resource\nallocation problem. The algorithm requires only one single pass through the\ninput data and is free of doing any matrix inversion. It can be viewed as both\nan approximate algorithm for solving binary integer LPs and a fast algorithm\nfor solving online LP problems. The algorithm is inspired by an equivalent form\nof the dual problem of the relaxed LP and it essentially performs (one-pass)\nprojected stochastic subgradient descent in the dual space. We analyze the\nalgorithm in two different models, stochastic input and random permutation,\nwith minimal technical assumptions on the input data. The algorithm achieves\n$O\\left(m \\sqrt{n}\\right)$ expected regret under the stochastic input model and\n$O\\left((m+\\log n)\\sqrt{n}\\right)$ expected regret under the random permutation\nmodel, and it achieves $O(m \\sqrt{n})$ expected constraint violation under both\nmodels, where $n$ is the number of decision variables and $m$ is the number of\nconstraints. The algorithm enjoys the same performance guarantee when\ngeneralized to a multi-dimensional LP setting which covers a wider range of\napplications. In addition, we employ the notion of permutational Rademacher\ncomplexity and derive regret bounds for two earlier online LP algorithms for\ncomparison. Both algorithms improve the regret bound with a factor of\n$\\sqrt{m}$ by paying more computational cost. Furthermore, we demonstrate how\nto convert the possibly infeasible solution to a feasible one through a\nrandomized procedure. Numerical experiments illustrate the general\napplicability and effectiveness of the algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 10:12:43 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 05:59:11 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 18:52:32 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Xiaocheng", ""], ["Sun", "Chunlin", ""], ["Ye", "Yinyu", ""]]}, {"id": "2003.02518", "submitter": "V\\'aclav Rozho\\v{n}", "authors": "V\\'aclav Rozho\\v{n}", "title": "Simple and sharp analysis of k-means||", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple analysis of k-means|| (Bahmani et al., PVLDB 2012) -- a\ndistributed variant of the k-means++ algorithm (Arthur and Vassilvitskii, SODA\n2007). Moreover, the bound on the number of rounds is improved from $O(\\log n)$\nto $O(\\log n / \\log\\log n)$, which we show to be tight.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 10:18:48 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 13:34:45 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2003.02583", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Nicolas Grelier, Tillmann Miltzow", "title": "Maximum Clique in Disk-Like Intersection Graphs", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of Maximum Clique in intersection graphs of convex\nobjects in the plane. On the algorithmic side, we extend the polynomial-time\nalgorithm for unit disks [Clark '90, Raghavan and Spinrad '03] to translates of\nany fixed convex set. We also generalize the efficient polynomial-time\napproximation scheme (EPTAS) and subexponential algorithm for disks [Bonnet et\nal. '18, Bonamy et al. '18] to homothets of a fixed centrally symmetric convex\nset. The main open question on that topic is the complexity of Maximum Clique\nin disk graphs. It is not known whether this problem is NP-hard. We observe\nthat, so far, all the hardness proofs for Maximum Clique in intersection graph\nclasses $\\mathcal I$ follow the same road. They show that, for every graph $G$\nof a large-enough class $\\mathcal C$, the complement of an even subdivision of\n$G$ belongs to the intersection class $\\mathcal I$. Then they conclude invoking\nthe hardness of Maximum Independent Set on the class $\\mathcal C$, and the fact\nthat the even subdivision preserves that hardness. However there is a strong\nevidence that this approach cannot work for disk graphs [Bonnet et al. '18]. We\nsuggest a new approach, based on a problem that we dub Max Interval Permutation\nAvoidance, which we prove unlikely to have a subexponential-time approximation\nscheme. We transfer that hardness to Maximum Clique in intersection graphs of\nobjects which can be either half-planes (or unit disks) or axis-parallel\nrectangles. That problem is not amenable to the previous approach. We hope that\na scaled down (merely NP-hard) variant of Max Interval Permutation Avoidance\ncould help making progress on the disk case, for instance by showing the\nNP-hardness for (convex) pseudo-disks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:56:26 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Grelier", "Nicolas", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "2003.02605", "submitter": "Stefan Neumann", "authors": "Monika Henzinger, Stefan Neumann, Andreas Wiese", "title": "Dynamic Approximate Maximum Independent Set of Intervals, Hypercubes and\n  Hyperrectangles", "comments": "The conference version of this paper will appear at the Symposium on\n  Computational Geometry (SoCG) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent set is a fundamental problem in combinatorial optimization. While\nin general graphs the problem is essentially inapproximable, for many important\ngraph classes there are approximation algorithms known in the offline setting.\nThese graph classes include interval graphs and geometric intersection graphs,\nwhere vertices correspond to intervals/geometric objects and an edge indicates\nthat the two corresponding objects intersect.\n  We present dynamic approximation algorithms for independent set of intervals,\nhypercubes and hyperrectangles in $d$ dimensions. They work in the fully\ndynamic model where each update inserts or deletes a geometric object. All our\nalgorithms are deterministic and have worst-case update times that are\npolylogarithmic for constant $d$ and $\\epsilon> 0$, assuming that the\ncoordinates of all input objects are in $[0, N]^d$ and each of their edges has\nlength at least 1. We obtain the following results:\n  $\\bullet$ For weighted intervals, we maintain a $(1+\\epsilon)$-approximate\nsolution.\n  $\\bullet$ For $d$-dimensional hypercubes we maintain a\n$(1+\\epsilon)2^{d}$-approximate solution in the unweighted case and a\n$O(2^{d})$-approximate solution in the weighted case. Also, we show that for\nmaintaining an unweighted $(1+\\epsilon)$-approximate solution one needs\npolynomial update time for $d\\ge2$ if the ETH holds.\n  $\\bullet$ For weighted $d$-dimensional hyperrectangles we present a dynamic\nalgorithm with approximation ratio $(1+\\epsilon)\\log^{d-1}N$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 13:33:40 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Henzinger", "Monika", ""], ["Neumann", "Stefan", ""], ["Wiese", "Andreas", ""]]}, {"id": "2003.02725", "submitter": "Svante Janson", "authors": "Svante Janson", "title": "Central limit theorems for additive functionals and fringe trees in\n  tries", "comments": "74 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give general theorems on asymptotic normality for additive functionals of\nrandom tries generated by a sequence of independent strings. These theorems are\napplied to show asymptotic normality of the distribution of random fringe trees\nin a random trie. Formulas for asymptotic mean and variance are given. In\nparticular, the proportion of fringe trees of size $k$ (defined as number of\nkeys) is asymptotically, ignoring oscillations, $c/(k(k-1))$ for $k\\ge2$, where\n$c=1/(1+H)$ with $H$ the entropy of the digits. Another application gives\nasymptotic normality of the number of $k$-protected nodes in a random trie. For\nsymmetric tries, it is shown that the asymptotic proportion of $k$-protected\nnodes (ignoring oscillations) decreases geometrically as $k\\to\\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 15:53:55 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Janson", "Svante", ""]]}, {"id": "2003.02808", "submitter": "Joseph Vargovich", "authors": "Toby Hocking, Joseph Vargovich", "title": "Linear time dynamic programming for the exact path of optimal models\n  selected from a finite set", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning algorithms are formulated in terms of finding model parameters\nwhich minimize a data-fitting loss function plus a regularizer. When the\nregularizer involves the l0 pseudo-norm, the resulting regularization path\nconsists of a finite set of models. The fastest existing algorithm for\ncomputing the breakpoints in the regularization path is quadratic in the number\nof models, so it scales poorly to high dimensional problems. We provide new\nformal proofs that a dynamic programming algorithm can be used to compute the\nbreakpoints in linear time. Empirical results on changepoint detection problems\ndemonstrate the improved accuracy and speed relative to grid search and the\nprevious quadratic time algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:16:58 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Hocking", "Toby", ""], ["Vargovich", "Joseph", ""]]}, {"id": "2003.02866", "submitter": "Jianer Chen", "authors": "Jianer Chen and Ying Guo and Qin Huang", "title": "Linear-Time Parameterized Algorithms with Limited Local Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new (theoretical) computational model for the study of massive\ndata processing with limited computational resources. Our model measures the\ncomplexity of reading the very large data sets in terms of the data size N and\nanalyzes the computational cost in terms of a parameter k that characterizes\nthe computational power provided by limited local computing resources. We\ndevelop new algorithmic techniques that implement algorithms for solving\nwell-known computational problems on the proposed model. In particular, we\npresent an algorithm that finds a k-matching in a general unweighted graph in\ntime O(N + k^{2.5}) and an algorithm that constructs a maximum weighted\nk-matching in a general weighted graph in time O(N + k^3 log k). Both\nalgorithms have their space complexity bounded by O(k^2).\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:04:57 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Jianer", ""], ["Guo", "Ying", ""], ["Huang", "Qin", ""]]}, {"id": "2003.02962", "submitter": "Daniel Kline", "authors": "Calin Chindris, Daniel Kline", "title": "Simultaneous robust subspace recovery and semi-stability of quiver\n  representations", "comments": "Minor typos/inaccuracies fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneously finding lower-dimensional subspace\nstructures in a given $m$-tuple of possibly corrupted, high-dimensional data\nsets all of the same size. We refer to this problem as simultaneous robust\nsubspace recovery (SRSR) and provide a quiver invariant theoretic approach to\nit. We show that SRSR is a particular case of the more general problem of\neffectively deciding whether a quiver representation is semi-stable (in the\nsense of Geometric Invariant Theory) and, in case it is not, finding a\nsubrepresentation certifying in an optimal way that the representation is not\nsemi-stable. In this paper, we show that SRSR and the more general quiver\nsemi-stability problem can be solved effectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 23:27:18 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 20:57:27 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 20:59:25 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chindris", "Calin", ""], ["Kline", "Daniel", ""]]}, {"id": "2003.02972", "submitter": "Cyrus Rashtchian", "authors": "Cyrus Rashtchian, Aneesh Sharma, David P. Woodruff", "title": "LSF-Join: Locality Sensitive Filtering for Distributed All-Pairs Set\n  Similarity Under Skew", "comments": "WWW (The Web Conference) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-pairs set similarity is a widely used data mining task, even for large\nand high-dimensional datasets. Traditionally, similarity search has focused on\ndiscovering very similar pairs, for which a variety of efficient algorithms are\nknown. However, recent work highlights the importance of finding pairs of sets\nwith relatively small intersection sizes. For example, in a recommender system,\ntwo users may be alike even though their interests only overlap on a small\npercentage of items. In such systems, some dimensions are often highly skewed\nbecause they are very popular. Together these two properties render previous\napproaches infeasible for large input sizes. To address this problem, we\npresent a new distributed algorithm, LSF-Join, for approximate all-pairs set\nsimilarity. The core of our algorithm is a randomized selection procedure based\non Locality Sensitive Filtering. Our method deviates from prior approximate\nalgorithms, which are based on Locality Sensitive Hashing. Theoretically, we\nshow that LSF-Join efficiently finds most close pairs, even for small\nsimilarity thresholds and for skewed input sets. We prove guarantees on the\ncommunication, work, and maximum load of LSF-Join, and we also experimentally\ndemonstrate its accuracy on multiple graphs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:06:20 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Rashtchian", "Cyrus", ""], ["Sharma", "Aneesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "2003.03058", "submitter": "Michal Dory", "authors": "Michal Dory, Merav Parter", "title": "Exponentially Faster Shortest Paths in the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved deterministic algorithms for approximating shortest paths\nin the Congested Clique model of distributed computing. We obtain\n$poly(\\log\\log n)$-round algorithms for the following problems in unweighted\nundirected $n$-vertex graphs:\n  -- $(1+\\epsilon)$-approximation of multi-source shortest paths (MSSP) from\n$O(\\sqrt{n})$ sources.\n  -- $(2+\\epsilon)$-approximation of all pairs shortest paths (APSP).\n  -- $(1+\\epsilon,\\beta)$-approximation of APSP where $\\beta=O(\\frac{\\log\\log\nn}{\\epsilon})^{\\log\\log n}$.\n  These bounds improve exponentially over the state-of-the-art poly-logarithmic\nbounds due to [Censor-Hillel et al., PODC19]. It also provides the first\nnearly-additive bounds for the APSP problem in sub-polynomial time. Our\napproach is based on distinguishing between short and long distances based on\nsome distance threshold $t = O(\\frac{\\beta}{\\epsilon})$ where\n$\\beta=O(\\frac{\\log\\log n}{\\epsilon})^{\\log\\log n}$. Handling the long\ndistances is done by devising a new algorithm for computing sparse\n$(1+\\epsilon,\\beta)$ emulator with $O(n\\log\\log n)$ edges. For the short\ndistances, we provide distance-sensitive variants for the distance tool-kit of\n[Censor-Hillel et al., PODC19]. By exploiting the fact that this tool-kit\nshould be applied only on local balls of radius $t$, their round complexities\nget improved from $poly(\\log n)$ to $poly(\\log t)$.\n  Finally, our deterministic solutions for these problems are based on a\nderandomization scheme of a novel variant of the hitting set problem, which\nmight be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 07:13:41 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Dory", "Michal", ""], ["Parter", "Merav", ""]]}, {"id": "2003.03108", "submitter": "Paloma Thome De Lima", "authors": "Paloma T. Lima, Erik Jan van Leeuwen, and Marieke van der Wegen", "title": "Algorithms for the rainbow vertex coloring problem on graph classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex-colored graph, we say a path is a rainbow vertex path if all\nits internal vertices have distinct colors. The graph is rainbow\nvertex-connected if there is a rainbow vertex path between every pair of its\nvertices. In the Rainbow Vertex Coloring (RVC) problem we want to decide\nwhether the vertices of a given graph can be colored with at most $k$ colors so\nthat the graph becomes rainbow vertex-connected. This problem is known to be\nNP-complete even in very restricted scenarios, and very few efficient\nalgorithms are known for it. In this work, we give polynomial-time algorithms\nfor RVC on permutation graphs, powers of trees and split strongly chordal\ngraphs. The algorithm for the latter class also works for the strong variant of\nthe problem, where the rainbow vertex paths between each vertex pair must be\nshortest paths. We complement the polynomial-time solvability results for split\nstrongly chordal graphs by showing that, for any fixed $p\\geq 3$ both variants\nof the problem become NP-complete when restricted to split\n$(S_3,\\ldots,S_p)$-free graphs, where $S_q$ denotes the $q$-sun graph.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 09:59:09 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 09:47:37 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lima", "Paloma T.", ""], ["van Leeuwen", "Erik Jan", ""], ["van der Wegen", "Marieke", ""]]}, {"id": "2003.03222", "submitter": "Gabriele Fici", "authors": "P\\'eter Burcsi and Gabriele Fici and Zsuzsanna Lipt\\'ak and Rajeev\n  Raman and Joe Sawada", "title": "Generating a Gray code for prefix normal words in amortized\n  polylogarithmic time per word", "comments": "To appear in Theoretical Computer Science. arXiv admin note: text\n  overlap with arXiv:1401.6346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prefix normal word is a binary word with the property that no substring has\nmore $1$s than the prefix of the same length. By proving that the set of prefix\nnormal words is a bubble language, we can exhaustively list all prefix normal\nwords of length $n$ as a combinatorial Gray code, where successive strings\ndiffer by at most two swaps or bit flips. This Gray code can be generated in\n$\\Oh(\\log^2 n)$ amortized time per word, while the best generation algorithm\nhitherto has $\\Oh(n)$ running time per word. We also present a membership\ntester for prefix normal words, as well as a novel characterization of bubble\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 07:43:53 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 07:17:47 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Burcsi", "P\u00e9ter", ""], ["Fici", "Gabriele", ""], ["Lipt\u00e1k", "Zsuzsanna", ""], ["Raman", "Rajeev", ""], ["Sawada", "Joe", ""]]}, {"id": "2003.03258", "submitter": "Llu\\'is Alemany-Puig", "authors": "Llu\\'is Alemany-Puig and Ramon Ferrer-i-Cancho", "title": "Fast calculation of the variance of edge crossings", "comments": "Better connection with graph theory (crossing number). Introduction\n  and discussion substantially rewritten. Minor corrections in other parts of\n  the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crossing number, i.e. the minimum number of edge crossings arising when\ndrawing a graph on a certain surface, is a very important problem of graph\ntheory. The opposite problem, i.e. the maximum crossing number, is receiving\ngrowing attention. Here we consider a complementary problem of the distribution\nof the number of edge crossings, namely the variance of the number of\ncrossings, when embedding the vertices of an arbitrary graph in some space at\nrandom. In his pioneering research, Moon derived that variance on random linear\narrangements of complete unipartite and bipartite graphs. Given the need of\nefficient algorithms to support this sort of research and given also the\ngrowing interest of the number of edge crossings in spatial networks, networks\nwhere vertices are embedded in some space, here we derive algorithms to\ncalculate the variance in arbitrary graphs in $o(nm^2)$-time, and in forests in\n$O(n)$-time. These algorithms work on a wide range of random layouts (not only\non Moon's) and are based on novel arithmetic expressions for the calculation of\nthe variance that we develop from previous theoretical work. This paves the way\nfor many applications that rely on a fast but exact calculation of the\nvariance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:55:28 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:21:56 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Alemany-Puig", "Llu\u00eds", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "2003.03595", "submitter": "Petteri Kaski", "authors": "Andreas Bj\\\"orklund, Petteri Kaski", "title": "The Fine-Grained Complexity of Computing the Tutte Polynomial of a\n  Linear Matroid", "comments": "This version adds Theorem 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that computing the Tutte polynomial of a linear matroid of dimension\n$k$ on $k^{O(1)}$ points over a field of $k^{O(1)}$ elements requires\n$k^{\\Omega(k)}$ time unless the \\#ETH---a counting extension of the Exponential\nTime Hypothesis of Impagliazzo and Paturi [CCC 1999] due to Dell {\\em et al.}\n[ACM TALG 2014]---is false. This holds also for linear matroids that admit a\nrepresentation where every point is associated to a vector with at most two\nnonzero coordinates. We also show that the same is true for computing the Tutte\npolynomial of a binary matroid of dimension $k$ on $k^{O(1)}$ points with at\nmost three nonzero coordinates in each point's vector. This is in sharp\ncontrast to computing the Tutte polynomial of a $k$-vertex graph (that is, the\nTutte polynomial of a {\\em graphic} matroid of dimension $k$---which is\nrepresentable in dimension $k$ over the binary field so that every vector has\ntwo nonzero coordinates), which is known to be computable in $2^k k^{O(1)}$\ntime [Bj\\\"orklund {\\em et al.}, FOCS 2008]. Our lower-bound proofs proceed via\n(i) a connection due to Crapo and Rota [1970] between the number of tuples of\ncodewords of full support and the Tutte polynomial of the matroid associated\nwith the code; (ii) an earlier-established \\#ETH-hardness of counting the\nsolutions to a bipartite $(d,2)$-CSP on $n$ vertices in $d^{o(n)}$ time; and\n(iii) new embeddings of such CSP instances as questions about codewords of full\nsupport in a linear code. We complement these lower bounds with two algorithm\ndesigns. The first design computes the Tutte polynomial of a linear matroid of\ndimension~$k$ on $k^{O(1)}$ points in $k^{O(k)}$ operations. The second design\ngeneralizes the Bj\\\"orklund~{\\em et al.} algorithm and runs in\n$q^{k+1}k^{O(1)}$ time for linear matroids of dimension $k$ defined over the\n$q$-element field by $k^{O(1)}$ points with at most two nonzero coordinates\neach.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 16:04:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 09:35:56 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kaski", "Petteri", ""]]}, {"id": "2003.03605", "submitter": "Micha{\\l} Pilipczuk", "authors": "Jaroslav Ne\\v{s}et\\v{r}il, Patrice Ossona de Mendez, Micha{\\l}\n  Pilipczuk, Xuding Zhu", "title": "Clustering powers of sparse graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that if $G$ is a sparse graph --- it belongs to a fixed class of\nbounded expansion $\\mathcal{C}$ --- and $d\\in \\mathbb{N}$ is fixed, then the\n$d$th power of $G$ can be partitioned into cliques so that contracting each of\nthese clique to a single vertex again yields a sparse graph. This result has\nseveral graph-theoretic and algorithmic consequences for powers of sparse\ngraphs, including bounds on their subchromatic number and efficient\napproximation algorithms for the chromatic number and the clique number.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 16:46:33 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ne\u0161et\u0159il", "Jaroslav", ""], ["de Mendez", "Patrice Ossona", ""], ["Pilipczuk", "Micha\u0142", ""], ["Zhu", "Xuding", ""]]}, {"id": "2003.03632", "submitter": "Kiril Solovey", "authors": "Kiril Solovey", "title": "Complexity of Planning", "comments": "To appear as a chapter in Motion Planning, the Encyclopedia of\n  Robotics, Eds. Marcelo H. Ang Jr., Oussama Khatib, and Bruno Siciliano.\n  Section Ed. Lydia E. Kavraki. Springer Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a chapter in the Encyclopedia of Robotics. It is devoted to the study\nof complexity of complete (or exact) algorithms for robot motion planning. The\nterm ``complete'' indicates that an approach is guaranteed to find the correct\nsolution (a motion path or trajectory in our setting), or to report that none\nexists otherwise (in case that for instance, no feasible path exists).\nComplexity theory is a fundamental tool in computer science for analyzing the\nperformance of algorithms, in terms of the amount of resources they require.\n(While complexity can express different quantities such as space and\ncommunication effort, our focus in this chapter is on time complexity.)\nMoreover, complexity theory helps to identify ``hard'' problems which require\nexcessive amount of computation time to solve. In the context of motion\nplanning, complexity theory can come in handy in various ways, some of which\nare illustrated here.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:37:56 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 19:34:22 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Solovey", "Kiril", ""]]}, {"id": "2003.03801", "submitter": "Shangsen Li", "authors": "Shangsen Li, Lailong Luo, Deke Guo", "title": "Multiset Synchronization with Counting Cuckoo Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set synchronization is a fundamental task in distributed applications and\nimplementations. Existing methods that synchronize simple sets are mainly based\non compact data structures such as Bloom filter and its variants. However,\nthese methods are infeasible to synchronize a pair of multisets which allow an\nelement to appear for multiple times. To this end, in this paper, we propose to\nleverage the counting cuckoo filter (CCF), a novel variant of cuckoo filter, to\nrepresent and thereafter synchronize a pair of multisets. The cuckoo filter\n(CF) is a minimized hash table that uses cuckoo hashing to resolve collisions.\nCF has an array of buckets, each of which has multiple slots to store element\nfingerprints. Based on CF, CCF extends each slot as two fields, the fingerprint\nfield and the counter field. The fingerprint field records the fingerprint of\nelement which is stored by this slot; while the counter field counts the\nmultiplicity of the stored element. With such a design, CCF is competent to\nrepresent any multiset. After generating and exchanging the respective CCFs\nwhich represent the local multi-sets, we propose the query-based and the\ndecoding-based methods to identify the different elements between the given\nmultisets. The comprehensive evaluation results indicate that CCF outperforms\nthe counting Bloom filter (CBF) when they are used to synchronize multisets, in\nterms of both synchronization accuracy and the space-efficiency, at the cost of\na little higher time-consumption.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 15:38:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Li", "Shangsen", ""], ["Luo", "Lailong", ""], ["Guo", "Deke", ""]]}, {"id": "2003.03830", "submitter": "Feras Saad", "authors": "Feras A. Saad, Cameron E. Freer, Martin C. Rinard, Vikash K.\n  Mansinghka", "title": "The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for Discrete\n  Probability Distributions", "comments": "12 pages, 5 figures, 1 table. Appearing in AISTATS 2020", "journal-ref": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics, PMLR 108:1036-1046, 2020", "doi": null, "report-no": null, "categories": "stat.CO cs.DM cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new algorithm for the fundamental problem of\ngenerating a random integer from a discrete probability distribution using a\nsource of independent and unbiased random coin flips. We prove that this\nalgorithm, which we call the Fast Loaded Dice Roller (FLDR), is highly\nefficient in both space and time: (i) the size of the sampler is guaranteed to\nbe linear in the number of bits needed to encode the input distribution; and\n(ii) the expected number of bits of entropy it consumes per sample is at most 6\nbits more than the information-theoretically optimal rate. We present fast\nimplementations of the linear-time preprocessing and near-optimal sampling\nalgorithms using unsigned integer arithmetic. Empirical evaluations on a broad\nset of probability distributions establish that FLDR is 2x-10x faster in both\npreprocessing and sampling than multiple baseline algorithms, including the\nwidely-used alias and interval samplers. It also uses up to 10000x less space\nthan the information-theoretically optimal sampler, at the expense of less than\n1.5x runtime overhead.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 19:17:08 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 15:37:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Saad", "Feras A.", ""], ["Freer", "Cameron E.", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2003.03892", "submitter": "Yihe Dong", "authors": "Yihe Dong, Will Sawin", "title": "COPT: Coordinated Optimal Transport for Graph Sketching", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce COPT, a novel distance metric between graphs defined via an\noptimization routine, computing a coordinated pair of optimal transport maps\nsimultaneously. This gives an unsupervised way to learn general-purpose graph\nrepresentation, applicable to both graph sketching and graph comparison. COPT\ninvolves simultaneously optimizing dual transport plans, one between the\nvertices of two graphs, and another between graph signal probability\ndistributions. We show theoretically that our method preserves important global\nstructural information on graphs, in particular spectral information, and\nanalyze connections to existing studies. Empirically, COPT outperforms state of\nthe art methods in graph classification on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:30:23 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 18:21:17 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dong", "Yihe", ""], ["Sawin", "Will", ""]]}, {"id": "2003.03959", "submitter": "Andrew Frohmader", "authors": "Andrew Frohmader", "title": "Adaptive Fibonacci and Pairing Heaps", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief note presents two adaptive heap data structures and conjectures on\nrunning times.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 07:55:06 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Frohmader", "Andrew", ""]]}, {"id": "2003.04101", "submitter": "Sven Kosub", "authors": "Stefan Eckhardt, Sven Kosub, Johannes Nowak", "title": "Smoothed Analysis of Trie Height by Star-like PFAs", "comments": "31 pages; slightly updated version of local technical report\n  TUM-I0715, Institut f\\\"ur Informatik, Technische Universit\\\"at M\\\"unchen,\n  July 2007", "journal-ref": null, "doi": null, "report-no": "TUM-I0715", "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tries are general purpose data structures for information retrieval. The most\nsignificant parameter of a trie is its height $H$ which equals the length of\nthe longest common prefix of any two string in the set $A$ over which the trie\nis built. Analytical investigations of random tries suggest that ${\\bf E}(H)\\in\nO(\\log(\\|A\\|))$, although $H$ is unbounded in the worst case. Moreover, sharp\nresults on the distribution function of $H$ are known for many different random\nstring sources. But because of the inherent weakness of the modeling behind\naverage-case analysis---analyses being dominated by random data---these results\ncan utterly explain the fact that in many practical situations the trie height\nis logarithmic. We propose a new semi-random string model and perform a\nsmoothed analysis in order to give a mathematically more rigorous explanation\nfor the practical findings. The perturbation functions which we consider are\nbased on probabilistic finite automata (PFA) and we show that the transition\nprobabilities of the representing PFA completely characterize the asymptotic\ngrowth of the smoothed trie height. Our main result is of dichotomous\nnature---logarithmic or unbounded---and is certainly not surprising at first\nglance, but we also give quantitative upper and lower bounds, which are derived\nusing multivariate generating function in order to express the computations of\nthe perturbing PFA. A direct consequence is the logarithmic trie height for\nedit perturbations(i.e., random insertions, deletions and substitutions).\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:55:36 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Eckhardt", "Stefan", ""], ["Kosub", "Sven", ""], ["Nowak", "Johannes", ""]]}, {"id": "2003.04254", "submitter": "Lars Jaffke", "authors": "Lars Jaffke, Paloma T. Lima, Daniel Lokshtanov", "title": "b-Coloring Parameterized by Clique-Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a polynomial-time algorithm for b-Coloring on graphs of constant\nclique-width. This unifies and extends previously known polynomial-time results\non several graph classes, and answers open questions posed by Campos and Silva\n[Algorithmica, 2018] and Bonomo et al. [Graphs Combin., 2009]. This constitutes\nthe first result concerning structural parameterizations of this problem. We\nshow that the problem is FPT when parameterized by the vertex cover number on\ngeneral graphs, and on chordal graphs when parameterized by the number of\ncolors. Additionally, we observe that our algorithm for graphs of bounded\nclique-width can be adapted to solve the Fall Coloring problem within the same\nruntime bound.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:58:43 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Jaffke", "Lars", ""], ["Lima", "Paloma T.", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "2003.04280", "submitter": "Gwena\\\"el Joret", "authors": "Vida Dujmovi\\'c, Louis Esperet, Gwena\\\"el Joret, Cyril Gavoille, Piotr\n  Micek, and Pat Morin", "title": "Adjacency Labelling for Planar Graphs (and Beyond)", "comments": "v4: referees' comments incorporated v3: minor changes v2: significant\n  revision v1: 35 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exists an adjacency labelling scheme for planar graphs\nwhere each vertex of an $n$-vertex planar graph $G$ is assigned a\n$(1+o(1))\\log_2 n$-bit label and the labels of two vertices $u$ and $v$ are\nsufficient to determine if $uv$ is an edge of $G$. This is optimal up to the\nlower order term and is the first such asymptotically optimal result. An\nalternative, but equivalent, interpretation of this result is that, for every\n$n$, there exists a graph $U_n$ with $n^{1+o(1)}$ vertices such that every\n$n$-vertex planar graph is an induced subgraph of $U_n$. These results\ngeneralize to bounded genus graphs, apex-minor-free graphs, bounded-degree\ngraphs from minor closed families, and $k$-planar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:39:55 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 17:03:15 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 19:41:42 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 19:39:41 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Dujmovi\u0107", "Vida", ""], ["Esperet", "Louis", ""], ["Joret", "Gwena\u00ebl", ""], ["Gavoille", "Cyril", ""], ["Micek", "Piotr", ""], ["Morin", "Pat", ""]]}, {"id": "2003.04578", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Pavel V. Smirnov", "title": "Optimal-size problem kernels for $d$-Hitting Set in linear time and\n  space", "comments": "More detailed algorithm descriptions, extended experimental section", "journal-ref": "Information Processing Letters 163:105998, 2020", "doi": "10.1016/j.ipl.2020.105998", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The known linear-time kernelizations for $d$-Hitting Set guarantee linear\nworst-case running times using a quadratic-size data structure (that is not\nfully initialized). Getting rid of this data structure, we show that problem\nkernels of asymptotically optimal size $O(k^d)$ for $d$-Hitting Set are\ncomputable in linear time and space. Additionally, we experimentally compare\nthe linear-time kernelizations for $d$-Hitting Set to each other and to a\nclassical data reduction algorithm due to Weihe.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:41:17 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 14:11:21 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Smirnov", "Pavel V.", ""]]}, {"id": "2003.04605", "submitter": "Danil Sagunov", "authors": "Ivan Bliznets and Danil Sagunov", "title": "Maximizing Happiness in Graphs of Bounded Clique-Width", "comments": "Accepted to LATIN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique-width is one of the most important parameters that describes\nstructural complexity of a graph. Probably, only treewidth is more studied\ngraph width parameter. In this paper we study how clique-width influences the\ncomplexity of the Maximum Happy Vertices (MHV) and Maximum Happy Edges (MHE)\nproblems. We answer a question of Choudhari and Reddy '18 about\nparameterization by the distance to threshold graphs by showing that MHE is\nNP-complete on threshold graphs. Hence, it is not even in XP when parameterized\nby clique-width, since threshold graphs have clique-width at most two. As a\ncomplement for this result we provide a $n^{\\mathcal{O}(\\ell \\cdot\n\\operatorname{cw})}$ algorithm for MHE, where $\\ell$ is the number of colors\nand $\\operatorname{cw}$ is the clique-width of the input graph. We also\nconstruct an FPT algorithm for MHV with running time\n$\\mathcal{O}^*((\\ell+1)^{\\mathcal{O}(\\operatorname{cw})})$, where $\\ell$ is the\nnumber of colors in the input. Additionally, we show $\\mathcal{O}(\\ell n^2)$\nalgorithm for MHV on interval graphs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:42:16 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Bliznets", "Ivan", ""], ["Sagunov", "Danil", ""]]}, {"id": "2003.04863", "submitter": "Adrian Vladu", "authors": "Kyriakos Axiotis, Aleksander M\\k{a}dry, Adrian Vladu", "title": "Circulation Control for Faster Minimum Cost Flow in Unit-Capacity Graphs", "comments": "improved running time to m^{4/3+o(1)} log W", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $m^{4/3+o(1)}\\log W$-time algorithm for solving the minimum\ncost flow problem in graphs with unit capacity, where $W$ is the maximum\nabsolute value of any edge weight. For sparse graphs, this improves over the\nbest known running time for this problem and, by well-known reductions, also\nimplies improved running times for the shortest path problem with negative\nweights, minimum cost bipartite $\\boldsymbol{\\mathit{b}}$-matching when\n$\\|\\boldsymbol{\\mathit{b}}\\|_1 = O(m)$, and recovers the running time of the\ncurrently fastest algorithm for maximum flow in graphs with unit capacities\n(Liu-Sidford, 2020).\n  Our algorithm relies on developing an interior point method-based framework\nwhich acts on the space of circulations in the underlying graph. From the\ncombinatorial point of view, this framework can be viewed as iteratively\nimproving the cost of a suboptimal solution by pushing flow around\ncirculations. These circulations are derived by computing a regularized version\nof the standard Newton step, which is partially inspired by previous work on\nthe unit-capacity maximum flow problem (Liu-Sidford, 2019), and subsequently\nrefined based on the very recent progress on this problem (Liu-Sidford, 2020).\nThe resulting step problem can then be computed efficiently using the recent\nwork on $\\ell_p$-norm minimizing flows (Kyng-Peng-Sachdeva-Wang, 2019). We\nobtain our faster algorithm by combining this new step primitive with a\ncustomized preconditioning method, which aims to ensure that the graph on which\nthese circulations are computed has sufficiently large conductance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:14:06 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 17:50:08 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Axiotis", "Kyriakos", ""], ["M\u0105dry", "Aleksander", ""], ["Vladu", "Adrian", ""]]}, {"id": "2003.04873", "submitter": "Haoyun Ying", "authors": "Haoyun Ying, Keheng Mao, Klaus Mosegaard", "title": "Moving Target Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov Chain Monte Carlo (MCMC) methods are popular when considering\nsampling from a high-dimensional random variable $\\mathbf{x}$ with possibly\nunnormalised probability density $p$ and observed data $\\mathbf{d}$. However,\nMCMC requires evaluating the posterior distribution $p(\\mathbf{x}|\\mathbf{d})$\nof the proposed candidate $\\mathbf{x}$ at each iteration when constructing the\nacceptance rate. This is costly when such evaluations are intractable. In this\npaper, we introduce a new non-Markovian sampling algorithm called Moving Target\nMonte Carlo (MTMC). The acceptance rate at $n$-th iteration is constructed\nusing an iteratively updated approximation of the posterior distribution\n$a_n(\\mathbf{x})$ instead of $p(\\mathbf{x}|\\mathbf{d})$. The true value of the\nposterior $p(\\mathbf{x}|\\mathbf{d})$ is only calculated if the candidate\n$\\mathbf{x}$ is accepted. The approximation $a_n$ utilises these evaluations\nand converges to $p$ as $n \\rightarrow \\infty$. A proof of convergence and\nestimation of convergence rate in different situations are given.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:38:36 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Ying", "Haoyun", ""], ["Mao", "Keheng", ""], ["Mosegaard", "Klaus", ""]]}, {"id": "2003.04969", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Peeyush Gupta, Dhrubajyoti Ghosh,\n  Sharad Mehrotra, Nalini Venkatasubramanian", "title": "IoT Expunge: Implementing Verifiable Retention of IoT Data", "comments": "This paper has been accepted in 10th ACM Conference on Data and\n  Application Security and Privacy (CODASPY), 2020", "journal-ref": null, "doi": "10.1145/3374664.3375737", "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing deployment of Internet of Things (IoT) systems aims to ease the\ndaily life of end-users by providing several value-added services. However, IoT\nsystems may capture and store sensitive, personal data about individuals in the\ncloud, thereby jeopardizing user-privacy. Emerging legislation, such as\nCalifornia's CalOPPA and GDPR in Europe, support strong privacy laws to protect\nan individual's data in the cloud. One such law relates to strict enforcement\nof data retention policies. This paper proposes a framework, entitled IoT\nExpunge that allows sensor data providers to store the data in cloud platforms\nthat will ensure enforcement of retention policies. Additionally, the cloud\nprovider produces verifiable proofs of its adherence to the retention policies.\nExperimental results on a real-world smart building testbed show that IoT\nExpunge imposes minimal overheads to the user to verify the data against data\nretention policies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 20:55:01 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Gupta", "Peeyush", ""], ["Ghosh", "Dhrubajyoti", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "2003.05101", "submitter": "Beheshteh Toloueirakhshan", "authors": "Beheshteh T. Rakhshan and Guillaume Rabusseau", "title": "Tensorized Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel random projection technique for efficiently reducing the\ndimension of very high-dimensional tensors. Building upon classical results on\nGaussian random projections and Johnson-Lindenstrauss transforms~(JLT), we\npropose two tensorized random projection maps relying on the tensor train~(TT)\nand CP decomposition format, respectively. The two maps offer very low memory\nrequirements and can be applied efficiently when the inputs are low rank\ntensors given in the CP or TT format. Our theoretical analysis shows that the\ndense Gaussian matrix in JLT can be replaced by a low-rank tensor implicitly\nrepresented in compressed form with random factors, while still approximately\npreserving the Euclidean distance of the projected inputs. In addition, our\nresults reveal that the TT format is substantially superior to CP in terms of\nthe size of the random projection needed to achieve the same distortion ratio.\nExperiments on synthetic data validate our theoretical analysis and demonstrate\nthe superiority of the TT decomposition.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 03:56:44 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Rakhshan", "Beheshteh T.", ""], ["Rabusseau", "Guillaume", ""]]}, {"id": "2003.05141", "submitter": "Shmuel Onn", "authors": "Shmuel Onn", "title": "On Degree Sequence Optimization", "comments": null, "journal-ref": "Operations Research Letters, 48:840--843, 2020", "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a subgraph of a given graph which\nmaximizes a given function evaluated at its degree sequence. While the problem\nis intractable already for convex functions, we show that it can be solved in\npolynomial time for convex multi-criteria objectives. We next consider the\nproblem with separable objectives, which is NP-hard already when all vertex\nfunctions are the square. We consider a colored extension of the separable\nproblem, which includes the notorious exact matching problem as a special case,\nand show that it can be solved in polynomial time on graphs of bounded\ntree-depth for any vertex functions. We mention some of the many remaining open\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 07:32:03 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 16:29:57 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Onn", "Shmuel", ""]]}, {"id": "2003.05175", "submitter": "Hyung-Chan An", "authors": "Yongho Shin, Kangsan Kim, Seungmin Lee, Hyung-Chan An", "title": "Online Graph Matching Problems with a Worst-Case Reassignment Budget", "comments": "16 pages, excluding the title page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online bipartite matching with reassignments problem, an algorithm is\ninitially given only one side of the vertex set of a bipartite graph; the\nvertices on the other side are revealed to the algorithm one by one, along with\nits incident edges. The algorithm is required to maintain a matching in the\ncurrent graph, where the algorithm revises the matching after each vertex\narrival by reassigning vertices. Bernstein, Holm, and Rotenberg showed that an\nonline algorithm can maintain a matching of maximum cardinality by performing\namortized $O(\\log^2 n)$ reassignments per arrival.\n  In this paper, we propose to consider the general question of how requiring a\nnon-amortized hard budget $k$ on the number of reassignments affects the\nalgorithms' performances, under various models from the literature.\n  We show that a simple, widely-used algorithm is a best-possible deterministic\nalgorithm for all these models. For the unweighted maximum-cardinality problem,\nthe algorithm is a $(1-\\frac{2}{k+2})$-competitive algorithm, which is the best\npossible for a deterministic algorithm both under vertex arrivals and edge\narrivals. Applied to the load balancing problem, this yields a bifactor online\nalgorithm. For the weighted problem, which is traditionally studied assuming\nthe triangle inequality, we show that the power of reassignment allows us to\nlift this assumption and the algorithm becomes a $\\frac{1}{2}$-competitive\nalgorithm for $k=4$, improving upon the $\\frac{1}{3}$ of the previous algorithm\nwithout reassignments. We show that this also is a best-possible deterministic\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 09:14:48 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Shin", "Yongho", ""], ["Kim", "Kangsan", ""], ["Lee", "Seungmin", ""], ["An", "Hyung-Chan", ""]]}, {"id": "2003.05185", "submitter": "Marcin Pilipczuk", "authors": "Tara Abrishami and Maria Chudnovsky and Marcin Pilipczuk and Pawe{\\l}\n  Rz\\k{a}\\.zewski and Paul Seymour", "title": "Induced subgraphs of bounded treewidth and the container method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hole in a graph is an induced cycle of length at least 4. A hole is long if\nits length is at least 5. By $P_t$ we denote a path on $t$ vertices. In this\npaper we give polynomial-time algorithms for the following problems: the\nMaximum Weight Independent Set problem in long-hole-free graphs, and the\nFeedback Vertex Set problem in $P_5$-free graphs. Each of the above results\nresolves a corresponding long-standing open problem.\n  An extended $C_5$ is a five-vertex hole with an additional vertex adjacent to\none or two consecutive vertices of the hole. Let $\\mathcal{C}$ be the class of\ngraphs excluding an extended $C_5$ and holes of length at least $6$ as induced\nsubgraphs; $\\mathcal{C}$ contains all long-hole-free graphs and all $P_5$-free\ngraphs. We show that, given an $n$-vertex graph $G \\in \\mathcal{C}$ with vertex\nweights and an integer $k$, one can in time $n^{\\Oh(k)}$ find a maximum-weight\ninduced subgraph of $G$ of treewidth less than $k$. This implies both\naforementioned results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 09:30:40 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Abrishami", "Tara", ""], ["Chudnovsky", "Maria", ""], ["Pilipczuk", "Marcin", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""], ["Seymour", "Paul", ""]]}, {"id": "2003.05217", "submitter": "Sascha Hunold", "authors": "Sascha Hunold and Bart{\\l}omiej Przybylski", "title": "Scheduling.jl -- Collaborative and Reproducible Scheduling Research with\n  Julia", "comments": "5 pages, 1 figure, 2 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Scheduling.jl Julia package, which is intended for\ncollaboratively conducting scheduling research and for sharing implementations\nof algorithms. It provides the fundamental building blocks for implementing\nscheduling algorithms following the three-field notation of Graham et al.,\ni.e., it has functionality to describe machine environments, job\ncharacteristics, and optimality criteria. Our goal is to foster algorithm and\ncode sharing in the scheduling community. Scheduling.jl can also be used to\nsupport teaching scheduling theory in classes. We will show the main\nfunctionalities of Scheduling.jl and give an example on how to use it by\ncomparing different algorithms for the problem of P||Cmax .\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:10:56 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Hunold", "Sascha", ""], ["Przybylski", "Bart\u0142omiej", ""]]}, {"id": "2003.05267", "submitter": "Matthias Mnich", "authors": "Alexander G\\\"oke and D\\'aniel Marx and Matthias Mnich", "title": "Hitting Long Directed Cycles is Fixed-Parameter Tractable", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Directed Long Cycle Hitting Set} problem we are given a directed graph\n$G$, and the task is to find a set $S$ of at most $k$ vertices/arcs such that\n$G-S$ has no cycle of length longer than $\\ell$. We show that the problem can\nbe solved in time $2^{\\mathcal O(\\ell k^3\\log k + k^5\\log k\\log\\ell)}\\cdot\nn^{\\mathcal O(1)}$, that is, it is fixed-parameter tractable (FPT)\nparameterized by $k$ and $\\ell$. This algorithm can be seen as a far-reaching\ngeneralization of the fixed-parameter tractability of {\\sc Mixed Graph Feedback\nVertex Set} [Bonsma and Lokshtanov WADS 2011], which is already a common\ngeneralization of the fixed-parameter tractability of (undirected) {\\sc\nFeedback Vertex Set} and the {\\sc Directed Feedback Vertex Set} problems, two\nclassic results in parameterized algorithms. The algorithm requires significant\ninsights into the structure of graphs without directed cycles length longer\nthan $\\ell$ and can be seen as an exact version of the approximation algorithm\nfollowing from the Erd{\\H{o}}s-P{\\'o}sa property for long cycles in directed\ngraphs proved by Kreutzer and Kawarabayashi [STOC 2015].\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 13:09:19 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["G\u00f6ke", "Alexander", ""], ["Marx", "D\u00e1niel", ""], ["Mnich", "Matthias", ""]]}, {"id": "2003.05289", "submitter": "Guangping Li", "authors": "Sujoy Bhore, Jan-Henrik Haunert, Fabian Klute, Guangping Li, Martin\n  N\\\"ollenburg", "title": "Balanced Independent and Dominating Sets on Colored Interval Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two new versions of independent and dominating set problems on\nvertex-colored interval graphs, namely $f$-Balanced Independent Set ($f$-BIS)\nand $f$-Balanced Dominating Set ($f$-BDS). Let $G=(V,E)$ be a vertex-colored\ninterval graph with a color assignment function $\\gamma \\colon V \\rightarrow\n\\{1,\\ldots,k\\}$ that maps all vertices in $G$ onto $k$ colors. A subset of\nvertices $S\\subseteq V$ is called $f$-balanced if $S$ contains $f$ vertices\nfrom each color class. In the $f$-BIS and $f$-BDS problems, the objective is to\ncompute an independent set or a dominating set that is $f$-balanced. We show\nthat both problems are NP-complete even on proper interval graphs. For the BIS\nproblem on interval graphs, we design two FPT algorithms, one parameterized by\n$(f,k)$ and the other by the vertex cover number of $G$. Moreover, for an\noptimization variant of BIS on interval graphs, we present a polynomial time\napproximation scheme (PTAS) and an $O(n\\log n)$ time $2$-approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 13:49:37 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 17:24:27 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bhore", "Sujoy", ""], ["Haunert", "Jan-Henrik", ""], ["Klute", "Fabian", ""], ["Li", "Guangping", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2003.05582", "submitter": "Majid Farhadi", "authors": "Majid Farhadi, Anand Louis, Prasad Tetali", "title": "On the Complexity of $\\lambda_\\infty\\,,$ Vertex Expansion, and Spread\n  Constant of Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bobkov, Houdr\\'e, and the last author introduced a Poincar\\'e-type functional\nparameter, $\\lambda_\\infty$, of a graph $G$. They related $\\lambda_\\infty$ to\nthe {\\em vertex expansion} of the graph via a Cheeger-type inequality,\nanalogous to the inequality relating the spectral gap of the graph,\n$\\lambda_2$, to its {\\em edge expansion}. While $\\lambda_2$ can be computed\nefficiently, the computational complexity of $\\lambda_\\infty$ has remained an\nopen question. Following the work of the second author with Raghavendra and\nVempala, wherein the complexity of $\\lambda_\\infty$ was related to the\nso-called small-set expansion (SSE) problem, it has been believed that\ncomputing $\\lambda_\\infty$ is a hard problem. We confirm this conjecture by\nproving that computing $\\lambda_\\infty$ is indeed NP-hard, even for weighted\ntrees.\n  Our gadget further proves NP-hardness of computing \\emph{spread constant} of\na weighted tree; i.e., a geometric measure of the graph, introduced by Alon,\nBoppana, and Spencer, in the context of deriving an asymptotic isoperimetric\ninequality of Cartesian products of graphs. We conclude this case by providing\na fully polynomial time approximation scheme.\n  We further study a generalization of spread constant in machine learning\nliterature, namely the {\\em maximum variance embedding} problem. For trees, we\nprovide fast combinatorial algorithms that avoid solving a semidefinite\nrelaxation of the problem. On the other hand, for general graphs, we propose a\nrandomized projection method that can outperform the optimal orthogonal\nprojection, i.e., PCA, classically used for rounding of the optimum lifted\nsolution (to SDP relaxation) of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 02:40:21 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 01:29:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Farhadi", "Majid", ""], ["Louis", "Anand", ""], ["Tetali", "Prasad", ""]]}, {"id": "2003.05699", "submitter": "Martin B\\\"ohm", "authors": "Martin B\\\"ohm, Ruben Hoeksma, Nicole Megow, Lukas N\\\"olke, Bertrand\n  Simon", "title": "Computing a Minimum-Cost $k$-hop Steiner Tree in Tree-Like Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a Steiner tree of minimum cost under a\n$k$-hop constraint which requires the depth of the tree to be at most $k$. Our\nmain result is an exact algorithm for metrics induced by graphs of bounded\ntreewidth that runs in time $n^{O(k)}$. For the special case of a path, we give\na simple algorithm that solves the problem in polynomial time, even if $k$ is\npart of the input. The main result can be used to obtain, in quasi-polynomial\ntime, a near-optimal solution that violates the $k$-hop constraint by at most\none hop for more general metrics induced by graphs of bounded highway\ndimension.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:40:57 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Hoeksma", "Ruben", ""], ["Megow", "Nicole", ""], ["N\u00f6lke", "Lukas", ""], ["Simon", "Bertrand", ""]]}, {"id": "2003.06076", "submitter": "Charalampos Tsourakakis", "authors": "Michael Mitzenmacher and Charalampos E. Tsourakakis", "title": "Joint Alignment From Pairwise Differences with a Noisy Oracle", "comments": "Paper appeared in the 15th Workshop on Algorithms and Models for the\n  Web Graph (WAW 2018), invited to Internet Mathematics special issue. Overlaps\n  in text with earlier unpublished note arxiv:1609.00750. (v2 minor updates)", "journal-ref": null, "doi": "10.1007/978-3-319-92871-5_5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of recovering $n$ discrete random\nvariables $x_i\\in \\{0,\\ldots,k-1\\}, 1 \\leq i \\leq n$ (where $k$ is constant)\nwith the smallest possible number of queries to a noisy oracle that returns for\na given query pair $(x_i,x_j)$ a noisy measurement of their modulo $k$ pairwise\ndifference, i.e., $y_{ij} = (x_i-x_j) \\mod k$. This is a joint discrete\nalignment problem with important applications in computer vision, graph mining,\nand spectroscopy imaging. Our main result is a polynomial time algorithm that\nlearns exactly with high probability the alignment (up to some unrecoverable\noffset) using $O(n^{1+o(1)})$ queries.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 01:01:58 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 14:08:22 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "2003.06292", "submitter": "Ayan Mahalanobis", "authors": "Sushil Bhunia, Ayan Mahalanobis, Pralhad Shinde and Anupam Singh", "title": "Algorithms in Linear Algebraic Groups", "comments": "arXiv admin note: text overlap with arXiv:1901.00892,\n  arXiv:1504.03794", "journal-ref": "Adv. Appl. Clifford Algebras 30, 31 (2020)", "doi": "10.1007/s00006-020-01054-y", "report-no": null, "categories": "math.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents some algorithms in linear algebraic groups. These\nalgorithms solve the word problem and compute the spinor norm for orthogonal\ngroups. This gives us an algorithmic definition of the spinor norm. We compute\nthe double coset decomposition with respect to a Siegel maximal parabolic\nsubgroup, which is important in computing infinite-dimensional representations\nfor some algebraic groups.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:20:47 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bhunia", "Sushil", ""], ["Mahalanobis", "Ayan", ""], ["Shinde", "Pralhad", ""], ["Singh", "Anupam", ""]]}, {"id": "2003.06622", "submitter": "Nikolaos Melissinos", "authors": "Nikolaos Melissinos, Aris Pagourtzis and Theofilos Triommatis", "title": "Approximation Schemes for Subset Sum Ratio Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Subset Sum Ratio Problem ($SSR$), in which given a set of\nintegers the goal is to find two subsets such that the ratio of their sums is\nas close to~1 as possible, and introduce a family of variations that capture\nadditional meaningful requirements. Our main contribution is a generic\nframework that yields fully polynomial time approximation schemes (FPTAS) for\nproblems in this family that meet certain conditions. We use our framework to\ndesign explicit FPTASs for two such problems, namely Two-Set Subset-Sum Ratio\nand Factor-$r$ Subset-Sum Ratio, with running time\n$\\mathcal{O}(n^4/\\varepsilon)$, which coincides with the best known running\ntime for the original $SSR$ problem [15].\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 12:56:52 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Melissinos", "Nikolaos", ""], ["Pagourtzis", "Aris", ""], ["Triommatis", "Theofilos", ""]]}, {"id": "2003.06639", "submitter": "Matthias Stallmann", "authors": "Matthias F. Stallmann, Yang Ho, Timothy D. Goodrich", "title": "Graph Profiling for Vertex Cover: Targeted Reductions in a Branch and\n  Reduce Solver", "comments": "33 pages, 9 figures, 24 tables software at\n  https://github.com/mfms-ncsu/VC-BR and https://github.com/mfms-ncsu/CPX-ILP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Akiba and Iwata [TCS, 2016] demonstrated that a branch and reduce (B&R)\nsolver for the vertex cover problem can compete favorably with integer linear\nprogramming solvers (e.g., CPLEX). Our research question is are there graph\ncharacteristics that determine which reductions will be most effective? Not\nonly is the answer affirmative, but relevant characteristics are easy to\nidentify. To explore our ideas, we provide an enhanced version of the\nAkiba-Iwata solver that can (a) be configured with any subset of reductions and\nlower bounds; (b) print statistics such as time taken and number of vertices\nreduced by each reduction. Based on extensive experiments with benchmark and\nrandom instances we demonstrate that (i) more reductions do not necessarily\nlead to better runtimes; (ii) the subset of reductions leading to the best (or\nnearly the best) runtime can be predicted based on measurable characteristics\nof a graph, e.g., density and degree distribution; and (iii) exceptions have\nstructural characteristics known in advance. Our primary contributions are 1. A\nthorough examination reduction routine performance in the context of graph\ncharacteristics. 2. Three primary hypotheses suggesting simple suites of\nreductions as the most efficient options. 3. Experiments with a large corpus of\ndata to validate our hypotheses. 4. Measures that quantify a problem instance\non two key dimensions to make our hypotheses concrete. 5. An enhanced\nopen-source version of the Akiba-Iwata solver that enables our investigations\nand creates opportunities for future exploration. Our main objective is to\nprovide guidance to a user so that, faced with a given problem instance or set\nof instances, they may most effectively use the available reductions.\nUltimately these efforts can lead to an automated process.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 14:23:31 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Stallmann", "Matthias F.", ""], ["Ho", "Yang", ""], ["Goodrich", "Timothy D.", ""]]}, {"id": "2003.06691", "submitter": "Wojciech Janczewski", "authors": "Pawe{\\l} Gawrychowski, Wojciech Janczewski, Jakub {\\L}opusza\\'nski", "title": "Shorter Labels for Routing in Trees", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A routing labeling scheme assigns a binary string, called a label, to each\nnode in a network, and chooses a distinct port number from $\\{1,\\ldots,d\\}$ for\nevery edge outgoing from a node of degree $d$. Then, given the labels of $u$\nand $w$ and no other information about the network, it should be possible to\ndetermine the port number corresponding to the first edge on the shortest path\nfrom $u$ to $w$. In their seminal paper, Thorup and Zwick [SPAA 2001] designed\nseveral routing methods for general weighted networks. An important technical\ningredient in their paper that according to the authors ``may be of independent\npractical and theoretical interest'' is a routing labeling scheme for trees of\narbitrary degrees. For a tree on $n$ nodes, their scheme constructs labels\nconsisting of $(1+o(1))\\log n$ bits such that the sought port number can be\ncomputed in constant time. Looking closer at their construction, the labels\nconsist of $\\log n + O(\\log n\\cdot \\log\\log\\log n / \\log\\log n)$ bits. Given\nthat the only known lower bound is $\\log n+\\Omega(\\log\\log n)$, a natural\nquestion that has been asked for other labeling problems in trees is to\ndetermine the asymptotics of the smaller-order term.\n  We make the first (and significant) progress in 19 years on determining the\ncorrect second-order term for the length of a label in a routing labeling\nscheme for trees on $n$ nodes. We design such a scheme with labels of length\n$\\log n+O((\\log\\log n)^{2})$. Furthermore, we modify the scheme to allow for\ncomputing the port number in constant time at the expense of slightly\nincreasing the length to $\\log n+O((\\log\\log n)^{3})$.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 19:44:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Janczewski", "Wojciech", ""], ["\u0141opusza\u0144ski", "Jakub", ""]]}, {"id": "2003.06706", "submitter": "Rickard Br\\\"uel Gabrielsson", "authors": "Rickard Br\\\"uel-Gabrielsson", "title": "Universal Function Approximation on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we produce a framework for constructing universal function\napproximators on graph isomorphism classes. We prove how this framework comes\nwith a collection of theoretically desirable properties and enables novel\nanalysis. We show how this allows us to achieve state-of-the-art performance on\nfour different well-known datasets in graph classification and separate classes\nof graphs that other graph-learning methods cannot. Our approach is inspired by\npersistent homology, dependency parsing for NLP, and multivalued functions. The\ncomplexity of the underlying algorithm is O(#edges x #nodes) and code is\npublicly available\n(https://github.com/bruel-gabrielsson/universal-function-approximation-on-graphs).\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 21:12:33 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 09:06:02 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 07:58:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Br\u00fcel-Gabrielsson", "Rickard", ""]]}, {"id": "2003.06742", "submitter": "Yakov Nekrich", "authors": "Yakov Nekrich", "title": "Four-Dimensional Dominance Range Reporting in Linear Space", "comments": "Extended version of a SoCG'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the four-dimensional dominance range reporting problem\nand present data structures with linear or almost-linear space usage. Our\nresults can be also used to answer four-dimensional queries that are bounded on\nfive sides. The first data structure presented in this paper uses linear space\nand answers queries in $O(\\log^{1+\\varepsilon}n + k\\log^{\\varepsilon} n)$ time,\nwhere $k$ is the number of reported points, $n$ is the number of points in the\ndata structure, and $\\varepsilon$ is an arbitrarily small positive constant.\nOur second data structure uses $O(n \\log^{\\varepsilon} n)$ space and answers\nqueries in $O(\\log n+k)$ time.\n  These are the first data structures for this problem that use linear (resp.\n$O(n\\log^{\\varepsilon} n)$) space and answer queries in poly-logarithmic time.\nFor comparison the fastest previously known linear-space or\n$O(n\\log^{\\varepsilon} n)$-space data structure supports queries in\n$O(n^{\\varepsilon} + k)$ time (Bentley and Mauer, 1980). Our results can be\ngeneralized to $d\\ge 4$ dimensions. For example, we can answer $d$-dimensional\ndominance range reporting queries in $O(\\log\\log n (\\log n/\\log\\log n)^{d-3} +\nk)$ time using $O(n\\log^{d-4+\\varepsilon}n)$ space. Compared to the fastest\npreviously known result (Chan, 2013), our data structure reduces the space\nusage by $O(\\log n)$ without increasing the query time.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 03:00:13 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nekrich", "Yakov", ""]]}, {"id": "2003.06875", "submitter": "Dong Wei", "authors": "Dong Wei, Senjuti Basu Roy, Sihem Amer-Yahia", "title": "Recommending Deployment Strategies for Collaborative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work contributes to aiding requesters in deploying collaborative tasks in\ncrowdsourcing. We initiate the study of recommending deployment strategies for\ncollaborative tasks to requesters that are consistent with deployment\nparameters they desire: a lower-bound on the quality of the crowd contribution,\nan upper-bound on the latency of task completion, and an upper-bound on the\ncost incurred by paying workers. A deployment strategy is a choice of value for\nthree dimensions: Structure (whether to solicit the workforce sequentially or\nsimultaneously), Organization (to organize it collaboratively or\nindependently), and Style (to rely solely on the crowd or to combine it with\nmachine algorithms). We propose StratRec, an optimization-driven middle layer\nthat recommends deployment strategies and alternative deployment parameters to\nrequesters by accounting for worker availability. Our solutions are grounded in\ndiscrete optimization and computational geometry techniques that produce\nresults with theoretical guarantees. We present extensive experiments on Amazon\nMechanical Turk and conduct synthetic experiments to validate the qualitative\nand scalability aspects of StratRec.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:36:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wei", "Dong", ""], ["Roy", "Senjuti Basu", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "2003.06936", "submitter": "Mourad EL Ouali", "authors": "Abbass Gorgi and Mourad El Ouali and Anand Srivastav and Mohamed\n  Hachimi", "title": "Approximation algorithm for the Multicovering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{H}=(V,\\mathcal{E})$ be a hypergraph with maximum edge size\n$\\ell$ and maximum degree $\\Delta$. For given numbers $b_v\\in \\mathbb{N}_{\\geq\n2}$, $v\\in V$, a set multicover in $\\mathcal{H}$ is a set of edges $C \\subseteq\n\\mathcal{E}$ such that every vertex $v$ in $V$ belongs to at least $b_v$ edges\nin $C$. Set Multicover is the problem of finding a minimum-cardinality set\nmulticover. Peleg, Schechtman and Wool conjectured that for any fixed $\\Delta$\nand $b:=\\min_{v\\in V}b_{v}$, the problem of \\sbmultcov is not approximable\nwithin a ratio less than $\\delta:=\\Delta-b+1$, unless $\\mathcal{P}\n=\\mathcal{NP}$. Hence it's a challenge to explore for which classes of\nhypergraph the conjecture doesn't hold.\n  We present a polynomial time algorithm for the Set Multicover problem which\ncombines a deterministic threshold algorithm with conditioned randomized\nrounding steps. Our algorithm yields an approximation ratio of $ \\max\\left\\{\n\\frac{148}{149}\\delta, \\left(1- \\frac{ (b-1)e^{\\frac{\\delta}{4}}}{94\\ell}\n\\right)\\delta \\right\\}$. Our result not only improves over the approximation\nratio presented by Srivastav et al (Algorithmica 2016) but it's more general\nsince we set no restriction on the parameter $\\ell$. Moreover we present a\nfurther polynomial time algorithm with an approximation ratio of\n$\\frac{5}{6}\\delta$ for hypergraphs with $\\ell\\leq (1+\\epsilon)\\bar{\\ell}$ for\nany fixed $\\epsilon \\in [0,\\frac{1}{2}]$, where $\\bar{\\ell}$ is the average\nedge size. The analysis of this algorithm relies on matching/covering duality\ndue to Ray-Chaudhuri (1960), which we convert into an approximative form. The\nsecond performance disprove the conjecture of peleg et al for a large subclass\nof hypergraphs.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 22:07:13 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Gorgi", "Abbass", ""], ["Ouali", "Mourad El", ""], ["Srivastav", "Anand", ""], ["Hachimi", "Mohamed", ""]]}, {"id": "2003.07010", "submitter": "Jason Gaitonde", "authors": "Jason Gaitonde, Jon Kleinberg, Eva Tardos", "title": "Adversarial Perturbations of Opinion Dynamics in Networks", "comments": "28 pages; added new related work, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the connections between network structure, opinion dynamics, and an\nadversary's power to artificially induce disagreements. We approach these\nquestions by extending models of opinion formation in the social sciences to\nrepresent scenarios, familiar from recent events, in which external actors seek\nto destabilize communities through sophisticated information warfare tactics\nvia fake news and bots. In many instances, the intrinsic goals of these efforts\nare not necessarily to shift the overall sentiment of the network, but rather\nto induce discord. These perturbations diffuse via opinion dynamics on the\nunderlying network, through mechanisms that have been analyzed and abstracted\nthrough work in computer science and the social sciences. We investigate the\nproperties of such attacks, considering optimal strategies both for the\nadversary seeking to create disagreement and for the entities tasked with\ndefending the network from attack. We show that for different formulations of\nthese types of objectives, different regimes of the spectral structure of the\nnetwork will limit the adversary's capacity to sow discord; this enables us to\nqualitatively describe which networks are most vulnerable against these\nperturbations. We then consider the algorithmic task of a network defender to\nmitigate these sorts of adversarial attacks by insulating nodes\nheterogeneously; we show that, by considering the geometry of this problem,\nthis optimization task can be efficiently solved via convex programming.\nFinally, we generalize these results to allow for two network structures, where\nthe opinion dynamics process and the measurement of disagreement become\nuncoupled, and determine how the adversary's power changes; for instance, this\nmay arise when opinion dynamics are controlled an online community via social\nmedia, while disagreement is measured along \"real-world\" connections.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 04:01:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 17:00:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gaitonde", "Jason", ""], ["Kleinberg", "Jon", ""], ["Tardos", "Eva", ""]]}, {"id": "2003.07104", "submitter": "Philip Wellnitz", "authors": "Karl Bringmann, Nick Fischer, Danny Hermelin, Dvir Shabtay, Philip\n  Wellnitz", "title": "Faster Minimization of Tardy Processing Time on a Single Machine", "comments": "12 pages; ICALP'20 (A)", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2020.19", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the $1||\\sum p_jU_j$ problem, the problem of\nminimizing the total processing time of tardy jobs on a single machine. This is\nnot only a fundamental scheduling problem, but also a very important problem\nfrom a theoretical point of view as it generalizes the Subset Sum problem and\nis closely related to the 0/1-Knapsack problem. The problem is well-known to be\nNP-hard, but only in a weak sense, meaning it admits pseudo-polynomial time\nalgorithms. The fastest known pseudo-polynomial time algorithm for the problem\nis the famous Lawler and Moore algorithm which runs in $O(P \\cdot n)$ time,\nwhere $P$ is the total processing time of all $n$ jobs in the input. This\nalgorithm has been developed in the late 60s, and has yet to be improved to\ndate.\n  In this paper we develop two new algorithms for $1||\\sum p_jU_j$, each\nimproving on Lawler and Moore's algorithm in a different scenario. Both\nalgorithms rely on basic primitive operations between sets of integers and\nvectors of integers for the speedup in their running times. The second\nalgorithm relies on fast polynomial multiplication as its main engine, while\nfor the first algorithm we define a new \"skewed\" version of\n$(\\max,\\min)$-convolution which is interesting in its own right.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 10:45:36 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 20:36:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bringmann", "Karl", ""], ["Fischer", "Nick", ""], ["Hermelin", "Danny", ""], ["Shabtay", "Dvir", ""], ["Wellnitz", "Philip", ""]]}, {"id": "2003.07113", "submitter": "Karl Bringmann", "authors": "Amir Abboud, Karl Bringmann, Danny Hermelin, Dvir Shabtay", "title": "Scheduling Lower Bounds via AND Subset Sum", "comments": "14 pages, ICALP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $N$ instances $(X_1,t_1),\\ldots,(X_N,t_N)$ of Subset Sum, the AND\nSubset Sum problem asks to determine whether all of these instances are\nyes-instances; that is, whether each set of integers $X_i$ has a subset that\nsums up to the target integer $t_i$. We prove that this problem cannot be\nsolved in time $\\tilde{O}((N \\cdot t_{max})^{1-\\epsilon})$, for $t_{max}=\\max_i\nt_i$ and any $\\epsilon > 0$, assuming the $\\forall \\exists$ Strong Exponential\nTime Hypothesis ($\\forall \\exists$-SETH). We then use this result to exclude\n$\\tilde{O}(n+P_{max} \\cdot n^{1-\\epsilon})$-time algorithms for several\nscheduling problems on $n$ jobs with maximum processing time $P_{max}$, based\non $\\forall \\exists$-SETH. These include classical problems such as $1||\\sum\nw_jU_j$, the problem of minimizing the total weight of tardy jobs on a single\nmachine, and $P_2||\\sum U_j$, the problem of minimizing the number of tardy\njobs on two identical parallel machines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 11:10:47 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 17:35:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abboud", "Amir", ""], ["Bringmann", "Karl", ""], ["Hermelin", "Danny", ""], ["Shabtay", "Dvir", ""]]}, {"id": "2003.07285", "submitter": "Masoud Seddighin", "authors": "MohammadTaghi Hajiaghayi, Masoud Seddighin, Saeed Seddighin, and\n  Xiaorui Sun", "title": "Approximating LCS in Linear Time: Beating the $\\sqrt{n}$ Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longest common subsequence (LCS) is one of the most fundamental problems in\ncombinatorial optimization. Apart from theoretical importance, LCS has enormous\napplications in bioinformatics, revision control systems, and data comparison\nprograms. Although a simple dynamic program computes LCS in quadratic time, it\nhas been recently proven that the problem admits a conditional lower bound and\nmay not be solved in truly subquadratic time. In addition to this, LCS is\nnotoriously hard with respect to approximation algorithms. Apart from a trivial\nsampling technique that obtains a $n^{x}$ approximation solution in time\n$O(n^{2-2x})$ nothing else is known for LCS. This is in sharp contrast to its\ndual problem edit distance for which several linear time solutions are obtained\nin the past two decades.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 15:45:53 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hajiaghayi", "MohammadTaghi", ""], ["Seddighin", "Masoud", ""], ["Seddighin", "Saeed", ""], ["Sun", "Xiaorui", ""]]}, {"id": "2003.07395", "submitter": "Thomas Dickerson", "authors": "Thomas Dickerson", "title": "Adapting Persistent Data Structures for Concurrency and Speculation", "comments": "PhD Thesis, Brown University (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work unifies insights from the systems and functional programming\ncommunities, in order to enable compositional reasoning about software which is\nnonetheless efficiently realizable in hardware. It exploits a correspondence\nbetween design goals for efficient concurrent data structures and efficient\nimmutable persistent data structures, to produce novel implementations of\nmutable concurrent trees with low contention and an efficient snapshot\noperation to support speculative execution models. It also exploits\ncommutativity to characterize a design space for integrating traditional\nhigh-performance concurrent data structures into Software Transactional Memory\n(STM) runtimes, and extends this technique to yield a novel algorithm for\nconcurrent execution of so-called ``smart contracts'' (specialized programs\nwhich manipulate the state of blockchain ledgers).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:26:08 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Dickerson", "Thomas", ""]]}, {"id": "2003.07427", "submitter": "Seri Khoury", "authors": "Yuval Efron, Ofer Grossman, and Seri Khoury", "title": "Beyond Alice and Bob: Improved Inapproximability for Maximum Independent\n  Set in CONGEST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By far the most fruitful technique for showing lower bounds for the CONGEST\nmodel is reductions to two-party communication complexity. This technique has\nyielded nearly tight results for various fundamental problems such as distance\ncomputations, minimum spanning tree, minimum vertex cover, and more.\n  In this work, we take this technique a step further, and we introduce a\nframework of reductions to $t$-party communication complexity, for every $t\\geq\n2$. Our framework enables us to show improved hardness results for maximum\nindependent set. Recently, Bachrach et al.[PODC 2019] used the two-party\nframework to show hardness of approximation for maximum independent set. They\nshow that finding a $(5/6+\\epsilon)$-approximation requires $\\Omega(n/\\log^6\nn)$ rounds, and finding a $(7/8+\\epsilon)$-approximation requires\n$\\Omega(n^2/\\log^7 n)$ rounds, in the CONGEST model where $n$ in the number of\nnodes in the network.\n  We improve the results of Bachrach et al. by using reductions to multi-party\ncommunication complexity. Our results:\n  (1) Any algorithm that finds a $(1/2+\\epsilon)$-approximation for maximum\nindependent set in the CONGEST model requires $\\Omega(n/\\log^3 n)$ rounds.\n  (2) Any algorithm that finds a $(3/4+\\epsilon)$-approximation for maximum\nindependent set in the CONGEST model requires $\\Omega(n^2/\\log^3 n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:07:35 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:21:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Efron", "Yuval", ""], ["Grossman", "Ofer", ""], ["Khoury", "Seri", ""]]}, {"id": "2003.07503", "submitter": "Federico Fusco", "authors": "Paul D\\\"utting, Federico Fusco, Philip Lazos, Stefano Leonardi,\n  Rebecca Reiffenh\\\"auser", "title": "Efficient Two-Sided Markets with Limited Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A celebrated impossibility result by Myerson and Satterthwaite (1983) shows\nthat any truthful mechanism for two-sided markets that maximizes social welfare\nmust run a deficit, resulting in a necessity to relax welfare efficiency and\nthe use of approximation mechanisms. Such mechanisms in general make extensive\nuse of the Bayesian priors. In this work, we investigate a question of\nincreasing theoretical and practical importance: how much prior information is\nrequired to design mechanisms with near-optimal approximations?\n  Our first contribution is a more general impossibility result stating that no\nmeaningful approximation is possible without any prior information, expanding\nthe famous impossibility result of Myerson and Satterthwaite.\n  Our second contribution is that one {\\em single sample} (one number per\nitem), arguably a minimum-possible amount of prior information, from each\nseller distribution is sufficient for a large class of two-sided markets. We\nprove matching upper and lower bounds on the best approximation that can be\nobtained with one single sample for subadditive buyers and additive sellers,\nregardless of computational considerations.\n  Our third contribution is the design of computationally efficient blackbox\nreductions that turn any one-sided mechanism into a two-sided mechanism with a\nsmall loss in the approximation, while using only one single sample from each\nseller. On the way, our blackbox-type mechanisms deliver several interesting\npositive results in their own right, often beating even the state of the art\nthat uses full prior information.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:45:38 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 19:36:16 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Fusco", "Federico", ""], ["Lazos", "Philip", ""], ["Leonardi", "Stefano", ""], ["Reiffenh\u00e4user", "Rebecca", ""]]}, {"id": "2003.07589", "submitter": "Ran Duan", "authors": "Ran Duan, Haoqing He, Tianyi Zhang", "title": "A Scaling Algorithm for Weighted $f$-Factors in General Graphs", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum weight perfect $f$-factor problem on any general simple\ngraph $G=(V,E,w)$ with positive integral edge weights $w$, and $n=|V|$,\n$m=|E|$. When we have a function $f:V\\rightarrow \\mathbb{N}_+$ on vertices, a\nperfect $f$-factor is a generalized matching so that every vertex $u$ is\nmatched to $f(u)$ different edges. The previous best algorithms on this problem\nhave running time $O(m f(V))$ [Gabow 2018] or $\\tilde{O}(W(f(V))^{2.373}))$\n[Gabow and Sankowski 2013], where $W$ is the maximum edge weight, and\n$f(V)=\\sum_{u\\in V}f(u)$. In this paper, we present a scaling algorithm for\nthis problem with running time $\\tilde{O}(mn^{2/3}\\log W)$. Previously this\nbound is only known for bipartite graphs [Gabow and Tarjan 1989]. The running\ntime of our algorithm is independent of $f(V)$, and consequently it first\nbreaks the $\\Omega(mn)$ barrier for large $f(V)$ even for the unweighted\n$f$-factor problem in general graphs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 09:05:16 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Duan", "Ran", ""], ["He", "Haoqing", ""], ["Zhang", "Tianyi", ""]]}, {"id": "2003.07746", "submitter": "Arya Tanmay Gupta", "authors": "Arya Tanmay Gupta, Swapnil A. Lokhande, Kaushik Mondal", "title": "NP-Completeness Results for Graph Burning on Geometric Graphs", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-67899-9_6", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph burning runs on discrete time steps. The aim is to burn all the\nvertices in a given graph in the least number of time steps. This number is\nknown to be the burning number of the graph. The spread of social influence, an\nalarm, or a social contagion can be modeled using graph burning. The less the\nburning number, the faster the spread.\n  Optimal burning of general graphs is NP-Hard. There is a 3-approximation\nalgorithm to burn general graphs where as better approximation factors are\nthere for many sub classes. Here we study burning of grids; provide a lower\nbound for burning arbitrary grids and a 2-approximation algorithm for burning\nsquare grids. On the other hand, burning path forests, spider graphs, and trees\nwith maximum degree three is already known to be NP-Complete. In this article\nwe show burning problem to be NP-Complete on connected interval graphs,\npermutation graphs and several other geometric graph classes as corollaries.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:33:46 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 01:39:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gupta", "Arya Tanmay", ""], ["Lokhande", "Swapnil A.", ""], ["Mondal", "Kaushik", ""]]}, {"id": "2003.07793", "submitter": "Akanksha Agrawal", "authors": "Akanksha Agrawal, Kristine V.K. Knudsen, Daniel Lokshtanov, Saket\n  Saurabh, and Meirav Zehavi", "title": "The Parameterized Complexity of Guarding Almost Convex Polygons", "comments": "To appear in SoCG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Art Gallery is a fundamental visibility problem in Computational Geometry.\nThe input consists of a simple polygon P, (possibly infinite) sets G and C of\npoints within P, and an integer k; the task is to decide if at most k guards\ncan be placed on points in G so that every point in C is visible to at least\none guard. In the classic formulation of Art Gallery, G and C consist of all\nthe points within P. Other well-known variants restrict G and C to consist\neither of all the points on the boundary of P or of all the vertices of P.\nRecently, three new important discoveries were made: the above mentioned\nvariants of Art Gallery are all W[1]-hard with respect to k [Bonnet and\nMiltzow, ESA'16], the classic variant has an O(log k)-approximation algorithm\n[Bonnet and Miltzow, SoCG'17], and it may require irrational guards [Abrahamsen\net al., SoCG'17]. Building upon the third result, the classic variant and the\ncase where G consists only of all the points on the boundary of P were both\nshown to be \\exists R-complete~[Abrahamsen et al., STOC'18]. Even when both G\nand C consist only of all the points on the boundary of P, the problem is not\nknown to be in NP.\n  Given the first discovery, the following question was posed by Giannopoulos\n[Lorentz Center Workshop, 2016]: Is Art Gallery FPT with respect to r, the\nnumber of reflex vertices? We focus on the variant where G and C are all the\nvertices of P, called Vertex-Vertex Art Gallery. We show that Vertex-Vertex Art\nGallery is solvable in time r^{O(r^2)}n^{O(1)}. Our approach also extends to\nassert that Vertex-Boundary Art Gallery and Boundary-Vertex Art Gallery are\nboth FPT. We utilize structural properties of \"almost convex polygons\" to\npresent a two-stage reduction from Vertex-Vertex Art Gallery to a new\nconstraint satisfaction problem (whose solution is also provided in this paper)\nwhere constraints have arity 2 and involve monotone functions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:16:07 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Knudsen", "Kristine V. K.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2003.07810", "submitter": "Hong Zhou", "authors": "Lap Chi Lau, Hong Zhou", "title": "A Spectral Approach to Network Design", "comments": "Improved bound on one-sided spectral rounding by a randomized\n  swapping algorithm. Added the proof of the deterministic algorithm for\n  additive sparsifers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a spectral approach to design approximation algorithms for network\ndesign problems. We observe that the underlying mathematical questions are the\nspectral rounding problems, which were studied in spectral sparsification and\nin discrepancy theory. We extend these results to incorporate additional\nnon-negative linear constraints, and show that they can be used to\nsignificantly extend the scope of network design problems that can be solved.\nOur algorithm for spectral rounding is an iterative randomized rounding\nalgorithm based on the regret minimization framework. In some settings, this\nprovides an alternative spectral algorithm to achieve constant factor\napproximation for the classical survivable network design problem, and\npartially answers a question of Bansal about survivable network design with\nconcentration property. We also show many other applications of the spectral\nrounding results, including weighted experimental design and additive spectral\nsparsification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:44:13 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 00:25:18 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Lau", "Lap Chi", ""], ["Zhou", "Hong", ""]]}, {"id": "2003.07903", "submitter": "Huck Bennett", "authors": "Huck Bennett, Chris Peikert", "title": "Hardness of Bounded Distance Decoding on Lattices in $\\ell_p$ Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\cc}[1]{\\mathsf{#1}} \\newcommand{\\NP}{\\cc{NP}}\n\\newcommand{\\problem}[1]{\\mathrm{#1}} \\newcommand{\\BDD}{\\problem{BDD}}\n  $Bounded Distance Decoding $\\BDD_{p,\\alpha}$ is the problem of decoding a\nlattice when the target point is promised to be within an $\\alpha$ factor of\nthe minimum distance of the lattice, in the $\\ell_{p}$ norm. We prove that\n$\\BDD_{p, \\alpha}$ is $\\NP$-hard under randomized reductions where $\\alpha \\to\n1/2$ as $p \\to \\infty$ (and for $\\alpha=1/2$ when $p=\\infty$), thereby showing\nthe hardness of decoding for distances approaching the unique-decoding radius\nfor large $p$. We also show fine-grained hardness for $\\BDD_{p,\\alpha}$. For\nexample, we prove that for all $p \\in [1,\\infty) \\setminus 2\\Z$ and constants\n$C > 1, \\eps > 0$, there is no $2^{(1-\\eps)n/C}$-time algorithm for\n$\\BDD_{p,\\alpha}$ for some constant $\\alpha$ (which approaches $1/2$ as $p \\to\n\\infty$), assuming the randomized Strong Exponential Time Hypothesis (SETH).\nMoreover, essentially all of our results also hold (under analogous non-uniform\nassumptions) for $\\BDD$ with preprocessing, in which unbounded precomputation\ncan be applied to the lattice before the target is available.\n  Compared to prior work on the hardness of $\\BDD_{p,\\alpha}$ by Liu,\nLyubashevsky, and Micciancio (APPROX-RANDOM 2008), our results improve the\nvalues of $\\alpha$ for which the problem is known to be $\\NP$-hard for all $p >\np_1 \\approx 4.2773$, and give the very first fine-grained hardness for $\\BDD$\n(in any norm). Our reductions rely on a special family of \"locally dense\"\nlattices in $\\ell_{p}$ norms, which we construct by modifying the\ninteger-lattice sparsification technique of Aggarwal and Stephens-Davidowitz\n(STOC 2018).\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:23:49 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Bennett", "Huck", ""], ["Peikert", "Chris", ""]]}, {"id": "2003.08072", "submitter": "Agniva Chowdhury", "authors": "Agniva Chowdhury, Palma London, Haim Avron, Petros Drineas", "title": "Speeding up Linear Programming using Randomized Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear programming (LP) is an extremely useful tool and has been successfully\napplied to solve various problems in a wide range of areas, including\noperations research, engineering, economics, or even more abstract mathematical\nareas such as combinatorics. It is also used in many machine learning\napplications, such as $\\ell_1$-regularized SVMs, basis pursuit, nonnegative\nmatrix factorization, etc. Interior Point Methods (IPMs) are one of the most\npopular methods to solve LPs both in theory and in practice. Their underlying\ncomplexity is dominated by the cost of solving a system of linear equations at\neach iteration. In this paper, we consider \\emph{infeasible} IPMs for the\nspecial case where the number of variables is much larger than the number of\nconstraints. Using tools from Randomized Linear Algebra, we present a\npreconditioning technique that, when combined with the Conjugate Gradient\niterative solver, provably guarantees that infeasible IPM algorithms (suitably\nmodified to account for the error incurred by the approximate solver), converge\nto a feasible, approximately optimal solution, without increasing their\niteration complexity. Our empirical evaluations verify our theoretical results\non both real-world and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:19:15 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Chowdhury", "Agniva", ""], ["London", "Palma", ""], ["Avron", "Haim", ""], ["Drineas", "Petros", ""]]}, {"id": "2003.08078", "submitter": "Kevin Tian", "authors": "Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Yin Tat Lee,\n  Aaron Sidford, Kevin Tian", "title": "Acceleration with a Ball Optimization Oracle", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an oracle which takes a point $x$ and returns the minimizer of a\nconvex function $f$ in an $\\ell_2$ ball of radius $r$ around $x$. It is\nstraightforward to show that roughly $r^{-1}\\log\\frac{1}{\\epsilon}$ calls to\nthe oracle suffice to find an $\\epsilon$-approximate minimizer of $f$ in an\n$\\ell_2$ unit ball. Perhaps surprisingly, this is not optimal: we design an\naccelerated algorithm which attains an $\\epsilon$-approximate minimizer with\nroughly $r^{-2/3} \\log \\frac{1}{\\epsilon}$ oracle queries, and give a matching\nlower bound. Further, we implement ball optimization oracles for functions with\nlocally stable Hessians using a variant of Newton's method. The resulting\nalgorithm applies to a number of problems of practical and theoretical import,\nimproving upon previous results for logistic and $\\ell_\\infty$ regression and\nachieving guarantees comparable to the state-of-the-art for $\\ell_p$\nregression.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:39:45 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Carmon", "Yair", ""], ["Jambulapati", "Arun", ""], ["Jiang", "Qijia", ""], ["Jin", "Yujia", ""], ["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "2003.08097", "submitter": "Diptarama Hendrian", "authors": "Hiroaki Naganuma, Diptarama Hendrian, Ryo Yoshinaka, Ayumi Shinohara,\n  Naoki Kobayashi", "title": "Grammar compression with probabilistic context-free grammar", "comments": "11 pages, 3 figures, accepted for poster presentation at DCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for universal lossless text compression, based on\ngrammar compression. In the literature, a target string $T$ has been compressed\nas a context-free grammar $G$ in Chomsky normal form satisfying $L(G) = \\{T\\}$.\nSuch a grammar is often called a \\emph{straight-line program} (SLP). In this\npaper, we consider a probabilistic grammar $G$ that generates $T$, but not\nnecessarily as a unique element of $L(G)$. In order to recover the original\ntext $T$ unambiguously, we keep both the grammar $G$ and the derivation tree of\n$T$ from the start symbol in $G$, in compressed form. We show some simple\nevidence that our proposal is indeed more efficient than SLPs for certain\ntexts, both from theoretical and practical points of view.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 08:47:16 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Naganuma", "Hiroaki", ""], ["Hendrian", "Diptarama", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""], ["Kobayashi", "Naoki", ""]]}, {"id": "2003.08144", "submitter": "Florian Ingels", "authors": "Florian Ingels and Romain Aza\\\"is", "title": "Enumeration of Unordered Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse search is a convenient method for enumerating structured objects,\nthat can be used both to address theoretical issues and to solve data mining\nproblems. This method has already been successfully developed to handle\nunordered trees. If the literature proposes solutions to enumerate singletons\nof trees, we study in this article a more general, higher combinatorial\nproblem, the enumeration of sets of trees - forests. By compressing each forest\ninto a Directed Acyclic Graph (DAG), we develop a reverse search like method to\nenumerate DAG compressing forests. Remarkably, we prove that these DAG are in\nbijection with the row-Fishburn matrices, a well-studied class of combinatorial\nobjects. In a second step, we derive our forest enumeration to provide\nalgorithms for tackling two related problems : (i) the enumeration of\n\"subforests\" of a forest, and (ii) the frequent \"subforest\" mining problem. All\nthe methods presented in this article enumerate each item uniquely, up to\nisomorphism.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 10:46:44 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 14:51:08 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ingels", "Florian", ""], ["Aza\u00efs", "Romain", ""]]}, {"id": "2003.08211", "submitter": "Shoupu Wan", "authors": "Shoupu Wan", "title": "An Efficient Implementation of Manacher's Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manacher's algorithm has been shown to be optimal to the longest palindromic\nsubstring problem. Many of the existing implementations of this algorithm,\nhowever, unanimously required in-memory construction of an augmented string\nthat is twice as long as the original string. Although it has found widespread\nuse, we found that this preprocessing is neither economic nor necessary. We\npresent a more efficient implementation of Manacher's algorithm based on index\nmapping that makes the string augmentation process obsolete.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 04:26:35 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 01:23:39 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Wan", "Shoupu", ""]]}, {"id": "2003.08288", "submitter": "Siu-Wing Cheng", "authors": "Siu-Wing Cheng and Man-Kit Lau", "title": "Dynamic Distribution-Sensitive Point Location", "comments": "To appear in Proceedings of the International Symposium of\n  Computational Geometry, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic data structure for the distribution-sensitive point\nlocation problem. Suppose that there is a fixed query distribution in\n$\\mathbb{R}^2$, and we are given an oracle that can return in $O(1)$ time the\nprobability of a query point falling into a polygonal region of constant\ncomplexity. We can maintain a convex subdivision $\\cal S$ with $n$ vertices\nsuch that each query is answered in $O(\\mathrm{OPT})$ expected time, where OPT\nis the minimum expected time of the best linear decision tree for point\nlocation in $\\cal S$. The space and construction time are $O(n\\log^2 n)$. An\nupdate of $\\cal S$ as a mixed sequence of $k$ edge insertions and deletions\ntakes $O(k\\log^5 n)$ amortized time. As a corollary, the randomized incremental\nconstruction of the Voronoi diagram of $n$ sites can be performed in $O(n\\log^5\nn)$ expected time so that, during the incremental construction, a nearest\nneighbor query at any time can be answered optimally with respect to the\nintermediate Voronoi diagram at that time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:51:52 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 03:01:49 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 04:37:46 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2020 06:38:38 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Cheng", "Siu-Wing", ""], ["Lau", "Man-Kit", ""]]}, {"id": "2003.08377", "submitter": "Miklos Z. Racz", "authors": "Mayee F. Chen and Miklos Z. Racz", "title": "Network disruption: maximizing disagreement and polarization in social\n  networks", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.GT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a marked increase in the spread of misinformation, a\nphenomenon which has been accelerated and amplified by social media such as\nFacebook and Twitter. While some actors spread misinformation to push a\nspecific agenda, it has also been widely documented that others aim to simply\ndisrupt the network by increasing disagreement and polarization across the\nnetwork and thereby destabilizing society. Popular social networks are also\nvulnerable to large-scale attacks. Motivated by this reality, we introduce a\nsimple model of network disruption where an adversary can take over a limited\nnumber of user profiles in a social network with the aim of maximizing\ndisagreement and/or polarization in the network.\n  We investigate this model both theoretically and empirically. We show that\nthe adversary will always change the opinion of a taken-over profile to an\nextreme in order to maximize disruption. We also prove that an adversary can\nincrease disagreement / polarization at most linearly in the number of user\nprofiles it takes over. Furthermore, we present a detailed empirical study of\nseveral natural algorithms for the adversary on both synthetic networks and\nreal world (Reddit and Twitter) data sets. These show that even simple,\nunsophisticated heuristics, such as targeting centrists, can disrupt a network\neffectively, causing a large increase in disagreement / polarization. Studying\nthe problem of network disruption through the lens of an adversary thus\nhighlights the seriousness of the problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:54:46 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:24:29 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Chen", "Mayee F.", ""], ["Racz", "Miklos Z.", ""]]}, {"id": "2003.08627", "submitter": "Marcin Jurdzi\\'nski", "authors": "Laure Daviaud and Marcin Jurdzi\\'nski and K. S. Thejaswini", "title": "The Strahler number of a parity game", "comments": "To appear in ICALP 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2020.123", "report-no": null, "categories": "cs.DS cs.FL cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Strahler number of a rooted tree is the largest height of a perfect\nbinary tree that is its minor. The Strahler number of a parity game is proposed\nto be defined as the smallest Strahler number of the tree of any of its\nattractor decompositions. It is proved that parity games can be solved in\nquasi-linear space and in time that is polynomial in the number of vertices~$n$\nand linear in $({d}/{2k})^k$, where $d$ is the number of priorities and $k$ is\nthe Strahler number. This complexity is quasi-polynomial because the Strahler\nnumber is at most logarithmic in the number of vertices. The proof is based on\na new construction of small Strahler-universal trees.\n  It is shown that the Strahler number of a parity game is a robust parameter:\nit coincides with its alternative version based on trees of progress measures\nand with the register number defined by Lehtinen~(2018). It follows that parity\ngames can be solved in quasi-linear space and in time that is polynomial in the\nnumber of vertices and linear in $({d}/{2k})^k$, where $k$ is the register\nnumber. This significantly improves the running times and space achieved for\nparity games of bounded register number by Lehtinen (2018) and by Parys (2020).\n  The running time of the algorithm based on small Strahler-universal trees\nyields a novel trade-off $k \\cdot \\lg(d/k) = O(\\log n)$ between the two natural\nparameters that measure the structural complexity of a parity game, which\nallows solving parity games in polynomial time. This includes as special cases\nthe asymptotic settings of those parameters covered by the results of Calude,\nJain Khoussainov, Li, and Stephan (2017), of Jurdzi\\'nski and Lazi\\'c (2017),\nand of Lehtinen (2018), and it significantly extends the range of such\nsettings, for example to $d = 2^{O\\left(\\sqrt{\\lg n}\\right)}$ and $k =\nO\\!\\left(\\sqrt{\\lg n}\\right)$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:36:48 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 12:36:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Daviaud", "Laure", ""], ["Jurdzi\u0144ski", "Marcin", ""], ["Thejaswini", "K. S.", ""]]}, {"id": "2003.08929", "submitter": "Yang P. Liu", "authors": "Yang P. Liu, Aaron Sidford", "title": "Faster Divergence Maximization for Faster Maximum Flow", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an algorithm which given any $m$-edge $n$-vertex\ndirected graph with integer capacities at most $U$ computes a maximum $s$-$t$\nflow for any vertices $s$ and $t$ in $m^{4/3+o(1)}U^{1/3}$ time. This improves\nupon the previous best running times of $m^{11/8+o(1)}U^{1/4}$ (Liu Sidford\n2019), $\\tilde{O}(m \\sqrt{n} \\log U)$ (Lee Sidford 2014), and $O(mn)$ (Orlin\n2013) when the graph is not too dense or has large capacities.\n  To achieve the results this paper we build upon previous algorithmic\napproaches to maximum flow based on interior point methods (IPMs). In\nparticular, we overcome a key bottleneck of previous advances in IPMs for\nmaxflow (M\\k{a}dry 2013, M\\k{a}dry 2016, Liu Sidford 2019), which make progress\nby maximizing the energy of local $\\ell_2$ norm minimizing electric flows. We\ngeneralize this approach and instead maximize the divergence of flows which\nminimize the Bregman divergence distance with respect to the weighted\nlogarithmic barrier. This allows our algorithm to avoid dependencies on the\n$\\ell_4$ norm that appear in other IPM frameworks (e.g. Cohen M\\k{a}dry\nSankowski Vladu 2017, Axiotis M\\k{a}dry Vladu 2020). Further, we show that\nsmoothed $\\ell_2$-$\\ell_p$ flows (Kyng, Peng, Sachdeva, Wang 2019), which we\npreviously used to efficiently maximize energy (Liu Sidford 2019), can also be\nused to efficiently maximize divergence, thereby yielding our desired runtimes.\nWe believe both this generalized view of energy maximization and generalized\nflow solvers we develop may be of further interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:51:14 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 19:29:15 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Liu", "Yang P.", ""], ["Sidford", "Aaron", ""]]}, {"id": "2003.09297", "submitter": "Giorgi Nadiradze", "authors": "Dan Alistarh, Giorgi Nadiradze, Amirmojtaba Sabour", "title": "Dynamic Averaging Load Balancing on Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following dynamic load-balancing process: given an underlying\ngraph $G$ with $n$ nodes, in each step $t\\geq 0$, one unit of load is created,\nand placed at a randomly chosen graph node. In the same step, the chosen node\npicks a random neighbor, and the two nodes balance their loads by averaging\nthem. We are interested in the expected gap between the minimum and maximum\nloads at nodes as the process progresses, and its dependence on $n$ and on the\ngraph structure.\n  Similar variants of the above graphical balanced allocation process have been\nstudied by Peres, Talwar, and Wieder, and by Sauerwald and Sun for regular\ngraphs. These authors left as open the question of characterizing the gap in\nthe case of \\emph{cycle graphs} in the \\emph{dynamic} case, where weights are\ncreated during the algorithm's execution. For this case, the only known upper\nbound is of $\\mathcal{O}( n \\log n )$, following from a majorization argument\ndue to Peres, Talwar, and Wieder, which analyzes a related graphical allocation\nprocess.\n  In this paper, we provide an upper bound of $\\mathcal{O} ( \\sqrt n \\log n )$\non the expected gap of the above process for cycles of length $n$. We introduce\na new potential analysis technique, which enables us to bound the difference in\nload between $k$-hop neighbors on the cycle, for any $k \\leq n / 2$. We\ncomplement this with a \"gap covering\" argument, which bounds the maximum value\nof the gap by bounding its value across all possible subsets of a certain\nstructure, and recursively bounding the gaps within each subset. We provide\nanalytical and experimental evidence that our upper bound on the gap is tight\nup to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 14:37:29 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Alistarh", "Dan", ""], ["Nadiradze", "Giorgi", ""], ["Sabour", "Amirmojtaba", ""]]}, {"id": "2003.09363", "submitter": "Giorgi Nadiradze", "authors": "Dan Alistarh, Nikita Koval, Giorgi Nadiradze", "title": "Efficiency Guarantees for Parallel Incremental Algorithms under Relaxed\n  Schedulers", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323201", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several classic problems in graph processing and computational geometry are\nsolved via incremental algorithms, which split computation into a series of\nsmall tasks acting on shared state, which gets updated progressively.\n  While the sequential variant of such algorithms usually specifies a fixed\n(but sometimes random) order in which the tasks should be performed, a standard\napproach to parallelizing such algorithms is to relax this constraint to allow\nfor out-of-order parallel execution. This is the case for parallel\nimplementations of Dijkstra's single-source shortest-paths algorithm (SSSP),\nand for parallel Delaunay mesh triangulation.\n  While many software frameworks parallelize incremental computation in this\nway, it is still not well understood whether this relaxed ordering approach can\nstill provide any complexity guarantees.\n  In this paper, we address this problem, and analyze the efficiency guarantees\nprovided by a range of incremental algorithms when parallelized via relaxed\nschedulers.\n  We show that, for algorithms such as Delaunay mesh triangulation and sorting\nby insertion, schedulers with a maximum relaxation factor of $k$ in terms of\nthe maximum priority inversion allowed will introduce a maximum amount of\nwasted work of $O(log(n) poly (k) ), $ where $n$ is the number of tasks to be\nexecuted.\n  For SSSP, we show that the additional work is $O(poly (k) d_{max} / w_{min}),\n$ where $d_{\\max}$ is the maximum distance between two nodes, and $w_{min}$ is\nthe minimum such distance. In practical settings where $n \\gg k$, this suggests\nthat the overheads of relaxation will be outweighed by the improved scalability\nof the relaxed scheduler.\n  On the negative side, we provide lower bounds showing that certain algorithms\nwill inherently incur a non-trivial amount of wasted work due to scheduler\nrelaxation, even for relatively benign relaxed schedulers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:33:29 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 00:31:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Alistarh", "Dan", ""], ["Koval", "Nikita", ""], ["Nadiradze", "Giorgi", ""]]}, {"id": "2003.09481", "submitter": "Simeon Krastnikov", "authors": "Simeon Krastnikov, Florian Kerschbaum, Douglas Stebila", "title": "Efficient Oblivious Database Joins", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), 13(11): 2132-2145, 2020", "doi": "10.14778/3407790.3407814", "report-no": null, "categories": "cs.DB cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major algorithmic challenge in designing applications intended for secure\nremote execution is ensuring that they are oblivious to their inputs, in the\nsense that their memory access patterns do not leak sensitive information to\nthe server. This problem is particularly relevant to cloud databases that wish\nto allow queries over the client's encrypted data. One of the major obstacles\nto such a goal is the join operator, which is non-trivial to implement\nobliviously without resorting to generic but inefficient solutions like\nOblivious RAM (ORAM).\n  We present an oblivious algorithm for equi-joins which (up to a logarithmic\nfactor) matches the optimal $O(n\\log n)$ complexity of the standard non-secure\nsort-merge join (on inputs producing $O(n)$ outputs). We do not use use\nexpensive primitives like ORAM or rely on unrealistic hardware or security\nassumptions. Our approach, which is based on sorting networks and novel\nprovably-oblivious constructions, is conceptually simple, easily verifiable,\nand very efficient in practice. Its data-independent algorithmic structure\nmakes it secure in various different settings for remote computation, even in\nthose that are known to be vulnerable to certain side-channel attacks (such as\nIntel SGX) or with strict requirements for low circuit complexity (like secure\nmultiparty computation). We confirm that our approach is easily realizable\nthrough a compact implementation which matches our expectations for performance\nand is shown, both formally and empirically, to possess the desired security\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 19:49:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 14:52:56 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 17:05:25 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Krastnikov", "Simeon", ""], ["Kerschbaum", "Florian", ""], ["Stebila", "Douglas", ""]]}, {"id": "2003.09584", "submitter": "Svante Janson", "authors": "Svante Janson and Wojciech Szpankowski", "title": "Hidden Words Statistics for Large Patterns", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study here the so called subsequence pattern matching also known as hidden\npattern matching in which one searches for a given pattern $w$ of length $m$ as\na subsequence in a random text of length $n$. The quantity of interest is the\nnumber of occurrences of $w$ as a subsequence (i.e., occurring in not\nnecessarily consecutive text locations). This problem finds many applications\nfrom intrusion detection, to trace reconstruction, to deletion channel, and to\nDNA-based storage systems. In all of these applications, the pattern $w$ is of\nvariable length. To the best of our knowledge this problem was only tackled for\na fixed length $m=O(1)$ [Flajolet, Szpankowski and Vall\\'ee, 2006]. In our main\nresult we prove that for $m=o(n^{1/3})$ the number of subsequence occurrences\nis normally distributed. In addition, we show that under some constraints on\nthe structure of $w$ the asymptotic normality can be extended to\n$m=o(\\sqrt{n})$. For a special pattern $w$ consisting of the same symbol, we\nindicate that for $m=o(n)$ the distribution of number of subsequences is either\nasymptotically normal or asymptotically log normal. We conjecture that this\ndichotomy is true for all patterns. We use Hoeffding's projection method for\n$U$-statistics to prove our findings.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 06:07:17 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Janson", "Svante", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "2003.09756", "submitter": "Amir Zandieh", "authors": "Michael Kapralov, Navid Nouri, Ilya Razenshteyn, Ameya Velingker, Amir\n  Zandieh", "title": "Scaling up Kernel Ridge Regression via Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random binning features, introduced in the seminal paper of Rahimi and Recht\n(2007), are an efficient method for approximating a kernel matrix using\nlocality sensitive hashing. Random binning features provide a very simple and\nefficient way of approximating the Laplace kernel but unfortunately do not\napply to many important classes of kernels, notably ones that generate smooth\nGaussian processes, such as the Gaussian kernel and Matern kernel. In this\npaper, we introduce a simple weighted version of random binning features and\nshow that the corresponding kernel function generates Gaussian processes of any\ndesired smoothness. We show that our weighted random binning features provide a\nspectral approximation to the corresponding kernel matrix, leading to efficient\nalgorithms for kernel ridge regression. Experiments on large scale regression\ndatasets show that our method outperforms the accuracy of random Fourier\nfeatures method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 21:41:16 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Kapralov", "Michael", ""], ["Nouri", "Navid", ""], ["Razenshteyn", "Ilya", ""], ["Velingker", "Ameya", ""], ["Zandieh", "Amir", ""]]}, {"id": "2003.09895", "submitter": "Peter Robinson", "authors": "Peter Robinson", "title": "Being Fast Means Being Chatty: The Local Information Cost of Graph\n  Spanners", "comments": "An earlier version of this paper appeared in the proceedings of SODA\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new measure for quantifying the amount of information that the\nnodes in a network need to learn to jointly solve a graph problem. We show that\nthe local information cost ($\\textsf{LIC}$) presents a natural lower bound on\nthe communication complexity of distributed algorithms. For the synchronous\nCONGEST-KT1 model, where each node has initial knowledge of its neighbors' IDs,\nwe prove that $\\Omega(\\textsf{LIC}_\\gamma(P)/ \\log\\tau \\log n)$ bits are\nrequired for solving a graph problem $P$ with a $\\tau$-round algorithm that\nerrs with probability at most $\\gamma$. Our result is the first lower bound\nthat yields a general trade-off between communication and time for graph\nproblems in the CONGEST-KT1 model.\n  We demonstrate how to apply the local information cost by deriving a lower\nbound on the communication complexity of computing a $(2t-1)$-spanner that\nconsists of at most $O(n^{1+1/t + \\epsilon})$ edges, where $\\epsilon =\n\\Theta(1/t^2)$. Our main result is that any $O(\\textsf{poly}(n))$-time\nalgorithm must send at least $\\tilde\\Omega((1/t^2) n^{1+1/2t})$ bits in the\nCONGEST model under the KT1 assumption. Previously, only a trivial lower bound\nof $\\tilde \\Omega(n)$ bits was known for this problem.\n  A consequence of our lower bound is that achieving both time- and\ncommunication-optimality is impossible when designing a distributed spanner\nalgorithm. In light of the work of King, Kutten, and Thorup (PODC 2015), this\nshows that computing a minimum spanning tree can be done significantly faster\nthan finding a spanner when considering algorithms with $\\tilde O(n)$\ncommunication complexity. Our result also implies time complexity lower bounds\nfor constructing a spanner in the node-congested clique of Augustine et al.\n(2019) and in the push-pull gossip model with limited bandwidth.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 13:54:26 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 06:42:25 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 22:36:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Robinson", "Peter", ""]]}, {"id": "2003.10069", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Natesh S. Pillai, Ashwin Sah, Mehtaab Sawhney, Aaron\n  Smith", "title": "Fast and memory-optimal dimension reduction using Kac's walk", "comments": "27 pages, comments welcome! This version: significant new results;\n  added two co-authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze dimension reduction algorithms based on the Kac walk\nand discrete variants.\n  (1) For $n$ points in $\\mathbb{R}^{d}$, we design an optimal\nJohnson-Lindenstrauss (JL) transform based on the Kac walk which can be applied\nto any vector in time $O(d\\log{d})$ for essentially the same restriction on $n$\nas in the best-known transforms due to Ailon and Liberty [SODA, 2008], and\nBamberger and Krahmer [arXiv, 2017]. Our algorithm is memory-optimal, and\noutperforms existing algorithms in regimes when $n$ is sufficiently large and\nthe distortion parameter is sufficiently small. In particular, this confirms a\nconjecture of Ailon and Chazelle [STOC, 2006] in a stronger form.\n  (2) The same construction gives a simple transform with optimal Restricted\nIsometry Property (RIP) which can be applied in time $O(d\\log{d})$ for\nessentially the same range of sparsity as in the best-known such transform due\nto Ailon and Rauhut [Discrete Comput. Geom., 2014].\n  (3) We show that by fixing the angle in the Kac walk to be $\\pi/4$\nthroughout, one obtains optimal JL and RIP transforms with almost the same\nrunning time, thereby confirming -- up to a $\\log\\log{d}$ factor -- a\nconjecture of Avron, Maymounkov, and Toledo [SIAM J. Sci. Comput., 2010]. Our\nmoment-based analysis of this modification of the Kac walk may also be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:56:19 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 01:40:32 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 13:40:36 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Jain", "Vishesh", ""], ["Pillai", "Natesh S.", ""], ["Sah", "Ashwin", ""], ["Sawhney", "Mehtaab", ""], ["Smith", "Aaron", ""]]}, {"id": "2003.10570", "submitter": "Florent Foucaud", "authors": "Florent Foucaud, Benjamin Gras, Anthony Perez, Florian Sikora", "title": "On the complexity of Broadcast Domination and Multipacking in digraphs", "comments": "Extended abstract to appear in proceedings of IWOCA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the two dual covering and packing distance-based\nproblems Broadcast Domination and Multipacking in digraphs. A dominating\nbroadcast of a digraph $D$ is a function $f:V(D)\\to\\mathbb{N}$ such that for\neach vertex $v$ of $D$, there exists a vertex $t$ with $f(t)>0$ having a\ndirected path to $v$ of length at most $f(t)$. The cost of $f$ is the sum of\n$f(v)$ over all vertices $v$. A multipacking is a set $S$ of vertices of $D$\nsuch that for each vertex $v$ of $D$ and for every integer $d$, there are at\nmost $d$ vertices from $S$ within directed distance at most $d$ from $v$. The\nmaximum size of a multipacking of $D$ is a lower bound to the minimum cost of a\ndominating broadcast of $D$. Let Broadcast Domination denote the problem of\ndeciding whether a given digraph $D$ has a dominating broadcast of cost at most\n$k$, and Multipacking the problem of deciding whether $D$ has a multipacking of\nsize at least $k$. It is known that Broadcast Domination is polynomial-time\nsolvable for the class of all undirected graphs (that is, symmetric digraphs),\nwhile polynomial-time algorithms for Multipacking are known only for a few\nclasses of undirected graphs. We prove that Broadcast Domination and\nMultipacking are both NP-complete for digraphs, even for planar layered acyclic\ndigraphs of small maximum degree. Moreover, when parameterized by the solution\ncost/solution size, we show that the problems are W-hard. We also show that\nBroadcast Domination is FPT on acyclic digraphs, and that it does not admit a\npolynomial kernel for such inputs, unless the polynomial hierarchy collapses to\nits third level. In addition, we show that both problems are FPT when\nparameterized by the solution cost/solution size together with the maximum\nout-degree, and as well, by the vertex cover number. Finally, we give for both\nproblems polynomial-time algorithms for some subclasses of acyclic digraphs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 22:31:41 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 14:59:21 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Foucaud", "Florent", ""], ["Gras", "Benjamin", ""], ["Perez", "Anthony", ""], ["Sikora", "Florian", ""]]}, {"id": "2003.10588", "submitter": "Alireza Samadian", "authors": "Mahmoud Abo-Khamis and Sungjin Im and Benjamin Moseley and Kirk Pruhs\n  and Alireza Samadian", "title": "Approximate Aggregate Queries Under Additive Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating certain types of functional aggregation\nqueries on relational data subject to additive inequalities. Such aggregation\nqueries, with a smallish number of additive inequalities, arise\nnaturally/commonly in many applications, particularly in learning applications.\nWe give a relatively complete categorization of the computational complexity of\nsuch problems. We first show that the problem is NP-hard, even in the case of\none additive inequality. Thus we turn to approximating the query. Our main\nresult is an efficient algorithm for approximating, with arbitrarily small\nrelative error, many natural aggregation queries with one additive inequality.\nWe give examples of natural queries that can be efficiently solved using this\nalgorithm. In contrast, we show that the situation with two additive\ninequalities is quite different, by showing that it is NP-hard to evaluate\nsimple aggregation queries, with two additive inequalities, with any bounded\nrelative error.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 00:24:18 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 20:52:34 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Abo-Khamis", "Mahmoud", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""], ["Samadian", "Alireza", ""]]}, {"id": "2003.10612", "submitter": "Marcos Villagra", "authors": "Fabricio Mendoza-Granada and Marcos Villagra", "title": "A Distributed Algorithm for Spectral Sparsification of Graphs with\n  Applications to Data Clustering", "comments": "11 pages. In v2 some typos were corrected", "journal-ref": null, "doi": "10.1007/978-3-030-63072-0_31", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral sparsification is a technique that is used to reduce the number of\nnon-zero entries in a positive semidefinite matrix with little changes to its\nspectrum. In particular, the main application of spectral sparsification is to\nconstruct sparse graphs whose spectra are close to a given dense graph. We\nstudy spectral sparsification under the assumption that the edges of a graph\nare allocated among sites which can communicate among each other. In this work\nwe show that if a graph is allocated among several sites, the union of the\nspectral sparsifiers of each induced subgraph give us an spectral sparsifier of\nthe original graph. In contrast to other works in the literature, we present\nprecise computations of the approximation factor of the union of spectral\nsparsifiers and give an explicit calculation of the edge weights. Then we\npresent an application of this result to data clustering in the\nNumber-On-Forehead model of multiparty communication complexity when input data\nis allocated as a sunflower among sites in the party.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:56:06 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 13:25:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mendoza-Granada", "Fabricio", ""], ["Villagra", "Marcos", ""]]}, {"id": "2003.10698", "submitter": "Yogesh Tripathi", "authors": "Vishnu Veerathu and Yogesh Tripathi", "title": "Parameterized Algorithms for Red-Blue Weighted Vertex Cover on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textproc{Weighted Vertex Cover} is a variation of an extensively studied\nNP-complete problem, \\textproc{Vertex Cover}, in which we are given a graph, $G\n= (V,E,w)$, where function $w:V \\rightarrow \\mathbb{Q}^{+}$ and a parameter\n$k$. The objective is to determine if there exists a vertex cover, $S$, such\nthat $\\sum_{v \\in S}w(v) \\leq k$. In our work, we first study the hardness of\n\\textproc{Weighted Vertex Cover} and then examine this problem under\nparameterization by $l$ and $k$, where $l$ is the number of vertices with\nfractional weights. Then, we study the \\textproc{Red-Blue Weighted Vertex\nCover} problem on trees in detail. In this problem, we are given a tree,\n$T=(V,E,w)$, where function $w:V \\rightarrow \\mathbb{Q}^{+}$, a function $c:V\n\\rightarrow \\{R,B\\}$ and two parameters $k$ and $k_R$. We have to determine if\nthere exists a vertex cover, $S$, such that $\\sum_{v \\in S}w(v) \\leq k$ and\n$\\sum_{\\substack{v \\in S\\\\ c(v) = R}}w(v) \\leq k_R$. We tackle this problem by\napplying different reduction techniques and meaningful parameterizations. We\nalso study some restrictive versions of this problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:39:20 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Veerathu", "Vishnu", ""], ["Tripathi", "Yogesh", ""]]}, {"id": "2003.10774", "submitter": "Kenjiro Takazawa", "authors": "Kenjiro Takazawa", "title": "Notes on Equitable Partition into Matching Forests in Mixed Graphs and\n  into $b$-branchings in Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An equitable partition into branchings in a digraph is a partition of the arc\nset into branchings such that the sizes of any two branchings differ at most by\none. For a digraph whose arc set can be partitioned into $k$ branchings, there\nalways exists an equitable partition into $k$ branchings. In this paper, we\npresent two extensions of equitable partitions into branchings in digraphs:\nthose into matching forests in mixed graphs; and into $b$-branchings in\ndigraphs. For matching forests, Kir\\'{a}ly and Yokoi (2018) considered a\ntricriteria equitability based on the sizes of the matching forest, and the\nmatching and branching therein. In contrast to this, we introduce a\nsingle-criterion equitability based on the number of the covered vertices. For\n$b$-branchings, we define an equitability based on the size of the\n$b$-branching and the indegrees of all vertices. For both matching forests and\n$b$-branchings, we prove that equitable partitions always exist.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:17:51 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Takazawa", "Kenjiro", ""]]}, {"id": "2003.10935", "submitter": "Martin Grohe", "authors": "Martin Grohe and Pascal Schweitzer and Daniel Wiebking", "title": "Deep Weisfeiler Leman", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the framework of Deep Weisfeiler Leman algorithms (DeepWL),\nwhich allows the design of purely combinatorial graph isomorphism tests that\nare more powerful than the well-known Weisfeiler-Leman algorithm.\n  We prove that, as an abstract computational model, polynomial time\nDeepWL-algorithms have exactly the same expressiveness as the logic Choiceless\nPolynomial Time (with counting) introduced by Blass, Gurevich, and Shelah (Ann.\nPure Appl. Logic., 1999)\n  It is a well-known open question whether the existence of a polynomial time\ngraph isomorphism test implies the existence of a polynomial time canonisation\nalgorithm. Our main technical result states that for each class of graphs\n(satisfying some mild closure condition), if there is a polynomial time DeepWL\nisomorphism test then there is a polynomial canonisation algorithm for this\nclass. This implies that there is also a logic capturing polynomial time on\nthis class.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 15:53:17 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Grohe", "Martin", ""], ["Schweitzer", "Pascal", ""], ["Wiebking", "Daniel", ""]]}, {"id": "2003.11086", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Jerry Li and Anastasia Voloshinov", "title": "Efficient Algorithms for Multidimensional Segmented Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of fixed design {\\em multidimensional\nsegmented regression}: Given noisy samples from a function $f$, promised to be\npiecewise linear on an unknown set of $k$ rectangles, we want to recover $f$ up\nto a desired accuracy in mean-squared error. We provide the first sample and\ncomputationally efficient algorithm for this problem in any fixed dimension.\nOur algorithm relies on a simple iterative merging approach, which is novel in\nthe multidimensional setting. Our experimental evaluation on both synthetic and\nreal datasets shows that our algorithm is competitive and in some cases\noutperforms state-of-the-art heuristics. Code of our implementation is\navailable at\n\\url{https://github.com/avoloshinov/multidimensional-segmented-regression}.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:39:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Voloshinov", "Anastasia", ""]]}, {"id": "2003.11238", "submitter": "Krishnakumar Balasubramanian", "authors": "Jiaxiang Li, Krishnakumar Balasubramanian, Shiqian Ma", "title": "Stochastic Zeroth-order Riemannian Derivative Estimation and\n  Optimization", "comments": "Additional experimental results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic zeroth-order optimization over Riemannian submanifolds\nembedded in Euclidean space, where the task is to solve Riemannian optimization\nproblem with only noisy objective function evaluations. Towards this, our main\ncontribution is to propose estimators of the Riemannian gradient and Hessian\nfrom noisy objective function evaluations, based on a Riemannian version of the\nGaussian smoothing technique. The proposed estimators overcome the difficulty\nof the non-linearity of the manifold constraint and the issues that arise in\nusing Euclidean Gaussian smoothing techniques when the function is defined only\nover the manifold. We use the proposed estimators to solve Riemannian\noptimization problems in the following settings for the objective function: (i)\nstochastic and gradient-Lipschitz (in both nonconvex and geodesic convex\nsettings), (ii) sum of gradient-Lipschitz and non-smooth functions, and (iii)\nHessian-Lipschitz. For these settings, we analyze the oracle complexity of our\nalgorithms to obtain appropriately defined notions of $\\epsilon$-stationary\npoint or $\\epsilon$-approximate local minimizer. Notably, our complexities are\nindependent of the dimension of the ambient Euclidean space and depend only on\nthe intrinsic dimension of the manifold under consideration. We demonstrate the\napplicability of our algorithms by simulation results and real-world\napplications on black-box stiffness control for robotics and black-box attacks\nto neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 06:58:19 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 06:07:06 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 04:05:52 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Li", "Jiaxiang", ""], ["Balasubramanian", "Krishnakumar", ""], ["Ma", "Shiqian", ""]]}, {"id": "2003.11273", "submitter": "Yixin Cao", "authors": "Yixin Cao and Yuping Ke and Hanchun Yuan", "title": "Polynomial Kernels for Paw-free Edge Modification Problems", "comments": "To appear in the proceedings of the 16th Annual Conference on Theory\n  and Applications of Models of Computation (TAMC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $H$ be a fixed graph. Given a graph $G$ and an integer $k$, the $H$-free\nedge modification problem asks whether it is possible to modify at most $k$\nedges in $G$ to make it $H$-free. Sandeep and Sivadasan (IPEC 2015) asks\nwhether the paw-free completion problem and the paw-free edge deletion problem\nadmit polynomial kernels. We answer both questions affirmatively by presenting,\nrespectively, $O(k)$-vertex and $O(k^4)$-vertex kernels for them. This is part\nof an ongoing program that aims at understanding compressibility of $H$-free\nedge modification problems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 08:40:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Cao", "Yixin", ""], ["Ke", "Yuping", ""], ["Yuan", "Hanchun", ""]]}, {"id": "2003.11313", "submitter": "Martin Milani\\v{c}", "authors": "Nina Chiarelli, Matja\\v{z} Krnc, Martin Milani\\v{c}, Ulrich Pferschy,\n  Nevena Piva\\v{c}, Joachim Schauer", "title": "Fair allocation of indivisible goods under conflict constraints", "comments": "A preliminary version containing some of the results presented here\n  appeared in the proceedings of IWOCA 2020. Version 3 contains an appendix\n  with a remark about biconvex bipartite graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fair allocation of indivisible items to several agents and\nadd a graph theoretical perspective to this classical problem. Thereby we\nintroduce an incompatibility relation between pairs of items described in terms\nof a conflict graph. Every subset of items assigned to one agent has to form an\nindependent set in this graph. Thus, the allocation of items to the agents\ncorresponds to a partial coloring of the conflict graph. Every agent has its\nown profit valuation for every item. Aiming at a fair allocation, our goal is\nthe maximization of the lowest total profit of items allocated to any one of\nthe agents. The resulting optimization problem contains, as special cases, both\n{\\sc Partition} and {\\sc Independent Set}. In our contribution we derive\ncomplexity and algorithmic results depending on the properties of the given\ngraph. We can show that the problem is strongly NP-hard for bipartite graphs\nand their line graphs, and solvable in pseudo-polynomial time for the classes\nof chordal graphs, cocomparability graphs, biconvex bipartite graphs, and\ngraphs of bounded treewidth. Each of the pseudo-polynomial algorithms can also\nbe turned into a fully polynomial approximation scheme (FPTAS).\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:45:21 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 17:39:21 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 16:38:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chiarelli", "Nina", ""], ["Krnc", "Matja\u017e", ""], ["Milani\u010d", "Martin", ""], ["Pferschy", "Ulrich", ""], ["Piva\u010d", "Nevena", ""], ["Schauer", "Joachim", ""]]}, {"id": "2003.11450", "submitter": "Zhijie Zhang", "authors": "Xiaoming Sun, Jialin Zhang, Zhijie Zhang", "title": "A Tight Deterministic Algorithm for the Submodular Multiple Knapsack\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function maximization has been a central topic in the theoretical\ncomputer science community over the last decade. Plenty of well-performing\napproximation algorithms have been designed for the maximization of (monotone\nor non-monotone) submodular functions over a variety of constraints. In this\npaper, we consider the submodular multiple knapsack problem (SMKP), which is\nthe submodular version of the well-studied multiple knapsack problem (MKP).\nRoughly speaking, the problem asks to maximize a monotone submodular function\nover multiple bins (knapsacks). Recently, Fairstein et al. (ESA20) presented a\ntight $(1-1/e-\\epsilon)$-approximation randomized algorithm for SMKP. Their\nalgorithm is based on the continuous greedy technique which inherently involves\nrandomness. However, the deterministic algorithm of this problem has not been\nunderstood very well previously. In this paper, we present a tight\n$(1-1/e-\\epsilon)$ deterministic algorithm for SMKP. Our algorithm is based on\nreducing SMKP to an exponential-size submodular maximizaion problem over a\nspecial partition matroid which enjoys a tight deterministic algorithm. We\ndevelop several techniques to mimic the algorithm, leading to a tight\ndeterministic approximation for SMKP.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 15:42:04 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 14:31:46 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 15:55:07 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 08:38:20 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""], ["Zhang", "Zhijie", ""]]}, {"id": "2003.11468", "submitter": "James Willis", "authors": "James S. Willis, Matthieu Schaller, Pedro Gonnet, John C. Helly", "title": "A Hybrid MPI+Threads Approach to Particle Group Finding Using Union-Find", "comments": "12 pages, 4 figures. Proceedings of the ParCo 2019 conference,\n  Prague, Czech Republic, September 10-13th, 2019", "journal-ref": "Advances in Parallel Computing, Volume 36: Parallel Computing:\n  Technology Trends (2020), Pages: 263 - 274, ISBN: 978-1-64368-070-5", "doi": "10.3233/APC200050", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Friends-of-Friends (FoF) algorithm is a standard technique used in\ncosmological $N$-body simulations to identify structures. Its goal is to find\nclusters of particles (called groups) that are separated by at most a cut-off\nradius. $N$-body simulations typically use most of the memory present on a\nnode, leaving very little free for a FoF algorithm to run on-the-fly. We\npropose a new method that utilises the common Union-Find data structure and a\nhybrid MPI+threads approach. The algorithm can also be expressed elegantly in a\ntask-based formalism if such a framework is used in the rest of the\napplication. We have implemented our algorithm in the open-source cosmological\ncode, SWIFT. Our implementation displays excellent strong- and weak-scaling\nbehaviour on realistic problems and compares favourably (speed-up of 18x) over\nother methods commonly used in the $N$-body community.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 16:00:08 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Willis", "James S.", ""], ["Schaller", "Matthieu", ""], ["Gonnet", "Pedro", ""], ["Helly", "John C.", ""]]}, {"id": "2003.11538", "submitter": "Mourad EL Ouali", "authors": "Moura El Ouali and Volkmar Sauerland", "title": "The Exact Query Complexity of Yes-No Permutation Mastermind", "comments": "12 pages, 2 figures, submitted to GAMES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastermind is famous two-players game. The first player (codemaker) chooses a\nsecret code which the second player (codebreaker) is supposed to crack within a\nminimum number of code guesses (queries). Therefore, codemaker's duty is to\nhelp codebreaker by providing a well-defined error measure between the secret\ncode and the guessed code after each query. We consider a variant, called\nYes-No AB-Mastermind, where both secret code and queries must be\nrepetition-free and the provided information by codemaker only indicates if a\nquery contains any correct position at all. For this Mastermind version with n\npositions and $k\\le n$ colors we prove a lower bound of\n$\\log_2(k+1-n)+\\log_2(k+2-n)+\\dots+\\log_2(k)$ and an upper bound of\n$n\\log_2(n)+k$ on the number of queries necessary to break the secret code. For\nthe important case $k=n$, where both secret code and queries represent\npermutations, our results imply an exact asymptotic complexity of\n$\\Theta(n\\log_2(n))$ queries.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:57:46 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ouali", "Moura El", ""], ["Sauerland", "Volkmar", ""]]}, {"id": "2003.11604", "submitter": "Yakov Nekrich", "authors": "Timothy M. Chan, Qizheng He, Yakov Nekrich", "title": "Further Results on Colored Range Searching", "comments": "full version of a SoCG'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a number of new results about range searching for colored (or\n\"categorical\") data:\n  1. For a set of $n$ colored points in three dimensions, we describe\nrandomized data structures with $O(n\\mathop{\\rm polylog}n)$ space that can\nreport the distinct colors in any query orthogonal range (axis-aligned box) in\n$O(k\\mathop{\\rm polyloglog} n)$ expected time, where $k$ is the number of\ndistinct colors in the range, assuming that coordinates are in\n$\\{1,\\ldots,n\\}$. Previous data structures require $O(\\frac{\\log n}{\\log\\log n}\n+ k)$ query time. Our result also implies improvements in higher constant\ndimensions.\n  2. Our data structures can be adapted to halfspace ranges in three dimensions\n(or circular ranges in two dimensions), achieving $O(k\\log n)$ expected query\ntime. Previous data structures require $O(k\\log^2n)$ query time.\n  3. For a set of $n$ colored points in two dimensions, we describe a data\nstructure with $O(n\\mathop{\\rm polylog}n)$ space that can answer colored\n\"type-2\" range counting queries: report the number of occurrences of every\ndistinct color in a query orthogonal range. The query time is $O(\\frac{\\log\nn}{\\log\\log n} + k\\log\\log n)$, where $k$ is the number of distinct colors in\nthe range. Naively performing $k$ uncolored range counting queries would\nrequire $O(k\\frac{\\log n}{\\log\\log n})$ time.\n  Our data structures are designed using a variety of techniques, including\ncolored variants of randomized incremental construction (which may be of\nindependent interest), colored variants of shallow cuttings, and bit-packing\ntricks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 20:04:20 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chan", "Timothy M.", ""], ["He", "Qizheng", ""], ["Nekrich", "Yakov", ""]]}, {"id": "2003.11775", "submitter": "Yasuaki Kobayashi", "authors": "Yasuaki Kobayashi", "title": "On Structural Parameterizations of Node Kayles", "comments": "A preliminary version was presented at JCDCG^3 2018. Fix some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node Kayles is a well-known two-player impartial game on graphs: Given an\nundirected graph, each player alternately chooses a vertex not adjacent to\npreviously chosen vertices, and a player who cannot choose a new vertex loses\nthe game. The problem of deciding if the first player has a winning strategy in\nthis game is known to be PSPACE-complete. There are a few studies on\nalgorithmic aspects of this problem. In this paper, we consider the problem\nfrom the viewpoint of fixed-parameter tractability. We show that the problem is\nfixed-parameter tractable parameterized by the size of a minimum vertex cover\nor the modular-width of a given graph. Moreover, we give a polynomial\nkernelization with respect to neighborhood diversity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 07:39:59 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 05:46:03 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kobayashi", "Yasuaki", ""]]}, {"id": "2003.11777", "submitter": "Patrick Rebentrost", "authors": "Yihui Quek, Clement Canonne, Patrick Rebentrost", "title": "Robust quantum minimum finding with an application to hypothesis\n  selection", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimum element in a list of length\n$N$ using a noisy comparator. The noise is modelled as follows: given two\nelements to compare, if the values of the elements differ by at least $\\alpha$\nby some metric defined on the elements, then the comparison will be made\ncorrectly; if the values of the elements are closer than $\\alpha$, the outcome\nof the comparison is not subject to any guarantees. We demonstrate a quantum\nalgorithm for noisy quantum minimum-finding that preserves the quadratic\nspeedup of the noiseless case: our algorithm runs in time $\\tilde O(\\sqrt{N\n(1+\\Delta)})$, where $\\Delta$ is an upper-bound on the number of elements\nwithin the interval $\\alpha$, and outputs a good approximation of the true\nminimum with high probability. Our noisy comparator model is motivated by the\nproblem of hypothesis selection, where given a set of $N$ known candidate\nprobability distributions and samples from an unknown target distribution, one\nseeks to output some candidate distribution $O(\\varepsilon)$-close to the\nunknown target. Much work on the classical front has been devoted to speeding\nup the run time of classical hypothesis selection from $O(N^2)$ to $O(N)$, in\npart by using statistical primitives such as the Scheff\\'{e} test. Assuming a\nquantum oracle generalization of the classical data access and applying our\nnoisy quantum minimum-finding algorithm, we take this run time into the\nsublinear regime. The final expected run time is $\\tilde O(\n\\sqrt{N(1+\\Delta)})$, with the same $O(\\log N)$ sample complexity from the\nunknown distribution as the classical algorithm. We expect robust quantum\nminimum-finding to be a useful building block for algorithms in situations\nwhere the comparator (which may be another quantum or classical algorithm) is\nresolution-limited or subject to some uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 07:42:05 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Quek", "Yihui", ""], ["Canonne", "Clement", ""], ["Rebentrost", "Patrick", ""]]}, {"id": "2003.11835", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Rossano Venturini", "title": "Succinct Dynamic Ordered Sets with Random Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation of a dynamic ordered set of $n$ integer keys drawn from a\nuniverse of size $m$ is a fundamental data structuring problem. Many solutions\nto this problem achieve optimal time but take polynomial space, therefore\npreserving time optimality in the \\emph{compressed} space regime is the problem\nwe address in this work. For a polynomial universe $m = n^{\\Theta(1)}$, we give\na solution that takes $\\textsf{EF}(n,m) + o(n)$ bits, where $\\textsf{EF}(n,m)\n\\leq n\\lceil \\log_2(m/n)\\rceil + 2n$ is the cost in bits of the\n\\emph{Elias-Fano} representation of the set, and supports random access to the\n$i$-th smallest element in $O(\\log n/ \\log\\log n)$ time, updates and\npredecessor search in $O(\\log\\log n)$ time. These time bounds are optimal.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 11:09:39 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "2003.11890", "submitter": "Jean Cardinal", "authors": "Boris Aronov and Jean Cardinal", "title": "Geometric Pattern Matching Reduces to k-SUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that some exact geometric pattern matching problems reduce in linear\ntime to $k$-SUM when the pattern has a fixed size $k$. This holds in the real\nRAM model for searching for a similar copy of a set of $k\\geq 3$ points within\na set of $n$ points in the plane, and for searching for an affine image of a\nset of $k\\geq d+2$ points within a set of $n$ points in $d$-space.\n  As corollaries, we obtain improved real RAM algorithms and decision trees for\nthe two problems. In particular, they can be solved by algebraic decision trees\nof near-linear height.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 13:30:58 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Aronov", "Boris", ""], ["Cardinal", "Jean", ""]]}, {"id": "2003.11998", "submitter": "Eric Barszcz", "authors": "Eric Barszcz", "title": "A Blind Permutation Similarity Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a polynomial blind algorithm that determines when two\nsquare matrices, $A$ and $B$, are permutation similar. The shifted and\ntranslated matrices $(A+\\beta I+\\gamma J)$ and $(B+\\beta I+\\gamma J)$ are used\nto color the vertices of two square, edge weighted, rook's graphs. Then the\norbits are found by repeated symbolic squaring of the vertex colored and edge\nweighted adjacency matrices. Multisets of the diagonal symbols from\nnon-permutation similar matrices are distinct within a few iterations,\ntypically four or less.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 16:03:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Barszcz", "Eric", ""]]}, {"id": "2003.12110", "submitter": "Lars Gottesb\\\"uren", "authors": "Lars Gottesb\\\"uren, Michael Hamann, Sebastian Schlag, Dorothea Wagner", "title": "Advanced Flow-Based Multilevel Hypergraph Partitioning", "comments": "To appear at SEA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The balanced hypergraph partitioning problem is to partition a hypergraph\ninto $k$ disjoint blocks of bounded size such that the sum of the number of\nblocks connected by each hyperedge is minimized. We present an improvement to\nthe flow-based refinement framework of KaHyPar-MF, the current state-of-the-art\nmultilevel $k$-way hypergraph partitioning algorithm for high-quality\nsolutions. Our improvement is based on the recently proposed HyperFlowCutter\nalgorithm for computing bipartitions of unweighted hypergraphs by solving a\nsequence of incremental maximum flow problems. Since vertices and hyperedges\nare aggregated during the coarsening phase, refinement algorithms employed in\nthe multilevel setting must be able to handle both weighted hyperedges and\nweighted vertices -- even if the initial input hypergraph is unweighted. We\ntherefore enhance HyperFlowCutter to handle weighted instances and propose a\ntechnique for computing maximum flows directly on weighted hypergraphs.\n  We compare the performance of two configurations of our new algorithm with\nKaHyPar-MF and seven other partitioning algorithms on a comprehensive benchmark\nset with instances from application areas such as VLSI design, scientific\ncomputing, and SAT solving. Our first configuration, KaHyPar-HFC, computes\nslightly better solutions than KaHyPar-MF using significantly less running\ntime. The second configuration, KaHyPar-HFC*, computes solutions of\nsignificantly better quality and is still slightly faster than KaHyPar-MF.\nFurthermore, in terms of solution quality, both configurations also outperform\nall other competing partitioners.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:01:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Hamann", "Michael", ""], ["Schlag", "Sebastian", ""], ["Wagner", "Dorothea", ""]]}, {"id": "2003.12134", "submitter": "Richard La", "authors": "Michael Lin, Richard J. La", "title": "Miniature Robot Path Planning for Bridge Inspection: Min-Max Cycle\n  Cover-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of planning the deployments of a group of mobile robots.\nWhile the problem and formulation can be used for many different problems, here\nwe use a bridge inspection as the motivating application for the purpose of\nexposition. The robots are initially stationed at a set of depots placed\nthroughout the bridge. Each robot is then assigned a set of sites on the bridge\nto inspect and, upon completion, must return to the same depot where it is\nstored.\n  The problem of robot planning is formulated as a rooted min-max cycle cover\nproblem, in which the vertex set consists of the sites to be inspected and\nrobot depots, and the weight of an edge captures either (i) the amount of time\nneeded to travel from one end vertex to the other vertex or (ii) the necessary\nenergy expenditure for the travel. In the first case, the objective function is\nthe total inspection time, whereas in the latter case, it is the maximum energy\nexpenditure among all deployed robots. We propose a novel algorithm with\napproximation ratio of $5 + \\epsilon$, where $0<\\epsilon<1$. In addition, the\ncomputational complexity of the proposed algorithm is shown to be $O\\big(\nn^2+2^{m-1} n \\log(n+k) \\big)$, where $n$ is the number of vertices, and $m$ is\nthe number of depots.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:17:15 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Lin", "Michael", ""], ["La", "Richard J.", ""]]}, {"id": "2003.12293", "submitter": "Raffaele Marino", "authors": "Raffaele Marino, Scott Kirkpatrick", "title": "Large independent sets on random $d$-regular graphs with $d$ small", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a prioritized local algorithm that computes a\nmaximal independent set on a random $d$-regular graph with small and fixed\nconnectivity $d$. Combining different strategies, we compute new lower bounds\non the independence ratio $\\forall d \\in [5,100]$, $d\\in \\mathbb{N}$. All the\nnew bounds improve upon the best previous bounds. Moreover, for independent set\nin random $3$-regular graph, we experimentally extrapolate, by finite-size\nanalysis, the asymptotic value of the independence ratio at\n$\\alpha_{LB}=0.445327$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 09:17:48 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Marino", "Raffaele", ""], ["Kirkpatrick", "Scott", ""]]}, {"id": "2003.12460", "submitter": "Federico Della Croce", "authors": "Federico Della Croce", "title": "An enhanced pinwheel algorithm for the bamboo garden trimming problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bamboo Garden Trimming Problem (BGT), there is a garden populated by n\nbamboos b(1), b(2), ... , b(n)$ with daily growth rates h(1) >= h(2) >= ... >=\nh(n). We assume that the initial heights of bamboos are zero. A gardener is in\ncharge of the bamboos and trims them to height zero according to some schedule.\nThe objective is to design a perpetual schedule of trimming so as to maintain\nthe height of the bamboo garden as low as possible. We consider the so-called\ndiscrete BGT variant, where the gardener is allowed to trim only one bamboo at\nthe end of each day. For discrete BGT, the current state-of-the-art\napproximation algorithm exploits the relationship between BGT and the classical\nPinwheel scheduling problem and provides a solution that guarantees a\n2-approximation ratio. We propose an alternative Pinwheel scheduling algorithm\nwith approximation ratio converging to 12/7 when sum h(j) > > h(1). Also, we\nshow that the approximation ratio of the proposed algorithm never exceeds\n32000/16947 approximately 1.888. This is the first algorithm reaching a ratio\nstrictly inferior to 19/10.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 15:10:17 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 17:57:50 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Della Croce", "Federico", ""]]}, {"id": "2003.12778", "submitter": "Mohammad Reza Zarrabi", "authors": "Mohammad Reza Zarrabi, Nasrollah Moghaddam Charkari", "title": "Single-Point Visibility Constraint Minimum Link Paths in Simple Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the following problem: Given a simple polygon $P$ with $n$\nvertices and two points $s$ and $t$ inside it, find a minimum link path between\nthem such that a given target point $q$ is visible from at least one point on\nthe path. The method is based on partitioning a portion of $P$ into a number of\nfaces of equal link distance from a source point. This partitioning is\nessentially a shortest path map (SPM). In this paper, we present an optimal\nalgorithm with $O(n)$ time bound, which is the same as the time complexity of\nthe standard minimum link paths problem.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 12:18:22 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 11:19:55 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 12:57:14 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 17:07:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zarrabi", "Mohammad Reza", ""], ["Charkari", "Nasrollah Moghaddam", ""]]}, {"id": "2003.12990", "submitter": "Baoxiang Wang", "authors": "Andrej Bogdanov and Baoxiang Wang", "title": "Learning and Testing Variable Partitions", "comments": "Innovations in Theoretical Computer Science (ITCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ $Let $F$ be a multivariate function from a product set $\\Sigma^n$ to an\nAbelian group $G$. A $k$-partition of $F$ with cost $\\delta$ is a partition of\nthe set of variables $\\mathbf{V}$ into $k$ non-empty subsets $(\\mathbf{X}_1,\n\\dots, \\mathbf{X}_k)$ such that $F(\\mathbf{V})$ is $\\delta$-close to\n$F_1(\\mathbf{X}_1)+\\dots+F_k(\\mathbf{X}_k)$ for some $F_1, \\dots, F_k$ with\nrespect to a given error metric. We study algorithms for agnostically learning\n$k$ partitions and testing $k$-partitionability over various groups and error\nmetrics given query access to $F$. In particular we show that\n  $1.$ Given a function that has a $k$-partition of cost $\\delta$, a partition\nof cost $\\mathcal{O}(k n^2)(\\delta + \\epsilon)$ can be learned in time\n$\\tilde{\\mathcal{O}}(n^2 \\mathrm{poly} (1/\\epsilon))$ for any $\\epsilon > 0$.\nIn contrast, for $k = 2$ and $n = 3$ learning a partition of cost $\\delta +\n\\epsilon$ is NP-hard.\n  $2.$ When $F$ is real-valued and the error metric is the 2-norm, a\n2-partition of cost $\\sqrt{\\delta^2 + \\epsilon}$ can be learned in time\n$\\tilde{\\mathcal{O}}(n^5/\\epsilon^2)$.\n  $3.$ When $F$ is $\\mathbb{Z}_q$-valued and the error metric is Hamming\nweight, $k$-partitionability is testable with one-sided error and\n$\\mathcal{O}(kn^3/\\epsilon)$ non-adaptive queries. We also show that even\ntwo-sided testers require $\\Omega(n)$ queries when $k = 2$.\n  This work was motivated by reinforcement learning control tasks in which the\nset of control variables can be partitioned. The partitioning reduces the task\ninto multiple lower-dimensional ones that are relatively easier to learn. Our\nsecond algorithm empirically increases the scores attained over previous\nheuristic partitioning methods applied in this context.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 10:12:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Bogdanov", "Andrej", ""], ["Wang", "Baoxiang", ""]]}, {"id": "2003.13041", "submitter": "Amos Korman", "authors": "Brieuc Guinard and Amos Korman", "title": "Intermittent Inverse-Square L\\'evy Walks are Optimal for Finding Targets\n  of All Sizes", "comments": null, "journal-ref": "Science Advances 2021", "doi": "10.1126/sciadv.abe8211", "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L\\'evy walks are random walk processes whose step-lengths follow a\nlong-tailed power-law distribution. Due to their abundance as movement patterns\nof biological organisms, significant theoretical efforts have been devoted to\nidentifying the foraging circumstances that would make such patterns\nadvantageous. However, despite extensive research, there is currently no\nmathematical proof indicating that L\\'evy walks are, in any manner, preferable\nstrategies in higher dimensions than one. Here we prove that in finite\ntwo-dimensional terrains, the inverse-square L\\'evy walk strategy is extremely\nefficient at finding sparse targets of arbitrary size and shape. Moreover, this\nholds even under the weak model of intermittent detection. Conversely, any\nother intermittent L\\'evy walk fails to efficiently find either large targets\nor small ones. Our results shed new light on the \\emph{L\\'evy foraging\nhypothesis}, and are thus expected to impact future experiments on animals\nperforming L\\'evy walks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:45:02 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 10:14:36 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 22:30:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Guinard", "Brieuc", ""], ["Korman", "Amos", ""]]}, {"id": "2003.13151", "submitter": "Suman Bera", "authors": "Suman K. Bera, C. Seshadhri", "title": "How the Degeneracy Helps for Triangle Counting in Graph Streams", "comments": "Accepted for publication in PODS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the well-studied problem of triangle count estimation in graph\nstreams. Given a graph represented as a stream of $m$ edges, our aim is to\ncompute a $(1\\pm\\varepsilon)$-approximation to the triangle count $T$, using a\nsmall space algorithm. For arbitrary order and a constant number of passes, the\nspace complexity is known to be essentially $\\Theta(\\min(m^{3/2}/T,\nm/\\sqrt{T}))$ (McGregor et al., PODS 2016, Bera et al., STACS 2017).\n  We give a (constant pass, arbitrary order) streaming algorithm that can\ncircumvent this lower bound for \\emph{low degeneracy graphs}. The degeneracy,\n$\\kappa$, is a nuanced measure of density, and the class of constant degeneracy\ngraphs is immensely rich (containing planar graphs, minor-closed families, and\npreferential attachment graphs). We design a streaming algorithm with space\ncomplexity $\\widetilde{O}(m\\kappa/T)$. For constant degeneracy graphs, this\nbound is $\\widetilde{O}(m/T)$, which is significantly smaller than both\n$m^{3/2}/T$ and $m/\\sqrt{T}$. We complement our algorithmic result with a\nnearly matching lower bound of $\\Omega(m\\kappa/T)$.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 22:25:38 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Bera", "Suman K.", ""], ["Seshadhri", "C.", ""]]}, {"id": "2003.13192", "submitter": "Uri Stemmer", "authors": "Haim Kaplan, Micha Sharir, Uri Stemmer", "title": "How to Find a Point in the Convex Hull Privately", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of how to compute a point in the convex hull of an\ninput set $S$ of $n$ points in ${\\mathbb R}^d$ in a differentially private\nmanner. This question, which is trivial non-privately, turns out to be quite\ndeep when imposing differential privacy. In particular, it is known that the\ninput points must reside on a fixed finite subset $G\\subseteq{\\mathbb R}^d$,\nand furthermore, the size of $S$ must grow with the size of $G$. Previous works\nfocused on understanding how $n$ needs to grow with $|G|$, and showed that\n$n=O\\left(d^{2.5}\\cdot8^{\\log^*|G|}\\right)$ suffices (so $n$ does not have to\ngrow significantly with $|G|$). However, the available constructions exhibit\nrunning time at least $|G|^{d^2}$, where typically $|G|=X^d$ for some (large)\ndiscretization parameter $X$, so the running time is in fact $\\Omega(X^{d^3})$.\n  In this paper we give a differentially private algorithm that runs in\n$O(n^d)$ time, assuming that $n=\\Omega(d^4\\log X)$. To get this result we study\nand exploit some structural properties of the Tukey levels (the regions $D_{\\ge\nk}$ consisting of points whose Tukey depth is at least $k$, for $k=0,1,...$).\nIn particular, we derive lower bounds on their volumes for point sets $S$ in\ngeneral position, and develop a rather subtle mechanism for handling point sets\n$S$ in degenerate position (where the deep Tukey regions have zero volume). A\nnaive approach to the construction of the Tukey regions requires $n^{O(d^2)}$\ntime. To reduce the cost to $O(n^d)$, we use an approximation scheme for\nestimating the volumes of the Tukey regions (within their affine spans in case\nof degeneracy), and for sampling a point from such a region, a scheme that is\nbased on the volume estimation framework of Lov\\'asz and Vempala (FOCS 2003)\nand of Cousins and Vempala (STOC 2015). Making this framework differentially\nprivate raises a set of technical challenges that we address.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 02:42:48 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kaplan", "Haim", ""], ["Sharir", "Micha", ""], ["Stemmer", "Uri", ""]]}, {"id": "2003.13420", "submitter": "Qizheng He", "authors": "Timothy M. Chan and Qizheng He", "title": "Faster Approximation Algorithms for Geometric Set Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the running times of $O(1)$-approximation algorithms for the set\ncover problem in geometric settings, specifically, covering points by disks in\nthe plane, or covering points by halfspaces in three dimensions. In the\nunweighted case, Agarwal and Pan [SoCG 2014] gave a randomized $O(n\\log^4\nn)$-time, $O(1)$-approximation algorithm, by using variants of the\nmultiplicative weight update (MWU) method combined with geometric data\nstructures. We simplify the data structure requirement in one of their methods\nand obtain a deterministic $O(n\\log^3 n\\log\\log n)$-time algorithm. With\nfurther new ideas, we obtain a still faster randomized $O(n\\log n(\\log\\log\nn)^{O(1)})$-time algorithm.\n  For the weighted problem, we also give a randomized $O(n\\log^4n\\log\\log\nn)$-time, $O(1)$-approximation algorithm, by simple modifications to the MWU\nmethod and the quasi-uniform sampling technique.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 22:49:17 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chan", "Timothy M.", ""], ["He", "Qizheng", ""]]}, {"id": "2003.13459", "submitter": "Moran Feldman", "authors": "Moran Feldman, Ashkan Norouzi-Fard, Ola Svensson and Rico Zenklusen", "title": "The One-way Communication Complexity of Submodular Maximization with\n  Applications to Streaming and Robustness", "comments": "56 pages, no figures, to appear in STOC 2020 in the form of an\n  extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of maximizing a monotone submodular\nfunction subject to a cardinality constraint, which, due to its numerous\napplications, has recently been studied in various computational models. We\nconsider a clean multi-player model that lies between the offline and streaming\nmodel, and study it under the aspect of one-way communication complexity. Our\nmodel captures the streaming setting (by considering a large number of\nplayers), and, in addition, two player approximation results for it translate\ninto the robust setting. We present tight one-way communication complexity\nresults for our model, which, due to the above-mentioned connections, have\nmultiple implications in the data stream and robust setting.\n  Even for just two players, a prior information-theoretic hardness result\nimplies that no approximation factor above $1/2$ can be achieved in our model,\nif only queries to feasible sets are allowed. We show that the possibility of\nquerying infeasible sets can actually be exploited to beat this bound, by\npresenting a tight $2/3$-approximation taking exponential time, and an\nefficient $0.514$-approximation. To the best of our knowledge, this is the\nfirst example where querying a submodular function on infeasible sets leads to\nprovably better results. Through the above-mentioned link to the robust\nsetting, both of these algorithms improve on the current state-of-the-art for\nrobust submodular maximization, showing that approximation factors beyond $1/2$\nare possible. Moreover, exploiting the link of our model to streaming, we\nsettle the approximability for streaming algorithms by presenting a tight\n$1/2+\\varepsilon$ hardness result, based on the construction of a new family of\ncoverage functions. This improves on a prior $1-1/e+\\varepsilon$ hardness and\nmatches, up to an arbitrarily small margin, the best known approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:17:00 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Feldman", "Moran", ""], ["Norouzi-Fard", "Ashkan", ""], ["Svensson", "Ola", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2003.13585", "submitter": "Quanquan C. Liu", "authors": "Laxman Dhulipala, Quanquan C. Liu, Julian Shun, Shangdi Yu", "title": "Parallel Batch-Dynamic $k$-Clique Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study new batch-dynamic algorithms for the $k$-clique\ncounting problem, which are dynamic algorithms where the updates are batches of\nedge insertions and deletions. We study this problem in the parallel setting,\nwhere the goal is to obtain algorithms with low (polylogarithmic) depth. Our\nfirst result is a new parallel batch-dynamic triangle counting algorithm with\n$O(\\Delta\\sqrt{\\Delta+m})$ amortized work and $O(\\log^* (\\Delta+m))$ depth with\nhigh probability, and $O(\\Delta+m)$ space for a batch of $\\Delta$ edge\ninsertions or deletions. Our second result is an algebraic algorithm based on\nparallel fast matrix multiplication. Assuming that a parallel fast matrix\nmultiplication algorithm exists with parallel matrix multiplication constant\n$\\omega_p$, the same algorithm solves dynamic $k$-clique counting with\n$O\\left(\\min\\left(\\Delta m^{\\frac{(2k - 1)\\omega_p}{3(\\omega_p + 1)}},\n(\\Delta+m)^{\\frac{2(k + 1)\\omega_p}{3(\\omega_p + 1)}}\\right)\\right)$ amortized\nwork and $O(\\log (\\Delta+m))$ depth with high probability, and\n$O\\left((\\Delta+m)^{\\frac{2(k + 1)\\omega_p}{3(\\omega_p + 1)}}\\right)$ space.\nUsing a recently developed parallel $k$-clique counting algorithm, we also\nobtain a simple batch-dynamic algorithm for $k$-clique counting on graphs with\narboricity $\\alpha$ running in $O(\\Delta(m+\\Delta)\\alpha^{k-4})$ expected work\nand $O(\\log^{k-2} n)$ depth with high probability, and $O(m + \\Delta)$ space.\nFinally, we present a multicore CPU implementation of our parallel\nbatch-dynamic triangle counting algorithm. On a 72-core machine with two-way\nhyper-threading, our implementation achieves 36.54--74.73x parallel speedup,\nand in certain cases achieves significant speedups over existing parallel\nalgorithms for the problem, which are not theoretically-efficient.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:02:14 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 08:08:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Liu", "Quanquan C.", ""], ["Shun", "Julian", ""], ["Yu", "Shangdi", ""]]}, {"id": "2003.13589", "submitter": "Pawe{\\l} Gawrychowski", "authors": "Anadi Agrawal and Pawe{\\l} Gawrychowski", "title": "A Faster Subquadratic Algorithm for the Longest Common Increasing\n  Subsequence Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Longest Common Increasing Subsequence (LCIS) is a variant of the\nclassical Longest Common Subsequence (LCS), in which we additionally require\nthe common subsequence to be strictly increasing. While the well-known \"Four\nRussians\" technique can be used to find LCS in subquadratic time, it does not\nseem applicable to LCIS. Recently, Duraj [STACS 2020] used a completely\ndifferent method based on the combinatorial properties of LCIS to design an\n$\\mathcal{O}(n^2(\\log\\log n)^2/\\log^{1/6}n)$ time algorithm. We show that an\napproach based on exploiting tabulation can be used to construct an\nasymptotically faster $\\mathcal{O}(n^2 \\log\\log n/\\sqrt{\\log n})$ time\nalgorithm. As our solution avoids using the specific combinatorial properties\nof LCIS, it can be also adapted for the Longest Common Weakly Increasing\nSubsequence (LCWIS).\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:07:07 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Agrawal", "Anadi", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "2003.13641", "submitter": "Hendrik Molter", "authors": "Roman Haag, Hendrik Molter, Rolf Niedermeier, Malte Renken", "title": "Feedback Edge Sets in Temporal Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-60440-0_16", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical, linear-time solvable Feedback Edge Set problem is concerned\nwith finding a minimum number of edges intersecting all cycles in a (static,\nunweighted) graph. We provide a first study of this problem in the setting of\ntemporal graphs, where edges are present only at certain points in time. We\nfind that there are four natural generalizations of Feedback Edge Set, all of\nwhich turn out to be NP-hard. We also study the tractability of these problems\nwith respect to several parameters (solution size, lifetime, and number of\ngraph vertices, among others) and obtain some parameterized hardness but also\nfixed-parameter tractability results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:17:47 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Haag", "Roman", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""]]}, {"id": "2003.13786", "submitter": "Asish Mukhopadhyay", "authors": "Sudiksha Khanduja, Aayushi Srivastava, Md. Zamilur Rahman and Asish\n  Mukhopadhyay", "title": "Generating Weakly Chordal Graphs from Arbitrary Graphs", "comments": "15 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme for generating a weakly chordal graph from a randomly\ngenerated input graph, G = (V, E). We reduce G to a chordal graph H by adding\nfill-edges, using the minimum vertex degree heuristic. Since H is necessarily a\nweakly chordal graph, we use an algorithm for deleting edges from a weakly\nchordal graph that preserves the weak chordality property of H. The edges that\nare candidates for deletion are the fill-edges that were inserted into G. In\norder to delete a maximal number of fill-edges, we maintain these in a queue. A\nfill-edge is removed from the front of the queue, which we then try to delete\nfrom H. If this violates the weak chordality property of H, we reinsert this\nedge at the back of the queue. This loop continues till no more fill-edges can\nbe removed from H. Operationally, we implement this by defining a deletion\nround as one in which the edge at the back of the queue is at the front.We stop\nwhen the size of the queue does not change over two successive deletion rounds\nand output H.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:45:21 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Khanduja", "Sudiksha", ""], ["Srivastava", "Aayushi", ""], ["Rahman", "Md. Zamilur", ""], ["Mukhopadhyay", "Asish", ""]]}, {"id": "2003.14265", "submitter": "Rajesh Jayaram", "authors": "Omri Ben-Eliezer and Rajesh Jayaram and David P. Woodruff and Eylon\n  Yogev", "title": "A Framework for Adversarially Robust Streaming Algorithms", "comments": "To appear in PODS 2020. Version 2: acknowledged all funding support", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the adversarial robustness of streaming algorithms. In this\ncontext, an algorithm is considered robust if its performance guarantees hold\neven if the stream is chosen adaptively by an adversary that observes the\noutputs of the algorithm along the stream and can react in an online manner.\nWhile deterministic streaming algorithms are inherently robust, many central\nproblems in the streaming literature do not admit sublinear-space deterministic\nalgorithms; on the other hand, classical space-efficient randomized algorithms\nfor these problems are generally not adversarially robust. This raises the\nnatural question of whether there exist efficient adversarially robust\n(randomized) streaming algorithms for these problems.\n  In this work, we show that the answer is positive for various important\nstreaming problems in the insertion-only model, including distinct elements and\nmore generally $F_p$-estimation, $F_p$-heavy hitters, entropy estimation, and\nothers. For all of these problems, we develop adversarially robust\n$(1+\\varepsilon)$-approximation algorithms whose required space matches that of\nthe best known non-robust algorithms up to a $\\text{poly}(\\log n,\n1/\\varepsilon)$ multiplicative factor (and in some cases even up to a constant\nfactor). Towards this end, we develop several generic tools allowing one to\nefficiently transform a non-robust streaming algorithm into a robust one in\nvarious scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:50:27 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 14:23:12 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""], ["Yogev", "Eylon", ""]]}, {"id": "2003.14317", "submitter": "Michael Hamann", "authors": "Lars Gottesb\\\"uren, Michael Hamann, Philipp Schoch, Ben Strasser,\n  Dorothea Wagner and Sven Z\\\"uhlsdorf", "title": "Engineering Exact Quasi-Threshold Editing", "comments": "22 pages, 8 figures, to appear at SEA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-threshold graphs are $\\{C_4, P_4\\}$-free graphs, i.e., they do not\ncontain any cycle or path of four nodes as an induced subgraph. We study the\n$\\{C_4, P_4\\}$-free editing problem, which is the problem of finding a minimum\nnumber of edge insertions or deletions to transform an input graph into a\nquasi-threshold graph. This problem is NP-hard but fixed-parameter tractable\n(FPT) in the number of edits by using a branch-and-bound algorithm and admits a\nsimple integer linear programming formulation (ILP). Both methods are also\napplicable to the general $F$-free editing problem for any finite set of graphs\n$F$. For the FPT algorithm, we introduce a fast heuristic for computing\nhigh-quality lower bounds and an improved branching strategy. For the ILP, we\nengineer several variants of row generation. We evaluate both methods for\nquasi-threshold editing on a large set of protein similarity graphs. For most\ninstances, our optimizations speed up the FPT algorithm by one to three orders\nof magnitude. The running time of the ILP, that we solve using Gurobi, becomes\nonly slightly faster. With all optimizations, the FPT algorithm is slightly\nfaster than the ILP, even when listing all solutions. Additionally, we show\nthat for almost all graphs, solutions of the previously proposed\nquasi-threshold editing heuristic QTM are close to optimal.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:51:32 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Hamann", "Michael", ""], ["Schoch", "Philipp", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""], ["Z\u00fchlsdorf", "Sven", ""]]}]