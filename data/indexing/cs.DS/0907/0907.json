[{"id": "0907.0305", "submitter": "Leah Epstein", "authors": "Leah Epstein, Asaf Levin, Julian Mestre and Danny Segev", "title": "Improved approximation guarantees for weighted matching in the\n  semi-streaming model", "comments": null, "journal-ref": "SIAM J. Discrete Math. 25(3): 1251-1265 (2011)", "doi": "10.1137/100801901", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum weight matching problem in the semi-streaming model, and\nimprove on the currently best one-pass algorithm due to Zelke (Proc. of\nSTACS2008, pages 669-680) by devising a deterministic approach whose\nperformance guarantee is 4.91+epsilon. In addition, we study preemptive online\nalgorithms, a sub-class of one-pass algorithms where we are only allowed to\nmaintain a feasible matching in memory at any point in time. All known results\nprior to Zelke's belong to this sub-class. We provide a lower bound of 4.967 on\nthe competitive ratio of any such deterministic algorithm, and hence show that\nfuture improvements will have to store in memory a set of edges which is not\nnecessarily a feasible matching.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2009 08:11:22 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Epstein", "Leah", ""], ["Levin", "Asaf", ""], ["Mestre", "Julian", ""], ["Segev", "Danny", ""]]}, {"id": "0907.0718", "submitter": "Jonathan Sorenson", "authors": "Jonathan P. Sorenson", "title": "A Randomized Sublinear Time Parallel GCD Algorithm for the EREW PRAM", "comments": null, "journal-ref": "Information Processing Letters 110 (2010) 198-201", "doi": "10.1016/j.ipl.2009.12.008", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized parallel algorithm that computes the greatest common\ndivisor of two integers of n bits in length with probability 1-o(1) that takes\nO(n loglog n / log n) expected time using n^{6+\\epsilon} processors on the EREW\nPRAM parallel model of computation. We believe this to be the first randomized\nsublinear time algorithm on the EREW PRAM for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2009 21:12:38 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2010 21:58:58 GMT"}], "update_date": "2010-01-15", "authors_parsed": [["Sorenson", "Jonathan P.", ""]]}, {"id": "0907.0726", "submitter": "Zoya Svitkina", "authors": "Zachary Friggstad, Mohammad R. Salavatipour, Zoya Svitkina", "title": "Asymmetric Traveling Salesman Path and Directed Latency Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study integrality gaps and approximability of two closely related problems\non directed graphs. Given a set V of n nodes in an underlying asymmetric metric\nand two specified nodes s and t, both problems ask to find an s-t path visiting\nall other nodes. In the asymmetric traveling salesman path problem (ATSPP), the\nobjective is to minimize the total cost of this path. In the directed latency\nproblem, the objective is to minimize the sum of distances on this path from s\nto each node. Both of these problems are NP-hard. The best known approximation\nalgorithms for ATSPP had ratio O(log n) until the very recent result that\nimproves it to O(log n/ log log n). However, only a bound of O(sqrt(n)) for the\nintegrality gap of its linear programming relaxation has been known. For\ndirected latency, the best previously known approximation algorithm has a\nguarantee of O(n^(1/2+eps)), for any constant eps > 0. We present a new\nalgorithm for the ATSPP problem that has an approximation ratio of O(log n),\nbut whose analysis also bounds the integrality gap of the standard LP\nrelaxation of ATSPP by the same factor. This solves an open problem posed by\nChekuri and Pal [2007]. We then pursue a deeper study of this linear program\nand its variations, which leads to an algorithm for the k-person ATSPP (where k\ns-t paths of minimum total length are sought) and an O(log n)-approximation for\nthe directed latency problem.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2009 22:42:23 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2010 23:44:37 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Friggstad", "Zachary", ""], ["Salavatipour", "Mohammad R.", ""], ["Svitkina", "Zoya", ""]]}, {"id": "0907.0741", "submitter": "Travis Gagie", "authors": "Travis Gagie and Yakov Nekrich", "title": "Tight Bounds for Online Stable Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many authors have considered how many ternary comparisons it takes\nto sort a multiset $S$ of size $n$, the best known upper and lower bounds still\ndiffer by a term linear in $n$. In this paper we restrict our attention to\nonline stable sorting and prove upper and lower bounds that are within (o (n))\nnot only of each other but also of the best known upper bound for offline\nsorting. Specifically, we first prove that if the number of distinct elements\n(\\sigma = o (n / \\log n)), then ((H + 1) n + o (n)) comparisons are sufficient,\nwhere $H$ is the entropy of the distribution of the elements in $S$. We then\ngive a simple proof that ((H + 1) n - o (n)) comparisons are necessary in the\nworst case.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 06:18:38 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Gagie", "Travis", ""], ["Nekrich", "Yakov", ""]]}, {"id": "0907.0774", "submitter": "Nitin Saxena", "authors": "G\\'abor Ivanyos, Marek Karpinski, Nitin Saxena", "title": "Deterministic Polynomial Time Algorithms for Matrix Completion Problems", "comments": "14 pages, preliminary", "journal-ref": "LMS J. of Computation and Mathematics 17 (2014)", "doi": "10.1112/S1461157013000296", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new deterministic algorithms for several cases of the maximum rank\nmatrix completion problem (for short matrix completion), i.e. the problem of\nassigning values to the variables in a given symbolic matrix as to maximize the\nresulting matrix rank. Matrix completion belongs to the fundamental problems in\ncomputational complexity with numerous important algorithmic applications,\namong others, in computing dynamic transitive closures or multicast network\ncodings (Harvey et al SODA 2005, Harvey et al SODA 2006).\n  We design efficient deterministic algorithms for common generalizations of\nthe results of Lovasz and Geelen on this problem by allowing linear functions\nin the entries of the input matrix such that the submatrices corresponding to\neach variable have rank one. We present also a deterministic polynomial time\nalgorithm for finding the minimal number of generators of a given module\nstructure given by matrices. We establish further several hardness results\nrelated to matrix algebras and modules. As a result we connect the classical\nproblem of polynomial identity testing with checking surjectivity (or\ninjectivity) between two given modules. One of the elements of our algorithm is\na construction of a greedy algorithm for finding a maximum rank element in the\nmore general setting of the problem. The proof methods used in this paper could\nbe also of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 17:10:33 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2009 01:17:38 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Ivanyos", "G\u00e1bor", ""], ["Karpinski", "Marek", ""], ["Saxena", "Nitin", ""]]}, {"id": "0907.0792", "submitter": "James Raynolds", "authors": "James E. Raynolds and Lenore M. Mullin", "title": "A generalized inner and outer product of arbitrary multi-dimensional\n  arrays using A Mathematics of Arrays (MoA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm has been devised to compute the inner and outer product between\ntwo arbitrary multi-dimensional arrays A and B in a single piece of code. It\nwas derived using A Mathematics of Arrays (MoA) and the $\\psi$-calculus.\nExtensive tests of the new algorithm are presented for running in sequential as\nwell as OpenMP multiple processor modes.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 20:17:28 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Raynolds", "James E.", ""], ["Mullin", "Lenore M.", ""]]}, {"id": "0907.0796", "submitter": "James Raynolds", "authors": "Lenore M. Mullin and James E. Raynolds", "title": "Tensors and n-d Arrays:A Mathematics of Arrays (MoA), psi-Calculus and\n  the Composition of Tensor and Array Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kronecker product is a key algorithm and is ubiquitous across the\nphysical, biological, and computation social sciences. Thus considerations of\noptimal implementation are important. The need to have high performance and\ncomputational reproducibility is paramount. Moreover, due to the need to\ncompose multiple Kronecker products, issues related to data structures, layout\nand indexing algebra require a new look at an old problem. This paper discusses\nthe outer product/tensor product and a special case of the tensor product: the\nKronecker product, along with optimal implementation when composed, and mapped\nto complex processor/memory hierarchies. We discuss how the use of ``A\nMathematics of Arrays\" (MoA), and the psi-Calculus, (a calculus of indexing\nwith shapes), provides optimal, verifiable, reproducible, scalable, and\nportable implementations of both hardware and software.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 20:38:39 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Mullin", "Lenore M.", ""], ["Raynolds", "James E.", ""]]}, {"id": "0907.0884", "submitter": "Wolfgang Mulzer", "authors": "Nir Ailon, Bernard Chazelle, Kenneth L. Clarkson, Ding Liu, Wolfgang\n  Mulzer, C. Seshadhri", "title": "Self-Improving Algorithms", "comments": "26 pages, 8 figures, preliminary versions appeared at SODA 2006 and\n  SoCG 2008. Thorough revision to improve the presentation of the paper", "journal-ref": "SIAM Journal on Computing (SICOMP), 40(2), 2011, pp. 350-375", "doi": "10.1137/090766437", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate ways in which an algorithm can improve its expected\nperformance by fine-tuning itself automatically with respect to an unknown\ninput distribution D. We assume here that D is of product type. More precisely,\nsuppose that we need to process a sequence I_1, I_2, ... of inputs I = (x_1,\nx_2, ..., x_n) of some fixed length n, where each x_i is drawn independently\nfrom some arbitrary, unknown distribution D_i. The goal is to design an\nalgorithm for these inputs so that eventually the expected running time will be\noptimal for the input distribution D = D_1 * D_2 * ... * D_n.\n  We give such self-improving algorithms for two problems: (i) sorting a\nsequence of numbers and (ii) computing the Delaunay triangulation of a planar\npoint set. Both algorithms achieve optimal expected limiting complexity. The\nalgorithms begin with a training phase during which they collect information\nabout the input distribution, followed by a stationary regime in which the\nalgorithms settle to their optimized incarnations.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2009 19:48:43 GMT"}, {"version": "v2", "created": "Mon, 18 Oct 2010 09:43:17 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Ailon", "Nir", ""], ["Chazelle", "Bernard", ""], ["Clarkson", "Kenneth L.", ""], ["Liu", "Ding", ""], ["Mulzer", "Wolfgang", ""], ["Seshadhri", "C.", ""]]}, {"id": "0907.1054", "submitter": "Kaushik Sinha", "authors": "Mikhail Belkin and Kaushik Sinha", "title": "Learning Gaussian Mixtures with Arbitrary Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for learning the parameters of a mixture of\n$k$ identical spherical Gaussians in $n$-dimensional space with an arbitrarily\nsmall separation between the components. Our algorithm is polynomial in all\nparameters other than $k$. The algorithm is based on an appropriate grid search\nover the space of parameters. The theoretical analysis of the algorithm hinges\non a reduction of the problem to 1 dimension and showing that two 1-dimensional\nmixtures whose densities are close in the $L^2$ norm must have similar means\nand mixing coefficients. To produce such a lower bound for the $L^2$ norm in\nterms of the distances between the corresponding means, we analyze the behavior\nof the Fourier transform of a mixture of Gaussians in 1 dimension around the\norigin, which turns out to be closely related to the properties of the\nVandermonde matrix obtained from the component means. Analysis of this matrix\ntogether with basic function approximation results allows us to provide a lower\nbound for the norm of the mixture in the Fourier domain.\n  In recent years much research has been aimed at understanding the\ncomputational aspects of learning parameters of Gaussians mixture distributions\nin high dimension. To the best of our knowledge all existing work on learning\nparameters of Gaussian mixtures assumes minimum separation between components\nof the mixture which is an increasing function of either the dimension of the\nspace $n$ or the number of components $k$. In our paper we prove the first\nresult showing that parameters of a $n$-dimensional Gaussian mixture model with\narbitrarily small component separation can be learned in time polynomial in\n$n$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2009 17:41:57 GMT"}, {"version": "v2", "created": "Thu, 13 May 2010 19:20:36 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Belkin", "Mikhail", ""], ["Sinha", "Kaushik", ""]]}, {"id": "0907.1103", "submitter": "Mihai Patrascu", "authors": "Mihai Patrascu", "title": "A Lower Bound for Succinct Rank Queries", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rank problem in succinct data structures asks to preprocess an array\nA[1..n] of bits into a data structure using as close to n bits as possible, and\nanswer queries of the form rank(k) = Sum_{i=1}^k A[i]. The problem has been\nintensely studied, and features as a subroutine in a majority of succinct data\nstructures.\n  We show that in the cell probe model with w-bit cells, if rank takes t time,\nthe space of the data structure must be at least n + n/w^{O(t)} bits. This\nredundancy/query trade-off is essentially optimal, matching our upper bound\nfrom [FOCS'08].\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2009 20:58:55 GMT"}], "update_date": "2009-07-08", "authors_parsed": [["Patrascu", "Mihai", ""]]}, {"id": "0907.1295", "submitter": "Jonathan Sorenson", "authors": "Ankur Gupta, Anna Kispert, Jonathan P. Sorenson", "title": "Online Sorting via Searching and Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework based on a simple data structure and\nparameterized algorithms for the problems of finding items in an unsorted list\nof linearly ordered items based on their rank (selection) or value (search). As\na side-effect of answering these online selection and search queries, we\nprogressively sort the list. Our algorithms are based on Hoare's Quickselect,\nand are parameterized based on the pivot selection method.\n  For example, if we choose the pivot as the last item in a subinterval, our\nframework yields algorithms that will answer q<=n unique selection and/or\nsearch queries in a total of O(n log q) average time. After q=\\Omega(n) queries\nthe list is sorted. Each repeated selection query takes constant time, and each\nrepeated search query takes O(log n) time. The two query types can be\ninterleaved freely. By plugging different pivot selection methods into our\nframework, these results can, for example, become randomized expected time or\ndeterministic worst-case time. Our methods are easy to implement, and we show\nthey perform well in practice.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2009 20:37:19 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Gupta", "Ankur", ""], ["Kispert", "Anna", ""], ["Sorenson", "Jonathan P.", ""]]}, {"id": "0907.1369", "submitter": "Manjish Pal", "authors": "Manjish Pal", "title": "Towards an $O(\\sqrt[3]{\\log n})$-Approximation Algorithm for {\\sc\n  Balanced Separator}", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\sc $c$-Balanced Separator} problem is a graph-partitioning problem in\nwhich given a graph $G$, one aims to find a cut of minimum size such that both\nthe sides of the cut have at least $cn$ vertices. In this paper, we present new\ndirections of progress in the {\\sc $c$-Balanced Separator} problem. More\nspecifically, we propose a new family of mathematical programs, which depends\nupon a parameter $\\epsilon > 0$, and extend the seminal work of\nArora-Rao-Vazirani ({\\sf ARV}) \\cite{ARV} to show that the polynomial time\nsolvability of the proposed family of programs implies an improvement in the\napproximation factor to $O(\\log^{{1/3} + \\epsilon} n)$ from the best-known\nfactor of $O(\\sqrt{\\log n})$ due to {\\sf ARV}. In fact, for $\\epsilon = 1/3$,\nthe program we get is the SDP proposed by {\\sf ARV}. For $\\epsilon < 1/3$, this\nfamily of programs is not convex but one can transform them into so called\n\\emph{\\textbf{concave programs}} in which one optimizes a concave function over\na convex feasible set. The properties of concave programs allows one to apply\ntechniques due to Hoffman \\cite{H81} or Tuy \\emph{et al} \\cite{TTT85} to solve\nsuch problems with arbitrary accuracy. But the problem of finding of a method\nto solve these programs that converges in polynomial time still remains open.\nOur result, although conditional, introduces a new family of programs which is\nmore powerful than semi-definite programming in the context of approximation\nalgorithms and hence it will of interest to investigate this family both in the\ndirection of designing efficient algorithms and proving hardness results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2009 09:32:02 GMT"}], "update_date": "2009-07-10", "authors_parsed": [["Pal", "Manjish", ""]]}, {"id": "0907.1779", "submitter": "Chien-Chung Huang", "authors": "Chien-Chung Huang", "title": "Classified Stable Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the {\\sc classified stable matching} problem, a problem\nmotivated by academic hiring. Suppose that a number of institutes are hiring\nfaculty members from a pool of applicants. Both institutes and applicants have\npreferences over the other side. An institute classifies the applicants based\non their research areas (or any other criterion), and, for each class, it sets\na lower bound and an upper bound on the number of applicants it would hire in\nthat class. The objective is to find a stable matching from which no group of\nparticipants has reason to deviate. Moreover, the matching should respect the\nupper/lower bounds of the classes.\n  In the first part of the paper, we study classified stable matching problems\nwhose classifications belong to a fixed set of ``order types.'' We show that if\nthe set consists entirely of downward forests, there is a polynomial-time\nalgorithm; otherwise, it is NP-complete to decide the existence of a stable\nmatching.\n  In the second part, we investigate the problem using a polyhedral approach.\nSuppose that all classifications are laminar families and there is no lower\nbound. We propose a set of linear inequalities to describe stable matching\npolytope and prove that it is integral. This integrality allows us to find\nvarious optimal stable matchings using Ellipsoid algorithm. A further\nramification of our result is the description of the stable matching polytope\nfor the many-to-many (unclassified) stable matching problem. This answers an\nopen question posed by Sethuraman, Teo and Qian.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 10:37:22 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2009 07:37:19 GMT"}], "update_date": "2009-10-25", "authors_parsed": [["Huang", "Chien-Chung", ""]]}, {"id": "0907.1840", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Gianluca Della Vedova, Riccardo Dondi", "title": "A PTAS for the Minimum Consensus Clustering Problem with a Fixed Number\n  of Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Consensus Clustering problem has been introduced as an effective way to\nanalyze the results of different microarray experiments. The problem consists\nof looking for a partition that best summarizes a set of input partitions (each\ncorresponding to a different microarray experiment) under a simple and\nintuitive cost function. The problem admits polynomial time algorithms on two\ninput partitions, but is APX-hard on three input partitions. We investigate the\nrestriction of Consensus Clustering when the output partition is required to\ncontain at most k sets, giving a polynomial time approximation scheme (PTAS)\nwhile proving the NP-hardness of this restriction.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 15:16:43 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Dondi", "Riccardo", ""]]}, {"id": "0907.2050", "submitter": "{\\L}ukasz Je\\.z", "authors": "{\\L}ukasz Je\\.z", "title": "Randomised Buffer Management with Bounded Delay against Adaptive\n  Adversary", "comments": "Obsolete; improved upon by arXiv:1102.1273 [cs.DS]. This comment is\n  the only difference from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new analysis of the RMix algorithm by Chin et al. for the Buffer\nManagement with Bounded Delay problem (or online scheduling of unit jobs to\nmaximise weighted throughput). Unlike the original proof of\ne/(e-1)-competitiveness, the new one holds even in adaptive-online adversary\nmodel. In fact, the proof works also for a slightly more general problem\nstudied by Bie{\\'n}kowski et al.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2009 16:11:45 GMT"}, {"version": "v2", "created": "Tue, 8 Feb 2011 09:29:06 GMT"}], "update_date": "2011-02-09", "authors_parsed": [["Je\u017c", "\u0141ukasz", ""]]}, {"id": "0907.2071", "submitter": "John Howat", "authors": "Prosenjit Bose, Karim Dou\\\"ieb, Vida Dujmovi\\'c and John Howat", "title": "Layered Working-Set Trees", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The working-set bound [Sleator and Tarjan, J. ACM, 1985] roughly states that\nsearching for an element is fast if the element was accessed recently. Binary\nsearch trees, such as splay trees, can achieve this property in the amortized\nsense, while data structures that are not binary search trees are known to have\nthis property in the worst case. We close this gap and present a binary search\ntree called a layered working-set tree that guarantees the working-set property\nin the worst case. The unified bound [Badoiu et al., TCS, 2007] roughly states\nthat searching for an element is fast if it is near (in terms of rank distance)\nto a recently accessed element. We show how layered working-set trees can be\nused to achieve the unified bound to within a small additive term in the\namortized sense while maintaining in the worst case an access time that is both\nlogarithmic and within a small multiplicative factor of the working-set bound.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 18:11:05 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Bose", "Prosenjit", ""], ["Dou\u00efeb", "Karim", ""], ["Dujmovi\u0107", "Vida", ""], ["Howat", "John", ""]]}, {"id": "0907.2157", "submitter": "Jakub Radoszewski", "authors": "Maxime Crochemore, Costas Iliopoulos, Marcin Kubica, Jakub\n  Radoszewski, Wojciech Rytter, Tomasz Walen", "title": "On the maximal number of highly periodic runs in a string", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A run is a maximal occurrence of a repetition $v$ with a period $p$ such that\n$2p \\le |v|$. The maximal number of runs in a string of length $n$ was studied\nby several authors and it is known to be between $0.944 n$ and $1.029 n$. We\ninvestigate highly periodic runs, in which the shortest period $p$ satisfies\n$3p \\le |v|$. We show the upper bound $0.5n$ on the maximal number of such runs\nin a string of length $n$ and construct a sequence of words for which we obtain\nthe lower bound $0.406 n$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 13:29:23 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas", ""], ["Kubica", "Marcin", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Walen", "Tomasz", ""]]}, {"id": "0907.2165", "submitter": "Serge Gaspers", "authors": "St\\'ephane Bessy, Fedor V. Fomin, Serge Gaspers, Christophe Paul,\n  Anthony Perez, Saket Saurabh, and St\\'ephan Thomass\\'e", "title": "Kernels for Feedback Arc Set In Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tournament T=(V,A) is a directed graph in which there is exactly one arc\nbetween every pair of distinct vertices. Given a digraph on n vertices and an\ninteger parameter k, the Feedback Arc Set problem asks whether the given\ndigraph has a set of k arcs whose removal results in an acyclic digraph. The\nFeedback Arc Set problem restricted to tournaments is known as the k-Feedback\nArc Set in Tournaments (k-FAST) problem. In this paper we obtain a linear\nvertex kernel for k-FAST. That is, we give a polynomial time algorithm which\ngiven an input instance T to k-FAST obtains an equivalent instance T' on O(k)\nvertices. In fact, given any fixed e>0, the kernelized instance has at most\n(2+e)k vertices. Our result improves the previous known bound of O(k^2) on the\nkernel size for k-FAST. Our kernelization algorithm solves the problem on a\nsubclass of tournaments in polynomial time and uses a known polynomial time\napproximation scheme for k-FAST.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 13:48:22 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2009 20:00:59 GMT"}], "update_date": "2009-10-29", "authors_parsed": [["Bessy", "St\u00e9phane", ""], ["Fomin", "Fedor V.", ""], ["Gaspers", "Serge", ""], ["Paul", "Christophe", ""], ["Perez", "Anthony", ""], ["Saurabh", "Saket", ""], ["Thomass\u00e9", "St\u00e9phan", ""]]}, {"id": "0907.2627", "submitter": "Paul Bonsma", "authors": "Paul Bonsma and Felix Breuer", "title": "Finding Fullerene Patches in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following question, motivated by the enumeration of\nfullerenes. A fullerene patch is a 2-connected plane graph G in which inner\nfaces have length 5 or 6, non-boundary vertices have degree 3, and boundary\nvertices have degree 2 or 3. The degree sequence along the boundary is called\nthe boundary code of G. We show that the question whether a given sequence S is\na boundary code of some fullerene patch can be answered in polynomial time when\nsuch patches have at most five 5-faces. We conjecture that our algorithm gives\nthe correct answer for any number of 5-faces, and sketch how to extend the\nalgorithm to the problem of counting the number of different patches with a\ngiven boundary code.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2009 15:39:00 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Bonsma", "Paul", ""], ["Breuer", "Felix", ""]]}, {"id": "0907.2639", "submitter": "Gabor Pataki", "authors": "Gabor Pataki and Mustafa Tural", "title": "Basis Reduction, and the Complexity of Branch-and-Bound", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical branch-and-bound algorithm for the integer feasibility problem\nhas exponential worst case complexity.\n  We prove that it is surprisingly efficient on reformulated problems, in which\nthe columns of the constraint matrix are short, and near orthogonal, i.e. a\nreduced basis of the generated lattice; when the entries of A (the dense part\nof the constraint matrix) are from {1, ..., M} for a large enough M,\nbranch-and-bound solves almost all reformulated instances at the rootnode. We\nalso prove an upper bound on the width of the reformulations along the last\nunit vector.\n  The analysis builds on the ideas of Furst and Kannan to bound the number of\nintegral matrices for which the shortest vectors of certain lattices are long,\nand also uses a bound on the size of the branch-and-bound tree based on the\nnorms of the Gram-Schmidt vectors of the constraint matrix.\n  We explore practical aspects of these results. First, we compute numerical\nvalues of M which guarantee that 90, and 99 percent of the reformulated\nproblems solve at the root: these turn out to be surprisingly small when the\nproblem size is moderate. Second, we confirm with a computational study that\nrandom integer programs become easier, as the coefficients grow.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2009 16:39:57 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2009 16:11:06 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Pataki", "Gabor", ""], ["Tural", "Mustafa", ""]]}, {"id": "0907.2741", "submitter": "Stanley P. Y. Fung", "authors": "Stanley P. Y. Fung", "title": "Bounded Delay Packet Scheduling in a Bounded Buffer", "comments": "5 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of buffer management in QoS-enabled network switches in\nthe bounded delay model where each packet is associated with a weight and a\ndeadline. We consider the more realistic situation where the network switch has\na finite buffer size. A 9.82-competitive algorithm is known for the case of\nmultiple buffers (Azar and Levy, SWAT'06). Recently, for the case of a single\nbuffer, a 3-competitive deterministic algorithm and a 2.618-competitive\nrandomized algorithm was known (Li, INFOCOM'09). In this paper we give a simple\ndeterministic 2-competitive algorithm for the case of a single buffer.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2009 04:05:05 GMT"}], "update_date": "2009-07-17", "authors_parsed": [["Fung", "Stanley P. Y.", ""]]}, {"id": "0907.2951", "submitter": "Luca Foschini", "authors": "Chiranjeeb Buragohain, Luca Foschini and Subhash Suri", "title": "Untangling the Braid: Finding Outliers in a Set of Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the performance of large shared computing systems such as the\ncloud computing infrastructure raises many challenging algorithmic problems.\nOne common problem is to track users with the largest deviation from the norm\n(outliers), for some measure of performance. Taking a stream-computing\nperspective, we can think of each user's performance profile as a stream of\nnumbers (such as response times), and the aggregate performance profile of the\nshared infrastructure as a \"braid\" of these intermixed streams. The monitoring\nsystem's goal then is to untangle this braid sufficiently to track the top k\noutliers. This paper investigates the space complexity of one-pass algorithms\nfor approximating outliers of this kind, proves lower bounds using multi-party\ncommunication complexity, and proposes small-memory heuristic algorithms. On\none hand, stream outliers are easily tracked for simple measures, such as max\nor min, but our theoretical results rule out even good approximations for most\nof the natural measures such as average, median, or the quantiles. On the other\nhand, we show through simulation that our proposed heuristics perform quite\nwell for a variety of synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2009 22:57:53 GMT"}], "update_date": "2009-07-20", "authors_parsed": [["Buragohain", "Chiranjeeb", ""], ["Foschini", "Luca", ""], ["Suri", "Subhash", ""]]}, {"id": "0907.3016", "submitter": "Arash Rafiey", "authors": "Arash Rafiey and Pavol Hell", "title": "Duality for Min-Max Orderings and Dichotomy for Min Cost Homomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Min-Max orderings correspond to conservative lattice polymorphisms. Digraphs\nwith Min-Max orderings have polynomial time solvable minimum cost homomorphism\nproblems. They can also be viewed as digraph analogues of proper interval\ngraphs and bigraphs.\n  We give a forbidden structure characterization of digraphs with a Min-Max\nordering which implies a polynomial time recognition algorithm. We also\nsimilarly characterize digraphs with an extended Min-Max ordering, and we apply\nthis characterization to prove a conjectured form of dichotomy for minimum cost\nhomomorphism problems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2009 08:14:15 GMT"}], "update_date": "2009-07-20", "authors_parsed": [["Rafiey", "Arash", ""], ["Hell", "Pavol", ""]]}, {"id": "0907.3076", "submitter": "Stephan Kreutzer", "authors": "Stephan Kreutzer and Siamak Tazari", "title": "On Brambles, Grid-Like Minors, and Parameterized Intractability of\n  Monadic Second-Order Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brambles were introduced as the dual notion to treewidth, one of the most\ncentral concepts of the graph minor theory of Robertson and Seymour. Recently,\nGrohe and Marx showed that there are graphs G, in which every bramble of order\nlarger than the square root of the treewidth is of exponential size in |G|. On\nthe positive side, they show the existence of polynomial-sized brambles of the\norder of the square root of the treewidth, up to log factors. We provide the\nfirst polynomial time algorithm to construct a bramble in general graphs and\nachieve this bound, up to log-factors. We use this algorithm to construct\ngrid-like minors, a replacement structure for grid-minors recently introduced\nby Reed and Wood, in polynomial time. Using the grid-like minors, we introduce\nthe notion of a perfect bramble and an algorithm to find one in polynomial\ntime. Perfect brambles are brambles with a particularly simple structure and\nthey also provide us with a subgraph that has bounded degree and still large\ntreewidth; we use them to obtain a meta-theorem on deciding certain\nparameterized subgraph-closed problems on general graphs in time singly\nexponential in the parameter.\n  The second part of our work deals with providing a lower bound to Courcelle's\nfamous theorem, stating that every graph property that can be expressed by a\nsentence in monadic second-order logic (MSO), can be decided by a linear time\nalgorithm on classes of graphs of bounded treewidth. Using our results from the\nfirst part of our work we establish a strong lower bound for tractability of\nMSO on classes of colored graphs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2009 13:45:20 GMT"}], "update_date": "2009-07-20", "authors_parsed": [["Kreutzer", "Stephan", ""], ["Tazari", "Siamak", ""]]}, {"id": "0907.3135", "submitter": "Philip Bille", "authors": "Philip Bille", "title": "Fast Searching in Packed Strings", "comments": "To appear in Journal of Discrete Algorithms. Special Issue on CPM\n  2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given strings $P$ and $Q$ the (exact) string matching problem is to find all\npositions of substrings in $Q$ matching $P$. The classical Knuth-Morris-Pratt\nalgorithm [SIAM J. Comput., 1977] solves the string matching problem in linear\ntime which is optimal if we can only read one character at the time. However,\nmost strings are stored in a computer in a packed representation with several\ncharacters in a single word, giving us the opportunity to read multiple\ncharacters simultaneously. In this paper we study the worst-case complexity of\nstring matching on strings given in packed representation. Let $m \\leq n$ be\nthe lengths $P$ and $Q$, respectively, and let $\\sigma$ denote the size of the\nalphabet. On a standard unit-cost word-RAM with logarithmic word size we\npresent an algorithm using time $$ O\\left(\\frac{n}{\\log_\\sigma n} + m +\n\\occ\\right). $$ Here $\\occ$ is the number of occurrences of $P$ in $Q$. For $m\n= o(n)$ this improves the $O(n)$ bound of the Knuth-Morris-Pratt algorithm.\nFurthermore, if $m = O(n/\\log_\\sigma n)$ our algorithm is optimal since any\nalgorithm must spend at least $\\Omega(\\frac{(n+m)\\log\n  \\sigma}{\\log n} + \\occ) = \\Omega(\\frac{n}{\\log_\\sigma n} + \\occ)$ time to\nread the input and report all occurrences. The result is obtained by a novel\nautomaton construction based on the Knuth-Morris-Pratt algorithm combined with\na new compact representation of subautomata allowing an optimal\ntabulation-based simulation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2009 19:29:13 GMT"}, {"version": "v2", "created": "Tue, 7 Sep 2010 19:43:47 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Bille", "Philip", ""]]}, {"id": "0907.3208", "submitter": "Serge Gaspers", "authors": "Fedor V. Fomin, Serge Gaspers, Saket Saurabh, and St\\'ephan Thomass\\'e", "title": "A Linear Vertex Kernel for Maximum Internal Spanning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial time algorithm that for any graph G and integer k >=\n0, either finds a spanning tree with at least k internal vertices, or outputs a\nnew graph G' on at most 3k vertices and an integer k' such that G has a\nspanning tree with at least k internal vertices if and only if G' has a\nspanning tree with at least k' internal vertices. In other words, we show that\nthe Maximum Internal Spanning Tree problem parameterized by the number of\ninternal vertices k, has a 3k-vertex kernel. Our result is based on an\ninnovative application of a classical min-max result about hypertrees in\nhypergraphs which states that \"a hypergraph H contains a hypertree if and only\nif H is partition connected.\"\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 18:50:55 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2012 18:23:27 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Gaspers", "Serge", ""], ["Saurabh", "Saket", ""], ["Thomass\u00e9", "St\u00e9phan", ""]]}, {"id": "0907.3414", "submitter": "Ashutosh Trivedi", "authors": "Marcin Jurdzi\\'nski and Ashutosh Trivedi", "title": "Reachability-time games on timed automata", "comments": null, "journal-ref": "In ICALP, Volume 4596 of LNCS, pages 838--849, Springer 2007", "doi": "10.1007/978-3-540-73420-8_72", "report-no": null, "categories": "cs.CC cs.DS cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a reachability-time game, players Min and Max choose moves so that the\ntime to reach a final state in a timed automaton is minimised or maximised,\nrespectively. Asarin and Maler showed decidability of reachability-time games\non strongly non-Zeno timed automata using a value iteration algorithm. This\npaper complements their work by providing a strategy improvement algorithm for\nthe problem. It also generalizes their decidability result because the proposed\nstrategy improvement algorithm solves reachability-time games on all timed\nautomata. The exact computational complexity of solving reachability-time games\nis also established: the problem is EXPTIME-complete for timed automata with at\nleast two clocks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 14:57:38 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Jurdzi\u0144ski", "Marcin", ""], ["Trivedi", "Ashutosh", ""]]}, {"id": "0907.3497", "submitter": "Chinh Hoang", "authors": "Daniel Bruce, Chinh T. Hoang, Joe Sawada", "title": "A certifying algorithm for 3-colorability of P5-free graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a certifying algorithm for the problem of deciding whether a P5-\nfree graph is 3-colorable by showing there are exactly six finite graphs that\nare P5-free and not 3-colorable and minimal with respect to this property.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 21:33:25 GMT"}], "update_date": "2009-07-22", "authors_parsed": [["Bruce", "Daniel", ""], ["Hoang", "Chinh T.", ""], ["Sawada", "Joe", ""]]}, {"id": "0907.3583", "submitter": "Yoo Chung", "authors": "Yoo Chung and Dongman Lee", "title": "Web of Lossy Adapters for Interface Interoperability: An Algorithm and\n  NP-completeness of Minimization", "comments": "7 pages", "journal-ref": "Proc. of ICSESS 2010", "doi": "10.1109/ICSESS.2010.5552291", "report-no": null, "categories": "cs.SE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By using different interface adapters for different methods, it is possible\nto construct a maximally covering web of interface adapters which incurs\nminimum loss during interface adaptation. We introduce a polynomial-time\nalgorithm that can achieve this. However, we also show that minimizing the\nnumber of adapters included in a maximally covering web of interface adapters\nis an NP-complete problem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2009 08:58:06 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Chung", "Yoo", ""], ["Lee", "Dongman", ""]]}, {"id": "0907.3631", "submitter": "Uriel Feige", "authors": "Reid Andersen and Uriel Feige", "title": "Interchanging distance and capacity in probabilistic mappings", "comments": "16 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harald Racke [STOC 2008] described a new method to obtain hierarchical\ndecompositions of networks in a way that minimizes the congestion. Racke's\napproach is based on an equivalence that he discovered between minimizing\ncongestion and minimizing stretch (in a certain setting). Here we present\nRacke's equivalence in an abstract setting that is more general than the one\ndescribed in Racke's work, and clarifies the power of Racke's result. In\naddition, we present a related (but different) equivalence that was developed\nby Yuval Emek [ESA 2009] and is only known to apply to planar graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2009 12:33:36 GMT"}], "update_date": "2009-07-22", "authors_parsed": [["Andersen", "Reid", ""], ["Feige", "Uriel", ""]]}, {"id": "0907.3754", "submitter": "Moritz Hardt", "authors": "Moritz Hardt, Kunal Talwar", "title": "On the Geometry of Differential Privacy", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the noise complexity of differentially private mechanisms in the\nsetting where the user asks $d$ linear queries $f\\colon\\Rn\\to\\Re$\nnon-adaptively. Here, the database is represented by a vector in $\\Rn$ and\nproximity between databases is measured in the $\\ell_1$-metric.\n  We show that the noise complexity is determined by two geometric parameters\nassociated with the set of queries.\n  We use this connection to give tight upper and lower bounds on the noise\ncomplexity for any $d \\leq n$. We show that for $d$ random linear queries of\nsensitivity~1, it is necessary and sufficient to add $\\ell_2$-error\n$\\Theta(\\min\\{d\\sqrt{d}/\\epsilon,d\\sqrt{\\log (n/d)}/\\epsilon\\})$ to achieve\n$\\epsilon$-differential privacy. Assuming the truth of a deep conjecture from\nconvex geometry, known as the Hyperplane conjecture, we can extend our results\nto arbitrary linear queries giving nearly matching upper and lower bounds.\n  Our bound translates to error\n$O(\\min\\{d/\\epsilon,\\sqrt{d\\log(n/d)}/\\epsilon\\})$ per answer. The best\nprevious upper bound (Laplacian mechanism) gives a bound of\n$O(\\min\\{d/\\eps,\\sqrt{n}/\\epsilon\\})$ per answer, while the best known lower\nbound was $\\Omega(\\sqrt{d}/\\epsilon)$. In contrast, our lower bound is strong\nenough to separate the concept of differential privacy from the notion of\napproximate differential privacy where an upper bound of $O(\\sqrt{d}/\\epsilon)$\ncan be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2009 22:26:02 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2009 00:40:40 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2009 20:53:59 GMT"}], "update_date": "2009-11-09", "authors_parsed": [["Hardt", "Moritz", ""], ["Talwar", "Kunal", ""]]}, {"id": "0907.3986", "submitter": "Aleksandrs Slivkins", "authors": "Aleksandrs Slivkins", "title": "Contextual Bandits with Similarity Information", "comments": "This is the full version of a conference paper in COLT 2011, to\n  appear in JMLR in 2014. A preliminary version of this manuscript (with all\n  the results) has been posted to arXiv in February 2011. An earlier version on\n  arXiv, which does not include the results in Section 6, dates back to July\n  2009. The present revision addresses various presentation issues pointed out\n  by journal referees", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence\nof choices. In each round it chooses from a time-invariant set of alternatives\nand receives the payoff associated with this alternative. While the case of\nsmall strategy sets is by now well-understood, a lot of recent work has focused\non MAB problems with exponentially or infinitely large strategy sets, where one\nneeds to assume extra structure in order to make the problem tractable. In\nparticular, recent literature considered information on similarity between\narms.\n  We consider similarity information in the setting of \"contextual bandits\", a\nnatural extension of the basic MAB problem where before each round an algorithm\nis given the \"context\" -- a hint about the payoffs in this round. Contextual\nbandits are directly motivated by placing advertisements on webpages, one of\nthe crucial problems in sponsored search. A particularly simple way to\nrepresent similarity information in the contextual bandit setting is via a\n\"similarity distance\" between the context-arm pairs which gives an upper bound\non the difference between the respective expected payoffs.\n  Prior work on contextual bandits with similarity uses \"uniform\" partitions of\nthe similarity space, which is potentially wasteful. We design more efficient\nalgorithms that are based on adaptive partitions adjusted to \"popular\" context\nand \"high-payoff\" arms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2009 06:41:33 GMT"}, {"version": "v2", "created": "Wed, 16 Feb 2011 00:49:41 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2011 17:32:29 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2014 17:30:07 GMT"}, {"version": "v5", "created": "Tue, 20 May 2014 03:52:46 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Slivkins", "Aleksandrs", ""]]}, {"id": "0907.4068", "submitter": "Masud Hasan", "authors": "Syed Ishtiaque Ahmed, Masud Hasan, and Md. Ariful Islam", "title": "Cutting a Convex Polyhedron Out of a Sphere", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a convex polyhedron $P$ of $n$ vertices inside a sphere $Q$, we give an\n$O(n^3)$-time algorithm that cuts $P$ out of $Q$ by using guillotine cuts and\nhas cutting cost $O((\\log n)^2)$ times the optimal.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2009 14:11:59 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2010 17:26:51 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Ahmed", "Syed Ishtiaque", ""], ["Hasan", "Masud", ""], ["Islam", "Md. Ariful", ""]]}, {"id": "0907.4166", "submitter": "Kamesh Munagala", "authors": "Sayan Bhattacharya, Gagan Goel, Sreenivas Gollapudi and Kamesh\n  Munagala", "title": "Budget Constrained Auctions with Heterogeneous Items", "comments": "Final version accepted to STOC '10. Incorporates significant reviewer\n  comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the first approximation algorithms for the problem\nof designing revenue optimal Bayesian incentive compatible auctions when there\nare multiple (heterogeneous) items and when bidders can have arbitrary demand\nand budget constraints. Our mechanisms are surprisingly simple: We show that a\nsequential all-pay mechanism is a 4 approximation to the revenue of the optimal\nex-interim truthful mechanism with discrete correlated type space for each\nbidder. We also show that a sequential posted price mechanism is a O(1)\napproximation to the revenue of the optimal ex-post truthful mechanism when the\ntype space of each bidder is a product distribution that satisfies the standard\nhazard rate condition. We further show a logarithmic approximation when the\nhazard rate condition is removed, and complete the picture by showing that\nachieving a sub-logarithmic approximation, even for regular distributions and\none bidder, requires pricing bundles of items. Our results are based on\nformulating novel LP relaxations for these problems, and developing generic\nrounding schemes from first principles. We believe this approach will be useful\nin other Bayesian mechanism design contexts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2009 12:39:22 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2009 12:21:58 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2009 12:29:45 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2009 19:07:11 GMT"}, {"version": "v5", "created": "Tue, 6 Oct 2009 12:19:00 GMT"}, {"version": "v6", "created": "Mon, 29 Mar 2010 11:45:54 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Goel", "Gagan", ""], ["Gollapudi", "Sreenivas", ""], ["Munagala", "Kamesh", ""]]}, {"id": "0907.4283", "submitter": "Stephan Kreutzer", "authors": "Anuj Dawar and Stephan Kreutzer", "title": "Domination Problems in Nowhere-Dense Classes of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity of generalisations and variations\nof the dominating set problem on classes of graphs that are nowhere dense. In\nparticular, we show that the distance-d dominating-set problem, also known as\nthe (k,d)-centres problem, is fixed-parameter tractable on any class that is\nnowhere dense and closed under induced subgraphs. This generalises known\nresults about the dominating set problem on H-minor free classes, classes with\nlocally excluded minors and classes of graphs of bounded expansion. A key\nfeature of our proof is that it is based simply on the fact that these graph\nclasses are uniformly quasi-wide, and does not rely on a structural\ndecomposition. Our result also establishes that the distance-d dominating-set\nproblem is FPT on classes of bounded expansion, answering a question of Ne{\\v\ns}et{\\v{r}}il and Ossona de Mendez.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2009 13:26:45 GMT"}], "update_date": "2009-07-27", "authors_parsed": [["Dawar", "Anuj", ""], ["Kreutzer", "Stephan", ""]]}, {"id": "0907.4311", "submitter": "Leah Epstein", "authors": "Leah Epstein, Elena Kleiman and Julian Mestre", "title": "Parametric packing of selfish items and the subset sum algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subset sum algorithm is a natural heuristic for the classical Bin Packing\nproblem: In each iteration, the algorithm finds among the unpacked items, a\nmaximum size set of items that fits into a new bin. More than 35 years after\nits first mention in the literature, establishing the worst-case performance of\nthis heuristic remains, surprisingly, an open problem.\n  Due to their simplicity and intuitive appeal, greedy algorithms are the\nheuristics of choice of many practitioners. Therefore, better understanding\nsimple greedy heuristics is, in general, an interesting topic in its own right.\nVery recently, Epstein and Kleiman (Proc. ESA 2008) provided another incentive\nto study the subset sum algorithm by showing that the Strong Price of Anarchy\nof the game theoretic version of the bin-packing problem is precisely the\napproximation ratio of this heuristic.\n  In this paper we establish the exact approximation ratio of the subset sum\nalgorithm, thus settling a long standing open problem. We generalize this\nresult to the parametric variant of the bin packing problem where item sizes\nlie on the interval (0,\\alpha] for some \\alpha \\leq 1, yielding tight bounds\nfor the Strong Price of Anarchy for all \\alpha \\leq 1. Finally, we study the\npure Price of Anarchy of the parametric Bin Packing game for which we show\nnearly tight upper and lower bounds for all \\alpha \\leq 1.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2009 15:06:00 GMT"}], "update_date": "2009-07-27", "authors_parsed": [["Epstein", "Leah", ""], ["Kleiman", "Elena", ""], ["Mestre", "Julian", ""]]}, {"id": "0907.4356", "submitter": "Benjamin Birnbaum", "authors": "Yossi Azar, Benjamin Birnbaum, L. Elisa Celis, Nikhil R. Devanur, and\n  Yuval Peres", "title": "Convergence of Local Dynamics to Balanced Outcomes in Exchange Networks", "comments": "Full version of FOCS 2009 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bargaining games on exchange networks have been studied by both economists\nand sociologists. A Balanced Outcome for such a game is an equilibrium concept\nthat combines notions of stability and fairness. In a recent paper, Kleinberg\nand Tardos introduced balanced outcomes to the computer science community and\nprovided a polynomial-time algorithm to compute the set of such outcomes. Their\nwork left open a pertinent question: are there natural, local dynamics that\nconverge quickly to a balanced outcome? In this paper, we provide a partial\nanswer to this question by showing that simple edge-balancing dynamics converge\nto a balanced outcome whenever one exists.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2009 18:35:20 GMT"}], "update_date": "2009-07-27", "authors_parsed": [["Azar", "Yossi", ""], ["Birnbaum", "Benjamin", ""], ["Celis", "L. Elisa", ""], ["Devanur", "Nikhil R.", ""], ["Peres", "Yuval", ""]]}, {"id": "0907.4488", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Eun Jung Kim, Michael Lampis, and Valia Mitsou", "title": "Vertex Cover Problem Parameterized Above and Below Tight Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the well-known Vertex Cover problem parameterized above and below\ntight bounds. We show that two of the parameterizations (both were suggested by\nMahajan, Raman and Sikdar, J. Computer and System Sciences, 75(2):137--153,\n2009) are fixed-parameter tractable and two other parameterizations are\nW[1]-hard (one of them is, in fact, W[2]-hard).\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2009 15:02:39 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2009 10:25:36 GMT"}], "update_date": "2009-08-28", "authors_parsed": [["Gutin", "Gregory", ""], ["Kim", "Eun Jung", ""], ["Lampis", "Michael", ""], ["Mitsou", "Valia", ""]]}, {"id": "0907.4573", "submitter": "Gregory Gutin", "authors": "Noga Alon, Gregory Gutin, Eun Jung Kim, Stefan Szeider, and Anders Yeo", "title": "Solving MAX-r-SAT Above a Tight Lower Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an exact algorithm that decides, for every fixed $r \\geq 2$ in\ntime $O(m) + 2^{O(k^2)}$ whether a given multiset of $m$ clauses of size $r$\nadmits a truth assignment that satisfies at least $((2^r-1)m+k)/2^r$ clauses.\nThus \\textsc{Max-$r$-Sat} is fixed-parameter tractable when parameterized by\nthe number of satisfied clauses above the tight lower bound $(1-2^{-r})m$. This\nsolves an open problem of Mahajan et al. (J. Comput. System Sci., 75, 2009).\n  Our algorithm is based on a polynomial-time data reduction procedure that\nreduces a problem instance to an equivalent algebraically represented problem\nwith $O(k^2)$ variables. This is done by representing the instance as an\nappropriate polynomial, and by applying a probabilistic argument combined with\nsome simple tools from Harmonic analysis to show that if the polynomial cannot\nbe reduced to one of size $O(k^2)$, then there is a truth assignment satisfying\nthe required number of clauses.\n  We introduce a new notion of bikernelization from a parameterized problem to\nanother one and apply it to prove that the above-mentioned parameterized\n\\textsc{Max-$r$-Sat} admits a polynomial-size kernel.\n  Combining another probabilistic argument with tools from graph matching\ntheory and signed graphs, we show that if an instance of \\textsc{Max-2-Sat}\nwith $m$ clauses has at least $3k$ variables after application of certain\npolynomial time reduction rules to it, then there is a truth assignment that\nsatisfies at least $(3m+k)/4$ clauses.\n  We also outline how the fixed-parameter tractability and polynomial-size\nkernel results on \\textsc{Max-$r$-Sat} can be extended to more general families\nof Boolean Constraint Satisfaction Problems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2009 09:10:25 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2009 07:45:35 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2009 14:48:23 GMT"}, {"version": "v4", "created": "Mon, 22 Aug 2011 13:26:56 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Alon", "Noga", ""], ["Gutin", "Gregory", ""], ["Kim", "Eun Jung", ""], ["Szeider", "Stefan", ""], ["Yeo", "Anders", ""]]}, {"id": "0907.5269", "submitter": "Dennis Luxen", "authors": "Robert Geisberger, Dennis Luxen, Sabine Neubauer, Peter Sanders, Lars\n  Volker", "title": "Fast Detour Computation for Ride Sharing", "comments": "5 pages, 2 figure environment, 4 includegraphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Todays ride sharing services still mimic a better billboard. They list the\noffers and allow to search for the source and target city, sometimes enriched\nwith radial search. So finding a connection between big cities is quite easy.\nThese places are on a list of designated origin and distination points. But\nwhen you want to go from a small town to another small town, even when they are\nnext to a freeway, you run into problems. You can't find offers that would or\ncould pass by the town easily with little or no detour. We solve this\ninteresting problem by presenting a fast algorithm that computes the offers\nwith the smallest detours w.r.t. a request. Our experiments show that the\nproblem is efficiently solvable in times suitable for a web service\nimplementation. For realistic database size we achieve lookup times of about\n5ms and a matching rate of 90% instead of just 70% for the simple matching\nalgorithms used today.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2009 11:31:05 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Geisberger", "Robert", ""], ["Luxen", "Dennis", ""], ["Neubauer", "Sabine", ""], ["Sanders", "Peter", ""], ["Volker", "Lars", ""]]}, {"id": "0907.5372", "submitter": "Barry Wittman", "authors": "Greg N. Frederickson, Barry Wittman", "title": "Speedup in the Traveling Repairman Problem with Unit Time Windows", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the unrooted traveling repairman problem is an undirected metric\ngraph and a subset of nodes, each of which has a time window of unit length.\nGiven that a repairman can start at any location, the goal is to plan a route\nthat visits as many nodes as possible during their respective time windows. A\npolynomial-time bicriteria approximation algorithm is presented for this\nproblem, gaining an increased fraction of repairman visits for increased\nspeedup of repairman motion. For speedup $s$, we find a $6\\gamma/(s +\n1)$-approximation for $s$ in the range $1 \\leq s \\leq 2$ and a\n$4\\gamma/s$-approximation for $s$ in the range $2 \\leq s \\leq 4$, where $\\gamma\n= 1$ on tree-shaped networks and $\\gamma = 2 + \\epsilon$ on general metric\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2009 16:30:15 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Frederickson", "Greg N.", ""], ["Wittman", "Barry", ""]]}, {"id": "0907.5427", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Eun Jung Kim, Matthias Mnich, Anders Yeo", "title": "Betweenness Parameterized Above Tight Lower Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study ordinal embedding relaxations in the realm of parameterized\ncomplexity. We prove the existence of a quadratic kernel for the {\\sc\nBetweenness} problem parameterized above its tight lower bound, which is stated\nas follows. For a set $V$ of variables and set $\\mathcal C$ of constraints\n\"$v_i$ \\mbox{is between} $v_j$ \\mbox{and} $v_k$\", decide whether there is a\nbijection from $V$ to the set $\\{1,\\ldots,|V|\\}$ satisfying at least $|\\mathcal\nC|/3 + \\kappa$ of the constraints in $\\mathcal C$. Our result solves an open\nproblem attributed to Benny Chor in Niedermeier's monograph \"Invitation to\nFixed-Parameter Algorithms.\" The betweenness problem is of interest in\nmolecular biology. An approach developed in this paper can be used to determine\nparameterized complexity of a number of other optimization problems on\npermutations parameterized above or below tight bounds.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2009 21:26:12 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2009 19:28:20 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2013 10:56:57 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Gutin", "Gregory", ""], ["Kim", "Eun Jung", ""], ["Mnich", "Matthias", ""], ["Yeo", "Anders", ""]]}, {"id": "0907.5474", "submitter": "Kevin Wortman", "authors": "David Eppstein and Kevin A. Wortman", "title": "Optimal Angular Resolution for Face-Symmetric Drawings", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G be a graph that may be drawn in the plane in such a way that all\ninternal faces are centrally symmetric convex polygons. We show how to find a\ndrawing of this type that maximizes the angular resolution of the drawing, the\nminimum angle between any two incident edges, in polynomial time, by reducing\nthe problem to one of finding parametric shortest paths in an auxiliary graph.\nThe running time is at most O(t^3), where t is a parameter of the input graph\nthat is at most O(n) but is more typically proportional to n^.5.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2009 06:27:19 GMT"}], "update_date": "2009-08-03", "authors_parsed": [["Eppstein", "David", ""], ["Wortman", "Kevin A.", ""]]}, {"id": "0907.5477", "submitter": "Lee-Ad Gottlieb", "authors": "Lee-Ad Gottlieb, and Robert Krauthgamer", "title": "A Nonlinear Approach to Dimension Reduction", "comments": null, "journal-ref": null, "doi": "10.1007/s00454-015-9707-9", "report-no": null, "categories": "cs.CG cs.DS math.FA math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $l_2$ flattening lemma of Johnson and Lindenstrauss [JL84] is a powerful\ntool for dimension reduction. It has been conjectured that the target dimension\nbounds can be refined and bounded in terms of the intrinsic dimensionality of\nthe data set (for example, the doubling dimension). One such problem was\nproposed by Lang and Plaut [LP01] (see also\n[GKL03,MatousekProblems07,ABN08,CGT10]), and is still open. We prove another\nresult in this line of work:\n  The snowflake metric $d^{1/2}$ of a doubling set $S \\subset l_2$ embeds with\nconstant distortion into $l_2^D$, for dimension $D$ that depends solely on the\ndoubling constant of the metric. In fact, the distortion can be made\narbitrarily close to 1, and the target dimension is polylogarithmic in the\ndoubling constant. Our techniques are robust and extend to the more difficult\nspaces $l_1$ and $l_\\infty$, although the dimension bounds here are\nquantitatively inferior than those for $l_2$.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2009 07:10:04 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2010 01:29:04 GMT"}, {"version": "v3", "created": "Sun, 3 Apr 2011 10:51:24 GMT"}, {"version": "v4", "created": "Thu, 14 May 2015 11:25:45 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Krauthgamer", "Robert", ""]]}]