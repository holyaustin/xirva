[{"id": "1701.00305", "submitter": "Arthur Milchior", "authors": "Arthur Milchior", "title": "(Quasi-)linear time algorithm to compute LexDFS, LexUP and LexDown\n  orderings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the three graph search algorithm LexDFS, LexUP and LexDOWN. We\nshow that LexUP orderings can be computed in linear time by an algorithm\nsimilar to the one which compute LexBFS. Furthermore, LexDOWN orderings and\nLexDFS orderings can be computed in time $\\left(n+m\\log m\\right)$ where $n$ is\nthe number of vertices and $m$ the number of edges.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 02:21:50 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 00:00:18 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Milchior", "Arthur", ""]]}, {"id": "1701.00381", "submitter": "Rui Zhang", "authors": "Feiping Nie, Rui Zhang, and Xuelong Li", "title": "A generalized power iteration method for solving quadratic problem on\n  the Stiefel manifold", "comments": null, "journal-ref": "Sci. China Inf. Sci. (2017) 60: 112101", "doi": "10.1007/s11432-016-9021-9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a novel generalized power iteration method\n(GPI) to solve the quadratic problem on the Stiefel manifold (QPSM) as\nmin_{W^TW=I}Tr(W^TAW-2W^TB) along with the theoretical analysis. Accordingly,\nits special case known as the orthogonal least square regression (OLSR) is\nunder further investigation. Based on the aforementioned studies, we then cast\nmajor focus on solving the unbalanced orthogonal procrustes problem (UOPP). As\na result, not only a general convergent algorithm is derived theoretically but\nthe efficiency of the proposed approach is verified empirically as well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 13:18:05 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Nie", "Feiping", ""], ["Zhang", "Rui", ""], ["Li", "Xuelong", ""]]}, {"id": "1701.00541", "submitter": "Mohammed Dosh", "authors": "Kun He, Mohammed Dosh, Shenghao Zou", "title": "Packing Unequal Circles into a Square Container by Partitioning Narrow\n  Action Spaces and Circle Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the NP-hard problem of finding a non-overlapping dense packing\npattern for n Unequal Circle items in a two-dimensional Square Container\n(PUC-SC) such that the size of the container is minimized. Based on our\nprevious work on an Action Space based Global Optimization (ASGO) that\napproximates each circle item as a square item to efficiently find the large\nunoccupied spaces, we propose an optimization algorithm based on the\nPartitioned Action Space and Partitioned Circle Items (PAS-PCI). The PAS is to\npartition the narrow action space on the long side to find two equal action\nspaces to fully utilize the unoccupied spaces. The PCI is to partition the\ncircle items into four groups based on size for the basin hopping strategy.\nExperiments on two sets of benchmark instances show the effectiveness of the\nproposed method. In comparison with our previous algorithm ASGO on the 68\ntested instances that ASGO published, PAS-PCI not only gains smaller containers\nin 64 instances and matches the other 4 but also runs faster in most instances.\nIn comparison with the best record of the Packomania website on a total of 98\ninstances, PAS-PCI finds smaller containers on 82 and matches the other 16.\nNote that we updated 19 records for (47-48, 51-54, 57, 61-72) that had been\nkept unchanged since 2013.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 13:11:56 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 10:00:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["He", "Kun", ""], ["Dosh", "Mohammed", ""], ["Zou", "Shenghao", ""]]}, {"id": "1701.00620", "submitter": "Assaf Naor", "authors": "Assaf Naor and Robert Young", "title": "Vertical perimeter versus horizontal perimeter", "comments": "suggestions of referees addressed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS math.CA math.CO math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete Heisenberg group $\\mathbb{H}_{\\mathbb{Z}}^{2k+1}$ is the group\ngenerated by $a_1,b_1,\\ldots,a_k,b_k,c$, subject to the relations\n$[a_1,b_1]=\\ldots=[a_k,b_k]=c$ and\n$[a_i,a_j]=[b_i,b_j]=[a_i,b_j]=[a_i,c]=[b_i,c]=1$ for every distinct $i,j\\in\n\\{1,\\ldots,k\\}$. Denote $S=\\{a_1^{\\pm 1},b_1^{\\pm 1},\\ldots,a_k^{\\pm\n1},b_k^{\\pm 1}\\}$. The horizontal boundary of $\\Omega\\subset\n\\mathbb{H}_{\\mathbb{Z}}^{2k+1}$, denoted $\\partial_{h}\\Omega$, is the set of\nall $(x,y)\\in \\Omega\\times (\\mathbb{H}_{\\mathbb{Z}}^{2k+1}\\setminus \\Omega)$\nsuch that $x^{-1}y\\in S$. The horizontal perimeter of $\\Omega$ is\n$|\\partial_{h}\\Omega|$. For $t\\in \\mathbb{N}$, define $\\partial^t_{v} \\Omega$\nto be the set of all $(x,y)\\in \\Omega\\times\n(\\mathbb{H}_{\\mathsf{Z}}^{2k+1}\\setminus \\Omega)$ such that $x^{-1}y\\in\n\\{c^t,c^{-t}\\}$. The vertical perimeter of $\\Omega$ is defined by\n$|\\partial_{v}\\Omega|= \\sqrt{\\sum_{t=1}^\\infty |\\partial^t_{v}\\Omega|^2/t^2}$.\nIt is shown here that if $k\\ge 2$, then $|\\partial_{v}\\Omega|\\lesssim\n\\frac{1}{k} |\\partial_{h}\\Omega|$. The proof of this \"vertical versus\nhorizontal isoperimetric inequality\" uses a new structural result that\ndecomposes sets of finite perimeter in the Heisenberg group into pieces that\nadmit an \"intrinsic corona decomposition.\" This allows one to deduce an\nendpoint $W^{1,1}\\to L_2(L_1)$ boundedness of a certain singular integral\noperator from a corresponding lower-dimensional $W^{1,2}\\to L_2(L_2)$\nboundedness. The above inequality has several applications, including that any\nembedding into $L_1$ of a ball of radius $n$ in the word metric on\n$\\mathbb{H}_{\\mathbb{Z}}^{5}$ incurs bi-Lipschitz distortion that is at least a\nconstant multiple of $\\sqrt{\\log n}$. It follows that the integrality gap of\nthe Goemans--Linial semidefinite program for the Sparsest Cut Problem on inputs\nof size $n$ is at least a constant multiple of $\\sqrt{\\log n}$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:25:43 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 13:51:24 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Naor", "Assaf", ""], ["Young", "Robert", ""]]}, {"id": "1701.00635", "submitter": "EPTCS", "authors": "Lukas Immanuel Schiller", "title": "An Agglomeration Law for Sorting Networks and its Application in\n  Functional Programming", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 165-179", "doi": "10.4204/EPTCS.234.12", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will present a general agglomeration law for sorting\nnetworks. Agglomeration is a common technique when designing parallel\nprogrammes to control the granularity of the computation thereby finding a\nbetter fit between the algorithm and the machine on which the algorithm runs.\nUsually this is done by grouping smaller tasks and computing them en bloc\nwithin one parallel process. In the case of sorting networks this could be done\nby computing bigger parts of the network with one process. The agglomeration\nlaw in this paper pursues a different strategy: The input data is grouped and\nthe algorithm is generalized to work on the agglomerated input while the\noriginal structure of the algorithm remains. This will result in a new access\nopportunity to sorting networks well-suited for efficient parallelization on\nmodern multicore computers, computer networks or GPGPU programming.\nAdditionally this enables us to use sorting networks as (parallel or\ndistributed) merging stages for arbitrary sorting algorithms, thereby creating\nnew hybrid sorting algorithms with ease. The expressiveness of functional\nprogramming languages helps us to apply this law to systematically constructed\nsorting networks, leading to efficient and easily adaptable sorting algorithms.\nAn application example is given, using the Eden programming language to show\nthe effectiveness of the law. The implementation is compared with different\nparallel sorting algorithms by runtime behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:33:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Schiller", "Lukas Immanuel", ""]]}, {"id": "1701.00821", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Partially Recursive Acceptance Rejection", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating random variates from high-dimensional distributions is often done\napproximately using Markov chain Monte Carlo. In certain cases, perfect\nsimulation algorithms exist that allow one to draw exactly from the stationary\ndistribution, but most require $O(n \\ln(n))$ time, where $n$ measures the size\nof the input. In this work a new protocol for creating perfect simulation\nalgorithms that runs in $O(n)$ time for a wider range of parameters on several\nmodels (such as Strauss, Ising, and random cluster) than was known previously.\nThis work represents an extension of the popping algorithms due to Wilson.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 02:19:20 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1701.00893", "submitter": "Jorge Luis Rivero", "authors": "Jorge Luis Rivero P\\'erez, Bernardete Ribeiro, Kadir Hector Ortiz", "title": "A Comparison of Algorithms for Intrusion Detection on Batch and Data\n  Stream Environments", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intruders detection in computer networks has some deficiencies from machine\nlearning approach, given by the nature of the application. The principal\nproblem is the modest display of detection systems based on learning algorithms\nunder the constraints imposed by real environments. This article focuses on the\nmachine learning approach for network intrusion detection in batch and data\nstream environments. First, we propose and describe three variants of KDD99\ndataset preprocessing including attribute selection. Secondly, a thoroughly\nexperimentation is performed from evaluating and comparing representative batch\nlearning algorithms on the variants obtained from KDD99 pre processing.\nFinally, since network traffic is a constant data stream, which can present\nconcept drifting with high rate of false positive, along with the fact that\nthere are not many researches addressing intrusion detection on streaming\nenvironments, lead us to make a comparison of various representative data\nstream classification algorithms. This research allows determining the\nalgorithms that better perform on the proposed variants of KDD99 for both batch\nand data stream environments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 03:55:55 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["P\u00e9rez", "Jorge Luis Rivero", ""], ["Ribeiro", "Bernardete", ""], ["Ortiz", "Kadir Hector", ""]]}, {"id": "1701.01093", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Kobbi Nissim, Hongxia Jin", "title": "Private Incremental Regression", "comments": "To appear in PODS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is continuously generated by modern data sources, and a recent challenge\nin machine learning has been to develop techniques that perform well in an\nincremental (streaming) setting. In this paper, we investigate the problem of\nprivate machine learning, where as common in practice, the data is not given at\nonce, but rather arrives incrementally over time.\n  We introduce the problems of private incremental ERM and private incremental\nregression where the general goal is to always maintain a good empirical risk\nminimizer for the history observed under differential privacy. Our first\ncontribution is a generic transformation of private batch ERM mechanisms into\nprivate incremental ERM mechanisms, based on a simple idea of invoking the\nprivate batch ERM procedure at some regular time intervals. We take this\nconstruction as a baseline for comparison. We then provide two mechanisms for\nthe private incremental regression problem. Our first mechanism is based on\nprivately constructing a noisy incremental gradient function, which is then\nused in a modified projected gradient procedure at every timestep. This\nmechanism has an excess empirical risk of $\\approx\\sqrt{d}$, where $d$ is the\ndimensionality of the data. While from the results of [Bassily et al. 2014]\nthis bound is tight in the worst-case, we show that certain geometric\nproperties of the input and constraint set can be used to derive significantly\nbetter results for certain interesting regression problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:18:07 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Nissim", "Kobbi", ""], ["Jin", "Hongxia", ""]]}, {"id": "1701.01337", "submitter": "Martin R. Schuster", "authors": "Martin R. Schuster, Maciej Liskiewicz", "title": "New Abilities and Limitations of Spectral Graph Bisection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral based heuristics belong to well-known commonly used methods which\ndetermines provably minimal graph bisection or outputs \"fail\" when the\noptimality cannot be certified. In this paper we focus on Boppana's algorithm\nwhich belongs to one of the most prominent methods of this type. It is well\nknown that the algorithm works well in the random \\emph{planted bisection\nmodel} -- the standard class of graphs for analysis minimum bisection and\nrelevant problems. In 2001 Feige and Kilian posed the question if Boppana's\nalgorithm works well in the semirandom model by Blum and Spencer. In our paper\nwe answer this question affirmatively. We show also that the algorithm achieves\nsimilar performance on graph classes which extend the semirandom model.\n  Since the behavior of Boppana's algorithm on the semirandom graphs remained\nunknown, Feige and Kilian proposed a new semidefinite programming (SDP) based\napproach and proved that it works on this model. The relationship between the\nperformance of the SDP based algorithm and Boppana's approach was left as an\nopen problem. In this paper we solve the problem in a complete way by proving\nthat the bisection algorithm of Feige and Kilian provides exactly the same\nresults as Boppana's algorithm. As a consequence we get that Boppana's\nalgorithm achieves the optimal threshold for exact cluster recovery in the\n\\emph{stochastic block model}. On the other hand we prove some limitations of\nBoppana's approach: we show that if the density difference on the parameters of\nthe planted bisection model is too small then the algorithm fails with high\nprobability in the model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 15:00:35 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 08:52:22 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Schuster", "Martin R.", ""], ["Liskiewicz", "Maciej", ""]]}, {"id": "1701.01394", "submitter": "Andrew Knyazev", "authors": "Andrew V. Knyazev", "title": "On spectral partitioning of signed graphs", "comments": "12 pages, 10 figures. Rev 2 to appear in proceedings of the SIAM\n  Workshop on Combinatorial Scientific Computing 2018 (CSC18)", "journal-ref": null, "doi": null, "report-no": "Rev. 1 MERL TR2017-001", "categories": "cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the standard graph Laplacian is preferable for spectral\npartitioning of signed graphs compared to the signed Laplacian. Simple examples\ndemonstrate that partitioning based on signs of components of the leading\neigenvectors of the signed Laplacian may be meaningless, in contrast to\npartitioning based on the Fiedler vector of the standard graph Laplacian for\nsigned graphs. We observe that negative eigenvalues are beneficial for spectral\npartitioning of signed graphs, making the Fiedler vector easier to compute.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 17:31:16 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:51:22 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Knyazev", "Andrew V.", ""]]}, {"id": "1701.01419", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "On Convex Programming Relaxations for the Permanent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several convex programming relaxations have been proposed to\nestimate the permanent of a non-negative matrix, notably in the works of\nGurvits and Samorodnitsky. However, the origins of these relaxations and their\nrelationships to each other have remained somewhat mysterious. We present a\nconceptual framework, implicit in the belief propagation literature, to\nsystematically arrive at these convex programming relaxations for estimating\nthe permanent -- as approximations to an exponential-sized max-entropy convex\nprogram for computing the permanent. Further, using standard convex programming\ntechniques such as duality, we establish equivalence of these aforementioned\nrelaxations to those based on capacity-like quantities studied by Gurvits and\nAnari et al.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 18:43:17 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1701.01539", "submitter": "K. Alex Mills", "authors": "K. Alex Mills, R. Chandrasekaran, Neeraj Mittal", "title": "Algorithms for Optimal Replica Placement Under Correlated Failure in\n  Hierarchical Failure Domains", "comments": "64 pages, 9 figures. Preprint submission to Theoretical Computer\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data centers, data replication is the primary method used to ensure\navailability of customer data. To avoid correlated failure, cloud storage\ninfrastructure providers model hierarchical failure domains using a tree, and\navoid placing a large number of data replicas within the same failure domain\n(i.e. on the same branch of the tree). Typical best practices ensure that\nreplicas are distributed across failure domains, but relatively little is known\nconcerning optimization algorithms for distributing data replicas. Using a\nhierarchical model, we answer how to distribute replicas across failure domains\noptimally. We formulate a novel optimization problem for replica placement in\ndata centers. As part of our problem, we formalize and explain a new criterion\nfor optimizing a replica placement. Our overall goal is to choose placements in\nwhich correlated failures disable as few replicas as possible. We provide two\noptimization algorithms for dependency models represented by trees. We first\npresent an $O(n + \\rho \\log \\rho)$ time dynamic programming algorithm for\nplacing $\\rho$ replicas of a single file on the leaves (representing servers)\nof a tree with $n$ vertices. We next consider the problem of placing replicas\nof $m$ blocks of data, where each block may have different replication factors.\nFor this problem, we give an exact algorithm which runs in polynomial time when\nthe skew, the difference in the number of replicas between the largest and\nsmallest blocks of data, is constant.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 04:05:09 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 18:43:01 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 19:40:37 GMT"}, {"version": "v4", "created": "Thu, 20 Apr 2017 00:30:50 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Mills", "K. Alex", ""], ["Chandrasekaran", "R.", ""], ["Mittal", "Neeraj", ""]]}, {"id": "1701.01540", "submitter": "Takanori Maehara", "authors": "Takanori Maehara, Hirofumi Suzuki, Masakazu Ishihata", "title": "Exact Computation of Influence Spread by Binary Decision Diagrams", "comments": "WWW'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating influence spread in social networks is a fundamental procedure to\nestimate the word-of-mouth effect in viral marketing. There are enormous\nstudies about this topic; however, under the standard stochastic cascade\nmodels, the exact computation of influence spread is known to be #P-hard. Thus,\nthe existing studies have used Monte-Carlo simulation-based approximations to\navoid exact computation.\n  We propose the first algorithm to compute influence spread exactly under the\nindependent cascade model. The algorithm first constructs binary decision\ndiagrams (BDDs) for all possible realizations of influence spread, then\ncomputes influence spread by dynamic programming on the constructed BDDs. To\nconstruct the BDDs efficiently, we designed a new frontier-based search-type\nprocedure. The constructed BDDs can also be used to solve other\ninfluence-spread related problems, such as random sampling without rejection,\nconditional influence spread evaluation, dynamic probability update, and\ngradient computation for probability optimization problems.\n  We conducted computational experiments to evaluate the proposed algorithm.\nThe algorithm successfully computed influence spread on real-world networks\nwith a hundred edges in a reasonable time, which is quite impossible by the\nnaive algorithm. We also conducted an experiment to evaluate the accuracy of\nthe Monte-Carlo simulation-based approximation by comparing exact influence\nspread obtained by the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 04:06:11 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Maehara", "Takanori", ""], ["Suzuki", "Hirofumi", ""], ["Ishihata", "Masakazu", ""]]}, {"id": "1701.01722", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 18:43:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 18:18:21 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 01:04:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1701.01936", "submitter": "Moran Feldman", "authors": "Niv Buchbinder, Moran Feldman, Joseph Naor and Ohad Talmon", "title": "$O(\\mbox{depth})$-Competitive Algorithm for Online Multi-level\n  Aggregation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-level aggregation problem in a weighted rooted tree,\nstudied recently by Bienkowski et al. (2015). In this problem requests arrive\nover time at the nodes of the tree, and each request specifies a deadline. A\nrequest is served by sending it to the root before its deadline at a cost equal\nto the weight of the path from the node in which it resides to the root.\nHowever, requests from different nodes can be aggregated, and served together,\nso as to save on cost. The cost of serving an aggregated set of requests is\nequal to the weight of the subtree spanning the nodes in which the requests\nreside. Thus, the problem is to find a competitive online aggregation algorithm\nthat minimizes the total cost of the aggregated requests. This problem arises\nnaturally in many scenarios, including multicasting, supply-chain management\nand sensor networks. It is also related to the well studied TCP-acknowledgement\nproblem and the online joint replenishment problem.\n  We present an online $O(D)$-competitive algorithm for the problem, where $D$\nis the depth, or number of levels, of the aggregation tree. This result\nimproves upon the $D^2 2^D$-competitive algorithm obtained recently by\nBienkowski et al. (2015).\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 10:00:51 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""], ["Naor", "Joseph", ""], ["Talmon", "Ohad", ""]]}, {"id": "1701.02201", "submitter": "Pavel S. Ruzankin", "authors": "Pavel S. Ruzankin", "title": "A fast algorithm for maximal propensity score matching", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability, 2019", "doi": "10.1007/s11009-019-09718-4", "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm which detects the maximal possible number of\nmatched disjoint pairs satisfying a given caliper when a bipartite matching is\ndone with respect to a scalar index (e.g., propensity score), and constructs a\ncorresponding matching. Variable width calipers are compatible with the\ntechnique, provided that the width of the caliper is a Lipschitz function of\nthe index. If the observations are ordered with respect to the index then the\nmatching needs $O(N)$ operations, where $N$ is the total number of subjects to\nbe matched. The case of 1-to-$n$ matching is also considered.\n  We offer also a new fast algorithm for optimal complete one-to-one matching\non a scalar index when the treatment and control groups are of the same size.\nThis allows us to improve greedy nearest neighbor matching on a scalar index.\n  Keywords: propensity score matching, nearest neighbor matching, matching with\ncaliper, variable width caliper.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 14:55:12 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 06:06:25 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 16:37:51 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 04:33:17 GMT"}, {"version": "v5", "created": "Fri, 23 Nov 2018 13:32:34 GMT"}, {"version": "v6", "created": "Sun, 10 Feb 2019 15:58:43 GMT"}, {"version": "v7", "created": "Tue, 12 Mar 2019 16:30:15 GMT"}, {"version": "v8", "created": "Mon, 29 Apr 2019 12:24:24 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Ruzankin", "Pavel S.", ""]]}, {"id": "1701.02409", "submitter": "Arash Rafiey", "authors": "Tom\\'as Feder, Jeff Kinne, Ashwin Murali, Arash Rafiey", "title": "Dichotomy for Digraph Homomorphism Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a homomorphism from an input digraph $G$\nto a fixed digraph $H$. We show that if $H$ admits a weak-near-unanimity\npolymorphism $\\phi$ then deciding whether $G$ admits a homomorphism to $H$\n(HOM($H$)) is polynomial time solvable? This gives a proof of the dichotomy\nconjecture (now dichotomy theorem) by Feder and Vardi [29]. Our approach is\ncombinatorial, and it is simpler than the two algorithms found by Bulatov [9]\nand Zhuk [46] in 2017. We have implemented our algorithm and show some\nexperimental results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 02:07:33 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 03:09:48 GMT"}, {"version": "v3", "created": "Sat, 1 Jul 2017 13:36:10 GMT"}, {"version": "v4", "created": "Sat, 29 Jul 2017 00:00:44 GMT"}, {"version": "v5", "created": "Sun, 9 Aug 2020 21:37:47 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Feder", "Tom\u00e1s", ""], ["Kinne", "Jeff", ""], ["Murali", "Ashwin", ""], ["Rafiey", "Arash", ""]]}, {"id": "1701.02472", "submitter": "Mikhail Batsyn", "authors": "Ilya Bychkov, Mikhail Batsyn", "title": "An efficient exact model for the cell formation problem with a variable\n  number of production cells", "comments": "12 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cell Formation Problem has been studied as an optimization problem in\nmanufacturing for more than 90 years. It consists of grouping machines and\nparts into manufacturing cells in order to maximize loading of cells and\nminimize movement of parts from one cell to another. Many heuristic algorithms\nhave been proposed which are doing well even for large-sized instances.\nHowever, only a few authors have aimed to develop exact methods and most of\nthese methods have some major restrictions such as a fixed number of production\ncells for example. In this paper we suggest a new mixed-integer linear\nprogramming model for solving the cell formation problem with a variable number\nof manufacturing cells. The popular grouping efficacy measure is used as an\nobjective function. To deal with its fractional nature we apply the Dinkelbach\napproach. Our computational experiments are performed on two testsets: the\nfirst consists of 35 well-known instances from the literature and the second\ncontains 32 instances less popular. We solve these instances using CPLEX\nsoftware. Optimal solutions have been found for 63 of the 67 considered problem\ninstances and several new solutions unknown before have been obtained. The\ncomputational times are greatly decreased comparing to the state-of-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:35:34 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 19:15:18 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bychkov", "Ilya", ""], ["Batsyn", "Mikhail", ""]]}, {"id": "1701.02628", "submitter": "Kamer Kaya Kamer Kaya", "authors": "Mustafa Kemal Ta\\c{s}, Kamer Kaya, Erik Saule", "title": "Greed is Good: Optimistic Algorithms for Bipartite-Graph Partial\n  Coloring on Multicore Architectures", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parallel computing, a valid graph coloring yields a lock-free processing\nof the colored tasks, data points, etc., without expensive synchronization\nmechanisms. However, coloring is not free and the overhead can be significant.\nIn particular, for the bipartite-graph partial coloring (BGPC) and distance-2\ngraph coloring (D2GC) problems, which have various use-cases within the\nscientific computing and numerical optimization domains, the coloring overhead\ncan be in the order of minutes with a single thread for many real-life graphs.\n  In this work, we propose parallel algorithms for bipartite-graph partial\ncoloring on shared-memory architectures. Compared to the existing shared-memory\nBGPC algorithms, the proposed ones employ greedier and more optimistic\ntechniques that yield a better parallel coloring performance. In particular, on\n16 cores, the proposed algorithms perform more than 4x faster than their\ncounterparts in the ColPack library which is, to the best of our knowledge, the\nonly publicly-available coloring library for multicore architectures. In\naddition to BGPC, the proposed techniques are employed to devise parallel\ndistance-2 graph coloring algorithms and similar performance improvements have\nbeen observed. Finally, we propose two costless balancing heuristics for BGPC\nthat can reduce the skewness and imbalance on the cardinality of color sets\n(almost) for free. The heuristics can also be used for the D2GC problem and in\ngeneral, they will probably yield a better color-based parallelization\nperformance especially on many-core architectures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:13:49 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Ta\u015f", "Mustafa Kemal", ""], ["Kaya", "Kamer", ""], ["Saule", "Erik", ""]]}, {"id": "1701.02740", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak, Manuel Gomez-Rodriguez", "title": "Uncovering the Spatiotemporal Patterns of Collective Social Activity", "comments": "To appear at the 2017 SIAM International Conference on Data Mining\n  (SIAM SDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media users and microbloggers post about a wide variety of (off-line)\ncollective social activities as they participate in them, ranging from concerts\nand sporting events to political rallies and civil protests. In this context,\npeople who take part in the same collective social activity often post closely\nrelated content from nearby locations at similar times, resulting in\ndistinctive spatiotemporal patterns. Can we automatically detect these patterns\nand thus provide insights into the associated activities? In this paper, we\npropose a modeling framework for clustering streaming spatiotemporal data, the\nSpatial Dirichlet Hawkes Process (SDHP), which allows us to automatically\nuncover a wide variety of spatiotemporal patterns of collective social activity\nfrom geolocated online traces. Moreover, we develop an efficient, online\ninference algorithm based on Sequential Monte Carlo that scales to millions of\ngeolocated posts. Experiments on synthetic data and real data gathered from\nTwitter show that our framework can recover a wide variety of meaningful social\nactivity patterns in terms of both content and spatiotemporal dynamics, that it\nyields interesting insights about these patterns, and that it can be used to\nestimate the location from where a tweet was posted.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 19:00:02 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Jankowiak", "Martin", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1701.02764", "submitter": "Yaroslav Shitov", "authors": "Yaroslav Shitov", "title": "Column subset selection is NP-complete", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M$ be a real $r\\times c$ matrix and let $k$ be a positive integer. In\nthe column subset selection problem (CSSP), we need to minimize the quantity\n$\\|M-SA\\|$, where $A$ can be an arbitrary $k\\times c$ matrix, and $S$ runs over\nall $r\\times k$ submatrices of $M$. This problem and its applications in\nnumerical linear algebra are being discussed for several decades, but its\nalgorithmic complexity remained an open issue. We show that CSSP is\nNP-complete.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 19:40:09 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Shitov", "Yaroslav", ""]]}, {"id": "1701.02836", "submitter": "Shiho Sugimoto", "authors": "Shiho Sugimoto, Naoki Noda, Shunsuke Inenaga, Hideo Bannai, and\n  Masayuki Takeda", "title": "Computing Abelian regularities on RLE strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two strings x and y are said to be Abelian equivalent if x is a permutation\nof y, or vice versa. If a string z satisfies z = xy with x and y being Abelian\nequivalent, then z is said to be an Abelian square. If a string w can be\nfactorized into a sequence v_1,...,v_s of strings such that v_1 ,..., v_{s-1}\nare all Abelian equivalent and vs is a substring of a permutation of v_1, then\nw is said to have a regular Abelian period (p,t) where p = |v_1| and t = |v_s|.\nIf a substring w_1[i..i+l-1] of a string w_1 and a substring w_2[j..j+l-1] of\nanother string w_2 are Abelian equivalent, then the substrings are said to be a\ncommon Abelian factor of w_1 and w_2 and if the length l is the maximum of such\nthen the substrings are said to be a longest common Abelian factor of w_1 and\nw_2. We propose efficient algorithms which compute these Abelian regularities\nusing the run length encoding (RLE) of strings. For a given string w of length\nn whose RLE is of size m, we propose algorithms which compute all Abelian\nsquares occurring in w in O(mn) time, and all regular Abelian periods of w in\nO(mn) time. For two given strings w_1 and w_2 of total length n and of total\nRLE size m, we propose an algorithm which computes all longest common Abelian\nfactors in O(m^2n) time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 03:17:23 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 06:59:31 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Sugimoto", "Shiho", ""], ["Noda", "Naoki", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1701.02844", "submitter": "Prabhav Kalaghatgi", "authors": "Prabhav Kalaghatgi and Thomas Lengauer", "title": "Selecting optimal minimum spanning trees that share a topological\n  correspondence with phylogenetic trees", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method\ncalled CLGrouping, for constructing tree-structured probabilistic graphical\nmodels, a statistical framework that is commonly used for inferring\nphylogenetic trees. While CLGrouping works correctly if there is a unique MST,\nwe observe an indeterminacy in the method in the case that there are multiple\nMSTs. In this work we remove this indeterminacy by introducing so-called\nvertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely\nrelated to the number of leaves in the MST. This motivates the problem of\nfinding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We\nprovide a polynomial time algorithm for the MLVRMST problem, and prove its\ncorrectness for graphs whose edges are weighted with tree-additive distances.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 04:17:20 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Kalaghatgi", "Prabhav", ""], ["Lengauer", "Thomas", ""]]}, {"id": "1701.02853", "submitter": "Ramanujan M. S.", "authors": "Manu Basavaraju, Pranabendu Misra, M. S. Ramanujan, Saket Saurabh", "title": "On finding highly connected spanning subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Survivable Network Design Problem (SNDP), the input is an\nedge-weighted (di)graph $G$ and an integer $r_{uv}$ for every pair of vertices\n$u,v\\in V(G)$. The objective is to construct a subgraph $H$ of minimum weight\nwhich contains $r_{uv}$ edge-disjoint (or node-disjoint) $u$-$v$ paths. This is\na fundamental problem in combinatorial optimization that captures numerous\nwell-studied problems in graph theory and graph algorithms. In this paper, we\nconsider the version of the problem where we are given a $\\lambda$-edge\nconnected (di)graph $G$ with a non-negative weight function $w$ on the edges\nand an integer $k$, and the objective is to find a minimum weight spanning\nsubgraph $H$ that is also $\\lambda$-edge connected, and has at least $k$ fewer\nedges than $G$. In other words, we are asked to compute a maximum weight subset\nof edges, of cardinality up to $k$, which may be safely deleted from $G$.\nMotivated by this question, we investigate the connectivity properties of\n$\\lambda$-edge connected (di)graphs and obtain algorithmically significant\nstructural results. We demonstrate the importance of our structural results by\npresenting an algorithm running in time $2^{O(k \\log k)} |V(G)|^{O(1)}$ for\n$\\lambda$-ECS, thus proving its fixed-parameter tractability. We follow up on\nthis result and obtain the {\\em first polynomial compression} for $\\lambda$-ECS\non unweighted graphs. As a consequence, we also obtain the first fixed\nparameter tractable algorithm, and a polynomial kernel for a parameterized\nversion of the classic Mininum Equivalent Graph problem. We believe that our\nstructural results are of independent interest and will play a crucial role in\nthe design of algorithms for connectivity-constrained problems in general and\nthe SNDP problem in particular.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 05:56:39 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Basavaraju", "Manu", ""], ["Misra", "Pranabendu", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1701.02989", "submitter": "Clemens Thielen", "authors": "Pascal Halffmann, Stefan Ruzika, Clemens Thielen and David Willems", "title": "A General Approximation Method for Bicriteria Minimization Problems", "comments": null, "journal-ref": "Theoretical Computer Science 695, pp. 1-15 (2017)", "doi": "10.1016/j.tcs.2017.07.003", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general technique for approximating bicriteria minimization\nproblems with positive-valued, polynomially computable objective functions.\nGiven $0<\\epsilon\\leq1$ and a polynomial-time $\\alpha$-approximation algorithm\nfor the corresponding weighted sum problem, we show how to obtain a bicriteria\n$(\\alpha\\cdot(1+2\\epsilon),\\alpha\\cdot(1+\\frac{2}{\\epsilon}))$-approximation\nalgorithm for the budget-constrained problem whose running time is polynomial\nin the encoding length of the input and linear in $\\frac{1}{\\epsilon}$.\n  Moreover, we show that our method can be extended to compute an\n$(\\alpha\\cdot(1+2\\epsilon),\\alpha\\cdot(1+\\frac{2}{\\epsilon}))$-approximate\nPareto curve under the same assumptions. Our technique applies to many\nminimization problems to which most previous algorithms for computing\napproximate Pareto curves cannot be applied because the corresponding gap\nproblem is $\\textsf{NP}$-hard to solve. For maximization problems, however, we\nshow that approximation results similar to the ones presented here for\nminimization problems are impossible to obtain in polynomial time unless\n$\\textsf{P}=\\textsf{NP}$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 14:36:54 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Halffmann", "Pascal", ""], ["Ruzika", "Stefan", ""], ["Thielen", "Clemens", ""], ["Willems", "David", ""]]}, {"id": "1701.03004", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Marco Pulimeno and Italo Epicoco", "title": "Parallel mining of time-faded heavy hitters", "comments": "arXiv admin note: text overlap with arXiv:1601.03892", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PFDCMSS, a novel message-passing based parallel algorithm for\nmining time-faded heavy hitters. The algorithm is a parallel version of the\nrecently published FDCMSS sequential algorithm. We formally prove its\ncorrectness by showing that the underlying data structure, a sketch augmented\nwith a Space Saving stream summary holding exactly two counters, is mergeable.\nWhilst mergeability of traditional sketches derives immediately from theory, we\nshow that merging our augmented sketch is non trivial. Nonetheless, the\nresulting parallel algorithm is fast and simple to implement. To the best of\nour knowledge, PFDCMSS is the first parallel algorithm solving the problem of\nmining time-faded heavy hitters on message-passing parallel architectures.\nExtensive experimental results confirm that PFDCMSS retains the extreme\naccuracy and error bound provided by FDCMSS whilst providing excellent parallel\nscalability.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 15:07:38 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""]]}, {"id": "1701.03047", "submitter": "Jaroslav Opatrny", "authors": "Jurek Czyzowicz, Evangelos Kranakis, Danny Krizanc, Lata Narayanan,\n  Jaroslav Opatrny, Sunil Shende", "title": "Linear Search with Terrain-Dependent Speeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the linear search problem where a robot, initially placed at the\norigin on an infinite line, tries to locate a stationary target placed at an\nunknown position on the line. Unlike previous studies, in which the robot\ntravels along the line at a constant speed, we consider settings where the\nrobot's speed can depend on the direction of travel along the line, or on the\nprofile of the terrain, e.g. when the line is inclined, and the robot can\naccelerate. Our objective is to design search algorithms that achieve good\ncompetitive ratios for the time spent by the robot to complete its search\nversus the time spent by an omniscient robot that knows the location of the\ntarget. We consider several new robot mobility models in which the speed of the\nrobot depends on the terrain.These include 1) different constant speeds for\ndifferent directions, 2) speed with constant acceleration and/or variability\ndepending on whether a certain segment has already been searched, 3) speed\ndependent on the incline of the terrain. We provide both upper and lower bounds\non the competitive ratios of search algorithms for these models, and in many\ncases, we derive optimal algorithms for the search time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:31:49 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Shende", "Sunil", ""]]}, {"id": "1701.03136", "submitter": "Timothy Williams", "authors": "Eric Lewin Altschuler and Timothy J. Williams", "title": "A practical efficient and effective method for the Hamiltonian cycle\n  problem that runs on a standard computer", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $N$ cities and $R < N^2 - N$ directed (unidirectional/one way) roads\ndoes there exist a tour of all $N$ cities stopping at each city exactly once\nusing the given roads (a Hamiltonian cycle)? This Hamiltonian cycle problem\n(HCP) is an NP-complete problem, for which there is no known polynomial time\nsolution algorithm. The HCP has important practical applications, for example,\nto logistical problems. It was claimed that an adiabatic quantum computer could\nsolve an NP-complete problem faster than classical algorithms, but claim\nappears to have been debunked. Here we demonstrate an algorithm which runs on a\nstandard computer that efficiently and effectively solves the HCP for at least\nup to 500 cities: We first optimized a simulated annealing based algorithm used\nfor smaller sized HCP problems. Then we found that when a tour was deliberately\ninserted in a list of otherwise randomly chosen roads, crucially, if \"extra\"\nrandom roads are added to bring the total number of roads up to $0.58 N log_e\nN$ or more there is a 100% chance our algorithm will find a HC, but conversely\nwhen a list of roads does not include a pre-inserted tour random roads have to\nbe added until there are $0.9 N log_e N$ roads to have a chance of finding a\nHC. We found similarly for a set of roads non-randomly chosen. Thus, the\npresence of a HC in a set of roads induces \"connectivity\" throughout the roads\nand a HC can be found with an insertion of a modest number of extra roads. Our\nalgorithm also shows that only weakly non-local information is needed to find\nan HCP that is a global state.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:38:20 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 21:11:21 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Altschuler", "Eric Lewin", ""], ["Williams", "Timothy J.", ""]]}, {"id": "1701.03263", "submitter": "Marten Maack", "authors": "Klaus Jansen and Marten Maack", "title": "An EPTAS for Scheduling on Unrelated Machines of Few Different Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical problem of scheduling on unrelated parallel machines, a set\nof jobs has to be assigned to a set of machines. The jobs have a processing\ntime depending on the machine and the goal is to minimize the makespan, that is\nthe maximum machine load. It is well known that this problem is NP-hard and\ndoes not allow polynomial time approximation algorithms with approximation\nguarantees smaller than $1.5$ unless P$=$NP. We consider the case that there\nare only a constant number $K$ of machine types. Two machines have the same\ntype if all jobs have the same processing time for them. This variant of the\nproblem is strongly NP-hard already for $K=1$. We present an efficient\npolynomial time approximation scheme (EPTAS) for the problem, that is, for any\n$\\varepsilon > 0$ an assignment with makespan of length at most\n$(1+\\varepsilon)$ times the optimum can be found in polynomial time in the\ninput length and the exponent is independent of $1/\\varepsilon$. In particular\nwe achieve a running time of $2^{\\mathcal{O}(K\\log(K)\n\\frac{1}{\\varepsilon}\\log^4 \\frac{1}{\\varepsilon})}+\\mathrm{poly}(|I|)$, where\n$|I|$ denotes the input length. Furthermore, we study three other problem\nvariants and present an EPTAS for each of them: The Santa Claus problem, where\nthe minimum machine load has to be maximized; the case of scheduling on\nunrelated parallel machines with a constant number of uniform types, where\nmachines of the same type behave like uniformly related machines; and the\nmultidimensional vector scheduling variant of the problem where both the\ndimension and the number of machine types are constant. For the Santa Claus\nproblem we achieve the same running time. The results are achieved, using mixed\ninteger linear programming and rounding techniques.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 08:12:36 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 16:22:53 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Jansen", "Klaus", ""], ["Maack", "Marten", ""]]}, {"id": "1701.03308", "submitter": "Neha Sengupta", "authors": "Neha Sengupta, Amitabha Bagchi, Srikanta Bedathur and Maya Ramanath", "title": "Sampling and Reconstruction Using Bloom Filters", "comments": null, "journal-ref": "IEEE T. Knowl. Data En. 30(7):1324-1337, July 2018", "doi": "10.1109/TKDE.2017.2785803", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of sampling from a set and\nreconstructing a set stored as a Bloom filter. To the best of our knowledge our\nwork is the first to address this question. We introduce a novel hierarchical\ndata structure called BloomSampleTree that helps us design efficient algorithms\nto extract an almost uniform sample from the set stored in a Bloom filter and\nalso allows us to reconstruct the set efficiently. In the case where the hash\nfunctions used in the Bloom filter implementation are partially invertible, in\nthe sense that it is easy to calculate the set of elements that map to a\nparticular hash value, we propose a second, more space-efficient method called\nHashInvert for the reconstruction. We study the properties of these two methods\nboth analytically as well as experimentally. We provide bounds on run times for\nboth methods and sample quality for the BloomSampleTree based algorithm, and\nshow through an extensive experimental evaluation that our methods are\nefficient and effective.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 11:17:57 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 09:47:15 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Sengupta", "Neha", ""], ["Bagchi", "Amitabha", ""], ["Bedathur", "Srikanta", ""], ["Ramanath", "Maya", ""]]}, {"id": "1701.03318", "submitter": "EPTCS", "authors": "Edelmira Pasarella (Universitat Politecnica de Catalunya),\n  Maria-Esther Vidal (Fraunhofer IAIS), Cristina Zoltan (Universitat\n  Politecnica de Catalunya)", "title": "Comparing MapReduce and Pipeline Implementations for Counting Triangles", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069", "journal-ref": "EPTCS 237, 2017, pp. 20-33", "doi": "10.4204/EPTCS.237.2", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common method to define a parallel solution for a computational problem\nconsists in finding a way to use the Divide and Conquer paradigm in order to\nhave processors acting on its own data and scheduled in a parallel fashion.\nMapReduce is a programming model that follows this paradigm, and allows for the\ndefinition of efficient solutions by both decomposing a problem into steps on\nsubsets of the input data and combining the results of each step to produce\nfinal results. Albeit used for the implementation of a wide variety of\ncomputational problems, MapReduce performance can be negatively affected\nwhenever the replication factor grows or the size of the input is larger than\nthe resources available at each processor. In this paper we show an alternative\napproach to implement the Divide and Conquer paradigm, named dynamic pipeline.\nThe main features of dynamic pipelines are illustrated on a parallel\nimplementation of the well-known problem of counting triangles in a graph. This\nproblem is especially interesting either when the input graph does not fit in\nmemory or is dynamically generated. To evaluate the properties of pipeline, a\ndynamic pipeline of processes and an ad-hoc version of MapReduce are\nimplemented in the language Go, exploiting its ability to deal with channels\nand spawned processes. An empirical evaluation is conducted on graphs of\ndifferent topologies, sizes, and densities. Observed results suggest that\ndynamic pipelines allows for an efficient implementation of the problem of\ncounting triangles in a graph, particularly, in dense and large graphs,\ndrastically reducing the execution time with respect to the MapReduce\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 12:04:15 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Pasarella", "Edelmira", "", "Universitat Politecnica de Catalunya"], ["Vidal", "Maria-Esther", "", "Fraunhofer IAIS"], ["Zoltan", "Cristina", "", "Universitat\n  Politecnica de Catalunya"]]}, {"id": "1701.03493", "submitter": "Thomas Steinke", "authors": "Thomas Steinke and Jonathan Ullman", "title": "Subgaussian Tail Bounds via Stability Arguments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sums of independent, bounded random variables concentrate around their\nexpectation approximately as well a Gaussian of the same variance. Well known\nresults of this form include the Bernstein, Hoeffding, and Chernoff\ninequalities and many others. We present an alternative proof of these tail\nbounds based on what we call a stability argument, which avoids bounding the\nmoment generating function or higher-order moments of the distribution. Our\nstability argument is inspired by recent work on the generalization properties\nof differential privacy and their connection to adaptive data analysis (Bassily\net al., STOC 2016).\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 20:20:39 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 18:35:30 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1701.03693", "submitter": "Javiel Rojas-Ledesma", "authors": "J\\'er\\'emy Barbay and Javiel Rojas", "title": "Multivariate Analysis for Computing Maxima in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing the \\textsc{Maxima} of a set of $n$\n$d$-dimensional points. For dimensions 2 and 3, there are algorithms to solve\nthe problem with order-oblivious instance-optimal running time. However, in\nhigher dimensions there is still room for improvements. We present an algorithm\nsensitive to the structural entropy of the input set, which improves the\nrunning time, for large classes of instances, on the best solution for\n\\textsc{Maxima} to date for $d \\ge 4$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 15:08:46 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["Rojas", "Javiel", ""]]}, {"id": "1701.03730", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari and David Wajc", "title": "Simplified and Space-Optimal Semi-Streaming for\n  $(2+\\epsilon)$-Approximate Matching", "comments": "Appears at the Symposium on Simplicity in Algorithms (SOSA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent breakthrough, Paz and Schwartzman (SODA'17) presented a\nsingle-pass ($2+\\epsilon$)-approximation algorithm for the maximum weight\nmatching problem in the semi-streaming model. Their algorithm uses $O(n\\log^2\nn)$ bits of space, for any constant $\\epsilon>0$.\n  We present two simplified and more intuitive analyses, for essentially the\nsame algorithm, which also improve the space complexity to the optimal bound of\n$O(n\\log n)$ bits --- this is optimal as the output matching requires\n$\\Omega(n\\log n)$ bits. Our analyses rely on a simple use of the primal-dual\nmethod and a simple accounting method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:02:05 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 09:36:22 GMT"}, {"version": "v3", "created": "Sun, 30 Dec 2018 16:49:10 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Wajc", "David", ""]]}, {"id": "1701.03826", "submitter": "Yu Zhang", "authors": "Yu Zhang, Kanat Tangwongsan and Srikanta Tirthapura", "title": "Streaming k-Means Clustering with Fast Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for k-means clustering on a stream with a focus on\nproviding fast responses to clustering queries. Compared to the current\nstate-of-the-art, our methods provide substantial improvement in the query time\nfor cluster centers while retaining the desirable properties of provably small\napproximation error and low space usage. Our algorithms rely on a novel idea of\n\"coreset caching\" that systematically reuses coresets (summaries of data)\ncomputed for recent queries in answering the current clustering query. We\npresent both theoretical analysis and detailed experiments demonstrating their\ncorrectness and efficiency\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 20:21:08 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 20:28:18 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zhang", "Yu", ""], ["Tangwongsan", "Kanat", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1701.03856", "submitter": "Catherine Greenhill", "authors": "Colin Cooper, Martin Dyer, Catherine Greenhill, Andrew Handley", "title": "The flip Markov chain for connected regular graphs", "comments": "40 pages, addresses referee comments. An earlier version of this\n  paper appeared as an extended abstract in PODC 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mahlmann and Schindelhauer (2005) defined a Markov chain which they called\n$k$-Flipper, and showed that it is irreducible on the set of all connected\nregular graphs of a given degree (at least 3). We study the 1-Flipper chain,\nwhich we call the flip chain, and prove that the flip chain converges rapidly\nto the uniform distribution over connected $2r$-regular graphs with $n$\nvertices, where $n\\geq 8$ and $r = r(n)\\geq 2$. Formally, we prove that the\ndistribution of the flip chain will be within $\\varepsilon$ of uniform in total\nvariation distance after $\\text{poly}(n,r,\\log(\\varepsilon^{-1}))$ steps. This\npolynomial upper bound on the mixing time is given explicitly, and improves\nmarkedly on a previous bound given by Feder et al.(2006). We achieve this\nimprovement by using a direct two-stage canonical path construction, which we\ndefine in a general setting.\n  This work has applications to decentralised networks based on random regular\nconnected graphs of even degree, as a self-stabilising protocol in which nodes\nspontaneously perform random flips in order to repair the network.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 00:54:22 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 03:57:11 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Cooper", "Colin", ""], ["Dyer", "Martin", ""], ["Greenhill", "Catherine", ""], ["Handley", "Andrew", ""]]}, {"id": "1701.03990", "submitter": "Shih-Han Hung", "authors": "Jianxin Chen, Andrew M. Childs, Shih-Han Hung", "title": "Quantum algorithm for multivariate polynomial interpolation", "comments": "15 pages, 0 figures. Comments are welcome", "journal-ref": "Proceedings of the Royal Society A 474: 20170480 (2017)", "doi": "10.1098/rspa.2017.0480", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many quantum queries are required to determine the coefficients of a\ndegree-$d$ polynomial in $n$ variables? We present and analyze quantum\nalgorithms for this multivariate polynomial interpolation problem over the\nfields $\\mathbb{F}_q$, $\\mathbb{R}$, and $\\mathbb{C}$. We show that\n$k_{\\mathbb{C}}$ and $2k_{\\mathbb{C}}$ queries suffice to achieve probability\n$1$ for $\\mathbb{C}$ and $\\mathbb{R}$, respectively, where\n$k_{\\mathbb{C}}=\\smash{\\lceil\\frac{1}{n+1}{n+d\\choose d}\\rceil}$ except for\n$d=2$ and four other special cases. For $\\mathbb{F}_q$, we show that\n$\\smash{\\lceil\\frac{d}{n+d}{n+d\\choose d}\\rceil}$ queries suffice to achieve\nprobability approaching $1$ for large field order $q$. The classical query\ncomplexity of this problem is $\\smash{n+d\\choose d}$, so our result provides a\nspeedup by a factor of $n+1$, $\\frac{n+1}{2}$, and $\\frac{n+d}{d}$ for\n$\\mathbb{C}$, $\\mathbb{R}$, and $\\mathbb{F}_q$, respectively. Thus we find a\nmuch larger gap between classical and quantum algorithms than the univariate\ncase, where the speedup is by a factor of $2$. For the case of $\\mathbb{F}_q$,\nwe conjecture that $2k_{\\mathbb{C}}$ queries also suffice to achieve\nprobability approaching $1$ for large field order $q$, although we leave this\nas an open problem.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 04:07:40 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 17:57:43 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Chen", "Jianxin", ""], ["Childs", "Andrew M.", ""], ["Hung", "Shih-Han", ""]]}, {"id": "1701.04021", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Gil Einziger, Roy Friedman, Yaron Kassner", "title": "Optimal Elephant Flow Detection", "comments": "Accepted to IEEE INFOCOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the traffic volumes of elephant flows, including the total byte\ncount per flow, is a fundamental capability for online network measurements. We\npresent an asymptotically optimal algorithm for solving this problem in terms\nof both space and time complexity. This improves on previous approaches, which\ncan only count the number of packets in constant time. We evaluate our work on\nreal packet traces, demonstrating an up to X2.5 speedup compared to the best\nalternative.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 10:30:38 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Friedman", "Roy", ""], ["Kassner", "Yaron", ""]]}, {"id": "1701.04148", "submitter": "Dongsheng Yang", "authors": "Tong Yang, Lingtong Liu, Yibo Yan, Muhammad Shahzad, Yulong Shen,\n  Xiaoming Li, Bin Cui, Gaogang Xie", "title": "SF-sketch: A Two-stage Sketch for Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sketch is a probabilistic data structure used to record frequencies of\nitems in a multi-set. Sketches are widely used in various fields, especially\nthose that involve processing and storing data streams. In streaming\napplications with high data rates, a sketch \"fills up\" very quickly. Thus, its\ncontents are periodically transferred to the remote collector, which is\nresponsible for answering queries. In this paper, we propose a new sketch,\ncalled Slim-Fat (SF) sketch, which has a significantly higher accuracy compared\nto prior art, a much smaller memory footprint, and at the same time achieves\nthe same speed as the best prior sketch. The key idea behind our proposed\nSF-sketch is to maintain two separate sketches: a small sketch called\nSlim-subsketch and a large sketch called Fat-subsketch. The Slim-subsketch is\nperiodically transferred to the remote collector for answering queries quickly\nand accurately. The Fat-subsketch, however, is not transferred to the remote\ncollector because it is used only to assist the Slim-subsketch during the\ninsertions and deletions and is not used to answer queries. We implemented and\nextensively evaluated SF-sketch along with several prior sketches and compared\nthem side by side. Our experimental results show that SF-sketch outperforms the\nmost widely used CM-sketch by up to 33.1 times in terms of accuracy. We have\nreleased the source codes of our proposed sketch as well as existing sketches\nat Github. The short version of this paper will appear in ICDE 2017.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 02:51:22 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 14:52:10 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 14:42:46 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Yang", "Tong", ""], ["Liu", "Lingtong", ""], ["Yan", "Yibo", ""], ["Shahzad", "Muhammad", ""], ["Shen", "Yulong", ""], ["Li", "Xiaoming", ""], ["Cui", "Bin", ""], ["Xie", "Gaogang", ""]]}, {"id": "1701.04364", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna and Yang Li", "title": "On Estimating Maximum Matching Size in Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the maximum matching size in graphs whose\nedges are revealed in a streaming manner. We consider both insertion-only\nstreams and dynamic streams and present new upper and lower bound results for\nboth models.\n  On the upper bound front, we show that an $\\alpha$-approximate estimate of\nthe matching size can be computed in dynamic streams using\n$\\widetilde{O}({n^2/\\alpha^4})$ space, and in insertion-only streams using\n$\\widetilde{O}(n/\\alpha^2)$-space. On the lower bound front, we prove that any\n$\\alpha$-approximation algorithm for estimating matching size in dynamic graph\nstreams requires $\\Omega(\\sqrt{n}/\\alpha^{2.5})$ bits of space, even if the\nunderlying graph is both sparse and has arboricity bounded by $O(\\alpha)$. We\nfurther improve our lower bound to $\\Omega(n/\\alpha^2)$ in the case of dense\ngraphs.\n  Furthermore, we prove that a $(1+\\epsilon)$-approximation to matching size in\ninsertion-only streams requires RS$(n) \\cdot n^{1-O(\\epsilon)}$ space; here,\nRS${n}$ denotes the maximum number of edge-disjoint induced matchings of size\n$\\Theta(n)$ in an $n$-vertex graph. It is a major open problem to determine the\nvalue of RS$(n)$, and current results leave open the possibility that RS$(n)$\nmay be as large as $n/\\log n$. We also show how to avoid the dependency on the\nparameter RS$(n)$ in proving lower bound for dynamic streams and present a\nnear-optimal lower bound of $n^{2-O(\\epsilon)}$ for\n$(1+\\epsilon)$-approximation in this model.\n  Using a well-known connection between matching size and matrix rank, all our\nlower bounds also hold for the problem of estimating matrix rank. In particular\nour results imply a near-optimal $n^{2-O(\\epsilon)}$ bit lower bound for\n$(1+\\epsilon)$-approximation of matrix ranks for dense matrices in dynamic\nstreams, answering an open question of Li and Woodruff (STOC 2016).\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 17:38:18 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""]]}, {"id": "1701.04399", "submitter": "Andreas Alpers", "authors": "Andreas Alpers and Peter Gritzmann", "title": "On double-resolution imaging and discrete tomography", "comments": "26 pages, to appear in SIAM Journal on Discrete Mathematics", "journal-ref": "SIAM Journal on Discrete Mathematics, 32 (2), pp. 1369-1399, 2018", "doi": "10.1137/17M1115629", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution imaging aims at improving the resolution of an image by\nenhancing it with other images or data that might have been acquired using\ndifferent imaging techniques or modalities. In this paper we consider the task\nof doubling, in each dimension, the resolution of grayscale images of binary\nobjects by fusion with double-resolution tomographic data that have been\nacquired from two viewing angles. We show that this task is polynomial-time\nsolvable if the gray levels have been reliably determined. The problem becomes\n$\\mathbb{N}\\mathbb{P}$-hard if the gray levels of some pixels come with an\nerror of $\\pm1$ or larger. The $\\mathbb{N}\\mathbb{P}$-hardness persists for any\nlarger resolution enhancement factor. This means that noise does not only\naffect the quality of a reconstructed image but, less expectedly, also the\nalgorithmic tractability of the inverse problem itself.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:23:01 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 16:40:18 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 16:14:17 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Alpers", "Andreas", ""], ["Gritzmann", "Peter", ""]]}, {"id": "1701.04634", "submitter": "Charis Papadopoulos", "authors": "Charis Papadopoulos and Spyridon Tzimas", "title": "Polynomial-time Algorithms for the Subset Feedback Vertex Set Problem on\n  Interval Graphs and Permutation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex-weighted graph $G=(V,E)$ and a set $S \\subseteq V$, a subset\nfeedback vertex set $X$ is a set of the vertices of $G$ such that the graph\ninduced by $V \\setminus X$ has no cycle containing a vertex of $S$. The\n\\textsc{Subset Feedback Vertex Set} problem takes as input $G$ and $S$ and asks\nfor the subset feedback vertex set of minimum total weight. In contrast to the\nclassical \\textsc{Feedback Vertex Set} problem which is obtained from the\n\\textsc{Subset Feedback Vertex Set} problem for $S=V$, restricted to graph\nclasses the \\textsc{Subset Feedback Vertex Set} problem is known to be\nNP-complete on split graphs and, consequently, on chordal graphs. However as\n\\textsc{Feedback Vertex Set} is polynomially solvable for AT-free graphs, no\nsuch result is known for the \\textsc{Subset Feedback Vertex Set} problem on any\nsubclass of AT-free graphs. Here we give the first polynomial-time algorithms\nfor the problem on two unrelated subclasses of AT-free graphs: interval graphs\nand permutation graphs. As a byproduct we show that there exists a\npolynomial-time algorithm for circular-arc graphs by suitably applying our\nalgorithm for interval graphs. Moreover towards the unknown complexity of the\nproblem for AT-free graphs, we give a polynomial-time algorithm for\nco-bipartite graphs. Thus we contribute to the first positive results of the\n\\textsc{Subset Feedback Vertex Set} problem when restricted to graph classes\nfor which \\textsc{Feedback Vertex Set} is solved in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 11:57:33 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 17:16:35 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Papadopoulos", "Charis", ""], ["Tzimas", "Spyridon", ""]]}, {"id": "1701.04777", "submitter": "Carlos Barron-Romero Prof.", "authors": "Carlos Barr\\'on-Romero", "title": "The fast parallel algorithm for CNF SAT without algebra", "comments": "Published in COMTEL 2017\n  (http://www.comtel.pe/memoriacomtel/COMTEL2017.pdf). See in\n  http://academicos.azc.uam.mx/cbr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel parallel algorithm for solving the classical Decision Boolean\nSatisfiability problem with clauses in conjunctive normal form is depicted. My\napproach for solving SAT is without using algebra or other computational search\nstrategies such as branch and bound, back-forward, tree representation, etc.\nThe method is based on the special class of SAT problems, Simple SAT (SSAT).\nThe algorithm's design includes parallel execution, object oriented, and short\ntermination as my previous versions but it keep track of the tested\nunsatisfactory binary values to improve the efficiency and to favor short\ntermination. The resulting algorithm is linear with respect to the number of\nclauses plus a process data on the partial solutions of the subproblems SSAT of\nan arbitrary SAT and it is bounded by $2^{n}$ iterations where $n$ is the\nnumber of logical variables. The novelty for the solution of arbitrary SAT\nproblems is a linear algorithm, such its complexity is less or equal than the\nalgorithms of the state of the art for solving SAT.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:26:00 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 13:17:38 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 23:14:24 GMT"}, {"version": "v4", "created": "Sun, 26 Nov 2017 21:33:24 GMT"}, {"version": "v5", "created": "Sat, 14 Apr 2018 09:21:39 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Barr\u00f3n-Romero", "Carlos", ""]]}, {"id": "1701.05141", "submitter": "Wouter van Toll", "authors": "Wouter van Toll, Atlas F. Cook IV, Marc J. van Kreveld, Roland\n  Geraerts", "title": "The Medial Axis of a Multi-Layered Environment and its Application as a\n  Navigation Mesh", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning for walking characters in complicated virtual environments is a\nfundamental task in simulations and games. A navigation mesh is a data\nstructure that allows efficient path planning. The Explicit Corridor Map (ECM)\nis a navigation mesh based on the medial axis. It enables path planning for\ndisk-shaped characters of any radius.\n  In this paper, we formally extend the medial axis (and therefore the ECM) to\n3D environments in which characters are constrained to walkable surfaces.\nTypical examples of such environments are multi-storey buildings, train\nstations, and sports stadiums. We give improved definitions of a walkable\nenvironment (WE: a description of walkable surfaces in 3D) and a multi-layered\nenvironment (MLE: a subdivision of a WE into connected layers). We define the\nmedial axis of such environments based on projected distances on the ground\nplane. For an MLE with $n$ boundary vertices and $k$ connections, we show that\nthe medial axis has size $O(n)$, and we present an improved algorithm that\nconstructs the medial axis in $O(n \\log n \\log k)$ time.\n  The medial axis can be annotated with nearest-obstacle information to obtain\nthe ECM navigation mesh. Our implementations show that the ECM can be computed\nefficiently for large 2D and multi-layered environments, and that it can be\nused to compute paths within milliseconds. This enables simulations of large\nvirtual crowds of heterogeneous characters in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:46:08 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 08:01:38 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["van Toll", "Wouter", ""], ["Cook", "Atlas F.", "IV"], ["van Kreveld", "Marc J.", ""], ["Geraerts", "Roland", ""]]}, {"id": "1701.05243", "submitter": "Ugo Vaccaro", "authors": "Ferdinando Cicalese, Luisa Gargano, Ugo Vaccaro", "title": "How to Find a Joint Probability Distribution of Minimum Entropy (almost)\n  given the Marginals", "comments": "This new version extend the results of the previous version from the\n  case of two random variables to the general case of $k\\geq 2$ random\n  variables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two discrete random variables $X$ and $Y$, with probability\ndistributions ${\\bf p} =(p_1, \\ldots , p_n)$ and ${\\bf q}=(q_1, \\ldots , q_m)$,\nrespectively, denote by ${\\cal C}({\\bf p}, {\\bf q})$ the set of all couplings\nof ${\\bf p}$ and ${\\bf q}$, that is, the set of all bivariate probability\ndistributions that have ${\\bf p}$ and ${\\bf q}$ as marginals. In this paper, we\nstudy the problem of finding the joint probability distribution in ${\\cal\nC}({\\bf p}, {\\bf q})$ of minimum entropy (equivalently, the joint probability\ndistribution that maximizes the mutual information between $X$ and $Y$), and we\ndiscuss several situations where the need for this kind of optimization\nnaturally arises. Since the optimization problem is known to be NP-hard, we\ngive an efficient algorithm to find a joint probability distribution in ${\\cal\nC}({\\bf p}, {\\bf q})$ with entropy exceeding the minimum possible by at most 1,\nthus providing an approximation algorithm with additive approximation factor of\n1. Leveraging on this algorithm, we extend our result to the problem of finding\na minimum--entropy joint distribution of arbitrary $k\\geq 2$ discrete random\nvariables $X_1, \\ldots , X_k$, consistent with the known $k$ marginal\ndistributions of $X_1, \\ldots , X_k$. In this case, our approximation algorithm\nhas an additive approximation factor of $\\log k$. We also discuss some related\napplications of our findings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 21:55:45 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 18:00:17 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 11:14:19 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Gargano", "Luisa", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1701.05339", "submitter": "Zhao Zhang", "authors": "Yishuo Shi, Zhao Zhang, Ding-Zhu Du", "title": "Randomized Bicriteria Approximation Algorithm for Minimum Submodular\n  Cost Partial Multi-Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies randomized approximation algorithm for a variant of the\nset cover problem called minimum submodular cost partial multi-cover (SCPMC),\nin which each element $e$ has a covering requirement $r_e$ and a profit $p_e$,\nand the cost function on sub-collection of sets is submodular, the goal is to\nfind a minimum cost sub-collection of sets which fully covers at least\n$q$-percentage of total profit, where an element $e$ is fully covered by\nsub-collection $S'$ if and only if it belongs to at least $r_e$ sets of\n$\\mathcal S'$. Previous work shows that such a combination enormously increases\nthe difficulty of studies, even when the cost function is linear.\n  In this paper, assuming that the maximum covering requirement\n$r_{\\max}=\\max_e r_e$ is a constant and the cost function is nonnegative,\nmonotone nondecreasing, and submodular, we give the first randomized bicriteria\nalgorithm for SCPMC the output of which fully covers at least\n$(q-\\varepsilon)$-percentage of all elements and the performance ratio is\n$O(b/\\varepsilon)$ with a high probability, where $b=\\max_e\\binom{f}{r_{e}}$\nand $f$ is the maximum number of sets containing a common element. The\nalgorithm is based on a novel non-linear program. Furthermore, in the case when\nthe covering requirement $r\\equiv 1$, a bicriteria\n$O(f/\\varepsilon)$-approximation can be achieved even when monotonicity\nrequirement is dropped off from the cost function.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 09:15:01 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 01:31:16 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Shi", "Yishuo", ""], ["Zhang", "Zhao", ""], ["Du", "Ding-Zhu", ""]]}, {"id": "1701.05378", "submitter": "Burak Civek", "authors": "Burak C. Civek and Suleyman S. Kozat", "title": "Efficient Implementation Of Newton-Raphson Methods For Sequential Data\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of sequential linear data prediction for real life\nbig data applications. The second order algorithms, i.e., Newton-Raphson\nMethods, asymptotically achieve the performance of the \"best\" possible linear\ndata predictor much faster compared to the first order algorithms, e.g., Online\nGradient Descent. However, implementation of these methods is not usually\nfeasible in big data applications because of the extremely high computational\nneeds. Regular implementation of the Newton-Raphson Methods requires a\ncomputational complexity in the order of $O(M^2)$ for an $M$ dimensional\nfeature vector, while the first order algorithms need only $O(M)$. To this end,\nin order to eliminate this gap, we introduce a highly efficient implementation\nreducing the computational complexity of the Newton-Raphson Methods from\nquadratic to linear scale. The presented algorithm provides the well-known\nmerits of the second order methods while offering the computational complexity\nof $O(M)$. We utilize the shifted nature of the consecutive feature vectors and\ndo not rely on any statistical assumptions. Therefore, both regular and fast\nimplementations achieve the same performance in the sense of mean square error.\nWe demonstrate the computational efficiency of our algorithm on real life\nsequential big datasets. We also illustrate that the presented algorithm is\nnumerically stable.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 11:34:17 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Civek", "Burak C.", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1701.05395", "submitter": "Christian Frey", "authors": "Christian Frey, Andreas Z\\\"ufle, Tobias Emrich, Matthias Renz", "title": "Efficient Information Flow Maximization in Probabilistic Graphs", "comments": null, "journal-ref": "IEEE Transactions on Knowledge & Data Engineering, vol. 30, no. 5,\n  pp. 880-894, 2018", "doi": "10.1109/TKDE.2017.2780123", "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable propagation of information through large networks, e.g.,\ncommunication networks, social networks or sensor networks is very important in\nmany applications concerning marketing, social networks, and wireless sensor\nnetworks. However, social ties of friendship may be obsolete, and communication\nlinks may fail, inducing the notion of uncertainty in such networks. In this\npaper, we address the problem of optimizing information propagation in\nuncertain networks given a constrained budget of edges. We show that this\nproblem requires to solve two NP-hard subproblems: the computation of expected\ninformation flow, and the optimal choice of edges. To compute the expected\ninformation flow to a source vertex, we propose the F-tree as a specialized\ndata structure, that identifies independent components of the graph for which\nthe information flow can either be computed analytically and efficiently, or\nfor which traditional Monte-Carlo sampling can be applied independently of the\nremaining network. For the problem of finding the optimal edges, we propose a\nseries of heuristics that exploit properties of this data structure. Our\nevaluation shows that these heuristics lead to high quality solutions, thus\nyielding high information flow, while maintaining low running time.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 12:44:17 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 11:02:36 GMT"}, {"version": "v3", "created": "Sat, 9 Dec 2017 22:46:20 GMT"}, {"version": "v4", "created": "Sat, 5 May 2018 19:41:06 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Frey", "Christian", ""], ["Z\u00fcfle", "Andreas", ""], ["Emrich", "Tobias", ""], ["Renz", "Matthias", ""]]}, {"id": "1701.05420", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino, Piotr Gawron, {\\L}ukasz Pawela", "title": "Efficient computation of higher order cumulant tensors", "comments": "22 pages, 6 figures", "journal-ref": "SIAM J. Sci. Comput., 40(3), A1590-A1610, 2018", "doi": "10.1137/17M1149365", "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel algorithm for calculating arbitrary order\ncumulants of multidimensional data. Since the $d^\\text{th}$ order cumulant can\nbe presented in the form of an $d$-dimensional tensor, the algorithm is\npresented using tensor operations. The algorithm provided in the paper takes\nadvantage of super-symmetry of cumulant and moment tensors. We show that the\nproposed algorithm considerably reduces the computational complexity and the\ncomputational memory requirement of cumulant calculation as compared with\nexisting algorithms. For the sizes of interest, the reduction is of the order\nof $d!$ compared to the naive algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:08:50 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 10:50:18 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 10:30:56 GMT"}, {"version": "v4", "created": "Tue, 10 Apr 2018 10:20:18 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Domino", "Krzysztof", ""], ["Gawron", "Piotr", ""], ["Pawela", "\u0141ukasz", ""]]}, {"id": "1701.05492", "submitter": "Martin Milani\\v{c}", "authors": "Ademir Hujdurovi\\'c, Edin Husi\\'c, Martin Milani\\v{c}, Romeo Rizzi and\n  Alexandru I. Tomescu", "title": "Perfect phylogenies via branchings in acyclic digraphs and a\n  generalization of Dilworth's theorem", "comments": "29 pages, 10 figures, extended abstract appeared in Proceedings of WG\n  2017, full paper accepted for publication in ACM Transactions on Algorithms", "journal-ref": null, "doi": "10.1145/3182178", "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in cancer genomics and following the work of\nHajirasouliha and Raphael (WABI 2014), Hujdurovi\\'c et al. (IEEE TCBB, to\nappear) introduced the minimum conflict-free row split (MCRS) problem: split\neach row of a given binary matrix into a bitwise OR of a set of rows so that\nthe resulting matrix corresponds to a perfect phylogeny and has the minimum\npossible number of rows among all matrices with this property. Hajirasouliha\nand Raphael also proposed the study of a similar problem, in which the task is\nto minimize the number of distinct rows of the resulting matrix. Hujdurovi\\'c\net al. proved that both problems are NP-hard, gave a related characterization\nof transitively orientable graphs, and proposed a polynomial-time heuristic\nalgorithm for the MCRS problem based on coloring cocomparability graphs.\n  We give new, more transparent formulations of the two problems, showing that\nthe problems are equivalent to two optimization problems on branchings in a\nderived directed acyclic graph. Building on these formulations, we obtain new\nresults on the two problems, including: (i) a strengthening of the heuristic by\nHujdurovi\\'c et al. via a new min-max result in digraphs generalizing\nDilworth's theorem, which may be of independent interest, (ii) APX-hardness\nresults for both problems, (iii) approximation algorithms, and (iv)\nexponential-time algorithms solving the two problems to optimality faster than\nthe na\\\"ive brute-force approach. Our work relates to several well studied\nnotions in combinatorial optimization: chain partitions in partially ordered\nsets, laminar hypergraphs, and (classical and weighted) colorings of graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 16:05:48 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 20:52:16 GMT"}, {"version": "v3", "created": "Sat, 27 Jan 2018 08:20:45 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Hujdurovi\u0107", "Ademir", ""], ["Husi\u0107", "Edin", ""], ["Milani\u010d", "Martin", ""], ["Rizzi", "Romeo", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "1701.05654", "submitter": "Young Woong Park", "authors": "Young Woong Park, Diego Klabjan", "title": "Bayesian Network Learning via Topological Order", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(99) 1-32, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mixed integer programming (MIP) model and iterative algorithms\nbased on topological orders to solve optimization problems with acyclic\nconstraints on a directed graph. The proposed MIP model has a significantly\nlower number of constraints compared to popular MIP models based on cycle\nelimination constraints and triangular inequalities. The proposed iterative\nalgorithms use gradient descent and iterative reordering approaches,\nrespectively, for searching topological orders. A computational experiment is\npresented for the Gaussian Bayesian network learning problem, an optimization\nproblem minimizing the sum of squared errors of regression models with L1\npenalty over a feature network with application of gene network inference in\nbioinformatics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 01:58:33 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 21:19:02 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1701.05674", "submitter": "Yifei Jin", "authors": "Lingxiao Huang, Yifei Jin, Jian Li, Haitao Wang", "title": "Improved Algorithms For Structured Sparse Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that certain structures of the signal in addition to the standard\nnotion of sparsity (called structured sparsity) can improve the sample\ncomplexity in several compressive sensing applications. Recently, Hegde et al.\nproposed a framework, called approximation-tolerant model-based compressive\nsensing, for recovering signals with structured sparsity. Their framework\nrequires two oracles, the head- and the tail-approximation projection oracles.\nThe two oracles should return approximate solutions in the model which is\nclosest to the query signal. In this paper, we consider two structured sparsity\nmodels and obtain improved projection algorithms. The first one is the tree\nsparsity model, which captures the support structure in the wavelet\ndecomposition of piecewise-smooth signals. We propose a linear time\n$(1-\\epsilon)$-approximation algorithm for head-approximation projection and a\nlinear time $(1+\\epsilon)$-approximation algorithm for tail-approximation\nprojection. The best previous result is an $\\tilde{O}(n\\log n)$ time\nbicriterion approximation algorithm (meaning that their algorithm may return a\nsolution of sparsity larger than $k$) by Hegde et al. Our result provides an\naffirmative answer to the open problem mentioned in the survey of Hegde and\nIndyk. As a corollary, we can recover a constant approximate $k$-sparse signal.\nThe other is the Constrained Earth Mover Distance (CEMD) model, which is useful\nto model the situation where the positions of the nonzero coefficients of a\nsignal do not change significantly as a function of spatial (or temporal)\nlocations. We obtain the first single criterion constant factor approximation\nalgorithm for the head-approximation projection. The previous best known\nalgorithm is a bicriterion approximation. Using this result, we can get a\nfaster constant approximation algorithm with fewer measurements for the\nrecovery problem in CEMD model.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 03:45:47 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Huang", "Lingxiao", ""], ["Jin", "Yifei", ""], ["Li", "Jian", ""], ["Wang", "Haitao", ""]]}, {"id": "1701.05975", "submitter": "Jichang Zhao", "authors": "Rui Fan, Ke Xu and Jichang Zhao", "title": "A GPU-Based Solution to Fast Calculation of Betweenness Centrality on\n  Large Weighted Networks", "comments": "The source code of the study can be downloaded through\n  https://dx.doi.org/10.6084/m9.figshare.4542405. Any issues please feel free\n  to contact jichang@buaa.edu.cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent decades have witnessed the tremendous development of network science,\nwhich indeed brings a new and insightful language to model real systems of\ndifferent domains. Betweenness, a widely employed centrality in network\nscience, is a decent proxy in investigating network loads and rankings.\nHowever, the extremely high computational cost greatly prevents its applying on\nlarge networks. Though several parallel algorithms have been presented to\nreduce its calculation cost on unweighted networks, a fast solution for\nweighted networks, which are in fact more ubiquitous than unweighted ones in\nreality, is still missing. In this study, we develop an efficient parallel\nGPU-based approach to boost the calculation of betweenness centrality on very\nlarge and weighted networks. Comprehensive and systematic evaluations on both\nsynthetic and real-world networks demonstrate that our solution can arrive the\nperformance of 30x to 150x speedup over the CPU implementation by integrating\nthe work-efficient and warp-centric strategies. Our algorithm is completely\nopen-sourced and free to the community and it is public available through\nhttps://dx.doi.org/10.6084/m9.figshare.4542405. Considering the pervasive\ndeployment and declining price of GPU on personal computers and servers, our\nsolution will indeed offer unprecedented opportunities for exploring the\nbetweenness related problems in network science.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 03:23:45 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Fan", "Rui", ""], ["Xu", "Ke", ""], ["Zhao", "Jichang", ""]]}, {"id": "1701.05982", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, P. K. Mishra", "title": "Observations on Factors Affecting Performance of MapReduce based Apriori\n  on Hadoop Cluster", "comments": "8 pages, 8 figures, International Conference on Computing,\n  Communication and Automation (ICCCA2016)", "journal-ref": "2016 International Conference on Computing, Communication and\n  Automation (ICCCA), Greater Noida, India, 2016, pp. 87-94", "doi": "10.1109/CCAA.2016.7813695", "report-no": "466", "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing fast and scalable algorithm for mining frequent itemsets is always\nbeing a most eminent and promising problem of data mining. Apriori is one of\nthe most broadly used and popular algorithm of frequent itemset mining.\nDesigning efficient algorithms on MapReduce framework to process and analyze\nbig datasets is contemporary research nowadays. In this paper, we have focused\non the performance of MapReduce based Apriori on homogeneous as well as on\nheterogeneous Hadoop cluster. We have investigated a number of factors that\nsignificantly affects the execution time of MapReduce based Apriori running on\nhomogeneous and heterogeneous Hadoop Cluster. Factors are specific to both\nalgorithmic and non-algorithmic improvements. Considered factors specific to\nalgorithmic improvements are filtered transactions and data structures.\nExperimental results show that how an appropriate data structure and filtered\ntransactions technique drastically reduce the execution time. The\nnon-algorithmic factors include speculative execution, nodes with poor\nperformance, data locality & distribution of data blocks, and parallelism\ncontrol with input split size. We have applied strategies against these factors\nand fine tuned the relevant parameters in our particular application.\nExperimental results show that if cluster specific parameters are taken care of\nthen there is a significant reduction in execution time. Also we have discussed\nthe issues regarding MapReduce implementation of Apriori which may\nsignificantly influence the performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 05:12:13 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1701.05999", "submitter": "Sandor P. Fekete", "authors": "Zachary Abel, Victor Alvarez, Aman Gour, Adam Hesterberg, Erik D.\n  Demaine, S\\'andor P. Fekete, Phillip Keldenich, Christian Scheffer", "title": "Conflict-Free Coloring of Planar Graphs", "comments": "30 pages, 17 figures; full version (to appear in SIAM Journal on\n  Discrete Mathematics) of extended abstract that appears in Proceeedings of\n  the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA\n  2017), pp. 1951-1963", "journal-ref": null, "doi": "10.1137/1.9781611974782.127", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conflict-free k-coloring of a graph assigns one of k different colors to\nsome of the vertices such that, for every vertex v, there is a color that is\nassigned to exactly one vertex among v and v's neighbors. Such colorings have\napplications in wireless networking, robotics, and geometry, and are\nwell-studied in graph theory. Here we study the natural problem of the\nconflict-free chromatic number chi_CF(G) (the smallest k for which\nconflict-free k-colorings exist). We provide results both for closed\nneighborhoods N[v], for which a vertex v is a member of its neighborhood, and\nfor open neighborhoods N(v), for which vertex v is not a member of its\nneighborhood.\n  For closed neighborhoods, we prove the conflict-free variant of the famous\nHadwiger Conjecture: If an arbitrary graph G does not contain K_{k+1} as a\nminor, then chi_CF(G) <= k. For planar graphs, we obtain a tight worst-case\nbound: three colors are sometimes necessary and always sufficient. We also give\na complete characterization of the computational complexity of conflict-free\ncoloring. Deciding whether chi_CF(G)<= 1 is NP-complete for planar graphs G,\nbut polynomial for outerplanar graphs. Furthermore, deciding whether\nchi_CF(G)<= 2 is NP-complete for planar graphs G, but always true for\nouterplanar graphs. For the bicriteria problem of minimizing the number of\ncolored vertices subject to a given bound k on the number of colors, we give a\nfull algorithmic characterization in terms of complexity and approximation for\nouterplanar and planar graphs.\n  For open neighborhoods, we show that every planar bipartite graph has a\nconflict-free coloring with at most four colors; on the other hand, we prove\nthat for k in {1,2,3}, it is NP-complete to decide whether a planar bipartite\ngraph has a conflict-free k-coloring. Moreover, we establish that any general}\nplanar graph has a conflict-free coloring with at most eight colors.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 09:06:00 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 13:01:25 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Abel", "Zachary", ""], ["Alvarez", "Victor", ""], ["Gour", "Aman", ""], ["Hesterberg", "Adam", ""], ["Demaine", "Erik D.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Keldenich", "Phillip", ""], ["Scheffer", "Christian", ""]]}, {"id": "1701.06005", "submitter": "Song Yang", "authors": "Song Yang, Philipp Wieder, Ramin Yahyapour, Stojan Trajanovski,\n  Xiaoming Fu", "title": "Reliable Virtual Machine Placement and Routing in Clouds", "comments": "An extended version of the paper accepted for publication in IEEE\n  Transactions on Parallel and Distributed Systems", "journal-ref": null, "doi": "10.1109/TPDS.2017.2693273", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current cloud computing systems, when leveraging virtualization\ntechnology, the customer's requested data computing or storing service is\naccommodated by a set of communicated virtual machines (VM) in a scalable and\nelastic manner. These VMs are placed in one or more server nodes according to\nthe node capacities or failure probabilities. The VM placement availability\nrefers to the probability that at least one set of all customer's requested VMs\noperates during the requested lifetime. In this paper, we first study the\nproblem of placing at most H groups of k requested VMs on a minimum number of\nnodes, such that the VM placement availability is no less than $\\delta$, and\nthat the specified communication delay and connection availability for each VM\npair under the same placement group are not violated. We consider this problem\nwith and without Shared-Risk Node Group (SRNG) failures, and prove this problem\nis NP-hard in both cases. We subsequently propose an exact Integer Nonlinear\nProgram (INLP) and an efficient heuristic to solve this problem. We conduct\nsimulations to compare the proposed algorithms with two existing heuristics in\nterms of performance. Finally, we study the related reliable routing problem of\nestablishing a connection over at most w link-disjoint paths from a source to a\ndestination, such that the connection availability requirement is satisfied and\neach path delay is no more than a given value. We devise an exact algorithm and\ntwo heuristics to solve this NP-hard problem, and evaluate them via\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 10:02:53 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 13:15:21 GMT"}, {"version": "v3", "created": "Sat, 8 Apr 2017 05:21:30 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Yang", "Song", ""], ["Wieder", "Philipp", ""], ["Yahyapour", "Ramin", ""], ["Trajanovski", "Stojan", ""], ["Fu", "Xiaoming", ""]]}, {"id": "1701.06064", "submitter": "Marc Goerigk", "authors": "Andr\\'e Chassein and Marc Goerigk and Adam Kasperski and Pawe{\\l}\n  Zieli\\'nski", "title": "On Recoverable and Two-Stage Robust Selection Problems with Budgeted\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of selecting $p$ out of $n$ available items is\ndiscussed, such that their total cost is minimized. We assume that costs are\nnot known exactly, but stem from a set of possible outcomes.\n  Robust recoverable and two-stage models of this selection problem are\nanalyzed. In the two-stage problem, up to $p$ items is chosen in the first\nstage, and the solution is completed once the scenario becomes revealed in the\nsecond stage. In the recoverable problem, a set of $p$ items is selected in the\nfirst stage, and can be modified by exchanging up to $k$ items in the second\nstage, after a scenario reveals.\n  We assume that uncertain costs are modeled through bounded uncertainty sets,\ni.e., the interval uncertainty sets with an additional linear (budget)\nconstraint, in their discrete and continuous variants. Polynomial algorithms\nfor recoverable and two-stage selection problems with continuous bounded\nuncertainty, and compact mixed integer formulations in the case of discrete\nbounded uncertainty are constructed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 18:11:46 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 14:51:36 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Chassein", "Andr\u00e9", ""], ["Goerigk", "Marc", ""], ["Kasperski", "Adam", ""], ["Zieli\u0144ski", "Pawe\u0142", ""]]}, {"id": "1701.06134", "submitter": "Hiroyuki Hanada", "authors": "Hiroyuki Hanada, Mineichi Kudo, Atsuyoshi Nakamura", "title": "On Practical Accuracy of Edit Distance Approximation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance is a basic string similarity measure used in many\napplications such as text mining, signal processing, bioinformatics, and so on.\nHowever, the computational cost can be a problem when we repeat many distance\ncalculations as seen in real-life searching situations. A promising solution to\ncope with the problem is to approximate the edit distance by another distance\nwith a lower computational cost. There are, indeed, many distances have been\nproposed for approximating the edit distance. However, their approximation\naccuracies are evaluated only theoretically: many of them are evaluated only\nwith big-oh (asymptotic) notations, and without experimental analysis.\nTherefore, it is beneficial to know their actual performance in real\napplications. In this study we compared existing six approximation distances in\ntwo approaches: (i) we refined their theoretical approximation accuracy by\ncalculating up to the constant coefficients, and (ii) we conducted some\nexperiments, in one artificial and two real-life data sets, to reveal under\nwhich situations they perform best. As a result we obtained the following\nresults: [Batu 2006] is the best theoretically and [Andoni 2010]\nexperimentally. Theoretical considerations show that [Batu 2006] is the best if\nthe string length n is large enough (n >= 300). [Andoni 2010] is experimentally\nthe best for most data sets and theoretically the second best. [Bar-Yossef\n2004], [Charikar 2006] and [Sokolov 2007], despite their middle-level\ntheoretical performance, are experimentally as good as [Andoni 2010] for pairs\nof strings with large alphabet size.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 07:40:52 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Hanada", "Hiroyuki", ""], ["Kudo", "Mineichi", ""], ["Nakamura", "Atsuyoshi", ""]]}, {"id": "1701.06321", "submitter": "Pravesh K Kothari", "authors": "Boaz Barak and Pravesh Kothari and David Steurer", "title": "Quantum entanglement, sum of squares, and the log rank conjecture", "comments": "23 pages + 1 title-page + 1 table-of-contents", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every $\\epsilon>0$, we give an\n$\\exp(\\tilde{O}(\\sqrt{n}/\\epsilon^2))$-time algorithm for the $1$ vs\n$1-\\epsilon$ \\emph{Best Separable State (BSS)} problem of distinguishing, given\nan $n^2\\times n^2$ matrix $\\mathcal{M}$ corresponding to a quantum measurement,\nbetween the case that there is a separable (i.e., non-entangled) state $\\rho$\nthat $\\mathcal{M}$ accepts with probability $1$, and the case that every\nseparable state is accepted with probability at most $1-\\epsilon$.\nEquivalently, our algorithm takes the description of a subspace $\\mathcal{W}\n\\subseteq \\mathbb{F}^{n^2}$ (where $\\mathbb{F}$ can be either the real or\ncomplex field) and distinguishes between the case that $\\mathcal{W}$ contains a\nrank one matrix, and the case that every rank one matrix is at least $\\epsilon$\nfar (in $\\ell_2$ distance) from $\\mathcal{W}$.\n  To the best of our knowledge, this is the first improvement over the\nbrute-force $\\exp(n)$-time algorithm for this problem. Our algorithm is based\non the \\emph{sum-of-squares} hierarchy and its analysis is inspired by Lovett's\nproof (STOC '14, JACM '16) that the communication complexity of every rank-$n$\nBoolean matrix is bounded by $\\tilde{O}(\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 10:25:42 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 18:01:22 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Barak", "Boaz", ""], ["Kothari", "Pravesh", ""], ["Steurer", "David", ""]]}, {"id": "1701.06446", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino, Piotr Gawron", "title": "Algorithm for an arbitrary-order cumulant tensor calculation in a\n  sliding window of data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High order cumulant tensors carry information about statistics of\nnon-normally distributed multivariate data. In this work we present a new\nefficient algorithm for calculation of cumulants of arbitrary order in a\nsliding window for data streams. We showed that this algorithms enables\nspeedups of cumulants updates compared to current algorithms. This algorithm\ncan be used for processing on-line high-frequency multivariate data and can\nfind applications in, e.g., on-line signal filtering and classification of data\nstreams.\n  To present an application of this algorithm, we propose an estimator of\nnon-Gaussianity of a data stream based on the norms of high-order cumulant\ntensors.\n  We show how to detect the transition from Gaussian distributed data to\nnon-Gaussian ones in a~data stream. In order to achieve high implementation\nefficiency of operations on super-symmetric tensors, such as cumulant tensors,\nwe employ the block structure to store and calculate only one hyper-pyramid\npart of such tensors.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 11:51:35 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 07:24:48 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 07:39:07 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Domino", "Krzysztof", ""], ["Gawron", "Piotr", ""]]}, {"id": "1701.06937", "submitter": "Micha{\\l} Pilipczuk", "authors": "Miko{\\l}aj Boja\\'nczyk and Micha{\\l} Pilipczuk", "title": "Optimizing tree decompositions in MSO", "comments": "Version 1: Extended abstract appeared in the proceedings of STACS\n  2017. The version from STACS 2017 did not include Sections 6 and 8\n  (implementation of MSO transductions). Version 2: Fixed an issue in Section\n  7, as a result the bounds in the Dealternation Lemma needed to be increased\n  from $O(k^2)$ and $O(k^3)$ to $O(k^3)$ and $O(k^4)$, respectively. Also,\n  updated the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic algorithm of Bodlaender and Kloks [J. Algorithms, 1996] solves\nthe following problem in linear fixed-parameter time: given a tree\ndecomposition of a graph of (possibly suboptimal) width $k$, compute an\noptimum-width tree decomposition of the graph. In this work, we prove that this\nproblem can also be solved in MSO in the following sense: for every positive\ninteger $k$, there is an MSO transduction from tree decompositions of width $k$\nto tree decompositions of optimum width. Together with our recent results [LICS\n2016], this implies that for every $k$ there exists an MSO transduction which\ninputs a graph of treewidth $k$, and nondeterministically outputs its tree\ndecomposition of optimum width. We also show that MSO transductions can be\nimplemented in linear fixed-parameter time, which enables us to derive the\nalgorithmic result of Bodlaender and Kloks as a corollary of our main result.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 15:39:49 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 19:45:51 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Boja\u0144czyk", "Miko\u0142aj", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1701.06985", "submitter": "Lars Jaffke", "authors": "Lars Jaffke and Bart M. P. Jansen", "title": "Fine-Grained Parameterized Complexity Analysis of Graph Coloring\n  Problems", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $q$-Coloring problem asks whether the vertices of a graph can be properly\ncolored with $q$ colors. Lokshtanov et al. [SODA 2011] showed that $q$-Coloring\non graphs with a feedback vertex set of size $k$ cannot be solved in time\n$\\mathcal{O}^*((q-\\varepsilon)^k)$, for any $\\varepsilon > 0$, unless the\nStrong Exponential-Time Hypothesis (SETH) fails. In this paper we perform a\nfine-grained analysis of the complexity of $q$-Coloring with respect to a\nhierarchy of parameters. We show that even when parameterized by the vertex\ncover number, $q$ must appear in the base of the exponent: Unless ETH fails,\nthere is no universal constant $\\theta$ such that $q$-Coloring parameterized by\nvertex cover can be solved in time $\\mathcal{O}^*(\\theta^k)$ for all fixed $q$.\nWe apply a method due to Jansen and Kratsch [Inform. & Comput. 2013] to prove\nthat there are $\\mathcal{O}^*((q - \\varepsilon)^k)$ time algorithms where $k$\nis the vertex deletion distance to several graph classes $\\mathcal{F}$ for\nwhich $q$-Coloring is known to be solvable in polynomial time. We generalize\nearlier ad-hoc results by showing that if $\\mathcal{F}$ is a class of graphs\nwhose $(q+1)$-colorable members have bounded treedepth, then there exists some\n$\\varepsilon > 0$ such that $q$-Coloring can be solved in time\n$\\mathcal{O}^*((q-\\varepsilon)^k)$ when parameterized by the size of a given\nmodulator to $\\mathcal{F}$. In contrast, we prove that if $\\mathcal{F}$ is the\nclass of paths - some of the simplest graphs of unbounded treedepth - then no\nsuch algorithm can exist unless SETH fails.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 17:13:26 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Jaffke", "Lars", ""], ["Jansen", "Bart M. P.", ""]]}, {"id": "1701.07204", "submitter": "Kasper Green Larsen", "authors": "Allan Gr{\\o}nlund and Kasper Green Larsen and Alexander Mathiasen and\n  Jesper Sindahl Nielsen and Stefan Schneider and Mingzhou Song", "title": "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension\n$d\\ge 2$, however, for the 1D case there exists exact polynomial time\nalgorithms. Previous literature reported an $O(kn^2)$ time dynamic programming\nalgorithm that uses $O(kn)$ space. It turns out that the problem has been\nconsidered under a different name more than twenty years ago. We present all\nthe existing work that had been overlooked and compare the various solutions\ntheoretically. Moreover, we show how to reduce the space usage for some of\nthem, as well as generalize them to data structures that can quickly report an\noptimal $k$-Means clustering for any $k$. Finally we also generalize all the\nalgorithms to work for the absolute distance and to work for any Bregman\nDivergence. We complement our theoretical contributions by experiments that\ncompare the practical performance of the various algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 08:44:04 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 20:40:50 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 10:37:16 GMT"}, {"version": "v4", "created": "Wed, 25 Apr 2018 10:36:08 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""], ["Mathiasen", "Alexander", ""], ["Nielsen", "Jesper Sindahl", ""], ["Schneider", "Stefan", ""], ["Song", "Mingzhou", ""]]}, {"id": "1701.07208", "submitter": "Lars Rohwedder", "authors": "Klaus Jansen and Lars Rohwedder", "title": "A Quasi-Polynomial Approximation for the Restricted Assignment Problem", "comments": "This article is an extended joint version of conference articles \"On\n  the configuration-LP of the restricted assignment problem\" [Jansen, Rohwedder\n  SODA'17] and \"A quasi-polynomial approximation for the restricted assignment\n  problem\" [Jansen, Rohwedder IPCO'17]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Restricted Assignment Problem is a prominent special case of Scheduling\non Parallel Unrelated Machines. For the strongest known linear programming\nrelaxation, the configuration LP, we improve the non-constructive bound on its\nintegrality gap from 1.9142 to 1.8334 and significantly simplify the proof.\nThen we give a constructive variant, yielding a 1.8334-approximation in\nquasi-polynomial time. This is the first quasi-polynomial algorithm for this\nproblem improving on the long-standing approximation rate of 2.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 08:55:14 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:04:42 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Jansen", "Klaus", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1701.07238", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "A Framework of Dynamic Data Structures for String Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present DYNAMIC, an open-source C++ library implementing\ndynamic compressed data structures for string manipulation. Our framework\nincludes useful tools such as searchable partial sums, succinct/gap-encoded\nbitvectors, and entropy/run-length compressed strings and FM-indexes. We prove\nclose-to-optimal theoretical bounds for the resources used by our structures,\nand show that our theoretical predictions are empirically tightly verified in\npractice. To conclude, we turn our attention to applications. We compare the\nperformance of four recently-published compression algorithms implemented using\nDYNAMIC with those of state-of-the-art tools performing the same task. Our\nexperiments show that algorithms making use of dynamic compressed data\nstructures can be up to three orders of magnitude more space-efficient (albeit\nslower) than classical ones performing the same tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:16:30 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "1701.07242", "submitter": "Marten Maack", "authors": "Klaus Jansen, Marten Maack, Roberto Solis-Oba", "title": "Structural Parameters for Scheduling with Assignment Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider scheduling on identical and unrelated parallel machines with job\nassignment restrictions. These problems are NP-hard and they do not admit\npolynomial time approximation algorithms with approximation ratios smaller than\n$1.5$ unless P$=$NP. However, if we impose limitations on the set of machines\nthat can process a job, the problem sometimes becomes easier in the sense that\nalgorithms with approximation ratios better than $1.5$ exist. We introduce\nthree graphs, based on the assignment restrictions and study the computational\ncomplexity of the scheduling problem with respect to structural properties of\nthese graphs, in particular their tree- and rankwidth. We identify cases that\nadmit polynomial time approximation schemes or FPT algorithms, generalizing and\nextending previous results in this area.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:24:44 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Jansen", "Klaus", ""], ["Maack", "Marten", ""], ["Solis-Oba", "Roberto", ""]]}, {"id": "1701.07299", "submitter": "Igor Malinovi\\'c", "authors": "Yuri Faenza and Igor Malinovic", "title": "A PTAS for the Time-Invariant Incremental Knapsack problem", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Time-Invariant Incremental Knapsack problem (IIK) is a generalization of\nMaximum Knapsack to a discrete multi-period setting. At each time, capacity\nincreases and items can be added, but not removed from the knapsack. The goal\nis to maximize the sum of profits over all times. IIK models various\napplications including specific financial markets and governmental decision\nprocesses. IIK is strongly NP-hard and there has been work on giving\napproximation algorithms for some special cases. In this paper, we settle the\ncomplexity of IIK by designing a PTAS based on rounding a disjuncive\nformulation, and provide several extensions of the technique.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 13:23:36 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 16:56:01 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 11:48:08 GMT"}, {"version": "v4", "created": "Tue, 30 Jan 2018 16:31:44 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Faenza", "Yuri", ""], ["Malinovic", "Igor", ""]]}, {"id": "1701.07336", "submitter": "Michele Coscia", "authors": "Michele Coscia, Frank Neffke", "title": "Network Backboning with Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are powerful instruments to study complex phenomena, but they become\nhard to analyze in data that contain noise. Network backbones provide a tool to\nextract the latent structure from noisy networks by pruning non-salient edges.\nWe describe a new approach to extract such backbones. We assume that edge\nweights are drawn from a binomial distribution, and estimate the error-variance\nin edge weights using a Bayesian framework. Our approach uses a more realistic\nnull model for the edge weight creation process than prior work. In particular,\nit simultaneously considers the propensity of nodes to send and receive\nconnections, whereas previous approaches only considered nodes as emitters of\nedges. We test our model with real world networks of different types (flows,\nstocks, co-occurrences, directed, undirected) and show that our Noise-Corrected\napproach returns backbones that outperform other approaches on a number of\ncriteria. Our approach is scalable, able to deal with networks with millions of\nedges.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 14:39:52 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Coscia", "Michele", ""], ["Neffke", "Frank", ""]]}, {"id": "1701.07473", "submitter": "Jimmy Dobler", "authors": "Jimmy Dobler, Atri Rudra", "title": "Implementation of Tetris as a Model Counter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving SharpSAT problems is an important area of work. In this paper, we\ndiscuss implementing Tetris, an algorithm originally designed for handling\nnatural joins, as an exact model counter for the SharpSAT problem. Tetris uses\na simple geometric framework, yet manages to achieve the fractional\nhypertree-width bound. Its design allows it to handle complex problems\ninvolving extremely large numbers of clauses on which other state-of-the-art\nmodel counters do not perform well, yet still performs strongly on standard SAT\nbenchmarks.\n  We have achieved the following objectives. First, we have found a natural set\nof model counting benchmarks on which Tetris outperforms other model counters.\nSecond, we have constructed a data structure capable of efficiently handling\nand caching all of the data Tetris needs to work on over the course of the\nalgorithm. Third, we have modified Tetris in order to move from a theoretical,\nasymptotic-time-focused environment to one that performs well in practice. In\nparticular, we have managed to produce results keeping us within a single order\nof magnitude as compared to other solvers on most benchmarks, and outperform\nthose solvers by multiple orders of magnitude on others.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:23:58 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Dobler", "Jimmy", ""], ["Rudra", "Atri", ""]]}, {"id": "1701.07498", "submitter": "Guohui Lin", "authors": "Wenchang Luo, Taibo Luo, Randy Goebel, and Guohui Lin", "title": "On rescheduling due to machine disruption while to minimize the total\n  weighted completion time", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a single machine rescheduling problem that arises from an\nunexpected machine unavailability, after the given set of jobs has already been\nscheduled to minimize the total weighted completion time. Such a disruption is\nrepresented as an unavailable time interval and is revealed to the production\nplanner before any job is processed; the production planner wishes to\nreschedule the jobs to minimize the alteration to the originally planned\nschedule, which is measured as the maximum time deviation between the original\nand the new schedules for all the jobs. The objective function in this\nrescheduling problem is to minimize the sum of the total weighted completion\ntime and the weighted maximum time deviation, under the constraint that the\nmaximum time deviation is bounded above by a given value. That is, the maximum\ntime deviation is taken both as a constraint and as part of the objective\nfunction. We present a pseudo-polynomial time exact algorithm and a fully\npolynomial time approximation scheme, the latter of which is the best possible\ngiven that the general problem is NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 21:44:24 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Luo", "Wenchang", ""], ["Luo", "Taibo", ""], ["Goebel", "Randy", ""], ["Lin", "Guohui", ""]]}, {"id": "1701.07681", "submitter": "Patrick Sch\\\"afer", "authors": "Patrick Sch\\\"afer and Ulf Leser", "title": "Fast and Accurate Time Series Classification with WEASEL", "comments": null, "journal-ref": "Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management (CIKM '17). ACM, 637-646", "doi": "10.1145/3132847.3132980", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series (TS) occur in many scientific and commercial applications,\nranging from earth surveillance to industry automation to the smart grids. An\nimportant type of TS analysis is classification, which can, for instance,\nimprove energy load forecasting in smart grids by detecting the types of\nelectronic devices based on their energy consumption profiles recorded by\nautomatic sensors. Such sensor-driven applications are very often characterized\nby (a) very long TS and (b) very large TS datasets needing classification.\nHowever, current methods to time series classification (TSC) cannot cope with\nsuch data volumes at acceptable accuracy; they are either scalable but offer\nonly inferior classification quality, or they achieve state-of-the-art\nclassification quality but cannot scale to large data volumes.\n  In this paper, we present WEASEL (Word ExtrAction for time SEries\ncLassification), a novel TSC method which is both scalable and accurate. Like\nother state-of-the-art TSC methods, WEASEL transforms time series into feature\nvectors, using a sliding-window approach, which are then analyzed through a\nmachine learning classifier. The novelty of WEASEL lies in its specific method\nfor deriving features, resulting in a much smaller yet much more discriminative\nfeature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more\naccurate than the best current non-ensemble algorithms at orders-of-magnitude\nlower classification and training times, and it is almost as accurate as\nensemble classifiers, whose computational complexity makes them inapplicable\neven for mid-size datasets. The outstanding robustness of WEASEL is also\nconfirmed by experiments on two real smart grid datasets, where it\nout-of-the-box achieves almost the same accuracy as highly tuned,\ndomain-specific methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 13:09:48 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Sch\u00e4fer", "Patrick", ""], ["Leser", "Ulf", ""]]}, {"id": "1701.07822", "submitter": "Michael Holzhauser", "authors": "Michael Holzhauser, Sven O. Krumke", "title": "An FPTAS for the parametric knapsack problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the parametric knapsack problem, in which the\nitem profits are affine functions depending on a real-valued parameter. The aim\nis to provide a solution for all values of the parameter. It is well-known that\nany exact algorithm for the problem may need to output an exponential number of\nknapsack solutions. We present a fully polynomial-time approximation scheme\n(FPTAS) for the problem that, for any desired precision $\\varepsilon \\in\n(0,1)$, computes $(1-\\varepsilon)$-approximate solutions for all values of the\nparameter. This is the first FPTAS for the parametric knapsack problem that\ndoes not require the slopes and intercepts of the affine functions to be\nnon-negative but works for arbitrary integral values. Our FPTAS outputs\n$\\mathcal{O}(\\frac{n^2}{\\varepsilon})$ knapsack solutions and runs in strongly\npolynomial-time $\\mathcal{O}(\\frac{n^4}{\\varepsilon^2})$. Even for the special\ncase of positive input data, this is the first FPTAS with a strongly polynomial\nrunning time. We also show that this time bound can be further improved to\n$\\mathcal{O}(\\frac{n^2}{\\varepsilon} \\cdot A(n,\\varepsilon))$, where\n$A(n,\\varepsilon)$ denotes the running time of any FPTAS for the traditional\n(non-parametric) knapsack problem.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 18:58:02 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 10:15:31 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Holzhauser", "Michael", ""], ["Krumke", "Sven O.", ""]]}, {"id": "1701.08022", "submitter": "Sebastian Deorowicz", "authors": "Marek Kokot and Maciej D{\\l}ugosz and Sebastian Deorowicz", "title": "KMC 3: counting and manipulating k-mer statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary: Counting all k-mers in a given dataset is a standard procedure in\nmany bioinformatics applications. We introduce KMC3, a significant improvement\nof the former KMC2 algorithm together with KMC tools for manipulating k-mer\ndatabases. Usefulness of the tools is shown on a few real problems.\nAvailability: Program is freely available at\nhttp://sun.aei.polsl.pl/REFRESH/kmc. Contact: sebastian.deorowicz@polsl.pl\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:04:30 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kokot", "Marek", ""], ["D\u0142ugosz", "Maciej", ""], ["Deorowicz", "Sebastian", ""]]}, {"id": "1701.08049", "submitter": "Guus Regts", "authors": "Han Peters, Guus Regts", "title": "On a conjecture of Sokal concerning roots of the independence polynomial", "comments": "We have updated the file partly based on some comments from a\n  referee. The file is now 20 pages and contains one figure. Accepted in\n  Michigan Mathematical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conjecture of Sokal (2001) regarding the domain of non-vanishing for\nindependence polynomials of graphs, states that given any natural number\n$\\Delta \\ge 3$, there exists a neighborhood in $\\mathbb C$ of the interval $[0,\n\\frac{(\\Delta-1)^{\\Delta-1}}{(\\Delta-2)^{\\Delta}})$ on which the independence\npolynomial of any graph with maximum degree at most $\\Delta$ does not vanish.\nWe show here that Sokal's Conjecture holds, as well as a multivariate version,\nand prove optimality for the domain of non-vanishing. An important step is to\ntranslate the setting to the language of complex dynamical systems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 13:37:11 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 14:02:26 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 10:35:11 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Peters", "Han", ""], ["Regts", "Guus", ""]]}, {"id": "1701.08128", "submitter": "Gabriele Santi Mr", "authors": "Gabriele Santi and Leonardo De Laurentiis", "title": "Evaluating a sublinear-time algorithm for the Minimum Spanning Tree\n  Weight problem", "comments": "23 pages, 13 figures, project developed during Master's Degree\n  studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation and an experimental evaluation of an algorithm\nthat, given a connected graph G (represented by adjacency lists), estimates in\nsublinear time, with a relative error, the Minimum Spanning Tree Weight of G;\nthe original algorithm has been presented in \"Approximating the minimum\nspanning tree weight in sublinear time\", by Bernard Chazelle, Ronitt Rubinfeld,\nand Luca Trevisan (published with SIAM, DOI 10.1137/S0097539702403244). Since\nthe theoretical performances have already been shown and demonstrated in the\nabove-mentioned paper, our goal is, exclusively, to experimental evaluate the\nalgorithm and at last to present the results. Initially we discuss about some\ntheoretical aspects that arose while we were valuating the asymptotic\ncomplexity of our specific implementation. Some technical insights are then\ngiven on the implementation of the algorithm and on the dataset used in the\ntest phase, hence to show how the experiment has been carried out even for\nreproducibility purposes; the results are then evaluated empirically and widely\ndiscussed, comparing these with the performances of the Prim algorithm and the\nKruskal algorithm, launching several runs on a heterogeneous set of graphs and\ndifferent theoretical models for them.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 17:45:04 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Santi", "Gabriele", ""], ["De Laurentiis", "Leonardo", ""]]}, {"id": "1701.08315", "submitter": "Baigong Zheng", "authors": "Baigong Zheng", "title": "Linear-time approximation schemes for planar minimum three-edge\n  connected and three-vertex connected spanning subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first polynomial-time approximation schemes, i.e., (1 +\n{\\epsilon})-approximation algorithm for any constant {\\epsilon} > 0, for the\nminimum three-edge connected spanning subgraph problem and the minimum\nthree-vertex connected spanning subgraph problem in undirected planar graphs.\nBoth the approximation schemes run in linear time.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 19:05:52 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Zheng", "Baigong", ""]]}, {"id": "1701.08423", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad, Chris Schwiegelshohn", "title": "On the Local Structure of Stable Clustering Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic $k$-median and $k$-means clustering objectives in the\nbeyond-worst-case scenario. We consider three well-studied notions of\nstructured data that aim at characterizing real-world inputs: Distribution\nStability (introduced by Awasthi, Blum, and Sheffet, FOCS 2010), Spectral\nSeparability (introduced by Kumar and Kannan, FOCS 2010), Perturbation\nResilience (introduced by Bilu and Linial, ICS 2010).\n  We prove structural results showing that inputs satisfying at least one of\nthe conditions are inherently \"local\". Namely, for any such input, any local\noptimum is close both in term of structure and in term of objective value to\nthe global optima.\n  As a corollary we obtain that the widely-used Local Search algorithm has\nstrong performance guarantees for both the tasks of recovering the underlying\noptimal clustering and obtaining a clustering of small cost. This is a\nsignificant step toward understanding the success of local search heuristics in\nclustering applications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 19:55:27 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 08:46:26 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 09:46:07 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Schwiegelshohn", "Chris", ""]]}, {"id": "1701.08517", "submitter": "Pieter Leyman", "authors": "Pieter Leyman, San Tu Pham, Patrick De Causmaecker", "title": "The Intermittent Traveling Salesman Problem with Different Temperature\n  Profiles: Greedy or not?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we discuss the intermittent traveling salesman problem\n(ITSP), which extends the traditional traveling salesman problem (TSP) by\nimposing temperature restrictions on each node. These additional constraints\nlimit the maximum allowable visit time per node, and result in multiple visits\nfor each node which cannot be serviced in a single visit. We discuss three\ndifferent temperature increase and decrease functions, namely a linear, a\nquadratic and an exponential function. To solve the problem, we consider three\ndifferent solution representations as part of a metaheuristic approach. We\nargue that in case of similar temperature increase and decrease profiles, it is\nalways beneficial to apply a greedy approach, i.e. to process as much as\npossible given the current node temperature.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 09:22:08 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Leyman", "Pieter", ""], ["Pham", "San Tu", ""], ["De Causmaecker", "Patrick", ""]]}, {"id": "1701.08688", "submitter": "Ibrahim Chegrane", "authors": "Ibrahim Chegrane", "title": "Approximate String Matching: Theory and Applications (La Recherche\n  Approch\\'ee de Motifs : Th\\'eorie et Applications)", "comments": "181 pages, in French, PhD Thesis, University of Sciences and\n  Technology Houari Boumediene (USTHB) Algiers Algeria, Text Algorithms, String\n  searching, String matching, Pattern matching, Data structures, Exact string\n  matching, Approximate string matching, Bioinformatics, Autocomplete", "journal-ref": null, "doi": null, "report-no": "61/2016-C/INF", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximate string matching is a fundamental and recurrent problem that\narises in most computer science fields. This problem can be defined as follows:\n  Let $D=\\{x_1,x_2,\\ldots x_d\\}$ be a set of $d$ words defined on an alphabet\n$\\Sigma$, let $q$ be a query defined also on $\\Sigma$, and let $k$ be a\npositive integer. We want to build a data structure on $D$ capable of answering\nthe following query: find all words in $D$ that are at most different from the\nquery word $q$ with $k$ errors.\n  In this thesis, we study the approximate string matching methods in\ndictionaries, texts, and indexes, to propose practical methods that solve this\nproblem efficiently. We explore this problem in three complementary directions:\n  1) The approximate string matching in the dictionary. We propose two\nsolutions to this problem, the first one uses hash tables for $k \\geq 2$, the\nsecond uses the Trie and reverse Trie, and it is restricted to (k = 1). The two\nsolutions are adaptable, without loss of performance, to the approximate string\nmatching in a text.\n  2) The approximate string matching for \\textit{autocompletion}, which is,\nfind all suffixes of a given prefix that may contain errors. We give a new\nsolution better in practice than all the previous proposed solutions.\n  3) The problem of the alignment of biological sequences can be interpreted as\nan approximate string matching problem. We propose a solution for peers and\nmultiple sequences alignment.\n  \\medskip All the results obtained showed that our algorithms, give the best\nperformance on sets of practical data (benchmark from the real world). All our\nmethods are proposed as libraries, and they are published online.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:32:41 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Chegrane", "Ibrahim", ""]]}, {"id": "1701.08809", "submitter": "Martin Gro{\\ss}", "authors": "Fidaa Abed and Lin Chen and Yann Disser and Martin Gro{\\ss} and Nicole\n  Megow and Julie Mei{\\ss}ner and Alexander T. Richter and Roman Rischke", "title": "Scheduling Maintenance Jobs in Networks", "comments": "CIAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of scheduling the maintenance of edges in a\nnetwork, motivated by the goal of minimizing outages in transportation or\ntelecommunication networks. We focus on maintaining connectivity between two\nnodes over time; for the special case of path networks, this is related to the\nproblem of minimizing the busy time of machines.\n  We show that the problem can be solved in polynomial time in arbitrary\nnetworks if preemption is allowed. If preemption is restricted to integral time\npoints, the problem is NP-hard and in the non-preemptive case we give strong\nnon-approximability results. Furthermore, we give tight bounds on the power of\npreemption, that is, the maximum ratio of the values of non-preemptive and\npreemptive optimal solutions.\n  Interestingly, the preemptive and the non-preemptive problem can be solved\nefficiently on paths, whereas we show that mixing both leads to a weakly\nNP-hard problem that allows for a simple 2-approximation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 20:12:57 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Abed", "Fidaa", ""], ["Chen", "Lin", ""], ["Disser", "Yann", ""], ["Gro\u00df", "Martin", ""], ["Megow", "Nicole", ""], ["Mei\u00dfner", "Julie", ""], ["Richter", "Alexander T.", ""], ["Rischke", "Roman", ""]]}, {"id": "1701.08897", "submitter": "Takuro Fukunaga", "authors": "Takuro Fukunaga and Takanori Maehara", "title": "Computing a tree having a small vertex cover", "comments": "appeared in COCOA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new Steiner tree problem, called vertex-cover-weighted Steiner\ntree problem. This problem defines the weight of a Steiner tree as the minimum\nweight of vertex covers in the tree, and seeks a minimum-weight Steiner tree in\na given vertex-weighted undirected graph. Since it is included by the Steiner\ntree activation problem, the problem admits an O(log n)-approximation algorithm\nin general graphs with n vertices. This approximation factor is tight up to a\nconstant because it is NP-hard to achieve an o(log n)-approximation for the\nvertex-cover-weighted Steiner tree problem on general graphs even if the given\nvertex weights are uniform and a spanning tree is required instead of a Steiner\ntree. In this paper, we present constant-factor approximation algorithms for\nthe problem with unit disk graphs and with graphs excluding a fixed minor. For\nthe latter graph class, our algorithm can be also applied for the Steiner tree\nactivation problem.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 02:46:09 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 01:04:26 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Fukunaga", "Takuro", ""], ["Maehara", "Takanori", ""]]}, {"id": "1701.08920", "submitter": "William Pettersson", "authors": "William Pettersson and Melih Ozlen", "title": "A parallel approach to bi-objective integer programming", "comments": "7 pages", "journal-ref": null, "doi": "10.21914/anziamj.v58i0.11724", "report-no": null, "categories": "math.OC cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain a better understanding of the trade-offs between various\nobjectives, Bi-Objective Integer Programming (BOIP) algorithms calculate the\nset of all non-dominated vectors and present these as the solution to a BOIP\nproblem. Historically, these algorithms have been compared in terms of the\nnumber of single-objective IPs solved and total CPU time taken to produce the\nsolution to a problem. This is equitable, as researchers can often have access\nto widely differing amounts of computing power. However, the real world has\nrecently seen a large uptake of multi-core processors in computers, laptops,\ntablets and even mobile phones. With this in mind, we look at how to best\nutilise parallel processing to improve the elapsed time of optimisation\nalgorithms. We present two methods of parallelising the recursive algorithm\npresented by Ozlen, Burton and MacRae. Both new methods utilise two threads and\nimprove running times. One of the new methods, the Meeting algorithm, halves\nrunning time to achieve near-perfect parallelisation. The results are compared\nwith the efficiency of parallelisation within the commercial IP solver IBM ILOG\nCPLEX, and the new methods are both shown to perform better.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 05:28:25 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Pettersson", "William", ""], ["Ozlen", "Melih", ""]]}, {"id": "1701.08995", "submitter": "Leo van Iersel", "authors": "Leo van Iersel, Vincent Moulton, Eveline de Swart and Taoyang Wu", "title": "Binets: fundamental building blocks for phylogenetic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks are a generalization of evolutionary trees that are\nused by biologists to represent the evolution of organisms which have undergone\nreticulate evolution. Essentially, a phylogenetic network is a directed acyclic\ngraph having a unique root in which the leaves are labelled by a given set of\nspecies. Recently, some approaches have been developed to construct\nphylogenetic networks from collections of networks on 2- and 3-leaved networks,\nwhich are known as binets and trinets, respectively. Here we study in more\ndepth properties of collections of binets, one of the simplest possible types\nof networks into which a phylogenetic network can be decomposed. More\nspecifically, we show that if a collection of level-1 binets is compatible with\nsome binary network, then it is also compatible with a binary level-1 network.\nOur proofs are based on useful structural results concerning lowest stable\nancestors in networks. In addition, we show that, although the binets do not\ndetermine the topology of the network, they do determine the number of\nreticulations in the network, which is one of its most important parameters. We\nalso consider algorithmic questions concerning binets. We show that deciding\nwhether an arbitrary set of binets is compatible with some network is at least\nas hard as the well-known Graph Isomorphism problem. However, if we restrict to\nlevel-1 binets, it is possible to decide in polynomial time whether there\nexists a binary network that displays all the binets. We also show that to find\na network that displays a maximum number of the binets is NP-hard, but that\nthere exists a simple polynomial-time 1/3-approximation algorithm for this\nproblem. It is hoped that these results will eventually assist in the\ndevelopment of new methods for constructing phylogenetic networks from\ncollections of smaller networks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 11:18:42 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["van Iersel", "Leo", ""], ["Moulton", "Vincent", ""], ["de Swart", "Eveline", ""], ["Wu", "Taoyang", ""]]}]