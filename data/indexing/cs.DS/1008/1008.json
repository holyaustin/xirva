[{"id": "1008.0125", "submitter": "Alistair Sinclair", "authors": "Fabio Martinelli and Alistair Sinclair", "title": "Mixing Time for the Solid-on-Solid Model", "comments": "A preliminary version of this paper appeared in Proceedings of the\n  41st ACM Symposium on Theory of Computer Science (STOC), 2009, pages 571-580", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cs.DS math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the mixing time of a natural local Markov chain (the Glauber\ndynamics) on configurations of the solid-on-solid model of statistical physics.\nThis model has been proposed, among other things, as an idealization of the\nbehavior of contours in the Ising model at low temperatures. Our main result is\nan upper bound on the mixing time of $O~(n^{3.5})$, which is tight within a\nfactor of $O~(sqrt{n})$. (The notation O~ hides factors that are logarithmic in\nn.) The proof, which in addition gives some insight into the actual evolution\nof the contours, requires the introduction of a number of novel analytical\ntechniques that we conjecture will have other applications.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 21:46:11 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Martinelli", "Fabio", ""], ["Sinclair", "Alistair", ""]]}, {"id": "1008.0213", "submitter": "Eun Jung Kim", "authors": "Eun Jung Kim and Ryan Williams", "title": "Improved Parameterized Algorithms for Constraint Satisfaction", "comments": "A preliminary version of this paper has been accepted for IPEC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many constraint satisfaction problems, the algorithm which chooses a\nrandom assignment achieves the best possible approximation ratio. For instance,\na simple random assignment for {\\sc Max-E3-Sat} allows 7/8-approximation and\nfor every $\\eps >0$ there is no polynomial-time ($7/8+\\eps$)-approximation\nunless P=NP. Another example is the {\\sc Permutation CSP} of bounded arity.\nGiven the expected fraction $\\rho$ of the constraints satisfied by a random\nassignment (i.e. permutation), there is no $(\\rho+\\eps)$-approximation\nalgorithm for every $\\eps >0$, assuming the Unique Games Conjecture (UGC).\n  In this work, we consider the following parameterization of constraint\nsatisfaction problems. Given a set of $m$ constraints of constant arity, can we\nsatisfy at least $\\rho m +k$ constraint, where $\\rho$ is the expected fraction\nof constraints satisfied by a random assignment? {\\sc Constraint Satisfaction\nProblems above Average} have been posed in different forms in the literature\n\\cite{Niedermeier2006,MahajanRamanSikdar09}. We present a faster parameterized\nalgorithm for deciding whether $m/2+k/2$ equations can be simultaneously\nsatisfied over ${\\mathbb F}_2$. As a consequence, we obtain $O(k)$-variable\nbikernels for {\\sc boolean CSPs} of arity $c$ for every fixed $c$, and for {\\sc\npermutation CSPs} of arity 3. This implies linear bikernels for many problems\nunder the \"above average\" parameterization, such as {\\sc Max-$c$-Sat}, {\\sc\nSet-Splitting}, {\\sc Betweenness} and {\\sc Max Acyclic Subgraph}. As a result,\nall the parameterized problems we consider in this paper admit $2^{O(k)}$-time\nalgorithms.\n  We also obtain non-trivial hybrid algorithms for every Max $c$-CSP: for every\ninstance $I$, we can either approximate $I$ beyond the random assignment\nthreshold in polynomial time, or we can find an optimal solution to $I$ in\nsubexponential time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Aug 2010 01:01:18 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2011 17:07:57 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Kim", "Eun Jung", ""], ["Williams", "Ryan", ""]]}, {"id": "1008.0390", "submitter": "Alan Frieze", "authors": "Alan Frieze, Gregory Sorkin", "title": "Efficient algorithms for three-dimensional axial and planar random\n  assignment problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beautiful formulas are known for the expected cost of random two-dimensional\nassignment problems, but in higher dimensions even the scaling is not known. In\nthree dimensions and above, the problem has natural \"Axial\" and \"Planar\"\nversions, both of which are NP-hard. For 3-dimensional Axial random assignment\ninstances of size $n$, the cost scales as $\\Omega(1/n)$, and a main result of\nthe present paper is a linear-time algorithm that, with high probability, finds\na solution of cost $O(n^{-1+o(1)})$. For 3-dimensional Planar assignment, the\nlower bound is $\\Omega(n)$, and we give a new efficient matching-based\nalgorithm that with high probability returns a solution with cost $O(n \\log\nn)$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Aug 2010 19:58:07 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2013 01:30:12 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2013 18:25:20 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Frieze", "Alan", ""], ["Sorkin", "Gregory", ""]]}, {"id": "1008.0501", "submitter": "Mahmoud Fouz", "authors": "Benjamin Doerr and Mahmoud Fouz", "title": "Quasi-Random Rumor Spreading: Reducing Randomness Can Be Costly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a time-randomness tradeoff for the quasi-random rumor spreading\nprotocol proposed by Doerr, Friedrich and Sauerwald [SODA 2008] on complete\ngraphs. In this protocol, the goal is to spread a piece of information\noriginating from one vertex throughout the network. Each vertex is assumed to\nhave a (cyclic) list of its neighbors. Once a vertex is informed by one of its\nneighbors, it chooses a position in its list uniformly at random and then\ninforms its neighbors starting from that position and proceeding in order of\nthe list. Angelopoulos, Doerr, Huber and Panagiotou [Electron.~J.~Combin.~2009]\nshowed that after $(1+o(1))(\\log_2 n + \\ln n)$ rounds, the rumor will have been\nbroadcasted to all nodes with probability $1 - o(1)$.\n  We study the broadcast time when the amount of randomness available at each\nnode is reduced in natural way. In particular, we prove that if each node can\nonly make its initial random selection from every $\\ell$-th node on its list,\nthen there exists lists such that $(1-\\varepsilon) (\\log_2 n + \\ln n - \\log_2\n\\ell - \\ln \\ell)+\\ell-1$ steps are needed to inform every vertex with\nprobability at least $1-O\\bigl(\\exp\\bigl(-\\frac{n^\\varepsilon}{2\\ln\nn}\\bigr)\\bigr)$. This shows that a further reduction of the amount of\nrandomness used in a simple quasi-random protocol comes at a loss of\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 09:59:29 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Doerr", "Benjamin", ""], ["Fouz", "Mahmoud", ""]]}, {"id": "1008.0541", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund", "title": "Determinant Sums for Undirected Hamiltonicity", "comments": "To appear at IEEE FOCS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Monte Carlo algorithm for Hamiltonicity detection in an\n$n$-vertex undirected graph running in $O^*(1.657^{n})$ time. To the best of\nour knowledge, this is the first superpolynomial improvement on the worst case\nruntime for the problem since the $O^*(2^n)$ bound established for TSP almost\nfifty years ago (Bellman 1962, Held and Karp 1962). It answers in part the\nfirst open problem in Woeginger's 2003 survey on exact algorithms for NP-hard\nproblems.\n  For bipartite graphs, we improve the bound to $O^*(1.414^{n})$ time. Both the\nbipartite and the general algorithm can be implemented to use space polynomial\nin $n$.\n  We combine several recently resurrected ideas to get the results. Our main\ntechnical contribution is a new reduction inspired by the algebraic sieving\nmethod for $k$-Path (Koutis ICALP 2008, Williams IPL 2009). We introduce the\nLabeled Cycle Cover Sum in which we are set to count weighted arc labeled cycle\ncovers over a finite field of characteristic two. We reduce Hamiltonicity to\nLabeled Cycle Cover Sum and apply the determinant summation technique for Exact\nSet Covers (Bj\\\"orklund STACS 2010) to evaluate it.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 13:10:49 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""]]}, {"id": "1008.0587", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail", "title": "Row Sampling for Matrix Algorithms via a Non-Commutative Bernstein Bound", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus the use of \\emph{row sampling} for approximating matrix algorithms.\nWe give applications to matrix multipication; sparse matrix reconstruction;\nand, \\math{\\ell_2} regression. For a matrix \\math{\\matA\\in\\R^{m\\times d}} which\nrepresents \\math{m} points in \\math{d\\ll m} dimensions, all of these tasks can\nbe achieved in \\math{O(md^2)} via the singular value decomposition (SVD). For\nappropriate row-sampling probabilities (which typically depend on the norms of\nthe rows of the \\math{m\\times d} left singular matrix of \\math{\\matA} (the\n\\emph{leverage scores}), we give row-sampling algorithms with linear (up to\npolylog factors) dependence on the stable rank of \\math{\\matA}. This result is\nachieved through the application of non-commutative Bernstein bounds.\n  We then give, to our knowledge, the first algorithms for computing\napproximations to the appropriate row-sampling probabilities without going\nthrough the SVD of \\math{\\matA}. Thus, these are the first \\math{o(md^2)}\nalgorithms for row-sampling based approximations to the matrix algorithms which\nuse leverage scores as the sampling probabilities. The techniques we use to\napproximate sampling according to the leverage scores uses some powerful recent\nresults in the theory of random projections for embedding, and may be of some\nindependent interest. We confess that one may perform all these matrix tasks\nmore efficiently using these same random projection methods, however the\nresulting algorithms are in terms of a small number of linear combinations of\nall the rows. In many applications, the actual rows of \\math{\\matA} have some\nphysical meaning and so methods based on a small number of the actual rows are\nof interest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 16:28:10 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Magdon-Ismail", "Malik", ""]]}, {"id": "1008.0831", "submitter": "C. Seshadhri", "authors": "C. Seshadhri and Jan Vondrak", "title": "Is submodularity testable?", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of property testing of submodularity on the boolean\nhypercube. Submodular functions come up in a variety of applications in\ncombinatorial optimization. For a vast range of algorithms, the existence of an\noracle to a submodular function is assumed. But how does one check if this\noracle indeed represents a submodular function?\n  Consider a function f:{0,1}^n \\rightarrow R. The distance to submodularity is\nthe minimum fraction of values of $f$ that need to be modified to make f\nsubmodular. If this distance is more than epsilon > 0, then we say that f is\nepsilon-far from being submodular. The aim is to have an efficient procedure\nthat, given input f that is epsilon-far from being submodular, certifies that f\nis not submodular. We analyze a very natural tester for this problem, and prove\nthat it runs in subexponential time. This gives the first non-trivial tester\nfor submodularity. On the other hand, we prove an interesting lower bound (that\nis, unfortunately, quite far from the upper bound) suggesting that this tester\ncannot be very efficient in terms of epsilon. This involves non-trivial\nexamples of functions which are far from submodular and yet do not exhibit too\nmany local violations.\n  We also provide some constructions indicating the difficulty in designing a\ntester for submodularity. We construct a partial function defined on\nexponentially many points that cannot be extended to a submodular function, but\nany strict subset of these values can be extended to a submodular function.\n", "versions": [{"version": "v1", "created": "Wed, 4 Aug 2010 17:47:18 GMT"}], "update_date": "2010-08-05", "authors_parsed": [["Seshadhri", "C.", ""], ["Vondrak", "Jan", ""]]}, {"id": "1008.1191", "submitter": "Dennis Luxen", "authors": "Daniel Karch, Dennis Luxen, Peter Sanders", "title": "Improved Fast Similarity Search in Dictionaries", "comments": "Full version of a short paper accepted for Spire 2010, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer an algorithm to solve the approximate dictionary matching\nproblem. Given a list of words $\\mathcal{W}$, maximum distance $d$ fixed at\npreprocessing time and a query word $q$, we would like to retrieve all words\nfrom $\\mathcal{W}$ that can be transformed into $q$ with $d$ or less edit\noperations. We present data structures that support fault tolerant queries by\ngenerating an index. On top of that, we present a generalization of the method\nthat eases memory consumption and preprocessing time significantly. At the same\ntime, running times of queries are virtually unaffected. We are able to match\nin lists of hundreds of thousands of words and beyond within microseconds for\nreasonable distances.\n", "versions": [{"version": "v1", "created": "Fri, 6 Aug 2010 13:30:31 GMT"}, {"version": "v2", "created": "Wed, 18 Aug 2010 12:38:24 GMT"}], "update_date": "2010-08-19", "authors_parsed": [["Karch", "Daniel", ""], ["Luxen", "Dennis", ""], ["Sanders", "Peter", ""]]}, {"id": "1008.1480", "submitter": "Lee-Ad Gottlieb", "authors": "Yair Bartal, Lee-Ad Gottlieb, Tsvi Kopelowitz, Moshe Lewenstein, Liam\n  Roditty", "title": "Fast, precise and dynamic distance queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximate distance oracle for a point set S with n points and\ndoubling dimension {\\lambda}. For every {\\epsilon}>0, the oracle supports\n(1+{\\epsilon})-approximate distance queries in (universal) constant time,\noccupies space [{\\epsilon}^{-O({\\lambda})} + 2^{O({\\lambda} log {\\lambda})}]n,\nand can be constructed in [2^{O({\\lambda})} log3 n + {\\epsilon}^{-O({\\lambda})}\n+ 2^{O({\\lambda} log {\\lambda})}]n expected time. This improves upon the best\npreviously known constructions, presented by Har-Peled and Mendel. Furthermore,\nthe oracle can be made fully dynamic with expected O(1) query time and only\n2^{O({\\lambda})} log n + {\\epsilon}^{-O({\\lambda})} + 2^{O({\\lambda} log\n{\\lambda})} update time. This is the first fully dynamic\n(1+{\\epsilon})-distance oracle.\n", "versions": [{"version": "v1", "created": "Mon, 9 Aug 2010 10:21:33 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Bartal", "Yair", ""], ["Gottlieb", "Lee-Ad", ""], ["Kopelowitz", "Tsvi", ""], ["Lewenstein", "Moshe", ""], ["Roditty", "Liam", ""]]}, {"id": "1008.1556", "submitter": "Pradipta Mitra", "authors": "Eyj\\'olfur Ingi \\'Asgeirsson, Pradipta Mitra", "title": "On a game theoretic approach to capacity maximization in wireless\n  networks", "comments": "16 pages, 5 figures (to appear in INFOCOM 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the capacity problem (or, the single slot scheduling problem) in\nwireless networks. Our goal is to maximize the number of successful connections\nin arbitrary wireless networks where a transmission is successful only if the\nsignal-to-interference-plus-noise ratio at the receiver is greater than some\nthreshold. We study a game theoretic approach towards capacity maximization\nintroduced by Andrews and Dinitz (INFOCOM 2009) and Dinitz (INFOCOM 2010). We\nprove vastly improved bounds for the game theoretic algorithm. In doing so, we\nachieve the first distributed constant factor approximation algorithm for\ncapacity maximization for the uniform power assignment. When compared to the\noptimum where links may use an arbitrary power assignment, we prove a $O(\\log\n\\Delta)$ approximation, where $\\Delta$ is the ratio between the largest and the\nsmallest link in the network. This is an exponential improvement of the\napproximation factor compared to existing results for distributed algorithms.\nAll our results work for links located in any metric space. In addition, we\nprovide simulation studies clarifying the picture on distributed algorithms for\ncapacity maximization.\n", "versions": [{"version": "v1", "created": "Mon, 9 Aug 2010 17:49:22 GMT"}, {"version": "v2", "created": "Tue, 10 Aug 2010 19:59:33 GMT"}, {"version": "v3", "created": "Wed, 3 Nov 2010 15:50:35 GMT"}, {"version": "v4", "created": "Sat, 20 Nov 2010 12:13:09 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["\u00c1sgeirsson", "Eyj\u00f3lfur Ingi", ""], ["Mitra", "Pradipta", ""]]}, {"id": "1008.1616", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty and Eyal Even-Dar and Sudipto Guha and Yishay\n  Mansour and S. Muthukrishnan", "title": "Approximation Schemes for Sequential Posted Pricing in Multi-Unit\n  Auctions", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design algorithms for computing approximately revenue-maximizing {\\em\nsequential posted-pricing mechanisms (SPM)} in $K$-unit auctions, in a standard\nBayesian model. A seller has $K$ copies of an item to sell, and there are $n$\nbuyers, each interested in only one copy, who have some value for the item. The\nseller must post a price for each buyer, the buyers arrive in a sequence\nenforced by the seller, and a buyer buys the item if its value exceeds the\nprice posted to it. The seller does not know the values of the buyers, but have\nBayesian information about them. An SPM specifies the ordering of buyers and\nthe posted prices, and may be {\\em adaptive} or {\\em non-adaptive} in its\nbehavior.\n  The goal is to design SPM in polynomial time to maximize expected revenue. We\ncompare against the expected revenue of optimal SPM, and provide a polynomial\ntime approximation scheme (PTAS) for both non-adaptive and adaptive SPMs. This\nis achieved by two algorithms: an efficient algorithm that gives a\n$(1-\\frac{1}{\\sqrt{2\\pi K}})$-approximation (and hence a PTAS for sufficiently\nlarge $K$), and another that is a PTAS for constant $K$. The first algorithm\nyields a non-adaptive SPM that yields its approximation guarantees against an\noptimal adaptive SPM -- this implies that the {\\em adaptivity gap} in SPMs\nvanishes as $K$ becomes larger.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 01:27:48 GMT"}], "update_date": "2010-08-11", "authors_parsed": [["Chakraborty", "Tanmoy", ""], ["Even-Dar", "Eyal", ""], ["Guha", "Sudipto", ""], ["Mansour", "Yishay", ""], ["Muthukrishnan", "S.", ""]]}, {"id": "1008.1687", "submitter": "Daniel \\v{S}tefankovi\\v{c}", "authors": "Daniel Stefankovic and Santosh Vempala and Eric Vigoda", "title": "A Deterministic Polynomial-time Approximation Scheme for Counting\n  Knapsack Solutions", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n elements with nonnegative integer weights w1,..., wn and an integer\ncapacity C, we consider the counting version of the classic knapsack problem:\nfind the number of distinct subsets whose weights add up to at most the given\ncapacity. We give a deterministic algorithm that estimates the number of\nsolutions to within relative error 1+-eps in time polynomial in n and 1/eps\n(fully polynomial approximation scheme). More precisely, our algorithm takes\ntime O(n^3 (1/eps) log (n/eps)). Our algorithm is based on dynamic programming.\nPreviously, randomized polynomial time approximation schemes were known first\nby Morris and Sinclair via Markov chain Monte Carlo techniques, and\nsubsequently by Dyer via dynamic programming and rejection sampling.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 10:54:31 GMT"}], "update_date": "2010-08-11", "authors_parsed": [["Stefankovic", "Daniel", ""], ["Vempala", "Santosh", ""], ["Vigoda", "Eric", ""]]}, {"id": "1008.1715", "submitter": "Daniel Lemire", "authors": "Daniel Lemire", "title": "The universality of iterated hashing over variable-length strings", "comments": null, "journal-ref": "Discrete Applied Mathematics 160 (4-5), 604--617 (2012)", "doi": "10.1016/j.dam.2011.11.009", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterated hash functions process strings recursively, one character at a time.\nAt each iteration, they compute a new hash value from the preceding hash value\nand the next character. We prove that iterated hashing can be pairwise\nindependent, but never 3-wise independent. We show that it can be almost\nuniversal over strings much longer than the number of hash values; we bound the\nmaximal string length given the collision probability.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 14:05:28 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2011 14:59:51 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2011 20:39:11 GMT"}, {"version": "v4", "created": "Tue, 1 Nov 2011 00:01:34 GMT"}, {"version": "v5", "created": "Thu, 24 Nov 2011 15:10:52 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Lemire", "Daniel", ""]]}, {"id": "1008.1827", "submitter": "Maria Florina Balcan", "authors": "Maria-Florina Balcan and Mark Braverman", "title": "Nash Equilibria in Perturbation Resilient Games", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fact that in many game-theoretic settings, the game analyzed\nis only an approximation to the game being played, in this work we analyze\nequilibrium computation for the broad and natural class of bimatrix games that\nare stable to perturbations. We specifically focus on games with the property\nthat small changes in the payoff matrices do not cause the Nash equilibria of\nthe game to fluctuate wildly. For such games we show how one can compute\napproximate Nash equilibria more efficiently than the general result of Lipton\net al. \\cite{LMM03}, by an amount that depends on the degree of stability of\nthe game and that reduces to their bound in the worst case. Furthermore, we\nshow that for stable games the approximate equilibria found will be close in\nvariation distance to true equilibria, and moreover this holds even if we are\ngiven as input only a perturbation of the actual underlying stable game.\n  For uniformly-stable games, where the equilibria fluctuate at most\nquasi-linearly in the extent of the perturbation, we get a particularly\ndramatic improvement. Here, we achieve a fully quasi-polynomial-time\napproximation scheme: that is, we can find $1/\\poly(n)$-approximate equilibria\nin quasi-polynomial time. This is in marked contrast to the general class of\nbimatrix games for which finding such approximate equilibria is PPAD-hard. In\nparticular, under the (widely believed) assumption that PPAD is not contained\nin quasi-polynomial time, our results imply that such uniformly stable games\nare inherently easier for computation of approximate equilibria than general\nbimatrix games.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 03:13:30 GMT"}, {"version": "v2", "created": "Thu, 12 Aug 2010 00:13:46 GMT"}, {"version": "v3", "created": "Mon, 15 Nov 2010 19:38:15 GMT"}, {"version": "v4", "created": "Wed, 29 Feb 2012 03:06:29 GMT"}, {"version": "v5", "created": "Mon, 12 Mar 2012 21:07:39 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Braverman", "Mark", ""]]}, {"id": "1008.1975", "submitter": "Aleksander M{\\ka}dry", "authors": "Aleksander Madry", "title": "Fast Approximation Algorithms for Cut-based Problems in Undirected\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general method of designing fast approximation algorithms for\ncut-based minimization problems in undirected graphs. In particular, we develop\na technique that given any such problem that can be approximated quickly on\ntrees, allows approximating it almost as quickly on general graphs while only\nlosing a poly-logarithmic factor in the approximation guarantee.\n  To illustrate the applicability of our paradigm, we focus our attention on\nthe undirected sparsest cut problem with general demands and the balanced\nseparator problem. By a simple use of our framework, we obtain poly-logarithmic\napproximation algorithms for these problems that run in time close to linear.\n  The main tool behind our result is an efficient procedure that decomposes\ngeneral graphs into simpler ones while approximately preserving the cut-flow\nstructure. This decomposition is inspired by the cut-based graph decomposition\nof R\\\"acke that was developed in the context of oblivious routing schemes, as\nwell as, by the construction of the ultrasparsifiers due to Spielman and Teng\nthat was employed to preconditioning symmetric diagonally-dominant matrices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 19:35:38 GMT"}, {"version": "v2", "created": "Fri, 13 Aug 2010 14:22:26 GMT"}, {"version": "v3", "created": "Sun, 26 Sep 2010 14:36:51 GMT"}, {"version": "v4", "created": "Sat, 6 Nov 2010 03:16:26 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Madry", "Aleksander", ""]]}, {"id": "1008.2136", "submitter": "Christophe Weibel", "authors": "Chandra Chekuri, F. Bruce Shepherd, Christophe Weibel", "title": "Flow-Cut Gaps for Integer and Fractional Multiflows", "comments": "24 pages, 10 figures. Results presented first at the Symposium on\n  Discrete Algorithms (SoDA 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a routing problem consisting of a demand graph H and a supply graph\nG. If the pair obeys the cut condition, then the flow-cut gap for this instance\nis the minimum value C such that there is a feasible multiflow for H if each\nedge of G is given capacity C. The flow-cut gap can be greater than 1 even when\nG is the (series-parallel) graph K_{2,3}. In this paper we are primarily\ninterested in the \"integer\" flow-cut gap. What is the minimum value C such that\nthere is a feasible integer valued multiflow for H if each edge of G is given\ncapacity C? We conjecture that the integer flow-cut gap is quantitatively\nrelated to the fractional flow-cut gap. This strengthens the well-known\nconjecture that the flow-cut gap in planar and minor-free graphs is O(1) to\nsuggest that the integer flow-cut gap is O(1). We give several results on\nnon-trivial special classes of graphs supporting this conjecture and further\nexplore the \"primal\" method for understanding flow-cut gaps. Our results\ninclude:\n  - Let G be obtained by series-parallel operations starting from an edge st,\nand consider orienting all edges in G in the direction from s to t. A demand is\ncompliant if its endpoints are joined by a directed path in the resulting\noriented graph. If the cut condition holds for a compliant instance and G+H is\nEulerian, then an integral routing of H exists.\n  - The integer flow-cut gap in series-parallel graphs is 5. We also give an\nexplicit class of instances that shows via elementary calculations that the\nflow-cut gap in series-parallel graphs is at least 2-o(1); this simplifies the\nproof by Lee and Raghavendra.\n  - The integer flow-cut gap in k-Outerplanar graphs is c^{O(k)} for some fixed\nconstant c.\n  - A simple proof that the flow-cut gap is O(\\log k^*) where k^* is the size\nof a node-cover in H; this was previously shown by G\\\"unl\\\"uk via a more\nintricate proof.\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 15:16:29 GMT"}], "update_date": "2010-08-16", "authors_parsed": [["Chekuri", "Chandra", ""], ["Shepherd", "F. Bruce", ""], ["Weibel", "Christophe", ""]]}, {"id": "1008.2159", "submitter": "Nicholas Harvey", "authors": "Maria-Florina Balcan and Nicholas J. A. Harvey", "title": "Submodular Functions: Learnability, Structure, and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions are discrete functions that model laws of diminishing\nreturns and enjoy numerous algorithmic applications. They have been used in\nmany areas, including combinatorial optimization, machine learning, and\neconomics. In this work we study submodular functions from a learning theoretic\nangle. We provide algorithms for learning submodular functions, as well as\nlower bounds on their learnability. In doing so, we uncover several novel\nstructural results revealing ways in which submodular functions can be both\nsurprisingly structured and surprisingly unstructured. We provide several\nconcrete implications of our work in other domains including algorithmic game\ntheory and combinatorial optimization.\n  At a technical level, this research combines ideas from many areas, including\nlearning theory (distributional learning and PAC-style analyses), combinatorics\nand optimization (matroids and submodular functions), and pseudorandomness\n(lossless expander graphs).\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 16:15:47 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2012 00:43:13 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2012 02:04:42 GMT"}], "update_date": "2012-08-24", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Harvey", "Nicholas J. A.", ""]]}, {"id": "1008.2555", "submitter": "Thomas Conway", "authors": "Thomas C Conway, Andrew J Bromage", "title": "Succinct Data Structures for Assembling Large Genomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Second generation sequencing technology makes it feasible for\nmany researches to obtain enough sequence reads to attempt the de novo assembly\nof higher eukaryotes (including mammals). De novo assembly not only provides a\ntool for understanding wide scale biological variation, but within human\nbio-medicine, it offers a direct way of observing both large scale structural\nvariation and fine scale sequence variation. Unfortunately, improvements in the\ncomputational feasibility for de novo assembly have not matched the\nimprovements in the gathering of sequence data. This is for two reasons: the\ninherent computational complexity of the problem, and the in-practice memory\nrequirements of tools.\n  Results: In this paper we use entropy compressed or succinct data structures\nto create a practical representation of the de Bruijn assembly graph, which\nrequires at least a factor of 10 less storage than the kinds of structures used\nby deployed methods. In particular we show that when stored succinctly, the de\nBruijn assembly graph for homo sapiens requires only 23 gigabytes of storage.\nMoreover, because our representation is entropy compressed, in the presence of\nsequencing errors it has better scaling behaviour asymptotically than\nconventional approaches.\n", "versions": [{"version": "v1", "created": "Sun, 15 Aug 2010 23:07:45 GMT"}], "update_date": "2010-08-17", "authors_parsed": [["Conway", "Thomas C", ""], ["Bromage", "Andrew J", ""]]}, {"id": "1008.2565", "submitter": "Minas Gjoka", "authors": "Minas Gjoka and Carter T. Butts and Maciej Kurant and Athina\n  Markopoulou", "title": "Multigraph Sampling of Online Social Networks", "comments": "IEEE Journal on Selected Areas in Communications (JSAC), Special\n  Issue on Measurement of Internet Topologies, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS cs.SI physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art techniques for probability sampling of users of online\nsocial networks (OSNs) are based on random walks on a single social relation\n(typically friendship). While powerful, these methods rely on the social graph\nbeing fully connected. Furthermore, the mixing time of the sampling process\nstrongly depends on the characteristics of this graph. In this paper, we\nobserve that there often exist other relations between OSN users, such as\nmembership in the same group or participation in the same event. We propose to\nexploit the graphs these relations induce, by performing a random walk on their\nunion multigraph. We design a computationally efficient way to perform\nmultigraph sampling by randomly selecting the graph on which to walk at each\niteration. We demonstrate the benefits of our approach through (i) simulation\nin synthetic graphs, and (ii) measurements of Last.fm - an Internet website for\nmusic with social networking features. More specifically, we show that\nmultigraph sampling can obtain a representative sample and faster convergence,\neven when the individual graphs fail, i.e., are disconnected or highly\nclustered.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 02:01:02 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2011 01:58:00 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Gjoka", "Minas", ""], ["Butts", "Carter T.", ""], ["Kurant", "Maciej", ""], ["Markopoulou", "Athina", ""]]}, {"id": "1008.2814", "submitter": "Brendan Ames", "authors": "Brendan P.W. Ames and Stephen A. Vavasis", "title": "Convex optimization for the planted k-disjoint-clique problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the k-disjoint-clique problem. The input is an undirected graph G\nin which the nodes represent data items, and edges indicate a similarity\nbetween the corresponding items. The problem is to find within the graph k\ndisjoint cliques that cover the maximum number of nodes of G. This problem may\nbe understood as a general way to pose the classical `clustering' problem. In\nclustering, one is given data items and a distance function, and one wishes to\npartition the data into disjoint clusters of data items, such that the items in\neach cluster are close to each other. Our formulation additionally allows\n`noise' nodes to be present in the input data that are not part of any of the\ncliques. The k-disjoint-clique problem is NP-hard, but we show that a convex\nrelaxation can solve it in polynomial time for input instances constructed in a\ncertain way. The input instances for which our algorithm finds the optimal\nsolution consist of k disjoint large cliques (called `planted cliques') that\nare then obscured by noise edges and noise nodes inserted either at random or\nby an adversary.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 04:07:22 GMT"}, {"version": "v2", "created": "Wed, 16 Feb 2011 18:16:28 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2013 19:03:32 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2013 19:50:03 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Ames", "Brendan P. W.", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1008.2849", "submitter": "Jan Wassenberg", "authors": "Jan Wassenberg and Peter Sanders", "title": "Faster Radix Sort via Virtual Memory and Write-Combining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting algorithms are the deciding factor for the performance of common\noperations such as removal of duplicates or database sort-merge joins. This\nwork focuses on 32-bit integer keys, optionally paired with a 32-bit value. We\npresent a fast radix sorting algorithm that builds upon a\nmicroarchitecture-aware variant of counting sort. Taking advantage of virtual\nmemory and making use of write-combining yields a per-pass throughput\ncorresponding to at least 88 % of the system's peak memory bandwidth. Our\nimplementation outperforms Intel's recently published radix sort by a factor of\n1.5. It also compares favorably to the reported performance of an algorithm for\nFermi GPUs when data-transfer overhead is included. These results indicate that\nscalar, bandwidth-sensitive sorting algorithms remain competitive on current\narchitectures. Various other memory-intensive applications can benefit from the\ntechniques described herein.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 08:39:59 GMT"}, {"version": "v2", "created": "Mon, 6 Sep 2010 10:41:11 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Wassenberg", "Jan", ""], ["Sanders", "Peter", ""]]}, {"id": "1008.2909", "submitter": "Bjoern Andres", "authors": "Bjoern Andres, Ullrich Koethe, Thorben Kroeger, and Fred A. Hamprecht", "title": "Runtime-Flexible Multi-dimensional Arrays and Views for C++98 and C++0x", "comments": "Free source code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional arrays are among the most fundamental and most useful data\nstructures of all. In C++, excellent template libraries exist for arrays whose\ndimension is fixed at runtime. Arrays whose dimension can change at runtime\nhave been implemented in C. However, a generic object-oriented C++\nimplementation of runtime-flexible arrays has so far been missing. In this\narticle, we discuss our new implementation called Marray, a package of class\ntemplates that fills this gap. Marray is based on views as an underlying\nconcept. This concept brings some of the flexibility known from script\nlanguages such as R and MATLAB to C++. Marray is free both for commercial and\nnon-commercial use and is publicly available from www.andres.sc/marray\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 14:50:56 GMT"}], "update_date": "2010-08-18", "authors_parsed": [["Andres", "Bjoern", ""], ["Koethe", "Ullrich", ""], ["Kroeger", "Thorben", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1008.2928", "submitter": "Gwena\\\"el Joret", "authors": "Jean Cardinal and Samuel Fiorini and Gwena\\\"el Joret", "title": "Minimum Entropy Combinatorial Optimization Problems", "comments": null, "journal-ref": "Theory of Computing Systems, 51/1:4--21, 2012", "doi": "10.1007/s00224-011-9371-2", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey recent results on combinatorial optimization problems in which the\nobjective function is the entropy of a discrete distribution. These include the\nminimum entropy set cover, minimum entropy orientation, and minimum entropy\ncoloring problems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 16:14:09 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Cardinal", "Jean", ""], ["Fiorini", "Samuel", ""], ["Joret", "Gwena\u00ebl", ""]]}, {"id": "1008.3091", "submitter": "Eric Thierry", "authors": "Aurelie Lagoutte", "title": "2-FREE-FLOOD-IT is polynomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a discrete diffusion process introduced in some combinatorial games\ncalled FLOODIT and MADVIRUS that can be played online and whose computational\ncomplexity has been recently studied by Arthur et al (FUN'2010). The flooding\ndynamics used in those games can be defined for any colored graph. It has been\nshown in a first report (in french, hal-00509488 on HAL archive) that studying\nthis dynamics directly on general graph is a valuable approach to understand\nits specificities and extract uncluttered key patterns or algorithms that can\nbe applied with success to particular cases like the square grid of FLOODIT or\nthe hexagonal grid of MADVIRUS, and many other classes of graphs. This report\nis the translation from french to english of the section in the french report\nshowing that the variant of the problem called 2-FREE-FLOOD-IT can be solved\nwith a polynomial algorithm, answering a question raised in the previous study\nof FLOODIT by Arthur et al.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 13:56:12 GMT"}], "update_date": "2010-08-19", "authors_parsed": [["Lagoutte", "Aurelie", ""]]}, {"id": "1008.3187", "submitter": "Raghu Meka", "authors": "Parikshit Gopalan, Adam Klivans, Raghu Meka", "title": "Polynomial-Time Approximation Schemes for Knapsack and Related Counting\n  Problems using Branching Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic, polynomial-time algorithm for approximately counting\nthe number of {0,1}-solutions to any instance of the knapsack problem. On an\ninstance of length n with total weight W and accuracy parameter eps, our\nalgorithm produces a (1 + eps)-multiplicative approximation in time poly(n,log\nW,1/eps). We also give algorithms with identical guarantees for general integer\nknapsack, the multidimensional knapsack problem (with a constant number of\nconstraints) and for contingency tables (with a constant number of rows).\nPreviously, only randomized approximation schemes were known for these problems\ndue to work by Morris and Sinclair and work by Dyer.\n  Our algorithms work by constructing small-width, read-once branching programs\nfor approximating the underlying solution space under a carefully chosen\ndistribution. As a byproduct of this approach, we obtain new query algorithms\nfor learning functions of k halfspaces with respect to the uniform distribution\non {0,1}^n. The running time of our algorithm is polynomial in the accuracy\nparameter eps. Previously even for the case of k=2, only algorithms with an\nexponential dependence on eps were known.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 23:45:28 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1008.3190", "submitter": "David Wood", "authors": "David R. Wood", "title": "Partitions and Coverings of Trees by Bounded-Degree Subtrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the following questions for a given tree $T$ and integer\n$d\\geq2$: (1) What is the minimum number of degree-$d$ subtrees that partition\n$E(T)$? (2) What is the minimum number of degree-$d$ subtrees that cover\n$E(T)$? We answer the first question by providing an explicit formula for the\nminimum number of subtrees, and we describe a linear time algorithm that finds\nthe corresponding partition. For the second question, we present a polynomial\ntime algorithm that computes a minimum covering. We then establish a tight\nbound on the number of subtrees in coverings of trees with given maximum degree\nand pathwidth. Our results show that pathwidth is the right parameter to\nconsider when studying coverings of trees by degree-3 subtrees. We briefly\nconsider coverings of general graphs by connected subgraphs of bounded degree.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 00:46:21 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["Wood", "David R.", ""]]}, {"id": "1008.3193", "submitter": "David Wood", "authors": "Ferran Hurtado, Giuseppe Liotta, David R. Wood", "title": "Proximity Drawings of High-Degree Trees", "comments": null, "journal-ref": "International J. of Computational Geometry and Applications\n  23:213-230, 2013", "doi": "10.1142/S0218195913500088", "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A drawing of a given (abstract) tree that is a minimum spanning tree of the\nvertex set is considered aesthetically pleasing. However, such a drawing can\nonly exist if the tree has maximum degree at most 6. What can be said for trees\nof higher degree? We approach this question by supposing that a partition or\ncovering of the tree by subtrees of bounded degree is given. Then we show that\nif the partition or covering satisfies some natural properties, then there is a\ndrawing of the entire tree such that each of the given subtrees is drawn as a\nminimum spanning tree of its vertex set.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 01:21:07 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Hurtado", "Ferran", ""], ["Liotta", "Giuseppe", ""], ["Wood", "David R.", ""]]}, {"id": "1008.3216", "submitter": "Pooya Hatami", "authors": "Pooya Hatami", "title": "An approximation algorithm for the total cover problem", "comments": "5 pages, 1 figure", "journal-ref": "Discussiones Mathematicae Graph Theory, Vol. 27, No.3 (2007) pp.\n  553-560", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a $2$-approximation algorithm for the minimum total covering\nnumber problem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 04:51:15 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["Hatami", "Pooya", ""]]}, {"id": "1008.3305", "submitter": "EPTCS", "authors": "Nicoletta De Francesco (Universit\\`a di Pisa), Giuseppe Lettieri\n  (Universit\\`a di Pisa), Luca Martini (Universit\\`a di Pisa)", "title": "Celer: an Efficient Program for Genotype Elimination", "comments": "In Proceedings AMCA-POP 2010, arXiv:1008.3147", "journal-ref": "EPTCS 33, 2010, pp. 56-70", "doi": "10.4204/EPTCS.33.4", "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient program for checking Mendelian consistency\nin a pedigree. Since pedigrees may contain incomplete and/or erroneous\ninformation, geneticists need to pre-process them before performing linkage\nanalysis. Removing superfluous genotypes that do not respect the Mendelian\ninheritance laws can speed up the linkage analysis. We have described in a\nformal way the Mendelian consistency problem and algorithms known in\nliterature. The formalization helped to polish the algorithms and to find\nefficient data structures. The performance of the tool has been tested on a\nwide range of benchmarks. The results are promising if compared to other\nprograms that treat Mendelian consistency.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 14:01:01 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["De Francesco", "Nicoletta", "", "Universit\u00e0 di Pisa"], ["Lettieri", "Giuseppe", "", "Universit\u00e0 di Pisa"], ["Martini", "Luca", "", "Universit\u00e0 di Pisa"]]}, {"id": "1008.3310", "submitter": "Ragnar Freij", "authors": "Ragnar Freij and Johan W\\\"astlund", "title": "Partially ordered secretaries", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elements of a finite nonempty partially ordered set are exposed at\nindependent uniform times in $[0,1]$ to a selector who, at any given time, can\nsee the structure of the induced partial order on the exposed elements. The\nselector's task is to choose online a maximal element. This generalizes the\nclassical linear order secretary problem, for which it is known that the\nselector can succeed with probability $1/e$ and that this is best possible. We\ndescribe a strategy for the general problem that achieves success probability\n$1/e$ for an arbitrary partial order.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 14:32:09 GMT"}, {"version": "v2", "created": "Wed, 6 Oct 2010 07:48:10 GMT"}], "update_date": "2010-10-07", "authors_parsed": [["Freij", "Ragnar", ""], ["W\u00e4stlund", "Johan", ""]]}, {"id": "1008.3503", "submitter": "Martin Fink", "authors": "Martin Fink and Joachim Spoerhase", "title": "Maximum Betweenness Centrality: Approximability and Tractable Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Betweenness Centrality problem (MBC) can be defined as follows.\nGiven a graph find a $k$-element node set $C$ that maximizes the probability of\ndetecting communication between a pair of nodes $s$ and $t$ chosen uniformly at\nrandom. It is assumed that the communication between $s$ and $t$ is realized\nalong a shortest $s$--$t$ path which is, again, selected uniformly at random.\nThe communication is detected if the communication path contains a node of $C$.\nRecently, Dolev et al. (2009) showed that MBC is NP-hard and gave a\n$(1-1/e)$-approximation using a greedy approach. We provide a reduction of MBC\nto Maximum Coverage that simplifies the analysis of the algorithm of Dolev et\nal. considerably. Our reduction allows us to obtain a new algorithm with the\nsame approximation ratio for a (generalized) budgeted version of MBC. We\nprovide tight examples showing that the analyses of both algorithms are best\npossible. Moreover, we prove that MBC is APX-complete and provide an exact\npolynomial-time algorithm for MBC on tree graphs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 13:52:33 GMT"}], "update_date": "2010-08-23", "authors_parsed": [["Fink", "Martin", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1008.3546", "submitter": "Ioannis Paparrizos", "authors": "Ioannis Paparrizos", "title": "Homogeneous and Non Homogeneous Algorithms", "comments": "This (full) paper was accepted for publication in the 13th\n  Panhellenic Conference on Informatics (PCI 2009), 10-12 September 2009,\n  Corfu, Greece (8 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent best case analyses for some sorting algorithms and based\non the type of complexity we partition the algorithms into two classes:\nhomogeneous and non homogeneous algorithms. Although both classes contain\nalgorithms with worst and best cases, homogeneous algorithms behave uniformly\non all instances. This partition clarifies in a completely mathematical way the\npreviously mentioned terms and reveals that in classifying an algorithm as\nhomogeneous or not best case analysis is equally important with worst case\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 17:58:00 GMT"}], "update_date": "2010-08-23", "authors_parsed": [["Paparrizos", "Ioannis", ""]]}, {"id": "1008.3594", "submitter": "James Lee", "authors": "Jonathan A. Kelner, James R. Lee, Gregory N. Price, Shang-Hua Teng", "title": "Metric uniformization and spectral bounds for graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for proving upper bounds on the eigenvalues of the graph\nLaplacian. A main step involves choosing an appropriate \"Riemannian\" metric to\nuniformize the geometry of the graph. In many interesting cases, the existence\nof such a metric is shown by examining the combinatorics of special types of\nflows. This involves proving new inequalities on the crossing number of graphs.\n  In particular, we use our method to show that for any positive integer k, the\nkth smallest eigenvalue of the Laplacian on an n-vertex, bounded-degree planar\ngraph is O(k/n). This bound is asymptotically tight for every k, as it is\neasily seen to be achieved for square planar grids. We also extend this\nspectral result to graphs with bounded genus, and graphs which forbid fixed\nminors. Previously, such spectral upper bounds were only known for the case\nk=2.\n", "versions": [{"version": "v1", "created": "Sat, 21 Aug 2010 02:26:10 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2011 06:13:59 GMT"}, {"version": "v3", "created": "Mon, 25 Jul 2011 07:40:23 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Kelner", "Jonathan A.", ""], ["Lee", "James R.", ""], ["Price", "Gregory N.", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1008.3672", "submitter": "Michael Kapralov", "authors": "Michael Kapralov and Rina Panigrahy", "title": "Prediction strategies without loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a sequence of bits where we are trying to predict the next bit from\nthe previous bits. Assume we are allowed to say 'predict 0' or 'predict 1', and\nour payoff is +1 if the prediction is correct and -1 otherwise. We will say\nthat at each point in time the loss of an algorithm is the number of wrong\npredictions minus the number of right predictions so far. In this paper we are\ninterested in algorithms that have essentially zero (expected) loss over any\nstring at any point in time and yet have small regret with respect to always\npredicting 0 or always predicting 1. For a sequence of length $T$ our algorithm\nhas regret $14\\epsilon T $ and loss $2\\sqrt{T}e^{-\\epsilon^2 T} $ in\nexpectation for all strings. We show that the tradeoff between loss and regret\nis optimal up to constant factors.\n  Our techniques extend to the general setting of $N$ experts, where the\nrelated problem of trading off regret to the best expert for regret to the\n`special' expert has been studied by Even-Dar et al. (COLT'07). We obtain\nessentially zero loss with respect to the special expert and optimal\nloss/regret tradeoff, improving upon the results of Even-Dar et al and settling\nthe main question left open in their paper.\n  The strong loss bounds of the algorithm have some surprising consequences. A\nsimple iterative application of our algorithm gives essentially optimal regret\nbounds at multiple time scales, bounds with respect to $k$-shifting optima as\nwell as regret bounds with respect to higher norms of the input sequence.\n", "versions": [{"version": "v1", "created": "Sun, 22 Aug 2010 01:09:56 GMT"}, {"version": "v2", "created": "Tue, 24 Aug 2010 17:41:52 GMT"}, {"version": "v3", "created": "Tue, 16 Nov 2010 02:12:05 GMT"}, {"version": "v4", "created": "Sat, 6 Oct 2012 22:39:51 GMT"}, {"version": "v5", "created": "Wed, 10 Oct 2012 18:16:48 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Kapralov", "Michael", ""], ["Panigrahy", "Rina", ""]]}, {"id": "1008.3773", "submitter": "Ernst Althaus", "authors": "Ernst Althaus and Peter Hachenberger", "title": "Fully Automatic Trunk Packing with Free Placements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm to compute the volume of a trunk according to the\nSAE J1100 standard. Our new algorithm uses state-of-the-art methods from\ncomputational geometry and from combinatorial optimization. It finds better\nsolutions than previous approaches for small trunks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Aug 2010 08:54:34 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Althaus", "Ernst", ""], ["Hachenberger", "Peter", ""]]}, {"id": "1008.3786", "submitter": "Mathieu Raffinot", "authors": "Mathieu Raffinot", "title": "Consecutive ones property testing: cut or swap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let C be a finite set of $N elements and R = {R_1,R_2, ..,R_m} a family of M\nsubsets of C. The family R verifies the consecutive ones property if there\nexists a permutation P of C such that each R_i in R is an interval of P. There\nalready exist several algorithms to test this property in sum_{i=1}^m |R_i|\ntime, all being involved. We present a simpler algorithm, based on a new\npartitioning scheme.\n", "versions": [{"version": "v1", "created": "Mon, 23 Aug 2010 10:09:34 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Raffinot", "Mathieu", ""]]}, {"id": "1008.3938", "submitter": "C. Seshadhri", "authors": "Satyen Kale and C. Seshadhri", "title": "Combinatorial Approximation Algorithms for MaxCut using Random Walks", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first combinatorial approximation algorithm for Maxcut that beats\nthe trivial 0.5 factor by a constant. The main partitioning procedure is very\nintuitive, natural, and easily described. It essentially performs a number of\nrandom walks and aggregates the information to provide the partition. We can\ncontrol the running time to get an approximation factor-running time tradeoff.\nWe show that for any constant b > 1.5, there is an O(n^{b}) algorithm that\noutputs a (0.5+delta)-approximation for Maxcut, where delta = delta(b) is some\npositive constant.\n  One of the components of our algorithm is a weak local graph partitioning\nprocedure that may be of independent interest. Given a starting vertex $i$ and\na conductance parameter phi, unless a random walk of length ell = O(log n)\nstarting from i mixes rapidly (in terms of phi and ell), we can find a cut of\nconductance at most phi close to the vertex. The work done per vertex found in\nthe cut is sublinear in n.\n", "versions": [{"version": "v1", "created": "Mon, 23 Aug 2010 23:51:46 GMT"}], "update_date": "2010-08-25", "authors_parsed": [["Kale", "Satyen", ""], ["Seshadhri", "C.", ""]]}, {"id": "1008.4067", "submitter": "Dominik Scheder", "authors": "Robin A. Moser and Dominik Scheder", "title": "A Full Derandomization of Schoening's k-SAT Algorithm", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schoening in 1999 presented a simple randomized algorithm for k-SAT with\nrunning time O(a^n * poly(n)) for a = 2(k-1)/k. We give a deterministic version\nof this algorithm running in time O((a+epsilon)^n * poly(n)), where epsilon > 0\ncan be made arbitrarily small.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 15:29:09 GMT"}], "update_date": "2010-08-25", "authors_parsed": [["Moser", "Robin A.", ""], ["Scheder", "Dominik", ""]]}, {"id": "1008.4071", "submitter": "Stanislav Zivny", "authors": "Martin C. Cooper, Stanislav Zivny", "title": "Hybrid tractability of soft constraint problems", "comments": "A full version of a CP'10 paper, 26 pages", "journal-ref": "Artificial Intelligence 175(9-10) 1555-1569 (2011)", "doi": "10.1016/j.artint.2011.02.003", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) is a central generic problem in\ncomputer science and artificial intelligence: it provides a common framework\nfor many theoretical problems as well as for many real-life applications. Soft\nconstraint problems are a generalisation of the CSP which allow the user to\nmodel optimisation problems. Considerable effort has been made in identifying\nproperties which ensure tractability in such problems. In this work, we\ninitiate the study of hybrid tractability of soft constraint problems; that is,\nproperties which guarantee tractability of the given soft constraint problem,\nbut which do not depend only on the underlying structure of the instance (such\nas being tree-structured) or only on the types of soft constraints in the\ninstance (such as submodularity). We present several novel hybrid classes of\nsoft constraint problems, which include a machine scheduling problem,\nconstraint problems of arbitrary arities with no overlapping nogoods, and the\nSoftAllDiff constraint with arbitrary unary soft constraints. An important tool\nin our investigation will be the notion of forbidden substructures.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 15:42:21 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Cooper", "Martin C.", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1008.4250", "submitter": "Yixin Cao", "authors": "Yixin Cao and Jianer Chen", "title": "Cluster Editing: Kernelization based on Edge Cuts", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization algorithms for the {\\sc cluster editing} problem have been a\npopular topic in the recent research in parameterized computation. Thus far\nmost kernelization algorithms for this problem are based on the concept of {\\it\ncritical cliques}. In this paper, we present new observations and new\ntechniques for the study of kernelization algorithms for the {\\sc cluster\nediting} problem. Our techniques are based on the study of the relationship\nbetween {\\sc cluster editing} and graph edge-cuts. As an application, we\npresent an ${\\cal O}(n^2)$-time algorithm that constructs a $2k$ kernel for the\n{\\it weighted} version of the {\\sc cluster editing} problem. Our result meets\nthe best kernel size for the unweighted version for the {\\sc cluster editing}\nproblem, and significantly improves the previous best kernel of quadratic size\nfor the weighted version of the problem.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 10:44:39 GMT"}, {"version": "v2", "created": "Fri, 3 Sep 2010 10:16:50 GMT"}, {"version": "v3", "created": "Fri, 10 Sep 2010 21:24:35 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Cao", "Yixin", ""], ["Chen", "Jianer", ""]]}, {"id": "1008.4420", "submitter": "Sandor P. Fekete", "authors": "Sandor P. Fekete, Chris Gray, Alexander Kroeller", "title": "Evacuation of rectilinear polygons", "comments": "15 pages, 7 figures; to appear in COCOA 2010", "journal-ref": null, "doi": "10.1007/978-3-642-17458-2_3", "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of creating fast evacuation plans for buildings\nthat are modeled as grid polygons, possibly containing exponentially many\ncells. We study this problem in two contexts: the ``confluent'' context in\nwhich the routes to exits remain fixed over time, and the ``non-confluent''\ncontext in which routes may change. Confluent evacuation plans are simpler to\ncarry out, as they allocate contiguous regions to exits; non-confluent\nallocation can possibly create faster evacuation plans. We give results on the\nhardness of creating the evacuation plans and strongly polynomial algorithms\nfor finding confluent evacuation plans when the building has two exits. We also\ngive a pseudo-polynomial time algorithm for non-confluent evacuation plans.\nFinally, we show that the worst-case bound between confluent and non-confluent\nplans is 2-2/(k+1).\n", "versions": [{"version": "v1", "created": "Thu, 26 Aug 2010 03:07:28 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Fekete", "Sandor P.", ""], ["Gray", "Chris", ""], ["Kroeller", "Alexander", ""]]}, {"id": "1008.4889", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal and Kirk Pruhs", "title": "The Geometry of Scheduling", "comments": "Conference version in FOCS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following general scheduling problem: The input consists of n\njobs, each with an arbitrary release time, size, and a monotone function\nspecifying the cost incurred when the job is completed at a particular time.\nThe objective is to find a preemptive schedule of minimum aggregate cost. This\nproblem formulation is general enough to include many natural scheduling\nobjectives, such as weighted flow, weighted tardiness, and sum of flow squared.\nOur main result is a randomized polynomial-time algorithm with an approximation\nratio O(log log nP), where P is the maximum job size. We also give an O(1)\napproximation in the special case when all jobs have identical release times.\nThe main idea is to reduce this scheduling problem to a particular geometric\nset-cover problem which is then solved using the local ratio technique and\nVaradarajan's quasi-uniform sampling technique. This general algorithmic\napproach improves the best known approximation ratios by at least an\nexponential factor (and much more in some cases) for essentially all of the\nnontrivial common special cases of this problem. Our geometric interpretation\nof scheduling may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 28 Aug 2010 20:00:24 GMT"}], "update_date": "2010-08-31", "authors_parsed": [["Bansal", "Nikhil", ""], ["Pruhs", "Kirk", ""]]}, {"id": "1008.4966", "submitter": "Christian Wulff-Nilsen", "authors": "Glencora Borradaile and Christian Wulff-Nilsen", "title": "Multiple source, single sink maximum flow in a planar graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(n^{1.5}\\log n)$ time algorithm for finding the maximum flow in\na directed planar graph with multiple sources and a single sink. The techniques\ngeneralize to a subquadratic time algorithm for bounded genus graphs.\n", "versions": [{"version": "v1", "created": "Sun, 29 Aug 2010 22:25:00 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Borradaile", "Glencora", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1008.5105", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "Indexability, concentration, and VC theory", "comments": "17 pages, final submission to J. Discrete Algorithms (an expanded,\n  improved and corrected version of the SISAP'2010 invited paper, this e-print,\n  v3)", "journal-ref": "J. Discrete Algorithms 13 (2012), pp. 2-18", "doi": "10.1016/j.jda.2011.10.002", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degrading performance of indexing schemes for exact similarity search in high\ndimensions has long since been linked to histograms of distributions of\ndistances and other 1-Lipschitz functions getting concentrated. We discuss this\nobservation in the framework of the phenomenon of concentration of measure on\nthe structures of high dimension and the Vapnik-Chervonenkis theory of\nstatistical learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 16:09:24 GMT"}, {"version": "v2", "created": "Tue, 31 Aug 2010 17:58:42 GMT"}, {"version": "v3", "created": "Wed, 1 Sep 2010 11:15:00 GMT"}, {"version": "v4", "created": "Mon, 10 Jan 2011 01:08:44 GMT"}, {"version": "v5", "created": "Sat, 21 May 2011 20:48:26 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1008.5332", "submitter": "Shay Mozes", "authors": "Philip N. Klein and Shay Mozes", "title": "Multiple-source single-sink maximum flow in directed planar graphs in\n  $O(n^{1.5} \\log n)$ time", "comments": "13 pages, 2 figures. Corrected spelling in one citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(n^{1.5} \\log n)$ algorithm that, given a directed planar graph\nwith arc capacities, a set of source nodes and a single sink node, finds a\nmaximum flow from the sources to the sink . This is the first subquadratic-time\nstrongly polynomial algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 14:47:35 GMT"}, {"version": "v2", "created": "Tue, 14 Sep 2010 14:42:26 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Klein", "Philip N.", ""], ["Mozes", "Shay", ""]]}, {"id": "1008.5356", "submitter": "Viswanath Nagarajan", "authors": "Nikhil Bansal, Anupam Gupta, Jian Li, Julian Mestre, Viswanath\n  Nagarajan, Atri Rudra", "title": "When LP is the Cure for Your Matching Woes: Improved Bounds for\n  Stochastic Matchings", "comments": "26 pages, preliminary version appears in ESA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random graph model where each possible edge $e$ is present\nindependently with some probability $p_e$. Given these probabilities, we want\nto build a large/heavy matching in the randomly generated graph. However, the\nonly way we can find out whether an edge is present or not is to query it, and\nif the edge is indeed present in the graph, we are forced to add it to our\nmatching. Further, each vertex $i$ is allowed to be queried at most $t_i$\ntimes. How should we adaptively query the edges to maximize the expected weight\nof the matching? We consider several matching problems in this general\nframework (some of which arise in kidney exchanges and online dating, and\nothers arise in modeling online advertisements); we give LP-rounding based\nconstant-factor approximation algorithms for these problems. Our main results\nare the following:\n  We give a 4 approximation for weighted stochastic matching on general graphs,\nand a 3 approximation on bipartite graphs. This answers an open question from\n[Chen etal ICALP 09]. Combining our LP-rounding algorithm with the natural\ngreedy algorithm, we give an improved 3.46 approximation for unweighted\nstochastic matching on general graphs.\n  We introduce a generalization of the stochastic online matching problem\n[Feldman etal FOCS 09] that also models preference-uncertainty and timeouts of\nbuyers, and give a constant factor approximation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 16:36:56 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Bansal", "Nikhil", ""], ["Gupta", "Anupam", ""], ["Li", "Jian", ""], ["Mestre", "Julian", ""], ["Nagarajan", "Viswanath", ""], ["Rudra", "Atri", ""]]}]