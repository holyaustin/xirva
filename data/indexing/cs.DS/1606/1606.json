[{"id": "1606.00001", "submitter": "Tom Portegys", "authors": "Thomas E. Portegys", "title": "Graph isomorphism testing boosted by path coloring", "comments": "arXiv admin note: text overlap with arXiv:1512.07263", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for improving the efficiency of graph isomorphism testing is\npresented. The method uses the structure of the graph colored by vertex hash\ncodes as a means of partitioning vertices into equivalence classes, which in\nturn reduces the combinatorial burden of isomorphism testing. Unrolling the\ngraph into a tree at each vertex allows structurally different regular graphs\nto be discriminated, a capability that the color refinement algorithm cannot\ndo.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 21:04:19 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Portegys", "Thomas E.", ""]]}, {"id": "1606.00117", "submitter": "John Dickerson", "authors": "Benjamin Plaut, John P. Dickerson, Tuomas Sandholm", "title": "Hardness of the Pricing Problem for Chains in Barter Exchanges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney exchange is a barter market where patients trade willing but medically\nincompatible donors. These trades occur via cycles, where each patient-donor\npair both gives and receives a kidney, and via chains, which begin with an\naltruistic donor who does not require a kidney in return. For logistical\nreasons, the maximum length of a cycle is typically limited to a small\nconstant, while chains can be much longer. Given a compatibility graph of\npatient-donor pairs, altruists, and feasible potential transplants between\nthem, finding even a maximum-cardinality set of vertex-disjoint cycles and\nchains is NP-hard. There has been much work on developing provably optimal\nsolvers that are efficient in practice. One of the leading techniques has been\nbranch and price, where column generation is used to incrementally bring cycles\nand chains into the optimization model on an as-needed basis. In particular,\nonly positive-price columns need to be brought into the model. We prove that\nfinding a positive-price chain is NP-complete. This shows incorrectness of two\nleading branch-and-price solvers that suggested polynomial-time chain pricing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 04:49:08 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Plaut", "Benjamin", ""], ["Dickerson", "John P.", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1606.00200", "submitter": "Yikai Zhang", "authors": "Yikai Zhang, Jeffrey Xu Yu, Ying Zhang, Lu Qin", "title": "A Fast Order-Based Approach for Core Maintenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have been widely used in many applications such as social networks,\ncollaboration networks, and biological networks. One important graph analytics\nis to explore cohesive subgraphs in a large graph. Among several cohesive\nsubgraphs studied, k-core is one that can be computed in linear time for a\nstatic graph. Since graphs are evolving in real applications, in this paper, we\nstudy core maintenance which is to reduce the computational cost to compute\nk-cores for a graph when graphs are updated from time to time dynamically. We\nidentify drawbacks of the existing efficient algorithm, which needs a large\nsearch space to find the vertices that need to be updated, and has high\noverhead to maintain the index built, when a graph is updated. We propose a new\norder-based approach to maintain an order, called k-order, among vertices,\nwhile a graph is updated. Our new algorithm can significantly outperform the\nstate-of-the-art algorithm up to 3 orders of magnitude for the 11 large real\ngraphs tested. We report our findings in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:59:17 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 17:53:39 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Zhang", "Yikai", ""], ["Yu", "Jeffrey Xu", ""], ["Zhang", "Ying", ""], ["Qin", "Lu", ""]]}, {"id": "1606.00484", "submitter": "Andrei Alexandrescu", "authors": "Andrei Alexandrescu", "title": "Fast Deterministic Selection", "comments": "Pre-publication draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Median of Medians (also known as BFPRT) algorithm, although a landmark\ntheoretical achievement, is seldom used in practice because it and its variants\nare slower than simple approaches based on sampling. The main contribution of\nthis paper is a fast linear-time deterministic selection algorithm\nQuickselectAdaptive based on a refined definition of MedianOfMedians. The\nalgorithm's performance brings deterministic selection---along with its\ndesirable properties of reproducible runs, predictable run times, and immunity\nto pathological inputs---in the range of practicality. We demonstrate results\non independent and identically distributed random inputs and on\nnormally-distributed inputs. Measurements show that QuickselectAdaptive is\nfaster than state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 22:14:06 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 19:43:46 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Alexandrescu", "Andrei", ""]]}, {"id": "1606.00581", "submitter": "Rahul Vaze", "authors": "Rahul Vaze, Marceau Coupechoux", "title": "Online Budgeted Truthful Matching", "comments": "To appear in NetEcon 2016 and Performance Evaluation Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An online truthful budgeted matching problem is considered for a bipartite\ngraph, where the right vertices are available ahead of time, and individual\nleft vertices arrive sequentially. On arrival of a left vertex, its edge\nutilities (or weights) to all the right vertices and a corresponding cost (or\nbid) are revealed. If a left vertex is matched to any of the right vertices,\nthen it has to be paid at least as much as its cost. The problem is to match\neach left vertex instantaneously and irrevocably to any one of the right\nvertices, if at all, to find the maximum weight matching that is truthful,\nunder a payment budget constraint. Truthfulness condition requires that no left\nvertex has any incentive of misreporting its cost. Assuming that the vertices\narrive in an uniformly random order (secretary model) with arbitrary utilities,\na truthful algorithm is proposed that is $24\\beta$-competitive (where $\\beta$\nis the ratio of the maximum and the minimum utility) and satisfies the payment\nbudget constraint. Direct applications of this problem include crowdsourcing\nauctions, and matching wireless users to cooperative relays in device-to-device\nenabled cellular network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:32:08 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Vaze", "Rahul", ""], ["Coupechoux", "Marceau", ""]]}, {"id": "1606.00726", "submitter": "Michael Otte", "authors": "Michael Otte", "title": "On Solving Floating Point SSSP Using an Integer Priority Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the single source shortest path planning problem (SSSP) in the\ncase of floating point edge weights. We show how any integer based Dijkstra\nsolution that relies on a monotone integer priority queue to create a full\nordering over path lengths in order to solve integer SSSP can be used as an\noracle to solve floating point SSSP with positive edge weights (floating point\nP-SSSP). Floating point P-SSSP is of particular interest to the robotics\ncommunity. This immediately yields a handful of faster runtimes for floating\npoint P-SSSP; for example, ${O({m + n\\log \\log \\frac{C}{\\delta}})}$, where $C$\nis the largest weight and $\\delta$ is the minimum edge weight in the graph. It\nalso ensures that many future advances for integer SSSP will be transferable to\nfloating point P-SSSP.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 15:43:29 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Otte", "Michael", ""]]}, {"id": "1606.00757", "submitter": "Jelani Nelson", "authors": "Tom Morgan, Jelani Nelson", "title": "A note on reductions between compressed sensing guarantees", "comments": "v2: main theorem strengthened to include larger range of p,q,r,s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, one wishes to acquire an approximately sparse\nhigh-dimensional signal $x\\in\\mathbb{R}^n$ via $m\\ll n$ noisy linear\nmeasurements, then later approximately recover $x$ given only those measurement\noutcomes. Various guarantees have been studied in terms of the notion of\napproximation in recovery, and some isolated folklore results are known stating\nthat some forms of recovery are stronger than others, via black-box reductions.\nIn this note we provide a general theorem concerning the hierarchy of strengths\nof various recovery guarantees. As a corollary of this theorem, by reducing\nfrom well-known results in the compressed sensing literature, we obtain an\nefficient $\\ell_p/\\ell_p$ scheme for any $0<p<1$ with the fewest number of\nmeasurements currently known amongst efficient schemes, improving recent bounds\nof [SomaY16].\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 16:44:12 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 12:48:26 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Morgan", "Tom", ""], ["Nelson", "Jelani", ""]]}, {"id": "1606.00898", "submitter": "Anand Kumar Narayanan", "authors": "Anand Kumar Narayanan", "title": "Factoring Polynomials over Finite Fields using Drinfeld Modules with\n  Complex Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel algorithms to factor polynomials over a finite field $\\F_q$\nof odd characteristic using rank $2$ Drinfeld modules with complex\nmultiplication. The main idea is to compute a lift of the Hasse invariant\n(modulo the polynomial $f(x) \\in \\F_q[x]$ to be factored) with respect to a\nDrinfeld module $\\phi$ with complex multiplication. Factors of $f(x)$ supported\non prime ideals with supersingular reduction at $\\phi$ have vanishing Hasse\ninvariant and can be separated from the rest. A Drinfeld module analogue of\nDeligne's congruence plays a key role in computing the Hasse invariant lift. We\npresent two algorithms based on this idea. The first algorithm chooses Drinfeld\nmodules with complex multiplication at random and has a quadratic expected run\ntime. The second is a deterministic algorithm with $O(\\sqrt{p})$ run time\ndependence on the characteristic $p$ of $\\F_q$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 21:09:01 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Narayanan", "Anand Kumar", ""]]}, {"id": "1606.00942", "submitter": "Insu Han", "authors": "Insu Han, Dmitry Malioutov, Haim Avron, Jinwoo Shin", "title": "Approximating the Spectral Sums of Large-scale Matrices using Chebyshev\n  Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of the trace of a matrix function plays an important role in many\nscientific computing applications, including applications in machine learning,\ncomputational physics (e.g., lattice quantum chromodynamics), network analysis\nand computational biology (e.g., protein folding), just to name a few\napplication areas. We propose a linear-time randomized algorithm for\napproximating the trace of matrix functions of large symmetric matrices. Our\nalgorithm is based on coupling function approximation using Chebyshev\ninterpolation with stochastic trace estimators (Hutchinson's method), and as\nsuch requires only implicit access to the matrix, in the form of a function\nthat maps a vector to the product of the matrix and the vector. We provide\nrigorous approximation error in terms of the extremal eigenvalue of the input\nmatrix, and the Bernstein ellipse that corresponds to the function at hand.\nBased on our general scheme, we provide algorithms with provable guarantees for\nimportant matrix computations, including log-determinant, trace of matrix\ninverse, Estrada index, Schatten p-norm, and testing positive definiteness. We\nexperimentally evaluate our algorithm and demonstrate its effectiveness on\nmatrices with tens of millions dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 01:36:29 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 10:30:24 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Han", "Insu", ""], ["Malioutov", "Dmitry", ""], ["Avron", "Haim", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1606.00996", "submitter": "Aviv Yehezkel", "authors": "Reuven Cohen, Liran Katzir and Aviv Yehezkel", "title": "A Minimal Variance Estimator for the Cardinality of Big Data Set\n  Intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a growing interest in developing \"streaming\nalgorithms\" for efficient processing and querying of continuous data streams.\nThese algorithms seek to provide accurate results while minimizing the required\nstorage and the processing time, at the price of a small inaccuracy in their\noutput. A fundamental query of interest is the intersection size of two big\ndata streams. This problem arises in many different application areas, such as\nnetwork monitoring, database systems, data integration and information\nretrieval. In this paper we develop a new algorithm for this problem, based on\nthe Maximum Likelihood (ML) method. We show that this algorithm outperforms all\nknown schemes and that it asymptotically achieves the optimal variance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 07:51:56 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Cohen", "Reuven", ""], ["Katzir", "Liran", ""], ["Yehezkel", "Aviv", ""]]}, {"id": "1606.01091", "submitter": "Eleni Akrida", "authors": "Eleni C. Akrida, Jurek Czyzowicz, Leszek Gasieniec, Lukasz Kuszner,\n  Paul G. Spirakis", "title": "Temporal flows in Temporal networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce temporal flows on temporal networks, i.e., networks the links of\nwhich exist only at certain moments of time. Such networks are ephemeral in the\nsense that no link exists after some time. Our flow model is new and differs\nfrom the \"flows over time\" model, also called \"dynamic flows\" in the\nliterature. We show that the problem of finding the maximum amount of flow that\ncan pass from a source vertex s to a sink vertex t up to a given time is\nsolvable in Polynomial time, even when node buffers are bounded. We then\nexamine mainly the case of unbounded node buffers. We provide a simplified\nstatic Time-Extended network (STEG), which is of polynomial size to the input\nand whose static flow rates are equivalent to the respective temporal flow of\nthe temporal network, using STEG, we prove that the maximum temporal flow is\nequal to the minimum temporal s-t cut. We further show that temporal flows can\nalways be decomposed into flows, each of which moves only through a journey,\ni.e., a directed path whose successive edges have strictly increasing moments\nof existence. We partially characterise networks with random edge\navailabilities that tend to eliminate the s-t temporal flow. We then consider\nmixed temporal networks, which have some edges with specified availabilities\nand some edges with random availabilities, we show that it is #P-hard to\ncompute the tails and expectations of the maximum temporal flow (which is now a\nrandom variable) in a mixed temporal network.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 13:56:54 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 13:04:05 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Akrida", "Eleni C.", ""], ["Czyzowicz", "Jurek", ""], ["Gasieniec", "Leszek", ""], ["Kuszner", "Lukasz", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1606.01275", "submitter": "Zhiwei Steven Wu", "authors": "Michael Kearns, Zhiwei Steven Wu", "title": "Predicting with Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new learning model in which a joint distribution over vector\npairs $(x,y)$ is determined by an unknown function $c(x)$ that maps input\nvectors $x$ not to individual outputs, but to entire {\\em distributions\\/} over\noutput vectors $y$. Our main results take the form of rather general reductions\nfrom our model to algorithms for PAC learning the function class and the\ndistribution class separately, and show that virtually every such combination\nyields an efficient algorithm in our model. Our methods include a randomized\nreduction to classification noise and an application of Le Cam's method to\nobtain robust learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 20:56:51 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:02:06 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 15:06:49 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kearns", "Michael", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1606.01316", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Srinadh Bhojanapalli, Constantine\n  Caramanis, Sujay Sanghavi", "title": "Provable Burer-Monteiro factorization for a class of norm-constrained\n  matrix problems", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the projected gradient descent method on low-rank matrix problems\nwith a strongly convex objective. We use the Burer-Monteiro factorization\napproach to implicitly enforce low-rankness; such factorization introduces\nnon-convexity in the objective. We focus on constraint sets that include both\npositive semi-definite (PSD) constraints and specific matrix norm-constraints.\nSuch criteria appear in quantum state tomography and phase retrieval\napplications.\n  We show that non-convex projected gradient descent favors local linear\nconvergence in the factored space. We build our theory on a novel descent\nlemma, that non-trivially extends recent results on the unconstrained problem.\nThe resulting algorithm is Projected Factored Gradient Descent, abbreviated as\nProjFGD, and shows superior performance compared to state of the art on quantum\nstate tomography and sparse phase retrieval applications.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 02:12:13 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 02:35:04 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 22:47:53 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Bhojanapalli", "Srinadh", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1606.01340", "submitter": "Ruifeng Liu", "authors": "Ruifeng Liu, Ada WaiChee Fu, Zitong Chen, Silu Huang, Yubao Liu", "title": "Finding Multiple New Optimal Locations in a Road Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimal location querying for location based services\nin road networks, which aims to find locations for new servers or facilities.\nThe existing optimal solutions on this problem consider only the cases with one\nnew server. When two or more new servers are to be set up, the problem with\nminmax cost criteria, MinMax, becomes NP-hard. In this work we identify some\nuseful properties about the potential locations for the new servers, from which\nwe derive a novel algorithm for MinMax, and show that it is efficient when the\nnumber of new servers is small. When the number of new servers is large, we\npropose an efficient 3-approximate algorithm. We verify with experiments on\nreal road networks that our solutions are effective and attains significantly\nbetter result quality compared to the existing greedy algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 07:42:16 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 02:56:15 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Liu", "Ruifeng", ""], ["Fu", "Ada WaiChee", ""], ["Chen", "Zitong", ""], ["Huang", "Silu", ""], ["Liu", "Yubao", ""]]}, {"id": "1606.01342", "submitter": "Adam Kasperski", "authors": "Mikita Hradovich, Adam Kasperski, Pawel Zielinski", "title": "Recoverable robust spanning tree problem under interval uncertainty\n  representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the recoverable robust spanning tree problem under\ninterval uncertainty representations. A polynomial time, combinatorial\nalgorithm for the recoverable spanning tree problem is first constructed. This\nproblem generalizes the incremental spanning tree problem, previously discussed\nin literature. The algorithm built is then applied to solve the recoverable\nrobust spanning tree problem, under the traditional interval uncertainty\nrepresentation, in polynomial time. Moreover, the algorithm allows to obtain,\nunder some mild assumptions about the uncertainty intervals,several\napproximation results for the recoverable robust spanning tree problem under\nthe Bertsimas and Sim interval uncertainty representation and the interval\nuncertainty representation with a budget constraint.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 08:02:23 GMT"}, {"version": "v2", "created": "Sat, 27 Aug 2016 08:02:41 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Hradovich", "Mikita", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1606.01371", "submitter": "Gwena\\\"el Joret", "authors": "Gerardo Berbeglia and Gwena\\\"el Joret", "title": "Assortment optimisation under a general discrete choice model: A tight\n  analysis of revenue-ordered assortments", "comments": "Minor changes following referees' comments", "journal-ref": "ACM EC 2017", "doi": "10.1145/3033274.3084084", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assortment problem in revenue management is the problem of deciding which\nsubset of products to offer to consumers in order to maximise revenue. A simple\nand natural strategy is to select the best assortment out of all those that are\nconstructed by fixing a threshold revenue $\\pi$ and then choosing all products\nwith revenue at least $\\pi$. This is known as the revenue-ordered assortments\nstrategy. In this paper we study the approximation guarantees provided by\nrevenue-ordered assortments when customers are rational in the following sense:\nthe probability of selecting a specific product from the set being offered\ncannot increase if the set is enlarged. This rationality assumption, known as\nregularity, is satisfied by almost all discrete choice models considered in the\nrevenue management and choice theory literature, and in particular by random\nutility models. The bounds we obtain are tight and improve on recent results in\nthat direction, such as for the Mixed Multinomial Logit model by\nRusmevichientong et al. (2014). An appealing feature of our analysis is its\nsimplicity, as it relies only on the regularity condition.\n  We also draw a connection between assortment optimisation and two pricing\nproblems called unit demand envy-free pricing and Stackelberg minimum spanning\ntree: These problems can be restated as assortment problems under discrete\nchoice models satisfying the regularity condition, and moreover revenue-ordered\nassortments correspond then to the well-studied uniform pricing heuristic. When\nspecialised to that setting, the general bounds we establish for\nrevenue-ordered assortments match and unify the best known results on uniform\npricing.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 11:31:26 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 08:57:47 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 09:13:26 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Berbeglia", "Gerardo", ""], ["Joret", "Gwena\u00ebl", ""]]}, {"id": "1606.01500", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Ho N. Phien, Hoang D. Tuan and Minh N. Do", "title": "Efficient tensor completion for color image and video recovery: Low-rank\n  tensor train", "comments": "Submitted to the IEEE Transactions on Image Processing. arXiv admin\n  note: substantial text overlap with arXiv:1601.01083", "journal-ref": null, "doi": "10.1109/TIP.2017.2672439", "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to tensor completion, which recovers\nmissing entries of data represented by tensors. The approach is based on the\ntensor train (TT) rank, which is able to capture hidden information from\ntensors thanks to its definition from a well-balanced matricization scheme.\nAccordingly, new optimization formulations for tensor completion are proposed\nas well as two new algorithms for their solution. The first one called simple\nlow-rank tensor completion via tensor train (SiLRTC-TT) is intimately related\nto minimizing a nuclear norm based on TT rank. The second one is from a\nmultilinear matrix factorization model to approximate the TT rank of a tensor,\nand is called tensor completion by parallel matrix factorization via tensor\ntrain (TMac-TT). A tensor augmentation scheme of transforming a low-order\ntensor to higher-orders is also proposed to enhance the effectiveness of\nSiLRTC-TT and TMac-TT. Simulation results for color image and video recovery\nshow the clear advantage of our method over all other methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 12:09:19 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1606.01530", "submitter": "Fatemeh Navidi", "authors": "Fatemeh Navidi, Prabhanjan Kambadur, Viswanath Nagarajan", "title": "Adaptive Submodular Ranking and Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general stochastic ranking problem where an algorithm needs to\nadaptively select a sequence of elements so as to \"cover\" a random scenario\n(drawn from a known distribution) at minimum expected cost. The coverage of\neach scenario is captured by an individual submodular function, where the\nscenario is said to be covered when its function value goes above a given\nthreshold. We obtain a logarithmic factor approximation algorithm for this\nadaptive ranking problem, which is the best possible (unless P=NP). This\nproblem unifies and generalizes many previously studied problems with\napplications in search ranking and active learning. The approximation ratio of\nour algorithm either matches or improves the best result known in each of these\nspecial cases. Furthermore, we extend our results to an adaptive vehicle\nrouting problem, where costs are determined by an underlying metric. This\nrouting problem is a significant generalization of the previously-studied\nadaptive traveling salesman and traveling repairman problems. Our approximation\nratio nearly matches the best bound known for these special cases. Finally, we\npresent experimental results for some applications of adaptive ranking.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 16:19:58 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 18:53:02 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Navidi", "Fatemeh", ""], ["Kambadur", "Prabhanjan", ""], ["Nagarajan", "Viswanath", ""]]}, {"id": "1606.01623", "submitter": "John Dickerson", "authors": "John P. Dickerson, David F. Manlove, Benjamin Plaut, Tuomas Sandholm,\n  James Trimble", "title": "Position-Indexed Formulations for Kidney Exchange", "comments": "Appeared at the ACM Conference on Economics and Computation (EC-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kidney exchange is an organized barter market where patients in need of a\nkidney swap willing but incompatible donors. Determining an optimal set of\nexchanges is theoretically and empirically hard. Traditionally, exchanges took\nplace in cycles, with each participating patient-donor pair both giving and\nreceiving a kidney. The recent introduction of chains, where a donor without a\npaired patient triggers a sequence of donations without requiring a kidney in\nreturn, increased the efficacy of fielded kidney exchanges---while also\ndramatically raising the empirical computational hardness of clearing the\nmarket in practice. While chains can be quite long, unbounded-length chains are\nnot desirable: planned donations can fail before transplant for a variety of\nreasons, and the failure of a single donation causes the rest of that chain to\nfail, so parallel shorter chains are better in practice.\n  In this paper, we address the tractable clearing of kidney exchanges with\nshort cycles and chains that are long but bounded. This corresponds to the\npractice at most modern fielded kidney exchanges. We introduce three new\ninteger programming formulations, two of which are compact. Furthermore, one of\nthese models has a linear programming relaxation that is exactly as tight as\nthe previous tightest formulation (which was not compact) for instances in\nwhich each donor has a paired patient. On real data from the UNOS nationwide\nexchange in the United States and the NLDKSS nationwide exchange in the United\nKingdom, as well as on generated realistic large-scale data, we show that our\nnew models are competitive with all existing solvers---in many cases\noutperforming all other solvers by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 06:16:36 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 22:39:37 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Dickerson", "John P.", ""], ["Manlove", "David F.", ""], ["Plaut", "Benjamin", ""], ["Sandholm", "Tuomas", ""], ["Trimble", "James", ""]]}, {"id": "1606.01754", "submitter": "Aravind Rajeswaran", "authors": "Aravind Rajeswaran, Sridharakumar Narasimhan, Shankar Narasimhan", "title": "A Graph Partitioning Algorithm for Leak Detection in Water Distribution\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leak detection in urban water distribution networks (WDNs) is challenging\ngiven their scale, complexity, and limited instrumentation. We present an\nalgorithm for leak detection in WDNs, which involves making additional flow\nmeasurements on-demand, and repeated use of water balance. Graph partitioning\nis used to determine the location of flow measurements, with the objective to\nminimize the measurement cost. We follow a multi-stage divide and conquer\napproach. In every stage, a section of the WDN identified to contain the leak\nis partitioned into two or more sub-networks, and water balance is used to\ntrace the leak to one of these sub-networks. This process is recursively\ncontinued until the desired resolution is achieved. We investigate different\nmethods for solving the arising graph partitioning problem like integer linear\nprogramming (ILP) and spectral bisection. The proposed methods are tested on\nlarge scale benchmark networks, and our results indicate that on average, less\nthan 3% of the pipes need to be measured for finding the leak in large\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 12:30:26 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Rajeswaran", "Aravind", ""], ["Narasimhan", "Sridharakumar", ""], ["Narasimhan", "Shankar", ""]]}, {"id": "1606.01824", "submitter": "Juan Jos\\'e \\'Alvarez", "authors": "J.V.\\'Alvarez-Bravo, J.J. \\'Alvarez-S\\'anchez, Ignacio Aparicio\n  Morgado", "title": "A Quantum Abacus based encoding system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formal description of a quantum abacus based encoding system is presented.\nThis way of representing data for processing purposes is based on a quantum\nalgorithm for counting qubits introduced by Lesovik et al.\n\\cite{LesovikEtal2010} and Suslov et al. \\cite{SuslovEtal2011}, but formally\ndeveloped in this work. Finally, in order to illustrate the potential of this\nproposal, the implementation of the basic quantum array operations through this\nencoding system is presented.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 16:51:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 08:04:05 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["\u00c1lvarez-Bravo", "J. V.", ""], ["\u00c1lvarez-S\u00e1nchez", "J. J.", ""], ["Morgado", "Ignacio Aparicio", ""]]}, {"id": "1606.01833", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Analyzing Distributed Join-Idle-Queue: A Fluid Limit Approach", "comments": "11 pages, draft paper, likely to be at Allerton 2016, possibly\n  improved before then", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of load balancing, Lu et al. introduced the distributed\nJoin-Idle-Queue algorithm, where a group of dispatchers distribute jobs to a\ncluster of parallel servers. Each dispatcher maintains a queue of idle servers;\nwhen a job arrives to a dispatcher, it sends it to a server on its queue, or to\na random server if the queue is empty. In turn, when a server has no jobs, it\nrequests to be placed on the idle queue of a randomly chosen dispatcher.\n  Although this algorithm was shown to be quite effective, the original\nasymptotic analysis makes simplifying assumptions that become increasingly\ninaccurate as the system load increases. Further, the analysis does not\nnaturally generalize to interesting variations, such as having a server request\nto be placed on the idle queue of a dispatcher before it has completed all\njobs, which can be beneficial under high loads.\n  We provide a new asymptotic analysis of Join-Idle-Queue systems based on mean\nfield fluid limit methods, deriving families of differential equations that\ndescribe these systems. Our analysis avoids previous simplifying assumptions,\nis empirically more accurate, and generalizes naturally to the variation\ndescribed above, as well as other simple variations. Our theoretical and\nempirical analyses shed further light on the performance of Join-Idle-Queue,\nincluding potential performance pitfalls under high load.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 17:15:50 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1606.01925", "submitter": "Yongjie Yang", "authors": "Wenjun Li, Yongjie Yang, Jianer Chen, Jianxin Wang", "title": "Further Kernelization of Proper Interval Vertex Deletion: New\n  Observations and Refined Analysis", "comments": "This paper is combined with another paper where a significantly\n  improved kernel of size O(k^4) is obtained. Due to this reason, the authors\n  withdraw this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Proper Interval Vertex Deletion problem (PIVD for short), we are given\na graph $G$ and an integer parameter $k>0$, and the question is whether there\nare at most $k$ vertices in $G$ whose removal results in a proper interval\ngraph. It is known that the PIVD problem is fixed-parameter tractable and\nadmits a polynomial but \"unreasonably\" large kernel of $O(k^{53})$ vertices. A\nnatural question is whether the problem admits a polynomial kernel of\n\"reasonable\" size. In this paper, we answer this question by deriving an\n$O(k^7)$-vertex kernel for the PIVD problem. Our kernelization is based on\nseveral new observations and a refined analysis of the kernelization.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 20:15:42 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 11:22:39 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 14:49:40 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Li", "Wenjun", ""], ["Yang", "Yongjie", ""], ["Chen", "Jianer", ""], ["Wang", "Jianxin", ""]]}, {"id": "1606.02015", "submitter": "Lei Guo", "authors": "Lei Guo, Dejun Teng, Rubao Lee, Feng Chen, Siyuan Ma, Xiaodong Zhang", "title": "Re-enabling high-speed caching for LSM-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSM-tree has been widely used in cloud computing systems by Google, Facebook,\nand Amazon, to achieve high performance for write-intensive workloads. However,\nin LSM-tree, random key-value queries can experience long latency and low\nthroughput due to the interference from the compaction, a basic operation in\nthe algorithm, to caching. LSM-tree relies on frequent compaction operations to\nmerge data into a sorted structure. After a compaction, the original data are\nreorganized and written to other locations on the disk. As a result, the cached\ndata are invalidated since their referencing addresses are changed, causing\nserious performance degradations.\n  We propose dLSM in order to re-enable high-speed caching during intensive\nwrites. dLSM is an LSM-tree with a compaction buffer on the disk, working as a\ncushion to minimize the cache invalidation caused by compactions. The\ncompaction buffer maintains a series of snapshots of the frequently compacted\ndata, which represent a consistent view of the corresponding data in the\nunderlying LSM-tree. Being updated in a much lower rate than that of\ncompactions, data in the compaction buffer are almost stationary. In dLSM, an\nobject is referenced by the disk address of the corresponding block either in\nthe compaction buffer for frequently compacted data, or in the underlying\nLSM-tree for infrequently compacted data. Thus, hot objects can be effectively\nkept in the cache without harmful invalidations. With the help of a small\non-disk compaction buffer, dLSM achieves a high query performance by enabling\neffective caching, while retaining all merits of LSM-tree for write-intensive\ndata processing. We have implemented dLSM based on LevelDB. Our evaluations\nshow that with a standard DRAM cache, dLSM can achieve 5--8x performance\nimprovement over LSM with the same cache on HDD storage.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 03:53:45 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Guo", "Lei", ""], ["Teng", "Dejun", ""], ["Lee", "Rubao", ""], ["Chen", "Feng", ""], ["Ma", "Siyuan", ""], ["Zhang", "Xiaodong", ""]]}, {"id": "1606.02162", "submitter": "Fabrizio Montecchiani", "authors": "Alessio Arleo, Walter Didimo, Giuseppe Liotta, Fabrizio Montecchiani", "title": "A Distributed Force-Directed Algorithm on Giraph: Design and Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of designing a distributed graph\nvisualization algorithm for large graphs. The algorithm must be simple to\nimplement and the computing infrastructure must not require major hardware or\nsoftware investments. We design, implement, and experiment a force-directed\nalgorithm in Giraph, a popular open source framework for distributed computing,\nbased on a vertex-centric design paradigm. The algorithm is tested both on real\nand artificial graphs with up to million edges, by using a rather inexpensive\nPaaS (Platform as a Service) infrastructure of Amazon. The experiments show the\nscalability and effectiveness of our technique when compared to a centralized\nimplementation of the same force-directed model. We show that graphs with about\none million edges can be drawn in less than 8 minutes, by spending about 1\\$\nper drawing in the cloud computing infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 14:56:59 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Arleo", "Alessio", ""], ["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "1606.02465", "submitter": "Dominik K\\\"oppl", "authors": "Johannes Fischer and Dominik K\\\"oppl and Florian Kurpicz", "title": "On the Benefit of Merging Suffix Array Intervals for Parallel Pattern\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present parallel algorithms for exact and approximate pattern matching\nwith suffix arrays, using a CREW-PRAM with $p$ processors. Given a static text\nof length $n$, we first show how to compute the suffix array interval of a\ngiven pattern of length $m$ in $O(\\frac{m}{p}+ \\lg p + \\lg\\lg p\\cdot\\lg\\lg n)$\ntime for $p \\le m$. For approximate pattern matching with $k$ differences or\nmismatches, we show how to compute all occurrences of a given pattern in\n$O(\\frac{m^k\\sigma^k}{p}\\max\\left(k,\\lg\\lg n\\right)\\!+\\!(1+\\frac{m}{p}) \\lg\np\\cdot \\lg\\lg n + \\text{occ})$ time, where $\\sigma$ is the size of the alphabet\nand $p \\le \\sigma^k m^k$. The workhorse of our algorithms is a data structure\nfor merging suffix array intervals quickly: Given the suffix array intervals\nfor two patterns $P$ and $P'$, we present a data structure for computing the\ninterval of $PP'$ in $O(\\lg\\lg n)$ sequential time, or in $O(1+\\lg_p\\lg n)$\nparallel time. All our data structures are of size $O(n)$ bits (in addition to\nthe suffix array).\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 09:17:37 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Fischer", "Johannes", ""], ["K\u00f6ppl", "Dominik", ""], ["Kurpicz", "Florian", ""]]}, {"id": "1606.02688", "submitter": "Pawel Komosa", "authors": "Ivan Bliznets, Marek Cygan, Pawel Komosa, Michal Pilipczuk", "title": "Hardness of approximation for H-free edge modification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $H$-Free Edge Deletion problem asks, for a given graph $G$ and an integer\n$k$, whether it is possible to delete at most $k$ edges from $G$ to make it\n$H$-free, that is, not containing $H$ as an induced subgraph. The $H$-Free Edge\nCompletion problem is defined similarly, but we add edges instead of deleting\nthem. The study of these two problem families has recently been the subject of\nintensive studies from the point of view of parameterized complexity and\nkernelization. In particular, it was shown that the problems do not admit\npolynomial kernels (under plausible complexity assumptions) for almost all\ngraphs $H$, with several important exceptions occurring when the class of\n$H$-free graphs exhibits some structural properties.\n  In this work we complement the parameterized study of edge modification\nproblems to $H$-free graphs by considering their approximability. We prove that\nwhenever $H$ is $3$-connected and has at least two non-edges, then both\n$H$-Free Edge Deletion and $H$-Free Edge Completion are very hard to\napproximate: they do not admit $\\mathrm{poly}(\\mathsf{OPT})$-approximation in\npolynomial time, unless $\\mathrm{P}=\\mathrm{NP}$, or even in time\nsubexponential in $\\mathsf{OPT}$, unless the Exponential Time Hypothesis fails.\nThe assumption of the existence of two non-edges appears to be important: we\nshow that whenever $H$ is a complete graph without one edge, then $H$-Free Edge\nDeletion is tightly connected to the Min Horn problem, whose approximability is\nstill open. Finally, in an attempt to extend our hardness results beyond\n$3$-connected graphs, we consider the cases of $H$ being a path or a cycle, and\nwe achieve an almost complete dichotomy there.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:01:02 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 11:14:35 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Bliznets", "Ivan", ""], ["Cygan", "Marek", ""], ["Komosa", "Pawel", ""], ["Pilipczuk", "Michal", ""]]}, {"id": "1606.02786", "submitter": "Moein  Falahatgar", "authors": "Jayadev Acharya, Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky,\n  Ananda Theertha Suresh", "title": "Maximum Selection and Sorting with Adversarial Comparators and an\n  Application to Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study maximum selection and sorting of $n$ numbers using pairwise\ncomparators that output the larger of their two inputs if the inputs are more\nthan a given threshold apart, and output an adversarially-chosen input\notherwise. We consider two adversarial models. A non-adaptive adversary that\ndecides on the outcomes in advance based solely on the inputs, and an adaptive\nadversary that can decide on the outcome of each query depending on previous\nqueries and outcomes.\n  Against the non-adaptive adversary, we derive a maximum-selection algorithm\nthat uses at most $2n$ comparisons in expectation, and a sorting algorithm that\nuses at most $2n \\ln n$ comparisons in expectation. These numbers are within\nsmall constant factors from the best possible. Against the adaptive adversary,\nwe propose a maximum-selection algorithm that uses $\\Theta(n\\log\n(1/{\\epsilon}))$ comparisons to output a correct answer with probability at\nleast $1-\\epsilon$. The existence of this algorithm affirmatively resolves an\nopen problem of Ajtai, Feldman, Hassadim, and Nelson.\n  Our study was motivated by a density-estimation problem where, given samples\nfrom an unknown underlying distribution, we would like to find a distribution\nin a known class of $n$ candidate distributions that is close to underlying\ndistribution in $\\ell_1$ distance. Scheffe's algorithm outputs a distribution\nat an $\\ell_1$ distance at most 9 times the minimum and runs in time\n$\\Theta(n^2\\log n)$. Using maximum selection, we propose an algorithm with the\nsame approximation guarantee but run time of $\\Theta(n\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 00:20:07 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Acharya", "Jayadev", ""], ["Falahatgar", "Moein", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1606.02889", "submitter": "Tarun Yadav", "authors": "Tarun Yadav, Koustav Sadhukhan, Rao Arvind Mallari", "title": "Approximation Algorithm for N-distance Minimal Vertex Cover Problem", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution of large scale networks demand for efficient way of communication\nin the networks. One way to propagate information in the network is to find\nvertex cover. In this paper we describe a variant of vertex cover problem\nnaming it N-distance Vertex Minimal Cover(N-MVC) Problem to optimize\ninformation propagation throughout the network. A minimum subset of vertices of\na unweighted and undirected graph G = (V, E) is called N-MVC if for all v in V\n, v is at distance less than or equal to N from at least one of the the\nvertices in N-MVC. In the following paper, this problem is defined, formulated\nand an approximation algorithm is proposed with discussion on its correctness\nand upper bound.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 09:58:17 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Yadav", "Tarun", ""], ["Sadhukhan", "Koustav", ""], ["Mallari", "Rao Arvind", ""]]}, {"id": "1606.02967", "submitter": "Sophie Spirkl", "authors": "Maria Chudnovsky, Oliver Schaudt, Sophie Spirkl, Maya Stein, Mingxian\n  Zhong", "title": "Approximately coloring graphs without long induced paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an open problem whether the 3-coloring problem can be solved in\npolynomial time in the class of graphs that do not contain an induced path on\n$t$ vertices, for fixed $t$. We propose an algorithm that, given a 3-colorable\ngraph without an induced path on $t$ vertices, computes a coloring with\n$\\max\\{5,2\\lceil{\\frac{t-1}{2}}\\rceil-2\\}$ many colors. If the input graph is\ntriangle-free, we only need $\\max\\{4,\\lceil{\\frac{t-1}{2}}\\rceil+1\\}$ many\ncolors. The running time of our algorithm is $O((3^{t-2}+t^2)m+n)$ if the input\ngraph has $n$ vertices and $m$ edges.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:04:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 16:11:43 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chudnovsky", "Maria", ""], ["Schaudt", "Oliver", ""], ["Spirkl", "Sophie", ""], ["Stein", "Maya", ""], ["Zhong", "Mingxian", ""]]}, {"id": "1606.03077", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Alistair Stewart", "title": "Efficient Robust Proper Learning of Log-concave Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em robust proper learning} of univariate log-concave\ndistributions (over continuous and discrete domains). Given a set of samples\ndrawn from an unknown target distribution, we want to compute a log-concave\nhypothesis distribution that is as close as possible to the target, in total\nvariation distance. In this work, we give the first computationally efficient\nalgorithm for this learning problem. Our algorithm achieves the\ninformation-theoretically optimal sample size (up to a constant factor), runs\nin polynomial time, and is robust to model misspecification with nearly-optimal\nerror guarantees.\n  Specifically, we give an algorithm that, on input $n=O(1/\\eps^{5/2})$ samples\nfrom an unknown distribution $f$, runs in time $\\widetilde{O}(n^{8/5})$, and\noutputs a log-concave hypothesis $h$ that (with high probability) satisfies\n$\\dtv(h, f) = O(\\opt)+\\eps$, where $\\opt$ is the minimum total variation\ndistance between $f$ and the class of log-concave distributions. Our approach\nto the robust proper learning problem is quite flexible and may be applicable\nto many other univariate distribution families.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 19:32:20 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1606.03168", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay\n  Sanghavi", "title": "Finding Low-Rank Solutions via Non-Convex Matrix Factorization,\n  Efficiently and Provably", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ can be written as a product\n$U V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n\n\\times r}$. One could exploit this observation in optimization: e.g., consider\nthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where the\nset of rank-$r$ matrices is modeled via the factorization $UV^\\top$. Though\nsuch parameterization reduces the number of variables, and is more\ncomputationally efficient (of particular interest is the case $r \\ll \\min\\{m,\nn\\}$), it comes at a cost: $f(UV^\\top)$ becomes a non-convex function w.r.t.\n$U$ and $V$.\n  We study such parameterization for optimization of generic convex objectives\n$f$, and focus on first-order, gradient descent algorithmic solutions. We\npropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient\nfirst-order method that operates on the $U, V$ factors. We show that when $f$\nis (restricted) smooth, BFGD has local sublinear convergence, and linear\nconvergence when $f$ is both (restricted) smooth and (restricted) strongly\nconvex. For several key applications, we provide simple and efficient\ninitialization schemes that provide approximate solutions good enough for the\nabove convergence results to hold.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 03:18:01 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 20:55:56 GMT"}, {"version": "v3", "created": "Sat, 29 Oct 2016 21:03:47 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1606.03185", "submitter": "Guohui Lin", "authors": "Yao Xu, Peng Zhang, Randy Goebel, Guohui Lin", "title": "Approximation algorithms for the vertex happiness", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the maximum happy vertices (MHV) problem and its complement,\nthe minimum unhappy vertices (MUHV) problem. We first show that the MHV and\nMUHV problems are a special case of the supermodular and submodular\nmulti-labeling (Sup-ML and Sub-ML) problems, respectively, by re-writing the\nobjective functions as set functions. The convex relaxation on the Lov\\'{a}sz\nextension, originally presented for the submodular multi-partitioning (Sub-MP)\nproblem, can be extended for the Sub-ML problem, thereby proving that the\nSub-ML (Sup-ML, respectively) can be approximated within a factor of $2 -\n\\frac{2}{k}$ ($\\frac{2}{k}$, respectively). These general results imply that\nthe MHV and the MUHV problems can also be approximated within $\\frac{2}{k}$ and\n$2 - \\frac{2}{k}$, respectively, using the same approximation algorithms. For\nMHV, this $\\frac{2}{k}$-approximation algorithm improves the previous best\napproximation ratio $\\max \\{\\frac{1}{k}, \\frac{1}{\\Delta + 1}\\}$, where\n$\\Delta$ is the maximum vertex degree of the input graph. We also show that an\nexisting LP relaxation is the same as the concave relaxation on the Lov\\'{a}sz\nextension for the Sup-ML problem; we then prove an upper bound of $\\frac{2}{k}$\non the integrality gap of the LP relaxation. These suggest that the\n$\\frac{2}{k}$-approximation algorithm is the best possible based on the LP\nrelaxation. For MUHV, we formulate a novel LP relaxation and prove that it is\nthe same as the convex relaxation on the Lov\\'{a}sz extension for the Sub-ML\nproblem; we then show a lower bound of $2 - \\frac{2}{k}$ on the integrality gap\nof the LP relaxation. Similarly, these suggest that the $(2 -\n\\frac{2}{k})$-approximation algorithm is the best possible based on the LP\nrelaxation. Lastly, we prove that this $(2 - \\frac{2}{k})$-approximation is\noptimal for the MUHV problem, assuming the Unique Games Conjecture.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 05:41:37 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 17:50:49 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Xu", "Yao", ""], ["Zhang", "Peng", ""], ["Goebel", "Randy", ""], ["Lin", "Guohui", ""]]}, {"id": "1606.03198", "submitter": "Annalisa De Bonis", "authors": "Annalisa De Bonis", "title": "Conflict Resolution in Multiple Access Channels Supporting Simultaneous\n  Successful Transmissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Conflict Resolution Problem in the context of a\nmultiple-access system in which several stations can transmit their messages\nsimultaneously to the channel. We assume that there are n stations and that at\nmost k<= n stations are active at the same time, i.e, are willing to transmit a\nmessage. If in a certain instant at most d<=k active stations transmit to the\nchannel then their messages are successfully transmitted, whereas if more than\nd active stations transmit simultaneously then their messages are lost. In this\nlatter case we say that a conflict occurs. The present paper investigates\nnon-adaptive conflict resolution algorithms working under the assumption that\nactive stations receive a feedback from the channel that informs them on\nwhether their messages have been successfully transmitted. If a station becomes\naware that its message has been correctly sent over the channel then it becomes\nimmediately inactive. The measure to optimize is the number of time slots\nneeded to solve conflicts among all active stations. The fundamental question\nis whether this measure decreases linearly with the number d of messages that\ncan be simultaneously transmitted with success. We give a positive answer to\nthis question by providing a conflict resolution algorithm that uses a 1/d\nratio of the number of time slots used by the optimal conflict resolution\nalgorithm for the case d=1. Moreover, we derive a lower bound on the number of\ntime slots needed to solve conflicts non-adaptively which is within a log (k/d)\nfactor from the upper bound. To this aim, we introduce a new combinatorial\nstructure that consists in a generalization of Komlos and Greenberg codes.\nConstructions of these new codes are obtained via a new generalization of\nselectors, whereas the non-existential result is implied by a non-existential\nresult for a new generalization of the locally thin families.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:09:42 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["De Bonis", "Annalisa", ""]]}, {"id": "1606.03200", "submitter": "Annalisa De Bonis", "authors": "Annalisa De Bonis", "title": "Constraining the Number of Positive Responses in Adaptive, Non-Adaptive,\n  and Two-Stage Group Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group testing is a well known search problem that consists in detecting the\ndefective members of a set of objects O by performing tests on properly chosen\nsubsets (pools) of the given set O. In classical group testing the goal is to\nfind all defectives by using as few tests as possible. We consider a variant of\nclassical group testing in which one is concerned not only with minimizing the\ntotal number of tests but aims also at reducing the number of tests involving\ndefective elements. The rationale behind this search model is that in many\npractical applications the devices used for the tests are subject to\ndeterioration due to exposure to or interaction with the defective elements. In\nthis paper we consider adaptive, non-adaptive and two-stage group testing. For\nall three considered scenarios, we derive upper and lower bounds on the number\nof \"yes\" responses that must be admitted by any strategy performing at most a\ncertain number t of tests. In particular, for the adaptive case we provide an\nalgorithm that uses a number of \"yes\" responses that exceeds the given lower\nbound by a small constant. Interestingly, this bound can be asymptotically\nattained also by our two-stage algorithm, which is a phenomenon analogous to\nthe one occurring in classical group testing. For the non-adaptive scenario we\ngive almost matching upper and lower bounds on the number of \"yes\" responses.\nIn particular, we give two constructions both achieving the same asymptotic\nbound. An interesting feature of one of these constructions is that it is an\nexplicit construction. The bounds for the non-adaptive and the two-stage cases\nfollow from the bounds on the optimal sizes of new variants of d-cover free\nfamilies and (p,d)-cover free families introduced in this paper, which we\nbelieve may be of interest also in other contexts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:13:53 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["De Bonis", "Annalisa", ""]]}, {"id": "1606.03268", "submitter": "Andr\\'e Nichterlein", "authors": "Christian Komusiewicz, Andr\\'e Nichterlein, Rolf Niedermeier", "title": "Parameterized Algorithmics for Graph Modification Problems: On\n  Interactions with Heuristics", "comments": "Invited Paper at the 41st International Workshop on Graph-Theoretic\n  Concepts in Computer Science (WG 15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph modification problems, one is given a graph G and the goal is to\napply a minimum number of modification operations (such as edge deletions) to G\nsuch that the resulting graph fulfills a certain property. For example, the\nCluster Deletion problem asks to delete as few edges as possible such that the\nresulting graph is a disjoint union of cliques. Graph modification problems\nappear in numerous applications, including the analysis of biological and\nsocial networks. Typically, graph modification problems are NP-hard, making\nthem natural candidates for parameterized complexity studies. We discuss\nseveral fruitful interactions between the development of fixed-parameter\nalgorithms and the design of heuristics for graph modification problems,\nfeaturing quite different aspects of mutual benefits.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 10:50:28 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Komusiewicz", "Christian", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1606.03427", "submitter": "Moritz von Looz-Corswarem", "authors": "Moritz von Looz and Mario Wolter and Christoph R. Jacob and Henning\n  Meyerhenke", "title": "Better partitions of protein graphs for subsystem quantum chemistry", "comments": "14 pages main part, 6 pages appendix. Published in the Proceedings of\n  the 15th International Symposium on Experimental Algorithms (SEA 2016)", "journal-ref": "Lecture Notes in Computer Science, Volume 9685, 2016, pp 353-368", "doi": "10.1007/978-3-319-38851-9_24", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the interaction strength between proteins and small molecules is\nkey to analyzing their biological function. Quantum-mechanical calculations\nsuch as \\emph{Density Functional Theory} (DFT) give accurate and theoretically\nwell-founded results. With common implementations the running time of DFT\ncalculations increases quadratically with molecule size. Thus, numerous\nsubsystem-based approaches have been developed to accelerate quantum-chemical\ncalculations. These approaches partition the protein into different fragments,\nwhich are treated separately. Interactions between different fragments are\napproximated and introduce inaccuracies in the calculated interaction energies.\n  To minimize these inaccuracies, we represent the amino acids and their\ninteractions as a weighted graph in order to apply graph partitioning. None of\nthe existing graph partitioning work can be directly used, though, due to the\nunique constraints in partitioning such protein graphs. We therefore present\nand evaluate several algorithms, partially building upon established concepts,\nbut adapted to handle the new constraints. For the special case of partitioning\na protein along the main chain, we also present an efficient dynamic\nprogramming algorithm that yields provably optimal results. In the general\nscenario our algorithms usually improve the previous approach significantly and\ntake at most a few seconds.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 18:59:39 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["von Looz", "Moritz", ""], ["Wolter", "Mario", ""], ["Jacob", "Christoph R.", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1606.03626", "submitter": "Maximilien Burq", "authors": "Itai Ashlagi, Maximillien Burq, Patrick Jaillet, Vahideh Manshadi", "title": "On Matching and Thickness in Heterogeneous Dynamic Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic matching in an infinite-horizon stochastic market. While all\nagents are potentially compatible with each other, some are hard-to-match and\nothers are easy-to-match. Agents prefer to be matched as soon as possible and\nmatches are formed either bilaterally or indirectly through chains. We adopt an\nasymptotic approach and compute tight bounds on the limit of waiting time of\nagents under myopic policies that differ in matching technology and\nprioritization.\n  We find that the market composition is a key factor in the desired matching\ntechnology and prioritization level. When hard-to-match agents arrive less\nfrequently than easy-to-match ones (i) bilateral matching is almost as\nefficient as chains (waiting times scale similarly under both, though chains\nalways outperform bilateral matching by a constant factor), and (ii) assigning\npriorities to hard-to-match agents improves their waiting times. When\nhard-to-match agents arrive more frequently, chains are much more efficient\nthan bilateral matching and prioritization has no impact.\n  We further conduct comparative statics on arrival rates. Somewhat\nsurprisingly, we find that in a heterogeneous market and under bilateral\nmatching, increasing arrival rate has a non-monotone effect on waiting times,\ndue to the fact that, under some market compositions, there is an adverse\neffect of competition. Our comparative statics shed light on the impact of\nmerging markets and attracting altruistic agents (that initiate chains) or\neasy-to-match agents.\n  This work uncovers fundamental differences between heterogeneous and\nhomogeneous dynamic markets, and potentially helps policy makers to generate\ninsights on the operations of matching markets such as kidney exchange\nprograms.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 21:10:21 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:36:25 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Ashlagi", "Itai", ""], ["Burq", "Maximillien", ""], ["Jaillet", "Patrick", ""], ["Manshadi", "Vahideh", ""]]}, {"id": "1606.03680", "submitter": "Radoslava Kraleva Dr.", "authors": "Velin Kralev, Radoslava Kraleva", "title": "Variable Neighborhood Search Based Algorithm for University Course\n  Timetabling Problem", "comments": "13 pages, 4 figures, 5 tables, in proc. of the Fifth International\n  Scientific Conference 2013 (FMNS2013), pp. 202-214, Bulgaria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper a variable neighborhood search approach as a method for solving\ncombinatoric optimization problems is presented. A variable neighborhood search\nbased algorithm for solving the problem concerning the university course\ntimetable design has been developed. This algorithm is used to solve the real\nproblem regarding the university course timetable design. It is compared with\nother algorithms that are tested on the same sets of input data. The object and\nthe methodology of study are presented. The main objectives of the experiment\nare formulated. The conditions for conducting the experiment are specified. The\nresults are analyzed and appropriate conclusions are made. The future trends of\nwork in this field are presented.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 08:20:18 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Kralev", "Velin", ""], ["Kraleva", "Radoslava", ""]]}, {"id": "1606.03800", "submitter": "Oscar Morales-Ponce", "authors": "Oscar Morales-Ponce and Burkhard Englert and Mehrdad Aliasgari", "title": "Optimal Queue Length in Torus Networks with Traffic Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the need for efficient traffic control in urban areas, we study\nthe problem of minimizing the longest queue length in traffic networks. In our\nmodel cars only move forward and do not overtake each other and time is divided\nin rounds of a given length. Cars always cross intersections when they have the\ngreen signal and they are at the front of the queue, otherwise they wait. Each\ncar can cross at most one intersection in each round. The only means to control\nthe movement of the cars is by scheduling the green times at the junctions,\nthat allow cars to cross. The objective is to determine the green time\nschedules to reach a pattern that attains the social optimum, i.e., the minimum\npossible longest queue length in the network.\n  We study this problem in an oriented torus of dimension $n\\times n$ where\neach horizontal and vertical ring has $kn$ cars arbitrarily deployed. We\nintroduce the concept of conflict graphs that represent the propagated negative\nimpact across the network when -through green time assignments - the queue\nlength in one segment is reduced. Using these graphs, we provide a lower bound\non the longest queue length that any algorithm can attain and show that\nmaximizing the number of cars that cross the intersections in every round can\nprevent reaching the minimum possible longest queue length. Next, we propose a\nquadratic running time algorithm that minimizes the longest queue length in\neach round assuming that the number of cars in each queue is at least the size\nof the round. Our technique is based on network flooding on the conflict\ngraphs. Finally, we propose an algorithm that guarantees that the cars\neventually form a social optimum queue length at the expense of having some\ngreen times where no cars cross an intersection.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 03:18:56 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Morales-Ponce", "Oscar", ""], ["Englert", "Burkhard", ""], ["Aliasgari", "Mehrdad", ""]]}, {"id": "1606.03881", "submitter": "Jeffrey Shallit", "authors": "Jeffrey Shallit", "title": "Length of the continued logarithm algorithm on rational inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continued logarithm algorithm was introduced by Gosper around 1978, and\nrecently studied by Borwein, Calkin, Lindstrom, and Mattingly. In this note I\nshow that the continued logarithm algorithm terminates in at most 2 log_2 p +\nO(1) steps on input a rational number p/q >= 1. Furthermore, this bound is\ntight, up to an additive constant.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 10:15:10 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 18:55:52 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Shallit", "Jeffrey", ""]]}, {"id": "1606.03894", "submitter": "Amine Balafrej", "authors": "Amine Balafrej and Xavier Lorca and Charlotte Truchet", "title": "A Probabilistic-Based Model for Binary CSP", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": "16/2/DAPI", "categories": "cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a probabilistic-based model for binary CSP that provides\na fine grained analysis of its internal structure. Assuming that a domain\nmodification could occur in the CSP, it shows how to express, in a predictive\nway, the probability that a domain value becomes inconsistent, then it express\nthe expectation of the number of arc-inconsistent values in each domain of the\nconstraint network. Thus, it express the expectation of the number of\narc-inconsistent values for the whole constraint network. Next, it provides\nbounds for each of these three probabilistic indicators. Finally, a polytime\nalgorithm, which propagates the probabilistic information, is presented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 11:03:26 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Balafrej", "Amine", ""], ["Lorca", "Xavier", ""], ["Truchet", "Charlotte", ""]]}, {"id": "1606.03897", "submitter": "Joong Chae Na", "authors": "Joong Chae Na, Hyunjoon Kim, Seunghwan Min, Heejin Park, Thierry\n  Lecroq, Martine Leonard, Laurent Mouchardd, Kunsoo Park", "title": "FM-index of Alignment with Gaps", "comments": "15pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a compressed index for similar strings, called the FM-index of\nalignment (FMA), has been proposed with the functionalities of pattern search\nand random access. The FMA is quite efficient in space requirement and pattern\nsearch time, but it is applicable only for an alignment of similar strings\nwithout gaps. In this paper we propose the FM-index of alignment with gaps, a\nrealistic index for similar strings, which allows gaps in their alignment. For\nthis, we design a new version of the suffix array of alignment by using\nalignment transformation and a new definition of the alignment-suffix. The new\nsuffix array of alignment enables us to support the LF-mapping and backward\nsearch, the key functionalities of the FM-index, regardless of gap existence in\nthe alignment. We experimentally compared our index with RLCSA due to Makinen\net al. on 100 genome sequences from the 1000 Genomes Project. The index size of\nour index is less than one third of that of RLCSA.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 11:23:45 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Na", "Joong Chae", ""], ["Kim", "Hyunjoon", ""], ["Min", "Seunghwan", ""], ["Park", "Heejin", ""], ["Lecroq", "Thierry", ""], ["Leonard", "Martine", ""], ["Mouchardd", "Laurent", ""], ["Park", "Kunsoo", ""]]}, {"id": "1606.04042", "submitter": "Nicolai Diethelm", "authors": "Nicolai Diethelm", "title": "Randomized Ternary Search Tries", "comments": "6 pages; v20: minor clarification in the \"Analysis\" section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new kind of self-balancing ternary search trie that\nuses a randomized balancing strategy adapted from Aragon and Seidel's\nrandomized binary search trees (\"treaps\"). After any sequence of insertions and\ndeletions of strings, the tree looks like a ternary search trie built by\ninserting strings in random order. As a result, the time cost of searching,\ninserting, or deleting a string of length k in a tree with n strings is at most\nO(k + log n) with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 17:26:52 GMT"}, {"version": "v10", "created": "Thu, 22 Sep 2016 19:26:53 GMT"}, {"version": "v11", "created": "Mon, 24 Oct 2016 21:03:47 GMT"}, {"version": "v12", "created": "Thu, 27 Oct 2016 19:57:02 GMT"}, {"version": "v13", "created": "Wed, 30 Nov 2016 20:12:53 GMT"}, {"version": "v14", "created": "Thu, 1 Dec 2016 19:47:17 GMT"}, {"version": "v15", "created": "Thu, 8 Dec 2016 18:56:36 GMT"}, {"version": "v16", "created": "Mon, 12 Dec 2016 08:29:53 GMT"}, {"version": "v17", "created": "Wed, 14 Dec 2016 17:58:13 GMT"}, {"version": "v18", "created": "Mon, 19 Dec 2016 19:59:56 GMT"}, {"version": "v19", "created": "Sat, 24 Dec 2016 10:16:46 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 18:47:45 GMT"}, {"version": "v20", "created": "Sat, 7 Jan 2017 17:31:32 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2016 19:48:20 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 14:55:58 GMT"}, {"version": "v5", "created": "Tue, 19 Jul 2016 17:55:29 GMT"}, {"version": "v6", "created": "Mon, 1 Aug 2016 07:01:01 GMT"}, {"version": "v7", "created": "Mon, 22 Aug 2016 16:45:47 GMT"}, {"version": "v8", "created": "Wed, 31 Aug 2016 16:33:01 GMT"}, {"version": "v9", "created": "Thu, 15 Sep 2016 19:55:13 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Diethelm", "Nicolai", ""]]}, {"id": "1606.04157", "submitter": "Guohui Lin", "authors": "Wenchang Luo, Yao Xu, Weitian Tong, Guohui Lin", "title": "Single machine scheduling with job-dependent machine deterioration", "comments": "15 pages", "journal-ref": "Proceedings of ISAAC 2016, LIPIcs55, pages 1-13", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the single machine scheduling problem with job-dependent machine\ndeterioration. In the problem, we are given a single machine with an initial\nnon-negative maintenance level, and a set of jobs each with a non-preemptive\nprocessing time and a machine deterioration. Such a machine deterioration\nquantifies the decrement in the machine maintenance level after processing the\njob. To avoid machine breakdown, one should guarantee a non-negative\nmaintenance level at any time point; and whenever necessary, a maintenance\nactivity must be allocated for restoring the machine maintenance level. The\ngoal of the problem is to schedule the jobs and the maintenance activities such\nthat the total completion time of jobs is minimized. There are two variants of\nmaintenance activities: in the partial maintenance case each activity can be\nallocated to increase the machine maintenance level to any level not exceeding\nthe maximum; in the full maintenance case every activity must be allocated to\nincrease the machine maintenance level to the maximum. In a recent work, the\nproblem in the full maintenance case has been proven NP-hard; several special\ncases of the problem in the partial maintenance case were shown solvable in\npolynomial time, but the complexity of the general problem is left open. In\nthis paper we first prove that the problem in the partial maintenance case is\nNP-hard, thus settling the open problem; we then design a $2$-approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:21:04 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Luo", "Wenchang", ""], ["Xu", "Yao", ""], ["Tong", "Weitian", ""], ["Lin", "Guohui", ""]]}, {"id": "1606.04225", "submitter": "Vincenzo Bonifaci", "authors": "Vincenzo Bonifaci", "title": "A revised model of fluid transport optimization in Physarum polycephalum", "comments": "To appear in Journal of Mathematical Biology", "journal-ref": "Journal of Mathematical Biology, 74(3):567-581, 2017", "doi": "10.1007/s00285-016-1036-y", "report-no": null, "categories": "q-bio.TO cs.DS cs.ET math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of fluid transport in the slime mold Physarum polycephalum has\nbeen the subject of several modeling efforts in recent literature. Existing\nmodels assume that the tube adaptation mechanism in P. polycephalum's tubular\nnetwork is controlled by the sheer amount of fluid flow through the tubes. We\nput forward the hypothesis that the controlling variable may instead be the\nflow's pressure gradient along the tube. We carry out the stability analysis of\nsuch a revised mathematical model for a parallel-edge network, proving that the\nrevised model supports the global flow-optimizing behavior of the slime mold\nfor a substantially wider class of response functions compared to previous\nmodels. Simulations also suggest that the same conclusion may be valid for\narbitrary network topologies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:25:49 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Bonifaci", "Vincenzo", ""]]}, {"id": "1606.04269", "submitter": "Alasdair Thomason", "authors": "Alasdair Thomason, Nathan Griffiths, Victor Sanchez", "title": "Context Trees: Augmenting Geospatial Trajectories with Context", "comments": null, "journal-ref": "ACM Transactions on Information Systems 2016 35(2) 14:1-14:37", "doi": "10.1145/2978578", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposing latent knowledge in geospatial trajectories has the potential to\nprovide a better understanding of the movements of individuals and groups.\nMotivated by such a desire, this work presents the context tree, a new\nhierarchical data structure that summarises the context behind user actions in\na single model. We propose a method for context tree construction that augments\ngeospatial trajectories with land usage data to identify such contexts. Through\nevaluation of the construction method and analysis of the properties of\ngenerated context trees, we demonstrate the foundation for understanding and\nmodelling behaviour afforded. Summarising user contexts into a single data\nstructure gives easy access to information that would otherwise remain latent,\nproviding the basis for better understanding and predicting the actions and\nbehaviours of individuals and groups. Finally, we also present a method for\npruning context trees, for use in applications where it is desirable to reduce\nthe size of the tree while retaining useful information.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:27:29 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Thomason", "Alasdair", ""], ["Griffiths", "Nathan", ""], ["Sanchez", "Victor", ""]]}, {"id": "1606.04495", "submitter": "Travis Gagie", "authors": "Djamal Belazzougui, Travis Gagie, J. Ian Munro, Gonzalo Navarro and\n  Yakov Nekrich", "title": "Range Majorities and Minorities in Arrays", "comments": "arXiv admin note: substantial text overlap with arXiv:1210.1765", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karpinski and Nekrich (2008) introduced the problem of parameterized range\nmajority, which asks us to preprocess a string of length $n$ such that, given\nthe endpoints of a range, one can quickly find all the distinct elements whose\nrelative frequencies in that range are more than a threshold $\\tau$. Subsequent\nauthors have reduced their time and space bounds such that, when $\\tau$ is\nfixed at preprocessing time, we need either $O(n \\log (1 / \\tau))$ space and\noptimal $O(1 / \\tau)$ query time or linear space and $O((1 / \\tau) \\log \\log\n\\sigma)$ query time, where $\\sigma$ is the alphabet size. In this paper we give\nthe first linear-space solution with optimal $O(1 / \\tau)$ query time, even\nwith variable $\\tau$ (i.e., specified with the query). For the case when\n$\\sigma$ is polynomial on the computer word size, our space is optimally\ncompressed according to the symbol frequencies in the string. Otherwise, either\nthe compressed space is increased by an arbitrarily small constant factor or\nthe time rises to any function in $(1/\\tau)\\cdot\\omega(1)$. We obtain the same\nresults on the complementary problem of parameterized range minority introduced\nby Chan et al. (2015), who had achieved linear space and $O(1 / \\tau)$ query\ntime with variable $\\tau$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 18:47:01 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Gagie", "Travis", ""], ["Munro", "J. Ian", ""], ["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1606.04573", "submitter": "Marcin Pi\\k{a}tkowski", "authors": "Juha K\\\"arkk\\\"ainen, Marcin Pi\\k{a}tkowski, Simon J. Puglisi", "title": "String Inference from the LCP Array", "comments": "Added algorithm for general alphabets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix array, perhaps the most important data structure in modern string\nprocessing, is often augmented with the longest common prefix (LCP) array which\nstores the lengths of the LCPs for lexicographically adjacent suffixes of a\nstring. Together the two arrays are roughly equivalent to the suffix tree with\nthe LCP array representing the tree shape.\n  In order to better understand the combinatorics of LCP arrays, we consider\nthe problem of inferring a string from an LCP array, i.e., determining whether\na given array of integers is a valid LCP array, and if it is, reconstructing\nsome string or all strings with that LCP array. There are recent studies of\ninferring a string from a suffix tree shape but using significantly more\ninformation (in the form of suffix links) than is available in the LCP array.\n  We provide two main results. (1) We describe two algorithms for inferring\nstrings from an LCP array when we allow a generalized form of LCP array defined\nfor a multiset of cyclic strings: a linear time algorithm for binary alphabet\nand a general algorithm with polynomial time complexity for a constant alphabet\nsize. (2) We prove that determining whether a given integer array is a valid\nLCP array is NP-complete when we require more restricted forms of LCP array\ndefined for a single cyclic or non-cyclic string or a multiset of non-cyclic\nstrings. The result holds whether or not the alphabet is restricted to be\nbinary. In combination, the two results show that the generalized form of LCP\narray for a multiset of cyclic strings is fundamentally different from the\nother more restricted forms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 21:21:17 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 21:32:38 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Juha", ""], ["Pi\u0105tkowski", "Marcin", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1606.04592", "submitter": "Anand Kumar Narayanan", "authors": "Zeyu Guo, Anand Kumar Narayanan and Chris Umans", "title": "Algebraic Problems Equivalent to Beating Exponent 3/2 for Polynomial\n  Factorization over Finite Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fastest known algorithm for factoring univariate polynomials over finite\nfields is the Kedlaya-Umans (fast modular composition) implementation of the\nKaltofen-Shoup algorithm. It is randomized and takes $\\widetilde{O}(n^{3/2}\\log\nq + n \\log^2 q)$ time to factor polynomials of degree $n$ over the finite field\n$\\mathbb{F}_q$ with $q$ elements. A significant open problem is if the $3/2$\nexponent can be improved. We study a collection of algebraic problems and\nestablish a web of reductions between them. A consequence is that an algorithm\nfor any one of these problems with exponent better than $3/2$ would yield an\nalgorithm for polynomial factorization with exponent better than $3/2$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 23:32:48 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Guo", "Zeyu", ""], ["Narayanan", "Anand Kumar", ""], ["Umans", "Chris", ""]]}, {"id": "1606.04679", "submitter": "Frank Kammer", "authors": "Frank Kammer, Dieter Kratsch and Moritz Laudahn", "title": "Space-Efficient Biconnected Components and Recognition of Outerplanar\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present space-efficient algorithms for computing cut vertices in a given\ngraph with $n$ vertices and $m$ edges in linear time using $O(n+\\min\\{m,n\\log\n\\log n\\})$ bits. With the same time and using $O(n+m)$ bits, we can compute the\nbiconnected components of a graph. We use this result to show an algorithm for\nthe recognition of (maximal) outerplanar graphs in $O(n\\log \\log n)$ time using\n$O(n)$ bits.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 08:44:16 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 18:33:00 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kammer", "Frank", ""], ["Kratsch", "Dieter", ""], ["Laudahn", "Moritz", ""]]}, {"id": "1606.04696", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee, Santosh S. Vempala", "title": "Geodesic Walks in Polytopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.DG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the geodesic walk for sampling Riemannian manifolds and apply it\nto the problem of generating uniform random points from polytopes in R^n\nspecified by m inequalities. The walk is a discrete-time simulation of a\nstochastic differential equation (SDE) on the Riemannian manifold equipped with\nthe metric induced by the Hessian of a convex function; each step is the\nsolution of an ordinary differential equation (ODE). The resulting sampling\nalgorithm for polytopes mixes in O*(mn^{3/4}) steps. This is the first walk\nthat breaks the quadratic barrier for mixing in high dimension, improving on\nthe previous best bound of O*(mn) by Kannan and Narayanan for the Dikin walk.\nWe also show that each step of the geodesic walk (solving an ODE) can be\nimplemented efficiently, thus improving the time complexity for sampling\npolytopes. Our analysis of the geodesic walk for general Hessian manifolds does\nnot assume positive curvature and might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 09:40:39 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 19:07:42 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 06:47:11 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Lee", "Yin Tat", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1606.04718", "submitter": "Sankardeep Chakraborty", "authors": "Niranka Banerjee, Sankardeep Chakraborty, Venkatesh Raman, Srinivasa\n  Rao Satti", "title": "Improved Space efficient linear time algorithms for BFS, DFS and\n  applications", "comments": "A preliminary version of this paper appears in the proceedings of\n  COCOON 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Elmasry et al. (STACS 2015) and Asano et al. (ISAAC 2014),\nreconsidered classical fundamental graph algorithms focusing on improving the\nspace complexity. We continue this line of work focusing on space. Our first\nresult is a simple data structure that can maintain any subset $S$ of a\nuniverse of $n$ elements using $n+o(n)$ bits and support in constant time,\napart from the standard insert, delete and membership queries, the operation\n{\\it findany} that finds and returns any element of the set (or outputs that\nthe set is empty). Using this we give a BFS implementation that takes $O(m+n)$\ntime using at most $2n+o(n)$ bits. Later, we further improve the space\nrequirement of BFS to at most $1.585n + o(n)$ bits. We demonstrate the use of\nour data structure by developing another data structure using it that can\nrepresent a sequence of $n$ non-negative integers $x_1, x_2, \\ldots x_n$ using\nat most $\\sum_{i=1}^n x_i + 2n + o(\\sum_{i=1}^n x_i+n)$ bits and, in constant\ntime, determine whether the $i$-th element is $0$ or decrement it otherwise. We\nalso discuss an algorithm for finding a minimum weight spanning tree of a\nweighted undirected graph using at most $n+o(n)$ bits. We also provide an\nimplementation for DFS that takes $O(m+n)$ time and $O(n \\lg(m/n))$ bits. Using\nthis DFS algorithm and other careful implementations, we can test\nbiconnectivity, 2-edge connectivity, and determine cut vertices, bridges etc\namong others, essentially within the same time and space bounds required for\nDFS. These improve the space required for earlier implementations from $\\Omega\n(n\\lg n)$ bits.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 10:51:14 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 11:36:54 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 07:02:49 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Banerjee", "Niranka", ""], ["Chakraborty", "Sankardeep", ""], ["Raman", "Venkatesh", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1606.04746", "submitter": "Vincenzo Gulisano", "authors": "Vincenzo Gulisano, Yiannis Nikolakopoulos, Daniel Cederman, Marina\n  Papatriantafilou and Philippas Tsigas", "title": "Efficient data streaming multiway aggregation through concurrent\n  algorithmic designs and new abstract data types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data streaming relies on continuous queries to process unbounded streams of\ndata in a real-time fashion. It is commonly demanding in computation capacity,\ngiven that the relevant applications involve very large volumes of data. Data\nstructures act as articulation points and maintain the state of data streaming\noperators, potentially supporting high parallelism and balancing the work\nbetween them. Prompted by this fact, in this work we study and analyze\nparallelization needs of these articulation points, focusing on the problem of\nstreaming multiway aggregation, where large data volumes are received from\nmultiple input streams. The analysis of the parallelization needs, as well as\nof the use and limitations of existing aggregate designs and their data\nstructures, leads us to identify needs for proper shared objects that can\nachieve low-latency and high throughput multiway aggregation. We present the\nrequirements of such objects as abstract data types and we provide efficient\nlock-free linearizable algorithmic implementations of them, along with new\nmultiway aggregate algorithmic designs that leverage them, supporting both\ndeterministic order-sensitive and order-insensitive aggregate functions.\nFurthermore, we point out future directions that open through these\ncontributions. The paper includes an extensive experimental study, based on a\nvariety of aggregation continuous queries on two large datasets extracted from\nSoundCloud, a music social network, and from a Smart Grid network. In all the\nexperiments, the proposed data structures and the enhanced aggregate operators\nimproved the processing performance significantly, up to one order of\nmagnitude, in terms of both throughput and latency, over the commonly-used\ntechniques based on queues.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:01:38 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Gulisano", "Vincenzo", ""], ["Nikolakopoulos", "Yiannis", ""], ["Cederman", "Daniel", ""], ["Papatriantafilou", "Marina", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1606.04763", "submitter": "V\\'aclav Bla\\v{z}ej", "authors": "V\\'aclav Bla\\v{z}ej, Ond\\v{r}ej Such\\'y, Tom\\'a\\v{s} Valla", "title": "A Simple Streaming Bit-parallel Algorithm for Swap Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pattern matching problem with swaps is to find all occurrences of a\npattern in a text while allowing the pattern to swap adjacent symbols. The goal\nis to design fast matching algorithm that takes advantage of the bit\nparallelism of bitwise machine instructions and has only streaming access to\nthe input. We introduce a new approach to solve this problem based on the graph\ntheoretic model and compare its performance to previously known algorithms. We\nalso show that an approach using deterministic finite automata cannot achieve\nsimilarly efficient algorithms. Furthermore, we describe a fatal flaw in some\nof the previously published algorithms based on the same model. Finally, we\nprovide experimental evaluation of our algorithm on real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:57:26 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 14:05:51 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 11:52:39 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Bla\u017eej", "V\u00e1clav", ""], ["Such\u00fd", "Ond\u0159ej", ""], ["Valla", "Tom\u00e1\u0161", ""]]}, {"id": "1606.04978", "submitter": "Rosiane De Freitas Rodrigues", "authors": "Rosiane de Freitas, Bruno Dias, Nelson Maculan, Jayme Szwarcfiter", "title": "Distance geometry approach for special graph coloring problems", "comments": "24 pages, 14 figures, 9 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important combinatorial optimization problems is graph\ncoloring. There are several variations of this problem involving additional\nconstraints either on vertices or edges. They constitute models for real\napplications, such as channel assignment in mobile wireless networks. In this\nwork, we consider some coloring problems involving distance constraints as\nweighted edges, modeling them as distance geometry problems. Thus, the vertices\nof the graph are considered as embedded on the real line and the coloring is\ntreated as an assignment of positive integers to the vertices, while the\ndistances correspond to line segments, where the goal is to find a feasible\nintersection of them. We formulate different such coloring problems and show\nfeasibility conditions for some problems. We also propose implicit enumeration\nmethods for some of the optimization problems based on branch-and-prune methods\nproposed for distance geometry problems in the literature. An empirical\nanalysis was undertaken, considering equality and inequality constraints,\nuniform and arbitrary set of distances, and the performance of each variant of\nthe method considering the handling and propagation of the set of distances\ninvolved.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 20:54:04 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["de Freitas", "Rosiane", ""], ["Dias", "Bruno", ""], ["Maculan", "Nelson", ""], ["Szwarcfiter", "Jayme", ""]]}, {"id": "1606.05031", "submitter": "Yasuo Tabei", "authors": "Yasuo Tabei, Hiroto Saigo, Yoshihiro Yamanishi and Simon J. Puglisi", "title": "Scalable Partial Least Squares Regression on Grammar-Compressed Data\n  Matrices", "comments": "To be appeared in the Proceedings of KDD'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With massive high-dimensional data now commonplace in research and industry,\nthere is a strong and growing demand for more scalable computational techniques\nfor data analysis and knowledge discovery. Key to turning these data into\nknowledge is the ability to learn statistical models with high\ninterpretability. Current methods for learning statistical models either\nproduce models that are not interpretable or have prohibitive computational\ncosts when applied to massive data. In this paper we address this need by\npresenting a scalable algorithm for partial least squares regression (PLS),\nwhich we call compression-based PLS (cPLS), to learn predictive linear models\nwith a high interpretability from massive high-dimensional data. We propose a\nnovel grammar-compressed representation of data matrices that supports fast row\nand column access while the data matrix is in a compressed form. The original\ndata matrix is grammar-compressed and then the linear model in PLS is learned\non the compressed data matrix, which results in a significant reduction in\nworking space, greatly improving scalability. We experimentally test cPLS on\nits ability to learn linear models for classification, regression and feature\nextraction with various massive high-dimensional data, and show that cPLS\nperforms superiorly in terms of prediction accuracy, computational efficiency,\nand interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 02:55:39 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Tabei", "Yasuo", ""], ["Saigo", "Hiroto", ""], ["Yamanishi", "Yoshihiro", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1606.05119", "submitter": "Nobutaka Shimizu", "authors": "Nobutaka Shimizu, Ryuhei Mori", "title": "Average Shortest Path Length of Graphs of Diameter 3", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network topology with low average shortest path length (ASPL) provides\nefficient data transmission while the number of nodes and the number of links\nincident to each node are often limited due to physical constraints. In this\npaper, we consider the construction of low ASPL graphs under these constraints\nby using stochastic local search (SLS) algorithms. Since the ASPL cannot be\ncalculated efficiently, the ASPL is not suitable for the evaluation function of\nSLS algorithms. We first derive an equality and bounds for the ASPL of graphs\nof diameter 3. Then, we propose use the simpliest upper bound represented by\nthe number of triangles and squares in the graph as an evaluation function for\ngraphs of diameter 3. We show that the proposed evaluation function can be\nevaluated in O(1) time as the number of nodes and the maximum degree tend to\ninfinity by using some data tables. By using the simulated annealing with the\nproposed evaluation function, we construct low ASPL regular graphs of diameter\n3 with 10 000 nodes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 10:04:18 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Shimizu", "Nobutaka", ""], ["Mori", "Ryuhei", ""]]}, {"id": "1606.05123", "submitter": "Anthony Kleerekoper", "authors": "Anthony Kleerekoper", "title": "Revisiting the Majority Problem: Average-Case Analysis with Arbitrarily\n  Many Colours", "comments": "13 pages (16 with appendices), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority problem is a special case of the heavy hitters problem. Given a\ncollection of coloured balls, the task is to identify the majority colour or\nstate that no such colour exists. Whilst the special case of two-colours has\nbeen well studied, the average-case performance for arbitrarily many colours\nhas not. In this paper, we present heuristic analysis of the average-case\nperformance of three deterministic algorithms that appear in the literature. We\nempirically validate our analysis with large scale simulations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 10:12:55 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 10:40:52 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kleerekoper", "Anthony", ""]]}, {"id": "1606.05183", "submitter": "Jean Honorio", "authors": "Zhaosen Wang, Jean Honorio", "title": "Reconstructing a Bounded-Degree Directed Tree Using Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized algorithm for reconstructing directed rooted trees of\n$n$ nodes and node degree at most $d$, by asking at most $O(dn\\log^2 n)$ path\nqueries. Each path query takes as input an origin node and a target node, and\nanswers whether there is a directed path from the origin to the target.\nRegarding lower bounds, we show that any randomized algorithm requires at least\n$\\Omega(n\\log n)$ queries, while any deterministic algorithm requires at least\n$\\Omega(dn)$ queries. Additionally, we present a $O(dn\\log^3 n)$ randomized\nalgorithm for noisy queries, and a $O(dn\\log^2 n)$ randomized algorithm for\nadditive queries on weighted trees.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 13:40:38 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 08:48:19 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 22:54:15 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Wang", "Zhaosen", ""], ["Honorio", "Jean", ""]]}, {"id": "1606.05198", "submitter": "Seeun William Umboh", "authors": "Nikhil Bansal, Daniel Reichman, Seeun William Umboh", "title": "LP-Based Robust Algorithms for Noisy Minor-Free and Bounded Treewidth\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a general approach for solving optimization problems on noisy minor\nfree graphs, where a \\delta-fraction of edges and vertices are adversarially\ncorrupted. The noisy setting was first considered by Magen and Moharrami and\nthey gave a (1 + \\delta)-estimation algorithm for the independent set problem.\nLater, Chan and Har-Peled designed a local search algorithm that finds a (1 +\nO(\\delta))-approximate independent set. However, nothing was known regarding\nother problems in the noisy setting. Our main contribution is a general\nLP-based framework that yields a (1 + O(\\delta log m log log m))-approximation\nalgorithm for noisy MAX-k-CSPs on m clauses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:23:23 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 02:41:40 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Bansal", "Nikhil", ""], ["Reichman", "Daniel", ""], ["Umboh", "Seeun William", ""]]}, {"id": "1606.05210", "submitter": "Lene M. Favrholdt", "authors": "Joan Boyar, Lene M. Favrholdt, Christian Kudahl and Jesper W.\n  Mikkelsen", "title": "Weighted Online Problems with Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the first online complexity class, AOC, was introduced. The class\nconsists of many online problems where each request must be either accepted or\nrejected, and the aim is to either minimize or maximize the number of accepted\nrequests, while maintaining a feasible solution. All AOC-complete problems\n(including Independent Set, Vertex Cover, Dominating Set, and Set Cover) have\nessentially the same advice complexity. In this paper, we study weighted\nversions of problems in AOC, i.e., each request comes with a weight and the aim\nis to either minimize or maximize the total weight of the accepted requests. In\ncontrast to the unweighted versions, we show that there is a significant\ndifference in the advice complexity of complete minimization and maximization\nproblems. We also show that our algorithmic techniques for dealing with\nweighted requests can be extended to work for non-complete AOC problems such as\nmaximum matching (giving better results than what follow from the general AOC\nresults) and even non-AOC problems such as scheduling.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 14:47:04 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 11:43:21 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Boyar", "Joan", ""], ["Favrholdt", "Lene M.", ""], ["Kudahl", "Christian", ""], ["Mikkelsen", "Jesper W.", ""]]}, {"id": "1606.05225", "submitter": "Aaron Sidford", "authors": "Michael B. Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, Aaron\n  Sidford", "title": "Geometric Median in Nearly Linear Time", "comments": "Symposium on Theory of Computing (STOC) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide faster algorithms for solving the geometric median\nproblem: given $n$ points in $\\mathbb{R}^{d}$ compute a point that minimizes\nthe sum of Euclidean distances to the points. This is one of the oldest\nnon-trivial problems in computational geometry yet despite an abundance of\nresearch the previous fastest algorithms for computing a\n$(1+\\epsilon)$-approximate geometric median were $O(d\\cdot\nn^{4/3}\\epsilon^{-8/3})$ by Chin et. al,\n$\\tilde{O}(d\\exp{\\epsilon^{-4}\\log\\epsilon^{-1}})$ by Badoiu et. al,\n$O(nd+\\mathrm{poly}(d,\\epsilon^{-1})$ by Feldman and Langberg, and\n$O((nd)^{O(1)}\\log\\frac{1}{\\epsilon})$ by Parrilo and Sturmfels and Xue and Ye.\n  In this paper we show how to compute a $(1+\\epsilon)$-approximate geometric\nmedian in time $O(nd\\log^{3}\\frac{1}{\\epsilon})$ and $O(d\\epsilon^{-2})$. While\nour $O(d\\epsilon^{-2})$ is a fairly straightforward application of stochastic\nsubgradient descent, our $O(nd\\log^{3}\\frac{1}{\\epsilon})$ time algorithm is a\nnovel long step interior point method. To achieve this running time we start\nwith a simple $O((nd)^{O(1)}\\log\\frac{1}{\\epsilon})$ time interior point method\nand show how to improve it, ultimately building an algorithm that is quite\nnon-standard from the perspective of interior point literature. Our result is\none of very few cases we are aware of outperforming traditional interior point\ntheory and the only we are aware of using interior point methods to obtain a\nnearly linear time algorithm for a canonical optimization problem that\ntraditionally requires superlinear time. We hope our work leads to further\nimprovements in this line of research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:34:58 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Cohen", "Michael B.", ""], ["Lee", "Yin Tat", ""], ["Miller", "Gary", ""], ["Pachocki", "Jakub", ""], ["Sidford", "Aaron", ""]]}, {"id": "1606.05240", "submitter": "Brahim Chaourar", "authors": "Brahim Chaourar", "title": "A linear time algorithm for a variant of the max cut problem in series\n  parallel graphs", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V, E)$, a connected sides cut $(U, V\\backslash U)$ or\n$\\delta (U)$ is the set of edges of E linking all vertices of U to all vertices\nof $V\\backslash U$ such that the induced subgraphs $G[U]$ and $G[V\\backslash\nU]$ are connected. Given a positive weight function $w$ defined on $E$, the\nmaximum connected sides cut problem (MAX CS CUT) is to find a connected sides\ncut $\\Omega$ such that $w(\\Omega)$ is maximum. MAX CS CUT is NP-hard. In this\npaper, we give a linear time algorithm to solve MAX CS CUT for series parallel\ngraphs. We deduce a linear time algorithm for the minimum cut problem in the\nsame class of graphs without computing the maximum flow.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 07:10:04 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 09:36:32 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Chaourar", "Brahim", ""]]}, {"id": "1606.05289", "submitter": "Joern Hees", "authors": "J\\\"orn Hees, Benjamin Adrian, Ralf Biedert, Thomas Roth-Berghofer,\n  Andreas Dengel", "title": "TSSort: Probabilistic Noise Resistant Sorting", "comments": "10 pages, 2 figures, as of Jan. 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present TSSort, a probabilistic, noise resistant, quickly\nconverging comparison sort algorithm based on Microsoft TrueSkill. The\nalgorithm combines TrueSkill's updating rules with a newly developed next item\npair selection strategy, enabling it to beat standard sorting algorithms w.r.t.\nconvergence speed and noise resistance, as shown in simulations. TSSort is\nuseful if comparisons of items are expensive or noisy, or if intermediate\nresults shall be approximately ordered.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:41:55 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Hees", "J\u00f6rn", ""], ["Adrian", "Benjamin", ""], ["Biedert", "Ralf", ""], ["Roth-Berghofer", "Thomas", ""], ["Dengel", "Andreas", ""]]}, {"id": "1606.05374", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Gregory Valiant and Moses Charikar", "title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer\n  Prediction", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a crowdsourcing model in which $n$ workers are asked to rate the\nquality of $n$ items previously generated by other workers. An unknown set of\n$\\alpha n$ workers generate reliable ratings, while the remaining workers may\nbehave arbitrarily and possibly adversarially. The manager of the experiment\ncan also manually evaluate the quality of a small number of items, and wishes\nto curate together almost all of the high-quality items with at most an\n$\\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that\nthis is possible with an amount of work required of the manager, and each\nworker, that does not scale with $n$: the dataset can be curated with\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\alpha^3\\epsilon^4}\\Big)$ ratings per worker, and\n$\\tilde{O}\\Big(\\frac{1}{\\beta\\epsilon^2}\\Big)$ ratings by the manager, where\n$\\beta$ is the fraction of high-quality items. Our results extend to the more\ngeneral setting of peer prediction, including peer grading in online\nclassrooms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 21:45:14 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Valiant", "Gregory", ""], ["Charikar", "Moses", ""]]}, {"id": "1606.05535", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej\n  Cichocki", "title": "Tensor Ring Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks have in recent years emerged as the powerful tools for\nsolving the large-scale optimization problems. One of the most popular tensor\nnetwork is tensor train (TT) decomposition that acts as the building blocks for\nthe complicated tensor networks. However, the TT decomposition highly depends\non permutations of tensor dimensions, due to its strictly sequential\nmultilinear products over latent cores, which leads to difficulties in finding\nthe optimal TT representation. In this paper, we introduce a fundamental tensor\ndecomposition model to represent a large dimensional tensor by a circular\nmultilinear products over a sequence of low dimensional cores, which can be\ngraphically interpreted as a cyclic interconnection of 3rd-order tensors, and\nthus termed as tensor ring (TR) decomposition. The key advantage of TR model is\nthe circular dimensional permutation invariance which is gained by employing\nthe trace operation and treating the latent cores equivalently. TR model can be\nviewed as a linear combination of TT decompositions, thus obtaining the\npowerful and generalized representation abilities. For optimization of latent\ncores, we present four different algorithms based on the sequential SVDs, ALS\nscheme, and block-wise ALS techniques. Furthermore, the mathematical properties\nof TR model are investigated, which shows that the basic multilinear algebra\ncan be performed efficiently by using TR representaions and the classical\ntensor decompositions can be conveniently transformed into the TR\nrepresentation. Finally, the experiments on both synthetic signals and\nreal-world datasets were conducted to evaluate the performance of different\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 14:40:18 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhou", "Guoxu", ""], ["Xie", "Shengli", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1606.05538", "submitter": "Daniel Graf", "authors": "Andreas B\\\"artschi, Barbara Geissmann, Daniel Graf, Tomas Hruz, Paolo\n  Penna, and Thomas Tschager", "title": "On computing the total displacement number via weighted Motzkin paths", "comments": "19 pages. An extended abstract of this paper will be published at the\n  27th International Workshop on Combinatorial Algorithms 2016, IWOCA'16", "journal-ref": "27th International Workshop on Combinatorial Algorithms, IWOCA'16,\n  423-434, 2016", "doi": "10.1007/978-3-319-44543-4_33", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the number of permutations of a given total displacement is\nequivalent to counting weighted Motzkin paths of a given area (Guay-Paquet and\nPetersen, 2014). The former combinatorial problem is still open. In this work,\nwe show that this connection allows to construct efficient algorithms for\ncounting and for sampling such permutations. These algorithms provide a tool to\nbetter understand the original combinatorial problem. A by-product of our\napproach is a different way of counting based on certain building sequences for\nMotzkin paths, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 14:42:39 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["B\u00e4rtschi", "Andreas", ""], ["Geissmann", "Barbara", ""], ["Graf", "Daniel", ""], ["Hruz", "Tomas", ""], ["Penna", "Paolo", ""], ["Tschager", "Thomas", ""]]}, {"id": "1606.05608", "submitter": "Jukka Kohonen", "authors": "Matti Karppa and Petteri Kaski and Jukka Kohonen and Padraig \\'O\n  Cath\\'ain", "title": "Explicit correlation amplifiers for finding outlier correlations in\n  deterministic subquadratic time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derandomize G. Valiant's [J. ACM 62 (2015) Art. 13] subquadratic-time\nalgorithm for finding outlier correlations in binary data. Our derandomized\nalgorithm gives deterministic subquadratic scaling essentially for the same\nparameter range as Valiant's randomized algorithm, but the precise constants we\nsave over quadratic scaling are more modest. Our main technical tool for\nderandomization is an explicit family of correlation amplifiers built via a\nfamily of zigzag-product expanders in Reingold, Vadhan, and Wigderson [Ann. of\nMath. 155 (2002) 157--187]. We say that a function\n$f:\\{-1,1\\}^d\\rightarrow\\{-1,1\\}^D$ is a correlation amplifier with threshold\n$0\\leq\\tau\\leq 1$, error $\\gamma\\geq 1$, and strength $p$ an even positive\ninteger if for all pairs of vectors $x,y\\in\\{-1,1\\}^d$ it holds that (i)\n$|\\langle x,y\\rangle|<\\tau d$ implies $|\\langle\nf(x),f(y)\\rangle|\\leq(\\tau\\gamma)^pD$; and (ii) $|\\langle x,y\\rangle|\\geq\\tau\nd$ implies $\\bigl(\\frac{\\langle x,y\\rangle}{\\gamma d}\\bigr)^pD\n  \\leq\\langle f(x),f(y)\\rangle\\leq\n  \\bigl(\\frac{\\gamma\\langle x,y\\rangle}{d}\\bigr)^pD$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:50:09 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 06:05:51 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Karppa", "Matti", ""], ["Kaski", "Petteri", ""], ["Kohonen", "Jukka", ""], ["Cath\u00e1in", "Padraig \u00d3", ""]]}, {"id": "1606.05615", "submitter": "Yatao A. Bian", "authors": "Andrew An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, Andreas\n  Krause", "title": "Guaranteed Non-convex Optimization: Submodular Maximization over\n  Continuous Domains", "comments": "Appears in the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular continuous functions are a category of (generally)\nnon-convex/non-concave functions with a wide spectrum of applications. We\ncharacterize these functions and demonstrate that they can be maximized\nefficiently with approximation guarantees. Specifically, i) We introduce the\nweak DR property that gives a unified characterization of submodularity for all\nset, integer-lattice and continuous functions; ii) for maximizing monotone\nDR-submodular continuous functions under general down-closed convex\nconstraints, we propose a Frank-Wolfe variant with $(1-1/e)$ approximation\nguarantee, and sub-linear convergence rate; iii) for maximizing general\nnon-monotone submodular continuous functions subject to box constraints, we\npropose a DoubleGreedy algorithm with $1/3$ approximation guarantee. Submodular\ncontinuous functions naturally find applications in various real-world\nsettings, including influence and revenue maximization with continuous\nassignments, sensor energy management, multi-resolution data summarization,\nfacility location, etc. Experimental results show that the proposed algorithms\nefficiently generate superior solutions compared to baseline algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 18:15:52 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 16:21:08 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 15:53:40 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 16:30:15 GMT"}, {"version": "v5", "created": "Mon, 6 May 2019 16:06:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Bian", "Andrew An", ""], ["Mirzasoleiman", "Baharan", ""], ["Buhmann", "Joachim M.", ""], ["Krause", "Andreas", ""]]}, {"id": "1606.05689", "submitter": "Dimitrios Thilikos", "authors": "Fedor V. Fomin and Daniel Lokshtanov and Saket Saurabh and Dimitrios\n  M. Thilikos", "title": "Bidimensionality and Kernels", "comments": "An an earlier version of this paper appeared in SODA 2010. That paper\n  contained preliminary versions of some of the results of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidimensionality Theory was introduced by [E.D. Demaine, F.V. Fomin,\nM.Hajiaghayi, and D.M. Thilikos. Subexponential parameterized algorithms on\ngraphs of bounded genus and H-minor-free graphs, J. ACM, 52 (2005),\npp.866--893] as a tool to obtain sub-exponential time parameterized algorithms\non H-minor-free graphs. In [E.D. Demaine and M.Hajiaghayi, Bidimensionality:\nnew connections between FPT algorithms and PTASs, in Proceedings of the 16th\nAnnual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, 2005,\npp.590--601] this theory was extended in order to obtain polynomial time\napproximation schemes (PTASs) for bidimensional problems. In this work, we\nestablish a third meta-algorithmic direction for bidimensionality theory by\nrelating it to the existence of linear kernels for parameterized problems. In\nparticular, we prove that every minor (respectively contraction) bidimensional\nproblem that satisfies a separation property and is expressible in Countable\nMonadic Second Order Logic (CMSO), admits a linear kernel for classes of graphs\nthat exclude a fixed graph (respectively an apex graph) H as a minor. Our\nresults imply that a multitude of bidimensional problems g graph classes. For\nmost of these problems no polynomial kernels on H-minor-free graphs were known\nprior to our work.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:17:21 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 10:11:15 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 10:00:14 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1606.05724", "submitter": "Giovanni Manzini", "authors": "Gianni Decaroli, Travis Gagie, Giovanni Manzini", "title": "A Compact Index for Order-Preserving Pattern Matching", "comments": "16 pages. A preliminary version appeared in the Proc. IEEE Data\n  Compression Conference, DCC 2017, Snowbird, UT, USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order-preserving pattern matching was introduced recently but it has already\nattracted much attention. Given a reference sequence and a pattern, we want to\nlocate all substrings of the reference sequence whose elements have the same\nrelative order as the pattern elements. For this problem we consider the\noffline version in which we build an index for the reference sequence so that\nsubsequent searches can be completed very efficiently. We propose a\nspace-efficient index that works well in practice despite its lack of good\nworst-case time bounds. Our solution is based on the new approach of\ndecomposing the indexed sequence into an order component, containing ordering\ninformation, and a delta component, containing information on the absolute\nvalues. Experiments show that this approach is viable, faster than the\navailable alternatives, and it is the first one offering simultaneously small\nspace usage and fast retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 07:30:59 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 14:50:51 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 17:10:31 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Decaroli", "Gianni", ""], ["Gagie", "Travis", ""], ["Manzini", "Giovanni", ""]]}, {"id": "1606.05732", "submitter": "Michael Kapralov", "authors": "Michael Kapralov and Vamsi K. Potluru and David P. Woodruff", "title": "How to Fake Multiply by a Gaussian Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Have you ever wanted to multiply an $n \\times d$ matrix $X$, with $n \\gg d$,\non the left by an $m \\times n$ matrix $\\tilde G$ of i.i.d. Gaussian random\nvariables, but could not afford to do it because it was too slow? In this work\nwe propose a new randomized $m \\times n$ matrix $T$, for which one can compute\n$T \\cdot X$ in only $O(\\text{nnz}(X)) + \\tilde O(m^2 \\cdot d^{3})$ time, for\nwhich the total variation distance between the distributions $T \\cdot X$ and\n$\\tilde G \\cdot X$ is as small as desired, i.e., less than any positive\nconstant. Here $\\text{nnz}(X)$ denotes the number of non-zero entries of $X$.\nAssuming $\\text{nnz}(X) \\gg m^2 \\cdot d^{3}$, this is a significant savings\nover the na\\\"ive $O(\\text{nnz}(X) m)$ time to compute $\\tilde G \\cdot X$.\nMoreover, since the total variation distance is small, we can provably use $T\n\\cdot X$ in place of $\\tilde G \\cdot X$ in any application and have the same\nguarantees as if we were using $\\tilde G \\cdot X$, up to a small positive\nconstant in error probability. We apply this transform to nonnegative matrix\nfactorization (NMF) and support vector machines (SVM).\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 09:12:37 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 15:21:12 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Kapralov", "Michael", ""], ["Potluru", "Vamsi K.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1606.05790", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Peter Aaltonen, David Bader, Ayd{\\i}n Buluc, Franz\n  Franchetti, John Gilbert, Dylan Hutchison, Manoj Kumar, Andrew Lumsdaine,\n  Henning Meyerhenke, Scott McMillan, Jose Moreira, John D. Owens, Carl Yang,\n  Marcin Zalewski, Timothy Mattson", "title": "Mathematical Foundations of the GraphBLAS", "comments": "9 pages; 11 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761646", "report-no": null, "categories": "cs.MS astro-ph.IM cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GraphBLAS standard (GraphBlas.org) is being developed to bring the\npotential of matrix based graph algorithms to the broadest possible audience.\nMathematically the Graph- BLAS defines a core set of matrix-based graph\noperations that can be used to implement a wide class of graph algorithms in a\nwide range of programming environments. This paper provides an introduction to\nthe mathematics of the GraphBLAS. Graphs represent connections between vertices\nwith edges. Matrices can represent a wide range of graphs using adjacency\nmatrices or incidence matrices. Adjacency matrices are often easier to analyze\nwhile incidence matrices are often better for representing data. Fortunately,\nthe two are easily connected by matrix mul- tiplication. A key feature of\nmatrix mathematics is that a very small number of matrix operations can be used\nto manipulate a very wide range of graphs. This composability of small number\nof operations is the foundation of the GraphBLAS. A standard such as the\nGraphBLAS can only be effective if it has low performance overhead. Performance\nmeasurements of prototype GraphBLAS implementations indicate that the overhead\nis low.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 18:46:20 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 02:52:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Kepner", "Jeremy", ""], ["Aaltonen", "Peter", ""], ["Bader", "David", ""], ["Buluc", "Ayd\u0131n", ""], ["Franchetti", "Franz", ""], ["Gilbert", "John", ""], ["Hutchison", "Dylan", ""], ["Kumar", "Manoj", ""], ["Lumsdaine", "Andrew", ""], ["Meyerhenke", "Henning", ""], ["McMillan", "Scott", ""], ["Moreira", "Jose", ""], ["Owens", "John D.", ""], ["Yang", "Carl", ""], ["Zalewski", "Marcin", ""], ["Mattson", "Timothy", ""]]}, {"id": "1606.05910", "submitter": "Daniel Doerr", "authors": "Daniel Doerr, Pedro Feijao, Metin Balaban, Cedric Chauve", "title": "The gene family-free median of three", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gene family-free framework for comparative genomics aims at developing\nmethods for gene order analysis that do not require prior gene family\nassignment, but work directly on a sequence similarity multipartite graph. We\npresent a model for constructing a median of three genomes in this family-free\nsetting, based on maximizing an objective function that generalizes the\nclassical breakpoint distance by integrating sequence similarity in the score\nof a gene adjacency. We show that the corresponding computational problem is\nMAX SNP-hard and we present a 0-1 linear program for its exact solution. The\nresult of our FF-median program is a median genome with median genes associated\nto extant genes, in which median adjacencies are assumed to define positional\northologs. We demonstrate through simulations and comparison with the OMA\northology database that the herein presented method is able compute accurate\nmedians and positional orthologs for genomes comparable in size of bacterial\ngenomes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 21:21:30 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Doerr", "Daniel", ""], ["Feijao", "Pedro", ""], ["Balaban", "Metin", ""], ["Chauve", "Cedric", ""]]}, {"id": "1606.05927", "submitter": "Andrew Connor", "authors": "Wilson S. Siringoringo, Andy M. Connor, Nick Clements and Nick\n  Alexander", "title": "Minimum cost polygon overlay with rectangular shape stock panels", "comments": null, "journal-ref": "International Journal of Construction Education & Research, 4(3),\n  1-24 (2008)", "doi": "10.1080/15578770802494516", "report-no": null, "categories": "cs.NE cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum Cost Polygon Overlay (MCPO) is a unique two-dimensional optimization\nproblem that involves the task of covering a polygon shaped area with a series\nof rectangular shaped panels. This has a number of applications in the\nconstruction industry. This work examines the MCPO problem in order to\nconstruct a model that captures essential parameters of the problem to be\nsolved automatically using numerical optimization algorithms. Three algorithms\nhave been implemented of the actual optimization task: the greedy search, the\nMonte Carlo (MC) method, and the Genetic Algorithm (GA). Results are presented\nto show the relative effectiveness of the algorithms. This is followed by\ncritical analysis of various findings of this research.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 23:50:15 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Siringoringo", "Wilson S.", ""], ["Connor", "Andy M.", ""], ["Clements", "Nick", ""], ["Alexander", "Nick", ""]]}, {"id": "1606.05973", "submitter": "Siddharth Gupta", "authors": "Siddharth Gupta, Diana Palsetia, Md. Mostofa Ali Patwary, Ankit\n  Agrawal, Alok Choudhary", "title": "A New Parallel Algorithm for Two-Pass Connected Component Labeling", "comments": "Parallel & Distributed Processing Symposium Workshops (IPDPSW), 2014", "journal-ref": null, "doi": "10.1109/IPDPSW.2014.152", "report-no": null, "categories": "cs.DS cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected Component Labeling (CCL) is an important step in pattern\nrecognition and image processing. It assigns labels to the pixels such that\nadjacent pixels sharing the same features are assigned the same label.\nTypically, CCL requires several passes over the data. We focus on two-pass\ntechnique where each pixel is given a provisional label in the first pass\nwhereas an actual label is assigned in the second pass.\n  We present a scalable parallel two-pass CCL algorithm, called PAREMSP, which\nemploys a scan strategy and the best union-find technique called REMSP, which\nuses REM's algorithm for storing label equivalence information of pixels in a\n2-D image. In the first pass, we divide the image among threads and each thread\nruns the scan phase along with REMSP simultaneously. In the second phase, we\nassign the final labels to the pixels. As REMSP is easily parallelizable, we\nuse the parallel version of REMSP for merging the pixels on the boundary. Our\nexperiments show the scalability of PAREMSP achieving speedups up to $20.1$\nusing $24$ cores on shared memory architecture using OpenMP for an image of\nsize $465.20$ MB. We find that our proposed parallel algorithm achieves linear\nscaling for a large resolution fixed problem size as the number of processing\nelements are increased. Additionally, the parallel algorithm does not make use\nof any hardware specific routines, and thus is highly portable.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 05:13:28 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Gupta", "Siddharth", ""], ["Palsetia", "Diana", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Agrawal", "Ankit", ""], ["Choudhary", "Alok", ""]]}, {"id": "1606.05975", "submitter": "Marcin Wrochna", "authors": "Archontia C. Giannopoulou, Micha{\\l} Pilipczuk, Jean-Florent Raymond,\n  Dimitrios M. Thilikos, Marcin Wrochna", "title": "Cutwidth: obstructions and algorithmic aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutwidth is one of the classic layout parameters for graphs. It measures how\nwell one can order the vertices of a graph in a linear manner, so that the\nmaximum number of edges between any prefix and its complement suffix is\nminimized. As graphs of cutwidth at most $k$ are closed under taking\nimmersions, the results of Robertson and Seymour imply that there is a finite\nlist of minimal immersion obstructions for admitting a cut layout of width at\nmost $k$. We prove that every minimal immersion obstruction for cutwidth at\nmost $k$ has size at most $2^{O(k^3\\log k)}$.\n  As an interesting algorithmic byproduct, we design a new fixed-parameter\nalgorithm for computing the cutwidth of a graph that runs in time $2^{O(k^2\\log\nk)}\\cdot n$, where $k$ is the optimum width and $n$ is the number of vertices.\nWhile being slower by a $\\log k$-factor in the exponent than the fastest known\nalgorithm, given by Thilikos, Bodlaender, and Serna in [Cutwidth I: A linear\ntime fixed parameter algorithm, J. Algorithms, 56(1):1--24, 2005] and [Cutwidth\nII: Algorithms for partial $w$-trees of bounded degree, J. Algorithms,\n56(1):25--49, 2005], our algorithm has the advantage of being simpler and\nself-contained; arguably, it explains better the combinatorics of optimum-width\nlayouts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 05:31:39 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 13:07:23 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Giannopoulou", "Archontia C.", ""], ["Pilipczuk", "Micha\u0142", ""], ["Raymond", "Jean-Florent", ""], ["Thilikos", "Dimitrios M.", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1606.05979", "submitter": "Seyed Mahdi Ghamkhari", "authors": "Mahdi Ghamkhari and Ashkan Sadeghi-Mobarakeh and Hamed Mohsenian-Rad", "title": "Strategic Bidding for Producers in Nodal Electricity Markets: A Convex\n  Relaxation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic bidding problems in electricity markets are widely studied in power\nsystems, often by formulating complex bi-level optimization problems that are\nhard to solve. The state-of-the-art approach to solve such problems is to\nreformulate them as mixed-integer linear programs (MILPs). However, the\ncomputational time of such MILP reformulations grows dramatically, once the\nnetwork size increases, scheduling horizon increases, or randomness is taken\ninto consideration. In this paper, we take a fundamentally different approach\nand propose effective and customized convex programming tools to solve the\nstrategic bidding problem for producers in nodal electricity markets. Our\napproach is inspired by the Schmudgen's Positivstellensatz Theorem in\nsemi-algebraic geometry; but then we go through several steps based upon both\nconvex optimization and mixed-integer programming that results in obtaining\nclose to optimal bidding solutions, as evidenced by several numerical case\nstudies, besides having a huge advantage on reducing computation time. While\nthe computation time of the state-of-the-art MILP approach grows exponentially\nwhen we increase the scheduling horizon or the number of random scenarios, the\ncomputation time of our approach increases rather linearly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 05:54:25 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Ghamkhari", "Mahdi", ""], ["Sadeghi-Mobarakeh", "Ashkan", ""], ["Mohsenian-Rad", "Hamed", ""]]}, {"id": "1606.06183", "submitter": "Hamidreza Jahanjou", "authors": "Hamidreza Jahanjou, Erez Kantor, Rajmohan Rajaraman", "title": "Asymptotically Optimal Approximation Algorithms for Coflow Scheduling", "comments": "Fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern datacenter applications involve large-scale computations composed\nof multiple data flows that need to be completed over a shared set of\ndistributed resources. Such a computation completes when all of its flows\ncomplete. A useful abstraction for modeling such scenarios is a {\\em coflow},\nwhich is a collection of flows (e.g., tasks, packets, data transmissions) that\nall share the same performance goal.\n  In this paper, we present the first approximation algorithms for scheduling\ncoflows over general network topologies with the objective of minimizing total\nweighted completion time. We consider two different models for coflows based on\nthe nature of individual flows: circuits, and packets. We design\nconstant-factor polynomial-time approximation algorithms for scheduling\npacket-based coflows with or without given flow paths, and circuit-based\ncoflows with given flow paths. Furthermore, we give an $O(\\log n/\\log \\log\nn)$-approximation polynomial time algorithm for scheduling circuit-based\ncoflows where flow paths are not given (here $n$ is the number of network\nedges).\n  We obtain our results by developing a general framework for coflow schedules,\nbased on interval-indexed linear programs, which may extend to other coflow\nmodels and objective functions and may also yield improved approximation bounds\nfor specific network scenarios. We also present an experimental evaluation of\nour approach for circuit-based coflows that show a performance improvement of\nat least 22% on average over competing heuristics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 15:51:31 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:36:38 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 23:06:01 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 02:51:24 GMT"}, {"version": "v5", "created": "Fri, 9 Mar 2018 01:17:29 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Jahanjou", "Hamidreza", ""], ["Kantor", "Erez", ""], ["Rajaraman", "Rajmohan", ""]]}, {"id": "1606.06204", "submitter": "Richard Barnes", "authors": "Richard Barnes", "title": "Parallel Priority-Flood Depression Filling For Trillion Cell Digital\n  Elevation Models On Desktops Or Clusters", "comments": "21 pages, 4 tables, 8 figures", "journal-ref": "Computers and Geosciences, Volume 96, November 2016, pp. 56-68", "doi": "10.1016/j.cageo.2016.07.001", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for extracting hydrologic features and properties from digital\nelevation models (DEMs) are challenged by large datasets, which often cannot\nfit within a computer's RAM. Depression filling is an important preconditioning\nstep to many of these algorithms. Here, I present a new, linearly-scaling\nalgorithm which parallelizes the Priority-Flood depression-filling algorithm by\nsubdividing a DEM into tiles. Using a single-producer, multi-consumer design,\nthe new algorithm works equally well on one core, multiple cores, or multiple\nmachines and can take advantage of large memories or cope with small ones.\nUnlike previous algorithms, the new algorithm guarantees a fixed number of\nmemory access and communication events per subdivision of the DEM. In\ncomparison testing, this results in the new algorithm running generally faster\nwhile using fewer resources than previous algorithms. For moderately sized\ntiles, the algorithm exhibits ~60% strong and weak scaling efficiencies up to\n48 cores, and linear time scaling across datasets ranging over three orders of\nmagnitude. The largest dataset on which I run the algorithm has 2 trillion\n(2*10^12) cells. With 48 cores, processing required 4.8 hours wall-time (9.3\ncompute-days). This test is three orders of magnitude larger than any\npreviously performed in the literature. Complete, well-commented source code\nand correctness tests are available for download from a repository.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 16:52:12 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 22:35:43 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Barnes", "Richard", ""]]}, {"id": "1606.06235", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos Tsourakakis, Jakub Pachocki, Michael Mitzenmacher", "title": "Scalable motif-aware graph clustering", "comments": "17 pages, to appear in WWW 2017 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new methods based on graph motifs for graph clustering, allowing\nmore efficient detection of communities within networks. We focus on triangles\nwithin graphs, but our techniques extend to other clique motifs as well. Our\nintuition, which has been suggested but not formalized similarly in previous\nworks, is that triangles are a better signature of community than edges. We\ntherefore generalize the notion of conductance for a graph to {\\em triangle\nconductance}, where the edges are weighted according to the number of triangles\ncontaining the edge. This methodology allows us to develop variations of\nseveral existing clustering techniques, including spectral clustering, that\nminimize triangles split by the cluster instead of edges cut by the cluster. We\nprovide theoretical results in a planted partition model to demonstrate the\npotential for triangle conductance in clustering problems. We then show\nexperimentally the effectiveness of our methods to multiple applications in\nmachine learning and graph mining.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:23:04 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 17:51:00 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Tsourakakis", "Charalampos", ""], ["Pachocki", "Jakub", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1606.06389", "submitter": "Mark Yagnatinsky", "authors": "John Iacono and Mark Yagnatinsky", "title": "A Linear Potential Function for Pairing Heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first potential function for pairing heaps with linear range.\nThis implies that the runtime of a short sequence of operations is faster than\npreviously known. It is also simpler than the only other potential function\nknown to give amortized constant amortized time for insertion.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 01:07:51 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 01:06:11 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Iacono", "John", ""], ["Yagnatinsky", "Mark", ""]]}, {"id": "1606.06395", "submitter": "Karthik Abinav Sankararaman", "authors": "Brian Brubach, Karthik Abinav Sankararaman, Aravind Srinivasan, Pan Xu", "title": "Online Stochastic Matching: New Algorithms and Bounds", "comments": "Preliminary Version appeared in European Symposium on Algorithms\n  (ESA) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online matching has received significant attention over the last 15 years due\nto its close connection to Internet advertising. As the seminal work of Karp,\nVazirani, and Vazirani has an optimal (1 - 1/e) competitive ratio in the\nstandard adversarial online model, much effort has gone into developing useful\nonline models that incorporate some stochasticity in the arrival process. One\nsuch popular model is the \"known I.I.D. model\" where different customer-types\narrive online from a known distribution. We develop algorithms with improved\ncompetitive ratios for some basic variants of this model with integral arrival\nrates, including (a) the case of general weighted edges, where we improve the\nbest-known ratio of 0.667 due to Haeupler, Mirrokni and Zadimoghaddam to 0.705;\nand (b) the vertex-weighted case, where we improve the 0.7250 ratio of Jaillet\nand Lu to 0.7299. We also consider an extension of stochastic rewards, a\nvariant where each edge has an independent probability of being present. For\nthe setting of stochastic rewards with non-integral arrival rates, we present a\nsimple optimal non-adaptive algorithm with a ratio of 1 - 1/e. For the special\ncase where each edge is unweighted and has a uniform constant probability of\nbeing present, we improve upon 1 - 1/e by proposing a strengthened LP\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 01:55:53 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 20:11:03 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 03:34:57 GMT"}, {"version": "v4", "created": "Tue, 23 Jul 2019 02:06:06 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Brubach", "Brian", ""], ["Sankararaman", "Karthik Abinav", ""], ["Srinivasan", "Aravind", ""], ["Xu", "Pan", ""]]}, {"id": "1606.06399", "submitter": "Jonathan Gorard", "authors": "Jonathan Gorard", "title": "Uniqueness Trees: A Possible Polynomial Approach to the Graph\n  Isomorphism Problem", "comments": "14 pages + appendix, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the novel `uniqueness tree' algorithm, as one possible\nmethod for determining whether two finite, undirected graphs are isomorphic. We\nprove that the algorithm has polynomial time complexity in the worst case, and\nthat it will always detect the presence of an isomorphism whenever one exists.\nWe also propose that the algorithm will equivalently discern the lack of an\nisomorphism whenever one does not exist, and some initial justifications are\ngiven for this proposition, although it cannot yet be rigorously proven.\nFinally, we present experimental evidence for both the effectiveness and\nefficiency of the uniqueness tree method, using data gathered from a practical\nimplementation of the algorithm. Some consequences and directions for further\nresearch are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 02:04:13 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Gorard", "Jonathan", ""]]}, {"id": "1606.06566", "submitter": "Martin F\\\"urer", "authors": "Martin F\\\"urer", "title": "Faster Computation of Path-Width", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-width and path-width are widely successful concepts. Many NP-hard\nproblems have efficient solutions when restricted to graphs of bounded\ntree-width. Many efficient algorithms are based on a tree decomposition.\nSometimes the more restricted path decomposition is required. The bottleneck\nfor such algorithms is often the computation of the width and a corresponding\ntree or path decomposition. For graphs with $n$ vertices and tree-width or\npath-width $k$, the standard linear time algorithm to compute these\ndecompositions dates back to 1996. Its running time is linear in $n$ and\nexponential in $k^3$ and not usable in practice. Here we present a more\nefficient algorithm to compute the path-width and provide a path decomposition.\nIts running time is $2^{O(k^2)} n$. In the classical algorithm of Bodlaender\nand Kloks, the path decomposition is computed from a tree decomposition. Here,\nan optimal path decomposition is computed from a path decomposition of about\ntwice the width. The latter is computed from a constant factor smaller graph.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 13:40:01 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["F\u00fcrer", "Martin", ""]]}, {"id": "1606.06617", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro", "title": "A Self-Index on Block Trees", "comments": "Version 3 is the final SPIRE 2017 version. Version 4 corrects some\n  errors and typos, the important one about what is inserted in the grid. It\n  also improves the time when O(w) space can be used", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Block Tree is a recently proposed data structure that reaches compression\nclose to Lempel-Ziv while supporting efficient direct access to text\nsubstrings. In this paper we show how a self-index can be built on top of a\nBlock Tree so that it provides efficient pattern searches while using space\nproportional to that of the original data structure. More precisely, if a\nLempel-Ziv parse cuts a text of length $n$ into $z$ non-overlapping phrases,\nthen our index uses $O(z\\log(n/z))$ words and finds the $occ$ occurrences of a\npattern of length $m$ in time $O(m\\log n+occ\\log^\\epsilon n)$ for any constant\n$\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:26:55 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 15:49:50 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 16:49:48 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 08:23:45 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Navarro", "Gonzalo", ""]]}, {"id": "1606.06629", "submitter": "Camille Coti", "authors": "Olivier Bodini and Camille Coti and Julien David", "title": "Parallel Galton Watson Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a parallel version of Galton-Watson processes for the\nrandom generation of tree-shaped structures. Random trees are useful in many\nsituations (testing, binary search, simulation of physics phenomena,...) as\nattests more than 49000 citations on Google scholar. Using standard analytic\ncombinatorics, we first give a theoretical, average-case study of the random\nprocess in order to evaluate how parallelism can be extracted from this\nprocess, and we deduce a parallel generation algorithm. Then we present how it\ncan be implemented in a task-based parallel paradigm for shared memory (here,\nIntel Cilk). This implementation faces several challenges, among which\nefficient, thread-safe random bit generation, memory management and algorithmic\nmodifications for small-grain parallelism. Finally, we evaluate the performance\nof our implementation and the impact of different choices and parameters. We\nobtain a significant efficiency improvement for the generation of big trees. We\nalso conduct empirical and theoretical studies of the average behaviour of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:54:59 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Bodini", "Olivier", ""], ["Coti", "Camille", ""], ["David", "Julien", ""]]}, {"id": "1606.06636", "submitter": "Ben Strasser", "authors": "Ben Strasser", "title": "Intriguingly Simple and Efficient Time-Dependent Routing in Road\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the earliest arrival problem in road networks with static\ntime-dependent functions as arc weights. We propose and evaluate the following\nsimple algorithm: (1) average the travel time in k time windows, (2) compute a\nshortest time-independent path within each window and mark the edges in these\npaths, and (3) compute a shortest time-dependent path in the original graph\nrestricted to the marked edges. Our experimental evaluation shows that this\nsimple algorithm yields near optimal results on well-established benchmark\ninstances. We additionally demonstrate that the error can be further reduced by\nadditionally considering alternative routes at the expense of more marked\nedges. Finally, we show that the achieved subgraphs are small enough to be able\nto efficiently implement profile queries using a simple sampling-based\napproach. A highlight of our introduced algorithms is that they do not rely on\nlinking and merging profile functions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 16:12:29 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 10:38:43 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Strasser", "Ben", ""]]}, {"id": "1606.06730", "submitter": "Kaushik Sarkar", "authors": "Kaushik Sarkar and Charles J. Colbourn", "title": "Two-stage algorithms for covering array construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SE math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software systems often consist of many different components, each with\na number of options. Although unit tests may reveal faulty options for\nindividual components, functionally correct components may interact in\nunforeseen ways to cause a fault. Covering arrays are used to test for\ninteractions among components systematically. A two-stage framework, providing\na number of concrete algorithms, is developed for the efficient construction of\ncovering arrays. %Our framework divides the construction in two stages. In the\nfirst stage, a time and memory efficient randomized algorithm covers most of\nthe interactions. In the second stage, a more sophisticated search covers the\nremainder in relatively few tests. In this way, the storage limitations of the\nsophisticated search algorithms are avoided; hence the range of the number of\ncomponents for which the algorithm can be applied is extended, without\nincreasing the number of tests. Many of the framework instantiations can be\ntuned to optimize a memory-quality trade-off, so that fewer tests can be\nachieved using more memory. The algorithms developed outperform the currently\nbest known methods when the number of components ranges from 20 to 60, the\nnumber of options for each ranges from 3 to 6, and $t$-way interactions are\ncovered for $t\\in \\{5,6\\}$. In some cases a reduction in the number of tests by\nmore than $50\\%$ is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 04:01:39 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Sarkar", "Kaushik", ""], ["Colbourn", "Charles J.", ""]]}, {"id": "1606.06797", "submitter": "Mohamed El Yafrani", "authors": "Mohamed El Yafrani, Bela\\\"id Ahiod", "title": "\\'Etude de Probl\\`emes d'Optimisation Combinatoire \\`a Multiples\n  Composantes Interd\\'ependantes", "comments": "in French. Extended abstract presented at the URAC days meeting in\n  Rabat, Morocco. The meeting website is available at\n  https://sites.google.com/site/lriturac29/j-urac-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract presents an overview on NP-hard optimization problems\nwith multiple interdependent components. These problems occur in many\nreal-world applications: industrial applications, engineering, and logistics.\nThe fact that these problems are composed of many sub-problems that are NP-hard\nmakes them even more challenging to solve using exact algorithms. This is\nmainly due to the high complexity of this class of algorithms and the hardness\nof the problems themselves. The main source of difficulty of these problems is\nthe presence of internal dependencies between sub-problems. This aspect of\ninterdependence of components is presented, and some outlines on solving\napproaches are briefly introduced from a (meta)heuristics and evolutionary\ncomputation perspective.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 01:34:53 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Yafrani", "Mohamed El", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1606.06846", "submitter": "Rad Niazadeh", "authors": "Rad Niazadeh, Christopher Wilkens", "title": "Competitive Equilibria for Non-quasilinear Bidders in Combinatorial\n  Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasiliearity is a ubiquitous and questionable assumption in the standard\nstudy of Walrasian equilibria. Quasilinearity implies that a buyer's value for\ngoods purchased in a Walrasian equilibrium is always additive with goods\npurchased with unspent money. It is a particularly suspect assumption in\ncombinatorial auctions, where buyers' complex preferences over goods would\nnaturally extend beyond the items obtained in the Walrasian equilibrium.\n  We study Walrasian equilibria in combinatorial auctions when quasilinearity\nis not assumed. We show that existence can be reduced to an Arrow-Debreu style\nmarket with one divisible good and many indivisible goods, and that a\n\"fractional\" Walrasian equilibrium always exists. We also show that standard\nintegral Walrasian equilibria are related to integral solutions of an induced\nconfiguration LP associated with a fractional Walrasian equilibrium,\ngeneralizing known results for both quasilinear and non-quasilnear settings.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 08:24:07 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Niazadeh", "Rad", ""], ["Wilkens", "Christopher", ""]]}, {"id": "1606.06926", "submitter": "Andreas T\\\"onnis", "authors": "Thomas Kesselheim and Andreas T\\\"onnis", "title": "Think Eternally: Improved Algorithms for the Temp Secretary Problem and\n  Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{Temp Secretary Problem} was recently introduced by Fiat et al. It\nis a generalization of the Secretary Problem, in which commitments are\ntemporary for a fixed duration. We present a simple online algorithm with\nimproved performance guarantees for cases already considered by Fiat et al.\\\nand give competitive ratios for new generalizations of the problem. In the\nclassical setting, where candidates have identical contract durations $\\gamma\n\\ll 1$ and we are allowed to hire up to $B$ candidates simultaneously, our\nalgorithm is $(\\frac{1}{2} - O(\\sqrt{\\gamma}))$-competitive. For large $B$, the\nbound improves to $1 - O\\left(\\frac{1}{\\sqrt{B}}\\right) - O(\\sqrt{\\gamma})$.\n  Furthermore we generalize the problem from cardinality constraints towards\ngeneral packing constraints. We achieve a competitive ratio of $1 -\nO\\left(\\sqrt{\\frac{(1+\\log d + \\log B)}{B}}\\right) -O(\\sqrt{\\gamma})$, where\n$d$ is the sparsity of the constraint matrix and $B$ is generalized to the\ncapacity ratio of linear constraints. Additionally we extend the problem\ntowards arbitrary hiring durations.\n  Our algorithmic approach is a relaxation that aggregates all temporal\nconstraints into a non-temporal constraint. Then we apply a linear scaling\nalgorithm that, on every arrival, computes a tentative solution on the input\nthat is known up to this point. This tentative solution uses the non-temporal,\nrelaxed constraints scaled down linearly by the amount of time that has already\npassed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 12:31:10 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Kesselheim", "Thomas", ""], ["T\u00f6nnis", "Andreas", ""]]}, {"id": "1606.07143", "submitter": "Justin Hsu", "authors": "Gilles Barthe, No\\'emie Fong, Marco Gaboardi, Benjamin Gr\\'egoire,\n  Justin Hsu, Pierre-Yves Strub", "title": "Advanced Probabilistic Couplings for Differential Privacy", "comments": null, "journal-ref": null, "doi": "10.1145/2976749.2978391", "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a promising formal approach to data privacy, which\nprovides a quantitative bound on the privacy cost of an algorithm that operates\non sensitive information. Several tools have been developed for the formal\nverification of differentially private algorithms, including program logics and\ntype systems. However, these tools do not capture fundamental techniques that\nhave emerged in recent years, and cannot be used for reasoning about\ncutting-edge differentially private algorithms. Existing techniques fail to\nhandle three broad classes of algorithms: 1) algorithms where privacy depends\naccuracy guarantees, 2) algorithms that are analyzed with the advanced\ncomposition theorem, which shows slower growth in the privacy cost, 3)\nalgorithms that interactively accept adaptive inputs.\n  We address these limitations with a new formalism extending apRHL, a\nrelational program logic that has been used for proving differential privacy of\nnon-interactive algorithms, and incorporating aHL, a (non-relational) program\nlogic for accuracy properties. We illustrate our approach through a single\nrunning example, which exemplifies the three classes of algorithms and explores\nnew variants of the Sparse Vector technique, a well-studied algorithm from the\nprivacy literature. We implement our logic in EasyCrypt, and formally verify\nprivacy. We also introduce a novel coupling technique called \\emph{optimal\nsubset coupling} that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 00:11:57 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 16:22:57 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Barthe", "Gilles", ""], ["Fong", "No\u00e9mie", ""], ["Gaboardi", "Marco", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1606.07208", "submitter": "John Augustine", "authors": "Guru Prakash Arumugam, John Augustine, Mordecai J. Golin, Yuya\n  Higashikawa, Naoki Katoh, Prashanth Srikanthan", "title": "Optimal Evacuation Flows on Dynamic Paths with General Edge Capacities", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Dynamic Graph Network is a graph in which each edge has an associated\ntravel time and a capacity (width) that limits the number of items that can\ntravel in parallel along that edge. Each vertex in this dynamic graph network\nbegins with the number of items that must be evacuated into designated sink\nvertices. A $k$-sink evacuation protocol finds the location of $k$ sinks and\nassociated evacuation movement protocol that allows evacuating all the items to\na sink in minimum time. The associated evacuation movement must impose a\nconfluent flow, i.e, all items passing through a particular vertex exit that\nvertex using the same edge. In this paper we address the $k$-sink evacuation\nproblem on a dynamic path network. We provide solutions that run in $O(n \\log\nn)$ time for $k=1$ and $O(k n \\log^2 n)$ for $k >1$ and work for arbitrary edge\ncapacities.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 07:13:55 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Arumugam", "Guru Prakash", ""], ["Augustine", "John", ""], ["Golin", "Mordecai J.", ""], ["Higashikawa", "Yuya", ""], ["Katoh", "Naoki", ""], ["Srikanthan", "Prashanth", ""]]}, {"id": "1606.07384", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Daniel Kane, Alistair Stewart", "title": "Robust Learning of Fixed-Structure Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning Bayesian networks in a robust model\nwhere an $\\epsilon$-fraction of the samples are adversarially corrupted. In\nthis work, we study the fully observable discrete case where the structure of\nthe network is given. Even in this basic setting, previous learning algorithms\neither run in exponential time or lose dimension-dependent factors in their\nerror guarantees. We provide the first computationally efficient robust\nlearning algorithm for this problem with dimension-independent error\nguarantees. Our algorithm has near-optimal sample complexity, runs in\npolynomial time, and achieves error that scales nearly-linearly with the\nfraction of adversarially corrupted samples. Finally, we show on both synthetic\nand semi-synthetic data that our algorithm performs well in practice.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 17:47:13 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 05:31:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Kane", "Daniel", ""], ["Stewart", "Alistair", ""]]}, {"id": "1606.07425", "submitter": "Jonah Sherman", "authors": "Jonah Sherman", "title": "Generalized Preconditioning and Network Flow Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximation algorithms for the problem of finding $x$ of\nminimal norm $\\|x\\|$ satisfying a linear system $\\mathbf{A} x = \\mathbf{b}$,\nwhere the norm $\\|\\cdot \\|$ is arbitrary and generally non-Euclidean. We show a\nsimple general technique for composing solvers, converting iterative solvers\nwith residual error $\\|\\mathbf{A} x - \\mathbf{b}\\| \\leq t^{-\\Omega(1)}$ into\nsolvers with residual error $\\exp(-\\Omega(t))$, at the cost of an increase in\n$\\|x\\|$, by recursively invoking the solver on the residual problem\n$\\tilde{\\mathbf{b}} = \\mathbf{b} - \\mathbf{A} x$. Convergence of the composed\nsolvers depends strongly on a generalization of the classical condition number\nto general norms, reducing the task of designing algorithms for many such\nproblems to that of designing a \\emph{generalized preconditioner} for\n$\\mathbf{A}$. The new ideas significantly generalize those introduced by the\nauthor's earlier work on maximum flow, making them more widely applicable.\n  As an application of the new technique, we present a nearly-linear time\napproximation algorithm for uncapacitated minimum-cost flow on undirected\ngraphs. Given an undirected graph with $m$ edges labelled with costs, and $n$\nvertices labelled with demands, the algorithm takes\n$\\epsilon^{-2}m^{1+o(1)}$-time and outputs a flow routing the demands with\ntotal cost at most $(1+\\epsilon)$ times larger than minimal, along with a dual\nsolution proving near-optimality. The generalized preconditioner is obtained by\nembedding the cost metric into $\\ell_1$, and then considering a simple\nhierarchical routing scheme in $\\ell_1$ where demands initially supported on a\ndense lattice are pulled from a sparser lattice by randomly rounding unaligned\ncoordinates to their aligned neighbors. Analysis of the generalized condition\nnumber for the preconditioner follows that of the classical multigrid algorithm\nfor lattice Laplacian systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:56:35 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 19:48:50 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Sherman", "Jonah", ""]]}, {"id": "1606.07802", "submitter": "Kyle Niemeyer", "authors": "Kyle E. Niemeyer and Chih-Jen Sung", "title": "On the importance of graph search algorithms for DRGEP-based mechanism\n  reduction methods", "comments": "17 pages, 2 figures", "journal-ref": "Combustion and Flame 158(8) (2011) 1439-1443", "doi": "10.1016/j.combustflame.2010.12.010", "report-no": null, "categories": "cs.DS physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of graph search algorithm choice to the directed relation\ngraph with error propagation (DRGEP) method is studied by comparing basic and\nmodified depth-first search, basic and R-value-based breadth-first search\n(RBFS), and Dijkstra's algorithm. By using each algorithm with DRGEP to produce\nskeletal mechanisms from a detailed mechanism for n-heptane with\nrandomly-shuffled species order, it is demonstrated that only Dijkstra's\nalgorithm and RBFS produce results independent of species order. In addition,\neach algorithm is used with DRGEP to generate skeletal mechanisms for n-heptane\ncovering a comprehensive range of autoignition conditions for pressure,\ntemperature, and equivalence ratio. Dijkstra's algorithm combined with a\ncoefficient scaling approach is demonstrated to produce the most compact\nskeletal mechanism with a similar performance compared to larger skeletal\nmechanisms resulting from the other algorithms. The computational efficiency of\neach algorithm is also compared by applying the DRGEP method with each search\nalgorithm on the large detailed mechanism for n-alkanes covering n-octane to\nn-hexadecane with 2115 species and 8157 reactions. Dijkstra's algorithm\nimplemented with a binary heap priority queue is demonstrated as the most\nefficient method, with a CPU cost two orders of magnitude less than the other\nsearch algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 00:56:21 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Niemeyer", "Kyle E.", ""], ["Sung", "Chih-Jen", ""]]}, {"id": "1606.07861", "submitter": "Sam Chiu-wai Wong", "authors": "Sam Chiu-wai Wong", "title": "Tight Algorithms for Vertex Cover with Hard Capacities on Multigraphs\n  and Hypergraphs", "comments": "15 pages; SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a f-approximation algorithm for the minimum unweighted\nVertex Cover problem with Hard Capacity constraints (VCHC) on f-hypergraphs.\nThis problem generalizes standard vertex cover for which the best known\napproximation ratio is also f and cannot be improved assuming the unique game\nconjecture. Our result is therefore essentially the best possible. This\nimproves over the previous 2.155 (for f=2) and 2f-approximation algorithms by\nCheung, Goemans and Wong (CGW).\n  At the heart of our approach is to apply iterative rounding to the problem\nwith ideas coming from several previous works. We also give a faster\nimplementation of the method based on certain iteratively rounding the solution\nto certain CGW-style covering LPs.\n  We note that independent of this work, Kao [#kao2017iterative] also recently\nobtained the same result.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 01:12:46 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 13:47:42 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wong", "Sam Chiu-wai", ""]]}, {"id": "1606.07863", "submitter": "Sam Chiu-wai Wong", "authors": "Yajun Wang and Sam Chiu-wai Wong", "title": "Matroid Online Bipartite Matching and Vertex Cover", "comments": "19 pages, to appear in EC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adwords and Online Bipartite Matching problems have enjoyed a renewed\nattention over the past decade due to their connection to Internet advertising.\nOur community has contributed, among other things, new models (notably\nstochastic) and extensions to the classical formulations to address the issues\nthat arise from practical needs. In this paper, we propose a new generalization\nbased on matroids and show that many of the previous results extend to this\nmore general setting. Because of the rich structures and expressive power of\nmatroids, our new setting is potentially of interest both in theory and in\npractice.\n  In the classical version of the problem, the offline side of a bipartite\ngraph is known initially while vertices from the online side arrive one at a\ntime along with their incident edges. The objective is to maintain a decent\napproximate matching from which no edge can be removed. Our generalization,\ncalled Matroid Online Bipartite Matching, additionally requires that the set of\nmatched offline vertices be independent in a given matroid. In particular, the\ncase of partition matroids corresponds to the natural scenario where each\nadvertiser manages multiple ads with a fixed total budget.\n  Our algorithms attain the same performance as the classical version of the\nproblems considered, which are often provably the best possible. We present\n$1-1/e$-competitive algorithms for Matroid Online Bipartite Matching under the\nsmall bid assumption, as well as a $1-1/e$-competitive algorithm for Matroid\nOnline Bipartite Matching in the random arrival model. A key technical\ningredient of our results is a carefully designed primal-dual waterfilling\nprocedure that accommodates for matroid constraints. This is inspired by the\nextension of our recent charging scheme for Online Bipartite Vertex Cover.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 01:21:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Wang", "Yajun", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1606.07864", "submitter": "Sebastian Krinninger", "authors": "Greg Bodwin and Sebastian Krinninger", "title": "Fully Dynamic Spanners with Worst-Case Update Time", "comments": "To be presented at the European Symposium on Algorithms (ESA) 2016", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2016.17", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An $\\alpha$-spanner of a graph $ G $ is a subgraph $ H $ such that $ H $\npreserves all distances of $ G $ within a factor of $ \\alpha $. In this paper,\nwe give fully dynamic algorithms for maintaining a spanner $ H $ of a graph $ G\n$ undergoing edge insertions and deletions with worst-case guarantees on the\nrunning time after each update. In particular, our algorithms maintain: (1) a\n$3$-spanner with $ \\tilde O (n^{1+1/2}) $ edges with worst-case update time $\n\\tilde O (n^{3/4}) $, or (2) a $5$-spanner with $ \\tilde O (n^{1+1/3}) $ edges\nwith worst-case update time $ \\tilde O (n^{5/9}) $. These size/stretch\ntradeoffs are best possible (up to logarithmic factors). They can be extended\nto the weighted setting at very minor cost. Our algorithms are randomized and\ncorrect with high probability against an oblivious adversary. We also further\nextend our techniques to construct a $5$-spanner with suboptimal size/stretch\ntradeoff, but improved worst-case update time.\n  To the best of our knowledge, these are the first dynamic spanner algorithms\nwith sublinear worst-case update time guarantees. Since it is known how to\nmaintain a spanner using small amortized but large worst-case update time\n[Baswana et al. SODA'08], obtaining algorithms with strong worst-case bounds,\nas presented in this paper, seems to be the next natural step for this problem.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 01:40:09 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bodwin", "Greg", ""], ["Krinninger", "Sebastian", ""]]}, {"id": "1606.07992", "submitter": "Rameshwar Pratap", "authors": "Rameshwar Pratap and Sandeep Sen", "title": "Faster Coreset Construction for Projective Clustering via Low-Rank\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a randomized coreset construction for projective\nclustering, which involves computing a set of $k$ closest $j$-dimensional\nlinear (affine) subspaces of a given set of $n$ vectors in $d$ dimensions. Let\n$A \\in \\mathbb{R}^{n\\times d}$ be an input matrix. An earlier deterministic\ncoreset construction of Feldman \\textit{et. al.} relied on computing the SVD of\n$A$. The best known algorithms for SVD require $\\min\\{nd^2, n^2d\\}$ time, which\nmay not be feasible for large values of $n$ and $d$. We present a coreset\nconstruction by projecting the rows of matrix $A$ on some orthonormal vectors\nthat closely approximate the right singular vectors of $A$. As a consequence,\nwhen the values of $k$ and $j$ are small, we are able to achieve a faster\nalgorithm, as compared to the algorithm of Feldman \\textit{et. al.}, while\nmaintaining almost the same approximation. We also benefit in terms of space as\nwell as exploit the sparsity of the input dataset. Another advantage of our\napproach is that it can be constructed in a streaming setting quite\nefficiently.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 03:31:02 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 16:16:58 GMT"}, {"version": "v3", "created": "Sat, 15 Apr 2017 08:17:27 GMT"}, {"version": "v4", "created": "Sat, 28 Apr 2018 07:23:03 GMT"}, {"version": "v5", "created": "Mon, 16 Jul 2018 07:08:49 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Pratap", "Rameshwar", ""], ["Sen", "Sandeep", ""]]}, {"id": "1606.08022", "submitter": "Neelima Gupta", "authors": "Sapna Grover, Neelima Gupta and Aditya Pancholi", "title": "Constant factor Approximation Algorithms for Uniform Hard Capacitated\n  Facility Location Problems: Natural LP is not too bad", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give first constant factor approximation for capacitated\nknapsack median problem (CKM) for hard uniform capacities, violating the budget\nonly by an additive factor of $f_{max}$ where $f_{max}$ is the maximum cost of\na facility opened by the optimal and violating capacities by $(2+\\epsilon)$\nfactor. Natural LP for the problem is known to have an unbounded integrality\ngap when any one of the two constraints is allowed to be violated by a factor\nless than $2$. Thus, we present a result which is very close to the best\nachievable from the natural LP. To the best of our knowledge, the problem has\nnot been studied earlier.\n  For capacitated facility location problem with uniform capacities, a constant\nfactor approximation algorithm is presented violating the capacities a little\n($1 + \\epsilon$). Though constant factor results are known for the problem\nwithout violating the capacities, the result is interesting as it is obtained\nby rounding the solution to the natural LP, which is known to have an unbounded\nintegrality gap without violating the capacities. Thus, we achieve the best\npossible from the natural LP for the problem. The result shows that natural LP\nis not too bad.\n  Finally, we raise some issues with the proofs of the results presented\nin~\\cite{capkmByrkaFRS2013} for capacitated $k$-facility location problem\n(C$k$FLP).~\\cite{capkmByrkaFRS2013} presents $O(1/\\epsilon^2)$ approximation\nviolating the capacities by a factor of $(2 + \\epsilon)$ using dependent\nrounding. We first fix these issues using our techniques. Also, it can be\nargued that (deterministic) pipage rounding cannot be used to open the\nfacilities instead of dependent rounding. Our techniques for CKM provide a\nconstant factor approximation for CkFLP violating the capacities by $(2 +\n\\epsilon)$.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 11:09:30 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 02:37:46 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 02:22:25 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 13:37:53 GMT"}, {"version": "v5", "created": "Fri, 21 Apr 2017 18:01:47 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Grover", "Sapna", ""], ["Gupta", "Neelima", ""], ["Pancholi", "Aditya", ""]]}, {"id": "1606.08083", "submitter": "Arman Yousefi", "authors": "Rafail Ostrovsky, Yuval Rabani, Arman Yousefi", "title": "Matrix Balancing in Lp Norms: A New Analysis of Osborne's Iteration", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an iterative matrix conditioning algorithm due to Osborne (1960).\nThe goal of the algorithm is to convert a square matrix into a balanced matrix\nwhere every row and corresponding column have the same norm. The original\nalgorithm was proposed for balancing rows and columns in the $L_2$ norm, and it\nworks by iterating over balancing a row-column pair in fixed round-robin order.\nVariants of the algorithm for other norms have been heavily studied and are\nimplemented as standard preconditioners in many numerical linear algebra\npackages. Recently, Schulman and Sinclair (2015), in a first result of its kind\nfor any norm, analyzed the rate of convergence of a variant of Osborne's\nalgorithm that uses the $L_{\\infty}$ norm and a different order of choosing\nrow-column pairs. In this paper we study matrix balancing in the $L_1$ norm and\nother $L_p$ norms. We show the following results for any matrix $A =\n(a_{ij})_{i,j=1}^n$, resolving in particular a main open problem mentioned by\nSchulman and Sinclair.\n  1) We analyze the iteration for the $L_1$ norm under a greedy order of\nbalancing. We show that it converges to an $\\epsilon$-balanced matrix in $K =\nO(\\min\\{\\epsilon^{-2}\\log w,\\epsilon^{-1}n^{3/2}\\log(w/\\epsilon)\\})$ iterations\nthat cost a total of $O(m + Kn\\log n)$ arithmetic operations over $O(n\\log\nw)$-bit numbers. Here $m$ is the number of non-zero entries of $A$, and $w =\n\\sum_{i,j} |a_{ij}|/a_{\\min}$ with $a_{\\min} = \\min\\{|a_{ij}|:\\ a_{ij}\\neq\n0\\}$.\n  2) We show that the original round-robin implementation converges to an\n$\\epsilon$-balanced matrix in $O(\\epsilon^{-2}n^2\\log w)$ iterations totalling\n$O(\\epsilon^{-2}mn\\log w)$ arithmetic operations over $O(n\\log w)$-bit numbers.\n  3) We demonstrate a lower bound of $\\Omega(1/\\sqrt{\\epsilon})$ on the\nconvergence rate of any implementation of the iteration.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 20:29:01 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Ostrovsky", "Rafail", ""], ["Rabani", "Yuval", ""], ["Yousefi", "Arman", ""]]}, {"id": "1606.08087", "submitter": "O-Joung Kwon", "authors": "Dong Yeap Kang, O-joung Kwon, Torstein J.F. Str{\\o}mme, Jan Arne Telle", "title": "A width parameter useful for chordal and co-comparability graphs", "comments": "24 pages, 5 figures; An extended abstract appeared in the proceedings\n  of WALCOM2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate new graph classes of bounded mim-width, strictly extending\ninterval graphs and permutation graphs. The graphs $K_t \\boxminus K_t$ and $K_t\n\\boxminus S_t$ are graphs obtained from the disjoint union of two cliques of\nsize $t$, and one clique of size $t$ and one independent set of size $t$\nrespectively, by adding a perfect matching. We prove that : (1) interval graphs\nare $(K_3\\boxminus S_3)$-free chordal graphs; and $(K_t\\boxminus S_t)$-free\nchordal graphs have mim-width at most $t-1$, (2) permutation graphs are\n$(K_3\\boxminus K_3)$-free co-comparability graphs; and $(K_t\\boxminus\nK_t)$-free co-comparability graphs have mim-width at most $t-1$, (3) chordal\ngraphs and co-comparability graphs have unbounded mim-width in general. We\nobtain several algorithmic consequences; for instance, while Minimum Dominating\nSet is NP-complete on chordal graphs, it can be solved in time\n$n^{\\mathcal{O}(t)}$ on $(K_t\\boxminus S_t)$-free chordal graphs. The third\nstatement strengthens a result of Belmonte and Vatshelle stating that either\nthose classes do not have constant mim-width or a decomposition with constant\nmim-width cannot be computed in polynomial time unless $P=NP$. We generalize\nthese ideas to bigger graph classes. We introduce a new width parameter\nsim-width, of stronger modelling power than mim-width, by making a small change\nin the definition of mim-width. We prove that chordal graphs and\nco-comparability graphs have sim-width at most 1. We investigate a way to bound\nmim-width for graphs of bounded sim-width by excluding $K_t\\boxminus K_t$ and\n$K_t\\boxminus S_t$ as induced minors or induced subgraphs, and give algorithmic\nconsequences. Lastly, we show that circle graphs have unbounded sim-width, and\nthus also unbounded mim-width.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 21:29:20 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 08:56:15 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 11:29:43 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Kang", "Dong Yeap", ""], ["Kwon", "O-joung", ""], ["Str\u00f8mme", "Torstein J. F.", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1606.08275", "submitter": "Tomasz Wale\\'n", "authors": "Maxime Crochemore, Costas S. Iliopoulos, Tomasz Kociumaka, Ritu Kundu,\n  Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter, Tomasz Wale\\'n", "title": "Near-Optimal Computation of Runs over General Alphabet via Non-Crossing\n  LCE Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longest common extension queries (LCE queries) and runs are ubiquitous in\nalgorithmic stringology. Linear-time algorithms computing runs and\npreprocessing for constant-time LCE queries have been known for over a decade.\nHowever, these algorithms assume a linearly-sortable integer alphabet. A recent\nbreakthrough paper by Bannai et.\\ al.\\ (SODA 2015) showed a link between the\ntwo notions: all the runs in a string can be computed via a linear number of\nLCE queries. The first to consider these problems over a general ordered\nalphabet was Kosolobov (\\emph{Inf.\\ Process.\\ Lett.}, 2016), who presented an\n$O(n (\\log n)^{2/3})$-time algorithm for answering $O(n)$ LCE queries. This\nresult was improved by Gawrychowski et.\\ al.\\ (accepted to CPM 2016) to $O(n\n\\log \\log n)$ time. In this work we note a special \\emph{non-crossing} property\nof LCE queries asked in the runs computation. We show that any $n$ such\nnon-crossing queries can be answered on-line in $O(n \\alpha(n))$ time, which\nyields an $O(n \\alpha(n))$-time algorithm for computing runs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 13:49:16 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kociumaka", "Tomasz", ""], ["Kundu", "Ritu", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1606.08362", "submitter": "Huy Nguyen", "authors": "Alina Ene, Huy L. Nguyen", "title": "A Reduction for Optimizing Lattice Submodular Functions with Diminishing\n  Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f: \\mathbb{Z}_+^E \\rightarrow \\mathbb{R}_+$ is DR-submodular if\nit satisfies $f({\\bf x} + \\chi_i) -f ({\\bf x}) \\ge f({\\bf y} + \\chi_i) - f({\\bf\ny})$ for all ${\\bf x}\\le {\\bf y}, i\\in E$. Recently, the problem of maximizing\na DR-submodular function $f: \\mathbb{Z}_+^E \\rightarrow \\mathbb{R}_+$ subject\nto a budget constraint $\\|{\\bf x}\\|_1 \\leq B$ as well as additional constraints\nhas received significant attention \\cite{SKIK14,SY15,MYK15,SY16}.\n  In this note, we give a generic reduction from the DR-submodular setting to\nthe submodular setting. The running time of the reduction and the size of the\nresulting submodular instance depends only \\emph{logarithmically} on $B$. Using\nthis reduction, one can translate the results for unconstrained and constrained\nsubmodular maximization to the DR-submodular setting for many types of\nconstraints in a unified manner.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:44:44 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 19:49:59 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1606.08645", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Venkatesh Raman, Srinivasa Rao Satti", "title": "Biconnectivity, $st$-numbering and other applications of DFS using\n  $O(n)$ bits", "comments": "18 pages, 4 figures, Preliminary version of this article appeared in\n  the proceedings of 27th ISAAC 2016, Journal version is accepted to JCSS and\n  will soon appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider space efficient implementations of some classical applications of\nDFS including the problem of testing biconnectivity and $2$-edge connectivity,\nfinding cut vertices and cut edges, computing chain decomposition and\n$st$-numbering of a given undirected graph $G$ on $n$ vertices and $m$ edges.\nClassical algorithms for them typically use DFS and some $\\Omega (\\lg n)$\nbits\\footnote{We use $\\lg$ to denote logarithm to the base $2$.} of information\nat each vertex. Building on a recent $O(n)$-bits implementation of DFS due to\nElmasry et al. (STACS 2015) we provide $O(n)$-bit implementations for all these\napplications of DFS. Our algorithms take $O(m \\lg^c n \\lg\\lg n)$ time for some\nsmall constant $c$ (where $c \\leq 2$). Central to our implementation is a\nsuccinct representation of the DFS tree and a space efficient partitioning of\nthe DFS tree into connected subtrees, which maybe of independent interest for\ndesigning other space efficient graph algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 10:48:51 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 10:17:19 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 06:50:52 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Raman", "Venkatesh", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1606.08667", "submitter": "Mong-Jen Kao", "authors": "Mong-Jen Kao", "title": "Iterative Partial Rounding for Vertex Cover with Hard Capacities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple and novel algorithmic design technique, for which we call\niterative partial rounding, that gives a tight rounding-based approximation for\nvertex cover with hard capacities (VC-HC). In particular, we obtain an\n$f$-approximation for VC-HC on hypergraphs, improving over a previous results\nof Cheung et al (SODA 2014) to the tight extent. This also closes the gap of\napproximation since it was posted by Chuzhoy and Naor in (FOCS 2002). We\nbelieve that our rounding technique is of independence interests when hard\nconstraints are considered.\n  Our main technical tool for establishing the approximation guarantee is a\nseparation lemma that certifies the existence of a strong partition for\nsolutions that are basic feasible in an extended version of the natural LP.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 12:09:31 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 10:35:41 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 02:18:22 GMT"}, {"version": "v4", "created": "Fri, 14 Oct 2016 13:40:19 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Kao", "Mong-Jen", ""]]}, {"id": "1606.08719", "submitter": "Tom\\'a\\v{s} Vina\\v{r}", "authors": "Rastislav Rabatin and Bro\\v{n}a Brejov\\'a and Tom\\'a\\v{s} Vina\\v{r}", "title": "Using Sequence Ensembles for Seeding Alignments of MinION Sequencing\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oxford Nanopore MinION sequencer is currently the smallest sequencing device\navailable. While being able to produce very long reads (reads of up to 100~kbp\nwere reported), it is prone to high sequencing error rates of up to 30%. Since\nmost of these errors are insertions or deletions, it is very difficult to adapt\npopular seed-based algorithms designed for aligning data sets with much lower\nerror rates.\n  Base calling of MinION reads is typically done using hidden Markov models. In\nthis paper, we propose to represent each sequencing read by an ensemble of\nsequences sampled from such a probabilistic model. This approach can improve\nthe sensitivity and false positive rate of seeding an alignment compared to\nusing a single representative base call sequence for each read.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 14:15:11 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Rabatin", "Rastislav", ""], ["Brejov\u00e1", "Bro\u0148a", ""], ["Vina\u0159", "Tom\u00e1\u0161", ""]]}, {"id": "1606.08817", "submitter": "Sungjin Im", "authors": "Sungjin Im and Shi Li", "title": "Better Unrelated Machine Scheduling for Weighted Completion Time via\n  Random Offsets from Non-Uniform Distributions", "comments": "24 pages. To apper in FOCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the classic scheduling problem of minimizing total\nweighted completion time on unrelated machines when jobs have release times,\ni.e, $R | r_{ij} | \\sum_j w_j C_j$ using the three-field notation. For this\nproblem, a 2-approximation is known based on a novel convex programming (J. ACM\n2001 by Skutella). It has been a long standing open problem if one can improve\nupon this 2-approximation (Open Problem 8 in J. of Sched. 1999 by Schuurman and\nWoeginger). We answer this question in the affirmative by giving a\n1.8786-approximation. We achieve this via a surprisingly simple linear\nprogramming, but a novel rounding algorithm and analysis. A key ingredient of\nour algorithm is the use of random offsets sampled from non-uniform\ndistributions.\n  We also consider the preemptive version of the problem, i.e, $R | r_{ij},pmtn\n| \\sum_j w_j C_j$. We again use the idea of sampling offsets from non-uniform\ndistributions to give the first better than 2-approximation for this problem.\nThis improvement also requires use of a configuration LP with variables for\neach job's complete schedules along with more careful analysis. For both\nnon-preemptive and preemptive versions, we break the approximation barrier of 2\nfor the first time.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:31:00 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Im", "Sungjin", ""], ["Li", "Shi", ""]]}, {"id": "1606.08893", "submitter": "Christopher Whidden", "authors": "Chris Whidden and Frederick A. Matsen IV", "title": "Efficiently Inferring Pairwise Subtree Prune-and-Regraft Adjacencies\n  between Phylogenetic Trees", "comments": "21 pages, 3 figures. Revised in response to peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a time-optimal $O(mn^2)$-time algorithm to construct the subtree\nprune-regraft (SPR) graph on a collection of m phylogenetic trees with n\nleaves. This improves on the previous bound of $O(mn^3)$. Such graphs are used\nto better understand the behaviour of phylogenetic methods and recommend\nparameter choices and diagnostic criteria. The limiting factor in these\nanalyses has been the difficulty in constructing such graphs for large numbers\nof trees. We also develop the first efficient algorithms for constructing the\nnearest-neighbor interchange (NNI) and tree bisection-and-reconnection (TBR)\ngraphs\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 21:23:04 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 00:57:50 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 19:19:25 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Whidden", "Chris", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1606.09000", "submitter": "Till Fluschnik", "authors": "Ren\\'e van Bevern, Till Fluschnik, George B. Mertzios, Hendrik Molter,\n  Manuel Sorge, and Ond\\v{r}ej Such\\'y", "title": "The parameterized complexity of finding secluded solutions to some\n  classical optimization problems on graphs", "comments": "Compared to the previous version, this version additionally shows\n  that Small Secluded s-t-Separator is fixed-parameter tractable parameterized\n  by the combination of the solution size and the open neighborhood size\n  (Theorem 3.5). To appear in Discrete Optimization", "journal-ref": "Discrete Optimzation 30:20-50, 2018", "doi": "10.1016/j.disopt.2018.05.002", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the parameterized complexity of finding secluded solutions\nto classical combinatorial optimization problems on graphs such as finding\nminimum s-t separators, feedback vertex sets, dominating sets, maximum\nindependent sets, and vertex deletion problems for hereditary graph properties:\nHerein, one searches not only to minimize or maximize the size of the solution,\nbut also to minimize the size of its neighborhood. This restriction has\napplications in secure routing and community detection.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 08:42:35 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 09:41:17 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 12:33:05 GMT"}, {"version": "v4", "created": "Thu, 1 Jun 2017 07:04:31 GMT"}, {"version": "v5", "created": "Tue, 22 May 2018 12:16:02 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Fluschnik", "Till", ""], ["Mertzios", "George B.", ""], ["Molter", "Hendrik", ""], ["Sorge", "Manuel", ""], ["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "1606.09279", "submitter": "Phillippe Samer", "authors": "Phillippe Samer, Evellyn Cavalcante, Sebasti\\'an Urrutia, Johan Oppen", "title": "The matching relaxation for a class of generalized set partitioning\n  problems", "comments": "33 pages. A preliminary (4-page) version of this paper was presented\n  at CTW 2016 (Cologne-Twente Workshop on Graphs and Combinatorial\n  Optimization), with proceedings on Electronic Notes in Discrete Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a discrete relaxation for the class of combinatorial\noptimization problems which can be described by a set partitioning formulation\nunder packing constraints. We present two combinatorial relaxations based on\ncomputing maximum weighted matchings in suitable graphs. Besides providing dual\nbounds, the relaxations are also used on a variable reduction technique and a\nmatheuristic. We show how that general method can be tailored to sample\napplications, and also perform a successful computational evaluation with\nbenchmark instances of a problem in maritime logistics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:46:08 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 14:07:40 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 01:11:46 GMT"}, {"version": "v4", "created": "Mon, 30 Jan 2017 17:32:59 GMT"}, {"version": "v5", "created": "Fri, 11 May 2018 20:56:44 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Samer", "Phillippe", ""], ["Cavalcante", "Evellyn", ""], ["Urrutia", "Sebasti\u00e1n", ""], ["Oppen", "Johan", ""]]}, {"id": "1606.09395", "submitter": "Pavel Vesel\\'y", "authors": "Martin B\\\"ohm, Marek Chrobak, {\\L}ukasz Je\\.z, Fei Li, Ji\\v{r}\\'i\n  Sgall, Pavel Vesel\\'y", "title": "Online Packet Scheduling with Bounded Delay and Lookahead", "comments": "18 pages, 4 figures, submitted to ISAAC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online bounded-delay packet scheduling problem (BDPS), where\npackets of unit size arrive at a router over time and need to be transmitted\nover a network link. Each packet has two attributes: a non-negative weight and\na deadline for its transmission. The objective is to maximize the total weight\nof the transmitted packets. This problem has been well studied in the\nliterature, yet its optimal competitive ratio remains unknown: the best upper\nbound is $1.828$, still quite far from the best lower bound of $\\phi \\approx\n1.618$.\n  In the variant of BDPS with $s$-bounded instances, each packet can be\nscheduled in at most $s$ consecutive slots, starting at its release time. The\nlower bound of $\\phi$ applies even to the special case of $2$-bounded\ninstances, and a $\\phi$-competitive algorithm for $3$-bounded instances was\ngiven in Chin et al. Improving that result, and addressing a question posed by\nGoldwasser, we present a $\\phi$-competitive algorithm for $4$-bounded\ninstances.\n  We also study a variant of BDPS where an online algorithm has the additional\npower of $1$-lookahead, knowing at time $t$ which packets will arrive at time\n$t+1$. For BDPS with $1$-lookahead restricted to $2$-bounded instances, we\npresent an online algorithm with competitive ratio $(\\sqrt{13} - 1)/2 \\approx\n1.303$ and we prove a nearly tight lower bound of $(1 + \\sqrt{17})/4 \\approx\n1.281$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 09:02:03 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Chrobak", "Marek", ""], ["Je\u017c", "\u0141ukasz", ""], ["Li", "Fei", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1606.09402", "submitter": "Wenjian Yu Prof.", "authors": "Wenjian Yu, Yu Gu, and Yaohang Li", "title": "Efficient Randomized Algorithms for the Fixed-Precision Low-Rank Matrix\n  Approximation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms for low-rank matrix approximation are investigated,\nwith the emphasis on the fixed-precision problem and computational efficiency\nfor handling large matrices. The algorithms are based on the so-called QB\nfactorization, where Q is an orthonormal matrix. Firstly, a mechanism for\ncalculating the approximation error in Frobenius norm is proposed, which\nenables efficient adaptive rank determination for large and/or sparse matrix.\nIt can be combined with any QB-form factorization algorithm in which B's rows\nare incrementally generated. Based on the blocked randQB algorithm by P.-G.\nMartinsson and S. Voronin, this results in an algorithm called randQB EI. Then,\nwe further revise the algorithm to obtain a pass-efficient algorithm, randQB\nFP, which is mathematically equivalent to the existing randQB algorithms and\nalso suitable for the fixed-precision problem. Especially, randQB FP can serve\nas a single-pass algorithm for calculating leading singular values, under\ncertain condition. With large and/or sparse test matrices, we have empirically\nvalidated the merits of the proposed techniques, which exhibit remarkable\nspeedup and memory saving over the blocked randQB algorithm. We have also\ndemonstrated that the single-pass algorithm derived by randQB FP is much more\naccurate than an existing single-pass algorithm. And with data from a scenic\nimage and an information retrieval application, we have shown the advantages of\nthe proposed algorithms over the adaptive range finder algorithm for solving\nthe fixed-precision problem.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 09:14:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 08:06:04 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 06:44:17 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yu", "Wenjian", ""], ["Gu", "Yu", ""], ["Li", "Yaohang", ""]]}, {"id": "1606.09449", "submitter": "Sebastian Ordyniak", "authors": "Bernhard Bliem, Sebastian Ordyniak, Stefan Woltran", "title": "Clique-Width and Directed Width Measures for Answer-Set Programming", "comments": "A short version of this paper has been accepted to ECAI 2016 and\n  TAASP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disjunctive Answer Set Programming (ASP) is a powerful declarative\nprogramming paradigm whose main decision problems are located on the second\nlevel of the polynomial hierarchy. Identifying tractable fragments and\ndeveloping efficient algorithms for such fragments are thus important\nobjectives in order to complement the sophisticated ASP systems available to\ndate. Hard problems can become tractable if some problem parameter is bounded\nby a fixed constant; such problems are then called fixed-parameter tractable\n(FPT). While several FPT results for ASP exist, parameters that relate to\ndirected or signed graphs representing the program at hand have been neglected\nso far. In this paper, we first give some negative observations showing that\ndirected width measures on the dependency graph of a program do not lead to FPT\nresults. We then consider the graph parameter of signed clique-width and\npresent a novel dynamic programming algorithm that is FPT w.r.t. this\nparameter. Clique-width is more general than the well-known treewidth, and, to\nthe best of our knowledge, ours is the first FPT algorithm for bounded\nclique-width for reasoning problems beyond SAT.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 12:14:33 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 12:00:28 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Bliem", "Bernhard", ""], ["Ordyniak", "Sebastian", ""], ["Woltran", "Stefan", ""]]}, {"id": "1606.09481", "submitter": "Moritz von Looz-Corswarem", "authors": "Moritz von Looz and Mustafa \\\"Ozdayi and S\\\"oren Laue and Henning\n  Meyerhenke", "title": "Generating massive complex networks with hyperbolic geometry faster in\n  practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative network models play an important role in algorithm development,\nscaling studies, network analysis, and realistic system benchmarks for graph\ndata sets. The commonly used graph-based benchmark model R-MAT has some\ndrawbacks concerning realism and the scaling behavior of network properties. A\ncomplex network model gaining considerable popularity builds random hyperbolic\ngraphs, generated by distributing points within a disk in the hyperbolic plane\nand then adding edges between points whose hyperbolic distance is below a\nthreshold.\n  We present in this paper a fast generation algorithm for such graphs. Our\nexperiments show that our new generator achieves speedup factors of 3-60 over\nthe best previous implementation. One billion edges can now be generated in\nunder one minute on a shared-memory workstation. Furthermore, we present a\ndynamic extension to model gradual network change, while preserving at each\nstep the point position probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 13:28:52 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["von Looz", "Moritz", ""], ["\u00d6zdayi", "Mustafa", ""], ["Laue", "S\u00f6ren", ""], ["Meyerhenke", "Henning", ""]]}]