[{"id": "1501.00008", "submitter": "Aram Harrow", "authors": "Aram W. Harrow", "title": "Review of Quantum Algorithms for Systems of Linear Equations", "comments": "3 pages", "journal-ref": "Encyclopedia of Algorithms, pp. 1680-1683. Springer New York, 2016", "doi": "10.1007/978-1-4939-2864-4_771", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews the 2008 quantum algorithm for linear systems of\nequations due to Harrow, Hassidim and Lloyd, as well as some of the followup\nand related work. It was submitted to the Springer Encyclopedia of Algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 21:00:13 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Harrow", "Aram W.", ""]]}, {"id": "1501.00014", "submitter": "Rama  Cont", "authors": "Rama Cont and Massoud Heidari", "title": "Optimal rounding under integer constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given real numbers whose sum is an integer, we study the problem of finding\nintegers which match these real numbers as closely as possible, in the sense of\nL^p norm, while preserving the sum. We describe the structure of solutions for\nthis integer optimization problem and propose an algorithm with complexity O(N\nlog N) for solving it. In contrast to fractional rounding and randomized\nrounding, which yield biased estimators of the solution when applied to this\nproblem, our method yields an exact solution which minimizes the relative\nrounding error across the set of all solutions for any value of p greater than\n1, while avoiding the complexity of exhaustive search. The proposed algorithm\nalso solves a class of integer optimization problems with integer constraints\nand may be used as the rounding step of relaxed integer programming problems,\nfor rounding real-valued solutions. We give several examples of applications\nfor the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 21:01:08 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Cont", "Rama", ""], ["Heidari", "Massoud", ""]]}, {"id": "1501.00162", "submitter": "Martin Babka", "authors": "Martin Babka", "title": "Expected number of uniformly distributed balls in a most loaded bin\n  using placement with simple linear functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the size of a most loaded bin in the setting when the balls are\nplaced into the bins using a random linear function in a finite field. The\nballs are chosen from a transformed interval. We show that in this setting the\nexpected load of the most loaded bins is constant.\n  This is an interesting fact because using fully random hash functions with\nthe same class of input sets leads to an expectation of $\\Theta\\left(\\frac{\\log\nm}{\\log \\log m}\\right)$ balls in most loaded bins where $m$ is the number of\nballs and bins.\n  Although the family of the functions is quite common the size of largest bins\nwas not known even in this simple case.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 16:07:46 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Babka", "Martin", ""]]}, {"id": "1501.00212", "submitter": "Harold Gabow", "authors": "Harold N. Gabow", "title": "Set-merging for the Matching Algorithm of Micali and Vazirani", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithm of Micali and Vazirani \\cite{MV} finds a maximum cardinality\nmatching in time $O(\\sqrt n m)$ if an efficient set-merging algorithm is used.\nThe latter is provided by the incremental-tree set-merging algorithm of\n\\cite{GabTar}. Details of this application to matching were omitted from\n\\cite{GabTar} and are presented in this note.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 21:17:16 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Gabow", "Harold N.", ""]]}, {"id": "1501.00267", "submitter": "Aleksander M\\k{a}dry", "authors": "Aleksander Madry, Damian Straszak, Jakub Tarnawski", "title": "Fast Generation of Random Spanning Trees and the Effective Resistance\n  Metric", "comments": null, "journal-ref": "Proc. of 26th Annual ACM-SIAM Symposium on Discrete Algorithms\n  (SODA), 2015, pages 2019-2036", "doi": "10.1137/1.9781611973730.134", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for generating a uniformly random spanning tree in\nan undirected graph. Our algorithm samples such a tree in expected\n$\\tilde{O}(m^{4/3})$ time. This improves over the best previously known bound\nof $\\min(\\tilde{O}(m\\sqrt{n}),O(n^{\\omega}))$ -- that follows from the work of\nKelner and M\\k{a}dry [FOCS'09] and of Colbourn et al. [J. Algorithms'96] --\nwhenever the input graph is sufficiently sparse.\n  At a high level, our result stems from carefully exploiting the interplay of\nrandom spanning trees, random walks, and the notion of effective resistance, as\nwell as from devising a way to algorithmically relate these concepts to the\ncombinatorial structure of the graph. This involves, in particular,\nestablishing a new connection between the effective resistance metric and the\ncut structure of the underlying graph.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 10:28:01 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Madry", "Aleksander", ""], ["Straszak", "Damian", ""], ["Tarnawski", "Jakub", ""]]}, {"id": "1501.00288", "submitter": "Daniel Bienstock", "authors": "Daniel Bienstock and Gonzalo Munoz", "title": "LP approximations to mixed-integer polynomial optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of linear programming approximations for constrained\noptimization problems. In the case of mixed-integer polynomial optimization\nproblems, if the intersection graph of the constraints has bounded tree-width\nour construction yields a class of linear size formulations that attain any\ndesired tolerance. As a result, we obtain an approximation scheme for the\n\"AC-OPF\" problem on graphs with bounded tree-width. We also describe a more\ngeneral construction for pure binary optimization problems where individual\nconstraints are available through a membership oracle; if the intersection\ngraph for the constraints has bounded tree-width our construction is of linear\nsize and exact. This improves on a number of results in the literature, both\nfrom the perspective of formulation size and generality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 16:49:21 GMT"}, {"version": "v10", "created": "Wed, 1 Apr 2015 03:18:45 GMT"}, {"version": "v11", "created": "Fri, 3 Apr 2015 14:18:15 GMT"}, {"version": "v12", "created": "Wed, 27 May 2015 12:26:33 GMT"}, {"version": "v13", "created": "Wed, 24 Jun 2015 20:37:52 GMT"}, {"version": "v14", "created": "Fri, 3 Jul 2015 12:26:23 GMT"}, {"version": "v15", "created": "Wed, 19 Oct 2016 01:08:40 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 15:33:11 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 14:27:10 GMT"}, {"version": "v4", "created": "Thu, 29 Jan 2015 13:29:01 GMT"}, {"version": "v5", "created": "Mon, 2 Feb 2015 17:57:34 GMT"}, {"version": "v6", "created": "Wed, 4 Feb 2015 16:57:24 GMT"}, {"version": "v7", "created": "Fri, 6 Feb 2015 16:03:47 GMT"}, {"version": "v8", "created": "Tue, 17 Mar 2015 21:05:02 GMT"}, {"version": "v9", "created": "Sat, 21 Mar 2015 00:55:08 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Bienstock", "Daniel", ""], ["Munoz", "Gonzalo", ""]]}, {"id": "1501.00563", "submitter": "Meirav Zehavi", "authors": "Andreas Bj\\\"orklund, Vikram Kamat, {\\L}ukasz Kowalik and Meirav Zehavi", "title": "Spotting Trees with Few Leaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show two results related to the Hamiltonicity and $k$-Path algorithms in\nundirected graphs by Bj\\\"orklund [FOCS'10], and Bj\\\"orklund et al., [arXiv'10].\nFirst, we demonstrate that the technique used can be generalized to finding\nsome $k$-vertex tree with $l$ leaves in an $n$-vertex undirected graph in\n$O^*(1.657^k2^{l/2})$ time. It can be applied as a subroutine to solve the\n$k$-Internal Spanning Tree ($k$-IST) problem in $O^*(\\min(3.455^k, 1.946^n))$\ntime using polynomial space, improving upon previous algorithms for this\nproblem. In particular, for the first time we break the natural barrier of\n$O^*(2^n)$. Second, we show that the iterated random bipartition employed by\nthe algorithm can be improved whenever the host graph admits a vertex coloring\nwith few colors; it can be an ordinary proper vertex coloring, a fractional\nvertex coloring, or a vector coloring. In effect, we show improved bounds for\n$k$-Path and Hamiltonicity in any graph of maximum degree $\\Delta=4,\\ldots,12$\nor with vector chromatic number at most 8.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 13:19:12 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 12:12:33 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kamat", "Vikram", ""], ["Kowalik", "\u0141ukasz", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1501.00611", "submitter": "Shihyen Chen", "authors": "Shihyen Chen", "title": "A Review on the Tree Edit Distance Problem and Related\n  Path-Decomposition Algorithms", "comments": "23 pages. 13 figures. Revisions: minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ordered labeled tree is a tree in which the nodes are labeled and the\nleft-to-right order among siblings is relevant. The edit distance between two\nordered labeled trees is the minimum cost of changing one tree into the other\nthrough a sequence of edit steps. In the literature, there are a class of\nalgorithms based on different yet closely related path-decomposition schemes.\nThis article reviews the principles of these algorithms, and studies the\nconcepts related to the algorithmic complexities as a consequence of the\ndecomposition schemes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 22:31:32 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 03:37:32 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 06:40:59 GMT"}, {"version": "v4", "created": "Thu, 29 Jan 2015 02:08:44 GMT"}, {"version": "v5", "created": "Tue, 3 Feb 2015 23:07:44 GMT"}, {"version": "v6", "created": "Mon, 9 Feb 2015 02:12:18 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Chen", "Shihyen", ""]]}, {"id": "1501.00624", "submitter": "Ran Gelles", "authors": "Klim Efremenko, Ran Gelles, Bernhard Haeupler", "title": "Maximal Noise in Interactive Communication over Erasure Channels and\n  Channels with Feedback", "comments": "A preliminary version of this work appears at ITCS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide tight upper and lower bounds on the noise resilience of\ninteractive communication over noisy channels with feedback. In this setting,\nwe show that the maximal fraction of noise that any robust protocol can resist\nis 1/3. Additionally, we provide a simple and efficient robust protocol that\nsucceeds as long as the fraction of noise is at most 1/3 - \\epsilon.\nSurprisingly, both bounds hold regardless of whether the parties send bits or\nsymbols from an arbitrarily large alphabet. We also consider interactive\ncommunication over erasure channels. We provide a protocol that matches the\noptimal tolerable erasure rate of 1/2 - \\epsilon of previous protocols\n(Franklin et al., CRYPTO '13) but operates in a much simpler and more efficient\nway. Our protocol works with an alphabet of size 4, in contrast to prior\nprotocols in which the alphabet size grows as epsilon goes to zero. Building on\nthe above algorithm with a fixed alphabet size, we are able to devise a\nprotocol for binary erasure channels that tolerates erasure rates of up to 1/3\n- \\epsilon.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 02:14:44 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Efremenko", "Klim", ""], ["Gelles", "Ran", ""], ["Haeupler", "Bernhard", ""]]}, {"id": "1501.00665", "submitter": "Shmuel Onn", "authors": "Shmuel Onn, Pauline Sarrabezolles", "title": "Huge Unimodular N-Fold Programs", "comments": null, "journal-ref": "SIAM Journal on Discrete Mathematics, 29:2277-2283, 2015", "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization over $l\\times m\\times n$ integer $3$-way tables with given\nline-sums is NP-hard already for fixed $l=3$, but is polynomial time solvable\nwith both $l,m$ fixed. In the {\\em huge} version of the problem, the variable\ndimension $n$ is encoded in {\\em binary}, with $t$ {\\em layer types}. It was\nrecently shown that the huge problem can be solved in polynomial time for fixed\n$t$, and the complexity of the problem for variable $t$ was raised as an open\nproblem. Here we solve this problem and show that the huge table problem can be\nsolved in polynomial time even when the number $t$ of types is {\\em variable}.\nThe complexity of the problem over $4$-way tables with variable $t$ remains\nopen. Our treatment goes through the more general class of {\\em huge $n$-fold\ninteger programming problems}. We show that huge integer programs over $n$-fold\nproducts of totally unimodular matrices can be solved in polynomial time even\nwhen the number $t$ of brick types is variable.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 11:04:09 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Onn", "Shmuel", ""], ["Sarrabezolles", "Pauline", ""]]}, {"id": "1501.00696", "submitter": "Pauli Miettinen", "authors": "Saskia Metzler and Pauli Miettinen", "title": "Clustering Boolean Tensors", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-015-0420-3", "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorizations are computationally hard problems, and in particular,\nare often significantly harder than their matrix counterparts. In case of\nBoolean tensor factorizations -- where the input tensor and all the factors are\nrequired to be binary and we use Boolean algebra -- much of that hardness comes\nfrom the possibility of overlapping components. Yet, in many applications we\nare perfectly happy to partition at least one of the modes. In this paper we\ninvestigate what consequences does this partitioning have on the computational\ncomplexity of the Boolean tensor factorizations and present a new algorithm for\nthe resulting clustering problem. This algorithm can alternatively be seen as a\nparticularly regularized clustering algorithm that can handle extremely\nhigh-dimensional observations. We analyse our algorithms with the goal of\nmaximizing the similarity and argue that this is more meaningful than\nminimizing the dissimilarity. As a by-product we obtain a PTAS and an efficient\n0.828-approximation algorithm for rank-1 binary factorizations. Our algorithm\nfor Boolean tensor clustering achieves high scalability, high similarity, and\ngood generalization to unseen data with both synthetic and real-world data\nsets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 17:01:03 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Metzler", "Saskia", ""], ["Miettinen", "Pauli", ""]]}, {"id": "1501.00828", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow and Galina Cariowa", "title": "A new algorithm for multiplying two Dirac numbers", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a rationalized algorithm for Dirac numbers multiplication is\npresented. This algorithm has a low computational complexity feature and is\nwell suited to FPGA implementation. The computation of two Dirac numbers\nproduct using the na\\\"ive method takes 256 real multiplications and 240 real\nadditions, while the proposed algorithm can compute the same result in only 88\nreal multiplications and 256 real additions. During synthesis of the discussed\nalgorithm we use the fact that Dirac numbers product may be represented as\nvector-matrix product. The matrix participating in the product has unique\nstructural properties that allow performing its advantageous decomposition.\nNamely this decomposition leads to significant reducing of the computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 12:00:11 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""]]}, {"id": "1501.01062", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Ilya Razenshteyn", "title": "Optimal Data-Dependent Hashing for Approximate Near Neighbors", "comments": "36 pages, 5 figures, an extended abstract appeared in the proceedings\n  of the 47th ACM Symposium on Theory of Computing (STOC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an optimal data-dependent hashing scheme for the approximate near\nneighbor problem. For an $n$-point data set in a $d$-dimensional space our data\nstructure achieves query time $O(d n^{\\rho+o(1)})$ and space $O(n^{1+\\rho+o(1)}\n+ dn)$, where $\\rho=\\tfrac{1}{2c^2-1}$ for the Euclidean space and\napproximation $c>1$. For the Hamming space, we obtain an exponent of\n$\\rho=\\tfrac{1}{2c-1}$.\n  Our result completes the direction set forth in [AINR14] who gave a\nproof-of-concept that data-dependent hashing can outperform classical Locality\nSensitive Hashing (LSH). In contrast to [AINR14], the new bound is not only\noptimal, but in fact improves over the best (optimal) LSH data structures\n[IM98,AI06] for all approximation factors $c>1$.\n  From the technical perspective, we proceed by decomposing an arbitrary\ndataset into several subsets that are, in a certain sense, pseudo-random.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 02:21:59 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 04:12:39 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 03:37:53 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Andoni", "Alexandr", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1501.01300", "submitter": "Christopher Griffin", "authors": "Elisabeth Paulson and Christopher Griffin", "title": "Minimum Probabilistic Finite State Learning Problem on Finite Data Sets:\n  Complexity, Solution and Approximations", "comments": "Updated from original paper with improved proofs of result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of determining a minimum state\nprobabilistic finite state machine capable of generating statistically\nidentical symbol sequences to samples provided. This problem is qualitatively\nsimilar to the classical Hidden Markov Model problem and has been studied from\na practical point of view in several works beginning with the work presented\nin: Shalizi, C.R., Shalizi, K.L., Crutchfield, J.P. (2002) \\textit{An algorithm\nfor pattern discovery in time series.} Technical Report 02-10-060, Santa Fe\nInstitute. arxiv.org/abs/cs.LG/0210025. We show that the underlying problem is\n$\\mathrm{NP}$-hard and thus all existing polynomial time algorithms must be\napproximations on finite data sets. Using our $\\mathrm{NP}$-hardness proof, we\nshow how to construct a provably correct algorithm for constructing a minimum\nstate probabilistic finite state machine given data and empirically study its\nrunning time.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 22:09:26 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 16:18:35 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Paulson", "Elisabeth", ""], ["Griffin", "Christopher", ""]]}, {"id": "1501.01429", "submitter": "Gabriele Fici", "authors": "Gabriele Fici, Thierry Lecroq, Arnaud Lefebvre, \\'Elise Prieur-Gaston", "title": "Online Computation of Abelian Runs", "comments": "To appear in the Proceedings of LATA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a word $w$ and a Parikh vector $\\mathcal{P}$, an abelian run of period\n$\\mathcal{P}$ in $w$ is a maximal occurrence of a substring of $w$ having\nabelian period $\\mathcal{P}$. We give an algorithm that finds all the abelian\nruns of period $\\mathcal{P}$ in a word of length $n$ in time $O(n\\times\n|\\mathcal{P}|)$ and space $O(\\sigma+|\\mathcal{P}|)$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 10:22:12 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Fici", "Gabriele", ""], ["Lecroq", "Thierry", ""], ["Lefebvre", "Arnaud", ""], ["Prieur-Gaston", "\u00c9lise", ""]]}, {"id": "1501.01571", "submitter": "Joel Tropp", "authors": "Joel A. Tropp", "title": "An Introduction to Matrix Concentration Inequalities", "comments": "163 pages. To appear in Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, random matrices have come to play a major role in\ncomputational mathematics, but most of the classical areas of random matrix\ntheory remain the province of experts. Over the last decade, with the advent of\nmatrix concentration inequalities, research has advanced to the point where we\ncan conquer many (formerly) challenging problems with a page or two of\narithmetic. The aim of this monograph is to describe the most successful\nmethods from this area along with some interesting examples that these\ntechniques can illuminate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 17:46:02 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Tropp", "Joel A.", ""]]}, {"id": "1501.01689", "submitter": "Ananda Theertha Suresh", "authors": "Aditya Bhaskara, Ananda Theertha Suresh, Morteza Zadimoghaddam", "title": "Sparse Solutions to Nonnegative Linear Systems and Applications", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm for finding sparse approximate solutions to\nlinear systems of equations with nonnegative coefficients. Unlike most known\nresults for sparse recovery, we do not require {\\em any} assumption on the\nmatrix other than non-negativity. Our algorithm is combinatorial in nature,\ninspired by techniques for the set cover problem, as well as the multiplicative\nweight update method.\n  We then present a natural application to learning mixture models in the PAC\nframework. For learning a mixture of $k$ axis-aligned Gaussians in $d$\ndimensions, we give an algorithm that outputs a mixture of $O(k/\\epsilon^3)$\nGaussians that is $\\epsilon$-close in statistical distance to the true\ndistribution, without any separation assumptions. The time and sample\ncomplexity is roughly $O(kd/\\epsilon^3)^{d}$. This is polynomial when $d$ is\nconstant -- precisely the regime in which known methods fail to identify the\ncomponents efficiently.\n  Given that non-negativity is a natural assumption, we believe that our result\nmay find use in other settings in which we wish to approximately explain data\nusing a small number of a (large) candidate set of components.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 23:38:46 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Suresh", "Ananda Theertha", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1501.01696", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal, Daniel P. Miranker", "title": "On the Complexity of Sorted Neighborhood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage concerns identifying semantically equivalent records in\ndatabases. Blocking methods are employed to avoid the cost of full pairwise\nsimilarity comparisons on $n$ records. In a seminal work, Hernandez and Stolfo\nproposed the Sorted Neighborhood blocking method. Several empirical variants\nhave been proposed in recent years. In this paper, we investigate the\ncomplexity of the Sorted Neighborhood procedure on which the variants are\nbuilt. We show that achieving maximum performance on the Sorted Neighborhood\nprocedure entails solving a sub-problem, which is shown to be NP-complete by\nreducing from the Travelling Salesman Problem. We also show that the\nsub-problem can occur in the traditional blocking method. Finally, we draw on\nrecent developments concerning approximate Travelling Salesman solutions to\ndefine and analyze three approximation algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 00:49:45 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Miranker", "Daniel P.", ""]]}, {"id": "1501.01711", "submitter": "Mina Ghashami", "authors": "Mina Ghashami, Edo Liberty, Jeff M. Phillips and David P. Woodruff", "title": "Frequent Directions : Simple and Deterministic Matrix Sketching", "comments": "28 pages , This paper contains Frequent Directions algorithm (see\n  arXiv:1206.0594) and relative error bound on it (see arXiv:1307.7454)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm called Frequent Directions for deterministic\nmatrix sketching in the row-updates model. The algorithm is presented an\narbitrary input matrix $A \\in R^{n \\times d}$ one row at a time. It performed\n$O(d \\times \\ell)$ operations per row and maintains a sketch matrix $B \\in\nR^{\\ell \\times d}$ such that for any $k < \\ell$\n  $\\|A^TA - B^TB \\|_2 \\leq \\|A - A_k\\|_F^2 / (\\ell-k)$ and $\\|A -\n\\pi_{B_k}(A)\\|_F^2 \\leq \\big(1 + \\frac{k}{\\ell-k}\\big) \\|A-A_k\\|_F^2 $ .\n  Here, $A_k$ stands for the minimizer of $\\|A - A_k\\|_F$ over all rank $k$\nmatrices (similarly $B_k$) and $\\pi_{B_k}(A)$ is the rank $k$ matrix resulting\nfrom projecting $A$ on the row span of $B_k$. We show both of these bounds are\nthe best possible for the space allowed. The summary is mergeable, and hence\ntrivially parallelizable. Moreover, Frequent Directions outperforms exemplar\nimplementations of existing streaming algorithms in the space-error tradeoff.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 02:32:32 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 16:17:01 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Ghashami", "Mina", ""], ["Liberty", "Edo", ""], ["Phillips", "Jeff M.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1501.01720", "submitter": "Yuanqing (Kai) Xiao", "authors": "Jeffrey Ling, Kai Xiao, Dai Yang", "title": "Online Algorithms Modeled After Mousehunt", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a variety of novel online algorithm problems inspired\nby the game Mousehunt. We consider a number of basic models that approximate\nthe game, and we provide solutions to these models using Markov Decision\nProcesses, deterministic online algorithms, and randomized online algorithms.\nWe analyze these solutions' performance by deriving results on their\ncompetitive ratios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 03:54:16 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Ling", "Jeffrey", ""], ["Xiao", "Kai", ""], ["Yang", "Dai", ""]]}, {"id": "1501.01741", "submitter": "Mary Radcliffe", "authors": "Franklin Kenter and Mary Radcliffe", "title": "A linear k-fold Cheeger inequality", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.PR math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G$, the classical Cheeger constant, $h_G$,\nmeasures the optimal partition of the vertices into 2 parts with relatively few\nedges between them based upon the sizes of the parts. The well-known Cheeger's\ninequality states that $2 \\lambda_1 \\le h_G \\le \\sqrt {2 \\lambda_1}$ where\n$\\lambda_1$ is the minimum nontrivial eigenvalue of the normalized Laplacian\nmatrix.\n  Recent work has generalized the concept of the Cheeger constant when\npartitioning the vertices of a graph into $k > 2$ parts. While there are\nseveral approaches, recent results have shown these higher-order Cheeger\nconstants to be tightly controlled by $\\lambda_{k-1}$, the $(k-1)$-th\nnontrivial eigenvalue, to within a quadratic factor.\n  We present a new higher-order Cheeger inequality with several new\nperspectives. First, we use an alternative higher-order Cheeger constant which\nconsiders an \"average case\" approach. We show this measure is related to the\naverage of the first $k-1$ nontrivial eigenvalues of the normalized Laplacian\nmatrix. Further, using recent techniques, our results provide linear\ninequalities using the $\\infty$-norms of the corresponding eigenvectors.\nConsequently, unlike previous results, this result is relevant even when\n$\\lambda_{k-1} \\to 1$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 06:31:31 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 19:34:10 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Kenter", "Franklin", ""], ["Radcliffe", "Mary", ""]]}, {"id": "1501.01783", "submitter": "Nicolas Bonichon", "authors": "Nicolas Bonichon (LaBRI), Prosenjit Bose, Jean-Lou De Carufel,\n  Ljubomir Perkovi\\'c (SOC), Andr\\'e Van Renssen (NII)", "title": "Upper and Lower Bounds for Competitive Online Routing on Delaunay\n  Triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a weighted graph G where vertices are points in the plane and edges\nare line segments. The weight of each edge is the Euclidean distance between\nits two endpoints. A routing algorithm on G has a competitive ratio of c if the\nlength of the path produced by the algorithm from any vertex s to any vertex t\nis at most c times the length of the shortest path from s to t in G. If the\nlength of the path is at most c times the Euclidean distance from s to t, we\nsay that the routing algorithm on G has a routing ratio of c.We present an\nonline routing algorithm on the Delaunay triangulation with competitive and\nrouting ratios of 5.90. This improves upon the best known algorithm that has\ncompetitive and routing ratio 15.48. The algorithm is a generalization of the\ndeterministic 1-local routing algorithm by Chew on the L1-Delaunay\ntriangulation. When a message follows the routing path produced by our\nalgorithm, its header need only contain the coordinates of s and t. This is an\nimprovement over the currently known competitive routing algorithms on the\nDelaunay triangulation, for which the header of a message must additionally\ncontain partial sums of distances along the routing path.We also show that the\nrouting ratio of any deterministic k-local algorithm is at least 1.70 for the\nDelaunay triangulation and 2.70 for the L1-Delaunay triangulation. In the case\nof the L1-Delaunay triangulation, this implies that even though there exists a\npath between two points x and y whose length is at most 2.61|[xy]| (where\n|[xy]| denotes the length of the line segment [xy]), it is not always possible\nto route a message along a path of length less than 2.70|[xy]|. From these\nbounds on the routing ratio, we derive lower bounds on the competitive ratio of\n1.23 for Delaunay triangulations and 1.12 for L1-Delaunay triangulations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 10:09:06 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Bonichon", "Nicolas", "", "LaBRI"], ["Bose", "Prosenjit", "", "SOC"], ["De Carufel", "Jean-Lou", "", "SOC"], ["Perkovi\u0107", "Ljubomir", "", "SOC"], ["Van Renssen", "Andr\u00e9", "", "NII"]]}, {"id": "1501.01939", "submitter": "Leman Akoglu", "authors": "Hau Chan and Shuchu Han and Leman Akoglu", "title": "Where Graph Topology Matters: The Robust Subgraph Problem", "comments": "13 pages, 10 Figures, 3 Tables, to appear at SDM 2015 (9 pages only)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness is a critical measure of the resilience of large networked\nsystems, such as transportation and communication networks. Most prior works\nfocus on the global robustness of a given graph at large, e.g., by measuring\nits overall vulnerability to external attacks or random failures. In this\npaper, we turn attention to local robustness and pose a novel problem in the\nlines of subgraph mining: given a large graph, how can we find its most robust\nlocal subgraph (RLS)?\n  We define a robust subgraph as a subset of nodes with high communicability\namong them, and formulate the RLS-PROBLEM of finding a subgraph of given size\nwith maximum robustness in the host graph. Our formulation is related to the\nrecently proposed general framework for the densest subgraph problem, however\ndiffers from it substantially in that besides the number of edges in the\nsubgraph, robustness also concerns with the placement of edges, i.e., the\nsubgraph topology. We show that the RLS-PROBLEM is NP-hard and propose two\nheuristic algorithms based on top-down and bottom-up search strategies.\nFurther, we present modifications of our algorithms to handle three practical\nvariants of the RLS-PROBLEM. Experiments on synthetic and real-world graphs\ndemonstrate that we find subgraphs with larger robustness than the densest\nsubgraphs even at lower densities, suggesting that the existing approaches are\nnot suitable for the new problem setting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 19:54:03 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Chan", "Hau", ""], ["Han", "Shuchu", ""], ["Akoglu", "Leman", ""]]}, {"id": "1501.01941", "submitter": "Daniel Lemire", "authors": "Adina Crainiceanu and Daniel Lemire", "title": "Bloofi: Multidimensional Bloom Filters", "comments": null, "journal-ref": "Information Systems Volume 54, December 2015, Pages 311-324", "doi": "10.1016/j.is.2015.01.002", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bloom filters are probabilistic data structures commonly used for approximate\nmembership problems in many areas of Computer Science (networking, distributed\nsystems, databases, etc.). With the increase in data size and distribution of\ndata, problems arise where a large number of Bloom filters are available, and\nall them need to be searched for potential matches. As an example, in a\nfederated cloud environment, each cloud provider could encode the information\nusing Bloom filters and share the Bloom filters with a central coordinator. The\nproblem of interest is not only whether a given element is in any of the sets\nrepresented by the Bloom filters, but which of the existing sets contain the\ngiven element. This problem cannot be solved by just constructing a Bloom\nfilter on the union of all the sets. Instead, we effectively have a\nmultidimensional Bloom filter problem: given an element, we wish to receive a\nlist of candidate sets where the element might be.\n  To solve this problem, we consider 3 alternatives. Firstly, we can naively\ncheck many Bloom filters. Secondly, we propose to organize the Bloom filters in\na hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally,\nwe propose another data structure that packs the Bloom filters in such a way as\nto exploit bit-level parallelism, which we call Flat-Bloofi.\n  Our theoretical and experimental results show that Bloofi and Flat-Bloofi\nprovide scalable and efficient solutions alternatives to search through a large\nnumber of Bloom filters.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 20:04:46 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 15:31:25 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 18:37:56 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Crainiceanu", "Adina", ""], ["Lemire", "Daniel", ""]]}, {"id": "1501.02062", "submitter": "Sankalp Bagaria", "authors": "Sankalp Bagaria", "title": "New Hashing Algorithm for Use in TCP Reassembly Module of IPS", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since last decade, IDS/ IPS has gained popularity in protecting large\nnetworks. They can employ signature based techniques and/or flow-based\ntechniques to prevent intrusion from outside/ inside the network they are\ntrying to protect. Signature based IDS/ IPS can be stateless or stateful.\nStateful IDS can store the state of the protocol and use it for better\ndetection of malware. In the case of TCP/IP networks, an attacker can also\nlaunch an attack such that the malicious code is distributed over many packets.\nThese packets pass through the traditional IDS/ IPS and reassemble inside the\nnetwork. Once re-assembled inside the network by the TCP/IP layer, the\nmalicious code launches an attack.\n  The TCP state and a copy of last few packets for each active connection has\nto be maintained in IDS/IPS. In TCP re-assembly, packets are re-assembled at\nIDS/IPS and searched for signature matches. A connection table has to be\nmaintained for active connections and their list of last few (atmost 11)\npackets that have already arrived. We need data structures for searching the\nconnection that the latest incoming packet belongs to. Popular hashing\nalgorithms like CRC, XOR, summing tuple, taking modulus are inefficient as hash\nkeys are not evenly distributed in hash-key space. Thus we show how an\nalgorithm based on cryptography concepts can be used for efficient hashing in\nnetwork connection management. We also show how to use full four tuple for\ncalculating hash key instead of simply summing the tuple and taking the modulus\nof the sum.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 07:58:08 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Bagaria", "Sankalp", ""]]}, {"id": "1501.02285", "submitter": "Sergio Cabello", "authors": "Sergio Cabello and Pablo P\\'erez-Lantero", "title": "Interval Selection in the Streaming Model", "comments": "Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of intervals is independent when the intervals are pairwise disjoint.\nIn the interval selection problem we are given a set $\\mathbb{I}$ of intervals\nand we want to find an independent subset of intervals of largest cardinality.\nLet $\\alpha(\\mathbb{I})$ denote the cardinality of an optimal solution. We\ndiscuss the estimation of $\\alpha(\\mathbb{I})$ in the streaming model, where we\nonly have one-time, sequential access to the input intervals, the endpoints of\nthe intervals lie in $\\{1,...,n \\}$, and the amount of the memory is\nconstrained.\n  For intervals of different sizes, we provide an algorithm in the data stream\nmodel that computes an estimate $\\hat\\alpha$ of $\\alpha(\\mathbb{I})$ that, with\nprobability at least $2/3$, satisfies $\\tfrac 12(1-\\varepsilon)\n\\alpha(\\mathbb{I}) \\le \\hat\\alpha \\le \\alpha(\\mathbb{I})$. For same-length\nintervals, we provide another algorithm in the data stream model that computes\nan estimate $\\hat\\alpha$ of $\\alpha(\\mathbb{I})$ that, with probability at\nleast $2/3$, satisfies $\\tfrac 23(1-\\varepsilon) \\alpha(\\mathbb{I}) \\le\n\\hat\\alpha \\le \\alpha(\\mathbb{I})$. The space used by our algorithms is bounded\nby a polynomial in $\\varepsilon^{-1}$ and $\\log n$. We also show that no better\nestimations can be achieved using $o(n)$ bits of storage.\n  We also develop new, approximate solutions to the interval selection problem,\nwhere we want to report a feasible solution, that use $O(\\alpha(\\mathbb{I}))$\nspace. Our algorithms for the interval selection problem match the optimal\nresults by Emek, Halld{\\'o}rsson and Ros{\\'e}n [Space-Constrained Interval\nSelection, ICALP 2012], but are much simpler.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 22:52:21 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 08:21:48 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Cabello", "Sergio", ""], ["P\u00e9rez-Lantero", "Pablo", ""]]}, {"id": "1501.02309", "submitter": "Haitao Wang", "authors": "Jian Li and Haitao Wang", "title": "Range Queries on Uncertain Data", "comments": "26 pages. A preliminary version of this paper appeared in ISAAC 2014.\n  In this full version, we also present solutions to the most general case of\n  the problem (i.e., the histogram bounded case), which were left as open\n  problems in the preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ uncertain points on the real line, each represented by\nits one-dimensional probability density function, we consider the problem of\nbuilding data structures on $P$ to answer range queries of the following three\ntypes for any query interval $I$: (1) top-$1$ query: find the point in $P$ that\nlies in $I$ with the highest probability, (2) top-$k$ query: given any integer\n$k\\leq n$ as part of the query, return the $k$ points in $P$ that lie in $I$\nwith the highest probabilities, and (3) threshold query: given any threshold\n$\\tau$ as part of the query, return all points of $P$ that lie in $I$ with\nprobabilities at least $\\tau$. We present data structures for these range\nqueries with linear or nearly linear space and efficient query time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 04:18:08 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Li", "Jian", ""], ["Wang", "Haitao", ""]]}, {"id": "1501.02388", "submitter": "Christian Blum", "authors": "Christian Blum and G\\\"unther R. Raidl", "title": "Computational Performance Evaluation of Two Integer Linear Programming\n  Models for the Minimum Common String Partition Problem", "comments": "arXiv admin note: text overlap with arXiv:1405.5646 This paper\n  version replaces the one submitted on January 10, 2015, due to detected error\n  in the calculation of the variables involved in the ILP models", "journal-ref": null, "doi": "10.1007/s11590-015-0921-4", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the minimum common string partition (MCSP) problem two related input\nstrings are given. \"Related\" refers to the property that both strings consist\nof the same set of letters appearing the same number of times in each of the\ntwo strings. The MCSP seeks a minimum cardinality partitioning of one string\ninto non-overlapping substrings that is also a valid partitioning for the\nsecond string. This problem has applications in bioinformatics e.g. in\nanalyzing related DNA or protein sequences. For strings with lengths less than\nabout 1000 letters, a previously published integer linear programming (ILP)\nformulation yields, when solved with a state-of-the-art solver such as CPLEX,\nsatisfactory results. In this work, we propose a new, alternative ILP model\nthat is compared to the former one. While a polyhedral study shows the linear\nprogramming relaxations of the two models to be equally strong, a comprehensive\nexperimental comparison using real-world as well as artificially created\nbenchmark instances indicates substantial computational advantages of the new\nformulation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 20:28:39 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 11:32:47 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Blum", "Christian", ""], ["Raidl", "G\u00fcnther R.", ""]]}, {"id": "1501.02487", "submitter": "Muhammad Omer Bin Saeed", "authors": "Muhammad Omer Bin Saeed", "title": "A Unified Analysis Approach for LMS-based Variable Step-Size Algorithms", "comments": "5 pages, 1 figure, 5 tables", "journal-ref": null, "doi": "10.1007/s13369-017-2453-y", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least-mean-squares (LMS) algorithm is the most popular algorithm in\nadaptive filtering. Several variable step-size strategies have been suggested\nto improve the performance of the LMS algorithm. These strategies enhance the\nperformance of the algorithm but a major drawback is the complexity in the\ntheoretical analysis of the resultant algorithms. Researchers use several\nassumptions to find closed-form analytical solutions. This work presents a\nunified approach for the analysis of variable step-size LMS algorithms. The\napproach is then applied to several variable step-size strategies and\ntheoretical and simulation results are compared.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 19:02:16 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Saeed", "Muhammad Omer Bin", ""]]}, {"id": "1501.02492", "submitter": "Ruslan Savchenko", "authors": "Maxim Babenko, Andrew V. Goldberg, Haim Kaplan, Ruslan Savchenko,\n  Mathias Weller", "title": "On the Complexity of Hub Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hub Labeling (HL) is a data structure for distance oracles. Hierarchical HL\n(HHL) is a special type of HL, that received a lot of attention from a\npractical point of view. However, theoretical questions such as NP-hardness and\napproximation guarantee for HHL algorithms have been left aside. In this paper\nwe study HL and HHL from the complexity theory point of view. We prove that\nboth HL and HHL are NP-hard, and present upper and lower bounds for the\napproximation ratios of greedy HHL algorithms used in practice. We also\nintroduce a new variant of the greedy HHL algorithm and a proof that it\nproduces small labels for graphs with small highway dimension.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 20:28:35 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Babenko", "Maxim", ""], ["Goldberg", "Andrew V.", ""], ["Kaplan", "Haim", ""], ["Savchenko", "Ruslan", ""], ["Weller", "Mathias", ""]]}, {"id": "1501.02716", "submitter": "Peter Robinson", "authors": "Martin Biely and Peter Robinson and Ulrich Schmid and Manfred Schwarz\n  and Kyrill Winkler", "title": "Gracefully Degrading Consensus and $k$-Set Agreement in Directed Dynamic\n  Networks", "comments": "arXiv admin note: text overlap with arXiv:1204.0641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed agreement in synchronous directed dynamic networks,\nwhere an omniscient message adversary controls the availability of\ncommunication links. We prove that consensus is impossible under a message\nadversary that guarantees weak connectivity only, and introduce vertex-stable\nroot components (VSRCs) as a means for circumventing this impossibility: A\nVSRC(k, d) message adversary guarantees that, eventually, there is an interval\nof $d$ consecutive rounds where every communication graph contains at most $k$\nstrongly (dynamic) connected components consisting of the same processes, which\nhave at most outgoing links to the remaining processes. We present a consensus\nalgorithm that works correctly under a VSRC(1, 4H + 2) message adversary, where\n$H$ is the dynamic causal network diameter. On the other hand, we show that\nconsensus is impossible against a VSRC(1, H - 1) or a VSRC(2, $\\infty$) message\nadversary, revealing that there is not much hope to deal with stronger message\nadversaries.\n  However, we show that gracefully degrading consensus, which degrades to\ngeneral $k$-set agreement in case of unfavourable network conditions, is\nfeasible against stronger message adversaries: We provide a $k$-uniform $k$-set\nagreement algorithm, where the number of system-wide decision values $k$ is not\nencoded in the algorithm, but rather determined by the actual power of the\nmessage adversary in a run: Our algorithm guarantees at most $k$ decision\nvalues under a VSRC(n, d) + MAJINF(k) message adversary, which combines VSRC(n,\nd) (for some small $d$, ensuring termination) with some information flow\nguarantee MAJINF(k) between certain VSRCs (ensuring $k$-agreement). Our results\nprovide a significant step towards the exact solvability/impossibility border\nof general $k$-set agreement in directed dynamic networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 16:54:52 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Biely", "Martin", ""], ["Robinson", "Peter", ""], ["Schmid", "Ulrich", ""], ["Schwarz", "Manfred", ""], ["Winkler", "Kyrill", ""]]}, {"id": "1501.02911", "submitter": "Vitaly Feldman", "authors": "Miklos Ajtai, Vitaly Feldman, Avinatan Hassidim and Jelani Nelson", "title": "Sorting and Selection with Imprecise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a simple model of imprecise comparisons: there exists some\n$\\delta>0$ such that when a subject is given two elements to compare, if the\nvalues of those elements (as perceived by the subject) differ by at least\n$\\delta$, then the comparison will be made correctly; when the two elements\nhave values that are within $\\delta$, the outcome of the comparison is\nunpredictable. This model is inspired by both imprecision in human judgment of\nvalues and also by bounded but potentially adversarial errors in the outcomes\nof sporting tournaments.\n  Our model is closely related to a number of models commonly considered in the\npsychophysics literature where $\\delta$ corresponds to the {\\em just noticeable\ndifference unit (JND)} or {\\em difference threshold}. In experimental\npsychology, the method of paired comparisons was proposed as a means for\nranking preferences amongst $n$ elements of a human subject. The method\nrequires performing all $\\binom{n}{2}$ comparisons, then sorting elements\naccording to the number of wins. The large number of comparisons is performed\nto counter the potentially faulty decision-making of the human subject, who\nacts as an imprecise comparator.\n  We show that in our model the method of paired comparisons has optimal\naccuracy, minimizing the errors introduced by the imprecise comparisons.\nHowever, it is also wasteful, as it requires all $\\binom{n}{2}$. We show that\nthe same optimal guarantees can be achieved using $4 n^{3/2}$ comparisons, and\nwe prove the optimality of our method. We then explore the general tradeoff\nbetween the guarantees on the error that can be made and number of comparisons\nfor the problems of sorting, max-finding, and selection. Our results provide\nstrong lower bounds and close-to-optimal solutions for each of these problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 08:24:19 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Ajtai", "Miklos", ""], ["Feldman", "Vitaly", ""], ["Hassidim", "Avinatan", ""], ["Nelson", "Jelani", ""]]}, {"id": "1501.03105", "submitter": "David Gleich", "authors": "Yao Zhu and David F. Gleich", "title": "A Parallel Min-Cut Algorithm using Iteratively Reweighted Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm for the undirected $s,t$-mincut problem with\nfloating-point valued weights. Our overarching algorithm uses an iteratively\nreweighted least squares framework. This generates a sequence of Laplacian\nlinear systems, which we solve using parallel matrix algorithms. Our overall\nimplementation is up to 30-times faster than a serial solver when using 128\ncores.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 18:48:27 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Zhu", "Yao", ""], ["Gleich", "David F.", ""]]}, {"id": "1501.03435", "submitter": "Hema Thiagarajan", "authors": "K.S. Easwarakumar and T. Hema", "title": "BITS-Tree-An Efficient Data Structure for Segment Storage and Query\n  Processing", "comments": "11 pages, 5 figures", "journal-ref": "International Journal of Computers and\n  Technology,11(10):3108-3116, December 2013", "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new and novel data structure is proposed to dynamically\ninsert and delete segments. Unlike the standard segment trees[3], the proposed\ndata structure permits insertion of a segment with interval range beyond the\ninterval range of the existing tree, which is the interval between minimum and\nmaximum values of the end points of all the segments. Moreover, the number of\nnodes in the proposed tree is lesser as compared to the dynamic version of the\nstandard segment trees, and is able to answer both stabbing and range queries\npractically much faster compared to the standard segment trees.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 18:28:45 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Easwarakumar", "K. S.", ""], ["Hema", "T.", ""]]}, {"id": "1501.03461", "submitter": "Ariful Azad", "authors": "Ariful Azad", "title": "An Algorithmic Pipeline for Analyzing Multi-parametric Flow Cytometry\n  Data", "comments": "PhD dissertation, May 2014, Purdue University", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry (FC) is a single-cell profiling platform for measuring the\nphenotypes of individual cells from millions of cells in biological samples. FC\nemploys high-throughput technologies and generates high-dimensional data, and\nhence algorithms for analyzing the data represent a bottleneck. This\ndissertation addresses several computational challenges arising in modern\ncytometry while mining information from high-dimensional and high-content\nbiological data. A collection of combinatorial and statistical algorithms for\nlocating, matching, prototyping, and classifying cellular populations from\nmulti-parametric FC data is developed.\n  The algorithmic pipeline, flowMatch, developed in this dissertation consists\nof five well-defined algorithmic modules to (1) transform data to stabilize\nwithin-population variance, (2) identify cell populations by robust clustering\nalgorithms, (3) register cell populations across samples, (4) encapsulate a\nclass of samples with templates, and (5) classify samples based on their\nsimilarity with the templates. Components of flowMatch can work independently\nor collaborate with each other to perform the complete data analysis. flowMatch\nis made available as an open-source R package in Bioconductor.\n  We have employed flowMatch for classifying leukemia samples, evaluating the\nphosphorylation effects on T cells, classifying healthy immune profiles, and\nclassifying the vaccination status of HIV patients. In these analyses, the\npipeline is able to reach biologically meaningful conclusions quickly and\nefficiently with the automated algorithms. The algorithms included in flowMatch\ncan also be applied to problems outside of flow cytometry such as in microarray\ndata analysis and image recognition. Therefore, this dissertation contributes\nto the solution of fundamental problems in computational cytometry and related\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 20:02:13 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Azad", "Ariful", ""]]}, {"id": "1501.03545", "submitter": "Moritz von Looz-Corswarem", "authors": "Moritz von Looz and Christian L. Staudt and Henning Meyerhenke and\n  Roman Prutkin", "title": "Fast generation of complex networks with underlying hyperbolic geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks have become increasingly popular for modeling various\nreal-world phenomena. Realistic generative network models are important in this\ncontext as they avoid privacy concerns of real data and simplify complex\nnetwork research regarding data sharing, reproducibility, and scalability\nstudies. \\emph{Random hyperbolic graphs} are a well-analyzed family of\ngeometric graphs. Previous work provided empirical and theoretical evidence\nthat this generative graph model creates networks with non-vanishing clustering\nand other realistic features. However, the investigated networks in previous\napplied work were small, possibly due to the quadratic running time of a\nprevious generator.\n  In this work we provide the first generation algorithm for these networks\nwith subquadratic running time. We prove a time complexity of $O((n^{3/2}+m)\n\\log n)$ with high probability for the generation process. This running time is\nconfirmed by experimental data with our implementation. The acceleration stems\nprimarily from the reduction of pairwise distance computations through a polar\nquadtree, which we adapt to hyperbolic space for this purpose. In practice we\nimprove the running time of a previous implementation by at least two orders of\nmagnitude this way. Networks with billions of edges can now be generated in a\nfew minutes.\n  Finally, we evaluate the largest networks of this model published so far. Our\nempirical analysis shows that important features are retained over different\ngraph densities and degree distributions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 00:52:44 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 18:28:26 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 10:01:00 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["von Looz", "Moritz", ""], ["Staudt", "Christian L.", ""], ["Meyerhenke", "Henning", ""], ["Prutkin", "Roman", ""]]}, {"id": "1501.04001", "submitter": "Simone Faro", "authors": "Simone Faro and O\\u{g}uzhan K\\\"ulekci", "title": "Efficient Algorithms for the Order Preserving Pattern Matching Problem", "comments": "16 pages, 3 figures, submitted to SEA 2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern x of length m and a text y of length n, both over an ordered\nalphabet, the order-preserving pattern matching problem consists in finding all\nsubstrings of the text with the same relative order as the pattern. It is an\napproximate variant of the well known exact pattern matching problem which has\ngained attention in recent years. This interesting problem finds applications\nin a lot of fields as time series analysis, like share prices on stock markets,\nweather data analysis or to musical melody matching. In this paper we present\ntwo new filtering approaches which turn out to be much more effective in\npractice than the previously presented methods. From our experimental results\nit turns out that our proposed solutions are up to 2 times faster than the\nprevious solutions reducing the number of false positives up to 99%\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 14:55:47 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Faro", "Simone", ""], ["K\u00fclekci", "O\u011fuzhan", ""]]}, {"id": "1501.04262", "submitter": "Colin White", "authors": "Colin White", "title": "Lower Bounds in the Preprocessing and Query Phases of Routing Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, there has been a substantial amount of research in\nfinding routing algorithms designed specifically to run on real-world graphs.\nIn 2010, Abraham et al. showed upper bounds on the query time in terms of a\ngraph's highway dimension and diameter for the current fastest routing\nalgorithms, including contraction hierarchies, transit node routing, and hub\nlabeling. In this paper, we show corresponding lower bounds for the same three\nalgorithms. We also show how to improve a result by Milosavljevic which lower\nbounds the number of shortcuts added in the preprocessing stage for contraction\nhierarchies. We relax the assumption of an optimal contraction order (which is\nNP-hard to compute), allowing the result to be applicable to real-world\ninstances. Finally, we give a proof that optimal preprocessing for hub labeling\nis NP-hard. Hardness of optimal preprocessing is known for most routing\nalgorithms, and was suspected to be true for hub labeling.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 03:45:37 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 14:32:20 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["White", "Colin", ""]]}, {"id": "1501.04343", "submitter": "Xiaohu Wu", "authors": "Xiaohu Wu, and Patrick Loiseau", "title": "Algorithms for Scheduling Malleable Cloud Tasks", "comments": "The conference version of this manuscript appeared at the 53rd Annual\n  Allerton Conference on Communication, Control, and Computing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ubiquity of batch data processing in cloud computing, the related\nproblem of scheduling malleable batch tasks and its extensions have received\nsignificant attention recently. In this paper, we consider a fundamental model\nwhere a set of n tasks is to be processed on C identical machines and each task\nis specified by a value, a workload, a deadline and a parallelism bound. Within\nthe parallelism bound, the number of machines assigned to a task can vary over\ntime without affecting its workload. For this model, we obtain two core\nresults: a sufficient and necessary condition such that a set of tasks can be\nfinished by their deadlines on C machines, and an algorithm to produce such a\nschedule. These core results provide a conceptual tool and an optimal\nscheduling algorithm that enable proposing new algorithmic analysis and design\nand improving existing algorithms under various objectives.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 20:18:15 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 17:41:51 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 03:47:37 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2015 04:50:48 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 12:13:13 GMT"}, {"version": "v6", "created": "Sun, 29 Jan 2017 16:06:51 GMT"}, {"version": "v7", "created": "Tue, 31 Jan 2017 23:54:14 GMT"}, {"version": "v8", "created": "Sun, 4 Feb 2018 19:58:30 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wu", "Xiaohu", ""], ["Loiseau", "Patrick", ""]]}, {"id": "1501.04560", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Timothy M. Hospedales, Tao Xiang and Shaogang Gong", "title": "Transductive Multi-view Zero-Shot Learning", "comments": "accepted by IEEE TPAMI, more info and longer report will be available\n  in :http://www.eecs.qmul.ac.uk/~yf300/embedding/index.html", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2408354", "report-no": null, "categories": "cs.CV cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing zero-shot learning approaches exploit transfer learning via an\nintermediate-level semantic representation shared between an annotated\nauxiliary dataset and a target dataset with different classes and no\nannotation. A projection from a low-level feature space to the semantic\nrepresentation space is learned from the auxiliary dataset and is applied\nwithout adaptation to the target dataset. In this paper we identify two\ninherent limitations with these approaches. First, due to having disjoint and\npotentially unrelated classes, the projection functions learned from the\nauxiliary dataset/domain are biased when applied directly to the target\ndataset/domain. We call this problem the projection domain shift problem and\npropose a novel framework, transductive multi-view embedding, to solve it. The\nsecond limitation is the prototype sparsity problem which refers to the fact\nthat for each target class, only a single prototype is available for zero-shot\nlearning given a semantic representation. To overcome this problem, a novel\nheterogeneous multi-view hypergraph label propagation method is formulated for\nzero-shot learning in the transductive embedding space. It effectively exploits\nthe complementary information offered by different semantic representations and\ntakes advantage of the manifold structures of multiple representation spaces in\na coherent manner. We demonstrate through extensive experiments that the\nproposed approach (1) rectifies the projection shift between the auxiliary and\ntarget domains, (2) exploits the complementarity of multiple semantic\nrepresentations, (3) significantly outperforms existing methods for both\nzero-shot and N-shot recognition on three image and video benchmark datasets,\nand (4) enables novel cross-view annotation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 17:04:11 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:43:44 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Fu", "Yanwei", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1501.04870", "submitter": "Luca Martino", "authors": "J. Read, L. Martino, P. Olmos, D. Luengo", "title": "Scalable Multi-Output Label Prediction: From Classifier Chains to\n  Classifier Trellises", "comments": "(accepted in Pattern Recognition)", "journal-ref": "Pattern Recognition, Volume 48, Issue 6, 2015, Pages 2096-2109", "doi": "10.1016/j.patcog.2015.01.004", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output inference tasks, such as multi-label classification, have become\nincreasingly important in recent years. A popular method for multi-label\nclassification is classifier chains, in which the predictions of individual\nclassifiers are cascaded along a chain, thus taking into account inter-label\ndependencies and improving the overall performance. Several varieties of\nclassifier chain methods have been introduced, and many of them perform very\ncompetitively across a wide range of benchmark datasets. However, scalability\nlimitations become apparent on larger datasets when modeling a fully-cascaded\nchain. In particular, the methods' strategies for discovering and modeling a\ngood chain structure constitutes a mayor computational bottleneck. In this\npaper, we present the classifier trellis (CT) method for scalable multi-label\nclassification. We compare CT with several recently proposed classifier chain\nmethods to show that it occupies an important niche: it is highly competitive\non standard multi-label problems, yet it can also scale up to thousands or even\ntens of thousands of labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:33:40 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Read", "J.", ""], ["Martino", "L.", ""], ["Olmos", "P.", ""], ["Luengo", "D.", ""]]}, {"id": "1501.04931", "submitter": "Dimitris Achlioptas", "authors": "Dimitris Achlioptas, Paris Siminelakis", "title": "Navigability is a Robust Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Small World phenomenon has inspired researchers across a number of\nfields. A breakthrough in its understanding was made by Kleinberg who\nintroduced Rank Based Augmentation (RBA): add to each vertex independently an\narc to a random destination selected from a carefully crafted probability\ndistribution. Kleinberg proved that RBA makes many networks navigable, i.e., it\nallows greedy routing to successfully deliver messages between any two vertices\nin a polylogarithmic number of steps. We prove that navigability is an inherent\nproperty of many random networks, arising without coordination, or even\nindependence assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 19:47:03 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Achlioptas", "Dimitris", ""], ["Siminelakis", "Paris", ""]]}, {"id": "1501.04948", "submitter": "Aleksander Cis{\\l}ak", "authors": "Aleksander Cis{\\l}ak, Szymon Grabowski", "title": "A practical index for approximate dictionary matching with few\n  mismatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate dictionary matching is a classic string matching problem\n(checking if a query string occurs in a collection of strings) with\napplications in, e.g., spellchecking, online catalogs, geolocation, and web\nsearchers. We present a surprisingly simple solution called a split index,\nwhich is based on the Dirichlet principle, for matching a keyword with few\nmismatches, and experimentally show that it offers competitive space-time\ntradeoffs. Our implementation in the C++ language is focused mostly on data\ncompaction, which is beneficial for the search speed (e.g., by being cache\nfriendly). We compare our solution with other algorithms and we show that it\nperforms better for the Hamming distance. Query times in the order of 1\nmicrosecond were reported for one mismatch for the dictionary size of a few\nmegabytes on a medium-end PC. We also demonstrate that a basic compression\ntechnique consisting in $q$-gram substitution can significantly reduce the\nindex size (up to 50% of the input text size for the DNA), while still keeping\nthe query time relatively low.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 20:32:45 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 20:27:15 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2015 18:27:49 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2016 00:33:52 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Cis\u0142ak", "Aleksander", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1501.04985", "submitter": "Konstantinos Georgiou", "authors": "Jurek Czyzowicz, Konstantinos Georgiou, Evangelos Kranakis, Lata\n  Narayanan, Jarda Opatrny, Birgit Vogtenhuber", "title": "Evacuating Robots from a Disk Using Face-to-Face Communication", "comments": "22 pages, 8 figures. An extended abstract of this work was accepted\n  for publication in the LNCS proceedings of the 9th International Conference\n  on Algorithms and Complexity (CIAC15)", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 22 no.\n  4, Distributed Computing and Networking (August 27, 2020) dmtcs:6732", "doi": "10.23638/DMTCS-22-4-4", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that two robots are located at the centre of a unit disk. Their goal\nis to evacuate from the disk through an exit at an unknown location on the\nboundary of the disk. At any time the robots can move anywhere they choose on\nthe disk, independently of each other, with maximum speed $1$. The robots can\ncooperate by exchanging information whenever they meet. We study algorithms for\nthe two robots to minimize the evacuation time: the time when both robots reach\nthe exit.\n  In [CGGKMP14] the authors gave an algorithm defining trajectories for the two\nrobots yielding evacuation time at most $5.740$ and also proved that any\nalgorithm has evacuation time at least $3+ \\frac{\\pi}{4} + \\sqrt{2} \\approx\n5.199$. We improve both the upper and lower bound on the evacuation time of a\nunit disk. Namely, we present a new non-trivial algorithm whose evacuation time\nis at most $5.628$ and show that any algorithm has evacuation time at least $3+\n\\frac{\\pi}{6} + \\sqrt{3} \\approx 5.255$. To achieve the upper bound, we\ndesigned an algorithm which proposes a forced meeting between the two robots,\neven if the exit has not been found by either of them. We also show that such a\nstrategy is provably optimal for a related problem of searching for an exit\nplaced at the vertices of a regular hexagon.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 21:36:08 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 02:07:34 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 02:12:31 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 04:08:18 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2020 04:13:48 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Georgiou", "Konstantinos", ""], ["Kranakis", "Evangelos", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jarda", ""], ["Vogtenhuber", "Birgit", ""]]}, {"id": "1501.05021", "submitter": "Anup Rao", "authors": "Peter Chin, Anup Rao, Van Vu", "title": "Stochastic Block Model and Community Detection in the Sparse Graphs: A\n  spectral algorithm with optimal rate of recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present and analyze a simple and robust spectral algorithm\nfor the stochastic block model with $k$ blocks, for any $k$ fixed. Our\nalgorithm works with graphs having constant edge density, under an optimal\ncondition on the gap between the density inside a block and the density between\nthe blocks. As a co-product, we settle an open question posed by Abbe et. al.\nconcerning censor block models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 23:35:59 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 12:40:30 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Chin", "Peter", ""], ["Rao", "Anup", ""], ["Vu", "Van", ""]]}, {"id": "1501.05197", "submitter": "Leah Epstein", "authors": "Ron Adar and Leah Epstein", "title": "The weighted 2-metric dimension of trees in the non-landmarks model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let T=(V,E) be a tree graph with non-negative weights defined on the\nvertices. A vertex z is called a separating vertex for u and v if the distances\nof z to u and v are not equal. A set of vertices L\\subseteq V is a feasible\nsolution for the non-landmarks model (NL), if for every pair of distinct\nvertices, u,v \\in V\\setminus L, there are at least two vertices of L separating\nthem. Such a feasible solution is called a \"landmark set\". We analyze the\nstructure of landmark sets for trees and design a linear time algorithm for\nfinding a minimum cost landmark set for a given tree graph.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 15:40:44 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Adar", "Ron", ""], ["Epstein", "Leah", ""]]}, {"id": "1501.05222", "submitter": "Ryan Curtin", "authors": "Ryan R. Curtin, Dongryeol Lee, William B. March, Parikshit Ram", "title": "Plug-and-play dual-tree algorithm runtime analysis", "comments": "Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous machine learning algorithms contain pairwise statistical problems at\ntheir core---that is, tasks that require computations over all pairs of input\npoints if implemented naively. Often, tree structures are used to solve these\nproblems efficiently. Dual-tree algorithms can efficiently solve or approximate\nmany of these problems. Using cover trees, rigorous worst-case runtime\nguarantees have been proven for some of these algorithms. In this paper, we\npresent a problem-independent runtime guarantee for any dual-tree algorithm\nusing the cover tree, separating out the problem-dependent and the\nproblem-independent elements. This allows us to just plug in bounds for the\nproblem-dependent elements to get runtime guarantees for dual-tree algorithms\nfor any pairwise statistical problem without re-deriving the entire proof. We\ndemonstrate this plug-and-play procedure for nearest-neighbor search and\napproximate kernel density estimation to get improved runtime guarantees. Under\nmild assumptions, we also present the first linear runtime guarantee for\ndual-tree based range search.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 16:39:43 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Lee", "Dongryeol", ""], ["March", "William B.", ""], ["Ram", "Parikshit", ""]]}, {"id": "1501.05296", "submitter": "Daniel Roche", "authors": "Andrew Arnold and Daniel S. Roche", "title": "Output-sensitive algorithms for sumset and sparse polynomial\n  multiplication", "comments": "Submitted to ISSAC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present randomized algorithms to compute the sumset (Minkowski sum) of two\ninteger sets, and to multiply two univariate integer polynomials given by\nsparse representations. Our algorithm for sumset has cost softly linear in the\ncombined size of the inputs and output. This is used as part of our sparse\nmultiplication algorithm, whose cost is softly linear in the combined size of\nthe inputs, output, and the sumset of the supports of the inputs. As a\nsubroutine, we present a new method for computing the coefficients of a sparse\npolynomial, given a set containing its support. Our multiplication algorithm\nextends to multivariate Laurent polynomials over finite fields and rational\nnumbers. Our techniques are based on sparse interpolation algorithms and\nresults from analytic number theory.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 04:43:58 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 20:40:10 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 11:10:00 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Arnold", "Andrew", ""], ["Roche", "Daniel S.", ""]]}, {"id": "1501.05354", "submitter": "Raphael Kramer", "authors": "Raphael Kramer, Nelson Maculan, Anand Subramanian, Thibaut Vidal", "title": "A speed and departure time optimization algorithm for the\n  Pollution-Routing Problem", "comments": "12 pages", "journal-ref": "European Journal of Operational Research, 2015, 247(3):782-787", "doi": "10.1016/j.ejor.2015.06.037", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new speed and departure time optimization algorithm for the\nPollution-Routing Problem (PRP), which runs in quadratic time and returns a\ncertified optimal schedule. This algorithm is embedded into an iterated local\nsearch-based metaheuristic to achieve a combined speed, scheduling and routing\noptimization. The start of the working day is set as a decision variable for\nindividual routes, thus enabling a better assignment of human resources to\nrequired demands. Some routes that were evaluated as unprofitable can now\nappear as viable candidates later in the day, leading to a larger search space\nand further opportunities of distance optimization via better service\nconsolidation. Extensive computational experiments on available PRP benchmark\ninstances demonstrate the good performance of the algorithms. The flexible\ndeparture times from the depot contribute to reduce the operational costs by\n8.36% on the considered instances.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 23:59:38 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 15:59:20 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kramer", "Raphael", ""], ["Maculan", "Nelson", ""], ["Subramanian", "Anand", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1501.05414", "submitter": "Weidong Li", "authors": "Weidong Li, Xi Liu, Xuejie Zhang, and Xiaobo Cai", "title": "A Task-Type-Based Algorithm for the Energy-Aware Profit Maximizing\n  Scheduling Problem in Heterogeneous Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design an efficient algorithm for the energy-aware profit\nmaximizing scheduling problem, where the high performance computing system\nadministrator is to maximize the profit per unit time. The running time of the\nproposed algorithm is depending on the number of task types, while the running\ntime of the previous algorithm is depending on the number of tasks. Moreover,\nwe prove that the worst-case performance ratio is close to 2, which maybe the\nbest result. Simulation experiments show that the proposed algorithm is more\naccurate than the previous method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 07:53:11 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Li", "Weidong", ""], ["Liu", "Xi", ""], ["Zhang", "Xuejie", ""], ["Cai", "Xiaobo", ""]]}, {"id": "1501.05493", "submitter": "Bodo Manthey", "authors": "Tobias Brunsch and Kamiel Cornelissen and Bodo Manthey and Heiko\n  R\\\"oglin and Clemens R\\\"osner", "title": "Smoothed Analysis of the Successive Shortest Path Algorithm", "comments": "A preliminary version has been presented at SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum-cost flow problem is a classic problem in combinatorial\noptimization with various applications. Several pseudo-polynomial, polynomial,\nand strongly polynomial algorithms have been developed in the past decades, and\nit seems that both the problem and the algorithms are well understood. However,\nsome of the algorithms' running times observed in empirical studies contrast\nthe running times obtained by worst-case analysis not only in the order of\nmagnitude but also in the ranking when compared to each other. For example, the\nSuccessive Shortest Path (SSP) algorithm, which has an exponential worst-case\nrunning time, seems to outperform the strongly polynomial Minimum-Mean Cycle\nCanceling algorithm.\n  To explain this discrepancy, we study the SSP algorithm in the framework of\nsmoothed analysis and establish a bound of $O(mn\\phi)$ for the number of\niterations, which implies a smoothed running time of $O(mn\\phi (m + n\\log n))$,\nwhere $n$ and $m$ denote the number of nodes and edges, respectively, and\n$\\phi$ is a measure for the amount of random noise. This shows that worst-case\ninstances for the SSP algorithm are not robust and unlikely to be encountered\nin practice. Furthermore, we prove a smoothed lower bound of $\\Omega(m \\phi\n\\min\\{n, \\phi\\})$ for the number of iterations of the SSP algorithm, showing\nthat the upper bound cannot be improved for $\\phi = \\Omega(n)$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 13:43:01 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 07:10:18 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Brunsch", "Tobias", ""], ["Cornelissen", "Kamiel", ""], ["Manthey", "Bodo", ""], ["R\u00f6glin", "Heiko", ""], ["R\u00f6sner", "Clemens", ""]]}, {"id": "1501.05542", "submitter": "Meo Mespotine", "authors": "Meo Mespotine", "title": "Mespotine-RLE-basic v0.9 - An overhead-reduced and improved\n  Run-Length-Encoding Method", "comments": "16 pages and algorithm-flowcharts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Run Length Encoding(RLE) is one of the oldest algorithms for data-compression\navailable, a method used for compression of large data into smaller and\ntherefore more compact data. It compresses by looking at the data for\nrepetitions of the same character in a row and storing the amount(called run)\nand the respective character(called run_value) as target-data. Unfortunately it\nonly compresses within strict and special cases. Outside of these cases, it\nincreases the data-size, even doubles the size in worst cases compared to the\noriginal, unprocessed data. In this paper, we will discuss modifications to\nRLE, with which we will only store the run for characters, that are actually\ncompressible, getting rid of a lot of useless data like the runs of the\ncharacters, that are uncompressible in the first place. This will be achieved\nby storing the character first and the run second. Additionally we create a\nbit-list of 256 positions(one for every possible ASCII-character), in which we\nwill store, if a specific (ASCII-)character is compressible(1) or not(0). Using\nthis list, we can now say, if a character is compressible (store [the\ncharacter]+[it's run]) or if it is not compressible (store [the character] only\nand the next character is NOT a run, but the following character instead).\nUsing this list, we can also successfully decode the data(if the character is\ncompressible, the next character is a run, if not compressible, the next\ncharacter is a normal character). With that, we store runs only for characters,\nthat are compressible in the first place. In fact, in the worst case scenario,\nthe encoded data will create always just an overhead of the size of the\nbit-list itself. With an alphabet of 256 different characters(i.e. ASCII) it\nwould be only a maximum of 32 bytes, no matter how big the original data was.\n[...]\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 15:51:32 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Mespotine", "Meo", ""]]}, {"id": "1501.05547", "submitter": "David Manlove", "authors": "Katarina Cechlarova, Tamas Fleiner, David F. Manlove, Iain McBride", "title": "Stable matchings of teachers to schools", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several countries successfully use centralized matching schemes for school or\nhigher education assignment, or for entry-level labour markets. In this paper\nwe explore the computational aspects of a possible similar scheme for assigning\nteachers to schools. Our model is motivated by a particular characteristic of\nthe education system in many countries where each teacher specializes in two\nsubjects. We seek stable matchings, which ensure that no teacher and school\nhave the incentive to deviate from their assignments. Indeed we propose two\nstability definitions depending on the precise format of schools' preferences.\nIf the schools' ranking of applicants is independent of their subjects of\nspecialism, we show that the problem of deciding whether a stable matching\nexists is NP-complete, even if there are only three subjects, unless there are\nmaster lists of applicants or of schools. By contrast, if the schools may order\napplicants differently in each of their specialization subjects, the problem of\ndeciding whether a stable matching exists is NP-complete even in the presence\nof subject-specific master lists plus a master list of schools. Finally, we\nprove a strong inapproximability result for the problem of finding a matching\nwith the minimum number of blocking pairs with respect to both stability\ndefinitions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 16:02:16 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 10:40:15 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Cechlarova", "Katarina", ""], ["Fleiner", "Tamas", ""], ["Manlove", "David F.", ""], ["McBride", "Iain", ""]]}, {"id": "1501.05579", "submitter": "Armin Wei{\\ss}", "authors": "Volker Diekert, Alexei G. Myasnikov, Armin Wei{\\ss}", "title": "Amenability of Schreier graphs and strongly generic algorithms for the\n  conjugacy problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various occasions the conjugacy problem in finitely generated amalgamated\nproducts and HNN extensions can be decided efficiently for elements which\ncannot be conjugated into the base groups. This observation asks for a bound on\nhow many such elements there are. Such bounds can be derived using the theory\nof amenable graphs:\n  In this work we examine Schreier graphs of amalgamated products and HNN\nextensions. For an amalgamated product $G = H *_A K $ with $[H:A] \\geq [K:A]\n\\geq 2$, the Schreier graph with respect to $H$ or $K$ turns out to be\nnon-amenable if and only if $[H:A] \\geq 3$. Moreover, for an HNN extension of\nthe form $G = <H,b | bab^{-1}=\\phi(a), a \\in A >$, we show that the Schreier\ngraph of $G$ with respect to the subgroup $H$ is non-amenable if and only if $A\n\\neq H \\neq \\phi(A)$.\n  As application of these characterizations we show that under certain\nconditions the conjugacy problem in fundamental groups of finite graphs of\ngroups with free abelian vertex groups can be solved in polynomial time on a\nstrongly generic set. Furthermore, the conjugacy problem in groups with more\nthan one end can be solved with a strongly generic algorithm which has\nessentially the same time complexity as the word problem. These are rather\nstriking results as the word problem might be easy, but the conjugacy problem\nmight be even undecidable. Finally, our results yield another proof that the\nset where the conjugacy problem of the Baumslag group is decidable in\npolynomial time is also strongly generic.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 17:29:18 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 20:28:55 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Diekert", "Volker", ""], ["Myasnikov", "Alexei G.", ""], ["Wei\u00df", "Armin", ""]]}, {"id": "1501.05800", "submitter": "Matthew  Johnson", "authors": "Carl Feghali, Matthew Johnson, Dani\\\"el Paulusma", "title": "A Reconfigurations Analogue of Brooks' Theorem and its Consequences", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a simple undirected graph on $n$ vertices with maximum\ndegree~$\\Delta$. Brooks' Theorem states that $G$ has a $\\Delta$-colouring\nunless~$G$ is a complete graph, or a cycle with an odd number of vertices. To\nrecolour $G$ is to obtain a new proper colouring by changing the colour of one\nvertex. We show an analogue of Brooks' Theorem by proving that from any\n$k$-colouring, $k>\\Delta$, a $\\Delta$-colouring of $G$ can be obtained by a\nsequence of $O(n^2)$ recolourings using only the original $k$ colours unless\n$G$ is a complete graph or a cycle with an odd number of vertices, or\n$k=\\Delta+1$, $G$ is $\\Delta$-regular and, for each vertex $v$ in $G$, no two\nneighbours of $v$ are coloured alike.\n  We use this result to study the reconfiguration graph $R_k(G)$ of the\n$k$-colourings of $G$. The vertex set of $R_k(G)$ is the set of all possible\n$k$-colourings of $G$ and two colourings are adjacent if they differ on exactly\none vertex. We prove that for $\\Delta\\geq 3$, $R_{\\Delta+1}(G)$ consists of\nisolated vertices and at most one further component which has diameter\n$O(n^2)$. This result enables us to complete both a structural classification\nand an algorithmic classification for reconfigurations of colourings of graphs\nof bounded maximum degree.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 13:50:07 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Feghali", "Carl", ""], ["Johnson", "Matthew", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1501.05801", "submitter": "Moran Feldman", "authors": "Niv Buchbinder, Moran Feldman and Roy Schwartz", "title": "Online Submodular Maximization with Preemption", "comments": "32 pages, an extended abstract of this work appeared in SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function maximization has been studied extensively in recent years\nunder various constraints and models. The problem plays a major role in various\ndisciplines. We study a natural online variant of this problem in which\nelements arrive one-by-one and the algorithm has to maintain a solution obeying\ncertain constraints at all times. Upon arrival of an element, the algorithm has\nto decide whether to accept the element into its solution and may preempt\npreviously chosen elements. The goal is to maximize a submodular function over\nthe set of elements in the solution.\n  We study two special cases of this general problem and derive upper and lower\nbounds on the competitive ratio. Specifically, we design a $1/e$-competitive\nalgorithm for the unconstrained case in which the algorithm may hold any subset\nof the elements, and constant competitive ratio algorithms for the case where\nthe algorithm may hold at most $k$ elements in its solution.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 13:50:56 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""], ["Schwartz", "Roy", ""]]}, {"id": "1501.05828", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty and Raghunath Tewari", "title": "An $O(n^{\\epsilon})$ Space and Polynomial Time Algorithm for\n  Reachability in Directed Layered Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and two vertices $s$ and $t$ in it, {\\em graph\nreachability} is the problem of checking whether there exists a path from $s$\nto $t$ in $G$. We show that reachability in directed layered planar graphs can\nbe decided in polynomial time and $O(n^\\epsilon)$ space, for any $\\epsilon >\n0$. The previous best known space bound for this problem with polynomial time\nwas approximately $O(\\sqrt{n})$ space \\cite{INPVW13}.\n  Deciding graph reachability in {\\SC} is an important open question in\ncomplexity theory and in this paper we make progress towards resolving this\nquestion.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 15:23:25 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Tewari", "Raghunath", ""]]}, {"id": "1501.05971", "submitter": "Laura Rebollo-Neira", "authors": "Laura Rebollo-Neira", "title": "Cooperative Greedy Pursuit Strategies for Sparse Signal Representation\n  by Partitioning", "comments": "A library of routines for implementing the proposed methods, as well\n  as scripts to reproduce the examples in the manuscript, is available on the\n  website http://www.nonlinear-approx.info/examples/node01.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative Greedy Pursuit Strategies are considered for approximating a\nsignal partition subjected to a global constraint on sparsity. The approach\naims at producing a high quality sparse approximation of the whole signal,\nusing highly coherent redundant dictionaries. The cooperation takes place by\nranking the partition units for their sequential stepwise approximation, and is\nrealized by means of i)forward steps for the upgrading of an approximation\nand/or ii) backward steps for the corresponding downgrading. The advantage of\nthe strategy is illustrated by producing high quality approximations of music\nsignals using redundant trigonometric dictionaries. In addition to rendering\nstunning improvements in sparsity with respect to the concomitant trigonometric\nbasis, these dictionaries enable a fast implementation of the approach via the\nFast Fourier Transform.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 21:47:23 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 19:03:49 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2015 20:25:49 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Rebollo-Neira", "Laura", ""]]}, {"id": "1501.06095", "submitter": "Jonathan Ullman", "authors": "Thomas Steinke and Jonathan Ullman", "title": "Between Pure and Approximate Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a new lower bound on the sample complexity of $(\\varepsilon,\n\\delta)$-differentially private algorithms that accurately answer statistical\nqueries on high-dimensional databases. The novelty of our bound is that it\ndepends optimally on the parameter $\\delta$, which loosely corresponds to the\nprobability that the algorithm fails to be private, and is the first to\nsmoothly interpolate between approximate differential privacy ($\\delta > 0$)\nand pure differential privacy ($\\delta = 0$).\n  Specifically, we consider a database $D \\in \\{\\pm1\\}^{n \\times d}$ and its\n\\emph{one-way marginals}, which are the $d$ queries of the form \"What fraction\nof individual records have the $i$-th bit set to $+1$?\" We show that in order\nto answer all of these queries to within error $\\pm \\alpha$ (on average) while\nsatisfying $(\\varepsilon, \\delta)$-differential privacy, it is necessary that\n$$ n \\geq \\Omega\\left( \\frac{\\sqrt{d \\log(1/\\delta)}}{\\alpha \\varepsilon}\n\\right), $$ which is optimal up to constant factors. To prove our lower bound,\nwe build on the connection between \\emph{fingerprinting codes} and lower bounds\nin differential privacy (Bun, Ullman, and Vadhan, STOC'14).\n  In addition to our lower bound, we give new purely and approximately\ndifferentially private algorithms for answering arbitrary statistical queries\nthat improve on the sample complexity of the standard Laplace and Gaussian\nmechanisms for achieving worst-case accuracy guarantees by a logarithmic\nfactor.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 23:26:21 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1501.06140", "submitter": "Moti Medina", "authors": "Guy Even and Moti Medina and Boaz Patt-Shamir", "title": "Better Online Deterministic Packet Routing on Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following fundamental routing problem. An adversary inputs\npackets arbitrarily at sources, each packet with an arbitrary destination.\nTraffic is constrained by link capacities and buffer sizes, and packets may be\ndropped at any time. The goal of the routing algorithm is to maximize\nthroughput, i.e., route as many packets as possible to their destination. Our\nmain result is an $O\\left(\\log n\\right)$-competitive deterministic algorithm\nfor an $n$-node line network (i.e., $1$-dimensional grid), requiring only that\nbuffers can store at least $5$ packets, and that links can deliver at least $5$\npackets per step. We note that $O(\\log n)$ is the best ratio known, even for\nrandomized algorithms, even when allowed large buffers and wide links. The best\nprevious deterministic algorithm for this problem with constant-size buffers\nand constant-capacity links was $O(\\log^5 n)$-competitive. Our algorithm works\nlike admission-control algorithms in the sense that if a packet is not dropped\nimmediately upon arrival, then it is \"accepted\" and guaranteed to be delivered.\nWe also show how to extend our algorithm to a polylog-competitive algorithm for\nany constant-dimension grid.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 11:22:58 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Even", "Guy", ""], ["Medina", "Moti", ""], ["Patt-Shamir", "Boaz", ""]]}, {"id": "1501.06148", "submitter": "Michel  Habib", "authors": "Derek G. Corneil and Jeremie Dusart and Michel Habib and Fabien de\n  Montgolfier", "title": "A tie-break model for graph search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of the recognition of various kinds of\norderings produced by graph searches. To this aim, we introduce a new\nframework, the Tie-Breaking Label Search (TBLS), in order to handle a broad\nvariety of searches. This new model is based on partial orders defined on the\nlabel set and it unifies the General Label Search (GLS) formalism of Krueger,\nSimonet and Berry (2011), and the \"pattern-conditions\" formalism of Corneil and\nKrueger (2008). It allows us to derive some general properties including new\npattern-conditions (yielding memory-efficient certificates) for many usual\nsearches, including BFS, DFS, LBFS and LDFS. Furthermore, the new model allows\neasy expression of multi-sweep uses of searches that depend on previous\n(search) orderings of the graph's vertex set.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 12:45:29 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Corneil", "Derek G.", ""], ["Dusart", "Jeremie", ""], ["Habib", "Michel", ""], ["de Montgolfier", "Fabien", ""]]}, {"id": "1501.06158", "submitter": "Adi Vardi", "authors": "Yossi Azar and Adi Vardi", "title": "TSP with Time Windows and Service Time", "comments": "arXiv admin note: substantial text overlap with arXiv:1309.0251", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider TSP with time windows and service time. In this problem we\nreceive a sequence of requests for a service at nodes in a metric space and a\ntime window for each request. The goal of the online algorithm is to maximize\nthe number of requests served during their time window. The time to traverse an\nedge is the distance between the incident nodes of that edge. Serving a request\nrequires unit time. We characterize the competitive ratio for each metric space\nseparately. The competitive ratio depends on the relation between the minimum\nlaxity (the minimum length of a time window) and the diameter of the metric\nspace. Specifically, there is a constant competitive algorithm depending\nwhether the laxity is larger or smaller than the diameter. In addition, we\ncharacterize the rate of convergence of the competitive ratio to $1$ as the\nlaxity increases. Specifically, we provide a matching lower and upper bounds\ndepending on the ratio between the laxity and the TSP of the metric space (the\nminimum distance to traverse all nodes). An application of our result improves\nthe lower bound for colored packets with transition cost and matches the upper\nbound. In proving our lower bounds we use an interesting non-standard embedding\nwith some special properties. This embedding may be interesting by its own.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 13:52:36 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Azar", "Yossi", ""], ["Vardi", "Adi", ""]]}, {"id": "1501.06195", "submitter": "Yun Yang", "authors": "Yun Yang, Mert Pilanci, Martin J. Wainwright", "title": "Randomized sketches for kernels: Fast and optimal non-parametric\n  regression", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression (KRR) is a standard method for performing\nnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$\nsamples, the time and space complexity of computing the KRR estimate scale as\n$\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ respectively, and so is prohibitive\nin many cases. We propose approximations of KRR based on $m$-dimensional\nrandomized sketches of the kernel matrix, and study how small the projection\ndimension $m$ can be chosen while still preserving minimax optimality of the\napproximate KRR estimate. For various classes of randomized sketches, including\nthose based on Gaussian and randomized Hadamard matrices, we prove that it\nsuffices to choose the sketch dimension $m$ proportional to the statistical\ndimension (modulo logarithmic factors). Thus, we obtain fast and minimax\noptimal approximations to the KRR estimate for non-parametric regression.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 19:06:59 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Yang", "Yun", ""], ["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1501.06259", "submitter": "Bojian Xu", "authors": "Atalay Mert \\.Ileri and M. O\\u{g}uzhan K\\\"ulekci and Bojian Xu", "title": "On Longest Repeat Queries", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeat finding in strings has important applications in subfields such as\ncomputational biology. Surprisingly, all prior work on repeat finding did not\nconsider the constraint on the locality of repeats. In this paper, we propose\nand study the problem of finding longest repetitive substrings covering\nparticular string positions. We propose an $O(n)$ time and space algorithm for\nfinding the longest repeat covering every position of a string of size $n$. Our\nwork is optimal since the reading and the storage of an input string of size\n$n$ takes $O(n)$ time and space. Because any substring of a repeat is also a\nrepeat, our solution to longest repeat queries effectively provides a\n\"stabbing\" tool for practitioners for finding most of the repeats that cover\nparticular string positions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 05:53:03 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["\u0130leri", "Atalay Mert", ""], ["K\u00fclekci", "M. O\u011fuzhan", ""], ["Xu", "Bojian", ""]]}, {"id": "1501.06350", "submitter": "The Dang Huynh", "authors": "Dohy Hong, The Dang Huynh, Fabien Mathieu", "title": "D-Iteration: diffusion approach for solving PageRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method that can accelerate the computation of\nthe PageRank importance vector. Our method, called D-Iteration (DI), is based\non the decomposition of the matrix-vector product that can be seen as a fluid\ndiffusion model and is potentially adapted to asynchronous implementation. We\ngive theoretical results about the convergence of our algorithm and we show\nthrough experimentations on a real Web graph that DI can improve the\ncomputation efficiency compared to other classical algorithm like Power\nIteration, Gauss-Seidel or OPIC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 11:51:27 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 09:16:56 GMT"}, {"version": "v3", "created": "Wed, 6 May 2015 13:43:55 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Hong", "Dohy", ""], ["Huynh", "The Dang", ""], ["Mathieu", "Fabien", ""]]}, {"id": "1501.06461", "submitter": "Paul Vitanyi", "authors": "Paul M.B. Vitanyi (National Research Center for Mathematics and\n  Computer Science in the Netherlands (CWI) and Univrsity of Amsterdam)", "title": "On The Average-Case Complexity of Shellsort", "comments": "13 pages LaTeX", "journal-ref": "Random Structures and Algorithms, 52:2(2018), 354-363", "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a lower bound expressed in the increment sequence on the\naverage-case complexity of the number of inversions of Shellsort. This lower\nbound is sharp in every case where it could be checked. A special case of this\nlower bound yields the general Jiang-Li-Vit\\'anyi lower bound. We obtain new\nresults e.g. determining the average-case complexity precisely in the\nYao-Janson-Knuth 3-pass case.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 16:18:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 16:45:25 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 16:38:57 GMT"}, {"version": "v4", "created": "Wed, 8 Feb 2017 17:35:52 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Vitanyi", "Paul M. B.", "", "National Research Center for Mathematics and\n  Computer Science in the Netherlands"]]}, {"id": "1501.06515", "submitter": "Shalabh Vidyarthi", "authors": "Shalabh Vidyarthi, Kaushal K Shukla", "title": "Approximation Algorithms for P2P Orienteering and Stochastic Vehicle\n  Routing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the P2P orienteering problem on general metrics and present a\n(2+{\\epsilon}) approximation algorithm. In the stochastic P2P orienteering\nproblem we are given a metric and each node has a fixed reward and random size.\nThe goal is to devise a strategy for visiting the nodes so as to maximize the\nexpected value of the reward without violating the budget constraints. We\npresent an approximation algorithm for the non-adaptive variant of the P2P\nStochastic orienteering. As an implication of the approximation to the\nstochastic P2P orienteering problem, we define a stochastic vehicle routing\nproblem with time-windows and present a constant factor approximation solution.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 18:37:40 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Vidyarthi", "Shalabh", ""], ["Shukla", "Kaushal K", ""]]}, {"id": "1501.06521", "submitter": "Ankur Moitra", "authors": "Boaz Barak and Ankur Moitra", "title": "Noisy Tensor Completion via the Sum-of-Squares Hierarchy", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the noisy tensor completion problem we observe $m$ entries (whose location\nis chosen uniformly at random) from an unknown $n_1 \\times n_2 \\times n_3$\ntensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goal\nis to fill in its missing entries using as few observations as possible. Let $n\n= \\max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is a\npolynomial time algorithm based on the sixth level of the sum-of-squares\nhierarchy for completing it. Our estimate agrees with almost all of $T$'s\nentries almost exactly and works even when our observations are corrupted by\nnoise. This is also the first algorithm for tensor completion that works in the\novercomplete case when $r > n$, and in fact it works all the way up to $r =\nn^{3/2-\\epsilon}$.\n  Our proofs are short and simple and are based on establishing a new\nconnection between noisy tensor completion (through the language of Rademacher\ncomplexity) and the task of refuting random constant satisfaction problems.\nThis connection seems to have gone unnoticed even in the context of matrix\ncompletion. Furthermore, we use this connection to show matching lower bounds.\nOur main technical result is in characterizing the Rademacher complexity of the\nsequence of norms that arise in the sum-of-squares relaxations to the tensor\nnuclear norm. These results point to an interesting new direction: Can we\nexplore computational vs. sample complexity tradeoffs through the\nsum-of-squares hierarchy?\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 18:48:55 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 19:54:52 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 16:37:39 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Barak", "Boaz", ""], ["Moitra", "Ankur", ""]]}, {"id": "1501.06561", "submitter": "Mina Ghashami", "authors": "Amey Desai, Mina Ghashami and Jeff M. Phillips", "title": "Improved Practical Matrix Sketching with Guarantees", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices have become essential data representations for many large-scale\nproblems in data analytics, and hence matrix sketching is a critical task.\nAlthough much research has focused on improving the error/size tradeoff under\nvarious sketching paradigms, the many forms of error bounds make these\napproaches hard to compare in theory and in practice. This paper attempts to\ncategorize and compare most known methods under row-wise streaming updates with\nprovable guarantees, and then to tweak some of these methods to gain practical\nimprovements while retaining guarantees.\n  For instance, we observe that a simple heuristic iSVD, with no guarantees,\ntends to outperform all known approaches in terms of size/error trade-off. We\nmodify the best performing method with guarantees FrequentDirections under the\nsize/error trade-off to match the performance of iSVD and retain its\nguarantees. We also demonstrate some adversarial datasets where iSVD performs\nquite poorly. In comparing techniques in the time/error trade-off, techniques\nbased on hashing or sampling tend to perform better. In this setting we modify\nthe most studied sampling regime to retain error guarantee but obtain dramatic\nimprovements in the time/error trade-off.\n  Finally, we provide easy replication of our studies on APT, a new testbed\nwhich makes available not only code and datasets, but also a computing platform\nwith fixed environmental settings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 20:44:31 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Desai", "Amey", ""], ["Ghashami", "Mina", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1501.06619", "submitter": "Tomohiro I", "authors": "Yuto Nakashima, Tomohiro I, Shunsuke Inenaga, Hideo Bannai, Masayuki\n  Takeda", "title": "Constructing LZ78 Tries and Position Heaps in Linear Time for Large\n  Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first worst-case linear-time algorithm to compute the\nLempel-Ziv 78 factorization of a given string over an integer alphabet. Our\nalgorithm is based on nearest marked ancestor queries on the suffix tree of the\ngiven string. We also show that the same technique can be used to construct the\nposition heap of a set of strings in worst-case linear time, when the set of\nstrings is given as a trie.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 23:15:45 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Nakashima", "Yuto", ""], ["I", "Tomohiro", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1501.06626", "submitter": "Haris Aziz", "authors": "Haris Aziz, Serge Gaspers, Simon Mackenzie, Nicholas Mattei, Nina\n  Narodytska, Toby Walsh", "title": "Manipulating the Probabilistic Serial Rule", "comments": "arXiv admin note: substantial text overlap with arXiv:1401.6523", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic serial (PS) rule is one of the most prominent randomized\nrules for the assignment problem. It is well-known for its superior fairness\nand welfare properties. However, PS is not immune to manipulative behaviour by\nthe agents. We initiate the study of the computational complexity of an agent\nmanipulating the PS rule. We show that computing an expected utility better\nresponse is NP- hard. On the other hand, we present a polynomial-time algorithm\nto compute a lexicographic best response. For the case of two agents, we show\nthat even an expected utility best response can be computed in polynomial time.\nOur result for the case of two agents relies on an interesting connection with\nsequential allocation of discrete objects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 00:10:43 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Aziz", "Haris", ""], ["Gaspers", "Serge", ""], ["Mackenzie", "Simon", ""], ["Mattei", "Nicholas", ""], ["Narodytska", "Nina", ""], ["Walsh", "Toby", ""]]}, {"id": "1501.06783", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne", "title": "Big Data on the Rise: Testing monotonicity of distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of property testing of probability distributions, or distribution\ntesting, aims to provide fast and (most likely) correct answers to questions\npertaining to specific aspects of very large datasets. In this work, we\nconsider a property of particular interest, monotonicity of distributions. We\nfocus on the complexity of monotonicity testing across different models of\naccess to the distributions; and obtain results in these new settings that\ndiffer significantly from the known bounds in the standard sampling model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 15:02:35 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 20:58:39 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""]]}, {"id": "1501.06794", "submitter": "Bernhard Sch\\\"olkopf", "authors": "Bernhard Sch\\\"olkopf, Krikamol Muandet, Kenji Fukumizu, Jonas Peters", "title": "Computing Functions of Random Variables via Reproducing Kernel Hilbert\n  Space Representations", "comments": null, "journal-ref": "Statistics and Computing 25:755-766 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to perform functional operations on probability\ndistributions of random variables. The method uses reproducing kernel Hilbert\nspace representations of probability distributions, and it is applicable to all\noperations which can be applied to points drawn from the respective\ndistributions. We refer to our approach as {\\em kernel probabilistic\nprogramming}. We illustrate it on synthetic data, and show how it can be used\nfor nonparametric structural equation models, with an application to causal\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 15:36:22 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Peters", "Jonas", ""]]}, {"id": "1501.06946", "submitter": "Mike M\\\"uller", "authors": "Thorsten Ehlers and Mike M\\\"uller", "title": "New Bounds on Optimal Sorting Networks", "comments": "Submitted to CiE. arXiv admin note: text overlap with arXiv:1410.2736", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new parallel sorting networks for $17$ to $20$ inputs. For $17,\n19,$ and $20$ inputs these new networks are faster (i.e., they require less\ncomputation steps) than the previously known best networks. Therefore, we\nimprove upon the known upper bounds for minimal depth sorting networks on $17,\n19,$ and $20$ channels. Furthermore, we show that our sorting network for $17$\ninputs is optimal in the sense that no sorting network using less layers\nexists. This solves the main open problem of [D. Bundala & J. Za\\'vodn\\'y.\nOptimal sorting networks, Proc. LATA 2014].\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 22:08:45 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Ehlers", "Thorsten", ""], ["M\u00fcller", "Mike", ""]]}, {"id": "1501.07019", "submitter": "Alex Fowler", "authors": "Alex Fowler, Steven Galbraith", "title": "Kangaroo Methods for Solving the Interval Discrete Logarithm Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interval discrete logarithm problem is defined as follows: Given some\n$g,h$ in a group $G$, and some $N \\in \\mathbb{N}$ such that $g^z=h$ for some\n$z$ where $0 \\leq z < N$, find $z$. At the moment, kangaroo methods are the\nbest low memory algorithm to solve the interval discrete logarithm problem. The\nfastest non parallelised kangaroo methods to solve this problem are the three\nkangaroo method, and the four kangaroo method. These respectively have expected\naverage running times of $\\big(1.818+o(1)\\big)\\sqrt{N}$, and $\\big(1.714 +\no(1)\\big)\\sqrt{N}$ group operations. It is currently an open question as to\nwhether it is possible to improve kangaroo methods by using more than four\nkangaroos. Before this dissertation, the fastest kangaroo method that used more\nthan four kangaroos required at least $2\\sqrt{N}$ group operations to solve the\ninterval discrete logarithm problem. In this thesis, I improve the running time\nof methods that use more than four kangaroos significantly, and almost beat the\nfastest kangaroo algorithm, by presenting a seven kangaroo method with an\nexpected average running time of $\\big(1.7195 + o(1)\\big)\\sqrt{N} \\pm O(1)$\ngroup operations. The question, 'Are five kangaroos worse than three?' is also\nanswered in this thesis, as I propose a five kangaroo algorithm that requires\non average $\\big(1.737+o(1)\\big)\\sqrt{N}$ group operations to solve the\ninterval discrete logarithm problem.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 08:09:23 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Fowler", "Alex", ""], ["Galbraith", "Steven", ""]]}, {"id": "1501.07053", "submitter": "Amir Abboud", "authors": "Amir Abboud and Arturs Backurs and Virginia Vassilevska Williams", "title": "Quadratic-Time Hardness of LCS and other Sequence Similarity Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two important similarity measures between sequences are the longest common\nsubsequence (LCS) and the dynamic time warping distance (DTWD). The\ncomputations of these measures for two given sequences are central tasks in a\nvariety of applications. Simple dynamic programming algorithms solve these\ntasks in $O(n^2)$ time, and despite an extensive amount of research, no\nalgorithms with significantly better worst case upper bounds are known.\n  In this paper, we show that an $O(n^{2-\\epsilon})$ time algorithm, for some\n$\\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$\nover a constant size alphabet, refutes the popular Strong Exponential Time\nHypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over\nan alphabet of size $O(k)$ cannot be done in $O(n^{k-\\epsilon})$ time, for any\n$\\epsilon>0$, under SETH. Finally, we also address the time complexity of\napproximating the DTWD of two strings in truly subquadratic time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 10:12:58 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 22:57:04 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Abboud", "Amir", ""], ["Backurs", "Arturs", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1501.07106", "submitter": "Giordano Da Lozzo", "authors": "Giordano Da Lozzo and Ignaz Rutter", "title": "Planarity of Streamed Graphs", "comments": "21 pages, 9 figures, extended version of \"Planarity of Streamed\n  Graphs\" (9th International Conference on Algorithms and Complexity, 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a notion of planarity for graphs that are\npresented in a streaming fashion. A $\\textit{streamed graph}$ is a stream of\nedges $e_1,e_2,...,e_m$ on a vertex set $V$. A streamed graph is\n$\\omega$-$\\textit{stream planar}$ with respect to a positive integer window\nsize $\\omega$ if there exists a sequence of planar topological drawings\n$\\Gamma_i$ of the graphs $G_i=(V,\\{e_j \\mid i\\leq j < i+\\omega\\})$ such that\nthe common graph $G^{i}_\\cap=G_i\\cap G_{i+1}$ is drawn the same in $\\Gamma_i$\nand in $\\Gamma_{i+1}$, for $1\\leq i < m-\\omega$. The $\\textit{Stream\nPlanarity}$ Problem with window size $\\omega$ asks whether a given streamed\ngraph is $\\omega$-stream planar. We also consider a generalization, where there\nis an additional $\\textit{backbone graph}$ whose edges have to be present\nduring each time step. These problems are related to several well-studied\nplanarity problems.\n  We show that the $\\textit{Stream Planarity}$ Problem is NP-complete even when\nthe window size is a constant and that the variant with a backbone graph is\nNP-complete for all $\\omega \\ge 2$. On the positive side, we provide\n$O(n+\\omega{}m)$-time algorithms for (i) the case $\\omega = 1$ and (ii) all\nvalues of $\\omega$ provided the backbone graph consists of one $2$-connected\ncomponent plus isolated vertices and no stream edge connects two isolated\nvertices. Our results improve on the Hanani-Tutte-style $O((nm)^3)$-time\nalgorithm proposed by Schaefer [GD'14] for $\\omega=1$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 13:59:10 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1501.07188", "submitter": "Andreas Gemsa", "authors": "Andreas Gemsa, Benjamin Niedermann, Martin N\\\"ollenburg", "title": "Label Placement in Road Maps", "comments": "extended version of a CIAC 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A road map can be interpreted as a graph embedded in the plane, in which each\nvertex corresponds to a road junction and each edge to a particular road\nsection. We consider the cartographic problem to place non-overlapping road\nlabels along the edges so that as many road sections as possible are identified\nby their name, i.e., covered by a label. We show that this is NP-hard in\ngeneral, but the problem can be solved in polynomial time if the road map is an\nembedded tree.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 16:53:18 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Gemsa", "Andreas", ""], ["Niedermann", "Benjamin", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1501.07348", "submitter": "Changjun Wang", "authors": "Xujin Chen, Xiaodong Hu, Changjun Wang", "title": "Finding Connected Dense $k$-Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a connected graph $G$ on $n$ vertices and a positive integer $k\\le n$,\na subgraph of $G$ on $k$ vertices is called a $k$-subgraph in $G$. We design\ncombinatorial approximation algorithms for finding a connected $k$-subgraph in\n$G$ such that its density is at least a factor\n$\\Omega(\\max\\{n^{-2/5},k^2/n^2\\})$ of the density of the densest $k$-subgraph\nin $G$ (which is not necessarily connected). These particularly provide the\nfirst non-trivial approximations for the densest connected $k$-subgraph problem\non general graphs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 05:46:55 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Chen", "Xujin", ""], ["Hu", "Xiaodong", ""], ["Wang", "Changjun", ""]]}, {"id": "1501.07496", "submitter": "Helio de Oliveira M.", "authors": "E.L.F. Da Silva and H.M. de Oliveira", "title": "Implementation of an Automatic Syllabic Division Algorithm from Speech\n  Files in Portuguese Language", "comments": "9 pages, 7 figures, 4 tables, conference: XIX Congresso Brasileiro de\n  Automatica CBA, Campina Grande, Setembro, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.DS eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm for voice automatic syllabic splitting in the Portuguese\nlanguage is proposed, which is based on the envelope of the speech signal of\nthe input audio file. A computational implementation in MatlabTM is presented\nand made available at the URL\nhttp://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its\nstraightforwardness, the proposed method is very attractive for embedded\nsystems (e.g. i-phones). It can also be used as a screen to assist more\nsophisticated methods. Voice excerpts containing more than one syllable and\nidentified by the same envelope are named as super-syllables and they are\nsubsequently separated. The results indicate which samples corresponds to the\nbeginning and end of each detected syllable. Preliminary tests were performed\nto fifty words at an identification rate circa 70% (further improvements may be\nincorporated to treat particular phonemes). This algorithm is also useful in\nvoice command systems, as a tool in the teaching of Portuguese language or even\nfor patients with speech pathology.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 16:09:17 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Da Silva", "E. L. F.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1501.07725", "submitter": "Martin Dyer", "authors": "Martin Dyer, Mark Jerrum and Haiko M\\\"uller", "title": "On the switch Markov chain for perfect matchings", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a simple Markov chain, the switch chain, on the set of all perfect\nmatchings in a bipartite graph. This Markov chain was proposed by Diaconis,\nGraham and Holmes as a possible approach to a sampling problem arising in\nStatistics. We ask: for which classes of graphs is the Markov chain ergodic and\nfor which is it rapidly mixing? We provide a precise answer to the ergodicity\nquestion and close bounds on the mixing question. We show for the first time\nthat the mixing time of the switch chain is polynomial in the case of monotone\ngraphs, a class that includes examples of interest in the statistical setting.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 11:00:07 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 09:24:21 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 18:52:14 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Dyer", "Martin", ""], ["Jerrum", "Mark", ""], ["M\u00fcller", "Haiko", ""]]}, {"id": "1501.07814", "submitter": "Daniel Karapetyan Dr", "authors": "Jason Crampton and Gregory Z. Gutin and Daniel Karapetyan", "title": "Valued Workflow Satisfiability Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A workflow is a collection of steps that must be executed in some specific\norder to achieve an objective. A computerised workflow management system may\nenforce authorisation policies and constraints, thereby restricting which users\ncan perform particular steps in a workflow. The existence of policies and\nconstraints may mean that a workflow is unsatisfiable, in the sense that it is\nimpossible to find an authorised user for each step in the workflow and satisfy\nall constraints. In this paper, we consider the problem of finding the \"least\nbad\" assignment of users to workflow steps by assigning a weight to each policy\nand constraint violation. To this end, we introduce a framework for associating\ncosts with the violation of workflow policies and constraints and define the\n\\emph{valued workflow satisfiability problem} (Valued WSP), whose solution is\nan assignment of steps to users of minimum cost. We establish the computational\ncomplexity of Valued WSP with user-independent constraints and show that it is\nfixed-parameter tractable. We then describe an algorithm for solving Valued WSP\nwith user-independent constraints and evaluate its performance, comparing it to\nthat of an off-the-shelf mixed integer programming package.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 15:47:06 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Crampton", "Jason", ""], ["Gutin", "Gregory Z.", ""], ["Karapetyan", "Daniel", ""]]}]