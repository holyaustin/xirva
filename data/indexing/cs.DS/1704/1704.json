[{"id": "1704.00145", "submitter": "Kien Nguyen Trung", "authors": "Kien Trung Nguyen and Huynh Duc Quoc", "title": "Inverse Fractional Knapsack Problem with Profits and Costs Modification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address in this paper the problem of modifying both profits and costs of a\nfractional knapsack problem optimally such that a prespecified solution becomes\nan optimal solution with prespect to new parameters. This problem is called the\ninverse fractional knapsack problem. Concerning the $l_1$-norm, we first prove\nthat the problem is NP-hard. The problem can be however solved in quadratic\ntime if we only modify profit parameters. Additionally, we develop a\nquadratic-time algorithm that solves the inverse fractional knapsack problem\nunder $l_\\infty$-norm.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 09:21:56 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Nguyen", "Kien Trung", ""], ["Quoc", "Huynh Duc", ""]]}, {"id": "1704.00355", "submitter": "Neha Gupta", "authors": "Moses Charikar, Neha Gupta, Roy Schwartz", "title": "Local Guarantees in Graph Cuts and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation Clustering is an elegant model that captures fundamental graph\ncut problems such as Min $s-t$ Cut, Multiway Cut, and Multicut, extensively\nstudied in combinatorial optimization. Here, we are given a graph with edges\nlabeled $+$ or $-$ and the goal is to produce a clustering that agrees with the\nlabels as much as possible: $+$ edges within clusters and $-$ edges across\nclusters. The classical approach towards Correlation Clustering (and other\ngraph cut problems) is to optimize a global objective. We depart from this and\nstudy local objectives: minimizing the maximum number of disagreements for\nedges incident on a single node, and the analogous max min agreements\nobjective. This naturally gives rise to a family of basic min-max graph cut\nproblems. A prototypical representative is Min Max $s-t$ Cut: find an $s-t$ cut\nminimizing the largest number of cut edges incident on any node. We present the\nfollowing results: $(1)$ an $O(\\sqrt{n})$-approximation for the problem of\nminimizing the maximum total weight of disagreement edges incident on any node\n(thus providing the first known approximation for the above family of min-max\ngraph cut problems), $(2)$ a remarkably simple $7$-approximation for minimizing\nlocal disagreements in complete graphs (improving upon the previous best known\napproximation of $48$), and $(3)$ a $1/(2+\\varepsilon)$-approximation for\nmaximizing the minimum total weight of agreement edges incident on any node,\nhence improving upon the $1/(4+\\varepsilon)$-approximation that follows from\nthe study of approximate pure Nash equilibria in cut and party affiliation\ngames.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 19:34:22 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Charikar", "Moses", ""], ["Gupta", "Neha", ""], ["Schwartz", "Roy", ""]]}, {"id": "1704.00386", "submitter": "A. Erdem Sariyuce", "authors": "Ahmet Erdem Sariyuce, C. Seshadhri, Ali Pinar", "title": "Local Algorithms for Hierarchical Dense Subgraph Discovery", "comments": null, "journal-ref": null, "doi": "10.14778/3275536.3275540", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the dense regions of a graph and relations among them is a\nfundamental problem in network analysis. Core and truss decompositions reveal\ndense subgraphs with hierarchical relations. The incremental nature of\nalgorithms for computing these decompositions and the need for global\ninformation at each step of the algorithm hinders scalable parallelization and\napproximations since the densest regions are not revealed until the end. In a\nprevious work, Lu et al. proposed to iteratively compute the $h$-indices of\nneighbor vertex degrees to obtain the core numbers and prove that the\nconvergence is obtained after a finite number of iterations. This work\ngeneralizes the iterative $h$-index computation for truss decomposition as well\nas nucleus decomposition which leverages higher-order structures to generalize\ncore and truss decompositions. In addition, we prove convergence bounds on the\nnumber of iterations. We present a framework of local algorithms to obtain the\ncore, truss, and nucleus decompositions. Our algorithms are local, parallel,\noffer high scalability, and enable approximations to explore time and quality\ntrade-offs. Our shared-memory implementation verifies the efficiency,\nscalability, and effectiveness of our local algorithms on real-world networks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 23:24:06 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 04:07:32 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Sariyuce", "Ahmet Erdem", ""], ["Seshadhri", "C.", ""], ["Pinar", "Ali", ""]]}, {"id": "1704.00395", "submitter": "Mohamed Mansour", "authors": "Mohamed Mansour", "title": "A Message-Passing Algorithm for Graph Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A message-passing procedure for solving the graph isomorphism problem is\nproposed. The procedure resembles the belief-propagation algorithm in the\ncontext of graphical models inference and LDPC decoding. To enable the\nalgorithm, the input graphs are transformed into intermediate canonical\nrepresentations of bipartite graphs. The matching procedure injects specially\ndesigned input patterns to the canonical graphs and runs a message-passing\nalgorithm to generate two output fingerprints that are matched if and only if\nthe input graphs are isomorphic.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 00:44:22 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Mansour", "Mohamed", ""]]}, {"id": "1704.00513", "submitter": "Julian Romera", "authors": "Julian Romera", "title": "Optimizing Communication by Compression for Multi-GPU Scalable\n  Breadth-First Searches", "comments": "Initial version, 105 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Breadth First Search (BFS) algorithm is the foundation and building block\nof many higher graph-based operations such as spanning trees, shortest paths\nand betweenness centrality. The importance of this algorithm increases each day\ndue to it is a key requirement for many data structures which are becoming\npopular nowadays. When the BFS algorithm is parallelized by distributing the\ngraph between several processors the interconnection network limits the\nperformance. Hence, improvements on this area may benefit the overall\nperformance of the algorithm.\n  This work presents an alternative compression scheme for communications in\ndistributed BFS processing. It focuses on BFS processors using General-Purpose\nGraphics Processing Units.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:22:32 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Romera", "Julian", ""]]}, {"id": "1704.00565", "submitter": "Eva Rotenberg", "authors": "Jacob Holm, Eva Rotenberg", "title": "Dynamic Planar Embeddings of Dynamic Graphs", "comments": "Announced at STACS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to support the dynamic embedding in the plane of a\ndynamic graph. An edge can be inserted across a face between two vertices on\nthe face boundary (we call such a vertex pair linkable), and edges can be\ndeleted. The planar embedding can also be changed locally by flipping\ncomponents that are connected to the rest of the graph by at most two vertices.\n  Given vertices $u,v$, linkable$(u,v)$ decides whether $u$ and $v$ are\nlinkable in the current embedding, and if so, returns a list of suggestions for\nthe placement of $(u,v)$ in the embedding. For non-linkable vertices $u,v$, we\ndefine a new query, one-flip-linkable$(u,v)$ providing a suggestion for a flip\nthat will make them linkable if one exists. We support all updates and queries\nin O(log$^2 n$) time. Our time bounds match those of Italiano et al. for a\nstatic (flipless) embedding of a dynamic graph.\n  Our new algorithm is simpler, exploiting that the complement of a spanning\ntree of a connected plane graph is a spanning tree of the dual graph. The\nprimal and dual trees are interpreted as having the same Euler tour, and a main\nidea of the new algorithm is an elegant interaction between top trees over the\ntwo trees via their common Euler tour.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:15:59 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Holm", "Jacob", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1704.00633", "submitter": "Jelani Nelson", "authors": "Michael Kapralov, Jelani Nelson, Jakub Pachocki, Zhengyu Wang, David\n  P. Woodruff, Mobin Yahyazadeh", "title": "Optimal lower bounds for universal relation, and for samplers and\n  finding duplicates in streams", "comments": "merge of arXiv:1703.08139 and of work of Kapralov, Woodruff, and\n  Yahyazadeh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x, y \\in\\{0,1\\}^n$ with the promise that\n$x\\neq y$. The last player to receive a message must output an index $i$ such\nthat $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly\n$\\Theta(\\min\\{n,\\log(1/\\delta)\\log^2(\\frac n{\\log(1/\\delta)})\\})$ for failure\nprobability $\\delta$. Our lower bound holds even if promised\n$\\mathop{support}(y)\\subset \\mathop{support}(x)$. As a corollary, we obtain\noptimal lower bounds for $\\ell_p$-sampling in strict turnstile streams for\n$0\\le p < 2$, as well as for the problem of finding duplicates in a stream. Our\nlower bounds do not need to use large weights, and hold even if promised\n$x\\in\\{0,1\\}^n$ at all points in the stream.\n  We give two different proofs of our main result. The first proof demonstrates\nthat any algorithm $\\mathcal A$ solving sampling problems in turnstile streams\nin low memory can be used to encode subsets of $[n]$ of certain sizes into a\nnumber of bits below the information theoretic minimum. Our encoder makes\nadaptive queries to $\\mathcal A$ throughout its execution, but done carefully\nso as to not violate correctness. This is accomplished by injecting random\nnoise into the encoder's interactions with $\\mathcal A$, which is loosely\nmotivated by techniques in differential privacy. Our second proof is via a\nnovel randomized reduction from Augmented Indexing [MNSW98] which needs to\ninteract with $\\mathcal A$ adaptively. To handle the adaptivity we identify\ncertain likely interaction patterns and union bound over them to guarantee\ncorrect interaction on all of them. To guarantee correctness, it is important\nthat the interaction hides some of its randomness from $\\mathcal A$ in the\nreduction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:12:38 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Kapralov", "Michael", ""], ["Nelson", "Jelani", ""], ["Pachocki", "Jakub", ""], ["Wang", "Zhengyu", ""], ["Woodruff", "David P.", ""], ["Yahyazadeh", "Mobin", ""]]}, {"id": "1704.00693", "submitter": "Istv\\'an Z Reguly", "authors": "Istvan Z Reguly, Gihan R Mudalige, Mike B Giles", "title": "Loop Tiling in Large-Scale Stencil Codes at Run-time with OPS", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2017.2778161", "report-no": null, "categories": "cs.PF cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key common bottleneck in most stencil codes is data movement, and prior\nresearch has shown that improving data locality through optimisations that\nschedule across loops do particularly well. However, in many large PDE\napplications it is not possible to apply such optimisations through compilers\nbecause there are many options, execution paths and data per grid point, many\ndependent on run-time parameters, and the code is distributed across different\ncompilation units. In this paper, we adapt the data locality improving\noptimisation called iteration space slicing for use in large OPS applications\nboth in shared-memory and distributed-memory systems, relying on run-time\nanalysis and delayed execution. We evaluate our approach on a number of\napplications, observing speedups of 2$\\times$ on the Cloverleaf 2D/3D proxy\napplication, which contain 83/141 loops respectively, $3.5\\times$ on the linear\nsolver TeaLeaf, and $1.7\\times$ on the compressible Navier-Stokes solver\nOpenSBLI. We demonstrate strong and weak scalability up to 4608 cores of\nCINECA's Marconi supercomputer. We also evaluate our algorithms on Intel's\nKnights Landing, demonstrating maintained throughput as the problem size grows\nbeyond 16GB, and we do scaling studies up to 8704 cores. The approach is\ngenerally applicable to any stencil DSL that provides per loop data access\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:16:39 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 14:57:19 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Reguly", "Istvan Z", ""], ["Mudalige", "Gihan R", ""], ["Giles", "Mike B", ""]]}, {"id": "1704.00705", "submitter": "Christian Schulz", "authors": "Orlando Moreira, Merten Popp, Christian Schulz", "title": "Graph Partitioning with Acyclicity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are widely used to model execution dependencies in applications. In\nparticular, the NP-complete problem of partitioning a graph under constraints\nreceives enormous attention by researchers because of its applicability in\nmultiprocessor scheduling. We identified the additional constraint of acyclic\ndependencies between blocks when mapping computer vision and imaging\napplications to a heterogeneous embedded multiprocessor. Existing algorithms\nand heuristics do not address this requirement and deliver results that are not\napplicable for our use-case. In this work, we show that this more constrained\nversion of the graph partitioning problem is NP-complete and present heuristics\nthat achieve a close approximation of the optimal solution found by an\nexhaustive search for small problem instances and much better scalability for\nlarger instances. In addition, we can show a positive impact on the schedule of\na real imaging application that improves communication volume and execution\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:45:10 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Moreira", "Orlando", ""], ["Popp", "Merten", ""], ["Schulz", "Christian", ""]]}, {"id": "1704.00765", "submitter": "Stacey Jeffery", "authors": "Stacey Jeffery and Shelby Kimmel", "title": "Quantum Algorithms for Graph Connectivity and Formula Evaluation", "comments": "This version fixes a bug in statement and proof of Lemma 32\n  (regarding time complexity of algorithms). This article supersedes\n  arXiv:1511.02235", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a new upper bound on the quantum query complexity of deciding\n$st$-connectivity on certain classes of planar graphs, and show the bound is\nsometimes exponentially better than previous results. We then show Boolean\nformula evaluation reduces to deciding connectivity on just such a class of\ngraphs. Applying the algorithm for $st$-connectivity to Boolean formula\nevaluation problems, we match the $O(\\sqrt{N})$ bound on the quantum query\ncomplexity of evaluating formulas on $N$ variables, give a quadratic speed-up\nover the classical query complexity of a certain class of promise Boolean\nformulas, and show this approach can yield superpolynomial quantum/classical\nseparations. These results indicate that this $st$-connectivity-based approach\nmay be the \"right\" way of looking at quantum algorithms for formula evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:07:48 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 02:14:06 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 12:24:29 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Jeffery", "Stacey", ""], ["Kimmel", "Shelby", ""]]}, {"id": "1704.00807", "submitter": "Amirbehshad Shahrasbi", "authors": "Bernhard Haeupler, Amirbehshad Shahrasbi", "title": "Synchronization Strings: Codes for Insertions and Deletions Approaching\n  the Singleton Bound", "comments": null, "journal-ref": null, "doi": "10.1145/3055399.3055498", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce synchronization strings as a novel way of efficiently dealing\nwith synchronization errors, i.e., insertions and deletions. Synchronization\nerrors are strictly more general and much harder to deal with than commonly\nconsidered half-errors, i.e., symbol corruptions and erasures. For every\n$\\epsilon >0$, synchronization strings allow to index a sequence with an\n$\\epsilon^{-O(1)}$ size alphabet such that one can efficiently transform $k$\nsynchronization errors into $(1+\\epsilon)k$ half-errors. This powerful new\ntechnique has many applications. In this paper, we focus on designing insdel\ncodes, i.e., error correcting block codes (ECCs) for insertion deletion\nchannels.\n  While ECCs for both half-errors and synchronization errors have been\nintensely studied, the later has largely resisted progress. Indeed, it took\nuntil 1999 for the first insdel codes with constant rate, constant distance,\nand constant alphabet size to be constructed by Schulman and Zuckerman. Insdel\ncodes for asymptotically large or small noise rates were given in 2016 by\nGuruswami et al. but these codes are still polynomially far from the optimal\nrate-distance tradeoff. This makes the understanding of insdel codes up to this\nwork equivalent to what was known for regular ECCs after Forney introduced\nconcatenated codes in his doctoral thesis 50 years ago.\n  A direct application of our synchronization strings based indexing method\ngives a simple black-box construction which transforms any ECC into an equally\nefficient insdel code with a slightly larger alphabet size. This instantly\ntransfers much of the highly developed understanding for regular ECCs over\nlarge constant alphabets into the realm of insdel codes. Most notably, we\nobtain efficient insdel codes which get arbitrarily close to the optimal\nrate-distance tradeoff given by the Singleton bound for the complete noise\nspectrum.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 20:56:55 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Shahrasbi", "Amirbehshad", ""]]}, {"id": "1704.00830", "submitter": "Sikder Huq", "authors": "Sikder Huq and Sukumar Ghosh", "title": "Locally Self-Adjusting Skip Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed self-adjusting algorithm for skip graphs that\nminimizes the average routing costs between arbitrary communication pairs by\nperforming topological adaptation to the communication pattern. Our algorithm\nis fully decentralized, conforms to the $\\mathcal{CONGEST}$ model (i.e. uses\n$O(\\log n)$ bit messages), and requires $O(\\log n)$ bits of memory for each\nnode, where $n$ is the total number of nodes. Upon each communication request,\nour algorithm first establishes communication by using the standard skip graph\nrouting, and then locally and partially reconstructs the skip graph topology to\nperform topological adaptation. We propose a computational model for such\nalgorithms, as well as a yardstick (working set property) to evaluate them. Our\nworking set property can also be used to evaluate self-adjusting algorithms for\nother graph classes where multiple tree-like subgraphs overlap (e.g. hypercube\nnetworks). We derive a lower bound of the amortized routing cost for any\nalgorithm that follows our model and serves an unknown sequence of\ncommunication requests. We show that the routing cost of our algorithm is at\nmost a constant factor more than the amortized routing cost of any algorithm\nconforming to our computational model. We also show that the expected\ntransformation cost for our algorithm is at most a logarithmic factor more than\nthe amortized routing cost of any algorithm conforming to our computational\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 22:53:58 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Huq", "Sikder", ""], ["Ghosh", "Sukumar", ""]]}, {"id": "1704.00899", "submitter": "Arvind Rameshwar Vupadhyayula", "authors": "Prajakta Nimbhorkar, Arvind Rameshwar V", "title": "Dynamic Rank Maximal Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matching applicants to posts where applicants have\npreferences over posts. Thus the input to our problem is a bipartite graph G =\n(A U P,E), where A denotes a set of applicants, P is a set of posts, and there\nare ranks on edges which denote the preferences of applicants over posts. A\nmatching M in G is called rank-maximal if it matches the maximum number of\napplicants to their rank 1 posts, subject to this the maximum number of\napplicants to their rank 2 posts, and so on.\n  We consider this problem in a dynamic setting, where vertices and edges can\nbe added and deleted at any point. Let n and m be the number of vertices and\nedges in an instance G, and r be the maximum rank used by any rank-maximal\nmatching in G. We give a simple O(r(m+n))-time algorithm to update an existing\nrank-maximal matching under each of these changes. When r = o(n), this is\nfaster than recomputing a rank-maximal matching completely using a known\nalgorithm like that of Irving et al., which takes time O(min((r + n,\nr*sqrt(n))m).\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 07:26:03 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Nimbhorkar", "Prajakta", ""], ["Rameshwar", "Arvind", "V"]]}, {"id": "1704.00908", "submitter": "Nikolay Lavnikevich", "authors": "Lavnikevich Nikolay", "title": "(1, k)-Swap Local Search for Maximum Clique Problem", "comments": "14 pages, 17 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given simple undirected graph G = (V, E), the Maximum Clique Problem(MCP) is\nthat of finding a maximum-cardinality subset Q of V such that any two vertices\nin Q are adjacent. We present a modified local search algorithm for this\nproblem. Our algorithm build some maximal solution and can determine in\npolynomial time if a maximal solution can be improved by replacing a single\nvertex with k, k > 1, others. We test our algorithms on DIMACS[5], Sloane[15],\nBHOSLIB[1], Iovanella[8] and our random instances.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 07:56:27 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Nikolay", "Lavnikevich", ""]]}, {"id": "1704.01023", "submitter": "Martin F\\\"urer", "authors": "Martin F\\\"urer", "title": "On the Combinatorial Power of the Weisfeiler-Lehman Algorithm", "comments": "12 pages, accepted to CIAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical Weisfeiler-Lehman method WL[2] uses edge colors to produce a\npowerful graph invariant. It is at least as powerful in its ability to\ndistinguish non-isomorphic graphs as the most prominent algebraic graph\ninvariants. It determines not only the spectrum of a graph, and the angles\nbetween standard basis vectors and the eigenspaces, but even the angles between\nprojections of standard basis vectors into the eigenspaces. Here, we\ninvestigate the combinatorial power of WL[2]. For sufficiently large k, WL[k]\ndetermines all combinatorial properties of a graph. Many traditionally used\ncombinatorial invariants are determined by WL[k] for small k. We focus on two\nfundamental invariants, the num- ber of cycles Cp of length p, and the number\nof cliques Kp of size p. We show that WL[2] determines the number of cycles of\nlengths up to 6, but not those of length 8. Also, WL[2] does not determine the\nnumber of 4-cliques.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 14:04:37 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["F\u00fcrer", "Martin", ""]]}, {"id": "1704.01077", "submitter": "Elisabetta Bergamini", "authors": "Elisabetta Bergamini, Michele Borassi, Pierluigi Crescenzi, Andrea\n  Marino and Henning Meyerhenke", "title": "Computing top-k Closeness Centrality Faster in Unweighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a connected graph $G=(V,E)$, the closeness centrality of a vertex $v$\nis defined as $\\frac{n-1}{\\sum_{w \\in V} d(v,w)}$. This measure is widely used\nin the analysis of real-world complex networks, and the problem of selecting\nthe $k$ most central vertices has been deeply analysed in the last decade.\nHowever, this problem is computationally not easy, especially for large\nnetworks: in the first part of the paper, we prove that it is not solvable in\ntime $\\O(|E|^{2-\\epsilon})$ on directed graphs, for any constant $\\epsilon>0$,\nunder reasonable complexity assumptions. Furthermore, we propose a new\nalgorithm for selecting the $k$ most central nodes in a graph: we\nexperimentally show that this algorithm improves significantly both the\ntextbook algorithm, which is based on computing the distance between all pairs\nof vertices, and the state of the art. For example, we are able to compute the\ntop $k$ nodes in few dozens of seconds in real-world networks with millions of\nnodes and edges. Finally, as a case study, we compute the $10$ most central\nactors in the IMDB collaboration network, where two actors are linked if they\nplayed together in a movie, and in the Wikipedia citation network, which\ncontains a directed edge from a page $p$ to a page $q$ if $p$ contains a link\nto $q$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:48:55 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 08:19:45 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Borassi", "Michele", ""], ["Crescenzi", "Pierluigi", ""], ["Marino", "Andrea", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1704.01200", "submitter": "Assaf Naor", "authors": "Assaf Naor and Robert Young", "title": "The integrality gap of the Goemans--Linial SDP relaxation for Sparsest\n  Cut is at least a constant multiple of $\\sqrt{\\log n}$", "comments": "This is an extended abstract that announces results whose complete\n  proofs appear in https://arxiv.org/abs/1701.00620 (though a large part of\n  this extended abstract is material that does not appear in\n  https://arxiv.org/abs/1701.00620). It will appear in the proceedings of STOC\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the integrality gap of the Goemans--Linial semidefinite\nprogramming relaxation for the Sparsest Cut Problem is $\\Omega(\\sqrt{\\log n})$\non inputs with $n$ vertices.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 21:52:15 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Naor", "Assaf", ""], ["Young", "Robert", ""]]}, {"id": "1704.01218", "submitter": "Stephen Smart", "authors": "Stephen Smart, Christan Grant", "title": "Storing complex data sharing policies with the Min Mask Sketch", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More data is currently being collected and shared by software applications\nthan ever before. In many cases, the user is asked if either all or none of\ntheir data can be shared. We hypothesize that in some cases, users would like\nto share data in more complex ways. In order to implement the sharing of data\nusing more complicated privacy preferences, complex data sharing policies must\nbe used. These complex sharing policies require more space to store than a\nsimple \"all or nothing\" approach to data sharing. In this paper, we present a\nnew probabilistic data structure, called the Min Mask Sketch, to efficiently\nstore these complex data sharing policies. We describe an implementation for\nthe Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility\nof using a probabilistic data structure for storing complex data sharing\npolicies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 23:29:54 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Smart", "Stephen", ""], ["Grant", "Christan", ""]]}, {"id": "1704.01254", "submitter": "Di Wang", "authors": "Monika Henzinger, Satish Rao, Di Wang", "title": "Local Flow Partitioning for Faster Edge Connectivity", "comments": "Submitted to the SIAM Journal on Computing. A previous version\n  appeared in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing a minimum cut in a simple, undirected graph\nand give a deterministic $O(m \\log^2 n \\log\\log^2 n)$ time algorithm. This\nimproves both on the best previously known deterministic running time of $O(m\n\\log^{12} n)$ (Kawarabayashi and Thorup, STOC 2015) and the best previously\nknown randomized running time of $O(m \\log^{3} n)$ (Karger, J.ACM 2000) for\nthis problem, though Karger's algorithm can be further applied to weighted\ngraphs. Moreover, our result extends to balanced directed graphs, where the\nbalance of a directed graph captures how close the graph is to being Eulerian.\n  Our approach is using the Kawarabayashi and Thorup graph compression\ntechnique, which repeatedly finds low-conductance cuts. To find these cuts they\nuse a diffusion-based local algorithm. We use instead a flow-based local\nalgorithm and suitably adjust their framework to work with our flow-based\nsubroutine. Both flow and diffusion based methods have a long history of being\napplied to finding low conductance cuts. Diffusion algorithms have several\nvariants that are naturally local while it is more complicated to make flow\nmethods local. Some prior work has proven nice properties for local flow based\nalgorithms with respect to improving or cleaning up low conductance cuts. Our\nflow subroutine, however, is the first that is both local and produces low\nconductance cuts. Thus, it may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 03:21:54 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 07:00:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Henzinger", "Monika", ""], ["Rao", "Satish", ""], ["Wang", "Di", ""]]}, {"id": "1704.01311", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Pawe{\\l} Gawrychowski and Przemys{\\l}aw Uzna\\'nski", "title": "Optimal trade-offs for pattern matching with $k$ mismatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern of length $m$ and a text of length $n$, the goal in\n$k$-mismatch pattern matching is to compute, for every $m$-substring of the\ntext, the exact Hamming distance to the pattern or report that it exceeds $k$.\nThis can be solved in either $\\widetilde{O}(n \\sqrt{k})$ time as shown by Amir\net al. [J. Algorithms 2004] or $\\widetilde{O}((m + k^2) \\cdot n/m)$ time due to\na result of Clifford et al. [SODA 2016]. We provide a smooth time trade-off\nbetween these two bounds by designing an algorithm working in time\n$\\widetilde{O}( (m + k \\sqrt{m}) \\cdot n/m)$. We complement this with a\nmatching conditional lower bound, showing that a significantly faster\ncombinatorial algorithm is not possible, unless the combinatorial matrix\nmultiplication conjecture fails.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 08:45:22 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1704.01396", "submitter": "Bilal Qasemi", "authors": "Belal Qasemi (University of Bonab, Bonab, Iran)", "title": "A new algorithm for Solving 3-CNF-SAT problem", "comments": "30 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NP-Complete problems have an important attribute that if one NP-Complete\nproblem can be solved in polynomial time, all NP-Complete problems will have a\npolynomial solution. The 3-CNF-SAT problem is a NP-Complete problem and the\nprimary method to solve it checks all values of the truth table. This task is\nof the {\\Omega}(2^n) time order. This paper shows that by changing the\nviewpoint towards the problem, it is possible to know if a 3-CNF-SAT problem is\nsatisfiable in time O(n^10) or not? In this paper, the value of all clauses are\nconsidered as false. With this presumption, any of the values inside the truth\ntable can be shown in string form in order to define the set of compatible\nclauses for each of the strings. So, rather than processing strings, their\nclauses will be processed implicating that instead of 2^n strings, (O(n^3))\nclauses are to be processed; therefore, the time and space complexity of the\nalgorithm would be polynomial.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 13:06:30 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 05:07:10 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Qasemi", "Belal", "", "University of Bonab, Bonab, Iran"]]}, {"id": "1704.01460", "submitter": "Siavash Haghiri", "authors": "Siavash Haghiri, Debarghya Ghoshdastidar and Ulrike von Luxburg", "title": "Comparison Based Nearest Neighbor Search", "comments": "16 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine learning in a comparison-based setting where we are given\na set of points in a metric space, but we have no access to the actual\ndistances between the points. Instead, we can only ask an oracle whether the\ndistance between two points $i$ and $j$ is smaller than the distance between\nthe points $i$ and $k$. We are concerned with data structures and algorithms to\nfind nearest neighbors based on such comparisons. We focus on a simple yet\neffective algorithm that recursively splits the space by first selecting two\nrandom pivot points and then assigning all other points to the closer of the\ntwo (comparison tree). We prove that if the metric space satisfies certain\nexpansion conditions, then with high probability the height of the comparison\ntree is logarithmic in the number of points, leading to efficient search\nperformance. We also provide an upper bound for the failure probability to\nreturn the true nearest neighbor. Experiments show that the comparison tree is\ncompetitive with algorithms that have access to the actual distance values, and\nneeds less triplet comparisons than other competitors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:54:28 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Haghiri", "Siavash", ""], ["Ghoshdastidar", "Debarghya", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1704.01646", "submitter": "Tsvi Kopelowitz", "authors": "Shay Golan, Tsvi Kopelowitz, Ely Porat", "title": "Streaming Pattern Matching with d Wildcards", "comments": "Extended abstract appeared in ESA 2016", "journal-ref": null, "doi": "10.1007/s00453-018-0521-7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the pattern matching with $d$ wildcards problem one is given a text $T$ of\nlength $n$ and a pattern $P$ of length $m$ that contains $d$ wildcard\ncharacters, each denoted by a special symbol $'?'$. A wildcard character\nmatches any other character. The goal is to establish for each $m$-length\nsubstring of $T$ whether it matches $P$. In the streaming model variant of the\npattern matching with $d$ wildcards problem the text $T$ arrives one character\nat a time and the goal is to report, before the next character arrives, if the\nlast $m$ characters match $P$ while using only $o(m)$ words of space.\n  In this paper we introduce two new algorithms for the $d$ wildcard pattern\nmatching problem in the streaming model. The first is a randomized Monte Carlo\nalgorithm that is parameterized by a constant $0\\leq \\delta \\leq 1$. This\nalgorithm uses $\\tilde{O}(d^{1-\\delta})$ amortized time per character and\n$\\tilde{O}(d^{1+\\delta})$ words of space. The second algorithm, which is used\nas a black box in the first algorithm, is a randomized Monte Carlo algorithm\nwhich uses $O(d+\\log m)$ worst-case time per character and $O(d\\log m)$ words\nof space.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 20:30:27 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Golan", "Shay", ""], ["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""]]}, {"id": "1704.01652", "submitter": "Christopher Harshaw", "authors": "Moran Feldman, Christopher Harshaw, Amin Karbasi", "title": "Greed is Good: Near-Optimal Submodular Maximization via Greedy\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that greedy methods perform well for maximizing monotone\nsubmodular functions. At the same time, such methods perform poorly in the face\nof non-monotonicity. In this paper, we show - arguably, surprisingly - that\ninvoking the classical greedy algorithm $O(\\sqrt{k})$-times leads to the\n(currently) fastest deterministic algorithm, called Repeated Greedy, for\nmaximizing a general submodular function subject to $k$-independent system\nconstraints. Repeated Greedy achieves $(1 + O(1/\\sqrt{k}))k$ approximation\nusing $O(nr\\sqrt{k})$ function evaluations (here, $n$ and $r$ denote the size\nof the ground set and the maximum size of a feasible solution, respectively).\nWe then show that by a careful sampling procedure, we can run the greedy\nalgorithm only once and obtain the (currently) fastest randomized algorithm,\ncalled Sample Greedy, for maximizing a submodular function subject to\n$k$-extendible system constraints (a subclass of $k$-independent system\nconstrains). Sample Greedy achieves $(k + 3)$-approximation with only $O(nr/k)$\nfunction evaluations. Finally, we derive an almost matching lower bound, and\nshow that no polynomial time algorithm can have an approximation ratio smaller\nthan $ k + 1/2 - \\varepsilon$. To further support our theoretical results, we\ncompare the performance of Repeated Greedy and Sample Greedy with prior art in\na concrete application (movie recommendation). We consistently observe that\nwhile Sample Greedy achieves practically the same utility as the best baseline,\nit performs at least two orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 21:03:53 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Feldman", "Moran", ""], ["Harshaw", "Christopher", ""], ["Karbasi", "Amin", ""]]}, {"id": "1704.01676", "submitter": "Aman Chadha Mr.", "authors": "Anthony Gregerson, Aman Chadha, Katherine Morrow", "title": "Multi-Personality Partitioning for Heterogeneous Systems", "comments": "International Conference on Field-Programmable Technology (ICFPT),\n  Kyoto Research Park, Japan, Dec. 9-11, 2013. hardware design; hardware\n  architecture; cad; computer aided design; IC design; integrated circuit\n  design; partitioning algorithms; field programmable gate arrays; benchmark\n  testing; heuristic algorithms; resource management; dynamic scheduling;\n  digital signal processing", "journal-ref": null, "doi": "10.1109/FPT.2013.6718375", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design flows use graph partitioning both as a precursor to place and route\nfor single devices, and to divide netlists or task graphs among multiple\ndevices. Partitioners have accommodated FPGA heterogeneity via multi-resource\nconstraints, but have not yet exploited the corresponding ability to implement\nsome computations in multiple ways (e.g., LUTs vs. DSP blocks), which could\nenable a superior solution. This paper introduces multi-personality graph\npartitioning, which incorporates aspects of resource mapping into partitioning.\nWe present a modified multi-level KLFM partitioning algorithm that also\nperforms heterogeneous resource mapping for nodes with multiple potential\nimplementations (multiple personalities). We evaluate several variants of our\nmulti-personality FPGA circuit partitioner using 21 circuits and benchmark\ngraphs, and show that dynamic resource mapping improves cut size on average by\n27% over static mapping for these circuits. We further show that it improves\ndeviation from target resource utilizations by 50% over post-partitioning\nresource mapping.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 01:04:24 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Gregerson", "Anthony", ""], ["Chadha", "Aman", ""], ["Morrow", "Katherine", ""]]}, {"id": "1704.01862", "submitter": "Ragesh Jaiswal", "authors": "Nir Ailon, Anup Bhattacharya, Ragesh Jaiswal, Amit Kumar", "title": "Approximate Clustering with Same-Cluster Queries", "comments": "Updated version has results for faulty queries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ashtiani et al. proposed a Semi-Supervised Active Clustering framework\n(SSAC), where the learner is allowed to make adaptive queries to a domain\nexpert. The queries are of the kind \"do two given points belong to the same\noptimal cluster?\" There are many clustering contexts where such same-cluster\nqueries are feasible. Ashtiani et al. exhibited the power of such queries by\nshowing that any instance of the $k$-means clustering problem, with additional\nmargin assumption, can be solved efficiently if one is allowed $O(k^2 \\log{k} +\nk \\log{n})$ same-cluster queries. This is interesting since the $k$-means\nproblem, even with the margin assumption, is $\\mathsf{NP}$-hard.\n  In this paper, we extend the work of Ashtiani et al. to the approximation\nsetting showing that a few of such same-cluster queries enables one to get a\npolynomial-time $(1 + \\varepsilon)$-approximation algorithm for the $k$-means\nproblem without any margin assumption on the input dataset. Again, this is\ninteresting since the $k$-means problem is $\\mathsf{NP}$-hard to approximate\nwithin a factor $(1 + c)$ for a fixed constant $0 < c < 1$. The number of\nsame-cluster queries used is $\\textrm{poly}(k/\\varepsilon)$ which is\nindependent of the size $n$ of the dataset. Our algorithm is based on the\n$D^2$-sampling technique. We also give a conditional lower bound on the number\nof same-cluster queries showing that if the Exponential Time Hypothesis (ETH)\nholds, then any such efficient query algorithm needs to make $\\Omega\n\\left(\\frac{k}{poly \\log k} \\right)$ same-cluster queries. Our algorithm can be\nextended for the case when the oracle is faulty. Another result we show with\nrespect to the $k$-means++ seeding algorithm is that a small modification to\nthe $k$-means++ seeding algorithm within the SSAC framework converts it to a\nconstant factor approximation algorithm instead of the well known\n$O(\\log{k})$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:36:58 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 07:16:51 GMT"}, {"version": "v3", "created": "Wed, 4 Oct 2017 13:02:34 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Ailon", "Nir", ""], ["Bhattacharya", "Anup", ""], ["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "1704.01869", "submitter": "Mengdi Wang", "authors": "Mengdi Wang", "title": "Randomized Linear Programming Solves the Discounted Markov Decision\n  Problem In Nearly-Linear (Sometimes Sublinear) Running Time", "comments": null, "journal-ref": "published by Mathematics of Operations Research, 2019", "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel randomized linear programming algorithm for approximating\nthe optimal policy of the discounted Markov decision problem. By leveraging the\nvalue-policy duality and binary-tree data structures, the algorithm adaptively\nsamples state-action-state transitions and makes exponentiated primal-dual\nupdates. We show that it finds an $\\epsilon$-optimal policy using nearly-linear\nrun time in the worst case. When the Markov decision process is ergodic and\nspecified in some special data formats, the algorithm finds an\n$\\epsilon$-optimal policy using run time linear in the total number of\nstate-action pairs, which is sublinear in the input size. These results provide\na new venue and complexity benchmarks for solving stochastic dynamic programs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:45:40 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 14:55:57 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 17:19:09 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Wang", "Mengdi", ""]]}, {"id": "1704.01983", "submitter": "Manuel Surek", "authors": "Tobias Harks, Anja Huber, Manuel Surek", "title": "A Characterization of Undirected Graphs Admitting Optimal Cost Shares", "comments": "60 pages, 69 figures, OR 2017 Berlin, WINE 2017 Bangalore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper, Chen, Roughgarden and Valiant studied cost sharing\nprotocols for network design with the objective to implement a low-cost Steiner\nforest as a Nash equilibrium of an induced cost-sharing game. One of the most\nintriguing open problems to date is to understand the power of budget-balanced\nand separable cost sharing protocols in order to induce low-cost Steiner\nforests. In this work, we focus on undirected networks and analyze topological\nproperties of the underlying graph so that an optimal Steiner forest can be\nimplemented as a Nash equilibrium (by some separable cost sharing protocol)\nindependent of the edge costs. We term a graph efficient if the above stated\nproperty holds. As our main result, we give a complete characterization of\nefficient undirected graphs for two-player network design games: an undirected\ngraph is efficient if and only if it does not contain (at least) one out of few\nforbidden subgraphs. Our characterization implies that several graph classes\nare efficient: generalized series-parallel graphs, fan and wheel graphs and\ngraphs with small cycles.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 18:21:19 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 08:54:58 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Harks", "Tobias", ""], ["Huber", "Anja", ""], ["Surek", "Manuel", ""]]}, {"id": "1704.01996", "submitter": "Timothy Goodrich", "authors": "Timothy D. Goodrich and Travis S. Humble and Blair D. Sullivan", "title": "Optimizing Adiabatic Quantum Program Compilation using a Graph-Theoretic\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adiabatic quantum computing has evolved in recent years from a theoretical\nfield into an immensely practical area, a change partially sparked by D-Wave\nSystem's quantum annealing hardware. These multimillion-dollar quantum\nannealers offer the potential to solve optimization problems millions of times\nfaster than classical heuristics, prompting researchers at Google, NASA and\nLockheed Martin to study how these computers can be applied to complex\nreal-world problems such as NASA rover missions. Unfortunately, compiling\n(embedding) an optimization problem into the annealing hardware is itself a\ndifficult optimization problem and a major bottleneck currently preventing\nwidespread adoption. Additionally, while finding a single embedding is\ndifficult, no generalized method is known for tuning embeddings to use minimal\nhardware resources. To address these barriers, we introduce a graph-theoretic\nframework for developing structured embedding algorithms. Using this framework,\nwe introduce a biclique virtual hardware layer to provide a simplified\ninterface to the physical hardware. Additionally, we exploit bipartite\nstructure in quantum programs using odd cycle transversal (OCT) decompositions.\nBy coupling an OCT-based embedding algorithm with new, generalized reduction\nmethods, we develop a new baseline for embedding a wide range of optimization\nproblems into fault-free D-Wave annealing hardware. To encourage the reuse and\nextension of these techniques, we provide an implementation of the framework\nand embedding algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 19:28:09 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 13:22:37 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Goodrich", "Timothy D.", ""], ["Humble", "Travis S.", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1704.02054", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas Dybdahl Ahle", "title": "Optimal Las Vegas Locality Sensitive Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that approximate similarity (near neighbour) search can be solved in\nhigh dimensions with performance matching state of the art (data independent)\nLocality Sensitive Hashing, but with a guarantee of no false negatives.\n  Specifically, we give two data structures for common problems.\n  For $c$-approximate near neighbour in Hamming space we get query time\n$dn^{1/c+o(1)}$ and space $dn^{1+1/c+o(1)}$ matching that of\n\\cite{indyk1998approximate} and answering a long standing open question\nfrom~\\cite{indyk2000dimensionality} and~\\cite{pagh2016locality} in the\naffirmative.\n  By means of a new deterministic reduction from $\\ell_1$ to Hamming we also\nsolve $\\ell_1$ and $\\ell_2$ with query time $d^2n^{1/c+o(1)}$ and space $d^2\nn^{1+1/c+o(1)}$.\n  For $(s_1,s_2)$-approximate Jaccard similarity we get query time\n$dn^{\\rho+o(1)}$ and space $dn^{1+\\rho+o(1)}$,\n$\\rho=\\log\\frac{1+s_1}{2s_1}\\big/\\log\\frac{1+s_2}{2s_2}$, when sets have equal\nsize, matching the performance of~\\cite{tobias2016}.\n  The algorithms are based on space partitions, as with classic LSH, but we\nconstruct these using a combination of brute force, tensoring, perfect hashing\nand splitter functions \\`a la~\\cite{naor1995splitters}. We also show a new\ndimensionality reduction lemma with 1-sided error.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 23:44:31 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 10:52:39 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 13:34:52 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Ahle", "Thomas Dybdahl", ""]]}, {"id": "1704.02061", "submitter": "Joseph Tassarotti", "authors": "Joseph Tassarotti", "title": "Probabilistic Recurrence Relations for Work and Span of Parallel\n  Algorithms", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for obtaining tail-bounds for random\nvariables satisfying certain probabilistic recurrences that arise in the\nanalysis of randomized parallel divide and conquer algorithms. In such\nalgorithms, some computation is initially done to process an input x, which is\nthen randomly split into subproblems $h_1(x), ..., h_n(x)$, and the algorithm\nproceeds recursively in parallel on each subproblem. The total work on input x,\nW(x), then satisfies a probabilistic recurrence of the form $W(x) = a(x) +\n\\sum_{i=1}^n W (h_i(x))$, and the span (the longest chain of sequential\ndependencies), satisfies $S(x) = b(x) + \\max_{i=1}^n S(h_i(x))$, where a(x) and\nb(x) are the work and span to split x and combine the results of the recursive\ncalls.\n  Karp has previously presented methods for obtaining tail-bounds in the case\nwhen n = 1, and under certain stronger assumptions for the work-recurrence when\nn > 1, but left open the question of the span-recurrence. We first show how to\nextend his technique to handle the span-recurrence. We then show that in some\ncases, the work-recurrence can be bounded under simpler assumptions than Karp's\nby transforming it into a related span-recurrence and applying our first\nresult. We demonstrate our results by deriving tail bounds for the work and\nspan of quicksort and the height of a randomly generated binary search tree.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 01:00:36 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Tassarotti", "Joseph", ""]]}, {"id": "1704.02062", "submitter": "Andrew Francis", "authors": "Andrew Francis, Katharina Huber, Vincent Moulton", "title": "Tree-based unrooted phylogenetic networks", "comments": "12 pages, 6 figures. This is a pre-print of an article published in\n  Bulletin of Mathematical Biology. The final authenticated version is\n  available online at the DOI listed below", "journal-ref": null, "doi": "10.1007/s11538-017-0381-3", "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks are a generalization of phylogenetic trees that are\nused to represent non-tree-like evolutionary histories that arise in organisms\nsuch as plants and bacteria, or uncertainty in evolutionary histories. An\n\\emph{unrooted} phylogenetic network on a nonempty, finite set $X$ of taxa, or\n\\emph{network}, is a connected graph in which every vertex has degree 1 or 3\nand whose leaf-set is $X$. It is called a \\emph{phylogenetic tree} if the\nunderlying graph is a tree. In this paper we consider properties of\n\\emph{tree-based networks}, that is, networks that can be constructed by adding\nedges into a phylogenetic tree. We show that although they have some properties\nin common with their rooted analogues which have recently drawn much attention\nin the literature, they have some striking differences in terms of both their\nstructural and computational properties. We expect that our results could\neventually have applications to, for example, detecting horizontal gene\ntransfer or hyrbridization which are important factors in the evolution of many\norganisms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 01:07:26 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 07:23:23 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 10:23:02 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Francis", "Andrew", ""], ["Huber", "Katharina", ""], ["Moulton", "Vincent", ""]]}, {"id": "1704.02147", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad and Varun Kanade and Frederik Mallmann-Trenn and\n  Claire Mathieu", "title": "Hierarchical Clustering: Objective Functions and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a recursive partitioning of a dataset into\nclusters at an increasingly finer granularity. Motivated by the fact that most\nwork on hierarchical clustering was based on providing algorithms, rather than\noptimizing a specific objective, Dasgupta framed similarity-based hierarchical\nclustering as a combinatorial optimization problem, where a `good' hierarchical\nclustering is one that minimizes some cost function. He showed that this cost\nfunction has certain desirable properties.\n  We take an axiomatic approach to defining `good' objective functions for both\nsimilarity and dissimilarity-based hierarchical clustering. We characterize a\nset of \"admissible\" objective functions (that includes Dasgupta's one) that\nhave the property that when the input admits a `natural' hierarchical\nclustering, it has an optimal value.\n  Equipped with a suitable objective function, we analyze the performance of\npractical algorithms, as well as develop better algorithms. For\nsimilarity-based hierarchical clustering, Dasgupta showed that the divisive\nsparsest-cut approach achieves an $O(\\log^{3/2} n)$-approximation. We give a\nrefined analysis of the algorithm and show that it in fact achieves an\n$O(\\sqrt{\\log n})$-approx. (Charikar and Chatziafratis independently proved\nthat it is a $O(\\sqrt{\\log n})$-approx.). This improves upon the LP-based\n$O(\\log n)$-approx. of Roy and Pokutta. For dissimilarity-based hierarchical\nclustering, we show that the classic average-linkage algorithm gives a factor 2\napprox., and provide a simple and better algorithm that gives a factor 3/2\napprox..\n  Finally, we consider `beyond-worst-case' scenario through a generalisation of\nthe stochastic block model for hierarchical clustering. We show that Dasgupta's\ncost function has desirable properties for these inputs and we provide a simple\n1 + o(1)-approximation in this setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:14:28 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Kanade", "Varun", ""], ["Mallmann-Trenn", "Frederik", ""], ["Mathieu", "Claire", ""]]}, {"id": "1704.02178", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen, Morten St\\\"ockel", "title": "New Subquadratic Approximation Algorithms for the Girth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating the girth, $g$, of an unweighted and\nundirected graph $G=(V,E)$ with $n$ nodes and $m$ edges. A seminal result of\nItai and Rodeh [SICOMP'78] gave an additive $1$-approximation in $O(n^2)$ time,\nand the main open question is thus how well we can do in subquadratic time.\n  In this paper we present two main results. The first is a\n$(1+\\varepsilon,O(1))$-approximation in truly subquadratic time. Specifically,\nfor any $k\\ge 2$ our algorithm returns a cycle of length $2\\lceil\ng/2\\rceil+2\\left\\lceil\\frac{g}{2(k-1)}\\right\\rceil$ in $\\tilde{O}(n^{2-1/k})$\ntime. This generalizes the results of Lingas and Lundell [IPL'09] who showed it\nfor the special case of $k=2$ and Roditty and Vassilevska Williams [SODA'12]\nwho showed it for $k=3$. Our second result is to present an\n$O(1)$-approximation running in $O(n^{1+\\varepsilon})$ time for any\n$\\varepsilon > 0$. Prior to this work the fastest constant-factor approximation\nwas the $\\tilde{O}(n^{3/2})$ time $8/3$-approximation of Lingas and Lundell\n[IPL'09] using the algorithm corresponding to the special case $k=2$ of our\nfirst result.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 10:43:25 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["St\u00f6ckel", "Morten", ""]]}, {"id": "1704.02183", "submitter": "Krzysztof Sornat", "authors": "Jaros{\\l}aw Byrka, Piotr Skowron, Krzysztof Sornat", "title": "Proportional Approval Voting, Harmonic k-median, and Negative\n  Association", "comments": "30 pages, 6 figures, 1 table, added: a constant factor approximation\n  algorithm for metric OWA k-median", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generic framework that provides a unified view on two important\nclasses of problems: (i) extensions of the k-median problem where clients are\ninterested in having multiple facilities in their vicinity (e.g., due to the\nfact that, with some small probability, the closest facility might be\nmalfunctioning and so might not be available for using), and (ii) finding\nwinners according to some appealing multiwinner election rules, i.e., election\nsystem aimed for choosing representatives bodies, such as parliaments, based on\npreferences of a population of voters over individual candidates. Each problem\nin our framework is associated with a vector of weights: we show that the\napproximability of the problem depends on structural properties of these\nvectors. We specifically focus on the harmonic sequence of weights for which\nthe objective function interpreted in a multiwinner election setup reflects to\nthe well-known Proportional Approval Voting (PAV) rule.\n  Our main result is that, due to the specific (harmonic) structure of weights,\nthe problem allows constant factor approximation. This is surprising since the\nproblem can be interpreted as a variant of the k-median problem where we do not\nassume that the connection costs satisfy the triangle inequality. The algorithm\nwe propose is based on dependent rounding [Srinivasan, FOCS'01] applied to the\nsolution of a natural LP-relaxation of the problem. The rounding process is\nwell known to produce distributions over integral solutions satisfying Negative\nCorrelation (NC), which is usually sufficient for the analysis of approximation\nguarantees offered by rounding procedures. In our analysis, however, we need to\nuse the fact that the carefully implemented rounding process satisfies a\nstronger property, called Negative Association (NA), which allows us to apply\nstandard concentration bounds for conditional random variables.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 10:57:15 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 13:57:08 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 12:42:32 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Skowron", "Piotr", ""], ["Sornat", "Krzysztof", ""]]}, {"id": "1704.02239", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay (1), Simon Barthelme (2), Pierre-Olivier Amblard (1)\n  ((1) CNRS, GIPSA-CICS (2) CNRS, GIPSA-VIBS)", "title": "\\'Echantillonnage de signaux sur graphes via des processus\n  d\\'eterminantaux", "comments": "in French", "journal-ref": "GRETSI, Sep 2017, Juan-les-Pins, France", "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling k-bandlimited graph signals, ie, linear\ncombinations of the first k graph Fourier modes. We know that a set of k nodes\nembedding all k-bandlimited signals always exists, thereby enabling their\nperfect reconstruction after sampling. Unfortunately, to exhibit such a set,\none needs to partially diagonalize the graph Laplacian, which becomes\nprohibitive at large scale. We propose a novel strategy based on determinantal\npoint processes that side-steps partial diagonalisation and enables\nreconstruction with only O(k) samples. While doing so, we exhibit a new general\nalgorithm to sample determinantal process, faster than the state-of-the-art\nalgorithm by an order k.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 14:11:36 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 09:34:13 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Tremblay", "Nicolas", "", "CNRS, GIPSA-CICS"], ["Barthelme", "Simon", "", "CNRS, GIPSA-VIBS"], ["Amblard", "Pierre-Olivier", "", "CNRS, GIPSA-CICS"]]}, {"id": "1704.02303", "submitter": "Nil Mamano", "authors": "David Eppstein (1), Michael T. Goodrich (1) and Nil Mamano (1) ((1)\n  University of California, Irvine)", "title": "Algorithms for Stable Matching and Clustering in a Grid", "comments": "23 pages, 12 figures. To appear (without the appendices) at the 18th\n  International Workshop on Combinatorial Image Analysis, June 19-21, 2017,\n  Plovdiv, Bulgaria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a discrete version of a geometric stable marriage problem originally\nproposed in a continuous setting by Hoffman, Holroyd, and Peres, in which\npoints in the plane are stably matched to cluster centers, as prioritized by\ntheir distances, so that each cluster center is apportioned a set of points of\nequal area. We show that, for a discretization of the problem to an $n\\times n$\ngrid of pixels with $k$ centers, the problem can be solved in time $O(n^2\n\\log^5 n)$, and we experiment with two slower but more practical algorithms and\na hybrid method that switches from one of these algorithms to the other to gain\ngreater efficiency than either algorithm alone. We also show how to combine\ngeometric stable matchings with a $k$-means clustering algorithm, so as to\nprovide a geometric political-districting algorithm that views distance in\neconomic terms, and we experiment with weighted versions of stable $k$-means in\norder to improve the connectivity of the resulting clusters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:37:44 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Mamano", "Nil", ""]]}, {"id": "1704.02310", "submitter": "Dimitris Tsipras", "authors": "Michael B. Cohen, Aleksander Madry, Dimitris Tsipras, Adrian Vladu", "title": "Matrix Scaling and Balancing via Box Constrained Newton's Method and\n  Interior Point Methods", "comments": "To appear in FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study matrix scaling and balancing, which are fundamental\nproblems in scientific computing, with a long line of work on them that dates\nback to the 1960s. We provide algorithms for both these problems that, ignoring\nlogarithmic factors involving the dimension of the input matrix and the size of\nits entries, both run in time $\\widetilde{O}\\left(m\\log \\kappa \\log^2\n(1/\\epsilon)\\right)$ where $\\epsilon$ is the amount of error we are willing to\ntolerate. Here, $\\kappa$ represents the ratio between the largest and the\nsmallest entries of the optimal scalings. This implies that our algorithms run\nin nearly-linear time whenever $\\kappa$ is quasi-polynomial, which includes, in\nparticular, the case of strictly positive matrices. We complement our results\nby providing a separate algorithm that uses an interior-point method and runs\nin time $\\widetilde{O}(m^{3/2} \\log (1/\\epsilon))$.\n  In order to establish these results, we develop a new second-order\noptimization framework that enables us to treat both problems in a unified and\nprincipled manner. This framework identifies a certain generalization of linear\nsystem solving that we can use to efficiently minimize a broad class of\nfunctions, which we call second-order robust. We then show that in the context\nof the specific functions capturing matrix scaling and balancing, we can\nleverage and generalize the work on Laplacian system solving to make the\nalgorithms obtained via this framework very efficient.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:50:43 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 05:07:51 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Cohen", "Michael B.", ""], ["Madry", "Aleksander", ""], ["Tsipras", "Dimitris", ""], ["Vladu", "Adrian", ""]]}, {"id": "1704.02315", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Oliveira, Avi Wigderson", "title": "Much Faster Algorithms for Matrix Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop several efficient algorithms for the classical \\emph{Matrix\nScaling} problem, which is used in many diverse areas, from preconditioning\nlinear systems to approximation of the permanent. On an input $n\\times n$\nmatrix $A$, this problem asks to find diagonal (scaling) matrices $X$ and $Y$\n(if they exist), so that $X A Y$ $\\varepsilon$-approximates a doubly\nstochastic, or more generally a matrix with prescribed row and column sums.\n  We address the general scaling problem as well as some important special\ncases. In particular, if $A$ has $m$ nonzero entries, and if there exist $X$\nand $Y$ with polynomially large entries such that $X A Y$ is doubly stochastic,\nthen we can solve the problem in total complexity $\\tilde{O}(m + n^{4/3})$.\nThis greatly improves on the best known previous results, which were either\n$\\tilde{O}(n^4)$ or $O(m n^{1/2}/\\varepsilon)$.\n  Our algorithms are based on tailor-made first and second order techniques,\ncombined with other recent advances in continuous optimization, which may be of\nindependent interest for solving similar problems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:57:19 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Oliveira", "Rafael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1704.02367", "submitter": "Omri Ben-Eliezer", "authors": "Noga Alon, Omri Ben-Eliezer, Eldar Fischer", "title": "Testing hereditary properties of ordered graphs and matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider properties of edge-colored vertex-ordered graphs, i.e., graphs\nwith a totally ordered vertex set and a finite set of possible edge colors. We\nshow that any hereditary property of such graphs is strongly testable, i.e.,\ntestable with a constant number of queries. We also explain how the proof can\nbe adapted to show that any hereditary property of $2$-dimensional matrices\nover a finite alphabet (where row and column order is not ignored) is strongly\ntestable. The first result generalizes the result of Alon and Shapira [FOCS'05,\nSICOMP'08], who showed that any hereditary graph property (without vertex\norder) is strongly testable. The second result answers and generalizes a\nconjecture of Alon, Fischer and Newman [SICOMP'07] concerning testing of matrix\nproperties.\n  The testability is proved by establishing a removal lemma for vertex-ordered\ngraphs. It states that for any finite or infinite family $\\mathcal{F}$ of\nforbidden vertex-ordered graphs, and any $\\epsilon > 0$, there exist $\\delta >\n0$ and $k$ so that any vertex-ordered graph which is $\\epsilon$-far from being\n$\\mathcal{F}$-free contains at least $\\delta n^{|F|}$ copies of some\n$F\\in\\mathcal{F}$ (with the correct vertex order) where $|F|\\leq k$. The proof\nbridges the gap between techniques related to the regularity lemma, used in the\nlong chain of papers investigating graph testing, and string testing\ntechniques. Along the way we develop a Ramsey-type lemma for $k$-partite graphs\nwith \"undesirable\" edges, stating that one can find a Ramsey-type structure in\nsuch a graph, in which the density of the undesirable edges is not much higher\nthan the density of those edges in the graph.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 20:29:56 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Alon", "Noga", ""], ["Ben-Eliezer", "Omri", ""], ["Fischer", "Eldar", ""]]}, {"id": "1704.02380", "submitter": "Oren Louidor", "authors": "Lihi Cohen, Yuval Emek, Oren Louidor, Jara Uitto", "title": "Exploring an Infinite Space with Finite Memory Scouts", "comments": "Added (forgotten) acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a small number of scouts exploring the infinite $d$-dimensional grid\nwith the aim of hitting a hidden target point. Each scout is controlled by a\nprobabilistic finite automaton that determines its movement (to a neighboring\ngrid point) based on its current state. The scouts, that operate under a fully\nsynchronous schedule, communicate with each other (in a way that affects their\nrespective states) when they share the same grid point and operate\nindependently otherwise. Our main research question is: How many scouts are\nrequired to guarantee that the target admits a finite mean hitting time?\nRecently, it was shown that $d + 1$ is an upper bound on the answer to this\nquestion for any dimension $d \\geq 1$ and the main contribution of this paper\ncomes in the form of proving that this bound is tight for $d \\in \\{ 1, 2 \\}$.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 21:43:30 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 11:10:12 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Cohen", "Lihi", ""], ["Emek", "Yuval", ""], ["Louidor", "Oren", ""], ["Uitto", "Jara", ""]]}, {"id": "1704.02608", "submitter": "Moran Feldman", "authors": "Moran Feldman, Ola Svensson and Rico Zenklusen", "title": "A Framework for the Secretary Problem on the Intersection of Matroids", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secretary problem became one of the most prominent online selection\nproblems due to its numerous applications in online mechanism design. The task\nis to select a maximum weight subset of elements subject to given constraints,\nwhere elements arrive one-by-one in random order, revealing a weight upon\narrival. The decision whether to select an element has to be taken immediately\nafter its arrival. The different applications that map to the secretary problem\nask for different constraint families to be handled. The most prominent ones\nare matroid constraints, which both capture many relevant settings and admit\nstrongly competitive secretary algorithms. However, dealing with more involved\nconstraints proved to be much more difficult, and strong algorithms are known\nonly for a few specific settings. In this paper, we present a general framework\nfor dealing with the secretary problem over the intersection of several\nmatroids. This framework allows us to combine and exploit the large set of\nmatroid secretary algorithms known in the literature. As one consequence, we\nget constant-competitive secretary algorithms over the intersection of any\nconstant number of matroids whose corresponding (single-)matroid secretary\nproblems are currently known to have a constant-competitive algorithm.\nMoreover, we show that our results extend to submodular objectives.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 14:19:17 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Feldman", "Moran", ""], ["Svensson", "Ola", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1704.02657", "submitter": "Thomas Lidbetter Dr", "authors": "Lisa Hellerstein, Thomas Lidbetter and Daniel Pirutinsky", "title": "Solving Zero-sum Games using Best Response Oracles with Applications to\n  Search Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient algorithms for computing optimal or approximately\noptimal strategies in a zero-sum game for which Player I has n pure strategies\nand Player II has an arbitrary number of pure strategies. We assume that for\nany given mixed strategy of Player I, a best response or \"approximate\" best\nresponse of Player II can be found by an oracle in time polynomial in n. We\nthen show how our algorithms may be applied to several search games with\napplications to security and counter-terrorism. We evaluate our main algorithm\nexperimentally on a prototypical search game. Our results show it performs well\ncompared to an existing, well-known algorithm for solving zero-sum games that\ncan also be used to solve search games, given a best response oracle.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 20:41:14 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 16:10:16 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 01:25:44 GMT"}, {"version": "v4", "created": "Wed, 20 Jun 2018 15:44:18 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hellerstein", "Lisa", ""], ["Lidbetter", "Thomas", ""], ["Pirutinsky", "Daniel", ""]]}, {"id": "1704.02700", "submitter": "Yoichi Iwata", "authors": "Yoichi Iwata, Yutaro Yamaguchi, Yuichi Yoshida", "title": "0/1/all CSPs, Half-Integral $A$-path Packing, and Linear-Time FPT\n  Algorithms", "comments": "Added new results on two-fan constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend in the design of FPT algorithms is exploiting the\nhalf-integrality of LP relaxations. In other words, starting with a\nhalf-integral optimal solution to an LP relaxation, we assign integral values\nto variables one-by-one by branch and bound. This technique is general and the\nresulting time complexity has a low dependency on the parameter. However, the\ntime complexity often becomes a large polynomial in the input size because we\nneed to compute half-integral optimal LP solutions.\n  In this paper, we address this issue by providing an $O(km)$-time algorithm\nfor solving the LPs arising from various FPT problems, where $k$ is the optimal\nvalue and $m$ is the number of edges/constraints. Our algorithm is based on\ninteresting connections among 0/1/all constraints, which has been studied in\nthe field of constraints satisfaction, $A$-path packing, which has been studied\nin the field of combinatorial optimization, and the LPs used in FPT algorithms.\nWith the aid of this algorithm, we obtain improved FPT algorithms for various\nproblems, including Group Feedback Vertex Set, Subset Feedback Vertex Set, Node\nMultiway Cut, Node Unique Label Cover, and Non-monochromatic Cycle Transversal.\nThe obtained running time for each of these problems is linear in the input\nsize and has the current smallest dependency on the parameter. In particular,\nthese algorithms are the first linear-time FPT algorithms for problems\nincluding Group Feedback Vertex Set and Non-monochromatic Cycle Transversal.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 03:50:16 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 07:54:13 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Iwata", "Yoichi", ""], ["Yamaguchi", "Yutaro", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1704.02767", "submitter": "Manuela Fischer", "authors": "Manuela Fischer, Mohsen Ghaffari, Fabian Kuhn", "title": "Deterministic Distributed Edge-Coloring via Hypergraph Maximal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed algorithm that computes a\n$(2\\Delta-1)$-edge-coloring, or even list-edge-coloring, in any $n$-node graph\nwith maximum degree $\\Delta$, in $O(\\log^7 \\Delta \\log n)$ rounds. This answers\none of the long-standing open questions of \\emph{distributed graph algorithms}\nfrom the late 1980s, which asked for a polylogarithmic-time algorithm. See,\ne.g., Open Problem 4 in the Distributed Graph Coloring book of Barenboim and\nElkin. The previous best round complexities were $2^{O(\\sqrt{\\log n})}$ by\nPanconesi and Srinivasan [STOC'92] and $\\tilde{O}(\\sqrt{\\Delta}) + O(\\log^* n)$\nby Fraigniaud, Heinrich, and Kosowski [FOCS'16]. A corollary of our\ndeterministic list-edge-coloring also improves the randomized complexity of\n$(2\\Delta-1)$-edge-coloring to poly$(\\log\\log n)$ rounds.\n  The key technical ingredient is a deterministic distributed algorithm for\n\\emph{hypergraph maximal matching}, which we believe will be of interest beyond\nthis result. In any hypergraph of rank $r$ --- where each hyperedge has at most\n$r$ vertices --- with $n$ nodes and maximum degree $\\Delta$, this algorithm\ncomputes a maximal matching in $O(r^5 \\log^{6+\\log r } \\Delta \\log n)$ rounds.\n  This hypergraph matching algorithm and its extensions lead to a number of\nother results. In particular, a polylogarithmic-time deterministic distributed\nmaximal independent set algorithm for graphs with bounded neighborhood\nindependence, hence answering Open Problem 5 of Barenboim and Elkin's book, a\n$((\\log \\Delta/\\varepsilon)^{O(\\log (1/\\varepsilon))})$-round deterministic\nalgorithm for $(1+\\varepsilon)$-approximation of maximum matching, and a\nquasi-polylogarithmic-time deterministic distributed algorithm for orienting\n$\\lambda$-arboricity graphs with out-degree at most $(1+\\varepsilon)\\lambda$,\nfor any constant $\\varepsilon>0$, hence partially answering Open Problem 10 of\nBarenboim and Elkin's book.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:03:11 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Fischer", "Manuela", ""], ["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1704.02782", "submitter": "Brahim Chaourar", "authors": "Brahim Chaourar", "title": "The Kth Traveling Salesman Problem is Pseudopolynomial when TSP is\n  polynomial", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G=(V, E)$ with a weight function $c\\in R^E$, and a\npositive integer $K$, the Kth Traveling Salesman Problem (KthTSP) is to find\n$K$ Hamilton cycles $H_1, H_2, , ..., H_K$ such that, for any Hamilton cycle\n$H\\not \\in \\{H_1, H_2, , ..., H_K \\}$, we have $c(H)\\geq c(H_i), i=1, 2, ...,\nK$. This problem is NP-hard even for $K$ fixed. We prove that KthTSP is\npseudopolynomial when TSP is polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:55:16 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Chaourar", "Brahim", ""]]}, {"id": "1704.02793", "submitter": "Shay Mozes", "authors": "Pawe{\\l} Gawrychowski, Haim Kaplan, Shay Mozes, Micha Sharir, Oren\n  Weimann", "title": "Voronoi diagrams on planar graphs, and computing the diameter in\n  deterministic $\\tilde{O}(n^{5/3})$ time", "comments": "SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an explicit and efficient construction of additively weighted\nVoronoi diagrams on planar graphs. Let $G$ be a planar graph with $n$ vertices\nand $b$ sites that lie on a constant number of faces. We show how to preprocess\n$G$ in $\\tilde O(nb^2)$ time (footnote: The $\\tilde O$ notation hides\npolylogarithmic factors.) so that one can compute any additively weighted\nVoronoi diagram for these sites in $\\tilde O(b)$ time.\n  We use this construction to compute the diameter of a directed planar graph\nwith real arc lengths in $\\tilde{O}(n^{5/3})$ time. This improves the recent\nbreakthrough result of Cabello (SODA'17), both by improving the running time\n(from $\\tilde{O}(n^{11/6})$), and by providing a deterministic algorithm. It is\nin fact the first truly subquadratic {\\em deterministic} algorithm for this\nproblem. Our use of Voronoi diagrams to compute the diameter follows that of\nCabello, but he used abstract Voronoi diagrams, which makes his diameter\nalgorithm more involved, more expensive, and randomized.\n  As in Cabello's work, our algorithm can compute, for every vertex $v$, both\nthe farthest vertex from $v$ (i.e., the eccentricity of $v$), and the sum of\ndistances from $v$ to all other vertices. Hence, our algorithm can also compute\nthe radius, median, and Wiener index (sum of all pairwise distances) of a\nplanar graph within the same time bounds. Our construction of Voronoi diagrams\nfor planar graphs is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:35:42 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 14:46:20 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 20:43:52 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Kaplan", "Haim", ""], ["Mozes", "Shay", ""], ["Sharir", "Micha", ""], ["Weimann", "Oren", ""]]}, {"id": "1704.02844", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Danupon Nanongkai", "title": "Fully Dynamic Approximate Maximum Matching and Minimum Vertex Cover in\n  $O(\\log^3 n)$ Worst Case Update Time", "comments": "An extended abstract of this paper appeared in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maintaining an approximately maximum (fractional)\nmatching and an approximately minimum vertex cover in a dynamic graph. Starting\nwith the seminal paper by Onak and Rubinfeld [STOC 2010], this problem has\nreceived significant attention in recent years. There remains, however, a\npolynomial gap between the best known worst case update time and the best known\namortised update time for this problem, even after allowing for randomisation.\nSpecifically, Bernstein and Stein [ICALP 2015, SODA 2016] have the best known\nworst case update time. They present a deterministic data structure with\napproximation ratio $(3/2+\\epsilon)$ and worst case update time\n$O(m^{1/4}/\\epsilon^2)$, where $m$ is the number of edges in the graph. In\nrecent past, Gupta and Peng [FOCS 2013] gave a deterministic data structure\nwith approximation ratio $(1+\\epsilon)$ and worst case update time\n$O(\\sqrt{m}/\\epsilon^2)$. No known randomised data structure beats the worst\ncase update times of these two results. In contrast, the paper by Onak and\nRubinfeld [STOC 2010] gave a randomised data structure with approximation ratio\n$O(1)$ and amortised update time $O(\\log^2 n)$, where $n$ is the number of\nnodes in the graph. This was later improved by Baswana, Gupta and Sen [FOCS\n2011] and Solomon [FOCS 2016], leading to a randomised date structure with\napproximation ratio $2$ and amortised update time $O(1)$.\n  We bridge the polynomial gap between the worst case and amortised update\ntimes for this problem, without using any randomisation. We present a\ndeterministic data structure with approximation ratio $(2+\\epsilon)$ and worst\ncase update time $O(\\log^3 n)$, for all sufficiently small constants\n$\\epsilon$.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:10:43 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1704.02939", "submitter": "Nikola Yolov", "authors": "Nikola Yolov", "title": "Minor-matching hypertree width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new width measure for a tree decomposition,\nminor-matching hypertree width, $\\mu\\text{-}tw$, for graphs and hypergraphs,\nsuch that bounding the width guarantees that set of maximal independent sets\nhas a polynomially-sized restriction to each decomposition bag. The relaxed\nconditions of the decomposition allow a much wider class of graphs and\nhypergraphs of bounded width compared to other tree decompositions. We show\nthat, for fixed $k$, there are $2^{(1 - \\frac1k + o(1)){n \\choose 2}}$\n$n$-vertex graphs of minor-matching hypertree width at most $k$. A number of\nproblems including Maximum Independence Set, $k$-Colouring, and Homomorphism of\nuniform hypergraphs permit polynomial-time solutions for hypergraphs with\nbounded minor-matching hypertree width and bounded rank. We show that for any\ngiven $k$ and any graph $G$, it is possible to construct a decomposition of\nminor-matching hypertree width at most $O(k^3)$, or to prove that\n$\\mu\\text{-}tw(G) > k$ in time $n^{O(k^3)}$. This is done by presenting a\ngeneral algorithm for approximating the hypertree width of well-behaved\nmeasures, and reducing $\\mu\\text{-}tw$ to such measure. The result relating the\nrestriction of the maximal independent sets to a set $S$ with the set of\ninduced matchings intersecting $S$ in graphs, and minor matchings intersecting\n$S$ in hypergraphs, might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 16:25:55 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 16:29:51 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Yolov", "Nikola", ""]]}, {"id": "1704.02958", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk, Ludwig Schmidt", "title": "On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:26:41 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1704.03024", "submitter": "Thomas Steinke", "authors": "Thomas Steinke, Jonathan Ullman", "title": "Tight Lower Bounds for Differentially Private Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pervasive task in the differential privacy literature is to select the $k$\nitems of \"highest quality\" out of a set of $d$ items, where the quality of each\nitem depends on a sensitive dataset that must be protected. Variants of this\ntask arise naturally in fundamental problems like feature selection and\nhypothesis testing, and also as subroutines for many sophisticated\ndifferentially private algorithms.\n  The standard approaches to these tasks---repeated use of the exponential\nmechanism or the sparse vector technique---approximately solve this problem\ngiven a dataset of $n = O(\\sqrt{k}\\log d)$ samples. We provide a tight lower\nbound for some very simple variants of the private selection problem. Our lower\nbound shows that a sample of size $n = \\Omega(\\sqrt{k} \\log d)$ is required\neven to achieve a very minimal accuracy guarantee.\n  Our results are based on an extension of the fingerprinting method to sparse\nselection problems. Previously, the fingerprinting method has been used to\nprovide tight lower bounds for answering an entire set of $d$ queries, but\noften only some much smaller set of $k$ queries are relevant. Our extension\nallows us to prove lower bounds that depend on both the number of relevant\nqueries and the total number of queries.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:16:12 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1704.03318", "submitter": "Marek Elias", "authors": "Nikhil Bansal, Marek Elias, Grigorios Koumoutsos", "title": "Weighted k-Server Bounds via Combinatorial Dichotomies", "comments": "accepted to FOCS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted $k$-server problem is a natural generalization of the $k$-server\nproblem where each server has a different weight. We consider the problem on\nuniform metrics, which corresponds to a natural generalization of paging. Our\nmain result is a doubly exponential lower bound on the competitive ratio of any\ndeterministic online algorithm, that essentially matches the known upper bounds\nfor the problem and closes a large and long-standing gap.\n  The lower bound is based on relating the weighted $k$-server problem to a\ncertain combinatorial problem and proving a Ramsey-theoretic lower bound for\nit. This combinatorial connection also reveals several structural properties of\nlow cost feasible solutions to serve a sequence of requests. We use this to\nshow that the generalized Work Function Algorithm achieves an almost optimum\ncompetitive ratio, and to obtain new refined upper bounds on the competitive\nratio for the case of $d$ different weight classes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:39:52 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 15:17:57 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Bansal", "Nikhil", ""], ["Elias", "Marek", ""], ["Koumoutsos", "Grigorios", ""]]}, {"id": "1704.03371", "submitter": "Cameron Musco", "authors": "Cameron Musco and David P. Woodruff", "title": "Sublinear Time Low-Rank Approximation of Positive Semidefinite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute a relative-error low-rank approximation to any\npositive semidefinite (PSD) matrix in sublinear time, i.e., for any $n \\times\nn$ PSD matrix $A$, in $\\tilde O(n \\cdot poly(k/\\epsilon))$ time we output a\nrank-$k$ matrix $B$, in factored form, for which $\\|A-B\\|_F^2 \\leq\n(1+\\epsilon)\\|A-A_k\\|_F^2$, where $A_k$ is the best rank-$k$ approximation to\n$A$. When $k$ and $1/\\epsilon$ are not too large compared to the sparsity of\n$A$, our algorithm does not need to read all entries of the matrix. Hence, we\nsignificantly improve upon previous $nnz(A)$ time algorithms based on oblivious\nsubspace embeddings, and bypass an $nnz(A)$ time lower bound for general\nmatrices (where $nnz(A)$ denotes the number of non-zero entries in the matrix).\nWe prove time lower bounds for low-rank approximation of PSD matrices, showing\nthat our algorithm is close to optimal. Finally, we extend our techniques to\ngive sublinear time algorithms for low-rank approximation of $A$ in the (often\nstronger) spectral norm metric $\\|A-B\\|_2^2$ and for ridge regression on PSD\nmatrices.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:44:49 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 02:59:15 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 15:24:34 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Musco", "Cameron", ""], ["Woodruff", "David P.", ""]]}, {"id": "1704.03486", "submitter": "Nima Anari", "authors": "Nima Anari, Leonid Gurvits, Shayan Oveis Gharan, Amin Saberi", "title": "Simply Exponential Approximation of the Permanent of Positive\n  Semidefinite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a deterministic polynomial time $c^n$ approximation algorithm for\nthe permanent of positive semidefinite matrices where $c=e^{\\gamma+1}\\simeq\n4.84$. We write a natural convex relaxation and show that its optimum solution\ngives a $c^n$ approximation of the permanent. We further show that this factor\nis asymptotically tight by constructing a family of positive semidefinite\nmatrices.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:28:13 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Anari", "Nima", ""], ["Gurvits", "Leonid", ""], ["Gharan", "Shayan Oveis", ""], ["Saberi", "Amin", ""]]}, {"id": "1704.03664", "submitter": "Francesco Quinzan", "authors": "Ankit Chauhan and Tobias Friedrich and Francesco Quinzan", "title": "Approximating Optimization Problems using EAs on Scale-Free Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed that many complex real-world networks have certain\nproperties, such as a high clustering coefficient, a low diameter, and a\npower-law degree distribution. A network with a power-law degree distribution\nis known as scale-free network. In order to study these networks, various\nrandom graph models have been proposed, e.g. Preferential Attachment, Chung-Lu,\nor Hyperbolic.\n  We look at the interplay between the power-law degree distribution and the\nrun time of optimization techniques for well known combinatorial problems. We\nobserve that on scale-free networks, simple evolutionary algorithms (EAs)\nquickly reach a constant-factor approximation ratio on common covering problems\n  We prove that the single-objective (1+1)EA reaches a constant-factor\napproximation ratio on the Minimum Dominating Set problem, the Minimum Vertex\nCover problem, the Minimum Connected Dominating Set problem, and the Maximum\nIndependent Set problem in expected polynomial number of calls to the fitness\nfunction.\n  Furthermore, we prove that the multi-objective GSEMO algorithm reaches a\nbetter approximation ratio than the (1+1)EA on those problems, within\npolynomial fitness evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 09:06:06 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 14:08:52 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 13:15:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chauhan", "Ankit", ""], ["Friedrich", "Tobias", ""], ["Quinzan", "Francesco", ""]]}, {"id": "1704.03777", "submitter": "Spyros Angelopoulos", "authors": "Spyros Angelopoulos and Konstantinos Panagiotou", "title": "Optimal strategies for weighted ray search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the general setting of weighted search in which a\nnumber of targets, each with a certain weight, are hidden in a star-like\nenvironment that consists of $m$ infinite, concurrent rays, with a common\norigin. A mobile searcher, initially located at the origin, explores this\nenvironment so as to locate a set of targets whose aggregate weight is at least\na given value $W$. The cost of the search strategy is defined as the total\ndistance traversed by the searcher, and its performance is measured by the\nworst-case ratio of the cost incurred by the searcher over the cost of an on\noptimal, offline strategy with complete access to the instance. This is the\nfirst study of a setting that generalizes several problems in search theory:\nthe problem in which only a single target is sought, as well as the problem in\nwhich all targets have unit weights.\n  We present and analyze a search strategy of near-optimal performance for the\nproblem at hand. We observe that the classical approaches that rely on\ngeometrically increasing search depths perform rather poorly in the context of\nweighted search. We bypass this problem by using a strategy that modifies the\nsearch depths adaptively, depending on the number of targets located up to the\ncurrent point in time.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 14:50:36 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:32:47 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Panagiotou", "Konstantinos", ""]]}, {"id": "1704.03864", "submitter": "Ankit Garg", "authors": "Ankit Garg and Yin Tat Lee and Zhao Song and Nikhil Srivastava", "title": "A Matrix Expander Chernoff Bound", "comments": "Fixed a minor bug in the proof of Theorem 3.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a Chernoff-type bound for sums of matrix-valued random variables\nsampled via a random walk on an expander, confirming a conjecture due to\nWigderson and Xiao. Our proof is based on a new multi-matrix extension of the\nGolden-Thompson inequality which improves in some ways the inequality of\nSutter, Berta, and Tomamichel, and may be of independent interest, as well as\nan adaptation of an argument for the scalar case due to Healy. Secondarily, we\nalso provide a generic reduction showing that any concentration inequality for\nvector-valued martingales implies a concentration inequality for the\ncorresponding expander walk, with a weakening of parameters proportional to the\nsquared mixing time.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:54:39 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 19:45:03 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 16:55:27 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Garg", "Ankit", ""], ["Lee", "Yin Tat", ""], ["Song", "Zhao", ""], ["Srivastava", "Nikhil", ""]]}, {"id": "1704.03866", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Robustly Learning a Gaussian: Getting Optimal Error, Efficiently", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of learning the parameters of a\nhigh-dimensional Gaussian in the presence of noise -- where an\n$\\varepsilon$-fraction of our samples were chosen by an adversary. We give\nrobust estimators that achieve estimation error $O(\\varepsilon)$ in the total\nvariation distance, which is optimal up to a universal constant that is\nindependent of the dimension.\n  In the case where just the mean is unknown, our robustness guarantee is\noptimal up to a factor of $\\sqrt{2}$ and the running time is polynomial in $d$\nand $1/\\epsilon$. When both the mean and covariance are unknown, the running\ntime is polynomial in $d$ and quasipolynomial in $1/\\varepsilon$. Moreover all\nof our algorithms require only a polynomial number of samples. Our work shows\nthat the same sorts of error guarantees that were established over fifty years\nago in the one-dimensional setting can also be achieved by efficient algorithms\nin high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 17:55:05 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 21:52:55 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel M.", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1704.03892", "submitter": "Nikhil Srivastava", "authors": "Nima Anari, Shayan Oveis Gharan, Amin Saberi, Nikhil Srivastava", "title": "Approximating the Largest Root and Applications to Interlacing Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating the largest root of a real-rooted\npolynomial of degree $n$ using its top $k$ coefficients and give nearly\nmatching upper and lower bounds. We present algorithms with running time\npolynomial in $k$ that use the top $k$ coefficients to approximate the maximum\nroot within a factor of $n^{1/k}$ and $1+O(\\tfrac{\\log n}{k})^2$ when $k\\leq\n\\log n$ and $k>\\log n$ respectively. We also prove corresponding\ninformation-theoretic lower bounds of $n^{\\Omega(1/k)}$ and\n$1+\\Omega\\left(\\frac{\\log \\frac{2n}{k}}{k}\\right)^2$, and show strong lower\nbounds for noisy version of the problem in which one is given access to\napproximate coefficients.\n  This problem has applications in the context of the method of interlacing\nfamilies of polynomials, which was used for proving the existence of Ramanujan\ngraphs of all degrees, the solution of the Kadison-Singer problem, and bounding\nthe integrality gap of the asymmetric traveling salesman problem. All of these\ninvolve computing the maximum root of certain real-rooted polynomials for which\nthe top few coefficients are accessible in subexponential time. Our results\nyield an algorithm with the running time of $2^{\\tilde O(\\sqrt[3]n)}$ for all\nof them.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 18:31:52 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""], ["Saberi", "Amin", ""], ["Srivastava", "Nikhil", ""]]}, {"id": "1704.03928", "submitter": "Noah Stephens-Davidowitz", "authors": "Huck Bennett, Alexander Golovnev, Noah Stephens-Davidowitz", "title": "On the Quantitative Hardness of CVP", "comments": null, "journal-ref": "FOCS 2017", "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ \\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\problem}[1]{\\ensuremath{\\mathrm{#1}} }\n\\newcommand{\\CVP}{\\problem{CVP}} \\newcommand{\\SVP}{\\problem{SVP}}\n\\newcommand{\\CVPP}{\\problem{CVPP}} \\newcommand{\\ensuremath}[1]{#1} $For odd\nintegers $p \\geq 1$ (and $p = \\infty$), we show that the Closest Vector Problem\nin the $\\ell_p$ norm ($\\CVP_p$) over rank $n$ lattices cannot be solved in\n$2^{(1-\\eps) n}$ time for any constant $\\eps > 0$ unless the Strong Exponential\nTime Hypothesis (SETH) fails. We then extend this result to \"almost all\" values\nof $p \\geq 1$, not including the even integers. This comes tantalizingly close\nto settling the quantitative time complexity of the important special case of\n$\\CVP_2$ (i.e., $\\CVP$ in the Euclidean norm), for which a $2^{n +o(n)}$-time\nalgorithm is known. In particular, our result applies for any $p = p(n) \\neq 2$\nthat approaches $2$ as $n \\to \\infty$.\n  We also show a similar SETH-hardness result for $\\SVP_\\infty$; hardness of\napproximating $\\CVP_p$ to within some constant factor under the so-called\nGap-ETH assumption; and other quantitative hardness results for $\\CVP_p$ and\n$\\CVPP_p$ for any $1 \\leq p < \\infty$ under different assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:55:59 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 19:05:01 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Bennett", "Huck", ""], ["Golovnev", "Alexander", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1704.04163", "submitter": "Cameron Musco", "authors": "Cameron Musco, Praneeth Netrapalli, Aaron Sidford, Shashanka Ubaru,\n  David P. Woodruff", "title": "Spectrum Approximation Beyond Fast Matrix Multiplication: Algorithms and\n  Hardness", "comments": "ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the singular value spectrum of a matrix $A \\in \\mathbb{R}^{n\n\\times n}$ is a fundamental task in countless applications. In matrix\nmultiplication time, it is possible to perform a full SVD and directly compute\nthe singular values $\\sigma_1,...,\\sigma_n$. However, little is known about\nalgorithms that break this runtime barrier.\n  Using tools from stochastic trace estimation, polynomial approximation, and\nfast system solvers, we show how to efficiently isolate different ranges of\n$A$'s spectrum and approximate the number of singular values in these ranges.\nWe thus effectively compute a histogram of the spectrum, which can stand in for\nthe true singular values in many applications.\n  We use this primitive to give the first algorithms for approximating a wide\nclass of symmetric matrix norms in faster than matrix multiplication time. For\nexample, we give a $(1 + \\epsilon)$ approximation algorithm for the\nSchatten-$1$ norm (the nuclear norm) running in just $\\tilde O((nnz(A)n^{1/3} +\nn^2)\\epsilon^{-3})$ time for $A$ with uniform row sparsity or $\\tilde\nO(n^{2.18} \\epsilon^{-3})$ time for dense matrices. The runtime scales smoothly\nfor general Schatten-$p$ norms, notably becoming $\\tilde O (p \\cdot nnz(A)\n\\epsilon^{-3})$ for any $p \\ge 2$.\n  At the same time, we show that the complexity of spectrum approximation is\ninherently tied to fast matrix multiplication in the small $\\epsilon$ regime.\nWe prove that achieving milder $\\epsilon$ dependencies in our algorithms would\nimply faster than matrix multiplication time triangle detection for general\ngraphs. This further implies that highly accurate algorithms running in\nsubcubic time yield subcubic time matrix multiplication. As an application of\nour bounds, we show that precisely computing all effective resistances in a\ngraph in less than matrix multiplication time is likely difficult, barring a\nmajor algorithmic breakthrough.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 14:55:23 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 14:15:31 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 15:37:07 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Musco", "Cameron", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""], ["Ubaru", "Shashanka", ""], ["Woodruff", "David P.", ""]]}, {"id": "1704.04205", "submitter": "Maxim Buzdalov", "authors": "Margarita Markina and Maxim Buzdalov", "title": "Hybridizing Non-dominated Sorting Algorithms: Divide-and-Conquer Meets\n  Best Order Sort", "comments": "A two-page abstract of this paper will appear in the proceedings\n  companion of the 2017 Genetic and Evolutionary Computation Conference (GECCO\n  2017)", "journal-ref": null, "doi": "10.1145/3067695.3076074", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many production-grade algorithms benefit from combining an asymptotically\nefficient algorithm for solving big problem instances, by splitting them into\nsmaller ones, and an asymptotically inefficient algorithm with a very small\nimplementation constant for solving small subproblems. A well-known example is\nstable sorting, where mergesort is often combined with insertion sort to\nachieve a constant but noticeable speed-up.\n  We apply this idea to non-dominated sorting. Namely, we combine the\ndivide-and-conquer algorithm, which has the currently best known asymptotic\nruntime of $O(N (\\log N)^{M - 1})$, with the Best Order Sort algorithm, which\nhas the runtime of $O(N^2 M)$ but demonstrates the best practical performance\nout of quadratic algorithms.\n  Empirical evaluation shows that the hybrid's running time is typically not\nworse than of both original algorithms, while for large numbers of points it\noutperforms them by at least 20%. For smaller numbers of objectives, the\nspeedup can be as large as four times.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 16:36:44 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Markina", "Margarita", ""], ["Buzdalov", "Maxim", ""]]}, {"id": "1704.04249", "submitter": "Ramanujan M. S.", "authors": "Daniel Lokshtanov and M. S. Ramanujan and Saket Saurabh and Meirav\n  Zehavi", "title": "Parameterized Complexity and Approximability of Directed Odd Cycle\n  Transversal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed odd cycle transversal of a directed graph (digraph) $D$ is a\nvertex set $S$ that intersects every odd directed cycle of $D$. In the Directed\nOdd Cycle Transversal (DOCT) problem, the input consists of a digraph $D$ and\nan integer $k$. The objective is to determine whether there exists a directed\nodd cycle transversal of $D$ of size at most $k$.\n  In this paper, we settle the parameterized complexity of DOCT when\nparameterized by the solution size $k$ by showing that DOCT does not admit an\nalgorithm with running time $f(k)n^{O(1)}$ unless FPT = W[1]. On the positive\nside, we give a factor $2$ fixed parameter tractable (FPT) approximation\nalgorithm for the problem. More precisely, our algorithm takes as input $D$ and\n$k$, runs in time $2^{O(k^2)}n^{O(1)}$, and either concludes that $D$ does not\nhave a directed odd cycle transversal of size at most $k$, or produces a\nsolution of size at most $2k$. Finally, we provide evidence that there exists\n$\\epsilon > 0$ such that DOCT does not admit a factor $(1+\\epsilon)$\nFPT-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 18:27:04 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1704.04332", "submitter": "Dieyan Liang", "authors": "Dieyan Liang, Hong Shen", "title": "Point Sweep Coverage on Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important application of wireless sensor networks is the deployment of\nmobile sensors to periodically monitor (cover) a set of points of interest\n(PoIs). The problem of Point Sweep Coverage is to deploy fewest sensors to\nperiodically cover the set of PoIs. For PoIs in a Eulerian graph, this problem\nis known NP-Hard even if all sensors are with uniform velocity. In this paper,\nwe study the problem when PoIs are on a line and prove that the decision\nversion of the problem is NP-Complete if the sensors are with different\nvelocities. We first formulate the problem of Max-PoI sweep coverage on path\n(MPSCP) to find the maximum number of PoIs covered by a given set of sensors,\nand then show it is NP-Hard. We also extend it to the weighted case, Max-Weight\nsweep coverage on path (MWSCP) problem to maximum the sum of the weight of PoIs\ncovered. For sensors with uniform velocity, we give a polynomial-time optimal\nsolution to MWSCP. For sensors with constant kinds of velocities, we present a\n$\\frac{1}{2}$-approximation algorithm. For the general case of arbitrary\nvelocities, we propose two algorithms. One is a\n$\\frac{1}{2\\alpha}$-approximation algorithm family scheme, where integer\n$\\alpha\\ge2$ is the tradeoff factor to balance the time complexity and\napproximation ratio. The other is a $\\frac{1}{2}(1-1/e)$-approximation\nalgorithm by randomized analysis.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 02:24:56 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 04:45:46 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Liang", "Dieyan", ""], ["Shen", "Hong", ""]]}, {"id": "1704.04362", "submitter": "Davoud Ataee Tarzanagh", "authors": "Davoud Ataee Tarzanagh and George Michailidis", "title": "Fast Randomized Algorithms for t-Product Based Tensor Operations and\n  Decompositions with Applications to Imaging Data", "comments": "31 pages, 6 figures, to appear in the SIAM Journal on Imaging\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors of order three or higher have found applications in diverse fields,\nincluding image and signal processing, data mining, biomedical engineering and\nlink analysis, to name a few. In many applications that involve for example\ntime series or other ordered data, the corresponding tensor has a\ndistinguishing orientation that exhibits a low tubal structure. This has\nmotivated the introduction of the tubal rank and the corresponding tubal\nsingular value decomposition in the literature. In this work, we develop\nrandomized algorithms for many common tensor operations, including tensor\nlow-rank approximation and decomposition, together with tensor multiplication.\nThe proposed tubal focused algorithms employ a small number of lateral and/or\nhorizontal slices of the underlying 3-rd order tensor, that come with {\\em\nrelative error guarantees} for the quality of the obtained solutions. The\nperformance of the proposed algorithms is illustrated on diverse imaging\napplications, including mass spectrometry data and image and video recovery\nfrom incomplete and noisy data. The results show both good computational\nspeed-up vis-a-vis conventional completion algorithms and good accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 06:30:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 18:55:17 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 19:00:33 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 18:33:35 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Tarzanagh", "Davoud Ataee", ""], ["Michailidis", "George", ""]]}, {"id": "1704.04370", "submitter": "Mathias B{\\ae}k Tejs Knudsen", "authors": "S{\\o}ren Dahlgaard and Mathias B{\\ae}k Tejs Knudsen and Mikkel Thorup", "title": "Fast Similarity Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Similarity Sketching problem: Given a universe $[u]=\n\\{0,\\ldots,u-1\\}$ we want a random function $S$ mapping subsets $A\\subseteq\n[u]$ into vectors $S(A)$ of size $t$, such that similarity is preserved. More\nprecisely: Given sets $A,B\\subseteq [u]$, define $X_i=[S(A)[i]= S(B)[i]]$ and\n$X=\\sum_{i\\in [t]}X_i$. We want to have $E[X]=t\\cdot J(A,B)$, where\n$J(A,B)=|A\\cap B|/|A\\cup B|$ and furthermore to have strong concentration\nguarantees (i.e. Chernoff-style bounds) for $X$. This is a fundamental problem\nwhich has found numerous applications in data mining, large-scale\nclassification, computer vision, similarity search, etc. via the classic\nMinHash algorithm. The vectors $S(A)$ are also called sketches.\n  The seminal $t\\times$MinHash algorithm uses $t$ random hash functions\n$h_1,\\ldots, h_t$, and stores $\\left(\\min_{a\\in A}h_1(A),\\ldots, \\min_{a\\in\nA}h_t(A)\\right)$ as the sketch of $A$. The main drawback of MinHash is,\nhowever, its $O(t\\cdot |A|)$ running time, and finding a sketch with similar\nproperties and faster running time has been the subject of several papers.\nAddressing this, Li et al. [NIPS'12] introduced one permutation hashing (OPH),\nwhich creates a sketch of size $t$ in $O(t + |A|)$ time, but with the drawback\nthat possibly some of the $t$ entries are \"empty\" when $|A| = O(t)$. One could\nargue that sketching is not necessary in this case, however the desire in most\napplications is to have one sketching procedure that works for sets of all\nsizes. Therefore, filling out these empty entries is the subject of several\nfollow-up papers initiated by Shrivastava and Li [ICML'14]. However, these\n\"densification\" schemes fail to provide good concentration bounds exactly in\nthe case $|A| = O(t)$, where they are needed. (continued...)\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 09:24:04 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1704.04389", "submitter": "Micha{\\l}  Karpi\\'nski", "authors": "Micha{\\l} Karpi\\'nski, Marek Piotr\\'ow", "title": "Encoding Cardinality Constraints using Generalized Selection Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean cardinality constraints state that at most (at least, or exactly) $k$\nout of $n$ propositional literals can be true. We propose a new class of\nselection networks that can be used for an efficient encoding of them. Several\ncomparator networks have been proposed recently for encoding cardinality\nconstraints and experiments have proved their efficiency. Those were based\nmainly on the odd-even or pairwise comparator networks. We use similar ideas,\nbut we extend the model of comparator networks so that the basic components are\nnot only comparators (2-sorters) but more general $m$-sorters, for $m \\geq 2$.\nThe inputs are organized into $m$ columns, in which elements are recursively\nselected and, after that, columns are merged using an idea of multi-way\nmerging. We present two algorithms parametrized by $m \\geq 2$. We call those\nnetworks $m$-Wise Selection Network and $m$-Odd-Even Selection Network. We give\ndetailed construction of the mergers when $m=4$. The construction can be\ndirectly applied to any values of $k$ and $n$. The proposed encoding of sorters\nis standard, therefore the arc-consistency is preserved. We prove correctness\nof the constructions and present the theoretical and experimental evaluation,\nwhich show that the new encodings are competitive to the other state-of-art\nencodings.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 10:53:34 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Karpi\u0144ski", "Micha\u0142", ""], ["Piotr\u00f3w", "Marek", ""]]}, {"id": "1704.04472", "submitter": "Tomasz Kociumaka", "authors": "Patrick Hagge Cording and Travis Gagie and Mathias B{\\ae}k Tejs\n  Knudsen and Tomasz Kociumaka", "title": "Maximal Unbordered Factors of Random Strings", "comments": "A preliminary version with weaker results was presented at the 23rd\n  Symposium on String Processing and Information Retrieval (SPIRE '16)", "journal-ref": null, "doi": "10.1007/978-3-319-46049-9_9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A border of a string is a non-empty prefix of the string that is also a\nsuffix of the string, and a string is unbordered if it has no border other than\nitself. Loptev, Kucherov, and Starikovskaya [CPM 2015] conjectured the\nfollowing: If we pick a string of length $n$ from a fixed non-unary alphabet\nuniformly at random, then the expected maximum length of its unbordered factors\nis $n - O(1)$. We confirm this conjecture by proving that the expected value\nis, in fact, ${n - \\Theta(\\sigma^{-1})}$, where $\\sigma$ is the size of the\nalphabet. This immediately implies that we can find such a maximal unbordered\nfactor in linear time on average. However, we go further and show that the\noptimum average-case running time is in $\\Omega (\\sqrt{n}) \\cap O (\\sqrt{n\n\\log_\\sigma n})$ due to analogous bounds by Czumaj and G\\k{a}sieniec [CPM 2000]\nfor the problem of computing the shortest period of a uniformly random string.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 16:34:08 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 12:13:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Cording", "Patrick Hagge", ""], ["Gagie", "Travis", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Kociumaka", "Tomasz", ""]]}, {"id": "1704.04473", "submitter": "Mathias B{\\ae}k Tejs Knudsen", "authors": "Mathias B{\\ae}k Tejs Knudsen", "title": "Additive Spanners and Distance Oracles in Quadratic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be an unweighted, undirected graph. An additive $k$-spanner of $G$ is\na subgraph $H$ that approximates all distances between pairs of nodes up to an\nadditive error of $+k$, that is, it satisfies $d_H(u,v) \\le d_G(u,v)+k$ for all\nnodes $u,v$, where $d$ is the shortest path distance. We give a deterministic\nalgorithm that constructs an additive $O\\!\\left(1\\right)$-spanner with\n$O\\!\\left(n^{4/3}\\right)$ edges in $O\\!\\left(n^2\\right)$ time. This should be\ncompared with the randomized Monte Carlo algorithm by Woodruff [ICALP 2010]\ngiving an additive $6$-spanner with $O\\!\\left(n^{4/3}\\log^3 n\\right)$ edges in\nexpected time $O\\!\\left(n^2\\log^2 n\\right)$.\n  An $(\\alpha,\\beta)$-approximate distance oracle for $G$ is a data structure\nthat supports the following distance queries between pairs of nodes in $G$.\nGiven two nodes $u$, $v$ it can in constant time compute a distance estimate\n$\\tilde{d}$ that satisfies $d \\le \\tilde{d} \\le \\alpha d + \\beta$ where $d$ is\nthe distance between $u$ and $v$ in $G$. Sommer [ICALP 2016] gave a randomized\nMonte Carlo $(2,1)$-distance oracle of size $O\\!\\left(n^{5/3}\\text{poly} \\log\nn\\right)$ in expected time $O\\!\\left(n^2\\text{poly} \\log n\\right)$. As an\napplication of the additive $O(1)$-spanner we improve the construction by\nSommer [ICALP 2016] and give a Las Vegas $(2,1)$-distance oracle of size\n$O\\!\\left(n^{5/3}\\right)$ in time $O\\!\\left(n^2\\right)$. This also implies an\nalgorithm that in $O\\!\\left(n^2\\right)$ gives approximate distance for all\npairs of nodes in $G$ improving on the $O\\!\\left(n^2 \\log n\\right)$ algorithm\nby Baswana and Kavitha [SICOMP 2010].\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 16:36:22 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Knudsen", "Mathias B\u00e6k Tejs", ""]]}, {"id": "1704.04509", "submitter": "Mathias B{\\ae}k Tejs Knudsen", "authors": "Mathias B{\\ae}k Tejs Knudsen and Mikkel Thorup", "title": "The Entropy of Backwards Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backwards analysis, first popularized by Seidel, is often the simplest most\nelegant way of analyzing a randomized algorithm. It applies to incremental\nalgorithms where elements are added incrementally, following some random\npermutation, e.g., incremental Delauney triangulation of a pointset, where\npoints are added one by one, and where we always maintain the Delauney\ntriangulation of the points added thus far. For backwards analysis, we think of\nthe permutation as generated backwards, implying that the $i$th point in the\npermutation is picked uniformly at random from the $i$ points not picked yet in\nthe backwards direction. Backwards analysis has also been applied elegantly by\nChan to the randomized linear time minimum spanning tree algorithm of Karger,\nKlein, and Tarjan.\n  The question considered in this paper is how much randomness we need in order\nto trust the expected bounds obtained using backwards analysis, exactly and\napproximately. For the exact case, it turns out that a random permutation works\nif and only if it is minwise, that is, for any given subset, each element has\nthe same chance of being first. Minwise permutations are known to have\n$\\Theta(n)$ entropy, and this is then also what we need for exact backwards\nanalysis.\n  However, when it comes to approximation, the two concepts diverge\ndramatically. To get backwards analysis to hold within a factor $\\alpha$, the\nrandom permutation needs entropy $\\Omega(n/\\alpha)$. This contrasts with\nminwise permutations, where it is known that a $1+\\varepsilon$ approximation\nonly needs $\\Theta(\\log (n/\\varepsilon))$ entropy. Our negative result for\nbackwards analysis essentially shows that it is as abstract as any analysis\nbased on full randomness.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 18:20:45 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1704.04538", "submitter": "Ali Dasdan", "authors": "Ali Dasdan", "title": "A Simple Randomized Algorithm to Compute Harmonic Numbers and Logarithms", "comments": "5 pages, 2 figures, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a list of N numbers, the maximum can be computed in N iterations.\nDuring these N iterations, the maximum gets updated on average as many times as\nthe Nth harmonic number. We first use this fact to approximate the Nth harmonic\nnumber as a side effect. Further, using the fact the Nth harmonic number is\nequal to the natural logarithm of N plus a constant that goes to zero with N,\nwe approximate the natural logarithm from the harmonic number. To improve\naccuracy, we repeat the computation over many lists of uniformly generated\nrandom numbers. The algorithm is easily extended to approximate logarithms with\ninteger bases or rational arguments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 20:38:32 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 13:01:58 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Dasdan", "Ali", ""]]}, {"id": "1704.04546", "submitter": "Karl Bringmann", "authors": "Amir Abboud and Karl Bringmann and Danny Hermelin and Dvir Shabtay", "title": "SETH-Based Lower Bounds for Subset Sum and Bicriteria Path", "comments": "23 pages, presented at SODA'19 and accepted at TALG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset-Sum and k-SAT are two of the most extensively studied problems in\ncomputer science, and conjectures about their hardness are among the\ncornerstones of fine-grained complexity. One of the most intriguing open\nproblems in this area is to base the hardness of one of these problems on the\nother.\n  Our main result is a tight reduction from k-SAT to Subset-Sum on dense\ninstances, proving that Bellman's 1962 pseudo-polynomial $O^{*}(T)$-time\nalgorithm for Subset-Sum on $n$ numbers and target $T$ cannot be improved to\ntime $T^{1-\\varepsilon}\\cdot 2^{o(n)}$ for any $\\varepsilon>0$, unless the\nStrong Exponential Time Hypothesis (SETH) fails. This is one of the strongest\nknown connections between any two of the core problems of fine-grained\ncomplexity.\n  As a corollary, we prove a \"Direct-OR\" theorem for Subset-Sum under SETH,\noffering a new tool for proving conditional lower bounds: It is now possible to\nassume that deciding whether one out of $N$ given instances of Subset-Sum is a\nYES instance requires time $(N T)^{1-o(1)}$. As an application of this\ncorollary, we prove a tight SETH-based lower bound for the classical Bicriteria\ns,t-Path problem, which is extensively studied in Operations Research. We\nseparate its complexity from that of Subset-Sum: On graphs with $m$ edges and\nedge lengths bounded by $L$, we show that the $O(Lm)$ pseudo-polynomial time\nalgorithm by Joksch from 1966 cannot be improved to $\\tilde{O}(L+m)$, in\ncontrast to a recent improvement for Subset Sum (Bringmann, SODA 2017).\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 21:36:08 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 08:49:04 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 08:18:50 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Abboud", "Amir", ""], ["Bringmann", "Karl", ""], ["Hermelin", "Danny", ""], ["Shabtay", "Dvir", ""]]}, {"id": "1704.04548", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Ahmed El Alaoui, Benjamin Recht", "title": "On the Gap Between Strict-Saddles and True Convexity: An Omega(log d)\n  Lower Bound for Eigenvector Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.CO math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a \\emph{query complexity} lower bound on rank-one principal\ncomponent analysis (PCA). We consider an oracle model where, given a symmetric\nmatrix $M \\in \\mathbb{R}^{d \\times d}$, an algorithm is allowed to make $T$\n\\emph{exact} queries of the form $w^{(i)} = Mv^{(i)}$ for $i \\in\n\\{1,\\dots,T\\}$, where $v^{(i)}$ is drawn from a distribution which depends\narbitrarily on the past queries and measurements $\\{v^{(j)},w^{(j)}\\}_{1 \\le j\n\\le i-1}$. We show that for a small constant $\\epsilon$, any adaptive,\nrandomized algorithm which can find a unit vector $\\widehat{v}$ for which\n$\\widehat{v}^{\\top}M\\widehat{v} \\ge (1-\\epsilon)\\|M\\|$, with even small\nprobability, must make $T = \\Omega(\\log d)$ queries. In addition to settling a\nwidely-held folk conjecture, this bound demonstrates a fundamental gap between\nconvex optimization and \"strict-saddle\" non-convex optimization of which PCA is\na canonical example: in the former, first-order methods can have dimension-free\niteration complexity, whereas in PCA, the iteration complexity of\ngradient-based methods must necessarily grow with the dimension. Our argument\nproceeds via a reduction to estimating the rank-one spike in a deformed Wigner\nmodel. We establish lower bounds for this model by developing a \"truncated\"\nanalogue of the $\\chi^2$ Bayes-risk lower bound of Chen et al.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 21:56:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Simchowitz", "Max", ""], ["Alaoui", "Ahmed El", ""], ["Recht", "Benjamin", ""]]}, {"id": "1704.04555", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle, Tianyi Pan, Victoria G. Crawford, Md Abdul Alim, My T.\n  Thai", "title": "Pseudo-Separation for Assessment of Structural Vulnerability of a\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based upon the idea that network functionality is impaired if two nodes in a\nnetwork are sufficiently separated in terms of a given metric, we introduce two\ncombinatorial \\emph{pseudocut} problems generalizing the classical min-cut and\nmulti-cut problems. We expect the pseudocut problems will find broad relevance\nto the study of network reliability. We comprehensively analyze the\ncomputational complexity of the pseudocut problems and provide three\napproximation algorithms for these problems.\n  Motivated by applications in communication networks with strict\nQuality-of-Service (QoS) requirements, we demonstrate the utility of the\npseudocut problems by proposing a targeted vulnerability assessment for the\nstructure of communication networks using QoS metrics; we perform experimental\nevaluations of our proposed approximation algorithms in this context.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 23:25:33 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kuhnle", "Alan", ""], ["Pan", "Tianyi", ""], ["Crawford", "Victoria G.", ""], ["Alim", "Md Abdul", ""], ["Thai", "My T.", ""]]}, {"id": "1704.04615", "submitter": "Haoyu Cheng", "authors": "Haoyu Cheng, Ming Wu and Yun Xu", "title": "FMtree: A fast locating algorithm of FM-indexes for genomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: As a fundamental task in bioinformatics, searching for massive\nshort patterns over a long text is widely accelerated by various compressed\nfull-text indexes. These indexes are able to provide similar searching\nfunctionalities to classical indexes, e.g., suffix trees and suffix arrays,\nwhile requiring less space. For genomic data, a well-known family of compressed\nfull-text index, called FM-indexes, presents unmatched performance in practice.\nOne major drawback of FM-indexes is that their locating operations, which\nreport all occurrence positions of patterns in a given text, are particularly\nslow, especially for the patterns with many occurrences.\n  Results: In this paper, we introduce a novel locating algorithm, FMtree, to\nfast retrieve all occurrence positions of any pattern via FM-indexes. When\nsearching for a pattern over a given text, FMtree organizes the search space of\nthe locating operation into a conceptual quadtree. As a result, multiple\noccurrence positions of this pattern can be retrieved simultaneously by\ntraversing the quadtree. Compared with the existing locating algorithms, our\ntree-based algorithm reduces large numbers of redundant operations and presents\nbetter data locality. Experimental results show that FMtree is usually one\norder of magnitude faster than the state-of-the-art algorithms, and still\nmemory-efficient.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 09:58:02 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 12:45:50 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Cheng", "Haoyu", ""], ["Wu", "Ming", ""], ["Xu", "Yun", ""]]}, {"id": "1704.04656", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Carlo Mannino, Antonio Sassano", "title": "Negative Cycle Separation in Wireless Network Design", "comments": "This is the authors' final version of the paper published in Pahl J.,\n  Reiners T., Voss S. (eds), Network Optimization - INOC 2011. Lecture Notes in\n  Computer Science, vol 6701, pp. 51-56. Springer, Berlin, Heidelberg, 2011,\n  DOI: 10.1007/978-3-642-21527-8_7. The final publication is available at\n  Springer via http://dx.doi.org/10.1007/978-3-642-21527-8_7", "journal-ref": "Network Optimization - INOC 2011, In: Pahl J., Reiners T., Voss S.\n  (eds). Lecture Notes in Computer Science, vol 6701, pp. 51-56. Springer,\n  Berlin, Heidelberg, 2011", "doi": "10.1007/978-3-642-21527-8_7", "report-no": null, "categories": "math.OC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wireless Network Design Problem (WND) consists in choosing values of\nradio-electrical parameters of transmitters of a wireless network, to maximize\nnetwork coverage. We present a pure 0-1 Linear Programming formulation for the\nWND that may contain an exponential number of constraints. Violated\ninequalities of this formulation are hard to separate both theoretically and in\npractice. However, a relevant subset of such inequalities can be separated more\nefficiently in practice and can be used to strengthen classical MILP\nformulations for the WND. Preliminary computational experience confirms the\neffectiveness of our new technique both in terms of quality of solutions found\nand provided bounds.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 16:26:50 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Mannino", "Carlo", ""], ["Sassano", "Antonio", ""]]}, {"id": "1704.04684", "submitter": "Luis Argerich", "authors": "Luis Argerich, Natalia Golmar", "title": "Generic LSH Families for the Angular Distance Based on\n  Johnson-Lindenstrauss Projections and Feature Hashing LSH", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the creation of generic LSH families for the angular\ndistance based on Johnson-Lindenstrauss projections. We show that feature\nhashing is a valid J-L projection and propose two new LSH families based on\nfeature hashing. These new LSH families are tested on both synthetic and real\ndatasets with very good results and a considerable performance improvement over\nother LSH families. While the theoretical analysis is done for the angular\ndistance, these families can also be used in practice for the euclidean\ndistance with excellent results [2]. Our tests using real datasets show that\nthe proposed LSH functions work well for the euclidean distance.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 19:32:51 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Argerich", "Luis", ""], ["Golmar", "Natalia", ""]]}, {"id": "1704.04794", "submitter": "Hung Nguyen", "authors": "Hung T. Nguyen and Tri P. Nguyen and Tam Vu and Thang N. Dinh", "title": "Outward Influence and Cascade Size Estimation in Billion-scale Networks", "comments": "16 pages, SIGMETRICS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating cascade size and nodes' influence is a fundamental task in social,\ntechnological, and biological networks. Yet this task is extremely challenging\ndue to the sheer size and the structural heterogeneity of networks. We\ninvestigate a new influence measure, termed outward influence (OI), defined as\nthe (expected) number of nodes that a subset of nodes $S$ will activate,\nexcluding the nodes in S. Thus, OI equals, the de facto standard measure,\ninfluence spread of S minus |S|. OI is not only more informative for nodes with\nsmall influence, but also, critical in designing new effective sampling and\nstatistical estimation methods.\n  Based on OI, we propose SIEA/SOIEA, novel methods to estimate influence\nspread/outward influence at scale and with rigorous theoretical guarantees. The\nproposed methods are built on two novel components 1) IICP an important\nsampling method for outward influence, and 2) RSA, a robust mean estimation\nmethod that minimize the number of samples through analyzing variance and range\nof random variables. Compared to the state-of-the art for influence estimation,\nSIEA is $\\Omega(\\log^4 n)$ times faster in theory and up to several orders of\nmagnitude faster in practice. For the first time, influence of nodes in the\nnetworks of billions of edges can be estimated with high accuracy within a few\nminutes. Our comprehensive experiments on real-world networks also give\nevidence against the popular practice of using a fixed number, e.g. 10K or 20K,\nof samples to compute the \"ground truth\" for influence spread.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 16:23:53 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Nguyen", "Hung T.", ""], ["Nguyen", "Tri P.", ""], ["Vu", "Tam", ""], ["Dinh", "Thang N.", ""]]}, {"id": "1704.04830", "submitter": "Matthew Fahrbach", "authors": "David Durfee, Matthew Fahrbach, Yu Gao, Tao Xiao", "title": "Nearly Tight Bounds for Sandpile Transience on the Grid", "comments": "36 pages, 4 figures", "journal-ref": "Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on\n  Discrete Algorithms (SODA 2018) 605-624", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use techniques from the theory of electrical networks to give nearly tight\nbounds for the transience class of the Abelian sandpile model on the\ntwo-dimensional grid up to polylogarithmic factors. The Abelian sandpile model\nis a discrete process on graphs that is intimately related to the phenomenon of\nself-organized criticality. In this process, vertices receive grains of sand,\nand once the number of grains exceeds their degree, they topple by sending\ngrains to their neighbors. The transience class of a model is the maximum\nnumber of grains that can be added to the system before it necessarily reaches\nits steady-state behavior or, equivalently, a recurrent state. Through a more\nrefined and global analysis of electrical potentials and random walks, we give\nan $O(n^4\\log^4{n})$ upper bound and an $\\Omega(n^4)$ lower bound for the\ntransience class of the $n \\times n$ grid. Our methods naturally extend to\n$n^d$-sized $d$-dimensional grids to give $O(n^{3d - 2}\\log^{d+2}{n})$ upper\nbounds and $\\Omega(n^{3d -2})$ lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 22:56:32 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 03:28:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Durfee", "David", ""], ["Fahrbach", "Matthew", ""], ["Gao", "Yu", ""], ["Xiao", "Tao", ""]]}, {"id": "1704.04937", "submitter": "Binanda Sengupta", "authors": "Abhishek Singh and Binanda Sengupta and Sushmita Ruj", "title": "Certificate Transparency with Enhancements and Short Proofs", "comments": "A preliminary version of the paper was published in ACISP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Browsers can detect malicious websites that are provisioned with forged or\nfake TLS/SSL certificates. However, they are not so good at detecting malicious\nwebsites if they are provisioned with mistakenly issued certificates or\ncertificates that have been issued by a compromised certificate authority.\nGoogle proposed certificate transparency which is an open framework to monitor\nand audit certificates in real time. Thereafter, a few other certificate\ntransparency schemes have been proposed which can even handle revocation. All\ncurrently known constructions use Merkle hash trees and have proof size\nlogarithmic in the number of certificates/domain owners.\n  We present a new certificate transparency scheme with short (constant size)\nproofs. Our construction makes use of dynamic bilinear-map accumulators. The\nscheme has many desirable properties like efficient revocation, low\nverification cost and update costs comparable to the existing schemes. We\nprovide proofs of security and evaluate the performance of our scheme.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 11:42:34 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 20:19:43 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Singh", "Abhishek", ""], ["Sengupta", "Binanda", ""], ["Ruj", "Sushmita", ""]]}, {"id": "1704.04947", "submitter": "Rati Gelashvili", "authors": "Dan Alistarh, James Aspnes, Rati Gelashvili", "title": "Space-Optimal Majority in Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a model of distributed computing, in which $n$\nagents with limited local state interact randomly, and cooperate to\ncollectively compute global predicates. An extensive series of papers, across\ndifferent communities, has examined the computability and complexity\ncharacteristics of this model. Majority, or consensus, is a central task, in\nwhich agents need to collectively reach a decision as to which one of two\nstates $A$ or $B$ had a higher initial count. Two complexity metrics are\nimportant: the time that a protocol requires to stabilize to an output\ndecision, and the state space size that each agent requires.\n  It is known that majority requires $\\Omega(\\log \\log n)$ states per agent to\nallow for poly-logarithmic time stabilization, and that $O(\\log^2 n)$ states\nare sufficient. Thus, there is an exponential gap between the upper and lower\nbounds.\n  We address this question. We provide a new lower bound of $\\Omega(\\log n)$\nstates for any protocol which stabilizes in $O( n^{1-c} )$ time, for any $c >\n0$ constant. This result is conditional on basic monotonicity and output\nassumptions, satisfied by all known protocols. Technically, it represents a\nsignificant departure from previous lower bounds. Instead of relying on dense\nconfigurations, we introduce a new surgery technique to construct executions\nwhich contradict the correctness of algorithms that stabilize too fast.\nSubsequently, our lower bound applies to general initial configurations.\n  We give an algorithm for majority which uses $O(\\log n)$ states, and\nstabilizes in $O(\\log^2 n)$ time. Central to the algorithm is a new leaderless\nphase clock, which allows nodes to synchronize in phases of $\\Theta(n \\log{n})$\nconsecutive interactions using $O(\\log n)$ states per node. We also employ our\nphase clock to build a leader election algorithm with $O(\\log n )$ states,\nwhich stabilizes in $O(\\log^2 n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 12:53:02 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 09:12:36 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 03:46:39 GMT"}, {"version": "v4", "created": "Sun, 28 May 2017 20:30:25 GMT"}, {"version": "v5", "created": "Thu, 13 Jul 2017 21:18:42 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Alistarh", "Dan", ""], ["Aspnes", "James", ""], ["Gelashvili", "Rati", ""]]}, {"id": "1704.05232", "submitter": "Ragesh Jaiswal", "authors": "Anup Bhattacharya and Ragesh Jaiswal", "title": "On the k-Means/Median Cost Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the $k$-means cost function. The (Euclidean) $k$-means\nproblem can be described as follows: given a dataset $X \\subseteq \\mathbb{R}^d$\nand a positive integer $k$, find a set of $k$ centers $C \\subseteq\n\\mathbb{R}^d$ such that $\\Phi(C, X) \\stackrel{def}{=} \\sum_{x \\in X} \\min_{c\n\\in C} ||x - c||^2$ is minimized. Let $\\Delta_k(X) \\stackrel{def}{=} \\min_{C\n\\subseteq \\mathbb{R}^d} \\Phi(C, X)$ denote the cost of the optimal $k$-means\nsolution. It is simple to observe that for any dataset $X$, $\\Delta_k(X)$\ndecreases as $k$ increases. We try to understand this behaviour more precisely.\nFor any dataset $X \\subseteq \\mathbb{R}^d$, integer $k \\geq 1$, and a small\nprecision parameter $\\varepsilon > 0$, let $\\mathcal{L}_{X}^{k, \\varepsilon}$\ndenote the smallest integer such that $\\Delta_{\\mathcal{L}_{X}^{k,\n\\varepsilon}}(X) \\leq \\varepsilon \\cdot \\Delta_{k}(X)$. We show upper and lower\nbounds on this quantity. Our techniques generalize for the metric $k$-median\nproblem in arbitrary metrics and we give bounds in terms of the doubling\ndimension of the metric. Finally, we observe that for any dataset $X$, we can\ncompute a set $S$ of size $O \\left(\\mathcal{L}_{X}^{k, \\frac{\\varepsilon}{c}}\n\\right)$ such that $\\Delta_{S}(X) \\leq \\varepsilon \\cdot \\Delta_k(X)$ using the\n$D^2$-sampling algorithm which is also known as the $k$-means++ seeding\nprocedure. In the previous statement, $c$ is some fixed constant. We also\ndiscuss some applications of our bounds.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 08:34:34 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Jaiswal", "Ragesh", ""]]}, {"id": "1704.05233", "submitter": "Tomohiro I", "authors": "Tatsuya Ohno, Yoshimasa Takabatake, Tomohiro I, Hiroshi Sakamoto", "title": "A Faster Implementation of Online Run-Length Burrows-Wheeler Transform", "comments": "In Proc. IWOCA2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Run-length encoding Burrows-Wheeler Transformed strings, resulting in\nRun-Length BWT (RLBWT), is a powerful tool for processing highly repetitive\nstrings. We propose a new algorithm for online RLBWT working in run-compressed\nspace, which runs in $O(n\\lg r)$ time and $O(r\\lg n)$ bits of space, where $n$\nis the length of input string $S$ received so far and $r$ is the number of runs\nin the BWT of the reversed $S$. We improve the state-of-the-art algorithm for\nonline RLBWT in terms of empirical construction time. Adopting the dynamic list\nfor maintaining a total order, we can replace rank queries in a dynamic wavelet\ntree on a run-length compressed string by the direct comparison of labels in a\ndynamic list. The empirical result for various benchmarks show the efficiency\nof our algorithm, especially for highly repetitive strings.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 08:36:44 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 03:11:30 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ohno", "Tatsuya", ""], ["Takabatake", "Yoshimasa", ""], ["I", "Tomohiro", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1704.05254", "submitter": "Fabian Peternek", "authors": "Sebastian Maneth and Fabian Peternek", "title": "Grammar-Based Graph Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new graph compressor that works by recursively detecting\nrepeated substructures and representing them through grammar rules. We show\nthat for a large number of graphs the compressor obtains smaller\nrepresentations than other approaches. Specific queries such as reachability\nbetween two nodes or regular path queries can be evaluated in linear time (or\nquadratic times, respectively), over the grammar, thus allowing speed-ups\nproportional to the compression ratio.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 09:49:45 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Maneth", "Sebastian", ""], ["Peternek", "Fabian", ""]]}, {"id": "1704.05286", "submitter": "Hisao Tamaki", "authors": "Hisao Tamaki", "title": "Positive-instance driven dynamic programming for treewidth", "comments": "A preliminary and abridged version appeared in ESA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a dynamic programming scheme for a decision problem in which all\nsubproblems involved are also decision problems. An implementation of such a\nscheme is {\\em positive-instance driven} (PID), if it generates positive\nsubproblem instances, but not negative ones, building each on smaller positive\ninstances.\n  We take the dynamic programming scheme due to Bouchitt\\'{e} and Todinca for\ntreewidth computation, which is based on minimal separators and potential\nmaximal cliques, and design a variant (for the decision version of the problem)\nwith a natural PID implementation. The resulting algorithm performs extremely\nwell: it solves a number of standard benchmark instances for which the optimal\nsolutions have not previously been known. Incorporating a new heuristic\nalgorithm for detecting safe separators, it also solves all of the 100 public\ninstances posed by the exact treewidth track in PACE 2017, a competition on\nalgorithm implementation.\n  We describe the algorithm, prove its correctness, and give a running time\nbound in terms of the number of positive subproblem instances. We perform an\nexperimental analysis which supports the practical importance of such a bound.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 11:58:56 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 01:10:04 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Tamaki", "Hisao", ""]]}, {"id": "1704.05303", "submitter": "Vinayak Prabhu", "authors": "Rayna Dimitrova, Ivan Gavran, Rupak Majumdar, Vinayak S. Prabhu, and\n  Sadegh Esmaeil Zadeh Soudjani", "title": "The Robot Routing Problem for Collecting Aggregate Stochastic Rewards", "comments": "20 Pages. Full version of the CONCUR (28th International Conference\n  on Concurrency Theory) 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CC cs.DS cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for formalizing reward collection problems on graphs\nwith dynamically generated rewards which may appear and disappear based on a\nstochastic model. The *robot routing problem* is modeled as a graph whose nodes\nare stochastic processes generating potential rewards over discrete time. The\nrewards are generated according to the stochastic process, but at each step, an\nexisting reward disappears with a given probability. The edges in the graph\nencode the (unit-distance) paths between the rewards' locations. On visiting a\nnode, the robot collects the accumulated reward at the node at that time, but\ntraveling between the nodes takes time. The optimization question asks to\ncompute an optimal (or epsilon-optimal) path that maximizes the expected\ncollected rewards.\n  We consider the finite and infinite-horizon robot routing problems. For\nfinite-horizon, the goal is to maximize the total expected reward, while for\ninfinite horizon we consider limit-average objectives. We study the\ncomputational and strategy complexity of these problems, establish NP-lower\nbounds and show that optimal strategies require memory in general. We also\nprovide an algorithm for computing epsilon-optimal infinite paths for arbitrary\nepsilon > 0.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 12:32:19 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 10:44:42 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Dimitrova", "Rayna", ""], ["Gavran", "Ivan", ""], ["Majumdar", "Rupak", ""], ["Prabhu", "Vinayak S.", ""], ["Soudjani", "Sadegh Esmaeil Zadeh", ""]]}, {"id": "1704.05384", "submitter": "Morteza Zadimoghaddam", "authors": "Matthew Fahrbach, Morteza Zadimoghaddam", "title": "Online Weighted Matching: Breaking the $\\frac{1}{2}$ Barrier", "comments": "28 pages, 1 figure. This is substantially revised version that\n  simplifies the presentation and fixes some minor problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online matching and its variants are some of the most fundamental problems in\nthe online algorithms literature. In this paper, we study the online weighted\nbipartite matching problem. Karp et al. (STOC 1990) gave an elegant algorithm\nin the unweighted case that achieves a tight competitive ratio of $1-1/e$. In\nthe weighted case, however, we can easily show that no competitive ratio is\nobtainable without the commonly accepted free disposal assumption. Under this\nassumption, it is not hard to prove that the greedy algorithm is $1/2$\ncompetitive, and that this is tight for deterministic algorithms. We present\nthe first randomized algorithm that breaks this long-standing $1/2$ barrier and\nachieves a competitive ratio of at least $0.501$. In light of the hardness\nresult of Kapralov et al. (SODA 2013) that restricts beating a $1/2$\ncompetitive ratio for the monotone submodular welfare maximization problem, our\nresult can be seen as strong evidence that solving the weighted bipartite\nmatching problem is strictly easier than submodular welfare maximization in the\nonline setting. Our approach relies on a very controlled use of randomness,\nwhich allows our algorithm to safely make adaptive decisions based on its\nprevious assignments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 15:01:09 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 15:34:32 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1704.05430", "submitter": "Soheil Ehsani", "authors": "Sina Dahghani, Soheil Ehsani, MohammadTaghi Hajiaghayi, Vahid Liaghat,\n  Harald Racke", "title": "Online Degree-Bounded Steiner Network Design", "comments": null, "journal-ref": "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on\n  Discrete Algorithms (pp. 164-175)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of degree-bounded network design problems in the online\nsetting. The degree-bounded Steiner tree problem { which asks for a subgraph\nwith minimum degree that connects a given set of vertices { is perhaps one of\nthe most representative problems in this class. This paper deals with its\nwell-studied generalization called the degree-bounded Steiner forest problem\nwhere the connectivity demands are represented by vertex pairs that need to be\nindividually connected. In the classical online model, the input graph is given\nonline but the demand pairs arrive sequentially in online steps. The selected\nsubgraph starts off as the empty subgraph, but has to be augmented to satisfy\nthe new connectivity constraint in each online step. The goal is to be\ncompetitive against an adversary that knows the input in advance. We design a\nsimple greedy-like algorithm that achieves a competitive ratio of O(log n)\nwhere n is the number of vertices. We show that no (randomized) algorithm can\nachieve a (multiplicative) competitive ratio o(log n); thus our result is\nasymptotically tight. We further show strong hardness results for the group\nSteiner tree and the edge-weighted variants of degree-bounded connectivity\nproblems. Fourer and Raghavachari resolved the online variant of degree-bounded\nSteiner forest in their paper in SODA'92. Since then, the natural family of\ndegree-bounded network design problems has been extensively studied in the\nliterature resulting in the development of many interesting tools and numerous\npapers on the topic. We hope that our approach in this paper, paves the way for\nsolving the online variants of the classical problems in this family of network\ndesign problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 17:13:20 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Dahghani", "Sina", ""], ["Ehsani", "Soheil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Liaghat", "Vahid", ""], ["Racke", "Harald", ""]]}, {"id": "1704.05576", "submitter": "Hamed Sadeghi", "authors": "Hamed Sadeghi, MohammadReza Soroushmehr, Shahrokh Valaee, Shahram\n  Shirani and Shadrokh Samavi", "title": "1D Modeling of Sensor Selection Problem for Weak Barrier Coverage and\n  Gap Mending in Wireless Sensor Networks", "comments": "10 Pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first remodel the line coverage as a 1D discrete problem\nwith co-linear targets. Then, an order-based greedy algorithm, called OGA, is\nproposed to solve the problem optimally. It will be shown that the existing\norder in the 1D modeling, and especially the resulted Markov property of the\nselected sensors can help design greedy algorithms such as OGA. These\nalgorithms demonstrate optimal/efficient performance and have lower complexity\ncompared to the state-of-the-art. Furthermore, it is demonstrated that the\nconventional continuous line coverage problem can be converted to an equivalent\ndiscrete problem and solved optimally by OGA. Next, we formulate the well-known\nweak barrier coverage problem as an instance of the continuous line coverage\nproblem (i.e. a 1D problem) as opposed to the conventional 2D graph-based\nmodels. We demonstrate that the equivalent discrete version of this problem can\nbe solved optimally and faster than the state-of-the-art methods using an\nextended version of OGA, called K-OGA. Moreover, an efficient local algorithm,\ncalled LOGM, is proposed to mend barrier gaps due to sensor failure. In the\ncase of m gaps, LOGM is proved to select at most 2m-1 sensors more than the\noptimal while being local and implementable in distributed fashion. We\ndemonstrate the optimal/efficient performance of the proposed algorithms via\nextensive simulations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 01:28:28 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Sadeghi", "Hamed", ""], ["Soroushmehr", "MohammadReza", ""], ["Valaee", "Shahrokh", ""], ["Shirani", "Shahram", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1704.05660", "submitter": "Freeson Kaniwa", "authors": "Freeson Kaniwa, Venu Madhav Kuthadi, Otlhapile Dinakenyane and Heiko\n  Schroeder", "title": "Alphabet-dependent Parallel Algorithm for Suffix Tree Construction for\n  Pattern Searching", "comments": null, "journal-ref": "International Journal of Grid and Distributed Computing, Vol. 10,\n  No. 1 (2017), pp.9-20", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix trees have recently become very successful data structures in handling\nlarge data sequences such as DNA or Protein sequences. Consequently parallel\narchitectures have become ubiquitous. We present a novel alphabet-dependent\nparallel algorithm which attempts to take advantage of the perverseness of the\nmulticore architecture. Microsatellites are important for their biological\nrelevance hence our algorithm is based on time efficient construction for\nidentification of such. We experimentally achieved up to 15x speedup over the\nsequential algorithm on different input sizes of biological sequences.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 09:08:55 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Kaniwa", "Freeson", ""], ["Kuthadi", "Venu Madhav", ""], ["Dinakenyane", "Otlhapile", ""], ["Schroeder", "Heiko", ""]]}, {"id": "1704.05682", "submitter": "Rajeev Raman", "authors": "Andreas Poyias, Simon J. Puglisi, Rajeev Raman", "title": "m-Bonsai: a Practical Compact Dynamic Trie", "comments": "Journal version of SPIRE 2015 paper by Poyias and Raman", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of implementing a space-efficient dynamic trie, with\nan emphasis on good practical performance. For a trie with $n$ nodes with an\nalphabet of size $\\sigma$, the information-theoretic lower bound is $n \\log\n\\sigma + O(n)$ bits. The Bonsai data structure is a compact trie proposed by\nDarragh et al. (Softw., Pract. Exper. 23(3), 1993, p. 277-291). Its\ndisadvantages include the user having to specify an upper bound $M$ on the trie\nsize in advance (which cannot be changed easily after initalization), a space\nusage of $M \\log \\sigma + O(M \\log \\log M)$ (which is asymptotically\nnon-optimal for smaller $\\sigma$ or if $n \\ll M$) and a lack of support for\ndeletions. It supports traversal and update operations in $O(1/\\epsilon)$\nexpected time (based on assumptions about the behaviour of hash functions),\nwhere $\\epsilon = (M-n)/M$ and has excellent speed performance in practice. We\npropose an alternative, m-Bonsai, that addresses the above problems, obtaining\na trie that uses $(1+\\beta) n (\\log \\sigma + O(1))$ bits in expectation, and\nsupports traversal and update operations in $O(1/\\beta)$ expected time and\n$O(1/\\beta^2)$ amortized expected time, for any user-specified parameter $\\beta\n> 0$ (again based on assumptions about the behaviour of hash functions). We\ngive an implementation of m-Bonsai which uses considerably less memory and is\nslightly faster than the original Bonsai.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 10:45:05 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Poyias", "Andreas", ""], ["Puglisi", "Simon J.", ""], ["Raman", "Rajeev", ""]]}, {"id": "1704.05795", "submitter": "Torsten Gross", "authors": "Torsten Gross and Nils Bl\\\"uthgen", "title": "Sorting sums of binary decision summands", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sum where each of the $N$ summands can be independently chosen from two\nchoices yields $2^N$ possible summation outcomes. There is an\n$\\mathcal{O}(K^2)$-algorithm that finds the $K$ smallest/largest of these sums\nby evading the enumeration of all sums.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 16:08:15 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Gross", "Torsten", ""], ["Bl\u00fcthgen", "Nils", ""]]}, {"id": "1704.05811", "submitter": "Soheil Ehsani", "authors": "Sina Dehghani, Soheil Ehsani, MohammadTaghi Hajiaghayi, Vahid Liaghat,\n  Harald Racke, Saeed Seddighin", "title": "Online Weighted Degree-Bounded Steiner Networks via Novel Online Mixed\n  Packing/Covering", "comments": null, "journal-ref": "Dehghani, Sina, Soheil Ehsani, Mohammad Taghi Hajiaghayi, Vahid\n  Liaghat, Harald Racke, and Saeed Seddighin. \"Online Weighted Degree-Bounded\n  Steiner Networks via Novel Online Mixed Packing/Covering.\" ICALP 2016", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design the first online algorithm with poly-logarithmic competitive ratio\nfor the edge-weighted degree-bounded Steiner forest(EW-DB-SF) problem and its\ngeneralized variant. We obtain our result by demonstrating a new generic\napproach for solving mixed packing/covering integer programs in the online\nparadigm. In EW-DB-SF we are given an edge-weighted graph with a degree bound\nfor every vertex. Given a root vertex in advance we receive a sequence of\nterminal vertices in an online manner. Upon the arrival of a terminal we need\nto augment our solution subgraph to connect the new terminal to the root. The\ngoal is to minimize the total weight of the solution while respecting the\ndegree bounds on the vertices. In the offline setting edge-weighted\ndegree-bounded Steiner tree (EW-DB-ST) and its many variations have been\nextensively studied since early eighties. Unfortunately the recent advancements\nin the online network design problems are inherently difficult to adapt for\ndegree-bounded problems. In contrast in this paper we obtain our result by\nusing structural properties of the optimal solution, and reducing the EW-DB-SF\nproblem to an exponential-size mixed packing/covering integer program in which\nevery variable appears only once in covering constraints. We then design a\ngeneric integral algorithm for solving this restricted family of IPs. We\ndemonstrate a new technique for solving mixed packing/covering integer\nprograms. Define the covering frequency k of a program as the maximum number of\ncovering constraints in which a variable can participate. Let m denote the\nnumber of packing constraints. We design an online deterministic integral\nalgorithm with competitive ratio of O(k log m) for the mixed packing/covering\ninteger programs. We believe this technique can be used as an interesting\nalternative for the standard primal-dual techniques in solving online problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 16:39:25 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Dehghani", "Sina", ""], ["Ehsani", "Soheil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Liaghat", "Vahid", ""], ["Racke", "Harald", ""], ["Seddighin", "Saeed", ""]]}, {"id": "1704.05836", "submitter": "Soheil Ehsani", "authors": "Melika Abolhasani, Soheil Ehsani, Hosein Esfandiari, MohammadTaghi\n  Hajiaghayi, Robert Kleinberg, Brendan Lucier", "title": "Beating 1-1/e for Ordered Prophets", "comments": null, "journal-ref": null, "doi": "10.1145/3055399.3055479", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hill and Kertz studied the prophet inequality on iid distributions [The\nAnnals of Probability 1982]. They proved a theoretical bound of $1-\\frac{1}{e}$\non the approximation factor of their algorithm. They conjectured that the best\napproximation factor for arbitrarily large n is $\\frac{1}{1+1/e} \\approx\n0.731$. This conjecture remained open prior to this paper for over 30 years. In\nthis paper we present a threshold-based algorithm for the prophet inequality\nwith n iid distributions. Using a nontrivial and novel approach we show that\nour algorithm is a 0.738-approximation algorithm. By beating the bound of\n$\\frac{1}{1+1/e}$, this refutes the conjecture of Hill and Kertz. Moreover, we\ngeneralize our results to non-iid distributions and discuss its applications in\nmechanism design.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 17:44:35 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 23:56:00 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 20:06:19 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Abolhasani", "Melika", ""], ["Ehsani", "Soheil", ""], ["Esfandiari", "Hosein", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Kleinberg", "Robert", ""], ["Lucier", "Brendan", ""]]}, {"id": "1704.05902", "submitter": "Piotr Wygocki", "authors": "Piotr Wygocki", "title": "On fast bounded locality sensitive hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the hash functions expressed as scalar products,\ni.e., $f(x)=<v,x>$, for some bounded random vector $v$. Such hash functions\nhave numerous applications, but often there is a need to optimize the choice of\nthe distribution of $v$. In the present work, we focus on so-called\nanti-concentration bounds, i.e. the upper bounds of $\\mathbb{P}\\left[|<v,x>| <\n\\alpha \\right]$. In many applications, $v$ is a vector of independent random\nvariables with standard normal distribution. In such case, the distribution of\n$<v,x>$ is also normal and it is easy to approximate $\\mathbb{P}\\left[|<v,x>| <\n\\alpha \\right]$. Here, we consider two bounded distributions in the context of\nthe anti-concentration bounds. Particularly, we analyze $v$ being a random\nvector from the unit ball in $l_{\\infty}$ and $v$ being a random vector from\nthe unit sphere in $l_{2}$. We show optimal up to a constant anti-concentration\nmeasures for functions $f(x)=<v,x>$.\n  As a consequence of our research, we obtain new best results for \\newline\n\\textit{$c$-approximate nearest neighbors without false negatives} for $l_p$ in\nhigh dimensional space for all $p\\in[1,\\infty]$, for\n$c=\\Omega(\\max\\{\\sqrt{d},d^{1/p}\\})$. These results improve over those\npresented in [16]. Finally, our paper reports progress on answering the open\nproblem by Pagh~[17], who considered the nearest neighbor search without false\nnegatives for the Hamming distance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 19:14:45 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Wygocki", "Piotr", ""]]}, {"id": "1704.05964", "submitter": "Alfred Rossi", "authors": "Tamal K. Dey, Alfred Rossi, Anastasios Sidiropoulos", "title": "Temporal Clustering", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of clustering sequences of unlabeled point sets taken\nfrom a common metric space. Such scenarios arise naturally in applications\nwhere a system or process is observed in distinct time intervals, such as\nbiological surveys and contagious disease surveillance. In this more general\nsetting existing algorithms for classical (i.e.~static) clustering problems are\nnot applicable anymore.\n  We propose a set of optimization problems which we collectively refer to as\n'temporal clustering'. The quality of a solution to a temporal clustering\ninstance can be quantified using three parameters: the number of clusters $k$,\nthe spatial clustering cost $r$, and the maximum cluster displacement $\\delta$\nbetween consecutive time steps. We consider spatial clustering costs which\ngeneralize the well-studied $k$-center, discrete $k$-median, and discrete\n$k$-means objectives of classical clustering problems. We develop new\nalgorithms that achieve trade-offs between the three objectives $k$, $r$, and\n$\\delta$. Our upper bounds are complemented by inapproximability results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 00:17:43 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Dey", "Tamal K.", ""], ["Rossi", "Alfred", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1704.06149", "submitter": "Patrick Nicholson", "authors": "Pawel Gawrychowski and Patrick K. Nicholson", "title": "Optimal Query Time for Encoding Range Majority", "comments": "To appear in WADS 2017 (modulo the appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the range $\\tau$-majority problem, which asks us to preprocess an\narray $A[1..n]$ for a fixed value of $\\tau \\in (0,1/2]$, such that for any\nquery range $[i,j]$ we can return a position in $A$ of each distinct\n$\\tau$-majority element. A $\\tau$-majority element is one that has relative\nfrequency at least $\\tau$ in the range $[i,j]$: i.e., frequency at least $\\tau\n(j-i+1)$. Belazzougui et al. [WADS 2013] presented a data structure that can\nanswer such queries in $O(1/\\tau)$ time, which is optimal, but the space can be\nas much as $\\Theta(n \\lg n)$ bits. Recently, Navarro and Thankachan\n[Algorithmica 2016] showed that this problem could be solved using an $O(n \\lg\n(1/\\tau))$ bit encoding, which is optimal in terms of space, but has suboptimal\nquery time. In this paper, we close this gap and present a data structure that\noccupies $O(n \\lg (1/\\tau))$ bits of space, and has $O(1/\\tau)$ query time. We\nalso show that this space bound is optimal, even for the much weaker query in\nwhich we must decide whether the query range contains at least one\n$\\tau$-majority element.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 14:03:31 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1704.06185", "submitter": "Huacheng Yu", "authors": "Josh Alman, Joshua R. Wang, Huacheng Yu", "title": "Cell-Probe Lower Bounds from Online Communication Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce an online model for communication complexity.\nAnalogous to how online algorithms receive their input piece-by-piece, our\nmodel presents one of the players, Bob, his input piece-by-piece, and has the\nplayers Alice and Bob cooperate to compute a result each time before the next\npiece is revealed to Bob. This model has a closer and more natural\ncorrespondence to dynamic data structures than classic communication models do,\nand hence presents a new perspective on data structures.\n  We first present a tight lower bound for the online set intersection problem\nin the online communication model, demonstrating a general approach for proving\nonline communication lower bounds. The online communication model prevents a\nbatching trick that classic communication complexity allows, and yields a\nstronger lower bound. We then apply the online communication model to prove\ndata structure lower bounds for two dynamic data structure problems: the Group\nRange problem and the Dynamic Connectivity problem for forests. Both of the\nproblems admit a worst case $O(\\log n)$-time data structure. Using online\ncommunication complexity, we prove a tight cell-probe lower bound for each:\nspending $o(\\log n)$ (even amortized) time per operation results in at best an\n$\\exp(-\\delta^2 n)$ probability of correctly answering a\n$(1/2+\\delta)$-fraction of the $n$ queries.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:27:26 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 18:05:06 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Alman", "Josh", ""], ["Wang", "Joshua R.", ""], ["Yu", "Huacheng", ""]]}, {"id": "1704.06297", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Seth Pettie", "title": "A Time Hierarchy Theorem for the LOCAL Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Time Hierarchy Theorem for Turing machines states, informally,\nthat more problems can be solved given more time. The extent to which a time\nhierarchy-type theorem holds in the distributed LOCAL model has been open for\nmany years. It is consistent with previous results that all natural problems in\nthe LOCAL model can be classified according to a small constant number of\ncomplexities, such as $O(1),O(\\log^* n), O(\\log n), 2^{O(\\sqrt{\\log n})}$, etc.\n  In this paper we establish the first time hierarchy theorem for the LOCAL\nmodel and prove that several gaps exist in the LOCAL time hierarchy.\n  1. We define an infinite set of simple coloring problems called Hierarchical\n$2\\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply\nchecking the neighborhood of each vertex, so this problem fits into the class\nof locally checkable labeling (LCL) problems. However, the complexity of the\n$k$-level Hierarchical $2\\frac{1}{2}$-Coloring problem is $\\Theta(n^{1/k})$,\nfor $k\\in\\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs\nand trees, and for both randomized and deterministic algorithms.\n  2. Consider any LCL problem on bounded degree trees. We prove an\nautomatic-speedup theorem that states that any randomized $n^{o(1)}$-time\nalgorithm solving the LCL can be transformed into a deterministic $O(\\log\nn)$-time algorithm. Together with a previous result, this establishes that on\ntrees, there are no natural deterministic complexities in the ranges\n$\\omega(\\log^* n)$---$o(\\log n)$ or $\\omega(\\log n)$---$n^{o(1)}$.\n  3. We expose a gap in the randomized time hierarchy on general graphs. Any\nrandomized algorithm that solves an LCL problem in sublogarithmic time can be\nsped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed\nLovasz local lemma problem, currently known to be $\\Omega(\\log\\log n)$ and\n$O(\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 18:49:12 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Pettie", "Seth", ""]]}, {"id": "1704.06361", "submitter": "Dariusz Dereniowski", "authors": "Dariusz Dereniowski and Wieslaw Kubiak", "title": "Shared processor scheduling", "comments": null, "journal-ref": "Journal of Scheduling 21(6): 583-593 (2018)", "doi": "10.1007/s10951-018-0566-0", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the shared processor scheduling problem with a single shared\nprocessor where a unit time saving (weight) obtained by processing a job on the\nshared processor depends on the job. A polynomial-time optimization algorithm\nhas been given for the problem with equal weights in the literature. This paper\nextends that result by showing an $O(n \\log n)$ optimization algorithm for a\nclass of instances in which non-decreasing order of jobs with respect to\nprocessing times provides a non-increasing order with respect to weights ---\nthis instance generalizes the unweighted case of the problem. This algorithm\nalso leads to a $\\frac{1}{2}$-approximation algorithm for the general weighted\nproblem. The complexity of the weighted problem remains open.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 23:10:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["Kubiak", "Wieslaw", ""]]}, {"id": "1704.06493", "submitter": "Jingcheng Liu", "authors": "Jingcheng Liu, Alistair Sinclair, Piyush Srivastava", "title": "The Ising Partition Function: Zeros and Deterministic Approximation", "comments": "clarified presentation of combinatorial arguments, added new results\n  on optimality of univariate Lee-Yang theorems", "journal-ref": null, "doi": "10.1007/s10955-018-2199-2", "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating the partition function of the\nferromagnetic Ising model in graphs and hypergraphs. Our first result is a\ndeterministic approximation scheme (an FPTAS) for the partition function in\nbounded degree graphs that is valid over the entire range of parameters $\\beta$\n(the interaction) and $\\lambda$ (the external field), except for the case\n$\\vert{\\lambda}\\vert=1$ (the \"zero-field\" case). A randomized algorithm (FPRAS)\nfor all graphs, and all $\\beta,\\lambda$, has long been known. Unlike most other\ndeterministic approximation algorithms for problems in statistical physics and\ncounting, our algorithm does not rely on the \"decay of correlations\" property.\nRather, we exploit and extend machinery developed recently by Barvinok, and\nPatel and Regts, based on the location of the complex zeros of the partition\nfunction, which can be seen as an algorithmic realization of the classical\nLee-Yang approach to phase transitions. Our approach extends to the more\ngeneral setting of the Ising model on hypergraphs of bounded degree and edge\nsize, where no previous algorithms (even randomized) were known for a wide\nrange of parameters. In order to achieve this extension, we establish a tight\nversion of the Lee-Yang theorem for the Ising model on hypergraphs, improving a\nclassical result of Suzuki and Fisher.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:46:22 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 18:31:10 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Liu", "Jingcheng", ""], ["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""]]}, {"id": "1704.06528", "submitter": "Khoa Trinh", "authors": "David G. Harris, Thomas Pensyl, Aravind Srinivasan, Khoa Trinh", "title": "Fairness in Resource Allocation and Slowed-down Dependent Rounding", "comments": "We decided to split these results to two separate papers. Please see\n  arXiv:1709.06995", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an issue of much current concern: could fairness, an issue that\nis already difficult to guarantee, worsen when algorithms run much of our\nlives? We consider this in the context of resource-allocation problems, we show\nthat algorithms can guarantee certain types of fairness in a verifiable way.\nOur conceptual contribution is a simple approach to fairness in this context,\nwhich only requires that all users trust some public lottery. Our technical\ncontributions are in ways to address the $k$-center and knapsack-center\nproblems that arise in this context: we develop a novel dependent-rounding\ntechnique that, via the new ingredients of \"slowing down\" and additional\nrandomization, guarantees stronger correlation properties than known before.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 13:21:25 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 23:08:49 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Harris", "David G.", ""], ["Pensyl", "Thomas", ""], ["Srinivasan", "Aravind", ""], ["Trinh", "Khoa", ""]]}, {"id": "1704.06622", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, M. S. Ramanujan, Felix Reidl, and Magnus Wahlstr\\\"om", "title": "Path-contractions, edge deletions and connectivity preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study several problems related to graph modification problems under\nconnectivity constraints from the perspective of parameterized complexity: {\\sc\n(Weighted) Biconnectivity Deletion}, where we are tasked with deleting~$k$\nedges while preserving biconnectivity in an undirected graph, {\\sc\nVertex-deletion Preserving Strong Connectivity}, where we want to maintain\nstrong connectivity of a digraph while deleting exactly~$k$ vertices, and {\\sc\nPath-contraction Preserving Strong Connectivity}, in which the operation of\npath contraction on arcs is used instead. The parameterized tractability of\nthis last problem was posed by Bang-Jensen and Yeo [DAM 2008] as an open\nquestion and we answer it here in the negative: both variants of preserving\nstrong connectivity are $\\sf W[1]$-hard. Preserving biconnectivity, on the\nother hand, turns out to be fixed parameter tractable and we provide a\n$2^{O(k\\log k)} n^{O(1)}$-algorithm that solves {\\sc Weighted Biconnectivity\nDeletion}. Further, we show that the unweighted case even admits a randomized\npolynomial kernel. All our results provide further interesting data points for\nthe systematic study of connectivity-preservation constraints in the\nparameterized setting.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:34:42 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Gutin", "Gregory", ""], ["Ramanujan", "M. S.", ""], ["Reidl", "Felix", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1704.06673", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni", "title": "Revisiting wireless network jamming by SIR-based considerations and\n  Multiband Robust Optimization", "comments": "This is the author's final version of the paper published in\n  Optimization Letters 9(8), 1495-1510, 2015, DOI: 10.1007/s11590-014-0839-2r.\n  The final publication is available at Springer\n  http://dx.doi.org/10.1007/s11590-014-0839-2", "journal-ref": "Optimization Letters 9(8) (2015) 1495-1510", "doi": "10.1007/s11590-014-0839-2r", "report-no": null, "categories": "math.OC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the mathematical models for wireless network jamming introduced by\nCommander et al.: we first point out the strong connections with classical\nwireless network design and then we propose a new model based on the explicit\nuse of signal-to-interference quantities. Moreover, to address the intrinsic\nuncertain nature of the jamming problem and tackle the peculiar right-hand-side\n(RHS) uncertainty of the problem, we propose an original robust cutting-plane\nalgorithm drawing inspiration from Multiband Robust Optimization. Finally, we\nassess the performance of the proposed cutting plane algorithm by experiments\non realistic network instances.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 18:30:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""]]}, {"id": "1704.06674", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Carlo Mannino, Antonio Sassano", "title": "GUB Covers and Power-Indexed formulations for Wireless Network Design", "comments": "This is the authors' final version of the paper published in\n  Management Science 59(1), 142-156, 2013. DOI: 10.1287/mnsc.1120.1571. The\n  final publication is available at INFORMS via\n  http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1120.1571", "journal-ref": "Management Science 59(1) (2013) 142-156", "doi": "10.1287/mnsc.1120.1571", "report-no": null, "categories": "math.OC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pure 0-1 formulation for the wireless network design problem,\ni.e. the problem of configuring a set of transmitters to provide service\ncoverage to a set of receivers. In contrast with classical mixed integer\nformulations, where power emissions are represented by continuous variables, we\nconsider only a finite set of powers values. This has two major advantages: it\nbetter fits the usual practice and eliminates the sources of numerical problems\nwhich heavily affect continuous models. A crucial ingredient of our approach is\nan effective basic formulation for the single knapsack problem representing the\ncoverage condition of a receiver. This formulation is based on the GUB cover\ninequalities introduced by Wolsey (1990) and its core is an extension of the\nexact formulation of the GUB knapsack polytope with two GUB constraints. This\nspecial case corresponds to the very common practical situation where only one\nmajor interferer is present. We assess the effectiveness of our formulation by\ncomprehensive computational results over realistic instances of two typical\ntechnologies, namely WiMAX and DVB-T.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 18:36:56 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Mannino", "Carlo", ""], ["Sassano", "Antonio", ""]]}, {"id": "1704.06677", "submitter": "Kevin Sun", "authors": "Samir Khuller, Jingling Li, Pascal Sturmfels, Kevin Sun, Prayaag\n  Venkat", "title": "Select and Permute: An Improved Online Framework for Scheduling to\n  Minimize Weighted Completion Time", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new online scheduling framework for minimizing\ntotal weighted completion time in a general setting. The framework is inspired\nby the work of Hall et al. [Mathematics of Operations Research, Vol\n22(3):513-544, 1997] and Garg et al. [Proc. of Foundations of Software\nTechnology and Theoretical Computer Science, pp. 96-107, 2007], who show how to\nconvert an offline approximation to an online scheme. Our framework uses two\noffline approximation algorithms (one for the simpler problem of scheduling\nwithout release times, and another for the minimum unscheduled weight problem)\nto create an online algorithm with provably good competitive ratios.\n  We illustrate multiple applications of this method that yield improved\ncompetitive ratios. Our framework gives algorithms with the best or only known\ncompetitive ratios for the concurrent open shop, coflow, and concurrent cluster\nmodels. We also introduce a randomized variant of our framework based on the\nideas of Chakrabarti et al. [Proc. of International Colloquium on Automata,\nLanguages, and Programming, pp. 646-657, 1996] and use it to achieve improved\ncompetitive ratios for these same problems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 18:42:43 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Khuller", "Samir", ""], ["Li", "Jingling", ""], ["Sturmfels", "Pascal", ""], ["Sun", "Kevin", ""], ["Venkat", "Prayaag", ""]]}, {"id": "1704.06683", "submitter": "Sergey Dovgal", "authors": "Sergey Dovgal and Vlady Ravelomanana", "title": "Shifting the Phase Transition Threshold for Random Graphs and 2-SAT\n  using Degree Constraints", "comments": "19 pages, coloured figures. Black-and-white printing is possible\n  without essential lost of information in most pictures. Accepted to LATIN\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS cs.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that by restricting the degrees of the vertices of a graph to an\narbitrary set \\( \\Delta \\), the threshold point $ \\alpha(\\Delta) $ of the phase\ntransition for a random graph with $ n $ vertices and $ m = \\alpha(\\Delta) n $\nedges can be either accelerated (e.g., $ \\alpha(\\Delta) \\approx 0.381 $ for $\n\\Delta = \\{0,1,4,5\\} $) or postponed (e.g., $ \\alpha(\\{ 2^0, 2^1, \\cdots, 2^k,\n\\cdots \\}) \\approx 0.795 $) compared to a classical Erd\\H{o}s--R\\'{e}nyi random\ngraph with $ \\alpha(\\mathbb Z_{\\geq 0}) = \\tfrac12 $. In particular, we prove\nthat the probability of graph being nonplanar and the probability of having a\ncomplex component, goes from $ 0 $ to $ 1 $ as $ m $ passes $ \\alpha(\\Delta) n\n$. We investigate these probabilities and also different graph statistics\ninside the critical window of transition (diameter, longest path and\ncircumference of a complex component).\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:08:22 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 12:05:44 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 17:40:12 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Dovgal", "Sergey", ""], ["Ravelomanana", "Vlady", ""]]}, {"id": "1704.06684", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni", "title": "A hybrid exact-ACO algorithm for the joint scheduling, power and cluster\n  assignment in cooperative wireless networks", "comments": "This is the author's final version of the paper published in G. Di\n  Caro, G. Theraulaz (eds.), BIONETICS 2012: Bio-Inspired Models of Network,\n  Information, and Computing Systems. LNICST, vol. 134, pp. 3-17. Springer,\n  Heidelberg, 2014, DOI: 10.1007/978-3-319-06944-9_1 ). The final publication\n  is available at Springer via http://dx.doi.org/10.1007/978-3-319-06944-9_1", "journal-ref": "G. Di Caro, G. Theraulaz (eds.), BIONETICS 2012: Bio-Inspired\n  Models of Network, Information, and Computing Systems. LNICST, vol. 134, pp.\n  3-17, Springer, Heidelberg, 2014, DOI: 10.1007/978-3-319-06944-9_1", "doi": "10.1007/978-3-319-06944-9_1", "report-no": null, "categories": "math.OC cs.DS cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Base station cooperation (BSC) has recently arisen as a promising way to\nincrease the capacity of a wireless network. Implementing BSC adds a new design\ndimension to the classical wireless network design problem: how to define the\nsubset of base stations (clusters) that coordinate to serve a user. Though the\nproblem of forming clusters has been extensively discussed from a technical\npoint of view, there is still a lack of effective optimization models for its\nrepresentation and algorithms for its solution. In this work, we make a further\nstep towards filling such gap: 1) we generalize the classical network design\nproblem by adding cooperation as an additional decision dimension; 2) we\ndevelop a strong formulation for the resulting problem; 3) we define a new\nhybrid solution algorithm that combines exact large neighborhood search and ant\ncolony optimization. Finally, we assess the performance of our new model and\nalgorithm on a set of realistic instances of a WiMAX network.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:14:05 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""]]}, {"id": "1704.06710", "submitter": "Jelani Nelson", "authors": "Jaros{\\l}aw B{\\l}asiok, Jian Ding, Jelani Nelson", "title": "Continuous monitoring of $\\ell_p$ norms in data streams", "comments": "v2: Lemma 10 proof now correctly bounds q <= (1/eps)^{O(1/p}) instead\n  of the previously erroneous 1/eps^4. All stated results still hold for p in\n  (0,2] bounded away from zero", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In insertion-only streaming, one sees a sequence of indices $a_1, a_2,\n\\ldots, a_m\\in [n]$. The stream defines a sequence of $m$ frequency vectors\n$x^{(1)},\\ldots,x^{(m)}\\in\\mathbb{R}^n$ with $(x^{(t)})_i = |\\{j : j\\in[t], a_j\n= i\\}|$. That is, $x^{(t)}$ is the frequency vector after seeing the first $t$\nitems in the stream. Much work in the streaming literature focuses on\nestimating some function $f(x^{(m)})$. Many applications though require\nobtaining estimates at time $t$ of $f(x^{(t)})$, for every $t\\in[m]$. Naively\nthis guarantee is obtained by devising an algorithm with failure probability\n$\\ll 1/m$, then performing a union bound over all stream updates to guarantee\nthat all $m$ estimates are simultaneously accurate with good probability. When\n$f(x)$ is some $\\ell_p$ norm of $x$, recent works have shown that this union\nbound is wasteful and better space complexity is possible for the continuous\nmonitoring problem, with the strongest known results being for $p=2$ [HTY14,\nBCIW16, BCINWW17]. In this work, we improve the state of the art for all\n$0<p<2$, which we obtain via a novel analysis of Indyk's $p$-stable sketch\n[Indyk06].\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 20:41:43 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:32:34 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 04:42:08 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Ding", "Jian", ""], ["Nelson", "Jelani", ""]]}, {"id": "1704.06724", "submitter": "Jakub Nalepa", "authors": "Miroslaw Blocho, Jakub Nalepa", "title": "Complexity Analysis of the Parallel Guided Ejection Search for the\n  Pickup and Delivery Problem with Time Windows", "comments": "4 pages, presented at the Work in Progress Session at PDP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the pessimistic time complexity analysis of the parallel\nalgorithm for minimizing the fleet size in the pickup and delivery problem with\ntime windows. We show how to estimate the pessimistic complexity step by step.\nThis approach can be easily adopted to other parallel algorithms for solving\ncomplex transportation problems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 23:31:44 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Blocho", "Miroslaw", ""], ["Nalepa", "Jakub", ""]]}, {"id": "1704.06757", "submitter": "O-Joung Kwon", "authors": "\\'Edouard Bonnet and Nick Brettell and O-joung Kwon and D\\'aniel Marx", "title": "Generalized feedback vertex set problems on bounded-treewidth graphs:\n  chordality is the key to single-exponential parameterized algorithms", "comments": "43 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been known that Feedback Vertex Set can be solved in time\n$2^{\\mathcal{O}(w\\log w)}n^{\\mathcal{O}(1)}$ on $n$-vertex graphs of treewidth\n$w$, but it was only recently that this running time was improved to\n$2^{\\mathcal{O}(w)}n^{\\mathcal{O}(1)}$, that is, to single-exponential\nparameterized by treewidth. We investigate which generalizations of Feedback\nVertex Set can be solved in a similar running time. Formally, for a class\n$\\mathcal{P}$ of graphs, the Bounded $\\mathcal{P}$-Block Vertex Deletion\nproblem asks, given a graph~$G$ on $n$ vertices and positive integers~$k$\nand~$d$, whether $G$ contains a set~$S$ of at most $k$ vertices such that each\nblock of $G-S$ has at most $d$ vertices and is in $\\mathcal{P}$. Assuming that\n$\\mathcal{P}$ is recognizable in polynomial time and satisfies a certain\nnatural hereditary condition, we give a sharp characterization of when\nsingle-exponential parameterized algorithms are possible for fixed values of\n$d$: if $\\mathcal{P}$ consists only of chordal graphs, then the problem can be\nsolved in time $2^{\\mathcal{O}(wd^2)} n^{\\mathcal{O}(1)}$, and if $\\mathcal{P}$\ncontains a graph with an induced cycle of length $\\ell\\ge 4$, then the problem\nis not solvable in time $2^{o(w\\log w)} n^{\\mathcal{O}(1)}$ even for fixed\n$d=\\ell$, unless the ETH fails. We also study a similar problem, called Bounded\n$\\mathcal{P}$-Component Vertex Deletion, where the target graphs have connected\ncomponents of small size rather than blocks of small size, and we present\nanalogous results. For this problem, we also show that if $d$ is part of the\ninput and $\\mathcal{P}$ contains all chordal graphs, then it cannot be solved\nin time $f(w)n^{o(w)}$ for some function $f$, unless the ETH fails.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 06:29:10 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 08:52:44 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Brettell", "Nick", ""], ["Kwon", "O-joung", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1704.06774", "submitter": "Martins Kokainis", "authors": "Andris Ambainis and Martins Kokainis", "title": "Quantum algorithm for tree size estimation, with applications to\n  backtracking and 2-player games", "comments": "Fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study quantum algorithms on search trees of unknown structure, in a model\nwhere the tree can be discovered by local exploration. That is, we are given\nthe root of the tree and access to a black box which, given a vertex $v$,\noutputs the children of $v$.\n  We construct a quantum algorithm which, given such access to a search tree of\ndepth at most $n$, estimates the size of the tree $T$ within a factor of $1\\pm\n\\delta$ in $\\tilde{O}(\\sqrt{nT})$ steps. More generally, the same algorithm can\nbe used to estimate size of directed acyclic graphs (DAGs) in a similar model.\n  We then show two applications of this result:\n  a) We show how to transform a classical backtracking search algorithm which\nexamines $T$ nodes of a search tree into an $\\tilde{O}(\\sqrt{T}n^{3/2})$ time\nquantum algorithm, improving over an earlier quantum backtracking algorithm of\nMontanaro (arXiv:1509.02374).\n  b) We give a quantum algorithm for evaluating AND-OR formulas in a model\nwhere the formula can be discovered by local exploration (modeling position\ntrees in 2-player games). We show that, in this setting, formulas of size $T$\nand depth $T^{o(1)}$ can be evaluated in quantum time $O(T^{1/2+o(1)})$. Thus,\nthe quantum speedup is essentially the same as in the case when the formula is\nknown in advance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 09:52:09 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 02:23:16 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Ambainis", "Andris", ""], ["Kokainis", "Martins", ""]]}, {"id": "1704.06818", "submitter": "Salvatore Pontarelli", "authors": "Michael Mitzenmacher, Salvatore Pontarelli, Pedro Reviriego", "title": "Adaptive Cuckoo Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the adaptive cuckoo filter (ACF), a data structure for\napproximate set membership that extends cuckoo filters by reacting to false\npositives, removing them for future queries. As an example application, in\npacket processing queries may correspond to flow identifiers, so a search for\nan element is likely to be followed by repeated searches for that element.\nRemoving false positives can therefore significantly lower the false positive\nrate. The ACF, like the cuckoo filter, uses a cuckoo hash table to store\nfingerprints. We allow fingerprint entries to be changed in response to a false\npositive in a manner designed to minimize the effect on the performance of the\nfilter. We show that the ACF is able to significantly reduce the false positive\nrate by presenting both a theoretical model for the false positive rate and\nsimulations using both synthetic data sets and real packet traces.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 16:55:28 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 05:54:56 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Pontarelli", "Salvatore", ""], ["Reviriego", "Pedro", ""]]}, {"id": "1704.06838", "submitter": "Neda Masoud", "authors": "Neda Masoud and R. Jayakrishnan", "title": "A Decomposition Algorithm to Solve the Multi-Hop Peer-to-Peer\n  Ride-Matching Problem", "comments": null, "journal-ref": "Transportation Research Part B: Methodological, Volume 99, May\n  2017, Pages 1-29, ISSN 0191-2615", "doi": "10.1016/j.trb.2017.01.004", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we mathematically model the multi-hop Peer-to-Peer (P2P)\nride-matching problem as a binary program. We formulate this problem as a\nmany-to-many problem in which a rider can travel by transferring between\nmultiple drivers, and a driver can carry multiple riders. We propose a\npre-processing procedure to reduce the size of the problem, and devise a\ndecomposition algorithm to solve the original ride-matching problem to\noptimality by means of solving multiple smaller problems. We conduct extensive\nnumerical experiments to demonstrate the computational efficiency of the\nproposed algorithm and show its practical applicability to reasonably-sized\ndynamic ride-matching contexts. Finally, in the interest of even lower solution\ntimes, we propose heuristic solution methods, and investigate the trade-offs\nbetween solution time and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 19:14:32 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Masoud", "Neda", ""], ["Jayakrishnan", "R.", ""]]}, {"id": "1704.06840", "submitter": "Damian Straszak", "authors": "L. Elisa Celis and Damian Straszak and Nisheeth K. Vishnoi", "title": "Ranking with Fairness Constraints", "comments": "appeared in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking algorithms are deployed widely to order a set of items in\napplications such as search engines, news feeds, and recommendation systems.\nRecent studies, however, have shown that, left unchecked, the output of ranking\nalgorithms can result in decreased diversity in the type of content presented,\npromote stereotypes, and polarize opinions. In order to address such issues, we\nstudy the following variant of the traditional ranking problem when, in\naddition, there are fairness or diversity constraints. Given a collection of\nitems along with 1) the value of placing an item in a particular position in\nthe ranking, 2) the collection of sensitive attributes (such as gender, race,\npolitical opinion) of each item and 3) a collection of constraints that, for\neach k, bound the number of items with each attribute that are allowed to\nappear in the top k positions of the ranking, the goal is to output a ranking\nthat maximizes the value with respect to the original rank quality metric while\nrespecting the constraints. This problem encapsulates various well-studied\nproblems related to bipartite and hypergraph matching as special cases and\nturns out to be hard to approximate even with simple constraints. Our main\ntechnical contributions are fast exact and approximation algorithms along with\ncomplementary hardness results that, together, come close to settling the\napproximability of this constrained ranking maximization problem. Unlike prior\nwork on the constrained matching problems, our algorithm runs in linear time,\neven when the number of constraints is large, its approximation ratio does not\ndepend on the number of constraints, and it produces solutions with small\nconstraint violations. Our results rely on insights about the constrained\nmatching problem when the objective satisfies properties that appear in common\nranking metrics such as Discounted Cumulative Gain, Spearman's rho or\nBradley-Terry.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 19:31:29 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 17:39:44 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 10:33:10 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 16:30:07 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Celis", "L. Elisa", ""], ["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1704.06847", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj", "title": "A hybrid primal heuristic for Robust Multiperiod Network Design", "comments": "This is the authors' final version of the paper published in:\n  Esparcia-Alc\\'azar A., Mora A. (eds), EvoApplications 2014: Applications of\n  Evolutionary Computation, LNCS 8602, pp. 15-26, 2014. DOI:\n  10.1007/978-3-662-45523-4\\_2. The final publication is available at Springer\n  via http://dx.doi.org/10.1007/978-3-662-45523-4_2. arXiv admin note:\n  substantial text overlap with arXiv:1410.5850", "journal-ref": "Esparcia-Alc\\'azar A., Mora A. (Eds), EvoApplications 2014:\n  Applications of Evolutionary Computation, Springer LNCS vol. 8602, pp. 15-26,\n  2014", "doi": "10.1007/978-3-662-45523-4_2", "report-no": null, "categories": "math.OC cs.DS cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Robust Multiperiod Network Design Problem, a\ngeneralization of the classical Capacitated Network Design Problem that\nadditionally considers multiple design periods and provides solutions protected\nagainst traffic uncertainty. Given the intrinsic difficulty of the problem,\nwhich proves challenging even for state-of-the art commercial solvers, we\npropose a hybrid primal heuristic based on the combination of ant colony\noptimization and an exact large neighborhood search. Computational experiments\non a set of realistic instances from the SNDlib show that our heuristic can\nfind solutions of extremely good quality with low optimality gap.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 20:44:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Krolikowski", "Jonatan", ""], ["Pulaj", "Jonad", ""]]}, {"id": "1704.06850", "submitter": "Nishanth Dikkala", "authors": "Constantinos Daskalakis, Nishanth Dikkala, Nick Gravin", "title": "Testing Symmetric Markov Chains from a Single Trajectory", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical distribution testing assumes access to i.i.d. samples from the\ndistribution that is being tested. We initiate the study of Markov chain\ntesting, assuming access to a single trajectory of a Markov Chain. In\nparticular, we observe a single trajectory X0,...,Xt,... of an unknown,\nsymmetric, and finite state Markov Chain M. We do not control the starting\nstate X0, and we cannot restart the chain. Given our single trajectory, the\ngoal is to test whether M is identical to a model Markov Chain M0 , or far from\nit under an appropriate notion of difference. We propose a measure of\ndifference between two Markov chains, motivated by the early work of Kazakos\n[Kaz78], which captures the scaling behavior of the total variation distance\nbetween trajectories sampled from the Markov chains as the length of these\ntrajectories grows. We provide efficient testers and information-theoretic\nlower bounds for testing identity of symmetric Markov chains under our proposed\nmeasure of difference, which are tight up to logarithmic factors if the hitting\ntimes of the model chain M0 is O(n) in the size of the state space n.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 21:02:31 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 03:28:50 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Gravin", "Nick", ""]]}, {"id": "1704.06870", "submitter": "Haitao Wang", "authors": "Shimin Li and Haitao Wang", "title": "Algorithms for Covering Multiple Barriers", "comments": "This version will be published in TCS. This version corrects an\n  algorithm time analysis error in the previous version for the\n  line-constrained problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problems for covering multiple intervals on a\nline. Given a set $B$ of $m$ line segments (called \"barriers\") on a horizontal\nline $L$ and another set $S$ of $n$ horizontal line segments of the same length\nin the plane, we want to move all segments of $S$ to $L$ so that their union\ncovers all barriers and the maximum movement of all segments of $S$ is\nminimized. Previously, an $O(n^3\\log n)$-time algorithm was given for the case\n$m=1$. In this paper, we propose an $O(n^2\\log n\\log \\log n+nm\\log m)$-time\nalgorithm for a more general setting with any $m\\geq 1$, which also improves\nthe previous work when $m=1$. We then consider a line-constrained version of\nthe problem in which the segments of $S$ are all initially on the line $L$.\nPreviously, an $O(n\\log n)$-time algorithm was known for the case $m=1$. We\npresent an algorithm of $O(m\\log m+n\\log m \\log n)$ time for any $m\\geq 1$.\nThese problems may have applications in mobile sensor barrier coverage in\nwireless sensor networks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 01:36:58 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 22:07:46 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 17:39:33 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Li", "Shimin", ""], ["Wang", "Haitao", ""]]}, {"id": "1704.06899", "submitter": "Jun Takahashi", "authors": "Jun Takahashi and Satoshi Takabe and Koji Hukushima", "title": "An exact algorithm exhibiting RS-RSB/easy-hard correspondence for the\n  maximum independent set problem", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": "10.7566/JPSJ.86.073001", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently proposed exact algorithm for the maximum independent set problem\nis analyzed. The typical running time is improved exponentially in some\nparameter regions compared to simple binary search. The algorithm also\novercomes the core transition point, where the conventional leaf removal\nalgorithm fails, and works up to the replica symmetry breaking (RSB) transition\npoint. This suggests that a leaf removal core itself is not enough for typical\nhardness in the random maximum independent set problem, providing further\nevidence for RSB being the obstacle for algorithms in general.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 08:48:04 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Takahashi", "Jun", ""], ["Takabe", "Satoshi", ""], ["Hukushima", "Koji", ""]]}, {"id": "1704.06907", "submitter": "Shahbaz Khan", "authors": "Manoj Gupta, Shahbaz Khan", "title": "Multiple Source Dual Fault Tolerant BFS Trees", "comments": "Accepted at ICALP 2017, 25 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a graph with $n$ vertices and $m$ edges, with a designated\nset of $\\sigma$ sources $S\\subseteq V$. The fault tolerant subgraph for any\ngraph problem maintains a sparse subgraph $H$ of $G$, such that for any set $F$\nof $k$ failures, the solution for the graph problem on $G\\setminus F$ is\nmaintained in $H\\setminus F$. We address the problem of maintaining a fault\ntolerant subgraph for Breath First Search tree (BFS) of the graph from a single\nsource $s\\in V$ (referred as $k$ FT-BFS) or multiple sources $S\\subseteq V$\n(referred as $k$ FT-MBFS).\n  The problem of $k$ FT-BFS was first studied by Parter and Peleg [ESA13]. They\ndesigned an algorithm to compute FT-BFS subgraph of size $O(n^{3/2})$. Further,\nthey showed how their algorithm can be easily extended to FT-MBFS requiring\n$O(\\sigma^{1/2}n^{3/2})$ space. They also presented matching lower bounds for\nthese results. The result was later extended to solve dual FT-BFS by Parter\n[PODC15] requiring $O(n^{5/3})$ space, again with matching lower bounds.\nHowever, their result was limited to only edge failures in undirected graphs\nand involved very complex analysis. Moreover, their solution doesn't seems to\nbe directly extendible for dual FT-MBFS problem.\n  We present a similar algorithm to solve dual FT-BFS problem with a much\nsimpler analysis. Moreover, our algorithm also works for vertex failures and\ndirected graphs, and can be easily extended to handle dual FT-MBFS problem,\nmatching the lower bound of $O(\\sigma^{1/3}n^{5/3})$ space described by Parter\n[PODC15].The key difference in our approach is a much simpler classification of\npath interactions which formed the basis of the analysis by Parter [PODC15].\nOur dual FT-MBFS structure also seamlessly gives a dual fault tolerant spanner\nwith additive stretch of +2 having size $O(n^{7/8})$.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 10:40:00 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Gupta", "Manoj", ""], ["Khan", "Shahbaz", ""]]}, {"id": "1704.06928", "submitter": "Ya-Feng Liu", "authors": "Rui Diao and Ya-Feng Liu and Yu-Hong Dai", "title": "A New Fully Polynomial Time Approximation Scheme for the Interval Subset\n  Sum Problem", "comments": "The paper will appear in the Journal of Global Optimization. The\n  paper has 31 pages and 4 tables. The C++ simulation codes of the proposed\n  FPTAS in the paper for solving the interval subset sum problem are available\n  at [http://bitbucket.org/diaorui/issp]", "journal-ref": null, "doi": "10.1007/s10898-017-0514-0", "report-no": null, "categories": "cs.DS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interval subset sum problem (ISSP) is a generalization of the well-known\nsubset sum problem. Given a set of intervals\n$\\left\\{[a_{i,1},a_{i,2}]\\right\\}_{i=1}^n$ and a target integer $T,$ the ISSP\nis to find a set of integers, at most one from each interval, such that their\nsum best approximates the target $T$ but cannot exceed it. In this paper, we\nfirst study the computational complexity of the ISSP. We show that the ISSP is\nrelatively easy to solve compared to the 0-1 Knapsack problem (KP). We also\nidentify several subclasses of the ISSP which are polynomial time solvable\n(with high probability), albeit the problem is generally NP-hard. Then, we\npropose a new fully polynomial time approximation scheme (FPTAS) for solving\nthe general ISSP problem. The time and space complexities of the proposed\nscheme are ${\\cal O}\\left(n \\max\\left\\{1 / \\epsilon,\\log n\\right\\}\\right)$ and\n${\\cal O}\\left(n+1/\\epsilon\\right),$ respectively, where $\\epsilon$ is the\nrelative approximation error. To the best of our knowledge, the proposed scheme\nhas almost the same time complexity but a significantly lower space complexity\ncompared to the best known scheme. Both the correctness and efficiency of the\nproposed scheme are validated by numerical simulations. In particular, the\nproposed scheme successfully solves ISSP instances with $n=100,000$ and\n$\\epsilon=0.1\\%$ within one second.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 14:41:26 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Diao", "Rui", ""], ["Liu", "Ya-Feng", ""], ["Dai", "Yu-Hong", ""]]}, {"id": "1704.06980", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Artur Kraska, Pawe{\\l} Schmidt", "title": "A Match in Time Saves Nine: Deterministic Online Matching With Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online Min-cost Perfect Matching with Delays\n(MPMD) introduced by Emek et al. (STOC 2016). In this problem, an even number\nof requests appear in a metric space at different times and the goal of an\nonline algorithm is to match them in pairs. In contrast to traditional online\nmatching problems, in MPMD all requests appear online and an algorithm can\nmatch any pair of requests, but such decision may be delayed (e.g., to find a\nbetter match). The cost is the sum of matching distances and the introduced\ndelays.\n  We present the first deterministic online algorithm for this problem. Its\ncompetitive ratio is $O(m^{\\log_2 5.5})$ $ = O(m^{2.46})$, where $2 m$ is the\nnumber of requests. This is polynomial in the number of metric space points if\nall requests are given at different points. In particular, the bound does not\ndepend on other parameters of the metric, such as its aspect ratio. Unlike\nprevious (randomized) solutions for the MPMD problem, our algorithm does not\nneed to know the metric space in advance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:53:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Kraska", "Artur", ""], ["Schmidt", "Pawe\u0142", ""]]}, {"id": "1704.07067", "submitter": "Jannik Matuschke", "authors": "Jannik Matuschke, S. Thomas McCormick, Gianpaolo Oriolo", "title": "Rerouting flows when links fail", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and investigate reroutable flows, a robust version of network\nflows in which link failures can be mitigated by rerouting the affected flow.\nGiven a capacitated network, a path flow is reroutable if after failure of an\narbitrary arc, we can reroute the interrupted flow from the tail of that arc to\nthe sink, without modifying the flow that is not affected by the failure.\nSimilar types of restoration, which are often termed \"local\", were previously\ninvestigated in the context of network design, such as min-cost capacity\nplanning. In this paper, our interest is in computing maximum flows under this\nrobustness assumption. An important new feature of our model, distinguishing it\nfrom existing max robust flow models, is that no flow can get lost in the\nnetwork.\n  We also study a tightening of reroutable flows, called strictly reroutable\nflows, making more restrictive assumptions on the capacities available for\nrerouting. For both variants, we devise a reroutable-flow equivalent of an\ns-t-cut and show that the corresponding max flow/min cut gap is bounded by 2.\nIt turns out that a strictly reroutable flow of maximum value can be found\nusing a compact LP formulation, whereas the problem of finding a maximum\nreroutable flow is NP-hard, even when all capacities are in {1, 2}. However,\nthe tightening can be used to get a 2-approximation for reroutable flows. This\nratio is tight in general networks, but we show that in the case of unit\ncapacities, every reroutable flow can be transformed into a strictly reroutable\nflow of same value. While it is NP-hard to compute a maximal integral flow even\nfor unit capacities, we devise a surprisingly simple combinatorial algorithm\nthat finds a half-integral strictly reroutable flow of value 1, or certifies\nthat no such solutions exits. Finally, we also give a hardness result for the\ncase of multiple arc failures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 07:36:52 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 12:35:26 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Matuschke", "Jannik", ""], ["McCormick", "S. Thomas", ""], ["Oriolo", "Gianpaolo", ""]]}, {"id": "1704.07254", "submitter": "Szabolcs Iv\\'an", "authors": "Kitti Gelle, Szabolcs Ivan", "title": "Recognizing Union-Find trees built up using union-by-rank strategy is\n  NP-complete", "comments": "Accepted at 19th International Conference on Descriptional Complexity\n  of Formal Systems, DCFS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disjoint-Set forests, consisting of Union-Find trees, are data structures\nhaving a widespread practical application due to their efficiency. Despite them\nbeing well-known, no exact structural characterization of these trees is known\n(such a characterization exists for Union trees which are constructed without\nusing path compression) for the case assuming union-by-rank strategy for\nmerging. In this paper we provide such a characterization by means of a simple\npush operation and show that the decision problem whether a given tree (along\nwith the rank info of its nodes) is a Union-Find tree is NP-complete,\ncomplementing our earlier similar result for the union-by-size strategy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 14:29:28 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Gelle", "Kitti", ""], ["Ivan", "Szabolcs", ""]]}, {"id": "1704.07279", "submitter": "Fahad Panolan", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh,\n  Meirav Zehavi", "title": "Finding, Hitting and Packing Cycles in Subexponential Time on Unit Disk\n  Graphs", "comments": "30 pages. To appear in ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms with running time $2^{O({\\sqrt{k}\\log{k}})} \\cdot\nn^{O(1)}$ for the following problems. Given an $n$-vertex unit disk graph $G$\nand an integer $k$, decide whether $G$ contains (1) a path on exactly/at least\n$k$ vertices, (2) a cycle on exactly $k$ vertices, (3) a cycle on at least $k$\nvertices, (4) a feedback vertex set of size at most $k$, and (5) a set of $k$\npairwise vertex-disjoint cycles. For the first three problems, no\nsubexponential time parameterized algorithms were previously known. For the\nremaining two problems, our algorithms significantly outperform the previously\nbest known parameterized algorithms that run in time $2^{O(k^{0.75}\\log{k})}\n\\cdot n^{O(1)}$. Our algorithms are based on a new kind of tree decompositions\nof unit disk graphs where the separators can have size up to $k^{O(1)}$ and\nthere exists a solution that crosses every separator at most $O(\\sqrt{k})$\ntimes. The running times of our algorithms are optimal up to the $\\log{k}$\nfactor in the exponent, assuming the Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:18:40 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1704.07284", "submitter": "Ignasi Sau", "authors": "Julien Baste, Ignasi Sau, Dimitrios M. Thilikos", "title": "Hitting minors on bounded treewidth graphs. I. General upper bounds", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finite collection of graphs ${\\cal F}$, the ${\\cal F}$-M-DELETION\nproblem consists in, given a graph $G$ and an integer $k$, deciding whether\nthere exists $S \\subseteq V(G)$ with $|S| \\leq k$ such that $G \\setminus S$\ndoes not contain any of the graphs in ${\\cal F}$ as a minor. We are interested\nin the parameterized complexity of ${\\cal F}$-M-DELETION when the parameter is\nthe treewidth of $G$, denoted by $tw$. Our objective is to determine, for a\nfixed ${\\cal F}$, the smallest function $f_{{\\cal F}}$ such that {${\\cal\nF}$-M-DELETION can be solved in time $f_{{\\cal F}}(tw) \\cdot n^{O(1)}$ on\n$n$-vertex graphs. We prove that $f_{{\\cal F}}(tw) = 2^{2^{O(tw \\cdot\\log\ntw)}}$ for every collection ${\\cal F}$, that $f_{{\\cal F}}(tw) = 2^{O(tw\n\\cdot\\log tw)}$ if ${\\cal F}$ contains a planar graph, and that $f_{{\\cal\nF}}(tw) = 2^{O(tw)}$ if in addition the input graph $G$ is planar or embedded\nin a surface. We also consider the version of the problem where the graphs in\n${\\cal F}$ are forbidden as topological minors, called ${\\cal F}$-TM-DELETION.\nWe prove similar results for this problem, except that in the last two\nalgorithms, instead of requiring ${\\cal F}$ to contain a planar graph, we need\nit to contain a subcubic planar graph. This is the first of a series of\narticles on this topic.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:27:40 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 09:52:25 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 19:34:13 GMT"}, {"version": "v4", "created": "Sat, 22 Dec 2018 12:22:15 GMT"}, {"version": "v5", "created": "Thu, 11 Mar 2021 08:47:51 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Baste", "Julien", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1704.07291", "submitter": "Michael Margaliot", "authors": "Eyal Weiss and Michael Margaliot and Guy Even", "title": "Minimal Controllability of Conjunctive Boolean Networks is NP-Complete", "comments": null, "journal-ref": "Automatica 92 (2018): 56-62", "doi": "10.1016/j.automatica.2018.02.014", "report-no": null, "categories": "cs.DS cs.CC cs.SY math.OC q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a conjunctive Boolean network (CBN) with $n$ state-variables, we\nconsider the problem of finding a minimal set of state-variables to directly\naffect with an input so that the resulting conjunctive Boolean control network\n(CBCN) is controllable. We give a necessary and sufficient condition for\ncontrollability of a CBCN; an $O(n^2)$-time algorithm for testing\ncontrollability; and prove that nonetheless the minimal controllability problem\nfor CBNs is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 04:26:23 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Weiss", "Eyal", ""], ["Margaliot", "Michael", ""], ["Even", "Guy", ""]]}, {"id": "1704.07313", "submitter": "Chen Chen", "authors": "Chen Chen, Changtong Luo, Zonglin Jiang", "title": "Elite Bases Regression: A Real-time Algorithm for Symbolic Regression", "comments": "The 2017 13th International Conference on Natural Computation, Fuzzy\n  Systems and Knowledge Discovery (ICNC-FSKD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic regression is an important but challenging research topic in data\nmining. It can detect the underlying mathematical models. Genetic programming\n(GP) is one of the most popular methods for symbolic regression. However, its\nconvergence speed might be too slow for large scale problems with a large\nnumber of variables. This drawback has become a bottleneck in practical\napplications. In this paper, a new non-evolutionary real-time algorithm for\nsymbolic regression, Elite Bases Regression (EBR), is proposed. EBR generates a\nset of candidate basis functions coded with parse-matrix in specific mapping\nrules. Meanwhile, a certain number of elite bases are preserved and updated\niteratively according to the correlation coefficients with respect to the\ntarget model. The regression model is then spanned by the elite bases. A\ncomparative study between EBR and a recent proposed machine learning method for\nsymbolic regression, Fast Function eXtraction (FFX), are conducted. Numerical\nresults indicate that EBR can solve symbolic regression problems more\neffectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 16:21:10 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 14:26:06 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Chen", "Chen", ""], ["Luo", "Changtong", ""], ["Jiang", "Zonglin", ""]]}, {"id": "1704.07322", "submitter": "Amanda Streib", "authors": "Sam Greenberg, Dana Randall, and Amanda Pascoe Streib", "title": "Sampling Biased Monotonic Surfaces using Exponential Metrics", "comments": null, "journal-ref": "Combinator. Probab. Comp. 29 (2020) 672-697", "doi": "10.1017/S0963548320000188", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotonic surfaces spanning finite regions of $Z^d$ arise in many contexts,\nincluding DNA-based self-assembly, card-shuffling and lozenge tilings. One\nmethod that has been used to uniformly generate these surfaces is a Markov\nchain that iteratively adds or removes a single cube below the surface during a\nstep. We consider a biased version of the chain, where we are more likely to\nadd a cube than to remove it, thereby favoring surfaces that are \"higher\" or\nhave more cubes below it. We prove that the chain is rapidly mixing for any\nuniform bias in $Z^2$ and for bias $\\lambda > d$ in $Z^d$ when $d>2$. In $Z^2$\nwe match the optimal mixing time achieved by Benjamini et al. in the context of\nbiased card shuffling, but using much simpler arguments. The proofs use a\ngeometric distance function and a variant of path coupling in order to handle\ndistances that can be exponentially large. We also provide the first results in\nthe case of fluctuating bias, where the bias can vary depending on the location\nof the tile. We show that the chain continues to be rapidly mixing if the\nbiases are close to uniform, but that the chain can converge exponentially\nslowly in the general setting.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 16:56:53 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 13:29:51 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Greenberg", "Sam", ""], ["Randall", "Dana", ""], ["Streib", "Amanda Pascoe", ""]]}, {"id": "1704.07351", "submitter": "Mostafa Haghir Chehreghani", "authors": "Mostafa Haghir Chehreghani and Talel Abdessalem and and Albert Bifet", "title": "Metropolis-Hastings Algorithms for Estimating Betweenness Centrality in\n  Large Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality is an important index widely used in different domains\nsuch as social networks, traffic networks and the world wide web. However, even\nfor mid-size networks that have only a few hundreds thousands vertices, it is\ncomputationally expensive to compute exact betweenness scores. Therefore in\nrecent years, several approximate algorithms have been developed. In this\npaper, first given a network $G$ and a vertex $r \\in V(G)$, we propose a\nMetropolis-Hastings MCMC algorithm that samples from the space $V(G)$ and\nestimates betweenness score of $r$. The stationary distribution of our MCMC\nsampler is the optimal sampling proposed for betweenness centrality estimation.\nWe show that our MCMC sampler provides an $(\\epsilon,\\delta)$-approximation,\nwhere the number of required samples depends on the position of $r$ in $G$ and\nin many cases, it is a constant. Then, given a network $G$ and a set $R \\subset\nV(G)$, we present a Metropolis-Hastings MCMC sampler that samples from the\njoint space $R$ and $V(G)$ and estimates relative betweenness scores of the\nvertices in $R$. We show that for any pair $r_i, r_j \\in R$, the ratio of the\nexpected values of the estimated relative betweenness scores of $r_i$ and $r_j$\nrespect to each other is equal to the ratio of their betweenness scores. We\nalso show that our joint-space MCMC sampler provides an\n$(\\epsilon,\\delta)$-approximation of the relative betweenness score of $r_i$\nrespect to $r_j$, where the number of required samples depends on the position\nof $r_j$ in $G$ and in many cases, it is a constant.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:45:31 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 09:22:58 GMT"}, {"version": "v3", "created": "Wed, 3 May 2017 22:54:01 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Chehreghani", "Mostafa Haghir", ""], ["Abdessalem", "Talel", ""], ["Bifet", "and Albert", ""]]}, {"id": "1704.07406", "submitter": "Arman Yousefi", "authors": "Rafail Ostrovsky, Yuval Rabani, Arman Yousefi", "title": "Strictly Balancing Matrices in Polynomial Time Using Osborne's Iteration", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osborne's iteration is a method for balancing $n\\times n$ matrices which is\nwidely used in linear algebra packages, as balancing preserves eigenvalues and\nstabilizes their numeral computation. The iteration can be implemented in any\nnorm over $\\mathbb{R}^n$, but it is normally used in the $L_2$ norm. The choice\nof norm not only affects the desired balance condition, but also defines the\niterated balancing step itself.\n  In this paper we focus on Osborne's iteration in any $L_p$ norm, where $p <\n\\infty$. We design a specific implementation of Osborne's iteration in any\n$L_p$ norm that converges to a strictly $\\epsilon$-balanced matrix in\n$\\tilde{O}(\\epsilon^{-2}n^{9} K)$ iterations, where $K$ measures, roughly, the\n{\\em number of bits} required to represent the entries of the input matrix.\n  This is the first result that proves that Osborne's iteration in the $L_2$\nnorm (or any $L_p$ norm, $p < \\infty$) strictly balances matrices in polynomial\ntime. This is a substantial improvement over our recent result (in SODA 2017)\nthat showed weak balancing in $L_p$ norms. Previously, Schulman and Sinclair\n(STOC 2015) showed strong balancing of Osborne's iteration in the $L_\\infty$\nnorm. Their result does not imply any bounds on strict balancing in other\nnorms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 18:37:25 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Ostrovsky", "Rafail", ""], ["Rabani", "Yuval", ""], ["Yousefi", "Arman", ""]]}, {"id": "1704.07468", "submitter": "Yanjun  Qi Dr.", "authors": "Ritambhara Singh, Arshdeep Sekhon, Kamran Kowsari, Jack Lanchantin,\n  Beilun Wang and Yanjun Qi", "title": "GaKCo: a Fast GApped k-mer string Kernel using COunting", "comments": "@ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String Kernel (SK) techniques, especially those using gapped $k$-mers as\nfeatures (gk), have obtained great success in classifying sequences like DNA,\nprotein, and text. However, the state-of-the-art gk-SK runs extremely slow when\nwe increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This\nis because current gk-SK uses a trie-based algorithm to calculate co-occurrence\nof mismatched substrings resulting in a time cost proportional to\n$O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating\n\\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting\n(GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of\nsubstrings using cumulative counting. This algorithm is fast, scalable to\nlarger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous\nasymptotic analysis that compares GaKCo with the state-of-the-art gk-SK.\nTheoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term\nthat slows down the trie-based approach. Experimentally, we observe that GaKCo\nachieves the same accuracy as the state-of-the-art and outperforms its speed by\nfactors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein\n(12 datasets), and character-based English text (2 datasets), respectively.\n  GaKCo is shared as an open source tool at\n\\url{https://github.com/QData/GaKCo-SVM}\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 21:43:21 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 20:12:01 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 17:25:17 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Singh", "Ritambhara", ""], ["Sekhon", "Arshdeep", ""], ["Kowsari", "Kamran", ""], ["Lanchantin", "Jack", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1704.07497", "submitter": "Jingru Zhang", "authors": "Haitao Wang and Jingru Zhang", "title": "Covering Uncertain Points in a Tree", "comments": "A preliminary version will appear in WADS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a coverage problem for uncertain points in a tree.\nLet T be a tree containing a set P of n (weighted) demand points, and the\nlocation of each demand point P_i\\in P is uncertain but is known to appear in\none of m_i points on T each associated with a probability. Given a covering\nrange \\lambda, the problem is to find a minimum number of points (called\ncenters) on T to build facilities for serving (or covering) these demand points\nin the sense that for each uncertain point P_i\\in P, the expected distance from\nP_i to at least one center is no more than $\\lambda$. The problem has not been\nstudied before. We present an O(|T|+M\\log^2 M) time algorithm for the problem,\nwhere |T| is the number of vertices of T and M is the total number of locations\nof all uncertain points of P, i.e., M=\\sum_{P_i\\in P}m_i. In addition, by using\nthis algorithm, we solve a k-center problem on T for the uncertain points of P.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 23:58:10 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Wang", "Haitao", ""], ["Zhang", "Jingru", ""]]}, {"id": "1704.07546", "submitter": "Prajakta Nimbhorkar", "authors": "Meghana Nasre and Prajakta Nimbhorkar", "title": "Popular Matching with Lower Quotas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the well-studied Hospital Residents (HR) problem in the presence\nof lower quotas (LQ). The input instance consists of a bipartite graph $G =\n(\\mathcal{R} \\cup \\mathcal{H}, E)$ where $\\mathcal{R}$ and $\\mathcal{H}$ denote\nsets of residents and hospitals respectively. Every vertex has a preference\nlist that imposes a strict ordering on its neighbors. In addition, each\nhospital $h$ has an associated upper-quota $q^+(h)$ and lower-quota $q^-(h)$. A\nmatching $M$ in $G$ is an assignment of residents to hospitals, and $M$ is said\nto be feasible if every resident is assigned to at most one hospital and a\nhospital $h$ is assigned at least $q^-(h)$ and at most $q^+(h)$ residents.\n  Stability is a de-facto notion of optimality in a model where both sets of\nvertices have preferences. A matching is stable if no unassigned pair has an\nincentive to deviate from it. It is well-known that an instance of the HRLQ\nproblem need not admit a feasible stable matching. In this paper, we consider\nthe notion of popularity for the HRLQ problem. A matching $M$ is popular if no\nother matching $M'$ gets more votes than $M$ when vertices vote between $M$ and\n$M'$. When there are no lower quotas, there always exists a stable matching and\nit is known that every stable matching is popular.\n  We show that in an HRLQ instance, although a feasible stable matching need\nnot exist, there is always a matching that is popular in the set of feasible\nmatchings. We give an efficient algorithm to compute a maximum cardinality\nmatching that is popular amongst all the feasible matchings in an HRLQ\ninstance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 06:10:54 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 05:48:19 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Nasre", "Meghana", ""], ["Nimbhorkar", "Prajakta", ""]]}, {"id": "1704.07625", "submitter": "Jakub Radoszewski", "authors": "Carl Barton, Tomasz Kociumaka, Chang Liu, Solon P. Pissis, and Jakub\n  Radoszewski", "title": "Indexing Weighted Sequences: Neat and Efficient", "comments": "A new, even simpler version of the index", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a \\emph{weighted sequence}, for every position of the sequence and every\nletter of the alphabet a probability of occurrence of this letter at this\nposition is specified. Weighted sequences are commonly used to represent\nimprecise or uncertain data, for example, in molecular biology where they are\nknown under the name of Position-Weight Matrices. Given a probability threshold\n$\\frac1z$, we say that a string $P$ of length $m$ occurs in a weighted sequence\n$X$ at position $i$ if the product of probabilities of the letters of $P$ at\npositions $i,\\ldots,i+m-1$ in $X$ is at least $\\frac1z$. In this article, we\nconsider an \\emph{indexing} variant of the problem, in which we are to\npreprocess a weighted sequence to answer multiple pattern matching queries. We\npresent an $O(nz)$-time construction of an $O(nz)$-sized index for a weighted\nsequence of length $n$ over a constant-sized alphabet that answers pattern\nmatching queries in optimal, $O(m+Occ)$ time, where $Occ$ is the number of\noccurrences reported. The cornerstone of our data structure is a novel\nconstruction of a family of $\\lfloor z \\rfloor$ special strings that carries\nthe information about all the strings that occur in the weighted sequence with\na sufficient probability. We obtain a weighted index with the same complexities\nas in the most efficient previously known index by Barton et al. (CPM 2016),\nbut our construction is significantly simpler. The most complex algorithmic\ntool required in the basic form of our index is the suffix tree which we use to\ndevelop a new, more straightforward index for the so-called property matching\nproblem. We provide an implementation of our data structure. Our construction\nallows us also to obtain a significant improvement over the complexities of the\napproximate variant of the weighted index presented by Biswas et al. (EDBT\n2016) and an improvement of the space complexity of their general index.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:46:49 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 12:29:20 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Barton", "Carl", ""], ["Kociumaka", "Tomasz", ""], ["Liu", "Chang", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1704.07669", "submitter": "Wenjian Yu Prof.", "authors": "Wenjian Yu, Yu Gu, Jian Li, Shenghua Liu, and Yaohang Li", "title": "Single-Pass PCA of Large High-Dimensional Data", "comments": "IJCAI 2017, 16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a fundamental dimension reduction tool\nin statistics and machine learning. For large and high-dimensional data,\ncomputing the PCA (i.e., the singular vectors corresponding to a number of\ndominant singular values of the data matrix) becomes a challenging task. In\nthis work, a single-pass randomized algorithm is proposed to compute PCA with\nonly one pass over the data. It is suitable for processing extremely large and\nhigh-dimensional data stored in slow memory (hard disk) or the data generated\nin a streaming fashion. Experiments with synthetic and real data validate the\nalgorithm's accuracy, which has orders of magnitude smaller error than an\nexisting single-pass algorithm. For a set of high-dimensional data stored as a\n150 GB file, the proposed algorithm is able to compute the first 50 principal\ncomponents in just 24 minutes on a typical 24-core computer, with less than 1\nGB memory cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:55:20 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Yu", "Wenjian", ""], ["Gu", "Yu", ""], ["Li", "Jian", ""], ["Liu", "Shenghua", ""], ["Li", "Yaohang", ""]]}, {"id": "1704.07708", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "An efficient data structure for counting all linear extensions of a\n  poset, calculating its jump number, and the likes", "comments": "nine pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving the goals in the title (and others) relies on a cardinality-wise\nscanning of the ideals of the poset. Specifically, the relevant numbers\nattached to the k+1 element ideals are inferred from the corresponding numbers\nof the k-element (order) ideals. Crucial in all of this is a compressed\nrepresentation (using wildcards) of the ideal lattice. The whole scheme invites\ndistributed computation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:15:08 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1704.07710", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat", "title": "Succinct Approximate Rank Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of summarizing a multi set of elements in $\\{1, 2,\n\\ldots , n\\}$ under the constraint that no element appears more than $\\ell$\ntimes. The goal is then to answer \\emph{rank} queries --- given $i\\in\\{1, 2,\n\\ldots , n\\}$, how many elements in the multi set are smaller than $i$? ---\nwith an additive error of at most $\\Delta$ and in constant time. For this\nproblem, we prove a lower bound of $\\mathcal B_{\\ell,n,\\Delta}\\triangleq$\n$\\left\\lfloor{\\frac{n}{\\left\\lceil{\\Delta / \\ell}\\right\\rceil}}\\right\\rfloor $\n$\\log\\big({\\max\\{\\left\\lfloor{\\ell / \\Delta}\\right\\rfloor,1\\} + 1}\\big)$ bits\nand provide a \\emph{succinct} construction that uses $\\mathcal\nB_{\\ell,n,\\Delta}(1+o(1))$ bits. Next, we generalize our data structure to\nsupport processing of a stream of integers in $\\{0,1,\\ldots,\\ell\\}$, where upon\na query for some $i\\le n$ we provide a $\\Delta$-additive approximation for the\nsum of the \\emph{last} $i$ elements. We show that this too can be done using\n$\\mathcal B_{\\ell,n,\\Delta}(1+o(1))$ bits and in constant time. This yields the\nfirst sub linear space algorithm that computes approximate sliding window sums\nin $O(1)$ time, where the window size is given at the query time; additionally,\nit requires only $(1+o(1))$ more space than is needed for a fixed window size.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:21:35 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Basat", "Ran Ben", ""]]}, {"id": "1704.07791", "submitter": "Anup Rao", "authors": "Tung Mai, Richard Peng, Anup B. Rao, Vijay V. Vazirani", "title": "Concave Flow on Small Depth Directed Networks", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small depth networks arise in a variety of network related applications,\noften in the form of maximum flow and maximum weighted matching. Recent works\nhave generalized such methods to include costs arising from concave functions.\nIn this paper we give an algorithm that takes a depth $D$ network and strictly\nincreasing concave weight functions of flows on the edges and computes a $(1 -\n\\epsilon)$-approximation to the maximum weight flow in time $mD \\epsilon^{-1}$\ntimes an overhead that is logarithmic in the various numerical parameters\nrelated to the magnitudes of gradients and capacities.\n  Our approach is based on extending the scaling algorithm for approximate\nmaximum weighted matchings by [Duan-Pettie JACM`14] to the setting of small\ndepth networks, and then generalizing it to concave functions. In this more\nrestricted setting of linear weights in the range $[w_{\\min}, w_{\\max}]$, it\nproduces a $(1 - \\epsilon)$-approximation in time $O(mD \\epsilon^{-1} \\log(\nw_{\\max} /w_{\\min}))$. The algorithm combines a variety of tools and provides a\nunified approach towards several problems involving small depth networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:12:16 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Mai", "Tung", ""], ["Peng", "Richard", ""], ["Rao", "Anup B.", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1704.07852", "submitter": "Nagaraj Thenkarai Janakiraman", "authors": "Nagaraj T. Janakiraman, Avinash Vem, Krishna R. Narayanan,\n  Jean-Francois Chamberland", "title": "Sub-string/Pattern Matching in Sub-linear Time Using a Sparse Fourier\n  Transform Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of querying a string (or, a database) of length $N$\nbits to determine all the locations where a substring (query) of length $M$\nappears either exactly or is within a Hamming distance of $K$ from the query.\nWe assume that sketches of the original signal can be computed off line and\nstored. Using the sparse Fourier transform computation based approach\nintroduced by Pawar and Ramchandran, we show that all such matches can be\ndetermined with high probability in sub-linear time. Specifically, if the query\nlength $M = O(N^\\mu)$ and the number of matches $L=O(N^\\lambda)$, we show that\nfor $\\lambda < 1-\\mu$ all the matching positions can be determined with a\nprobability that approaches 1 as $N \\rightarrow \\infty$ for $K \\leq\n\\frac{1}{6}M$. More importantly our scheme has a worst-case computational\ncomplexity that is only $O\\left(\\max\\{N^{1-\\mu}\\log^2 N, N^{\\mu+\\lambda}\\log N\n\\}\\right)$, which means we can recover all the matching positions in {\\it\nsub-linear} time for $\\lambda<1-\\mu$. This is a substantial improvement over\nthe best known computational complexity of $O\\left(N^{1-0.359 \\mu} \\right)$ for\nrecovering one matching position by Andoni {\\em et al.} \\cite{andoni2013shift}.\nFurther, the number of Fourier transform coefficients that need to be computed,\nstored and accessed, i.e., the sketching complexity of this algorithm is only\n$O\\left(N^{1-\\mu}\\log N\\right)$. Several extensions of the main theme are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 18:17:07 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Janakiraman", "Nagaraj T.", ""], ["Vem", "Avinash", ""], ["Narayanan", "Krishna R.", ""], ["Chamberland", "Jean-Francois", ""]]}, {"id": "1704.07982", "submitter": "Edward Lee", "authors": "Serge Gaspers, Edward Lee", "title": "Exact Algorithms via Multivariate Subroutines", "comments": "Accepted to ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the family of $\\Phi$-Subset problems, where the input consists of\nan instance $I$ of size $N$ over a universe $U_I$ of size $n$ and the task is\nto check whether the universe contains a subset with property $\\Phi$ (e.g.,\n$\\Phi$ could be the property of being a feedback vertex set for the input graph\nof size at most $k$). Our main tool is a simple randomized algorithm which\nsolves $\\Phi$-Subset in time $(1+b-\\frac{1}{c})^n N^{O(1)}$, provided that\nthere is an algorithm for the $\\Phi$-Extension problem with running time\n$b^{n-|X|} c^k N^{O(1)}$. Here, the input for $\\Phi$-Extension is an instance\n$I$ of size $N$ over a universe $U_I$ of size $n$, a subset $X\\subseteq U_I$,\nand an integer $k$, and the task is to check whether there is a set $Y$ with\n$X\\subseteq Y \\subseteq U_I$ and $|Y\\setminus X|\\le k$ with property $\\Phi$. We\nderandomize this algorithm at the cost of increasing the running time by a\nsubexponential factor in $n$, and we adapt it to the enumeration setting where\nwe need to enumerate all subsets of the universe with property $\\Phi$. This\ngeneralizes the results of Fomin et al. [STOC 2016] who proved the case where\n$b=1$. As case studies, we use these results to design faster deterministic\nalgorithms for: - checking whether a graph has a feedback vertex set of size at\nmost $k$ - enumerating all minimal feedback vertex sets - enumerating all\nminimal vertex covers of size at most $k$, and - enumerating all minimal\n3-hitting sets. We obtain these results by deriving new $b^{n-|X|} c^k\nN^{O(1)}$-time algorithms for the corresponding $\\Phi$-Extension problems (or\nenumeration variant). In some cases, this is done by adapting the analysis of\nan existing algorithm, or in other cases by designing a new algorithm. Our\nanalyses are based on Measure and Conquer, but the value to minimize,\n$1+b-\\frac{1}{c}$, is unconventional and requires non-convex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 06:23:21 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Gaspers", "Serge", ""], ["Lee", "Edward", ""]]}, {"id": "1704.08000", "submitter": "Jules Wulms", "authors": "Wouter Meulemans, Bettina Speckmann, Kevin Verbeek, Jules Wulms", "title": "A Framework for Algorithm Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say that an algorithm is stable if small changes in the input result in\nsmall changes in the output. This kind of algorithm stability is particularly\nrelevant when analyzing and visualizing time-varying data. Stability in general\nplays an important role in a wide variety of areas, such as numerical analysis,\nmachine learning, and topology, but is poorly understood in the context of\n(combinatorial) algorithms. In this paper we present a framework for analyzing\nthe stability of algorithms. We focus in particular on the tradeoff between the\nstability of an algorithm and the quality of the solution it computes. Our\nframework allows for three types of stability analysis with increasing degrees\nof complexity: event stability, topological stability, and Lipschitz stability.\nWe demonstrate the use of our stability framework by applying it to kinetic\nEuclidean minimum spanning trees.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 07:56:38 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 12:23:04 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Meulemans", "Wouter", ""], ["Speckmann", "Bettina", ""], ["Verbeek", "Kevin", ""], ["Wulms", "Jules", ""]]}, {"id": "1704.08101", "submitter": "Sebastiaan J. van Zelst", "authors": "Sebastiaan J. van Zelst, Boudewijn F. van Dongen, Wil M.P. van der\n  Aalst", "title": "Event Stream-Based Process Discovery using Abstract Representations", "comments": "Accepted for publication in \"Knowledge and Information Systems; \"\n  (Springer: http://link.springer.com/journal/10115)", "journal-ref": null, "doi": "10.1007/s10115-017-1060-2", "report-no": null, "categories": "cs.DB cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of process discovery, originating from the area of process mining, is\nto discover a process model based on business process execution data. A\nmajority of process discovery techniques relies on an event log as an input. An\nevent log is a static source of historical data capturing the execution of a\nbusiness process. In this paper we focus on process discovery relying on online\nstreams of business process execution events. Learning process models from\nevent streams poses both challenges and opportunities, i.e. we need to handle\nunlimited amounts of data using finite memory and, preferably, constant time.\nWe propose a generic architecture that allows for adopting several classes of\nexisting process discovery techniques in context of event streams. Moreover, we\nprovide several instantiations of the architecture, accompanied by\nimplementations in the process mining tool-kit ProM (http://promtools.org).\nUsing these instantiations, we evaluate several dimensions of stream-based\nprocess discovery. The evaluation shows that the proposed architecture allows\nus to lift process discovery to the streaming domain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:10:35 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["van Zelst", "Sebastiaan J.", ""], ["van Dongen", "Boudewijn F.", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1704.08122", "submitter": "Sebastian Krinninger", "authors": "Karl Bringmann, Thomas Dueholm Hansen, Sebastian Krinninger", "title": "Improved Algorithms for Computing the Cycle of Minimum Cost-to-Time\n  Ratio in Directed Graphs", "comments": "Accepted to the 44th International Colloquium on Automata, Languages,\n  and Programming (ICALP 2017)", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.124", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of finding the cycle of minimum cost-to-time ratio in a\ndirected graph with $ n $ nodes and $ m $ edges. This problem has a long\nhistory in combinatorial optimization and has recently seen interesting\napplications in the context of quantitative verification. We focus on strongly\npolynomial algorithms to cover the use-case where the weights are relatively\nlarge compared to the size of the graph. Our main result is an algorithm with\nrunning time $ \\tilde O (m^{3/4} n^{3/2}) $, which gives the first improvement\nover Megiddo's $ \\tilde O (n^3) $ algorithm [JACM'83] for sparse graphs. We\nfurther demonstrate how to obtain both an algorithm with running time $ n^3 /\n2^{\\Omega{(\\sqrt{\\log n})}} $ on general graphs and an algorithm with running\ntime $ \\tilde O (n) $ on constant treewidth graphs. To obtain our main result,\nwe develop a parallel algorithm for negative cycle detection and single-source\nshortest paths that might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:55:02 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bringmann", "Karl", ""], ["Hansen", "Thomas Dueholm", ""], ["Krinninger", "Sebastian", ""]]}, {"id": "1704.08235", "submitter": "Nikos Parotsidis", "authors": "Loukas Georgiadis, Thomas Dueholm Hansen, Giuseppe F. Italiano,\n  Sebastian Krinninger and Nikos Parotsidis", "title": "Decremental Data Structures for Connectivity and Dominators in Directed\n  Graphs", "comments": "Accepted to the 44th International Colloquium on Automata, Languages,\n  and Programming (ICALP 2017)", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.42", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dynamic data structure for maintaining the strongly\nconnected components (SCCs) of a directed graph (digraph) under edge deletions,\nso as to answer a rich repertoire of connectivity queries. Our main technical\ncontribution is a decremental data structure that supports sensitivity queries\nof the form \"are $ u $ and $ v $ strongly connected in the graph $ G \\setminus\nw $?\", for any triple of vertices $ u, v, w $, while $ G $ undergoes deletions\nof edges. Our data structure processes a sequence of edge deletions in a\ndigraph with $n$ vertices in $O(m n \\log{n})$ total time and $O(n^2 \\log{n})$\nspace, where $m$ is the number of edges before any deletion, and answers the\nabove queries in constant time. We can leverage our data structure to obtain\ndecremental data structures for many more types of queries within the same time\nand space complexity. For instance for edge-related queries, such as testing\nwhether two query vertices $u$ and $v$ are strongly connected in $G \\setminus\ne$, for some query edge $e$.\n  As another important application of our decremental data structure, we\nprovide the first nontrivial algorithm for maintaining the dominator tree of a\nflow graph under edge deletions. We present an algorithm that processes a\nsequence of edge deletions in a flow graph in $O(m n \\log{n})$ total time and\n$O(n^2 \\log{n})$ space. For reducible flow graphs we provide an $O(mn)$-time\nand $O(m + n)$-space algorithm. We give a conditional lower bound that provides\nevidence that these running times may be tight up to subpolynomial factors.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:44:20 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Hansen", "Thomas Dueholm", ""], ["Italiano", "Giuseppe F.", ""], ["Krinninger", "Sebastian", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1704.08246", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Relative Error Tensor Low Rank Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider relative error low rank approximation of $tensors$ with respect\nto the Frobenius norm: given an order-$q$ tensor $A \\in\n\\mathbb{R}^{\\prod_{i=1}^q n_i}$, output a rank-$k$ tensor $B$ for which\n$\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT, where OPT $= \\inf_{\\textrm{rank-}k~A'}\n\\|A-A'\\|_F^2$. Despite the success on obtaining relative error low rank\napproximations for matrices, no such results were known for tensors. One\nstructural issue is that there may be no rank-$k$ tensor $A_k$ achieving the\nabove infinum. Another, computational issue, is that an efficient relative\nerror low rank approximation algorithm for tensors would allow one to compute\nthe rank of a tensor, which is NP-hard. We bypass these issues via (1)\nbicriteria and (2) parameterized complexity solutions:\n  (1) We give an algorithm which outputs a rank $k' = O((k/\\epsilon)^{q-1})$\ntensor $B$ for which $\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT in $nnz(A) + n \\cdot\n\\textrm{poly}(k/\\epsilon)$ time in the real RAM model. Here $nnz(A)$ is the\nnumber of non-zero entries in $A$.\n  (2) We give an algorithm for any $\\delta >0$ which outputs a rank $k$ tensor\n$B$ for which $\\|A-B\\|_F^2 \\leq (1+\\epsilon)$OPT and runs in $ ( nnz(A) + n\n\\cdot \\textrm{poly}(k/\\epsilon) + \\exp(k^2/\\epsilon) ) \\cdot n^\\delta$ time in\nthe unit cost RAM model.\n  For outputting a rank-$k$ tensor, or even a bicriteria solution with\nrank-$Ck$ for a certain constant $C > 1$, we show a $2^{\\Omega(k^{1-o(1)})}$\ntime lower bound under the Exponential Time Hypothesis.\n  Our results give the first relative error low rank approximations for tensors\nfor a large number of robust error measures for which nothing was known, as\nwell as column row and tube subset selection. We also obtain new results for\nmatrices, such as $nnz(A)$-time CUR decompositions, improving previous\n$nnz(A)\\log n$-time algorithms, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:59:11 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 20:25:01 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1704.08357", "submitter": "Mehrnoosh Shafiee", "authors": "Mehrnoosh Shafiee, and Javad Ghaderi", "title": "An Improved Bound for Minimizing the Total Weighted Completion Time of\n  Coflows in Datacenters", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data-parallel computing frameworks, intermediate parallel data is often\nproduced at various stages which needs to be transferred among servers in the\ndatacenter network (e.g. the shuffle phase in MapReduce). A stage often cannot\nstart or be completed unless all the required data pieces from the preceding\nstage are received. \\emph{Coflow} is a recently proposed networking abstraction\nto capture such communication patterns. We consider the problem of efficiently\nscheduling coflows with release dates in a shared datacenter network so as to\nminimize the total weighted completion time of coflows.\n  Several heuristics have been proposed recently to address this problem, as\nwell as a few polynomial-time approximation algorithms with provable\nperformance guarantees. Our main result in this paper is a polynomial-time\ndeterministic algorithm that improves the prior known results. Specifically, we\npropose a deterministic algorithm with approximation ratio of $5$, which\nimproves the prior best known ratio of $12$. For the special case when all\ncoflows are released at time zero, our deterministic algorithm obtains\napproximation ratio of $4$ which improves the prior best known ratio of $8$.\nThe key ingredient of our approach is an improved linear program formulation\nfor sorting the coflows followed by a simple list scheduling policy. Extensive\nsimulation results, using both synthetic and real traffic traces, are presented\nthat verify the performance of our algorithm and show improvement over the\nprior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:33:14 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Shafiee", "Mehrnoosh", ""], ["Ghaderi", "Javad", ""]]}, {"id": "1704.08445", "submitter": "Spyros Kontogiannis", "authors": "Spyros Kontogiannis and Georgia Papastavrou and Andreas\n  Paraskevopoulos and Dorothea Wagner and Christos Zaroliagis", "title": "Improved Oracles for Time-Dependent Road Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel landmark-based oracle (CFLAT) is presented, which provides\nearliest-arrival-time route plans in time-dependent road networks. To our\nknowledge, this is the first oracle that preprocesses combinatorial structures\n(collections of time-stamped min-travel-time-path trees) rather than\ntravel-time functions. The preprocessed data structure is exploited by a new\nquery algorithm (CFCA) which computes (and pays for it), apart from\nearliest-arrival-time estimations, the actual connecting path that preserves\nthe theoretical approximation guarantees. To make it practical and tackle the\nmain burden of landmark-based oracles (the large preprocessing requirements),\nCFLAT is extensively engineered. A thorough experimental evaluation on two\nreal-world benchmark instances shows that CFLAT achieves a significant\nimprovement on preprocessing, approximation guarantees and query-times, in\ncomparison to previous landmark-based oracles, whose query algorithms do not\naccount for the path construction. It also achieves competitive query-time\nperformance and approximation guarantees compared to state-of-art speedup\nheuristics for time-dependent road networks, whose query-times in most cases do\nnot account for path construction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:17:24 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Kontogiannis", "Spyros", ""], ["Papastavrou", "Georgia", ""], ["Paraskevopoulos", "Andreas", ""], ["Wagner", "Dorothea", ""], ["Zaroliagis", "Christos", ""]]}, {"id": "1704.08462", "submitter": "Zengfeng Huang", "authors": "Zengfeng Huang, Bozidar Radunovic, Milan Vojnovic, Qin Zhang", "title": "Communication complexity of approximate maximum matching in the\n  message-passing model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the communication complexity of finding an approximate maximum\nmatching in a graph in a multi-party message-passing communication model. The\nmaximum matching problem is one of the most fundamental graph combinatorial\nproblems, with a variety of applications.\n  The input to the problem is a graph $G$ that has $n$ vertices and the set of\nedges partitioned over $k$ sites, and an approximation ratio parameter\n$\\alpha$. The output is required to be a matching in $G$ that has to be\nreported by one of the sites, whose size is at least factor $\\alpha$ of the\nsize of a maximum matching in $G$.\n  We show that the communication complexity of this problem is $\\Omega(\\alpha^2\nk n)$ information bits. This bound is shown to be tight up to a $\\log n$\nfactor, by constructing an algorithm, establishing its correctness, and an\nupper bound on the communication cost. The lower bound also applies to other\ngraph combinatorial problems in the message-passing communication model,\nincluding max-flow and graph sparsification.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 07:49:13 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Huang", "Zengfeng", ""], ["Radunovic", "Bozidar", ""], ["Vojnovic", "Milan", ""], ["Zhang", "Qin", ""]]}, {"id": "1704.08468", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Linear-Size Hopsets with Small Hopbound, and Distributed Routing with\n  Low Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a positive parameter $\\beta$, the $\\beta$-bounded distance between a pair\nof vertices $u,v$ in a weighted undirected graph $G = (V,E,\\omega)$ is the\nlength of the shortest $u-v$ path in $G$ with at most $\\beta$ edges, aka {\\em\nhops}. For $\\beta$ as above and $\\epsilon>0$, a {\\em $(\\beta,\\epsilon)$-hopset}\nof $G = (V,E,\\omega)$ is a graph $G' =(V,H,\\omega_H)$ on the same vertex set,\nsuch that all distances in $G$ are $(1+\\epsilon)$-approximated by\n$\\beta$-bounded distances in $G\\cup G'$.\n  Hopsets are a fundamental graph-theoretic and graph-algorithmic construct,\nand they are widely used for distance-related problems in a variety of\ncomputational settings. Currently existing constructions of hopsets produce\nhopsets either with $\\Omega(n \\log n)$ edges, or with a hopbound\n$n^{\\Omega(1)}$. In this paper we devise a construction of {\\em linear-size}\nhopsets with hopbound $(\\log n)^{\\log^{(3)}n+O(1)}$. This improves the previous\nbound almost exponentially.\n  We also devise efficient implementations of our construction in PRAM and\ndistributed settings. The only existing PRAM algorithm \\cite{EN16} for\ncomputing hopsets with a constant (i.e., independent of $n$) hopbound requires\n$n^{\\Omega(1)}$ time. We devise a PRAM algorithm with polylogarithmic running\ntime for computing hopsets with a constant hopbound, i.e., our running time is\nexponentially better than the previous one. Moreover, these hopsets are also\nsignificantly sparser than their counterparts from \\cite{EN16}.\n  We use our hopsets to devise a distributed routing scheme that exhibits\nnear-optimal tradeoff between individual memory requirement\n$\\tilde{O}(n^{1/k})$ of vertices throughout preprocessing and routing phases of\nthe algorithm, and stretch $O(k)$, along with a near-optimal construction time\n$\\approx D + n^{1/2 + 1/k}$, where $D$ is the hop-diameter of the input graph.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 08:08:22 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1704.08470", "submitter": "Marc Goerigk", "authors": "Trivikram Dokka and Marc Goerigk", "title": "An Experimental Comparison of Uncertainty Sets for Robust Shortest Path\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the development of efficient algorithms, data structures and\npreprocessing techniques, real-world shortest path problems in street networks\nare now very fast to solve. But in reality, the exact travel times along each\narc in the network may not be known. This lead to the development of robust\nshortest path problems, where all possible arc travel times are contained in a\nso-called uncertainty set of possible outcomes.\n  Research in robust shortest path problems typically assumes this set to be\ngiven, and provides complexity results as well as algorithms depending on its\nshape. However, what can actually be observed in real-world problems are only\ndiscrete raw data points. The shape of the uncertainty is already a modelling\nassumption. In this paper we test several of the most widely used assumptions\non the uncertainty set using real-world traffic measurements provided by the\nCity of Chicago. We calculate the resulting different robust solutions, and\nevaluate which uncertainty approach is actually reasonable for our data. This\nanchors theoretical research in a real-world application and allows us to point\nout which robust models should be the future focus of algorithmic development.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 08:15:11 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Dokka", "Trivikram", ""], ["Goerigk", "Marc", ""]]}, {"id": "1704.08529", "submitter": "Pascal Schweitzer", "authors": "Pascal Schweitzer", "title": "A polynomial-time randomized reduction from tournament isomorphism to\n  tournament asymmetry", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper develops a new technique to extract a characteristic subset from a\nrandom source that repeatedly samples from a set of elements. Here a\ncharacteristic subset is a set that when containing an element contains all\nelements that have the same probability. With this technique at hand the paper\nlooks at the special case of the tournament isomorphism problem that stands in\nthe way towards a polynomial-time algorithm for the graph isomorphism problem.\nNoting that there is a reduction from the automorphism (asymmetry) problem to\nthe isomorphism problem, a reduction in the other direction is nevertheless not\nknown and remains a thorny open problem. Applying the new technique, we develop\na randomized polynomial-time Turing-reduction from the tournament isomorphism\nproblem to the tournament automorphism problem. This is the first such\nreduction for any kind of combinatorial object not known to have a\npolynomial-time solvable isomorphism problem.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 12:18:27 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Schweitzer", "Pascal", ""]]}, {"id": "1704.08558", "submitter": "Nicola Prezza", "authors": "Philip Bille, Inge Li G{\\o}rtz, Nicola Prezza", "title": "Practical and Effective Re-Pair Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-Pair is an efficient grammar compressor that operates by recursively\nreplacing high-frequency character pairs with new grammar symbols. The most\nspace-efficient linear-time algorithm computing Re-Pair uses\n$(1+\\epsilon)n+\\sqrt n$ words on top of the re-writable text (of length $n$ and\nstored in $n$ words), for any constant $\\epsilon>0$; in practice however, this\nsolution uses complex sub-procedures preventing it from being practical. In\nthis paper, we present an implementation of the above-mentioned result making\nuse of more practical solutions; our tool further improves the working space to\n$(1.5+\\epsilon)n$ words (text included), for some small constant $\\epsilon$. As\na second contribution, we focus on compact representations of the output\ngrammar. The lower bound for storing a grammar with $d$ rules is\n$\\log(d!)+2d\\approx d\\log d+0.557 d$ bits, and the most efficient encoding\nalgorithm in the literature uses at most $d\\log d + 2d$ bits and runs in\n$\\mathcal O(d^{1.5})$ time. We describe a linear-time heuristic maximizing the\ncompressibility of the output Re-Pair grammar. On real datasets, our grammar\nencoding uses---on average---only $2.8\\%$ more bits than the\ninformation-theoretic minimum. In half of the tested cases, our compressor\nimproves the output size of 7-Zip with maximum compression rate turned on.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 13:28:45 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Prezza", "Nicola", ""]]}, {"id": "1704.08592", "submitter": "Elisabetta Bergamini", "authors": "Elisabetta Bergamini, Henning Meyerhenke, Mark Ortmann and Arie Slobbe", "title": "Faster Betweenness Centrality Updates in Evolving Networks", "comments": "Accepted at the 16th International Symposium on Experimental\n  Algorithms (SEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding central nodes is a fundamental problem in network analysis.\nBetweenness centrality is a well-known measure which quantifies the importance\nof a node based on the fraction of shortest paths going though it. Due to the\ndynamic nature of many today's networks, algorithms that quickly update\ncentrality scores have become a necessity. For betweenness, several dynamic\nalgorithms have been proposed over the years, targeting different update types\n(incremental- and decremental-only, fully-dynamic). In this paper we introduce\na new dynamic algorithm for updating betweenness centrality after an edge\ninsertion or an edge weight decrease. Our method is a combination of two\nindependent contributions: a faster algorithm for updating pairwise distances\nas well as number of shortest paths, and a faster algorithm for updating\ndependencies. Whereas the worst-case running time of our algorithm is the same\nas recomputation, our techniques considerably reduce the number of operations\nperformed by existing dynamic betweenness algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 14:21:19 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Meyerhenke", "Henning", ""], ["Ortmann", "Mark", ""], ["Slobbe", "Arie", ""]]}, {"id": "1704.08620", "submitter": "Amey Bhangale", "authors": "Amey Bhangale, Rajiv Gandhi and Guy Kortsarz", "title": "Improved approximation algorithm for the Dense-3-Subhypergraph Problem", "comments": "Claim 4.6 does not hold for the algorithm; we erroneously claimed\n  that we could nullify kD'_i edges in the ith step of the algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of Dense-$3$-Subhypergraph problem was initiated in Chlamt{\\'{a}}c\net al. [Approx'16]. The input is a universe $U$ and collection ${\\cal S}$ of\nsubsets of $U$, each of size $3$, and a number $k$. The goal is to choose a set\n$W$ of $k$ elements from the universe, and maximize the number of sets, $S\\in\n{\\cal S}$ so that $S\\subseteq W$. The members in $U$ are called {\\em vertices}\nand the sets of ${\\cal S}$ are called the {\\em hyperedges}. This is the\nsimplest extension into hyperedges of the case of sets of size $2$ which is the\nwell known Dense $k$-subgraph problem.\n  The best known ratio for the Dense-$3$-Subhypergraph is $O(n^{0.69783..})$ by\nChlamt{\\'{a}}c et al. We improve this ratio to $n^{0.61802..}$. More\nimportantly, we give a new algorithm that approximates Dense-$3$-Subhypergraph\nwithin a ratio of $\\tilde O(n/k)$, which improves the ratio of $O(n^2/k^2)$ of\nChlamt{\\'{a}}c et al.\n  We prove that under the {\\em log density conjecture} (see Bhaskara et al.\n[STOC'10]) the ratio cannot be better than $\\Omega(\\sqrt{n})$ and demonstrate\nsome cases in which this optimum can be attained.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:18:03 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 02:17:33 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 12:44:24 GMT"}, {"version": "v4", "created": "Wed, 24 Jan 2018 16:57:41 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Bhangale", "Amey", ""], ["Gandhi", "Rajiv", ""], ["Kortsarz", "Guy", ""]]}, {"id": "1704.08680", "submitter": "Ali \\c{C}ivril", "authors": "Ali \\c{C}ivril", "title": "Dual Growth with Variable Rates: An Improved Integrality Gap for Steiner\n  Tree", "comments": "Algorithm consists of several phases instead of 2, which was\n  erroneous. The analysis changed accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising approach for obtaining improved approximation algorithms for\nSteiner tree is to use the bidirected cut relaxation (BCR). The integrality gap\nof this relaxation is at least $36/31$, and it has long been conjectured that\nits true value is very close to this lower bound. However, the best upper bound\nfor general graphs is still $2$. With the aim of circumventing the asymmetric\nnature of BCR, Chakrabarty, Devanur and Vazirani [Math. Program., 130 (2011),\npp. 1--32] introduced the simplex-embedding LP, which is equivalent to it.\nUsing this, they gave a $\\sqrt{2}$-approximation algorithm for quasi-bipartite\ngraphs and showed that the integrality gap of the relaxation is at most $4/3$\nfor this class of graphs.\n  In this paper, we extend the approach provided by these authors and show that\nthe integrality gap of BCR is at most $7/6$ on quasi-bipartite graphs via a\nfast combinatorial algorithm. In doing so, we introduce a general technique, in\nparticular a potentially widely applicable extension of the primal-dual schema.\nRoughly speaking, we apply the schema twice with variable rates of growth for\nthe duals in the second phase, where the rates depend on the degrees of the\nduals computed in the first phase. This technique breaks the disadvantage of\nincreasing dual variables in a monotone manner and creates a larger total dual\nvalue, thus presumably attaining the true integrality gap.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:50:20 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 11:34:19 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 12:58:21 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 17:37:24 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["\u00c7ivril", "Ali", ""]]}, {"id": "1704.08683", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Yingyu Liang and David P. Woodruff and\n  Hongyang Zhang", "title": "Matrix Completion and Related Problems via Strong Duality", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the strong duality of non-convex matrix factorization\nproblems: we show that under certain dual conditions, these problems and its\ndual have the same optimum. This has been well understood for convex\noptimization, but little was known for non-convex problems. We propose a novel\nanalytical framework and show that under certain dual conditions, the optimal\nsolution of the matrix factorization program is the same as its bi-dual and\nthus the global optimality of the non-convex program can be achieved by solving\nits bi-dual which is convex. These dual conditions are satisfied by a wide\nclass of matrix factorization problems, although matrix factorization problems\nare hard to solve in full generality. This analytical framework may be of\nindependent interest to non-convex optimization more broadly.\n  We apply our framework to two prototypical matrix factorization problems:\nmatrix completion and robust Principal Component Analysis (PCA). These are\nexamples of efficiently recovering a hidden matrix given limited reliable\nobservations of it. Our framework shows that exact recoverability and strong\nduality hold with nearly-optimal sample complexity guarantees for matrix\ncompletion and robust PCA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:54:46 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 17:51:47 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 11:39:03 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 14:48:14 GMT"}, {"version": "v5", "created": "Wed, 25 Apr 2018 14:14:39 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Liang", "Yingyu", ""], ["Woodruff", "David P.", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1704.08835", "submitter": "Lene M. Favrholdt", "authors": "Joan Boyar, Lene M. Favrholdt, Kim S. Larsen, Michal Kotrb\\v{c}\\'ik", "title": "Relaxing the Irrevocability Requirement for Online Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online graph problems are considered in models where the irrevocability\nrequirement is relaxed. Motivated by practical examples where, for example,\nthere is a cost associated with building a facility and no extra cost\nassociated with doing it later, we consider the Late Accept model, where a\nrequest can be accepted at a later point, but any acceptance is irrevocable.\nSimilarly, we also consider a Late Reject model, where an accepted request can\nlater be rejected, but any rejection is irrevocable (this is sometimes called\npreemption). Finally, we consider the Late Accept/Reject model, where late\naccepts and rejects are both allowed, but any late reject is irrevocable. For\nIndependent Set, the Late Accept/Reject model is necessary to obtain a constant\ncompetitive ratio, but for Vertex Cover the Late Accept model is sufficient and\nfor Minimum Spanning Forest the Late Reject model is sufficient. The Matching\nproblem has a competitive ratio of 2, but in the Late Accept/Reject model, its\ncompetitive ratio is 3/2.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:04:06 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Boyar", "Joan", ""], ["Favrholdt", "Lene M.", ""], ["Larsen", "Kim S.", ""], ["Kotrb\u010d\u00edk", "Michal", ""]]}, {"id": "1704.08852", "submitter": "Kai Jin", "authors": "Kai Jin", "title": "On 1-factorizations of Bipartite Kneser Graphs", "comments": "We design the first explicit 1-factorization of H(2,q), where q is a\n  odd prime power", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging open problem to construct an explicit 1-factorization of\nthe bipartite Kneser graph $H(v,t)$, which contains as vertices all $t$-element\nand $(v-t)$-element subsets of $[v]:=\\{1,\\ldots,v\\}$ and an edge between any\ntwo vertices when one is a subset of the other. In this paper, we propose a new\nframework for designing such 1-factorizations, by which we solve a nontrivial\ncase where $t=2$ and $v$ is an odd prime power. We also revisit two classic\nconstructions for the case $v=2t+1$ --- the \\emph{lexical factorization} and\n\\emph{modular factorization}. We provide their simplified definitions and study\ntheir inner structures. As a result, an optimal algorithm is designed for\ncomputing the lexical factorizations. (An analogous algorithm for the modular\nfactorization is trivial.)\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:59:15 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 03:26:42 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 08:22:46 GMT"}, {"version": "v4", "created": "Mon, 12 Feb 2018 07:23:14 GMT"}, {"version": "v5", "created": "Tue, 17 Jul 2018 08:03:14 GMT"}, {"version": "v6", "created": "Tue, 20 Nov 2018 07:52:39 GMT"}, {"version": "v7", "created": "Wed, 3 Apr 2019 09:51:57 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Jin", "Kai", ""]]}, {"id": "1704.08868", "submitter": "Ioannis Katsikarelis", "authors": "Ioannis Katsikarelis, Michael Lampis, Vangelis Th. Paschos", "title": "Structural Parameters, Tight Bounds, and Approximation for (k,r)-Center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In $(k,r)$-Center we are given a (possibly edge-weighted) graph and are asked\nto select at most $k$ vertices (centers), so that all other vertices are at\ndistance at most $r$ from a center. In this paper we provide a number of tight\nfine-grained bounds on the complexity of this problem with respect to various\nstandard graph parameters. Specifically:\n  - For any $r\\ge 1$, we show an algorithm that solves the problem in\n$O^*((3r+1)^{\\textrm{cw}})$ time, where $\\textrm{cw}$ is the clique-width of\nthe input graph, as well as a tight SETH lower bound matching this algorithm's\nperformance. As a corollary, for $r=1$, this closes the gap that previously\nexisted on the complexity of Dominating Set parameterized by $\\textrm{cw}$.\n  - We strengthen previously known FPT lower bounds, by showing that\n$(k,r)$-Center is W[1]-hard parameterized by the input graph's vertex cover (if\nedge weights are allowed), or feedback vertex set, even if $k$ is an additional\nparameter. Our reductions imply tight ETH-based lower bounds. Finally, we\ndevise an algorithm parameterized by vertex cover for unweighted graphs.\n  - We show that the complexity of the problem parameterized by tree-depth is\n$2^{\\Theta(\\textrm{td}^2)}$ by showing an algorithm of this complexity and a\ntight ETH-based lower bound.\n  We complement these mostly negative results by providing FPT approximation\nschemes parameterized by clique-width or treewidth which work efficiently\nindependently of the values of $k,r$. In particular, we give algorithms which,\nfor any $\\epsilon>0$, run in time\n$O^*((\\textrm{tw}/\\epsilon)^{O(\\textrm{tw})})$,\n$O^*((\\textrm{cw}/\\epsilon)^{O(\\textrm{cw})})$ and return a\n$(k,(1+\\epsilon)r)$-center, if a $(k,r)$-center exists, thus circumventing the\nproblem's W-hardness.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 10:34:16 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 15:55:04 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 17:49:25 GMT"}, {"version": "v4", "created": "Tue, 13 Nov 2018 12:47:55 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Katsikarelis", "Ioannis", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1704.09014", "submitter": "Michael Jarret", "authors": "Michael Jarret and Brad Lackey", "title": "Substochastic Monte Carlo Algorithms", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce and formalize Substochastic Monte Carlo (SSMC)\nalgorithms. These algorithms, originally intended to be a better classical foil\nto quantum annealing than simulated annealing, prove to be worthy optimization\nalgorithms in their own right. In SSMC, a population of walkers is initialized\naccording to a known distribution on an arbitrary search space and varied into\nthe solution of some optimization problem of interest. The first argument of\nthis paper shows how an existing classical algorithm, \"Go-With-The-Winners\"\n(GWW), is a limiting case of SSMC when restricted to binary search and\nparticular driving dynamics.\n  Although limiting to GWW, SSMC is more general. We show that (1) GWW can be\nefficiently simulated within the SSMC framework, (2) SSMC can be exponentially\nfaster than GWW, (3) by naturally incorporating structural information, SSMC\ncan exponentially outperform the quantum algorithm that first inspired it, and\n(4) SSMC exhibits desirable search features in general spaces. Our approach\ncombines ideas from genetic algorithms (GWW), theoretical probability\n(Fleming-Viot processes), and quantum computing. Not only do we demonstrate\nthat SSMC is often more efficient than competing algorithms, but we also hope\nthat our results connecting these disciplines will impact each independently.\nAn implemented version of SSMC has previously enjoyed some success as a\ncompetitive optimization algorithm for Max-$k$-SAT.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 17:18:08 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Jarret", "Michael", ""], ["Lackey", "Brad", ""]]}]