[{"id": "2104.00034", "submitter": "Zuguang Gao", "authors": "Zuguang Gao, John R. Birge, Varun Gupta", "title": "Approximation Schemes for Multiperiod Binary Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An instance of the multiperiod binary knapsack problem (MPBKP) is given by a\nhorizon length $T$, a non-decreasing vector of knapsack sizes $(c_1, \\ldots,\nc_T)$ where $c_t$ denotes the cumulative size for periods $1,\\ldots,t$, and a\nlist of $n$ items. Each item is a triple $(r, q, d)$ where $r$ denotes the\nreward of the item, $q$ its size, and $d$ its time index (or, deadline). The\ngoal is to choose, for each deadline $t$, which items to include to maximize\nthe total reward, subject to the constraints that for all $t=1,\\ldots,T$, the\ntotal size of selected items with deadlines at most $t$ does not exceed the\ncumulative capacity of the knapsack up to time $t$. We also consider the\nmultiperiod binary knapsack problem with soft capacity constraints (MPBKP-S)\nwhere the capacity constraints are allowed to be violated by paying a penalty\nthat is linear in the violation. The goal is to maximize the total profit,\ni.e., the total reward of selected items less the total penalty. Finally, we\nconsider the multiperiod binary knapsack problem with soft stochastic capacity\nconstraints (MPBKP-SS), where the non-decreasing vector of knapsack sizes\n$(c_1, \\ldots, c_T)$ follow some arbitrary joint distribution but we are given\naccess to the profit as an oracle, and we choose a subset of items to maximize\nthe total expected profit, i.e., the total reward less the total expected\npenalty. For MPBKP, we exhibit a fully polynomial-time approximation scheme\nwith runtime\n$\\tilde{\\mathcal{O}}\\left(\\min\\left\\{n+\\frac{T^{3.25}}{\\epsilon^{2.25}},n+\\frac{T^{2}}{\\epsilon^{3}},\\frac{nT}{\\epsilon^2},\\frac{n^2}{\\epsilon}\\right\\}\\right)$\nthat achieves $(1+\\epsilon)$ approximation; for MPBKP-S, the $(1+\\epsilon)$\napproximation can be achieved in $\\mathcal{O}\\left(\\frac{n\\log\nn}{\\epsilon}\\cdot\\min\\left\\{\\frac{T}{\\epsilon},n\\right\\}\\right)$; for MPBKP-SS,\na greedy algorithm is a 2-approximation when items have the same size.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:04:57 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gao", "Zuguang", ""], ["Birge", "John R.", ""], ["Gupta", "Varun", ""]]}, {"id": "2104.00104", "submitter": "Sorrachai Yingchareonthawornchai", "authors": "Jason Li, Danupon Nanongkai, Debmalya Panigrahi, Thatchaphol\n  Saranurak, Sorrachai Yingchareonthawornchai", "title": "Vertex Connectivity in Poly-logarithmic Max-flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vertex connectivity of an $m$-edge $n$-vertex undirected graph is the\nsmallest number of vertices whose removal disconnects the graph, or leaves only\na singleton vertex. In this paper, we give a reduction from the vertex\nconnectivity problem to a set of maxflow instances. Using this reduction, we\ncan solve vertex connectivity in $\\tilde O(m^{\\alpha})$ time for any $\\alpha\n\\ge 1$, if there is a $m^{\\alpha}$-time maxflow algorithm. Using the current\nbest maxflow algorithm that runs in $m^{4/3+o(1)}$ time (Kathuria, Liu and\nSidford, FOCS 2020), this yields a $m^{4/3+o(1)}$-time vertex connectivity\nalgorithm. This is the first improvement in the running time of the vertex\nconnectivity problem in over 20 years, the previous best being an $\\tilde\nO(mn)$-time algorithm due to Henzinger, Rao, and Gabow (FOCS 1996). Indeed, no\nalgorithm with an $o(mn)$ running time was known before our work, even if we\nassume an $\\tilde O(m)$-time maxflow algorithm. Our new technique is robust\nenough to also improve the best $\\tilde O(mn)$-time bound for directed vertex\nconnectivity to $mn^{1-1/12+o(1)}$ time\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:44:18 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 17:27:23 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Li", "Jason", ""], ["Nanongkai", "Danupon", ""], ["Panigrahi", "Debmalya", ""], ["Saranurak", "Thatchaphol", ""], ["Yingchareonthawornchai", "Sorrachai", ""]]}, {"id": "2104.00207", "submitter": "Manjanna Basappa", "authors": "Monith S. Reyunuru, Kriti Jethlia and Manjanna Basappa", "title": "The $k$-Colorable Unit Disk Cover Problem", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider colorable variations of the Unit Disk Cover\n({\\it UDC}) problem as follows.\n  {\\it $k$-Colorable Discrete Unit Disk Cover ({\\it $k$-CDUDC})}: Given a set\n$P$ of $n$ points, and a set $D$ of $m$ unit disks (of radius=1), both lying in\nthe plane, and a parameter $k$, the objective is to compute a set $D'\\subseteq\nD$ such that every point in $P$ is covered by at least one disk in $D'$ and\nthere exists a function $\\chi:D'\\rightarrow C$ that assigns colors to disks in\n$D'$ such that for any $d$ and $d'$ in $D'$ if $d\\cap d'\\neq\\emptyset$, then\n$\\chi(d)\\neq\\chi(d')$, where $C$ denotes a set containing $k$ distinct colors.\n  For the {\\it $k$-CDUDC} problem, our proposed algorithms approximate the\nnumber of colors used in the coloring if there exists a $k$-colorable cover. We\nfirst propose a 4-approximation algorithm in $O(m^{7k}n\\log k)$ time for this\nproblem and then show that the running time can be improved by a multiplicative\nfactor of $m^k$, where a positive integer $k$ denotes the cardinality of a\ncolor-set. The previous best known result for the problem when $k=3$ is due to\nthe recent work of Biedl et al., (2021), who proposed a 2-approximation\nalgorithm in $O(m^{25}n)$ time. For $k=3$, our algorithm runs in $O(m^{18}n)$\ntime, faster than the previous best algorithm, but gives a 4-approximate\nresult. We then generalize our approach to exhibit a\n$O((1+\\lceil\\frac{2}{\\tau}\\rceil)^2)$-approximation algorithm in\n$O(m^{(\\lfloor\\frac{4\\pi+8\\tau+\\tau^2}{\\sqrt{12}}\\rfloor)k}n\\log k)$ time for a\ngiven $1 \\leq \\tau \\leq 2$. We also extend our algorithm to solve the {\\it\n$k$-Colorable Line Segment Disk Cover ({\\it $k$-CLSDC})} and {\\it $k$-Colorable\nRectangular Region Cover ({\\it $k$-CRRC})} problems, in which instead of the\nset $P$ of $n$ points, we are given a set $S$ of $n$ line segments, and a\nrectangular region $\\cal R$, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:23:34 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 14:32:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Reyunuru", "Monith S.", ""], ["Jethlia", "Kriti", ""], ["Basappa", "Manjanna", ""]]}, {"id": "2104.00415", "submitter": "Amir Zandieh", "authors": "Amir Zandieh", "title": "Learning with Neural Tangent Kernels in Near Input Sparsity Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide\nneural nets trained under least squares loss by gradient descent. However,\ndespite its importance, the super-quadratic runtime of kernel methods limits\nthe use of NTK in large-scale learning tasks. To accelerate kernel machines\nwith NTK, we propose a near input sparsity time algorithm that maps the input\ndata to a randomized low-dimensional feature space so that the inner product of\nthe transformed data approximates their NTK evaluation. Our transformation\nworks by sketching the polynomial expansions of arc-cosine kernels.\nFurthermore, we propose a feature map for approximating the convolutional\ncounterpart of the NTK, which can transform any image using a runtime that is\nonly linear in the number of pixels. We show that in standard large-scale\nregression and classification tasks a linear regressor trained on our features\noutperforms trained Neural Nets and Nystrom approximation of NTK kernel.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:56:58 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:04:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zandieh", "Amir", ""]]}, {"id": "2104.00750", "submitter": "Ellis Hershkowitz", "authors": "D Ellis Hershkowitz, Jason Li", "title": "$O(1)$ Steiner Point Removal in Series-Parallel Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how to vertex-sparsify a graph while preserving both the graph's\nmetric and structure. Specifically, we study the Steiner point removal (SPR)\nproblem where we are given a weighted graph $G=(V,E,w)$ and terminal set $V'\n\\subseteq V$ and must compute a weighted minor $G'=(V',E', w')$ of $G$ which\napproximates $G$'s metric on $V'$. A major open question in the area of metric\nembeddings is the existence of $O(1)$ multiplicative distortion SPR solutions\nfor every (non-trivial) minor-closed family of graphs. To this end prior work\nhas studied SPR on trees, cactus and outerplanar graphs and showed that in\nthese graphs such a minor exists with $O(1)$ distortion.\n  We give $O(1)$ distortion SPR solutions for series-parallel graphs, extending\nthe frontier of this line of work. The main engine of our approach is a new\nmetric decomposition for series-parallel graphs which we call a hammock\ndecomposition. Roughly, a hammock decomposition is a forest-like structure that\npreserves certain critical parts of the metric induced by a series-parallel\ngraph.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:06:13 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hershkowitz", "D Ellis", ""], ["Li", "Jason", ""]]}, {"id": "2104.00792", "submitter": "Alok Tripathy", "authors": "Alok Tripathy, Oded Green", "title": "Scalable Hash Table for NUMA Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are used in a plethora of applications, including database\noperations, DNA sequencing, string searching, and many more. As such, there are\nmany parallelized hash tables targeting multicore, distributed, and\naccelerator-based systems. We present in this work a multi-GPU hash table\nimplementation that can process keys at a throughput comparable to that of\ndistributed hash tables. Distributed CPU hash tables have received\nsignificantly more attention than GPU-based hash tables. We show that a single\nnode with multiple GPUs offers roughly the same performance as a 500-1,000-core\nCPU-based cluster. Our algorithm's key component is our use of multiple\nsparse-graph data structures and binning techniques to build the hash table. As\nhas been shown individually, these components can be written with massive\nparallelism that is amenable to GPU acceleration. Since we focus on an\nindividual node, we also leverage communication primitives that are typically\nprohibitive in distributed environments. We show that our new multi-GPU\nalgorithm shares many of the same features of the single GPU algorithm -- thus\nwe have efficient collision management capabilities and can deal with a large\nnumber of duplicates. We evaluate our algorithm on two multi-GPU compute nodes:\n1) an NVIDIA DGX2 server with 16 GPUs and 2) an IBM Power 9 Processor with 6\nNVIDIA GPUs. With 32-bit keys, our implementation processes 8B keys per second,\ncomparable to some 500-1,000-core CPU-based clusters and 4X faster than prior\nsingle-GPU implementations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:34:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Tripathy", "Alok", ""], ["Green", "Oded", ""]]}, {"id": "2104.00927", "submitter": "Rameesh Paul", "authors": "Yash Khanna, Anand Louis, Rameesh Paul", "title": "Independent Sets in Semi-random Hypergraphs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of vertices in a hypergraph is called an independent set if no\nhyperedge is completely contained inside the set. Given a hypergraph, computing\nits largest size independent set is an NP-hard problem.\n  In this work, we study the independent set problem on hypergraphs in a\nnatural semi-random family of instances. Our semi-random model is inspired by\nthe Feige-Kilian model [FK01]. This popular model has also been studied in the\nworks of [FK01, Ste17, MMT20] etc. McKenzie, Mehta, and Trevisan [MMT20] gave\nalgorithms for computing independent sets in such a semi-random family of\ngraphs. The algorithms by McKenzie et al. [MMT20] are based on rounding a\n\"crude-SDP\". We generalize their results and techniques to hypergraphs for an\nanalogous family of hypergraph instances. Our algorithms are based on rounding\nthe \"crude-SDP\" of McKenzie et al. [MMT20], augmented with \"Lasserre/SoS like\"\nhierarchy of constraints. Analogous to the results of McKenzie et al. [MMT20],\nwe study the ranges of input parameters where we can recover the planted\nindependent set or a large independent set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:10:42 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Khanna", "Yash", ""], ["Louis", "Anand", ""], ["Paul", "Rameesh", ""]]}, {"id": "2104.00979", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Prathamesh Mayekar, and\n  Himanshu Tyagi", "title": "Information-constrained optimization: can adaptive processing of\n  gradients help?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit first-order optimization under local information constraints such\nas local privacy, gradient quantization, and computational constraints limiting\naccess to a few coordinates of the gradient. In this setting, the optimization\nalgorithm is not allowed to directly access the complete output of the gradient\noracle, but only gets limited information about it subject to the local\ninformation constraints.\n  We study the role of adaptivity in processing the gradient output to obtain\nthis limited information from it.We consider optimization for both convex and\nstrongly convex functions and obtain tight or nearly tight lower bounds for the\nconvergence rate, when adaptive gradient processing is allowed. Prior work was\nrestricted to convex functions and allowed only nonadaptive processing of\ngradients. For both of these function classes and for the three information\nconstraints mentioned above, our lower bound implies that adaptive processing\nof gradients cannot outperform nonadaptive processing in most regimes of\ninterest. We complement these results by exhibiting a natural optimization\nproblem under information constraints for which adaptive processing of gradient\nstrictly outperforms nonadaptive processing.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 10:45:52 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Mayekar", "Prathamesh", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2104.01101", "submitter": "Linjian Ma", "authors": "Linjian Ma and Edgar Solomonik", "title": "Fast and Accurate Randomized Algorithms for Low-rank Tensor\n  Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-rank Tucker and CP tensor decompositions are powerful tools in data\nanalytics. The widely used alternating least squares (ALS) method, which solves\na sequence of over-determined least squares subproblems, is inefficient for\nlarge and sparse tensors. We propose a fast and accurate sketched ALS algorithm\nfor Tucker decomposition, which solves a sequence of sketched rank-constrained\nlinear least squares subproblems. Theoretical sketch size upper bounds are\nprovided to achieve $O(\\epsilon)$-relative error for each subproblem with two\nsketching techniques, TensorSketch and leverage score sampling. Experimental\nresults show that this new ALS algorithm, combined with a new initialization\nscheme based on randomized range finder, yields up to $22.0\\%$ relative\ndecomposition residual improvement compared to the state-of-the-art sketched\nrandomized algorithm for Tucker decomposition of various synthetic datasets.\nThis Tucker-ALS algorithm is further used to accelerate CP decomposition, by\nusing randomized Tucker compression followed by CP decomposition of the Tucker\ncore tensor. Experimental results show that this algorithm not only converges\nfaster, but also yields more accurate CP decompositions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:55:02 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ma", "Linjian", ""], ["Solomonik", "Edgar", ""]]}, {"id": "2104.01126", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Shangdi Yu, Yan Gu, Julian Shun", "title": "Fast Parallel Algorithms for Euclidean Minimum Spanning Tree and\n  Hierarchical Spatial Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new parallel algorithms for generating Euclidean minimum\nspanning trees and spatial clustering hierarchies (known as HDBSCAN$^*$). Our\napproach is based on generating a well-separated pair decomposition followed by\nusing Kruskal's minimum spanning tree algorithm and bichromatic closest pair\ncomputations. We introduce a new notion of well-separation to reduce the work\nand space of our algorithm for HDBSCAN$^*$. We also present a parallel\napproximate algorithm for OPTICS based on a recent sequential algorithm by Gan\nand Tao. Finally, we give a new parallel divide-and-conquer algorithm for\ncomputing the dendrogram and reachability plots, which are used in visualizing\nclusters of different scale that arise for both EMST and HDBSCAN$^*$. We show\nthat our algorithms are theoretically efficient: they have work (number of\noperations) matching their sequential counterparts, and polylogarithmic depth\n(parallel time).\n  We implement our algorithms and propose a memory optimization that requires\nonly a subset of well-separated pairs to be computed and materialized, leading\nto savings in both space (up to 10x) and time (up to 8x). Our experiments on\nlarge real-world and synthetic data sets using a 48-core machine show that our\nfastest algorithms outperform the best serial algorithms for the problems by\n11.13--55.89x, and existing parallel algorithms by at least an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:05:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Yiqiu", ""], ["Yu", "Shangdi", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "2104.01173", "submitter": "Lucas Maziero", "authors": "Lucas Porto Maziero, F\\'abio Luiz Usberti and Celso Cavellucci", "title": "Branch-and-cut algorithms for the covering salesman problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Covering Salesman Problem (CSP) is a generalization of the Traveling\nSalesman Problem in which the tour is not required to visit all vertices, as\nlong as all vertices are covered by the tour. The objective of CSP is to find a\nminimum length Hamiltonian cycle over a subset of vertices that covers an\nundirected graph. In this paper, valid inequalities from the generalized\ntraveling salesman problem are applied to the CSP in addition to new valid\ninequalities that explore distinct aspects of the problem. A branch-and-cut\nframework assembles exact and heuristic separation routines for integer and\nfractional CSP solutions. Computational experiments show that the proposed\nframework outperformed methodologies from literature with respect to optimality\ngaps. Moreover, optimal solutions were proven for several previously unsolved\ninstances.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 17:47:07 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Maziero", "Lucas Porto", ""], ["Usberti", "F\u00e1bio Luiz", ""], ["Cavellucci", "Celso", ""]]}, {"id": "2104.01661", "submitter": "G\\'abor Sz\\'arnyas", "authors": "G\\'abor Sz\\'arnyas, David A. Bader, Timothy A. Davis, James Kitchen,\n  Timothy G. Mattson, Scott McMillan, Erik Welch", "title": "LAGraph: Linear Algebra, Network Analysis Libraries, and the Study of\n  Graph Algorithms", "comments": "Accepted to GrAPL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms can be expressed in terms of linear algebra. GraphBLAS is a\nlibrary of low-level building blocks for such algorithms that targets algorithm\ndevelopers. LAGraph builds on top of the GraphBLAS to target users of graph\nalgorithms with high-level algorithms common in network analysis. In this\npaper, we describe the first release of the LAGraph library, the design\ndecisions behind the library, and performance using the GAP benchmark suite.\nLAGraph, however, is much more than a library. It is also a project to document\nand analyze the full range of algorithms enabled by the GraphBLAS. To that end,\nwe have developed a compact and intuitive notation for describing these\nalgorithms. In this paper, we present that notation with examples from the GAP\nbenchmark suite.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 18:49:58 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Bader", "David A.", ""], ["Davis", "Timothy A.", ""], ["Kitchen", "James", ""], ["Mattson", "Timothy G.", ""], ["McMillan", "Scott", ""], ["Welch", "Erik", ""]]}, {"id": "2104.01808", "submitter": "Ziyue Huang", "authors": "Ziyue Huang, Yuan Qiu, Ke Yi, Graham Cormode", "title": "Frequency Estimation Under Multiparty Differential Privacy: One-shot and\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of frequency estimation under both privacy\nand communication constraints, where the data is distributed among $k$ parties.\nWe consider two application scenarios: (1) one-shot, where the data is static\nand the aggregator conducts a one-time computation; and (2) streaming, where\neach party receives a stream of items over time and the aggregator continuously\nmonitors the frequencies. We adopt the model of multiparty differential privacy\n(MDP), which is more general than local differential privacy (LDP) and\n(centralized) differential privacy. Our protocols achieve optimality (up to\nlogarithmic factors) permissible by the more stringent of the two constraints.\nIn particular, when specialized to the $\\varepsilon$-LDP model, our protocol\nachieves an error of $\\sqrt{k}/(e^{\\Theta(\\varepsilon)}-1)$ using $O(k\\max\\{\n\\varepsilon, \\frac{1}{\\varepsilon} \\})$ bits of communication and $O(k \\log u)$\nbits of public randomness, where $u$ is the size of the domain.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 08:15:20 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 13:24:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Ziyue", ""], ["Qiu", "Yuan", ""], ["Yi", "Ke", ""], ["Cormode", "Graham", ""]]}, {"id": "2104.02050", "submitter": "Philip Lazos", "authors": "Paul D\\\"utting, Federico Fusco, Philip Lazos, Stefano Leonardi,\n  Rebecca Reiffenh\\\"auser", "title": "Prophet Inequalities for Matching with a Single Sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the prophet inequality problem for (not necessarily bipartite)\nmatching problems with independent edge values, under both edge arrivals and\nvertex arrivals. We show constant-factor prophet inequalities for the case\nwhere the online algorithm has only limited access to the value distributions\nthrough samples. First, we give a $16$-approximate prophet inequality for\nmatching in general graphs under edge arrivals that uses only a single sample\nfrom each value distribution as prior information. Then, for bipartite matching\nand (one-sided) vertex arrivals, we show an improved bound of $8$ that also\nuses just a single sample from each distribution. Finally, we show how to turn\nour $16$-approximate single-sample prophet inequality into a truthful\nsingle-sample mechanism for online bipartite matching with vertex arrivals.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:58:36 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Fusco", "Federico", ""], ["Lazos", "Philip", ""], ["Leonardi", "Stefano", ""], ["Reiffenh\u00e4user", "Rebecca", ""]]}, {"id": "2104.02103", "submitter": "David Fern\\'andez-Baca", "authors": "Ghazaleh Parvini and David Fern\\'andez-Baca", "title": "Exact Algorithms for No-Rainbow Coloring and Phylogenetic Decisiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The input to the no-rainbow hypergraph coloring problem is a hypergraph $H$\nwhere every hyperedge has $r$ nodes. The question is whether there exists an\n$r$-coloring of the nodes of $H$ such that all $r$ colors are used and there is\nno rainbow hyperedge -- i.e., no hyperedge uses all $r$ colors. The no-rainbow\nhypergraph $r$-coloring problem is known to be NP-complete for $r \\geq 3$. The\nspecial case of $r=4$ is the complement of the phylogenetic decisiveness\nproblem. Here we present a deterministic algorithm that solves the no-rainbow\n$r$-coloring problem in $O^*((r-1)^{(r-1)n/r})$ time and a randomized algorithm\nthat solves the problem in $O^*((\\frac{r}{2})^n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:19:18 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Parvini", "Ghazaleh", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "2104.02461", "submitter": "Sanjeev Saxena", "authors": "Waseem Akram and Sanjeev Saxena", "title": "Sorted Range Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sorted range selection problem, the aim is to preprocess a given array\nA[1: n] so as to answers queries of type: given two indices i,j ($1 \\le i\\le j\n\\le n$) and an integer k, report k smallest elements in sorted order present in\nthe sub-array A[i: j] Brodal et.al.[2] have shown that the problem can be\nsolved in O(k) time after O(n log n) preprocessing in linear space. In this\npaper we discuss another tradeoff. We reduce preprocessing time to O(n), but\nquery time is O(k log k), again using linear space. Our method is very simple.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:39:28 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Akram", "Waseem", ""], ["Saxena", "Sanjeev", ""]]}, {"id": "2104.02739", "submitter": "Albert Cheu", "authors": "Albert Cheu, Maxim Zhilyaev", "title": "Differentially Private Histograms in the Shuffle Model from Fake Users", "comments": "29 pages. May 3 updates: (1) experiments that compared results with\n  prior work. (2) moved count-min section to the main body (3) expanded\n  introduction May 29 updates: (1) further improved introduction (2) enhanced\n  reduction in communication complexity (3) included reference to GKMP20\n  protocol", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent work in the shuffle model of differential privacy,\nparticularly for approximate $d$-bin histograms. While these protocols achieve\nlow error, the number of messages sent by each user -- the message complexity\n-- has so far scaled with $d$ or the privacy parameters. The message complexity\nis an informative predictor of a shuffle protocol's resource consumption. We\npresent a protocol whose message complexity is two when there are sufficiently\nmany users. The protocol essentially pairs each row in the dataset with a fake\nrow and performs a simple randomization on all rows. We show that the error\nintroduced by the protocol is small, using rigorous analysis as well as\nexperiments on real-world data. We also prove that corrupt users have a\nrelatively low impact on our protocol's estimates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:24:57 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 18:55:35 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 16:31:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cheu", "Albert", ""], ["Zhilyaev", "Maxim", ""]]}, {"id": "2104.02772", "submitter": "Christopher Harshaw", "authors": "Christopher Harshaw, Ehsan Kazemi, Moran Feldman, Amin Karbasi", "title": "The Power of Subsampling in Submodular Maximization", "comments": "arXiv admin note: text overlap with arXiv:1802.07098", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose subsampling as a unified algorithmic technique for submodular\nmaximization in centralized and online settings. The idea is simple:\nindependently sample elements from the ground set, and use simple combinatorial\ntechniques (such as greedy or local search) on these sampled elements. We show\nthat this approach leads to optimal/state-of-the-art results despite being much\nsimpler than existing methods. In the usual offline setting, we present\nSampleGreedy, which obtains a $(p + 2 + o(1))$-approximation for maximizing a\nsubmodular function subject to a $p$-extendible system using $O(n + nk/p)$\nevaluation and feasibility queries, where $k$ is the size of the largest\nfeasible set. The approximation ratio improves to $p+1$ and $p$ for monotone\nsubmodular and linear objectives, respectively. In the streaming setting, we\npresent SampleStreaming, which obtains a $(4p +2 - o(1))$-approximation for\nmaximizing a submodular function subject to a $p$-matchoid using $O(k)$ memory\nand $O(km/p)$ evaluation and feasibility queries per element, where $m$ is the\nnumber of matroids defining the $p$-matchoid. The approximation ratio improves\nto $4p$ for monotone submodular objectives. We empirically demonstrate the\neffectiveness of our algorithms on video summarization, location summarization,\nand movie recommendation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:25:57 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Harshaw", "Christopher", ""], ["Kazemi", "Ehsan", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "2104.02998", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, Dimitrios M. Thilikos", "title": "Parameterized Complexity of Elimination Distance to First-Order Logic\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The elimination distance to some target graph property P is a general graph\nmodification parameter introduced by Bulian and Dawar. We initiate the study of\nelimination distances to graph properties expressible in first-order logic. We\ndelimit the problem's fixed-parameter tractability by identifying sufficient\nand necessary conditions on the structure of prefixes of first-order logic\nformulas. Our main result is the following meta-theorem: for every graph\nproperty P expressible by a first order-logic formula \\phi\\in \\Sigma_3, that\nis, of the form \\phi=\\exists x_1\\exists x_2\\cdots \\exists x_r \\forall\ny_1\\forall y_2\\cdots \\forall y_s \\exists z_1\\exists z_2\\cdots \\exists z_t \\psi,\nwhere \\psi is a quantifier-free first-order formula, checking whether the\nelimination distance of a graph to P does not exceed k, is fixed-parameter\ntractable parameterized by k. Properties of graphs expressible by formulas from\n\\Sigma_3 include being of bounded degree, excluding a forbidden subgraph, or\ncontaining a bounded dominating set. We complement this theorem by showing that\nsuch a general statement does not hold for formulas with even slightly more\nexpressive prefix structure: there are formulas \\phi\\in \\Pi_3, for which\ncomputing elimination distance is W[2]-hard.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:55:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "2104.03024", "submitter": "Rolf Drechsler", "authors": "Rolf Drechsler", "title": "Polynomial Circuit Verification using BDDs", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification is one of the central tasks during circuit design. While most of\nthe approaches have exponential worst-case behaviour, in the following\ntechniques are discussed for proving polynomial circuit verification based on\nBinary Decision Diagrams (BDDs). It is shown that for circuits with specific\nstructural properties, like e.g. tree-like circuits, and circuits based on\nmultiplexers derived from BDDs complete formal verification can be carried out\nin polynomial time and space.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:56:42 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Drechsler", "Rolf", ""]]}, {"id": "2104.03043", "submitter": "Marc Goerigk", "authors": "Marc Goerigk and Stefan Lendl and Lasse Wulf", "title": "Two-Stage Robust Optimization Problems with Two-Stage Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two-stage robust optimization problems, which can be seen as\ngames between a decision maker and an adversary. After the decision maker fixes\npart of the solution, the adversary chooses a scenario from a specified\nuncertainty set. Afterwards, the decision maker can react to this scenario by\ncompleting the partial first-stage solution to a full solution.\n  We extend this classic setting by adding another adversary stage after the\nsecond decision-maker stage, which results in min-max-min-max problems, thus\npushing two-stage settings further towards more general multi-stage problems.\nWe focus on budgeted uncertainty sets and consider both the continuous and\ndiscrete case. For the former, we show that a wide range of robust\ncombinatorial optimization problems can be decomposed into polynomially many\nsubproblems, which can be solved in polynomial time for example in the case of\n(representative) selection. For the latter, we prove NP-hardness for a wide\nrange of problems, but note that the special case where first- and second-stage\nadversarial costs are equal can remain solvable in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:44:17 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 07:53:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Goerigk", "Marc", ""], ["Lendl", "Stefan", ""], ["Wulf", "Lasse", ""]]}, {"id": "2104.03138", "submitter": "Niels Gr\\\"uttemeier", "authors": "Nils Jakob Eckstein, Niels Gr\\\"uttemeier, Christian Komusiewicz, Frank\n  Sommer", "title": "Destroying Multicolored Paths and Cycles in Edge-Colored Graphs", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the computational complexity of $c$-Colored $P_\\ell$ Deletion and\n$c$-Colored $C_\\ell$ Deletion. In these problems, one is given a\n$c$-edge-colored graph and wants to destroy all induced $c$-colored paths or\ncycles, respectively, on $\\ell$ vertices by deleting at most $k$ edges. Herein,\na path or cycle is $c$-colored if it contains edges of $c$ distinct colors. We\nshow that $c$-Colored $P_\\ell$ Deletion and $c$-Colored $C_\\ell$ Deletion are\nNP-hard for each non-trivial combination of $c$ and $\\ell$. We then analyze the\nparameterized complexity of these problems. We extend the notion of\nneighborhood diversity to edge-colored graphs and show that both problems are\nfixed-parameter tractable with respect to the colored neighborhood diversity of\nthe input graph. We also provide hardness results to outline the limits of\nparameterization by the standard parameter solution size $k$. Finally, we\nconsider bicolored input graphs and show a special case of $2$-Colored $P_4$\nDeletion that can be solved in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:11:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Eckstein", "Nils Jakob", ""], ["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""], ["Sommer", "Frank", ""]]}, {"id": "2104.03219", "submitter": "Yeganeh Alimohammadi", "authors": "Mohammad Akbarpour, Yeganeh Alimohammadi, Shengwu Li, Amin Saberi", "title": "The Value of Excess Supply in Spatial Matching Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic matching in a spatial setting. Drivers are distributed at\nrandom on some interval. Riders arrive in some (possibly adversarial) order at\nrandomly drawn points. The platform observes the location of the drivers, and\ncan match newly arrived riders immediately, or can wait for more riders to\narrive. Unmatched riders incur a waiting cost $c$ per period. The platform can\nmatch riders and drivers, irrevocably. The cost of matching a driver to a rider\nis equal to the distance between them. We quantify the value of slightly\nincreasing supply. We prove that when there are $(1+\\epsilon)$ drivers per\nrider (for any $\\epsilon > 0$), the cost of matching returned by a simple\ngreedy algorithm which pairs each arriving rider to the closest available\ndriver is $O(\\log^3(n))$, where $n$ is the number of riders. On the other hand,\nwith equal number of drivers and riders, even the \\emph{ex post} optimal\nmatching does not have a cost less than $\\Theta(\\sqrt{n})$. Our results shed\nlight on the important role of (small) excess supply in spatial matching\nmarkets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:15:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Akbarpour", "Mohammad", ""], ["Alimohammadi", "Yeganeh", ""], ["Li", "Shengwu", ""], ["Saberi", "Amin", ""]]}, {"id": "2104.03221", "submitter": "Benjamin Coleman", "authors": "Benjamin Coleman, Santiago Segarra, Anshumali Shrivastava, Alex Smola", "title": "Graph Reordering for Cache-Efficient Near Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph search is one of the most successful algorithmic trends in near\nneighbor search. Several of the most popular and empirically successful\nalgorithms are, at their core, a simple walk along a pruned near neighbor\ngraph. Such algorithms consistently perform at the top of industrial speed\nbenchmarks for applications such as embedding search. However, graph traversal\napplications often suffer from poor memory access patterns, and near neighbor\nsearch is no exception to this rule. Our measurements show that popular search\nindices such as the hierarchical navigable small-world graph (HNSW) can have\npoor cache miss performance. To address this problem, we apply graph reordering\nalgorithms to near neighbor graphs. Graph reordering is a memory layout\noptimization that groups commonly-accessed nodes together in memory. We present\nexhaustive experiments applying several reordering algorithms to a leading\ngraph-based near neighbor method based on the HNSW index. We find that\nreordering improves the query time by up to 40%, and we demonstrate that the\ntime needed to reorder the graph is negligible compared to the time required to\nconstruct the index.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:18:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Coleman", "Benjamin", ""], ["Segarra", "Santiago", ""], ["Shrivastava", "Anshumali", ""], ["Smola", "Alex", ""]]}, {"id": "2104.03353", "submitter": "A\\'ecio Solano Rodrigues Santos", "authors": "A\\'ecio Santos, Aline Bessa, Fernando Chirigati, Christopher Musco,\n  Juliana Freire", "title": "Correlation Sketches for Approximate Join-Correlation Queries", "comments": "Proceedings of the 2021 International Conference on Management of\n  Data (SIGMOD '21)", "journal-ref": null, "doi": "10.1145/3448016.3458456", "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of structured datasets, from Web tables and\nopen-data portals to enterprise data, opens up opportunities~to enrich\nanalytics and improve machine learning models through relational data\naugmentation. In this paper, we introduce a new class of data augmentation\nqueries: join-correlation queries. Given a column $Q$ and a join column $K_Q$\nfrom a query table $\\mathcal{T}_Q$, retrieve tables $\\mathcal{T}_X$ in a\ndataset collection such that $\\mathcal{T}_X$ is joinable with $\\mathcal{T}_Q$\non $K_Q$ and there is a column $C \\in \\mathcal{T}_X$ such that $Q$ is\ncorrelated with $C$. A na\\\"ive approach to evaluate these queries, which first\nfinds joinable tables and then explicitly joins and computes correlations\nbetween $Q$ and all columns of the discovered tables, is prohibitively\nexpensive. To efficiently support correlated column discovery, we 1) propose a\nsketching method that enables the construction of an index for a large number\nof tables and that provides accurate estimates for join-correlation queries,\nand 2) explore different scoring strategies that effectively rank the query\nresults based on how well the columns are correlated with the query. We carry\nout a detailed experimental evaluation, using both synthetic and real data,\nwhich shows that our sketches attain high accuracy and the scoring strategies\nlead to high-quality rankings.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:08:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Santos", "A\u00e9cio", ""], ["Bessa", "Aline", ""], ["Chirigati", "Fernando", ""], ["Musco", "Christopher", ""], ["Freire", "Juliana", ""]]}, {"id": "2104.03461", "submitter": "Christopher Musco", "authors": "Vladimir Braverman, Aditya Krishnan and Christopher Musco", "title": "Linear and Sublinear Time Spectral Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the popular kernel polynomial method (KPM) for approximating the\nspectral density (eigenvalue distribution) of an $n\\times n$ Hermitian matrix\n$A$. We prove that a simple and practical variant of the KPM algorithm can\napproximate the spectral density to $\\epsilon$ accuracy in the Wasserstein-1\ndistance with roughly $O({1}/{\\epsilon})$ matrix-vector multiplications with\n$A$. This yields a provable linear time result for the problem with better\n$\\epsilon$ dependence than prior work.\n  The KPM variant we study is based on damped Chebyshev polynomial expansions.\nWe show that it is stable, meaning that it can be combined with any approximate\nmatrix-vector multiplication algorithm for $A$. As an application, we develop\nan $O(n\\cdot \\text{poly}(1/\\epsilon))$ time algorithm for computing the\nspectral density of any $n\\times n$ normalized graph adjacency or Laplacian\nmatrix. This runtime is sublinear in the size of the matrix, and assumes sample\naccess to the graph.\n  Our approach leverages several tools from approximation theory, including\nJackson's seminal work on approximation with positive kernels [Jackson, 1912],\nand stability properties of three-term recurrence relations for orthogonal\npolynomials.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 01:38:20 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 00:47:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Braverman", "Vladimir", ""], ["Krishnan", "Aditya", ""], ["Musco", "Christopher", ""]]}, {"id": "2104.03484", "submitter": "Yair Bartal", "authors": "Yair Bartal", "title": "Advances in Metric Ramsey Theory and its Applications", "comments": "This is paper is still in stages of preparation, this version is not\n  intended for distribution. A preliminary version of this article was written\n  by the author in 2006, and was presented in the 2007 ICMS Workshop on\n  Geometry and Algorithms. The basic result on constructive metric Ramsey\n  decomposition and metric Ramsey theorem has also appeared in the author's\n  lectures notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric Ramsey theory is concerned with finding large well-structured subsets\nof more complex metric spaces. For finite metric spaces this problem was first\nstudies by Bourgain, Figiel and Milman \\cite{bfm}, and studied further in depth\nby Bartal et. al \\cite{BLMN03}. In this paper we provide deterministic\nconstructions for this problem via a novel notion of \\emph{metric Ramsey\ndecomposition}. This method yields several more applications, reflecting on\nsome basic results in metric embedding theory.\n  The applications include various results in metric Ramsey theory including\nthe first deterministic construction yielding Ramsey theorems with tight\nbounds, a well as stronger theorems and properties, implying appropriate\ndistance oracle applications.\n  In addition, this decomposition provides the first deterministic\nBourgain-type embedding of finite metric spaces into Euclidean space, and an\noptimal multi-embedding into ultrametrics, thus improving its applications in\napproximation and online algorithms.\n  The decomposition presented here, the techniques and its consequences have\nalready been used in recent research in the field of metric embedding for\nvarious applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 02:52:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bartal", "Yair", ""]]}, {"id": "2104.03673", "submitter": "J\\'er\\'emie Decouchant", "authors": "Silvia Bonomi and J\\'er\\'emie Decouchant and Giovanni Farina and\n  Vincent Rahli and S\\'ebastien Tixeuil", "title": "Practical Byzantine Reliable Broadcast on Partially Connected Networks", "comments": "This is a preprint of a paper that will appear at the IEEE ICDCS 2021\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we consider the Byzantine reliable broadcast problem on\nauthenticated and partially connected networks. The state-of-the-art method to\nsolve this problem consists in combining two algorithms from the literature.\nHandling asynchrony and faulty senders is typically done thanks to Gabriel\nBracha's authenticated double-echo broadcast protocol, which assumes an\nasynchronous fully connected network. Danny Dolev's algorithm can then be used\nto provide reliable communications between processes in the global fault model,\nwhere up to f processes among N can be faulty in a communication network that\nis at least 2f+1-connected. Following recent works that showed that Dolev's\nprotocol can be made more practical thanks to several optimizations, we show\nthat the state-of-the-art methods to solve our problem can be optimized thanks\nto layer-specific and cross-layer optimizations. Our simulations with the\nOmnet++ network simulator show that these optimizations can be efficiently\ncombined to decrease the total amount of information transmitted or the\nprotocol's latency (e.g., respectively, -25% and -50% with a 16B payload, N=31\nand f=4) compared to the state-of-the-art combination of Bracha's and Dolev's\nprotocols.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:43:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bonomi", "Silvia", ""], ["Decouchant", "J\u00e9r\u00e9mie", ""], ["Farina", "Giovanni", ""], ["Rahli", "Vincent", ""], ["Tixeuil", "S\u00e9bastien", ""]]}, {"id": "2104.03932", "submitter": "Goran \\v{Z}u\\v{z}i\\'c", "authors": "Bernhard Haeupler, David Wajc, Goran Zuzic", "title": "Universally-Optimal Distributed Algorithms for Known Topologies", "comments": "Full version of extended abstract in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed optimization algorithms achieve existentially-optimal\nrunning times, meaning that there exists some pathological worst-case topology\non which no algorithm can do better. Still, most networks of interest allow for\nexponentially faster algorithms. This motivates two questions: (1) What network\ntopology parameters determine the complexity of distributed optimization? (2)\nAre there universally-optimal algorithms that are as fast as possible on every\ntopology?\n  We resolve these 25-year-old open problems in the known-topology setting\n(i.e., supported CONGEST) for a wide class of global network optimization\nproblems including MST, $(1+\\varepsilon)$-min cut, various approximate shortest\npaths problems, sub-graph connectivity, etc.\n  In particular, we provide several (equivalent) graph parameters and show they\nare tight universal lower bounds for the above problems, fully characterizing\ntheir inherent complexity. Our results also imply that algorithms based on the\nlow-congestion shortcut framework match the above lower bound, making them\nuniversally optimal if shortcuts are efficiently approximable. We leverage a\nrecent result in hop-constrained oblivious routing to show this is the case if\nthe topology is known -- giving universally-optimal algorithms for all above\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:26:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Wajc", "David", ""], ["Zuzic", "Goran", ""]]}, {"id": "2104.04037", "submitter": "Amnon  Rosenmann", "authors": "Amnon Rosenmann", "title": "Computing the sequence of $k$-cardinality assignments", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-cardinality assignment problem asks for finding a maximal (minimal)\nweight of a matching of cardinality $k$ in a weighted bipartite graph\n$K_{n,n}$, $k \\leq n$. The algorithm of Gassner and Klinz from 2010 for the\nparametric assignment problem computes in time $O(n^3)$ the set of\n$k$-cardinality assignments for those integers $k \\leq n$ which refer to\n\"essential\" terms of a corresponding maxpolynomial. We show here that one can\nextend this algorithm and compute in a second stage the other \"semi-essential\"\nterms in time $O(n^2)$, which results in a time complexity of $O(n^3)$ for the\nwhole sequence of $k=1,...,n$-cardinality assignments. The more there are\nassignments left to be computed at the second stage the faster the two-stage\nalgorithm runs. In general, however, there is no benefit for this two-stage\nalgorithm on the existing algorithms, e.g. the simpler network flow algorithm\nbased on the successive shortest path algorithm which also computes all the\n$k$-cardinality assignments in time $O(n^3)$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:15:12 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Rosenmann", "Amnon", ""]]}, {"id": "2104.04040", "submitter": "Paul Beaujean", "authors": "Paul Beaujean and Florian Sikora and Florian Yger", "title": "Scaling up graph homomorphism for classification via sampling", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature generation is an open topic of investigation in graph machine\nlearning. In this paper, we study the use of graph homomorphism density\nfeatures as a scalable alternative to homomorphism numbers which retain similar\ntheoretical properties and ability to take into account inductive bias. For\nthis, we propose a high-performance implementation of a simple sampling\nalgorithm which computes additive approximations of homomorphism densities. In\nthe context of graph machine learning, we demonstrate in experiments that\nsimple linear models trained on sample homomorphism densities can achieve\nperformance comparable to graph neural networks on standard graph\nclassification datasets. Finally, we show in experiments on synthetic data that\nthis algorithm scales to very large graphs when implemented with Bloom filters.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:25:37 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Beaujean", "Paul", ""], ["Sikora", "Florian", ""], ["Yger", "Florian", ""]]}, {"id": "2104.04324", "submitter": "Duncan Adamson", "authors": "Duncan Adamson, Argyrios Deligkas, Vladimir V. Gusev, Igor Potapov", "title": "Ranking Bracelets in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main result of the paper is the first polynomial-time algorithm for\nranking bracelets. The time-complexity of the algorithm is O(k^2 n^4), where k\nis the size of the alphabet and n is the length of the considered bracelets.\nThe key part of the algorithm is to compute the rank of any word with respect\nto the set of bracelets by finding three other ranks: the rank over all\nnecklaces, the rank over palindromic necklaces, and the rank over enclosing\napalindromic necklaces. The last two concepts are introduced in this paper.\nThese ranks are key components to our algorithm in order to decompose the\nproblem into parts. Additionally, this ranking procedure is used to build a\npolynomial-time unranking algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 12:12:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Adamson", "Duncan", ""], ["Deligkas", "Argyrios", ""], ["Gusev", "Vladimir V.", ""], ["Potapov", "Igor", ""]]}, {"id": "2104.04853", "submitter": "Shaojie Tang", "authors": "Shaojie Tang", "title": "Beyond Pointwise Submodularity: Non-Monotone Adaptive Submodular\n  Maximization subject to a Knapsack Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the non-monotone adaptive submodular maximization\nproblem subject to a knapsack constraint. The input of our problem is a set of\nitems, where each item has a particular state drawn from a known prior\ndistribution. However, the state of an item is initially unknown, one must\nselect an item in order to reveal the state of that item. Moreover, each item\nhas a fixed cost. There is a utility function which is defined over items and\nstates. Our objective is to sequentially select a group of items to maximize\nthe expected utility subject to a knapsack constraint. Although the\ncardinality-constrained, as well as the more general matroid-constrained,\nadaptive submodular maximization has been well studied in the literature,\nwhether there exists a constant approximation solution for the\nknapsack-constrained adaptive submodular maximization problem remains an open\nproblem. We fill this gap by proposing the first constant approximation\nsolution. In particular, our main contribution is to develop a sampling-based\nrandomized algorithm that achieves a $\\frac{1}{10}$ approximation for\nmaximizing an adaptive submodular function subject to a knapsack constraint.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 20:11:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tang", "Shaojie", ""]]}, {"id": "2104.04908", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi and Vishvajeet N", "title": "Graph Streaming Lower Bounds for Parameter Estimation and Property\n  Testing via a Streaming XOR Lemma", "comments": "Full version of the paper accepted to STOC 2021. 50 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study space-pass tradeoffs in graph streaming algorithms for parameter\nestimation and property testing problems such as estimating the size of maximum\nmatchings and maximum cuts, weight of minimum spanning trees, or testing if a\ngraph is connected or cycle-free versus being far from these properties. We\ndevelop a new lower bound technique that proves that for many problems of\ninterest, including all the above, obtaining a $(1+\\epsilon)$-approximation\nrequires either $n^{\\Omega(1)}$ space or $\\Omega(1/\\epsilon)$ passes, even on\nhighly restricted families of graphs such as bounded-degree planar graphs. For\nmultiple of these problems, this bound matches those of existing algorithms and\nis thus (asymptotically) optimal.\n  Our results considerably strengthen prior lower bounds even for arbitrary\ngraphs: starting from the influential work of [Verbin, Yu; SODA 2011], there\nhas been a plethora of lower bounds for single-pass algorithms for these\nproblems; however, the only multi-pass lower bounds proven very recently in\n[Assadi, Kol, Saxena, Yu; FOCS 2020] rules out sublinear-space algorithms with\nexponentially smaller $o(\\log{(1/\\epsilon)})$ passes for these problems.\n  One key ingredient of our proofs is a simple streaming XOR Lemma, a generic\nhardness amplification result, that we prove: informally speaking, if a\n$p$-pass $s$-space streaming algorithm can only solve a decision problem with\nadvantage $\\delta > 0$ over random guessing, then it cannot solve XOR of $\\ell$\nindependent copies of the problem with advantage much better than\n$\\delta^{\\ell}$. This result can be of independent interest and useful for\nother streaming lower bounds as well.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 03:36:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Assadi", "Sepehr", ""], ["N", "Vishvajeet", ""]]}, {"id": "2104.05091", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Simple, Optimal Algorithms for Random Sampling Without Replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the fundamental problem of drawing a simple random sample of size k\nwithout replacement from [n] := {1, . . . , n}. Although a number of classical\nalgorithms exist for this problem, we construct algorithms that are even\nsimpler, easier to implement, and have optimal space and time complexity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:06:13 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "2104.05093", "submitter": "Anders Aamand", "authors": "Anders Aamand, Jakob B{\\ae}k Tejs Knudsen, Mikkel Thorup", "title": "Load Balancing with Dynamic Set of Balls and Bins", "comments": "Accepted at STOC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In dynamic load balancing, we wish to distribute balls into bins in an\nenvironment where both balls and bins can be added and removed. We want to\nminimize the maximum load of any bin but we also want to minimize the number of\nballs and bins affected when adding or removing a ball or a bin. We want a\nhashing-style solution where we given the ID of a ball can find its bin\nefficiently.\n  We are given a balancing parameter $c=1+\\epsilon$, where $\\epsilon\\in (0,1)$.\nWith $n$ and $m$ the current numbers of balls and bins, we want no bin with\nload above $C=\\lceil c n/m\\rceil$, referred to as the capacity of the bins.\n  We present a scheme where we can locate a ball checking $1+O(\\log\n1/\\epsilon)$ bins in expectation. When inserting or deleting a ball, we expect\nto move $O(1/\\epsilon)$ balls, and when inserting or deleting a bin, we expect\nto move $O(C/\\epsilon)$ balls. Previous bounds were off by a factor\n$1/\\epsilon$.\n  These bounds are best possible when $C=O(1)$ but for larger $C$, we can do\nmuch better: Let $f=\\epsilon C$ if $C\\leq \\log 1/\\epsilon$,\n$f=\\epsilon\\sqrt{C}\\cdot \\sqrt{\\log(1/(\\epsilon\\sqrt{C}))}$ if $\\log\n1/\\epsilon\\leq C<\\tfrac{1}{2\\epsilon^2}$, and $C=1$ if $C\\geq\n\\tfrac{1}{2\\epsilon^2}$. We show that we expect to move $O(1/f)$ balls when\ninserting or deleting a ball, and $O(C/f)$ balls when inserting or deleting a\nbin.\n  For the bounds with larger $C$, we first have to resolve a much simpler\nprobabilistic problem. Place $n$ balls in $m$ bins of capacity $C$, one ball at\nthe time. Each ball picks a uniformly random non-full bin. We show that in\nexpectation and with high probability, the fraction of non-full bins is\n$\\Theta(f)$. Then the expected number of bins that a new ball would have to\nvisit to find one that is not full is $\\Theta(1/f)$. As it turns out, we obtain\nthe same complexity in our more complicated scheme where both balls and bins\ncan be added and removed.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:07:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Aamand", "Anders", ""], ["Knudsen", "Jakob B\u00e6k Tejs", ""], ["Thorup", "Mikkel", ""]]}, {"id": "2104.05172", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "How Asymmetry Helps Buffer Management: Achieving Optimal Tail Size in\n  Cup Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cup game on $n$ cups is a multi-step game with two players, a filler and\nan emptier. At each step, the filler distributes $1$ unit of water among the\ncups, and then the emptier selects a single cup to remove (up to) $1$ unit of\nwater from.\n  There are several objective functions that the emptier might wish to\nminimize. One of the strongest guarantees would be to minimize tail size, which\nis defined to be the number of cups with fill $2$ or greater. A simple\nlower-bound construction shows that the optimal tail size for deterministic\nemptying algorithms is $\\Theta(n)$, however.\n  We present a simple randomized emptying algorithm that achieves tail size\n$\\tilde{O}(\\log n)$ with high probability in $n$ for $\\operatorname{poly} n$\nsteps. Moreover, we show that this is tight up to doubly logarithmic factors.\nWe also extend our results to the multi-processor cup game, achieving tail size\n$\\tilde{O}(\\log n + p)$ on $p$ processors with high probability in $n$. We show\nthat the dependence on $p$ is near optimal for any emptying algorithm that\nachieves polynomial-bounded backlog.\n  A natural question is whether our results can be extended to give unending\nguarantees, which apply to arbitrarily long games. We give a lower bound\nconstruction showing that no monotone memoryless emptying algorithm can achieve\nan unending guarantee on either tail size or the related objective function of\nbacklog. On the other hand, we show that even a very small (i.e., $1 /\n\\operatorname{poly} n$) amount of resource augmentation is sufficient to\novercome this barrier.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:05:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "2104.05200", "submitter": "Valeriu Motroi", "authors": "Valeriu Motroi, Stefan Ciobaca", "title": "A Note on the Performance of Algorithms for Solving Linear Diophantine\n  Equations in the Naturals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We implement four algorithms for solving linear Diophantine equations in the\nnaturals: a lexicographic enumeration algorithm, a completion procedure, a\ngraph-based algorithm, and the Slopes algorithm. As already known, the\nlexicographic enumeration algorithm and the completion procedure are slower\nthan the other two algorithms. We compare in more detail the graph-based\nalgorithm and the Slopes algorithm. In contrast to previous comparisons, our\nwork suggests that they are equally fast on small inputs, but the graph-based\nalgorithm gets much faster as the input grows. We conclude that implementations\nof AC-unification algorithms should use the graph-based algorithm for maximum\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:45:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Motroi", "Valeriu", ""], ["Ciobaca", "Stefan", ""]]}, {"id": "2104.05288", "submitter": "Till Heller", "authors": "Rebekka Haese and Till Heller and Sven O. Krumke", "title": "Algorithms and Complexity for the Almost Equal Maximum Flow Problem", "comments": null, "journal-ref": "Operations Research Proceedings 2019. Springer, Cham, 2020.\n  323-329", "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Equal Maximum Flow Problem (EMFP), we aim for a maximum flow where we\nrequire the same flow value on all edges in some given subsets of the edge set.\nIn this paper, we study the closely related Almost Equal Maximum Flow Problems\n(AEMFP) where the flow values on edges of one homologous edge set differ at\nmost by the valuation of a so called deviation function~$\\Delta$. We prove that\nthe integer almost equal maximum flow problem (integer AEMFP) is in general\n$\\mathcal{NP}$-complete, and show that even the problem of finding a fractional\nmaximum flow in the case of convex deviation functions is also\n$\\mathcal{NP}$-complete. This is in contrast to the EMFP, which is polynomial\ntime solvable in the fractional case. We provide inapproximability results for\nthe integral AEMFP. For the integer AEMFP we state a polynomial algorithm for\nthe constant deviation and concave case for a fixed number of homologous sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:38:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Haese", "Rebekka", ""], ["Heller", "Till", ""], ["Krumke", "Sven O.", ""]]}, {"id": "2104.05480", "submitter": "Tiantian Liu", "authors": "Tiantian Liu, Huan Li, Hua Lu, Muhammad Aamir Cheema, Lidan Shou", "title": "Towards Crowd-aware Indoor Path Planning (Extended Version)", "comments": "The extension of a VLDB'21 paper \"Towards Crowd-aware Indoor Path\n  Planning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Indoor venues accommodate many people who collectively form crowds. Such\ncrowds in turn influence people's routing choices, e.g., people may prefer to\navoid crowded rooms when walking from A to B. This paper studies two types of\ncrowd-aware indoor path planning queries. The Indoor Crowd-Aware Fastest Path\nQuery (FPQ) finds a path with the shortest travel time in the presence of\ncrowds, whereas the Indoor Least Crowded Path Query (LCPQ) finds a path\nencountering the least objects en route. To process the queries, we design a\nunified framework with three major components. First, an indoor crowd model\norganizes indoor topology and captures object flows between rooms. Second, a\ntime-evolving population estimator derives room populations for a future\ntimestamp to support crowd-aware routing cost computations in query processing.\nThird, two exact and two approximate query processing algorithms process each\ntype of query. All algorithms are based on graph traversal over the indoor\ncrowd model and use the same search framework with different strategies of\nupdating the populations during the search process. All proposals are evaluated\nexperimentally on synthetic and real data. The experimental results demonstrate\nthe efficiency and scalability of our framework and query processing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:04:32 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 10:13:46 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Liu", "Tiantian", ""], ["Li", "Huan", ""], ["Lu", "Hua", ""], ["Cheema", "Muhammad Aamir", ""], ["Shou", "Lidan", ""]]}, {"id": "2104.05771", "submitter": "David Naori", "authors": "Haim Kaplan, David Naori, Danny Raz", "title": "Online Weighted Matching with a Sample", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the greedy-based online algorithm for edge-weighted matching with\n(one-sided) vertex arrivals in bipartite graphs, and edge arrivals in general\ngraphs. This algorithm was first studied more than a decade ago by Korula and\nP\\'al for the bipartite case in the random-order model. While the weighted\nbipartite matching problem is solved in the random-order model, this is not the\ncase in recent and exciting online models in which the online player is\nprovided with a sample, and the arrival order is adversarial. The greedy-based\nalgorithm is arguably the most natural and practical algorithm to be applied in\nthese models. Despite its simplicity and appeal, and despite being studied in\nmultiple works, the greedy-based algorithm was not fully understood in any of\nthe studied online models, and its actual performance remained an open question\nfor more than a decade.\n  We provide a thorough analysis of the greedy-based algorithm in several\nonline models. For vertex arrivals in bipartite graphs, we characterize the\nexact competitive-ratio of this algorithm in the random-order model, for any\narrival order of the vertices subsequent to the sampling phase (adversarial and\nrandom orders in particular). We use it to derive tight analysis in the recent\nadversarial-order model with a sample (AOS model) for any sample size,\nproviding the first result in this model beyond the simple secretary problem.\nThen, we generalize and strengthen the black box method of converting results\nin the random-order model to single-sample prophet inequalities, and use it to\nderive the state-of-the-art single-sample prophet inequality for the problem.\nFinally, we use our new techniques to analyze the greedy-based algorithm for\nedge arrivals in general graphs and derive results in all the mentioned online\nmodels. In this case as well, we improve upon the state-of-the-art\nsingle-sample prophet inequality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:02:48 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 09:21:59 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kaplan", "Haim", ""], ["Naori", "David", ""], ["Raz", "Danny", ""]]}, {"id": "2104.05983", "submitter": "Diptapriyo Majumdar", "authors": "Jason Crampton, Gregory Gutin, Diptapriyo Majumdar", "title": "Towards Better Understanding of User Authorization Query Problem via\n  Multi-variable Complexity Analysis", "comments": "Accepted for publication in ACM Transactions on Privacy and Security\n  (TOPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User authorization queries in the context of role-based access control have\nattracted considerable interest in the last 15 years. Such queries are used to\ndetermine whether it is possible to allocate a set of roles to a user that\nenables the user to complete a task, in the sense that all the permissions\nrequired to complete the task are assigned to the roles in that set. Answering\nsuch a query, in general, must take into account a number of factors,\nincluding, but not limited to, the roles to which the user is assigned and\nconstraints on the sets of roles that can be activated. Answering such a query\nis known to be NP-hard. The presence of multiple parameters and the need to\nfind efficient and exact solutions to the problem suggest that a multi-variate\napproach will enable us to better understand the complexity of the user\nauthorization query problem (UAQ). In this paper, we establish a number of\ncomplexity results for UAQ. Specifically, we show the problem remains hard even\nwhen quite restrictive conditions are imposed on the structure of the problem.\nOur FPT results show that we have to use either a parameter with potentially\nquite large values or quite a restricted version of UAQ. Moreover, our second\nFPT algorithm is complex and requires sophisticated, state-of-the-art\ntechniques. In short, our results show that it is unlikely that all variants of\nUAQ that arise in practice can be solved reasonably quickly in general.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:31:00 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Crampton", "Jason", ""], ["Gutin", "Gregory", ""], ["Majumdar", "Diptapriyo", ""]]}, {"id": "2104.06133", "submitter": "David Saulpic", "authors": "Vincent Cohen-Addad, David Saulpic, Chris Schwiegelshohn", "title": "A New Coreset Framework for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a metric space, the $(k,z)$-clustering problem consists of finding $k$\ncenters such that the sum of the of distances raised to the power $z$ of every\npoint to its closest center is minimized. This encapsulates the famous\n$k$-median ($z=1$) and $k$-means ($z=2$) clustering problems. Designing\nsmall-space sketches of the data that approximately preserves the cost of the\nsolutions, also known as \\emph{coresets}, has been an important research\ndirection over the last 15 years.\n  In this paper, we present a new, simple coreset framework that simultaneously\nimproves upon the best known bounds for a large variety of settings, ranging\nfrom Euclidean space, doubling metric, minor-free metric, and the general\nmetric cases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 12:15:36 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 17:43:56 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Saulpic", "David", ""], ["Schwiegelshohn", "Chris", ""]]}, {"id": "2104.06210", "submitter": "R Ravi", "authors": "Joseph Cheriyan, R. Ravi, Martin Skutella", "title": "A simple proof of the Moore-Hodgson Algorithm for minimizing the number\n  of late jobs", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF math.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Moore-Hodgson Algorithm minimizes the number of late jobs on a single\nmachine. That is, it finds an optimal schedule for the classical problem\n$1~|\\;|~\\sum{U_j}$. Several proofs of the correctness of this algorithm have\nbeen published. We present a new short proof.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:01:58 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cheriyan", "Joseph", ""], ["Ravi", "R.", ""], ["Skutella", "Martin", ""]]}, {"id": "2104.06576", "submitter": "Noah Stephens-Davidowitz", "authors": "Divesh Aggarwal, Yanlin Chen, Rajendra Kumar, Zeyong Li, Noah\n  Stephens-Davidowitz", "title": "Dimension-Preserving Reductions Between SVP and CVP in Different\n  $p$-Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $ \\newcommand{\\SVP}{\\textsf{SVP}} \\newcommand{\\CVP}{\\textsf{CVP}}\n\\newcommand{\\eps}{\\varepsilon} $We show a number of reductions between the\nShortest Vector Problem and the Closest Vector Problem over lattices in\ndifferent $\\ell_p$ norms ($\\SVP_p$ and $\\CVP_p$ respectively). Specifically, we\npresent the following $2^{\\eps m}$-time reductions for $1 \\leq p \\leq q \\leq\n\\infty$, which all increase the rank $n$ and dimension $m$ of the input lattice\nby at most one:\n  $\\bullet$ a reduction from $\\widetilde{O}(1/\\eps^{1/p})\\gamma$-approximate\n$\\SVP_q$ to $\\gamma$-approximate $\\SVP_p$;\n  $\\bullet$ a reduction from $\\widetilde{O}(1/\\eps^{1/p}) \\gamma$-approximate\n$\\CVP_p$ to $\\gamma$-approximate $\\CVP_q$; and\n  $\\bullet$ a reduction from $\\widetilde{O}(1/\\eps^{1+1/p})$-$\\CVP_q$ to\n$(1+\\eps)$-unique $\\SVP_p$ (which in turn trivially reduces to\n$(1+\\eps)$-approximate $\\SVP_p$).\n  The last reduction is interesting even in the case $p = q$. In particular,\nthis special case subsumes much prior work adapting $2^{O(m)}$-time $\\SVP_p$\nalgorithms to solve $O(1)$-approximate $\\CVP_p$. In the (important) special\ncase when $p = q$, $1 \\leq p \\leq 2$, and the $\\SVP_p$ oracle is exact, we show\na stronger reduction, from $O(1/\\eps^{1/p})\\text{-}\\CVP_p$ to (exact) $\\SVP_p$\nin $2^{\\eps m}$ time. For example, taking $\\eps = \\log m/m$ and $p = 2$ gives a\nslight improvement over Kannan's celebrated polynomial-time reduction from\n$\\sqrt{m}\\text{-}\\CVP_2$ to $\\SVP_2$. We also note that the last two reductions\ncan be combined to give a reduction from approximate-$\\CVP_p$ to $\\SVP_q$ for\nany $p$ and $q$, regardless of whether $p \\leq q$ or $p > q$.\n  Our techniques combine those from the recent breakthrough work of Eisenbrand\nand Venzin (which showed how to adapt the current fastest known algorithm for\nthese problems in the $\\ell_2$ norm to all $\\ell_p$ norms) together with\nsparsification-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:41:24 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Chen", "Yanlin", ""], ["Kumar", "Rajendra", ""], ["Li", "Zeyong", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "2104.06636", "submitter": "Arne Leitert", "authors": "Arne Leitert", "title": "Computing the Union Join and Subset Graph of Acyclic Hypergraphs in\n  Subquadratic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the two problems of computing the union join graph as well as\ncomputing the subset graph for acyclic hypergraphs and their subclasses. In the\nunion join graph $G$ of an acyclic hypergraph $H$, each vertex of $G$\nrepresents a hyperedge of $H$ and two vertices of $G$ are adjacent if there\nexits a join tree $T$ for $H$ such that the corresponding hyperedges are\nadjacent in $T$. The subset graph of a hypergraph $H$ is a directed graph where\neach vertex represents a hyperedge of $H$ and there is a directed edge from a\nvertex $u$ to a vertex $v$ if the hyperedge corresponding to $u$ is a subset of\nthe hyperedge corresponding to $v$.\n  For a given hypergraph $H = (V, \\mathcal{E})$, let $n = |V|$, $m =\n|\\mathcal{E}|$, and $N = \\sum_{E \\in \\mathcal{E}} |E|$. We show that, if the\nStrong Exponential Time Hypothesis is true, both problems cannot be solved in\n$\\mathcal{O} \\bigl( N^{2 - \\varepsilon} \\bigr)$ time for $\\alpha$-acyclic\nhypergraphs and any constant $\\varepsilon > 0$, even if the created graph is\nsparse. Additionally, we present algorithms that solve both problems in\n$\\mathcal{O} \\bigl( N^2 / \\log N + |G| \\bigr)$ time for $\\alpha$-acyclic\nhypergraphs, in $\\mathcal{O} \\bigl( N \\log (n + m) + |G| \\bigr)$ time for\n$\\beta$-acyclic hypergaphs, and in $\\mathcal{O} \\bigl( N + |G| \\bigr)$ time for\n$\\gamma$-acyclic hypergraphs as well as for interval hypergraphs, where $|G|$\nis the size of the computed graph.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:46:15 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Leitert", "Arne", ""]]}, {"id": "2104.06696", "submitter": "Yuya Sasaki", "authors": "Yuya Sasaki", "title": "Cost-constrained Minimal Steiner Tree Enumeration by Binary Decision\n  Diagram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Steiner tree enumeration problem is a well known problem that asks for\nenumerating Steiner trees. Numerous theoretical works proposed algorithms for\nthe problem and analyzed their complexity, but there are no practical\nalgorithms and experimental studies. In this paper, we study the Steiner tree\nenumeration problem practically. We define a problem cost-constrained minimal\nSteiner tree enumeration problem, which enumerates minimal Steiner trees with\ncosts not larger than a given threshold. To solve the problem, we propose a\nbinary decision diagram (BDD)-based algorithm. The BDD-based algorithm\nconstructs a BDD that compactly represents the set of minimal Steiner trees and\nthen traverses the BDD for enumeration. We develop a novel {\\it frontier-based\nalgorithm} to efficiently construct BDDs. Our BDD traverse algorithm prunes\nSteiner trees with costs larger than the threshold. We also extend our\nalgorithm by preprocessing the given graph and controlling the number of\ngenerated Steiner trees in order to reduce the memory and computation costs.\nThe extension makes our algorithm scalable by generating a subset of the\nminimal Steiner trees. We validate that our algorithm can enumerate Steiner\ntrees in real-world graphs more efficiently than existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:39:33 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:50:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sasaki", "Yuya", ""]]}, {"id": "2104.06740", "submitter": "Patrick Dinklage", "authors": "Patrick Dinklage, Johannes Fischer and Alexander Herlez", "title": "Engineering Predecessor Data Structures for Dynamic Integer Sets", "comments": "16 pages plus 5 page appendix, to be published in the proceedings of\n  the 19th Symposium on Experimental Algorithms (SEA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present highly optimized data structures for the dynamic predecessor\nproblem, where the task is to maintain a set $S$ of $w$-bit numbers under\ninsertions, deletions, and predecessor queries (return the largest element in\n$S$ no larger than a given key). The problem of finding predecessors can be\nviewed as a generalized form of the membership problem, or as a simple version\nof the nearest neighbour problem. It lies at the core of various real-world\nproblems such as internet routing.\n  In this work, we engineer (1) a simple implementation of the idea of universe\nreduction, similar to van-Emde-Boas trees (2) variants of y-fast tries\n[Willard, IPL'83], and (3) B-trees with different strategies for organizing the\nkeys contained in the nodes, including an implementation of dynamic fusion\nnodes [P\\v{a}tra\\c{s}cu and Thorup, FOCS'14]. We implement our data structures\nfor $w=32,40,64$, which covers most typical scenarios.\n  Our data structures finish workloads faster than previous approaches while\nbeing significantly more space-efficient, e.g., they clearly outperform\nstandard implementations of the STL by finishing up to four times as fast using\nless than a third of the memory. Our tests also provide more general insights\non data structure design, such as how small sets should be stored and handled\nand if and when new CPU instructions such as advanced vector extensions pay\noff.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:47:06 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Dinklage", "Patrick", ""], ["Fischer", "Johannes", ""], ["Herlez", "Alexander", ""]]}, {"id": "2104.06873", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle", "title": "Streaming Algorithms for Cardinality-Constrained Maximization of\n  Non-Monotone Submodular Functions in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of maximizing a nonnegative, (not necessarily monotone)\nsubmodular function with respect to a cardinality constraint, we propose\ndeterministic algorithms with linear time complexity; these are the first\nalgorithms to obtain constant approximation ratio with high probability in\nlinear time. Our first algorithm is a single-pass streaming algorithm that\nobtains ratio 9.298 + $\\epsilon$ and makes only two queries per received\nelement. Our second algorithm is a multi-pass streaming algorithm that obtains\nratio 4 + $\\epsilon$. Empirically, the algorithms are validated to use fewer\nqueries than and to obtain comparable objective values to state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:16:28 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kuhnle", "Alan", ""]]}, {"id": "2104.06874", "submitter": "Georgios Chatzigeorgakidis", "authors": "Georgios Chatzigeorgakidis, Dimitrios Skoutas, Kostas Patroumpas,\n  Themis Palpanas, Spiros Athanasiou and Spiros Skiadopoulos", "title": "Twin Subsequence Search in Time Series", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We address the problem of subsequence search in time series using Chebyshev\ndistance, to which we refer as twin subsequence search. We first show how\nexisting time series indices can be extended to perform twin subsequence\nsearch. Then, we introduce TS-Index, a novel index tailored to this problem.\nOur experimental evaluation compares these approaches against real time series\ndatasets, and demonstrates that TS-Index can retrieve twin subsequences much\nfaster under various query conditions. This paper has been published in the\n24th International Conference on Extending Database Technology (EDBT 2021).\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:16:56 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chatzigeorgakidis", "Georgios", ""], ["Skoutas", "Dimitrios", ""], ["Patroumpas", "Kostas", ""], ["Palpanas", "Themis", ""], ["Athanasiou", "Spiros", ""], ["Skiadopoulos", "Spiros", ""]]}, {"id": "2104.06933", "submitter": "Kent Quanrud", "authors": "Kent Quanrud", "title": "Fast Approximations for Rooted Connectivity in Weighted Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximations for computing minimum weighted cuts in directed\ngraphs. We consider both rooted and global minimum cuts, and both edge-cuts and\nvertex-cuts. For these problems we give randomized Monte Carlo algorithms that\ncompute a $(1+\\epsilon)$-approximate minimum cut in $\\tilde{O}(n^2 /\n\\epsilon^2)$ time. These results extend and build on recent work [4] that\nobtained exact algorithms with similar running times in directed graphs with\nsmall integer capacities.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:32:46 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Quanrud", "Kent", ""]]}, {"id": "2104.07009", "submitter": "Yang P. Liu", "authors": "Yang P. Liu, Ashwin Sah, Mehtaab Sawhney", "title": "A Gaussian fixed point random walk", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we design a discrete random walk on the real line which takes\nsteps $0, \\pm 1$ (and one with steps in $\\{\\pm 1, 2\\}$) where at least $96\\%$\nof the signs are $\\pm 1$ in expectation, and which has $\\mathcal{N}(0,1)$ as a\nstationary distribution. As an immediate corollary, we obtain an online version\nof Banaszczyk's discrepancy result for partial colorings and $\\pm 1, 2$\nsignings. Additionally, we recover linear time algorithms for logarithmic\nbounds for the Koml\\'{o}s conjecture in an oblivious online setting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:40:25 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Liu", "Yang P.", ""], ["Sah", "Ashwin", ""], ["Sawhney", "Mehtaab", ""]]}, {"id": "2104.07061", "submitter": "Sebastian Macaluso", "authors": "Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Avinava\n  Dubey, Patrick Flaherty, Manzil Zaheer, Amr Ahmed, Kyle Cranmer, Andrew\n  McCallum", "title": "Exact and Approximate Hierarchical Clustering Using A*", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a critical task in numerous domains. Many\napproaches are based on heuristics and the properties of the resulting\nclusterings are studied post hoc. However, in several applications, there is a\nnatural cost function that can be used to characterize the quality of the\nclustering. In those cases, hierarchical clustering can be seen as a\ncombinatorial optimization problem. To that end, we introduce a new approach\nbased on A* search. We overcome the prohibitively large search space by\ncombining A* with a novel \\emph{trellis} data structure. This combination\nresults in an exact algorithm that scales beyond previous state of the art,\nfrom a search space with $10^{12}$ trees to $10^{15}$ trees, and an approximate\nalgorithm that improves over baselines, even in enormous search spaces that\ncontain more than $10^{1000}$ trees. We empirically demonstrate that our method\nachieves substantially higher quality results than baselines for a particle\nphysics use case and other clustering benchmarks. We describe how our method\nprovides significantly improved theoretical bounds on the time and space\ncomplexity of A* for clustering.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:15:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Greenberg", "Craig S.", ""], ["Macaluso", "Sebastian", ""], ["Monath", "Nicholas", ""], ["Dubey", "Avinava", ""], ["Flaherty", "Patrick", ""], ["Zaheer", "Manzil", ""], ["Ahmed", "Amr", ""], ["Cranmer", "Kyle", ""], ["McCallum", "Andrew", ""]]}, {"id": "2104.07114", "submitter": "Rico Zenklusen", "authors": "Vera Traub and Rico Zenklusen", "title": "A Better-Than-2 Approximation for Weighted Tree Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximation algorithm for Weighted Tree Augmentation with\napproximation factor $1+\\ln 2 + \\varepsilon < 1.7$. This is the first algorithm\nbeating the longstanding factor of $2$, which can be achieved through many\nstandard techniques.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:28:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Traub", "Vera", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2104.07205", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri and Kent Quanrud", "title": "Faster Algorithms for Rooted Connectivity in Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problems of determining the rooted and global\nedge and vertex connectivities (and computing the corresponding cuts) in\ndirected graphs. For rooted (and hence also global) edge connectivity with\nsmall integer capacities we give a new randomized Monte Carlo algorithm that\nruns in time $\\tilde{O}(n^2)$. For rooted edge connectivity this is the first\nalgorithm to improve on the $\\Omega(n^3)$ time bound in the dense-graph\nhigh-connectivity regime. Our result relies on a simple combination of sampling\ncoupled with sparsification that appears new, and could lead to further\ntradeoffs for directed graph connectivity problems.\n  We extend the edge connectivity ideas to rooted and global vertex\nconnectivity in directed graphs. We obtain a $(1 + \\epsilon)$-approximation for\nrooted vertex connectivity in $\\tilde{O}(nW/\\epsilon)$ time where $W$ is the\ntotal vertex weight (assuming integral vertex weights); in particular this\nyields an $\\tilde{O}(n^2/\\epsilon)$ time randomized algorithm for unweighted\ngraphs. This translates to a $\\tilde{O}(\\kappa nW)$ time exact algorithm where\n$\\kappa$ is the rooted connectivity. We build on this to obtain similar bounds\nfor global vertex connectivity.\n  Our results complement the known results for these problems in the low\nconnectivity regime due to work of Gabow [9] for edge connectivity from 1991,\nand the very recent work of Nanongkai et al. [24] and Forster et al. [7] for\nvertex connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 02:39:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""]]}, {"id": "2104.07381", "submitter": "David Issa Mattos", "authors": "David Issa Mattos, Lucas Ruud, Jan Bosch, Helena Holmstr\\\"om Olsson", "title": "On the Assessment of Benchmark Suites for Algorithm Comparison", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmark suites, i.e. a collection of benchmark functions, are widely used\nin the comparison of black-box optimization algorithms. Over the years,\nresearch has identified many desired qualities for benchmark suites, such as\ndiverse topology, different difficulties, scalability, representativeness of\nreal-world problems among others. However, while the topology characteristics\nhave been subjected to previous studies, there is no study that has\nstatistically evaluated the difficulty level of benchmark functions, how well\nthey discriminate optimization algorithms and how suitable is a benchmark suite\nfor algorithm comparison. In this paper, we propose the use of an item response\ntheory (IRT) model, the Bayesian two-parameter logistic model for multiple\nattempts, to statistically evaluate these aspects with respect to the empirical\nsuccess rate of algorithms. With this model, we can assess the difficulty level\nof each benchmark, how well they discriminate different algorithms, the ability\nscore of an algorithm, and how much information the benchmark suite adds in the\nestimation of the ability scores. We demonstrate the use of this model in two\nwell-known benchmark suites, the Black-Box Optimization Benchmark (BBOB) for\ncontinuous optimization and the Pseudo Boolean Optimization (PBO) for discrete\noptimization. We found that most benchmark functions of BBOB suite have high\ndifficulty levels (compared to the optimization algorithms) and low\ndiscrimination. For the PBO, most functions have good discrimination parameters\nbut are often considered too easy. We discuss potential uses of IRT in\nbenchmarking, including its use to improve the design of benchmark suites, to\nmeasure multiple aspects of the algorithms, and to design adaptive suites.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:20:11 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mattos", "David Issa", ""], ["Ruud", "Lucas", ""], ["Bosch", "Jan", ""], ["Olsson", "Helena Holmstr\u00f6m", ""]]}, {"id": "2104.07463", "submitter": "Tuukka Korhonen", "authors": "Tuukka Korhonen", "title": "A Single-Exponential Time 2-Approximation Algorithm for Treewidth", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm, that given an $n$-vertex graph $G$ and an integer $k$,\nin time $2^{O(k)} n$ either outputs a tree decomposition of $G$ of width at\nmost $2k + 1$ or determines that the treewidth of $G$ is larger than $k$. This\nis the first 2-approximation algorithm for treewidth that is faster than the\nknown exact algorithms. In particular, our algorithm improves upon both the\nprevious best approximation ratio of 5 in time $2^{O(k)} n$ and the previous\nbest approximation ratio of 3 in time $2^{O(k)} n^{O(1)}$, both given by\nBodlaender et al. [FOCS 2013, SICOMP 2016]. Our algorithm is based on a local\nimprovement method adapted from a proof of Bellenbaum and Diestel [Comb.\nProbab. Comput. 2002].\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:50:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 15:01:02 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Korhonen", "Tuukka", ""]]}, {"id": "2104.07487", "submitter": "Charles Argue", "authors": "C.J. Argue, Anupam Gupta, Marco Molinaro", "title": "Lipschitz Selectors may not Yield Competitive Algorithms for Convex Body\n  Chasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current best algorithms for convex body chasing problem in online\nalgorithms use the notion of the Steiner point of a convex set. In particular,\nthe algorithm which always moves to the Steiner point of the request set is\n$O(d)$ competitive for nested convex body chasing, and this is optimal among\nmemoryless algorithms [Bubeck et al. 2020]. A memoryless algorithm coincides\nwith the notion of a selector in functional analysis. The Steiner point is\nnoted for being Lipschitz with respect to the Hausdorff metric, and for\nachieving the minimal Lipschitz constant possible. It is natural to ask whether\nevery selector with this Lipschitz property yields a competitive algorithm for\nnested convex body chasing. We answer this question in the negative by\nexhibiting a selector which yields a non-competitive algorithm for nested\nconvex body chasing but is Lipschitz with respect to Hausdorff distance.\nFurthermore, we show that being Lipschitz with respect to an $L_p$-type analog\nto the Hausdorff distance is sufficient to guarantee competitiveness if and\nonly if $p=1$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:32:30 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Argue", "C. J.", ""], ["Gupta", "Anupam", ""], ["Molinaro", "Marco", ""]]}, {"id": "2104.07582", "submitter": "Maciej Besta", "authors": "Maciej Besta, Raghavendra Kanakagiri, Grzegorz Kwasniewski, Rachata\n  Ausavarungnirun, Jakub Ber\\'anek, Konstantinos Kanellopoulos, Kacper Janda,\n  Zur Vonarburg-Shmaria, Lukas Gianinazzi, Ioana Stefan, Juan G\\'omez Luna,\n  Marcin Copik, Lukas Kapp-Schwoerer, Salvatore Di Girolamo, Marek Konieczny,\n  Onur Mutlu, Torsten Hoefler", "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on\n  Processing-in-Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple graph algorithms such as PageRank have recently been the target of\nnumerous hardware accelerators. Yet, there also exist much more complex graph\nmining algorithms for problems such as clustering or maximal clique listing.\nThese algorithms are memory-bound and thus could be accelerated by hardware\ntechniques such as Processing-in-Memory (PIM). However, they also come with\nnon-straightforward parallelism and complicated memory access patterns. In this\nwork, we address this with a simple yet surprisingly powerful observation:\noperations on sets of vertices, such as intersection or union, form a large\npart of many complex graph mining algorithms, and can offer rich and simple\nparallelism at multiple levels. This observation drives our cross-layer design,\nin which we (1) expose set operations using a novel programming paradigm, (2)\nexpress and execute these operations efficiently with carefully designed\nset-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA\ninstructions. The key design idea is to alleviate the bandwidth needs of SISA\ninstructions by mapping set operations to two types of PIM: in-DRAM bulk\nbitwise computing for bitvectors representing high-degree vertices, and\nnear-memory logic layers for integer arrays representing low-degree vertices.\nSet-centric SISA-enhanced algorithms are efficient and outperform hand-tuned\nbaselines, offering more than 10x speedup over the established Bron-Kerbosch\nalgorithm for listing maximal cliques. We deliver more than 10 SISA set-centric\nalgorithm formulations, illustrating SISA's wide applicability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:37:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Besta", "Maciej", ""], ["Kanakagiri", "Raghavendra", ""], ["Kwasniewski", "Grzegorz", ""], ["Ausavarungnirun", "Rachata", ""], ["Ber\u00e1nek", "Jakub", ""], ["Kanellopoulos", "Konstantinos", ""], ["Janda", "Kacper", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Gianinazzi", "Lukas", ""], ["Stefan", "Ioana", ""], ["Luna", "Juan G\u00f3mez", ""], ["Copik", "Marcin", ""], ["Kapp-Schwoerer", "Lukas", ""], ["Di Girolamo", "Salvatore", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2104.07631", "submitter": "Cyrus Hettle", "authors": "Cyrus Hettle, Swati Gupta, Daniel Molzahn", "title": "Fair and Reliable Reconnections for Temporary Disruptions in Electric\n  Distribution Networks using Submodularity", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a distributed approach for automatically reconfiguring\ndistribution systems into an operational radial network after a fault occurs by\ncreating an ordering in which switches automatically close upon detection of a\ndownstream fault. The switches' reconnection ordering significantly impacts the\nexpected time to reconnect under normal disruptions and thus affects\nreliability metrics such as SAIDI and CAIDI, which are the basis for\nregulator-imposed financial incentives for performance.\n  We model the problem of finding a switch reconnection ordering that minimizes\nSAIDI and the expected reconnection time as Minimum Reconnection Time (MRT),\nwhich we show is a special case of the well-known minimum linear ordering\nproblem from the submodular optimization literature, and in particular the Min\nSum Set Cover problem (MSSC). We prove that MRT is also NP-hard.\n  We generalize the kernel-based rounding approaches of Bansal et al. for Min\nSum Vertex Cover to give tight approximation guarantees for MSSC on c-uniform\nhypergraphs for all c. For all instances of MSSC, our methods have a strictly\nbetter approximation ratio guarantee than the best possible methods for general\nMSSC.\n  Finally, we consider optimizing multiple metrics simultaneously using local\nsearch methods that also reconfigure the system's base tree to ensure fairness\nin service disruptions and reconnection times and reduce energy loss. We\ncomputationally validate our approach on the NREL SMART-DS Greensboro synthetic\nurban-suburban network. We evaluate the performance of our reconfiguration\nmethods and show significant reductions compared to single-metric-based\noptimizations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:48:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hettle", "Cyrus", ""], ["Gupta", "Swati", ""], ["Molzahn", "Daniel", ""]]}, {"id": "2104.07710", "submitter": "Samantha Chen", "authors": "Samantha Chen, Yusu Wang", "title": "Approximation algorithms for 1-Wasserstein distance between persistence\n  diagrams", "comments": "To be published in LIPIcs, Volume 190, SEA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed a tremendous growth using topological summaries,\nespecially the persistence diagrams (encoding the so-called persistent\nhomology) for analyzing complex shapes. Intuitively, persistent homology maps a\npotentially complex input object (be it a graph, an image, or a point set and\nso on) to a unified type of feature summary, called the persistence diagrams.\nOne can then carry out downstream data analysis tasks using such persistence\ndiagram representations. A key problem is to compute the distance between two\npersistence diagrams efficiently. In particular, a persistence diagram is\nessentially a multiset of points in the plane, and one popular distance is the\nso-called 1-Wasserstein distance between persistence diagrams. In this paper,\nwe present two algorithms to approximate the 1-Wasserstein distance for\npersistence diagrams in near-linear time. These algorithms primarily follow the\nsame ideas as two existing algorithms to approximate optimal transport between\ntwo finite point-sets in Euclidean spaces via randomly shifted quadtrees. We\nshow how these algorithms can be effectively adapted for the case of\npersistence diagrams. Our algorithms are much more efficient than previous\nexact and approximate algorithms, both in theory and in practice, and we\ndemonstrate its efficiency via extensive experiments. They are conceptually\nsimple and easy to implement, and the code is publicly available in github.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:27:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Samantha", ""], ["Wang", "Yusu", ""]]}, {"id": "2104.07898", "submitter": "Thatchaphol Saranurak", "authors": "Ruoxu Cen, Jason Li, Danupon Nanongkai, Debmalya Panigrahi,\n  Thatchaphol Saranurak", "title": "Minimum Cuts in Directed Graphs via $\\sqrt{n}$ Max-Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an algorithm to find a mincut in an $n$-vertex, $m$-edge weighted\ndirected graph using $\\tilde O(\\sqrt{n})$ calls to any maxflow subroutine.\nUsing state of the art maxflow algorithms, this yields a directed mincut\nalgorithm that runs in $\\tilde O(m\\sqrt{n} + n^2)$ time. This improves on the\n30 year old bound of $\\tilde O(mn)$ obtained by Hao and Orlin for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 05:47:57 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cen", "Ruoxu", ""], ["Li", "Jason", ""], ["Nanongkai", "Danupon", ""], ["Panigrahi", "Debmalya", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2104.07974", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, and Nidhi Purohit", "title": "Parameterized Complexity of Categorical Clustering with Size Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Categorical Clustering problem, we are given a set of vectors (matrix)\nA={a_1,\\ldots,a_n} over \\Sigma^m, where \\Sigma is a finite alphabet, and\nintegers k and B. The task is to partition A into k clusters such that the\nmedian objective of the clustering in the Hamming norm is at most B. That is,\nwe seek a partition {I_1,\\ldots,I_k} of {1,\\ldots,n} and vectors\nc_1,\\ldots,c_k\\in\\Sigma^m such that \\sum_{i=1}^k\\sum_{j\\in I_i}d_h(c_i,a_j)\\leq\nB, where d_H(a,b) is the Hamming distance between vectors a and b. Fomin,\nGolovach, and Panolan [ICALP 2018] proved that the problem is fixed-parameter\ntractable (for binary case \\Sigma={0,1}) by giving an algorithm that solves the\nproblem in time 2^{O(B\\log B)} (mn)^{O(1)}. We extend this algorithmic result\nto a popular capacitated clustering model, where in addition the sizes of the\nclusters should satisfy certain constraints. More precisely, in Capacitated\nClustering, in addition, we are given two non-negative integers p and q, and\nseek a clustering with p\\leq |I_i|\\leq q for all i\\in{1,\\ldots,k}. Our main\ntheorem is that Capacitated Clustering is solvable in time 2^{O(B\\log\nB)}|\\Sigma|^B(mn)^{O(1)}. The theorem not only extends the previous algorithmic\nresults to a significantly more general model, it also implies algorithms for\nseveral other variants of Categorical Clustering with constraints on cluster\nsizes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:49:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Purohit", "Nidhi", ""]]}, {"id": "2104.08003", "submitter": "Florent Foucaud", "authors": "Florent Foucaud, Herv\\'e Hocquard, Dimitri Lajou", "title": "Complexity and algorithms for injective edge-coloring in graphs", "comments": "12 pages, 5 figures", "journal-ref": "Information Processing Letters 170:106121, 2021", "doi": "10.1016/j.ipl.2021.106121", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An injective $k$-edge-coloring of a graph $G$ is an assignment of colors,\ni.e. integers in $\\{1, \\ldots , k\\}$, to the edges of $G$ such that any two\nedges each incident with one distinct endpoint of a third edge, receive\ndistinct colors. The problem of determining whether such a $k$-coloring exists\nis called k-INJECTIVE EDGE-COLORING. We show that 3-INJECTIVE EDGE-COLORING is\nNP-complete, even for triangle-free cubic graphs, planar subcubic graphs of\narbitrarily large girth, and planar bipartite subcubic graphs of girth~6.\n4-INJECTIVE EDGE-COLORING remains NP-complete for cubic graphs. For any $k\\geq\n45$, we show that k-INJECTIVE EDGE-COLORING remains NP-complete even for graphs\nof maximum degree at most $5\\sqrt{3k}$. In contrast with these negative\nresults, we show that \\InjPbName{k} is linear-time solvable on graphs of\nbounded treewidth. Moreover, we show that all planar bipartite subcubic graphs\nof girth at least~16 are injectively $3$-edge-colorable. In addition, any graph\nof maximum degree at most $\\sqrt{k/2}$ is injectively $k$-edge-colorable.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:56:52 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Foucaud", "Florent", ""], ["Hocquard", "Herv\u00e9", ""], ["Lajou", "Dimitri", ""]]}, {"id": "2104.08107", "submitter": "Tobias Heuer", "authors": "Lars Gottesb\\\"uren and Tobias Heuer and Peter Sanders and Sebastian\n  Schlag", "title": "Shared-Memory n-level Hypergraph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a shared-memory algorithm to compute high-quality solutions to the\nbalanced $k$-way hypergraph partitioning problem. This problem asks for a\npartition of the vertex set into $k$ disjoint blocks of bounded size that\nminimizes the connectivity metric (i.e., the sum of the number of different\nblocks connected by each hyperedge). High solution quality is achieved by\nparallelizing the core technique of the currently best sequential partitioner\nKaHyPar: the most extreme $n$-level version of the widely used multilevel\nparadigm, where only a single vertex is contracted on each level. This approach\nis made fast and scalable through intrusive algorithms and data structures that\nallow precise control of parallelism through atomic operations and fine-grained\nlocking. We perform extensive experiments on more than 500 real-world\nhypergraphs with up to $140$ million vertices and two billion pins (sum of\nhyperedge sizes). We find that our algorithm computes solutions that are on par\nwith a comparable configuration of KaHyPar while being an order of magnitude\nfaster on average. Moreover, we show that recent non-multilevel algorithms\nspecifically designed to partition large instances have considerable quality\npenalties and no clear advantage in running time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:23:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Heuer", "Tobias", ""], ["Sanders", "Peter", ""], ["Schlag", "Sebastian", ""]]}, {"id": "2104.08163", "submitter": "Peter Bloem", "authors": "Peter Bloem", "title": "Finding Motifs in Knowledge Graphs using Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a method to find network motifs in knowledge graphs. Network\nmotifs are useful patterns or meaningful subunits of the graph that recur\nfrequently. We extend the common definition of a network motif to coincide with\na basic graph pattern. We introduce an approach, inspired by recent work for\nsimple graphs, to induce these from a given knowledge graph, and show that the\nmotifs found reflect the basic structure of the graph. Specifically, we show\nthat in random graphs, no motifs are found, and that when we insert a motif\nartificially, it can be detected. Finally, we show the results of motif\ninduction on three real-world knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:20:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Bloem", "Peter", ""]]}, {"id": "2104.08501", "submitter": "Jakub Tetek", "authors": "Jakub T\\v{e}tek", "title": "Approximate Triangle Counting via Sampling and Fast Matrix\n  Multiplication", "comments": "Improved presentation, many minor edits, improved comparison to\n  related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a trivial $O(\\frac{n^3}{T})$ time algorithm for approximate triangle\ncounting where $T$ is the number of triangles in the graph and $n$ the number\nof vertices. At the same time, one may count triangles exactly using fast\nmatrix multiplication in time $\\tilde{O}(n^\\omega)$. Is it possible to get a\nnegative dependency on the number of triangles $T$ while retaining the\n$n^\\omega$ dependency on $n$? We answer this question positively by providing\nan algorithm which runs in time $O\\big(\\frac{n^\\omega}{T^{\\omega - 2}}\\big)\n\\cdot \\text{poly}(n^{o(1)}/\\epsilon)$. This is optimal in the sense that as\nlong as the exponent of $T$ is independent of $n, T$, it cannot be improved\nwhile retaining the dependency on $n$; this as follows from the lower bound of\nEden and Rosenbaum [APPROX/RANDOM 2018]. Our algorithm improves upon the state\nof the art when $T = \\omega(1)$ and $T = o(n)$.\n  We also consider the problem of approximate triangle counting in sparse\ngraphs, parameterizing by the number of edges $m$. The best known algorithm\nruns in time $\\tilde{O}\\big(\\frac{m^{3/2}}{T}\\big)$ [Eden et al., SIAM Journal\non Computing, 2017]. There is also a well known algorithm for exact triangle\ncounting that runs in time $\\tilde{O}(m^{2\\omega/(\\omega + 1)})$. We again get\nan algorithm that retains the exponent of $m$ while running faster on graphs\nwith larger number of triangles. Specifically, our algorithm runs in time\n$O\\Big(\\frac{m^{2\\omega/(\\omega+1)}}{ T^{2(\\omega-1)/(\\omega+1)}}\\Big) \\cdot\n\\text{poly}(n^{o(1)}/\\epsilon)$. This is again optimal in the sense that if the\nexponent of $T$ is to be constant, it cannot be improved without worsening the\ndependency on $m$. This algorithm improves upon the state of the art when $T =\n\\omega(1)$ and $T = o(\\sqrt{m})$.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 09:51:00 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 18:47:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["T\u011btek", "Jakub", ""]]}, {"id": "2104.08751", "submitter": "Dominik K\\\"oppl", "authors": "Tomohiro I and Dominik K\\\"oppl", "title": "Load-Balancing Succinct B Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a B tree representation storing $n$ keys, each of $k$ bits, in\neither (a) $nk + O(nk / \\lg n)$ bits or (b) $nk + O(nk \\lg \\lg n/ \\lg n)$ bits\nof space supporting all B tree operations in either (a) $O(\\lg n )$ time or (b)\n$O(\\lg n / \\lg \\lg n)$ time, respectively. We can augment each node with an\naggregate value such as the minimum value within its subtree, and maintain\nthese aggregate values within the same space and time complexities. Finally, we\ngive the sparse suffix tree as an application, and present a linear-time\nalgorithm computing the sparse longest common prefix array from the suffix AVL\ntree of Irving et al. [JDA'2003].\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:22:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["I", "Tomohiro", ""], ["K\u00f6ppl", "Dominik", ""]]}, {"id": "2104.08865", "submitter": "Jim Apple", "authors": "Jim Apple", "title": "HalftimeHash: Modern Hashing without 64-bit Multipliers or Finite Fields", "comments": "To be published in the proceedings of the 17th Algorithm and Data\n  Structures Symposium (WADS) 2021. Code available at\n  https://github.com/jbapple/HalftimeHash", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HalftimeHash is a new algorithm for hashing long strings. The goals are few\ncollisions (different inputs that produce identical output hash values) and\nhigh performance.\n  Compared to the fastest universal hash functions on long strings (clhash and\nUMASH) HalftimeHash decreases collision probability while also increasing\nperformance by over 50%, exceeding 16 bytes per cycle. In addition,\nHalftimeHash does not use any widening 64-bit multiplications or any finite\nfield arithmetic that could limit its portability.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 14:19:26 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 02:47:56 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Apple", "Jim", ""]]}, {"id": "2104.08949", "submitter": "Maciej Pacut", "authors": "Maciej Pacut, Juan Vanerio, Vamsi Addanki, Arash Pourdamghani, Gabor\n  Retvari, Stefan Schmid", "title": "Online List Access with Precedence Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a natural generalization of the online list access\nproblem in the paid exchange model, where additionally there can be precedence\nconstraints (\"dependencies\") among the nodes in the list. For example, this\ngeneralization is motivated by applications in the context of packet\nclassification. Our main contributions are constant-competitive deterministic\nand randomized online algorithms, designed around a procedure\nMove-Recursively-Forward, a generalization of Move-To-Front tailored to handle\nnode dependencies. Parts of the analysis build upon ideas of the classic online\nalgorithms Move-To-Front and BIT, and address the challenges of the extended\nmodel. We further discuss the challenges related to insertions and deletions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 20:03:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pacut", "Maciej", ""], ["Vanerio", "Juan", ""], ["Addanki", "Vamsi", ""], ["Pourdamghani", "Arash", ""], ["Retvari", "Gabor", ""], ["Schmid", "Stefan", ""]]}, {"id": "2104.09096", "submitter": "Thomas Hayes", "authors": "Varsha Dani, Aayush Gupta, Thomas P. Hayes, Seth Pettie", "title": "Wake Up and Join Me! An Energy-Efficient Algorithm for Maximal Matching\n  in Radio Networks", "comments": "14 pages, 2 figures, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks of small, autonomous devices that communicate with each\nother wirelessly. Minimizing energy usage is an important consideration in\ndesigning algorithms for such networks, as battery life is a crucial and\nlimited resource. Working in a model where both sending and listening for\nmessages deplete energy, we consider the problem of finding a maximal matching\nof the nodes in a radio network of arbitrary and unknown topology.\n  We present a distributed randomized algorithm that produces, with high\nprobability, a maximal matching. The maximum energy cost per node is $O(\\log^2\nn)$, where $n$ is the size of the network. The total latency of our algorithm\nis $O(n \\log n)$ time steps. We observe that there exist families of network\ntopologies for which both of these bounds are simultaneously optimal up to\npolylog factors, so any significant improvement will require additional\nassumptions about the network topology.\n  We also consider the related problem of assigning, for each node in the\nnetwork, a neighbor to back up its data in case of node failure. Here, a key\ngoal is to minimize the maximum load, defined as the number of nodes assigned\nto a single node. We present a decentralized low-energy algorithm that finds a\nneighbor assignment whose maximum load is at most a polylog($n$) factor bigger\nthat the optimum.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:37:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dani", "Varsha", ""], ["Gupta", "Aayush", ""], ["Hayes", "Thomas P.", ""], ["Pettie", "Seth", ""]]}, {"id": "2104.09178", "submitter": "Tamas Kis", "authors": "P\\'eter Gy\\\"ogyi, Tam\\'as Kis, T\\'imea Tam\\'asi, J\\'ozsef B\\'ek\\'esi", "title": "Joint replenishment meets scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a combination of the joint replenishment problem\n(JRP) and single machine scheduling with release dates. There is a single\nmachine and one or more item types. Each job has a release date, a positive\nprocessing time, and it requires a subset of items. A job can be started at\ntime $t$ only if all the required item types were replenished between the\nrelease date of the job and time point $t$. The ordering of item types for\ndistinct jobs can be combined. The objective is to minimize the total ordering\ncost plus a scheduling criterion, such as total weighted completion time or\nmaximum flow time, where the cost of ordering a subset of items simultaneously\nis the sum of a joint ordering cost, and an additional item ordering cost for\neach item type in the subset. We provide several complexity results for the\noffline problem, and competitive analysis for online variants with min-sum and\nmin-max criteria, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 10:08:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gy\u00f6gyi", "P\u00e9ter", ""], ["Kis", "Tam\u00e1s", ""], ["Tam\u00e1si", "T\u00edmea", ""], ["B\u00e9k\u00e9si", "J\u00f3zsef", ""]]}, {"id": "2104.09312", "submitter": "Igor Averbakh", "authors": "Igor Averbakh", "title": "Minimizing the total weighted pairwise connection time in network\n  construction problems", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is required to find an optimal order of constructing the edges of a\nnetwork so as to minimize the sum of the weighted connection times of relevant\npairs of vertices. Construction can be performed anytime anywhere in the\nnetwork, with a fixed overall construction speed. The problem is strongly\nNP-hard even on stars. We present polynomial algorithms for the problem on\ntrees with a fixed number of leaves, and on general networks with a fixed\nnumber of relevant pairs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:51:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Averbakh", "Igor", ""]]}, {"id": "2104.09413", "submitter": "Andrii Arman", "authors": "Andrii Arman, Pu Gao, Nicholas Wormald", "title": "Linear-time uniform generation of random sparse contingency tables with\n  specified marginals", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm that generates a uniformly random contingency table with\nspecified marginals, i.e. a matrix with non-negative integer values and\nspecified row and column sums. Such algorithms are useful in statistics and\ncombinatorics. When $\\Delta^4< M/5$, where $\\Delta$ is the maximum of the row\nand column sums and $M$ is the sum of all entries of the matrix, our algorithm\nruns in time linear in $M$ in expectation. Most previously published algorithms\nfor this problem are approximate samplers based on Markov chain Monte Carlo,\nwhose provable bounds on the mixing time are typically polynomials with rather\nlarge degrees.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:03:24 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 20:02:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Arman", "Andrii", ""], ["Gao", "Pu", ""], ["Wormald", "Nicholas", ""]]}, {"id": "2104.09417", "submitter": "Georgios Chatzigeorgakidis", "authors": "Georgios Chatzigeorgakidis, Dimitrios Skoutas, Kostas Patroumpas,\n  Themis Palpanas, Spiros Athanasiou, and Spiros Skiadopoulos", "title": "Local Pair and Bundle Discovery over Co-Evolving Time Series", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Time series exploration and mining has many applications across several\nindustrial and scientific domains. In this paper, we consider the problem of\ndetecting locally similar pairs and groups, called bundles, over co-evolving\ntime series. These are pairs or groups of subsequences whose values do not\ndiffer by more than {\\epsilon} for at least delta consecutive timestamps, thus\nindicating common local patterns and trends. We first present a baseline\nalgorithm that performs a sweep line scan across all timestamps to identify\nmatches. Then, we propose a filter-verification technique that only examines\ncandidate matches at judiciously chosen checkpoints across time. Specifically,\nwe introduce two block scanning algorithms for discovering local pairs and\nbundles respectively, which leverage the potential of checkpoints to\naggressively prune the search space. We experimentally evaluate our methods\nagainst real-world and synthetic datasets, demonstrating a speed-up in\nexecution time by an order of magnitude over the baseline. This paper has been\npublished in the 16th International Symposium on Spatial and Temporal Databases\n(SSTD19).\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:10:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chatzigeorgakidis", "Georgios", ""], ["Skoutas", "Dimitrios", ""], ["Patroumpas", "Kostas", ""], ["Palpanas", "Themis", ""], ["Athanasiou", "Spiros", ""], ["Skiadopoulos", "Spiros", ""]]}, {"id": "2104.09665", "submitter": "Allen Liu", "authors": "Allen Liu, Ankur Moitra", "title": "Learning GMMs with Nearly Optimal Robustness Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we solve the problem of robustly learning a high-dimensional\nGaussian mixture model with $k$ components from $\\epsilon$-corrupted samples up\nto accuracy $\\widetilde{O}(\\epsilon)$ in total variation distance for any\nconstant $k$ and with mild assumptions on the mixture. This robustness\nguarantee is optimal up to polylogarithmic factors. At the heart of our\nalgorithm is a new way to relax a system of polynomial equations which\ncorresponds to solving an improper learning problem where we are allowed to\noutput a Gaussian mixture model whose weights are low-degree polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:14:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2104.09734", "submitter": "Pasin Manurangsi", "authors": "Alisa Chang, Badih Ghazi, Ravi Kumar, Pasin Manurangsi", "title": "Locally Private k-Means in One Round", "comments": "35 pages. To appear in ICML'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an approximation algorithm for k-means clustering in the one-round\n(aka non-interactive) local model of differential privacy (DP). This algorithm\nachieves an approximation ratio arbitrarily close to the best non private\napproximation algorithm, improving upon previously known algorithms that only\nguarantee large (constant) approximation ratios. Furthermore, this is the first\nconstant-factor approximation algorithm for k-means that requires only one\nround of communication in the local DP model, positively resolving an open\nquestion of Stemmer (SODA 2020). Our algorithmic framework is quite flexible;\nwe demonstrate this by showing that it also yields a similar near-optimal\napproximation algorithm in the (one-round) shuffle DP model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:07:31 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 05:27:59 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chang", "Alisa", ""], ["Ghazi", "Badih", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2104.09803", "submitter": "Sebastian Berndt", "authors": "Sebastian Berndt, Kilian Grage, Klaus Jansen, Lukas Johannsen, Maria\n  Kosche", "title": "Robust Online Algorithms for Dynamic Choosing Problems", "comments": "CIE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-online algorithms that are allowed to perform a bounded amount of\nrepacking achieve guaranteed good worst-case behaviour in a more realistic\nsetting. Most of the previous works focused on minimization problems that aim\nto minimize some costs. In this work, we study maximization problems that aim\nto maximize their profit.\n  We mostly focus on a class of problems that we call choosing problems, where\na maximum profit subset of a set objects has to be maintained. Many known\nproblems, such as Knapsack, MaximumIndependentSet and variations of these, are\npart of this class. We present a framework for choosing problems that allows us\nto transfer offline $\\alpha$-approximation algorithms into\n$(\\alpha-epsilon)$-competitive semi-online algorithms with amortized migration\n$O(1/\\epsilon)$. Moreover we complement these positive results with lower\nbounds that show that our results are tight in the sense that no amortized\nmigration of $o(1/\\epsilon)$ is possible.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:32:01 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Berndt", "Sebastian", ""], ["Grage", "Kilian", ""], ["Jansen", "Klaus", ""], ["Johannsen", "Lukas", ""], ["Kosche", "Maria", ""]]}, {"id": "2104.09950", "submitter": "Fahad Panolan", "authors": "Akanksha Agrawal, Lawqueen Kanesh, Daniel Lokshtanov, Fahad Panolan,\n  M. S. Ramanujan, Saket Saurabh", "title": "Elimination Distance to Topological-minor-free Graphs is FPT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the literature on parameterized graph problems, there has been an\nincreased effort in recent years aimed at exploring novel notions of graph\nedit-distance that are more powerful than the size of a modulator to a specific\ngraph class. In this line of research, Bulian and Dawar [Algorithmica, 2016]\nintroduced the notion of elimination distance and showed that deciding whether\na given graph has elimination distance at most $k$ to any minor-closed class of\ngraphs is fixed-parameter tractable parameterized by $k$ [Algorithmica, 2017].\nThere has been a subsequent series of results on the fixed-parameter\ntractability of elimination distance to various graph classes. However, one\nclass of graph classes to which the computation of elimination distance has\nremained open is the class of graphs that are characterized by the exclusion of\na family ${\\cal F}$ of finite graphs as topological minors.\n  In this paper, we settle this question by showing that the problem of\ndetermining elimination distance to such graphs is also fixed-parameter\ntractable.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:22:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Kanesh", "Lawqueen", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "2104.09982", "submitter": "David Naccache", "authors": "Leon M\\\"achler, David Naccache", "title": "Explaining the Entombed Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \\cite{entombed}, John Aycock and Tara Copplestone pose an open question,\nnamely the explanation of the mysterious lookup table used in the Entombed\nGame's Algorithm for two dimensional maze generation. The question attracted\nmedia attention (BBC etc) and was open until today. This paper answers this\nquestion, explains the algorithm and even extends it to three dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:07:19 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 22:38:43 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["M\u00e4chler", "Leon", ""], ["Naccache", "David", ""]]}, {"id": "2104.10124", "submitter": "Robert Bredereck", "authors": "Niclas Boehmer, Robert Bredereck, Du\\v{s}an Knop, Junjie Luo", "title": "Finding Small Multi-Demand Set Covers with Ubiquitous Elements and Large\n  Sets is Fixed-Parameter Tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of Set Cover where each element of the universe has some\ndemand that determines how many times the element needs to be covered.\nMoreover, we examine two generalizations of this problem when a set can be\nincluded multiple times and when sets have different prices. We prove that all\nthree problems are fixed-parameter tractable with respect to the combined\nparameter budget, the maximum number of elements missing in one of the sets,\nand the maximum number of sets in which one of the elements does not occur.\nLastly, we point out how our fixed-parameter tractability algorithm can be\napplied in the context of bribery for the (collective-decision) group\nidentification problem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:12:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Boehmer", "Niclas", ""], ["Bredereck", "Robert", ""], ["Knop", "Du\u0161an", ""], ["Luo", "Junjie", ""]]}, {"id": "2104.10402", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Roberto Trani", "title": "PTHash: Revisiting FCH Minimal Perfect Hashing", "comments": "Accepted to SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462849", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a set $S$ of $n$ distinct keys, a function $f$ that bijectively maps\nthe keys of $S$ into the range $\\{0,\\ldots,n-1\\}$ is called a minimal perfect\nhash function for $S$. Algorithms that find such functions when $n$ is large\nand retain constant evaluation time are of practical interest; for instance,\nsearch engines and databases typically use minimal perfect hash functions to\nquickly assign identifiers to static sets of variable-length keys such as\nstrings. The challenge is to design an algorithm which is efficient in three\ndifferent aspects: time to find $f$ (construction time), time to evaluate $f$\non a key of $S$ (lookup time), and space of representation for $f$. Several\nalgorithms have been proposed to trade-off between these aspects. In 1992, Fox,\nChen, and Heath (FCH) presented an algorithm at SIGIR providing very fast\nlookup evaluation. However, the approach received little attention because of\nits large construction time and higher space consumption compared to other\nsubsequent techniques. Almost thirty years later we revisit their framework and\npresent an improved algorithm that scales well to large sets and reduces space\nconsumption altogether, without compromising the lookup time. We conduct an\nextensive experimental assessment and show that the algorithm finds functions\nthat are competitive in space with state-of-the art techniques and provide\n$2-4\\times$ better lookup time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:22:07 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 08:58:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Trani", "Roberto", ""]]}, {"id": "2104.10593", "submitter": "Daniel Paulusma", "authors": "Christoph Brause and Petr Golovach and Barnaby Martin and Pascal Ochem\n  and Dani\\\"el Paulusma and Siani Smith", "title": "Acyclic, Star, and Injective Colouring: Bounding the Diameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effect of bounding the diameter for well-studied variants of\nthe Colouring problem. A colouring is acyclic, star, or injective if any two\ncolour classes induce a forest, star forest or disjoint union of vertices and\nedges, respectively. The corresponding decision problems are Acyclic Colouring,\nStar Colouring and Injective Colouring. The last problem is also known as\n$L(1,1)$-Labelling and we also consider the framework of $L(a,b)$-Labelling. We\nprove a number of (almost-)complete complexity classifications. In particular,\nwe show that for graphs of diameter at most $d$, Acyclic $3$-Colouring is\npolynomial-time solvable if $d\\leq 2$ but NP-complete if $d\\geq 4$, and Star\n$3$-Colouring is polynomial-time solvable if $d\\leq 3$ but NP-complete for\n$d\\geq 8$. As far as we are aware, Star $3$-Colouring is the first problem that\nexhibits a complexity jump for some $d\\geq 3$. Our third main result is that\n$L(1,2)$-Labelling is NP-complete for graphs of diameter $2$; we relate the\nlatter problem to a special case of Hamiltonian Path.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:45:07 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:16:03 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 13:43:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Brause", "Christoph", ""], ["Golovach", "Petr", ""], ["Martin", "Barnaby", ""], ["Ochem", "Pascal", ""], ["Paulusma", "Dani\u00ebl", ""], ["Smith", "Siani", ""]]}, {"id": "2104.10668", "submitter": "Eugene Kogan", "authors": "Eugene Kogan", "title": "On the rank of Z_2-matrices with free entries on the diagonal", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For an $n \\times n$ matrix $M$ with entries in $\\mathbb{Z}_2$ denote by\n$R(M)$ the minimal rank of all the matrices obtained by changing some numbers\non the main diagonal of $M$. We prove that for each non-negative integer $k$\nthere is a polynomial in $n$ algorithm deciding whether $R(M) \\leq k$ (whose\ncomplexity may depend on $k$). We also give a polynomial in $n$ algorithm\ncomputing a number $m$ such that $m/2 \\leq R(M) \\leq m$. These results have\napplications to graph drawings on non-orientable surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:47:15 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kogan", "Eugene", ""]]}, {"id": "2104.10939", "submitter": "Panagiotis Bouros", "authors": "George Christodoulou and Panagiotis Bouros and Nikos Mamoulis", "title": "HINT: A Hierarchical Index for Intervals in Main Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing intervals is a fundamental problem, finding a wide range of\napplications. Recent work on managing large collections of intervals in main\nmemory focused on overlap joins and temporal aggregation problems. In this\npaper, we propose novel and efficient in-memory indexing techniques for\nintervals, with a focus on interval range queries, which are a basic component\nof many search and analysis tasks. First, we propose an optimized version of a\nsingle-level (flat) domain-partitioning approach, which may have large space\nrequirements due to excessive replication. Then, we propose a hierarchical\npartitioning approach, which assigns each interval to at most two partitions\nper level and has controlled space requirements. Novel elements of our\ntechniques include the division of the intervals at each partition into groups\nbased on whether they begin inside or before the partition boundaries, reducing\nthe information stored at each partition to the absolutely necessary, and the\neffective handling of data sparsity and skew. Experimental results on real and\nsynthetic interval sets of different characteristics show that our approaches\nare typically one order of magnitude faster than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:10:31 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Christodoulou", "George", ""], ["Bouros", "Panagiotis", ""], ["Mamoulis", "Nikos", ""]]}, {"id": "2104.11420", "submitter": "Arun Kumar Das", "authors": "Sergio Cabello and Arun Kumar Das and Sandip Das and Joydeep Mukherjee", "title": "Finding a Largest-Area Triangle in a Terrain in Near-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A terrain is an $x$-monotone polygon whose lower boundary is a single line\nsegment. We present an algorithm to find in a terrain a triangle of largest\narea in $O(n \\log n)$ time, where $n$ is the number of vertices defining the\nterrain. The best previous algorithm for this problem has a running time of\n$O(n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 05:35:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Cabello", "Sergio", ""], ["Das", "Arun Kumar", ""], ["Das", "Sandip", ""], ["Mukherjee", "Joydeep", ""]]}, {"id": "2104.11618", "submitter": "Pascal Lenzner", "authors": "Wilhelm Friedemann, Tobias Friedrich, Hans Gawendowicz, Pascal\n  Lenzner, Anna Melnichenko, Jannik Peters, Daniel Stephan, Michael Vaichenker", "title": "Efficiency and Stability in Euclidean Network Design", "comments": "To appear at the 33rd ACM Symposium on Paralellism in Algorithms and\n  Architectures (SPAA), full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Design problems typically ask for a minimum cost sub-network from a\ngiven host network. This classical point-of-view assumes a central authority\nenforcing the optimum solution. But how should networks be designed to cope\nwith selfish agents that own parts of the network? In this setting, agents will\ndeviate from a minimum cost network if this decreases their individual cost.\nHence, designed networks should be both efficient in terms of total cost and\nstable in terms of the agents' willingness to accept the network.\n  We study this novel type of Network Design problem by investigating the\ncreation of $(\\beta,\\gamma)$-networks, that are in $\\beta$-approximate Nash\nequilibrium and have a total cost of at most $\\gamma$ times the optimal cost,\nfor the recently proposed Euclidean Generalized Network Creation Game by Bil\\`o\net al. [SPAA 2019]. There, $n$ agents corresponding to points in Euclidean\nspace create costly edges among themselves to optimize their centrality in the\ncreated network. Our main result is a simple $\\mathcal{O}(n^2)$-time algorithm\nthat computes a $(\\beta,\\beta)$-network with low $\\beta$ for any given set of\npoints. Moreover, on integer grid point sets or random point sets our algorithm\nachieves a low constant $\\beta$. Besides these results, we discuss a\ngeneralization of our algorithm to instances with arbitrary, even non-metric,\nedge lengths. Moreover, we show that no such positive results are possible when\nfocusing on either optimal networks or perfectly stable networks as in both\ncases NP-hard problems arise, there exist instances with very unstable optimal\nnetworks, and there are instances for perfectly stable networks with high total\ncost. Along the way, we significantly improve several results from Bil\\`o et\nal. and we asymptotically resolve their conjecture about the Price of Anarchy\nby providing a tight bound.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:12:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Friedemann", "Wilhelm", ""], ["Friedrich", "Tobias", ""], ["Gawendowicz", "Hans", ""], ["Lenzner", "Pascal", ""], ["Melnichenko", "Anna", ""], ["Peters", "Jannik", ""], ["Stephan", "Daniel", ""], ["Vaichenker", "Michael", ""]]}, {"id": "2104.11670", "submitter": "Nitzan Tur", "authors": "Roy Schwartz, Nitzan Tur", "title": "The Metric Relaxation for $0$-Extension Admits an\n  $\\Omega(\\log^{2/3}{k})$ Gap", "comments": "27 pages, 3 figures, will appear in STOC 2021", "journal-ref": null, "doi": "10.1145/3406325.3451071", "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $0$-Extension problem, where we are given an undirected graph\n$\\mathcal{G}=(V,E)$ equipped with non-negative edge weights $w:E\\rightarrow\n\\mathbb{R}^+$, a collection $ T=\\{ t_1,\\ldots,t_k\\}\\subseteq V$ of $k$ special\nvertices called terminals, and a semi-metric $D$ over $T$. The goal is to\nassign every non-terminal vertex to a terminal while minimizing the sum over\nall edges of the weight of the edge multiplied by the distance in $D$ between\nthe terminals to which the endpoints of the edge are assigned. $0$-Extension\nadmits two known algorithms, achieving approximations of $O(\\log{k})$\n[C{\\u{a}}linescu-Karloff-Rabani SICOMP '05] and $O(\\log{k}/\\log{\\log{k}})$\n[Fakcharoenphol-Harrelson-Rao-Talwar SODA '03]. Both known algorithms are based\non rounding a natural linear programming relaxation called the metric\nrelaxation, in which $D$ is extended from $T$ to the entire of $V$. The current\nbest known integrality gap for the metric relaxation is $\\Omega\n(\\sqrt{\\log{k}})$. In this work we present an improved integrality gap of\n$\\Omega(\\log^{\\frac{2}{3}}k)$ for the metric relaxation. Our construction is\nbased on the randomized extension of one graph by another, a notion that\ncaptures lifts of graphs as a special case and might be of independent\ninterest. Inspired by algebraic topology, our analysis of the gap instance is\nbased on proving no continuous section (in the topological sense) exists in the\nrandomized extension.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 15:53:06 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Schwartz", "Roy", ""], ["Tur", "Nitzan", ""]]}, {"id": "2104.12101", "submitter": "Steffan S{\\o}lvsten", "authors": "Steffan Christ S{\\o}lvsten, Jaco van de Pol, Anna Blume Jakobsen and\n  Mathias Weller Berg Thomasen", "title": "Efficient Binary Decision Diagram Manipulation in External Memory", "comments": "32 pages, 11 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We follow up on the idea of Lars Arge to rephrase the Reduce and Apply\nalgorithms of Binary Decision Diagrams (BDDs) as iterative I/O-efficient\nalgorithms. We identify multiple avenues to improve the performance of his\nproposed algorithms and extend the technique to other basic BDD algorithms.\n  These algorithms are implemented in a new BDD library, named Adiar. We see\nvery promising results when comparing the performance of Adiar with\nconventional BDD libraries that use recursive depth-first algorithms. For\ninstances of about 50 GiB, our algorithms, using external memory, are only up\nto 3.9 times slower compared to Sylvan, exclusively using internal memory. Yet,\nour proposed techniques are able to obtain this performance at a fraction of\nthe internal memory needed by Sylvan to function. Furthermore, with Adiar we\nare able to manipulate BDDs that outgrow main memory and so surpass the limits\nof the other BDD libraries.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:34:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:15:15 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["S\u00f8lvsten", "Steffan Christ", ""], ["van de Pol", "Jaco", ""], ["Jakobsen", "Anna Blume", ""], ["Thomasen", "Mathias Weller Berg", ""]]}, {"id": "2104.12235", "submitter": "Bastian Rieck", "authors": "Bastian Rieck", "title": "Basic Analysis of Bin-Packing Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bin-packing problem continues to remain relevant in numerous application\nareas. This technical report discusses the empirical performance of different\nbin-packing heuristics for certain test problems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:22:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rieck", "Bastian", ""]]}, {"id": "2104.12286", "submitter": "Sam Barr", "authors": "Sam Barr and Therese Biedl", "title": "Efficiently Partitioning the Edges of a 1-Planar Graph into a Planar\n  Graph and a Forest", "comments": "15 Pages, 2 figures, 6 pseudocodes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  1-planar graphs are graphs that can be drawn in the plane such that any edge\nintersects with at most one other edge. Ackerman showed that the edges of a\n1-planar graph can be partitioned into a planar graph and a forest, and claims\nthat the proof leads to a linear time algorithm. However, it is not clear how\none would obtain such an algorithm from his proof. In this paper, we first\nreprove Ackerman's result (in fact, we prove a slightly more general statement)\nand then show that the split can be found in linear time by using an\nedge-contraction data structure by Holm et al.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 23:36:57 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 20:38:11 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Barr", "Sam", ""], ["Biedl", "Therese", ""]]}, {"id": "2104.12337", "submitter": "Pedro Matias", "authors": "Michael T. Goodrich, Siddharth Gupta, Hadi Khodabandeh, Pedro Matias", "title": "How to Catch Marathon Cheaters: New Approximation Algorithms for\n  Tracking Paths", "comments": "Full version of WADS 2021 conference proceedings paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph, $G$, and vertices, $s$ and $t$ in $G$, the\ntracking paths problem is that of finding the smallest subset of vertices in\n$G$ whose intersection with any $s$-$t$ path results in a unique sequence. This\nproblem is known to be NP-complete and has applications to animal migration\ntracking and detecting marathon course-cutting, but its approximability is\nlargely unknown. In this paper, we address this latter issue, giving novel\nalgorithms having approximation ratios of $(1+\\epsilon)$, $O(\\lg OPT)$ and\n$O(\\lg n)$, for $H$-minor-free, general, and weighted graphs, respectively. We\nalso give a linear kernel for $H$-minor-free graphs and make improvements to\nthe quadratic kernel for general graphs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 04:00:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Gupta", "Siddharth", ""], ["Khodabandeh", "Hadi", ""], ["Matias", "Pedro", ""]]}, {"id": "2104.12523", "submitter": "Andr\\'e Nusser", "authors": "David Coudert, Andr\\'e Nusser, Laurent Viennot", "title": "Enumeration of Far-Apart Pairs by Decreasing Distance for Faster\n  Hyperbolicity Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperbolicity is a graph parameter which indicates how much the shortest-path\ndistance metric of a graph deviates from a tree metric. It is used in various\nfields such as networking, security, and bioinformatics for the classification\nof complex networks, the design of routing schemes, and the analysis of graph\nalgorithms. Despite recent progress, computing the hyperbolicity of a graph\nremains challenging. Indeed, the best known algorithm has time complexity\n$O(n^{3.69})$, which is prohibitive for large graphs, and the most efficient\nalgorithms in practice have space complexity $O(n^2)$. Thus, time as well as\nspace are bottlenecks for computing hyperbolicity.\n  In this paper, we design a tool for enumerating all far-apart pairs of a\ngraph by decreasing distances. A node pair $(u, v)$ of a graph is far-apart if\nboth $v$ is a leaf of all shortest-path trees rooted at $u$ and $u$ is a leaf\nof all shortest-path trees rooted at $v$. This notion was previously used to\ndrastically reduce the computation time for hyperbolicity in practice. However,\nit required the computation of the distance matrix to sort all pairs of nodes\nby decreasing distance, which requires an infeasible amount of memory already\nfor medium-sized graphs. We present a new data structure that avoids this\nmemory bottleneck in practice and for the first time enables computing the\nhyperbolicity of several large graphs that were far out-of-reach using previous\nalgorithms. For some instances, we reduce the memory consumption by at least\ntwo orders of magnitude. Furthermore, we show that for many graphs, only a very\nsmall fraction of far-apart pairs have to be considered for the hyperbolicity\ncomputation, explaining this drastic reduction of memory.\n  As iterating over far-apart pairs in decreasing order without storing them\nexplicitly is a very general tool, we believe that our approach might also be\nrelevant to other problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:46:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Coudert", "David", ""], ["Nusser", "Andr\u00e9", ""], ["Viennot", "Laurent", ""]]}, {"id": "2104.12946", "submitter": "Taisuke Yasuda", "authors": "Yi Li, David P. Woodruff, Taisuke Yasuda", "title": "Exponentially Improved Dimensionality Reduction for $\\ell_1$: Subspace\n  Embeddings and Independence Testing", "comments": "To appear in COLT 2021; v2: minor fixes for camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite many applications, dimensionality reduction in the $\\ell_1$-norm is\nmuch less understood than in the Euclidean norm. We give two new oblivious\ndimensionality reduction techniques for the $\\ell_1$-norm which improve\nexponentially over prior ones:\n  1. We design a distribution over random matrices $S \\in \\mathbb{R}^{r \\times\nn}$, where $r = 2^{\\textrm{poly}(d/(\\varepsilon \\delta))}$, such that given any\nmatrix $A \\in \\mathbb{R}^{n \\times d}$, with probability at least $1-\\delta$,\nsimultaneously for all $x$, $\\|SAx\\|_1 = (1 \\pm \\varepsilon)\\|Ax\\|_1$. Note\nthat $S$ is linear, does not depend on $A$, and maps $\\ell_1$ into $\\ell_1$.\nOur distribution provides an exponential improvement on the previous best known\nmap of Wang and Woodruff (SODA, 2019), which required $r = 2^{2^{\\Omega(d)}}$,\neven for constant $\\varepsilon$ and $\\delta$. Our bound is optimal, up to a\npolynomial factor in the exponent, given a known $2^{\\sqrt d}$ lower bound for\nconstant $\\varepsilon$ and $\\delta$.\n  2. We design a distribution over matrices $S \\in \\mathbb{R}^{k \\times n}$,\nwhere $k = 2^{O(q^2)}(\\varepsilon^{-1} q \\log d)^{O(q)}$, such that given any\n$q$-mode tensor $A \\in (\\mathbb{R}^{d})^{\\otimes q}$, one can estimate the\nentrywise $\\ell_1$-norm $\\|A\\|_1$ from $S(A)$. Moreover, $S = S^1 \\otimes S^2\n\\otimes \\cdots \\otimes S^q$ and so given vectors $u_1, \\ldots, u_q \\in\n\\mathbb{R}^d$, one can compute $S(u_1 \\otimes u_2 \\otimes \\cdots \\otimes u_q)$\nin time $2^{O(q^2)}(\\varepsilon^{-1} q \\log d)^{O(q)}$, which is much faster\nthan the $d^q$ time required to form $u_1 \\otimes u_2 \\otimes \\cdots \\otimes\nu_q$. Our linear map gives a streaming algorithm for independence testing using\nspace $2^{O(q^2)}(\\varepsilon^{-1} q \\log d)^{O(q)}$, improving the previous\ndoubly exponential $(\\varepsilon^{-1} \\log d)^{q^{O(q)}}$ space bound of\nBraverman and Ostrovsky (STOC, 2010).\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:35:49 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:40:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Li", "Yi", ""], ["Woodruff", "David P.", ""], ["Yasuda", "Taisuke", ""]]}, {"id": "2104.13097", "submitter": "Michael Lampis", "authors": "Michael Lampis", "title": "Minimum Stable Cut and Treewidth", "comments": "Full version of ICALP 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stable cut of a graph is a cut whose weight cannot be increased by changing\nthe side of a single vertex. Equivalently, a cut is stable if all vertices have\nthe (weighted) majority of their neighbors on the other side. In this paper we\nstudy Min Stable Cut, the problem of finding a stable cut of minimum weight,\nwhich is closely related to the Price of Anarchy of the Max Cut game. Since\nthis problem is NP-hard, we study its complexity on graphs of low treewidth,\nlow degree, or both. We show that the problem is weakly NP-hard on severely\nrestricted trees, so bounding treewidth alone cannot make it tractable. We\nmatch this with a pseudo-polynomial DP algorithm running in time $(\\Delta\\cdot\nW)^{O(tw)}n^{O(1)}$, where $tw$ is the treewidth, $\\Delta$ the maximum degree,\nand $W$ the maximum weight. On the other hand, bounding $\\Delta$ is also not\nenough, as the problem is NP-hard for unweighted graphs of bounded degree. We\ntherefore parameterize Min Stable Cut by both $tw+\\Delta$ and obtain an FPT\nalgorithm running in time $2^{O(\\Delta tw)}(n+\\log W)^{O(1)}$. Our main result\nis to provide a reduction showing that both aforementioned algorithms are\nessentially optimal, even if we replace treewidth by pathwidth: if there exists\nan algorithm running in $(nW)^{o(pw)}$ or $2^{o(\\Delta pw)}(n+\\log W)^{O(1)}$,\nthen the ETH is false. Complementing this, we show that we can obtain an FPT\napproximation scheme parameterized by treewidth, if we consider almost-stable\nsolutions.\n  Motivated by these mostly negative results, we consider Unweighted Min Stable\nCut. Here our results already imply a much faster exact algorithm running in\ntime $\\Delta^{O(tw)}n^{O(1)}$. We show that this is also probably essentially\noptimal: an algorithm running in $n^{o(pw)}$ would contradict the ETH.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:42:04 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lampis", "Michael", ""]]}, {"id": "2104.13098", "submitter": "Christian Schulz", "authors": "Eugenio Angriman, Henning Meyerhenke, Christian Schulz and Bora\n  U\\c{c}ar", "title": "Fully-dynamic Weighted Matching Approximation in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding large or heavy matchings in graphs is a ubiquitous combinatorial\noptimization problem. In this paper, we engineer the first non-trivial\nimplementations for approximating the dynamic weighted matching problem. Our\nfirst algorithm is based on random walks/paths combined with dynamic\nprogramming. The second algorithm has been introduced by Stubbs and Williams\nwithout an implementation. Roughly speaking, their algorithm uses dynamic\nunweighted matching algorithms as a subroutine (within a multilevel approach);\nthis allows us to use previous work on dynamic unweighted matching algorithms\nas a black box in order to obtain a fully-dynamic weighted matching algorithm.\nWe empirically study the algorithms on an extensive set of dynamic instances\nand compare them with optimal weighted matchings. Our experiments show that the\nrandom walk algorithm typically fares much better than Stubbs/Williams\n(regarding the time/quality tradeoff), and its results are often not far from\nthe optimum.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:42:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Angriman", "Eugenio", ""], ["Meyerhenke", "Henning", ""], ["Schulz", "Christian", ""], ["U\u00e7ar", "Bora", ""]]}, {"id": "2104.13209", "submitter": "Mohammad Almasri", "authors": "Mohammad Almasri, Izzat El Hajj, Rakesh Nagi, Jinjun Xiong, Wen-mei\n  Hwu", "title": "K-Clique Counting on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting k-cliques in a graph is an important problem in graph analysis with\nmany applications. Counting k-cliques is typically done by traversing search\ntrees starting at each vertex in the graph. An important optimization is to\neliminate search tree branches that discover the same clique redundantly.\nEliminating redundant clique discovery is typically done via graph orientation\nor pivoting. Parallel implementations for both of these approaches have\ndemonstrated promising performance on CPUs. In this paper, we present our GPU\nimplementations of k-clique counting for both the graph orientation and\npivoting approaches. Our implementations explore both vertex-centric and\nedge-centric parallelization schemes, and replace recursive search tree\ntraversal with iterative traversal based on an explicitly-managed shared stack.\nWe also apply various optimizations to reduce memory consumption and improve\nthe utilization of parallel execution resources. Our evaluation shows that our\nbest GPU implementation outperforms the best state-of-the-art parallel CPU\nimplementation by a geometric mean speedup of 12.39x, 6.21x, and 18.99x for k =\n4, 7, and 10, respectively. We also evaluate the impact of the choice of\nparallelization scheme and the incremental speedup of each optimization. Our\ncode will be open-sourced to enable further research on parallelizing k-clique\ncounting on GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:18:03 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Almasri", "Mohammad", ""], ["Hajj", "Izzat El", ""], ["Nagi", "Rakesh", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2104.13362", "submitter": "Arka Ray", "authors": "Arka Ray", "title": "There is no APTAS for 2-dimensional vector bin packing: Revisited", "comments": "11 pages, LIPIcs format, changes: fixed typos, added vector bin\n  covering result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study vector bin packing and vector bin covering problems,\nmultidimensional generalizations of the classical bin packing and bin covering\nproblems, respectively. In Vector Bin Packing we are given a set of\n$d$-dimensional vectors from $(0,1]^d$, and the aim is to partition the set\ninto the minimum number of bins such that for each bin $B$, we have\n$\\left\\|\\sum_{v\\in B}v\\right\\|_\\infty\\leq 1$. Woeginger [Woe97] claimed that\nthe problem has no APTAS for dimensions greater than or equal to 2. We note\nthat there was a slight oversight in the original proof. Hence, we give a\nrevised proof using some additional ideas from [BCKS06, CC09]. In fact, we show\nthat it is NP-hard to get an asymptotic approximation ratio better than\n$\\frac{600}{599}$, for $d=2$.\n  In the vector bin covering problem given a set of $d$-dimensional vectors\nfrom $(0,1]^d$, the aim is to obtain a family of disjoint subsets (called bins)\nwith the maximum cardinality such that for each bin $B$, we have $\\sum_{v\\in\nB}v\\geq \\mathbf 1$. Using ideas similar to our vector bin packing result, we\nshow that for vector bin covering there is no APTAS for dimensions greater than\nor equal to 2. In fact, we show that it is NP-hard to get an asymptotic\napproximation ratio better than $\\frac{998}{997}$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:43:33 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 18:35:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ray", "Arka", ""]]}, {"id": "2104.13457", "submitter": "Sebastian Wild", "authors": "J. Ian Munro and Patrick K. Nicholson and Louisa Seelbach Benkner and\n  Sebastian Wild", "title": "Hypersuccinct Trees -- New universal tree source codes for optimal\n  compressed tree data structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new universal source code for unlabeled binary and ordinal trees\nthat achieves asymptotically optimal compression for all tree sources covered\nby existing universal codes. At the same time, it supports answering many\nnavigational queries on the compressed representation in constant time on the\nword-RAM; this is not known to be possible for any existing tree compression\nmethod. The resulting data structures, \"hypersuccinct trees\", hence combine the\ncompression achieved by the best known universal codes with the operation\nsupport of the best succinct tree data structures. Compared to prior work on\nsuccinct data structures, we do not have to tailor our data structure to\nspecific applications; hypersuccinct trees automatically adapt to the trees at\nhand. We show that it simultaneously achieves the asymptotically optimal space\nusage for a wide range of distributions over tree shapes, including: random\nbinary search trees (BSTs) / Cartesian trees of random arrays, random\nfringe-balanced BSTs, binary trees with a given number of binary/unary/leaf\nnodes, random binary tries generated from memoryless sources, full binary\ntrees, unary paths, as well as uniformly chosen weight-balanced BSTs, AVL\ntrees, and left-leaning red-black trees. Using hypersuccinct trees, we further\nobtain the first data structure that answers range-minimum queries on a random\npermutation of $n$ elements in constant time and using the optimal $1.736n +\no(n)$ bits on average, solving an open problem of Davoodi et al. (2014) and\nGolin et al. (2016).\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:11:47 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Munro", "J. Ian", ""], ["Nicholson", "Patrick K.", ""], ["Benkner", "Louisa Seelbach", ""], ["Wild", "Sebastian", ""]]}, {"id": "2104.13681", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate, Cosmin Bonchis and Adrian Craciun", "title": "Kernelization, Proof Complexity and Social Choice", "comments": "Revised version will appear in the Proceedings of ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.GT cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We display an application of the notions of kernelization and data reduction\nfrom parameterized complexity to proof complexity: Specifically, we show that\nthe existence of data reduction rules for a parameterized problem having (a). a\nsmall-length reduction chain, and (b). small-size (extended) Frege proofs\ncertifying the soundness of reduction steps implies the existence of\nsubexponential size (extended) Frege proofs for propositional formalizations of\nthe given problem.\n  We apply our result to infer the existence of subexponential Frege and\nextended Frege proofs for a variety of problems. Improving earlier results of\nAisenberg et al. (ICALP 2015), we show that propositional formulas expressing\n(a stronger form of) the Kneser-Lov\\'asz Theorem have polynomial size Frege\nproofs for each constant value of the parameter k. Previously only\nquasipolynomial bounds were known (and only for the ordinary Kneser-Lov\\'asz\nTheorem).\n  Another notable application of our framework is to impossibility results in\ncomputational social choice: we show that, for any fixed number of agents,\npropositional translations of the Arrow and Gibbard-Satterthwaite impossibility\ntheorems have subexponential size Frege proofs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:09:14 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Istrate", "Gabriel", ""], ["Bonchis", "Cosmin", ""], ["Craciun", "Adrian", ""]]}, {"id": "2104.13851", "submitter": "Simon Robillard", "authors": "Robin E{\\ss}mann, Tobias Nipkow, Simon Robillard", "title": "Verified Approximation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first formal verification of approximation algorithms for\nNP-complete optimization problems: vertex cover, independent set, set cover,\nload balancing, and bin packing. We uncover incompletenesses in existing proofs\nand improve the approximation ratio in one case.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:56:16 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["E\u00dfmann", "Robin", ""], ["Nipkow", "Tobias", ""], ["Robillard", "Simon", ""]]}, {"id": "2104.13860", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Micha{\\l} D\\k{e}bski, Marta Piecyk, Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Faster 3-coloring of small-diameter graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the 3-\\textsc{Coloring} problem in graphs with small diameter. In\n2013, Mertzios and Spirakis showed that for $n$-vertex diameter-2 graphs this\nproblem can be solved in subexponential time $2^{\\mathcal{O}(\\sqrt{n \\log\nn})}$. Whether the problem can be solved in polynomial time remains a\nwell-known open question in the area of algorithmic graphs theory.\n  In this paper we present an algorithm that solves 3-\\textsc{Coloring} in\n$n$-vertex diameter-2 graphs in time $2^{\\mathcal{O}(n^{1/3} \\log^{2} n)}$.\nThis is the first improvement upon the algorithm of Mertzios and Spirakis in\nthe general case, i.e., without putting any further restrictions on the\ninstance graph.\n  In addition to standard branchings and reducing the problem to an instance of\n2-\\textsc{Sat}, the crucial building block of our algorithm is a combinatorial\nobservation about 3-colorable diameter-2 graphs, which is proven using a\nprobabilistic argument.\n  As a side result, we show that 3-\\textsc{Coloring} can be solved in time\n$2^{\\mathcal{O}( (n \\log n)^{2/3})}$ in $n$-vertex diameter-3 graphs. We also\ngeneralize our algorithms to the problem of finding a list homomorphism from a\nsmall-diameter graph to a cycle.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:11:28 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["D\u0119bski", "Micha\u0142", ""], ["Piecyk", "Marta", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "2104.14404", "submitter": "Stefan Steinerberger", "authors": "Stefan Steinerberger", "title": "A 0.502$\\cdot$MaxCut Approximation using Quadratic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the MaxCut problem for graphs $G=(V,E)$. The problem is NP-hard,\nthere are two main approximation algorithms with theoretical guarantees: (1)\nthe Goemans \\& Williamson algorithm uses semi-definite programming to provide a\n0.878MaxCut approximation (which, if the Unique Games Conjecture is true, is\nthe best that can be done in polynomial time) and (2) Trevisan proposed an\nalgorithm using spectral graph theory from which a 0.614MaxCut approximation\ncan be obtained. We discuss a new approach using a specific quadratic program\nand prove that its solution can be used to obtain at least a 0.502MaxCut\napproximation. The algorithm seems to perform well in practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:18:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Steinerberger", "Stefan", ""]]}, {"id": "2104.14436", "submitter": "Ashwin Nayak", "authors": "Ashwin Nayak", "title": "Deterministic Algorithms for the Hidden Subgroup Problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we present deterministic algorithms for the Hidden Subgroup\nProblem. The algorithm for abelian groups achieves the same asymptotic query\ncomplexity as the optimal randomized algorithm. The algorithm for non-abelian\ngroups comes within a polylogarithmic factor of the optimal randomized query\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:55:15 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Nayak", "Ashwin", ""]]}, {"id": "2104.14476", "submitter": "Yiming Zhao", "authors": "Haitao Wang and Yiming Zhao", "title": "Reverse Shortest Path Problem for Unit-Disk Graphs", "comments": "A preliminary version to appear in WADS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set P of n points in the plane, a unit-disk graph G_{r}(P) with\nrespect to a radius r is an undirected graph whose vertex set is P such that an\nedge connects two points p, q \\in P if the Euclidean distance between p and q\nis at most r. The length of any path in G_r(P) is the number of edges of the\npath. Given a value \\lambda>0 and two points s and t of P, we consider the\nfollowing reverse shortest path problem: finding the smallest r such that the\nshortest path length between s and t in G_r(P) is at most \\lambda. It was known\npreviously that the problem can be solved in O(n^{4/3} \\log^3 n) time. In this\npaper, we present an algorithm of O(\\lfloor \\lambda \\rfloor \\cdot n \\log n)\ntime and another algorithm of O(n^{5/4} \\log^2 n) time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:38:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wang", "Haitao", ""], ["Zhao", "Yiming", ""]]}, {"id": "2104.14510", "submitter": "Yixin Cao", "authors": "Yixin Cao and Yuping Ke", "title": "Improved Kernels for Edge Modification Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In an edge modification problem, we are asked to modify at most $k$ edges to\na given graph to make the graph satisfy a certain property. Depending on the\noperations allowed, we have the completion problems and the edge deletion\nproblems. A great amount of efforts have been devoted to understanding the\nkernelization complexity of these problems. We revisit several well-studied\nedge modification problems, and develop improved kernels for them:\n  \\begin{itemize}\n  \\item a $2 k$-vertex kernel for the cluster edge deletion problem,\n  \\item a $3 k^2$-vertex kernel for the trivially perfect completion problem,\n  \\item a $5 k^{1.5}$-vertex kernel for the split completion problem and the\nsplit edge deletion problem, and\n  \\item a $5 k^{1.5}$-vertex kernel for the pseudo-split completion problem and\nthe pseudo-split edge deletion problem.\n  \\end{itemize}\n  Moreover, our kernels for split completion and pseudo-split completion have\nonly $O(k^{2.5})$ edges. Our results also include a $2 k$-vertex kernel for the\nstrong triadic closure problem, which is related to cluster edge deletion.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:22:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Cao", "Yixin", ""], ["Ke", "Yuping", ""]]}, {"id": "2104.14680", "submitter": "Haitao Wang", "authors": "Logan Pedersen and Haitao Wang", "title": "Algorithms for the Line-Constrained Disk Coverage and Related Problems", "comments": "A preliminary version to appear in WADS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ points and a set $S$ of $m$ weighted disks in the\nplane, the disk coverage problem asks for a subset of disks of minimum total\nweight that cover all points of $P$. The problem is NP-hard. In this paper, we\nconsider a line-constrained version in which all disks are centered on a line\n$L$ (while points of $P$ can be anywhere in the plane). We present an\n$O((m+n)\\log(m+n)+\\kappa\\log m)$ time algorithm for the problem, where $\\kappa$\nis the number of pairs of disks that intersect. Alternatively, we can also\nsolve the problem in $O(nm\\log(m+n))$ time. For the unit-disk case where all\ndisks have the same radius, the running time can be reduced to\n$O((n+m)\\log(m+n))$. In addition, we solve in $O((m+n)\\log(m+n))$ time the\n$L_{\\infty}$ and $L_1$ cases of the problem, in which the disks are squares and\ndiamonds, respectively. As a by-product, the 1D version of the problem where\nall points of $P$ are on $L$ and the disks are line segments on $L$ is also\nsolved in $O((m+n)\\log(m+n))$ time. We also show that the problem has an\n$\\Omega((m+n)\\log (m+n))$ time lower bound even for the 1D case.\n  We further demonstrate that our techniques can also be used to solve other\ngeometric coverage problems. For example, given in the plane a set $P$ of $n$\npoints and a set $S$ of $n$ weighted half-planes, we solve in $O(n^4\\log n)$\ntime the problem of finding a subset of half-planes to cover $P$ so that their\ntotal weight is minimized. This improves the previous best algorithm of\n$O(n^5)$ time by almost a linear factor. If all half-planes are lower ones,\nthen our algorithm runs in $O(n^2\\log n)$ time, which improves the previous\nbest algorithm of $O(n^4)$ time by almost a quadratic factor.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:19:33 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Pedersen", "Logan", ""], ["Wang", "Haitao", ""]]}, {"id": "2104.14818", "submitter": "Philippe Blaettchen", "authors": "Philippe Blaettchen, Andre P. Calmon, Georgina Hall", "title": "Traceability Technology Adoption in Supply Chain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modern traceability technologies promise to improve supply chain management\nby simplifying recalls, increasing demand visibility, or ascertaining\nsustainable supplier practices. Managers in the traceability initiatives\ndeveloping such technologies face a difficult question: which companies should\nthey target as early adopters to ensure that their technology is broadly\nemployed? To answer this question, managers must consider that supply chains\nare interlinked in complex networks and that a supply chain effect is inherent\nto traceability technologies. More specifically, the benefits obtained from\ntraceability are conditional on technology adoption throughout a product's\nsupply chain.\n  We introduce a model of the dynamics of traceability technology adoption in\nsupply chain networks to tackle the problem of selecting the smallest set of\nearly adopters guaranteeing broad dissemination. Our model builds on extant\ndiffusion models while incorporating that a firm's adoption decision depends on\nprevious adoption decisions throughout its supply chains. We show that the\nproblem is NP-hard and that no approximation within a polylogarithmic factor\ncan be guaranteed for any polynomial-time algorithm. Nevertheless, we introduce\nan algorithm that identifies an exact solution in polynomial time under certain\nassumptions on the network structure and provide evidence that it is tractable\nfor real-world supply chain networks. We further propose a random generative\nmodel that outputs networks consistent with real-world supply chain networks.\nThe networks obtained display, whp, structures that allow us to find the\noptimal seed set in subexponential time using our algorithm. Our generative\nmodel also provides approximate seed sets when information on the network is\nlimited.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:05:21 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Blaettchen", "Philippe", ""], ["Calmon", "Andre P.", ""], ["Hall", "Georgina", ""]]}, {"id": "2104.14962", "submitter": "Yuncong Yu", "authors": "Yuncong Yu, Dylan Kruyff, Tim Becker, Michael Behrisch", "title": "PSEUDo: Interactive Pattern Search in Multivariate Time Series with\n  Locality-Sensitive Hashing and Relevance Feedback", "comments": "11 pages including 2 pages for references, 10 figures including 1\n  teaser figure, sumbitted to IEEE VIS 2021, gitlab repository\n  https://git.science.uu.nl/vig/sublinear-algorithms-for-va/locality-sensitive-hashing-visual-analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PSEUDo, an adaptive feature learning technique for exploring\nvisual patterns in multi-track sequential data. Our approach is designed with\nthe primary focus to overcome the uneconomic retraining requirements and\ninflexible representation learning in current deep learning-based systems.\nMulti-track time series data are generated on an unprecedented scale due to\nincreased sensors and data storage. These datasets hold valuable patterns, like\nin neuromarketing, where researchers try to link patterns in multivariate\nsequential data from physiological sensors to the purchase behavior of products\nand services. But a lack of ground truth and high variance make automatic\npattern detection unreliable. Our advancements are based on a novel query-aware\nlocality-sensitive hashing technique to create a feature-based representation\nof multivariate time series windows. Most importantly, our algorithm features\nsub-linear training and inference time. We can even accomplish both the\nmodeling and comparison of 10,000 different 64-track time series, each with 100\ntime steps (a typical EEG dataset) under 0.8 seconds. This performance gain\nallows for a rapid relevance feedback-driven adaption of the underlying pattern\nsimilarity model and enables the user to modify the speed-vs-accuracy trade-off\ngradually. We demonstrate superiority of PSEUDo in terms of efficiency,\naccuracy, and steerability through a quantitative performance comparison and a\nqualitative visual quality comparison to the state-of-the-art algorithms in the\nfield. Moreover, we showcase the usability of PSEUDo through a case study\ndemonstrating our visual pattern retrieval concepts in a large meteorological\ndataset. We find that our adaptive models can accurately capture the user's\nnotion of similarity and allow for an understandable exploratory visual pattern\nretrieval in large multivariate time series datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:00:44 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:04:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "Yuncong", ""], ["Kruyff", "Dylan", ""], ["Becker", "Tim", ""], ["Behrisch", "Michael", ""]]}, {"id": "2104.15003", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov, Nikita Koval, Petr Kuznetsov", "title": "Memory-Optimality for Non-Blocking Containers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A bounded container maintains a collection of elements that can be inserted\nand extracted as long as the number of stored elements does not exceed the\npredefined capacity. We consider the concurrent implementations of a bounded\ncontainer more or less memory-friendly depending on how much memory they use in\naddition to storing the elements. This way, memory-optimal implementation\nemploys the minimal possible memory overhead and ensures that data and metadata\nare stored most compactly, reducing cache misses and unnecessary memory\nreclamation.\n  In this paper, we show that non-blocking implementations of a large class of\nelement-independent bounded containers, including queues, stacks, and pools,\nmust incur linear in the number of concurrent processes memory overhead. In the\nspecial cases when the ABA problem is excluded, e.g., by assuming that the\nhardware supports LL/SC instructions or by storing distinct elements, we show\nthat the lower bound can be circumvented, by presenting lock-free bounded\nqueues with constant memory overhead. For the general case, we present a\nCAS-based bounded queue implementation that asymptotically matches our lower\nbound. We believe that these results open a new research avenue devoted to the\nmemory-optimality phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:51:48 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Koval", "Nikita", ""], ["Kuznetsov", "Petr", ""]]}, {"id": "2104.15075", "submitter": "Jiehua Chen", "authors": "Jiehua Chen, Martin Lackner, Jan Maly", "title": "Participatory Budgeting with Donations and Diversity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CC cs.DS cs.GT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Participatory budgeting (PB) is a democratic process where citizens jointly\ndecide on how to allocate public funds to indivisible projects. This paper\nfocuses on PB processes where citizens may give additional money to projects\nthey want to see funded. We introduce a formal framework for this kind of PB\nwith donations. Our framework also allows for diversity constraints, meaning\nthat each project belongs to one or more types, and there are lower and upper\nbounds on the number of projects of the same type that can be funded. We\npropose three general classes of methods for aggregating the citizens'\npreferences in the presence of donations and analyze their axiomatic\nproperties. Furthermore, we investigate the computational complexity of\ndetermining the outcome of a PB process with donations and of finding a\ncitizen's optimal donation strategy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:48:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chen", "Jiehua", ""], ["Lackner", "Martin", ""], ["Maly", "Jan", ""]]}]