[{"id": "1907.00033", "submitter": "David Harris", "authors": "Alessandra Graf, David G. Harris, Penny Haxell", "title": "Algorithms for weighted independent transversals and strong colouring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An independent transversal (IT) in a graph with a given vertex partition is\nan independent set consisting of one vertex in each partition class. Several\nsufficient conditions are known for the existence of an IT in a given graph\nwith a given vertex partition, which have been used over the years to solve\nmany combinatorial problems. Some of these IT existence theorems have\nalgorithmic proofs, but there remains a gap between the best bounds given by\nnonconstructive results, and those obtainable by efficient algorithms.\n  Recently, Graf and Haxell (2018) described a new (deterministic) algorithm\nthat asymptotically closes this gap, but there are limitations on its\napplicability. In this paper we develop a randomized version of this algorithm\nthat is much more widely applicable, and demonstrate its use by giving\nefficient algorithms for two problems concerning the strong chromatic number of\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 18:36:58 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 21:44:32 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 18:19:32 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 14:04:14 GMT"}, {"version": "v5", "created": "Sun, 8 Nov 2020 15:44:45 GMT"}, {"version": "v6", "created": "Mon, 24 May 2021 00:58:40 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Graf", "Alessandra", ""], ["Harris", "David G.", ""], ["Haxell", "Penny", ""]]}, {"id": "1907.00117", "submitter": "Saba Ahmadi", "authors": "Saba Ahmadi, Sainyam Galhotra, Samir Khuller, Barna Saha and Roy\n  Schwartz", "title": "Min-Max Correlation Clustering via MultiCut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation clustering is a fundamental combinatorial optimization problem\narising in many contexts and applications that has been the subject of dozens\nof papers in the literature. In this problem we are given a general weighted\ngraph where each edge is labeled positive or negative. The goal is to obtain a\npartitioning (clustering) of the vertices that minimizes disagreements - weight\nof negative edges trapped inside a cluster plus positive edges between\ndifferent clusters. Most of the papers on this topic mainly focus on minimizing\ntotal disagreement, a global objective for this problem. In this paper, we\nstudy a cluster-wise objective function that asks to minimize the maximum\nnumber of disagreements of each cluster, which we call min-max correlation\nclustering. The min-max objective is a natural objective that respects the\nquality of every cluster. In this paper, we provide the first nontrivial\napproximation algorithm for this problem achieving an $\\mathcal{O}(\\sqrt{\\log\nn\\cdot\\max\\{\\log(|E^-|),\\log(k)\\}})$ approximation for general weighted graphs,\nwhere $|E^-|$ denotes the number of negative edges and $k$ is the number of\nclusters in the optimum solution. To do so, we also obtain a corresponding\nresult for multicut where we wish to find a multicut solution while trying to\nminimize the total weight of cut edges on every component. The results are then\nfurther improved to obtain (i) $\\mathcal{O}(r^2)$-approximation for min-max\ncorrelation clustering and min-max multicut for graphs that exclude $K_{r,r}$\nminors (ii) a 14-approximation for the min-max correlation clustering on\ncomplete graphs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 23:32:06 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ahmadi", "Saba", ""], ["Galhotra", "Sainyam", ""], ["Khuller", "Samir", ""], ["Saha", "Barna", ""], ["Schwartz", "Roy", ""]]}, {"id": "1907.00141", "submitter": "Alireza Heidari", "authors": "Alireza Heidari, Ihab F. Ilyas, Theodoros Rekatsinas", "title": "Approximate Inference in Structured Instances with Noisy Categorical\n  Observations", "comments": "UAI 2019, 33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering the latent ground truth labeling of a\nstructured instance with categorical random variables in the presence of noisy\nobservations. We present a new approximate algorithm for graphs with\ncategorical variables that achieves low Hamming error in the presence of noisy\nvertex and edge observations. Our main result shows a logarithmic dependency of\nthe Hamming error to the number of categories of the random variables. Our\napproach draws connections to correlation clustering with a fixed number of\nclusters. Our results generalize the works of Globerson et al. (2015) and\nFoster et al. (2018), who study the hardness of structured prediction under\nbinary labels, to the case of categorical labels.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 04:15:33 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 02:30:56 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Heidari", "Alireza", ""], ["Ilyas", "Ihab F.", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "1907.00203", "submitter": "David B. Blumenthal", "authors": "David B. Blumenthal, Johann Gamper, S\\'ebastien Bougleux, Luc Brun", "title": "Upper Bounding the Graph Edit Distance Based on Rings and Machine\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1142/S0218001421510083", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph edit distance (GED) is a flexible distance measure which is widely\nused for inexact graph matching. Since its exact computation is NP-hard,\nheuristics are used in practice. A popular approach is to obtain upper bounds\nfor GED via transformations to the linear sum assignment problem with\nerror-correction (LSAPE). Typically, local structures and distances between\nthem are employed for carrying out this transformation, but recently also\nmachine learning techniques have been used. In this paper, we formally define a\nunifying framework LSAPE-GED for transformations from GED to LSAPE. We also\nintroduce rings, a new kind of local structures designed for graphs where most\ninformation resides in the topology rather than in the node labels.\nFurthermore, we propose two new ring based heuristics RING and RING-ML, which\ninstantiate LSAPE-GED using the traditional and the machine learning based\napproach for transforming GED to LSAPE, respectively. Extensive experiments\nshow that using rings for upper bounding GED significantly improves the state\nof the art on datasets where most information resides in the graphs'\ntopologies. This closes the gap between fast but rather inaccurate LSAPE based\nheuristics and more accurate but significantly slower GED algorithms based on\nlocal search.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 13:28:09 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 07:59:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Blumenthal", "David B.", ""], ["Gamper", "Johann", ""], ["Bougleux", "S\u00e9bastien", ""], ["Brun", "Luc", ""]]}, {"id": "1907.00236", "submitter": "Nikita Ivkin", "authors": "Nikita Ivkin, Edo Liberty, Kevin Lang, Zohar Karnin and Vladimir\n  Braverman", "title": "Streaming Quantiles Algorithms with Small Space and Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating quantiles and distributions over streaming data has been\nstudied for roughly two decades now. Recently, Karnin, Lang, and Liberty\nproposed the first asymptotically optimal algorithm for doing so. This\nmanuscript complements their theoretical result by providing a practical\nvariants of their algorithm with improved constants. For a given sketch size,\nour techniques provably reduce the upper bound on the sketch error by a factor\nof two. These improvements are verified experimentally. Our modified quantile\nsketch improves the latency as well by reducing the worst case update time from\n$O(1/\\varepsilon)$ down to $O(\\log (1/\\varepsilon))$. We also suggest two\nalgorithms for weighted item streams which offer improved asymptotic update\ntimes compared to na\\\"ive extensions. Finally, we provide a specialized data\nstructure for these sketches which reduces both their memory footprints and\nupdate times.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 16:37:33 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ivkin", "Nikita", ""], ["Liberty", "Edo", ""], ["Lang", "Kevin", ""], ["Karnin", "Zohar", ""], ["Braverman", "Vladimir", ""]]}, {"id": "1907.00253", "submitter": "Evgenii Safronov", "authors": "Evgenii Safronov, Michael Vilzmann, Dzmitry Tsetserukou, Konstantin\n  Kondak", "title": "Asynchronous Behavior Trees with Memory aimed at Aerial Vehicles with\n  Redundancy in Flight Controller", "comments": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2019), IEEE copyright", "journal-ref": null, "doi": "10.1109/IROS40897.2019.8967928", "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex aircraft systems are becoming a target for automation. For successful\noperation, they require both efficient and readable mission execution system.\nFlight control computer (FCC) units, as well as all important subsystems, are\noften duplicated. Discrete nature of mission execution systems does not allow\nsmall differences in data flow among redundant FCCs which are acceptable for\ncontinuous control algorithms. Therefore, mission state consistency has to be\nspecifically maintained. We present a novel mission execution system which\nincludes FCC state synchronization. To achieve this result we developed a new\nconcept of Asynchronous Behavior Tree with Memory and proposed a state\nsynchronization algorithm. The implemented system was tested and proven to work\nin a real-time simulation of High Altitude Pseudo Satellite (HAPS) mission.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 18:36:04 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 11:05:42 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Safronov", "Evgenii", ""], ["Vilzmann", "Michael", ""], ["Tsetserukou", "Dzmitry", ""], ["Kondak", "Konstantin", ""]]}, {"id": "1907.00278", "submitter": "Oliver Serang", "authors": "Patrick Kreitzberg and Kyle Lucke and Oliver Serang", "title": "Most abundant isotope peaks and efficient selection on $Y=X_1+X_2+\\cdots\n  + X_m$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The isotope masses and relative abundances for each element are fundamental\nchemical knowledge. Computing the isotope masses of a compound and their\nrelative abundances is an important and difficult analytical chemistry problem.\nWe demonstrate that this problem is equivalent to sorting\n$Y=X_1+X_2+\\cdots+X_m$. We introduce a novel, practically efficient method for\ncomputing the top values in $Y$. then demonstrate the applicability of this\nmethod by computing the most abundant isotope masses (and their abundances)\nfrom compounds of nontrivial size.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 21:09:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kreitzberg", "Patrick", ""], ["Lucke", "Kyle", ""], ["Serang", "Oliver", ""]]}, {"id": "1907.00289", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Lorenzo Orecchia", "title": "Conjugate Gradients and Accelerated Methods Unified: The Approximate\n  Duality Gap View", "comments": "8 pages. v1 -> v2: corrected a reference to the paper with Nemirovski\n  acceleration with line search. v2 -> v3: updated affiliations, corrected a\n  few typos on p.7 and added an acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note provides a novel, simple analysis of the method of conjugate\ngradients for the minimization of convex quadratic functions. In contrast with\nstandard arguments, our proof is entirely self-contained and does not rely on\nthe existence of Chebyshev polynomials. Another advantage of our development is\nthat it clarifies the relation between the method of conjugate gradients and\ngeneral accelerated methods for smooth minimization by unifying their analyses\nwithin the framework of the Approximate Duality Gap Technique that was\nintroduced by the authors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 22:40:35 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 22:39:23 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 18:06:44 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1907.00317", "submitter": "Hao-Ting Wei", "authors": "Pei-Chuan Chen, Erik D. Demaine, Chung-Shou Liao, Hao-Ting Wei", "title": "Waiting is not easy but worth it: the online TSP on the line revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online traveling salesman problem on the real line (OLTSPL)\nin which a salesman begins at the origin, traveling at no faster than unit\nspeed along the real line, and wants to serve a sequence of requests, arriving\nonline over time on the real line and return to the origin as quickly as\npossible. The problem has been widely investigated for more than two decades,\nbut was just optimally solved by a deterministic algorithm with a competitive\nratio of $(9+\\sqrt{17})/8$, reported in~[Bjelde A. et al., in Proc. SODA 2017,\npp.994--1005].\n  In this study we present lower bounds and upper bounds for randomized\nalgorithms in the OLTSPL. Precisely, we show, for the first time, that a simple\nrandomized \\emph{zealous} algorithm can improve the optimal deterministic\nalgorithm. Here an algorithm is called zealous if waiting strategies are not\nallowed to use for the salesman as long as there are unserved requests.\nMoreover, we incorporate a natural waiting scheme into the randomized\nalgorithm, which can even achieve the lower bound we propose for any randomized\nalgorithms, and thus it is optimal. We also consider randomized algorithms\nagainst a \\emph{fair} adversary, i.e. an adversary with restricted power that\nrequires the salesman to move within the convex hull of the origin and the\nrequests released so far. The randomized non-zealous algorithm can outperform\nthe optimal deterministic algorithm against the fair adversary as well.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 04:40:56 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Chen", "Pei-Chuan", ""], ["Demaine", "Erik D.", ""], ["Liao", "Chung-Shou", ""], ["Wei", "Hao-Ting", ""]]}, {"id": "1907.00484", "submitter": "Yangguang Shi", "authors": "Yuval Emek, Shay Kutten, Ron Lavi, Yangguang Shi", "title": "Bayesian Generalized Network Design", "comments": "25 pages, 0 figure. An extended abstract of this paper is to appear\n  in the 27th Annual European Symposium on Algorithms (ESA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study network coordination problems, as captured by the setting of\ngeneralized network design (Emek et al., STOC 2018), in the face of uncertainty\nresulting from partial information that the network users hold regarding the\nactions of their peers. This uncertainty is formalized using Alon et al.'s\nBayesian ignorance framework (TCS 2012). While the approach of Alon et al. is\npurely combinatorial, the current paper takes into account computational\nconsiderations: Our main technical contribution is the development of\n(strongly) polynomial time algorithms for local decision making in the face of\nBayesian uncertainty.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:26:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Emek", "Yuval", ""], ["Kutten", "Shay", ""], ["Lavi", "Ron", ""], ["Shi", "Yangguang", ""]]}, {"id": "1907.00524", "submitter": "Samson Zhou", "authors": "Grigory Yaroslavtsev and Samson Zhou", "title": "Approximate $\\mathbb{F}_2$-Sketching of Valuation Functions", "comments": "To appear in RANDOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of constructing a linear sketch of minimum dimension\nthat allows approximation of a given real-valued function $f \\colon\n\\mathbb{F}_2^n \\rightarrow \\mathbb R$ with small expected squared error. We\ndevelop a general theory of linear sketching for such functions through which\nwe analyze their dimension for most commonly studied types of valuation\nfunctions: additive, budget-additive, coverage, $\\alpha$-Lipschitz submodular\nand matroid rank functions. This gives a characterization of how many bits of\ninformation have to be stored about the input $x$ so that one can compute $f$\nunder additive updates to its coordinates.\n  Our results are tight in most cases and we also give extensions to the\ndistributional version of the problem where the input $x \\in \\mathbb{F}_2^n$ is\ngenerated uniformly at random. Using known connections with dynamic streaming\nalgorithms, both upper and lower bounds on dimension obtained in our work\nextend to the space complexity of algorithms evaluating $f(x)$ under long\nsequences of additive updates to the input $x$ presented as a stream. Similar\nresults hold for simultaneous communication in a distributed setting.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 03:07:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Yaroslavtsev", "Grigory", ""], ["Zhou", "Samson", ""]]}, {"id": "1907.00529", "submitter": "Ryuhei Mori", "authors": "Kazuya Shimizu and Ryuhei Mori", "title": "Exponential-time quantum algorithms for graph coloring problems", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fastest known classical algorithm deciding the $k$-colorability of\n$n$-vertex graph requires running time $\\Omega(2^n)$ for $k\\ge 5$. In this\nwork, we present an exponential-space quantum algorithm computing the chromatic\nnumber with running time $O(1.9140^n)$ using quantum random access memory\n(QRAM). Our approach is based on Ambainis et al's quantum dynamic programming\nwith applications of Grover's search to branching algorithms. We also present a\npolynomial-space quantum algorithm not using QRAM for the graph $20$-coloring\nproblem with running time $O(1.9575^n)$. In the polynomial-space quantum\nalgorithm, we essentially show $(4-\\epsilon)^n$-time classical algorithms that\ncan be improved quadratically by Grover's search.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 03:35:48 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Shimizu", "Kazuya", ""], ["Mori", "Ryuhei", ""]]}, {"id": "1907.00533", "submitter": "Travis Dick", "authors": "Maria-Florina Balcan, Travis Dick, Manuel Lang", "title": "Learning to Link", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an important part of many modern data analysis pipelines,\nincluding network analysis and data retrieval. There are many different\nclustering algorithms developed by various communities, and it is often not\nclear which algorithm will give the best performance on a specific clustering\ntask. Similarly, we often have multiple ways to measure distances between data\npoints, and the best clustering performance might require a non-trivial\ncombination of those metrics. In this work, we study data-driven algorithm\nselection and metric learning for clustering problems, where the goal is to\nsimultaneously learn the best algorithm and metric for a specific application.\nThe family of clustering algorithms we consider is parameterized linkage based\nprocedures that includes single and complete linkage. The family of distance\nfunctions we learn over are convex combinations of base distance functions. We\ndesign efficient learning algorithms which receive samples from an\napplication-specific distribution over clustering instances and simultaneously\nlearn both a near-optimal distance and clustering algorithm from these classes.\nWe also carry out a comprehensive empirical evaluation of our techniques\nshowing that they can lead to significantly improved clustering performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 04:08:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 18:36:24 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 20:32:34 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Dick", "Travis", ""], ["Lang", "Manuel", ""]]}, {"id": "1907.00605", "submitter": "David Naori", "authors": "David Naori (1) and Danny Raz (1) ((1) Computer Science Department,\n  Technion, Israel)", "title": "Online Multidimensional Packing Problems in the Random-Order Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online multidimensional variants of the generalized assignment\nproblem which are used to model prominent real-world applications, such as the\nassignment of virtual machines with multiple resource requirements to physical\ninfrastructure in cloud computing. These problems can be seen as an extension\nof the well known secretary problem and thus the standard online worst-case\nmodel cannot provide any performance guarantee. The prevailing model in this\ncase is the random-order model, which provides a useful realistic and robust\nalternative. Using this model, we study the $d$-dimensional generalized\nassignment problem, where we introduce a novel technique that achieves an\n$O(d)$-competitive algorithms and prove a matching lower bound of $\\Omega(d)$.\nFurthermore, our algorithm improves upon the best-known competitive-ratio for\nthe online (one-dimensional) generalized assignment problem and the online\nknapsack problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 08:27:47 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Naori", "David", ""], ["Raz", "Danny", ""]]}, {"id": "1907.00676", "submitter": "Andrej Sajenko", "authors": "Frank Kammer, Johannes Meintrup, Andrej Sajenko", "title": "Space-Efficient Vertex Separators for Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For $n$-vertex graphs with treewidth $k = O(n^{1/2-\\epsilon})$ and an\narbitrary $\\epsilon>0$, we present a word-RAM algorithm to compute vertex\nseparators using only $O(n)$ bits of working memory. As an application of our\nalgorithm, we give an $O(1)$-approximation algorithm for tree decomposition.\nOur algorithm computes a tree decomposition in $c^k n (\\log \\log n) \\log^* n$\ntime using $O(n)$ bits for some constant $c > 0$.\n  We finally use the tree decomposition obtained by our algorithm to solve\nVertex Cover, Independent Set, Dominating Set, MaxCut and $q$-Coloring by using\n$O(n)$ bits as long as the treewidth of the graph is smaller than $c' \\log n$\nfor some problem dependent constant $0 < c' < 1$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:03:52 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 08:44:32 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 12:00:07 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 17:07:01 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kammer", "Frank", ""], ["Meintrup", "Johannes", ""], ["Sajenko", "Andrej", ""]]}, {"id": "1907.00817", "submitter": "Joergen Bang-Jensen", "authors": "J{\\o}rgen Bang-Jensen, Thomas Bellitto, William Lochet, Anders Yeo", "title": "The directed 2-linkage problem with length constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\sc weak 2-linkage} problem for digraphs asks for a given digraph and\nvertices $s_1,s_2,t_1,t_2$ whether $D$ contains a pair of arc-disjoint paths\n$P_1,P_2$ such that $P_i$ is an $(s_i,t_i)$-path. This problem is NP-complete\nfor general digraphs but polynomially solvable for acyclic digraphs\n\\cite{fortuneTCS10}. Recently it was shown \\cite{bercziESA17} that if $D$ is\nequipped with a weight function $w$ on the arcs\n  which satisfies that all edges have positive weight, then there is a\npolynomial algorithm for the variant of the weak-2-linkage problem when both\npaths have to be shortest paths in $D$. In this paper we consider the unit\nweight case and prove that for every pair constants $k_1,k_2$, there is a\npolynomial algorithm which decides whether the input digraph $D$ has a pair of\narc-disjoint paths $P_1,P_2$ such that $P_i$ is an $(s_i,t_i)$-path and the\nlength of $P_i$ is no more than $d(s_i,t_i)+k_i$, for $i=1,2$, where\n$d(s_i,t_i)$ denotes the length of the shortest $(s_i,t_i)$-path. We prove\nthat, unless the exponential time hypothesis (ETH) fails, there is no\npolynomial algorithm for deciding the existence of a solution $P_1,P_2$ to the\n{\\sc weak 2-linkage} problem where each path $P_i$ has length at most\n$d(s_i,t_i)+ c\\log^{1+\\epsilon}{}n$ for some constant $c$.\n  We also prove that the {\\sc weak 2-linkage} problem remains NP-complete if we\nrequire one of the two paths to be a shortest path while the other path has no\nrestriction on the length.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:25:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bang-Jensen", "J\u00f8rgen", ""], ["Bellitto", "Thomas", ""], ["Lochet", "William", ""], ["Yeo", "Anders", ""]]}, {"id": "1907.00845", "submitter": "Liudmila Ostroumova Prokhorenkova", "authors": "Liudmila Prokhorenkova and Aleksandr Shekhovtsov", "title": "Graph-based Nearest Neighbor Search: From Practice to Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based approaches are empirically shown to be very successful for the\nnearest neighbor search (NNS). However, there has been very little research on\ntheir theoretical guarantees. We fill this gap and rigorously analyze the\nperformance of graph-based NNS algorithms, specifically focusing on the\nlow-dimensional (d << \\log n) regime. In addition to the basic greedy algorithm\non nearest neighbor graphs, we also analyze the most successful heuristics\ncommonly used in practice: speeding up via adding shortcut edges and improving\naccuracy via maintaining a dynamic list of candidates. We believe that our\ntheoretical insights supported by experimental analysis are an important step\ntowards understanding the limits and benefits of graph-based NNS algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:11:26 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 15:25:35 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 14:06:52 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2020 08:55:38 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Prokhorenkova", "Liudmila", ""], ["Shekhovtsov", "Aleksandr", ""]]}, {"id": "1907.01005", "submitter": "Denis Davydov", "authors": "Denis Davydov and Martin Kronbichler", "title": "Algorithms and data structures for matrix-free finite element operators\n  with MPI-parallel sparse multi-vectors", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional solution approaches for problems in quantum mechanics scale as\n$\\mathcal O(M^3)$, where $M$ is the number of electrons. Various methods have\nbeen proposed to address this issue and obtain linear scaling $\\mathcal O(M)$.\nOne promising formulation is the direct minimization of energy. Such methods\ntake advantage of physical localization of the solution, namely that the\nsolution can be sought in terms of non-orthogonal orbitals with local support.\nIn this work a numerically efficient implementation of sparse parallel vectors\nwithin the open-source finite element library deal.II is proposed. The main\nalgorithmic ingredient is the matrix-free evaluation of the Hamiltonian\noperator by cell-wise quadrature. Based on an a-priori chosen support for each\nvector we develop algorithms and data structures to perform (i) matrix-free\nsparse matrix multivector products (SpMM), (ii) the projection of an operator\nonto a sparse sub-space (inner products), and (iii) post-multiplication of a\nsparse multivector with a square matrix. The node-level performance is analyzed\nusing a roofline model. Our matrix-free implementation of finite element\noperators with sparse multivectors achieves the performance of 157 GFlop/s on\nIntel Cascade Lake architecture. Strong and weak scaling results are reported\nfor a typical benchmark problem using quadratic and quartic finite element\nbases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:24:46 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Davydov", "Denis", ""], ["Kronbichler", "Martin", ""]]}, {"id": "1907.01006", "submitter": "Ray Li", "authors": "Serge Gaspers and Ray Li", "title": "Enumeration of Preferred Extensions in Almost Oriented Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present enumeration algorithms to list all preferred\nextensions of an argumentation framework. This task is equivalent to\nenumerating all maximal semikernels of a directed graph. For directed graphs on\n$n$ vertices, all preferred extensions can be enumerated in $O^*(3^{n/3})$ time\nand there are directed graphs with $\\Omega(3^{n/3})$ preferred extensions. We\ngive faster enumeration algorithms for directed graphs with at most\n$0.8004\\cdot n$ vertices occurring in $2$-cycles. In particular, for oriented\ngraphs (digraphs with no 2-cycles) one of our algorithms runs in time\n$O(1.2321^n)$, and we show that there are oriented graphs with $\\Omega(3^{n/6})\n> \\Omega(1.2009^n)$ preferred extensions.\n  A combination of three algorithms leads to the fastest enumeration times for\nvarious proportions of the number of vertices in $2$-cycles. The most\ninnovative one is a new 2-stage sampling algorithm, combined with a new\nparameterized enumeration algorithm, analyzed with a combination of the recent\nmonotone local search technique (STOC 2016) and an extension thereof (ICALP\n2017).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:25:57 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Gaspers", "Serge", ""], ["Li", "Ray", ""]]}, {"id": "1907.01032", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri", "title": "On Slicing Sorted Integer Sequences", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing sorted integer sequences in small space is a central problem for\nlarge-scale retrieval systems such as Web search engines. Efficient query\nresolution, e.g., intersection or random access, is achieved by carefully\npartitioning the sequences. In this work we describe and compare two different\npartitioning paradigms: partitioning by cardinality and partitioning by\nuniverse. Although the ideas behind such paradigms have been known in the\ncoding and algorithmic community since many years, inverted index compression\nhas extensively adopted the former paradigm, whereas the latter has received\nonly little attention. As a result, an experimental comparison between these\ntwo is missing for the setting of inverted index compression. We also propose\nand implement a solution that recursively slices the universe of representation\nof a sequence to achieve compact storage and attain to fast query execution.\nAlbeit larger than some state-of-the-art representations, this slicing approach\nsubstantially improves the performance of list intersections and unions while\noperating in compressed space, thus offering an excellent space/time trade-off\nfor the problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:33:29 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 07:35:46 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""]]}, {"id": "1907.01218", "submitter": "Artem Kaznatcheev", "authors": "Artem Kaznatcheev, David A. Cohen, Peter G. Jeavons", "title": "Representing fitness landscapes by valued constraints to understand the\n  complexity of local search", "comments": "26 pages, 9 figures. Extended journal version to appear in Journal of\n  Artificial Intelligence Research; conference version appeared in CP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search is widely used to solve combinatorial optimisation problems and\nto model biological evolution, but the performance of local search algorithms\non different kinds of fitness landscapes is poorly understood. Here we consider\nhow fitness landscapes can be represented using valued constraints, and\ninvestigate what the structure of such representations reveals about the\ncomplexity of local search.\n  First, we show that for fitness landscapes representable by binary Boolean\nvalued constraints there is a minimal necessary constraint graph that can be\neasily computed. Second, we consider landscapes as equivalent if they allow the\nsame (improving) local search moves; we show that a minimal constraint graph\nstill exists, but is NP-hard to compute.\n  We then develop several techniques to bound the length of any sequence of\nlocal search moves. We show that such a bound can be obtained from the\nnumerical values of the constraints in the representation, and show how this\nbound may be tightened by considering equivalent representations. In the binary\nBoolean case, we prove that a degree 2 or tree-structured constraint graph\ngives a quadratic bound on the number of improving moves made by any local\nsearch; hence, any landscape that can be represented by such a model will be\ntractable for any form of local search.\n  Finally, we build two families of examples to show that the conditions in our\ntractability results are essential. With domain size three, even just a path of\nbinary constraints can model a landscape with an exponentially long sequence of\nimproving moves. With a treewidth-two constraint graph, even with a maximum\ndegree of three, binary Boolean constraints can model a landscape with an\nexponentially long sequence of improving moves.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:05:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 20:56:49 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 12:21:07 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 01:47:52 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kaznatcheev", "Artem", ""], ["Cohen", "David A.", ""], ["Jeavons", "Peter G.", ""]]}, {"id": "1907.01241", "submitter": "Nicolas Grelier", "authors": "Nicolas Grelier, Saeed Gh. Ilchi, Tillmann Miltzow, Shakhar\n  Smorodinsky", "title": "On the VC-dimension of half-spaces with respect to convex sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family S of convex sets in the plane defines a hypergraph H = (S, E) as\nfollows. Every subfamily S' of S defines a hyperedge of H if and only if there\nexists a halfspace h that fully contains S' , and no other set of S is fully\ncontained in h. In this case, we say that h realizes S'. We say a set S is\nshattered, if all its subsets are realized. The VC-dimension of a hypergraph H\nis the size of the largest shattered set.\n  We show that the VC-dimension for pairwise disjoint convex sets in the plane\nis bounded by 3, and this is tight. In contrast, we show the VC-dimension of\nconvex sets in the plane (not necessarily disjoint) is unbounded. We provide a\nquadratic lower bound in the number of pairs of intersecting sets in a\nshattered family of convex sets in the plane. We also show that the\nVC-dimension is unbounded for pairwise disjoint convex sets in R^d , for d > 2.\nWe focus on, possibly intersecting, segments in the plane and determine that\nthe VC-dimension is always at most 5. And this is tight, as we construct a set\nof five segments that can be shattered. We give two exemplary applications. One\nfor a geometric set cover problem and one for a range-query data structure\nproblem, to motivate our findings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:53:20 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 15:27:34 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 13:50:48 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 15:18:03 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Grelier", "Nicolas", ""], ["Ilchi", "Saeed Gh.", ""], ["Miltzow", "Tillmann", ""], ["Smorodinsky", "Shakhar", ""]]}, {"id": "1907.01243", "submitter": "Marcel Radermacher", "authors": "Marcel Radermacher and Ignaz Rutter", "title": "Geometric Crossing-Minimization -- A Scalable Randomized Approach", "comments": "Appears in the Proceedings of the 27th Annual European Symposium on\n  Algorithms (ESA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of edge-crossings in geometric drawings of\ngraphs $G=(V, E)$, i.e., in drawings where each edge is depicted as a line\nsegment. The respective decision problem is NP-hard [Bienstock, '91]. In\ncontrast to theory and the topological setting, the geometric setting did not\nreceive a lot of attention in practice. Prior work [Radermacher et al.,\nALENEX'18] is limited to the crossing-minimization in geometric graphs with\nless than $200$ edges. The described heuristics base on the primitive operation\nof moving a single vertex $v$ to its crossing-minimal position, i.e., the\nposition in $\\mathbb{R}^2$ that minimizes the number of crossings on edges\nincident to $v$.\n  In this paper, we introduce a technique to speed-up the computation by a\nfactor of $20$. This is necessary but not sufficient to cope with graphs with a\nfew thousand edges. In order to handle larger graphs, we drop the condition\nthat each vertex $v$ has to be moved to its crossing-minimal position and\ncompute a position that is only optimal with respect to a small random subset\nof the edges. In our theoretical contribution, we consider drawings that\ncontain for each edge $uv \\in E$ and each position $p \\in \\mathbb{R}^2$ for $v$\n$o(|E|)$ crossings. In this case, we prove that with a random subset of the\nedges of size $\\Theta(k \\log k)$ the co-crossing number of a degree-$k$ vertex\n$v$, i.e., the number of edge pairs $uv \\in E, e \\in E$ that do not cross, can\nbe approximated by an arbitrary but fixed factor $\\delta$ with high\nprobability. In our experimental evaluation, we show that the randomized\napproach reduces the number of crossings in graphs with up to $13\\,000$ edges\nconsiderably. The evaluation suggests that depending on the degree-distribution\ndifferent strategies result in the fewest number of crossings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:59:21 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Radermacher", "Marcel", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1907.01258", "submitter": "Yimin Ge", "authors": "Yimin Ge and Vedran Dunjko", "title": "A hybrid algorithm framework for small quantum computers with\n  application to finding Hamiltonian cycles", "comments": "20+2 pages", "journal-ref": null, "doi": "10.1063/1.5119235", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that quantum computers can polynomially speed up\ncertain SAT-solving algorithms even when the number of available qubits is\nsignificantly smaller than the number of variables. Here we generalise this\napproach. We present a framework for hybrid quantum-classical algorithms which\nutilise quantum computers significantly smaller than the problem size. Given an\narbitrarily small ratio of the quantum computer to the instance size, we\nachieve polynomial speedups for classical divide-and-conquer algorithms,\nprovided that certain criteria on the time- and space-efficiency are met. We\ndemonstrate how this approach can be used to enhance Eppstein's algorithm for\nthe cubic Hamiltonian cycle problem, and achieve a polynomial speedup for any\nratio of the number of qubits to the size of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:36:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ge", "Yimin", ""], ["Dunjko", "Vedran", ""]]}, {"id": "1907.01495", "submitter": "Petr Hlin\\v{e}n\\'y", "authors": "Deniz A\\u{g}ao\\u{g}lu, Petr Hlin\\v{e}n\\'y", "title": "Efficient Isomorphism for $S_d$-graphs and $T$-graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $H$-graph is one representable as the intersection graph of connected\nsubgraphs of a suitable subdivision of a fixed graph $H$, introduced by\nBir\\'{o}, Hujter and Tuza (1992). An $H$-graph is proper if the representing\nsubgraphs of $H$ can be chosen incomparable by the inclusion. In this paper, we\nfocus on the isomorphism problem for $S_d$-graphs and $T$-graphs, where $S_d$\nis the star with $d$ rays and $T$ is an arbitrary fixed tree.\n  Answering an open problem of Chaplick, T\\\"{o}pfer, Voborn\\'{\\i}k and Zeman\n(2016), we provide an FPT-time algorithm for testing isomorphism and computing\nthe automorphism group of $S_d$-graphs when parameterized by~$d$, which\ninvolves the classical group-computing machinery by Furst, Hopcroft, and Luks\n(1980). We also show that the isomorphism problem of $S_d$-graphs is at least\nas hard as the isomorphism problem of posets of bounded width, for which no\nefficient combinatorial-only algorithm is known to date. Then we extend our\napproach to an XP-time algorithm for isomorphism of $T$-graphs when\nparameterized by the size of $T$. Lastly, we contribute a simple FPT-time\ncombinatorial algorithm for isomorphism testing in the special case of proper\n$S_d$- and $T$-graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 16:57:11 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 18:58:17 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 09:48:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["A\u011fao\u011flu", "Deniz", ""], ["Hlin\u011bn\u00fd", "Petr", ""]]}, {"id": "1907.01600", "submitter": "Samuel McCauley", "authors": "Samuel McCauley", "title": "Approximate Similarity Search Under Edit Distance Using\n  Locality-Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit distance similarity search, also called approximate pattern matching, is\na fundamental problem with widespread database applications. The goal of the\nproblem is to preprocess $n$ strings of length $d$, to quickly answer queries\n$q$ of the form: if there is a database string within edit distance $r$ of $q$,\nreturn a database string within edit distance $cr$ of $q$. Previous approaches\nto this problem either rely on very large (superconstant) approximation ratios\n$c$, or very small search radii $r$. Outside of a narrow parameter range, these\nsolutions are not competitive with trivially searching through all $n$ strings.\n  In this work give a simple and easy-to-implement hash function that can\nquickly answer queries for a wide range of parameters. Specifically, our\nstrategy can answer queries in time $\\tilde{O}(d3^rn^{1/c})$. The best known\npractical results require $c \\gg r$ to achieve any correctness guarantee;\nmeanwhile, the best known theoretical results are very involved and difficult\nto implement, and require query time at least $24^r$. Our results significantly\nbroaden the range of parameters for which we can achieve nontrivial bounds,\nwhile retaining the practicality of a locality-sensitive hash function.\n  We also show how to apply our ideas to the closely-related Approximate\nNearest Neighbor problem for edit distance, obtaining similar time bounds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 19:45:34 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:57:33 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["McCauley", "Samuel", ""]]}, {"id": "1907.01619", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne and Anindya De and Rocco A. Servedio", "title": "Learning from satisfying assignments under continuous distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  What kinds of functions are learnable from their satisfying assignments?\nMotivated by this simple question, we extend the framework of De, Diakonikolas,\nand Servedio [DDS15], which studied the learnability of probability\ndistributions over $\\{0,1\\}^n$ defined by the set of satisfying assignments to\n\"low-complexity\" Boolean functions, to Boolean-valued functions defined over\ncontinuous domains. In our learning scenario there is a known \"background\ndistribution\" $\\mathcal{D}$ over $\\mathbb{R}^n$ (such as a known normal\ndistribution or a known log-concave distribution) and the learner is given\ni.i.d. samples drawn from a target distribution $\\mathcal{D}_f$, where\n$\\mathcal{D}_f$ is $\\mathcal{D}$ restricted to the satisfying assignments of an\nunknown low-complexity Boolean-valued function $f$. The problem is to learn an\napproximation $\\mathcal{D}'$ of the target distribution $\\mathcal{D}_f$ which\nhas small error as measured in total variation distance.\n  We give a range of efficient algorithms and hardness results for this\nproblem, focusing on the case when $f$ is a low-degree polynomial threshold\nfunction (PTF). When the background distribution $\\mathcal{D}$ is log-concave,\nwe show that this learning problem is efficiently solvable for degree-1 PTFs\n(i.e.,~linear threshold functions) but not for degree-2 PTFs. In contrast, when\n$\\mathcal{D}$ is a normal distribution, we show that this learning problem is\nefficiently solvable for degree-2 PTFs but not for degree-4 PTFs. Our hardness\nresults rely on standard assumptions about secure signature schemes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:17:59 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["De", "Anindya", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1907.01630", "submitter": "Giordano Da Lozzo", "authors": "Juan Jose Besa, Giordano Da Lozzo, and Michael T. Goodrich", "title": "Computing k-Modal Embeddings of Planar Digraphs", "comments": "Extended version of a paper to appear in the Proceedings of the 27th\n  Annual European Symposium on Algorithms (ESA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a planar digraph $G$ and a positive even integer $k$, an embedding of\n$G$ in the plane is k-modal, if every vertex of $G$ is incident to at most $k$\npairs of consecutive edges with opposite orientations, i.e., the incoming and\nthe outgoing edges at each vertex are grouped by the embedding into at most k\nsets of consecutive edges with the same orientation. In this paper, we study\nthe $k$-Modality problem, which asks for the existence of a $k$-modal embedding\nof a planar digraph. This combinatorial problem is at the very core of a\nvariety of constrained embedding questions for planar digraphs and flat\nclustered networks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:53:41 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Besa", "Juan Jose", ""], ["Da Lozzo", "Giordano", ""], ["Goodrich", "Michael T.", ""]]}, {"id": "1907.01631", "submitter": "Jeffrey Barratt", "authors": "Jeffrey Barratt, Brian Zhang", "title": "Cache-Friendly Search Trees; or, In Which Everything Beats std::set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a lot of work in theoretical computer science has gone into optimizing\nthe runtime and space usage of data structures, such work very often neglects a\nvery important component of modern computers: the cache. In doing so, very\noften, data structures are developed that achieve theoretically-good runtimes\nbut are slow in practice due to a large number of cache misses. In 1999, Frigo\net al. introduced the notion of a cache-oblivious algorithm: an algorithm that\nuses the cache to its advantage, regardless of the size or structure of said\ncache. Since then, various authors have designed cache-oblivious algorithms and\ndata structures for problems from matrix multiplication to array sorting. We\nfocus in this work on cache-oblivious search trees; i.e. implementing an\nordered dictionary in a cache-friendly manner. We will start by presenting an\noverview of cache-oblivious data structures, especially cache-oblivious search\ntrees. We then give practical results using these cache-oblivious structures on\nmodern-day machinery, comparing them to the standard std::set and other\ncache-friendly dictionaries such as B-trees.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:55:47 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Barratt", "Jeffrey", ""], ["Zhang", "Brian", ""]]}, {"id": "1907.01700", "submitter": "Takehiro Ito", "authors": "Takehiro Ito, Naonori Kakimura, Naoyuki Kamiyama, Yusuke Kobayashi,\n  Yoshio Okamoto", "title": "Shortest Reconfiguration of Perfect Matchings via Alternating Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by adjacency in perfect matching polytopes, we study the shortest\nreconfiguration problem of perfect matchings via alternating cycles. Namely, we\nwant to find a shortest sequence of perfect matchings which transforms one\ngiven perfect matching to another given perfect matching such that the\nsymmetric difference of each pair of consecutive perfect matchings is a single\ncycle. The problem is equivalent to the combinatorial shortest path problem in\nperfect matching polytopes. We prove that the problem is NP-hard even when a\ngiven graph is planar or bipartite, but it can be solved in polynomial time\nwhen the graph is outerplanar.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 01:51:33 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ito", "Takehiro", ""], ["Kakimura", "Naonori", ""], ["Kamiyama", "Naoyuki", ""], ["Kobayashi", "Yusuke", ""], ["Okamoto", "Yoshio", ""]]}, {"id": "1907.01745", "submitter": "Kanthi Sarpatwar", "authors": "Ariel Kulik, Kanthi Sarpatwar, Baruch Schieber, Hadas Shachnai", "title": "Generalized Assignment via Submodular Optimization with Reserved\n  Capacity", "comments": "Preliminary version to appear in European Symposium on Algorithms\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the \\emph{generalized assignment problem} ({\\sf GAP})\nwith group constraints. An instance of {\\sf Group GAP} is a set $I$ of items,\npartitioned into $L$ groups, and a set of $m$ uniform (unit-sized) bins. Each\nitem $i \\in I$ has a size $s_i >0$, and a profit $p_{i,j} \\geq 0$ if packed in\nbin $j$. A group of items is \\emph{satisfied} if all of its items are packed.\nThe goal is to find a feasible packing of a subset of the items in the bins\nsuch that the total profit from satisfied groups is maximized. We point to\ncentral applications of {\\sf Group GAP} in Video-on-Demand services, mobile\nDevice-to-Device network caching and base station cooperation in 5G networks.\n  Our main result is a $\\frac{1}{6}$-approximation algorithm for {\\sf Group\nGAP} instances where the total size of each group is at most $\\frac{m}{2}$. At\nthe heart of our algorithm lies an interesting derivation of a submodular\nfunction from the classic LP formulation of {\\sf GAP}, which facilitates the\nconstruction of a high profit solution utilizing at most half the total bin\ncapacity, while the other half is \\emph{reserved} for later use. In particular,\nwe give an algorithm for submodular maximization subject to a knapsack\nconstraint, which finds a solution of profit at least $\\frac{1}{3}$ of the\noptimum, using at most half the knapsack capacity, under mild restrictions on\nelement sizes. Our novel approach of submodular optimization subject to a\nknapsack \\emph{with reserved capacity} constraint may find applications in\nsolving other group assignment problems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 05:43:44 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 04:18:29 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kulik", "Ariel", ""], ["Sarpatwar", "Kanthi", ""], ["Schieber", "Baruch", ""], ["Shachnai", "Hadas", ""]]}, {"id": "1907.01766", "submitter": "Simina Br\\^anzei", "authors": "Simina Br\\^anzei and Fedor Sandomirskiy", "title": "Algorithms for Competitive Division of Chores", "comments": "38 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of allocating divisible bads (chores) among multiple\nagents with additive utilities, when money transfers are not allowed. The\ncompetitive rule is known to be the best mechanism for goods with additive\nutilities and was recently extended to chores by Bogomolnaia et al (2017). For\nboth goods and chores, the rule produces Pareto optimal and envy-free\nallocations. In the case of goods, the outcome of the competitive rule can be\neasily computed. Competitive allocations solve the Eisenberg-Gale convex\nprogram; hence the outcome is unique and can be approximately found by standard\ngradient methods. An exact algorithm that runs in polynomial time in the number\nof agents and goods was given by Orlin.\n  In the case of chores, the competitive rule does not solve any convex\noptimization problem; instead, competitive allocations correspond to local\nminima, local maxima, and saddle points of the Nash Social Welfare on the\nPareto frontier of the set of feasible utilities. The rule becomes multivalued\nand none of the standard methods can be applied to compute its outcome.\n  In this paper, we show that all the outcomes of the competitive rule for\nchores can be computed in strongly polynomial time if either the number of\nagents or the number of chores is fixed. The approach is based on a combination\nof three ideas: all consumption graphs of Pareto optimal allocations can be\nlisted in polynomial time; for a given consumption graph, a candidate for a\ncompetitive allocation can be constructed via explicit formula; and a given\nallocation can be checked for being competitive using a maximum flow\ncomputation as in Devanur et al (2002).\n  Our algorithm immediately gives an approximately-fair allocation of\nindivisible chores by the rounding technique of Barman and Krishnamurthy\n(2018).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 06:55:04 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Br\u00e2nzei", "Simina", ""], ["Sandomirskiy", "Fedor", ""]]}, {"id": "1907.01773", "submitter": "Kaijie Tu", "authors": "Dawen Xu, Ying Wang, Kaijie Tu, Cheng Liu, Bingsheng He, and Lei Zhang", "title": "Accelerating Generative Neural Networks on Unmodified Deep Learning\n  Processors -- A Software Approach", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural network is a new category of neural networks and it has\nbeen widely utilized in applications such as content generation, unsupervised\nlearning, segmentation and pose estimation. It typically involves massive\ncomputing-intensive deconvolution operations that cannot be fitted to\nconventional neural network processors directly. However, prior works mainly\ninvestigated specialized hardware architectures through intensive hardware\nmodifications to the existing deep learning processors to accelerate\ndeconvolution together with the convolution. In contrast, this work proposes a\nnovel deconvolution implementation with a software approach and enables fast\nand efficient deconvolution execution on the legacy deep learning processors.\nOur proposed method reorganizes the computation of deconvolution and allows the\ndeep learning processors to treat it as the standard convolution by splitting\nthe original deconvolution filters into multiple small filters. Compared to\nprior acceleration schemes, the implemented acceleration scheme achieves 2.41x\n- 4.34x performance speedup and reduces the energy consumption by 27.7% - 54.5%\non a set of realistic benchmarks. In addition, we also applied the\ndeconvolution computing approach to the off-the-shelf commodity deep learning\nprocessors. The performance of deconvolution also exhibits significant\nperformance speedup over prior deconvolution implementations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 07:18:57 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 08:19:41 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 02:50:01 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xu", "Dawen", ""], ["Wang", "Ying", ""], ["Tu", "Kaijie", ""], ["Liu", "Cheng", ""], ["He", "Bingsheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1907.01815", "submitter": "Jakub Radoszewski", "authors": "Panagiotis Charalampopoulos, Tomasz Kociumaka, Solon P. Pissis, Jakub\n  Radoszewski, Wojciech Rytter, Juliusz Straszy\\'nski, Tomasz Wale\\'n, Wiktor\n  Zuba", "title": "Circular Pattern Matching with $k$ Mismatches", "comments": "Extended version of a paper from FCT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The $k$-mismatch problem consists in computing the Hamming distance between a\npattern $P$ of length $m$ and every length-$m$ substring of a text $T$ of\nlength $n$, if this distance is no more than $k$. In many real-world\napplications, any cyclic rotation of $P$ is a relevant pattern, and thus one is\ninterested in computing the minimal distance of every length-$m$ substring of\n$T$ and any cyclic rotation of $P$. This is the circular pattern matching with\n$k$ mismatches ($k$-CPM) problem. A multitude of papers have been devoted to\nsolving this problem but, to the best of our knowledge, only average-case upper\nbounds are known. In this paper, we present the first non-trivial worst-case\nupper bounds for the $k$-CPM problem. Specifically, we show an $O(nk)$-time\nalgorithm and an $O(n+\\frac{n}{m}\\,k^4)$-time algorithm. The latter algorithm\napplies in an extended way a technique that was very recently developed for the\n$k$-mismatch problem [Bringmann et al., SODA 2019].\n  A preliminary version of this work appeared at FCT 2019. In this version we\nimprove the time complexity of the main algorithm from $O(n+\\frac{n}{m}\\,k^5)$\nto $O(n+\\frac{n}{m}\\,k^4)$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 09:35:53 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 12:29:55 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Kociumaka", "Tomasz", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "1907.02053", "submitter": "Lars Gottesb\\\"uren", "authors": "Lars Gottesb\\\"uren, Michael Hamann, Dorothea Wagner", "title": "Evaluation of a Flow-Based Hypergraph Bipartitioning Algorithm", "comments": "22 pages, 7 figures, 2 tables. Full version of the paper appearing in\n  the Proceedings of the 27th Annual European Symposium on Algorithms (ESA\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose HyperFlowCutter, an algorithm for balanced\nhypergraph bipartitioning. It is based on minimum S-T hyperedge cuts and\nmaximum flows. It computes a sequence of bipartitions that optimize cut size\nand balance in the Pareto sense, being able to trade one for the other.\nHyperFlowCutter builds on the FlowCutter algorithm for partitioning graphs. We\npropose additional features, such as handling disconnected hypergraphs, novel\nmethods for obtaining starting S,T pairs as well as an approach to refine a\ngiven partition with HyperFlowCutter. Our main contribution is ReBaHFC, a new\nalgorithm which obtains an initial partition with the fast multilevel\nhypergraph partitioner PaToH and then improves it using HyperFlowCutter as a\nrefinement algorithm. ReBaHFC is able to significantly improve the solution\nquality of PaToH at little additional running time. The solution quality is\nonly marginally worse than that of the best-performing hypergraph partitioners\nKaHyPar and hMETIS, while being one order of magnitude faster. Thus ReBaHFC\noffers a new time-quality trade-off in the current spectrum of hypergraph\npartitioners. For the special case of perfectly balanced bipartitioning, only\nthe much slower plain HyperFlowCutter yields slightly better solutions than\nReBaHFC, while only PaToH is faster than ReBaHFC.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:44:41 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Gottesb\u00fcren", "Lars", ""], ["Hamann", "Michael", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1907.02056", "submitter": "Yair Carmon", "authors": "Yair Carmon, Yujia Jin, Aaron Sidford and Kevin Tian", "title": "Variance Reduction for Matrix Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized primal-dual algorithm that solves the problem\n$\\min_{x} \\max_{y} y^\\top A x$ to additive error $\\epsilon$ in time\n$\\mathrm{nnz}(A) + \\sqrt{\\mathrm{nnz}(A)n}/\\epsilon$, for matrix $A$ with\nlarger dimension $n$ and $\\mathrm{nnz}(A)$ nonzero entries. This improves the\nbest known exact gradient methods by a factor of $\\sqrt{\\mathrm{nnz}(A)/n}$ and\nis faster than fully stochastic gradient methods in the accurate and/or sparse\nregime $\\epsilon \\le \\sqrt{n/\\mathrm{nnz}(A)}$. Our results hold for $x,y$ in\nthe simplex (matrix games, linear programming) and for $x$ in an $\\ell_2$ ball\nand $y$ in the simplex (perceptron / SVM, minimum enclosing ball). Our\nalgorithm combines Nemirovski's \"conceptual prox-method\" and a novel\nreduced-variance gradient estimator based on \"sampling from the difference\"\nbetween the current iterate and a reference point.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:51:35 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 03:43:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Carmon", "Yair", ""], ["Jin", "Yujia", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "1907.02120", "submitter": "Arash Haddadan", "authors": "Arash Haddadan and Alantha Newman", "title": "Towards Improving Christofides Algorithm on Fundamental Classes by\n  Gluing Convex Combinations of Tours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for gluing tours over certain tight, 3-edge cuts.\nGluing over 3-edge cuts has been used in algorithms for finding Hamilton cycles\nin special graph classes and in proving bounds for 2-edge-connected subgraph\nproblem, but not much was known in this direction for gluing connected\nmultigraphs. We apply this approach to the traveling salesman problem (TSP) in\nthe case when the objective function of the subtour elimination relaxation is\nminimized by a $\\theta$-cyclic point: $x_e \\in \\{0,\\theta, 1-\\theta, 1\\}$,\nwhere the support graph is subcubic and each vertex is incident to at least one\nedge with $x$-value 1. Such points are sufficient to resolve TSP in general.\nFor these points, we construct a convex combination of tours in which we can\nreduce the usage of edges with $x$-value 1 from the $\\frac{3}{2}$ of\nChristofides algorithm to $\\frac{3}{2}-\\frac{\\theta}{10}$ while keeping the\nusage of edges with fractional $x$-value the same as Christofides algorithm. A\ndirect consequence of this result is for the Uniform Cover Problem for TSP: In\nthe case when the objective function of the subtour elimination relaxation is\nminimized by a $\\frac{2}{3}$-uniform point: $x_e \\in \\{0, \\frac{2}{3}\\}$, we\ngive a $\\frac{17}{12}$-approximation algorithm for TSP. For such points, this\nlands us halfway between the approximation ratios of $\\frac{3}{2}$ of\nChristofides algorithm and $\\frac{4}{3}$ implied by the famous \"four-thirds\nconjecture\".\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 20:08:19 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 20:05:25 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Haddadan", "Arash", ""], ["Newman", "Alantha", ""]]}, {"id": "1907.02145", "submitter": "Victor Reis", "authors": "Victor Reis, Thomas Rothvoss", "title": "Linear Size Sparsifier and the Geometry of the Operator Norm Ball", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Matrix Spencer Conjecture asks whether given $n$ symmetric matrices in\n$\\mathbb{R}^{n \\times n}$ with eigenvalues in $[-1,1]$ one can always find\nsigns so that their signed sum has singular values bounded by $O(\\sqrt{n})$.\nThe standard approach in discrepancy requires proving that the convex body of\nall good fractional signings is large enough. However, this question has\nremained wide open due to the lack of tools to certify measure lower bounds for\nrather small non-polyhedral convex sets.\n  A seminal result by Batson, Spielman and Srivastava from 2008 shows that any\nundirected graph admits a linear size spectral sparsifier. Again, one can\ndefine a convex body of all good fractional signings. We can indeed prove that\nthis body is close to most of the Gaussian measure. This implies that a\ndiscrepancy algorithm by the second author can be used to sample a linear size\nsparsifer. In contrast to previous methods, we require only a logarithmic\nnumber of sampling phases.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 22:04:51 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 20:49:30 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 18:05:22 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Reis", "Victor", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1907.02211", "submitter": "Haroldo Gambini Santos D.Sc.", "authors": "Matheus Guedes Vilas Boas, Haroldo Gambini Santos, Luiz Henrique de\n  Campos Merschmann, and Greet Vanden Berghe", "title": "Optimal Decision Trees for the Algorithm Selection Problem: Integer\n  Programming Based Approaches", "comments": "International Transactions in Operational Research. 2019", "journal-ref": null, "doi": "10.1111/itor.12724", "report-no": null, "categories": "cs.LG cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Even though it is well known that for most relevant computational problems\ndifferent algorithms may perform better on different classes of problem\ninstances, most researchers still focus on determining a single best\nalgorithmic configuration based on aggregate results such as the average. In\nthis paper, we propose Integer Programming based approaches to build decision\ntrees for the Algorithm Selection Problem. These techniques allow automate\nthree crucial decisions: (i) discerning the most important problem features to\ndetermine problem classes; (ii) grouping the problems into classes and (iii)\nselect the best algorithm configuration for each class. To evaluate this new\napproach, extensive computational experiments were executed using the linear\nprogramming algorithms implemented in the COIN-OR Branch & Cut solver across a\ncomprehensive set of instances, including all MIPLIB benchmark instances. The\nresults exceeded our expectations. While selecting the single best parameter\nsetting across all instances decreased the total running time by 22%, our\napproach decreased the total running time by 40% on average across 10-fold\ncross validation experiments. These results indicate that our method\ngeneralizes quite well and does not overfit.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:39:46 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 17:43:15 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 15:53:26 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Boas", "Matheus Guedes Vilas", ""], ["Santos", "Haroldo Gambini", ""], ["Merschmann", "Luiz Henrique de Campos", ""], ["Berghe", "Greet Vanden", ""]]}, {"id": "1907.02218", "submitter": "Ofir Geri", "authors": "Edith Cohen and Ofir Geri", "title": "Sampling Sketches for Concave Sublinear Functions of Frequencies", "comments": "Full version of a NeurIPS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider massive distributed datasets that consist of elements modeled as\nkey-value pairs and the task of computing statistics or aggregates where the\ncontribution of each key is weighted by a function of its frequency (sum of\nvalues of its elements). This fundamental problem has a wealth of applications\nin data analytics and machine learning, in particular, with concave sublinear\nfunctions of the frequencies that mitigate the disproportionate effect of keys\nwith high frequency. The family of concave sublinear functions includes low\nfrequency moments ($p \\leq 1$), capping, logarithms, and their compositions. A\ncommon approach is to sample keys, ideally, proportionally to their\ncontributions and estimate statistics from the sample. A simple but costly way\nto do this is by aggregating the data to produce a table of keys and their\nfrequencies, apply our function to the frequency values, and then apply a\nweighted sampling scheme. Our main contribution is the design of composable\nsampling sketches that can be tailored to any concave sublinear function of the\nfrequencies. Our sketch structure size is very close to the desired sample size\nand our samples provide statistical guarantees on the estimation quality that\nare very close to that of an ideal sample of the same size computed over\naggregated data. Finally, we demonstrate experimentally the simplicity and\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 04:55:21 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 00:12:09 GMT"}, {"version": "v3", "created": "Sun, 22 Dec 2019 16:28:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Cohen", "Edith", ""], ["Geri", "Ofir", ""]]}, {"id": "1907.02266", "submitter": "Adam Karczmarz", "authors": "Adam Karczmarz, Jakub {\\L}\\k{a}cki", "title": "Reliable Hubs for Partially-Dynamic All-Pairs Shortest Paths in Directed\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new partially-dynamic algorithms for the all-pairs shortest paths\nproblem in weighted directed graphs. Most importantly, we give a new\ndeterministic incremental algorithm for the problem that handles updates in\n$\\widetilde{O}(mn^{4/3}\\log{W}/\\epsilon)$ total time (where the edge weights\nare from $[1,W]$) and explicitly maintains a $(1+\\epsilon)$-approximate\ndistance matrix. For a fixed $\\epsilon>0$, this is the first deterministic\npartially dynamic algorithm for all-pairs shortest paths in directed graphs,\nwhose update time is $o(n^2)$ regardless of the number of edges. Furthermore,\nwe also show how to improve the state-of-the-art partially dynamic randomized\nalgorithms for all-pairs shortest paths [Baswana et al. STOC'02, Bernstein\nSTOC'13] from Monte Carlo randomized to Las Vegas randomized without increasing\nthe running time bounds (with respect to the $\\widetilde{O}(\\cdot)$ notation).\n  Our results are obtained by giving new algorithms for the problem of\ndynamically maintaining hubs, that is a set of $\\widetilde{O}(n/d)$ vertices\nwhich hit a shortest path between each pair of vertices, provided it has\nhop-length $\\Omega(d)$. We give new subquadratic deterministic and Las Vegas\nalgorithms for maintenance of hubs under either edge insertions or deletions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 08:22:03 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Karczmarz", "Adam", ""], ["\u0141\u0105cki", "Jakub", ""]]}, {"id": "1907.02274", "submitter": "Adam Karczmarz", "authors": "Adam Karczmarz, Piotr Sankowski", "title": "Min-Cost Flow in Unit-Capacity Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give an $\\widetilde{O}((nm)^{2/3}\\log C)$ time algorithm for\ncomputing min-cost flow (or min-cost circulation) in unit capacity planar\nmultigraphs where edge costs are integers bounded by $C$. For planar\nmultigraphs, this improves upon the best known algorithms for general graphs:\nthe $\\widetilde{O}(m^{10/7}\\log C)$ time algorithm of Cohen et al. [SODA 2017],\nthe $O(m^{3/2}\\log(nC))$ time algorithm of Gabow and Tarjan [SIAM J. Comput.\n1989] and the $\\widetilde{O}(\\sqrt{n}m \\log C)$ time algorithm of Lee and\nSidford [FOCS 2014]. In particular, our result constitutes the first known\nfully combinatorial algorithm that breaks the $\\widetilde{O}(m^{3/2})$ time\nbarrier for min-cost flow problem in planar graphs.\n  To obtain our result we first give a very simple successive shortest paths\nbased scaling algorithm for unit-capacity min-cost flow problem that does not\nexplicitly operate on dual variables. This algorithm also runs in\n$\\widetilde{O}(m^{3/2}\\log{C})$ time for general graphs, and, to the best of\nour knowledge, it has not been described before. We subsequently show how to\nimplement this algorithm faster on planar graphs using well-established tools:\n$r$-divisions and efficient algorithms for computing (shortest) paths in\nso-called dense distance graphs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 08:35:02 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Karczmarz", "Adam", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1907.02308", "submitter": "Marinella Sciortino", "authors": "Raffaele Giancarlo and Giovanni Manzini and Antonio Restivo and\n  Giovanna Rosone and Marinella Sciortino", "title": "The Alternating BWT: an algorithmic perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform (BWT) is a word transformation introduced in\n1994 for Data Compression. It has become a fundamental tool for designing\nself-indexing data structures, with important applications in several area in\nscience and engineering. The Alternating Burrows-Wheeler Transform (ABWT) is\nanother transformation recently introduced in [Gessel et al. 2012] and studied\nin the field of Combinatorics on Words. It is analogous to the BWT, except that\nit uses an alternating lexicographical order instead of the usual one. Building\non results in [Giancarlo et al. 2018], where we have shown that BWT and ABWT\nare part of a larger class of reversible transformations, here we provide a\ncombinatorial and algorithmic study of the novel transform ABWT. We establish a\ndeep analogy between BWT and ABWT by proving they are the only ones in the\nabove mentioned class to be rank-invertible, a novel notion guaranteeing\nefficient invertibility. In addition, we show that the backward-search\nprocedure can be efficiently generalized to the ABWT; this result implies that\nalso the ABWT can be used as a basis for efficient compressed full text\nindices. Finally, we prove that the ABWT can be efficiently computed by using a\ncombination of the Difference Cover suffix sorting algorithm\n[K\\\"{a}rkk\\\"{a}inen et al., 2006] with a linear time algorithm for finding the\nminimal cyclic rotation of a word with respect to the alternating\nlexicographical order.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 09:56:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Giancarlo", "Raffaele", ""], ["Manzini", "Giovanni", ""], ["Restivo", "Antonio", ""], ["Rosone", "Giovanna", ""], ["Sciortino", "Marinella", ""]]}, {"id": "1907.02320", "submitter": "Arthur Charpentier", "authors": "Arthur Charpentier, Alfred Galichon, Lucas Vernet", "title": "Optimal transport on large networks, a practitioner's guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.DS econ.EM q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a set of tools for the modeling of a spatial allocation\nproblem in a large geographic market and gives examples of applications. In our\nsettings, the market is described by a network that maps the cost of travel\nbetween each pair of adjacent locations. Two types of agents are located at the\nnodes of this network. The buyers choose the most competitive sellers depending\non their prices and the cost to reach them. Their utility is assumed additive\nin both these quantities. Each seller, taking as given other sellers prices,\nsets her own price to have a demand equal to the one we observed. We give a\nlinear programming formulation for the equilibrium conditions. After formally\nintroducing our model we apply it on two examples: prices offered by petrol\nstations and quality of services provided by maternity wards. These examples\nillustrate the applicability of our model to aggregate demand, rank prices and\nestimate cost structure over the network. We insist on the possibility of\napplications to large scale data sets using modern linear programming solvers\nsuch as Gurobi. In addition to this paper we released a R toolbox to implement\nour results and an online tutorial (http://optimalnetwork.github.io)\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 10:45:57 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 20:07:06 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Charpentier", "Arthur", ""], ["Galichon", "Alfred", ""], ["Vernet", "Lucas", ""]]}, {"id": "1907.02353", "submitter": "Pierre Berg\\'e", "authors": "Pierre Berg\\'e, Benjamin Mouscadet, Arpad Rimmel, Joanna Tomasik", "title": "Fixed-parameter tractability of counting small minimum $(S,T)$-cuts", "comments": "13 pages, 10 figures, full version of the paper accepted in WG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameterized complexity of counting minimum cuts stands as a natural\nquestion because Ball and Provan showed its #P-completeness. For any undirected\ngraph $G=(V,E)$ and two disjoint sets of its vertices $S,T$, we design a\nfixed-parameter tractable algorithm which counts minimum edge $(S,T)$-cuts\nparameterized by their size $p$. Our algorithm operates on a transformed graph\ninstance. This transformation, called drainage, reveals a collection of at most\n$n=\\left| V \\right|$ successive minimum $(S,T)$-cuts $Z_i$. We prove that any\nminimum $(S,T)$-cut $X$ contains edges of at least one cut $Z_i$. This\nobservation, together with Menger's theorem, allows us to build the algorithm\ncounting all minimum $(S,T)$-cuts with running time $2^{O(p^2)}n^{O(1)}$.\nInitially dedicated to counting minimum cuts, it can be modified to obtain an\nFPT sampling of minimum edge $(S,T)$-cuts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:16:34 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 08:59:25 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Berg\u00e9", "Pierre", ""], ["Mouscadet", "Benjamin", ""], ["Rimmel", "Arpad", ""], ["Tomasik", "Joanna", ""]]}, {"id": "1907.02369", "submitter": "Simon Apers", "authors": "Simon Apers", "title": "Expansion Testing using Quantum Fast-Forwarding and Seed Sets", "comments": "v3: final version to appear in Quantum", "journal-ref": "Quantum 4, 323 (2020)", "doi": "10.22331/q-2020-09-16-323", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Expansion testing aims to decide whether an $n$-node graph has expansion at\nleast $\\Phi$, or is far from any such graph. We propose a quantum expansion\ntester with complexity $\\widetilde{O}(n^{1/3}\\Phi^{-1})$. This accelerates the\n$\\widetilde{O}(n^{1/2}\\Phi^{-2})$ classical tester by Goldreich and Ron\n[Algorithmica '02], and combines the $\\widetilde{O}(n^{1/3}\\Phi^{-2})$ and\n$\\widetilde{O}(n^{1/2}\\Phi^{-1})$ quantum speedups by Ambainis, Childs and Liu\n[RANDOM '11] and Apers and Sarlette [QIC '19], respectively. The latter\napproach builds on a quantum fast-forwarding scheme, which we improve upon by\ninitially growing a seed set in the graph. To grow this seed set we use a\nso-called evolving set process from the graph clustering literature, which\nallows to grow an appropriately local seed set.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:41:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 10:25:29 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 13:29:43 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Apers", "Simon", ""]]}, {"id": "1907.02513", "submitter": "Uri Stemmer", "authors": "Uri Stemmer", "title": "Locally Private k-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new algorithm for the Euclidean $k$-means problem that operates\nin the local model of differential privacy. Unlike in the non-private\nliterature, differentially private algorithms for the $k$-means objective incur\nboth additive and multiplicative errors. Our algorithm significantly reduces\nthe additive error while keeping the multiplicative error the same as in\nprevious state-of-the-art results. Specifically, on a database of size $n$, our\nalgorithm guarantees $O(1)$ multiplicative error and $\\approx n^{1/2+a}$\nadditive error for an arbitrarily small constant $a>0$. All previous algorithms\nin the local model had additive error $\\approx n^{2/3+a}$. Our techniques\nextend to $k$-median clustering.\n  We show that the additive error we obtain is almost optimal in terms of its\ndependency on the database size $n$. Specifically, we give a simple lower bound\nshowing that every locally-private algorithm for the $k$-means objective must\nhave additive error at least $\\approx\\sqrt{n}$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:50:33 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 00:29:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Stemmer", "Uri", ""]]}, {"id": "1907.02686", "submitter": "Toshinari Itoko", "authors": "Toshinari Itoko, Rudy Raymond, Takashi Imamichi and Atsushi Matsuo", "title": "Optimization of Quantum Circuit Mapping using Gate Transformation and\n  Commutation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses quantum circuit mapping for Noisy Intermediate-Scale\nQuantum (NISQ) computers. Since NISQ computers constraint two-qubit operations\non limited couplings, an input circuit must be transformed into an equivalent\noutput circuit obeying the constraints. The transformation often requires\nadditional gates that can affect the accuracy of running the circuit. Based\nupon a previous work of quantum circuit mapping that leverages gate commutation\nrules, this paper shows algorithms that utilize both transformation and\ncommutation rules. Experiments on a standard benchmark dataset confirm the\nalgorithms with more rules can find even better circuit mappings compared with\nthe previously-known best algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 06:08:08 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 03:31:29 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Itoko", "Toshinari", ""], ["Raymond", "Rudy", ""], ["Imamichi", "Takashi", ""], ["Matsuo", "Atsushi", ""]]}, {"id": "1907.02900", "submitter": "Oded Green", "authors": "Oded Green", "title": "HashGraph -- Scalable Hash Tables Using A Sparse Graph Data Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are ubiquitous and used in a wide range of applications for\nefficient probing of large and unsorted data. If designed properly, hash-tables\ncan enable efficients look ups in a constant number of operations or commonly\nreferred to as O(1) operations. As data sizes continue to grow and data becomes\nless structured (as is common for big-data applications), the need for\nefficient and scalable hash table also grows. In this paper we introduce\nHashGraph, a new scalable approach for building hash tables that uses concepts\ntaken from sparse graph representations--hence the name HashGraph. We show two\ndifferent variants of HashGraph, a simple algorithm that outlines the method to\ncreate the hash-table and an advanced method that creates the hash table in a\nmore efficient manner (with an improved memory access pattern). HashGraph shows\na new way to deal with hash-collisions that does not use \"open-addressing\" or\n\"chaining\", yet has all the benefits of both these approaches. HashGraph\ncurrently works for static inputs, though recent progress with dynamic graph\ndata structures suggest that HashGraph might be extended to dynamic inputs as\nwell. We show that HashGraph can deal with a large number of hash-values per\nentry without loss of performance as most open-addressing and chaining\napproaches have. Further, we show that HashGraph is indifferent to the\nload-factor. Lastly, we show a new probing algorithm for the second phase of\nvalue lookups. Given the above, HashGraph is extremely fast and outperforms\nseveral state of the art hash-table implementations. The implementation of\nHashGraph in this paper is for NVIDIA GPUs, though HashGraph is not\narchitecture dependent. Using a NVIDIA GV100 GPU, HashGraph is anywhere from\n2X-8X faster than cuDPP, WarpDrive, and cuDF. HashGraph is able to build a\nhash-table at a rate of 2.5 billion keys per second and can probe at nearly the\nsame rate.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:44:27 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Green", "Oded", ""]]}, {"id": "1907.02919", "submitter": "Dimitrios Thilikos", "authors": "Petr A. Golovach, Giannos Stamoulis, Dimitrios M. Thilikos", "title": "Hitting Topological Minor Models in Planar Graphs is Fixed Parameter\n  Tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finite collection of graphs ${\\cal F}$, the ${\\cal F}$-TM-Deletion\nproblem has as input an $n$-vertex graph $G$ and an integer $k$ and asks\nwhether there exists a set $S \\subseteq V(G)$ with $|S| \\leq k$ such that $G\n\\setminus S$ does not contain any of the graphs in ${\\cal F}$ as a topological\nminor. We prove that for every such ${\\cal F}$, ${\\cal F}$-TM-Deletion is fixed\nparameter tractable on planar graphs. In particular, we provide an $f(h,k)\\cdot\nn^{2}$ algorithm where $h$ is an upper bound to the vertices of the graphs in\n${\\cal F}$.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 16:39:22 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Golovach", "Petr A.", ""], ["Stamoulis", "Giannos", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1907.02929", "submitter": "S\\'ebastien Bougleux", "authors": "Nicolas Boria, David B. Blumenthal, S\\'ebastien Bougleux and Luc Brun", "title": "Improved local search for graph edit distance", "comments": null, "journal-ref": "Pattern Recognition Letters 129, pages 19-25, 2020", "doi": "10.1016/j.patrec.2019.10.028", "report-no": null, "categories": "cs.DS cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The graph edit distance (GED) measures the dissimilarity between two graphs\nas the minimal cost of a sequence of elementary operations transforming one\ngraph into another. This measure is fundamental in many areas such as\nstructural pattern recognition or classification. However, exactly computing\nGED is NP-hard. Among different classes of heuristic algorithms that were\nproposed to compute approximate solutions, local search based algorithms\nprovide the tightest upper bounds for GED. In this paper, we present K-REFINE\nand RANDPOST. K-REFINE generalizes and improves an existing local search\nalgorithm and performs particularly well on small graphs. RANDPOST is a general\nwarm start framework that stochastically generates promising initial solutions\nto be used by any local search based GED algorithm. It is particularly\nefficient on large graphs. An extensive empirical evaluation demonstrates that\nboth K-REFINE and RANDPOST perform excellently in practice.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 16:52:40 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 11:55:25 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Boria", "Nicolas", ""], ["Blumenthal", "David B.", ""], ["Bougleux", "S\u00e9bastien", ""], ["Brun", "Luc", ""]]}, {"id": "1907.03037", "submitter": "Saurabh Sawlani", "authors": "Saurabh Sawlani and Junxing Wang", "title": "Near-Optimal Fully Dynamic Densest Subgraph", "comments": "Updated version. Accepted at STOC '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first fully dynamic algorithm which maintains a\n$(1-\\epsilon)$-approximate densest subgraph in worst-case time\n$\\text{poly}(\\log n, \\epsilon^{-1})$ per update. Dense subgraph discovery is an\nimportant primitive for many real-world applications such as community\ndetection, link spam detection, distance query indexing, and computational\nbiology. We approach the densest subgraph problem by framing its dual as a\ngraph orientation problem, which we solve using an augmenting path-like\nadjustment technique. Our result improves upon the previous best approximation\nfactor of $\\left(\\frac{1}{4} - \\epsilon\\right)$ for fully dynamic densest\nsubgraph [Bhattacharya et. al., STOC `15]. We also extend our techniques to\nsolving the problem on vertex-weighted graphs with similar runtimes.\n  Additionally, we reduce the $(1-\\epsilon)$-approximate densest subgraph\nproblem on directed graphs to $O(\\log n/\\epsilon)$ instances of\n$(1-\\epsilon)$-approximate densest subgraph on vertex-weighted graphs. This\nreduction, together with our algorithm for vertex-weighted graphs, gives the\nfirst fully-dynamic algorithm for directed densest subgraph in worst-case time\n$\\text{poly}(\\log n, \\epsilon^{-1})$ per update. Moreover, combined with a\nnear-linear time algorithm for densest subgraph [Bahmani et. al., WAW `14],\nthis gives the first near-linear time algorithm for directed densest subgraph.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 22:37:35 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 22:12:33 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Sawlani", "Saurabh", ""], ["Wang", "Junxing", ""]]}, {"id": "1907.03129", "submitter": "Hyung-Chan An", "authors": "Kangsan Kim, Yongho Shin, Hyung-Chan An", "title": "Constant-Factor Approximation Algorithms for Parity-Constrained Facility\n  Location Problems", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facility location is a prominent optimization problem that has inspired a\nlarge quantity of both theoretical and practical studies in combinatorial\noptimization. Although the problem has been investigated under various settings\nreflecting typical structures within the optimization problems of practical\ninterest, little is known on how the problem behaves in conjunction with parity\nconstraints. This shortfall of understanding was rather disturbing when we\nconsider the central role of parity in the field of combinatorics. In this\npaper, we present the first constant-factor approximation algorithm for the\nfacility location problem with parity constraints. We are given as the input a\nmetric on a set of facilities and clients, the opening cost of each facility,\nand the parity requirement--odd, even, or unconstrained--of every facility in\nthis problem. The objective is to open a subset of facilities and assign every\nclient to an open facility so as to minimize the sum of the total opening costs\nand the assignment distances, but subject to the condition that the number of\nclients assigned to each open facility must have the same parity as its\nrequirement.\n  Although the unconstrained facility location problem as a relaxation for this\nparity-constrained generalization has unbounded gap, we demonstrate that it\nyields a structured solution whose parity violation can be corrected at small\ncost. This correction is prescribed by a T-join on an auxiliary graph\nconstructed by the algorithm. This graph does not satisfy the triangle\ninequality, but we show that a carefully chosen set of shortcutting operations\nleads to a cheap and sparse T-join. Finally, we bound the correction cost by\nexhibiting a combinatorial multi-step construction of an upper bound. We also\npresent the first constant-factor approximation algorithm for the\nparity-constrained k-center problem, the bottleneck optimization variant.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 14:20:06 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Kim", "Kangsan", ""], ["Shin", "Yongho", ""], ["An", "Hyung-Chan", ""]]}, {"id": "1907.03182", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Themis Gouleakis, John Peebles, Ronitt Rubinfeld,\n  Anak Yodpinyanee", "title": "Towards Testing Monotonicity of Distributions Over General Posets", "comments": "Appeared in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the sample complexity required for testing the\nmonotonicity of distributions over partial orders. A distribution $p$ over a\nposet is monotone if, for any pair of domain elements $x$ and $y$ such that $x\n\\preceq y$, $p(x) \\leq p(y)$. To understand the sample complexity of this\nproblem, we introduce a new property called bigness over a finite domain, where\nthe distribution is $T$-big if the minimum probability for any domain element\nis at least $T$. We establish a lower bound of $\\Omega(n/\\log n)$ for testing\nbigness of distributions on domains of size $n$. We then build on these lower\nbounds to give $\\Omega(n/\\log{n})$ lower bounds for testing monotonicity over a\nmatching poset of size $n$ and significantly improved lower bounds over the\nhypercube poset. We give sublinear sample complexity bounds for testing bigness\nand for testing monotonicity over the matching poset.\n  We then give a number of tools for analyzing upper bounds on the sample\ncomplexity of\n  the monotonicity testing problem.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:45:01 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Gouleakis", "Themis", ""], ["Peebles", "John", ""], ["Rubinfeld", "Ronitt", ""], ["Yodpinyanee", "Anak", ""]]}, {"id": "1907.03190", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Ravi Kumar, Ronitt Rubinfeld", "title": "Testing Mixtures of Discrete Distributions", "comments": "Appeared in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant study on the sample complexity of testing\nproperties of distributions over large domains. For many properties, it is\nknown that the sample complexity can be substantially smaller than the domain\nsize. For example, over a domain of size $n$, distinguishing the uniform\ndistribution from distributions that are far from uniform in $\\ell_1$-distance\nuses only $O(\\sqrt{n})$ samples.\n  However, the picture is very different in the presence of arbitrary noise,\neven when the amount of noise is quite small. In this case, one must\ndistinguish if samples are coming from a distribution that is $\\epsilon$-close\nto uniform from the case where the distribution is $(1-\\epsilon)$-far from\nuniform. The latter task requires nearly linear in $n$ samples [Valiant 2008,\nValian and Valiant 2011].\n  In this work, we present a noise model that on one hand is more tractable for\nthe testing problem, and on the other hand represents a rich class of noise\nfamilies. In our model, the noisy distribution is a mixture of the original\ndistribution and noise, where the latter is known to the tester either\nexplicitly or via sample access; the form of the noise is also known a priori.\nFocusing on the identity and closeness testing problems leads to the following\nmixture testing question: Given samples of distributions $p, q_1,q_2$, can we\ntest if $p$ is a mixture of $q_1$ and $q_2$? We consider this general question\nin various scenarios that differ in terms of how the tester can access the\ndistributions, and show that indeed this problem is more tractable. Our results\nshow that the sample complexity of our testers are exactly the same as for the\nclassical non-mixture case.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 21:24:54 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Kumar", "Ravi", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1907.03197", "submitter": "Sepideh Mahabadi", "authors": "Piotr Indyk, Sepideh Mahabadi, Shayan Oveis Gharan, Alireza Rezaei", "title": "Composable Core-sets for Determinant Maximization: A Simple Near-Optimal\n  Algorithm", "comments": "This paper has appeared in the 36th International Conference on\n  Machine Learning (ICML), 2019. This is an equal contribution paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ``Composable core-sets'' are an efficient framework for solving optimization\nproblems in massive data models. In this work, we consider efficient\nconstruction of composable core-sets for the determinant maximization problem.\nThis can also be cast as the MAP inference task for determinantal point\nprocesses, that have recently gained a lot of interest for modeling diversity\nand fairness. The problem was recently studied in [IMOR'18], where they\ndesigned composable core-sets with the optimal approximation bound of $\\tilde\nO(k)^k$. On the other hand, the more practical Greedy algorithm has been\npreviously used in similar contexts. In this work, first we provide a\ntheoretical approximation guarantee of $O(C^{k^2})$ for the Greedy algorithm in\nthe context of composable core-sets; Further, we propose to use a Local Search\nbased algorithm that while being still practical, achieves a nearly optimal\napproximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms\nand show the effectiveness of our proposed algorithm on standard data sets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:22:34 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Indyk", "Piotr", ""], ["Mahabadi", "Sepideh", ""], ["Gharan", "Shayan Oveis", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1907.03201", "submitter": "Corwin Sinnamon", "authors": "Corwin Sinnamon", "title": "Fast and Simple Edge-Coloring Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop sequential algorithms for constructing edge-colorings of graphs\nand multigraphs efficiently and using few colors. Our primary focus is\nedge-coloring arbitrary simple graphs using $d+1$ colors, where $d$ is the\nlargest vertex degree in the graph. Vizing's Theorem states that every simple\ngraph can be edge-colored using $d+1$ colors. Although some graphs can be\nedge-colored using only $d$ colors, it is NP-hard to recognize graphs of this\ntype [Holyer, 1981]. So using $d+1$ colors is a natural goal. Efficient\ntechniques for $(d+1)$-edge-coloring were developed by Gabow, Nishizeki, Kariv,\nLeven, and Terada in 1985, and independently by Arjomandi in 1982, leading to\nalgorithms that run in $O(|E| \\sqrt{|V| \\log |V|})$ time. They have remained\nthe fastest known algorithms for this task.\n  We improve the runtime to $O(|E| \\sqrt{|V|})$ with a small modification and\ncareful analysis. We then develop a randomized version of the algorithm that is\nmuch simpler to implement and has the same asymptotic runtime, with very high\nprobability. On the way to these results, we give a simple algorithm for\n$(2d-1)$-edge-coloring of multigraphs that runs in $O(|E|\\log d)$ time.\nUnderlying these algorithms is a general edge-coloring strategy which may lend\nitself to further applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:36:21 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 05:28:21 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 16:58:36 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 01:32:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sinnamon", "Corwin", ""]]}, {"id": "1907.03235", "submitter": "Jonas Ellert", "authors": "Patrick Dinklage, Jonas Ellert, Johannes Fischer, Dominik K\\\"oppl,\n  Manuel Penschuck", "title": "Bidirectional Text Compression in External Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional compression algorithms work by substituting repeated substrings\nby references that, unlike in the famous LZ77-scheme, can point to either\ndirection. We present such an algorithm that is particularly suited for an\nexternal memory implementation. We evaluate it experimentally on large data\nsets of size up to 128 GiB (using only 16 GiB of RAM) and show that it is\nsignificantly faster than all known LZ77 compressors, while producing a roughly\nsimilar number of factors. We also introduce an external memory decompressor\nfor texts compressed with any uni- or bidirectional compression scheme.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 07:33:13 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 03:37:03 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 12:05:34 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Dinklage", "Patrick", ""], ["Ellert", "Jonas", ""], ["Fischer", "Johannes", ""], ["K\u00f6ppl", "Dominik", ""], ["Penschuck", "Manuel", ""]]}, {"id": "1907.03266", "submitter": "Florent Foucaud", "authors": "Fran\\c{c}ois Dross, Florent Foucaud, Valia Mitsou, Pascal Ochem,\n  Th\\'eo Pierron", "title": "Complexity of planar signed graph homomorphisms to cycles", "comments": "17 pages, 10 figures", "journal-ref": "Discrete Applied Mathematics 284:166-178, 2020", "doi": "10.1016/j.dam.2020.03.029", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study homomorphism problems of signed graphs. A signed graph is an\nundirected graph where each edge is given a sign, positive or negative. An\nimportant concept for signed graphs is the operation of switching at a vertex,\nwhich is to change the sign of each incident edge. A homomorphism of a graph is\na vertex-mapping that preserves the adjacencies; in the case of signed graphs,\nwe also preserve the edge-signs. Special homomorphisms of signed graphs, called\ns-homomorphisms, have been studied. In an s-homomorphism, we allow, before the\nmapping, to perform any number of switchings on the source signed graph. This\nconcept has been extensively studied, and a full complexity classification\n(polynomial or NP-complete) for s-homomorphism to a fixed target signed graph\nhas recently been obtained. Such a dichotomy is not known when we restrict the\ninput graph to be planar (not even for non-signed graph homomorphisms).\n  We show that deciding whether a (non-signed) planar graph admits a\nhomomorphism to the square $C_t^2$ of a cycle with $t\\ge 6$, or to the circular\nclique $K_{4t/(2t-1)}$ with $t\\ge2$, are NP-complete problems. We use these\nresults to show that deciding whether a planar signed graph admits an\ns-homomorphism to an unbalanced even cycle is NP-complete. (A cycle is\nunbalanced if it has an odd number of negative edges). We deduce a complete\ncomplexity dichotomy for the planar s-homomorphism problem with any signed\ncycle as a target.\n  We also study further restrictions involving the maximum degree and the girth\nof the input signed graph. We prove that planar s-homomorphism problems to\nsigned cycles remain NP-complete even for inputs of maximum degree~$3$ (except\nfor the case of unbalanced $4$-cycles, for which we show this for maximum\ndegree~$4$). We also show that for a given integer $g$, the problem for signed\nbipartite planar inputs of girth $g$ is either trivial or NP-complete.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 10:44:00 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 08:48:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dross", "Fran\u00e7ois", ""], ["Foucaud", "Florent", ""], ["Mitsou", "Valia", ""], ["Ochem", "Pascal", ""], ["Pierron", "Th\u00e9o", ""]]}, {"id": "1907.03437", "submitter": "Po-Chun Kuo", "authors": "Tzu-Wei Chao and Hao Chung and Po-Chun Kuo", "title": "Fair Byzantine Agreements for Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine general problem is the core problem of the consensus algorithm, and\nmany protocols are proposed recently to improve the decentralization level, the\nperformance and the security of the blockchain. There are two challenging\nissues when the blockchain is operating in practice. First, the outcomes of the\nconsensus algorithm are usually related to the incentive model, so whether each\nparticipant's value has an equal probability of being chosen becomes essential.\nHowever, the issues of fairness are not captured in the traditional security\ndefinition of Byzantine agreement. Second, the blockchain should be resistant\nto network failures, such as cloud services shut down or malicious attack,\nwhile remains the high performance most of the time.\n  This paper has two main contributions. First, we propose a novel notion\ncalled fair validity for Byzantine agreement. Intuitively, fair validity\nlower-bounds the expected numbers that honest nodes' values being decided if\nthe protocol is executed many times. However, we also show that any Byzantine\nagreement could not achieve fair validity in an asynchronous network, so we\nfocus on synchronous protocols. This leads to our second contribution: we\npropose a fair, responsive and partition-resilient Byzantine agreement protocol\ntolerating up to 1/3 corruptions. Fairness means that our protocol achieves\nfair validity. Responsiveness means that the termination time only depends on\nthe actual network delay instead of depending on any pre-determined time bound.\nPartition-resilience means that the safety still holds even if the network is\npartitioned, and the termination will hold if the partition is resolved.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 07:43:24 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Chao", "Tzu-Wei", ""], ["Chung", "Hao", ""], ["Kuo", "Po-Chun", ""]]}, {"id": "1907.03526", "submitter": "Marten Maack", "authors": "Marten Maack and Klaus Jansen", "title": "Inapproximability Results for Scheduling with Interval and Resource\n  Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the restricted assignment problem, the input consists of a set of machines\nand a set of jobs each with a processing time and a subset of eligible\nmachines. The goal is to find an assignment of the jobs to the machines\nminimizing the makespan, that is, the maximum summed up processing time any\nmachine receives. Herein, jobs should only be assigned to those machines on\nwhich they are eligible. It is well-known that there is no polynomial time\napproximation algorithm with an approximation guarantee of less than 1.5 for\nthe restricted assignment problem unless P=NP. In this work, we show hardness\nresults for variants of the restricted assignment problem with particular types\nof restrictions.\n  In the case of interval restrictions the machines can be totally ordered such\nthat jobs are eligible on consecutive machines. We resolve the open question of\nwhether the problem admits a polynomial time approximation scheme (PTAS) in the\nnegative (unless P=NP). There are several special cases of this problem known\nto admit a PTAS.\n  Furthermore, we consider a variant with resource restriction where each\nmachine has capacities and each job demands for a fixed number of resources. A\njob is eligible on a machine if its demand is at most the capacity of the\nmachine for each resource. For one resource, this problem is known to admit a\nPTAS, for two, the case of interval restrictions is contained, and in general,\nthe problem is closely related to unrelated scheduling with a low rank\nprocessing time matrix. We show that there is no polynomial time approximation\nalgorithm with a rate smaller than 48/47 or 1.5 for scheduling with resource\nrestrictions with 2 or 4 resources, respectively, unless P=NP. All our results\ncan be extended to the so called Santa Claus variants of the problems where the\ngoal is to maximize the minimal processing time any machine receives.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:47:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Maack", "Marten", ""], ["Jansen", "Klaus", ""]]}, {"id": "1907.03535", "submitter": "Demian Hespe", "authors": "Demian Hespe, Peter Sanders", "title": "More Hierarchy in Route Planning Using Edge Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly successful approach to route planning in networks (particularly road\nnetworks) is to identify a hierarchy in the network that allows faster queries\nafter some preprocessing that basically inserts additional \"shortcut\"-edges\ninto a graph. In the past there has been a succession of techniques that infer\na more and more fine grained hierarchy enabling increasingly more efficient\nqueries. This appeared to culminate in contraction hierarchies that assign one\nhierarchy level to each vertex.\n  In this paper we show how to identify an even more fine grained hierarchy\nthat assigns one level to each edge of the network. Our findings indicate that\nthis can lead to considerably smaller search spaces in terms of visited edges.\nCurrently, this rarely implies improved query times so that it remains an open\nquestion whether edge hierarchies can lead to consistently improved\nperformance. However, we believe that the technique as such is a noteworthy\nenrichment of the portfolio of available techniques that might prove useful in\nthe future.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 12:04:28 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 08:22:38 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Hespe", "Demian", ""], ["Sanders", "Peter", ""]]}, {"id": "1907.03620", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "Contraction Clustering (RASTER): A Very Fast Big Data Algorithm for\n  Sequential and Parallel Density-Based Clustering in Linear Time, Constant\n  Memory, and a Single Pass", "comments": "19 pages; journal paper extending a previous conference publication\n  (cf. https://doi.org/10.1007/978-3-319-72926-8_6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an essential data mining tool for analyzing and grouping\nsimilar objects. In big data applications, however, many clustering algorithms\nare infeasible due to their high memory requirements and/or unfavorable runtime\ncomplexity. In contrast, Contraction Clustering (RASTER) is a single-pass\nalgorithm for identifying density-based clusters with linear time complexity.\nDue to its favorable runtime and the fact that its memory requirements are\nconstant, this algorithm is highly suitable for big data applications where the\namount of data to be processed is huge. It consists of two steps: (1) a\ncontraction step which projects objects onto tiles and (2) an agglomeration\nstep which groups tiles into clusters. This algorithm is extremely fast in both\nsequential and parallel execution. Our quantitative evaluation shows that a\nsequential implementation of RASTER performs significantly better than various\nstandard clustering algorithms. Furthermore, the parallel speedup is\nsignificant: on a contemporary workstation, an implementation in Rust processes\na batch of 500 million points with 1 million clusters in less than 50 seconds\non one core. With 8 cores, the algorithm is about four times faster.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:00:07 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 15:50:32 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1907.03797", "submitter": "Fabian Kuhn", "authors": "Fabian Kuhn", "title": "Faster Deterministic Distributed Coloring Through Recursive List\n  Coloring", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel deterministic distributed vertex coloring algorithms. As our\nmain result, we give a deterministic distributed algorithm to compute a\n$(\\Delta+1)$-coloring of an $n$-node graph with maximum degree $\\Delta$ in\n$2^{O(\\sqrt{\\log\\Delta})}\\cdot\\log n$ rounds. This improves on the best\npreviously known time complexity for a large range of values of $\\Delta$. For\ngraphs with arboricity $a$, we obtain a deterministic distributed algorithm to\ncompute a $(2+o(1))a$-coloring in time $2^{O(\\sqrt{\\log a})}\\cdot\\log^2 n$.\nFurther, for graphs with bounded neighborhood independence, we show that a\n$(\\Delta+1)$-coloring can be computed more efficiently in time\n$2^{O(\\sqrt{\\log\\Delta})} + O(\\log^* n)$. This in particular implies that also\na $(2\\Delta-1)$-edge coloring can be computed deterministically in\n$2^{O(\\sqrt{\\log\\Delta})} + O(\\log^* n)$ rounds, which improves the best known\ntime bound for small values of $\\Delta$. All results even hold for the list\ncoloring variants of the problems. As a consequence, we also obtain an improved\ndeterministic $2^{O(\\sqrt{\\log\\Delta})}\\cdot\\log^3 n$-round algorithm for\n$\\Delta$-coloring non-complete graphs with maximum degree $\\Delta\\geq 3$. Most\nof our algorithms only require messages of $O(\\log n)$ bits (including the\n$(\\Delta+1)$-vertex coloring algorithms).\n  Our main technical contribution is a recursive deterministic distributed list\ncoloring algorithm to solve list coloring problems with lists of size\n$\\Delta^{1+o(1)}$. Given some list coloring problem and an orientation of the\nedges, we show how to recursively divide the global color space into smaller\nsubspaces, assign one of the subspaces to each node of the graph, and compute a\nnew edge orientation such that for each node, the list size to out-degree ratio\ndegrades at most by a constant factor on each recursion level.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:22:43 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kuhn", "Fabian", ""]]}, {"id": "1907.03850", "submitter": "Philip Wellnitz", "authors": "Marc Roth, Philip Wellnitz", "title": "Counting and Finding Homomorphisms is Universal for Parameterized\n  Complexity Theory", "comments": "42 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting homomorphisms from a graph $H$ into another graph $G$ is a\nfundamental problem of (parameterized) counting complexity theory. In this\nwork, we study the case where \\emph{both} graphs $H$ and $G$ stem from given\nclasses of graphs: $H\\in \\mathcal{H}$ and $G\\in \\mathcal{G}$. By this, we\ncombine the structurally restricted version of this problem, with the\nlanguage-restricted version.\n  Our main result is a construction based on Kneser graphs that associates\nevery problem $\\tt P$ in $\\#\\mathsf{W[1]}$ with two classes of graphs\n$\\mathcal{H}$ and $\\mathcal{G}$ such that the problem $\\tt P$ is\n\\emph{equivalent} to the problem $\\#{\\tt HOM}(\\mathcal{H}\\to \\mathcal{G})$ of\ncounting homomorphisms from a graph in $\\mathcal{H}$ to a graph in\n$\\mathcal{G}$. In view of Ladner's seminal work on the existence of\n$\\mathsf{NP}$-intermediate problems [J.ACM'75] and its adaptations to the\nparameterized setting, a classification of the class $\\#\\mathsf{W[1]}$ in\nfixed-parameter tractable and $\\#\\mathsf{W[1]}$-complete cases is unlikely.\nHence, obtaining a complete classification for the problem $\\#{\\tt\nHOM}(\\mathcal{H}\\to \\mathcal{G})$ seems unlikely. Further, our proofs easily\nadapt to $\\mathsf{W[1]}$.\n  In search of complexity dichotomies, we hence turn to special graph classes.\nThose classes include line graphs, claw-free graphs, perfect graphs, and\ncombinations thereof, and $F$-colorable graphs for fixed graphs $F$: If the\nclass $\\mathcal{G}$ is one of those classes and the class $\\mathcal{H}$ is\nclosed under taking minors, then we establish explicit criteria for the class\n$\\mathcal{H}$ that partition the family of problems $\\#{\\tt\nHOM}(\\mathcal{H}\\to\\mathcal{G})$ into polynomial-time solvable and\n$\\#\\mathsf{W[1]}$-hard cases. In particular, we can drop the condition of\n$\\mathcal{H}$ being minor-closed for $F$-colorable graphs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:18:58 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Roth", "Marc", ""], ["Wellnitz", "Philip", ""]]}, {"id": "1907.03863", "submitter": "Theresa Migler-VonDollen", "authors": "Sean Gonzales and Theresa Migler", "title": "The Densest k Subgraph Problem in b-Outerplanar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an exact $O(nk^2)$ algorithm for finding the densest k subgraph in\nouterplanar graphs. We extend this to an exact $O(nk^2 8^b)$ algorithm for\nfinding the densest k subgraph in b-outerplanar graphs. Finally, we hypothesize\nthat Baker's PTAS technique will not work for the densest k subgraph problem in\nplanar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:45:56 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Gonzales", "Sean", ""], ["Migler", "Theresa", ""]]}, {"id": "1907.03931", "submitter": "Haotian Jiang", "authors": "Venkatesan Guruswami and Haotian Jiang", "title": "Near-optimal Repair of Reed-Solomon Codes with Low Sub-packetization", "comments": "In ISIT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum storage regenerating (MSR) codes are MDS codes which allow for\nrecovery of any single erased symbol with optimal repair bandwidth, based on\nthe smallest possible fraction of the contents downloaded from each of the\nother symbols. Recently, certain Reed-Solomon codes were constructed which are\nMSR. However, the sub-packetization of these codes is exponentially large,\ngrowing like $n^{\\Omega(n)}$ in the constant-rate regime. In this work, we\nstudy the relaxed notion of $\\epsilon$-MSR codes, which incur a factor of\n$(1+\\epsilon)$ higher than the optimal repair bandwidth, in the context of\nReed-Solomon codes. We give constructions of constant-rate $\\epsilon$-MSR\nReed-Solomon codes with polynomial sub-packetization of $n^{O(1/\\epsilon)}$ and\nthereby giving an explicit tradeoff between the repair bandwidth and\nsub-packetization.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:21:27 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Jiang", "Haotian", ""]]}, {"id": "1907.03963", "submitter": "Nathaniel Grammel", "authors": "Brian Brubach, Nathaniel Grammel, Will Ma and Aravind Srinivasan", "title": "Follow Your Star: New Frameworks for Online Stochastic Matching with\n  Known and Unknown Patience", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.MA math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study several generalizations of the Online Bipartite Matching problem. We\nconsider settings with stochastic rewards, patience constraints, and weights\n(both vertex- and edge-weighted variants). We introduce a stochastic variant of\nthe patience-constrained problem, where the patience is chosen randomly\naccording to some known distribution and is not known until the point at which\npatience has been exhausted. We also consider stochastic arrival settings\n(i.e., online vertex arrival is determined by a known random process), which\nare natural settings that are able to beat the hard worst-case bounds of more\npessimistic adversarial arrivals.\n  Our approach to online matching utilizes black-box algorithms for matching on\nstar graphs under various models of patience. In support of this, we design\nalgorithms which solve the star graph problem optimally for patience with a\nconstant hazard rate and yield a 1/2-approximation for any patience\ndistribution. This 1/2-approximation also improves existing guarantees for\ncascade-click models in the product ranking literature, in which a user must be\nshown a sequence of items with various click-through-rates and the user's\npatience could run out at any time.\n  We then build a framework which uses these star graph algorithms as black\nboxes to solve the online matching problems under different arrival settings.\nWe show improved (or first-known) competitive ratios for these problems.\nFinally, we present negative results that include formalizing the concept of a\nstochasticity gap for LP upper bounds on these problems, bounding the\nworst-case performance of some popular greedy approaches, and showing the\nimpossibility of having an adversarial patience in the product ranking setting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:31:24 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 01:13:41 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 23:44:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Brubach", "Brian", ""], ["Grammel", "Nathaniel", ""], ["Ma", "Will", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1907.04065", "submitter": "Kurt Mehlhorn", "authors": "Mohammad Abdulaziz and Kurt Mehlhorn and Tobias Nipkow", "title": "Trustworthy Graph Algorithms", "comments": "to appear in MFCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the LEDA project was to build an easy-to-use and extendable\nlibrary of correct and efficient data structures, graph algorithms and\ngeometric algorithms. We report on the use of formal program verification to\nachieve an even higher level of trustworthiness. Specifically, we report on an\nongoing and largely finished verification of the blossom-shrinking algorithm\nfor maximum cardinality matching.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 10:06:30 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Abdulaziz", "Mohammad", ""], ["Mehlhorn", "Kurt", ""], ["Nipkow", "Tobias", ""]]}, {"id": "1907.04083", "submitter": "Takanori Maehara", "authors": "Takanori Maehara and Yutaro Yamaguchi", "title": "Stochastic Monotone Submodular Maximization with Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a stochastic variant of monotone submodular maximization problem as\nfollows. We are given a monotone submodular function as an objective function\nand a feasible domain defined on a finite set, and our goal is to find a\nfeasible solution that maximizes the objective function. A special part of the\nproblem is that each element in the finite set has a random hidden state,\nactive or inactive, only the active elements contribute to the objective value,\nand we can conduct a query to an element to reveal its hidden state. The goal\nis to obtain a feasible solution having a large objective value by conducting a\nsmall number of queries. This is the first attempt to consider nonlinear\nobjective functions in such a stochastic model. We prove that the problem\nadmits a good query strategy if the feasible domain has a uniform exchange\nproperty. This result generalizes Blum et al.'s result on the unweighted\nmatching problem and Behnezhad and Reyhani's result on the weighted matching\nproblem in both objective function and feasible domain.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:20:51 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 11:08:54 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Maehara", "Takanori", ""], ["Yamaguchi", "Yutaro", ""]]}, {"id": "1907.04087", "submitter": "Takanori Maehara", "authors": "Soh Kumabe and Takanori Maehara", "title": "PTAS and Exact Algorithms for $r$-Gathering Problems on Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  r-gathering problem is a variant of facility location problems. In this\nproblem, we are given a set of users and a set of facilities on same metric\nspace. We open some of the facilities and assign each user to an open facility,\nso that at least r users are assigned to every open facility. We aim to\nminimize the maximum distance between user and assigned facility. In general,\nthis problem is NP-hard and admit an approximation algorithm with factor 3. It\nis known that the problem does not admit any approximation algorithm within a\nfactor less than 3. In our another paper, we proved that this problem is\nNP-hard even on spider, which is a special case of tree metric. In this paper,\nwe concentrate on the problems on a tree. First, we give a PTAS for r-gathering\nproblem on a tree. Furthermore, we give PTAS for some variants of the problems\non a tree, and also give exact polynomial-time algorithms for another variants\nof r-gathering problem on a tree.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:29:43 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kumabe", "Soh", ""], ["Maehara", "Takanori", ""]]}, {"id": "1907.04088", "submitter": "Takanori Maehara", "authors": "Soh Kumabe and Takanori Maehara", "title": "$r$-Gather Clustering and $r$-Gathering on Spider: FPT Algorithms and\n  Hardness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider min-max $r$-gather clustering problem and min-max $r$-gathering\nproblem. In the min-max $r$-gather clustering problem, we are given a set of\nusers and divide them into clusters with size at least $r$; the goal is to\nminimize the maximum diameter of clusters. In the min-max $r$-gathering\nproblem, we are additionally given a set of facilities and assign each cluster\nto a facility; the goal is to minimize the maximum distance between the users\nand the assigned facility. In this study, we consider the case that the users\nand facilities are located on a ``spider'' and propose the first\nfixed-parameter tractable (FPT) algorithms for both problems, which are\nparametrized by only the number of legs. Furthermore, we prove that these\nproblems are NP-hard when the number of legs is arbitrarily large.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:33:01 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kumabe", "Soh", ""], ["Maehara", "Takanori", ""]]}, {"id": "1907.04117", "submitter": "Aditya Potukuchi", "authors": "Aditya Potukuchi", "title": "A spectral bound on hypergraph discrepancy", "comments": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:1811.01491, several changes to the presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{H}$ be a $t$-regular hypergraph on $n$ vertices and $m$ edges.\nLet $M$ be the $m \\times n$ incidence matrix of $\\mathcal{H}$ and let us denote\n$\\lambda =\\max_{v \\perp \\overline{1},\\|v\\| = 1}\\|Mv\\|$. We show that the\ndiscrepancy of $\\mathcal{H}$ is $O(\\sqrt{t} + \\lambda)$. As a corollary, this\ngives us that for every $t$, the discrepancy of a random $t$-regular hypergraph\nwith $n$ vertices and $m \\geq n$ edges is almost surely $O(\\sqrt{t})$ as $n$\ngrows. The proof also gives a polynomial time algorithm that takes a hypergraph\nas input and outputs a coloring with the above guarantee.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 15:32:35 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:45:07 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 14:02:05 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2019 13:15:59 GMT"}, {"version": "v5", "created": "Mon, 4 May 2020 17:41:29 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Potukuchi", "Aditya", ""]]}, {"id": "1907.04132", "submitter": "Erlend Raa Vaagset", "authors": "Svein H{\\o}gemo, Jan Arne Telle and Erlend Raa V{\\aa}gset", "title": "Linear MIM-Width of Trees", "comments": "19 pages, 7 figures, full version of WG19 paper of same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an $O(n \\log n)$ algorithm computing the linear maximum induced\nmatching width of a tree and an optimal layout.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:14:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["H\u00f8gemo", "Svein", ""], ["Telle", "Jan Arne", ""], ["V\u00e5gset", "Erlend Raa", ""]]}, {"id": "1907.04211", "submitter": "Genaro J. Martinez", "authors": "Sergio J. Martinez, Ivan M. Mendoza, Genaro J. Martinez, Shigeru\n  Ninagawa", "title": "Universal One-Dimensional Cellular Automata Derived for Turing Machines\n  and its Dynamical Behaviour", "comments": "18 pages, 8 tables, 3 figures.\n  https://www.oldcitypublishing.com/journals/ijuc-home/ijuc-issue-contents/ijuc-volume-14-number-2-2019/ijuc-14-2-p-121-138/", "journal-ref": "International Journal of Unconventional Computing 14(2) pages\n  121-138, 2019", "doi": null, "report-no": null, "categories": "nlin.CG cs.CL cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universality in cellular automata theory is a central problem studied and\ndeveloped from their origins by John von Neumann. In this paper, we present an\nalgorithm where any Turing machine can be converted to one-dimensional cellular\nautomaton with a 2-linear time and display its spatial dynamics. Three\nparticular Turing machines are converted in three universal one-dimensional\ncellular automata, they are: binary sum, rule 110 and a universal reversible\nTuring machine.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:12:13 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Martinez", "Sergio J.", ""], ["Mendoza", "Ivan M.", ""], ["Martinez", "Genaro J.", ""], ["Ninagawa", "Shigeru", ""]]}, {"id": "1907.04217", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Michael Houle, Michael Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "comments": "6 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) Conference 2019. arXiv admin note: text overlap with\n  arXiv:1807.05308, arXiv:1902.00846", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916508", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Distributed Dimensional Data Model (D4M) library implements\nassociative arrays in a variety of languages (Python, Julia, and Matlab/Octave)\nand provides a lightweight in-memory database implementation of hypersparse\narrays that are ideal for analyzing many types of network data. D4M relies on\nassociative arrays which combine properties of spreadsheets, databases,\nmatrices, graphs, and networks, while providing rigorous mathematical\nguarantees, such as linearity. Streaming updates of D4M associative arrays put\nenormous pressure on the memory hierarchy. This work describes the design and\nperformance optimization of an implementation of hierarchical associative\narrays that reduces memory pressure and dramatically increases the update rate\ninto an associative array. The parameters of hierarchical associative arrays\nrely on controlling the number of entries in each level in the hierarchy before\nan update is cascaded. The parameters are easily tunable to achieve optimal\nperformance for a variety of applications. Hierarchical arrays achieve over\n40,000 updates per second in a single instance. Scaling to 34,000 instances of\nhierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud\nachieved a sustained update rate of 1,900,000,000 updates per second. This\ncapability allows the MIT SuperCloud to analyze extremely large streaming\nnetwork data sets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:55:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.04274", "submitter": "Xue Chen", "authors": "Xue Chen and Anindya De", "title": "Reconstruction under outliers for Fourier-sparse functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning an unknown $f$ with a sparse Fourier\nspectrum in the presence of outlier noise. In particular, the algorithm has\naccess to a noisy oracle for (an unknown) $f$ such that (i) the Fourier\nspectrum of $f$ is $k$-sparse; (ii) at any query point $x$, the oracle returns\n$y$ such that with probability $1-\\rho$, $|y-f(x)| \\le \\epsilon$. However, with\nprobability $\\rho$, the error $y-f(x)$ can be arbitrarily large.\n  We study Fourier sparse functions over both the discrete cube $\\{0,1\\}^n$ and\nthe torus $[0,1)$ and for both these domains, we design efficient algorithms\nwhich can tolerate any $\\rho<1/2$ fraction of outliers. We note that the\nanalogous problem for low-degree polynomials has recently been studied in\nseveral works~[AK03, GZ16, KKP17] and similar algorithmic guarantees are known\nin that setting.\n  While our main results pertain to the case where the location of the\noutliers, i.e., $x$ such that $|y-f(x)|>\\epsilon$ is randomly distributed, we\nalso study the case where the outliers are adversarially located. In\nparticular, we show that over the torus, assuming that the Fourier transform\nsatisfies a certain \\emph{granularity} condition, there is a sample efficient\nalgorithm to tolerate $\\rho =\\Omega(1)$ fraction of outliers and further, that\nthis is not possible without such a granularity condition. Finally, while not\nthe principal thrust, our techniques also allow us non-trivially improve on\nlearning low-degree functions $f$ on the hypercube in the presence of\nadversarial outlier noise.\n  Our techniques combine a diverse array of tools from compressive sensing,\nsparse Fourier transform, chaining arguments and complex analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:24:06 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 22:06:07 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Xue", ""], ["De", "Anindya", ""]]}, {"id": "1907.04279", "submitter": "So Nakashima", "authors": "Takanori Maehara, So Nakashima, Yutaro Yamaguchi", "title": "Multiple Knapsack-Constrained Monotone DR-Submodular Maximization on\n  Distributive Lattice --- Continuous Greedy Algorithm on Median Complex ---", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of maximizing a monotone DR-submodular function under\nmultiple order-consistent knapsack constraints on a distributive lattice. Since\na distributive lattice is used to represent a dependency constraint, the\nproblem can represent a dependency constrained version of a submodular\nmaximization problem on a set. We propose a $1 - 1/e$ approximation algorithm\nfor this problem. To achieve this result, we generalize the continuous greedy\nalgorithm to distributive lattices: We choose a median complex as a continuous\nrelaxation of a distributive lattice and define the multilinear extension on\nit. We show that the median complex admits special curves, named uniform linear\nmotions, such that the multilinear extension of a DR-submodular function is\nconcave along a positive uniform linear motion, which is a key property of the\ncontinuous greedy algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:28:43 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Maehara", "Takanori", ""], ["Nakashima", "So", ""], ["Yamaguchi", "Yutaro", ""]]}, {"id": "1907.04295", "submitter": "Andre Esser", "authors": "Andre Esser and Alexander May", "title": "Better Sample -- Random Subset Sum in $2^{0.255n}$ and its Impact on\n  Decoding Random Linear Codes", "comments": "Issue with counting duplicate representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new heuristic algorithm for solving random subset sum instances\n$a_1, \\ldots, a_n, t \\in \\mathbb{Z}_{2^n}$, which play a crucial role in\ncryptographic constructions. Our algorithm is search tree-based and solves the\ninstances in a divide-and-conquer method using the representation method. From\na high level perspective, our algorithm is similar to the algorithm of\nHowgrave-Graham-Joux (HGJ) and Becker-Coron-Joux (BCJ), but instead of\nenumerating the initial lists we sample candidate solutions. So whereas HGJ and\nBCJ are based on combinatorics, our analysis is stochastic. Our sampling\ntechnique introduces variance that increases the amount of representations and\ngives our algorithm more optimization flexibility. This results in the\nremarkable and natural property that we improve with increasing search tree\ndepth.\n  Whereas BCJ achieves the currently best known (heuristic) run time\n$2^{0.291n}$ for random subset sum, we improve (heuristically) down to\n$2^{0.255n}$ using a search tree of depth at least $13$.\n  We also apply our subset algorithm to the decoding of random binary linear\ncodes, where we improve the best known run time of the Becker-Joux-May-Meurer\nalgorithm from $2^{0.048n}$ in the half distance decoding setting down to\n$2^{0.042n}$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:22:57 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 07:29:43 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Esser", "Andre", ""], ["May", "Alexander", ""]]}, {"id": "1907.04381", "submitter": "Amit Levi", "authors": "Xi Chen, Amit Levi, Erik Waingarten", "title": "Nearly optimal edge estimation with independent set queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the number of edges of an unknown,\nundirected graph $G=([n],E)$ with access to an independent set oracle. When\nqueried about a subset $S\\subseteq [n]$ of vertices the independent set oracle\nanswers whether $S$ is an independent set in $G$ or not. Our first main result\nis an algorithm that computes a $(1+\\epsilon)$-approximation of the number of\nedges $m$ of the graph using $\\min(\\sqrt{m},n /\n\\sqrt{m})\\cdot\\textrm{poly}(\\log n,1/\\epsilon)$ independent set queries. This\nimproves the upper bound of $\\min(\\sqrt{m},n^2/m)\\cdot\\textrm{poly}(\\log\nn,1/\\epsilon)$ by Beame et al. \\cite{BHRRS18}. Our second main result shows\nthat ${\\min(\\sqrt{m},n/\\sqrt{m}))/\\textrm{polylog}(n)}$ independent set queries\nare necessary, thus establishing that our algorithm is optimal up to a factor\nof $\\textrm{poly}(\\log n, 1/\\epsilon)$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:48:24 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Chen", "Xi", ""], ["Levi", "Amit", ""], ["Waingarten", "Erik", ""]]}, {"id": "1907.04383", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Venkatesan Guruswami, Marcin Wrochna, and Stanislav\n  \\v{Z}ivn\\'y", "title": "The Power of the Combined Basic LP and Affine Relaxation for Promise\n  CSPs", "comments": "17 pages, to appear in SICOMP", "journal-ref": "SIAM Journal on Computing 49(6) (2020) 1232-1248", "doi": "10.1137/20M1312745", "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of constraint satisfaction problems (CSP), promise CSPs are an\nexciting new direction of study. In a promise CSP, each constraint comes in two\nforms: \"strict\" and \"weak,\" and in the associated decision problem one must\ndistinguish between being able to satisfy all the strict constraints versus not\nbeing able to satisfy all the weak constraints. The most commonly cited example\nof a promise CSP is the approximate graph coloring problem--which has recently\nseen exciting progress [BKO19, WZ20] benefiting from a systematic algebraic\napproach to promise CSPs based on \"polymorphisms,\" operations that map tuples\nin the strict form of each constraint to tuples in the corresponding weak form.\n  In this work, we present a simple algorithm which in polynomial time solves\nthe decision problem for all promise CSPs that admit infinitely many symmetric\npolymorphisms, which are invariant under arbitrary coordinate permutations.\nThis generalizes previous work of the first two authors [BG19]. We also extend\nthis algorithm to a more general class of block-symmetric polymorphisms. As a\ncorollary, this single algorithm solves all polynomial-time tractable Boolean\nCSPs simultaneously. These results give a new perspective on Schaefer's classic\ndichotomy theorem and shed further light on how symmetries of polymorphisms\nenable algorithms. Finally, we show that block symmetric polymorphisms are not\nonly sufficient but also necessary for this algorithm to work, thus\nestablishing its precise power\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 21:35:32 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 14:35:12 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Guruswami", "Venkatesan", ""], ["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1907.04385", "submitter": "Laurent Viennot", "authors": "Guillaume Ducoffe, Michel Habib, Laurent Viennot", "title": "Diameter computation on $H$-minor free graphs and graphs of bounded\n  (distance) VC-dimension", "comments": "Submitted. Abstract shortened for the ArXiv listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study unweighted graphs of constant distance VC-dimension as a\nbroad generalization of many graph classes for which we can compute the\ndiameter in truly subquadratic-time. In particular for any fixed $H$, the class\nof $H$-minor free graphs has distance VC-dimension at most $|V(H)|-1$. Our\nfirst main result is that on graphs of distance VC-dimension at most $d$, for\nany fixed $k$ we can either compute the diameter or conclude that it is larger\nthan $k$ in time $\\tilde{\\cal O}(k\\cdot mn^{1-\\varepsilon_d})$, where\n$\\varepsilon_d \\in (0;1)$ only depends on $d$. Then as a byproduct of our\napproach, we get the first truly subquadratic-time algorithm for constant\ndiameter computation on all the nowhere dense graph classes. Finally, we show\nhow to remove the dependency on $k$ for any graph class that excludes a fixed\ngraph $H$ as a minor. More generally, our techniques apply to any graph with\nconstant distance VC-dimension and polynomial expansion. As a result for all\nsuch graphs one obtains a truly subquadratic-time algorithm for computing their\ndiameter. Our approach is based on the work of Chazelle and Welzl who proved\nthe existence of spanning paths with strongly sublinear stabbing number for\nevery hypergraph of constant VC-dimension. We show how to compute such paths\nefficiently by combining the best known approximation algorithms for the\nstabbing number problem with a clever use of $\\varepsilon$-nets, region\ndecomposition and other partition techniques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:57:19 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 10:01:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Ducoffe", "Guillaume", ""], ["Habib", "Michel", ""], ["Viennot", "Laurent", ""]]}, {"id": "1907.04399", "submitter": "Sergey Nikolenko", "authors": "Ivan Bochkov, Alex Davydow, Nikita Gaevoy, Sergey I. Nikolenko", "title": "New Competitiveness Bounds for the Shared Memory Switch", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider one of the simplest and best known buffer management\narchitectures: the shared memory switch with multiple output queues and uniform\npackets. It was one of the first models studied by competitive analysis, with\nthe Longest Queue Drop (LQD) buffer management policy shown to be at least\n$\\sqrt{2}$- and at most $2$-competitive; a general lower bound of $4/3$ has\nbeen proven for all deterministic online algorithms. Closing the gap between\n$\\sqrt{2}$ and $2$ has remained an open problem in competitive analysis for\nmore than a decade, with only marginal success in reducing the upper bound of\n$2$. In this work, we first present a simplified proof for the $\\sqrt{2}$ lower\nbound for LQD and then, using a reduction to the continuous case, improve the\ngeneral lower bound for all deterministic online algorithms from $\\frac 43$ to\n$\\sqrt{2}$. Then, we proceed to improve the lower bound of $\\sqrt{2}$\nspecifically for LQD, showing that LQD is at least $1.44546086$-competitive. We\nare able to prove the bound by presenting an explicit construction of the\noptimal clairvoyant algorithm which then allows for two different ways to prove\nlower bounds: by direct computer simulations and by proving lower bounds via\nlinear programming. The linear programming approach yields a lower bound for\nLQD of $1.4427902$ (still larger than $\\sqrt{2}$).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:33:39 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Bochkov", "Ivan", ""], ["Davydow", "Alex", ""], ["Gaevoy", "Nikita", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1907.04405", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Tatiana Starikovskaya, Michal Svagerka, Przemys{\\l}aw Uzna\\'nski", "title": "$L_p$ Pattern Matching in a Stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing distance between a pattern of length $n$\nand all $n$-length subwords of a text in the streaming model. In the streaming\nsetting, only the Hamming distance ($L_0$) has been studied. It is known that\ncomputing the exact Hamming distance between a pattern and a streaming text\nrequires $\\Omega(n)$ space (folklore). Therefore, to develop sublinear-space\nsolutions, one must relax their requirements. One possibility to do so is to\ncompute only the distances bounded by a threshold $k$, see~[SODA'19, Clifford,\nKociumaka, Porat] and references therein. The motivation for this variant of\nthis problem is that we are interested in subwords of the text that are similar\nto the pattern, i.e. in subwords such that the distance between them and the\npattern is relatively small. On the other hand, the main application of the\nstreaming setting is processing large-scale data, such as biological data.\nRecent advances in hardware technology allow generating such data at a very\nhigh speed, but unfortunately, the produced data may contain about 10\\% of\nnoise~[Biol. Direct.'07, Klebanov and Yakovlev]. To analyse such data, it is\nnot sufficient to consider small distances only. A possible workaround for this\nissue is the $(1\\pm\\varepsilon)$-approximation. This line of research was\ninitiated in [ICALP'16, Clifford and Starikovskaya] who gave a\n$(1\\pm\\varepsilon)$-approximation algorithm with\nspace~$\\tilde{O}(\\varepsilon^{-5}\\sqrt{n})$. In this work, we show a suite of\nnew streaming algorithms for computing the Hamming, $L_1$, $L_2$ and general\n$L_p$ ($0 < p < 2$) distances between the pattern and the text. Our results\nsignificantly extend over the previous result in this setting. In particular,\nfor the Hamming distance and for the $L_p$ distance when $0 < p \\le 1$ we show\na streaming algorithm that uses $\\tilde{O}(\\varepsilon^{-2}\\sqrt{n})$ space for\npolynomial-size alphabets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:42:46 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:38:23 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 12:08:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Starikovskaya", "Tatiana", ""], ["Svagerka", "Michal", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1907.04406", "submitter": "Priyanka Mukhopadhyay Ms", "authors": "Priyanka Mukhopadhyay", "title": "Faster provable sieving algorithms for the Shortest Vector Problem and\n  the Closest Vector Problem on lattices in $\\ell_p$ norm", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.02358", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give provable sieving algorithms for the Shortest Vector\nProblem (SVP) and the Closest Vector Problem (CVP) on lattices in $\\ell_p$ norm\nfor $1\\leq p\\leq\\infty$. The running time we get is better than existing\nprovable sieving algorithms, except the Discrete Gaussian based algorithm by\nAggarwal et al. [2015], but this algorithm works only for the Euclidean norm.\nWe build on the randomized sieving framework of Ajtai, Kumar and Sivakumar\n[2001,2002], where they used a sieving sub-routine that runs in time quadratic\nin the number of sampled vectors. We give a new sieving procedure that works\nfor all $\\ell_p$ norm and runs in time linear in the number of sampled vectors.\nThe main idea is to divide the space (hyperball) into sub-regions (hypercubes)\nsuch that each vector can be mapped efficiently to a sub-region. This is an\nextension of the sieving technique in Aggarwal and Mukhopadhyay [2018], where\nit has been used only for the $\\ell_{\\infty}$ norm. Prior to these works Blomer\nand Naewe [2009] generalised the AKS algorithm and our analysis shows that\ntheir algorithm can achieve a time complexity of $2^{3.849n+o(n)}$, while our\nalgorithm has a running time $2^{2.751n+o(n)}$.\n  We further modify our linear sieving technique and introduce a mixed sieving\nprocedure. At first a point is mapped to a hypercube within a ball and then\nwithin each hypercube we perform a quadratic seve like AKS. This improves the\nrunning time, specially in the $\\ell_2$ norm, where we achieve a time\ncomplexity of $2^{2.25n+o(n)}$, while the List Sieve Birthday algorithm [Pujol\nan Stehle, 2009] has a running time $2^{2.465n+o(n)}$ in the same norm. We also\nadopt our sieving techniques to approximation algorithms for SVP and CVP in\n$\\ell_p$ norm and achieve a running time of $2^{2.001n+o(n)}$, while algorithms\nlike [BN, 2009] have a time complexity of $2^{3.169n+o(n)}$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:42:48 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 18:46:29 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mukhopadhyay", "Priyanka", ""]]}, {"id": "1907.04413", "submitter": "Chandra Chekuri", "authors": "Chandra Chekuri and Kent Quanrud and Zhao Zhang", "title": "On Approximating Partial Set Cover and Generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial Set Cover (PSC) is a generalization of the well-studied Set Cover\nproblem (SC). In PSC the input consists of an integer $k$ and a set system\n$(U,S)$ where $U$ is a finite set, and $S \\subseteq 2^U$ is a collection of\nsubsets of $U$. The goal is to find a subcollection $S' \\subseteq S$ of\nsmallest cardinality such that sets in $S'$ cover at least $k$ elements of $U$;\nthat is $|\\cup_{A \\in S'} A| \\ge k$. SC is a special case of PSC when $k =\n|U|$. In the weighted version each set $X \\in S$ has a non-negative weight\n$w(X)$ and the goal is to find a minimum weight subcollection to cover $k$\nelements. Approximation algorithms for SC have been adapted to obtain\ncomparable algorithms for PSC in various interesting cases. In recent work\nInamdar and Varadarajan, motivated by geometric set systems, obtained a simple\nand elegant approach to reduce PSC to SC via the natural LP relaxation. They\nshowed that if a deletion-closed family of SC admits a $\\beta$-approximation\nvia the natural LP relaxation, then one can obtain a $2(\\beta +\n1)$-approximation for PSC on the same family. In a subsequent paper, they also\nconsidered a generalization of PSC that has multiple partial covering\nconstraints which is partly inspired by and generalizes previous work of Bera\net al on the Vertex Cover problem. Our main goal in this paper is to\ndemonstrate some useful connections between the results in previous work and\nsubmodularity. This allows us to simplify, and in some cases improve their\nresults. We improve the approximation for PSC to $(1-1/e)(\\beta + 1)$. We\nextend the previous work to the sparse setting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 21:00:48 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""], ["Zhang", "Zhao", ""]]}, {"id": "1907.04442", "submitter": "Ignasi Sau", "authors": "Julien Baste, Ignasi Sau, Dimitrios M. Thilikos", "title": "Hitting minors on bounded treewidth graphs. IV. An optimal algorithm", "comments": "51 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a fixed finite collection of graphs ${\\cal F}$, the ${\\cal F}$-M-DELETION\nproblem asks, given an $n$-vertex input graph $G,$ for the minimum number of\nvertices that intersect all minor models in $G$ of the graphs in ${\\cal F}$. by\nCourcelle Theorem, this problem can be solved in time $f_{{\\cal F}}(tw)\\cdot\nn^{O(1)},$ where $tw$ is the treewidth of $G$, for some function $f_{{\\cal F}}$\ndepending on ${\\cal F}$ In a recent series of articles, we have initiated the\nprogramme of optimizing asymptotically the function $f_{{\\cal F}}$. Here we\nprovide an algorithm showing that $f_{{\\cal F}}(tw) = 2^{O(tw\\cdot \\log tw)}$\nfor every collection ${\\cal F}$. Prior to this work, the best known function\n$f_{{\\cal F}}$ was double-exponential in $tw$. In particular, our algorithm\nvastly extends the results of Jansen et al. [SODA 2014] for the particular case\n${\\cal F}=\\{K_5,K_{3,3}\\}$ and of Kociumaka and Pilipczuk [Algorithmica 2019]\nfor graphs of bounded genus, and answers an open problem posed by Cygan et al.\n[Inf Comput 2017]. We combine several ingredients such as the machinery of\nboundaried graphs in dynamic programming via representatives, the Flat Wall\nTheorem, Bidimensionality, the irrelevant vertex technique, treewidth\nmodulators, and protrusion replacement. Together with our previous results\nproviding single-exponential algorithms for particular collections ${\\cal F}$\n[Theor Comput Sci 2020] and general lower bounds [J Comput Syst Sci 2020], our\nalgorithm yields the following complexity dichotomy when ${\\cal F} = \\{H\\}$\ncontains a single connected graph $H,$ assuming the Exponential Time\nHypothesis: $f_H(tw)=2^{\\Theta(tw)}$ if $H$ is a contraction of the chair or\nthe banner, and $f_H(tw)=2^{\\Theta(tw\\cdot \\log tw)}$ otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:29:50 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 15:18:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Baste", "Julien", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1907.04498", "submitter": "Rahul Vaze", "authors": "Rahul Vaze, Jayakrishnan Nair", "title": "Speed Scaling with Tandem Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speed scaling for a tandem server setting is considered, where there is a\nseries of servers, and each job has to be processed by each of the servers in\nsequence. Servers have a variable speed, their power consumption being a convex\nincreasing function of the speed. We consider the worst case setting as well as\nthe stochastic setting. In the worst case setting, the jobs are assumed to be\nof unit size with arbitrary (possibly adversarially determined) arrival\ninstants. For this problem, we devise an online speed scaling algorithm that is\nconstant competitive with respect to the optimal offline algorithm that has\nnon-causal information. The proposed algorithm, at all times, uses the same\nspeed on all active servers, such that the total power consumption equals the\nnumber of outstanding jobs. In the stochastic setting, we consider a more\ngeneral tandem network, with a parallel bank of servers at each stage. In this\nsetting, we show that random routing with a simple gated static speed selection\nis constant competitive. In both cases, the competitive ratio depends only on\nthe power functions, and is independent of the workload and the number of\nservers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 03:42:25 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Vaze", "Rahul", ""], ["Nair", "Jayakrishnan", ""]]}, {"id": "1907.04512", "submitter": "Taihei Oki", "authors": "Taihei Oki", "title": "Computing Valuations of the Dieudonn\\'e Determinants", "comments": "A preliminary version of the part of this paper about Edmonds'\n  problem has been appeared at the 47th International Colloquium on Automata,\n  Languages and Programming (ICALP '20), July 2020, under the title of \"On\n  solving (non)commutative weighted Edmonds' problem\". The previous version of\n  this paper was titled \"Computing the maximum degree of minors in skew\n  polynomial matrices\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of computing valuations of the Dieudonn\\'e\ndeterminants of matrices over discrete valuation skew fields (DVSFs). Under a\nreasonable computational model, we propose two algorithms for a class of DVSFs,\ncalled split. Our algorithms are extensions of the combinatorial relaxation of\nMurota (1995) and the matrix expansion by Moriyama--Murota (2013), both of\nwhich are based on combinatorial optimization. While our algorithms require an\nupper bound on the output, we give an estimation of the bound for skew\npolynomial matrices and show that the estimation is valid only for skew\npolynomial matrices.\n  We consider two applications of this problem. The first one is the\nnoncommutative weighted Edmonds' problem (nc-WEP), which is to compute the\ndegree of the Dieudonn\\'e determinants of matrices having noncommutative\nsymbols. We show that the presented algorithms reduce the nc-WEP to the\nunweighted problem in polynomial time. In particular, we show that the nc-WEP\nover the rational field is solvable in time polynomial in the input bit-length.\nWe also present an application to analyses of degrees of freedom of linear\ntime-varying systems by establishing formulas on the solution spaces of linear\ndifferential/difference equations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 05:27:04 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 09:29:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Oki", "Taihei", ""]]}, {"id": "1907.04565", "submitter": "Jules Vidal", "authors": "Jules Vidal, Joseph Budin, and Julien Tierny", "title": "Progressive Wasserstein Barycenters of Persistence Diagrams", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934256", "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient algorithm for the progressive approximation\nof Wasserstein barycenters of persistence diagrams, with applications to the\nvisual analysis of ensemble data. Given a set of scalar fields, our approach\nenables the computation of a persistence diagram which is representative of the\nset, and which visually conveys the number, data ranges and saliences of the\nmain features of interest found in the set. Such representative diagrams are\nobtained by computing explicitly the discrete Wasserstein barycenter of the set\nof persistence diagrams, a notoriously computationally intensive task. In\nparticular, we revisit efficient algorithms for Wasserstein distance\napproximation [12,51] to extend previous work on barycenter estimation [94]. We\npresent a new fast algorithm, which progressively approximates the barycenter\nby iteratively increasing the computation accuracy as well as the number of\npersistent features in the output diagram. Such a progressivity drastically\nimproves convergence in practice and allows to design an interruptible\nalgorithm, capable of respecting computation time constraints. This enables the\napproximation of Wasserstein barycenters within interactive times. We present\nan application to ensemble clustering where we revisit the k-means algorithm to\nexploit our barycenters and compute, within execution time constraints,\nmeaningful clusters of ensemble data along with their barycenter diagram.\nExtensive experiments on synthetic and real-life data sets report that our\nalgorithm converges to barycenters that are qualitatively meaningful with\nregard to the applications, and quantitatively comparable to previous\ntechniques, while offering an order of magnitude speedup when run until\nconvergence (without time constraint). Our algorithm can be trivially\nparallelized to provide additional speedups in practice on standard\nworkstations. [...]\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 08:24:11 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:36:24 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Vidal", "Jules", ""], ["Budin", "Joseph", ""], ["Tierny", "Julien", ""]]}, {"id": "1907.04585", "submitter": "Marcin Pilipczuk", "authors": "Maria Chudnovsky, Marcin Pilipczuk, Micha{\\l} Pilipczuk, and St\\'ephan\n  Thomass\\'e", "title": "Quasi-polynomial time approximation schemes for the Maximum Weight\n  Independent Set Problem in H-free graphs", "comments": "v2: added results on subexponential algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Maximum Independent Set problem we are asked to find a set of pairwise\nnonadjacent vertices in a given graph with the maximum possible cardinality. In\ngeneral graphs, this classical problem is known to be NP-hard and hard to\napproximate within a factor of $n^{1-\\varepsilon}$ for any $\\varepsilon > 0$.\nDue to this, investigating the complexity of Maximum Independent Set in various\ngraph classes in hope of finding better tractability results is an active\nresearch direction.\n  In $H$-free graphs, that is, graphs not containing a fixed graph $H$ as an\ninduced subgraph, the problem is known to remain NP-hard and APX-hard whenever\n$H$ contains a cycle, a vertex of degree at least four, or two vertices of\ndegree at least three in one connected component. For the remaining cases,\nwhere every component of $H$ is a path or a subdivided claw, the complexity of\nMaximum Independent Set remains widely open, with only a handful of\npolynomial-time solvability results for small graphs $H$ such as $P_5$, $P_6$,\nthe claw, or the fork.\n  We show that for every graph $H$ for which Maximum Independent Set is not\nknown to be APX-hard and SUBEXP-hard in $H$-free graphs, the problem admits a\nquasi-polynomial time approximation scheme and a subexponential-time exact\nalgorithm in this graph class. Our algorithm works also in the more general\nweighted setting, where the input graph is supplied with a weight function on\nvertices and we are maximizing the total weight of an independent set.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 09:21:33 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 11:51:57 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Chudnovsky", "Maria", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Thomass\u00e9", "St\u00e9phan", ""]]}, {"id": "1907.04628", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Polytopes, lattices, and spherical codes for the nearest neighbor\n  problem", "comments": "This is the full version of the paper published in the proceedings of\n  ICALP 2020 under the same title, which only contains Section 1", "journal-ref": "ICALP 2020", "doi": "10.4230/LIPIcs.ICALP.2020.76", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study locality-sensitive hash methods for the nearest neighbor problem for\nthe angular distance, focusing on the approach of first projecting down onto a\nlow-dimensional subspace, and then partitioning the projected vectors according\nto Voronoi cells induced by a suitable spherical code. This approach\ngeneralizes and interpolates between the fast but suboptimal hyperplane hashing\nof Charikar [STOC'02] and the asymptotically optimal but practically often\nslower hash families of Andoni-Indyk [FOCS'06], Andoni-Indyk-Nguyen-Razenshteyn\n[SODA'14] and Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt [NIPS'15]. We set up a\nframework for analyzing the performance of any spherical code in this context,\nand we provide results for various codes from the literature, such as those\nrelated to regular polytopes and root lattices. Similar to hyperplane hashing,\nand unlike cross-polytope hashing, our analysis of collision probabilities and\nquery exponents is exact and does not hide order terms which vanish only for\nlarge $d$, facilitating an easy parameter selection.\n  For the two-dimensional case, we derive closed-form expressions for arbitrary\nspherical codes, and we show that the equilateral triangle is optimal,\nachieving a better performance than the two-dimensional analogues of hyperplane\nand cross-polytope hashing. In three and four dimensions, we numerically find\nthat the tetrahedron, $5$-cell, and $16$-cell achieve the best query exponents,\nwhile in five or more dimensions orthoplices appear to outperform regular\nsimplices, as well as the root lattice families $A_k$ and $D_k$. We argue that\nin higher dimensions, larger spherical codes will likely exist which will\noutperform orthoplices in theory, and we argue why using the $D_k$ root\nlattices will likely lead to better results in practice, due to a better\ntrade-off between the asymptotic query exponent and the concrete costs of\nhashing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:38:27 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 03:50:06 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1907.04629", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Evolutionary techniques in lattice sieving algorithms", "comments": "9 pages, 2 figures", "journal-ref": "11th International Conference on Evolutionary Computation Theory\n  and Applications (ECTA), pp. 31-39, 2019", "doi": "10.5220/0007968800310039", "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattice-based cryptography has recently emerged as a prominent candidate for\nsecure communication in the quantum age. Its security relies on the hardness of\ncertain lattice problems, and the inability of known lattice algorithms, such\nas lattice sieving, to solve these problems efficiently. In this paper we\ninvestigate the similarities between lattice sieving and evolutionary\nalgorithms, how various improvements to lattice sieving can be viewed as\napplications of known techniques from evolutionary computation, and how other\nevolutionary techniques can benefit lattice sieving in practice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:38:33 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1907.04630", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Approximate Voronoi cells for lattices, revisited", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the approximate Voronoi cells approach for solving the closest\nvector problem with preprocessing (CVPP) on high-dimensional lattices, and\nsettle the open problem of Doulgerakis-Laarhoven-De Weger [PQCrypto, 2019] of\ndetermining exact asymptotics on the volume of these Voronoi cells under the\nGaussian heuristic. As a result, we obtain improved upper bounds on the time\ncomplexity of the randomized iterative slicer when using less than $2^{0.076d +\no(d)}$ memory, and we show how to obtain time-memory trade-offs even when using\nless than $2^{0.048d + o(d)}$ memory. We also settle the open problem of\nobtaining a continuous trade-off between the size of the advice and the query\ntime complexity, as the time complexity with subexponential advice in our\napproach scales as $d^{d/2 + o(d)}$, matching worst-case enumeration bounds,\nand achieving the same asymptotic scaling as average-case enumeration\nalgorithms for the closest vector problem.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:38:38 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1907.04645", "submitter": "Tillmann Miltzow", "authors": "Ivor van der Hoog, Tillmann Miltzow, Martijn van Schaik", "title": "Smoothed Analysis of Order Types", "comments": "15 pages, 6 figures, long introduction and short proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an ordered point set $P = (p_1,\\ldots,p_n)$, its order type (denoted\nby $\\chi_P$) is a map which assigns to every triple of points a value in\n$\\{+,-,0\\}$ based on whether the points are collinear(0), oriented clockwise(-)\nor counter-clockwise(+). An abstract order type is a map $\\chi :\n\\left[\\substack{n\\\\3}\\right] \\rightarrow \\{+,-,0\\}$ (where\n$\\left[\\substack{n\\\\3}\\right]$ is the collection of all triples of a set of $n$\nelements) that satisfies the following condition: for every set of five\nelements $S\\subset [n]$ its induced order type $\\chi_{|S}$ is realizable by a\npoint set. To be precise, a point set $P$ realizes an order type $\\chi$,if\n$\\chi_P(p_i,p_j,p_k) = \\chi(i,j,k)$, for all $i<j<k$. Planar point sets are\namong the most basic and natural geometric objects of study in Discrete and\nComputational Geometry. Properties of point sets are relevant in theory and\npractice alike. It is known, that deciding if an abstract order type is\nrealizable is complete for the existential theory of the reals. Our results\nshow that order type realizability is much easier for realistic instances than\nin the worst case. In particular, we can recognize instances in \"expected\n\\NP-time\". This is one of the first $\\exists\\mathbb{R}$-complete problems\nanalyzed under the lens of Smoothed Analysis.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 12:10:09 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["van der Hoog", "Ivor", ""], ["Miltzow", "Tillmann", ""], ["van Schaik", "Martijn", ""]]}, {"id": "1907.04660", "submitter": "Marinella Sciortino", "authors": "Sabrina Mantaci and Antonio Restivo and Giuseppe Romana and Giovanna\n  Rosone and Marinella Sciortino", "title": "String Attractors and Combinatorics on Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of \\emph{string attractor} has recently been introduced in\n[Prezza, 2017] and studied in [Kempa and Prezza, 2018] to provide a unifying\nframework for known dictionary-based compressors. A string attractor for a word\n$w=w[1]w[2]\\cdots w[n]$ is a subset $\\Gamma$ of the positions $\\{1,\\ldots,n\\}$,\nsuch that all distinct factors of $w$ have an occurrence crossing at least one\nof the elements of $\\Gamma$. While finding the smallest string attractor for a\nword is a NP-complete problem, it has been proved in [Kempa and Prezza, 2018]\nthat dictionary compressors can be interpreted as algorithms approximating the\nsmallest string attractor for a given word.\n  In this paper we explore the notion of string attractor from a combinatorial\npoint of view, by focusing on several families of finite words. The results\npresented in the paper suggest that the notion of string attractor can be used\nto define new tools to investigate combinatorial properties of the words.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 12:21:12 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Mantaci", "Sabrina", ""], ["Restivo", "Antonio", ""], ["Romana", "Giuseppe", ""], ["Rosone", "Giovanna", ""], ["Sciortino", "Marinella", ""]]}, {"id": "1907.04733", "submitter": "Shaofeng Jiang", "authors": "Daniel Baker and Vladimir Braverman and Lingxiao Huang and Shaofeng\n  H.-C. Jiang and Robert Krauthgamer and Xuan Wu", "title": "Coresets for Clustering in Graphs of Bounded Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of coresets for clustering in graph metrics, i.e., the\nshortest-path metric of edge-weighted graphs. Such clustering problems are\nessential to data analysis and used for example in road networks and data\nvisualization. A coreset is a compact summary of the data that approximately\npreserves the clustering objective for every possible center set, and it offers\nsignificant efficiency improvements in terms of running time, storage, and\ncommunication, including in streaming and distributed settings. Our main result\nis a near-linear time construction of a coreset for k-Median in a general graph\n$G$, with size $O_{\\epsilon, k}(\\mathrm{tw}(G))$ where $\\mathrm{tw}(G)$ is the\ntreewidth of $G$, and we complement the construction with a nearly-tight size\nlower bound. The construction is based on the framework of Feldman and Langberg\n[STOC 2011], and our main technical contribution, as required by this\nframework, is a uniform bound of $O(\\mathrm{tw}(G))$ on the shattering\ndimension under any point weights. We validate our coreset on real-world road\nnetworks, and our scalable algorithm constructs tiny coresets with high\naccuracy, which translates to a massive speedup of existing approximation\nalgorithms such as local search for graph k-Median.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:59:17 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 09:20:37 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 02:59:55 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2020 07:24:10 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Baker", "Daniel", ""], ["Braverman", "Vladimir", ""], ["Huang", "Lingxiao", ""], ["Jiang", "Shaofeng H. -C.", ""], ["Krauthgamer", "Robert", ""], ["Wu", "Xuan", ""]]}, {"id": "1907.04741", "submitter": "Stefan Lendl", "authors": "Stefan Lendl, Britta Peis, Veerle Timmermans", "title": "Matroid Bases with Cardinality Constraints on the Intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two matroids $\\mathcal{M}_{1} = (E, \\mathcal{B}_{1})$ and\n$\\mathcal{M}_{2} = (E, \\mathcal{B}_{2})$ on a common ground set $E$ with base\nsets $\\mathcal{B}_{1}$ and $\\mathcal{B}_{2}$, some integer $k \\in \\mathbb{N}$,\nand two cost functions $c_{1}, c_{2} \\colon E \\rightarrow \\mathbb{R}$, we\nconsider the optimization problem to find a basis $X \\in \\mathcal{B}_{1}$ and a\nbasis $Y \\in \\mathcal{B}_{2}$ minimizing cost $\\sum_{e\\in X} c_1(e)+\\sum_{e\\in\nY} c_2(e)$\n  subject to either a lower bound constraint $|X \\cap Y| \\le k$, an upper bound\nconstraint $|X \\cap Y| \\ge k$, or an equality constraint $|X \\cap Y| = k$ on\nthe size of the intersection of the two bases $X$ and $Y$. The problem with\nlower bound constraint turns out to be a generalization of the Recoverable\nRobust Matroid problem under interval uncertainty representation for which the\nquestion for a strongly polynomial-time algorithm was left as an open question\nby Hradovich et al.\n  We show that the two problems with lower and upper bound constraints on the\nsize of the intersection can be reduced to weighted matroid intersection, and\nthus be solved with a strongly polynomial-time primal-dual algorithm. The\nquestion whether the problem with equality constraint can also be solved\nefficiently turned out to be a lot harder. As our main result, we present a\nstrongly-polynomial, primal-dual algorithm for the problem with equality\nconstraint on the size of the intersection.\n  Additionally, we discuss generalizations of the problems from matroids to\npolymatroids, and from two to three or more matroids.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:16:04 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 16:19:27 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Lendl", "Stefan", ""], ["Peis", "Britta", ""], ["Timmermans", "Veerle", ""]]}, {"id": "1907.04745", "submitter": "Pan Peng", "authors": "Monika Henzinger, Pan Peng", "title": "Constant-Time Dynamic $(\\Delta+1)$-Coloring and Weight Approximation for\n  Minimum Spanning Forest: Dynamic Algorithms Meet Property Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With few exceptions (namely, algorithms for maximal matching, $2$-approximate\nvertex cover, and certain constant-stretch spanners), all known fully dynamic\nalgorithms in general graphs require (amortized) $\\Omega(\\log n)$ update/query\ntime. Showing for the first time that techniques from property testing can lead\nto constant-time fully dynamic graph algorithms we prove the following results:\n  (1) We give a fully dynamic (Las-Vegas style) algorithm with constant\nexpected amortized time per update that maintains a proper $(\\Delta+1)$-vertex\ncoloring of a graph with maximum degree at most $\\Delta$. This improves upon\nthe previous $O(\\log \\Delta)$-time algorithm by Bhattacharya et al. (SODA\n2018). We show that our result does not only have optimal running time, but is\nalso optimal in the sense that already deciding whether a $\\Delta$-coloring\nexists in a dynamically changing graph with maximum degree at most $\\Delta$\ntakes $\\Omega(\\log n)$ time per operation.\n  (2) We give two fully dynamic algorithms that maintain a\n$(1+\\varepsilon)$-approximation of the weight $M$ of the minimum spanning\nforest of a graph $G$ with edges weights in $[1,W]$. Our deterministic\nalgorithm takes $O({W^2 \\log W}/{\\varepsilon^3})$ worst-case time, which is\nconstant if both $W$ and $\\varepsilon$ are constant. This is somewhat\nsurprising as a lower bound by Patrascu and Demaine (SIAM J. Comput. 2006)\nshows that it takes $\\Omega(\\log n)$ time per operation to maintain the exact\nweight of the MSF that holds even for $W=1$. Our randomized (Monte-Carlo style)\nalgorithm works with high probability and runs in worst-case\n$O(\\frac{1}{\\varepsilon^4}\\log^2(\\frac{1}{\\varepsilon}))$ time if $W=\nO({(m^*)^{1/3}}/{\\log^3 n})$, where $m^*$ is the minimum number of edges in the\ngraph throughout all the updates. It works even against an adaptive adversary.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:21:06 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Henzinger", "Monika", ""], ["Peng", "Pan", ""]]}, {"id": "1907.04749", "submitter": "Stefan Walzer", "authors": "Martin Dietzfelbinger, Stefan Walzer", "title": "Dense Peelable Random Uniform Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new family of $k$-uniform hypergraphs with independent random\nedges. The hypergraphs have a high probability of being peelable, i.e. to admit\nno sub-hypergraph of minimum degree $2$, even when the edge density (number of\nedges over vertices) is close to $1$. In our construction, the vertex set is\npartitioned into linearly arranged segments and each edge is incident to random\nvertices of $k$ consecutive segments. Quite surprisingly, the linear geometry\nallows our graphs to be peeled \"from the outside in\". The density thresholds\n$f_k$ for peelability of our hypergraphs ($f_3 \\approx 0.918$, $f_4 \\approx\n0.977$, $f_5 \\approx 0.992$, ...) are well beyond the corresponding thresholds\n($c_3 \\approx 0.818$, $c_4 \\approx 0.772$, $c_5 \\approx 0.702$, ...) of\nstandard $k$-uniform random hypergraphs. To get a grip on $f_k$, we analyse an\nidealised peeling process on the random weak limit of our hypergraph family.\nThe process can be described in terms of an operator on functions and $f_k$ can\nbe linked to thresholds relating to the operator. These thresholds are then\ntractable with numerical methods.\n  Random hypergraphs underlie the construction of various data structures based\non hashing. These data structures frequently rely on peelability of the\nhypergraph or peelability allows for simple linear time algorithms. To\ndemonstrate the usefulness of our construction, we used our $3$-uniform\nhypergraphs as a drop-in replacement for the standard $3$-uniform hypergraphs\nin a retrieval data structure by Botelho et al. This reduces memory usage from\n$1.23m$ bits to $1.12m$ bits ($m$ being the input size) with almost no change\nin running time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:23:39 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Walzer", "Stefan", ""]]}, {"id": "1907.04750", "submitter": "Stefan Walzer", "authors": "Martin Dietzfelbinger, Stefan Walzer", "title": "Efficient Gauss Elimination for Near-Quadratic Matrices with One Short\n  Random Block per Row, with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we identify a new class of sparse near-quadratic random Boolean\nmatrices that have full row rank over $\\mathbb{F}_2=\\{0,1\\}$ with high\nprobability and can be transformed into echelon form in almost linear time by a\nsimple version of Gauss elimination. The random matrix with dimensions\n$n(1-\\varepsilon) \\times n$ is generated as follows: In each row, identify a\nblock of length $L=O((\\log n)/\\varepsilon)$ at a random position. The entries\noutside the block are 0, the entries inside the block are given by fair coin\ntosses. Sorting the rows according to the positions of the blocks transforms\nthe matrix into a kind of band matrix, on which, as it turns out, Gauss\nelimination works very efficiently with high probability. For the proof, the\neffects of Gauss elimination are interpreted as a (\"coin-flipping\") variant of\nRobin Hood hashing, whose behaviour can be captured in terms of a simple Markov\nmodel from queuing theory. Bounds for expected construction time and high\nsuccess probability follow from results in this area.\n  By employing hashing, this matrix family leads to a new implementation of a\nretrieval data structure, which represents an arbitrary function $f\\colon S \\to\n\\{0,1\\}$ for some set $S$ of $m=(1-\\varepsilon)n$ keys. It requires\n$m/(1-\\varepsilon)$ bits of space, construction takes $O(m/\\varepsilon^2$)\nexpected time on a word RAM, while queries take $O(1/\\varepsilon)$ time and\naccess only one contiguous segment of $O((\\log m)/\\varepsilon)$ bits in the\nrepresentation. The method is competitive with state-of-the-art methods. By\nwell-established methods the retrieval data structure leads to efficient\nconstructions of (static) perfect hash functions and (static) Bloom filters\nwith almost optimal space and very local storage access patterns for queries.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:23:48 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Walzer", "Stefan", ""]]}, {"id": "1907.04752", "submitter": "Philip Bille", "authors": "Philip Bille and Inge Li G{\\o}rtz", "title": "Sparse Regular Expression Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first algorithm for regular expression matching that can take\nadvantage of sparsity in the input instance. Our main result is a new algorithm\nthat solves regular expression matching in $O\\left(\\Delta \\log \\log\n\\frac{nm}{\\Delta} + n + m\\right)$ time, where $m$ is the number of positions in\nthe regular expression, $n$ is the length of the string, and $\\Delta$ is the\n\\emph{density} of the instance, defined as the total number of active states in\na simulation of the position automaton. This measure is a lower bound on the\ntotal number of active states in simulations of all classic polynomial sized\nfinite automata. Our bound improves the best known bounds for regular\nexpression matching by almost a linear factor in the density of the problem.\nThe key component in the result is a novel linear space representation of the\nposition automaton that supports state-set transition computation in\nnear-linear time in the size of the input and output state sets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:29:22 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""]]}, {"id": "1907.04824", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico", "title": "Scheduling With Inexact Job Sizes: The Merits of Shortest Processing\n  Time First", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that size-based scheduling policies, which take into account\njob size (i.e., the time it takes to run them), can perform very desirably in\nterms of both response time and fairness. Unfortunately, the requirement of\nknowing a priori the exact job size is a major obstacle which is frequently\ninsurmountable in practice. Often, it is possible to get a coarse estimation of\njob size, but unfortunately analytical results with inexact job sizes are\nchallenging to obtain, and simulation-based studies show that several\nsize-based algorithm are severely impacted by job estimation errors. For\nexample, Shortest Remaining Processing Time (SRPT), which yields optimal mean\nsojourn time when job sizes are known exactly, can drastically underperform\nwhen it is fed inexact job sizes.\n  Some algorithms have been proposed to better handle size estimation errors,\nbut they are somewhat complex and this makes their analysis challenging. We\nconsider Shortest Processing Time (SPT), a simplification of SRPT that skips\nthe update of \"remaining\" job size and results in a preemptive algorithm that\nsimply schedules the job with the shortest estimated processing time. When job\nsize is inexact, SPT performs comparably to the best known algorithms in the\npresence of errors, while being definitely simpler. In this work, SPT is\nevaluated through simulation, showing near-optimal performance in many cases,\nwith the hope that its simplicity can open the way to analytical evaluation\neven when inexact inputs are considered.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 16:52:10 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dell'Amico", "Matteo", ""]]}, {"id": "1907.04826", "submitter": "Holger Dell", "authors": "Holger Dell, John Lapinskas, Kitty Meeks", "title": "Approximately counting and sampling small witnesses using a colourful\n  decision oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we prove \"black box\" results for turning algorithms which\ndecide whether or not a witness exists into algorithms to approximately count\nthe number of witnesses, or to sample from the set of witnesses approximately\nuniformly, with essentially the same running time. We do so by extending the\nframework of Dell and Lapinskas (STOC 2018), which covers decision problems\nthat can be expressed as edge detection in bipartite graphs given limited\noracle access; our framework covers problems which can be expressed as edge\ndetection in arbitrary $k$-hypergraphs given limited oracle access. (Simulating\nthis oracle generally corresponds to invoking a decision algorithm.) This\nincludes many key problems in both the fine-grained setting (such as $k$-SUM,\n$k$-OV and weighted $k$-Clique) and the parameterised setting (such as induced\nsubgraphs of size $k$ or weight-$k$ solutions to CSPs). From an algorithmic\nstandpoint, our results will make the development of new approximate counting\nalgorithms substantially easier; indeed, it already yields a new\nstate-of-the-art algorithm for approximately counting graph motifs, improving\non Jerrum and Meeks (JCSS 2015) unless the input graph is very dense and the\ndesired motif very small. Our $k$-hypergraph reduction framework generalises\nand strengthens results in the graph oracle literature due to Beame et al.\n(ITCS 2018) and Bhattacharya et al. (CoRR abs/1808.00691).\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:11:58 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dell", "Holger", ""], ["Lapinskas", "John", ""], ["Meeks", "Kitty", ""]]}, {"id": "1907.04844", "submitter": "Karol Suchan", "authors": "Sylwia Cichacz and Karol Suchan", "title": "Minimum k-critical bipartite graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of Minimum $k$-Critical Bipartite Graph of order $(n,m)$\n- M$k$CBG-$(n,m)$: to find a bipartite $G=(U,V;E)$, with $|U|=n$, $|V|=m$, and\n$n>m>1$, which is $k$-critical bipartite, and the tuple $(|E|, \\Delta_U,\n\\Delta_V)$, where $\\Delta_U$ and $\\Delta_V$ denote the maximum degree in $U$\nand $V$, respectively, is lexicographically minimum over all such graphs. $G$\nis $k$-critical bipartite if deleting at most $k=n-m$ vertices from $U$ creates\n$G'$ that has a complete matching, i.e., a matching of size $m$. We show that,\nif $m(n-m+1)/n$ is an integer, then a solution of the M$k$CBG-$(n,m)$ problem\ncan be found among $(a,b)$-regular bipartite graphs of order $(n,m)$, with\n$a=m(n-m+1)/n$, and $b=n-m+1$. If $a=m-1$, then all $(a,b)$-regular bipartite\ngraphs of order $(n,m)$ are $k$-critical bipartite. For $a<m-1$, it is not the\ncase. We characterize the values of $n$, $m$, $a$, and $b$ that admit an\n$(a,b)$-regular bipartite graph of order $(n,m)$, with $b=n-m+1$, and give a\nsimple construction that creates such a $k$-critical bipartite graph whenever\npossible. Our techniques are based on Hall's marriage theorem, elementary\nnumber theory, linear Diophantine equations, properties of integer functions\nand congruences, and equations involving them.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:48:07 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 02:44:39 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 00:03:17 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 12:59:31 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cichacz", "Sylwia", ""], ["Suchan", "Karol", ""]]}, {"id": "1907.04904", "submitter": "Kasra Khosoussi", "authors": "Yulun Tian, Kasra Khosoussi, Jonathan P. How", "title": "A Resource-Aware Approach to Collaborative Loop Closure Detection with\n  Provable Performance Guarantees", "comments": "submitted to IJRR (extension of WAFR 2018 paper invited to IJRR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents resource-aware algorithms for distributed inter-robot\nloop closure detection for applications such as collaborative simultaneous\nlocalization and mapping (CSLAM) and distributed image retrieval. In real-world\nscenarios, this process is resource-intensive as it involves exchanging many\nobservations and geometrically verifying a large number of potential matches.\nThis poses severe challenges for small-size and low-cost robots with various\noperational and resource constraints that limit, e.g., energy consumption,\ncommunication bandwidth, and computation capacity. This paper proposes a\nframework in which robots first exchange compact queries to identify a set of\npotential loop closures. We then seek to select a subset of potential\ninter-robot loop closures for geometric verification that maximizes a monotone\nsubmodular performance metric without exceeding budgets on computation (number\nof geometric verifications) and communication (amount of exchanged data for\ngeometric verification). We demonstrate that this problem is in general\nNP-hard, and present efficient approximation algorithms with provable\nperformance guarantees. The proposed framework is extensively evaluated on real\nand synthetic datasets. A natural convex relaxation scheme is also presented to\ncertify the near-optimal performance of the proposed framework in practice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 19:57:24 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Tian", "Yulun", ""], ["Khosoussi", "Kasra", ""], ["How", "Jonathan P.", ""]]}, {"id": "1907.05000", "submitter": "Vu Hoang Nguyen Phan", "authors": "Jeffrey M. Dudek, Vu H. N. Phan, Moshe Y. Vardi", "title": "ADDMC: Weighted Model Counting with Algebraic Decision Diagrams", "comments": "Presented at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to compute exact literal-weighted model counts of\nBoolean formulas in Conjunctive Normal Form. Our algorithm employs dynamic\nprogramming and uses Algebraic Decision Diagrams as the primary data structure.\nWe implement this technique in ADDMC, a new model counter. We empirically\nevaluate various heuristics that can be used with ADDMC. We then compare ADDMC\nto state-of-the-art exact weighted model counters (Cachet, c2d, d4, and\nminiC2D) on 1914 standard model counting benchmarks and show that ADDMC\nsignificantly improves the virtual best solver.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 05:21:10 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 05:42:12 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Dudek", "Jeffrey M.", ""], ["Phan", "Vu H. N.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1907.05083", "submitter": "Zhijie Zhang", "authors": "Xiaohui Bei, Xiaoming Sun, Hao Wu, Jialin Zhang, Zhijie Zhang, Wei Zi", "title": "Cake Cutting on Graphs: A Discrete and Bounded Proportional Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical cake cutting problem studies how to find fair allocations of a\nheterogeneous and divisible resource among multiple agents. Two of the most\ncommonly studied fairness concepts in cake cutting are proportionality and\nenvy-freeness. It is well known that a proportional allocation among $n$ agents\ncan be found efficiently via simple protocols [16]. For envy-freeness, in a\nrecent breakthrough, Aziz and Mackenzie [5] proposed a discrete and bounded\nenvy-free protocol for any number of players. However, the protocol suffers\nfrom high multiple-exponential query complexity and it remains open to find\nsimpler and more efficient envy-free protocols.\n  In this paper we consider a variation of the cake cutting problem by assuming\nan underlying graph over the agents whose edges describe their acquaintance\nrelationships, and agents evaluate their shares relatively to those of their\nneighbors. An allocation is called locally proportional if each agent thinks\nshe receives at least the average value over her neighbors. Local\nproportionality generalizes proportionality and is in an interesting middle\nground between proportionality and envy-freeness: its existence is guaranteed\nby that of an envy-free allocation, but no simple protocol is known to produce\nsuch a locally proportional allocation for general graphs. Previous works\nshowed locally proportional protocols for special classes of graphs, and it is\nlisted in both [1] and [8] as an open question to design simple locally\nproportional protocols for more general classes of graphs. In this paper we\ncompletely resolved this open question by presenting a discrete and bounded\nlocally proportional protocol for any given graphs. Our protocol has a query\ncomplexity of only single exponential, which is significantly smaller than the\nsix towers of $n$ query complexity of the envy-free protocol given in [5].\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:05:55 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Bei", "Xiaohui", ""], ["Sun", "Xiaoming", ""], ["Wu", "Hao", ""], ["Zhang", "Jialin", ""], ["Zhang", "Zhijie", ""], ["Zi", "Wei", ""]]}, {"id": "1907.05087", "submitter": "Jiaqing Jiang", "authors": "Jiaqing Jiang, Xiaoming Sun, Shang-Hua Teng, Bujiao Wu, Kewen Wu and\n  Jialin Zhang", "title": "Optimal Space-Depth Trade-Off of CNOT Circuits in Quantum Logic\n  Synthesis", "comments": "25 pages, 5 figures. Fixed several minor typos and a mistake about\n  CNOT+Rz circuit", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the decoherence of the state-of-the-art physical implementations of\nquantum computers, it is essential to parallelize the quantum circuits to\nreduce their depth. Two decades ago, Moore et al. demonstrated that additional\nqubits (or ancillae) could be used to design \"shallow\" parallel circuits for\nquantum operators. They proved that any $n$-qubit CNOT circuit could be\nparallelized to $O(\\log n)$ depth, with $O(n^2)$ ancillae. However, the\nnear-term quantum technologies can only support limited amount of qubits,\nmaking space-depth trade-off a fundamental research subject for quantum-circuit\nsynthesis.\n  In this work, we establish an asymptotically optimal space-depth trade-off\nfor the design of CNOT circuits. We prove that for any $m\\geq0$, any $n$-qubit\nCNOT circuit can be parallelized to $O\\left(\\max \\left\\{\\log n,\n\\frac{n^{2}}{(n+m)\\log (n+m)}\\right\\} \\right)$ depth, with $O(m)$ ancillae. We\nshow that this bound is tight by a counting argument, and further show that\neven with arbitrary two-qubit quantum gates to approximate CNOT circuits, the\ndepth lower bound still meets our construction, illustrating the robustness of\nour result. Our work improves upon two previous results, one by Moore et al.\nfor $O(\\log n)$-depth quantum synthesis, and one by Patel et al. for $m = 0$:\nfor the former, we reduce the need of ancillae by a factor of $\\log^2 n$ by\nshowing that $m=O(n^2/\\log^2 n)$ additional qubits suffice to build $O(\\log\nn)$-depth, $O(n^2/\\log n)$ size --- which is asymptotically optimal --- CNOT\ncircuits; for the later, we reduce the depth by a factor of $n$ to the\nasymptotically optimal bound $O(n/\\log n)$. Our results can be directly\nextended to stabilizer circuits using an earlier result by Aaronson et al. In\naddition, we provide relevant hardness evidences for synthesis optimization of\nCNOT circuits in term of both size and depth.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:19:13 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 07:30:28 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Jiang", "Jiaqing", ""], ["Sun", "Xiaoming", ""], ["Teng", "Shang-Hua", ""], ["Wu", "Bujiao", ""], ["Wu", "Kewen", ""], ["Zhang", "Jialin", ""]]}, {"id": "1907.05094", "submitter": "Melanie Schmidt", "authors": "Anna Gro{\\ss}wendt, Heiko R\\\"oglin, Melanie Schmidt", "title": "Analysis of Ward's Method", "comments": "appeared at SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Ward's method for the hierarchical $k$-means problem. This popular\ngreedy heuristic is based on the \\emph{complete linkage} paradigm: Starting\nwith all data points as singleton clusters, it successively merges two clusters\nto form a clustering with one cluster less. The pair of clusters is chosen to\n(locally) minimize the $k$-means cost of the clustering in the next step.\n  Complete linkage algorithms are very popular for hierarchical clustering\nproblems, yet their theoretical properties have been studied relatively little.\nFor the Euclidean $k$-center problem, Ackermann et al. show that the\n$k$-clustering in the hierarchy computed by complete linkage has a worst-case\napproximation ratio of $\\Theta(\\log k)$. If the data lies in $\\mathbb{R}^d$ for\nconstant dimension $d$, the guarantee improves to $\\mathcal{O}(1)$, but the\n$\\mathcal{O}$-notation hides a linear dependence on $d$. Complete linkage for\n$k$-median or $k$-means has not been analyzed so far.\n  In this paper, we show that Ward's method computes a $2$-approximation with\nrespect to the $k$-means objective function if the optimal $k$-clustering is\nwell separated. If additionally the optimal clustering also satisfies a balance\ncondition, then Ward's method fully recovers the optimum solution. These\nresults hold in arbitrary dimension. We accompany our positive results with a\nlower bound of $\\Omega((3/2)^d)$ for data sets in $\\mathbb{R}^d$ that holds if\nno separation is guaranteed, and with lower bounds when the guaranteed\nseparation is not sufficiently strong. Finally, we show that Ward produces an\n$\\mathcal{O}(1)$-approximative clustering for one-dimensional data sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:35:03 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Gro\u00dfwendt", "Anna", ""], ["R\u00f6glin", "Heiko", ""], ["Schmidt", "Melanie", ""]]}, {"id": "1907.05121", "submitter": "Daniele Gorla", "authors": "Michele Boreale, Daniele Gorla", "title": "Approximate Model Counting, Sparse XOR Constraints and Minimum Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counting the number of models of a given Boolean formula has\nnumerous applications, including computing the leakage of deterministic\nprograms in Quantitative Information Flow. Model counting is a hard,\n#P-complete problem. For this reason, many approximate counters have been\ndeveloped in the last decade, offering formal guarantees of confidence and\naccuracy. A popular approach is based on the idea of using random XOR\nconstraints to, roughly, successively halving the solution set until no model\nis left: this is checked by invocations to a SAT solver. The effectiveness of\nthis procedure hinges on the ability of the SAT solver to deal with XOR\nconstraints, which in turn crucially depends on the length of such constraints.\nWe study to what extent one can employ sparse, hence short, constraints,\nkeeping guarantees of correctness. We show that the resulting bounds are\nclosely related to the geometry of the set of models, in particular to the\nminimum Hamming distance between models. We evaluate our theoretical results on\na few concrete formulae. Based on our findings, we finally discuss possible\ndirections for improvements of the current state of the art in approximate\nmodel counting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 11:38:44 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Boreale", "Michele", ""], ["Gorla", "Daniele", ""]]}, {"id": "1907.05135", "submitter": "Zhihao Gavin Tang", "authors": "Zhihao Gavin Tang, Xiaowei Wu and Yuhao Zhang", "title": "Towards a Better Understanding of Randomized Greedy Matching", "comments": "30 pages. To appear in STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a long history for studying randomized greedy matching\nalgorithms since the work by Dyer and Frieze~(RSA 1991). We follow this trend\nand consider the problem formulated in the oblivious setting, in which the\nalgorithm makes (random) decisions that are essentially oblivious to the input\ngraph.\n  We revisit the \\textsf{Modified Randomized Greedy (MRG)} algorithm by Aronson\net al.~(RSA 1995) that is proved to be $(0.5+\\epsilon)$-approximate. In\nparticular, we study a weaker version of the algorithm named \\textsf{Random\nDecision Order (RDO)} that in each step, randomly picks an unmatched vertex and\nmatches it to an arbitrary neighbor if exists. We prove the \\textsf{RDO}\nalgorithm is $0.639$-approximate and $0.531$-approximate for bipartite graphs\nand general graphs respectively. As a corollary, we substantially improve the\napproximation ratio of \\textsf{MRG}.\n  Furthermore, we generalize the \\textsf{RDO} algorithm to the edge-weighted\ncase and prove that it achieves a $0.501$ approximation ratio. This result\nsolves the open question by Chan et al.~(SICOMP 2018) about the existence of an\nalgorithm that beats greedy in this setting. As a corollary, it also solves the\nopen questions by Gamlath et al.~(SODA 2019) in the stochastic setting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 12:14:23 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 04:21:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""]]}, {"id": "1907.05309", "submitter": "Radim Janal\\'ik", "authors": "Matthias Bollh\\\"ofer and Olaf Schenk and Radim Janal\\'ik and Steve\n  Hamm and Kiran Gullapalli", "title": "State-of-The-Art Sparse Direct Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we will give an insight into modern sparse elimination\nmethods. These are driven by a preprocessing phase based on combinatorial\nalgorithms which improve diagonal dominance, reduce fill-in, and improve\nconcurrency to allow for parallel treatment. Moreover, these methods detect\ndense submatrices which can be handled by dense matrix kernels based on\nmultithreaded level-3 BLAS. We will demonstrate for problems arising from\ncircuit simulation, how the improvements in recent years have advanced direct\nsolution methods significantly.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:36:41 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Bollh\u00f6fer", "Matthias", ""], ["Schenk", "Olaf", ""], ["Janal\u00edk", "Radim", ""], ["Hamm", "Steve", ""], ["Gullapalli", "Kiran", ""]]}, {"id": "1907.05350", "submitter": "David Naori", "authors": "Haim Kaplan, David Naori and Danny Raz", "title": "Competitive Analysis with a Sample and the Secretary Problem", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the standard online worst-case model to accommodate past experience\nwhich is available to the online player in many practical scenarios. We do this\nby revealing a random sample of the adversarial input to the online player\nahead of time. The online player competes with the expected optimal value on\nthe part of the input that arrives online. Our model bridges between existing\nonline stochastic models (e.g., items are drawn i.i.d. from a distribution) and\nthe online worst-case model. We also extend in a similar manner (by revealing a\nsample) the online random-order model.\n  We study the classical secretary problem in our new models. In the worst-case\nmodel we present a simple online algorithm with optimal competitive-ratio for\nany sample size. In the random-order model, we also give a simple online\nalgorithm with an almost tight competitive-ratio for small sample sizes.\nInterestingly, we prove that for a large enough sample, no algorithm can be\nsimultaneously optimal both in the worst-cast and random-order models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:18:28 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Kaplan", "Haim", ""], ["Naori", "David", ""], ["Raz", "Danny", ""]]}, {"id": "1907.05378", "submitter": "Yassine Hamoudi", "authors": "Yassine Hamoudi, Patrick Rebentrost, Ansis Rosmanis, Miklos Santha", "title": "Quantum and Classical Algorithms for Approximate Submodular Function\n  Minimization", "comments": "24 pages, Journal version", "journal-ref": "Quantum Information & Computation, vol. 19, pp. 1325-1349 (2019)", "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions are set functions mapping every subset of some ground\nset of size $n$ into the real numbers and satisfying the diminishing returns\nproperty. Submodular minimization is an important field in discrete\noptimization theory due to its relevance for various branches of mathematics,\ncomputer science and economics. The currently fastest strongly polynomial\nalgorithm for exact minimization [LSW15] runs in time $\\widetilde{O}(n^3 \\cdot\n\\mathrm{EO} + n^4)$ where $\\mathrm{EO}$ denotes the cost to evaluate the\nfunction on any set. For functions with range $[-1,1]$, the best\n$\\epsilon$-additive approximation algorithm [CLSW17] runs in time\n$\\widetilde{O}(n^{5/3}/\\epsilon^{2} \\cdot \\mathrm{EO})$. In this paper we\npresent a classical and a quantum algorithm for approximate submodular\nminimization. Our classical result improves on the algorithm of [CLSW17] and\nruns in time $\\widetilde{O}(n^{3/2}/\\epsilon^2 \\cdot \\mathrm{EO})$. Our quantum\nalgorithm is, up to our knowledge, the first attempt to use quantum computing\nfor submodular optimization. The algorithm runs in time\n$\\widetilde{O}(n^{5/4}/\\epsilon^{5/2} \\cdot \\log(1/\\epsilon) \\cdot\n\\mathrm{EO})$. The main ingredient of the quantum result is a new method for\nsampling with high probability $T$ independent elements from any discrete\nprobability distribution of support size $n$ in time $O(\\sqrt{Tn})$. Previous\nquantum algorithms for this problem were of complexity $O(T\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:54:20 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 10:42:39 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Hamoudi", "Yassine", ""], ["Rebentrost", "Patrick", ""], ["Rosmanis", "Ansis", ""], ["Santha", "Miklos", ""]]}, {"id": "1907.05391", "submitter": "Slobodan Mitrovi\\'c", "authors": "Jakub {\\L}\\k{a}cki, Slobodan Mitrovi\\'c, Krzysztof Onak, Piotr\n  Sankowski", "title": "Walking Randomly, Massively, and Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a set of techniques that allow for efficiently generating many\nindependent random walks in the Massive Parallel Computation (MPC) model with\nspace per machine strongly sublinear in the number of vertices. In this\nspace-per-machine regime, many natural approaches to graph problems struggle to\novercome the $\\Theta(\\log n)$ MPC round complexity barrier. Our techniques\nenable breaking this barrier for PageRank---one of the most important\napplications of random walks---even in more challenging directed graphs, and\nfor approximate bipartiteness and expansion testing.\n  In the undirected case, we start our random walks from the stationary\ndistribution, which implies that we approximately know the empirical\ndistribution of their next steps. This allows for preparing continuations of\nrandom walks in advance and applying a doubling approach. As a result we can\ngenerate multiple random walks of length $l$ in $\\Theta(\\log l)$ rounds on MPC.\nMoreover, we show that under the popular 1-vs.-2-Cycles conjecture, this round\ncomplexity is asymptotically tight.\n  For directed graphs, our approach stems from our treatment of the PageRank\nMarkov chain. We first compute the PageRank for the undirected version of the\ninput graph and then slowly transition towards the directed case, considering\nconvex combinations of the transition matrices in the process.\n  For PageRank, we achieve the following round complexities for damping factor\nequal to $1 - \\epsilon$:\n  * in $O(\\log \\log n + \\log 1 / \\epsilon)$ rounds for undirected graphs (with\n$\\tilde O(m / \\epsilon^2)$ total space),\n  * in $\\tilde O(\\log^2 \\log n + \\log^2 1/\\epsilon)$ rounds for directed graphs\n(with $\\tilde O((m+n^{1+o(1)}) / poly\\, \\epsilon)$ total space).\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:13:26 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 09:30:04 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 09:50:10 GMT"}, {"version": "v4", "created": "Wed, 6 Nov 2019 02:27:31 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Onak", "Krzysztof", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1907.05401", "submitter": "Omid Etesami", "authors": "Omid Etesami, Saeed Mahloujifar, Mohammad Mahmoody", "title": "Computational Concentration of Measure: Optimal Bounds, Reductions, and\n  More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product measures of dimension $n$ are known to be concentrated in Hamming\ndistance: for any set $S$ in the product space of probability $\\epsilon$, a\nrandom point in the space, with probability $1-\\delta$, has a neighbor in $S$\nthat is different from the original point in only\n$O(\\sqrt{n\\ln(1/(\\epsilon\\delta))})$ coordinates. We obtain the tight\ncomputational version of this result, showing how given a random point and\naccess to an $S$-membership oracle, we can find such a close point in\npolynomial time. This resolves an open question of [Mahloujifar and Mahmoody,\nALT 2019]. As corollaries, we obtain polynomial-time poisoning and (in certain\nsettings) evasion attacks against learning algorithms when the original\nvulnerabilities have any cryptographically non-negligible probability.\n  We call our algorithm MUCIO (\"MUltiplicative Conditional Influence\nOptimizer\") since proceeding through the coordinates, it decides to change each\ncoordinate of the given point based on a multiplicative version of the\ninfluence of that coordinate, where influence is computed conditioned on\npreviously updated coordinates.\n  We also define a new notion of algorithmic reduction between computational\nconcentration of measure in different metric probability spaces. As an\napplication, we get computational concentration of measure for high-dimensional\nGaussian distributions under the $\\ell_1$ metric.\n  We prove several extensions to the results above: (1) Our computational\nconcentration result is also true when the Hamming distance is weighted. (2) We\nobtain an algorithmic version of concentration around mean, more specifically,\nMcDiarmid's inequality. (3) Our result generalizes to discrete random\nprocesses, and this leads to new tampering algorithms for collective coin\ntossing protocols. (4) We prove exponential lower bounds on the average running\ntime of non-adaptive query algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:33:03 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Etesami", "Omid", ""], ["Mahloujifar", "Saeed", ""], ["Mahmoody", "Mohammad", ""]]}, {"id": "1907.05445", "submitter": "Heather Guarnera", "authors": "Feodor F. Dragan and Heather M. Guarnera", "title": "Eccentricity function in distance-hereditary graphs", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.tcs.2020.05.004", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G=(V,E)$ is distance hereditary if every induced path of $G$ is a\nshortest path. In this paper, we show that the eccentricity function\n$e(v)=\\max\\{d(v,u): u\\in V\\}$ in any distance-hereditary graph $G$ is almost\nunimodal, that is, every vertex $v$ with $e(v)> rad(G)+1$ has a neighbor with\nsmaller eccentricity. Here, $rad(G)=\\min\\{e(v): v\\in V\\}$ is the radius of\ngraph $G$. Moreover, we use this result to fully characterize the centers of\ndistance-hereditary graphs. Several bounds on the eccentricity of a vertex with\nrespect to its distance to the center of $G$ or to the ends of a diametral path\nare established. Finally, we propose a new linear time algorithm to compute all\neccentricities in a distance-hereditary graph.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 18:40:26 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 15:36:46 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Dragan", "Feodor F.", ""], ["Guarnera", "Heather M.", ""]]}, {"id": "1907.05457", "submitter": "Aditya Krishnan", "authors": "Vladimir Braverman, Robert Krauthgamer, Aditya Krishnan, Roi Sinoff", "title": "Schatten Norms in Matrix Streams: Hello Sparsity, Goodbye Dimension", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral functions of large matrices contains important structural\ninformation about the underlying data, and is thus becoming increasingly\nimportant. Many times, large matrices representing real-world data are\n\\emph{sparse} or \\emph{doubly sparse} (i.e., sparse in both rows and columns),\nand are accessed as a \\emph{stream} of updates, typically organized in\n\\emph{row-order}. In this setting, where space (memory) is the limiting\nresource, all known algorithms require space that is polynomial in the\ndimension of the matrix, even for sparse matrices. We address this challenge by\nproviding the first algorithms whose space requirement is \\emph{independent of\nthe matrix dimension}, assuming the matrix is doubly-sparse and presented in\nrow-order. Our algorithms approximate the Schatten $p$-norms, which we use in\nturn to approximate other spectral functions, such as logarithm of the\ndeterminant, trace of matrix inverse, and Estrada index. We validate these\ntheoretical performance bounds by numerical experiments on real-world matrices\nrepresenting social networks. We further prove that multiple passes are\nunavoidable in this setting, and show extensions of our primary technique,\nincluding a trade-off between space requirements and number of passes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 19:27:26 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 15:55:58 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Braverman", "Vladimir", ""], ["Krauthgamer", "Robert", ""], ["Krishnan", "Aditya", ""], ["Sinoff", "Roi", ""]]}, {"id": "1907.05473", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal and Jatin Batra", "title": "Non-uniform Geometric Set Cover and Scheduling on Multiple Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following general scheduling problem studied recently by\nMoseley. There are $n$ jobs, all released at time $0$, where job $j$ has size\n$p_j$ and an associated arbitrary non-decreasing cost function $f_j$ of its\ncompletion time. The goal is to find a schedule on $m$ machines with minimum\ntotal cost. We give an $O(1)$ approximation for the problem, improving upon the\nprevious $O(\\log \\log nP)$ bound ($P$ is the maximum to minimum size ratio),\nand resolving the open question of Moseley.\n  We first note that the scheduling problem can be reduced to a clean geometric\nset cover problem where points on a line with arbitrary demands, must be\ncovered by a minimum cost collection of given intervals with non-uniform\ncapacity profiles. Unfortunately, current techniques for such problems based on\nknapsack cover inequalities and low union complexity, completely lose the\ngeometric structure in the non-uniform capacity profiles and incur at least an\n$\\Omega(\\log\\log P)$ loss.\n  To this end, we consider general covering problems with non-uniform\ncapacities, and give a new method to handle capacities in a way that completely\npreserves their geometric structure. This allows us to use sophisticated\ngeometric ideas in a black-box way to avoid the $\\Omega(\\log \\log P)$ loss in\nprevious approaches. In addition to the scheduling problem above, we use this\napproach to obtain $O(1)$ or inverse Ackermann type bounds for several basic\ncapacitated covering problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 20:10:16 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 18:40:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bansal", "Nikhil", ""], ["Batra", "Jatin", ""]]}, {"id": "1907.05568", "submitter": "Zhihuai Chen", "authors": "Zhihuai Chen, Yinan Li, Xiaoming Sun, Pei Yuan and Jialin Zhang", "title": "A Quantum-inspired Classical Algorithm for Separable Non-negative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative Matrix Factorization (NMF) asks to decompose a (entry-wise)\nnon-negative matrix into the product of two smaller-sized nonnegative matrices,\nwhich has been shown intractable in general. In order to overcome this issue,\nthe separability assumption is introduced which assumes all data points are in\na conical hull. This assumption makes NMF tractable and is widely used in text\nanalysis and image processing, but still impractical for huge-scale datasets.\nIn this paper, inspired by recent development on dequantizing techniques, we\npropose a new classical algorithm for separable NMF problem. Our new algorithm\nruns in polynomial time in the rank and logarithmic in the size of input\nmatrices, which achieves an exponential speedup in the low-rank setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 03:49:26 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Chen", "Zhihuai", ""], ["Li", "Yinan", ""], ["Sun", "Xiaoming", ""], ["Yuan", "Pei", ""], ["Zhang", "Jialin", ""]]}, {"id": "1907.05725", "submitter": "Jakab Tardos", "authors": "Michael Kapralov, Slobodan Mitrovi\\'c, Ashkan Norouzi-Fard, Jakab\n  Tardos", "title": "Space Efficient Approximation to Maximum Matching Size from Uniform Edge\n  Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a source of iid samples of edges of an input graph $G$ with $n$\nvertices and $m$ edges, how many samples does one need to compute a constant\nfactor approximation to the maximum matching size in $G$? Moreover, is it\npossible to obtain such an estimate in a small amount of space? We show that,\non the one hand, this problem cannot be solved using a nontrivially sublinear\n(in $m$) number of samples: $m^{1-o(1)}$ samples are needed. On the other hand,\na surprisingly space efficient algorithm for processing the samples exists:\n$O(\\log^2 n)$ bits of space suffice to compute an estimate.\n  Our main technical tool is a new peeling type algorithm for matching that we\nsimulate using a recursive sampling process that crucially ensures that local\nneighborhood information from `dense' regions of the graph is provided at\nappropriately higher sampling rates. We show that a delicate balance between\nexploration depth and sampling rate allows our simulation to not lose precision\nover a logarithmic number of levels of recursion and achieve a constant factor\napproximation. The previous best result on matching size estimation from random\nsamples was a $\\log^{O(1)} n$ approximation [Kapralov et al'14].\n  Our algorithm also yields a constant factor approximate local computation\nalgorithm (LCA) for matching with $O(d\\log n)$ exploration starting from any\nvertex. Previous approaches were based on local simulations of randomized\ngreedy, which take $O(d)$ time {\\em in expectation over the starting vertex or\nedge} (Yoshida et al'09, Onak et al'12), and could not achieve a better than\n$d^2$ runtime. Interestingly, we also show that unlike our algorithm, the local\nsimulation of randomized greedy that is the basis of the most efficient prior\nresults does take $\\wt{\\Omega}(d^2)\\gg O(d\\log n)$ time for a worst case edge\neven for $d=\\exp(\\Theta(\\sqrt{\\log n}))$.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 13:12:45 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Kapralov", "Michael", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Norouzi-Fard", "Ashkan", ""], ["Tardos", "Jakab", ""]]}, {"id": "1907.05816", "submitter": "Rajesh Jayaram", "authors": "Rajesh Jayaram, David P. Woodruff", "title": "Towards Optimal Moment Estimation in Streaming and Distributed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the oldest problems in the data stream model is to approximate the\n$p$-th moment $\\|\\mathcal{X}\\|_p^p = \\sum_{i=1}^n |\\mathcal{X}_i|^p$ of an\nunderlying vector $\\mathcal{X} \\in \\mathbb{R}^n$, which is presented as a\nsequence of poly$(n)$ updates to its coordinates. Of particular interest is\nwhen $p \\in (0,2]$. Although a tight space bound of $\\Theta(\\epsilon^{-2} \\log\nn)$ bits is known for this problem when both positive and negative updates are\nallowed, surprisingly there is still a gap in the space complexity when all\nupdates are positive. Specifically, the upper bound is $O(\\epsilon^{-2} \\log\nn)$ bits, while the lower bound is only $\\Omega(\\epsilon^{-2} + \\log n)$ bits.\nRecently, an upper bound of $\\tilde{O}(\\epsilon^{-2} + \\log n)$ bits was\nobtained assuming that the updates arrive in a random order.\n  We show that for $p \\in (0, 1]$, the random order assumption is not needed.\nNamely, we give an upper bound for worst-case streams of\n$\\tilde{O}(\\epsilon^{-2} + \\log n)$ bits for estimating $\\|\\mathcal{X}\\|_p^p$.\nOur techniques also give new upper bounds for estimating the empirical entropy\nin a stream. On the other hand, we show that for $p \\in (1,2]$, in the natural\ncoordinator and blackboard communication topologies, there is an\n$\\tilde{O}(\\epsilon^{-2})$ bit max-communication upper bound based on a\nrandomized rounding scheme. Our protocols also give rise to protocols for heavy\nhitters and approximate matrix product. We generalize our results to arbitrary\ncommunication topologies $G$, obtaining an $\\tilde{O}(\\epsilon^{2} \\log d)$\nmax-communication upper bound, where $d$ is the diameter of $G$. Interestingly,\nour upper bound rules out natural communication complexity-based approaches for\nproving an $\\Omega(\\epsilon^{-2} \\log n)$ bit lower bound for $p \\in (1,2]$ for\nstreaming algorithms. In particular, any such lower bound must come from a\ntopology with large diameter.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 16:10:46 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "1907.05870", "submitter": "Jonathan Lenchner", "authors": "Jonathan Lenchner", "title": "On a Generalization of the Marriage Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the marriage problem underlying Hall's famous\nMarriage Theorem to what we call the Symmetric Marriage Problem, a problem that\ncan be thought of as a special case of Maximal Weighted Bipartite Matching. We\nshow that there is a solution to the Symmetric Marriage Problem if and only if\na variation on Hall's Condition holds on each of the bipartitions. We prove\nboth finite and infinite versions of this result and provide applications. We\nalso introduce a non-bipartite version of the problem and show that a\ngeneralization of Tutte's Theorem applies.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 17:37:06 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 16:22:58 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 03:25:19 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lenchner", "Jonathan", ""]]}, {"id": "1907.05940", "submitter": "Dimitrios Thilikos", "authors": "Petr A. Golovach and Stavros G. Kolliopoulos and Giannos Stamoulis and\n  Dimitrios M. Thilikos", "title": "Planar Disjoint Paths in Linear Time", "comments": "These results are subsumed to a large extent by existing literature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Disjoint Paths problem asks whether a fixed number of pairs of terminals\nin a graph $G$ can be linked by pairwise disjoint paths. In the context of this\nproblem, Robertson and Seymour introduced the celebrated irrelevant vertex\ntechnique that has since become standard in graph algorithms. The technique\nconsists of detecting a vertex that is irrelevant in the sense that its removal\ncreates an equivalent instance of the problem. That way, one may solve the\nproblem in $O(n^2)$ steps, as the detection of an irrelevant vertex takes\n$O(n)$ time and at most $n$ vertices may need to be removed. In this paper we\nstudy the Planar Disjoint Paths problem where the input graph is planar. We\nintroduce an extension of the irrelevant vertex technique where all the\nirrelevant vertices are removed simultaneously so that an instance of the\nPlanar Disjoint Paths problem can be transformed in a linear number of steps to\nan equivalent one that has bounded treewidth. As a consequence, the Planar\nDisjoint Paths problem can be solved in linear time for every fixed number of\nterminals.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 20:27:46 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 18:54:48 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 08:41:21 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Golovach", "Petr A.", ""], ["Kolliopoulos", "Stavros G.", ""], ["Stamoulis", "Giannos", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1907.05944", "submitter": "Nguyen Kim Thang", "authors": "Evripidis Bampis, Dimitris Christou, Bruno Escoffier, Nguyen Kim Thang", "title": "Online learning for min-max discrete problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study various discrete nonlinear combinatorial optimization problems in an\nonline learning framework. In the first part, we address the question of\nwhether there are negative results showing that getting a vanishing (or even\nvanishing approximate) regret is computational hard. We provide a general\nreduction showing that many (min-max) polynomial time solvable problems not\nonly do not have a vanishing regret, but also no vanishing approximation\n$\\alpha$-regret, for some $\\alpha$ (unless $NP=BPP$). Then, we focus on a\nparticular min-max problem, the min-max version of the vertex cover problem\nwhich is solvable in polynomial time in the offline case. The previous\nreduction proves that there is no $(2-\\epsilon)$-regret online algorithm,\nunless Unique Game is in $BPP$; we prove a matching upper bound providing an\nonline algorithm based on the online gradient descent method. Then, we turn our\nattention to online learning algorithms that are based on an offline\noptimization oracle that, given a set of instances of the problem, is able to\ncompute the optimum static solution. We show that for different nonlinear\ndiscrete optimization problems, it is strongly $NP$-hard to solve the offline\noptimization oracle, even for problems that can be solved in polynomial time in\nthe static case (e.g. min-max vertex cover, min-max perfect matching, etc.). On\nthe positive side, we present an online algorithm with vanishing regret that is\nbased on the follow the perturbed leader algorithm for a generalized knapsack\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 20:37:07 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 07:37:51 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Bampis", "Evripidis", ""], ["Christou", "Dimitris", ""], ["Escoffier", "Bruno", ""], ["Thang", "Nguyen Kim", ""]]}, {"id": "1907.05964", "submitter": "Sandip Sinha", "authors": "Frank Ban, Xi Chen, Rocco A. Servedio, Sandip Sinha", "title": "Efficient average-case population recovery in the presence of insertions\n  and deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have considered the \\emph{trace reconstruction problem},\nin which an unknown source string $x\\in\\{0,1\\}^n$ is transmitted through a\nprobabilistic channel which may randomly delete coordinates or insert random\nbits, resulting in a \\emph{trace} of $x$. The goal is to reconstruct the\noriginal string~$x$ from independent traces of $x$. While the best algorithms\nknown for worst-case strings use $\\exp(O(n^{1/3}))$ traces\n\\cite{DOS17,NazarovPeres17}, highly efficient algorithms are known\n\\cite{PZ17,HPP18} for the \\emph{average-case} version, in which $x$ is\nuniformly random. We consider a generalization of this average-case trace\nreconstruction problem, which we call \\emph{average-case population recovery in\nthe presence of insertions and deletions}. In this problem, there is an unknown\ndistribution $\\cal{D}$ over $s$ unknown source strings $x^1,\\dots,x^s \\in\n\\{0,1\\}^n$, and each sample is independently generated by drawing some $x^i$\nfrom $\\cal{D}$ and returning an independent trace of $x^i$.\n  Building on \\cite{PZ17} and \\cite{HPP18}, we give an efficient algorithm for\nthis problem. For any support size $s \\leq \\smash{\\exp(\\Theta(n^{1/3}))}$, for\na $1-o(1)$ fraction of all $s$-element support sets $\\{x^1,\\dots,x^s\\} \\subset\n\\{0,1\\}^n$, for every distribution $\\cal{D}$ supported on $\\{x^1,\\dots,x^s\\}$,\nour algorithm efficiently recovers ${\\cal D}$ up to total variation distance\n$\\epsilon$ with high probability, given access to independent traces of\nindependent draws from $\\cal{D}$. The algorithm runs in time\npoly$(n,s,1/\\epsilon)$ and its sample complexity is\npoly$(s,1/\\epsilon,\\exp(\\log^{1/3}n)).$ This polynomial dependence on the\nsupport size $s$ is in sharp contrast with the \\emph{worst-case} version (when\n$x^1,\\dots,x^s$ may be any strings in $\\{0,1\\}^n$), in which the sample\ncomplexity of the most efficient known algorithm \\cite{BCFSS19} is doubly\nexponential in $s$.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 21:39:43 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ban", "Frank", ""], ["Chen", "Xi", ""], ["Servedio", "Rocco A.", ""], ["Sinha", "Sandip", ""]]}, {"id": "1907.06001", "submitter": "Andr\\'es Cristi", "authors": "Jos\\'e Correa, Andr\\'es Cristi, Boris Epstein, Jos\\'e A. Soto", "title": "The Two-Sided Game of Googol and Sample-Based Prophet Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secretary problem or the game of Googol are classic models for online\nselection problems that have received significant attention in the last five\ndecades. We consider a variant of the problem and explore its connections to\ndata-driven online selection. Specifically, we are given $n$ cards with\narbitrary non-negative numbers written on both sides. The cards are randomly\nplaced on $n$ consecutive positions on a table, and for each card, the visible\nside is also selected at random. The player sees the visible side of all cards\nand wants to select the card with the maximum hidden value. To this end, the\nplayer flips the first card, sees its hidden value and decides whether to pick\nit or drop it and continue with the next card.\n  We study algorithms for two natural objectives. In the first one, as in the\nsecretary problem, the player wants to maximize the probability of selecting\nthe maximum hidden value. We show that this can be done with probability at\nleast $0.45292$. In the second one, similar to the prophet inequality, the\nplayer maximizes the expectation of the selected hidden value. We show a\nguarantee of at least $0.63518$ with respect to the expected maximum hidden\nvalue.\n  Our algorithms result from combining three basic strategies. One is to stop\nwhenever we see a value larger than the initial $n$ visible numbers. The second\none is to stop the first time the last flipped card's value is the largest of\nthe currently $n$ visible numbers in the table. And the third one is similar to\nthe latter but it additionally requires that the last flipped value is larger\nthan the value on the other side of its card.\n  We apply our results to the prophet secretary problem with unknown\ndistributions, but with access to a single sample from each distribution. Our\nguarantee improves upon $1-1/e$ for this problem, which is the currently best\nknown guarantee and only works for the i.i.d. case.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 02:45:24 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Correa", "Jos\u00e9", ""], ["Cristi", "Andr\u00e9s", ""], ["Epstein", "Boris", ""], ["Soto", "Jos\u00e9 A.", ""]]}, {"id": "1907.06033", "submitter": "Weiming Feng", "authors": "Weiming Feng, Heng Guo, Yitong Yin", "title": "Perfect sampling from spatial mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new perfect sampling technique that can be applied to general\nGibbs distributions and runs in linear time if the correlation decays faster\nthan the neighborhood growth. In particular, in graphs with sub-exponential\nneighborhood growth like $\\mathbb{Z}^d$, our algorithm achieves linear running\ntime as long as Gibbs sampling is rapidly mixing. As concrete applications, we\nobtain the currently best perfect samplers for colorings and for monomer-dimer\nmodels in such graphs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 09:15:58 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 03:59:05 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Feng", "Weiming", ""], ["Guo", "Heng", ""], ["Yin", "Yitong", ""]]}, {"id": "1907.06156", "submitter": "Jingcheng Liu", "authors": "Heng Guo, Jingcheng Liu, Pinyan Lu", "title": "Zeros of ferromagnetic 2-spin systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study zeros of the partition functions of ferromagnetic 2-state spin\nsystems in terms of the external field, and obtain new zero-free regions of\nthese systems via a refinement of Asano's and Ruelle's contraction method. The\nstrength of our results is that they do not depend on the maximum degree of the\nunderlying graph. Via Barvinok's method, we also obtain new efficient and\ndeterministic approximate counting algorithms. In certain regimes, our\nalgorithm outperforms all other methods such as Markov chain Monte Carlo and\ncorrelation decay.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 02:46:14 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Guo", "Heng", ""], ["Liu", "Jingcheng", ""], ["Lu", "Pinyan", ""]]}, {"id": "1907.06172", "submitter": "Ivan Bliznets", "authors": "Ivan Bliznets and Danil Sagunov", "title": "On Happy Colorings, Cuts, and Structural Parameterizations", "comments": "Accepted to WG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Maximum Happy Vertices and Maximum Happy Edges problems. The\nformer problem is a variant of clusterization, where some vertices have already\nbeen assigned to clusters. The second problem gives a natural generalization of\nMultiway Uncut, which is the complement of the classical Multiway Cut problem.\nDue to their fundamental role in theory and practice, clusterization and cut\nproblems has always attracted a lot of attention. We establish a new connection\nbetween these two classes of problems by providing a reduction between Maximum\nHappy Vertices and Node Multiway Cut. Moreover, we study structural and\ndistance to triviality parameterizations of Maximum Happy Vertices and Maximum\nHappy Edges. Obtained results in these directions answer questions explicitly\nasked in four works: Agrawal '17, Aravind et al. '16, Choudhari and Reddy '18,\nMisra and Reddy '17.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 06:28:28 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bliznets", "Ivan", ""], ["Sagunov", "Danil", ""]]}, {"id": "1907.06173", "submitter": "Eric Balkanski", "authors": "Adam Breuer, Eric Balkanski, Yaron Singer", "title": "The FAST Algorithm for Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a new algorithm called Fast Adaptive Sequencing\nTechnique (FAST) for maximizing a monotone submodular function under a\ncardinality constraint $k$ whose approximation ratio is arbitrarily close to\n$1-1/e$, is $O(\\log(n) \\log^2(\\log k))$ adaptive, and uses a total of $O(n\n\\log\\log(k))$ queries. Recent algorithms have comparable guarantees in terms of\nasymptotic worst case analysis, but their actual number of rounds and query\ncomplexity depend on very large constants and polynomials in terms of precision\nand confidence, making them impractical for large data sets. Our main\ncontribution is a design that is extremely efficient both in terms of its\nnon-asymptotic worst case query complexity and number of rounds, and in terms\nof its practical runtime. We show that this algorithm outperforms any algorithm\nfor submodular maximization we are aware of, including hyper-optimized parallel\nversions of state-of-the-art serial algorithms, by running experiments on large\ndata sets. These experiments show FAST is orders of magnitude faster than the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 06:37:24 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Breuer", "Adam", ""], ["Balkanski", "Eric", ""], ["Singer", "Yaron", ""]]}, {"id": "1907.06309", "submitter": "Caleb Levy", "authors": "Caleb C. Levy and Robert E. Tarjan", "title": "Splaying Preorders and Postorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T$ be a binary search tree. We prove two results about the behavior of\nthe Splay algorithm (Sleator and Tarjan 1985). Our first result is that\ninserting keys into an empty binary search tree via splaying in the order of\neither $T$'s preorder or $T$'s postorder takes linear time. Our proof uses the\nfact that preorders and postorders are pattern-avoiding: i.e. they contain no\nsubsequences that are order-isomorphic to $(2,3,1)$ and $(3,1,2)$,\nrespectively. Pattern-avoidance implies certain constraints on the manner in\nwhich items are inserted. We exploit this structure with a simple potential\nfunction that counts inserted nodes lying on access paths to uninserted nodes.\nOur methods can likely be extended to permutations that avoid more general\npatterns. Second, if $T'$ is any other binary search tree with the same keys as\n$T$ and $T$ is weight-balanced (Nievergelt and Reingold 1973), then splaying\n$T$'s preorder sequence or $T$'s postorder sequence starting from $T'$ takes\nlinear time. To prove this, we demonstrate that preorders and postorders of\nbalanced search trees do not contain many large \"jumps\" in symmetric order, and\nexploit this fact by using the dynamic finger theorem (Cole et al. 2000). Both\nof our results provide further evidence in favor of the elusive \"dynamic\noptimality conjecture.\"\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 01:45:56 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Levy", "Caleb C.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1907.06310", "submitter": "Caleb Levy", "authors": "Caleb C. Levy and Robert E. Tarjan", "title": "New Paths from Splay to Dynamic Optimality", "comments": "An earlier version of this work appeared in the Proceedings of the\n  Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms. arXiv admin note:\n  text overlap with arXiv:1907.06309", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the task of performing a sequence of searches in a binary search\ntree. After each search, an algorithm is allowed to arbitrarily restructure the\ntree, at a cost proportional to the amount of restructuring performed. The cost\nof an execution is the sum of the time spent searching and the time spent\noptimizing those searches with restructuring operations. This notion was\nintroduced by Sleator and Tarjan in (JACM, 1985), along with an algorithm and a\nconjecture. The algorithm, Splay, is an elegant procedure for performing\nadjustments while moving searched items to the top of the tree. The conjecture,\ncalled \"dynamic optimality,\" is that the cost of splaying is always within a\nconstant factor of the optimal algorithm for performing searches. The\nconjecture stands to this day. In this work, we attempt to lay the foundations\nfor a proof of the dynamic optimality conjecture.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 02:02:40 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Levy", "Caleb C.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1907.06334", "submitter": "Matin Hashemi", "authors": "Mahdi Bozorg, Saber Salehkaleybar, Matin Hashemi", "title": "Seedless Graph Matching via Tail of Degree Distribution for Correlated\n  Erdos-Renyi Graphs", "comments": "submitted for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network alignment (or graph matching) problem refers to recovering the\nnode-to-node correspondence between two correlated networks. In this paper, we\npropose a network alignment algorithm which works without using a seed set of\npre-matched node pairs or any other auxiliary information (e.g., node or edge\nlabels) as an input. The algorithm assigns structurally innovative features to\nnodes based on the tail of empirical degree distribution of their neighbor\nnodes. Then, it matches the nodes according to these features. We evaluate the\nperformance of proposed algorithm on both synthetic and real networks. For\nsynthetic networks, we generate Erdos-Renyi graphs in the regions of\n$\\Theta(\\log(n)/n)$ and $\\Theta(\\log^{2}(n)/n)$, where a previous work\ntheoretically showed that recovering is feasible in sparse Erdos-Renyi graphs\nif and only if the probability of having an edge between a pair of nodes in one\nof the graphs and also between the corresponding nodes in the other graph is in\nthe order of $\\Omega(\\log(n)/n)$, where $n$ is the number of nodes. Experiments\non both real and synthetic networks show that it outperforms previous works in\nterms of probability of correct matching.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 05:28:07 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 06:13:13 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 06:53:22 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bozorg", "Mahdi", ""], ["Salehkaleybar", "Saber", ""], ["Hashemi", "Matin", ""]]}, {"id": "1907.06529", "submitter": "Michal Wlodarczyk", "authors": "Micha{\\l} W{\\l}odarczyk", "title": "Parameterized inapproximability for Steiner Orientation by Gap\n  Amplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-Steiner Orientation problem, we are given a mixed graph, that is,\nwith both directed and undirected edges, and a set of $k$ terminal pairs. The\ngoal is to find an orientation of the undirected edges that maximizes the\nnumber of terminal pairs for which there is a path from the source to the sink.\nThe problem is known to be W[1]-hard when parameterized by k and hard to\napproximate up to some constant for FPT algorithms assuming Gap-ETH. On the\nother hand, no approximation factor better than $O(k)$ is known.\n  We show that $k$-Steiner Orientation is unlikely to admit an approximation\nalgorithm with any constant factor, even within FPT running time. To obtain\nthis result, we construct a self-reduction via a hashing-based gap\namplification technique, which turns out useful even outside of the FPT\nparadigm. Precisely, we rule out any approximation factor of the form $(\\log\nk)^{o(1)}$ for FPT algorithms (assuming FPT $\\ne$ W[1]) and $(\\log n)^{o(1)}$\nfor~purely polynomial-time algorithms (assuming that the class W[1] does not\nadmit randomized FPT algorithms). Moreover, we prove $k$-Steiner Orientation to\nbelong to W[1], which entails W[1]-completeness of $(\\log\nk)^{o(1)}$-approximation for $k$-Steiner Orientation This provides an example\nof a natural approximation task that is complete in a parameterized complexity\nclass.\n  Finally, we apply our technique to the maximization version of directed\nmulticut - Max $(k,p)$-Directed Multicut - where we are given a directed graph,\n$k$ terminals pairs, and a budget $p$. The goal is to maximize the number of\nseparated terminal pairs by removing $p$ edges. We present a simple proof that\nthe problem admits no FPT approximation with factor $O(k^{\\frac 1 2 -\n\\epsilon})$ (assuming FPT $\\ne$ W[1]) and no polynomial-time approximation with\nratio $O(|E(G)|^{\\frac 1 2 - \\epsilon})$ (assuming NP $\\not\\subseteq$ co-RP).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 14:55:41 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 09:36:22 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 13:30:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1907.06565", "submitter": "Jasjeet Dhaliwal", "authors": "Jasjeet Dhaliwal, Kyle Hambrook", "title": "Recovery Guarantees for Compressible Signals with Adversarial Noise", "comments": "Theorem 1 updated, \\ell_\\infty defense added, Lemma 9 added, comp.\n  section updated, abstract updated, and other minor writing edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.DS cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide recovery guarantees for compressible signals that have been\ncorrupted with noise and extend the framework introduced in\n\\cite{bafna2018thwarting} to defend neural networks against $\\ell_0$-norm,\n$\\ell_2$-norm, and $\\ell_{\\infty}$-norm attacks. Our results are general as\nthey can be applied to most unitary transforms used in practice and hold for\n$\\ell_0$-norm, $\\ell_2$-norm, and $\\ell_\\infty$-norm bounded noise. In the case\nof $\\ell_0$-norm noise, we prove recovery guarantees for Iterative Hard\nThresholding (IHT) and Basis Pursuit (BP). For $\\ell_2$-norm bounded noise, we\nprovide recovery guarantees for BP and for the case of $\\ell_\\infty$-norm\nbounded noise, we provide recovery guarantees for Dantzig Selector (DS). These\nguarantees theoretically bolster the defense framework introduced in\n\\cite{bafna2018thwarting} for defending neural networks against adversarial\ninputs. Finally, we experimentally demonstrate the effectiveness of this\ndefense framework against an array of $\\ell_0$, $\\ell_2$ and $\\ell_\\infty$ norm\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:15:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 12:56:03 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 16:53:34 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Dhaliwal", "Jasjeet", ""], ["Hambrook", "Kyle", ""]]}, {"id": "1907.06576", "submitter": "Ioannis Lamprou", "authors": "Ioannis Lamprou, Ioannis Sigalas, Vassilis Zissimopoulos", "title": "Improved Budgeted Connected Domination and Budgeted Edge-Vertex\n  Domination", "comments": "17 pages, improved results, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the \\emph{Budgeted} version of the classical \\emph{Connected\nDominating Set} problem (BCDS). Given a graph $G$ and a budget $k$, we seek a\nconnected subset of at most $k$ vertices maximizing the number of dominated\nvertices in $G$. We improve over the previous $(1-1/e)/13$ approximation in\n[Khuller, Purohit, and Sarpatwar,\\ \\emph{SODA 2014}] by introducing a new\nmethod for performing tree decompositions in the analysis of the last part of\nthe algorithm. This new approach provides a $(1-1/e)/12$ approximation\nguarantee. By generalizing the analysis of the first part of the algorithm, we\nare able to modify it appropriately and obtain a further improvement to\n$(1-e^{-7/8})/11$. On the other hand, we prove a $(1-1/e+\\epsilon)$\ninapproximability bound, for any $\\epsilon > 0$.\n  We also examine the \\emph{edge-vertex domination} variant, where an edge\ndominates its endpoints and all vertices neighboring them. In \\emph{Budgeted\nEdge-Vertex Domination} (BEVD), we are given a graph $G$, and a budget $k$, and\nwe seek a, not necessarily connected, subset of $k$ edges such that the number\nof dominated vertices in $G$ is maximized. We prove there exists a\n$(1-1/e)$-approximation algorithm. Also, for any $\\epsilon > 0$, we present a\n$(1-1/e+\\epsilon)$-inapproximability result by a gap-preserving reduction from\nthe \\emph{maximum coverage} problem. Finally, we examine the \"dual\"\n\\emph{Partial Edge-Vertex Domination} (PEVD) problem, where a graph $G$ and a\nquota $n'$ are given. The goal is to select a minimum-size set of edges to\ndominate at least $n'$ vertices in $G$. In this case, we present a\n$H(n')$-approximation algorithm by a reduction to the \\emph{partial cover}\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:32:26 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 17:12:12 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 11:57:32 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Lamprou", "Ioannis", ""], ["Sigalas", "Ioannis", ""], ["Zissimopoulos", "Vassilis", ""]]}, {"id": "1907.06688", "submitter": "Timothy Fong Nam Chan", "authors": "Timothy F. N. Chan, Jacob W. Cooper, Martin Koutecky, Daniel Kral,\n  Kristyna Pekarkova", "title": "Matrices of optimal tree-depth and a row-invariant parameterized\n  algorithm for integer programming", "comments": "Full version. 48 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long line of research on fixed parameter tractability of integer\nprogramming culminated with showing that integer programs with n variables and\na constraint matrix with dual tree-depth d and largest entry D are solvable in\ntime g(d,D)poly(n) for some function g. However, the dual tree-depth of a\nconstraint matrix is not preserved by row operations, i.e., a given integer\nprogram can be equivalent to another with a smaller dual tree-depth, and thus\ndoes not reflect its geometric structure.\n  We prove that the minimum dual tree-depth of a row-equivalent matrix is equal\nto the branch-depth of the matroid defined by the columns of the matrix. We\ndesign a fixed parameter algorithm for computing branch-depth of matroids\nrepresented over a finite field and a fixed parameter algorithm for computing a\nrow-equivalent matrix with minimum dual tree-depth. Finally, we use these\nresults to obtain an algorithm for integer programming running in time\ng(d*,D)poly(n) where d* is the branch-depth of the constraint matrix; the\nbranch-depth cannot be replaced by the more permissive notion of branch-width.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 18:37:50 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 20:20:41 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 15:45:35 GMT"}, {"version": "v4", "created": "Sun, 27 Jun 2021 13:08:16 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chan", "Timothy F. N.", ""], ["Cooper", "Jacob W.", ""], ["Koutecky", "Martin", ""], ["Kral", "Daniel", ""], ["Pekarkova", "Kristyna", ""]]}, {"id": "1907.06743", "submitter": "Antoine Genitrini", "authors": "Julien Cl\\'ement and Antoine Genitrini", "title": "Binary Decision Diagrams: from Tree Compaction to Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any Boolean function corresponds with a complete full binary decision tree.\nThis tree can in turn be represented in a maximally compact form as a direct\nacyclic graph where common subtrees are factored and shared, keeping only one\ncopy of each unique subtree. This yields the celebrated and widely used\nstructure called reduced ordered binary decision diagram (ROBDD). We propose to\nrevisit the classical compaction process to give a new way of enumerating\nROBDDs of a given size without considering fully expanded trees and the\ncompaction step. Our method also provides an unranking procedure for the set of\nROBDDs. As a by-product we get a random uniform and exhaustive sampler for\nROBDDs for a given number of variables and size.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:43:18 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 09:14:30 GMT"}, {"version": "v3", "created": "Sun, 24 May 2020 08:52:16 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cl\u00e9ment", "Julien", ""], ["Genitrini", "Antoine", ""]]}, {"id": "1907.06748", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Designing Perfect Simulation Algorithms using Local Correctness", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a randomized algorithm that draws samples exactly from a\ndistribution using recursion. Such an algorithm is called a perfect simulation,\nand here a variety of methods for building this type of algorithm are shown to\nderive from the same result: the Fundamental Theorem of Perfect Simulation\n(FTPS). The FTPS gives two necessary and sufficient conditions for the output\nof a recursive probabilistic algorithm to come exactly from the desired\ndistribution. First, the algorithm must terminate with probability 1. Second,\nthe algorithm must be locally correct, which means that if the recursive calls\nin the original algorithm are replaced by oracles that draw from the desired\ndistribution, then this new algorithm can be proven to be correct. While it is\nusually straightforward to verify these conditions, they are surprisingly\npowerful, giving the correctness of Acceptance/Rejection, Coupling from the\nPast, the Randomness Recycler, Read-once CFTP, Partial Rejection Sampling,\nPartially Recursive Acceptance Rejection, and various Bernoulli Factories. We\nillustrate the use of this algorithm by building a new Bernoulli Factory for\nlinear functions that is 41\\% faster than the previous method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:46:43 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1907.06786", "submitter": "Khaled Elbassioni", "authors": "Khaled Elbassioni", "title": "Some Black-box Reductions for Objective-robust Discrete Optimization\n  Problems Based on their LP-Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider robust discrete minimization problems where uncertainty is\ndefined by a convex set in the objective. We show how an integrality gap\nverifier for the linear programming relaxation of the non-robust version of the\nproblem can be used to derive approximation algorithms for the robust version.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 23:17:48 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Elbassioni", "Khaled", ""]]}, {"id": "1907.06814", "submitter": "Yuxuan Du", "authors": "Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao", "title": "A Quantum-inspired Algorithm for General Minimum Conical Hull Problems", "comments": null, "journal-ref": "Phys. Rev. Research 2, 033199 (2020)", "doi": "10.1103/PhysRevResearch.2.033199", "report-no": null, "categories": "cs.LG cs.CG cs.DS quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of fundamental machine learning tasks that are addressed by the\nmaximum a posteriori estimation can be reduced to a general minimum conical\nhull problem. The best-known solution to tackle general minimum conical hull\nproblems is the divide-and-conquer anchoring learning scheme (DCA), whose\nruntime complexity is polynomial in size. However, big data is pushing these\npolynomial algorithms to their performance limits. In this paper, we propose a\nsublinear classical algorithm to tackle general minimum conical hull problems\nwhen the input has stored in a sample-based low-overhead data structure. The\nalgorithm's runtime complexity is polynomial in the rank and polylogarithmic in\nsize. The proposed algorithm achieves the exponential speedup over DCA and,\ntherefore, provides advantages for high dimensional problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 02:42:19 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Du", "Yuxuan", ""], ["Hsieh", "Min-Hsiu", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1907.06857", "submitter": "Arnold Filtser", "authors": "Arnold Filtser, Lee-Ad Gottlieb, Robert Krauthgamer", "title": "Labelings vs. Embeddings: On Distributed Representations of Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate for which metric spaces the performance of distance labeling\nand of $\\ell_\\infty$-embeddings differ, and how significant can this difference\nbe. Recall that a distance labeling is a distributed representation of\ndistances in a metric space $(X,d)$, where each point $x\\in X$ is assigned a\nsuccinct label, such that the distance between any two points $x,y \\in X$ can\nbe approximated given only their labels. A highly structured special case is an\nembedding into $\\ell_\\infty$, where each point $x\\in X$ is assigned a vector\n$f(x)$ such that $\\|f(x)-f(y)\\|_\\infty$ is approximately $d(x,y)$. The\nperformance of a distance labeling or an $\\ell_\\infty$-embedding is measured\nvia its distortion and its label-size/dimension.\n  We also study the analogous question for the prioritized versions of these\ntwo measures. Here, a priority order $\\pi=(x_1,\\dots,x_n)$ of the point set $X$\nis given, and higher-priority points should have shorter labels. Formally, a\ndistance labeling has prioritized label-size $\\alpha(.)$ if every $x_j$ has\nlabel size at most $\\alpha(j)$. Similarly, an embedding $f: X \\to \\ell_\\infty$\nhas prioritized dimension $\\alpha(.)$ if $f(x_j)$ is non-zero only in the first\n$\\alpha(j)$ coordinates. In addition, we compare these their prioritized\nmeasures to their classical (worst-case) versions.\n  We answer these questions in several scenarios, uncovering a surprisingly\ndiverse range of behaviors. First, in some cases labelings and embeddings have\nvery similar worst-case performance, but in other cases there is a huge\ndisparity. However in the prioritized setting, we most often find a strict\nseparation between the performance of labelings and embeddings. And finally,\nwhen comparing the classical and prioritized settings, we find that the\nworst-case bound for label size often ``translates'' to a prioritized one, but\nalso a surprising exception to this rule.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 06:25:48 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Filtser", "Arnold", ""], ["Gottlieb", "Lee-Ad", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1907.06983", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Lossless Prioritized Embeddings", "comments": "abstract was shortened", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given metric spaces $(X,d)$ and $(Y,\\rho)$ and an ordering\n$x_1,x_2,\\ldots,x_n$ of $(X,d)$, an embedding $f: X \\rightarrow Y$ is said to\nhave a prioritized distortion $\\alpha(\\cdot)$, if for any pair $x_j,x'$ of\ndistinct points in $X$, the distortion provided by $f$ for this pair is at most\n$\\alpha(j)$. If $Y$ is a normed space, the embedding is said to have\nprioritized dimension $\\beta(\\cdot)$, if $f(x_j)$ may have nonzero entries only\nin its first $\\beta(j)$ coordinates.\n  The notion of prioritized embedding was introduced by \\cite{EFN15}, where a\ngeneral methodology for constructing such embeddings was developed. Though this\nmethodology enables \\cite{EFN15} to come up with many prioritized embeddings,\nit typically incurs some loss in the distortion. This loss is problematic for\nisometric embeddings. It is also troublesome for Matousek's embedding of\ngeneral metrics into $\\ell_\\infty$, which for a parameter $k = 1,2,\\ldots$,\nprovides distortion $2k-1$ and dimension $O(k \\log n \\cdot n^{1/k})$.\n  In this paper we devise two lossless prioritized embeddings. The first one is\nan isometric prioritized embedding of tree metrics into $\\ell_\\infty$ with\ndimension $O(\\log j)$. The second one is a prioritized Matousek's embedding of\ngeneral metrics into $\\ell_\\infty$, which provides prioritized distortion $2\n\\lceil k {{\\log j} \\over {\\log n}} \\rceil - 1$ and dimension $O(k \\log n \\cdot\nn^{1/k})$, again matching the worst-case guarantee $2k-1$ in the distortion of\nthe classical Matousek's embedding. We also provide a dimension-prioritized\nvariant of Matousek's embedding. Finally, we devise prioritized embeddings of\ngeneral metrics into (single) ultra-metric and of general graphs into (single)\nspanning tree with asymptotically optimal distortion.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:29:02 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1907.07078", "submitter": "Amitabh Trehan", "authors": "Walter Hussak and Amitabh Trehan", "title": "On The Termination of a Flooding Process", "comments": "A summary to appear as a Brief Announcement at ACM PODC'19. Full\n  version under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flooding is among the simplest and most fundamental of all distributed\nnetwork algorithms. A node begins the process by sending a message to all its\nneighbours and the neighbours, in the next round forward the message to all the\nneighbours they did not receive the message from and so on. We assume that the\nnodes do not keep a record of the flooding event. We call this amnesiac\nflooding (AF). Since the node forgets, if the message is received again in\nsubsequent rounds, it will be forwarded again raising the possibility that the\nmessage may be circulated infinitely even on a finite graph. As far as we know,\nthe question of termination for such a flooding process has not been settled -\nrather, non-termination is implicitly assumed.\n  In this paper, we show that synchronous AF always terminates on any arbitrary\nfinite graph and derive exact termination times which differ sharply in\nbipartite and non-bipartite graphs. Let $G$ be a finite connected graph. We\nshow that synchronous AF from a single source node terminates on $G$ in $e$\nrounds, where $e$ is the eccentricity of the source node, if and only if $G$ is\nbipartite. For non-bipartite $G$, synchronous AF from a single source\nterminates in $j$ rounds where $e < j \\leq e+d+1$ and $d$ is the diameter of\n$G$. This limits termination time to at most $d$ and at most $2d + 1$ for\nbipartite and non-bipartite graphs respectively. If communication/broadcast to\nall nodes is the motivation, our results show that AF is asymptotically time\noptimal and obviates the need for construction and maintenance of spanning\nstructures like spanning trees. The clear separation in the termination times\nof bipartite and non-bipartite graphs also suggests mechanisms for distributed\ndiscovery of the topology/distances in arbitrary graphs.\n  For comparison, we show that, in asynchronous networks, an adaptive adversary\ncan force AF to be non-terminating.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:45:12 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hussak", "Walter", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1907.07149", "submitter": "Emilio Cruciani", "authors": "Luca Becchetti, Emilio Cruciani, Francesco Pasquale, Sara Rizzo", "title": "Step-by-Step Community Detection in Volume-Regular Graphs", "comments": "Preliminary version appeared in Proceedings of ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral techniques have proved amongst the most effective approaches to\ngraph clustering. However, in general they require explicit computation of the\nmain eigenvectors of a suitable matrix (usually the Laplacian matrix of the\ngraph). Recent work (e.g., Becchetti et al., SODA 2017) suggests that observing\nthe temporal evolution of the power method applied to an initial random vector\nmay, at least in some cases, provide enough information on the space spanned by\nthe first two eigenvectors, so as to allow recovery of a hidden partition\nwithout explicit eigenvector computations. While the results of Becchetti et\nal. apply to perfectly balanced partitions and/or graphs that exhibit very\nstrong forms of regularity, we extend their approach to graphs containing a\nhidden $k$ partition and characterized by a milder form of volume-regularity.\nWe show that the class of $k$-volume-regular graphs is the largest class of\nundirected (possibly weighted) graphs whose transition matrix admits $k$\n\"stepwise\" eigenvectors (i.e., vectors that are constant over each set of the\nhidden partition). To obtain this result, we highlight a connection between\nvolume regularity and lumpability of Markov chains. Moreover, we prove that if\nthe stepwise eigenvectors are those associated to the first $k$ eigenvalues and\nthe gap between the $k$-th and the ($k$+1)-th eigenvalues is sufficiently\nlarge, the averaging dynamics of Becchetti et al. recovers the underlying\ncommunity structure of the graph in logarithmic time, with high probability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:15:42 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 10:47:30 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Becchetti", "Luca", ""], ["Cruciani", "Emilio", ""], ["Pasquale", "Francesco", ""], ["Rizzo", "Sara", ""]]}, {"id": "1907.07167", "submitter": "Deeksha Adil", "authors": "Deeksha Adil, Richard Peng, Sushant Sachdeva", "title": "Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression", "comments": "Code for this work is available at\n  https://github.com/utoronto-theory/pIRLS", "journal-ref": "In Advances in Neural Information Processing Systems (pp.\n  14166-14177) 2019", "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression in $\\ell_p$-norm is a canonical optimization problem that\narises in several applications, including sparse recovery, semi-supervised\nlearning, and signal processing. Generic convex optimization algorithms for\nsolving $\\ell_p$-regression are slow in practice. Iteratively Reweighted Least\nSquares (IRLS) is an easy to implement family of algorithms for solving these\nproblems that has been studied for over 50 years. However, these algorithms\noften diverge for p > 3, and since the work of Osborne (1985), it has been an\nopen problem whether there is an IRLS algorithm that is guaranteed to converge\nrapidly for p > 3. We propose p-IRLS, the first IRLS algorithm that provably\nconverges geometrically for any $p \\in [2,\\infty).$ Our algorithm is simple to\nimplement and is guaranteed to find a $(1+\\varepsilon)$-approximate solution in\n$O(p^{3.5} m^{\\frac{p-2}{2(p-1)}} \\log \\frac{m}{\\varepsilon}) \\le O_p(\\sqrt{m}\n\\log \\frac{m}{\\varepsilon} )$ iterations. Our experiments demonstrate that it\nperforms even better than our theoretical bounds, beats the standard Matlab/CVX\nimplementation for solving these problems by 10--50x, and is the fastest among\navailable implementations in the high-accuracy regime.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:50:45 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 09:31:08 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Adil", "Deeksha", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1907.07230", "submitter": "Umang Bhaskar", "authors": "Umang Bhaskar and Gunjan Kumar", "title": "The Complexity of Partial Function Extension for Coverage Functions", "comments": "To appear in Approx 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coverage functions are an important subclass of submodular functions, finding\napplications in machine learning, game theory, social networks, and facility\nlocation. We study the complexity of partial function extension to coverage\nfunctions. That is, given a partial function consisting of a family of subsets\nof $[m]$ and a value at each point, does there exist a coverage function\ndefined on all subsets of $[m]$ that extends this partial function? Partial\nfunction extension is previously studied for other function classes, including\nboolean functions and convex functions, and is useful in many fields, such as\nobtaining bounds on learning these function classes.\n  We show that determining extendibility of a partial function to a coverage\nfunction is NP-complete, establishing in the process that there is a\npolynomial-sized certificate of extendibility. The hardness also gives us a\nlower bound for learning coverage functions. We then study two natural notions\nof approximate extension, to account for errors in the data set. The two\nnotions correspond roughly to multiplicative point-wise approximation and\nadditive $L_1$ approximation. We show upper and lower bounds for both notions\nof approximation. In the second case we obtain nearly tight bounds.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 19:39:55 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Bhaskar", "Umang", ""], ["Kumar", "Gunjan", ""]]}, {"id": "1907.07574", "submitter": "Samson Zhou", "authors": "Vladimir Braverman, Harry Lang, Enayat Ullah, Samson Zhou", "title": "Improved Algorithms for Time Decay Streams", "comments": "To appear at APPROX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the time-decay model for data streams, elements of an underlying data set\narrive sequentially with the recently arrived elements being more important. A\ncommon approach for handling large data sets is to maintain a \\emph{coreset}, a\nsuccinct summary of the processed data that allows approximate recovery of a\npredetermined query. We provide a general framework that takes any\noffline-coreset and gives a time-decay coreset for polynomial time decay\nfunctions.\n  We also consider the exponential time decay model for $k$-median clustering,\nwhere we provide a constant factor approximation algorithm that utilizes the\nonline facility location algorithm. Our algorithm stores\n$\\mathcal{O}(k\\log(h\\Delta)+h)$ points where $h$ is the half-life of the decay\nfunction and $\\Delta$ is the aspect ratio of the dataset. Our techniques extend\nto $k$-means clustering and $M$-estimators as well.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:20:43 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Braverman", "Vladimir", ""], ["Lang", "Harry", ""], ["Ullah", "Enayat", ""], ["Zhou", "Samson", ""]]}, {"id": "1907.07795", "submitter": "Niels M\\\"oller", "authors": "Niels M\\\"oller", "title": "Efficient computation of the Jacobi symbol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The family of left-to-right GCD algorithms reduces input numbers by\nrepeatedly subtracting the smaller number, or multiple of the smaller number,\nfrom the larger number. This paper describes how to extend any such algorithm\nto compute the Jacobi symbol, using a single table lookup per reduction. For\nboth quadratic time GCD algorithms (Euclid, Lehmer) and subquadratic algorithms\n(Knuth, Sch\\\"onhage, M\\\"oller), the additional cost is linear, roughly one\ntable lookup per quotient in the quotient sequence. This method was used for\nthe 2010 rewrite of the Jacobi symbol computation in GMP.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:11:22 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["M\u00f6ller", "Niels", ""]]}, {"id": "1907.07833", "submitter": "Fernando Granha Jeronimo", "authors": "Vedat Levi Alev, Fernando Granha Jeronimo, Madhur Tulsiani", "title": "Approximating Constraint Satisfaction Problems on High-Dimensional\n  Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximately solving constraint satisfaction\nproblems with arity $k > 2$ ($k$-CSPs) on instances satisfying certain\nexpansion properties, when viewed as hypergraphs. Random instances of $k$-CSPs,\nwhich are also highly expanding, are well-known to be hard to approximate using\nknown algorithmic techniques (and are widely believed to be hard to approximate\nin polynomial time). However, we show that this is not necessarily the case for\ninstances where the hypergraph is a high-dimensional expander.\n  We consider the spectral definition of high-dimensional expansion used by\nDinur and Kaufman [FOCS 2017] to construct certain primitives related to PCPs.\nThey measure the expansion in terms of a parameter $\\gamma$ which is the\nanalogue of the second singular value for expanding graphs. Extending the\nresults by Barak, Raghavendra and Steurer [FOCS 2011] for 2-CSPs, we show that\nif an instance of MAX k-CSP over alphabet $[q]$ is a high-dimensional expander\nwith parameter $\\gamma$, then it is possible to approximate the maximum\nfraction of satisfiable constraints up to an additive error $\\epsilon$ using\n$q^{O(k)} \\cdot (k/\\epsilon)^{O(1)}$ levels of the sum-of-squares SDP\nhierarchy, provided $\\gamma \\leq \\epsilon^{O(1)} \\cdot (1/(kq))^{O(k)}$.\n  Based on our analysis, we also suggest a notion of threshold-rank for\nhypergraphs, which can be used to extend the results for approximating 2-CSPs\non low threshold-rank graphs. We show that if an instance of MAX k-CSP has\nthreshold rank $r$ for a threshold $\\tau = (\\epsilon/k)^{O(1)} \\cdot\n(1/q)^{O(k)}$, then it is possible to approximately solve the instance up to\nadditive error $\\epsilon$, using $r \\cdot q^{O(k)} \\cdot (k/\\epsilon)^{O(1)}$\nlevels of the sum-of-squares hierarchy. As in the case of graphs,\nhigh-dimensional expanders (with sufficiently small $\\gamma$) have threshold\nrank 1 according to our definition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 01:31:52 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Alev", "Vedat Levi", ""], ["Jeronimo", "Fernando Granha", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1907.07885", "submitter": "Abhishek Kr Singh", "authors": "Suneel Sarswat and Abhishek Kr Singh", "title": "Formal verification of trading in financial markets", "comments": "Preprint of 12 pages in lipicsv2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.FL cs.GT cs.SC q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formal framework for analyzing trades in financial markets. An\nexchange is where multiple buyers and sellers participate to trade. These days,\nall big exchanges use computer algorithms that implement double sided auctions\nto match buy and sell requests and these algorithms must abide by certain\nregulatory guidelines. For example, market regulators enforce that a matching\nproduced by exchanges should be \\emph{fair}, \\emph{uniform} and\n\\emph{individual rational}. To verify these properties of trades, we first\nformally define these notions in a theorem prover and then give formal proofs\nof relevant results on matchings. Finally, we use this framework to verify\nproperties of two important classes of double sided auctions. All the\ndefinitions and results presented in this paper are completely formalised in\nthe Coq proof assistant without adding any additional axioms to it.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 05:50:29 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sarswat", "Suneel", ""], ["Singh", "Abhishek Kr", ""]]}, {"id": "1907.07889", "submitter": "Andrej (Andy) Brodnik", "authors": "Andrej Brodnik and Aleksander Malni\\v{c} and Rok Po\\v{z}ar", "title": "The simultaneous conjugacy problem in the symmetric group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transitive simultaneous conjugacy problem asks whether there exists a\npermutation $\\tau \\in S_n$ such that $b_j = \\tau^{-1} a_j \\tau$ holds for all\n$j = 1,2, \\ldots, d$, where $a_1, a_2, \\ldots, a_d$ and $b_1, b_2, \\ldots, b_d$\nare given sequences of $d$ permutations in $S_n$, each of which generates a\ntransitive subgroup of $S_n$. As from mid 70' it has been known that the\nproblem can be solved in $O(dn^2)$ time. An algorithm with running time $O(dn\n\\log(dn))$, proposed in late 80', does not work correctly on all input data. In\nthis paper we solve the transitive simultaneous conjugacy problem in $O(n^2\n\\log d / \\log n + dn\\log n)$ time and $O(n^{3/ 2} + dn)$ space. Experimental\nevaluation on random instances shows that the expected running time of our\nalgorithm is considerably better, perhaps even nearly linear in $n$ at given\n$d$.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 06:28:18 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 07:15:31 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 20:13:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Brodnik", "Andrej", ""], ["Malni\u010d", "Aleksander", ""], ["Po\u017ear", "Rok", ""]]}, {"id": "1907.07910", "submitter": "V\\'aclav Bla\\v{z}ej", "authors": "V\\'aclav Bla\\v{z}ej, Jan Maty\\'a\\v{s} K\\v{r}i\\v{s}\\v{t}an, Tom\\'a\\v{s}\n  Valla", "title": "On the m-eternal Domination Number of Cactus Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, guards are placed on vertices of $G$. Then vertices are\nsubject to an infinite sequence of attacks so that each attack must be defended\nby a guard moving from a neighboring vertex. The m-eternal domination number is\nthe minimum number of guards such that the graph can be defended indefinitely.\nIn this paper we study the m-eternal domination number of cactus graphs, that\nis, connected graphs where each edge lies in at most two cycles, and we\nconsider three variants of the m-eternal domination number: first variant\nallows multiple guards to occupy a single vertex, second variant does not allow\nit, and in the third variant additional \"eviction\" attacks must be defended. We\nprovide a new upper bound for the m-eternal domination number of cactus graphs,\nand for a subclass of cactus graphs called Christmas cactus graphs, where each\nvertex lies in at most two cycles, we prove that these three numbers are equal.\nMoreover, we present a linear-time algorithm for computing them.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 07:23:23 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bla\u017eej", "V\u00e1clav", ""], ["K\u0159i\u0161\u0165an", "Jan Maty\u00e1\u0161", ""], ["Valla", "Tom\u00e1\u0161", ""]]}, {"id": "1907.07982", "submitter": "Jan van den Brand", "authors": "Jan van den Brand and Thatchaphol Saranurak", "title": "Sensitive Distance and Reachability Oracles for Large Batch Updates", "comments": "To appear in FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sensitive distance oracle problem, there are three phases. We first\npreprocess a given directed graph $G$ with $n$ nodes and integer weights from\n$[-W,W]$. Second, given a single batch of $f$ edge insertions and deletions, we\nupdate the data structure. Third, given a query pair of nodes $(u,v)$, return\nthe distance from $u$ to $v$. In the easier problem called sensitive\nreachability oracle problem, we only ask if there exists a directed path from\n$u$ to $v$.\n  Our first result is a sensitive distance oracle with\n$\\tilde{O}(Wn^{\\omega+(3-\\omega)\\mu})$ preprocessing time,\n$\\tilde{O}(Wn^{2-\\mu}f^{2}+Wnf^{\\omega})$ update time, and\n$\\tilde{O}(Wn^{2-\\mu}f+Wnf^{2})$ query time where the parameter $\\mu\\in[0,1]$\ncan be chosen. The data-structure requires $O(Wn^{2+\\mu} \\log n)$ bits of\nmemory. This is the first algorithm that can handle $f\\ge\\log n$ updates.\nPrevious results (e.g. [Demetrescu et al. SICOMP'08; Bernstein and Karger\nSODA'08 and FOCS'09; Duan and Pettie SODA'09; Grandoni and Williams FOCS'12])\ncan handle at most 2 updates. When $3\\le f\\le\\log n$, the only non-trivial\nalgorithm was by [Weimann and Yuster FOCS'10]. When $W=\\tilde{O}(1)$, our\nalgorithm simultaneously improves their preprocessing time, update time, and\nquery time. In particular, when $f=\\omega(1)$, their update and query time is\n$\\Omega(n^{2-o(1)})$, while our update and query time are truly subquadratic in\n$n$, i.e., ours is faster by a polynomial factor of $n$. To highlight the\ntechnique, ours is the first graph algorithm that exploits the kernel basis\ndecomposition of polynomial matrices by [Jeannerod and Villard J.Comp'05; Zhou,\nLabahn and Storjohann J.Comp'15] developed in the symbolic computation\ncommunity.\n  [...]\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:52:13 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 14:50:50 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Brand", "Jan van den", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1907.08093", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet and Nidhi Purohit", "title": "Metric Dimension Parameterized by Treewidth", "comments": "23 pages, 4 figures, long version of IPEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A resolving set $S$ of a graph $G$ is a subset of its vertices such that no\ntwo vertices of $G$ have the same distance vector to $S$. The Metric Dimension\nproblem asks for a resolving set of minimum size, and in its decision form, a\nresolving set of size at most some specified integer. This problem is\nNP-complete, and remains so in very restricted classes of graphs. It is also\nW[2]-complete with respect to the size of the solution. Metric Dimension has\nproven elusive on graphs of bounded treewidth. On the algorithmic side, a\npolytime algorithm is known for trees, and even for outerplanar graphs, but the\ngeneral case of treewidth at most two is open. On the complexity side, no\nparameterized hardness is known. This has led several papers on the topic to\nask for the parameterized complexity of Metric Dimension with respect to\ntreewidth. We provide a first answer to the question.\n  We show that Metric Dimension parameterized by the treewidth of the input\ngraph is W[1]-hard. More refinedly we prove that, unless the Exponential Time\nHypothesis fails, there is no algorithm solving Metric Dimension in time\n$f(\\text{pw})n^{o(\\text{pw})}$ on $n$-vertex graphs of constant degree, with\n$\\text{pw}$ the pathwidth of the input graph, and $f$ any computable function.\nThis is in stark contrast with an FPT algorithm of Belmonte et al. [SIAM J.\nDiscrete Math. '17] with respect to the combined parameter $\\text{tl}+\\Delta$,\nwhere $\\text{tl}$ is the tree-length and $\\Delta$ the maximum-degree of the\ninput graph.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:46:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Purohit", "Nidhi", ""]]}, {"id": "1907.08111", "submitter": "Felix Happach", "authors": "Felix Happach", "title": "Makespan Minimization with OR-Precedence Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the NP-hard problem of assigning jobs to machines to\nminimize the completion time of the last job. Usually, precedence constraints\nare given by a partial order on the set of jobs, and each job requires all its\npredecessors to be completed before it can start. In his seminal paper, Graham\n(1966) presented a simple 2-approximation algorithm, and, more than 40 years\nlater, Svensson (2010) proved that 2 is essentially the best approximation\nratio one can hope for in general. In this paper, we consider a different type\nof precedence relation that has not been discussed as extensively and is called\nOR-precedence. In order for a job to start, we require that at least one of its\npredecessors is completed - in contrast to all its predecessors. Additionally,\nwe assume that each job has a release date before which it must not start. We\nprove that Graham's algorithm has an approximation guarantee of 2 also in this\nsetting, and present a polynomial-time algorithm that solves the problem to\noptimality, if preemptions are allowed. The latter result is in contrast to\nclassical precedence constraints, for which Ullman (1975) showed that the\npreemptive variant is already NP-hard. Our algorithm generalizes a result of\nJohannes (2005) who gave a polynomial-time algorithm for unit processing time\njobs subject to OR-precedence constraints, but without release dates. The\nperformance guarantees presented here match the best-known ones for special\ncases where classical precedence constraints and OR-precedence constraints\ncoincide.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 15:25:37 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 12:24:59 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Happach", "Felix", ""]]}, {"id": "1907.08142", "submitter": "Luca Ferrari", "authors": "Giulio Cerbai, Anders Claesson, Luca Ferrari", "title": "Stack sorting with restricted stacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (classical) problem of characterizing and enumerating permutations that\ncan be sorted using two stacks connected in series is still largely open. In\nthe present paper we address a related problem, in which we impose restrictions\nboth on the procedure and on the stacks. More precisely, we consider a greedy\nalgorithm where we perform the rightmost legal operation (here \"rightmost\"\nrefers to the usual representation of stack sorting problems). Moreover, the\nfirst stack is required to be $\\sigma$-avoiding, for some permutation $\\sigma$,\nmeaning that, at each step, the elements maintained in the stack avoid the\npattern $\\sigma$ when read from top to bottom. Since the set of permutations\nwhich can be sorted by such a device (which we call $\\sigma$-machine) is not\nalways a class, it would be interesting to understand when it happens. We will\nprove that the set of $\\sigma$-machines whose associated sortable permutations\nare not a class is counted by Catalan numbers. Moreover, we will analyze two\nspecific $\\sigma$-machines in full details (namely when $\\sigma=321$ and\n$\\sigma=123$), providing for each of them a complete characterization and\nenumeration of sortable permutations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 16:24:30 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Cerbai", "Giulio", ""], ["Claesson", "Anders", ""], ["Ferrari", "Luca", ""]]}, {"id": "1907.08246", "submitter": "Matteo Fischetti", "authors": "Matteo Fischetti and Domenico Salvagnin", "title": "Finding First and Most-Beautiful Queens by Integer Programming", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The n-queens puzzle is a well-known combinatorial problem that requires to\nplace n queens on an n x n chessboard so that no two queens can attack each\nother. Since the 19th century, this problem was studied by many mathematicians\nand computer scientists. While finding any solution to the n-queens puzzle is\nrather straightforward, it is very challenging to find the lexicographically\nfirst (or smallest) feasible solution. Solutions for this type are known in the\nliterature for n <= 55, while for some larger chessboards only partial\nsolutions are known. The present paper was motivated by the question of whether\nInteger Linear Programming (ILP) can be used to compute solutions for some open\ninstances. We describe alternative ILP-based solution approaches, and show that\nthey are indeed able to compute (sometimes in unexpectedly-short computing\ntimes) many new lexicographically optimal solutions for n ranging from 56 to\n115. One of the proposed algorithms is a pure cutting plane method based on a\ncombinatorial variant of classical Gomory cuts. We also address an intriguing\n\"lexicographic bottleneck\" (or min-max) variant of the problem that requires\nfinding a most beautiful (in a well defined sense) placement, and report its\nsolution for n up to 176.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:55:41 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Fischetti", "Matteo", ""], ["Salvagnin", "Domenico", ""]]}, {"id": "1907.08304", "submitter": "Lavina Jain", "authors": "Syamantak Das, Lavina Jain, Nikhil Kumar", "title": "A Constant Factor Approximation for Capacitated Min-Max Tree Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ with non-negative real edge lengths and an integer\nparameter $k$, the Min-Max k-Tree Cover problem seeks to find a set of at most\n$k$ subtrees of $G$, such that the union of the trees is the vertex set $V$.\nThe objective is to minimize the maximum length among all the trees. We give\nthe first constant factor approximation for the hard uniform capacitated\nversion of this problem, where, an input parameter $\\lambda$ upper bounds the\nnumber of vertices that can be covered by any of the trees. Our result extends\nto the rooted version of the problem, where we are given a set of $k$ root\nvertices, $R$ and each of the covering trees is required to include a distinct\nvertex in $R$ as the root. Prior to our work, the only result known was a\n$(2k-1)$-approximation algorithm for the special case when the total number of\nvertices in the graph is $k\\lambda$ [Guttmann-Beck and Hassin, J. of\nAlgorithms, 1997].\n  Our technique circumvents the difficulty of using the minimum spanning tree\nof the graph as a lower bound, which is standard for the uncapacitated version\nof the problem [Even et al., OR Letters 2004] [Khani et al., Algorithmica\n2010]. Instead, we use Steiner trees that cover $\\lambda$ vertices along with\nan iterative refinement procedure that ensures that the output trees have low\ncost and the vertices are well distributed among the trees.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 21:59:33 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 13:44:40 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Das", "Syamantak", ""], ["Jain", "Lavina", ""], ["Kumar", "Nikhil", ""]]}, {"id": "1907.08306", "submitter": "Brian Axelrod", "authors": "Brian Axelrod, Ilias Diakonikolas, Anastasios Sidiropoulos, Alistair\n  Stewart and Gregory Valiant", "title": "A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via\n  Locally Exponential Families", "comments": "The present paper is a merger of two independent works\n  arXiv:1811.03204 and arXiv:1812.05524, proposing essentially the same\n  algorithm to compute the log-concave MLE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the maximum likelihood multivariate\nlog-concave distribution for a set of points. Specifically, we present an\nalgorithm which, given $n$ points in $\\mathbb{R}^d$ and an accuracy parameter\n$\\epsilon>0$, runs in time $poly(n,d,1/\\epsilon),$ and returns a log-concave\ndistribution which, with high probability, has the property that the likelihood\nof the $n$ points under the returned distribution is at most an additive\n$\\epsilon$ less than the maximum likelihood that could be achieved via any\nlog-concave distribution. This is the first computationally efficient\n(polynomial time) algorithm for this fundamental and practically important\ntask. Our algorithm rests on a novel connection with exponential families: the\nmaximum likelihood log-concave distribution belongs to a class of structured\ndistributions which, while not an exponential family, \"locally\" possesses key\nproperties of exponential families. This connection then allows the problem of\ncomputing the log-concave maximum likelihood distribution to be formulated as a\nconvex optimization problem, and solved via an approximate first-order method.\nEfficiently approximating the (sub) gradients of the objective function of this\noptimization problem is quite delicate, and is the main technical challenge in\nthis work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 22:04:21 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Axelrod", "Brian", ""], ["Diakonikolas", "Ilias", ""], ["Sidiropoulos", "Anastasios", ""], ["Stewart", "Alistair", ""], ["Valiant", "Gregory", ""]]}, {"id": "1907.08355", "submitter": "Alexander Golovnev", "authors": "Alexander Golovnev, Siyao Guo, Thibaut Horel, Sunoo Park, Vinod\n  Vaikuntanathan", "title": "Data Structures Meet Cryptography: 3SUM with Preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows several connections between data structure problems and\ncryptography against preprocessing attacks. Our results span data structure\nupper bounds, cryptographic applications, and data structure lower bounds, as\nsummarized next.\n  First, we apply Fiat--Naor inversion, a technique with cryptographic origins,\nto obtain a data structure upper bound. In particular, our technique yields a\nsuite of algorithms with space $S$ and (online) time $T$ for a preprocessing\nversion of the $N$-input 3SUM problem where $S^3\\cdot T = \\widetilde{O}(N^6)$.\nThis disproves a strong conjecture (Goldstein et al., WADS 2017) that there is\nno data structure that solves this problem for $S=N^{2-\\delta}$ and $T =\nN^{1-\\delta}$ for any constant $\\delta>0$.\n  Secondly, we show equivalence between lower bounds for a broad class of\n(static) data structure problems and one-way functions in the random oracle\nmodel that resist a very strong form of preprocessing attack. Concretely, given\na random function $F: [N] \\to [N]$ (accessed as an oracle) we show how to\ncompile it into a function $G^F: [N^2] \\to [N^2]$ which resists $S$-bit\npreprocessing attacks that run in query time $T$ where\n$ST=O(N^{2-\\varepsilon})$ (assuming a corresponding data structure lower bound\non 3SUM). In contrast, a classical result of Hellman tells us that $F$ itself\ncan be more easily inverted, say with $N^{2/3}$-bit preprocessing in $N^{2/3}$\ntime. We also show that much stronger lower bounds follow from the hardness of\nkSUM. Our results can be equivalently interpreted as security against\nadversaries that are very non-uniform, or have large auxiliary input, or as\nsecurity in the face of a powerfully backdoored random oracle.\n  Thirdly, we give non-adaptive lower bounds for 3SUM and a range of geometric\nproblems which match the best known lower bounds for static data structure\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 03:02:25 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 23:21:07 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 20:59:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Golovnev", "Alexander", ""], ["Guo", "Siyao", ""], ["Horel", "Thibaut", ""], ["Park", "Sunoo", ""], ["Vaikuntanathan", "Vinod", ""]]}, {"id": "1907.08362", "submitter": "Atri Rudra", "authors": "Anna Gilbert and Albert Gu and Christopher Re and Atri Rudra and Mary\n  Wootters", "title": "Sparse Recovery for Orthogonal Polynomial Transforms", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the following sparse recovery problem. We have\nquery access to a vector $\\vx \\in \\R^N$ such that $\\vhx = \\vF \\vx$ is\n$k$-sparse (or nearly $k$-sparse) for some orthogonal transform $\\vF$. The goal\nis to output an approximation (in an $\\ell_2$ sense) to $\\vhx$ in sublinear\ntime. This problem has been well-studied in the special case that $\\vF$ is the\nDiscrete Fourier Transform (DFT), and a long line of work has resulted in\nsparse Fast Fourier Transforms that run in time $O(k \\cdot \\mathrm{polylog}\nN)$. However, for transforms $\\vF$ other than the DFT (or closely related\ntransforms like the Discrete Cosine Transform), the question is much less\nsettled.\n  In this paper we give sublinear-time algorithms---running in time $\\poly(k\n\\log(N))$---for solving the sparse recovery problem for orthogonal transforms\n$\\vF$ that arise from orthogonal polynomials. More precisely, our algorithm\nworks for any $\\vF$ that is an orthogonal polynomial transform derived from\nJacobi polynomials. The Jacobi polynomials are a large class of classical\northogonal polynomials (and include Chebyshev and Legendre polynomials as\nspecial cases), and show up extensively in applications like numerical analysis\nand signal processing. One caveat of our work is that we require an assumption\non the sparsity structure of the sparse vector, although we note that vectors\nwith random support have this property with high probability.\n  Our approach is to give a very general reduction from the $k$-sparse sparse\nrecovery problem to the $1$-sparse sparse recovery problem that holds for any\nflat orthogonal polynomial transform; then we solve this one-sparse recovery\nproblem for transforms derived from Jacobi polynomials.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 03:44:05 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Gilbert", "Anna", ""], ["Gu", "Albert", ""], ["Re", "Christopher", ""], ["Rudra", "Atri", ""], ["Wootters", "Mary", ""]]}, {"id": "1907.08399", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Cluster deletion revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Cluster Deletion problem the input is a graph $G$ and an integer $k$,\nand the goal is to decide whether there is a set of at most $k$ edges whose\nremoval from $G$ results a graph in which every connected component is a\nclique. In this paper we give an algorithm for Cluster Deletion whose running\ntime is $O^*(1.404^k)$.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:16:22 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1907.08559", "submitter": "Jonathan Sorenson", "authors": "Brianna Sorenson and Jonathan P Sorenson and Jonathan Webster", "title": "An Algorithm and Estimates for the Erd\\H{o}s-Selfridge Function (work in\n  progress)", "comments": "25 pages, 5 figures; See the DOI link for the published version in\n  ANTS; This update contains a few new minor results", "journal-ref": null, "doi": "10.2140/obs.2020.4.371", "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $p(n)$ denote the smallest prime divisor of the integer $n$. Define the\nfunction $g(k)$ to be the smallest integer $>k+1$ such that\n$p(\\binom{g(k)}{k})>k$. So we have $g(2)=6$ and $g(3)=g(4)=7$. In this paper we\npresent the following new results on the Erd\\H{o}s-Selfridge function $g(k)$:\nWe present a new algorithm to compute the value of $g(k)$, and use it to both\nverify previous work and compute new values of $g(k)$, with our current limit\nbeing $$ g(323)= 1\\ 69829\\ 77104\\ 46041\\ 21145\\ 63251\\ 22499. $$ We define a\nnew function $\\hat{g}(k)$, and under the assumption of our Uniform Distribution\nHeuristic we show that $$ \\log g(k) = \\log \\hat{g}(k) + O(\\log k) $$ with high\n\"probability\". We also provide computational evidence to support our claim that\n$\\hat{g}(k)$ estimates $g(k)$ reasonably well in practice. There are several\nopen conjectures on the behavior of $g(k)$ which we are able to prove for\n$\\hat{g}(k)$, namely that $$ 0.525\\ldots +o(1) \\quad \\le \\quad \\frac{\\log\n\\hat{g}(k)}{k/\\log k} \\quad \\le \\quad 1+o(1), $$ and that $$\n\\limsup_{k\\rightarrow\\infty} \\frac{\\hat{g}(k+1)}{\\hat{g}(k)}=\\infty.$$ Let\n$G(x,k)$ count the number of integers $n\\le x$ such that $p(\\binom{n}{k})>k$.\nUnconditionally, we prove that for large $x$, $G(x,k)$ is asymptotic to\n$x/\\hat{g}(k)$. And finally, we show that the running time of our new algorithm\nis at most $g(k) \\exp[ -c (k\\log\\log k) /(\\log k)^2 (1+o(1))]$ for a constant\n$c>0$.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 16:08:00 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 16:54:14 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 17:09:28 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Sorenson", "Brianna", ""], ["Sorenson", "Jonathan P", ""], ["Webster", "Jonathan", ""]]}, {"id": "1907.08579", "submitter": "Bryce Sandlund", "authors": "Hicham El-Zein, Meng He, J. Ian Munro, Yakov Nekrich, Bryce Sandlund", "title": "On Approximate Range Mode and Range Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any $\\epsilon \\in (0,1)$, a $(1+\\epsilon)$-approximate range mode query\nasks for the position of an element whose frequency in the query range is at\nmost a factor $(1+\\epsilon)$ smaller than the true mode. For this problem, we\ndesign an $O(n/\\epsilon)$ bit data structure supporting queries in\n$O(\\lg(1/\\epsilon))$ time. This is an encoding data structure which does not\nrequire access to the input sequence; we prove the space cost is asymptotically\noptimal for constant $\\epsilon$. Our solution improves the previous best result\nof Greve et al. (Cell Probe Lower Bounds and Approximations for Range Mode,\nICALP'10) by reducing the space cost by a factor of $\\lg n$ while achieving the\nsame query time. We also design an $O(n)$-word dynamic data structure that\nanswers queries in $O(\\lg n /\\lg\\lg n)$ time and supports insertions and\ndeletions in $O(\\lg n)$ time, for any constant $\\epsilon \\in (0,1)$. This is\nthe first result on dynamic approximate range mode; it can also be used to\nobtain the first static data structure for approximate 3-sided range mode\nqueries in two dimensions.\n  We also consider approximate range selection. For any $\\alpha \\in (0,1/2)$,\nan $\\alpha$-approximate range selection query asks for the position of an\nelement whose rank in the query range is in $[k - \\alpha s, k + \\alpha s]$,\nwhere $k$ is a rank given by the query and $s$ is the size of the query range.\nWhen $\\alpha$ is a constant, we design an $O(n)$-bit encoding data structure\nthat can answer queries in constant time and prove this space cost is\nasymptotically optimal. The previous best result by Krizanc et al. (Range Mode\nand Range Median Queries on Lists and Trees, Nordic Journal of Computing, 2005)\nuses $O(n\\lg n)$ bits, or $O(n)$ words, to achieve constant approximation for\nrange median only. Thus we not only improve the space cost, but also provide\nsupport for any arbitrary $k$ given at query time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:17:44 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["El-Zein", "Hicham", ""], ["He", "Meng", ""], ["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""], ["Sandlund", "Bryce", ""]]}, {"id": "1907.08735", "submitter": "Jinglong Zhao", "authors": "Will Ma, David Simchi-Levi, Jinglong Zhao", "title": "The Competitive Ratio of Threshold Policies for Online Unit-density\n  Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online knapsack problem where the items arrive sequentially and\nmust be either immediately packed into the knapsack or irrevocably discarded.\nEach item has a different size and the objective is to maximize the total size\nof items packed. We focus on the class of randomized algorithms which initially\ndraw a threshold from some distribution, and then pack every fitting item whose\nsize is at least that threshold. Threshold policies satisfy many desiderata\nincluding simplicity, fairness, and incentive-alignment. We derive two optimal\nthreshold distributions, the first of which implies a competitive ratio of\n0.432 relative to the optimal offline packing, and the second of which implies\na competitive ratio of 0.428 relative to the optimal fractional packing. We\nalso consider the generalization to multiple knapsacks, where an arriving item\nhas a different size in each knapsack and must be placed in at most one. This\nis equivalent to the AdWords problem where item truncation is not allowed. We\nderive a randomized threshold algorithm for this problem which is\n0.214-competitive. We also show that any randomized algorithm for this problem\ncannot be more than 0.461-competitive, providing the first upper bound which is\nstrictly less than 0.5. This online knapsack problem finds applications in many\nareas, like supply chain ordering, online advertising, and healthcare\nscheduling, refugee integration, and crowdsourcing. We show how our optimal\nthreshold distributions can be naturally implemented in the warehouses for a\nLatin American chain department store. We run simulations on their large-scale\norder data, which demonstrate the robustness of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 01:35:45 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 04:00:44 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ma", "Will", ""], ["Simchi-Levi", "David", ""], ["Zhao", "Jinglong", ""]]}, {"id": "1907.08743", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Yanjun Han, Ziteng Sun, and\n  Himanshu Tyagi", "title": "Domain Compression and its Application to Randomness-Optimal Distributed\n  Goodness-of-Fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study goodness-of-fit of discrete distributions in the distributed\nsetting, where samples are divided between multiple users who can only release\na limited amount of information about their samples due to various information\nconstraints. Recently, a subset of the authors showed that having access to a\ncommon random seed (i.e., shared randomness) leads to a significant reduction\nin the sample complexity of this problem. In this work, we provide a complete\nunderstanding of the interplay between the amount of shared randomness\navailable, the stringency of information constraints, and the sample complexity\nof the testing problem by characterizing a tight trade-off between these three\nparameters. We provide a general distributed goodness-of-fit protocol that as a\nfunction of the amount of shared randomness interpolates smoothly between the\nprivate- and public-coin sample complexities. We complement our upper bound\nwith a general framework to prove lower bounds on the sample complexity of this\ntesting problems under limited shared randomness. Finally, we instantiate our\nbounds for the two archetypal information constraints of communication and\nlocal privacy, and show that our sample complexity bounds are optimal as a\nfunction of all the parameters of the problem, including the amount of shared\nrandomness.\n  A key component of our upper bounds is a new primitive of domain compression,\na tool that allows us to map distributions to a much smaller domain size while\npreserving their pairwise distances, using a limited amount of randomness.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 03:03:56 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Han", "Yanjun", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1907.08813", "submitter": "Firdevs Ulus", "authors": "Irfan Caner Kaya, Firdevs Ulus", "title": "An Iterative Vertex Enumeration Method for Objective Space Based Vector\n  Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An application area of vertex enumeration problem (VEP) is the usage within\nobjective space based linear/convex {vector} optimization algorithms whose aim\nis to generate (an approximation of) the Pareto frontier. In such algorithms,\nVEP, which is defined in the objective space, is solved in each iteration and\nit has a special structure. Namely, the recession cone of the polyhedron to be\ngenerated is the {ordering} cone. We {consider and give a detailed description\nof} a vertex enumeration procedure, which iterates by calling a modified\n`double description (DD) method' that works for such unbounded polyhedrons. We\nemploy this procedure as a function of an existing objective space based\n{vector} optimization algorithm (Algorithm 1); and test the performance of it\nfor randomly generated linear multiobjective optimization problems. We compare\nthe efficiency of this procedure with another existing DD method as well as\nwith the current vertex enumeration subroutine of Algorithm 1. We observe that\nthe modified procedure excels the others especially as the dimension of the\nvertex enumeration problem (the number of objectives of the corresponding\nmultiobjective problem) increases.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 13:49:12 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 14:43:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kaya", "Irfan Caner", ""], ["Ulus", "Firdevs", ""]]}, {"id": "1907.08817", "submitter": "Xiao-ke Zhu", "authors": "Xiaoke Zhu, Taining Cheng, Qi Zhang, Ling Liu, Jing He, Shaowen Yao\n  and Wei Zhou", "title": "NN-sort: Neural Network based Data Distribution-aware Sorting", "comments": "12 pages, Submitted to PODS 2020", "journal-ref": null, "doi": null, "report-no": "2792148", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is a fundamental operation in computing. However, the speed of\nstate-of-the-art sorting algorithms on a single thread has reached their\nlimits. Meanwhile, deep learning has demonstrated its potential to provide\nsignificant performance improvements in data mining and machine learning tasks.\nTherefore, it is interesting to explore whether sorting can also speed up by\ndeep learning techniques. In this paper, a neural network-based data\ndistribution aware sorting method named NN-sort is presented. Compared to\ntraditional comparison-based sorting algorithms, which need to compare the data\nelements in pairwise, NN-sort leverages the neural network model to learn the\ndata distribution and uses it to map disordered data elements into ordered\nones. Although the complexity of NN-sort is $nlogn$ in theory, it can run in\nnear-linear time as being observed in most of the cases. Experimental results\non both synthetic and real-world datasets show that NN-sort yields performance\nimprovement by up to 10.9x over traditional sorting algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 14:25:18 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 09:44:02 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 04:43:25 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhu", "Xiaoke", ""], ["Cheng", "Taining", ""], ["Zhang", "Qi", ""], ["Liu", "Ling", ""], ["He", "Jing", ""], ["Yao", "Shaowen", ""], ["Zhou", "Wei", ""]]}, {"id": "1907.08843", "submitter": "Alex Wang", "authors": "Alex L. Wang and Fatma Kilinc-Karzan", "title": "The Generalized Trust Region Subproblem: solution complexity and convex\n  hull results", "comments": null, "journal-ref": null, "doi": "10.1007/s10107-020-01560-8", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Generalized Trust Region Subproblem (GTRS) of minimizing a\nnonconvex quadratic objective over a nonconvex quadratic constraint. A lifting\nof this problem recasts the GTRS as minimizing a linear objective subject to\ntwo nonconvex quadratic constraints. Our first main contribution is structural:\nwe give an explicit description of the convex hull of this nonconvex set in\nterms of the generalized eigenvalues of an associated matrix pencil. This\nresult may be of interest in building relaxations for nonconvex quadratic\nprograms. Moreover, this result allows us to reformulate the GTRS as the\nminimization of two convex quadratic functions in the original space. Our next\nset of contributions is algorithmic: we present an algorithm for solving the\nGTRS up to an epsilon additive error based on this reformulation. We carefully\nhandle numerical issues that arise from inexact generalized eigenvalue and\neigenvector computations and establish explicit running time guarantees for\nthese algorithms. Notably, our algorithms run in linear (in the size of the\ninput) time. Furthermore, our algorithm for computing an epsilon-optimal\nsolution has a slightly-improved running time dependence on epsilon over the\nstate-of-the-art algorithm. Our analysis shows that the dominant cost in\nsolving the GTRS lies in solving a generalized eigenvalue problem --\nestablishing a natural connection between these problems. Finally,\ngeneralizations of our convex hull results allow us to apply our algorithms and\ntheir theoretical guarantees directly to equality-, interval-, and hollow-\nconstrained variants of the GTRS. This gives the first linear-time algorithm in\nthe literature for these variants of the GTRS.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 17:21:18 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 19:03:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Alex L.", ""], ["Kilinc-Karzan", "Fatma", ""]]}, {"id": "1907.08865", "submitter": "Marc Hellmuth", "authors": "Marc Hellmuth, Manuela Gei{\\ss} and Peter F. Stadler", "title": "Complexity of Modification Problems for Reciprocal Best Match Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocal best match graphs (RBMGs) are vertex colored graphs whose vertices\nrepresent genes and the colors the species where the genes reside. Edges\nidentify pairs of genes that are most closely related with respect to an\nunderlying evolutionary tree. In practical applications this tree is unknown\nand the edges of the RBMGs are inferred by quantifying sequence similarity. Due\nto noise in the data, these empirically determined graphs in general violate\nthe condition of being a ``biologically feasible'' RBMG. Therefore, it is of\npractical interest in computational biology to correct the initial estimate.\nHere we consider deletion (remove at most $k$ edges) and editing (add or delete\nat most $k$ edges) problems. We show that the decision version of the deletion\nand editing problem to obtain RBMGs from vertex colored graphs is NP-hard.\nUsing known results for the so-called bicluster editing, we show that the RBMG\nediting problem for $2$-colored graphs is fixed-parameter tractable.\n  A restricted class of RBMGs appears in the context of orthology detection.\nThese are cographs with a specific type of vertex coloring known as\nhierarchical coloring. We show that the decision problem of modifying a\nvertex-colored graph (either by edge-deletion or editing) into an RBMG with\ncograph structure or, equivalently, to an hierarchically colored cograph is\nNP-complete.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 20:22:08 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hellmuth", "Marc", ""], ["Gei\u00df", "Manuela", ""], ["Stadler", "Peter F.", ""]]}, {"id": "1907.08906", "submitter": "Shreyas Pai", "authors": "Sayan Bandyapadhyay, Tanmay Inamdar, Shreyas Pai, Kasturi Varadarajan", "title": "A Constant Approximation for Colorful k-Center", "comments": "14 pages, Published in ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the colorful $k$-center problem, which is a\ngeneralization of the well-known $k$-center problem. Here, we are given red and\nblue points in a metric space, and a coverage requirement for each color. The\ngoal is to find the smallest radius $\\rho$, such that with $k$ balls of radius\n$\\rho$, the desired number of points of each color can be covered. We obtain a\nconstant approximation for this problem in the Euclidean plane. We obtain this\nresult by combining a \"pseudo-approximation\" algorithm that works in any metric\nspace, and an approximation algorithm that works for a special class of\ninstances in the plane. The latter algorithm uses a novel connection to a\ncertain matching problem in graphs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 03:53:54 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Inamdar", "Tanmay", ""], ["Pai", "Shreyas", ""], ["Varadarajan", "Kasturi", ""]]}, {"id": "1907.08918", "submitter": "Yuhao Yao", "authors": "Minming Li, Pinyan Lu, Yuhao Yao, Jialin Zhang", "title": "Strategyproof Mechanism for Two Heterogeneous Facilities with Constant\n  Approximation Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the two-facility location game on a line with\noptional preference where the acceptable set of facilities for each agent could\nbe different and an agent's cost is his distance to the closest facility within\nhis acceptable set. The objective is to minimize the total cost of all agents\nwhile achieving strategyproofness. We design a deterministic strategyproof\nmechanism for the problem with approximation ratio of 2.75, improving upon the\nearlier best ratio of n/2+1.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 05:55:46 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Li", "Minming", ""], ["Lu", "Pinyan", ""], ["Yao", "Yuhao", ""], ["Zhang", "Jialin", ""]]}, {"id": "1907.09041", "submitter": "Jacob Turner", "authors": "Jacob Turner", "title": "Combining the Connection Scan Algorithm with Contraction Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first solutions finding minimally weighted routes in weighted\ndigraphs, a plethora of literature has appeared improving the performance of\nshortest-path queries for use in real-world applications. In this paper, we\ndetail how an advanced pre-processing technique for routing algorithms (which\ncreate objects known as Contraction Hierarchies) may be combined with the\nconnection scan algorithm, an algorithm originally designed to work with public\ntransportation networks using time tables. This provides an improvement over\nbi-directional Dijkstra or A${}^*$ search on Contraction Hierarchies.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 22:01:20 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Turner", "Jacob", ""]]}, {"id": "1907.09049", "submitter": "Jayakrishnan Nair", "authors": "Rahul Vaze and Jayakrishnan Nair", "title": "Multiple Server SRPT with speed scaling is competitive", "comments": "To appear in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can the popular shortest remaining processing time (SRPT) algorithm achieve a\nconstant competitive ratio on multiple servers when server speeds are\nadjustable (speed scaling) with respect to the flow time plus energy\nconsumption metric? This question has remained open for a while, where a\nnegative result in the absence of speed scaling is well known. The main result\nof this paper is to show that multi-server SRPT can be constant competitive,\nwith a competitive ratio that only depends on the power-usage function of the\nservers, but not on the number of jobs/servers or the job sizes (unlike when\nspeed scaling is not allowed). When all job sizes are unity, we show that\nround-robin routing is optimal and can achieve the same competitive ratio as\nthe best known algorithm for the single server problem. Finally, we show that a\nclass of greedy dispatch policies, including policies that route to the least\nloaded or the shortest queue, do not admit a constant competitive ratio. When\njob arrivals are stochastic, with Poisson arrivals and i.i.d. job sizes, we\nshow that random routing and a simple gated-static speed scaling algorithm\nachieves a constant competitive ratio.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:03:42 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 13:21:43 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Vaze", "Rahul", ""], ["Nair", "Jayakrishnan", ""]]}, {"id": "1907.09054", "submitter": "Samuel Gutekunst", "authors": "Samuel C. Gutekunst and David P. Williamson", "title": "Semidefinite Programming Relaxations of the Traveling Salesman Problem\n  and Their Integrality Gaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traveling salesman problem (TSP) is a fundamental problem in\ncombinatorial optimization. Several semidefinite programming relaxations have\nbeen proposed recently that exploit a variety of mathematical structures\nincluding, e.g., algebraic connectivity, permutation matrices, and association\nschemes. The main results of this paper are twofold. First, de Klerk and\nSotirov [9] present an SDP based on permutation matrices and symmetry\nreduction; they show that it is incomparable to the subtour elimination linear\nprogram, but generally dominates it on small instances. We provide a family of\n\\emph{simplicial TSP instances} that shows that the integrality gap of this SDP\nis unbounded. Second, we show that these simplicial TSP instances imply the\nunbounded integrality gap of every SDP relaxation of the TSP mentioned in the\nsurvey on SDP relaxations of the TSP in Section 2 of Sotirov [24]. In contrast,\nthe subtour LP performs perfectly on simplicial instances. The simplicial\ninstances thus form a natural litmus test for future SDP relaxations of the\nTSP.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:46:32 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Gutekunst", "Samuel C.", ""], ["Williamson", "David P.", ""]]}, {"id": "1907.09242", "submitter": "Maciej Drwal", "authors": "Maciej Drwal", "title": "Robust Approach to Restricted Items Selection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the robust version of items selection problem, in which the goal\nis to choose representatives from a family of sets, preserving constraints on\nthe allowed items' combinations. We prove NP-hardness of the deterministic\nversion, and establish polynomially solvable special cases. Next, we consider\nthe robust version in which we aim at minimizing the maximum regret of the\nsolution under interval parameter uncertainty. We show that this problem is\nhard for the second level of polynomial-time hierarchy. We develop an exact\nsolution algorithm for the robust problem, based on cut generation, and present\nthe results of computational experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:34:05 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Drwal", "Maciej", ""]]}, {"id": "1907.09271", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Roberto Grossi, Kunihiko Sadakane, Srinivasa\n  Rao Satti", "title": "Succinct Representation for (Non)Deterministic Finite Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic finite automata are one of the simplest and most practical\nmodels of computation studied in automata theory. Their conceptual extension is\nthe non-deterministic finite automata which also have plenty of applications.\nIn this article, we study these models through the lens of succinct data\nstructures where our ultimate goal is to encode these mathematical objects\nusing information-theoretically optimal number of bits along with supporting\nqueries on them efficiently. Towards this goal, we first design a succinct data\nstructure for representing any deterministic finite automaton $\\mathcal{D}$\nhaving $n$ states over a $\\sigma$-letter alphabet $\\Sigma$ using $(\\sigma-1)\nn\\log n + O(n \\log \\sigma)$ bits of space, which can determine, given an input\nstring $x$ over $\\Sigma$, whether $\\mathcal{D}$ accepts $x$ in $O(|x| \\log\n\\sigma)$ time, using constant words of working space. When the input\ndeterministic finite automaton is acyclic, not only we can improve the above\nspace-bound significantly to $(\\sigma -1) (n-1)\\log n+ 3n + O(\\log^2 \\sigma) +\no(n)$ bits, we also obtain optimal query time for string acceptance checking.\nMore specifically, using our succinct representation, we can check if a given\ninput string $x$ can be accepted by the acyclic deterministic finite automaton\nusing time proportional to the length of $x$, hence, the optimal query time. We\nalso exhibit a succinct data structure for representing a non-deterministic\nfinite automaton $\\mathcal{N}$ having $n$ states over a $\\sigma$-letter\nalphabet $\\Sigma$ using $\\sigma n^2+n$ bits of space, such that given an input\nstring $x$, we can decide whether $\\mathcal{N}$ accepts $x$ efficiently in\n$O(n^2|x|)$ time. Finally, we also provide time and space-efficient algorithms\nfor performing several standard operations such as union, intersection, and\ncomplement on the languages accepted by deterministic finite automata.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:29:03 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Grossi", "Roberto", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1907.09280", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Kunihiko Sadakane, Srinivasa Rao Satti", "title": "Optimal In-place Algorithms for Basic Graph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present linear time {\\it in-place} algorithms for several basic and\nfundamental graph problems including the well-known graph search methods (like\ndepth-first search, breadth-first search, maximum cardinality search),\nconnectivity problems (like biconnectivity, $2$-edge connectivity),\ndecomposition problem (like chain decomposition) among various others,\nimproving the running time (by polynomial multiplicative factor) of the recent\nresults of Chakraborty et al. [ESA, 2018] who designed $O(n^3 \\lg n)$ time\nin-place algorithms for a strict subset of the above mentioned problems. The\nrunning times of all our algorithms are essentially optimal as they run in\nlinear time. One of the main ideas behind obtaining these algorithms is the\ndetection and careful exploitation of sortedness present in the input\nrepresentation for any graph without loss of generality. This observation alone\nis powerful enough to design some basic linear time in-place algorithms, but\nmore non-trivial graph problems require extra techniques which, we believe, may\nfind other applications while designing in-place algorithms for different graph\nproblems in the future.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:41:59 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1907.09356", "submitter": "Anastasiia Koloskova", "authors": "Anastasia Koloskova, Tao Lin, Sebastian U. Stich, Martin Jaggi", "title": "Decentralized Deep Learning with Arbitrary Communication Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized training of deep learning models is a key element for enabling\ndata privacy and on-device learning over networks, as well as for efficient\nscaling to large compute clusters. As current approaches suffer from limited\nbandwidth of the network, we propose the use of communication compression in\nthe decentralized training context. We show that Choco-SGD $-$ recently\nintroduced and analyzed for strongly-convex objectives only $-$ converges under\narbitrary high compression ratio on general non-convex functions at the rate\n$O\\bigl(1/\\sqrt{nT}\\bigr)$ where $T$ denotes the number of iterations and $n$\nthe number of workers. The algorithm achieves linear speedup in the number of\nworkers and supports higher compression than previous state-of-the art methods.\nWe demonstrate the practical performance of the algorithm in two key scenarios:\nthe training of deep learning models (i) over distributed user devices,\nconnected by a social network and (ii) in a datacenter (outperforming\nall-reduce time-wise).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:53:02 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 11:06:08 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 17:24:35 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Koloskova", "Anastasia", ""], ["Lin", "Tao", ""], ["Stich", "Sebastian U.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1907.09415", "submitter": "Ronald de Wolf", "authors": "Ronald de Wolf (QuSoft, CWI and University of Amsterdam)", "title": "Quantum Computing: Lecture Notes", "comments": "184 pages. Version 2: added a new chapter about QMA and local\n  Hamiltonian, more exercises in several chapters, and some small\n  corrections/clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a set of lecture notes suitable for a Master's course on quantum\ncomputation and information from the perspective of theoretical computer\nscience. The first version was written in 2011, with many extensions and\nimprovements in subsequent years. The first 10 chapters cover the circuit model\nand the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup\nProblem, Grover, quantum walks, Hamiltonian simulation and HHL). They are\nfollowed by 3 chapters about complexity, 4 chapters about distributed (\"Alice\nand Bob\") settings, and a final chapter about quantum error correction.\nAppendices A and B give a brief introduction to the required linear algebra and\nsome other mathematical and computer science background. All chapters come with\nexercises, with some hints provided in Appendix C.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:56:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 14:51:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["de Wolf", "Ronald", "", "QuSoft, CWI and University of Amsterdam"]]}, {"id": "1907.09433", "submitter": "Oscar Defrain", "authors": "Oscar Defrain and Lhouari Nourine and Simon Vilmin", "title": "Translating between the representations of a ranked convex geometry", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that every closure system can be represented by an\nimplicational base, or by the set of its meet-irreducible elements. In Horn\nlogic, these are respectively known as the Horn expressions and the\ncharacteristic models. In this paper, we consider the problem of translating\nbetween the two representations in acyclic convex geometries. Quite\nsurprisingly, we show that the problem in this context is already harder than\nthe dualization in distributive lattices, a generalization of the well-known\nhypergraph dualization problem for which the existence of an output\nquasi-polynomial time algorithm is open. In light of this result, we consider a\nproper subclass of acyclic convex geometries, namely ranked convex geometries,\nas those that admit a ranked implicational base analogous to that of ranked\nposets. For this class, we provide output quasi-polynomial time algorithms\nbased on hypergraph dualization for translating between the two\nrepresentations. This improves the understanding of a long-standing open\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 17:05:47 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:28:12 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Defrain", "Oscar", ""], ["Nourine", "Lhouari", ""], ["Vilmin", "Simon", ""]]}, {"id": "1907.09582", "submitter": "Rik Sengupta", "authors": "Neil Immerman and Rik Sengupta", "title": "The $k$-Dimensional Weisfeiler-Leman Algorithm", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we provide details of the $k$-dimensional Weisfeiler-Leman\nAlgorithm and its analysis from Immerman-Lander (1990). In particular, we\npresent an optimized version of the algorithm that runs in time $O(n^{k+1}\\log\nn)$, where $k$ is fixed (not varying with $n$).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:16:29 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Immerman", "Neil", ""], ["Sengupta", "Rik", ""]]}, {"id": "1907.09668", "submitter": "Jing Tang", "authors": "Jing Tang, Keke Huang, Xiaokui Xiao, Laks V.S. Lakshmanan, Xueyan\n  Tang, Aixin Sun, and Andrew Lim", "title": "Efficient Approximation Algorithms for Adaptive Seed Minimization", "comments": "A short version of the paper appeared in 2019 International\n  Conference on Management of Data (SIGMOD '19), June 30--July 5, 2019,\n  Amsterdam, Netherlands. ACM, New York, NY, USA, 18 pages", "journal-ref": null, "doi": "10.1145/3299869.3319881", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a dual problem of influence maximization, the seed minimization problem\nasks for the minimum number of seed nodes to influence a required number $\\eta$\nof users in a given social network $G$. Existing algorithms for seed\nminimization mostly consider the non-adaptive setting, where all seed nodes are\nselected in one batch without observing how they may influence other users. In\nthis paper, we study seed minimization in the adaptive setting, where the seed\nnodes are selected in several batches, such that the choice of a batch may\nexploit information about the actual influence of the previous batches. We\npropose a novel algorithm, ASTI, which addresses the adaptive seed minimization\nproblem in $O\\Big(\\frac{\\eta \\cdot (m+n)}{\\varepsilon^2}\\ln n \\Big)$ expected\ntime and offers an approximation guarantee of $\\frac{(\\ln \\eta+1)^2}{(1 -\n(1-1/b)^b) (1-1/e)(1-\\varepsilon)}$ in expectation, where $\\eta$ is the\ntargeted number of influenced nodes, $b$ is size of each seed node batch, and\n$\\varepsilon \\in (0, 1)$ is a user-specified parameter. To the best of our\nknowledge, ASTI is the first algorithm that provides such an approximation\nguarantee without incurring prohibitive computation overhead. With extensive\nexperiments on a variety of datasets, we demonstrate the effectiveness and\nefficiency of ASTI over competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 03:03:11 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Tang", "Jing", ""], ["Huang", "Keke", ""], ["Xiao", "Xiaokui", ""], ["Lakshmanan", "Laks V. S.", ""], ["Tang", "Xueyan", ""], ["Sun", "Aixin", ""], ["Lim", "Andrew", ""]]}, {"id": "1907.09834", "submitter": "Bj\\\"orn Feldkord", "authors": "Bj\\\"orn Feldkord and Till Knollmann and Manuel Malatyali and Friedhelm\n  Meyer auf der Heide", "title": "Managing Multiple Mobile Resources", "comments": "To be presented at WAOA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Mobile Server Problem, introduced in SPAA'17, to a model where\nk identical mobile resources, here named servers, answer requests appearing at\npoints in the Euclidean space. In order to reduce communication costs, the\npositions of the servers can be adapted by a limited distance m_s per round for\neach server. The costs are measured similar to the classical Page Migration\nProblem, i.e., answering a request induces costs proportional to the distance\nto the nearest server, and moving a server induces costs proportional to the\ndistance multiplied with a weight D.\n  We show that, in our model, no online algorithm can have a constant\ncompetitive ratio, i.e., one which is independent of the input length n, even\nif an augmented moving distance of (1+\\delta)m_s is allowed for the online\nalgorithm. Therefore we investigate a restriction of the power of the adversary\ndictating the sequence of requests: We demand locality of requests, i.e., that\nconsecutive requests come from points in the Euclidean space with distance\nbounded by some constant m_c. We show constant lower bounds on the\ncompetitiveness in this setting (independent of n, but dependent on k, m_s and\nm_c).\n  On the positive side, we present a deterministic online algorithm with\nbounded competitiveness when augmented moving distance and locality of requests\nis assumed. Our algorithm simulates any given algorithm for the classical\nk-Page Migration problem as guidance for its servers and extends it by a greedy\nmove of one server in every round. The resulting competitive ratio is\npolynomial in the number of servers k, the ratio between m_c and m_s, the\ninverse of the augmentation factor 1/\\delta and the competitive ratio of the\nsimulated k-Page Migration algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 12:07:33 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Feldkord", "Bj\u00f6rn", ""], ["Knollmann", "Till", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1907.10121", "submitter": "Tyler Reddy", "authors": "Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland,\n  Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren\n  Weckesser, Jonathan Bright, St\\'efan J. van der Walt, Matthew Brett, Joshua\n  Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones,\n  Robert Kern, Eric Larson, CJ Carey, \\.Ilhan Polat, Yu Feng, Eric W. Moore,\n  Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian\n  Henriksen, E.A. Quintero, Charles R Harris, Anne M. Archibald, Ant\\^onio H.\n  Ribeiro, Fabian Pedregosa, Paul van Mulbregt, SciPy 1.0 Contributors", "title": "SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python", "comments": "Article source data is available here:\n  https://github.com/scipy/scipy-articles", "journal-ref": "Nature Methods 17, 261 (2020)", "doi": "10.1038/s41592-019-0686-2", "report-no": null, "categories": "cs.MS cs.DS cs.SE physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SciPy is an open source scientific computing library for the Python\nprogramming language. SciPy 1.0 was released in late 2017, about 16 years after\nthe original version 0.1 release. SciPy has become a de facto standard for\nleveraging scientific algorithms in the Python programming language, with more\nthan 600 unique code contributors, thousands of dependent packages, over\n100,000 dependent repositories, and millions of downloads per year. This\nincludes usage of SciPy in almost half of all machine learning projects on\nGitHub, and usage by high profile projects including LIGO gravitational wave\nanalysis and creation of the first-ever image of a black hole (M87). The\nlibrary includes functionality spanning clustering, Fourier transforms,\nintegration, interpolation, file I/O, linear algebra, image processing,\northogonal distance regression, minimization algorithms, signal processing,\nsparse matrix handling, computational geometry, and statistics. In this work,\nwe provide an overview of the capabilities and development practices of the\nSciPy library and highlight some recent technical developments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:31:36 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Virtanen", "Pauli", ""], ["Gommers", "Ralf", ""], ["Oliphant", "Travis E.", ""], ["Haberland", "Matt", ""], ["Reddy", "Tyler", ""], ["Cournapeau", "David", ""], ["Burovski", "Evgeni", ""], ["Peterson", "Pearu", ""], ["Weckesser", "Warren", ""], ["Bright", "Jonathan", ""], ["van der Walt", "St\u00e9fan J.", ""], ["Brett", "Matthew", ""], ["Wilson", "Joshua", ""], ["Millman", "K. Jarrod", ""], ["Mayorov", "Nikolay", ""], ["Nelson", "Andrew R. J.", ""], ["Jones", "Eric", ""], ["Kern", "Robert", ""], ["Larson", "Eric", ""], ["Carey", "CJ", ""], ["Polat", "\u0130lhan", ""], ["Feng", "Yu", ""], ["Moore", "Eric W.", ""], ["VanderPlas", "Jake", ""], ["Laxalde", "Denis", ""], ["Perktold", "Josef", ""], ["Cimrman", "Robert", ""], ["Henriksen", "Ian", ""], ["Quintero", "E. A.", ""], ["Harris", "Charles R", ""], ["Archibald", "Anne M.", ""], ["Ribeiro", "Ant\u00f4nio H.", ""], ["Pedregosa", "Fabian", ""], ["van Mulbregt", "Paul", ""], ["Contributors", "SciPy 1. 0", ""]]}, {"id": "1907.10230", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "An FPT algorithm for orthogonal buttons and scissors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the puzzle game Buttons and Scissors in which the goal is to remove\nall buttons from an $n\\times m$ grid by a series of horizontal and vertical\ncuts. We show that the corresponding parameterized problem has an algorithm\nwith time complexity $2^{O(k^2 \\log k)} (n+m)^{O(1)}$, where $k$ is an upper\nbound on the number of cuts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 04:21:12 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1907.10308", "submitter": "John Augustine", "authors": "John Augustine, Valerie King, Anisur R. Molla, Gopal Pandurangan, and\n  Jared Saia", "title": "Scalable and Secure Computation Among Strangers: Resource-Competitive\n  Byzantine Protocols", "comments": "24 pages, one figure. The author list has been corrected in the\n  metadata. There are no other changes from version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated, in part, by the rise of permissionless systems such as Bitcoin\nwhere arbitrary nodes (whose identities are not known apriori) can join and\nleave at will, we extend established research in scalable Byzantine agreement\nto a more practical model where each node (initially) does not know the\nidentity of other nodes. A node can send to new destinations only by sending to\nrandom (or arbitrary) nodes, or responding (if it chooses) to messages received\nfrom those destinations. We assume a synchronous and fully-connected network,\nwith a full-information, but static Byzantine adversary. A general drawback of\nexisting Byzantine protocols is that the communication cost incurred by the\nhonest nodes may not be proportional to those incurred by the Byzantine nodes;\nin fact, they can be significantly higher. Our goal is to design Byzantine\nprotocols for fundamental problems which are {\\em resource competitive}, i.e.,\nthe number of bits sent by honest nodes is not much more than those sent by\nByzantine nodes.\n  We describe a randomized scalable algorithm to solve Byzantine agreement,\nleader election, and committee election in this model. Our algorithm sends an\nexpected $O((T+n)\\log n)$ bits and has latency $O(polylog(n))$, where $n$ is\nthe number of nodes, and $T$ is the minimum of $n^2$ and the number of bits\nsent by adversarially controlled nodes. The algorithm is resilient to\n$(1/4-\\epsilon)n$ Byzantine nodes for any fixed $\\epsilon > 0$, and succeeds\nwith high probability. Our work can be considered as a first application of\nresource-competitive analysis to fundamental Byzantine problems.\n  To complement our algorithm we also show lower bounds for\nresource-competitive Byzantine agreement. We prove that, in general, one cannot\nhope to design Byzantine protocols that have communication cost that is\nsignificantly smaller than the cost of the Byzantine adversary.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:59:44 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 05:11:00 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Augustine", "John", ""], ["King", "Valerie", ""], ["Molla", "Anisur R.", ""], ["Pandurangan", "Gopal", ""], ["Saia", "Jared", ""]]}, {"id": "1907.10363", "submitter": "Stefka Bouyuklieva", "authors": "Iliya Bouyukliev, Stefka Bouyuklieva", "title": "Classification of linear codes using canonical augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for classification of linear codes over different\nfinite fields based on canonical augmentation. We apply this algorithm to\nobtain classification results over fields with 2, 3 and 4 elements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 11:02:23 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Bouyukliev", "Iliya", ""], ["Bouyuklieva", "Stefka", ""]]}, {"id": "1907.10376", "submitter": "Rico Zenklusen", "authors": "Vera Traub and Jens Vygen and Rico Zenklusen", "title": "Reducing Path TSP to TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a black-box reduction from the path version of the Traveling\nSalesman Problem (Path TSP) to the classical tour version (TSP). More\nprecisely, we show that given an $\\alpha$-approximation algorithm for TSP,\nthen, for any $\\epsilon >0$, there is an $(\\alpha+\\epsilon)$-approximation\nalgorithm for the more general Path TSP. This reduction implies that the\napproximability of Path TSP is the same as for TSP, up to an arbitrarily small\nerror. This avoids future discrepancies between the best known approximation\nfactors achievable for these two problems, as they have existed until very\nrecently.\n  A well-studied special case of TSP, Graph TSP, asks for tours in unit-weight\ngraphs. Our reduction shows that any $\\alpha$-approximation algorithm for Graph\nTSP implies an $(\\alpha+\\epsilon)$-approximation algorithm for its path\nversion. By applying our reduction to the $1.4$-approximation algorithm for\nGraph TSP by Seb\\H{o} and Vygen, we obtain a polynomial-time\n$(1.4+\\epsilon)$-approximation algorithm for Graph Path TSP, improving on a\nrecent $1.497$-approximation algorithm of Traub and Vygen.\n  We obtain our results through a variety of new techniques, including a novel\nway to set up a recursive dynamic program to guess significant parts of an\noptimal solution. At the core of our dynamic program we deal with instances of\na new generalization of (Path) TSP which combines parity constraints with\ncertain connectivity requirements. This problem, which we call $\\Phi$-TSP, has\na constant-factor approximation algorithm and can be reduced to TSP in certain\ncases when the dynamic program would not make sufficient progress.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:03:10 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Traub", "Vera", ""], ["Vygen", "Jens", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1907.10398", "submitter": "J\\'er\\'emie Chalopin", "authors": "Laurine B\\'en\\'eteau, J\\'er\\'emie Chalopin, Victor Chepoi, Yann\n  Vax\\`es", "title": "Medians in median graphs and their cube complexes in linear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The median of a set of vertices $P$ of a graph $G$ is the set of all vertices\n$x$ of $G$ minimizing the sum of distances from $x$ to all vertices of $P$. In\nthis paper, we present a linear time algorithm to compute medians in median\ngraphs, improving over the existing quadratic time algorithm. We also present a\nlinear time algorithm to compute medians in the $\\ell_1$-cube complexes\nassociated with median graphs. Median graphs constitute the principal class of\ngraphs investigated in metric graph theory and have a rich geometric and\ncombinatorial structure, due to their bijections with CAT(0) cube complexes and\ndomains of event structures. Our algorithm is based on the majority rule\ncharacterization of medians in median graphs and on a fast computation of\nparallelism classes of edges ($\\Theta$-classes or hyperplanes) via\nLexicographic Breadth First Search (LexBFS). To prove the correctness of our\nalgorithm, we show that any LexBFS ordering of the vertices of $G$ satisfies\nthe following fellow traveler property of independent interest: the parents of\nany two adjacent vertices of $G$ are also adjacent. Using the fast computation\nof the $\\Theta$-classes, we also compute the Wiener index (total distance) of\n$G$ in linear time and the distance matrix in optimal quadratic time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:42:03 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:14:14 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["B\u00e9n\u00e9teau", "Laurine", ""], ["Chalopin", "J\u00e9r\u00e9mie", ""], ["Chepoi", "Victor", ""], ["Vax\u00e8s", "Yann", ""]]}, {"id": "1907.10444", "submitter": "Fabian Peternek", "authors": "Sebastian Maneth, Fabian Peternek", "title": "Constant Delay Traversal of Grammar-Compressed Graphs with Bounded Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pointer-based data structure for constant time traversal of the\nedges of an edge-labeled (alphabet $\\Sigma$) directed hypergraph (a graph where\nedges can be incident to more than two vertices, and the incident vertices are\nordered) given as hyperedge-replacement grammar $G$. It is assumed that the\ngrammar has a fixed rank $\\kappa$ (maximal number of vertices connected to a\nnonterminal hyperedge) and that each vertex of the represented graph is\nincident to at most one $\\sigma$-edge per direction ($\\sigma \\in \\Sigma$).\nPrecomputing the data structure needs $O(|G||\\Sigma|\\kappa r h)$ space and\n$O(|G||\\Sigma|\\kappa rh^2)$ time, where $h$ is the height of the derivation\ntree of $G$ and $r$ is the maximal rank of a terminal edge occurring in the\ngrammar.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:39:59 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Maneth", "Sebastian", ""], ["Peternek", "Fabian", ""]]}, {"id": "1907.10499", "submitter": "Yannic Maus", "authors": "Yannic Maus", "title": "P-SLOCAL-Completeness of Maximum Independent Set Approximation", "comments": "added author's name. Please note the breakthrough: arXiv:1907.10937", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the maximum independent set approximation problem with\npolylogarithmic approximation factor is P-SLOCAL-complete. Thus an efficient\nalgorithm for the maximum independent set approximation in the LOCAL model\nimplies efficient algorithms for many problems in the LOCAL model including the\ncomputation of (polylog n, polylog n) network decompositions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:08:07 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 21:53:56 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 09:14:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Maus", "Yannic", ""]]}, {"id": "1907.10531", "submitter": "Abhishek Shetty", "authors": "Navin Goyal and Abhishek Shetty", "title": "Sampling and Optimization on Convex Sets in Riemannian Manifolds of\n  Non-Negative Curvature", "comments": "Appeared at COLT 2019", "journal-ref": null, "doi": null, "report-no": "PMLR 99:1519-1561, 2019", "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euclidean space notion of convex sets (and functions) generalizes to\nRiemannian manifolds in a natural sense and is called geodesic convexity.\nExtensively studied computational problems such as convex optimization and\nsampling in convex sets also have meaningful counterparts in the manifold\nsetting. Geodesically convex optimization is a well-studied problem with\nongoing research and considerable recent interest in machine learning and\ntheoretical computer science. In this paper, we study sampling and convex\noptimization problems over manifolds of non-negative curvature proving\npolynomial running time in the dimension and other relevant parameters. Our\nalgorithms assume a warm start. We first present a random walk based sampling\nalgorithm and then combine it with simulated annealing for solving convex\noptimization problems. To our knowledge, these are the first algorithms in the\ngeneral setting of positively curved manifolds with provable polynomial\nguarantees under reasonable assumptions, and the first study of the connection\nbetween sampling and optimization in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:49:54 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Goyal", "Navin", ""], ["Shetty", "Abhishek", ""]]}, {"id": "1907.10779", "submitter": "Yang P. Liu", "authors": "Shiri Chechik, Yang P. Liu, Omer Rotem, Aaron Sidford", "title": "Constant Girth Approximation for Directed Graphs in Subquadratic Time", "comments": "32 pages, accepted to STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a $\\tilde{O}(m\\sqrt{n})$ time algorithm that\ncomputes a $3$-multiplicative approximation of the girth of a $n$-node $m$-edge\ndirected graph with non-negative edge lengths. This is the first algorithm\nwhich approximates the girth of a directed graph up to a constant\nmultiplicative factor faster than All-Pairs Shortest Paths (APSP) time, i.e.\n$O(mn)$. Additionally, for any integer $k \\ge 1$, we provide a deterministic\nalgorithm for a $O(k\\log\\log n)$-multiplicative approximation to the girth in\ndirected graphs in $\\tilde{O}(m^{1+1/k})$ time. Combining the techniques from\nthese two results gives us an algorithm for a $O(k\\log k)$-multiplicative\napproximation to the girth in directed graphs in $\\tilde{O}(m^{1+1/k})$ time.\nOur results naturally also provide algorithms for improved constructions of\nroundtrip spanners, the analog of spanners in directed graphs.\n  The previous fastest algorithms for these problems either ran in All-Pairs\nShortest Paths (APSP) time, i.e. $O(mn)$, or were due Pachocki et al. (PRSTV18)\nwhich provided a randomized algorithm that for any integer $k \\ge 1$ in time\n$\\tilde{O}(m^{1+1/k})$ computed with high probability a $O(k\\log n)$\nmultiplicative approximation of the girth. Our first algorithm constitutes the\nfirst sub-APSP-time algorithm for approximating the girth to constant accuracy,\nour second removes the need for randomness and improves the approximation\nfactor in Pachocki et al. (PRSTV18), and our third is the first time versus\nquality trade-off for obtaining constant approximations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 00:54:31 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 04:23:12 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Chechik", "Shiri", ""], ["Liu", "Yang P.", ""], ["Rotem", "Omer", ""], ["Sidford", "Aaron", ""]]}, {"id": "1907.10874", "submitter": "Omri Weinstein", "authors": "Emanuele Viola, Omri Weinstein, Huacheng Yu", "title": "How to Store a Random Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by storage applications, we study the following data structure\nproblem: An encoder wishes to store a collection of jointly-distributed files\n$\\overline{X}:=(X_1,X_2,\\ldots, X_n) \\sim \\mu$ which are \\emph{correlated}\n($H_\\mu(\\overline{X}) \\ll \\sum_i H_\\mu(X_i)$), using as little (expected)\nmemory as possible, such that each individual file $X_i$ can be recovered\nquickly with few (ideally constant) memory accesses.\n  In the case of independent random files, a dramatic result by \\Pat (FOCS'08)\nand subsequently by Dodis, \\Pat and Thorup (STOC'10) shows that it is possible\nto store $\\overline{X}$ using just a \\emph{constant} number of extra bits\nbeyond the information-theoretic minimum space, while at the same time decoding\neach $X_i$ in constant time. However, in the (realistic) case where the files\nare correlated, much weaker results are known, requiring at least\n$\\Omega(n/poly\\lg n)$ extra bits for constant decoding time, even for \"simple\"\njoint distributions $\\mu$.\n  We focus on the natural case of compressing\\emph{Markov chains}, i.e.,\nstoring a length-$n$ random walk on any (possibly directed) graph $G$. Denoting\nby $\\kappa(G,n)$ the number of length-$n$ walks on $G$, we show that there is a\nsuccinct data structure storing a random walk using $\\lg_2 \\kappa(G,n) + O(\\lg\nn)$ bits of space, such that any vertex along the walk can be decoded in $O(1)$\ntime on a word-RAM. For the harder task of matching the \\emph{point-wise}\noptimal space of the walk, i.e., the empirical entropy $\\sum_{i=1}^{n-1} \\lg\n(deg(v_i))$, we present a data structure with $O(1)$ extra bits at the price of\n$O(\\lg n)$ decoding time, and show that any improvement on this would lead to\nan improved solution on the long-standing Dictionary problem. All of our data\nstructures support the \\emph{online} version of the problem with constant\nupdate and query time.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:37:42 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Viola", "Emanuele", ""], ["Weinstein", "Omri", ""], ["Yu", "Huacheng", ""]]}, {"id": "1907.10895", "submitter": "Shaked Matar", "authors": "Michael Elkin and Shaked Matar", "title": "Fast Deterministic Constructions of Linear-Size Spanners and Skeletons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distributed setting, the only existing constructions of \\textit{sparse\nskeletons}, (i.e., subgraphs with $O(n)$ edges) either use randomization or\nlarge messages, or require $\\Omega(D)$ time, where $D$ is the hop-diameter of\nthe input graph $G$. We devise the first deterministic distributed algorithm in\nthe CONGEST model (i.e., uses small messages) for constructing linear-size\nskeletons in time $2^{O(\\sqrt{{\\log n}\\cdot{\\log{\\log n}}})}$. We can also\ncompute a linear-size spanner with stretch $polylog(n)$ in low deterministic\npolynomial time, i.e., $O(n^\\rho)$ for an arbitrarily small constant $\\rho >0$,\nin the CONGEST model. Yet another algorithm that we devise runs in $O({\\log\nn})^{\\kappa-1}$ time, for a parameter $\\kappa=1,2,\\dots,$ and constructs an\n$O({\\log n})^{\\kappa-1}$ spanner with $O(n^{1+1/\\kappa})$ edges. All our\ndistributed algorithms are lightweight from the computational perspective,\ni.e., none of them employs any heavy computations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:21:54 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Elkin", "Michael", ""], ["Matar", "Shaked", ""]]}, {"id": "1907.10930", "submitter": "Hedayat Alghassi", "authors": "Hedayat Alghassi, Raouf Dridi, Sridhar Tayur", "title": "GAMA: A Novel Algorithm for Non-Convex Integer Programs", "comments": "Keywords: Graver basis, test sets, non-linear non-convex integer\n  programming, contingency tables, (0,1)- matrices, computational testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the decomposition in the hybrid quantum-classical optimization\nalgorithm we introduced in arXiv:1902.04215, we propose here a new (fully\nclassical) approach to solving certain non-convex integer programs using Graver\nbases. This method is well suited when (a) the constraint matrix $A$ has a\nspecial structure so that its Graver basis can be computed systematically, (b)\nseveral feasible solutions can also be constructed easily and (c) the objective\nfunction can be viewed as many convex functions quilted together. Classes of\nproblems that satisfy these conditions include Cardinality Boolean Quadratic\nProblems (CBQP), Quadratic Semi-Assignment Problems (QSAP) and Quadratic\nAssignment Problems (QAP). Our Graver Augmented Multi-seed Algorithm (GAMA)\nutilizes augmentation along Graver basis elements (the improvement direction is\nobtained by comparing objective function values) from these multiple initial\nfeasible solutions. We compare our approach with a best-in-class commercially\navailable solver (Gurobi). Sensitivity analysis indicates that the rate at\nwhich GAMA slows down as the problem size increases is much lower than that of\nGurobi. We find that for several instances of practical relevance, GAMA not\nonly vastly outperforms in terms of time to find the optimal solution (by two\nor three orders of magnitude), but also finds optimal solutions within minutes\nwhen the commercial solver is not able to do so in 4 or 10 hours (depending on\nthe problem class) in several cases.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:43:59 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Alghassi", "Hedayat", ""], ["Dridi", "Raouf", ""], ["Tayur", "Sridhar", ""]]}, {"id": "1907.10937", "submitter": "Mohsen Ghaffari", "authors": "V\\'aclav Rozho\\v{n} and Mohsen Ghaffari", "title": "Polylogarithmic-Time Deterministic Network Decomposition and Distributed\n  Derandomization", "comments": "Extended version of an article that appears at the Symposium on\n  Theory of Computing (STOC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple polylogarithmic-time deterministic distributed algorithm\nfor network decomposition. This improves on a celebrated $2^{O(\\sqrt{\\log\nn})}$-time algorithm of Panconesi and Srinivasan [STOC'92] and settles a\ncentral and long-standing question in distributed graph algorithms. It also\nleads to the first polylogarithmic-time deterministic distributed algorithms\nfor numerous other problems, hence resolving several well-known and decades-old\nopen problems, including Linial's question about the deterministic complexity\nof maximal independent set [FOCS'87; SICOMP'92]---which had been called the\nmost outstanding problem in the area.\n  The main implication is a more general distributed derandomization theorem:\nPut together with the results of Ghaffari, Kuhn, and Maus [STOC'17] and\nGhaffari, Harris, and Kuhn [FOCS'18], our network decomposition implies that\n$$\\mathsf{P}\\textit{-}\\mathsf{RLOCAL} = \\mathsf{P}\\textit{-}\\mathsf{LOCAL}.$$\nThat is, for any problem whose solution can be checked deterministically in\npolylogarithmic-time, any polylogarithmic-time randomized algorithm can be\nderandomized to a polylogarithmic-time deterministic algorithm. Informally, for\nthe standard first-order interpretation of efficiency as polylogarithmic-time,\ndistributed algorithms do not need randomness for efficiency.\n  By known connections, our result leads also to substantially faster\nrandomized distributed algorithms for a number of well-studied problems\nincluding $(\\Delta+1)$-coloring, maximal independent set, and Lov\\'{a}sz Local\nLemma, as well as massively parallel algorithms for $(\\Delta+1)$-coloring.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:01:49 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 18:24:18 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Rozho\u0148", "V\u00e1clav", ""], ["Ghaffari", "Mohsen", ""]]}, {"id": "1907.10984", "submitter": "Sankardeep Chakraborty", "authors": "Kentaro Sumigawa, Sankardeep Chakraborty, Kunihiko Sadakane, Srinivasa\n  Rao Satti", "title": "Enumerating Range Modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the range mode problem where given a sequence and a query range\nin it, we want to find items with maximum frequency in the range. We give time-\nand space- efficient algorithms for this problem. Our algorithms are efficient\nfor small maximum frequency cases. We also consider a natural generalization of\nthe problem: the range mode enumeration problem, for which there has been no\nknown efficient algorithms. Our algorithms have query time complexities which\nis linear to the output size plus small terms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:48:57 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Sumigawa", "Kentaro", ""], ["Chakraborty", "Sankardeep", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1907.11078", "submitter": "Karol W\\k{e}grzycki", "authors": "Karl Bringmann, Marvin K\\\"unnemann, Karol W\\k{e}grzycki", "title": "Approximating APSP without Scaling: Equivalence of Approximate Min-Plus\n  and Exact Min-Max", "comments": "Presented at STOC'19. Full Version. 35 pages", "journal-ref": null, "doi": "10.1145/3313276.3316373", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zwick's $(1+\\varepsilon)$-approximation algorithm for the All Pairs Shortest\nPath (APSP) problem runs in time $\\widetilde{O}(\\frac{n^\\omega}{\\varepsilon}\n\\log{W})$, where $\\omega \\le 2.373$ is the exponent of matrix multiplication\nand $W$ denotes the largest weight. This can be used to approximate several\ngraph characteristics including the diameter, radius, median, minimum-weight\ntriangle, and minimum-weight cycle in the same time bound.\n  Since Zwick's algorithm uses the scaling technique, it has a factor $\\log W$\nin the running time. In this paper, we study whether APSP and related problems\nadmit approximation schemes avoiding the scaling technique. That is, the number\nof arithmetic operations should be independent of $W$; this is called strongly\npolynomial. Our main results are as follows.\n  - We design approximation schemes in strongly polynomial time\n$O(\\frac{n^\\omega}{\\varepsilon} \\text{polylog}(\\frac{n}{\\varepsilon}))$ for\nAPSP on undirected graphs as well as for the graph characteristics diameter,\nradius, median, minimum-weight triangle, and minimum-weight cycle on directed\nor undirected graphs.\n  - For APSP on directed graphs we design an approximation scheme in strongly\npolynomial time $O(n^{\\frac{\\omega + 3}{2}} \\varepsilon^{-1}\n\\text{polylog}(\\frac{n}{\\varepsilon}))$. This is significantly faster than the\nbest exact algorithm.\n  - We explain why our approximation scheme for APSP on directed graphs has a\nworse exponent than $\\omega$: Any improvement over our exponent $\\frac{\\omega +\n3}{2}$ would improve the best known algorithm for Min-Max Product In fact, we\nprove that approximating directed APSP and exactly computing the Min-Max\nProduct are equivalent.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:14:06 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "1907.11206", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz and Ely Porat", "title": "The Strong 3SUM-INDEXING Conjecture is False", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 3SUM-Indexing problem the goal is to preprocess two lists of elements\nfrom $U$, $A=(a_1,a_2,\\ldots,a_n)$ and $B=(b_1,b_2,...,b_n)$, such that given\nan element $c\\in U$ one can quickly determine whether there exists a pair\n$(a,b)\\in A \\times B$ where $a+b=c$. Goldstein et al.~[WADS'2017] conjectured\nthat there is no algorithm for 3SUM-Indexing which uses $n^{2-\\Omega(1)}$ space\nand $n^{1-\\Omega(1)}$ query time.\n  We show that the conjecture is false by reducing the 3SUM-Indexing problem to\nthe problem of inverting functions, and then applying an algorithm of Fiat and\nNaor [SICOMP'1999] for inverting functions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 17:11:11 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""]]}, {"id": "1907.11209", "submitter": "Mohit Singh", "authors": "Mohit Singh", "title": "Integrality Gap of the Vertex Cover Linear Programming Relaxation", "comments": "6 pages", "journal-ref": "Operations Research Letters 47 (4), 288-290, 2019", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a characterization result for the integrality gap of the natural\nlinear programming relaxation for the vertex cover problem. We show that\nintegrality gap of the standard linear programming relaxation for any graph G\nequals $\\left(2-\\frac{2}{\\chi^f(G)}\\right)$ where $\\chi^f(G)$ denotes the\nfractional chromatic number of G.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 17:19:43 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Singh", "Mohit", ""]]}, {"id": "1907.11232", "submitter": "Konstantinos Xylogiannopoulos", "authors": "Konstantinos F. Xylogiannopoulos", "title": "Exhaustive Exact String Matching: The Analysis of the Full Human Genome", "comments": "Paper accepted for publication at IEEE/ACM ASONAM 2019 conference,\n  Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3341161.3343517", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact string matching has been a fundamental problem in computer science for\ndecades because of many practical applications. Some are related to common\nprocedures, such as searching in files and text editors, or, more recently, to\nmore advanced problems such as pattern detection in Artificial Intelligence and\nBioinformatics. Tens of algorithms and methodologies have been developed for\npattern matching and several programming languages, packages, applications and\nonline systems exist that can perform exact string matching in biological\nsequences. These techniques, however, are limited to searching for specific and\npredefined strings in a sequence. In this paper a novel methodology (called\nEx2SM) is presented, which is a pipeline of execution of advanced data\nstructures and algorithms, explicitly designed for text mining, that can detect\nevery possible repeated string in multivariate biological sequences. In\ncontrast to known algorithms in literature, the methodology presented here is\nstring agnostic, i.e., it does not require an input string to search for it,\nrather it can detect every string that exists at least twice, regardless of its\nattributes such as length, frequency, alphabet, overlapping etc. The complexity\nof the problem solved and the potential of the proposed methodology is\ndemonstrated with the experimental analysis performed on the entire human\ngenome. More specifically, all repeated strings with a length of up to 50\ncharacters have been detected, an achievement which is practically impossible\nusing other algorithms due to the exponential number of possible permutations\nof such long strings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:50:16 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Xylogiannopoulos", "Konstantinos F.", ""]]}, {"id": "1907.11402", "submitter": "Parter Merav", "authors": "Uri Ben-Levy and Merav Parter", "title": "New $(\\alpha,\\beta)$ Spanners and Hopsets", "comments": "Appeared in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $f(d)$-spanner of an unweighted $n$-vertex graph $G=(V,E)$ is a subgraph\n$H$ satisfying that $dist_H(u, v)$ is at most $f(dist_G(u, v))$ for every $u,v\n\\in V$. We present new spanner constructions that achieve a nearly optimal\nstretch of $O(\\lceil k /d \\rceil)$ for any distance value $d \\in\n[1,k^{1-o(1)}]$, and $d \\geq k^{1+o(1)}$. We show the following:\n  1. There exists an $f(d)$-spanner $H \\subseteq G$ with $f(d)\\leq 7k$ for any\n$d \\in [1,\\sqrt{k}/2]$ with expected size $O_{k}(n^{1+1/k})$. This in\nparticular gives $(\\alpha,\\beta)$ spanners with $\\alpha=O(\\sqrt{k})$ and\n$\\beta=O(k)$.\n  2. For any $\\epsilon \\in (0,1/2]$, there exists an $(\\alpha,\\beta)$-spanner\nwith $\\alpha=O(k^{\\epsilon})$, $\\beta=O_{\\epsilon}(k)$ and of expected size\n$O_{k}(n^{1+1/k})$. This implies a stretch of $O(\\lceil k/d \\rceil)$ for any $d\n\\in [\\sqrt{k}/2, k^{1-\\epsilon}]$, and for every $d\\geq k^{1+\\epsilon}$. In\nparticular, it provides a constant stretch already for vertex pairs at distance\n$k^{1+o(1)}$ (improving upon $d=(\\log k)^{\\log k}$ that was known before). Up\nto the $o(1)$ factor in the exponent, and the constant factor in the stretch,\nthis is the best possible by the girth argument.\n  3. For any $\\epsilon \\in (0,1)$ and integer $k\\geq 1$, there is a\n$(3+\\epsilon, \\beta)$-spanner with $\\beta=O_{\\epsilon}(k^{\\log(3+8/\\epsilon)})$\nand $O_{k,\\epsilon}(n^{1+1/k})$ edges.\n  We also consider the related graph concept of hopsets introduced by [Cohen,\nJ. ACM '00]. We present a new family of $(\\alpha,\\beta)$ hopsets with\n$\\widetilde{O}(k \\cdot n^{1+1/k})$ edges and $\\alpha \\cdot \\beta=O(k)$. Most\nnotably, we show a construction of $(3+\\epsilon,\\beta)$ hopset with\n$\\widetilde{O}_{k,\\epsilon}(n^{1+1/k})$ edges and hop-bound of\n$\\beta=O_{\\epsilon}(k^{\\log(3+9/\\epsilon)})$, improving upon the\nstate-of-the-art hop-bound of $\\beta=O(\\log k /\\epsilon)^{\\log k}$ by\n[Elkin-Neiman, '17] and [Huang-Pettie, '17].\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 07:07:52 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 20:32:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ben-Levy", "Uri", ""], ["Parter", "Merav", ""]]}, {"id": "1907.11404", "submitter": "Shi Li", "authors": "Xiangyu Guo, Guy Kortsarz, Bundit Laekhanukit, Shi Li, Daniel Vaz,\n  Jiayi Xian", "title": "On Approximating Degree-Bounded Network Design Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed Steiner Tree (DST) is a central problem in combinatorial\noptimization and theoretical computer science: Given a directed graph $G=(V,\nE)$ with edge costs $c \\in \\mathbb{R}_{\\geq 0}^E$, a root $r \\in V$ and $k$\nterminals $K\\subseteq V$, we need to output the minimum-cost arborescence in\n$G$ that contains an $r$\\textrightarrow $t$ path for every $t \\in K$. Recently,\nGrandoni, Laekhanukit and Li, and independently Ghuge and Nagarajan, gave\nquasi-polynomial time $O(\\log^2k/\\log \\log k)$-approximation algorithms for the\nproblem, which are tight under popular complexity assumptions.\n  In this paper, we consider the more general Degree-Bounded Directed Steiner\nTree (DB-DST) problem, where we are additionally given a degree bound $d_v$ on\neach vertex $v \\in V$, and we require that every vertex $v$ in the output tree\nhas at most $d_v$ children. We give a quasi-polynomial time $(O(\\log n \\log k),\nO(\\log^2 n))$-bicriteria approximation: The algorithm produces a solution with\ncost at most $O(\\log n\\log k)$ times the cost of the optimum solution that\nviolates the degree constraints by at most a factor of $O(\\log^2n)$. This is\nthe first non-trivial result for the problem.\n  While our cost-guarantee is nearly optimal, the degree violation factor of\n$O(\\log^2n)$ is an $O(\\log n)$-factor away from the approximation lower bound\nof $\\Omega(\\log n)$ from the set-cover hardness. The hardness result holds even\non the special case of the {\\em Degree-Bounded Group Steiner Tree} problem on\ntrees (DB-GST-T). With the hope of closing the gap, we study the question of\nwhether the degree violation factor can be made tight for this special case. We\nanswer the question in the affirmative by giving an $(O(\\log n\\log k), O(\\log\nn))$-bicriteria approximation algorithm for DB-GST-T.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 07:11:36 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 15:45:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Guo", "Xiangyu", ""], ["Kortsarz", "Guy", ""], ["Laekhanukit", "Bundit", ""], ["Li", "Shi", ""], ["Vaz", "Daniel", ""], ["Xian", "Jiayi", ""]]}, {"id": "1907.11422", "submitter": "Ofer Neiman", "authors": "Michael Elkin, Yuval Gitlitz, Ofer Neiman", "title": "Almost Shortest Paths with Near-Additive Error in Weighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E,w)$ be a weighted undirected graph with $n$ vertices and $m$\nedges, and fix a set of $s$ sources $S\\subseteq V$. We study the problem of\ncomputing {\\em almost shortest paths} (ASP) for all pairs in $S \\times V$ in\nboth classical centralized and parallel (PRAM) models of computation. Consider\nthe regime of multiplicative approximation of $1+\\epsilon$, for an arbitrarily\nsmall constant $\\epsilon > 0$ . In this regime existing centralized algorithms\nrequire $\\Omega(\\min\\{|E|s,n^\\omega\\})$ time, where $\\omega < 2.372$ is the\nmatrix multiplication exponent. Existing PRAM algorithms with polylogarithmic\ndepth (aka time) require work $\\Omega(\\min\\{|E|s,n^\\omega\\})$.\n  Our centralized algorithm has running time $O((m+ ns)n^\\rho)$, and its PRAM\ncounterpart has polylogarithmic depth and work $O((m + ns)n^\\rho)$, for an\narbitrarily small constant $\\rho > 0$. For a pair $(s,v) \\in S\\times V$, it\nprovides a path of length $\\hat{d}(s,v)$ that satisfies $\\hat{d}(s,v) \\le\n(1+\\epsilon)d_G(s,v) + \\beta \\cdot W(s,v)$, where $W(s,v)$ is the weight of the\nheaviest edge on some shortest $s-v$ path. Hence our additive term depends\nlinearly on a {\\em local} maximum edge weight, as opposed to the global maximum\nedge weight in previous works. Finally, our $\\beta = (1/\\rho)^{O(1/\\rho)}$.\n  We also extend a centralized algorithm of Dor et al. \\cite{DHZ00}. For a\nparameter $\\kappa = 1,2,\\ldots$, this algorithm provides for {\\em unweighted}\ngraphs a purely additive approximation of $2(\\kappa -1)$ for {\\em all pairs\nshortest paths} (APASP) in time $\\tilde{O}(n^{2+1/\\kappa})$. Within the same\nrunning time, our algorithm for {\\em weighted} graphs provides a purely\nadditive error of $2(\\kappa - 1) W(u,v)$, for every vertex pair $(u,v) \\in {V\n\\choose 2}$, with $W(u,v)$ defined as above.\n  On the way to these results we devise a suit of novel constructions of\nspanners, emulators and hopsets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:23:44 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 11:20:54 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Elkin", "Michael", ""], ["Gitlitz", "Yuval", ""], ["Neiman", "Ofer", ""]]}, {"id": "1907.11635", "submitter": "Alexander Wein", "authors": "Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "Subexponential-Time Algorithms for Sparse PCA", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational cost of recovering a unit-norm sparse principal\ncomponent $x \\in \\mathbb{R}^n$ planted in a random matrix, in either the Wigner\nor Wishart spiked model (observing either $W + \\lambda xx^\\top$ with $W$ drawn\nfrom the Gaussian orthogonal ensemble, or $N$ independent samples from\n$\\mathcal{N}(0, I_n + \\beta xx^\\top)$, respectively). Prior work has shown that\nwhen the signal-to-noise ratio ($\\lambda$ or $\\beta\\sqrt{N/n}$, respectively)\nis a small constant and the fraction of nonzero entries in the planted vector\nis $\\|x\\|_0 / n = \\rho$, it is possible to recover $x$ in polynomial time if\n$\\rho \\lesssim 1/\\sqrt{n}$. While it is possible to recover $x$ in exponential\ntime under the weaker condition $\\rho \\ll 1$, it is believed that\npolynomial-time recovery is impossible unless $\\rho \\lesssim 1/\\sqrt{n}$. We\ninvestigate the precise amount of time required for recovery in the \"possible\nbut hard\" regime $1/\\sqrt{n} \\ll \\rho \\ll 1$ by exploring the power of\nsubexponential-time algorithms, i.e., algorithms running in time\n$\\exp(n^\\delta)$ for some constant $\\delta \\in (0,1)$. For any $1/\\sqrt{n} \\ll\n\\rho \\ll 1$, we give a recovery algorithm with runtime roughly $\\exp(\\rho^2\nn)$, demonstrating a smooth tradeoff between sparsity and runtime. Our family\nof algorithms interpolates smoothly between two existing algorithms: the\npolynomial-time diagonal thresholding algorithm and the $\\exp(\\rho n)$-time\nexhaustive search algorithm. Furthermore, by analyzing the low-degree\nlikelihood ratio, we give rigorous evidence suggesting that the tradeoff\nachieved by our algorithms is optimal.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:45:13 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 02:24:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ding", "Yunzi", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.11636", "submitter": "Alexander Wein", "authors": "Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "Notes on Computational Hardness of Hypothesis Testing: Predictions using\n  the Low-Degree Likelihood Ratio", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes survey and explore an emerging method, which we call the\nlow-degree method, for predicting and understanding\nstatistical-versus-computational tradeoffs in high-dimensional inference\nproblems. In short, the method posits that a certain quantity -- the second\nmoment of the low-degree likelihood ratio -- gives insight into how much\ncomputational time is required to solve a given hypothesis testing problem,\nwhich can in turn be used to predict the computational hardness of a variety of\nstatistical inference tasks. While this method originated in the study of the\nsum-of-squares (SoS) hierarchy of convex programs, we present a self-contained\nintroduction that does not require knowledge of SoS. In addition to showing how\nto carry out predictions using the method, we include a discussion\ninvestigating both rigorous and conjectural consequences of these predictions.\n  These notes include some new results, simplified proofs, and refined\nconjectures. For instance, we point out a formal connection between spectral\nmethods and the low-degree likelihood ratio, and we give a sharp low-degree\nlower bound against subexponential-time algorithms for tensor PCA.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:46:05 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.11662", "submitter": "Giacomo Ortali", "authors": "Panagiotis Lionakis, Giacomo Ortali, Ioannis G. Tollis", "title": "Adventures in Abstraction: Reachability in Hierarchical Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms and experiments for the visualization of directed\ngraphs that focus on displaying their reachability information. Our algorithms\nare based on the concepts of the path and channel decomposition as proposed in\nthe framework presented in GD 2018 (pp. 579-592) and focus on showing the\nexistence of paths clearly. In this paper we customize these concepts and\npresent experimental results that clearly show the interplay between bends,\ncrossings and clarity. Additionally, our algorithms have direct applications to\nthe important problem of showing and storing transitivity information of very\nlarge graphs and databases. Only a subset of the edges is drawn, thus reducing\nthe visual complexity of the resulting drawing, and the memory requirements for\nstoring the transitivity information. Our algorithms require almost linear\ntime, $O(kn+m)$, where $k$ is the number of paths/channels, $n$ and $m$ is the\nnumber of vertices and edges, respectively. They produce progressively more\nabstract drawings of the input graph. No dummy vertices are introduced and the\nvertices of each path/channel are vertically aligned.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:16:41 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Lionakis", "Panagiotis", ""], ["Ortali", "Giacomo", ""], ["Tollis", "Ioannis G.", ""]]}, {"id": "1907.11669", "submitter": "Samuel Gutekunst", "authors": "Samuel C. Gutekunst, David P. Williamson", "title": "Subtour Elimination Constraints Imply a Matrix-Tree Theorem SDP\n  Constraint for the TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De Klerk, Pasechnik, and Sotirov give a semidefinite programming constraint\nfor the Traveling Salesman Problem (TSP) based on the matrix-tree Theorem. This\nconstraint says that the aggregate weight of all spanning trees in a solution\nto a TSP relaxation is at least that of a cycle graph. In this note, we show\nthat the semidefinite constraint holds for any weighted 2-edge-connected graph\nand, in particular, is implied by the subtour elimination constraints of the\nsubtour elimination linear program. Hence, this semidefinite constraint is\nimplied by a finite set of linear inequality constraints.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:34:00 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Gutekunst", "Samuel C.", ""], ["Williamson", "David P.", ""]]}, {"id": "1907.11686", "submitter": "Dmitriy Kunisky", "authors": "Dmitriy Kunisky and Afonso S. Bandeira", "title": "A Tight Degree 4 Sum-of-Squares Lower Bound for the\n  Sherrington-Kirkpatrick Hamiltonian", "comments": "34 pages. Minor text revisions; closest to published version to\n  appear in Mathematical Programming", "journal-ref": null, "doi": "10.1007/s10107-020-01558-2", "report-no": null, "categories": "cs.DS cond-mat.stat-mech math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, if $\\mathbf{W} \\in \\mathbb{R}^{N \\times N}_{\\mathsf{sym}}$ is\ndrawn from the gaussian orthogonal ensemble, then with high probability the\ndegree 4 sum-of-squares relaxation cannot certify an upper bound on the\nobjective $N^{-1} \\cdot \\mathbf{x}^\\top \\mathbf{W} \\mathbf{x}$ under the\nconstraints $x_i^2 - 1 = 0$ (i.e. $\\mathbf{x} \\in \\{ \\pm 1 \\}^N$) that is\nasymptotically smaller than $\\lambda_{\\max}(\\mathbf{W}) \\approx 2$. We also\nconjecture a proof technique for lower bounds against sum-of-squares\nrelaxations of any degree held constant as $N \\to \\infty$, by proposing an\napproximate pseudomoment construction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:35:26 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 20:28:48 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kunisky", "Dmitriy", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.11705", "submitter": "Luong Nguyen", "authors": "Luong Trung Nguyen, Junhan Kim, Byonghyo Shim", "title": "Low-Rank Matrix Completion: A Contemporary Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a paradigm to recover unknown entries of a matrix from partial\nobservations, low-rank matrix completion (LRMC) has generated a great deal of\ninterest. Over the years, there have been lots of works on this topic but it\nmight not be easy to grasp the essential knowledge from these studies. This is\nmainly because many of these works are highly theoretical or a proposal of new\nLRMC technique. In this paper, we give a contemporary survey on LRMC. In order\nto provide better view, insight, and understanding of potentials and\nlimitations of LRMC, we present early scattered results in a structured and\naccessible way. Specifically, we classify the state-of-the-art LRMC techniques\ninto two main categories and then explain each category in detail. We next\ndiscuss issues to be considered when one considers using LRMC techniques. These\ninclude intrinsic properties required for the matrix recovery and how to\nexploit a special structure in LRMC design. We also discuss the convolutional\nneural network (CNN) based LRMC algorithms exploiting the graph structure of a\nlow-rank matrix. Further, we present the recovery performance and the\ncomputational complexity of the state-of-the-art LRMC techniques. Our hope is\nthat this survey article will serve as a useful guide for practitioners and\nnon-experts to catch the gist of LRMC.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 09:52:26 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Nguyen", "Luong Trung", ""], ["Kim", "Junhan", ""], ["Shim", "Byonghyo", ""]]}, {"id": "1907.11797", "submitter": "Rahmadi Trimananda", "authors": "Rahmadi Trimananda, Janus Varmarken, Athina Markopoulou, and Brian\n  Demsky", "title": "PingPong: Packet-Level Signatures for Smart Home Device Events", "comments": "This is the technical report for the paper titled Packet-Level\n  Signatures for Smart Home Devices published at the Network and Distributed\n  System Security (NDSS) Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart home devices are vulnerable to passive inference attacks based on\nnetwork traffic, even in the presence of encryption. In this paper, we present\nPINGPONG, a tool that can automatically extract packet-level signatures for\ndevice events (e.g., light bulb turning ON/OFF) from network traffic. We\nevaluated PINGPONG on popular smart home devices ranging from smart plugs and\nthermostats to cameras, voice-activated devices, and smart TVs. We were able\nto: (1) automatically extract previously unknown signatures that consist of\nsimple sequences of packet lengths and directions; (2) use those signatures to\ndetect the devices or specific events with an average recall of more than 97%;\n(3) show that the signatures are unique among hundreds of millions of packets\nof real world network traffic; (4) show that our methodology is also applicable\nto publicly available datasets; and (5) demonstrate its robustness in different\nsettings: events triggered by local and remote smartphones, as well as by\nhomeautomation systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 21:48:13 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 17:26:19 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 19:07:10 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Trimananda", "Rahmadi", ""], ["Varmarken", "Janus", ""], ["Markopoulou", "Athina", ""], ["Demsky", "Brian", ""]]}, {"id": "1907.12028", "submitter": "Sumathi Sivasubramaniam", "authors": "John Augustine and Sumathi Sivasubramaniam", "title": "Spartan: Sparse Robust Addressable Networks", "comments": "25 pages, 10 figures. A preliminary version of this paper appeared in\n  the Proceedings of the IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Peer-to-Peer (P2P) network is a dynamic collection of nodes that connect\nwith each other via virtual overlay links built upon an underlying network\n(usually, the Internet). P2P networks are highly dynamic and can experience\nvery heavy churn, i.e., a large number of nodes join/leave the network\ncontinuously. Thus, building and maintaining a stable overlay network is an\nimportant problem that has been studied extensively for two decades.\n  In this paper, we present our \\Pe overlay network called Sparse Robust\nAddressable Network (Spartan). Spartan can be quickly and efficiently built in\na fully distributed fashion within $O(\\log n)$ rounds. Furthermore, the Spartan\noverlay structure can be maintained, again, in a fully distributed manner\ndespite adversarially controlled churn (i.e., nodes joining and leaving) and\nsignificant variation in the number of nodes. Moreover, new nodes can join a\ncommittee within $O(1)$ rounds and leaving nodes can leave without any notice.\n  The number of nodes in the network lies in $[n, fn]$ for any fixed $f\\ge 1$.\nUp to $\\epsilon n$ nodes (for some small but fixed $\\epsilon > 0$) can be\nadversarially added/deleted within {\\em any} period of $P$ rounds for some $P\n\\in O(\\log \\log n)$. Despite such uncertainty in the network, Spartan maintains\n$\\Theta(n/\\log n)$ committees that are stable and addressable collections of\n$\\Theta(\\log n)$ nodes each for $O(polylog(n))$ rounds with high probability.\n  Spartan's committees are also capable of performing sustained computation and\npassing messages between each other. Thus, any protocol designed for static\nnetworks can be simulated on Spartan with minimal overhead. This makes Spartan\nan ideal platform for developing applications. We experimentally show that\nSpartan will remain robust as long as each committee, on average, contains 24\nnodes for networks of size up to $10240$.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:58:43 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 07:54:30 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Augustine", "John", ""], ["Sivasubramaniam", "Sumathi", ""]]}, {"id": "1907.12034", "submitter": "Gabriele Fici", "authors": "Gabriele Fici and Pawe{\\l} Gawrychowski", "title": "Minimal Absent Words in Rooted and Unrooted Trees", "comments": "This is a slightly modified version of the paper that appeared in the\n  proceedings of SPIRE 2019, which contained an error in the example showed in\n  Fig.1, now corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theory of minimal absent words to (rooted and unrooted) trees,\nhaving edges labeled by letters from an alphabet $\\Sigma$ of cardinality\n$\\sigma$. We show that the set $\\text{MAW}(T)$ of minimal absent words of a\nrooted (resp. unrooted) tree $T$ with $n$ nodes has cardinality $O(n\\sigma)$\n(resp. $O(n^{2}\\sigma)$), and we show that these bounds are realized. Then, we\nexhibit algorithms to compute all minimal absent words in a rooted (resp.\nunrooted) tree in output-sensitive time $O(n+|\\text{MAW}(T)|)$ (resp.\n$O(n^{2}+|\\text{MAW}(T)|)$ assuming an integer alphabet of size polynomial in\n$n$.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 07:32:58 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 07:58:05 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Fici", "Gabriele", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "1907.12061", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Diptapriyo Majumdar, Sebastian Ordyniak, Magnus\n  Wahlstr\\\"om", "title": "Parameterized Pre-coloring Extension and List Coloring Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Golovach, Paulusma and Song (Inf. Comput. 2014) asked to determine the\nparameterized complexity of the following problems parameterized by $k$: (1)\nGiven a graph $G$, a clique modulator $D$ (a clique modulator is a set of\nvertices, whose removal results in a clique) of size $k$ for $G$, and a list\n$L(v)$ of colors for every $v\\in V(G)$, decide whether $G$ has a proper list\ncoloring; (2) Given a graph $G$, a clique modulator $D$ of size $k$ for $G$,\nand a pre-coloring $\\lambda_P: X \\rightarrow Q$ for $X \\subseteq V(G),$ decide\nwhether $\\lambda_P$ can be extended to a proper coloring of $G$ using only\ncolors from $Q.$ For Problem 1 we design an $O^*(2^k)$-time randomized\nalgorithm and for Problem 2 we obtain a kernel with at most $3k$ vertices.\nBanik et al. (IWOCA 2019) proved the the following problem is fixed-parameter\ntractable and asked whether it admits a polynomial kernel: Given a graph $G$,\nan integer $k$, and a list $L(v)$ of exactly $n-k$ colors for every $v \\in\nV(G),$ decide whether there is a proper list coloring for $G.$ We obtain a\nkernel with $O(k^2)$ vertices and colors and a compression to a variation of\nthe problem with $O(k)$ vertices and $O(k^2)$ colors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 10:02:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Gutin", "Gregory", ""], ["Majumdar", "Diptapriyo", ""], ["Ordyniak", "Sebastian", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1907.12106", "submitter": "Timothy Sun", "authors": "Xi Chen, Tim Randolph, Rocco A. Servedio, Timothy Sun", "title": "A Lower Bound on Cycle-Finding in Sparse Digraphs", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a cycle in a sparse directed graph $G$\nthat is promised to be far from acyclic, meaning that the smallest feedback arc\nset in $G$ is large. We prove an information-theoretic lower bound, showing\nthat for $N$-vertex graphs with constant outdegree any algorithm for this\nproblem must make $\\tilde{\\Omega}(N^{5/9})$ queries to an adjacency list\nrepresentation of $G$. In the language of property testing, our result is an\n$\\tilde{\\Omega}(N^{5/9})$ lower bound on the query complexity of one-sided\nalgorithms for testing whether sparse digraphs with constant outdegree are far\nfrom acyclic. This is the first improvement on the $\\Omega(\\sqrt{N})$ lower\nbound, implicit in Bender and Ron, which follows from a simple birthday paradox\nargument.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 16:39:42 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Chen", "Xi", ""], ["Randolph", "Tim", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Timothy", ""]]}, {"id": "1907.12119", "submitter": "Matthew Fahrbach", "authors": "Robert Cummings, Matthew Fahrbach, Animesh Fatehpuria", "title": "A Fast Minimum Degree Algorithm and Matching Lower Bound", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum degree algorithm is one of the most widely-used heuristics for\nreducing the cost of solving large sparse systems of linear equations. It has\nbeen studied for nearly half a century and has a rich history of bridging\ntechniques from data structures, graph algorithms, and scientific computing. In\nthis paper, we present a simple but novel combinatorial algorithm for computing\nan exact minimum degree elimination ordering in $O(nm)$ time, which improves on\nthe best known time complexity of $O(n^3)$ and offers practical improvements\nfor sparse systems with small values of $m$. Our approach leverages a careful\namortized analysis, which also allows us to derive output-sensitive bounds for\nthe running time of $O(\\min\\{m\\sqrt{m^+}, \\Delta m^+\\} \\log n)$, where $m^+$ is\nthe number of unique fill edges and original edges that the algorithm\nencounters and $\\Delta$ is the maximum degree of the input graph.\n  Furthermore, we show there cannot exist an exact minimum degree algorithm\nthat runs in $O(nm^{1-\\varepsilon})$ time, for any $\\varepsilon > 0$, assuming\nthe strong exponential time hypothesis. This fine-grained reduction goes\nthrough the orthogonal vectors problem and uses a new low-degree graph\nconstruction called $U$-fillers, which act as pathological inputs and cause any\nminimum degree algorithm to exhibit nearly worst-case performance. With these\ntwo results, we nearly characterize the time complexity of computing an exact\nminimum degree ordering.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 17:57:21 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:06:10 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Cummings", "Robert", ""], ["Fahrbach", "Matthew", ""], ["Fatehpuria", "Animesh", ""]]}, {"id": "1907.12130", "submitter": "Patrick Rodler", "authors": "Patrick Rodler", "title": "Towards Optimizing Reiter's HS-Tree for Sequential Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reiter's HS-Tree is one of the most popular diagnostic search algorithms due\nto its desirable properties and general applicability. In sequential diagnosis,\nwhere the addressed diagnosis problem is subject to successive change through\nthe acquisition of additional knowledge about the diagnosed system, HS-Tree is\nused in a stateless fashion. That is, the existing search tree is discarded\nwhen new knowledge is obtained, albeit often large parts of the tree are still\nrelevant and have to be rebuilt in the next iteration, involving redundant\noperations and costly reasoner calls. As a remedy to this, we propose\nDynamicHS, a variant of HS-Tree that avoids these redundancy issues by\nmaintaining state throughout sequential diagnosis while preserving all\ndesirable properties of HS-Tree. Preliminary results of ongoing evaluations in\na problem domain where HS-Tree is the state-of-the-art diagnostic method\nsuggest significant time savings achieved by DynamicHS by reducing expensive\nreasoner calls.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 19:26:08 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Rodler", "Patrick", ""]]}, {"id": "1907.12335", "submitter": "Robert Ganian", "authors": "Robert Ganian, Sebastian Ordyniak, Stefan Szeider", "title": "A Join-Based Hybrid Parameter for Constraint Satisfaction", "comments": "Accepted at CP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose joinwidth, a new complexity parameter for the Constraint\nSatisfaction Problem (CSP). The definition of joinwidth is based on the\narrangement of basic operations on relations (joins, projections, and pruning),\nwhich inherently reflects the steps required to solve the instance. We use\njoinwidth to obtain polynomial-time algorithms (if a corresponding\ndecomposition is provided in the input) as well as fixed-parameter algorithms\n(if no such decomposition is provided) for solving the CSP.\n  Joinwidth is a hybrid parameter, as it takes both the graphical structure as\nwell as the constraint relations that appear in the instance into account. It\nhas, therefore, the potential to capture larger classes of tractable instances\nthan purely structural parameters like hypertree width and the more general\nfractional hypertree width (fhtw). Indeed, we show that any class of instances\nof bounded fhtw also has bounded joinwidth, and that there exist classes of\ninstances of bounded joinwidth and unbounded fhtw, so bounded joinwidth\nproperly generalizes bounded fhtw.\n  We further show that bounded joinwidth also properly generalizes several\nother known hybrid restrictions, such as fhtw with degree constraints and\nfunctional dependencies. In this sense, bounded joinwidth can be seen as a\nunifying principle that explains the tractability of several seemingly\nunrelated classes of CSP instances.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:23:55 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ganian", "Robert", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1907.12343", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "Blue-Noise Dithered QMC Hierarchical Russian Roulette", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to efficiently sample specular-diffuse-glossy and\nglossy-diffuse-glossy transport phenomena, Tokuyoshi and Harada introduced\nhierarchical Russian roulette, a smart algorithm that allows to compute the\nminimum of the random numbers associated to leaves of a tree at each internal\nnode. The algorithm is used to efficiently cull the connections between the\nproduct set of eye and light vertices belonging to large caches of eye and\nlight subpaths produced through bidirectional path tracing. The original\nversion of the algorithm is entirely based on the generation of semi-stratified\npseudo-random numbers. Our paper proposes a novel variant based on\ndeterministic blue-noise dithered Quasi Monte Carlo samples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:38:05 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 07:22:01 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "1907.12383", "submitter": "Michael Fr\\\"owis", "authors": "Michael Fr\\\"owis and Rainer B\\\"ohme", "title": "The Operational Cost of Ethereum Airdrops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient transfers to many recipients present a host of issues on Ethereum.\nFirst, accounts are identified by long and incompressible constants. Second,\nthese constants have to be stored and communicated for each payment. Third, the\nstandard interface for token transfers does not support lists of recipients,\nadding repeated communication to the overhead. Since Ethereum charges resource\nusage, even small optimizations translate to cost savings. Airdrops, a popular\nmarketing tool used to boost coin uptake, present a relevant example for the\nvalue of optimizing bulk transfers. Therefore, we review technical solutions\nfor airdrops of Ethereum-based tokens, discuss features and prerequisites, and\ncompare the operational costs by simulating 35 scenarios. We find that cost\nsavings of factor two are possible, but require specific provisions in the\nsmart contract implementing the token system. Pull-based approaches, which use\non-chain interaction with the recipients, promise moderate savings for the\ndistributor while imposing a disproportional cost on each recipient. Total\ncosts are broadly linear in the number of recipients independent of the\ntechnical approach. We publish the code of the simulation framework for\nreproducibility, to support future airdrop decisions, and to benchmark\ninnovative bulk payment solutions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:39:05 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Fr\u00f6wis", "Michael", ""], ["B\u00f6hme", "Rainer", ""]]}, {"id": "1907.12443", "submitter": "Hoa Vu", "authors": "Hsin-Hao Su, Hoa T. Vu", "title": "Distributed Dense Subgraph Detection and Low Outdegree Orientation", "comments": "To appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The densest subgraph problem, introduced in the 80s by Picard and Queyranne\nas well as Goldberg, is a classic problem in combinatorial optimization with a\nwide range of applications. The lowest outdegree orientation problem is known\nto be its dual problem. We study both the problem of finding dense subgraphs\nand the problem of computing a low outdegree orientation in the distributed\nsettings.\n  Suppose $G=(V,E)$ is the underlying network as well as the input graph. Let\n$D$ denote the density of the maximum density subgraph of $G$. Our main results\nare as follows.\n  Given a value $\\tilde{D} \\leq D$ and $0 < \\epsilon < 1$, we show that a\nsubgraph with density at least $(1-\\epsilon)\\tilde{D}$ can be identified\ndeterministically in $O((\\log n) / \\epsilon)$ rounds in the LOCAL model. We\nalso present a lower bound showing that our result for the LOCAL model is tight\nup to an $O(\\log n)$ factor.\n  In the CONGEST model, we show that such a subgraph can be identified in\n$O((\\log^3 n) / \\epsilon^3)$ rounds with high probability. Our techniques also\nlead to an $O(diameter + (\\log^4 n)/\\epsilon^4)$-round algorithm that yields a\n$1-\\epsilon$ approximation to the densest subgraph. This improves upon the\nprevious $O(diameter /\\epsilon \\cdot \\log n)$-round algorithm by Das Sarma et\nal. [DISC 2012] that only yields a $1/2-\\epsilon$ approximation.\n  Given an integer $\\tilde{D} \\geq D$ and $\\Omega(1/\\tilde{D}) < \\epsilon <\n1/4$, we give a deterministic, $\\tilde{O}((\\log^2 n) /\\epsilon^2)$-round\nalgorithm in the CONGEST model that computes an orientation where the outdegree\nof every vertex is upper bounded by $(1+\\epsilon)\\tilde{D}$. Previously, the\nbest deterministic algorithm and randomized algorithm by Harris [FOCS 2019] run\nin $\\tilde{O}((\\log^6 n)/ \\epsilon^4)$ rounds and $\\tilde{O}((\\log^3 n)\n/\\epsilon^3)$ rounds respectively and only work in the LOCAL model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 14:04:17 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 04:39:21 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 09:46:39 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 11:59:32 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1907.12468", "submitter": "Moira MacNeil", "authors": "Moira MacNeil and Merve Bodur", "title": "Integer Programming, Constraint Programming, and Hybrid Decomposition\n  Approaches to Discretizable Distance Geometry Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an integer dimension K and a simple, undirected graph G with positive\nedge weights, the Distance Geometry Problem (DGP) aims to find a realization\nfunction mapping each vertex to a coordinate in K-dimensional space such that\nthe distance between pairs of vertex coordinates is equal to the corresponding\nedge weights in G. The so-called discretization assumptions reduce the search\nspace of the realization to a finite discrete one which can be explored via the\nbranch-and-prune (BP) algorithm. Given a discretization vertex order in G, the\nBP algorithm constructs a binary tree where the nodes at a layer provide all\npossible coordinates of the vertex corresponding to that layer. The focus of\nthis paper is finding optimal BP trees for a class of Discretizable DGPs. More\nspecifically, we aim to find a discretization vertex order in G that yields a\nBP tree with the least number of branches. We propose an integer programming\nformulation and three constraint programming formulations that all\nsignificantly outperform the state-of-the-art cutting plane algorithm for this\nproblem. Moreover, motivated by the difficulty in solving instances with a\nlarge and low density input graph, we develop two hybrid decomposition\nalgorithms, strengthened by a set of valid inequalities, which further improve\nthe solvability of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:06:52 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 19:09:12 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 20:07:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["MacNeil", "Moira", ""], ["Bodur", "Merve", ""]]}, {"id": "1907.12724", "submitter": "Matthew Hastings", "authors": "M. B. Hastings", "title": "Classical and Quantum Algorithms for Tensor Principal Component Analysis", "comments": "29 pages; v2 accepted in Quantum", "journal-ref": "Quantum 4, 237 (2020)", "doi": "10.22331/q-2020-02-27-237", "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present classical and quantum algorithms based on spectral methods for a\nproblem in tensor principal component analysis. The quantum algorithm achieves\na quartic speedup while using exponentially smaller space than the fastest\nclassical spectral algorithm, and a super-polynomial speedup over classical\nalgorithms that use only polynomial space. The classical algorithms that we\npresent are related to, but slightly different from those presented recently in\nRef. 1. In particular, we have an improved threshold for recovery and the\nalgorithms we present work for both even and odd order tensors. These results\nsuggest that large-scale inference problems are a promising future application\nfor quantum computers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 03:45:27 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 18:10:19 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Hastings", "M. B.", ""]]}, {"id": "1907.12857", "submitter": "Piotr Krysta", "authors": "Dariusz R. Kowalski and Piotr Krysta", "title": "Deterministic coloring algorithms in the LOCAL model", "comments": "Version date: 10 July 2019; some typos corrected; added explanation\n  p. 5; paper submitted to ACM-SIAM SODA 2020; 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of bi-chromatic coloring of hypergraphs in the LOCAL\ndistributed model of computation. This problem can easily be solved by a\nrandomized local algorithm with no communication. However, it is not known how\nto solve it deterministically with only a polylogarithmic number of\ncommunication rounds. In this paper we indeed design such a deterministic\nalgorithm that solves this problem with polylogarithmic number of communication\nrounds. This is an almost exponential improvement on the previously known\ndeterministic local algorithms for this problem. Because the bi-chromatic\ncoloring of hypergraphs problem is known to be complete in the class of all\nlocally checkable graph problems, our result implies deterministic local\nalgorithms with polylogarithmic number of communication rounds for all such\nproblems for which an efficient randomized algorithm exists. This solves one of\nthe fundamental open problems in the area of local distributed graph\nalgorithms. By reductions due to Ghaffari, Kuhn and Maus [STOC 2017] this\nimplies such polylogarithmically efficient deterministic local algorithms for\nmany graph problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:28:52 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 02:40:14 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kowalski", "Dariusz R.", ""], ["Krysta", "Piotr", ""]]}, {"id": "1907.12942", "submitter": "Hiroki Oshima", "authors": "Hiroki Oshima", "title": "Improved randomized algorithm for $k$-submodular function maximization", "comments": "22 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodularity is one of the most important properties in combinatorial\noptimization, and $k$-submodularity is a generalization of submodularity.\nMaximization of a $k$-submodular function requires an exponential number of\nvalue oracle queries, and approximation algorithms have been studied. For\nunconstrained $k$-submodular maximization, Iwata et al. gave randomized\n$k/(2k-1)$-approximation algorithm for monotone functions, and randomized\n$1/2$-approximation algorithm for nonmonotone functions. In this paper, we\npresent improved randomized algorithms for nonmonotone functions. Our algorithm\ngives $\\frac{k^2+1}{2k^2+1}$-approximation for $k\\geq 3$. We also give a\nrandomized $\\frac{\\sqrt{17}-3}{2}$-approximation algorithm for $k=3$. We use\nthe same framework used in Iwata et al. and Ward and \\v{Z}ivn\\'{y} with\ndifferent probabilities.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 06:53:37 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Oshima", "Hiroki", ""]]}, {"id": "1907.13062", "submitter": "Laurent Orseau", "authors": "Malte Helmert, Tor Lattimore, Levi H. S. Lelis, Laurent Orseau, Nathan\n  R. Sturtevant", "title": "Iterative Budgeted Exponential Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle two long-standing problems related to re-expansions in heuristic\nsearch algorithms. For graph search, A* can require $\\Omega(2^{n})$ expansions,\nwhere $n$ is the number of states within the final $f$ bound. Existing\nalgorithms that address this problem like B and B' improve this bound to\n$\\Omega(n^2)$. For tree search, IDA* can also require $\\Omega(n^2)$ expansions.\nWe describe a new algorithmic framework that iteratively controls an expansion\nbudget and solution cost limit, giving rise to new graph and tree search\nalgorithms for which the number of expansions is $O(n \\log C)$, where $C$ is\nthe optimal solution cost. Our experiments show that the new algorithms are\nrobust in scenarios where existing algorithms fail. In the case of tree search,\nour new algorithms have no overhead over IDA* in scenarios to which IDA* is\nwell suited and can therefore be recommended as a general replacement for IDA*.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:40:41 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Helmert", "Malte", ""], ["Lattimore", "Tor", ""], ["Lelis", "Levi H. S.", ""], ["Orseau", "Laurent", ""], ["Sturtevant", "Nathan R.", ""]]}, {"id": "1907.13301", "submitter": "Gal Sadeh", "authors": "Gal Sadeh and Edith Cohen and Haim Kaplan", "title": "Sample Complexity Bounds for Influence Maximization", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization (IM) is the problem of finding for a given $s\\geq 1$ a\nset $S$ of $|S|=s$ nodes in a network with maximum influence. With stochastic\ndiffusion models, the influence of a set $S$ of seed nodes is defined as the\nexpectation of its reachability over simulations, where each simulation\nspecifies a deterministic reachability function. Two well-studied special cases\nare the Independent Cascade (IC) and the Linear Threshold (LT) models of Kempe,\nKleinberg, and Tardos. The influence function in stochastic diffusion is\nunbiasedly estimated by averaging reachability values over i.i.d. simulations.\nWe study the IM sample complexity: the number of simulations needed to\ndetermine a $(1-\\epsilon)$-approximate maximizer with confidence $1-\\delta$.\nOur main result is a surprising upper bound of $O( s \\tau \\epsilon^{-2} \\ln\n\\frac{n}{\\delta})$ for a broad class of models that includes IC and LT models\nand their mixtures, where $n$ is the number of nodes and $\\tau$ is the number\nof diffusion steps. Generally $\\tau \\ll n$, so this significantly improves over\nthe generic upper bound of $O(s n \\epsilon^{-2} \\ln \\frac{n}{\\delta})$. Our\nsample complexity bounds are derived from novel upper bounds on the variance of\nthe reachability that allow for small relative error for influential sets and\nadditive error when influence is small. Moreover, we provide a data-adaptive\nmethod that can detect and utilize fewer simulations on models where it\nsuffices. Finally, we provide an efficient greedy design that computes an\n$(1-1/e-\\epsilon)$-approximate maximizer from simulations and applies to any\nsubmodular stochastic diffusion model that satisfies the variance bounds.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 03:57:00 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 22:30:43 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Sadeh", "Gal", ""], ["Cohen", "Edith", ""], ["Kaplan", "Haim", ""]]}, {"id": "1907.13508", "submitter": "Henry Wilde", "authors": "Henry Wilde, Vincent Knight, Jonathan Gillard", "title": "Evolutionary Dataset Optimisation: learning algorithm quality through\n  evolution", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a novel method for learning how algorithms perform.\nClassically, algorithms are compared on a finite number of existing (or newly\nsimulated) benchmark datasets based on some fixed metrics. The algorithm(s)\nwith the smallest value of this metric are chosen to be the `best performing'.\nWe offer a new approach to flip this paradigm. We instead aim to gain a richer\npicture of the performance of an algorithm by generating artificial data\nthrough genetic evolution, the purpose of which is to create populations of\ndatasets for which a particular algorithm performs well on a given metric.\nThese datasets can be studied so as to learn what attributes lead to a\nparticular progression of a given algorithm. Following a detailed description\nof the algorithm as well as a brief description of an open source\nimplementation, a case study in clustering is presented. This case study\ndemonstrates the performance and nuances of the method which we call\nEvolutionary Dataset Optimisation. In this study, a number of known properties\nabout preferable datasets for the clustering algorithms known as (k)-means and\nDBSCAN are realised in the generated datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:03:20 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 16:05:28 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 10:07:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Wilde", "Henry", ""], ["Knight", "Vincent", ""], ["Gillard", "Jonathan", ""]]}, {"id": "1907.13558", "submitter": "Guido Br\\\"uckner", "authors": "Guido Br\\\"uckner, Nadine Davina Krisam, and Tamara Mchedlidze", "title": "Level-Planar Drawings with Few Slopes", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study level-planar straight-line drawings with a fixed\nnumber $\\lambda$ of slopes. For proper level graphs, we give an $O(n \\log^2 n /\n\\log \\log n)$-time algorithm that either finds such a drawing or determines\nthat no such drawing exists. Moreover, we consider the partial drawing\nextension problem, where we seek to extend an immutable drawing of a subgraph\nto a drawing of the whole graph, and the simultaneous drawing problem, which\nasks about the existence of drawings of two graphs whose restrictions to their\nshared subgraph coincide. We present $O(n^{4/3} \\log n)$-time and $O({\\lambda}\nn^{10/3} \\log n)$-time algorithms for these respective problems on proper\nlevel-planar graphs. We complement these positive results by showing that\ntesting whether non-proper level graphs admit level-planar drawings with\n$\\lambda$ slopes is $\\textsf{NP}$-hard even in restricted cases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:33:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Br\u00fcckner", "Guido", ""], ["Krisam", "Nadine Davina", ""], ["Mchedlidze", "Tamara", ""]]}, {"id": "1907.13602", "submitter": "Richard Kueng", "authors": "Richard Kueng and Joel A. Tropp", "title": "Binary component decomposition Part II: The asymmetric case", "comments": "18(+9) pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of decomposing a low-rank matrix into a factor\nwith binary entries, either from $\\{\\pm 1\\}$ or from $\\{0,1\\}$, and an\nunconstrained factor. The research answers fundamental questions about the\nexistence and uniqueness of these decompositions. It also leads to tractable\nfactorization algorithms that succeed under a mild deterministic condition.\nThis work builds on a companion paper that addresses the related problem of\ndecomposing a low-rank positive-semidefinite matrix into symmetric binary\nfactors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:07:08 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kueng", "Richard", ""], ["Tropp", "Joel A.", ""]]}, {"id": "1907.13603", "submitter": "Richard Kueng", "authors": "Richard Kueng and Joel A. Tropp", "title": "Binary Component Decomposition Part I: The Positive-Semidefinite Case", "comments": "21(+4) pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of decomposing a low-rank\npositive-semidefinite matrix into symmetric factors with binary entries, either\n$\\{\\pm 1\\}$ or $\\{0,1\\}$. This research answers fundamental questions about the\nexistence and uniqueness of these decompositions. It also leads to tractable\nfactorization algorithms that succeed under a mild deterministic condition. A\ncompanion paper addresses the related problem of decomposing a low-rank\nrectangular matrix into a binary factor and an unconstrained factor.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:07:13 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kueng", "Richard", ""], ["Tropp", "Joel A.", ""]]}, {"id": "1907.13616", "submitter": "Krishnakumar Balasubramanian", "authors": "Abhishek Roy, Krishnakumar Balasubramanian, Saeed Ghadimi, Prasant\n  Mohapatra", "title": "Multi-Point Bandit Algorithms for Nonstationary Online Nonconvex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit algorithms have been predominantly analyzed in the convex setting with\nfunction-value based stationary regret as the performance measure. In this\npaper, motivated by online reinforcement learning problems, we propose and\nanalyze bandit algorithms for both general and structured nonconvex problems\nwith nonstationary (or dynamic) regret as the performance measure, in both\nstochastic and non-stochastic settings. First, for general nonconvex functions,\nwe consider nonstationary versions of first-order and second-order stationary\nsolutions as a regret measure, motivated by similar performance measures for\noffline nonconvex optimization. In the case of second-order stationary solution\nbased regret, we propose and analyze online and bandit versions of the cubic\nregularized Newton's method. The bandit version is based on estimating the\nHessian matrices in the bandit setting, based on second-order Gaussian Stein's\nidentity. Our nonstationary regret bounds in terms of second-order stationary\nsolutions have interesting consequences for avoiding saddle points in the\nbandit setting. Next, for weakly quasi convex functions and monotone weakly\nsubmodular functions we consider nonstationary regret measures in terms of\nfunction-values; such structured classes of nonconvex functions enable one to\nconsider regret measure defined in terms of function values, similar to convex\nfunctions. For this case of function-value, and first-order stationary solution\nbased regret measures, we provide regret bounds in both the low- and\nhigh-dimensional settings, for some scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:32:07 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 07:06:09 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Roy", "Abhishek", ""], ["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""], ["Mohapatra", "Prasant", ""]]}]