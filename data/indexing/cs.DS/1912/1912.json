[{"id": "1912.00082", "submitter": "Dario Frascaria", "authors": "Dario Frascaria and Neil Olver", "title": "Algorithms for flows over time with scheduling costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flows over time have received substantial attention from both an optimization\nand (more recently) a game-theoretic perspective. In this model, each arc has\nan associated delay for traversing the arc, and a bound on the rate of flow\nentering the arc; flows are time-varying. We consider a setting which is very\nstandard within the transportation economic literature, but has received little\nattention from an algorithmic perspective. The flow consists of users who are\nable to choose their route but also their departure time, and who desire to\narrive at their destination at a particular time, incurring a 'scheduling cost'\nif they arrive earlier or later. The total cost of a user is then a combination\nof the time they spend commuting, and the scheduling cost they incur. We\npresent a combinatorial algorithm for the natural optimization problem, that of\nminimizing the average total cost of all users (i.e., maximizing the social\nwelfare). Based on this, we also show how to set tolls so that this optimal\nflow is induced as an equilibrium of the underlying game.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 22:49:17 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Frascaria", "Dario", ""], ["Olver", "Neil", ""]]}, {"id": "1912.00101", "submitter": "Neil Olver", "authors": "Thomas Bosman, Neil Olver", "title": "Improved Approximation Algorithms for Inventory Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new approximation algorithms for the submodular joint replenishment\nproblem and the inventory routing problem, using an iterative rounding\napproach. In both problems, we are given a set of $N$ items and a discrete time\nhorizon of $T$ days in which given demands for the items must be satisfied.\nOrdering a set of items incurs a cost according to a set function, with\nproperties depending on the problem under consideration. Demand for an item at\ntime $t$ can be satisfied by an order on any day prior to $t$, but a holding\ncost is charged for storing the items during the intermediate period; the goal\nis to minimize the sum of the ordering and holding cost.\n  Our approximation factor for both problems is $O(\\log \\log \\min(N,T))$; this\nimproves exponentially on the previous best results.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 00:16:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bosman", "Thomas", ""], ["Olver", "Neil", ""]]}, {"id": "1912.00143", "submitter": "Siddhartha Jain", "authors": "Siddhartha Jain", "title": "Inapproximability of Additive Weak Contraction under SSEH and Strong UGC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Succinct representations of a graph have been objects of central study in\ncomputer science for decades. In this paper, we study the operation called\n\\emph{Distance Preserving Graph Contractions}, which was introduced by\nBernstein et al. (ITCS, 2018). This operation gives a minor as a succinct\nrepresentation of a graph that preserves all the distances of the original (up\nto some factor). The graph minor given from contractions can be seen as a dual\nof spanners as the distances can only shrink (while distances are stretched in\nthe case of spanners). Bernstein et al. proved inapproximability results for\nthe problems of finding maximum subset of edges that yields distance preserving\ngraph contractions for almost major classes of graphs except for that of\nAdditive Weak Contraction.\n  The main result in this paper is filling the gap in the paper of Bernstein et\nal. We show that the Maximum Additive Weak Contraction problem on a graph with\n$n$ vertices is inapproximable up to a factor of $n^{1-\\epsilon}$ for every\nconstant $\\epsilon>0$. Our hardness results follow from that of the Maximum\nEdge Biclique (\\textsc{MEB}) problem whose inapproximability of\n$n^{1-\\epsilon}$ has been recently shown by Manurangsi (ICALP, 2017) under the\n\\textsc{Small Set Expansion Hypothesis (SSEH)} and by Bhangale et al. (APPROX,\n2016) under the \\textsc{Strong Unique Games Conjecture (SUGC)} (both results\nalso assume $\\mathrm{NP}\\not\\subseteq\\mathrm{BPP}$).\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 06:47:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Jain", "Siddhartha", ""]]}, {"id": "1912.00225", "submitter": "Michael Curry", "authors": "Michael J. Curry, John P. Dickerson, Karthik Abinav Sankararaman,\n  Aravind Srinivasan, Yuhao Wan, Pan Xu", "title": "Mix and Match: Markov Chains & Mixing Times for Matching in Rideshare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rideshare platforms such as Uber and Lyft dynamically dispatch drivers to\nmatch riders' requests. We model the dispatching process in rideshare as a\nMarkov chain that takes into account the geographic mobility of both drivers\nand riders over time. Prior work explores dispatch policies in the limit of\nsuch Markov chains; we characterize when this limit assumption is valid, under\na variety of natural dispatch policies. We give explicit bounds on convergence\nin general, and exact (including constants) convergence rates for special\ncases. Then, on simulated and real transit data, we show that our bounds\ncharacterize convergence rates -- even when the necessary theoretical\nassumptions are relaxed. Additionally these policies compare well against a\nstandard reinforcement learning algorithm which optimizes for profit without\nany convergence properties.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 16:26:33 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Curry", "Michael J.", ""], ["Dickerson", "John P.", ""], ["Sankararaman", "Karthik Abinav", ""], ["Srinivasan", "Aravind", ""], ["Wan", "Yuhao", ""], ["Xu", "Pan", ""]]}, {"id": "1912.00231", "submitter": "Luca Ganassali", "authors": "Luca Ganassali, Marc Lelarge, Laurent Massouli\\'e", "title": "Spectral Alignment of Correlated Gaussian matrices", "comments": "26 pages, 4 figures. Figures and paper organization updated, typos\n  corrected. Remark 4.2. added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze a simple spectral method (EIG1) for the problem of\nmatrix alignment, consisting in aligning their leading eigenvectors: given two\nmatrices $A$ and $B$, we compute $v_1$ and $v'_1$ two corresponding leading\neigenvectors. The algorithm returns the permutation $\\hat{\\pi}$ such that the\nrank of coordinate $\\hat{\\pi}(i)$ in $v_1$ and that of coordinate $i$ in $v'_1$\n(up to the sign of $v'_1$) are the same. We consider a model of weighted graphs\nwhere the adjacency matrix $A$ belongs to the Gaussian Orthogonal Ensemble\n(GOE) of size $N \\times N$, and $B$ is a noisy version of $A$ where all nodes\nhave been relabeled according to some planted permutation $\\pi$, namely $B=\n\\Pi^T (A+\\sigma H) \\Pi $, where $\\Pi$ is the permutation matrix associated with\n$\\pi$ and $H$ is an independent copy of $A$. We show the following zero-one\nlaw: with high probability, under the condition $\\sigma N^{7/6+\\epsilon} \\to 0$\nfor some $\\epsilon>0$, EIG1 recovers all but a vanishing part of the underlying\npermutation $\\pi$, whereas if $\\sigma N^{7/6-\\epsilon} \\to \\infty$, this method\ncannot recover more than $o(N)$ correct matches. This result gives an\nunderstanding of the simplest and fastest spectral method for matrix alignment\n(or complete weighted graph alignment), and involves proof methods and\ntechniques which could be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 16:45:33 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 16:09:36 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ganassali", "Luca", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1912.00245", "submitter": "Christian Schulz", "authors": "Christian Schulz", "title": "Scalable Graph Algorithms", "comments": "Habilitation thesis of Christian Schulz", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large complex networks recently attracted considerable interest.\nComplex graphs are useful in a wide range of applications from technological\nnetworks to biological systems like the human brain. Sometimes these networks\nare composed of billions of entities that give rise to emerging properties and\nstructures. Analyzing these structures aids us in gaining new insights about\nour surroundings. As huge networks become abundant, there is a need for\nscalable algorithms to perform analysis. A prominent example is the PageRank\nalgorithm, which is one of the measures used by web search engines such as\nGoogle to rank web pages displayed to the user. In order to find these\npatterns, massive amounts of data have to be acquired and processed. Designing\nand evaluating scalable graph algorithms to handle these data sets is a crucial\ntask on the road to understanding the underlying systems.\n  This habilitation thesis is a summary a broad spectrum of scalable graph\nalgorithms that I developed over the last six years with many coauthors. In\ngeneral, this research is based on four pillars: multilevel algorithms,\npractical kernelization, parallelization and memetic algorithms that are highly\ninterconnected. Experiments conducted indicate that our algorithms find better\nsolutions and/or are much more scalable than the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 18:01:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Schulz", "Christian", ""]]}, {"id": "1912.00272", "submitter": "Guangmo Tong", "authors": "Guangmo Tong, Ruiqi Wang, Zheng Dong", "title": "On Multi-Cascade Influence Maximization: Model, Hardness and Algorithmic\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the multi-cascade influence maximization problem, which\nexplores strategies for launching one information cascade in a social network\nwith multiple existing cascades. With natural extensions to the classic models,\nwe first propose the independent multi-cascade model where the diffusion\nprocess is governed by the so-called activation function. We show that the\nproposed model is sufficiently flexible as it generalizes most of the existing\ncascade-based models. We then study the multi-cascade influence maximization\nproblem under the designed model and provide approximation hardness under\ncommon complexity assumptions, namely Exponential Time Hypothesis and $NP\n\\subseteq DTIME(n^{\\poly \\log n})$. Given the hardness results, we build a\nframework for designing heuristic seed selection algorithms with a testable\ndata-dependent approximation ratio. The designed algorithm leverages upper and\nlower bounds, which reveal the key combinatorial structure behind the\nmulti-cascade influence maximization problem. The performance of the framework\nis theoretically analyzed and practically evaluated through extensive\nsimulations. The superiority of the proposed solution is supported by\nencouraging experimental results, in terms of effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 22:07:07 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Tong", "Guangmo", ""], ["Wang", "Ruiqi", ""], ["Dong", "Zheng", ""]]}, {"id": "1912.00620", "submitter": "Taihei Oki", "authors": "Kazuki Matoya, Taihei Oki", "title": "Pfaffian Pairs and Parities: Counting on Linear Matroid Intersection and\n  Parity Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanning trees are a representative example of linear matroid bases that are\nefficiently countable. Perfect matchings of Pfaffian bipartite graphs are a\ncountable example of common bases of two matrices. Generalizing these two\nexamples, Webb (2004) introduced the notion of Pfaffian pairs as a pair of\nmatrices for which counting of their common bases is tractable via the\nCauchy-Binet formula.\n  This paper studies counting on linear matroid problems extending Webb's work.\nWe first introduce \"Pfaffian parities\" as an extension of Pfaffian pairs to the\nlinear matroid parity problem, which is a common generalization of the linear\nmatroid intersection problem and the matching problem. We enumerate\ncombinatorial examples of Pfaffian pairs and parities. The variety of the\nexamples illustrates that Pfaffian pairs and parities serve as a unified\nframework of efficiently countable discrete structures. Based on this\nframework, we derive celebrated counting theorems, such as Kirchhoff's\nmatrix-tree theorem, Tutte's directed matrix-tree theorem, the Pfaffian\nmatrix-tree theorem, and the Lindstr\\\"om-Gessel-Viennot lemma.\n  Our study then turns to algorithmic aspects. We observe that the fastest\nrandomized algorithms for the linear matroid intersection and parity problems\nby Harvey (2009) and Cheung-Lau-Leung (2014) can be derandomized for Pfaffian\npairs and parities. We further present polynomial-time algorithms to count the\nnumber of minimum-weight solutions on weighted Pfaffian pairs and parities. Our\nalgorithms make use of Frank's weight splitting lemma for the weighted matroid\nintersection problem and the algebraic optimality criterion of the weighted\nlinear matroid parity problem given by Iwata-Kobayashi (2017).\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:18:10 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 09:15:56 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Matoya", "Kazuki", ""], ["Oki", "Taihei", ""]]}, {"id": "1912.00653", "submitter": "Melanie Schmidt", "authors": "Anup Bhattacharya, Jan Eube, Heiko R\\\"oglin, Melanie Schmidt", "title": "Noisy, Greedy and Not So Greedy k-means++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means++ algorithm due to Arthur and Vassilvitskii has become the most\npopular seeding method for Lloyd's algorithm. It samples the first center\nuniformly at random from the data set and the other $k-1$ centers iteratively\naccording to $D^2$-sampling where the probability that a data point becomes the\nnext center is proportional to its squared distance to the closest center\nchosen so far. k-means++ is known to achieve an approximation factor of $O(\\log\nk)$ in expectation. Already in the original paper on k-means++, Arthur and\nVassilvitskii suggested a variation called greedy k-means++ algorithm in which\nin each iteration multiple possible centers are sampled according to\n$D^2$-sampling and only the one that decreases the objective the most is chosen\nas a center for that iteration. It is stated as an open question whether this\nalso leads to an $O(\\log k)$-approximation (or even better). We show that this\nis not the case by presenting a family of instances on which greedy k-means++\nyields only an $\\Omega(\\ell\\cdot \\log k)$-approximation in expectation where\n$\\ell$ is the number of possible centers that are sampled in each iteration. We\nalso study a variation, which we call noisy k-means++ algorithm. In this\nvariation only one center is sampled in every iteration but not exactly by\n$D^2$-sampling anymore. Instead in each iteration an adversary is allowed to\nchange the probabilities arising from $D^2$-sampling individually for each\npoint by a factor between $1-\\epsilon_1$ and $1+\\epsilon_2$ for parameters\n$\\epsilon_1 \\in [0,1)$ and $\\epsilon_2 \\ge 0$. We prove that noisy k-means++\ncompute an $O(\\log^2 k)$-approximation in expectation. We also discuss some\napplications of this result.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:46:03 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Eube", "Jan", ""], ["R\u00f6glin", "Heiko", ""], ["Schmidt", "Melanie", ""]]}, {"id": "1912.00670", "submitter": "Vera Traub", "authors": "Vera Traub and Jens Vygen", "title": "An improved approximation algorithm for ATSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the constant-factor approximation algorithm for the asymmetric\ntraveling salesman problem by Svensson, Tarnawski, and V\\'egh. We improve on\neach part of this algorithm. We avoid the reduction to irreducible instances\nand thus obtain a simpler and much better reduction to vertebrate pairs. We\nalso show that a slight variant of their algorithm for vertebrate pairs has a\nmuch smaller approximation ratio. Overall we improve the approximation ratio\nfrom $506$ to $22+\\epsilon$ for any $\\epsilon > 0$. This also improves the\nupper bound on the integrality ratio from $319$ to $22$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:27:16 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 19:02:59 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Traub", "Vera", ""], ["Vygen", "Jens", ""]]}, {"id": "1912.00717", "submitter": "Mateusz Lewandowski", "authors": "Jaros{\\l}aw Byrka, Mateusz Lewandowski, Syed Mohammad Meesum, Joachim\n  Spoerhase, Sumedha Uniyal", "title": "PTAS for Steiner Tree on Map Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Steiner tree problem on map graphs, which substantially\ngeneralize planar graphs as they allow arbitrarily large cliques. We obtain a\nPTAS for Steiner tree on map graphs, which builds on the result for planar edge\nweighted instances of Borradaile et al. The Steiner tree problem on map graphs\ncan be casted as a special case of the planar node-weighted Steiner tree\nproblem, for which only a 2.4-approximation is known. We prove and use a\ncontraction decomposition theorem for planar node weighted instances. This\nreadily reduces the problem of finding a PTAS for planar node-weighted Steiner\ntree to finding a spanner, i.e., a constant-factor approximation containing a\nnearly optimum solution. Finally, we pin-point places where known techniques\nfor constructing such spanner fail on node weighted instances and further\nprogress requires new ideas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:32:12 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Lewandowski", "Mateusz", ""], ["Meesum", "Syed Mohammad", ""], ["Spoerhase", "Joachim", ""], ["Uniyal", "Sumedha", ""]]}, {"id": "1912.00770", "submitter": "Mateusz Lewandowski", "authors": "Jaros{\\l}aw Byrka and Mateusz Lewandowski", "title": "Concave connection cost Facility Location and the Star Inventory Routing\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the uncapacitated facility location problem (UFL),\nwhere connection costs of clients are defined by (client specific) concave\nnondecreasing functions of the connection distance in the underlying metric. A\nspecial case capturing the complexity of this variant is the setting called\nfacility location with penalties where clients may either connect to a facility\nor pay a (client specific) penalty. We show that the best known approximation\nalgorithms for UFL may be adapted to the concave connection cost setting. The\nkey technical contribution is an argument that the JMS algorithm for UFL may be\nadapted to provide the same approximation guarantee for the more general\nconcave connection cost variant. We also study the star inventory routing with\nfacility location (SIRPFL) problem that was recently introduced by Jiao and\nRavi, which asks to jointly optimize the task of clustering of demand points\nwith the later serving of requests within created clusters. We show that the\nproblem may be reduced to the concave connection cost facility location and\nsubstantially improve the approximation ratio for all three variants of SIRPFL.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:34:48 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 19:23:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Lewandowski", "Mateusz", ""]]}, {"id": "1912.00786", "submitter": "Tao Lin", "authors": "Xiaoming Li, Tao Lin", "title": "On Clearing Prices in Matching Markets: A Simple Characterization\n  without Duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duality of linear programming is a standard approach to the classical\nweighted maximum matching problem. From an economic perspective, the dual\nvariables can be regarded as prices of products and payoffs of buyers in a\ntwo-sided matching market. Traditional duality-based algorithms, e.g.,\nHungarian, essentially aims at finding a set of prices that clears the market.\nUnder such market-clearing prices, a maximum matching is formed when buyers buy\ntheir most preferred products respectively. We study the property of\nmarket-clearing prices without the use of duality, showing that: (1) the space\nof market-clearing prices is convex and closed under element-wise maximum and\nminimum operations; (2) any market-clearing prices induce all maximum\nmatchings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:56:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Li", "Xiaoming", ""], ["Lin", "Tao", ""]]}, {"id": "1912.00837", "submitter": "Xiaojin Zhang", "authors": "Xiaojin Zhang", "title": "Improved Algorithm for Tolerant Junta Testing", "comments": "Will refine this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of tolerant junta testing for boolean\nfunction. Compared with the prior work by Blais et al., we provide improved\nresults in terms of both the parameter gap and query complexity. Compared with\nthe prior work by De et al., our work did not close the parameter gap, but we\nremoved the $2^k$ term of the query complexity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:59:51 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 13:52:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Xiaojin", ""]]}, {"id": "1912.00948", "submitter": "Grzegorz Fabia\\'nski", "authors": "Grzegorz Fabia\\'nski", "title": "Properties of nowhere dense graph classes related to independent set\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A set is called r-independent, if every two vertices of it are in distance\ngreater then r. In the r-independent set problem with parameter k, we ask\nwhether in a given graph G there exists an r-independent set of size k. In this\nwork we present an algorithm for this problem, which applied to a graph from\nany fixed nowhere dense class, works in time bounded by f(k, r)*G, for some\nfunction f. We also present alternative algorithm, with running time bounded by\ng(k, r)*G, working on slightly more general classes of graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:46:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fabia\u0144ski", "Grzegorz", ""]]}, {"id": "1912.01059", "submitter": "Patrick Wieschollek", "authors": "Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P.A. Lensch", "title": "GGNN: Graph-based GPU Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor (ANN) search in high dimensions is an integral\npart of several computer vision systems and gains importance in deep learning\nwith explicit memory representations. Since PQT and FAISS started to leverage\nthe massive parallelism offered by GPUs, GPU-based implementations are a\ncrucial resource for today's state-of-the-art ANN methods. While most of these\nmethods allow for faster queries, less emphasis is devoted to accelerate the\nconstruction of the underlying index structures. In this paper, we propose a\nnovel search structure based on nearest neighbor graphs and information\npropagation on graphs. Our method is designed to take advantage of GPU\narchitectures to accelerate the hierarchical building of the index structure\nand for performing the query. Empirical evaluation shows that GGNN\nsignificantly surpasses the state-of-the-art GPU- and CPU-based systems in\nterms of build-time, accuracy and search speed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:46:13 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:15:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:49:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Groh", "Fabian", ""], ["Ruppert", "Lukas", ""], ["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1912.01081", "submitter": "Yoshiharu Kohayakawa", "authors": "Carlos Hoppen, Yoshiharu Kohayakawa, Richard Lang, Hanno Lefmann,\n  Henrique Stagni", "title": "On the query complexity of estimating the distance to hereditary graph\n  properties", "comments": "To appear in SIDMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a family of graphs $\\mathcal{F}$, we prove that the normalized edit\ndistance of any given graph $\\Gamma$ to being induced $\\mathcal{F}$-free is\nestimable with a query complexity that depends only on the bounds of the\nFrieze--Kannan Regularity Lemma and on a Removal Lemma for $\\mathcal{F}$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:22:21 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 19:18:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoppen", "Carlos", ""], ["Kohayakawa", "Yoshiharu", ""], ["Lang", "Richard", ""], ["Lefmann", "Hanno", ""], ["Stagni", "Henrique", ""]]}, {"id": "1912.01155", "submitter": "Matthew Groff S.", "authors": "Matt Groff", "title": "The Polynomial Transform", "comments": "6 pages, 3 figures, 1 algorithm figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.NT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We explore a new form of DFT, which we call the Polynomial Transform. It\nfunctions over finite fields, and a size $n$ transform takes $O(n)$ operations.\nIn the multitape Turing machine model, it allows us to multiply two $n$ bit\nnumbers in time $n(k^{\\log^*{n}} + \\log{p})$, where $k$ is a constant and\n$\\log^*{n}$ is the iterated logarithm. One important consequence is that the\nNetwork Coding Conjecture is false.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:24:19 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 04:57:23 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Groff", "Matt", ""]]}, {"id": "1912.01195", "submitter": "Vadim Grinberg", "authors": "Buddhima Gamlath, Vadim Grinberg", "title": "Approximating Star Cover Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a metric space $(F \\cup C, d)$, we consider star covers of $C$ with\nbalanced loads. A star is a pair $(f, C_f)$ where $f \\in F$ and $C_f \\subseteq\nC$, and the load of a star is $\\sum_{c \\in C_f} d(f, c)$. In minimum load\n$k$-star cover problem $(\\mathrm{MLkSC})$, one tries to cover the set of\nclients $C$ using $k$ stars that minimize the maximum load of a star, and in\nminimum size star cover $(\\mathrm{MSSC})$ one aims to find the minimum number\nof stars of load at most $T$ needed to cover $C$, where $T$ is a given\nparameter.\n  We obtain new bicriteria approximations for the two problems using novel\nrounding algorithms for their standard LP relaxations. For $\\mathrm{MLkSC}$, we\nfind a star cover with $(1+\\varepsilon)k$ stars and\n$O(1/\\varepsilon^2)\\mathrm{OPT}_{\\mathrm{MLk}}$ load where\n$\\mathrm{OPT}_{\\mathrm{MLk}}$ is the optimum load. For $\\mathrm{MSSC}$, we find\na star cover with $O(1/\\varepsilon^2) \\mathrm{OPT}_{\\mathrm{MS}}$ stars of load\nat most $(2 + \\varepsilon) T$ where $\\mathrm{OPT}_{\\mathrm{MS}}$ is the optimal\nnumber of stars for the problem. Previously, non-trivial bicriteria\napproximations were known only when $F = C$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:20:11 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Gamlath", "Buddhima", ""], ["Grinberg", "Vadim", ""]]}, {"id": "1912.01516", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Soft robust solutions to possibilistic optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a class of uncertain optimization problems, in which\nunknown parameters are modeled by fuzzy intervals. The membership functions of\nthe fuzzy intervals are interpreted as possibility distributions for the values\nof the uncertain parameters. It is shown how the known concepts of robustness\nand light robustness, for the interval uncertainty representation of the\nparameters, can be generalized to choose solutions under the assumed model of\nuncertainty in the possibilistic setting. Furthermore, these solutions can be\ncomputed efficiently for a wide class of problems, in particular for linear\nprogramming problems with fuzzy parameters in constraints and objective\nfunction. In this paper a theoretical framework is presented and results of\nsome computational tests are shown.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:42:47 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 06:43:14 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1912.01668", "submitter": "Jialin Ding", "authors": "Vikram Nathan, Jialin Ding, Mohammad Alizadeh, Tim Kraska", "title": "Learning Multi-dimensional Indexes", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3380579", "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scanning and filtering over multi-dimensional tables are key operations in\nmodern analytical database engines. To optimize the performance of these\noperations, databases often create clustered indexes over a single dimension or\nmulti-dimensional indexes such as R-trees, or use complex sort orders (e.g.,\nZ-ordering). However, these schemes are often hard to tune and their\nperformance is inconsistent across different datasets and queries. In this\npaper, we introduce Flood, a multi-dimensional in-memory index that\nautomatically adapts itself to a particular dataset and workload by jointly\noptimizing the index structure and data storage. Flood achieves up to three\norders of magnitude faster performance for range scans with predicates than\nstate-of-the-art multi-dimensional indexes or sort orders on real-world\ndatasets and workloads. Our work serves as a building block towards an\nend-to-end learned database system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:10:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nathan", "Vikram", ""], ["Ding", "Jialin", ""], ["Alizadeh", "Mohammad", ""], ["Kraska", "Tim", ""]]}, {"id": "1912.01698", "submitter": "Krishnakumar Balasubramanian", "authors": "Abhishek Roy, Yifang Chen, Krishnakumar Balasubramanian, Prasant\n  Mohapatra", "title": "Online and Bandit Algorithms for Nonstationary Stochastic Saddle-Point\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saddle-point optimization problems are an important class of optimization\nproblems with applications to game theory, multi-agent reinforcement learning\nand machine learning. A majority of the rich literature available for\nsaddle-point optimization has focused on the offline setting. In this paper, we\nstudy nonstationary versions of stochastic, smooth, strongly-convex and\nstrongly-concave saddle-point optimization problem, in both online (or\nfirst-order) and multi-point bandit (or zeroth-order) settings. We first\npropose natural notions of regret for such nonstationary saddle-point\noptimization problems. We then analyze extragradient and Frank-Wolfe\nalgorithms, for the unconstrained and constrained settings respectively, for\nthe above class of nonstationary saddle-point optimization problems. We\nestablish sub-linear regret bounds on the proposed notions of regret in both\nthe online and bandit setting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:52:38 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Roy", "Abhishek", ""], ["Chen", "Yifang", ""], ["Balasubramanian", "Krishnakumar", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1912.01781", "submitter": "Mithilesh Kumar", "authors": "Mithilesh Kumar", "title": "Faster Lattice Enumeration", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lattice reduction is an algorithm that transforms the given basis of the\nlattice to another lattice basis such that problems like finding a shortest\nvector and closest vector become easier to solve. Some of the famous lattice\nreduction algorithms are LLL and BKZ reductions. We define a class of bases\ncalled \\emph{obtuse bases} and show that any lattice basis can be transformed\nto an obtuse basis in $\\mathcal{O}(n^4)$ time. A shortest vector s can be\nwritten as $v_1b_1+\\cdots+v_nb_n$ where $b_1,\\dots,b_n$ are the input basis\nvectors and $v_1,\\dots,v_n$ are integers. When the input basis is obtuse, all\nthese integers can be chosen to be positive for a shortest vector. This\nproperty of the obtuse basis makes lattice enumeration algorithm for finding a\nshortest vector exponentially faster. Moreover, extreme pruning, the current\nfastest algorithm for lattice enumeration, can be run on an obtuse basis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:23:03 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Kumar", "Mithilesh", ""]]}, {"id": "1912.01854", "submitter": "Ulrike Schmidt-Kraepelin", "authors": "Telikepalli Kavitha, Tam\\'as Kir\\'aly, Jannik Matuschke, Ildik\\'o\n  Schlotter and Ulrike Schmidt-Kraepelin", "title": "Popular Branchings and Their Dual Certificates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a digraph where every node has preferences over its incoming\nedges. The preferences of a node extend naturally to preferences over\nbranchings, i.e., directed forests; a branching $B$ is popular if $B$ does not\nlose a head-to-head election (where nodes cast votes) against any branching.\nSuch popular branchings have a natural application in liquid democracy. The\npopular branching problem is to decide if $G$ admits a popular branching or\nnot. We give a characterization of popular branchings in terms of dual\ncertificates and use this characterization to design an efficient combinatorial\nalgorithm for the popular branching problem. When preferences are weak\nrankings, we use our characterization to formulate the popular branching\npolytope in the original space and also show that our algorithm can be modified\nto compute a branching with least unpopularity margin. When preferences are\nstrict rankings, we show that \"approximately popular\" branchings always exist.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:08:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Kavitha", "Telikepalli", ""], ["Kir\u00e1ly", "Tam\u00e1s", ""], ["Matuschke", "Jannik", ""], ["Schlotter", "Ildik\u00f3", ""], ["Schmidt-Kraepelin", "Ulrike", ""]]}, {"id": "1912.01861", "submitter": "Siddharth Dawar", "authors": "Anuj S. Saxena, Siddharth Dawar, Vikram Goyal, Debajyoti Bera", "title": "Mining Top-k Trajectory-Patterns from Anonymized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ubiquity of GPS enabled devices result into the generation of an enormous\namount of user movement data consisting of a sequence of locations together\nwith activities performed at those locations. Such data, commonly known as {\\it\nactivity-trajectory data}, can be analysed to find common user movements and\npreferred activities, which will have tremendous business value. However,\nvarious countries have now introduced data privacy regulations that make it\nmandatory to anonymize any data before releasing it. This makes it difficult to\nlook for patterns as the existing mining techniques may not be directly\napplicable over anonymized data. User locations in an activity-trajectory\ndataset are anonymized to regions of different shapes and sizes making them\nuncomparable; therefore, it is unclear how to define suitable patterns over\nthose regions. In this paper, we propose a top-k pattern mining technique\ncalled TopKMintra that employs a pre-processing step to transform anonymized\nactivity-trajectory into an intermediate representation that address the above\nproblem. Unlike usual sequential data, activity-trajectory data is\n2-dimensional that can lead to generation of duplicate patterns. To handle\nthis, TopKMintra restricts arbitrary extensions in the two dimensions by\nimposing an order over the search space; this also helps in addressing the\ncommon problem of updating the threshold in top-k pattern mining algorithms\nduring various stages. We perform extensive experiments on real datasets to\ndemonstrate the efficiency and the effectiveness of TopKMintra. Our results\nshow that even after anonymization, certain patterns may remain in a dataset\nand those could be mined efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:19:07 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Saxena", "Anuj S.", ""], ["Dawar", "Siddharth", ""], ["Goyal", "Vikram", ""], ["Bera", "Debajyoti", ""]]}, {"id": "1912.01934", "submitter": "Roni Zoller", "authors": "Roni Zoller, Meirav Zehavi and Michal Ziv-Ukelson", "title": "A New Paradigm for Identifying Reconciliation-Scenario Altering\n  Mutations Conferring Environmental Adaptation", "comments": "A conference version of this paper appeared in WABI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal in microbial computational genomics is to identify crucial\nevents in the evolution of a gene that severely alter the duplication, loss and\nmobilization patterns of the gene within the genomes in which it disseminates.\nIn this paper, we formalize this microbiological goal as a new pattern-matching\nproblem in the domain of Gene tree and Species tree reconciliation, denoted\n\"Reconciliation-Scenario Altering Mutation (RSAM) Discovery\". We propose an\n$O(m\\cdot n\\cdot k)$ time algorithm to solve this new problem, where $m$ and\n$n$ are the number of vertices of the input Gene tree and Species tree,\nrespectively, and $k$ is a user-specified parameter that bounds from above the\nnumber of optimal solutions of interest. The algorithm first constructs a\nhypergraph representing the $k$ highest scoring reconciliation scenarios\nbetween the given Gene tree and Species tree, and then interrogates this\nhypergraph for subtrees matching a pre-specified RSAM Pattern. Our algorithm is\noptimal in the sense that the number of hypernodes in the hypergraph can be\nlower bounded by $\\Omega(m\\cdot n\\cdot k)$. We implement the new algorithm as a\ntool, called RSAM-finder, and demonstrate its application to -the\nidentification of RSAMs in toxins and drug resistance elements across a dataset\nspanning hundreds of species.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 12:47:46 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zoller", "Roni", ""], ["Zehavi", "Meirav", ""], ["Ziv-Ukelson", "Michal", ""]]}, {"id": "1912.01990", "submitter": "Geevarghese Philip", "authors": "Geevarghese Philip and Rani M. R. and Subashini R", "title": "On Computing the Hamiltonian Index of Graphs", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $r$-th iterated line graph $L^{r}(G)$ of a graph $G$ is defined by: (i)\n$L^{0}(G) = G$ and (ii) $L^{r}(G) = L(L^{(r- 1)}(G))$ for $r > 0$, where $L(G)$\ndenotes the line graph of $G$. The Hamiltonian Index $h(G)$ of $G$ is the\nsmallest $r$ such that $L^{r}(G)$ has a Hamiltonian cycle. Checking if $h(G) =\nk$ is NP-hard for any fixed integer $k \\geq 0$ even for subcubic graphs $G$. We\nstudy the parameterized complexity of this problem with the parameter\ntreewidth, $tw(G)$, and show that we can find $h(G)$ in time $O*((1 +\n2^{(\\omega + 3)})^{tw(G)})$ where $\\omega$ is the matrix multiplication\nexponent and the $O*$ notation hides polynomial factors in input size.\n  The NP-hard Eulerian Steiner Subgraph problem takes as input a graph $G$ and\na specified subset $K$ of terminal vertices of $G$ and asks if $G$ has an\nEulerian (that is: connected, and with all vertices of even degree.) subgraph\n$H$ containing all the terminals. A second result (and a key ingredient of our\nalgorithm for finding $h(G)$) in this work is an algorithm which solves\nEulerian Steiner Subgraph in $O*((1 + 2^{(\\omega + 3)})^{tw(G)})$ time.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:58:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Philip", "Geevarghese", ""], ["R.", "Rani M.", ""], ["R", "Subashini", ""]]}, {"id": "1912.02217", "submitter": "Pedro Mirabal", "authors": "P. Mirabal, J. Abreu, D. Seco", "title": "Assessing the best edit in perturbation-based iterative refinement\n  algorithms to compute the median string", "comments": "14 pages, 4 figures", "journal-ref": "Pattern Recognition Letters, Volume 120, 1 April 2019, Pages\n  104-111", "doi": "10.1016/j.patrec.2019.02.004", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strings are a natural representation of biological data such as DNA, RNA and\nprotein sequences. The problem of finding a string that summarizes a set of\nsequences has direct application in relative compression algorithms for genome\nand proteome analysis, where reference sequences need to be chosen. Median\nstrings have been used as representatives of a set of strings in different\ndomains. However, several formulations of those problems are NP-Complete.\nAlternatively, heuristic approaches that iteratively refine an initial coarse\nsolution by applying edit operations have been proposed. Recently, we\ninvestigated the selection of the optimal edit operations to speed up\nconvergence without spoiling the quality of the approximated median string. We\npropose a novel algorithm that outperforms state of the art heuristic\napproximations to the median string in terms of convergence speed by estimating\nthe effect of a perturbation in the minimization of the expressions that define\nthe median strings. We present corpus of comparative experiments to validate\nthese results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 19:09:15 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Mirabal", "P.", ""], ["Abreu", "J.", ""], ["Seco", "D.", ""]]}, {"id": "1912.02278", "submitter": "Ivor van der Hoog", "authors": "Jeff Erickson, Ivor van der Hoog, Tillmann Miltzow", "title": "Smoothing the gap between NP and ER", "comments": "43 pages, 13 figures, accepted to FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithmic problems that belong to the complexity class of the\nexistential theory of the reals (ER). A problem is ER-complete if it is as hard\nas the problem ETR and if it can be written as an ETR formula. Traditionally,\nthese problems are studied in the real RAM, a model of computation that assumes\nthat the storage and comparison of real-valued numbers can be done in constant\nspace and time, with infinite precision. The complexity class ER is often\ncalled a real RAM analogue of NP, since the problem ETR can be viewed as the\nreal-valued variant of SAT.\n  In this paper we prove a real RAM analogue to the Cook-Levin theorem which\nshows that ER membership is equivalent to having a verification algorithm that\nruns in polynomial-time on a real RAM. This gives an easy proof of\nER-membership, as verification algorithms on a real RAM are much more versatile\nthan ETR-formulas. We use this result to construct a framework to study\nER-complete problems under smoothed analysis. We show that for a wide class of\nER-complete problems, its witness can be represented with logarithmic\ninput-precision by using smoothed analysis on its real RAM verification\nalgorithm. This shows in a formal way that the boundary between NP and ER\n(formed by inputs whose solution witness needs high input-precision) consists\nof contrived input. We apply our framework to well-studied ER-complete\nrecognition problems which have the exponential bit phenomenon such as the\nrecognition of realizable order types or the Steinitz problem in fixed\ndimension.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:12:17 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 11:28:42 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Erickson", "Jeff", ""], ["van der Hoog", "Ivor", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "1912.02283", "submitter": "Benjamin Coleman", "authors": "Benjamin Coleman, Anshumali Shrivastava", "title": "Sub-linear RACE Sketches for Approximate Kernel Density Estimation on\n  Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation is a simple and effective method that lies at the\nheart of many important machine learning applications. Unfortunately, kernel\nmethods scale poorly for large, high dimensional datasets. Approximate kernel\ndensity estimation has a prohibitively high memory and computation cost,\nespecially in the streaming setting. Recent sampling algorithms for high\ndimensional densities can reduce the computation cost but cannot operate\nonline, while streaming algorithms cannot handle high dimensional datasets due\nto the curse of dimensionality. We propose RACE, an efficient sketching\nalgorithm for kernel density estimation on high-dimensional streaming data.\nRACE compresses a set of N high dimensional vectors into a small array of\ninteger counters. This array is sufficient to estimate the kernel density for a\nlarge class of kernels. Our sketch is practical to implement and comes with\nstrong theoretical guarantees. We evaluate our method on real-world\nhigh-dimensional datasets and show that our sketch achieves 10x better\ncompression compared to competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 22:17:36 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Coleman", "Benjamin", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1912.02430", "submitter": "Ondrej Benedikt", "authors": "Ond\\v{r}ej Benedikt and Istv\\'an M\\'odos and Zden\\v{e}k Hanz\\'alek", "title": "Power of Pre-Processing: Production Scheduling with Variable Energy\n  Pricing and Power-Saving States", "comments": null, "journal-ref": "Constraints volume 25, pages 300-318 (2020)", "doi": "10.1007/s10601-020-09317-y", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a single machine scheduling problem with non-preemptive\njobs to minimize the total electricity cost. Two latest trends in the area of\nthe energy-aware scheduling are considered, namely the variable energy pricing\nand the power-saving states of a machine. Scheduling of the jobs and the\nmachine states are considered jointly to achieve the highest possible savings.\nAlthough this problem has been previously addressed in the literature, the\nreported results of the state-of-the-art method show that the optimal solutions\ncan be found only for instances with up to 35 jobs and 209 intervals within 3\nhours of computation. We propose an elegant pre-processing technique called\nSPACES for computing the optimal switching of the machine states with respect\nto the energy costs. The optimal switchings are associated with the shortest\npaths in an interval-state graph that describes all possible transitions\nbetween the machine states in time. This idea allows us to implement efficient\ninteger linear programming and constraint programming models of the problem\nwhile preserving the optimality. The efficiency of the models lies in the\nsimplification of the optimal switching representation. The results of the\nexperiments show that our approach outperforms the existing state-of-the-art\nexact method. On a set of benchmark instances with varying sizes and different\nstate transition graphs, the proposed approach finds the optimal solutions even\nfor the large instances with up to 190 jobs and 1277 intervals within an hour\nof computation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 08:30:13 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 07:12:03 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Benedikt", "Ond\u0159ej", ""], ["M\u00f3dos", "Istv\u00e1n", ""], ["Hanz\u00e1lek", "Zden\u011bk", ""]]}, {"id": "1912.02728", "submitter": "Xi Li", "authors": "Xi Li, Mingyou Wu and Hanwu Chen", "title": "Algorithm for Finding the Maximum Clique Based on Continuous Time\n  Quantum Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the application of continuous time quantum\nwalking(CTQW) to the Maximum Clique(MC) Problem. Performing CTQW on graphs will\ngenerate distinct periodic probability amplitude for different vertices. We\nwill show that the intensity of the probability amplitude at frequency indeed\nimplies the clique structure of some special kinds of graph. And recursive\nalgorithms with time complexity $O(N^5)$ in classical computers for finding the\nmaximum clique are proposed. We have experimented on random graphs where each\nedge exists with probabilities 0.3, 0.5 and 0.7. Although counter examples are\nnot found for random graphs, whether these algorithms are universal is not\nknown to us.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:08:37 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 07:48:56 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 08:09:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Li", "Xi", ""], ["Wu", "Mingyou", ""], ["Chen", "Hanwu", ""]]}, {"id": "1912.02814", "submitter": "Yannic Maus", "authors": "Philipp Bamberger, Fabian Kuhn, Yannic Maus", "title": "Efficient Deterministic Distributed Coloring with Small Bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the $(degree+1)$-list coloring problem can be solved\ndeterministically in $O(D \\cdot \\log n \\cdot\\log^2\\Delta)$ rounds in the\n\\CONGEST model, where $D$ is the diameter of the graph, $n$ the number of\nnodes, and $\\Delta$ the maximum degree. Using the recent polylogarithmic-time\ndeterministic network decomposition algorithm by Rozho\\v{n} and Ghaffari [STOC\n2020], this implies the first efficient (i.e., $\\poly\\log n$-time)\ndeterministic \\CONGEST algorithm for the $(\\Delta+1)$-coloring and the\n$(\\mathit{degree}+1)$-list coloring problem. Previously the best known\nalgorithm required $2^{O(\\sqrt{\\log n})}$ rounds and was not based on network\ndecompositions.\n  Our techniques also lead to deterministic $(\\mathit{degree}+1)$-list coloring\nalgorithms for the congested clique and the massively parallel computation\n(MPC) model. For the congested clique, we obtain an algorithm with time\ncomplexity $O(\\log\\Delta\\cdot\\log\\log\\Delta)$, for the MPC model, we obtain\nalgorithms with round complexity $O(\\log^2\\Delta)$ for the linear-memory regime\nand $O(\\log^2\\Delta + \\log n)$ for the sublinear memory regime.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:57:01 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 16:09:22 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 07:10:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bamberger", "Philipp", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1912.02820", "submitter": "Vikram Sharma", "authors": "Prashant Batra, Vikram Sharma", "title": "Complexity of a Root Clustering Algorithm", "comments": "52 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating the roots of a holomorphic function in an input box is a\nfundamental problem in many domains. Most algorithms in the literature for\nsolving this problem are conditional, i.e., they make some simplifying\nassumptions, such as, all the roots are simple or there are no roots on the\nboundary of the input box, or the underlying machine model is Real RAM. Root\nclustering is a generalization of the root approximation problem that allows\nfor errors in the computation and makes no assumption on the multiplicity of\nthe roots. An unconditional algorithm for computing a root clustering of a\nholomorphic function was given by Yap, Sagraloff and Sharma in 2013. They\nproposed a subdivision based algorithm using effective predicates based on\nPellet's test while avoiding any comparison with zeros (using soft zero\ncomparisons instead). In this paper, we analyze the running time of their\nalgorithm. We use the continuous amortization framework to derive an upper\nbound on the size of the subdivision tree. We specialize this bound to the case\nof polynomials and some simple transcendental functions such as exponential and\ntrigonometric sine. We show that the algorithm takes exponential time even for\nthese simple functions, unlike the case of polynomials. We also derive a bound\non the bit-precision used by the algorithm. To the best of our knowledge, this\nis the first such result for holomorphic functions. We introduce new geometric\nparameters, such as the relative growth of the function on the input box, for\nanalyzing the algorithm. Thus, our estimates naturally generalize the known\nresults, i.e., for the case of polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 08:59:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Batra", "Prashant", ""], ["Sharma", "Vikram", ""]]}, {"id": "1912.02858", "submitter": "Victor Lecomte", "authors": "Victor Lecomte and Omri Weinstein", "title": "Settling the relationship between Wilber's bounds for dynamic optimality", "comments": "ESA 2020; 25 pages, 18 figures; v3 applies reviewers' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In FOCS 1986, Wilber proposed two combinatorial lower bounds on the\noperational cost of any binary search tree (BST) for a given access sequence $X\n\\in [n]^m$. Both bounds play a central role in the ongoing pursuit of the\ndynamic optimality conjecture (Sleator and Tarjan, 1985), but their\nrelationship remained unknown for more than three decades. We show that\nWilber's Funnel bound dominates his Alternation bound for all $X$, and give a\ntight $\\Theta(\\lg\\lg n)$ separation for some $X$, answering Wilber's conjecture\nand an open problem of Iacono, Demaine et. al. The main ingredient of the proof\nis a new \"symmetric\" characterization of Wilber's Funnel bound, which proves\nthat it is invariant under rotations of $X$. We use this characterization to\nprovide initial indication that the Funnel bound matches the Independent\nRectangle bound (Demaine et al., 2009), by proving that when the Funnel bound\nis constant, $\\mathsf{IRB}_{\\diagup\\hspace{-.6em}\\square}$ is linear. To the\nbest of our knowledge, our results provide the first progress on Wilber's\nconjecture that the Funnel bound is dynamically optimal (1986).\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:17:15 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 20:49:57 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 20:51:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lecomte", "Victor", ""], ["Weinstein", "Omri", ""]]}, {"id": "1912.02900", "submitter": "Thatchaphol Saranurak", "authors": "Parinya Chalermsook, Julia Chuzhoy, Thatchaphol Saranurak", "title": "Pinning Down the Strong Wilber 1 Bound for Binary Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic optimality conjecture, postulating the existence of an\n$O(1)$-competitive online algorithm for binary search trees (BSTs), is among\nthe most fundamental open problems in dynamic data structures. Despite\nextensive work and some notable progress, including, for example, the Tango\nTrees (Demaine et al., FOCS 2004), that give the best currently known $O(\\log\n\\log n)$-competitive algorithm, the conjecture remains widely open. One of the\nmain hurdles towards settling the conjecture is that we currently do not have\napproximation algorithms achieving better than an $O(\\log \\log\nn)$-approximation, even in the offline setting. All known non-trivial\nalgorithms for BST's so far rely on comparing the algorithm's cost with the\nso-called Wilber's first bound (WB-1). Therefore, establishing the worst-case\nrelationship between this bound and the optimal solution cost appears crucial\nfor further progress, and it is an interesting open question in its own right.\n  Our contribution is two-fold. First, we show that the gap between the WB-1\nbound and the optimal solution value can be as large as $\\Omega(\\log \\log n/\n\\log \\log \\log n)$; in fact, the gap holds even for several stronger variants\nof the bound. Second, we provide a simple algorithm, that, given an integer\n$D>0$, obtains an $O(D)$-approximation in time $\\exp\\left(O\\left\n(n^{1/2^{\\Omega(D)}}\\log n\\right )\\right )$. In particular, this gives a\nconstant-factor approximation sub-exponential time algorithm. Moreover, we\nobtain a simpler and cleaner efficient $O(\\log \\log n)$-approximation algorithm\nthat can be used in an online setting. Finally, we suggest a new bound, that we\ncall {\\em Guillotine Bound}, that is stronger than WB, while maintaining its\nalgorithm-friendly nature, that we hope will lead to better algorithms. All our\nresults use the geometric interpretation of the problem, leading to cleaner and\nsimpler analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 22:13:34 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 20:45:16 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Chuzhoy", "Julia", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1912.02938", "submitter": "Akshay Kamath", "authors": "Akshay Kamath and Sushrut Karmalkar and Eric Price", "title": "Lower Bounds for Compressed Sensing with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of compressed sensing is to learn a structured signal $x$ from a\nlimited number of noisy linear measurements $y \\approx Ax$. In traditional\ncompressed sensing, \"structure\" is represented by sparsity in some known basis.\nInspired by the success of deep learning in modeling images, recent work\nstarting with~\\cite{BJPD17} has instead considered structure to come from a\ngenerative model $G: \\mathbb{R}^k \\to \\mathbb{R}^n$. We present two results\nestablishing the difficulty of this latter task, showing that existing bounds\nare tight. First, we provide a lower bound matching the~\\cite{BJPD17} upper\nbound for compressed sensing from $L$-Lipschitz generative models $G$. In\nparticular, there exists such a function that requires roughly $\\Omega(k \\log\nL)$ linear measurements for sparse recovery to be possible. This holds even for\nthe more relaxed goal of \\emph{nonuniform} recovery. Second, we show that\ngenerative models generalize sparsity as a representation of structure. In\nparticular, we construct a ReLU-based neural network $G: \\mathbb{R}^{2k} \\to\n\\mathbb{R}^n$ with $O(1)$ layers and $O(kn)$ activations per layer, such that\nthe range of $G$ contains all $k$-sparse vectors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:51:51 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kamath", "Akshay", ""], ["Karmalkar", "Sushrut", ""], ["Price", "Eric", ""]]}, {"id": "1912.02953", "submitter": "Diego Maldonado", "authors": "Eric Goles, Diego Maldonado, Pedro Montealegre, Nicolas Ollinger", "title": "On the Complexity of the Stability Problem of Binary Freezing Totalistic\n  Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the family of two-state Totalistic Freezing Cellular\nAutomata (TFCA) defined over the triangular and square grids with von Neumann\nneighborhoods. We say that a Cellular Automaton is Freezing and Totalistic if\nthe active cells remain unchanged, and the new value of an inactive cell\ndepends only on the sum of its active neighbors. We classify all the Cellular\nAutomata in the class of TFCA, grouping them in five different classes: the\nTrivial rules, Turing Universal rules,Algebraic rules, Topological rules and\nFractal Growing rules. At the same time, we study in this family the Stability\nproblem, consisting in deciding whether an inactive cell becomes active, given\nan initial configuration.We exploit the properties of the automata in each\ngroup to show that:\n  - For Algebraic and Topological Rules the Stability problem is in\n$\\text{NC}$.\n  - For Turing Universal rules the Stability problem is $\\text{P}$-Complete.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 02:39:31 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Goles", "Eric", ""], ["Maldonado", "Diego", ""], ["Montealegre", "Pedro", ""], ["Ollinger", "Nicolas", ""]]}, {"id": "1912.03042", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Jane Lange, Li-Yang Tan", "title": "Constructive derandomization of query algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient deterministic algorithms for converting randomized query\nalgorithms into deterministic ones. We first give an algorithm that takes as\ninput a randomized $q$-query algorithm $R$ with description length $N$ and a\nparameter $\\varepsilon$, runs in time $\\mathrm{poly}(N) \\cdot\n2^{O(q/\\varepsilon)}$, and returns a deterministic $O(q/\\varepsilon)$-query\nalgorithm $D$ that $\\varepsilon$-approximates the acceptance probabilities of\n$R$. These parameters are near-optimal: runtime $N + 2^{\\Omega(q/\\varepsilon)}$\nand query complexity $\\Omega(q/\\varepsilon)$ are necessary.\n  Next, we give algorithms for instance-optimal and online versions of the\nproblem:\n  $\\circ$ Instance optimal: Construct a deterministic $q^\\star_R$-query\nalgorithm $D$, where $q^\\star_R$ is minimum query complexity of any\ndeterministic algorithm that $\\varepsilon$-approximates $R$.\n  $\\circ$ Online: Deterministically approximate the acceptance probability of\n$R$ for a specific input $\\underline{x}$ in time\n$\\mathrm{poly}(N,q,1/\\varepsilon)$, without constructing $D$ in its entirety.\n  Applying the techniques we develop for these extensions, we constructivize\nclassic results that relate the deterministic, randomized, and quantum query\ncomplexities of boolean functions (Nisan, STOC 1989; Beals et al., FOCS 1998).\nThis has direct implications for the Turing machine model of computation:\nsublinear-time algorithms for total decision problems can be efficiently\nderandomized and dequantized with a subexponential-time preprocessing step.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:41:10 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1912.03088", "submitter": "Bertrand Simon", "authors": "Vincent Fagnon, Imed Kacem, Giorgio Lucarelli and Bertrand Simon", "title": "Scheduling on Hybrid Platforms: Improved Approximability Window", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern platforms are using accelerators in conjunction with standard\nprocessing units in order to reduce the running time of specific operations,\nsuch as matrix operations, and improve their performance. Scheduling on such\nhybrid platforms is a challenging problem since the algorithms used for the\ncase of homogeneous resources do not adapt well. In this paper we consider the\nproblem of scheduling a set of tasks subject to precedence constraints on\nhybrid platforms, composed of two types of processing units. We propose a\n$(3+2\\sqrt{2})$-approximation algorithm and a conditional lower bound of 3 on\nthe approximation ratio. These results improve upon the 6-approximation\nalgorithm proposed by Kedad-Sidhoum et al. as well as the lower bound of 2 due\nto Svensson for identical machines. Our algorithm is inspired by the former one\nand distinguishes the allocation and the scheduling phases. However, we propose\na different allocation procedure which, although is less efficient for the\nallocation sub-problem, leads to an improved approximation ratio for the whole\nscheduling problem. This approximation ratio actually decreases when the number\nof processing units of each type is close and matches the conditional lower\nbound when they are equal.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 12:41:24 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 15:27:38 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Fagnon", "Vincent", ""], ["Kacem", "Imed", ""], ["Lucarelli", "Giorgio", ""], ["Simon", "Bertrand", ""]]}, {"id": "1912.03185", "submitter": "Celine Swennenhuis", "authors": "Jesper Nederlof, C\\'eline Swennenhuis", "title": "Parameterized Complexity of Partial Scheduling", "comments": "22 pages, 3 figues. Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a natural variant of scheduling that we call \\emph{partial\nscheduling}: In this variant an instance of a scheduling problem along with an\ninteger $k$ is given and one seeks an optimal schedule where not all, but only\n$k$ jobs, have to be processed.\n  Specifically, we aim to determine the fine-grained parameterized complexity\nof partial scheduling problems parameterized by $k$ for all variants of\nscheduling problems that minimize the makespan and involve unit/arbitrary\nprocessing times, identical/unrelated parallel machines, release/due dates, and\nprecedence constraints. That is, we investigate whether algorithms with\nruntimes of the type $f(k)n^{\\mathcal{O}(1)}$ or $n^{\\mathcal{O}(f(k))}$ exist\nfor a function $f$ that is as small as possible.\n  Our contribution is two-fold: First, we categorize each variant to be either\nin $\\mathsf{P}$, $\\mathsf{NP}$-complete and fixed-parameter tractable by $k$,\nor $\\mathsf{W}[1]$-hard parameterized by $k$. Second, for many interesting\ncases we further investigate the run time on a finer scale and obtain run times\nthat are (almost) optimal assuming the Exponential Time Hypothesis. As one of\nour main technical contributions, we give an $\\mathcal{O}(8^kk(|V|+|E|))$ time\nalgorithm to solve instances of partial scheduling problems minimizing the\nmakespan with unit length jobs, precedence constraints and release dates, where\n$G=(V,E)$ is the graph with precedence constraints.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:32:29 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 14:13:21 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Nederlof", "Jesper", ""], ["Swennenhuis", "C\u00e9line", ""]]}, {"id": "1912.03309", "submitter": "Wei-Chang Yeh", "authors": "Zhifeng Hao, Wei-Chang Yeh, Zhenyao Liu", "title": "General Multi-State Rework Network and Reliability Algorithm", "comments": "28 pages", "journal-ref": null, "doi": "10.1016/j.ress.2020.107048", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rework network is a common manufacturing system, in which flows (products)\nare processed in a sequence of workstations (nodes), which often results in\ndefective products. To improve the productivity and utility of the system, the\nrework network allows some of the defective products to go back to the \"as\nnormal\" condition after the rework process. In a recent study, Song proposed an\nalgorithm to correct more than 21 archive publications regarding the rework\nnetwork reliability problem, which is an important real-life problem. However,\nwe prove that Song's proposed algorithm is still incorrect. Additionally, we\nprovide an accurate general model based on the novel state distribution with a\nsmaller number of limitations. Furthermore, we propose an algorithm to\ncalculate the reliability of the multi-state rework networks using the proposed\nnovel state distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 22:52:27 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hao", "Zhifeng", ""], ["Yeh", "Wei-Chang", ""], ["Liu", "Zhenyao", ""]]}, {"id": "1912.03350", "submitter": "Sahil Singla", "authors": "Nikhil Bansal, Haotian Jiang, Sahil Singla, Makrand Sinha", "title": "Online Vector Balancing and Geometric Discrepancy", "comments": "Appears in STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online vector balancing question where $T$ vectors, chosen\nfrom an arbitrary distribution over $[-1,1]^n$, arrive one-by-one and must be\nimmediately given a $\\pm$ sign. The goal is to keep the discrepancy small as\npossible. A concrete example is the online interval discrepancy problem where T\npoints are sampled uniformly in [0,1], and the goal is to immediately color\nthem $\\pm$ such that every sub-interval remains nearly balanced. As random\ncoloring incurs $\\Omega(T^{1/2})$ discrepancy, while the offline bounds are\n$\\Theta(\\sqrt{n \\log (T/n)})$ for vector balancing and $1$ for interval\nbalancing, a natural question is whether one can (nearly) match the offline\nbounds in the online setting for these problems. One must utilize the\nstochasticity as in the worst-case scenario it is known that discrepancy is\n$\\Omega(T^{1/2})$ for any online algorithm.\n  Bansal and Spencer recently show an $O(\\sqrt{n}\\log T)$ bound when each\ncoordinate is independent. When there are dependencies among the coordinates,\nthe problem becomes much more challenging, as evidenced by a recent work of\nJiang, Kulkarni, and Singla that gives a non-trivial $O(T^{1/\\log\\log T})$\nbound for online interval discrepancy. Although this beats random coloring, it\nis still far from the offline bound.\n  In this work, we introduce a new framework for online vector balancing when\nthe input distribution has dependencies across coordinates. This lets us obtain\na $poly(n, \\log T)$ bound for online vector balancing under arbitrary input\ndistributions, and a $poly(\\log T)$ bound for online interval discrepancy. Our\nframework is powerful enough to capture other well-studied geometric\ndiscrepancy problems; e.g., a $poly(\\log^d (T))$ bound for the online\n$d$-dimensional Tusn\\'ady's problem. A key new technical ingredient is an\n{anti-concentration} inequality for sums of pairwise uncorrelated random\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:45:49 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 00:24:42 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bansal", "Nikhil", ""], ["Jiang", "Haotian", ""], ["Singla", "Sahil", ""], ["Sinha", "Makrand", ""]]}, {"id": "1912.03501", "submitter": "Martin Kouteck\\'y", "authors": "Cornelius Brand, Martin Kouteck\\'y, Sebastian Ordyniak", "title": "Parameterized Algorithms for MILPs with Small Treedepth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving (mixed) integer linear programs, (M)ILPs for short, is a fundamental\noptimization task. While hard in general, recent years have brought about vast\nprogress for solving structurally restricted, (non-mixed) ILPs: $n$-fold,\ntree-fold, 2-stage stochastic and multi-stage stochastic programs admit\nefficient algorithms, and all of these special cases are subsumed by the class\nof ILPs of small treedepth.\n  In this paper, we extend this line of work to the mixed case, by showing an\nalgorithm solving MILP in time $f(a,d) \\textrm{poly}(n)$, where $a$ is the\nlargest coefficient of the constraint matrix, $d$ is its treedepth, and $n$ is\nthe number of variables.\n  This is enabled by proving bounds on the denominators of the vertices of\nbounded-treedepth (non-integer) linear programs. We do so by carefully\nanalyzing the inverses of invertible submatrices of the constraint matrix. This\nallows us to afford scaling up the mixed program to the integer grid, and\napplying the known methods for integer programs.\n  We trace the limiting boundary of our approach, showing that naturally\nrelated classes of linear programs have vertices of unbounded fractionality.\nFinally, we show that restricting the structure of only the integral variables\nin the constraint matrix does not yield tractable special cases.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 13:22:50 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Brand", "Cornelius", ""], ["Kouteck\u00fd", "Martin", ""], ["Ordyniak", "Sebastian", ""]]}, {"id": "1912.03526", "submitter": "Dagang Li", "authors": "Shuhao Sun, Dagang Li", "title": "Flattened Exponential Histogram for Sliding Window Queries over Data\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Basic Counting problem [1] is one of the most fundamental and critical\nstreaming problems of sliding window queries over data streams. Given a stream\nof 0's and 1's, the purpose of this problem is to estimate the number of 1's in\nthe last N elements (or time units) seen from the stream. Its solution can be\nused as building blocks to solve numerous more complex problems such as heavy\nhitter, frequency estimation, distinct counting, etc. In this paper, we present\nthe flattened exponential histogram (FEH) model for the Basic Counting problem.\nOur model improves over the exponential histogram [1], [2], a well-received\ndeterministic technique for Basic Counting problem, with respect to accuracy\nand memory utilization most of the time in practice. Extensive experimental\nresults on real-world datasets show that with the same memory footprint, the\naccuracy of our model is between 4 to 15 and on average 7 times better than\nthat of the exponential histogram, while the speed is roughly the same.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 16:45:49 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sun", "Shuhao", ""], ["Li", "Dagang", ""]]}, {"id": "1912.03700", "submitter": "Dibyendu Das", "authors": "Dibyendu Das, Shahid Asghar Ahmad, Kumar Venkataramanan", "title": "Deep Learning-based Hybrid Graph-Coloring Algorithm for Register\n  Allocation", "comments": "11 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Register allocation, which is a crucial phase of a good optimizing compiler,\nrelies on graph coloring. Hence, an efficient graph coloring algorithm is of\nparamount importance. In this work we try to learn a good heuristic for\ncoloring interference graphs that are used in the register allocation phase. We\naim to handle moderate sized interference graphs which have 100 nodes or less.\nFor such graphs we can get the optimal allocation of colors to the nodes. Such\noptimal coloring is then used to train our Deep Learning network which is based\non several layers of LSTM that output a color for each node of the graph.\nHowever, the current network may allocate the same color to the nodes connected\nby an edge resulting in an invalid coloring of the interference graph. Since it\nis difficult to encode constraints in an LSTM to avoid invalid coloring, we\naugment our deep learning network with a color correction phase that runs after\nthe colors have been allocated by the network. Thus, our algorithm is hybrid in\nnature consisting of a mix of a deep learning algorithm followed by a more\ntraditional correction phase. We have trained our network using several\nthousand random graphs of varying sparsity. On application of our hybrid\nalgorithm to various popular graphs found in literature we see that our\nalgorithm does very well when compared to the optimal coloring of these graphs.\nWe have also run our algorithm against LLVMs popular greedy register allocator\nfor several SPEC CPU 2017 benchmarks and notice that the hybrid algorithm\nperforms on par or better than such a well-tuned allocator for most of these\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 15:36:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Das", "Dibyendu", ""], ["Ahmad", "Shahid Asghar", ""], ["Venkataramanan", "Kumar", ""]]}, {"id": "1912.03727", "submitter": "Andrea Tagarelli", "authors": "Antonio Cali\\`o, Andrea Tagarelli", "title": "Monotone Submodular Diversity functions for Categorical Vectors with\n  Application to Diversification of Seeds for Targeted Influence Maximization", "comments": "Initially conceived: October 2018. First article-version: February 1,\n  2019. Last update: September 11, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding diversity into knowledge discovery tasks is of crucial importance\nto enhance the meaningfulness of the mined patterns with high-impact aspects\nrelated to novelty, serendipity, and ethics. Surprisingly, in the classic\nproblem of influence maximization in social networks, relatively little study\nhas been devoted to diversity and its integration into the objective function\nof an influence maximization method.\n  In this work, we propose the integration of a side-information-based notion\nof seed diversity into the objective function of a targeted influence\nmaximization problem. Starting from the assumption that side-information is\navailable at node level in the general form of categorical attribute values, we\ndesign a class of monotone submodular functions specifically conceived for\ndetermining the diversity within a set of categorical profiles associated with\nthe seeds to be discovered. This allows us to develop an efficient scalable\napproximate method, with a constant-factor guarantee of optimality. More\nprecisely, we formulate the attribute-based diversity-sensitive targeted\ninfluence maximization problem under the state-of-the-art reverse influence\nsampling framework, and we develop a method, dubbed ADITUM, that ensures a\n(1-1/e-\\epsilon)-approximate solution under the general triggering diffusion\nmodel. We experimentally evaluated ADITUM on five real-world networks,\nincluding comparison with methods that exploit numerical-attribute-based\ndiversity and topology-driven diversity in influence maximization.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:59:07 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Cali\u00f2", "Antonio", ""], ["Tagarelli", "Andrea", ""]]}, {"id": "1912.03824", "submitter": "Enric Boix Adser\\`a", "authors": "Enric Boix-Adser\\`a, Lior Eldar, Saeed Mehraban", "title": "Approximating the Determinant of Well-Conditioned Matrices by Shallow\n  Circuits", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determinant can be computed by classical circuits of depth $O(\\log^2 n)$,\nand therefore it can also be computed in classical space $O(\\log^2 n)$. Recent\nprogress by Ta-Shma [Ta13] implies a method to approximate the determinant of\nHermitian matrices with condition number $\\kappa$ in quantum space $O(\\log n +\n\\log \\kappa)$. However, it is not known how to perform the task in less than\n$O(\\log^2 n)$ space using classical resources only. In this work, we show that\nthe condition number of a matrix implies an upper bound on the depth complexity\n(and therefore also on the space complexity) for this task: the determinant of\nHermitian matrices with condition number $\\kappa$ can be approximated to\ninverse polynomial relative error with classical circuits of depth $\\tilde\nO(\\log n \\cdot \\log \\kappa)$, and in particular one can approximate the\ndeterminant for sufficiently well-conditioned matrices in depth $\\tilde{O}(\\log\nn)$. Our algorithm combines Barvinok's recent complex-analytic approach for\napproximating combinatorial counting problems [Bar16] with the\nValiant-Berkowitz-Skyum-Rackoff depth-reduction theorem for low-degree\narithmetic circuits [Val83].\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:57:12 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Boix-Adser\u00e0", "Enric", ""], ["Eldar", "Lior", ""], ["Mehraban", "Saeed", ""]]}, {"id": "1912.04062", "submitter": "Carolin Penke", "authors": "Carolin Penke, Andreas Marek, Christian Vorwerk, Claudia Draxl, Peter\n  Benner", "title": "High Performance Solution of Skew-symmetric Eigenvalue Problems with\n  Applications in Solving the Bethe-Salpeter Eigenvalue Problem", "comments": null, "journal-ref": null, "doi": "10.1016/j.parco.2020.102639", "report-no": null, "categories": "math.NA cs.DS cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a high-performance solver for dense skew-symmetric matrix\neigenvalue problems. Our work is motivated by applications in computational\nquantum physics, where one solution approach to solve the so-called\nBethe-Salpeter equation involves the solution of a large, dense, skew-symmetric\neigenvalue problem. The computed eigenpairs can be used to compute the optical\nabsorption spectrum of molecules and crystalline systems. One state-of-the art\nhigh-performance solver package for symmetric matrices is the ELPA (Eigenvalue\nSoLvers for Petascale Applications) library. We extend the methods available in\nELPA to skew-symmetric matrices. This way, the presented solution method can\nbenefit from the optimizations available in ELPA that make it a\nwell-established, efficient and scalable library, such as GPU support. We\ncompare performance and scalability of our method to the only available\nhigh-performance approach for skew-symmetric matrices, an indirect route\ninvolving complex arithmetic. In total, we achieve a performance that is up to\n3.67 higher than the reference method using Intel's ScaLAPACK implementation.\nThe runtime to solve the Bethe-Salpeter-Eigenvalue problem can be improved by a\nfactor of 10. Our method is freely available in the current release of the ELPA\nlibrary.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:10:45 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 15:21:13 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Penke", "Carolin", ""], ["Marek", "Andreas", ""], ["Vorwerk", "Christian", ""], ["Draxl", "Claudia", ""], ["Benner", "Peter", ""]]}, {"id": "1912.04177", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Nadiia Chepurko and David P. Woodruff", "title": "Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation", "comments": "minor edits in technical overview", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Musco and Woodruff (FOCS, 2017) showed that given an $n \\times n$\npositive semidefinite (PSD) matrix $A$, it is possible to compute a\n$(1+\\epsilon)$-approximate relative-error low-rank approximation to $A$ by\nquerying $O(nk/\\epsilon^{2.5})$ entries of $A$ in time $O(nk/\\epsilon^{2.5} +n\nk^{\\omega-1}/\\epsilon^{2(\\omega-1)})$. They also showed that any relative-error\nlow-rank approximation algorithm must query $\\Omega(nk/\\epsilon)$ entries of\n$A$, this gap has since remained open. Our main result is to resolve this\nquestion by obtaining an optimal algorithm that queries $O(nk/\\epsilon)$\nentries of $A$ and outputs a relative-error low-rank approximation in\n$O(n(k/\\epsilon)^{\\omega-1})$ time. Note, our running time improves that of\nMusco and Woodruff, and matches the information-theoretic lower bound if the\nmatrix-multiplication exponent $\\omega$ is $2$.\n  We then extend our techniques to negative-type distance matrices. Bakshi and\nWoodruff (NeurIPS, 2018) showed a bi-criteria, relative-error low-rank\napproximation which queries $O(nk/\\epsilon^{2.5})$ entries and outputs a\nrank-$(k+4)$ matrix. We show that the bi-criteria guarantee is not necessary\nand obtain an $O(nk/\\epsilon)$ query algorithm, which is optimal. Our algorithm\napplies to all distance matrices that arise from metrics satisfying\nnegative-type inequalities, including $\\ell_1, \\ell_2,$ spherical metrics and\nhypermetrics.\n  Next, we introduce a new robust low-rank approximation model which captures\nPSD matrices that have been corrupted with noise. While a sample complexity\nlower bound precludes sublinear algorithms for arbitrary PSD matrices, we\nprovide the first sublinear time and query algorithms when the corruption on\nthe diagonal entries is bounded. As a special case, we show sample-optimal\nsublinear time algorithms for low-rank approximation of correlation matrices\ncorrupted by noise.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:52:12 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 20:11:03 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 03:12:59 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 02:09:36 GMT"}, {"version": "v5", "created": "Tue, 15 Jun 2021 17:18:36 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Chepurko", "Nadiia", ""], ["Woodruff", "David P.", ""]]}, {"id": "1912.04233", "submitter": "Simon Apers", "authors": "Simon Apers, Andr\\'as Gily\\'en, Stacey Jeffery", "title": "A Unified Framework of Quantum Walk Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main results on quantum walk search are scattered over different,\nincomparable frameworks, most notably the hitting time framework, originally by\nSzegedy, the electric network framework by Belovs, and the MNRS framework by\nMagniez, Nayak, Roland and Santha. As a result, a number of pieces are\ncurrently missing. For instance, the electric network framework allows quantum\nwalks to start from an arbitrary initial state, but it only detects marked\nelements. In recent work by Ambainis et al., this problem was resolved for the\nmore restricted hitting time framework, in which quantum walks must start from\nthe stationary distribution.\n  We present a new quantum walk search framework that unifies and strengthens\nthese frameworks. This leads to a number of new results. For instance, the new\nframework not only detects, but finds marked elements in the electric network\nsetting. The new framework also allows one to interpolate between the hitting\ntime framework, which minimizes the number of walk steps, and the MNRS\nframework, which minimizes the number of times elements are checked for being\nmarked. This allows for a more natural tradeoff between resources. Whereas the\noriginal frameworks only rely on quantum walks and phase estimation, our new\nalgorithm makes use of a technique called quantum fast-forwarding, similar to\nthe recent results by Ambainis et al. As a final result we show how in certain\ncases we can simplify this more involved algorithm to merely applying the\nquantum walk operator some number of times. This answers an open question of\nAmbainis et al.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:20:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Apers", "Simon", ""], ["Gily\u00e9n", "Andr\u00e1s", ""], ["Jeffery", "Stacey", ""]]}, {"id": "1912.04239", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki", "title": "A Deterministic Algorithm for the MST Problem in Constant Rounds of\n  Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the Minimum Spanning Tree problem can be solved\n\\emph{deterministically}, in $\\mathcal{O}(1)$ rounds of the\n$\\mathsf{Congested}$ $\\mathsf{Clique}$ model.\n  In the $\\mathsf{Congested}$ $\\mathsf{Clique}$ model, there are $n$ players\nthat perform computation in synchronous rounds. Each round consist of a phase\nof local computation and a phase of communication, in which each pair of\nplayers is allowed to exchange $\\mathcal{O}(\\log n)$ bit messages.\n  The studies of this model began with the MST problem: in the paper by Lotker\net al.[SPAA'03, SICOMP'05] that defines the $\\mathsf{Congested}$\n$\\mathsf{Clique}$ model the authors give a deterministic $\\mathcal{O}(\\log \\log\nn)$ round algorithm that improved over a trivial $\\mathcal{O}(\\log n)$ round\nadaptation of Bor\\r{u}vka's algorithm.\n  There was a sequence of gradual improvements to this result: an\n$\\mathcal{O}(\\log \\log \\log n)$ round algorithm by Hegeman et al. [PODC'15], an\n$\\mathcal{O}(\\log^* n)$ round algorithm by Ghaffari and Parter, [PODC'16] and\nan $\\mathcal{O}(1)$ round algorithm by Jurdzi\\'nski and Nowicki, [SODA'18], but\nall those algorithms were randomized, which left the question about the\nexistence of any deterministic $o(\\log \\log n)$ round algorithms for the\nMinimum Spanning Tree problem open.\n  Our result resolves this question and establishes that $\\mathcal{O}(1)$\nrounds is enough to solve the MST problem in the $\\mathsf{Congested}$\n$\\mathsf{Clique}$ model, even if we are not allowed to use any randomness.\nFurthermore, the amount of communication needed by the algorithm makes it\napplicable to some variants of the $\\mathsf{MPC}$ model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:27:50 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 15:19:05 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 16:50:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Nowicki", "Krzysztof", ""]]}, {"id": "1912.04392", "submitter": "Andrej Sajenko", "authors": "Klaus Heeger, Anne-Sophie Himmel, Frank Kammer, Rolf Niedermeier,\n  Malte Renken, Andrej Sajenko", "title": "Multistage Graph Problems on a Global Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-evolving or temporal graphs gain more and more popularity when studying\nthe behavior of complex networks. In this context, the multistage view on\ncomputational problems is among the most natural frameworks. Roughly speaking,\nherein one studies the different (time) layers of a temporal graph (effectively\nmeaning that the edge set may change over time, but the vertex set remains\nunchanged), and one searches for a solution of a given graph problem for each\nlayer. The twist in the multistage setting is that the solutions found must not\ndiffer too much between subsequent layers. We relax on this already established\nnotion by introducing a global instead of the local budget view studied so far.\nMore specifically, we allow for few disruptive changes between subsequent\nlayers but request that overall, that is, summing over all layers, the degree\nof change is moderate. Studying several classical graph problems (both NP-hard\nand polynomial-time solvable ones) from a parameterized complexity angle, we\nencounter both fixed-parameter tractability and parameterized hardness results.\nSomewhat surprisingly, we find that sometimes the global multistage versions of\nNP-hard problems such as Vertex Cover turn out to be computationally more\ntractable than the ones of polynomial-time solvable problems such as Matching.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 21:54:00 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 09:50:18 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 13:39:57 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Heeger", "Klaus", ""], ["Himmel", "Anne-Sophie", ""], ["Kammer", "Frank", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""], ["Sajenko", "Andrej", ""]]}, {"id": "1912.04586", "submitter": "Thomas Espitau", "authors": "Thomas Espitau and Paul Kirchner and Pierre-Alain Fouque", "title": "Algebraic and Euclidean Lattices: Optimal Lattice Reduction and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework generalizing lattice reduction algorithms to module\nlattices in order to practically and efficiently solve the $\\gamma$-Hermite\nModule-SVP problem over arbitrary cyclotomic fields. The core idea is to\nexploit the structure of the subfields for designing a doubly-recursive\nstrategy of reduction: both recursive in the rank of the module and in the\nfield we are working in. Besides, we demonstrate how to leverage the inherent\nsymplectic geometry existing in the tower of fields to provide a significant\nspeed-up of the reduction for rank two modules. The recursive strategy over the\nrank can also be applied to the reduction of Euclidean lattices, and we can\nperform a reduction in asymptotically almost the same time as matrix\nmultiplication. As a byproduct of the design of these fast reductions, we also\ngeneralize to all cyclotomic fields and provide speedups for many previous\nnumber theoretical algorithms. Quantitatively, we show that a module of rank 2\nover a cyclotomic field of degree $n$ can be heuristically reduced within\napproximation factor $2^{\\tilde{O}(n)}$ in time $\\tilde{O}(n^2B)$, where $B$ is\nthe bitlength of the entries. For $B$ large enough, this complexity shrinks to\n$\\tilde{O}(n^{\\log_2 3}B)$. This last result is particularly striking as it\ngoes below the estimate of $n^2B$ swaps given by the classical analysis of the\nLLL algorithm using the so-called potential.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:09:19 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Espitau", "Thomas", ""], ["Kirchner", "Paul", ""], ["Fouque", "Pierre-Alain", ""]]}, {"id": "1912.04701", "submitter": "Alexei Kanel-Belov Prof.", "authors": "E.G. Kondakova, A. Ya. Kanel-Belov", "title": "Probabilistic methods of bypassing the maze using stones and a random\n  number sensor", "comments": "25 pages, in Russian, MIPT, BIU, SZU, This work was supported by the\n  Russian Science Foundation, grant No. 17-11-01377, to appear in Chebyshevskyi\n  sbornik", "journal-ref": "Chebyshevskii Sb., 2019, Volume 20, Issue 3, Pages 296-315", "doi": "10.22405/2226-8383-2018-20-3-296-315", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, some open questions that are posed in Ajans' dissertation\ncontinue to be addressed: a robot bypass with a generator of random bits of\ninteger spaces in the presence of a stone and a subspace of flags. This work is\ndevoted to bypassing the maze with a finite state machine with a random bit\ngenerator. This task is part of the rapidly evolving theme of bypassing the\nmaze by various finite state machines. or their teams, which is closely related\nto problems from the theory of computational complexity and probability theory.\nIn this paper, it is shown at what dimensions a robot with a random bit\ngenerator and a stone can bypass integer space with a subspace of flags. In\nthis paper, we will study the behavior of a finite state machine with a random\nbit generator on integer spaces. In particular, it was proved that the robot\nbypasses $ \\ zs ^ 2 $ and cannot bypass $ \\ zs ^ 3 $; a robot with a stone\nbypasses $ \\ zs ^ 4 $ and cannot bypass $ \\ zs ^ 5 $; a robot with a stone and\na flag bypasses $ \\ zs ^ 6 $ and cannot bypass $ \\ zs ^ 7 $; a robot with a\nstone and a plane of flags bypasses $ \\ zs ^ 8 $ and cannot bypass $ \\ zs ^ 9\n$.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 03:14:06 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 03:13:14 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kondakova", "E. G.", ""], ["Kanel-Belov", "A. Ya.", ""]]}, {"id": "1912.04753", "submitter": "Zhipeng Gui", "authors": "Yuan Wang, Zhipeng Gui, Huayi Wu, Dehua Peng, Jinghang Wu, Zousen Cui", "title": "Optimizing and accelerating space-time Ripley's K function based on\n  Apache Spark for distributed spatiotemporal point pattern analysis", "comments": "35 pages, 23 figures, Future Generation Computer Systems", "journal-ref": "Future Generation Computer Systems, 2020", "doi": "10.1016/j.future.2019.11.036", "report-no": null, "categories": "stat.CO cs.CG cs.DC cs.DS cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing point of interest (POI) datasets available with fine-grained\nspatial and temporal attributes, space-time Ripley's K function has been\nregarded as a powerful approach to analyze spatiotemporal point process.\nHowever, space-time Ripley's K function is computationally intensive for\npoint-wise distance comparisons, edge correction and simulations for\nsignificance testing. Parallel computing technologies like OpenMP, MPI and CUDA\nhave been leveraged to accelerate the K function, and related experiments have\ndemonstrated the substantial acceleration. Nevertheless, previous works have\nnot extended optimization of Ripley's K function from space dimension to\nspace-time dimension. Without sophisticated spatiotemporal query and\npartitioning mechanisms, extra computational overhead can be problematic.\nMeanwhile, these researches were limited by the restricted scalability and\nrelative expensive programming cost of parallel frameworks and impeded their\napplications for large POI dataset and Ripley's K function variations. This\npaper presents a distributed computing method to accelerate space-time Ripley's\nK function upon state-of-the-art distributed computing framework Apache Spark,\nand four strategies are adopted to simplify calculation procedures and\naccelerate distributed computing respectively. Based on the optimized method, a\nweb-based visual analytics framework prototype has been developed. Experiments\nprove the feasibility and time efficiency of the proposed method, and also\ndemonstrate its value on promoting applications of space-time Ripley's K\nfunction in ecology, geography, sociology, economics, urban transportation and\nother fields.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:15:37 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Yuan", ""], ["Gui", "Zhipeng", ""], ["Wu", "Huayi", ""], ["Peng", "Dehua", ""], ["Wu", "Jinghang", ""], ["Cui", "Zousen", ""]]}, {"id": "1912.04772", "submitter": "Yuval Filmus", "authors": "Yuval Filmus", "title": "Asymptotic performance of the Grimmett-McDiarmid heuristic", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grimmett and McDiarmid suggested a simple heuristic for finding stable sets\nin random graphs. They showed that the heuristic finds a stable set of size\n$\\sim\\log_2 n$ (with high probability) on a $G(n, 1/2)$ random graph. We\ndetermine the asymptotic distribution of the size of the stable set found by\nthe algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:36:35 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Filmus", "Yuval", ""]]}, {"id": "1912.04897", "submitter": "Jesse Geneson", "authors": "Jesse Geneson", "title": "An algorithm for bounding extremal functions of forbidden sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Davenport-Schinzel sequences are sequences that avoid a forbidden\nsubsequence and have a sparsity requirement on their letters. Upper bounds on\nthe lengths of generalized Davenport-Schinzel sequences have been applied to a\nnumber of problems in discrete geometry and extremal combinatorics. Sharp\nbounds on the maximum lengths of generalized Davenport-Schinzel sequences are\nknown for some families of forbidden subsequences, but in general there are\nonly rough bounds on the maximum lengths of most generalized Davenport-Schinzel\nsequences. One method that was developed for finding upper bounds on the\nlengths of generalized Davenport-Schinzel sequences uses a family of sequences\ncalled formations.\n  An $(r, s)$-formation is a concatenation of $s$ permutations of $r$ distinct\nletters. The formation width function $fw(u)$ is defined as the minimum $s$ for\nwhich there exists $r$ such that every $(r, s)$-formation contains $u$. The\nfunction $fw(u)$ has been used with upper bounds on extremal functions of $(r,\ns)$-formations to find tight bounds on the maximum possible lengths of many\nfamilies of generalized Davenport-Schinzel sequences. Algorithms have been\nfound for computing $fw(u)$ for sequences $u$ of length $n$, but they have\nworst-case run time exponential in $n$, even for sequences $u$ with only three\ndistinct letters.\n  We present an algorithm for computing $fw(u)$ with run time\n$O(n^{\\alpha_r})$, where $r$ is the number of distinct letters in $u$ and\n$\\alpha_r$ is a constant that only depends on $r$. We implement the new\nalgorithm in Python and compare its run time to the next fastest algorithm for\ncomputing formation width. We also apply the new algorithm to find sharp upper\nbounds on the lengths of several families of generalized Davenport-Schinzel\nsequences with $3$-letter forbidden patterns.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:54:49 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Geneson", "Jesse", ""]]}, {"id": "1912.05010", "submitter": "Zachary Friggstad", "authors": "Zachary Friggstad and Maryam Mahboub", "title": "Graph Pricing with Limited Supply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximation algorithms for graph pricing with vertex capacities\nyet without the traditional envy-free constraint. Specifically, we have a set\nof items $V$ and a set of customers $X$ where each customer $i \\in X$ has a\nbudget $b_i$ and is interested in a bundle of items $S_i \\subseteq V$ with\n$|S_i| \\leq 2$. However, there is a limited supply of each item: we only have\n$\\mu_v$ copies of item $v$ to sell for each $v \\in V$. We should assign prices\n$p(v)$ to each $v \\in V$ and chose a subset $Y \\subseteq X$ of customers so\nthat each $i \\in Y$ can afford their bundle ($p(S_i) \\leq b_i$) and at most\n$\\mu_v$ chosen customers have item $v$ in their bundle for each item $v \\in V$.\nEach customer $i \\in Y$ pays $p(S_i)$ for the bundle they purchased: our goal\nis to do this in a way that maximizes revenue. Such pricing problems have been\nstudied from the perspective of envy-freeness where we also must ensure that\n$p(S_i) \\geq b_i$ for each $i \\notin Y$. However, the version where we simply\nallocate items to customers after setting prices and do not worry about the\nenvy-free condition has received less attention.\n  Our main result is an 8-approximation for the capacitated case via local\nsearch and a 7.8096-approximation in simple graphs with uniform vertex\ncapacities. The latter is obtained by combing a more involved analysis of a\nmulti-swap local search algorithm for constant capacities and an LP-rounding\nalgorithm for larger capacities. If all capacities are bounded by a constant\n$C$, we further show a multi-swap local search algorithm yields an $\\left(4\n\\cdot \\frac{2C-1}{C} + \\epsilon\\right)$-approximation. We also give a\n$(4+\\epsilon)$-approximation in simple graphs through LP rounding when all\ncapacities are very large as a function of $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:09:22 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Friggstad", "Zachary", ""], ["Mahboub", "Maryam", ""]]}, {"id": "1912.05153", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nhat Ho, Martin J. Wainwright, Peter L. Bartlett, Michael\n  I. Jordan", "title": "Sampling for Bayesian Mixture Models: MCMC with Polynomial-Time Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from the power posterior distribution in\nBayesian Gaussian mixture models, a robust version of the classical posterior.\nThis power posterior is known to be non-log-concave and multi-modal, which\nleads to exponential mixing times for some standard MCMC algorithms. We\nintroduce and study the Reflected Metropolis-Hastings Random Walk (RMRW)\nalgorithm for sampling. For symmetric two-component Gaussian mixtures, we prove\nthat its mixing time is bounded as $d^{1.5}(d + \\Vert \\theta_{0}\n\\Vert^2)^{4.5}$ as long as the sample size $n$ is of the order $d (d + \\Vert\n\\theta_{0} \\Vert^2)$. Notably, this result requires no conditions on the\nseparation of the two means. En route to proving this bound, we establish some\nnew results of possible independent interest that allow for combining\nPoincar\\'{e} inequalities for conditional and marginal densities.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:48:49 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mou", "Wenlong", ""], ["Ho", "Nhat", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1912.05339", "submitter": "Shiying Li", "authors": "He Chen Li, Shi Ying Li, Bo Wen Tan and Shuai Cheng Li", "title": "Crossing Reduction of Sankey Diagram with Barycentre Ordering via Markov\n  Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sankey diagram is popular for analyzing primary flows in network data.\nHowever, the growing complexity of data and hence crossings in the diagram\nbegin to reduce its readability. In this work, we studied the NP-hard weighted\ncrossing reduction problem of the Sankey diagram with both the common parallel\nform and the circular form. We expect to obtain an ordering of entities that\nreduces weighted crossings of links. We proposed a two-staged heuristic method\nbased on the idea of barycentre ordering and used Markov chain to formulate the\nrecursive process of obtaining such ordering. In the experiments, our method\nachieved 300.89 weighted crossings, compared with the optimum 278.68 from an\ninteger linear programming method. Also, we obtained much less weighted\ncrossings (87.855) than the state-of-art heuristic method (146.77). We also\nconducted a robust test which provided evidence that our method performed\nconsistently against the change of complexity in the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:21:24 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 05:31:07 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Li", "He Chen", ""], ["Li", "Shi Ying", ""], ["Tan", "Bo Wen", ""], ["Li", "Shuai Cheng", ""]]}, {"id": "1912.05390", "submitter": "Merav Parter", "authors": "Artur Czumaj, Peter Davies and Merav Parter", "title": "Graph Sparsification for Derandomizing Massively Parallel Computation\n  with Low Space", "comments": "he coloring part was omitted from the current version, and will\n  appear soon on a separate arXiv manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massively Parallel Computation (MPC) model is an emerging model which\ndistills core aspects of distributed and parallel computation. It has been\ndeveloped as a tool to solve (typically graph) problems in systems where the\ninput is distributed over many machines with limited space. Recent work has\nfocused on the regime in which machines have sublinear (in $n$, the number of\nnodes in the input graph) memory, with randomized algorithms presented for\nfundamental graph problems of Maximal Matching and Maximal Independent Set.\nHowever, there have been no prior corresponding \\emph{deterministic}\nalgorithms.\n  A major challenge underlying the sublinear space setting is that the local\nspace of each machine might be too small to store all the edges incident to a\nsingle node. This poses a considerable obstacle compared to the classical\nmodels in which each node is assumed to know and have easy access to its\nincident edges. To overcome this barrier we introduce a new \\emph{graph\nsparsification technique} that \\emph{deterministically} computes a low-degree\nsubgraph with additional desired properties. Using this framework to\nderandomize the well-known randomized algorithm of Luby [SICOMP'86], we obtain\n$O(\\log \\Delta+\\log\\log n)$-round \\emph{deterministic} MPC algorithms for\nsolving the fundamental problems of \\emph{Maximal Matching} and \\emph{Maximal\nIndependent Set} with $O(n^{\\epsilon})$ space on each machine for any constant\n$\\epsilon > 0$. Based on the recent work of Ghaffari et al. [FOCS'18], this\nadditive $O(\\log\\log n)$ factor is \\emph{conditionally} essential. These\nalgorithms can also be shown to run in $O(\\log \\Delta)$ rounds in the closely\nrelated model of \\congc, improving upon the state-of-the-art bound of $O(\\log^2\n\\Delta)$ rounds by Censor-Hillel et al. [DISC'17].\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 15:31:32 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 09:14:58 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 13:27:13 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""], ["Parter", "Merav", ""]]}, {"id": "1912.05506", "submitter": "Katina Russell", "authors": "Nairen Cao, Jeremy T. Fineman, Katina Russell", "title": "Efficient Construction of Directed Hopsets and Parallel Approximate\n  Shortest Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximate single-source shortest-path problem is as follows: given a\ngraph with nonnegative edge weights and a designated source vertex $s$, return\nestimates of the distances from~$s$ to each other vertex such that the estimate\nfalls between the true distance and $(1+\\epsilon)$ times the distance. This\npaper provides the first nearly work-efficient parallel algorithm with\nsublinear span (also called depth) for the approximate shortest-path problem on\n\\emph{directed} graphs. Specifically, for constant $\\epsilon$ and\npolynomially-bounded edge weights, our algorithm has work $\\tilde{O}(m)$ and\nspan $n^{1/2+o(1)}$. Several algorithms were previously known for the case of\n\\emph{undirected} graphs, but none of the techniques seem to translate to the\ndirected setting.\n  The main technical contribution is the first nearly linear-work algorithm for\nconstructing hopsets on directed graphs. A $(\\beta,\\epsilon)$-hopset is a set\nof weighted edges (sometimes called shortcuts) which, when added to the graph,\nadmit $\\beta$-hop paths with weight no more than $(1+\\epsilon)$ times the true\nshortest-path distances. There is a simple sequential algorithm that takes as\ninput a directed graph and produces a linear-cardinality hopset with\n$\\beta=O(\\sqrt{n})$, but its running time is quite high---specifically\n$\\tilde{O}(m\\sqrt{n})$. Our algorithm is the first more efficient algorithm\nthat produces a directed hopset with similar characteristics. Specifically, our\nsequential algorithm runs in $\\tilde{O}(m)$ time and constructs a hopset with\n$\\tilde{O}(n)$ edges and $\\beta = n^{1/2+o(1)}$. A parallel version of the\nalgorithm has work $\\tilde{O}(m)$ and span $n^{1/2+o(1)}$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:12:15 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Cao", "Nairen", ""], ["Fineman", "Jeremy T.", ""], ["Russell", "Katina", ""]]}, {"id": "1912.05819", "submitter": "Mathew Francis", "authors": "Mathew C. Francis and Dalu Jacob", "title": "The Lexicographic Method for the Threshold Cover Problem", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold graphs are a class of graphs that have many equivalent definitions\nand have applications in integer programming and set packing problems. A graph\nis said to have a threshold cover of size $k$ if its edges can be covered using\n$k$ threshold graphs. Chv\\'atal and Hammer, in 1977, defined the threshold\ndimension $\\mathrm{th}(G)$ of a graph $G$ to be the least integer $k$ such that\n$G$ has a threshold cover of size $k$ and observed that\n$\\mathrm{th}(G)\\geq\\chi(G^*)$, where $G^*$ is a suitably constructed auxiliary\ngraph. Raschle and Simon~[Proceedings of the Twenty-seventh Annual ACM\nSymposium on Theory of Computing, STOC '95, pages 650--661, 1995] proved that\n$\\mathrm{th}(G)=\\chi(G^*)$ whenever $G^*$ is bipartite. We show how the\nlexicographic method of Hell and Huang can be used to obtain a completely new\nand, we believe, simpler proof for this result. For the case when $G$ is a\nsplit graph, our method yields a proof that is much shorter than the ones known\nin the literature.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 07:59:41 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:45:05 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Francis", "Mathew C.", ""], ["Jacob", "Dalu", ""]]}, {"id": "1912.06198", "submitter": "Zachary Friggstad", "authors": "Zachary Friggstad and Chaitanya Swamy", "title": "A Constant-Factor Approximation for Directed Latency in Quasi-Polynomial\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first constant-factor approximation for the Directed Latency\nproblem in quasi-polynomial time. Here, the goal is to visit all nodes in an\nasymmetric metric with a single vehicle starting at a depot $r$ to minimize the\naverage time a node waits to be visited by the vehicle. The approximation\nguarantee is an improvement over the polynomial-time $O(\\log n)$-approximation\n[Friggstad, Salavatipour, Svitkina, 2013] and no better quasi-polynomial time\napproximation algorithm was known.\n  To obtain this, we must extend a recent result showing the integrality gap of\nthe Asymmetric TSP-Path LP relaxation is bounded by a constant [K\\\"{o}hne,\nTraub, and Vygen, 2019], which itself builds on the breakthrough result that\nthe integrality gap for standard Asymmetric TSP is also a constant [Svensson,\nTarnawsi, and Vegh, 2018]. We show the standard Asymmetric TSP-Path integrality\ngap is bounded by a constant even if the cut requirements of the LP relaxation\nare relaxed from $x(\\delta^{in}(S)) \\geq 1$ to $x(\\delta^{in}(S)) \\geq \\rho$\nfor some constant $1/2 < \\rho \\leq 1$. We also give a better approximation\nguarantee in the special case of Directed Latency in regret metrics where the\ngoal is to find a path $P$ minimize the average time a node $v$ waits in excess\nof $c_{rv}$, i.e. $\\frac{1}{|V|} \\cdot \\sum_{v \\in V} (c_v(P)-c_{rv})$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:32:38 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 18:40:09 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Friggstad", "Zachary", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1912.06202", "submitter": "Emily Diana", "authors": "Emily Diana, Michael Kearns, Seth Neel, Aaron Roth", "title": "Optimal, Truthful, and Private Securities Lending", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental dynamic allocation problem motivated by the problem\nof $\\textit{securities lending}$ in financial markets, the mechanism underlying\nthe short selling of stocks. A lender would like to distribute a finite number\nof identical copies of some scarce resource to $n$ clients, each of whom has a\nprivate demand that is unknown to the lender. The lender would like to maximize\nthe usage of the resource $\\mbox{---}$ avoiding allocating more to a client\nthan her true demand $\\mbox{---}$ but is constrained to sell the resource at a\npre-specified price per unit, and thus cannot use prices to incentivize\ntruthful reporting. We first show that the Bayesian optimal algorithm for the\none-shot problem $\\mbox{---}$ which maximizes the resource's expected usage\naccording to the posterior expectation of demand, given reports $\\mbox{---}$\nactually incentivizes truthful reporting as a dominant strategy. Because true\ndemands in the securities lending problem are often sensitive information that\nthe client would like to hide from competitors, we then consider the problem\nunder the additional desideratum of (joint) differential privacy. We give an\nalgorithm, based on simple dynamics for computing market equilibria, that is\nsimultaneously private, approximately optimal, and approximately\ndominant-strategy truthful. Finally, we leverage this private algorithm to\nconstruct an approximately truthful, optimal mechanism for the extensive form\nmulti-round auction where the lender does not have access to the true joint\ndistributions between clients' requests and demands.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:45:04 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Diana", "Emily", ""], ["Kearns", "Michael", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""]]}, {"id": "1912.06252", "submitter": "Bento Natura", "authors": "Daniel Dadush and Sophie Huiberts and Bento Natura and L\\'aszl\\'o A.\n  V\\'egh", "title": "A scaling-invariant algorithm for linear programming whose running time\n  depends only on the constraint matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the breakthrough work of Tardos in the bit-complexity model,\nVavasis and Ye gave the first exact algorithm for linear programming in the\nreal model of computation with running time depending only on the constraint\nmatrix. For solving a linear program (LP) $\\max\\, c^\\top x,\\: Ax = b,\\: x \\geq\n0,\\: A \\in \\mathbb{R}^{m \\times n}$, Vavasis and Ye developed a primal-dual\ninterior point method using a 'layered least squares' (LLS) step, and showed\nthat $O(n^{3.5} \\log (\\bar{\\chi}_A+n))$ iterations suffice to solve (LP)\nexactly, where $\\bar{\\chi}_A$ is a condition measure controlling the size of\nsolutions to linear systems related to $A$.\n  Monteiro and Tsuchiya, noting that the central path is invariant under\nrescalings of the columns of $A$ and $c$, asked whether there exists an LP\nalgorithm depending instead on the measure $\\bar{\\chi}^*_A$, defined as the\nminimum $\\bar{\\chi}_{AD}$ value achievable by a column rescaling $AD$ of $A$,\nand gave strong evidence that this should be the case. We resolve this open\nquestion affirmatively.\n  Our first main contribution is an $O(m^2 n^2 + n^3)$ time algorithm which\nworks on the linear matroid of $A$ to compute a nearly optimal diagonal\nrescaling $D$ satisfying $\\bar{\\chi}_{AD} \\leq n(\\bar{\\chi}^*)^3$. This\nalgorithm also allows us to approximate the value of $\\bar{\\chi}_A$ up to a\nfactor $n (\\bar{\\chi}^*)^2$. As our second main contribution, we develop a\nscaling invariant LLS algorithm, together with a refined potential function\nbased analysis for LLS algorithms in general. With this analysis, we derive an\nimproved $O(n^{2.5} \\log n\\log (\\bar{\\chi}^*_A+n))$ iteration bound for\noptimally solving (LP) using our algorithm. The same argument also yields a\nfactor $n/\\log n$ improvement on the iteration complexity bound of the original\nVavasis-Ye algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:01:02 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 16:52:15 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Dadush", "Daniel", ""], ["Huiberts", "Sophie", ""], ["Natura", "Bento", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1912.06255", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Yan Gu, Julian Shun", "title": "Theoretically-Efficient and Practical Parallel DBSCAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DBSCAN method for spatial clustering has received significant attention\ndue to its applicability in a variety of data analysis tasks. There are fast\nsequential algorithms for DBSCAN in Euclidean space that take $O(n\\log n)$ work\nfor two dimensions, sub-quadratic work for three or more dimensions, and can be\ncomputed approximately in linear work for any constant number of dimensions.\nHowever, existing parallel DBSCAN algorithms require quadratic work in the\nworst case, making them inefficient for large datasets. This paper bridges the\ngap between theory and practice of parallel DBSCAN by presenting new parallel\nalgorithms for Euclidean exact DBSCAN and approximate DBSCAN that match the\nwork bounds of their sequential counterparts, and are highly parallel\n(polylogarithmic depth). We present implementations of our algorithms along\nwith optimizations that improve their practical performance. We perform a\ncomprehensive experimental evaluation of our algorithms on a variety of\ndatasets and parameter settings. Our experiments on a 36-core machine with\nhyper-threading show that we outperform existing parallel DBSCAN\nimplementations by up to several orders of magnitude, and achieve speedups by\nup to 33x over the best sequential algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:09:20 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 19:41:12 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 21:10:23 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 23:55:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Yiqiu", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "1912.06300", "submitter": "Tianzhi Li", "authors": "Ananya Christman, Christine Chung, Nicholas Jaczko, Tianzhi Li, Scott\n  Westvold, Xinyue Xu", "title": "The SBP Algorithm for Maximizing Revenue in Online Dial-a-Ride", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Online-Dial-a-Ride Problem (OLDARP) a server travels through a metric\nspace to serve requests for rides. We consider a variant where each request\nspecifies a source, destination, release time, and revenue that is earned for\nserving the request. The goal is to maximize the total revenue earned within a\ngiven time limit. We prove that no non-preemptive deterministic online\nalgorithm for OLDARP can be guaranteed to earn more than twice the revenue\nearned by an optimal offline solution. We then investigate the\n\\textsc{segmented best path} ($SBP$) algorithm of~\\cite{atmos17} for the\ngeneral case of weighted graphs. The previously-established lower and upper\nbounds for the competitive ratio of $SBP$ are 4 and 6, respectively, under\nreasonable assumptions about the input instance. We eliminate the gap by\nproving that the competitive ratio is 5 (under the same reasonable\nassumptions). We also prove that when revenues are uniform, $SBP$ has\ncompetitive ratio 4. Finally, we provide a competitive analysis of $SBP$ on\ncomplete bipartite graphs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 02:46:43 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Christman", "Ananya", ""], ["Chung", "Christine", ""], ["Jaczko", "Nicholas", ""], ["Li", "Tianzhi", ""], ["Westvold", "Scott", ""], ["Xu", "Xinyue", ""]]}, {"id": "1912.06360", "submitter": "Kiril Danilchenko", "authors": "Kiril Danilchenko and Michael Segal", "title": "Construction and Maintenance of Swarm Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the dynamic version of the covering problem motivated\nby the coverage of drones' swarm: Let $S$ be a set of $n$ non-negative weighted\npoints in the plane representing users. Also, consider a set $P$ of $m$ disks\nthat correspond to the covering radius of each drone. We want to place (and\nmaintain) set $P$ such that the sum of the weights of the points in $S$ covered\nby disks from $P$ is maximized. We present a data structure that maintains a\nsmall constant factor approximate solution efficiently, under insertions and\ndeletions of points to/from $S$ where each update operation can be performed\n$O(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 08:45:16 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Danilchenko", "Kiril", ""], ["Segal", "Michael", ""]]}, {"id": "1912.06415", "submitter": "Sudhakar Singh", "authors": "Pankaj Singh, Sudhakar Singh, P. K. Mishra, Rakhi Garg", "title": "RDD-Eclat: Approaches to Parallelize Eclat Algorithm on Spark RDD\n  Framework", "comments": "16 pages, 6 figures, ICCNCT 2019", "journal-ref": "ICCNCT 2019, LNDECT 44", "doi": "10.1007/978-3-030-37051-0_85", "report-no": "ICCNCT-171", "categories": "cs.DC cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially, a number of frequent itemset mining (FIM) algorithms have been\ndesigned on the Hadoop MapReduce, a distributed big data processing framework.\nBut, due to heavy disk I/O, MapReduce is found to be inefficient for such\nhighly iterative algorithms. Therefore, Spark, a more efficient distributed\ndata processing framework, has been developed with in-memory computation and\nresilient distributed dataset (RDD) features to support the iterative\nalgorithms. On the Spark RDD framework, Apriori and FP-Growth based FIM\nalgorithms have been designed, but Eclat-based algorithm has not been explored\nyet. In this paper, RDD-Eclat, a parallel Eclat algorithm on the Spark RDD\nframework is proposed with its five variants. The proposed algorithms are\nevaluated on the various benchmark datasets, which shows that RDD-Eclat\noutperforms the Spark-based Apriori by many times. Also, the experimental\nresults show the scalability of the proposed algorithms on increasing the\nnumber of cores and size of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:23:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Singh", "Pankaj", ""], ["Singh", "Sudhakar", ""], ["Mishra", "P. K.", ""], ["Garg", "Rakhi", ""]]}, {"id": "1912.06428", "submitter": "Kira Goldner", "authors": "Kira Goldner, Nicole Immorlica, Brendan Lucier", "title": "Reducing Inefficiency in Carbon Auctions with Imperfect Competition", "comments": "To appear in the 11th Innovations in Theoretical Computer Science\n  (ITCS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study auctions for carbon licenses, a policy tool used to control the\nsocial cost of pollution. Each identical license grants the right to produce a\nunit of pollution. Each buyer (i.e., firm that pollutes during the\nmanufacturing process) enjoys a decreasing marginal value for licenses, but\nsociety suffers an increasing marginal cost for each license distributed. The\nseller (i.e., the government) can choose a number of licenses to put up for\nauction, and wishes to maximize the societal welfare: the total economic value\nof the buyers minus the social cost. Motivated by emission license markets\ndeployed in practice, we focus on uniform price auctions with a price floor\nand/or price ceiling. The seller has distributional information about the\nmarket, and their goal is to tune the auction parameters to maximize expected\nwelfare. The target benchmark is the maximum expected welfare achievable by any\nsuch auction under truth-telling behavior. Unfortunately, the uniform price\nauction is not truthful, and strategic behavior can significantly reduce (even\nbelow zero) the welfare of a given auction configuration.\n  We describe a subclass of \"safe-price'\" auctions for which the welfare at any\nBayes-Nash equilibrium will approximate the welfare under truth-telling\nbehavior. We then show that the better of a safe-price auction, or a truthful\nauction that allocates licenses to only a single buyer, will approximate the\ntarget benchmark. In particular, we show how to choose a number of licenses and\na price floor so that the worst-case welfare, at any equilibrium, is a constant\napproximation to the best achievable welfare under truth-telling after\nexcluding the welfare contribution of a single buyer.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:51:47 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Goldner", "Kira", ""], ["Immorlica", "Nicole", ""], ["Lucier", "Brendan", ""]]}, {"id": "1912.06432", "submitter": "Luis Ignacio Lopera Gonz\\'alez", "authors": "Luis Ignacio Lopera Gonz\\'alez, Adrian Derungs, Oliver Amft\n  (Friedrich-Alexander University Erlangen-N\\\"urnberg, Erlangen, Germany)", "title": "A Bayesian Approach to Rule Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the increasing belief criterion in association\nrule mining. The criterion uses a recursive application of Bayes' theorem to\ncompute a rule's belief. Extracted rules are required to have their belief\nincrease with their last observation. We extend the taxonomy of association\nrule mining algorithms with a new branch for Bayesian rule mining~(BRM), which\nuses increasing belief as the rule selection criterion. In contrast, the\nwell-established frequent association rule mining~(FRM) branch relies on the\nminimum-support concept to extract rules. We derive properties of the\nincreasing belief criterion, such as the increasing belief boundary,\nno-prior-worries, and conjunctive premises. Subsequently, we implement a BRM\nalgorithm using the increasing belief criterion, and illustrate its\nfunctionality in three experiments: (1)~a proof-of-concept to illustrate BRM\nproperties, (2)~an analysis relating socioeconomic information and chemical\nexposure data, and (3)~mining behaviour routines in patients undergoing\nneurological rehabilitation. We illustrate how BRM is capable of extracting\nrare rules and does not suffer from support dilution. Furthermore, we show that\nBRM focuses on the individual event generating processes, while FRM focuses on\ntheir commonalities. We consider BRM's increasing belief as an alternative\ncriterion to thresholds on rule support, as often applied in FRM, to determine\nrule usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 12:06:38 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 20:47:15 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Gonz\u00e1lez", "Luis Ignacio Lopera", "", "Friedrich-Alexander University Erlangen-N\u00fcrnberg, Erlangen, Germany"], ["Derungs", "Adrian", "", "Friedrich-Alexander University Erlangen-N\u00fcrnberg, Erlangen, Germany"], ["Amft", "Oliver", "", "Friedrich-Alexander University Erlangen-N\u00fcrnberg, Erlangen, Germany"]]}, {"id": "1912.06862", "submitter": "Dimitrios Letsios", "authors": "Dimitrios Letsios, Jeremy T. Bradley, Suraj G, Ruth Misener, Natasha\n  Page", "title": "Approximate and Robust Bounded Job Start Scheduling for Royal Mail\n  Delivery Offices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by mail delivery scheduling problems arising in Royal Mail, we\nstudy a generalization of the fundamental makespan scheduling P||Cmax problem\nwhich we call the bounded job start scheduling problem. Given a set of jobs,\neach specified by an integer processing time p_j, that have to be executed\nnon-preemptively by a set of m parallel identical machines, the objective is to\ncompute a minimum makespan schedule subject to an upper bound g<=m on the\nnumber of jobs that may simultaneously begin per unit of time. With perfect\ninput knowledge, we show that Longest Processing Time First (LPT) algorithm is\ntightly 2-approximate. After proving that the problem is strongly NP-hard even\nwhen g=1, we elaborate on improving the 2-approximation ratio for this case. We\ndistinguish the classes of long and short instances satisfying p_j>=m and\np_j<m, respectively, for each job j. We show that LPT is 5/3-approximate for\nthe former and optimal for the latter. Then, we explore scheduling long jobs in\nparallel with short jobs to obtain tightly satisfied packing and bounded job\nstart constraints. For a broad family of instances excluding degenerate\ninstances with many very long jobs, we derive a 1.985-approximation ratio. For\ngeneral instances, we require machine augmentation to obtain better than\n2-approximate schedules. Under uncertain job processing times, we exploit\nmachine augmentation and lexicographic optimization to propose a two-stage\nrobust optimization approach for bounded job start scheduling under uncertainty\naiming in a low number of used machines. Given a collection of schedules of\nmakespan <= D, this approach allows distinguishing which are the more robust.\nWe substantiate both the heuristics and our recovery approach numerically using\nRoyal Mail data. We show that, for the Royal Mail application, machine\naugmentation, i.e. short-term van rental, is especially relevant.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 15:16:15 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 00:38:50 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Letsios", "Dimitrios", ""], ["Bradley", "Jeremy T.", ""], ["G", "Suraj", ""], ["Misener", "Ruth", ""], ["Page", "Natasha", ""]]}, {"id": "1912.06941", "submitter": "Bernardo Martin-Iradi", "authors": "Bernardo Martin-Iradi, Stefan Ropke", "title": "A column-generation-based matheuristic for periodic train timetabling\n  with integrated passenger routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the periodic train timetabling problem is formulated using a\ntime-space graph formulation. Three solution methods are proposed and compared\nwhere solutions are built by what we define as a dive-and-cut-and-price\nprocedure. An LP relaxed version of the problem with a subset of constraints is\nsolved using column generation where each column corresponds to the train paths\nof a line. Violated constraints are added by separation and a heuristic process\nis applied to help to find integer solutions. The passenger travel time is\ncomputed based on a solution timetable and Benders' optimality cuts are\ngenerated allowing the method to integrate the routing of the passengers. We\npropose two large neighborhood search methods where the solution is iteratively\ndestroyed and repaired into a new one and one random iterative method. The\nproblem is tested on the morning rush hour period of the Regional and InterCity\ntrain network of Zealand, Denmark. The solution approaches show robust\nperformance in a variety of scenarios, being able to find good quality\nsolutions in terms of travel time and path length relatively fast. The\ninclusion of the proposed Benders' cuts provides stronger relaxations to the\nproblem. In addition, the graph formulation covers different real-life\nconstraints and has the potential to easily be extended to accommodate more\nconstraints.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 22:38:13 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 15:08:02 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 16:25:48 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 10:32:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Martin-Iradi", "Bernardo", ""], ["Ropke", "Stefan", ""]]}, {"id": "1912.06966", "submitter": "Bin Sheng", "authors": "Bin Sheng", "title": "FPT algorithms for generalized feedback vertex set problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An r-pseudoforest is a graph in which each component can be made into a\nforest by deleting at most r edges, and a d-quasi-forest is a graph in which\neach component can be made into a forest by deleting at most d vertices. In\nthis paper, we study the parameterized tractability of deleting minimum number\nof vertices to obtain r-pseudoforest and d-quasi-forest, generalizing the well\nstudied feedback vertex set problem. We first provide improved FPT algorithm\nand kernelization results for the r-pseudoforest deletion problem and then we\nshow that the d-quasi-forest deletion problem is also FPT.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 02:42:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sheng", "Bin", ""]]}, {"id": "1912.06983", "submitter": "Vaggos Chatziafratis", "authors": "Sara Ahmadian, Vaggos Chatziafratis, Alessandro Epasto, Euiwoong Lee,\n  Mohammad Mahdian, Konstantin Makarychev, Grigory Yaroslavtsev", "title": "Bisect and Conquer: Hierarchical Clustering via Max-Uncut Bisection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Clustering is an unsupervised data analysis method which has\nbeen widely used for decades. Despite its popularity, it had an underdeveloped\nanalytical foundation and to address this, Dasgupta recently introduced an\noptimization viewpoint of hierarchical clustering with pairwise similarity\ninformation that spurred a line of work shedding light on old algorithms (e.g.,\nAverage-Linkage), but also designing new algorithms. Here, for the maximization\ndual of Dasgupta's objective (introduced by Moseley-Wang), we present\npolynomial-time .4246 approximation algorithms that use Max-Uncut Bisection as\na subroutine. The previous best worst-case approximation factor in polynomial\ntime was .336, improving only slightly over Average-Linkage which achieves 1/3.\nFinally, we complement our positive results by providing APX-hardness (even for\n0-1 similarities), under the Small Set Expansion hypothesis.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:48:48 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Ahmadian", "Sara", ""], ["Chatziafratis", "Vaggos", ""], ["Epasto", "Alessandro", ""], ["Lee", "Euiwoong", ""], ["Mahdian", "Mohammad", ""], ["Makarychev", "Konstantin", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1912.07153", "submitter": "Yue Fu", "authors": "Yue Fu, Rong Du, Haibo Hu, Man Ho Au, Dagang Li", "title": "Matrix Bloom Filter: An Efficient Probabilistic Data Structure for\n  2-tuple Batch Lookup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing scale of big data, probabilistic structures receive\nincreasing popularity for efficient approximate storage and query processing.\nFor example, Bloom filters (BF) can achieve satisfactory performance for\napproximate membership existence query at the expense of false positives.\nHowever, a standard Bloom filter can only handle univariate data and single\nmembership existence query, which is insufficient for OLAP and machine learning\napplications. In this paper, we focus on a common multivariate data type,\nnamely, 2-tuples, or equivalently, key-value pairs. We design the matrix Bloom\nfilter as a high-dimensional extension of the standard Bloom filter. This new\nprobabilistic data structure can not only insert and lookup a single 2-tuple\nefficiently, but also support these operations efficiently in batches --- a key\nrequirement for OLAP and machine learning tasks. To further balance the\ninsertion and query efficiency for different workload patterns, we propose two\nvariants, namely, the maximum adaptive matrix BF and minimum storage matrix BF.\nThrough both theoretical and empirical studies, we show the performance of\nmatrix Bloom filter is superior on datasets with common statistical\ndistributions; and even without them, it just degrades to a standard Bloom\nfilter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 01:28:59 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Fu", "Yue", ""], ["Du", "Rong", ""], ["Hu", "Haibo", ""], ["Au", "Man Ho", ""], ["Li", "Dagang", ""]]}, {"id": "1912.07168", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Michael. I. Jordan", "title": "A Control-Theoretic Perspective on Optimal High-Order Optimization", "comments": "Improve the paper significantly; 45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a control-theoretic perspective on optimal tensor algorithms for\nminimizing a convex function in a finite-dimensional Euclidean space. Given a\nfunction $\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ that is convex and twice\ncontinuously differentiable, we study a closed-loop control system that is\ngoverned by the operators $\\nabla \\Phi$ and $\\nabla^2 \\Phi$ together with a\nfeedback control law $\\lambda(\\cdot)$ satisfying the algebraic equation\n$(\\lambda(t))^p\\|\\nabla\\Phi(x(t))\\|^{p-1} = \\theta$ for some $\\theta \\in (0,\n1)$. Our first contribution is to prove the existence and uniqueness of a local\nsolution to this system via the Banach fixed-point theorem. We present a simple\nyet nontrivial Lyapunov function that allows us to establish the existence and\nuniqueness of a global solution under certain regularity conditions and analyze\nthe convergence properties of trajectories. The rate of convergence is\n$O(1/t^{(3p+1)/2})$ in terms of objective function gap and $O(1/t^{3p})$ in\nterms of squared gradient norm. Our second contribution is to provide two\nalgorithmic frameworks obtained from discretization of our continuous-time\nsystem, one of which generalizes the large-step A-HPE framework and the other\nof which leads to a new optimal $p$-th order tensor algorithm. While our\ndiscrete-time analysis can be seen as a simplification and generalization of\nexisiting analysis, it is largely motivated by the aforementioned\ncontinuous-time analysis, demonstrating the fundamental role that the feedback\ncontrol plays in optimal acceleration and the clear advantage that the\ncontinuous-time perspective brings to algorithmic design. A highlight of our\nanalysis is that we show that all of the $p$-th order optimal tensor algorithms\nthat we discuss minimize the squared gradient norm at a rate of $O(k^{-3p})$,\nwhich complements the recent analysis.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 02:46:47 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 20:28:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lin", "Tianyi", ""], ["Jordan", "Michael. I.", ""]]}, {"id": "1912.07561", "submitter": "Grzegorz G{\\l}uch", "authors": "Grzegorz G{\\l}uch, R\\\"udiger Urbanke", "title": "Constructing a provably adversarially-robust classifier from a high\n  accuracy one", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning models with very high accuracy have been shown to be\nvulnerable to small, adversarially chosen perturbations of the input. Given\nblack-box access to a high-accuracy classifier $f$, we show how to construct a\nnew classifier $g$ that has high accuracy and is also robust to adversarial\n$\\ell_2$-bounded perturbations. Our algorithm builds upon the framework of\n\\textit{randomized smoothing} that has been recently shown to outperform all\nprevious defenses against $\\ell_2$-bounded adversaries. Using techniques like\nrandom partitions and doubling dimension, we are able to bound the adversarial\nerror of $g$ in terms of the optimum error. In this paper we focus on our\nconceptual contribution, but we do present two examples to illustrate our\nframework. We will argue that, under some assumptions, our bounds are optimal\nfor these cases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:19:59 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Urbanke", "R\u00fcdiger", ""]]}, {"id": "1912.07599", "submitter": "Ning Li", "authors": "Jianen Yan, Ning Li, Zhaoxin Zhang, Alex X. Liu, Jose Fernan Martinez,\n  Xin Yuan", "title": "Game Theory based Joint Task Offloading and Resources Allocation\n  Algorithm for Mobile Edge Computing", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.02182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) has emerged for reducing energy consumption and\nlatency by allowing mobile users to offload computationally intensive tasks to\nthe MEC server. Due to the spectrum reuse in small cell network, the inter-cell\ninterference has a great effect on MEC performances. In this paper, for\nreducing the energy consumption and latency of MEC, we propose a game theory\nbased approach to join task offloading decision and resources allocation\ntogether in the MEC system. In this algorithm, the offloading decision, the CPU\ncapacity adjustment, the transmission power control, and the network\ninterference management of mobile users are regarded as a game. In this game,\nbased on the best response strategy, each mobile user makes their own utility\nmaximum rather than the utility of the whole system. We prove that this game is\nan exact potential game and the Nash equilibrium (NE) of this game exists. For\nreaching the NE, the best response approach is applied. We calculate the best\nresponse of these three variables. Moreover, we investigate the properties of\nthis algorithm, including the convergence, the computational complexity, and\nthe Price of anarchy (PoA). The theoretical analysis shows that the inter-cell\ninterference affects on the performances of MEC greatly. The NE of this game is\nPareto efficiency. Finally, we evaluate the performances of this algorithm by\nsimulation. The simulation results illustrate that this algorithm is effective\nin improving the performances of the multi-user MEC system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:02:36 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Yan", "Jianen", ""], ["Li", "Ning", ""], ["Zhang", "Zhaoxin", ""], ["Liu", "Alex X.", ""], ["Martinez", "Jose Fernan", ""], ["Yuan", "Xin", ""]]}, {"id": "1912.07600", "submitter": "Ning Li", "authors": "Ning Li", "title": "A new Frequency Estimation Sketch for Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data stream applications, one of the critical issues is to estimate the\nfrequency of each item in the specific multiset. The multiset means that each\nitem in this set can appear multiple times. The data streams in many\napplications are high-speed streams which contain massive data, such as\nreal-time IP traffic, graph streams, web clicks and crawls, sensor database,\nand natural language processing (NLP) [2][6], etc. In these applications, the\nstream information needs to be recorded by the servers in real time. However,\nsince the data streams in these applications are high-speed, the accurate\nrecording and estimation of item frequencies is always impractical. An\nalternative approach for addressing this problem is to estimate the item\nfrequencies based on probabilistic data structures, and this approach has been\nwidely used in the high-speed data streams estimation [7][9]. Sketches is one\nof the typical probabilistic data structures, which are initially designed for\nthe estimation of item frequencies in data streams [10][15]. At present, the\nsketches have been used in many different scenarios, such as sparse\napproximation in compressed sensing [16], natural language processing [17, 18],\ndata graph [19, 20], and more [21]. In this paper, we mainly focus on the\nsketches used for frequency estimation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:16:37 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 01:51:21 GMT"}, {"version": "v3", "created": "Sat, 4 Jan 2020 14:22:49 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Li", "Ning", ""]]}, {"id": "1912.07629", "submitter": "Sitan Chen", "authors": "Sitan Chen, Jerry Li, Zhao Song", "title": "Learning Mixtures of Linear Regressions in Subexponential Time via\n  Fourier Moments", "comments": "83 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a mixture of linear regressions (MLRs).\nAn MLR is specified by $k$ nonnegative mixing weights $p_1, \\ldots, p_k$\nsumming to $1$, and $k$ unknown regressors $w_1,...,w_k\\in\\mathbb{R}^d$. A\nsample from the MLR is drawn by sampling $i$ with probability $p_i$, then\noutputting $(x, y)$ where $y = \\langle x, w_i \\rangle + \\eta$, where\n$\\eta\\sim\\mathcal{N}(0,\\varsigma^2)$ for noise rate $\\varsigma$. Mixtures of\nlinear regressions are a popular generative model and have been studied\nextensively in machine learning and theoretical computer science. However, all\nprevious algorithms for learning the parameters of an MLR require running time\nand sample complexity scaling exponentially with $k$.\n  In this paper, we give the first algorithm for learning an MLR that runs in\ntime which is sub-exponential in $k$. Specifically, we give an algorithm which\nruns in time $\\widetilde{O}(d)\\cdot\\exp(\\widetilde{O}(\\sqrt{k}))$ and outputs\nthe parameters of the MLR to high accuracy, even in the presence of nontrivial\nregression noise. We demonstrate a new method that we call \"Fourier moment\ndescent\" which uses univariate density estimation and low-degree moments of the\nFourier transform of suitable univariate projections of the MLR to iteratively\nrefine our estimate of the parameters. To the best of our knowledge, these\ntechniques have never been used in the context of high dimensional distribution\nlearning, and may be of independent interest. We also show that our techniques\ncan be used to give a sub-exponential time algorithm for learning mixtures of\nhyperplanes, a natural hard instance of the subspace clustering problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 19:00:19 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chen", "Sitan", ""], ["Li", "Jerry", ""], ["Song", "Zhao", ""]]}, {"id": "1912.07673", "submitter": "Wai Ming Tai", "authors": "Jasper C.H. Lee, Jerry Li, Christopher Musco, Jeff M. Phillips, Wai\n  Ming Tai", "title": "Finding the Mode of a Kernel Density Estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given points $p_1, \\dots, p_n$ in $\\mathbb{R}^d$, how do we find a point $x$\nwhich maximizes $\\frac{1}{n} \\sum_{i=1}^n e^{-\\|p_i - x\\|^2}$? In other words,\nhow do we find the maximizing point, or mode of a Gaussian kernel density\nestimation (KDE) centered at $p_1, \\dots, p_n$? Given the power of KDEs in\nrepresenting probability distributions and other continuous functions, the\nbasic mode finding problem is widely applicable. However, it is poorly\nunderstood algorithmically. Few provable algorithms are known, so practitioners\nrely on heuristics like the \"mean-shift\" algorithm, which are not guaranteed to\nfind a global optimum. We address this challenge by providing fast and provably\naccurate approximation algorithms for mode finding in both the low and high\ndimensional settings. For low dimension $d$, our main contribution is to reduce\nthe mode finding problem to a solving a small number of systems of polynomial\ninequalities. For high dimension $d$, we prove the first dimensionality\nreduction result for KDE mode finding, which allows for reduction to the low\ndimensional case. Our result leverages Johnson-Lindenstrauss random projection,\nKirszbraun's classic extension theorem, and perhaps surprisingly, the\nmean-shift heuristic for mode finding.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 20:08:35 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Lee", "Jasper C. H.", ""], ["Li", "Jerry", ""], ["Musco", "Christopher", ""], ["Phillips", "Jeff M.", ""], ["Tai", "Wai Ming", ""]]}, {"id": "1912.07820", "submitter": "Sainyam Galhotra Mr", "authors": "Sandhya Saisubramanian, Sainyam Galhotra and Shlomo Zilberstein", "title": "Balancing the Tradeoff Between Clustering Value and Interpretability", "comments": "Accepted at AIES 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph clustering groups entities -- the vertices of a graph -- based on their\nsimilarity, typically using a complex distance function over a large number of\nfeatures. Successful integration of clustering approaches in automated\ndecision-support systems hinges on the interpretability of the resulting\nclusters. This paper addresses the problem of generating interpretable\nclusters, given features of interest that signify interpretability to an\nend-user, by optimizing interpretability in addition to common clustering\nobjectives. We propose a $\\beta$-interpretable clustering algorithm that\nensures that at least $\\beta$ fraction of nodes in each cluster share the same\nfeature value. The tunable parameter $\\beta$ is user-specified. We also present\na more efficient algorithm for scenarios with $\\beta\\!=\\!1$ and analyze the\ntheoretical guarantees of the two algorithms. Finally, we empirically\ndemonstrate the benefits of our approaches in generating interpretable clusters\nusing four real-world datasets. The interpretability of the clusters is\ncomplemented by generating simple explanations denoting the feature values of\nthe nodes in the clusters, using frequent pattern mining.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:08:34 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 03:24:54 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 00:17:28 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Saisubramanian", "Sandhya", ""], ["Galhotra", "Sainyam", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1912.07957", "submitter": "C R Subramanian", "authors": "Abhiruk Lahiri and Joydeep Mukherjee and C.R. Subramanian", "title": "Approximating MIS over equilateral $B_1$-VPG graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximation algorithm for the maximum independent set (MIS)\nproblem over the class of equilateral $B_1$-VPG graphs. These are intersection\ngraphs of $L$-shaped planar objects % (and their rotations by multiples of\n$90^o$) with both arms of each object being equal. We obtain a $36(\\log\n2d)$-approximate algorithm running in $O(n(\\log n)^2)$ time for this problem,\nwhere $d$ is the ratio $d_{max}/d_{min}$ and $d_{max}$ and $d_{min}$ denote\nrespectively the maximum and minimum length of any arm in the input equilateral\n$L$-representation of the graph. In particular, we obtain $O(1)$-factor\napproximation of MIS for $B_1$-VPG -graphs for which the ratio $d$ is bounded\nby a constant. % formed by unit length $L$-shapes. In fact, algorithm can be\ngeneralized to an $O(n(\\log n)^2)$ time and a $36(\\log 2d_x)(\\log\n2d_y)$-approximate MIS algorithm over arbitrary $B_1$-VPG graphs. Here, $d_x$\nand $d_y$ denote respectively the analogues of $d$ when restricted to only\nhorizontal and vertical arms of members of the input. This is an improvement\nover the previously best $n^\\epsilon$-approximate algorithm \\cite{FoxP} (for\nsome fixed $\\epsilon>0$), unless the ratio $d$ is exponentially large in $n$.\nIn particular, $O(1)$-approximation of MIS is achieved for graphs with\n$\\max\\{d_x,d_y\\}=O(1)$.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:15:30 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Lahiri", "Abhiruk", ""], ["Mukherjee", "Joydeep", ""], ["Subramanian", "C. R.", ""]]}, {"id": "1912.08066", "submitter": "Panayiotis Danassis", "authors": "Panayiotis Danassis, Marija Sakota, Aris Filos-Ratsikas, Boi Faltings", "title": "Putting Ridesharing to the Test: Efficient and Scalable Solutions and\n  the Power of Dynamic Vehicle Relocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a systematic evaluation of a diverse set of algorithms for the\nridesharing problem which is, to the best of our knowledge, one of the largest\nand most comprehensive to date. In particular, we evaluate 12 different\nalgorithms over 12 metrics related to global efficiency, complexity, passenger,\ndriver, and platform incentives. Our evaluation setting is specifically\ndesigned to resemble reality as closely as possible. We achieve this by (a)\nusing actual data from the NYC's yellow taxi trip records, both for modeling\ncustomer requests, and taxis (b) following closely the pricing model employed\nby ridesharing platforms and (c) running our simulations to the scale of the\nactual problem faced by the ridesharing platforms.\n  Our results provide a clear-cut recommendation to ridesharing platforms on\nwhich solutions can be employed in practice and demonstrate the large potential\nfor efficiency gains. Moreover, we show that simple, lightweight relocation\nschemes -- which can be used as independent components to any ridesharing\nalgorithm -- can significantly improve Quality of Service metrics by up to 50%.\nAs a highlight of our findings, we identify a scalable, on-device heuristic\nthat offers an efficient, end-to-end solution for the Dynamic Ridesharing and\nFleet Relocation problem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:10:03 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:34:19 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Danassis", "Panayiotis", ""], ["Sakota", "Marija", ""], ["Filos-Ratsikas", "Aris", ""], ["Faltings", "Boi", ""]]}, {"id": "1912.08258", "submitter": "Daniel Lemire", "authors": "Thomas Mueller Graf and Daniel Lemire", "title": "Xor Filters: Faster and Smaller Than Bloom and Cuckoo Filters", "comments": null, "journal-ref": "Journal of Experimental Algorithmics 25 (1), 2020", "doi": "10.1145/3376122", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Bloom filter provides fast approximate set membership while using little\nmemory. Engineers often use these filters to avoid slow operations such as disk\nor network accesses. As an alternative, a cuckoo filter may need less space\nthan a Bloom filter and it is faster. Chazelle et al. proposed a generalization\nof the Bloom filter called the Bloomier filter. Dietzfelbinger and Pagh\ndescribed a variation on the Bloomier filter that can be used effectively for\napproximate membership queries. It has never been tested empirically, to our\nknowledge. We review an efficient implementation of their approach, which we\ncall the xor filter. We find that xor filters can be faster than Bloom and\ncuckoo filters while using less memory. We further show that a more compact\nversion of xor filters (xor+) can use even less space than highly compact\nalternatives (e.g., Golomb-compressed sequences) while providing speeds\ncompetitive with Bloom filters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 20:07:53 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 16:02:25 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 18:27:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Graf", "Thomas Mueller", ""], ["Lemire", "Daniel", ""]]}, {"id": "1912.08322", "submitter": "Lu Chen", "authors": "Lu Chen, Chengfei Liu, Rui Zhou, Jiajie Xu, Jianxin Li", "title": "Finding Effective Geo-Social Group for Impromptu Activity with Multiple\n  Demands", "comments": "12 Pages with 1 Page References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-social group search aims to find a group of people proximate to a\nlocation while socially related. One of the driven applications for geo-social\ngroup search is organizing an impromptu activity. This is because the social\ncohesiveness of a found geo-social group ensures a good communication\natmosphere and the spatial closeness of the geo-social group reduces the\npreparation time for the activity. Most existing works treat geo-social group\nsearch as a problem that finds a group satisfying a single social constraint\nwhile optimizing the spatial proximity. However, when an impromptu activity has\nadditional demands on attendees, e.g., the activity requires that the attendees\nhave certain set of skills, the existing works cannot find an effective\ngeo-social group efficiently. In this paper, we study how to find a group that\nis most proximate to a query location while satisfying multiple constraints.\nSpecifically, the multiple constraints on which we focus include social\nconstraint, size constraint and keyword constraint. We propose a novel search\nframework which first effectively narrows down the search space with\ntheoretical guarantees and then efficiently finds the optimum result. Although\nour model considers multiple constraints, novel techniques devised in this\npaper ensure that search cost is equivalent to parameterized constant times of\none time social constraint checking on a vastly restricted search space. We\nconduct extensive experiments on both real and semi-synthetic datasets for\ndemonstrating the efficiency of the proposed search algorithm. To evaluate the\neffectiveness, we conduct two case studies on real datasets, demonstrating the\nsuperiority of our proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 00:01:27 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chen", "Lu", ""], ["Liu", "Chengfei", ""], ["Zhou", "Rui", ""], ["Xu", "Jiajie", ""], ["Li", "Jianxin", ""]]}, {"id": "1912.08674", "submitter": "Tillmann Miltzow", "authors": "Mikkel Abrahamsen and Tillmann Miltzow", "title": "Dynamic Toolbox for ETRINV", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, various natural algorithmic problems have been shown to be $\\exists\n\\mathbb{R}$-complete. The reduction relied in many cases on the $\\exists\n\\mathbb{R}$-completeness of the problem ETR-INV, which served as a useful\nintermediate problem. Often some strengthening and modification of ETR-INV was\nrequired. This lead to a cluttered situation where no paper included all the\nprevious details. Here, we give a streamlined exposition in a self-contained\nmanner. We also explain and prove various universality results regarding\nETR-INV. These notes should not be seen as a research paper with new results.\nHowever, we do describe some refinements of earlier results which might be\nuseful for future research. We plan to extend and update this exposition as\nseems fit.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 15:46:01 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Abrahamsen", "Mikkel", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "1912.08805", "submitter": "Jorge Garza-Vargas", "authors": "Jess Banks, Jorge Garza-Vargas, Archit Kulkarni, Nikhil Srivastava", "title": "Pseudospectral Shattering, the Sign Function, and Diagonalization in\n  Nearly Matrix Multiplication Time", "comments": "78 pages, 3 figures, comments welcome. Slightly edited intro from\n  previous version + explicit statement of forward error Theorem (Corolary\n  1.7). Minor corrections made", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.DS cs.NA math.FA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit a randomized algorithm which given a square $n\\times n$ complex\nmatrix $A$ with $\\|A\\| \\le 1$ and $\\delta>0$, computes with high probability\ninvertible $V$ and diagonal $D$ such that $$\\|A-VDV^{-1}\\|\\le \\delta $$ and\n$\\|V\\|\\|V^{-1}\\| \\le O(n^{2.5}/\\delta)$ in $O(T_{MM}\\>(n)\\log^2(n/\\delta))$\narithmetic operations on a floating point machine with $O(\\log^4(n/\\delta)\\log\nn)$ bits of precision. Here $T_{MM}\\>(n)$ is the number of arithmetic\noperations required to multiply two $n\\times n$ complex matrices numerically\nstably, with $T_{MM}\\,\\,(n)=O(n^{\\omega+\\eta}\\>\\>)$ for every $\\eta>0$, where\n$\\omega$ is the exponent of matrix multiplication. The algorithm is a variant\nof the spectral bisection algorithm in numerical linear algebra (Beavers and\nDenman, 1974). This running time is optimal up to polylogarithmic factors, in\nthe sense that verifying that a given similarity diagonalizes a matrix requires\nat least matrix multiplication time. It significantly improves best previously\nprovable running times of $O(n^{10}/\\delta^2)$ arithmetic operations for\ndiagonalization of general matrices (Armentano et al., 2018), and (w.r.t.\ndependence on $n$) $O(n^3)$ arithmetic operations for Hermitian matrices\n(Parlett, 1998).\n  The proof rests on two new ingredients. (1) We show that adding a small\ncomplex Gaussian perturbation to any matrix splits its pseudospectrum into $n$\nsmall well-separated components. This implies that the eigenvalues of the\nperturbation have a large minimum gap, a property of independent interest in\nrandom matrix theory. (2) We rigorously analyze Roberts' Newton iteration\nmethod for computing the matrix sign function in finite arithmetic, itself an\nopen problem in numerical analysis since at least 1986. This is achieved by\ncontrolling the evolution the iterates' pseudospectra using a carefully chosen\nsequence of shrinking contour integrals in the complex plane.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:59:08 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 17:58:01 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 18:22:37 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Banks", "Jess", ""], ["Garza-Vargas", "Jorge", ""], ["Kulkarni", "Archit", ""], ["Srivastava", "Nikhil", ""]]}, {"id": "1912.08854", "submitter": "Yuan Su", "authors": "Andrew M. Childs, Yuan Su, Minh C. Tran, Nathan Wiebe, Shuchen Zhu", "title": "A Theory of Trotter Error", "comments": "82 pages, 5 figures. Enhanced version of the article published in\n  Physical Review X at\n  http://journals.aps.org/prx/abstract/10.1103/PhysRevX.11.011020", "journal-ref": "Phys. Rev. X 11, 011020 (2021)", "doi": "10.1103/PhysRevX.11.011020", "report-no": null, "categories": "quant-ph cond-mat.str-el cs.DS cs.NA math.NA physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lie-Trotter formula, together with its higher-order generalizations,\nprovides a direct approach to decomposing the exponential of a sum of\noperators. Despite significant effort, the error scaling of such product\nformulas remains poorly understood. We develop a theory of Trotter error that\novercomes the limitations of prior approaches based on truncating the\nBaker-Campbell-Hausdorff expansion. Our analysis directly exploits the\ncommutativity of operator summands, producing tighter error bounds for both\nreal- and imaginary-time evolutions. Whereas previous work achieves similar\ngoals for systems with geometric locality or Lie-algebraic structure, our\napproach holds in general. We give a host of improved algorithms for digital\nquantum simulation and quantum Monte Carlo methods, including simulations of\nsecond-quantized plane-wave electronic structure, $k$-local Hamiltonians,\nrapidly decaying power-law interactions, clustered Hamiltonians, the transverse\nfield Ising model, and quantum ferromagnets, nearly matching or even\noutperforming the best previous results. We obtain further speedups using the\nfact that product formulas can preserve the locality of the simulated system.\nSpecifically, we show that local observables can be simulated with complexity\nindependent of the system size for power-law interacting systems, which implies\na Lieb-Robinson bound as a byproduct. Our analysis reproduces known tight\nbounds for first- and second-order formulas. Our higher-order bound\noverestimates the complexity of simulating a one-dimensional Heisenberg model\nwith an even-odd ordering of terms by only a factor of $5$, and is close to\ntight for power-law interactions and other orderings of terms. This suggests\nthat our theory can accurately characterize Trotter error in terms of both\nasymptotic scaling and constant prefactor.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:36:10 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 19:00:02 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 19:00:02 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Childs", "Andrew M.", ""], ["Su", "Yuan", ""], ["Tran", "Minh C.", ""], ["Wiebe", "Nathan", ""], ["Zhu", "Shuchen", ""]]}, {"id": "1912.08880", "submitter": "Mehrdad Moharrami", "authors": "Mehrdad Moharrami, Cristopher Moore, and Jiaming Xu", "title": "The Planted Matching Problem: Phase Transitions and Exact Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering a planted matching in randomly weighted\ncomplete bipartite graphs $K_{n,n}$. For some unknown perfect matching $M^*$,\nthe weight of an edge is drawn from one distribution $P$ if $e \\in M^*$ and\nanother distribution $Q$ if $e \\notin M^*$. Our goal is to infer $M^*$, exactly\nor approximately, from the edge weights. In this paper we take\n$P=\\exp(\\lambda)$ and $Q=\\exp(1/n)$, in which case the maximum-likelihood\nestimator of $M^*$ is the minimum-weight matching $M_{\\text{min}}$. We obtain\nprecise results on the overlap between $M^*$ and $M_{\\text{min}}$, i.e., the\nfraction of edges they have in common. For $\\lambda \\ge 4$ we have almost\nperfect recovery, with overlap $1-o(1)$ with high probability. For $\\lambda <\n4$ the expected overlap is an explicit function $\\alpha(\\lambda) < 1$: we\ncompute it by generalizing Aldous' celebrated proof of the $\\zeta(2)$\nconjecture for the un-planted model, using local weak convergence to relate\n$K_{n,n}$ to a type of weighted infinite tree, and then deriving a system of\ndifferential equations from a message-passing algorithm on this tree.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:42:29 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:01:56 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 01:53:55 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2020 05:59:42 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2020 22:00:58 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Moharrami", "Mehrdad", ""], ["Moore", "Cristopher", ""], ["Xu", "Jiaming", ""]]}, {"id": "1912.08950", "submitter": "Maciej Besta", "authors": "Maciej Besta, Simon Weber, Lukas Gianinazzi, Robert Gerstenberger,\n  Andrey Ivanov, Yishai Oltchik, Torsten Hoefler", "title": "Slim Graph: Practical Lossy Graph Compression for Approximate Graph\n  Processing, Storage, and Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Slim Graph: the first programming model and framework for\npractical lossy graph compression that facilitates high-performance approximate\ngraph processing, storage, and analytics. Slim Graph enables the developer to\nexpress numerous compression schemes using small and programmable compression\nkernels that can access and modify local parts of input graphs. Such kernels\nare executed in parallel by the underlying engine, isolating developers from\ncomplexities of parallel programming. Our kernels implement novel graph\ncompression schemes that preserve numerous graph properties, for example\nconnected components, minimum spanning trees, or graph spectra. Finally, Slim\nGraph uses statistical divergences and other metrics to analyze the accuracy of\nlossy graph compression. We illustrate both theoretically and empirically that\nSlim Graph accelerates numerous graph algorithms, reduces storage used by graph\ndatasets, and ensures high accuracy of results. Slim Graph may become the\ncommon ground for developing, executing, and analyzing emerging lossy graph\ncompression schemes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:48:29 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Besta", "Maciej", ""], ["Weber", "Simon", ""], ["Gianinazzi", "Lukas", ""], ["Gerstenberger", "Robert", ""], ["Ivanov", "Andrey", ""], ["Oltchik", "Yishai", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.08951", "submitter": "Amos Beimel", "authors": "Amos Beimel, Aleksandra Korolova, Kobbi Nissim, Or Sheffet, Uri\n  Stemmer", "title": "The power of synergy in differential privacy: Combining a small curator\n  with local randomizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the desire to bridge the utility gap between local and trusted\ncurator models of differential privacy for practical applications, we initiate\nthe theoretical study of a hybrid model introduced by \"Blender\" [Avent et al.,\\\nUSENIX Security '17], in which differentially private protocols of n agents\nthat work in the local-model are assisted by a differentially private curator\nthat has access to the data of m additional users. We focus on the regime where\nm << n and study the new capabilities of this (m,n)-hybrid model. We show that,\ndespite the fact that the hybrid model adds no significant new capabilities for\nthe basic task of simple hypothesis-testing, there are many other tasks (under\na wide range of parameters) that can be solved in the hybrid model yet cannot\nbe solved either by the curator or by the local-users separately. Moreover, we\nexhibit additional tasks where at least one round of interaction between the\ncurator and the local-users is necessary -- namely, no hybrid model protocol\nwithout such interaction can solve these tasks. Taken together, our results\nshow that the combination of the local model with a small curator can become\npart of a promising toolkit for designing and implementing differential\nprivacy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:49:11 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 17:49:55 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Beimel", "Amos", ""], ["Korolova", "Aleksandra", ""], ["Nissim", "Kobbi", ""], ["Sheffet", "Or", ""], ["Stemmer", "Uri", ""]]}, {"id": "1912.09144", "submitter": "Ernst Althaus", "authors": "Ernst Althaus and Sarah Ziegler", "title": "Optimal Tree Decompositions Revisited: A Simpler Linear-Time FPT\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1996, Bodlaender showed the celebrated result that an optimal tree\ndecomposition of a graph of bounded treewidth can be found in linear time. The\nalgorithm is based on an algorithm of Bodlaender and Kloks that computes an\noptimal tree decomposition given a non-optimal tree decomposition of bounded\nwidth. Both algorithms, in particular the second, are hardly accessible. In our\nreview, we present them in a much simpler way than the original presentations.\nIn our description of the second algorithm, we start by explaining how all tree\ndecompositions of subtrees defined by the nodes of the given tree decomposition\ncan be enumerated. We group tree decompositions into equivalence classes\ndepending on the current node of the given tree decomposition, such that it\nsuffices to enumerate one tree decomposition per equivalence class and, for\neach node of the given tree decomposition, there are only a constant number of\nclasses which can be represented in constant space. Our description of the\nfirst algorithm further simplifies Perkovic and Reed's simplification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:53:21 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:29:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Althaus", "Ernst", ""], ["Ziegler", "Sarah", ""]]}, {"id": "1912.09170", "submitter": "Bertrand Simon", "authors": "Bertrand Simon, Joachim Falk, Nicole Megow, and J\\\"urgen Teich", "title": "Energy Minimization in DAG Scheduling on MPSoCs at Run-Time: Theory and\n  Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static (offline) techniques for mapping applications given by task graphs to\nMPSoC systems often deliver overly pessimistic and thus suboptimal results\nw.r.t. exploiting time slack in order to minimize the energy consumption. This\nholds true in particular in case computation times of tasks may be\nworkload-dependent and becoming known only at runtime or in case of\nconditionally executed tasks or scenarios. This paper studies and\nquantitatively evaluates different classes of algorithms for scheduling\nperiodic applications given by task graphs (i.e., DAGs) with precedence\nconstraints and a global deadline on homogeneous MPSoCs purely at runtime on a\nper-instance base. We present and analyze algorithms providing provably optimal\nresults as well as approximation algorithms with proven guarantees on the\nachieved energy savings. For problem instances taken from realistic embedded\nsystem benchmarks as well as synthetic scalable problems, we provide results on\nthe computation time and quality of each algorithm to perform a) scheduling and\nb) voltage/speed assignments for each task at runtime. In our portfolio, we\ndistinguish as well continuous and discrete speed (e.g., DVFS-related)\nassignment problems. In summary, the presented ties between theory (algorithmic\ncomplexity and optimality) and execution time analysis deliver important\ninsights on the practical usability of the presented algorithms for runtime\noptimization of task scheduling and speed assignment on MPSoCs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 13:04:53 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Simon", "Bertrand", ""], ["Falk", "Joachim", ""], ["Megow", "Nicole", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "1912.09264", "submitter": "Yang Li", "authors": "Yang Li and Hongbo Li", "title": "Improved quantum algorithm for the random subset sum problem", "comments": "arXiv admin note: text overlap with arXiv:1907.04295 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving random subset sum instances plays an important role in constructing\ncryptographic systems. For the random subset sum problem, in 2013 Bernstein et\nal. proposed a quantum algorithm with heuristic time complexity\n$\\widetilde{O}(2^{0.241n})$, where the \"$\\widetilde{O}$\" symbol is used to omit\npoly($\\log n$) factors. In 2018, Helm and May proposed another quantum\nalgorithm that reduces the heuristic time and memory complexity to\n$\\widetilde{O}(2^{0.226n})$. In this paper, a new quantum algorithm is\nproposed, with heuristic time and memory complexity\n$\\widetilde{O}(2^{0.209n})$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 05:51:03 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 18:17:30 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Li", "Yang", ""], ["Li", "Hongbo", ""]]}, {"id": "1912.09360", "submitter": "Jean-Charles Regin", "authors": "Nicolas Isoart and Jean-Charles R\\'egin", "title": "Imposing edges in Minimum Spanning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the consequences of imposing edges in $T$ a minimum\nspanning tree. We prove that the sum of the replacement costs in $T$ of the\nimposed edges is a lower bounds of the additional costs. More precisely if\nr-cost$(T,e)$ is the replacement cost of the edge $e$, we prove that if we\nimpose a set $I$ of nontree edges of $T$ then $\\sum_{e \\in I} $ r-cost$(T,e)\n\\leq$ cost$(T_{e \\in I})$, where $I$ is the set of imposed edges and $T_{e \\in\nI}$ a minimum spanning tree containing all the edges of $I$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:43:56 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Isoart", "Nicolas", ""], ["R\u00e9gin", "Jean-Charles", ""]]}, {"id": "1912.09552", "submitter": "Tien Mai", "authors": "Tien Mai and Patrick Jaillet", "title": "Robust Multi-product Pricing under General Extreme Value Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study robust versions of pricing problems where customers choose products\naccording to a general extreme value (GEV) choice model, and the choice\nparameters are not given exactly but lie in an uncertainty set. We show that,\nwhen the robust problem is unconstrained and the price sensitivity parameters\nare homogeneous, the robust optimal prices have a constant markup over products\nand we provide formulas that allow to compute this constant markup by binary\nsearch. We also show that, in the case that the price sensitivity parameters\nare only homogeneous in each subset of the products and the uncertainty set is\nrectangular, the robust problem can be converted into a deterministic pricing\nproblem and the robust optimal prices have a constant markup in each subset,\nand we also provide explicit formulas to compute them. For constrained pricing\nproblems, we propose a formulation where, instead of requiring that the\nexpected sale constraints be satisfied, we add a penalty cost to the objective\nfunction for violated constraints. We then show that the robust pricing problem\nwith over-expected-sale penalties can be reformulated as a convex optimization\nprogram where the purchase probabilities are the decision variables. We provide\nnumerical results for the logit and nested logit model to illustrate the\nadvantages of our approach. Our results generally hold for any arbitrary GEV\nmodel, including the multinomial logit, nested or cross-nested logit.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:32:50 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Mai", "Tien", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1912.09596", "submitter": "Stefan Zellmann", "authors": "Stefan Zellmann", "title": "Comparing Hierarchical Data Structures for Sparse Volume Rendering with\n  Empty Space Skipping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empty space skipping can be efficiently implemented with hierarchical data\nstructures such as k-d trees and bounding volume hierarchies. This paper\ncompares several recently published hierarchical data structures with regard to\nconstruction and rendering performance. The papers that form our prior work\nhave primarily focused on interactively building the data structures and only\nshowed that rendering performance is superior to using simple acceleration data\nstructures such as uniform grids with macro cells. In the area of surface ray\ntracing, there exists a trade-off between construction and rendering\nperformance of hierarchical data structures. In this paper we present\nperformance comparisons for several empty space skipping data structures in\norder to determine if such a trade-off also exists for volume rendering with\nuniform data topologies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:33:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zellmann", "Stefan", ""]]}, {"id": "1912.09605", "submitter": "Yuichi Nagata", "authors": "Yuichi Nagata and Shinji Imahori", "title": "An Efficient Algorithm for the Escherization Problem in the Polygon\n  Representation", "comments": "This version has been submitted to an international journal", "journal-ref": "Algorithmica 82, 2502-2534 (2020)", "doi": "10.1007/s00453-020-00695-6", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Escherization problem, given a closed figure in a plane, the objective\nis to find a closed figure that is as close as possible to the input figure and\ntiles the plane. Koizumi and Sugihara's formulation reduces this problem to an\neigenvalue problem in which the tile and input figures are represented as\n$n$-point polygons. In their formulation, the same number of points are\nassigned to every tiling edge, which forms a tiling template, to parameterize\nthe tile shape. By considering all possible configurations for the assignment\nof the $n$ points to the tiling edges, we can achieve much flexibility in terms\nof the possible tile shapes and the quality of the optimal tile shape improves\ndrastically, at the cost of enormous computational effort. In this paper, we\npropose an efficient algorithm to find the optimal tile shape for this extended\nformulation of the Escherization problem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 01:22:28 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Nagata", "Yuichi", ""], ["Imahori", "Shinji", ""]]}, {"id": "1912.09662", "submitter": "Zeev Nutov", "authors": "Amir Belgi and Zeev Nutov", "title": "An $\\tilde{O}(\\log^2 n)$-approximation algorithm for $2$-edge-connected\n  dominating set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Connected Dominating Set problem we are given a graph $G=(V,E)$ and\nseek a minimum size dominating set $S \\subseteq V$ such that the subgraph\n$G[S]$ of $G$ induced by $S$ is connected. In the $2$-Edge-Connected Dominating\nSet problem $G[S]$ should be $2$-edge-connected. We give the first non-trivial\napproximation algorithm for this problem, with expected approximation ratio\n$\\tilde{O}(\\log^2n)$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 06:55:42 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Belgi", "Amir", ""], ["Nutov", "Zeev", ""]]}, {"id": "1912.09783", "submitter": "Chundong Wang", "authors": "Chundong Wang, Gunavaran Brihadiswarn, Xingbin Jiang and Sudipta\n  Chattopadhyay", "title": "Circ-Tree: A B+-Tree Variant with Circular Design for Persistent Memory", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several B+-tree variants have been developed to exploit the performance\npotential of byte-addressable non-volatile memory (NVM). In this paper, we\nattentively investigate the properties of B+-tree and find that, a conventional\nB+-tree node is a linear structure in which key-value (KV) pairs are maintained\nfrom the zero offset of the node. These pairs are shifted in a unidirectional\nfashion for insertions and deletions. Inserting and deleting one KV pair may\ninflict a large amount of write amplifications due to shifting KV pairs. This\nbadly impairs the performance of in-NVM B+-tree. In this paper, we propose a\nnovel circular design for B+-tree. With regard to NVM's byte-addressability,\nour Circ-tree design embraces tree nodes in a circular structure without a\nfixed base address, and bidirectionally shifts KV pairs in a node for\ninsertions and deletions to minimize write amplifications. We have implemented\na prototype for Circ-Tree and conducted extensive experiments. Experimental\nresults show that Circ-Tree significantly outperforms two state-of-the-art\nin-NVM B+-tree variants, i.e., NV-tree and FAST+FAIR, by up to 1.6x and 8.6x,\nrespectively, in terms of write performance. The end-to-end comparison by\nrunning YCSB to KV store systems built on NV-tree, FAST+FAIR, and Circ-Tree\nreveals that Circ-Tree yields up to 29.3% and 47.4% higher write performance,\nrespectively, than NV-tree and FAST+FAIR.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 12:16:29 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 08:33:09 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wang", "Chundong", ""], ["Brihadiswarn", "Gunavaran", ""], ["Jiang", "Xingbin", ""], ["Chattopadhyay", "Sudipta", ""]]}, {"id": "1912.10140", "submitter": "Blerina Sinaimeri", "authors": "Angelo Monti and Blerina Sinaimeri", "title": "String factorisations with maximum or minimum dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two problems concerning string factorisation.\nSpecifically given a string $w$ and an integer $k$ find a factorisation of $w$\nwhere each factor has length bounded by $k$ and has the minimum (the FmD\nproblem) or the maximum (the FMD problem) number of different factors. The FmD\nhas been proved to be NP-hard even if $k=2$ in [9] and for this case we provide\na $3/2$-approximation algorithm. The FMD problem, up to our knowledge has not\nbeen considered in the literature. We show that this problem is NP-hard for any\n$k\\geq 3$. In view of this we propose a $2$-approximation algorithm (for any\n$k$) an exact exponential algorithm. We conclude with some open problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 23:12:09 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Monti", "Angelo", ""], ["Sinaimeri", "Blerina", ""]]}, {"id": "1912.10486", "submitter": "William Lochet", "authors": "William Lochet", "title": "A Polynomial Time Algorithm for the $k$-Disjoint Shortest Paths Problem", "comments": "To appear in SODA 2021. Revised following referees' comments and\n  changed the denomination of the problem (and thus the title) to follow\n  existing literature", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disjoint paths problem is a fundamental problem in algorithmic graph\ntheory and combinatorial optimization. For a given graph $G$ and a set of $k$\npairs of terminals in $G$, it asks for the existence of $k$ vertex-disjoint\npaths connecting each pair of terminals. The proof of Robertson and Seymour\n[JCTB 1995] of the existence of an $n^3$ algorithm for any fixed $k$ is one of\nthe highlights of their Graph Minors project. In this paper, we focus on the\nversion of the problem where all the paths are required to be shortest paths.\nThis problem, called the disjoint shortest paths problem, was introduced by\nEilam-Tzoreff [DAM 1998] where she proved that the case $k = 2$ admits a\npolynomial time algorithm. This problem has received some attention lately,\nespecially since the proof of the existence of a polynomial time algorithm in\nthe directed case when $k = 2$ by B\\'erczi and Kobayashi [ESA 2017]. However,\nthe existence of a polynomial algorithm when $k = 3$ in the undirected version\nremained open since 1998. In this paper we show that for any fixed $k$, the\ndisjoint shortest paths problem admits a polynomial time algorithm. In fact for\nany fixed $C$, the algorithm can be extended to treat the case where each path\nconnecting the pair $(s,t)$ has length at most $d(s,t) + C$.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 17:26:37 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 15:53:20 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Lochet", "William", ""]]}, {"id": "1912.10497", "submitter": "Alireza Farhadi", "authors": "Alireza Farhadi, MohammadTaghi Hajiaghayi, Tung Mai, Anup Rao, Ryan A.\n  Rossi", "title": "Approximate Maximum Matching in Random Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of finding a maximum matching in the\nsemi-streaming model when edges arrive in a random order. In the semi-streaming\nmodel, an algorithm receives a stream of edges and it is allowed to have a\nmemory of $\\tilde{O}(n)$ where $n$ is the number of vertices in the graph. A\nrecent inspiring work by Assadi et al. shows that there exists a streaming\nalgorithm with the approximation ratio of $\\frac{2}{3}$ that uses\n$\\tilde{O}(n^{1.5})$ memory. However, the memory of their algorithm is much\nlarger than the memory constraint of the semi-streaming algorithms. In this\nwork, we further investigate this problem in the semi-streaming model, and we\npresent simple algorithms for approximating maximum matching in the\nsemi-streaming model. Our main results are as follows.\n  We show that there exists a single-pass deterministic semi-streaming\nalgorithm that finds a $\\frac{3}{5} (= 0.6)$ approximation of the maximum\nmatching in bipartite graphs using $\\tilde{O}(n)$ memory. This result\nsignificantly outperforms the state-of-the-art result of Konrad that finds a\n$0.539$ approximation of the maximum matching using $\\tilde{O}(n)$ memory.\n  By giving a black-box reduction from finding a matching in general graphs to\nfinding a matching in bipartite graphs, we show there exists a single-pass\ndeterministic semi-streaming algorithm that finds a $\\frac{6}{11} (\\approx\n0.545)$ approximation of the maximum matching in general graphs, improving upon\nthe state-of-art result $0.506$ approximation by Gamlath et al.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 18:00:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Mai", "Tung", ""], ["Rao", "Anup", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1912.10769", "submitter": "Franziska Eberle", "authors": "Franziska Eberle, Nicole Megow, Kevin Schewior", "title": "Optimally handling commitment issues in online throughput maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental online scheduling problem in which jobs with\nprocessing times and deadlines arrive online over time at their release dates.\nThe task is to determine a feasible preemptive schedule on a single machine\nthat maximizes the number of jobs that complete before their deadline. Due to\nstrong impossibility results for competitive analysis, it is commonly required\nthat jobs contain some slack $\\varepsilon>0$, which means that the feasible\ntime window for scheduling a job is at least $1+\\varepsilon$ times its\nprocessing time. In this paper, we resolve the question on how to handle\ncommitment requirements which enforce that a scheduler has to guarantee at a\ncertain point in time the completion of admitted jobs. This is very relevant,\ne.g., in providing cloud-computing services and disallows last-minute\nrejections of critical tasks. We give an algorithm with an optimal competitive\nratio of $\\Theta(1/\\varepsilon)$ for the online throughput maximization problem\nwhen a scheduler must commit upon starting a job. Somewhat surprisingly, this\nis the same optimal performance bound (up to constants) as for scheduling\nwithout commitment. If commitment decisions must be made before a job's slack\nbecomes less than a $\\delta$-fraction of its size, we prove a competitive ratio\nof $\\mathcal{O}(\\varepsilon/((\\varepsilon - \\delta)\\delta))$ for $0 < \\delta <\n\\varepsilon$. This result interpolates between commitment upon starting a job\nand commitment upon arrival. For the latter commitment model, it is known that\nno (randomized) online algorithms does admit any bounded competitive ratio.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:44:21 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Eberle", "Franziska", ""], ["Megow", "Nicole", ""], ["Schewior", "Kevin", ""]]}, {"id": "1912.10989", "submitter": "Tuukka Korhonen", "authors": "Tuukka Korhonen", "title": "Finding Optimal Triangulations Parameterized by Edge Clique Cover", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph problems can be formulated as a task of finding an optimal\ntriangulation of a given graph with respect to some notion of optimality. In\nthis paper we give algorithms to such problems parameterized by the size of a\nminimum edge clique cover ($cc$) of the graph. The parameter $cc$ is both\nnatural and well-motivated in many problems on this setting. For example, in\nthe perfect phylogeny problem $cc$ is at most the number of taxa, in fractional\nhypertreewidth $cc$ is at most the number of hyperedges, and in treewidth of\nBayesian networks $cc$ is at most the number of non-root nodes of the Bayesian\nnetwork.\n  Our results are based on the framework of potential maximal cliques. We show\nthat the number of minimal separators of graphs is at most $2^{cc}$ and the\nnumber of potential maximal cliques is at most $3^{cc}$. Furthermore, these\nobjects can be listed in times $O^*(2^{cc})$ and $O^*(3^{cc})$, respectively,\neven when no edge clique cover is given as input; the $O^*(\\cdot)$ notation\nomits factors polynomial in the input size. Using these enumeration algorithms\nwe obtain $O^*(3^{cc})$ time algorithms for problems in the potential maximal\nclique framework, including for example treewidth, minimum fill-in, and\nfeedback vertex set. We also obtain an $O^*(3^m)$ time algorithm for fractional\nhypertreewidth, where $m$ is the number of hyperedges. In the case when an edge\nclique cover of size $cc'$ is given as an input we further improve the time\ncomplexity to $O^*(2^{cc'})$ for treewidth, minimum fill-in, and chordal\nsandwich. This implies an $O^*(2^n)$ time algorithm for perfect phylogeny,\nwhere $n$ is the number of taxa. We also give polynomial space algorithms with\ntime complexities $O^*(9^{cc'})$ and $O^*(9.001^{cc})$ for problems in this\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:31:04 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:05:17 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Korhonen", "Tuukka", ""]]}, {"id": "1912.11071", "submitter": "Samuel Hopkins", "authors": "Yeshwanth Cherapanamjeri, Samuel B. Hopkins, Tarun Kathuria, Prasad\n  Raghavendra, Nilesh Tripuraneni", "title": "Algorithms for Heavy-Tailed Statistics: Regression, Covariance\n  Estimation, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficient algorithms for linear regression and covariance estimation\nin the absence of Gaussian assumptions on the underlying distributions of\nsamples, making assumptions instead about only finitely-many moments. We focus\non how many samples are needed to do estimation and regression with high\naccuracy and exponentially-good success probability.\n  For covariance estimation, linear regression, and several other problems,\nestimators have recently been constructed with sample complexities and rates of\nerror matching what is possible when the underlying distribution is Gaussian,\nbut algorithms for these estimators require exponential time. We narrow the gap\nbetween the Gaussian and heavy-tailed settings for polynomial-time estimators\nwith:\n  1. A polynomial-time estimator which takes $n$ samples from a random vector\n$X \\in R^d$ with covariance $\\Sigma$ and produces $\\hat{\\Sigma}$ such that in\nspectral norm $\\|\\hat{\\Sigma} - \\Sigma \\|_2 \\leq \\tilde{O}(d^{3/4}/\\sqrt{n})$\nw.p. $1-2^{-d}$. The information-theoretically optimal error bound is\n$\\tilde{O}(\\sqrt{d/n})$; previous approaches to polynomial-time algorithms were\nstuck at $\\tilde{O}(d/\\sqrt{n})$.\n  2. A polynomial-time algorithm which takes $n$ samples $(X_i,Y_i)$ where $Y_i\n= \\langle u,X_i \\rangle + \\varepsilon_i$ and produces $\\hat{u}$ such that the\nloss $\\|u - \\hat{u}\\|^2 \\leq O(d/n)$ w.p. $1-2^{-d}$ for any $n \\geq d^{3/2}\n\\log(d)^{O(1)}$. This (information-theoretically optimal) error is achieved by\ninefficient algorithms for any $n \\gg d$; previous polynomial-time algorithms\nsuffer loss $\\Omega(d^2/n)$ and require $n \\gg d^2$.\n  Our algorithms use degree-$8$ sum-of-squares semidefinite programs. We offer\npreliminary evidence that improving these rates of error in polynomial time is\nnot possible in the median of means framework our algorithms employ.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:22:48 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Hopkins", "Samuel B.", ""], ["Kathuria", "Tarun", ""], ["Raghavendra", "Prasad", ""], ["Tripuraneni", "Nilesh", ""]]}, {"id": "1912.11103", "submitter": "Stephen Jue", "authors": "Stephen Jue, Philip N. Klein", "title": "A near-linear time minimum Steiner cut algorithm for planar graphs", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Minimum Steiner Cut problem on undirected planar graphs with\nnon-negative edge weights. This problem involves finding the minimum cut of the\ngraph that separates a specified subset $X$ of vertices (terminals) into two\nparts. This problem is of theoretical interest because it generalizes two\nclassical optimization problems, Minimum $s$-$t$ Cut and Minimum Cut, and of\npractical importance because of its application to computing a lower bound for\nSteiner (Subset) TSP. Our algorithm has running time $O(n\\log{n}\\log{k})$ where\n$k$ is the number of terminals.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 20:51:32 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 05:30:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Jue", "Stephen", ""], ["Klein", "Philip N.", ""]]}, {"id": "1912.11417", "submitter": "Sergio Sainz", "authors": "Sergio Sainz-Palacios", "title": "Flat combined Red Black Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flat combining is a concurrency threaded technique whereby one thread\nperforms all the operations in batch by scanning a queue of operations\nto-be-done and performing them together. Flat combining makes sense as long as\nk operations each taking O(n) separately can be batched together and done in\nless than O(k*n). Red black tree is a balanced binary search tree with\npermanent balancing warranties. Operations in red black tree are hard to batch\ntogether: for example inserting nodes in two different branches of the tree\naffect different areas of the tree. In this paper we investigate alternatives\nto making a flat combine approach work for red black trees.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 15:39:46 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Sainz-Palacios", "Sergio", ""]]}, {"id": "1912.11444", "submitter": "Hau-Wen Huang", "authors": "Hau-Wen Huang", "title": "An algorithm to evaluate the spectral expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that $X$ is a connected $(q+1)$-regular undirected graph of finite\norder $n$. Let $A$ denote the adjacency matrix of $X$. Let\n$\\lambda_1=q+1>\\lambda_2\\geq \\lambda_3\\geq \\ldots \\geq \\lambda_n$ denote the\neigenvalues of $A$. The spectral expansion of $X$ is defined by $$\n\\Delta(X)=\\lambda_1-\\max_{2\\leq i\\leq n}|\\lambda_i|. $$ By the Alon--Boppana\ntheorem, when $n$ is sufficiently large, $\\Delta(X)$ is quite high if $$\n\\mu(X)=q^{-\\frac{1}{2}} \\max_{2\\leq i\\leq n}|\\lambda_i| $$ is close to $2$. In\nthis paper, with the inputs $A$ and a real number $\\varepsilon>0$ we design an\nalgorithm to estimate if $\\mu(X)\\leq 2+\\varepsilon$ in $O(n^\\omega \\log\n\\log_{1+\\varepsilon} n )$ time, where $\\omega<2.3729$ is the exponent of matrix\nmultiplication.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 17:20:49 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 09:09:34 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 07:03:08 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 06:53:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Huang", "Hau-Wen", ""]]}, {"id": "1912.11491", "submitter": "Parter Merav", "authors": "Jason Li and Merav Parter", "title": "Planar Diameter via Metric Compression", "comments": "Appeared in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for distributed distance computation in planar\ngraphs that is based on a variant of the metric compression problem recently\nintroduced by Abboud et al. [SODA'18]. One of our key technical contributions\nis in providing a compression scheme that encodes all $S \\times T$ distances\nusing $\\widetilde{O}(|S|\\cdot poly(D)+|T|)$ bits for unweighted graphs with\ndiameter $D$. This significantly improves the state of the art of\n$\\widetilde{O}(|S|\\cdot 2^{D}+|T| \\cdot D)$ bits. We also consider an\napproximate version of the problem for \\emph{weighted} graphs, where the goal\nis to encode $(1+\\epsilon)$ approximation of the $S \\times T$ distances. At the\nheart of this compact compression scheme lies a VC-dimension type argument on\nplanar graphs. This efficient compression scheme leads to several improvements\nand simplifications in the setting of diameter computation, most notably in the\ndistributed setting:\n  - There is an $\\widetilde{O}(D^5)$-round randomized distributed algorithm for\ncomputing the diameter in planar graphs, w.h.p.\n  - There is an $\\widetilde{O}(D^3)+ poly(\\log n/\\epsilon)\\cdot D^2$-round\nrandomized distributed algorithm for computing an $(1+\\epsilon)$ approximation\nof the diameter in weighted graphs with polynomially bounded weights, w.h.p.\n  No sublinear round algorithms were known for these problems before. These\ndistributed constructions are based on a new recursive graph decomposition that\npreserves the (unweighted) diameter of each of the subgraphs up to a\nlogarithmic term. Using this decomposition, we also get an \\emph{exact} SSSP\ntree computation within $\\widetilde{O}(D^2)$ rounds.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 19:04:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Li", "Jason", ""], ["Parter", "Merav", ""]]}, {"id": "1912.11494", "submitter": "Narciso L\\'opez", "authors": "Andrea V\\'azquez, Narciso L\\'opez-L\\'opez, Nicole Labra, Miguel\n  Figueroa, Cyril Poupon, Jean-Fran\\c{c}ois Mangin, Cecilia Hern\\'andez, Pamela\n  Guevara", "title": "Parallel optimization of fiber bundle segmentation for massive\n  tractography datasets", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, CONICYT PFCHA/ DOCTORADO\n  NACIONAL/2016-21160342, CONICYT FONDECYT 1161427, CONICYT PIA/Anillo de\n  Investigaci\\'on en Ciencia y Tecnolog\\'ia ACT172121, CONICYT BASAL FB0008 and\n  from CONICYT Basal FB0001", "journal-ref": null, "doi": "10.1109/ISBI.2019.8759208", "report-no": null, "categories": "cs.DS cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimized algorithm that performs automatic classification of\nwhite matter fibers based on a multi-subject bundle atlas. We implemented a\nparallel algorithm that improves upon its previous version in both execution\ntime and memory usage. Our new version uses the local memory of each processor,\nwhich leads to a reduction in execution time. Hence, it allows the analysis of\nbigger subject and/or atlas datasets. As a result, the segmentation of a\nsubject of 4,145,000 fibers is reduced from about 14 minutes in the previous\nversion to about 6 minutes, yielding an acceleration of 2.34. In addition, the\nnew algorithm reduces the memory consumption of the previous version by a\nfactor of 0.79.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 19:08:51 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["V\u00e1zquez", "Andrea", ""], ["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["Labra", "Nicole", ""], ["Figueroa", "Miguel", ""], ["Poupon", "Cyril", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Hern\u00e1ndez", "Cecilia", ""], ["Guevara", "Pamela", ""]]}, {"id": "1912.11799", "submitter": "R Jaberi", "authors": "Raed Jaberi", "title": "Twinless articulation points and some related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a twinless strongly connected graph. a vertex $v\\in V$ is a\ntwinless articulation point if the subrgraph obtained from $G$ by removing the\nvertex $v$ is not twinless strongly connected. An edge $e\\in E$ is a twinless\nbridge if the subgraph obtained from $G$ by deleting $e$ is not twiless\nstrongly connected graph. In this paper we study twinless articulation points\nand twinless bridges. We also study the problem of finding a minimum\ncardinality edge subset $E_{1} \\subseteq E$ such that the subgraph $(V,E_{1})$\nis twinless strongly connected. Moreover, we present an algorithm for computing\nthe $2$-vertex-twinless connected components of $G$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:41:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "1912.11859", "submitter": "Fernando Silva-Coira", "authors": "Susana Ladra, Miguel R. Luaces, Jos\\'e R. Param\\'a, Fernando\n  Silva-Coira", "title": "Space- and Time-Efficient Storage of LiDAR Point Clouds", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "In: String Processing and Information Retrieval. SPIRE 2019.\n  Lecture Notes in Computer Science, vol 11811. Springer", "doi": "10.1007/978-3-030-32686-9_36", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR devices obtain a 3D representation of a space. Due to the large size of\nthe resulting datasets, there already exist storage methods that use\ncompression and present some properties that resemble those of compact data\nstructures. Specifically, LAZ format allows accesses to a given datum or\nportion of the data without having to decompress the whole dataset and provides\nindexation of the stored data. However, LAZ format still have some drawbacks\nthat should be faced. In this work, we propose a new compact data structure for\nthe representation of a cloud of LiDAR points that supports efficient queries,\nproviding indexing capabilities that are superior to those of LAZ format.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:22:19 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ladra", "Susana", ""], ["Luaces", "Miguel R.", ""], ["Param\u00e1", "Jos\u00e9 R.", ""], ["Silva-Coira", "Fernando", ""]]}, {"id": "1912.11866", "submitter": "Fernando Silva-Coira", "authors": "Fernando Silva-Coira, Jos\\'e R. Param\\'a, Susana Ladra, Juan R.\n  L\\'opez, Gilberto Guti\\'errez", "title": "Efficient processing of raster and vector data", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "PLoS ONE 15(1): e0226943", "doi": "10.1371/journal.pone.0226943", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a framework to store and manage spatial data, which\nincludes new efficient algorithms to perform operations accepting as input a\nraster dataset and a vector dataset. More concretely, we present algorithms for\nsolving a spatial join between a raster and a vector dataset imposing a\nrestriction on the values of the cells of the raster; and an algorithm for\nretrieving K objects of a vector dataset that overlap cells of a raster\ndataset, such that the K objects are those overlapping the highest (or lowest)\ncell values among all objects. The raster data is stored using a compact data\nstructure, which can directly manipulate compressed data without the need for\nprior decompression. This leads to better running times and lower memory\nconsumption. In our experimental evaluation comparing our solution to other\nbaselines, we obtain the best space/time trade-offs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 13:50:46 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 16:25:37 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Silva-Coira", "Fernando", ""], ["Param\u00e1", "Jos\u00e9 R.", ""], ["Ladra", "Susana", ""], ["L\u00f3pez", "Juan R.", ""], ["Guti\u00e9rrez", "Gilberto", ""]]}, {"id": "1912.11944", "submitter": "Antonio Fari\\~na", "authors": "Antonio Fari\\~na and Miguel A. Mart\\'inez-Prieto and Francisco Claude\n  and Gonzalo Navarro and Juan J. Lastra-D\\'iaz and Nicola Prezza and Diego\n  Seco", "title": "On the Reproducibility of Experiments of Indexing Repetitive Document\n  Collections", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Replication framework\n  available at: https://github.com/migumar2/uiHRDC/", "journal-ref": "Information Systems; Volume 83, July 2019; pages 181-194", "doi": "10.1016/j.is.2019.03.007", "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a companion reproducible paper with the aim of allowing\nthe exact replication of the methods, experiments, and results discussed in a\nprevious work [5]. In that parent paper, we proposed many and varied techniques\nfor compressing indexes which exploit that highly repetitive collections are\nformed mostly of documents that are near-copies of others. More concretely, we\ndescribe a replication framework, called uiHRDC (universal indexes for Highly\nRepetitive Document Collections), that allows our original experimental setup\nto be easily replicated using various document collections. The corresponding\nexperimentation is carefully explained, providing precise details about the\nparameters that can be tuned for each indexing solution. Finally, note that we\nalso provide uiHRDC as reproducibility package.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 22:51:13 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fari\u00f1a", "Antonio", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Claude", "Francisco", ""], ["Navarro", "Gonzalo", ""], ["Lastra-D\u00edaz", "Juan J.", ""], ["Prezza", "Nicola", ""], ["Seco", "Diego", ""]]}, {"id": "1912.12003", "submitter": "Praneeth Kacham", "authors": "Zhili Feng, Praneeth Kacham, David P. Woodruff", "title": "Dimensionality Reduction for Sum-of-Distances Metric", "comments": "27 pages, 2 figures. To appear at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We give a dimensionality reduction procedure to approximate the sum of\ndistances of a given set of $n$ points in $R^d$ to any \"shape\" that lies in a\n$k$-dimensional subspace. Here, by \"shape\" we mean any set of points in $R^d$.\nOur algorithm takes an input in the form of an $n \\times d$ matrix $A$, where\neach row of $A$ denotes a data point, and outputs a subspace $P$ of dimension\n$O(k^{3}/\\epsilon^6)$ such that the projections of each of the $n$ points onto\nthe subspace $P$ and the distances of each of the points to the subspace $P$\nare sufficient to obtain an $\\epsilon$-approximation to the sum of distances to\nany arbitrary shape that lies in a $k$-dimensional subspace of $R^d$. These\ninclude important problems such as $k$-median, $k$-subspace approximation, and\n$(j,l)$ subspace clustering with $j \\cdot l \\leq k$. Dimensionality reduction\nreduces the data storage requirement to $(n+d)k^{3}/\\epsilon^6$ from nnz$(A)$.\nHere nnz$(A)$ could potentially be as large as $nd$. Our algorithm runs in time\nnnz$(A)/\\epsilon^2 + (n+d)$poly$(k/\\epsilon)$, up to logarithmic factors. For\ndense matrices, where nnz$(A) \\approx nd$, we give a faster algorithm, that\nruns in time $nd + (n+d)$poly$(k/\\epsilon)$ up to logarithmic factors. Our\ndimensionality reduction algorithm can also be used to obtain\npoly$(k/\\epsilon)$ size coresets for $k$-median and $(k,1)$-subspace\napproximation problems in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 06:49:49 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 00:39:58 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 15:39:34 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 21:04:28 GMT"}, {"version": "v5", "created": "Thu, 24 Jun 2021 15:38:24 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Feng", "Zhili", ""], ["Kacham", "Praneeth", ""], ["Woodruff", "David P.", ""]]}, {"id": "1912.12151", "submitter": "Jos\\'e Verschae", "authors": "Andr\\'es Fielbaum, Ignacio Morales, and Jos\\'e Verschae", "title": "A Water-Filling Primal-Dual Algorithm for Approximating Non-Linear\n  Covering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining strong linear relaxations of capacitated covering problems\nconstitute a major technical challenge even for simple settings. For one of the\nmost basic cases, the Knapsack-Cover (Min-Knapsack) problem, the relaxation\nbased on knapsack-cover inequalities achieves an integrality gap of 2. These\ninequalities have been exploited in more general environments, many of which\nadmit primal-dual approximation algorithms.\n  Inspired by problems from power and transport systems, we introduce a new\ngeneral setting in which items can be taken fractionally to cover a given\ndemand. The cost incurred by an item is given by an arbitrary non-decreasing\nfunction of the chosen fraction. We generalize the knapsack-cover inequalities\nto this setting an use them to obtain a $(2+\\varepsilon)$-approximate\nprimal-dual algorithm. Our procedure has a natural interpretation as a\nbucket-filling algorithm, which effectively balances the difficulties given by\nhaving different slopes in the cost functions: when some superior portion of an\nitem presents a low slope, it helps to increase the priority with which the\ninferior portions may be taken. We also present a rounding algorithm with an\napproximation guarantee of 2.\n  We generalize our algorithm to the Unsplittable Flow-Cover problem on a line,\nalso for the setting where items can be taken fractionally. For this problem we\nobtain a $(4+\\varepsilon)$-approximation algorithm in polynomial time, almost\nmatching the $4$-approximation known for the classical setting.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:20:56 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fielbaum", "Andr\u00e9s", ""], ["Morales", "Ignacio", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "1912.12366", "submitter": "Josh Payne", "authors": "Josh Payne and Mario Srouji", "title": "Approximate Graph Spectral Decomposition with the Variational Quantum\n  Eigensolver", "comments": "SPIE conference on Quantum Information Science, Sensing, and\n  Computation XII", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral graph theory is a branch of mathematics that studies the\nrelationships between the eigenvectors and eigenvalues of Laplacian and\nadjacency matrices and their associated graphs. The Variational Quantum\nEigensolver (VQE) algorithm was proposed as a hybrid quantum/classical\nalgorithm that is used to quickly determine the ground state of a Hamiltonian,\nand more generally, the lowest eigenvalue of a matrix $M\\in \\mathbb{R}^{n\\times\nn}$. There are many interesting problems associated with the spectral\ndecompositions of associated matrices, such as partitioning, embedding, and the\ndetermination of other properties. In this paper, we will expand upon the VQE\nalgorithm to analyze the spectra of directed and undirected graphs. We evaluate\nruntime and accuracy comparisons (empirically and theoretically) between\ndifferent choices of ansatz parameters, graph sizes, graph densities, and\nmatrix types, and demonstrate the effectiveness of our approach on Rigetti's\nQCS platform on graphs of up to 64 vertices, finding eigenvalues of adjacency\nand Laplacian matrices. We finally make direct comparisons to classical\nperformance with the Quantum Virtual Machine (QVM) in the appendix, observing a\nsuperpolynomial runtime improvement of our algorithm when run using a quantum\ncomputer.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 23:27:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Payne", "Josh", ""], ["Srouji", "Mario", ""]]}, {"id": "1912.12409", "submitter": "Rakesh Mohanty", "authors": "Debasis Dwibedy, Rakesh Mohanty, Arun Khamari", "title": "Online Rainbow Coloring In Graphs", "comments": "9 pages, 1 figure, Appeared in Proceeding of International Conference\n  on Discrete Mathematics and its Applications to Network Science(ICDMANS-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainbow coloring is a special case of edge coloring, where there must be at\nleast one path between every distinct pair of vertices that consists of\ndifferent color edges. Here, we may use the same color for the adjacent edges\nof a graph representing two different paths from a single vertex. In online\nrainbow coloring, we have no priori knowledge about the vertices and edges of\nthe graph, in fact the edges are available one by one. We have to color an edge\nas soon as it arrives and before the arrival of the next edge. We can not\nrevoke the coloring decision once it is made. According to our knowledge, there\nis no study of online rainbow coloring for graphs. In this paper, we make a\nfirst attempt to propose an online algorithm named Least Recently Used\nColor(LRUC) for online rainbow coloring. We analyze the performance of LRUC\nthrough competitive analysis. We show that LRUC is optimal for line graph, tree\nand star graph. For 1-cyclic graph, LRUC is shown to be (2-2/n)-competitive,\nwhere n>3. We obtain the competitive ratios of (n-1)/3 and n-1 for wheel and\ncomplete graphs respectively, where n is the number of vertices.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 06:22:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Dwibedy", "Debasis", ""], ["Mohanty", "Rakesh", ""], ["Khamari", "Arun", ""]]}, {"id": "1912.12447", "submitter": "Mordecai Golin", "authors": "Mordecai Golin and Sai Sandeep", "title": "Minmax Regret for sink location on paths with general capacities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic flow networks, every vertex starts with items (flow) that need to\nbe shipped to designated sinks.\n  All edges have two associated quantities: length, the amount of time required\nfor a particle to traverse the edge, and capacity, the number of units of flow\nthat can enter the edge in unit time. The goal is move all flow to the sinks. A\nvariation of the problem, modelling evacuation protocols, is to find the sink\nlocation(s) that minimize evacuation time, restricting the flow to be\nCONFLUENT. Solving this problem is is NP-hard on general graphs, and thus\nresearch into optimal algorithms has traditionally been restricted to special\ngraphs such as paths, and trees.\n  A specialized version of robust optimization is minmax REGRET, in which the\ninput flows at the vertices are only partially defined by constraints. The goal\nis to find a sink location that has the minimum{ regret} over all input flows\nthat satisfy the partially defined constraints. Regret for a fully defined\ninput flow and a sink is defined to be the difference between the evacuation\ntime to that sink and the optimal evacuation time.\n  A large recent literature derives polynomial time algorithms for the minmax\nregret $k$-sink location problem on paths and trees under the simplifying\ncondition that all edges have the same (uniform) capacity.\n  This paper develops a $O(n^4 \\log n)$ time algorithm for the minmax regret\n$1$-sink problem on paths with general (non-uniform) capacities. To the best of\nour knowledge, this is the first minmax regret result for dynamic flow problems\nin any type of graph with general capacities.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 11:56:48 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 08:08:35 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Golin", "Mordecai", ""], ["Sandeep", "Sai", ""]]}, {"id": "1912.12529", "submitter": "Karl Bringmann", "authors": "Karl Bringmann and Vasileios Nakos", "title": "A Fine-Grained Perspective on Approximating Subset Sum and Partition", "comments": "accepted at SODA'21, 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating Subset Sum is a classic and fundamental problem in computer\nscience and mathematical optimization. The state-of-the-art approximation\nscheme for Subset Sum computes a $(1-\\varepsilon)$-approximation in time\n$\\tilde{O}(\\min\\{n/\\varepsilon, n+1/\\varepsilon^2\\})$ [Gens, Levner'78,\nKellerer et al.'97]. In particular, a $(1-1/n)$-approximation can be computed\nin time $O(n^2)$.\n  We establish a connection to Min-Plus-Convolution, a problem that is of\nparticular interest in fine-grained complexity theory and can be solved naively\nin time $O(n^2)$. Our main result is that computing a $(1-1/n)$-approximation\nfor Subset Sum is subquadratically equivalent to Min-Plus-Convolution. Thus,\nassuming the Min-Plus-Convolution conjecture from fine-grained complexity\ntheory, there is no approximation scheme for Subset Sum with strongly\nsubquadratic dependence on $n$ and $1/\\varepsilon$. In the other direction, our\nreduction allows us to transfer known lower order improvements from\nMin-Plus-Convolution to Subset Sum, which yields a mildly subquadratic\nrandomized approximation scheme. This adds the first approximation problem to\nthe list of problems that are equivalent to Min-Plus-Convolution.\n  For the related Partition problem, an important special case of Subset Sum,\nthe state of the art is a randomized approximation scheme running in time\n$\\tilde{O}(n+1/\\varepsilon^{5/3})$ [Mucha~et~al.'19]. We adapt our reduction\nfrom Subset Sum to Min-Plus-Convolution to obtain a related reduction from\nPartition to Min-Plus-Convolution. This yields an improved approximation scheme\nfor Partition running in time $\\tilde{O}(n + 1/\\varepsilon^{3/2})$. Our\nalgorithm is the first deterministic approximation scheme for Partition that\nbreaks the quadratic barrier.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:45:55 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 21:40:06 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bringmann", "Karl", ""], ["Nakos", "Vasileios", ""]]}, {"id": "1912.12541", "submitter": "Jugal Garg", "authors": "Jugal Garg, Pooja Kulkarni, Rucha Kulkarni", "title": "Approximating Nash Social Welfare under Submodular Valuations through\n  (Un)Matchings", "comments": "Full version of SODA 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating maximum Nash social welfare (NSW) when\nallocating m indivisible items among n asymmetric agents with submodular\nvaluations. The NSW is a well-established notion of fairness and efficiency,\ndefined as the weighted geometric mean of agents' valuations. For special cases\nof the problem with symmetric agents and additive(-like) valuation functions,\napproximation algorithms have been designed using approaches customized for\nthese specific settings, and they fail to extend to more general settings.\nHence, no approximation algorithm with factor independent of m is known either\nfor asymmetric agents with additive valuations or for symmetric agents beyond\nadditive(-like) valuations.\n  In this paper, we extend our understanding of the NSW problem to far more\ngeneral settings. Our main contribution is two approximation algorithms for\nasymmetric agents with additive and submodular valuations respectively. Both\nalgorithms are simple to understand and involve non-trivial modifications of a\ngreedy repeated matchings approach. Allocations of high valued items are done\nseparately by un-matching certain items and re-matching them, by processes that\nare different in both algorithms. We show that these approaches achieve\napproximation factors of O(n) and O(n log n) for additive and submodular case\nrespectively, which is independent of the number of items. For additive\nvaluations, our algorithm outputs an allocation that also achieves the fairness\nproperty of envy-free up to one item (EF1).\n  Furthermore, we show that the NSW problem under submodular valuations is\nstrictly harder than all currently known settings with an e/(e-1) factor of the\nhardness of approximation, even for constantly many agents. For this case, we\nprovide a different approximation algorithm that achieves a factor of e/(e-1),\nhence resolving it completely.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 23:15:47 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Garg", "Jugal", ""], ["Kulkarni", "Pooja", ""], ["Kulkarni", "Rucha", ""]]}, {"id": "1912.12599", "submitter": "Matthew Younatan", "authors": "Younatan Matthew and Ghose Shohini", "title": "Quantum Image Preparation Based on Exclusive Sum-of-Product Minimization\n  and Ternary Trees", "comments": "15 pages (including references), 2 tables, 15 figures and 20\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum image processing is one of the promising fields of quantum\ninformation. The complexity overhead to design circuits to represent quantum\nimages is a significant problem. So, we proposed a new method to minimize the\ntotal number required of quantum gates to represent the quantum image. Our\napproach uses ternary trees to reduce the number of Toffoli gates in a quantum\nimage circuit. Also, it uses the complement property of Boolean algebra on a\nset of Toffoli gates to combine two Toffoli gates into one, therefore reducing\nthe number of overall gates. Ternary trees are used to represent Toffoli gates\nas they significantly increase run time and is supported through experiments on\nsample images. The experimental results show that there is a high-speed up\ncompared with previous methods, bringing the processing time for thousands of\nToffoli gates from minutes to seconds.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 07:31:42 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Matthew", "Younatan", ""], ["Shohini", "Ghose", ""]]}, {"id": "1912.12665", "submitter": "Takuro Fukunaga", "authors": "Takuro Fukunaga", "title": "Adaptive Algorithm for Finding Connected Dominating Sets in Uncertain\n  Graphs", "comments": "This is the accepted version of a paper to be published by IEEE/ACM\n  Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a minimum-weight connected dominating set (CDS) of a\ngiven undirected graph has been studied actively, motivated by operations of\nwireless ad hoc networks. In this paper, we formulate a new stochastic variant\nof the problem. In this problem, each node in the graph has a hidden random\nstate, which represents whether the node is active or inactive, and we seek a\nCDS of the graph that consists of the active nodes. We consider an adaptive\nalgorithm for this problem, which repeat choosing nodes and observing the\nstates of the nodes around the chosen nodes until a CDS is found. Our\nalgorithms have a theoretical performance guarantee that the sum of the weights\nof the nodes chosen by the algorithm is at most $O(\\alpha \\log (1/\\delta))$\ntimes that of any adaptive algorithm in expectation, where $\\alpha$ is an\napproximation factor for the node-weighted polymatroid Steiner tree problem and\n$\\delta$ is the minimum probability of possible scenarios on the node states.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 14:49:36 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Fukunaga", "Takuro", ""]]}, {"id": "1912.12740", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Vasiliki Kalavri, Michael Kapralov,\n  Torsten Hoefler", "title": "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models,\n  and Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of various areas of computing,\nincluding machine learning, medical applications, social network analysis,\ncomputational sciences, and others. A growing amount of the associated graph\nprocessing workloads are dynamic, with millions of edges added or removed per\nsecond. Graph streaming frameworks are specifically crafted to enable the\nprocessing of such highly dynamic workloads. Recent years have seen the\ndevelopment of many such frameworks. However, they differ in their general\narchitectures (with key details such as the support for the concurrent\nexecution of graph updates and queries, or the incorporated graph data\norganization), the types of updates and workloads allowed, and many others. To\nfacilitate the understanding of this growing field, we provide the first\nanalysis and taxonomy of dynamic and streaming graph processing. We focus on\nidentifying the fundamental system designs and on understanding their support\nfor concurrency, and for different graph updates as well as analytics\nworkloads. We also crystallize the meaning of different concepts associated\nwith streaming graph processing, such as dynamic, temporal, online, and\ntime-evolving graphs, edge-centric processing, models for the maintenance of\nupdates, and graph databases. Moreover, we provide a bridge with the very rich\nlandscape of graph streaming theory by giving a broad overview of recent\ntheoretical related advances, and by discussing which graph streaming models\nand settings could be helpful in developing more powerful streaming frameworks\nand designs. We also outline graph streaming workloads and research challenges.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 21:41:14 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:24:01 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 13:00:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Kalavri", "Vasiliki", ""], ["Kapralov", "Michael", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1912.12747", "submitter": "Bernhard Scholz", "authors": "Alan Fekete and Brody Franks and Herbert Jordan and Bernhard Scholz", "title": "Worst-Case Optimal Radix Triejoin", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relatively recently, the field of join processing has been swayed by the\ndiscovery of a new class of multi-way join algorithms. The new algorithms join\nmultiple relations simultaneously rather than perform a series of pairwise\njoins. The new join algorithms satisfy stronger worst-case runtime complexity\nguarantees than any of the existing approaches based on pairwise joins -- they\nare worst-case optimal in data complexity. These research efforts have resulted\nin a flurry of papers documenting theoretical and some practical contributions.\nHowever, there is still the quest of making the new worst-case optimal join\nalgorithms truly practical in terms of (1) ease of implementation and (2)\nsecondary index efficiency in terms of number of indexes created to answer a\nquery.\n  In this paper, we present a simple worst-case optimal multi-way join\nalgorithm called the radix triejoin. Radix triejoin uses a binary encoding for\nreducing the domain of a database. Our main technical contribution is that\ndomain reduction allows a bit-interleaving of attribute values that gives rise\nto a query-independent relation representation, permitting the computation of\nmultiple queries over the same relations worst-case optimally without having to\nconstruct additional secondary indexes. We also generalise the core algorithm\nto conjunctive queries with inequality constraints and provide a new proof\ntechnique for the worst-case optimal join result.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 22:00:58 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Fekete", "Alan", ""], ["Franks", "Brody", ""], ["Jordan", "Herbert", ""], ["Scholz", "Bernhard", ""]]}, {"id": "1912.12765", "submitter": "Neeldhara Misra", "authors": "Pratyush Dayal and Neeldhara Misra", "title": "Deleting to Structured Trees", "comments": "16 pages, 4 figures. A shorter version of this work was published in\n  the proceedings of the 25th International Computing and Combinatorics\n  Conference (COCOON 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural variant of the well-known Feedback Vertex Set problem,\nnamely the problem of deleting a small subset of vertices or edges to a full\nbinary tree. This version of the problem is motivated by real-world scenarios\nthat are best modeled by full binary trees. We establish that both versions of\nthe problem are NP-hard, which stands in contrast to the fact that deleting\nedges to obtain a forest or a tree is equivalent to the problem of finding a\nminimum cost spanning tree, which can be solved in polynomial time. We also\nestablish that both problems are FPT by the standard parameter.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 00:41:22 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Dayal", "Pratyush", ""], ["Misra", "Neeldhara", ""]]}, {"id": "1912.12790", "submitter": "R Jaberi", "authors": "Raed Jaberi", "title": "Computing $2$-twinless blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E))$ be a directed graph. A $2$-twinless block in $G$ is a maximal\nvertex set $B\\subseteq V$ of size at least $2$ such that for each pair of\ndistinct vertices $x,y \\in B$, and for each vertex $w\\in V\\setminus\\left\\lbrace\nx,y \\right\\rbrace $, the vertices $x,y$ are in the same twinless strongly\nconnected component of $G\\setminus\\left \\lbrace w \\right\\rbrace $.\n  In this paper we present algorithms for computing the $2$-twinless blocks of\na directed graph.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 02:56:47 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 22:35:17 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "1912.13117", "submitter": "L\\'aszl\\'o Kozma", "authors": "L\\'aszl\\'o Kozma", "title": "Exact exponential algorithms for two poset problems", "comments": "small fixes and improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially ordered sets (posets) are fundamental combinatorial objects with\nimportant applications in computer science. Perhaps the most natural\nalgorithmic task, given a size-$n$ poset, is to compute its number of linear\nextensions. In 1991 Brightwell and Winkler showed this problem to be\n$\\#P$-hard. In spite of extensive research, the fastest known algorithm is\nstill the straightforward $O(n 2^n)$-time dynamic programming (an adaptation of\nthe Bellman-Held-Karp algorithm for the TSP). Very recently, Dittmer and Pak\nshowed that the problem remains $\\#P$-hard for two-dimensional posets, and no\nalgorithm was known to break the $2^n$-barrier even in this special case. The\nquestion of whether the two-dimensional problem is easier than the general case\nwas raised decades ago by M\\\"ohring, Felsner and Wernisch, and others. In this\npaper we show that the number of linear extensions of a two-dimensional poset\ncan be computed in time $O(1.8172^n)$.\n  The related jump number problem asks for a linear extension of a poset,\nminimizing the number of neighboring incomparable pairs. The problem has\napplications in scheduling, and has been widely studied. In 1981 Pulleyblank\nshowed it to be NP-complete. We show that the jump number problem can be solved\n(in arbitrary posets) in time $O(1.824^n)$. This improves (slightly) the\nprevious best bound of Kratsch and Kratsch.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 23:57:19 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 14:42:41 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 22:59:14 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Kozma", "L\u00e1szl\u00f3", ""]]}, {"id": "1912.13190", "submitter": "He Zhang", "authors": "He Zhang, Liang Zhang, David H. Mathews, Liang Huang", "title": "LinearPartition: Linear-Time Approximation of RNA Folding Partition\n  Function and Base Pairing Probabilities", "comments": "12 pages main text (10 figures); 6 pages SI (7 figures).\n  Bioinformatics, July 2020 (Proceedings of ISMB 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.DS physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA secondary structure prediction is widely used to understand RNA function.\nRecently, there has been a shift away from the classical minimum free energy\n(MFE) methods to partition function-based methods that account for folding\nensembles and can therefore estimate structure and base pair probabilities.\nHowever, the classical partition function algorithm scales cubically with\nsequence length, and is therefore a slow calculation for long sequences. This\nslowness is even more severe than cubic-time MFE-based methods due to a larger\nconstant factor in runtime. Inspired by the success of our recently proposed\nLinearFold algorithm that predicts the approximate MFE structure in linear\ntime, we design a similar linear-time heuristic algorithm, LinearPartition, to\napproximate the partition function and base pairing probabilities, which is\nshown to be orders of magnitude faster than Vienna RNAfold and CONTRAfold\n(e.g., 2.5 days vs. 1.3 minutes on a sequence with length 32,753 nt). More\ninterestingly, the resulting base pairing probabilities are even better\ncorrelated with the ground truth structures. LinearPartition also leads to a\nsmall accuracy improvement when used for downstream structure prediction on\nfamilies with the longest length sequences (16S and 23S rRNA), as well as a\nsubstantial improvement on long-distance base pairs (500+ nt apart).\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:28:28 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 22:32:20 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2020 04:39:41 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zhang", "He", ""], ["Zhang", "Liang", ""], ["Mathews", "David H.", ""], ["Huang", "Liang", ""]]}, {"id": "1912.13286", "submitter": "Keerti Choudhary", "authors": "Amotz Bar-Noy, Keerti Choudhary, David Peleg, Dror Rawitz", "title": "Graph Realizations: Maximum and Minimum Degree in Vertex Neighborhoods", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical problem of degree sequence realizability asks whether or not a\ngiven sequence of $n$ positive integers is equal to the degree sequence of some\n$n$-vertex undirected simple graph. While the realizability problem of degree\nsequences has been well studied for different classes of graphs, there has been\nrelatively little work concerning the realizability of other types of\ninformation profiles, such as the vertex neighborhood profiles.\n  In this paper, we initiate the study of neighborhood degree profiles. We\nfocus on the natural problem of realizing maximum and minimum neighborhood\ndegrees. More specifically, we ask the following question: Given a sequence $D$\nof $n$ non-negative integers $0\\leq d_1\\leq \\cdots \\leq d_n$, does there exist\na simple graph with vertices $v_1,\\ldots, v_n$ such that for every $1\\le i \\le\nn$, the maximum (resp. minimum) degree in the neighborhood of $v_i$ is exactly\n$d_i$?\n  We provide in this work various results for both maximum as well as minimum\nneighborhood degree for general $n$ vertex graphs. Our results are first of its\nkind that studies extremal neighborhood degree profiles. For maximum\nneighborhood degree profiles, we provide a {\\em complete realizability\ncriteria}. In comparison, we observe that the minimum neighborhood profiles are\nnot so well-behaved, for these our necessary and sufficient conditions for\nrealizability {\\em differ by a factor of at most two}.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:16:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bar-Noy", "Amotz", ""], ["Choudhary", "Keerti", ""], ["Peleg", "David", ""], ["Rawitz", "Dror", ""]]}, {"id": "1912.13287", "submitter": "Keerti Choudhary", "authors": "Amotz Bar-Noy, Keerti Choudhary, David Peleg, Dror Rawitz", "title": "Efficiently Realizing Interval Sequences", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of realizable interval-sequences. An interval\nsequence comprises of $n$ integer intervals $[a_i,b_i]$ such that $0\\leq a_i\n\\leq b_i \\leq n-1$, and is said to be graphic/realizable if there exists a\ngraph with degree sequence, say, $D=(d_1,\\ldots,d_n)$ satisfying the condition\n$a_i \\leq d_i \\leq b_i$, for each $i \\in [1,n]$. There is a characterisation\n(also implying an $O(n)$ verifying algorithm) known for realizability of\ninterval-sequences, which is a generalization of the Erdos-Gallai\ncharacterisation for graphic sequences. However, given any realizable\ninterval-sequence, there is no known algorithm for computing a corresponding\ngraphic certificate in $o(n^2)$ time.\n  In this paper, we provide an $O(n \\log n)$ time algorithm for computing a\ngraphic sequence for any realizable interval sequence. In addition, when the\ninterval sequence is non-realizable, we show how to find a graphic sequence\nhaving minimum deviation with respect to the given interval sequence, in the\nsame time. Finally, we consider variants of the problem such as computing the\nmost regular graphic sequence, and computing a minimum extension of a length\n$p$ non-graphic sequence to a graphic one.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:19:13 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bar-Noy", "Amotz", ""], ["Choudhary", "Keerti", ""], ["Peleg", "David", ""], ["Rawitz", "Dror", ""]]}, {"id": "1912.13347", "submitter": "R Jaberi", "authors": "Raed Jaberi", "title": "$2$-edge-twinless blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a directed graph. A $2$-edge-twinless block in $G$ is a\nmaximal vertex set $C^{t}\\subseteq V$ with $|C^{t}|>1$ such that for any\ndistinct vertices $v,w \\in C^{t}$, and for every edge $e\\in E$, the vertices\n$v,w$ are in the same twinless strongly connected component of $G\\setminus\\left\n\\lbrace e \\right\\rbrace $.\n  In this paper we study this concept and describe algorithms for computing\n$2$-edge-twinless blocks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:12:35 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 10:20:53 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "1912.13446", "submitter": "Alessio Conte", "authors": "Alessio Conte, Andrea Marino, Roberto Grossi, Takeaki Uno, Luca\n  Versari", "title": "Proximity Search For Maximal Subgraph Enumeration", "comments": "A preliminary version of this work appeared in STOC 2019:\n  https://dl.acm.org/doi/10.1145/3313276.3316402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the enumeration of subgraphs of an input graph that\nsatisfy a given property and are maximal under inclusion. The main result is a\nseemingly novel technique, proximity search, to list these subgraphs in\npolynomial delay. These include Maximal Bipartite Subgraphs, Maximal\n$k$-Degenerate Subgraphs (for bounded $k$), Maximal Induced Chordal Subgraphs,\nand Maximal Induced Trees. Using known techniques, such as reverse search, the\nspace of all maximal solutions induces an implicit directed graph called\n\"solution graph\" or \"supergraph\", and solutions are enumerated by traversing\nit; however, this graph can have exponential out-degree, thus requiring\nexponential time to be spent on each solution. The novelty of proximity search\nis a technique for generating a better solution graph, significantly reducing\nthe out-degree with respect to existing approaches, but such that it remains\nstrongly connected, so all solutions can be enumerated in polynomial delay by a\ntraversal. A drawback of this approach is the space required to keep track of\nvisited solutions, which can be exponential: we further propose a technique to\ninduce a parent-child relationship and achieve polynomial space when suitable\nconditions are met.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:29:54 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 15:11:50 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Conte", "Alessio", ""], ["Marino", "Andrea", ""], ["Grossi", "Roberto", ""], ["Uno", "Takeaki", ""], ["Versari", "Luca", ""]]}]