[{"id": "1611.00029", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller, Martin Dietzfelbinger, Philipp Woelfel", "title": "A Simple Hash Class with Strong Randomness Properties in Graphs and\n  Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomness properties of graphs and hypergraphs generated by simple\nhash functions. Several hashing applications can be analyzed by studying the\nstructure of $d$-uniform random ($d$-partite) hypergraphs obtained from a set\n$S$ of $n$ keys and $d$ randomly chosen hash functions $h_1,\\dots,h_d$ by\nassociating each key $x\\in S$ with a hyperedge $\\{h_1(x),\\dots, h_d(x)\\}$.\nOften it is assumed that $h_1,\\dots,h_d$ exhibit a high degree of independence.\nWe present a simple construction of a hash class whose hash functions have\nsmall constant evaluation time and can be stored in sublinear space. We devise\ngeneral techniques to analyze the randomness properties of the graphs and\nhypergraphs generated by these hash functions, and we show that they can\nreplace other, less efficient constructions in cuckoo hashing (with and without\nstash), the simulation of a uniform hash function, the construction of a\nperfect hash function, generalized cuckoo hashing and different load balancing\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 20:20:21 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Dietzfelbinger", "Martin", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1611.00052", "submitter": "Seeun William Umboh", "authors": "Anupam Gupta, R. Ravi, Kunal Talwar, Seeun William Umboh", "title": "LAST but not Least: Online Spanners for Buy-at-Bulk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online (uniform) buy-at-bulk network design problem asks us to design a\nnetwork, where the edge-costs exhibit economy-of-scale. Previous approaches to\nthis problem used tree- embeddings, giving us randomized algorithms. Moreover,\nthe optimal results with a logarithmic competitive ratio requires the metric on\nwhich the network is being built to be known up-front; the competitive ratios\nthen depend on the size of this metric (which could be much larger than the\nnumber of terminals that arrive).\n  We consider the buy-at-bulk problem in the least restrictive model where the\nmetric is not known in advance, but revealed in parts along with the demand\npoints seeking connectivity arriving online. For the single sink buy-at-bulk\nproblem, we give a deterministic online algorithm with competitive ratio that\nis logarithmic in k, the number of terminals that have arrived, matching the\nlower bound known even for the online Steiner tree problem. In the oblivious\ncase when the buy-at-bulk function used to compute the edge-costs of the\nnetwork is not known in advance (but is the same across all edges), we give a\ndeterministic algorithm with competitive ratio polylogarithmic in k, the number\nof terminals.\n  At the heart of our algorithms are optimal constructions for online Light\nApproximate Shortest-path Trees (LASTs) and spanners, and their variants. We\ngive constructions that have optimal trade-offs in terms of cost and stretch.\nWe also define and give constructions for a new notion of LASTs where the set\nof roots (in addition to the points) expands over time. We expect these\ntechniques will find applications in other online network-design problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 21:34:05 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Gupta", "Anupam", ""], ["Ravi", "R.", ""], ["Talwar", "Kunal", ""], ["Umboh", "Seeun William", ""]]}, {"id": "1611.00129", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Huy L. Nguyen, Qin Zhang", "title": "Submodular Maximization over Sliding Windows", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the extraction of representative elements in the data\nstream model in the form of submodular maximization. Different from the\nprevious work on streaming submodular maximization, we are interested only in\nthe recent data, and study the maximization problem over sliding windows. We\nprovide a general reduction from the sliding window model to the standard\nstreaming model, and thus our approach works for general constraints as long as\nthere is a corresponding streaming algorithm in the standard streaming model.\nAs a consequence, we obtain the first algorithms in the sliding window model\nfor maximizing a monotone/non-monotone submodular function under cardinality\nand matroid constraints. We also propose several heuristics and show their\nefficiency in real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 05:06:37 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Chen", "Jiecao", ""], ["Nguyen", "Huy L.", ""], ["Zhang", "Qin", ""]]}, {"id": "1611.00198", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Deeparnab Chakrabarty and Monika Henzinger", "title": "Deterministic Fully Dynamic Approximate Vertex Cover and Fractional\n  Matching in $O(1)$ Amortized Update Time", "comments": "Independent of our work, Gupta, Krishnaswamy, Kumar, and Panigrahy\n  have also obtained similar results even for the hypergraph setting. In this\n  version we show how our techniques also easily generalize to this setting\n  giving identical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of maintaining an approximate maximum matching and\nan approximate minimum vertex cover in a dynamic graph undergoing a sequence of\nedge insertions/deletions. Starting with the seminal work of Onak and Rubinfeld\n[STOC 2010], this problem has received significant attention in recent years.\nVery recently, extending the framework of Baswana, Gupta and Sen [FOCS 2011],\nSolomon [FOCS 2016] gave a randomized dynamic algorithm for this problem that\nhas an approximation ratio of $2$ and an amortised update time of $O(1)$ with\nhigh probability. This algorithm requires the assumption of an {\\em oblivious\nadversary}, meaning that the future sequence of edge insertions/deletions in\nthe graph cannot depend in any way on the algorithm's past output. A natural\nway to remove the assumption on oblivious adversary is to give a deterministic\ndynamic algorithm for the same problem in $O(1)$ update time. In this paper, we\nresolve this question.\n  We present a new {\\em deterministic} fully dynamic algorithm that maintains a\n$O(1)$-approximate minimum vertex cover and maximum fractional matching, with\nan amortised update time of $O(1)$. Previously, the best deterministic\nalgorithm for this problem was due to Bhattacharya, Henzinger and Italiano\n[SODA 2015]; it had an approximation ratio of $(2+\\epsilon)$ and an amortised\nupdate time of $O(\\log n/\\epsilon^2)$. Our results also extend to a fully\ndynamic $O(f^3)$-approximate algorithm with $O(f^2)$ amortized update time for\nthe hypergraph vertex cover and fractional hypergraph matching problems, where\nevery hyperedge has at most $f$ vertices.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 12:18:55 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 14:45:40 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Chakrabarty", "Deeparnab", ""], ["Henzinger", "Monika", ""]]}, {"id": "1611.00255", "submitter": "Nathanael Perraudin N. P.", "authors": "Andreas Loukas and Nathana\\\"el Perraudin", "title": "Stationary time-vertex signal processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers regression tasks involving high-dimensional multivariate\nprocesses whose structure is dependent on some {known} graph topology. We put\nforth a new definition of time-vertex wide-sense stationarity, or joint\nstationarity for short, that goes beyond product graphs. Joint stationarity\nhelps by reducing the estimation variance and recovery complexity. In\nparticular, for any jointly stationary process (a) one reliably learns the\ncovariance structure from as little as a single realization of the process, and\n(b) solves MMSE recovery problems, such as interpolation and denoising, in\ncomputational time nearly linear on the number of edges and timesteps.\nExperiments with three datasets suggest that joint stationarity can yield\naccuracy improvements in the recovery of high-dimensional processes evolving\nover a graph, even when the latter is only approximately known, or the process\nis not strictly stationary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 14:56:33 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 18:35:44 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 12:34:27 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Loukas", "Andreas", ""], ["Perraudin", "Nathana\u00ebl", ""]]}, {"id": "1611.00258", "submitter": "Daniel Krenn", "authors": "Martin Aum\\\"uller, Martin Dietzfelbinger, Clemens Heuberger, Daniel\n  Krenn, Helmut Prodinger", "title": "Dual-Pivot Quicksort: Optimality, Analysis and Zeros of Associated\n  Lattice Paths", "comments": "This article supersedes arXiv:1602.04031", "journal-ref": "Combin. Probab. Comput. 28 (2019), no. 4, 485-518", "doi": "10.1017/S096354831800041X", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an average case analysis of a variant of dual-pivot quicksort. We\nshow that the used algorithmic partitioning strategy is optimal, i.e., it\nminimizes the expected number of key comparisons. For the analysis, we\ncalculate the expected number of comparisons exactly as well as asymptotically,\nin particular, we provide exact expressions for the linear, logarithmic, and\nconstant terms.\n  An essential step is the analysis of zeros of lattice paths in a certain\nprobability model. Along the way a combinatorial identity is proven.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 14:57:32 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:59:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Dietzfelbinger", "Martin", ""], ["Heuberger", "Clemens", ""], ["Krenn", "Daniel", ""], ["Prodinger", "Helmut", ""]]}, {"id": "1611.00423", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Qin Zhang", "title": "Computing Skylines on Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study skyline queries in the distributed computational\nmodel, where we have $s$ remote sites and a central coordinator (the query\nnode); each site holds a piece of data, and the coordinator wants to compute\nthe skyline of the union of the $s$ datasets. The computation is in terms of\nrounds, and the goal is to minimize both the total communication cost and the\nround cost.\n  Viewing data objects as points in the Euclidean space, we consider both the\nhorizontal data partition case where each site holds a subset of points, and\nthe vertical data partition case where each site holds one coordinate of all\nthe points. We give a set of algorithms that have provable theoretical\nguarantees, and complement them with information theoretical lower bounds. We\nalso demonstrate the superiority of our algorithms over existing heuristics by\nan extensive set of experiments on both synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 23:41:03 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Zhang", "Haoyu", ""], ["Zhang", "Qin", ""]]}, {"id": "1611.00507", "submitter": "Reza Eghbali", "authors": "Reza Eghbali, Maryam Fazel", "title": "Worst Case Competitive Analysis of Online Algorithms for Conic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization covers problems such as online resource allocation,\nonline bipartite matching, adwords (a central problem in e-commerce and\nadvertising), and adwords with separable concave returns. We analyze the worst\ncase competitive ratio of two primal-dual algorithms for a class of online\nconvex (conic) optimization problems that contains the previous examples as\nspecial cases defined on the positive orthant. We derive a sufficient condition\non the objective function that guarantees a constant worst case competitive\nratio (greater than or equal to $\\frac{1}{2}$) for monotone objective\nfunctions. We provide new examples of online problems on the positive orthant\nand the positive semidefinite cone that satisfy the sufficient condition. We\nshow how smoothing can improve the competitive ratio of these algorithms, and\nin particular for separable functions, we show that the optimal smoothing can\nbe derived by solving a convex optimization problem. This result allows us to\ndirectly optimize the competitive ratio bound over a class of smoothing\nfunctions, and hence design effective smoothing customized for a given cost\nfunction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 08:55:04 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Eghbali", "Reza", ""], ["Fazel", "Maryam", ""]]}, {"id": "1611.00532", "submitter": "Micha{\\l} Startek", "authors": "Micha{\\l} Startek", "title": "An asymptotically optimal, online algorithm for weighted random sampling\n  with replacement", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm solving the classic problem of\ngenerating a random sample of size s from population of size n with non-uniform\nprobabilities. The sampling is done with replacement. The algorithm requires\nconstant additional memory, and works in O(n) time (even when s >> n, in which\ncase the algorithm produces a list containing, for every population member, the\nnumber of times it has been selected for sample). The algorithm works online,\nand as such is well-suited to processing streams. In addition, a novel method\nof mass-sampling from any discrete distribution using the algorithm is\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 10:20:21 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Startek", "Micha\u0142", ""]]}, {"id": "1611.00559", "submitter": "Chi-Kin Chau", "authors": "Areg Karapetyan, Majid Khonji, Chi-Kin Chau, and Khaled Elbassioni", "title": "Online Algorithm for Demand Response with Inelastic Demands and Apparent\n  Power Constraint", "comments": null, "journal-ref": "Extended version of \"A Competitive Scheduling Algorithm for Online\n  Demand Response in Islanded Microgrids\", IEEE Transactions on Power Systems,\n  Vol 36, No 4, Pages 3430-3440, Dec 2020", "doi": "10.1109/TPWRS.2020.3046144", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical problem in power systems is to allocate in-coming (elastic or\ninelastic) demands without violating the operating constraints of electric\nnetworks in an online fashion. Although online decision problems have been\nwell-studied in the literature, a unique challenge arising in power systems is\nthe presence of non-linear constraints, a departure from the traditional\nsettings. A particular example is the capacity constraint of apparent power,\nwhich gives rise to a quadratic constraint, rather than typical linear\nconstraints. In this paper, we present a competitive randomized online\nalgorithm for deciding whether a sequence of inelastic demands can be allocated\nfor the requested intervals, subject to the total satisfiable apparent power\nwithin a time-varying capacity constraint. We also consider an alternative\nsetting with nodal voltage constraint, using a variant of the online algorithm.\nFinally, simulation studies are provided to evaluate the algorithms\nempirically.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 11:53:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Karapetyan", "Areg", ""], ["Khonji", "Majid", ""], ["Chau", "Chi-Kin", ""], ["Elbassioni", "Khaled", ""]]}, {"id": "1611.00665", "submitter": "Sahil Singla", "authors": "Aviad Rubinstein and Sahil Singla", "title": "Combinatorial Prophet Inequalities", "comments": "28 Pages, SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework of Prophet Inequalities for combinatorial\nvaluation functions. For a (non-monotone) submodular objective function over an\narbitrary matroid feasibility constraint, we give an $O(1)$-competitive\nalgorithm. For a monotone subadditive objective function over an arbitrary\ndownward-closed feasibility constraint, we give an $O(\\log n \\log^2\nr)$-competitive algorithm (where $r$ is the cardinality of the largest feasible\nsubset).\n  Inspired by the proof of our subadditive prophet inequality, we also obtain\nan $O(\\log n \\cdot \\log^2 r)$-competitive algorithm for the Secretary Problem\nwith a monotone subadditive objective function subject to an arbitrary\ndownward-closed feasibility constraint. Even for the special case of a\ncardinality feasibility constraint, our algorithm circumvents an\n$\\Omega(\\sqrt{n})$ lower bound by Bateni, Hajiaghayi, and Zadimoghaddam\n\\cite{BHZ13-submodular-secretary_original} in a restricted query model.\n  En route to our submodular prophet inequality, we prove a technical result of\nindependent interest: we show a variant of the Correlation Gap Lemma for\nnon-monotone submodular functions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:09:26 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Rubinstein", "Aviad", ""], ["Singla", "Sahil", ""]]}, {"id": "1611.00711", "submitter": "Reza Takapoui", "authors": "Reza Takapoui and Stephen Boyd", "title": "Linear Programming Heuristics for the Graph Isomorphism Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An isomorphism between two graphs is a bijection between their vertices that\npreserves the edges. We consider the problem of determining whether two finite\nundirected weighted graphs are isomorphic, and finding an isomorphism relating\nthem if the answer is positive. In this paper we introduce effective\nprobabilistic linear programming (LP) heuristics to solve the graph isomorphism\nproblem. We motivate our heuristics by showing guarantees under some\nconditions, and present numerical experiments that show effectiveness of these\nheuristics in the general case.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 18:24:40 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Takapoui", "Reza", ""], ["Boyd", "Stephen", ""]]}, {"id": "1611.00721", "submitter": "Virginia Vassilevska Williams", "authors": "Jakub Pachocki, Liam Roditty, Aaron Sidford, Roei Tov, Virginia\n  Vassilevska Williams", "title": "Approximating Cycles in Directed Graphs: Fast Algorithms for Girth and\n  Roundtrip Spanners", "comments": "this is a revision of the original submission that fixes an error\n  (the original submission claimed that the additive approximation algorithm\n  also works for roundtrip spanners; this is false)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The girth of a graph, i.e. the length of its shortest cycle, is a fundamental\ngraph parameter. Unfortunately all known algorithms for computing, even\napproximately, the girth and girth-related structures in directed weighted\n$m$-edge and $n$-node graphs require $\\Omega(\\min\\{n^{\\omega}, mn\\})$ time (for\n$2\\leq\\omega<2.373$). In this paper, we drastically improve these runtimes as\nfollows:\n  * Multiplicative Approximations in Nearly Linear Time: We give an algorithm\nthat in $\\widetilde{O}(m)$ time computes an $\\widetilde{O}(1)$-multiplicative\napproximation of the girth as well as an $\\widetilde{O}(1)$-multiplicative\nroundtrip spanner with $\\widetilde{O}(n)$ edges with high probability (w.h.p).\n  * Nearly Tight Additive Approximations: For unweighted graphs and any $\\alpha\n\\in (0,1)$ we give an algorithm that in $\\widetilde{O}(mn^{1 - \\alpha})$ time\ncomputes an $O(n^\\alpha)$-additive approximation of the girth w.h.p, and\npartially derandomize it. We show that the runtime of our algorithm cannot be\nsignificantly improved without a breakthrough in combinatorial Boolean matrix\nmultiplication.\n  Our main technical contribution to achieve these results is the first nearly\nlinear time algorithm for computing roundtrip covers, a directed graph\ndecomposition concept key to previous roundtrip spanner constructions.\nPreviously it was not known how to compute these significantly faster than\n$\\Omega(\\min\\{n^\\omega, mn\\})$ time. Given the traditional difficulty in\nefficiently processing directed graphs, we hope our techniques may find further\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 18:40:54 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 20:32:04 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Pachocki", "Jakub", ""], ["Roditty", "Liam", ""], ["Sidford", "Aaron", ""], ["Tov", "Roei", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1611.00755", "submitter": "Jonathan A. Kelner", "authors": "Michael B. Cohen, Jonathan Kelner, John Peebles, Richard Peng, Anup\n  Rao, Aaron Sidford, Adrian Vladu", "title": "Almost-Linear-Time Algorithms for Markov Chains and New Spectral\n  Primitives for Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a notion of spectral approximation for directed\ngraphs. While there are many potential ways one might define approximation for\ndirected graphs, most of them are too strong to allow sparse approximations in\ngeneral. In contrast, we prove that for our notion of approximation, such\nsparsifiers do exist, and we show how to compute them in almost linear time.\n  Using this notion of approximation, we provide a general framework for\nsolving asymmetric linear systems that is broadly inspired by the work of\n[Peng-Spielman, STOC`14]. Applying this framework in conjunction with our\nsparsification algorithm, we obtain an almost linear time algorithm for solving\ndirected Laplacian systems associated with Eulerian Graphs. Using this solver\nin the recent framework of [Cohen-Kelner-Peebles-Peng-Sidford-Vladu, FOCS`16],\nwe obtain almost linear time algorithms for solving a directed Laplacian linear\nsystem, computing the stationary distribution of a Markov chain, computing\nexpected commute times in a directed graph, and more.\n  For each of these problems, our algorithms improves the previous best running\ntimes of $O((nm^{3/4} + n^{2/3} m) \\log^{O(1)} (n \\kappa \\epsilon^{-1}))$ to\n$O((m + n2^{O(\\sqrt{\\log{n}\\log\\log{n}})}) \\log^{O(1)} (n \\kappa\n\\epsilon^{-1}))$ where $n$ is the number of vertices in the graph, $m$ is the\nnumber of edges, $\\kappa$ is a natural condition number associated with the\nproblem, and $\\epsilon$ is the desired accuracy. We hope these results open the\ndoor for further studies into directed spectral graph theory, and will serve as\na stepping stone for designing a new generation of fast algorithms for directed\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 19:56:36 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Cohen", "Michael B.", ""], ["Kelner", "Jonathan", ""], ["Peebles", "John", ""], ["Peng", "Richard", ""], ["Rao", "Anup", ""], ["Sidford", "Aaron", ""], ["Vladu", "Adrian", ""]]}, {"id": "1611.00756", "submitter": "Yair Carmon", "authors": "Yair Carmon, John C. Duchi, Oliver Hinder and Aaron Sidford", "title": "Accelerated Methods for Non-Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accelerated gradient method for non-convex optimization\nproblems with Lipschitz continuous first and second derivatives. The method\nrequires time $O(\\epsilon^{-7/4} \\log(1/ \\epsilon) )$ to find an\n$\\epsilon$-stationary point, meaning a point $x$ such that $\\|\\nabla f(x)\\| \\le\n\\epsilon$. The method improves upon the $O(\\epsilon^{-2} )$ complexity of\ngradient descent and provides the additional second-order guarantee that\n$\\nabla^2 f(x) \\succeq -O(\\epsilon^{1/2})I$ for the computed $x$. Furthermore,\nour method is Hessian free, i.e. it only requires gradient computations, and is\ntherefore suitable for large scale applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 19:58:30 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 18:56:35 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Carmon", "Yair", ""], ["Duchi", "John C.", ""], ["Hinder", "Oliver", ""], ["Sidford", "Aaron", ""]]}, {"id": "1611.00783", "submitter": "William Hoza", "authors": "William M. Hoza, Adam R. Klivans", "title": "Preserving Randomness for Adaptive Algorithms", "comments": "To appear in RANDOM 2018. 32 pages, 2 figures. Added sections 1.5.3\n  and 7.1, changed terminology, fixed typos, improved presentation, added\n  appendix C, simplified abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose $\\mathsf{Est}$ is a randomized estimation algorithm that uses $n$\nrandom bits and outputs values in $\\mathbb{R}^d$. We show how to execute\n$\\mathsf{Est}$ on $k$ adaptively chosen inputs using only $n + O(k \\log(d +\n1))$ random bits instead of the trivial $nk$ (at the cost of mild increases in\nthe error and failure probability). Our algorithm combines a variant of the INW\npseudorandom generator (STOC '94) with a new scheme for shifting and rounding\nthe outputs of $\\mathsf{Est}$. We prove that modifying the outputs of\n$\\mathsf{Est}$ is necessary in this setting, and furthermore, our algorithm's\nrandomness complexity is near-optimal in the case $d \\leq O(1)$. As an\napplication, we give a randomness-efficient version of the Goldreich-Levin\nalgorithm; our algorithm finds all Fourier coefficients with absolute value at\nleast $\\theta$ of a function $F: \\{0, 1\\}^n \\to \\{-1, 1\\}$ using $O(n \\log n)\n\\cdot \\text{poly}(1/\\theta)$ queries to $F$ and $O(n)$ random bits (independent\nof $\\theta$), improving previous work by Bshouty et al. (JCSS '04).\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 20:06:31 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 20:48:30 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 04:16:22 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 19:16:52 GMT"}, {"version": "v5", "created": "Wed, 13 Jun 2018 16:32:22 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Hoza", "William M.", ""], ["Klivans", "Adam R.", ""]]}, {"id": "1611.00829", "submitter": "Adrian Vladu", "authors": "Ilan Lobel, Renato Paes Leme, Adrian Vladu", "title": "Multidimensional Binary Search for Contextual Decision-Making", "comments": "Appears in EC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multidimensional search problem that is motivated by questions\nin contextual decision-making, such as dynamic pricing and personalized\nmedicine. Nature selects a state from a $d$-dimensional unit ball and then\ngenerates a sequence of $d$-dimensional directions. We are given access to the\ndirections, but not access to the state. After receiving a direction, we have\nto guess the value of the dot product between the state and the direction. Our\ngoal is to minimize the number of times when our guess is more than $\\epsilon$\naway from the true answer. We construct a polynomial time algorithm that we\ncall Projected Volume achieving regret $O(d\\log(d/\\epsilon))$, which is optimal\nup to a $\\log d$ factor. The algorithm combines a volume cutting strategy with\na new geometric technique that we call cylindrification.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 22:38:32 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 02:29:51 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Lobel", "Ilan", ""], ["Leme", "Renato Paes", ""], ["Vladu", "Adrian", ""]]}, {"id": "1611.00840", "submitter": "Micha{\\l} Pilipczuk", "authors": "Daniel Lokshtanov, Micha{\\l} Pilipczuk, Saket Saurabh", "title": "Below all subsets for Minimal Connected Dominating Set", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex subset $S$ in a graph $G$ is a dominating set if every vertex not\ncontained in $S$ has a neighbor in $S$. A dominating set $S$ is a connected\ndominating set if the subgraph $G[S]$ induced by $S$ is connected. A connected\ndominating set $S$ is a minimal connected dominating set if no proper subset of\n$S$ is also a connected dominating set. We prove that there exists a constant\n$\\varepsilon > 10^{-50}$ such that every graph $G$ on $n$ vertices has at most\n$O(2^{(1-\\varepsilon)n})$ minimal connected dominating sets. For the same\n$\\varepsilon$ we also give an algorithm with running time\n$2^{(1-\\varepsilon)n}\\cdot n^{O(1)}$ to enumerate all minimal connected\ndominating sets in an input graph $G$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 23:21:18 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""]]}, {"id": "1611.00889", "submitter": "Kasra Khosoussi", "authors": "Kasra Khosoussi, Gaurav S. Sukhatme, Shoudong Huang, Gamini\n  Dissanayake", "title": "Designing Sparse Reliable Pose-Graph SLAM: A Graph-Theoretic Approach", "comments": "WAFR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to design sparse D-optimal (determinantoptimal)\npose-graph SLAM problems through the synthesis of sparse graphs with the\nmaximum weighted number of spanning trees. Characterizing graphs with the\nmaximum number of spanning trees is an open problem in general. To tackle this\nproblem, several new theoretical results are established in this paper,\nincluding the monotone log-submodularity of the weighted number of spanning\ntrees. By exploiting these structures, we design a complementary pair of\nnear-optimal efficient approximation algorithms with provable guarantees. Our\ntheoretical results are validated using random graphs and a publicly available\npose-graph SLAM dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 05:52:37 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Khosoussi", "Kasra", ""], ["Sukhatme", "Gaurav S.", ""], ["Huang", "Shoudong", ""], ["Dissanayake", "Gamini", ""]]}, {"id": "1611.00898", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Low Rank Approximation with Entrywise $\\ell_1$-Norm Error", "comments": "STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the $\\ell_1$-low rank approximation problem, where for a given $n\n\\times d$ matrix $A$ and approximation factor $\\alpha \\geq 1$, the goal is to\noutput a rank-$k$ matrix $\\widehat{A}$ for which\n  $$\\|A-\\widehat{A}\\|_1 \\leq \\alpha \\cdot \\min_{\\textrm{rank-}k\\textrm{\nmatrices}~A'}\\|A-A'\\|_1,$$ where for an $n \\times d$ matrix $C$, we let\n$\\|C\\|_1 = \\sum_{i=1}^n \\sum_{j=1}^d |C_{i,j}|$. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is\nindicated in models where Gaussian assumptions on the noise may not apply. The\nproblem was shown to be NP-hard by Gillis and Vavasis and a number of\nheuristics have been proposed. It was asked in multiple places if there are any\napproximation algorithms.\n  We give the first provable approximation algorithms for $\\ell_1$-low rank\napproximation, showing that it is possible to achieve approximation factor\n$\\alpha = (\\log d) \\cdot \\mathrm{poly}(k)$ in $\\mathrm{nnz}(A) + (n+d)\n\\mathrm{poly}(k)$ time, where $\\mathrm{nnz}(A)$ denotes the number of non-zero\nentries of $A$. If $k$ is constant, we further improve the approximation ratio\nto $O(1)$ with a $\\mathrm{poly}(nd)$-time algorithm. Under the Exponential Time\nHypothesis, we show there is no $\\mathrm{poly}(nd)$-time algorithm achieving a\n$(1+\\frac{1}{\\log^{1+\\gamma}(nd)})$-approximation, for $\\gamma > 0$ an\narbitrarily small constant, even when $k = 1$.\n  We give a number of additional results for $\\ell_1$-low rank approximation:\nnearly tight upper and lower bounds for column subset selection, CUR\ndecompositions, extensions to low rank approximation with respect to\n$\\ell_p$-norms for $1 \\leq p < 2$ and earthmover distance, low-communication\ndistributed protocols and low-memory streaming algorithms, algorithms with\nlimited randomness, and bicriteria algorithms. We also give a preliminary\nempirical evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 07:13:20 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 13:57:43 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1611.00911", "submitter": "Kasper Green Larsen", "authors": "Kasper Eenberg, Kasper Green Larsen, Huacheng Yu", "title": "DecreaseKeys are Expensive for External Memory Priority Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest open problems in external memory data structures is the\npriority queue problem with DecreaseKey operations. If only Insert and\nExtractMin operations need to be supported, one can design a comparison-based\npriority queue performing $O((N/B)\\lg_{M/B} N)$ I/Os over a sequence of $N$\noperations, where $B$ is the disk block size in number of words and $M$ is the\nmain memory size in number of words. This matches the lower bound for\ncomparison-based sorting and is hence optimal for comparison-based priority\nqueues. However, if we also need to support DecreaseKeys, the performance of\nthe best known priority queue is only $O((N/B) \\lg_2 N)$ I/Os. The big open\nquestion is whether a degradation in performance really is necessary. We answer\nthis question affirmatively by proving a lower bound of $\\Omega((N/B) \\lg_{\\lg\nN} B)$ I/Os for processing a sequence of $N$ intermixed Insert, ExtraxtMin and\nDecreaseKey operations. Our lower bound is proved in the cell probe model and\nthus holds also for non-comparison-based priority queues.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 08:30:38 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Eenberg", "Kasper", ""], ["Larsen", "Kasper Green", ""], ["Yu", "Huacheng", ""]]}, {"id": "1611.00918", "submitter": "Kasper Green Larsen", "authors": "Karl Bringmann, Allan Gr{\\o}nlund, Kasper Green Larsen", "title": "A Dichotomy for Regular Expression Membership Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regular expression membership testing: Given a regular expression of\nsize $m$ and a string of size $n$, decide whether the string is in the language\ndescribed by the regular expression. Its classic $O(nm)$ algorithm is one of\nthe big success stories of the 70s, which allowed pattern matching to develop\ninto the standard tool that it is today.\n  Many special cases of pattern matching have been studied that can be solved\nfaster than in quadratic time. However, a systematic study of tractable cases\nwas made possible only recently, with the first conditional lower bounds\nreported by Backurs and Indyk [FOCS'16]. Restricted to any \"type\" of\nhomogeneous regular expressions of depth 2 or 3, they either presented a\nnear-linear time algorithm or a quadratic conditional lower bound, with one\nexception known as the Word Break problem.\n  In this paper we complete their work as follows:\n  1) We present two almost-linear time algorithms that generalize all known\nalmost-linear time algorithms for special cases of regular expression\nmembership testing.\n  2) We classify all types, except for the Word Break problem, into\nalmost-linear time or quadratic time assuming the Strong Exponential Time\nHypothesis. This extends the classification from depth 2 and 3 to any constant\ndepth.\n  3) For the Word Break problem we give an improved $\\tilde{O}(n m^{1/3} + m)$\nalgorithm. Surprisingly, we also prove a matching conditional lower bound for\ncombinatorial algorithms. This establishes Word Break as the only intermediate\nproblem.\n  In total, we prove matching upper and lower bounds for any type of\nbounded-depth homogeneous regular expressions, which yields a full dichotomy\nfor regular expression membership testing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 08:55:07 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 06:44:04 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Bringmann", "Karl", ""], ["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1611.00922", "submitter": "Magomed Malsagov Yusupovich", "authors": "Yakov M. Karandashev and Magomed Yu. Malsagov", "title": "Polynomial algorithm for exact calculation of partition function for\n  binary spin model on planar graphs", "comments": "9 pages, 10 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and realize (the code is publicly available at\nhttps://github.com/Thrawn1985/2D-Partition-Function) an algorithm for exact\ncalculation of partition function for planar graph models with binary spins.\nThe complexity of the algorithm is O(N^2). Test experiments shows good\nagreement with Onsager's analytical solution for two-dimensional Ising model of\ninfinite size.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 09:12:53 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Karandashev", "Yakov M.", ""], ["Malsagov", "Magomed Yu.", ""]]}, {"id": "1611.00938", "submitter": "Johann Paratte", "authors": "Johan Paratte and Lionel Martin", "title": "Fast Eigenspace Approximation using Random Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus in this work on the estimation of the first $k$ eigenvectors of any\ngraph Laplacian using filtering of Gaussian random signals. We prove that we\nonly need $k$ such signals to be able to exactly recover as many of the\nsmallest eigenvectors, regardless of the number of nodes in the graph. In\naddition, we address key issues in implementing the theoretical concepts in\npractice using accurate approximated methods. We also propose fast algorithms\nboth for eigenspace approximation and for the determination of the $k$th\nsmallest eigenvalue $\\lambda_k$. The latter proves to be extremely efficient\nunder the assumption of locally uniform distribution of the eigenvalue over the\nspectrum. Finally, we present experiments which show the validity of our method\nin practice and compare it to state-of-the-art methods for clustering and\nvisualization both on synthetic small-scale datasets and larger real-world\nproblems of millions of nodes. We show that our method allows a better scaling\nwith the number of nodes than all previous methods while achieving an almost\nperfect reconstruction of the eigenspace formed by the first $k$ eigenvectors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:08:22 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 09:25:41 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Paratte", "Johan", ""], ["Martin", "Lionel", ""]]}, {"id": "1611.00992", "submitter": "Tzvi Alon", "authors": "Tzvi Alon", "title": "Fully polynomial time approximation schemes (FPTAS) for some counting\n  problems", "comments": "MA thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we develop FPTASs for the counting problems of m-tuples,\ncontingency tables with two rows, and 0/1 knapsack. For the problem of counting\nm-tuples, we design two algorithms, one is strongly polynomial. As far as we\nknow, these are the first FPTASs for this problem. For the problem of counting\ncontingency tables we improve significantly over the running time of existing\nalgorithms. For the problem of counting 0/1 knapsack solutions, we design a\nsimple strongly polynomial algorithm, with similar running times to the\nexisting algorithms. Our results are derived by using, as well as expanding,\nthe method of K-approximation sets and functions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 13:10:17 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Alon", "Tzvi", ""]]}, {"id": "1611.01004", "submitter": "Katherine Edwards", "authors": "Katherine Edwards, Irene Muzi, Paul Wollan", "title": "Half-integral linkages in highly connected directed graphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the half-integral $k$-Directed Disjoint Paths Problem\n($\\tfrac12$kDDPP) in highly strongly connected digraphs. The integral kDDPP is\nNP-complete even when restricted to instances where $k=2$, and the input graph\nis $L$-strongly connected, for any $L\\geq 1$. We show that when the integrality\ncondition is relaxed to allow each vertex to be used in two paths, the problem\nbecomes efficiently solvable in highly connected digraphs (even with $k$ as\npart of the input). Specifically, we show that there is an absolute constant\n$c$ such that for each $k\\geq 2$ there exists $L(k)$ such that $\\tfrac12$kDDPP\nis solvable in time $O(|V(G)|^c)$ for a $L(k)$-strongly connected directed\ngraph $G$. As the function $L(k)$ grows rather quickly, we also show that\n$\\tfrac12$kDDPP is solvable in time $O(|V(G)|^{f(k)})$ in $(36k^3+2k)$-strongly\nconnected directed graphs. We also show that for each $\\epsilon<1$ deciding\nhalf-integral feasibility of kDDPP instances is NP-complete when $k$ is given\nas part of the input, even when restricted to graphs with strong connectivity\n$\\epsilon k$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 13:35:51 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Edwards", "Katherine", ""], ["Muzi", "Irene", ""], ["Wollan", "Paul", ""]]}, {"id": "1611.01017", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Gianluca Della Vedova, Gabriella Trucco", "title": "Solving the Persistent Phylogeny Problem in polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The notion of a Persistent Phylogeny generalizes the well-known Perfect\nphylogeny model that has been thoroughly investigated and is used to explain a\nwide range of evolutionary phenomena. More precisely, while the Perfect\nPhylogeny model allows each character to be acquired once in the entire\nevolutionary history while character losses are not allowed, the Persistent\nPhylogeny model allows each character to be both acquired and lost exactly once\nin the evolutionary history. The Persistent Phylogeny Problem (PPP) is the\nproblem of reconstructing a Persistent phylogeny tree, if it exists, from a\nbinary matrix where the rows represent the species (or the individuals) studied\nand the columns represent the characters that each species can have.\n  While the Perfect Phylogeny has a linear-time algorithm, the computational\ncomplexity of PPP has been posed, albeit in an equivalent formulation, 20 years\nago. We settle the question by providing a polynomial time algorithm for the\nPersistent Phylogeny problem.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 13:56:58 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Trucco", "Gabriella", ""]]}, {"id": "1611.01032", "submitter": "Chi-Kin Chau", "authors": "Chi-Kin Chau, Khaled Elbassioni and Chien-Ming Tseng", "title": "Drive Mode Optimization and Path Planning for Plug-in Hybrid Electric\n  Vehicles", "comments": "To appear in IEEE Transactions on Intelligent Transportation Systems", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems ( Volume:\n  18, Issue: 12, Dec. 2017 ), pp 3421 - 3432", "doi": "10.1109/TITS.2017.2691606", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drive modes are driver-selectable pre-set configurations of powertrain and\ncertain vehicle parameters. Plug-in hybrid electric vehicles (PHEVs) typically\nfeature special options of drive modes that can affect the hybrid energy source\nmanagement system, for example, electric vehicle (EV) mode (that draws fully on\nbattery) and charge sustaining (CS) mode (that utilizes internal combustion\nengine to charge battery while propelling the vehicle). This paper studies an\noptimization problem to enable the driver to select the appropriate drive modes\nfor fuel minimization. We develop optimization algorithms that optimize the\ndecisions of drive modes based on trip information, and integrated with path\nplanning to find an optimal path, considering intermediate filling and charging\nstations. We further provide an online algorithm that is based on the revealed\ntrip information. We evaluate our algorithms empirically on a Chevrolet Volt,\nwhich shows significant fuel savings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:06:15 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 20:56:12 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chau", "Chi-Kin", ""], ["Elbassioni", "Khaled", ""], ["Tseng", "Chien-Ming", ""]]}, {"id": "1611.01137", "submitter": "Elias Stehle", "authors": "Elias Stehle, Hans-Arno Jacobsen", "title": "A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs", "comments": "16 pages, accepted at SIGMOD 2017", "journal-ref": "SIGMOD (2017) 417-432", "doi": "10.1145/3035918.3064043", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is at the core of many database operations, such as index creation,\nsort-merge joins, and user-requested output sorting. As GPUs are emerging as a\npromising platform to accelerate various operations, sorting on GPUs becomes a\nviable endeavour. Over the past few years, several improvements have been\nproposed for sorting on GPUs, leading to the first radix sort implementations\nthat achieve a sorting rate of over one billion 32-bit keys per second. Yet,\nstate-of-the-art approaches are heavily memory bandwidth-bound, as they require\nsubstantially more memory transfers than their CPU-based counterparts.\n  Our work proposes a novel approach that almost halves the amount of memory\ntransfers and, therefore, considerably lifts the memory bandwidth limitation.\nBeing able to sort two gigabytes of eight-byte records in as little as 50\nmilliseconds, our approach achieves a 2.32-fold improvement over the\nstate-of-the-art GPU-based radix sort for uniform distributions, sustaining a\nminimum speed-up of no less than a factor of 1.66 for skewed distributions.\n  To address inputs that either do not reside on the GPU or exceed the\navailable device memory, we build on our efficient GPU sorting approach with a\npipelined heterogeneous sorting algorithm that mitigates the overhead\nassociated with PCIe data transfers. Comparing the end-to-end sorting\nperformance to the state-of-the-art CPU-based radix sort running 16 threads,\nour heterogeneous approach achieves a 2.06-fold and a 1.53-fold improvement for\nsorting 64 GB key-value pairs with a skewed and a uniform distribution,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:33:06 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 12:22:16 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stehle", "Elias", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1611.01146", "submitter": "Zeyuan Allen-Zhu", "authors": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, Tengyu Ma", "title": "Finding Approximate Local Minima Faster than Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a non-convex second-order optimization algorithm that is guaranteed\nto return an approximate local minimum in time which scales linearly in the\nunderlying dimension and the number of training examples. The time complexity\nof our algorithm to find an approximate local minimum is even faster than that\nof gradient descent to find a critical point. Our algorithm applies to a\ngeneral class of optimization problems including training a neural network and\nother non-convex objectives arising in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:50:32 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 18:38:50 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 18:13:20 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 19:20:07 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Agarwal", "Naman", ""], ["Allen-Zhu", "Zeyuan", ""], ["Bullins", "Brian", ""], ["Hazan", "Elad", ""], ["Ma", "Tengyu", ""]]}, {"id": "1611.01190", "submitter": "Igor Carboni Oliveira", "authors": "Igor C. Oliveira, Rahul Santhanam", "title": "Conspiracies between Learning Algorithms, Circuit Lower Bounds and\n  Pseudorandomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove several results giving new and stronger connections between\nlearning, circuit lower bounds and pseudorandomness. Among other results, we\nshow a generic learning speedup lemma, equivalences between various learning\nmodels in the exponential time and subexponential time regimes, a dichotomy\nbetween learning and pseudorandomness, consequences of non-trivial learning for\ncircuit lower bounds, Karp-Lipton theorems for probabilistic exponential time,\nand NC$^1$-hardness for the Minimum Circuit Size Problem.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 21:08:38 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Oliveira", "Igor C.", ""], ["Santhanam", "Rahul", ""]]}, {"id": "1611.01210", "submitter": "Mauricio Resende", "authors": "David S. Johnson, Lee Breslau, Ilias Diakonikolas, Nick Duffield, Yu\n  Gu, MohammadTaghi Hajiaghayi, Howard Karloff, Mauricio G. C. Resende, and\n  Subhabrata Sen", "title": "Near-Optimal Disjoint-Path Facility Location Through Set Cover by Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two special cases of the \"cover-by-pairs\"\noptimization problem that arise when we need to place facilities so that each\ncustomer is served by two facilities that reach it by disjoint shortest paths.\nThese problems arise in a network traffic monitoring scheme proposed by Breslau\net al. and have potential applications to content distribution. The\n\"set-disjoint\" variant applies to networks that use the OSPF routing protocol,\nand the \"path-disjoint\" variant applies when MPLS routing is enabled, making\nbetter solutions possible at the cost of greater operational expense. Although\nwe can prove that no polynomial-time algorithm can guarantee good solutions for\neither version, we are able to provide heuristics that do very well in practice\non instances with real-world network structure. Fast implementations of the\nheuristics, made possible by exploiting mathematical observations about the\nrelationship between the network instances and the corresponding instances of\nthe cover-by-pairs problem, allow us to perform an extensive experimental\nevaluation of the heuristics and what the solutions they produce tell us about\nthe effectiveness of the proposed monitoring scheme. For the set-disjoint\nvariant, we validate our claim of near-optimality via a new lower-bounding\ninteger programming formulation. Although computing this lower bound requires\nsolving the NP-hard Hitting Set problem and can underestimate the optimal value\nby a linear factor in the worst case, it can be computed quickly by CPLEX, and\nit equals the optimal solution value for all the instances in our extensive\ntestbed.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 22:27:57 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Johnson", "David S.", ""], ["Breslau", "Lee", ""], ["Diakonikolas", "Ilias", ""], ["Duffield", "Nick", ""], ["Gu", "Yu", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Karloff", "Howard", ""], ["Resende", "Mauricio G. C.", ""], ["Sen", "Subhabrata", ""]]}, {"id": "1611.01259", "submitter": "Nika Haghtalab", "authors": "Avrim Blum, Nika Haghtalab", "title": "Generalized Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 03:45:03 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Blum", "Avrim", ""], ["Haghtalab", "Nika", ""]]}, {"id": "1611.01364", "submitter": "Shankar Sivarajan", "authors": "Shankar N. Sivarajan", "title": "A Generalization of the Minisum and Minimax Voting Methods", "comments": "11 pages", "journal-ref": "SIAM Undergraduate Research Online (SIURO), Volume 11 (2018)", "doi": "10.1137/16S014870", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a family of approval voting-schemes for electing\ncommittees based on the preferences of voters. In our schemes, we calculate the\nvector of distances of the possible committees from each of the ballots and,\nfor a given $ p $-norm, choose the one that minimizes the magnitude of the\ndistance vector under that norm. The minisum and minimax methods suggested by\nprevious authors and analyzed extensively in the literature naturally appear as\nspecial cases corresponding to $ p = 1 $ and $ p = \\infty, $ respectively.\nSupported by examples, we suggest that using a small value of $ p, $ such as 2\nor 3, provides a good compromise between the minisum and minimax voting methods\nwith regard to the weightage given to approvals and disapprovals. For large but\nfinite $ p, $ our method reduces to finding the committee that covers the\nmaximum number of voters, and this is far superior to the minimax method which\nis prone to ties. We also discuss extensions of our methods to ternary voting.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 18:48:07 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 22:47:52 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Sivarajan", "Shankar N.", ""]]}, {"id": "1611.01403", "submitter": "Lucas Boczkowski", "authors": "Lucas Boczkowski, Uriel Feige, Amos Korman and Yoav Rodeh", "title": "Searching Trees with Permanently Noisy Advice: Walking and Query\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a search problem on trees in which the goal is to find an\nadversarially placed treasure, while relying on local, partial information.\nSpecifically, each node in the tree holds a pointer to one of its neighbors,\ntermed \\emph{advice}. A node is faulty with probability $q$. The advice at a\nnon-faulty node points to the neighbor that is closer to the treasure, and the\nadvice at a faulty node points to a uniformly random neighbor. Crucially, the\nadvice is {\\em permanent}, in the sense that querying the same node again would\nyield the same answer. Let $\\Delta$ denote the maximal degree. Roughly\nspeaking, when considering the expected number of {\\em moves}, i.e., edge\ntraversals, we show that a phase transition occurs when the {\\em noise\nparameter} $q$ is about $1/\\sqrt{\\Delta}$. Below the threshold, there exists an\nalgorithm with expected move complexity $O(D\\sqrt{\\Delta})$, where $D$ is the\ndepth of the treasure, whereas above the threshold, every search algorithm has\nexpected number of moves which is both exponential in $D$ and polynomial in the\nnumber of nodes~$n$. In contrast, if we require to find the treasure with\nprobability at least $1-\\delta$, then for every fixed $\\varepsilon > 0$, if\n$q<1/\\Delta^{\\varepsilon}$ then there exists a search strategy that with\nprobability $1-\\delta$ finds the treasure using $(\\delta^{-1}D)^{O(\\frac 1\n\\varepsilon)}$ moves. Moreover, we show that $(\\delta^{-1}D)^{\\Omega(\\frac 1\n\\varepsilon)}$ moves are necessary. Besides the number of moves, we also study\nthe number of advice {\\em queries} required to find the treasure. Roughly\nspeaking, for this complexity, we show similar threshold results to those\npreviously stated, where the parameter $D$ is replaced by $\\log n$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 14:45:03 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 11:44:16 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 11:59:06 GMT"}, {"version": "v4", "created": "Thu, 16 Jan 2020 06:34:27 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Boczkowski", "Lucas", ""], ["Feige", "Uriel", ""], ["Korman", "Amos", ""], ["Rodeh", "Yoav", ""]]}, {"id": "1611.01479", "submitter": "Nicola Prezza", "authors": "Philip Bille, Inge Li G{\\o}rtz, Nicola Prezza", "title": "Space-Efficient Re-Pair Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-Pair is an effective grammar-based compression scheme achieving strong\ncompression rates in practice. Let $n$, $\\sigma$, and $d$ be the text length,\nalphabet size, and dictionary size of the final grammar, respectively. In their\noriginal paper, the authors show how to compute the Re-Pair grammar in expected\nlinear time and $5n + 4\\sigma^2 + 4d + \\sqrt{n}$ words of working space on top\nof the text. In this work, we propose two algorithms improving on the space of\ntheir original solution. Our model assumes a memory word of $\\lceil\\log_2\nn\\rceil$ bits and a re-writable input text composed by $n$ such words. Our\nfirst algorithm runs in expected $\\mathcal O(n/\\epsilon)$ time and uses\n$(1+\\epsilon)n +\\sqrt n$ words of space on top of the text for any parameter\n$0<\\epsilon \\leq 1$ chosen in advance. Our second algorithm runs in expected\n$\\mathcal O(n\\log n)$ time and improves the space to $n +\\sqrt n$ words.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:25:21 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Prezza", "Nicola", ""]]}, {"id": "1611.01569", "submitter": "Albert Gu", "authors": "Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\\'e, Atri\n  Rudra", "title": "A Two Pronged Progress in Structured Dense Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-vector multiplication is one of the most fundamental computing\nprimitives. Given a matrix $A\\in\\mathbb{F}^{N\\times N}$ and a vector $b$, it is\nknown that in the worst case $\\Theta(N^2)$ operations over $\\mathbb{F}$ are\nneeded to compute $Ab$. A broad question is to identify classes of structured\ndense matrices that can be represented with $O(N)$ parameters, and for which\nmatrix-vector multiplication can be performed sub-quadratically. One such class\nof structured matrices is the orthogonal polynomial transforms, whose rows\ncorrespond to a family of orthogonal polynomials. Other well known classes\ninclude the Toeplitz, Hankel, Vandermonde, Cauchy matrices and their extensions\nthat are all special cases of a ldisplacement rank property. In this paper, we\nmake progress on two fronts:\n  1. We introduce the notion of recurrence width of matrices. For matrices with\nconstant recurrence width, we design algorithms to compute $Ab$ and $A^Tb$ with\na near-linear number of operations. This notion of width is finer than all the\nabove classes of structured matrices and thus we can compute multiplication for\nall of them using the same core algorithm.\n  2. We additionally adapt this algorithm to an algorithm for a much more\ngeneral class of matrices with displacement structure: those with low\ndisplacement rank with respect to quasiseparable matrices. This class includes\nToeplitz-plus-Hankel-like matrices, Discrete Cosine/Sine Transforms, and more,\nand captures all previously known matrices with displacement structure that we\nare aware of under a unified parametrization and algorithm.\n  Our work unifies, generalizes, and simplifies existing state-of-the-art\nresults in structured matrix-vector multiplication. Finally, we show how\napplications in areas such as multipoint evaluations of multivariate\npolynomials can be reduced to problems involving low recurrence width matrices.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 23:33:41 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 05:07:48 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 02:07:28 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["De Sa", "Christopher", ""], ["Gu", "Albert", ""], ["Puttagunta", "Rohan", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1611.01644", "submitter": "Bundit Laekhanukit", "authors": "Fabrizio Grandoni and Bundit Laekhanukit", "title": "Surviving in Directed Graphs: A Polylogarithmic Approximation for\n  Two-Connected Directed Steiner Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a survivable network design problem on directed\ngraphs, 2-Connected Directed Steiner Tree (2-DST): given an $n$-vertex weighted\ndirected graph, a root $r$, and a set of $h$ terminals $S$, find a min-cost\nsubgraph $H$ that has two edge/vertex disjoint paths from $r$ to any $t\\in S$.\n2-DST is a natural generalization of the classical Directed Steiner Tree\nproblem (DST), where we have an additional requirement that the network must\ntolerate one failure. No non-trivial approximation is known for 2-DST. This was\nleft as an open problem by Feldman et al., [SODA'09; JCSS] and has then been\nstudied by Cheriyan et al. [SODA'12; TALG] and Laekhanukit SODA'14]. However,\nno positive result was known except for the special case of a $D$-shallow\ninstance [Laekhanukit, ICALP'16].\n  We present an $O(D^3\\log D\\cdot h^{2/D}\\cdot \\log n)$ approximation algorithm\nfor 2-DST that runs in time $O(n^{O(D)})$, for any $D\\in[\\log_2h]$. This\nimplies a polynomial-time $O(h^\\epsilon \\log n)$ approximation for any constant\n$\\epsilon>0$, and a poly-logarithmic approximation running in quasi-polynomial\ntime. We remark that this is essentially the best-known even for the classical\nDST, and the latter problem is $O(\\log^{2-\\epsilon}n)$-hard to approximate\n[Halperin and Krauthgamer, STOC'03]. As a by product, we obtain an algorithm\nwith the same approximation guarantee for the $2$-Connected Directed Steiner\nSubgraph problem, where the goal is to find a min-cost subgraph such that every\npair of terminals are $2$-edge/vertex connected.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 12:56:15 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Laekhanukit", "Bundit", ""]]}, {"id": "1611.01647", "submitter": "Heng Guo", "authors": "Heng Guo, Mark Jerrum, Jingcheng Liu", "title": "Uniform Sampling through the Lov\\'asz Local Lemma", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithmic framework, called \"partial rejection sampling\",\nto draw samples exactly from a product distribution, conditioned on none of a\nnumber of bad events occurring. Our framework builds (perhaps surprising) new\nconnections between the variable framework of the Lov\\'asz Local Lemma and some\nclassical sampling algorithms such as the \"cycle-popping\" algorithm for rooted\nspanning trees. Among other applications, we discover new algorithms to sample\nsatisfying assignments of k-CNF formulas with bounded variable occurrences.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 13:18:47 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 19:38:14 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 07:30:08 GMT"}, {"version": "v4", "created": "Tue, 15 Jan 2019 10:13:47 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Guo", "Heng", ""], ["Jerrum", "Mark", ""], ["Liu", "Jingcheng", ""]]}, {"id": "1611.01655", "submitter": "Yuval Filmus", "authors": "Yuval Dagan, Yuval Filmus, Ariel Gabizon, Shay Moran", "title": "Twenty (simple) questions", "comments": "33 pages; to appear in STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.IT cs.LG math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic combinatorial interpretation of Shannon's entropy function is via the\n\"20 questions\" game. This cooperative game is played by two players, Alice and\nBob: Alice picks a distribution $\\pi$ over the numbers $\\{1,\\ldots,n\\}$, and\nannounces it to Bob. She then chooses a number $x$ according to $\\pi$, and Bob\nattempts to identify $x$ using as few Yes/No queries as possible, on average.\n  An optimal strategy for the \"20 questions\" game is given by a Huffman code\nfor $\\pi$: Bob's questions reveal the codeword for $x$ bit by bit. This\nstrategy finds $x$ using fewer than $H(\\pi)+1$ questions on average. However,\nthe questions asked by Bob could be arbitrary. In this paper, we investigate\nthe following question: Are there restricted sets of questions that match the\nperformance of Huffman codes, either exactly or approximately?\n  Our first main result shows that for every distribution $\\pi$, Bob has a\nstrategy that uses only questions of the form \"$x < c$?\" and \"$x = c$?\", and\nuncovers $x$ using at most $H(\\pi)+1$ questions on average, matching the\nperformance of Huffman codes in this sense. We also give a natural set of\n$O(rn^{1/r})$ questions that achieve a performance of at most $H(\\pi)+r$, and\nshow that $\\Omega(rn^{1/r})$ questions are required to achieve such a\nguarantee.\n  Our second main result gives a set $\\mathcal{Q}$ of $1.25^{n+o(n)}$ questions\nsuch that for every distribution $\\pi$, Bob can implement an optimal strategy\nfor $\\pi$ using only questions from $\\mathcal{Q}$. We also show that\n$1.25^{n-o(n)}$ questions are needed, for infinitely many $n$. If we allow a\nsmall slack of $r$ over the optimal strategy, then roughly $(rn)^{\\Theta(1/r)}$\nquestions are necessary and sufficient.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 13:55:25 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 14:00:21 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 10:44:06 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Dagan", "Yuval", ""], ["Filmus", "Yuval", ""], ["Gabizon", "Ariel", ""], ["Moran", "Shay", ""]]}, {"id": "1611.01688", "submitter": "Nika Haghtalab", "authors": "Miroslav Dud\\'ik, Nika Haghtalab, Haipeng Luo, Robert E. Schapire,\n  Vasilis Syrgkanis, Jennifer Wortman Vaughan", "title": "Oracle-Efficient Online Learning and Auction Design", "comments": "An earlier version of this paper appeared in FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of computationally efficient online learning\nalgorithms in an adversarial setting in which the learner has access to an\noffline optimization oracle. We present an algorithm called Generalized\nFollow-the-Perturbed-Leader and provide conditions under which it is\noracle-efficient while achieving vanishing regret. Our results make significant\nprogress on an open problem raised by Hazan and Koren, who showed that\noracle-efficient algorithms do not exist in general and asked whether one can\nidentify properties under which oracle-efficient online learning may be\npossible.\n  Our auction-design framework considers an auctioneer learning an optimal\nauction for a sequence of adversarially selected valuations with the goal of\nachieving revenue that is almost as good as the optimal auction in hindsight,\namong a class of auctions. We give oracle-efficient learning results for: (1)\nVCG auctions with bidder-specific reserves in single-parameter settings, (2)\nenvy-free item pricing in multi-item auctions, and (3) s-level auctions of\nMorgenstern and Roughgarden for single-item settings. The last result leads to\nan approximation of the overall optimal Myerson auction when bidders'\nvaluations are drawn according to a fast-mixing Markov process, extending prior\nwork that only gave such guarantees for the i.i.d. setting.\n  Finally, we derive various extensions, including: (1) oracle-efficient\nalgorithms for the contextual learning setting in which the learner has access\nto side information (such as bidder demographics), (2) learning with\napproximate oracles such as those based on Maximal-in-Range algorithms, and (3)\nno-regret bidding in simultaneous auctions, resolving an open problem of\nDaskalakis and Syrgkanis.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 18:54:59 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 08:21:30 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 14:10:08 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Dud\u00edk", "Miroslav", ""], ["Haghtalab", "Nika", ""], ["Luo", "Haipeng", ""], ["Schapire", "Robert E.", ""], ["Syrgkanis", "Vasilis", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1611.01706", "submitter": "Eleni Bakali", "authors": "Eleni Bakali", "title": "Self-reducible with easy decision version counting problems admit\n  additive error approximation. Connections to counting complexity, exponential\n  time complexity, and circuit lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the class of counting problems,i.e. functions in $\\#$P, which are\nself reducible, and have easy decision version, i.e. for every input it is easy\nto decide if the value of the function $f(x)$ is zero. For example,\n$\\#$independent-sets of all sizes, is such a problem, and one of the hardest of\nthis class, since it is equivalent to $\\#$SAT under multiplicative\napproximation preserving reductions.\n  Using these two powerful properties, self reducibility and easy decision, we\nprove that all problems/ functions $f$ in this class can be approximated in\nprobabilistic polynomial time within an absolute exponential error\n$\\epsilon\\cdot 2^{n'}, \\forall\\epsilon>0$, which for many of those problems\n(when $n'=n+$constant) implies additive approximation to the fraction\n$f(x)/2^n$. (Where $n'$ is the amount of non-determinism of some associated\nNPTM).\n  Moreover we show that for all these problems we can have multiplicative error\nto the value $f(x)$, of any desired accuracy (i.e. a RAS), in time of order\n$2^{2n'/3}poly(n)$, which is strictly smaller than exhaustive search. We also\nshow that $f(x)<g(x)$ can be decided deterministically in time $g(x)poly(n),\n\\forall g$.\n  Finally we show that the Circuit Acceptance Probability Problem, which is\nrelated to derandomization and circuit lower bounds, can be solved with high\nprobability and in polynomial time, for the family of all circuits for which\nthe problems of counting either satisfying or unsatisfying assignments belong\nto TotP (which is the Karp-closure of self reducible problems with easy\ndecision version).\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 22:55:45 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Bakali", "Eleni", ""]]}, {"id": "1611.01710", "submitter": "Stephen Gismondi", "authors": "E. R. Swart, S. J. Gismondi, N. R. Swart, C. E. Bell, A. Lee", "title": "Deciding Graph non-Hamiltonicity via a Closure Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a matching and LP based heuristic algorithm that decides graph\nnon-Hamiltonicity. Each of the $n!$ Hamilton cycles in a complete directed\ngraph on $n+1$ vertices corresponds with each of the $n!$ $n$-permutation\nmatrices $P$, such that $p_{u,i}=1$ if and only if the $i^{th}$ arc in a cycle\nenters vertex $u$, starting and ending at vertex $n+1$. A graph instance ($G$)\nis initially coded as exclusion set $E$, whose members are pairs of components\nof $P$, $\\{p_{u,i} ,p_{v,i+1}\\}, i=1,n-1$, for each arc $(u,v)$ not in $G$. For\neach $\\{p_{u,i} ,p_{v,i+1}\\}\\in E$, the set of $P$ satisfying\n$p_{u,i}=p_{v,i+1}=1$ correspond with a set of cycles not in $G$. Accounting\nfor all arcs not in $G$, $E$ codes precisely the set of cycles not in $G$. A\ndoubly stochastic-like $\\mathcal{O}$($n^4$) formulation of the Hamilton cycle\ndecision problem is then constructed. Each $\\{p_{u,i} ,p_{v,j}\\}$ is coded as\nvariable $q_{u,i,v,j}$ such that the set of integer extrema is the set of all\npermutations. We model $G$ by setting each $q_{u,i,v,j}=0$ in correspondence\nwith each $\\{p_{u,i} ,p_{v,j}\\}\\in E$ such that for non-Hamiltonian $G$,\ninteger solutions cannot exist. We recognize non-Hamiltonicity by iteratively\ndeducing additional $q_{u,i,v,j}$ that can be set zero and expanding $E$ until\nthe formulation becomes infeasible, in which case we recognize that no integer\nsolutions exists i.e. $G$ is decided non-Hamiltonian. Over 100 non-Hamiltonian\ngraphs (10 through 104 vertices) and 2000 randomized 31 vertex non-Hamiltonian\ngraphs are tested and correctly decided non-Hamiltonian. For Hamiltonian $G$,\nthe complement of $E$ provides information about covers of matchings, perhaps\nuseful in searching for cycles. We also present an example where the algorithm\nfails to deduce any integral value for any $q_{u,i,v,j}$ i.e. $G$ is undecided.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 23:38:23 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Swart", "E. R.", ""], ["Gismondi", "S. J.", ""], ["Swart", "N. R.", ""], ["Bell", "C. E.", ""], ["Lee", "A.", ""]]}, {"id": "1611.01769", "submitter": "Dmitry Kosolobov", "authors": "Dominik Kempa and Dmitry Kosolobov", "title": "LZ-End Parsing in Compressed Space", "comments": "12 pages, 4 figure", "journal-ref": null, "doi": "10.1109/DCC.2017.73", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that constructs the LZ-End parsing (a variation of\nLZ77) of a given string of length $n$ in $O(n\\log\\ell)$ expected time and $O(z\n+ \\ell)$ space, where $z$ is the number of phrases in the parsing and $\\ell$ is\nthe length of the longest phrase. As an option, we can fix $\\ell$ (e.g., to the\nsize of RAM) thus obtaining a reasonable LZ-End approximation with the same\nfunctionality and the length of phrases restricted by $\\ell$. This modified\nalgorithm constructs the parsing in streaming fashion in one left to right pass\non the input string w.h.p. and performs one right to left pass to verify the\ncorrectness of the result. Experimentally comparing this version to other\nLZ77-based analogs, we show that it is of practical interest.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 12:47:25 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 21:38:21 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kempa", "Dominik", ""], ["Kosolobov", "Dmitry", ""]]}, {"id": "1611.01778", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Neil Olver and L\\'aszl\\'o A. V\\'egh", "title": "A Simpler and Faster Strongly Polynomial Algorithm for Generalized Flow\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new strongly polynomial algorithm for generalized flow\nmaximization that is significantly simpler and faster than the previous\nstrongly polynomial algorithm [V\\'egh16]. For the uncapacitated problem\nformulation, the complexity bound $O(mn(m+n\\log n)\\log (n^2/m))$ improves on\nthe previous estimate by almost a factor $O(n^2)$. Even for small numerical\nparameter values, our running time bound is comparable to the best weakly\npolynomial algorithms. The key new technical idea is relaxing the primal\nfeasibility conditions. This allows us to work almost exclusively with integral\nflows, in contrast to all previous algorithms for the problem.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 13:41:18 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 20:04:03 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 14:12:23 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Olver", "Neil", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1611.01805", "submitter": "Shashwat Garg", "authors": "Nikhil Bansal, Shashwat Garg", "title": "Algorithmic Discrepancy Beyond Partial Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial coloring method is one of the most powerful and widely used\nmethod in combinatorial discrepancy problems. However, in many cases it leads\nto sub-optimal bounds as the partial coloring step must be iterated a\nlogarithmic number of times, and the errors can add up in an adversarial way.\nWe give a new and general algorithmic framework that overcomes the limitations\nof the partial coloring method and can be applied in a black-box manner to\nvarious problems. Using this framework, we give new improved bounds and\nalgorithms for several classic problems in discrepancy. In particular, for\nTusnady's problem, we give an improved $O(\\log^2 n)$ bound for discrepancy of\naxis-parallel rectangles and more generally an $O_d(\\log^dn)$ bound for\n$d$-dimensional boxes in $\\mathbb{R}^d$. Previously, even non-constructively,\nthe best bounds were $O(\\log^{2.5} n)$ and $O_d(\\log^{d+0.5}n)$ respectively.\nSimilarly, for the Steinitz problem we give the first algorithm that matches\nthe best known non-constructive bounds due to Banaszczyk [Banaszczyk 2012] in\nthe $\\ell_\\infty$ case, and improves the previous algorithmic bounds\nsubstantially in the $\\ell_2$ case. Our framework is based upon a substantial\ngeneralization of the techniques developed recently in the context of the\nKoml\\'os discrepancy problem [BDG16].\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 16:35:24 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 21:34:32 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Bansal", "Nikhil", ""], ["Garg", "Shashwat", ""]]}, {"id": "1611.01835", "submitter": "Travis Gagie", "authors": "Travis Gagie, Meng He and Gonzalo Navarro", "title": "Compressed Dynamic Range Majority and Minority Data Structures", "comments": "Partially supported by Fondecyt grant 1-171058, Chile; NSERC, Canada;\n  basal funds FB0001, Conicyt, Chile; and the Millenium Institute for\n  Foundational Research on Data, Chile. A preliminary partial version of this\n  article appeared in Proc. DCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the range $\\alpha$-majority query problem, we are given a sequence\n$S[1..n]$ and a fixed threshold $\\alpha \\in (0, 1)$, and are asked to\npreprocess $S$ such that, given a query range $[i..j]$, we can efficiently\nreport the symbols that occur more than $\\alpha (j-i+1)$ times in $S[i..j]$,\nwhich are called the range $\\alpha$-majorities. In this article we first\ndescribe a dynamic data structure that represents $S$ in compressed space ---\n$nH_k+ o(n\\lg \\sigma)$ bits for any $k = o(\\log_{\\sigma} n)$, where $\\sigma$ is\nthe alphabet size and $H_k \\le H_0 \\le \\lg\\sigma $ is the $k$-th order\nempirical entropy of $S$ --- and answers queries in $O \\left(\\frac{\\log\nn}{\\alpha \\log \\log n} \\right)$ time while supporting insertions and deletions\nin $S$ in $O \\left( \\frac{\\lg n}{\\alpha} \\right)$ amortized time. We then show\nhow to modify our data structure to receive some $\\beta \\ge \\alpha$ at query\ntime and report the range $\\beta$-majorities in $O \\left( \\frac{\\log n}{\\beta\n\\log \\log n} \\right)$ time, without increasing the asymptotic space or\nupdate-time bounds. The best previous dynamic solution has the same query and\nupdate times as ours, but it occupies $O(n)$ words and cannot take advantage of\nbeing given a larger threshold $\\beta$ at query time.\n  [ABSTRACT CLIPPED DUE TO LENGTH.]\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 20:07:59 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 16:39:11 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Gagie", "Travis", ""], ["He", "Meng", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1611.01853", "submitter": "Aviv Yehezkel", "authors": "Reuven Cohen, Liran Katzir and Aviv Yehezkel", "title": "MTS Sketch for Accurate Estimation of Set-Expression Cardinalities from\n  Small Samples", "comments": "arXiv admin note: text overlap with arXiv:1508.06216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based streaming algorithms allow efficient processing of big data.\nThese algorithms use small fixed-size storage to store a summary (\"sketch\") of\nthe input data, and use probabilistic algorithms to estimate the desired\nquantity. However, in many real-world applications it is impractical to collect\nand process the entire data stream, the common practice is thus to sample and\nprocess only a small part of it. While sampling is crucial for handling massive\ndata sets, it may reduce accuracy. In this paper we present a new framework\nthat can accurately estimate the cardinality of any set expression between any\nnumber of streams using only a small sample of each stream. The proposed\nframework consists of a new sketch, called Maximal-Term with Subsample (MTS),\nand a family of algorithms that use this sketch. An example of a possible query\nthat can be efficiently answered using the proposed sketch is, How many\ndistinct tuples appear in tables $T_1$ and $T_2$, but not in $T_3$? The\nalgorithms presented in this paper answer such queries accurately, processing\nonly a small sample of the tuples in each table and using a constant amount of\nmemory. Such estimations are useful for the optimization of queries over very\nlarge database systems. We show that all our algorithms are unbiased, and we\nanalyze their asymptotic variance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 22:22:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Cohen", "Reuven", ""], ["Katzir", "Liran", ""], ["Yehezkel", "Aviv", ""]]}, {"id": "1611.01856", "submitter": "Mayank Gupta", "authors": "Mayank Gupta and Bahman Kalantari", "title": "A Comparison of the Triangle Algorithm and SMO for Solving the Hard\n  Margin Problem", "comments": "arXiv admin note: text overlap with arXiv:1412.0356", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the problem of testing, for two finite sets of\npoints in the Euclidean space, if their convex hulls are disjoint and computing\nan optimal supporting hyperplane if so. This is a fundamental problem of\nclassification in machine learning known as the hard-margin SVM. The problem\ncan be formulated as a quadratic programming problem. The SMO algorithm is the\ncurrent state of art algorithm for solving it, but it does not answer the\nquestion of separability. An alternative to solving both problems is the\nTriangle Algorithm, a geometrically inspired algorithm, initially described for\nthe convex hull membership problem, a fundamental problem in linear\nprogramming. First, we describe the experimental performance of the Triangle\nAlgorithm for testing the intersection of two convex hulls. Next, we compare\nthe performance of Triangle Algorithm with SMO for finding the optimal\nsupporting hyperplane. Based on experimental results ranging up to 5000 points\nin each set in dimensions up to 10000, the Triangle Algorithm outperforms SMO.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 22:24:10 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 12:19:07 GMT"}, {"version": "v3", "created": "Mon, 14 Nov 2016 03:05:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Gupta", "Mayank", ""], ["Kalantari", "Bahman", ""]]}, {"id": "1611.01879", "submitter": "Grigory Yaroslavtsev", "authors": "Sampath Kannan, Elchanan Mossel, Grigory Yaroslavtsev", "title": "Linear Sketching over $\\mathbb F_2$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a systematic study of linear sketching over $\\mathbb F_2$. For a\ngiven Boolean function $f \\colon \\{0,1\\}^n \\to \\{0,1\\}$ a randomized $\\mathbb\nF_2$-sketch is a distribution $\\mathcal M$ over $d \\times n$ matrices with\nelements over $\\mathbb F_2$ such that $\\mathcal Mx$ suffices for computing\n$f(x)$ with high probability. We study a connection between $\\mathbb\nF_2$-sketching and a two-player one-way communication game for the\ncorresponding XOR-function. Our results show that this communication game\ncharacterizes $\\mathbb F_2$-sketching under the uniform distribution (up to\ndependence on error). Implications of this result include: 1) a composition\ntheorem for $\\mathbb F_2$-sketching complexity of a recursive majority\nfunction, 2) a tight relationship between $\\mathbb F_2$-sketching complexity\nand Fourier sparsity, 3) lower bounds for a certain subclass of symmetric\nfunctions. We also fully resolve a conjecture of Montanaro and Osborne\nregarding one-way communication complexity of linear threshold functions by\ndesigning an $\\mathbb F_2$-sketch of optimal size.\n  Furthermore, we show that (non-uniform) streaming algorithms that have to\nprocess random updates over $\\mathbb F_2$ can be constructed as $\\mathbb\nF_2$-sketches for the uniform distribution with only a minor loss. In contrast\nwith the previous work of Li, Nguyen and Woodruff (STOC'14) who show an\nanalogous result for linear sketches over integers in the adversarial setting\nour result doesn't require the stream length to be triply exponential in $n$\nand holds for streams of length $\\tilde O(n)$ constructed through uniformly\nrandom updates. Finally, we state a conjecture that asks whether optimal\none-way communication protocols for XOR-functions can be constructed as\n$\\mathbb F_2$-sketches with only a small loss.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 02:47:12 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 15:17:36 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Kannan", "Sampath", ""], ["Mossel", "Elchanan", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1611.01934", "submitter": "Lars Rohwedder", "authors": "Klaus Jansen and Lars Rohwedder", "title": "On the Configuration-LP of the Restricted Assignment Problem", "comments": "Fixed minor errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of Scheduling on Unrelated Machines. In\nthis problem a set of jobs is to be distributed among a set of machines and the\nmaximum load (makespan) is to be minimized. The processing time $p_{ij}$ of a\njob $j$ depends on the machine $i$ it is assigned to. Lenstra, Shmoys and\nTardos gave a polynomial time $2$-approximation for this problem. In this paper\nwe focus on a prominent special case, the Restricted Assignment problem, in\nwhich $p_{ij}\\in\\{p_j,\\infty\\}$. The configuration-LP is a linear programming\nrelaxation for the Restricted Assignment problem. It was shown by Svensson that\nthe multiplicative gap between integral and fractional solution, the\nintegrality gap, is at most $2 - 1/17 \\approx 1.9412$. In this paper we\nsignificantly simplify his proof and achieve a bound of $2 - 1/6 \\approx\n1.8333$. As a direct consequence this provides a polynomial $(2 - 1/6 +\n\\epsilon)$-estimation algorithm for the Restricted Assignment problem by\napproximating the configuration-LP. The best lower bound known for the\nintegrality gap is $1.5$ and no estimation algorithm with a guarantee better\nthan $1.5$ exists unless $\\mathrm{P} = \\mathrm{NP}$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 08:35:47 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 10:41:57 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Jansen", "Klaus", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1611.02308", "submitter": "Blagoj Delipetrev", "authors": "Blagoj Delipetrev", "title": "Nested algorithms for optimal reservoir operation and their embedding in\n  a decision support platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This is a PhD thesis of Blagoj Delipetrev explaining nested dynamic\nprogramming, nested stochastic dynamic programming and nested reinforcement\nlearning algorithms that are applied in reservoir optimization problem.\nAdditionally there are also multi-objective version of these algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 13:44:33 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Delipetrev", "Blagoj", ""]]}, {"id": "1611.02506", "submitter": "Stefano Quer", "authors": "Nicolas Boria and Gianpiero Cabodi and Paolo Camurati and Marco Palena\n  and Paolo Pasini and Stefano Quer", "title": "A Greedy Approach to Answer Reachability Queries on DAGs", "comments": "24 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several modern applications involve huge graphs and require fast answers to\nreachability queries. In more than two decades since first proposals, several\napproaches have been presented adopting on-line searches, hop labelling or\ntransitive closure compression. Transitive closure compression techniques\nusually construct a graph reachability index, for example by decomposing the\ngraph into disjoint chains. As memory consumption is proportional to the number\nof chains, the target of those algorithms is to decompose the graph into an\noptimal number \\width\\ of chains. However, commonly used techniques fail to\nmeet general expectations, are exceedingly complex, and their application on\nlarge graphs is impractical. The main contribution of this paper is a novel\napproach to construct such reachability indexes. The proposed method decomposes\nthe graph into a sub-optimal number $\\widehat{c}$ of chains by following a\ngreedy strategy. We show that, given a vertex topological order, such a\ndecomposition is obtained in $\\mathcal{O}(\\widehat{c} m)$ time, and requires\n$\\mathcal{O}(\\widehat{c} n)$ space, with $\\widehat{c}$ bounded by $[c\n\\log(\\frac{n}{c})]$. We provide experimental evidence suggesting that, on\ndifferent categories of automatically generated benchmarks as well as on graphs\narising from the field of logic synthesis and formal verification, the proposed\nmethod produces a number of chains very close to the optimum, while\nsignificantly reducing computation time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 13:07:18 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Boria", "Nicolas", ""], ["Cabodi", "Gianpiero", ""], ["Camurati", "Paolo", ""], ["Palena", "Marco", ""], ["Pasini", "Paolo", ""], ["Quer", "Stefano", ""]]}, {"id": "1611.02538", "submitter": "Jeethu Devasia", "authors": "Jeethu V. Devasia and Priya Chandran", "title": "Inferring disease causing genes and their pathways: A mathematical\n  perspective", "comments": "This article had submitted in the journals Bioinformatics and\n  Computer Methods and Programs in Biomedicine. But, it was not accepted for\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system level view of cellular processes for human and several organisms can\nbe cap- tured by analyzing molecular interaction networks. A molecular\ninteraction network formed of differentially expressed genes and their\ninteractions helps to understand key players behind disease development. So, if\nthe functions of these genes are blocked by altering their interactions, it\nwould have a great impact in controlling the disease. Due to this promising\nconsequence, the problem of inferring disease causing genes and their pathways\nhas attained a crucial position in computational biology research. However,\nconsidering the huge size of interaction networks, executing computations can\nbe costly. Review of literatures shows that the methods proposed for finding\nthe set of disease causing genes could be assessed in terms of their accuracy\nwhich a perfect algorithm would find. Along with accuracy, the time complexity\nof the method is also important, as high time complexities would limit the\nnumber of pathways that could be found within a pragmatic time interval.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 10:16:22 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Devasia", "Jeethu V.", ""], ["Chandran", "Priya", ""]]}, {"id": "1611.02589", "submitter": "Amos Korman", "authors": "Pierre Fraigniaud (IRIF), Amos Korman (IRIF)", "title": "An Optimal Ancestry Labeling Scheme with Applications to XML Trees and\n  Universal Posets", "comments": null, "journal-ref": "Journal of the ACM, ACM, 2016, 63, pp.1 - 31", "doi": "10.1145/2794076", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we solve the ancestry-labeling scheme problem which aims at\nassigning the shortest possible labels (bit strings) to nodes of rooted trees,\nso that ancestry queries between any two nodes can be answered by inspecting\ntheir assigned labels only. This problem was introduced more than twenty years\nago by Kannan et al. [STOC '88], and is among the most well-studied problems in\nthe field of informative labeling schemes. We construct an ancestry-labeling\nscheme for $n$-node trees with label size $\\log_2 n + O(\\log \\log n)$ bits,\nthus matching the $\\log_2 n + O(\\log \\log n)$ bits lower bound given by Alstrup\net al. [SODA '03]. Our scheme is based on a simplified ancestry scheme that\noperates extremely well on a restricted set of trees. In particular, for the\nset of n-node trees with depth at most d, the simplified ancestry scheme enjoys\nlabel size of $\\log_2 n + 2 \\log_2 d + O(1)$ bits. Since the depth of most XML\ntrees is at most some small constant, such an ancestry scheme may be of\npractical use. In addition, we also obtain an adjacency-labeling scheme that\nlabels n-node trees of depth d with labels of size $\\log_2 n + 3 \\log_2 d +\nO(1)$ bits. All our schemes assign the labels in linear time, and guarantee\nthat any query can be answered in constant time. Finally, our ancestry scheme\nfinds applications to the construction of small universal partially ordered\nsets (posets). Specifically, for any fixed integer k, it enables the\nconstruction of a universal poset of size $\\tilde O(n^k)$ for the family of\n$n$-element posets with tree-dimension at most $k$. Up to lower order terms,\nthis bound is tight thanks to a lower bound of $n^{k-o(1)}$ due to Alon and\nScheinerman [Order '88].\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 16:19:19 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Fraigniaud", "Pierre", "", "IRIF"], ["Korman", "Amos", "", "IRIF"]]}, {"id": "1611.02635", "submitter": "Ashia Wilson", "authors": "Ashia C. Wilson, Benjamin Recht, Michael I. Jordan", "title": "A Lyapunov Analysis of Momentum Methods in Optimization", "comments": "Major revision. Cleaned up presentation and added results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Momentum methods play a significant role in optimization. Examples include\nNesterov's accelerated gradient method and the conditional gradient algorithm.\nSeveral momentum methods are provably optimal under standard oracle models, and\nall use a technique called estimate sequences to analyze their convergence\nproperties. The technique of estimate sequences has long been considered\ndifficult to understand, leading many researchers to generate alternative,\n\"more intuitive\" methods and analyses. We show there is an equivalence between\nthe technique of estimate sequences and a family of Lyapunov functions in both\ncontinuous and discrete time. This connection allows us to develop a simple and\nunified analysis of many existing momentum algorithms, introduce several new\nalgorithms, and strengthen the connection between algorithms and\ncontinuous-time dynamical systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 17:55:21 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 21:11:26 GMT"}, {"version": "v3", "created": "Sat, 31 Dec 2016 00:39:23 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2018 04:20:12 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Wilson", "Ashia C.", ""], ["Recht", "Benjamin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1611.02663", "submitter": "Yannic Maus", "authors": "Mohsen Ghaffari, Fabian Kuhn, Yannic Maus", "title": "On the Complexity of Local Distributed Graph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is centered on the complexity of graph problems in the\nwell-studied LOCAL model of distributed computing, introduced by Linial [FOCS\n'87]. It is widely known that for many of the classic distributed graph\nproblems (including maximal independent set (MIS) and $(\\Delta+1)$-vertex\ncoloring), the randomized complexity is at most polylogarithmic in the size $n$\nof the network, while the best deterministic complexity is typically\n$2^{O(\\sqrt{\\log n})}$. Understanding and narrowing down this exponential gap\nis considered to be one of the central long-standing open questions in the area\nof distributed graph algorithms. We investigate the problem by introducing a\ncomplexity-theoretic framework that allows us to shed some light on the role of\nrandomness in the LOCAL model. We define the SLOCAL model as a sequential\nversion of the LOCAL model. Our framework allows us to prove completeness\nresults with respect to the class of problems which can be solved efficiently\nin the SLOCAL model, implying that if any of the complete problems can be\nsolved deterministically in $\\log^{O(1)} n$ rounds in the LOCAL model, we can\ndeterministically solve all efficient SLOCAL-problems (including MIS and\n$(\\Delta+1)$-coloring) in $\\log^{O(1)} n$ rounds in the LOCAL model. We show\nthat a rather rudimentary looking graph coloring problem is complete in the\nabove sense: Color the nodes of a graph with colors red and blue such that each\nnode of sufficiently large polylogarithmic degree has at least one neighbor of\neach color. The problem admits a trivial zero-round randomized solution. The\nresult can be viewed as showing that the only obstacle to getting efficient\ndeterminstic algorithms in the LOCAL model is an efficient algorithm to\napproximately round fractional values into integer values.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 19:28:01 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 14:31:51 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1611.02864", "submitter": "Christian Wulff-Nilsen", "authors": "Christian Wulff-Nilsen", "title": "Fully-Dynamic Minimum Spanning Forest with Improved Worst-Case Update\n  Time", "comments": "Small changes to Section 1.1 and a minor fix of the analysis for\n  maintaining an MSF of small clusters. 61 pages, 7 figures, 3 with pseudocode.\n  Submitted to STOC'17. Builds on an earlier (unpublished, submitted to\n  FOCS'16) version by the same author which had a similar bound for\n  fully-dynamic connectivity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a Las Vegas data structure which maintains a minimum spanning forest\nin an n-vertex edge-weighted dynamic graph undergoing updates consisting of any\nmixture of edge insertions and deletions. Each update is supported in O(n^{1/2\n- c}) expected worst-case time for some constant c > 0 and this worst-case\nbound holds with probability at least 1 - n^{-d} where d is a constant that can\nbe made arbitrarily large. This is the first data structure achieving an\nimprovement over the O(n^{1/2}) deterministic worst-case update time of\nEppstein et al., a bound that has been standing for nearly 25 years. In fact,\nit was previously not even known how to maintain a spanning forest of an\nunweighted graph in worst-case time polynomially faster than Theta(n^{1/2}).\nOur result is achieved by first giving a reduction from fully-dynamic to\ndecremental minimum spanning forest preserving worst-case update time up to\nlogarithmic factors. Then decremental minimum spanning forest is solved using\nseveral novel techniques, one of which involves keeping track of\nlow-conductance cuts in a dynamic graph. An immediate corollary of our result\nis the first Las Vegas data structure for fully-dynamic connectivity where each\nupdate is handled in worst-case time polynomially faster than Theta(n^{1/2})\nw.h.p.; this data structure has O(1) worst-case query time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 09:34:37 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 14:05:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Wulff-Nilsen", "Christian", ""]]}, {"id": "1611.02891", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Tight Lower Bounds for the Longest Common Extension Problem", "comments": "5 pages, accepted to Information Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common extension problem is to preprocess a given string of\nlength $n$ into a data structure that uses $S(n)$ bits on top of the input and\nanswers in $T(n)$ time the queries $\\mathit{LCE}(i,j)$ computing the length of\nthe longest string that occurs at both positions $i$ and $j$ in the input. We\nprove that the trade-off $S(n)T(n) = \\Omega(n\\log n)$ holds in the non-uniform\ncell-probe model provided that the input string is read-only, each letter\noccupies a separate memory cell, $S(n) = \\Omega(n)$, and the size of the input\nalphabet is at least $2^{8\\lceil S(n) / n\\rceil}$. It is known that this\ntrade-off is tight.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 11:07:31 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 17:59:44 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 14:39:51 GMT"}, {"version": "v4", "created": "Mon, 3 Apr 2017 18:51:50 GMT"}, {"version": "v5", "created": "Thu, 11 May 2017 13:34:29 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1611.02960", "submitter": "Jayadev Acharya", "authors": "Jayadev Acharya, Hirakendu Das, Alon Orlitsky, Ananda Theertha Suresh", "title": "A Unified Maximum Likelihood Approach for Optimal Distribution Property\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of data science has spurred interest in estimating properties of\ndistributions over large alphabets. Fundamental symmetric properties such as\nsupport size, support coverage, entropy, and proximity to uniformity, received\nmost attention, with each property estimated using a different technique and\noften intricate analysis tools.\n  We prove that for all these properties, a single, simple, plug-in\nestimator---profile maximum likelihood (PML)---performs as well as the best\nspecialized techniques. This raises the possibility that PML may optimally\nestimate many other symmetric properties.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 14:59:23 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 16:36:44 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Acharya", "Jayadev", ""], ["Das", "Hirakendu", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1611.02966", "submitter": "Arnaud de Mesmay", "authors": "Vincent Cohen-Addad, \\'Eric Colin de Verdi\\`ere, Arnaud de Mesmay", "title": "A Near-Linear Approximation Scheme for Multicuts of Embedded Graphs with\n  a Fixed Number of Terminals", "comments": "Final version, to appear in SICOMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an undirected edge-weighted graph $G$ and a set $R$ of pairs of vertices\ncalled pairs of terminals, a multicut is a set of edges such that removing\nthese edges from $G$ disconnects each pair in $R$. We provide an algorithm\ncomputing a $(1+\\varepsilon)$-approximation of the minimum multicut of a graph\n$G$ in time $(g+t)^{(O(g+t)^3)}\\cdot(1/\\varepsilon)^{O(g+t)} \\cdot n \\log n$,\nwhere $g$ is the genus of $G$ and $t$ is the number of terminals.\n  This result is tight in several aspects, as the minimum multicut problem is\nboth APX-hard and W[1]-hard (parameterized by the number of terminals), even on\nplanar graphs (equivalently, when $g=0$).\n  In order to achieve this, our article leverages on a novel characterization\nof a minimum multicut as a family of Steiner trees in the universal cover of a\nsurface on which $G$ is embedded. The algorithm heavily relies on topological\ntechniques, and in particular on the use of homotopical tools and computations\nin covering spaces, which we blend with classic ideas stemming from\napproximation schemes for planar graphs and low-dimensional geometric inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 15:05:21 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 13:21:36 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 12:18:01 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 09:45:12 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["de Verdi\u00e8re", "\u00c9ric Colin", ""], ["de Mesmay", "Arnaud", ""]]}, {"id": "1611.03159", "submitter": "Kifayat Khan", "authors": "Kifayat Ullah Khan, Waqas Nawaz, Young-Koo Lee", "title": "Scalable Compression of a Weighted Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph is a useful data structure to model various real life aspects like\nemail communications, co-authorship among researchers, interactions among\nchemical compounds, and so on. Supporting such real life interactions produce a\nknowledge rich massive repository of data. However, efficiently understanding\nunderlying trends and patterns is hard due to large size of the graph.\nTherefore, this paper presents a scalable compression solution to compute\nsummary of a weighted graph. All the aforementioned interactions from various\ndomains are represented as edge weights in a graph. Therefore, creating a\nsummary graph while considering this vital aspect is necessary to learn\ninsights of different communication patterns. By experimenting the proposed\nmethod on two real world and publically available datasets against a state of\nthe art technique, we obtain order of magnitude performance gain and better\nsummarization accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 01:52:49 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Khan", "Kifayat Ullah", ""], ["Nawaz", "Waqas", ""], ["Lee", "Young-Koo", ""]]}, {"id": "1611.03220", "submitter": "Haim Avron", "authors": "Haim Avron and Kenneth L. Clarkson and David P. Woodruff", "title": "Faster Kernel Ridge Regression Using Sketching and Preconditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Ridge Regression (KRR) is a simple yet powerful technique for\nnon-parametric regression whose computation amounts to solving a linear system.\nThis system is usually dense and highly ill-conditioned. In addition, the\ndimensions of the matrix are the same as the number of data points, so direct\nmethods are unrealistic for large-scale datasets. In this paper, we propose a\npreconditioning technique for accelerating the solution of the aforementioned\nlinear system. The preconditioner is based on random feature maps, such as\nrandom Fourier features, which have recently emerged as a powerful technique\nfor speeding up and scaling the training of kernel-based methods, such as\nkernel ridge regression, by resorting to approximations. However, random\nfeature maps only provide crude approximations to the kernel function, so\ndelivering state-of-the-art results by directly solving the approximated system\nrequires the number of random features to be very large. We show that random\nfeature maps can be much more effective in forming preconditioners, since under\ncertain conditions a not-too-large number of random features is sufficient to\nyield an effective preconditioner. We empirically evaluate our method and show\nit is highly effective for datasets of up to one million training examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 08:50:05 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 21:57:20 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 06:10:52 GMT"}, {"version": "v4", "created": "Sat, 15 Jul 2017 06:31:03 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Avron", "Haim", ""], ["Clarkson", "Kenneth L.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1611.03225", "submitter": "Haim Avron", "authors": "Haim Avron and Kenneth L. Clarkson and David P. Woodruff", "title": "Sharper Bounds for Regularized Data Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study matrix sketching methods for regularized variants of linear\nregression, low rank approximation, and canonical correlation analysis. Our\nmain focus is on sketching techniques which preserve the objective function\nvalue for regularized problems, which is an area that has remained largely\nunexplored. We study regularization both in a fairly broad setting, and in the\nspecific context of the popular and widely used technique of ridge\nregularization; for the latter, as applied to each of these problems, we show\nalgorithmic resource bounds in which the {\\em statistical dimension} appears in\nplaces where in previous bounds the rank would appear. The statistical\ndimension is always smaller than the rank, and decreases as the amount of\nregularization increases. In particular, for the ridge low-rank approximation\nproblem $\\min_{Y,X} \\lVert YX - A \\rVert_F^2 + \\lambda \\lVert Y\\rVert_F^2 +\n\\lambda\\lVert X \\rVert_F^2$, where $Y\\in\\mathbb{R}^{n\\times k}$ and\n$X\\in\\mathbb{R}^{k\\times d}$, we give an approximation algorithm needing \\[\nO(\\mathtt{nnz}(A)) + \\tilde{O}((n+d)\\varepsilon^{-1}k \\min\\{k,\n\\varepsilon^{-1}\\mathtt{sd}_\\lambda(Y^*)\\})+\n\\mathtt{poly}(\\mathtt{sd}_\\lambda(Y^*) \\varepsilon^{-1}) \\] time, where\n$s_{\\lambda}(Y^*)\\le k$ is the statistical dimension of $Y^*$, $Y^*$ is an\noptimal $Y$, $\\varepsilon$ is an error parameter, and $\\mathtt{nnz}(A)$ is the\nnumber of nonzero entries of $A$.This is faster than prior work, even when\n$\\lambda=0$.\n  We also study regularization in a much more general setting. For example, we\nobtain sketching-based algorithms for the low-rank approximation problem\n$\\min_{X,Y} \\lVert YX - A \\rVert_F^2 + f(Y,X)$ where $f(\\cdot,\\cdot)$ is a\nregularizing function satisfying some very general conditions (chiefly,\ninvariance under orthogonal transformations).\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 09:05:43 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 12:55:39 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Avron", "Haim", ""], ["Clarkson", "Kenneth L.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1611.03253", "submitter": "Moran Feldman", "authors": "Niv Buchbinder and Moran Feldman", "title": "Constrained Submodular Maximization via a Non-symmetric Technique", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of combinatorial optimization problems with a submodular objective\nhas attracted much attention in recent years. Such problems are important in\nboth theory and practice because their objective functions are very general.\nObtaining further improvements for many submodular maximization problems boils\ndown to finding better algorithms for optimizing a relaxation of them known as\nthe multilinear extension.\n  In this work we present an algorithm for optimizing the multilinear\nrelaxation whose guarantee improves over the guarantee of the best previous\nalgorithm (which was given by Ene and Nguyen (2016)). Moreover, our algorithm\nis based on a new technique which is, arguably, simpler and more natural for\nthe problem at hand. In a nutshell, previous algorithms for this problem rely\non symmetry properties which are natural only in the absence of a constraint.\nOur technique avoids the need to resort to such properties, and thus, seems to\nbe a better fit for constrained problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 10:54:20 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""]]}, {"id": "1611.03310", "submitter": "Mario Ziller", "authors": "Mario Ziller and John F. Morack", "title": "Algorithmic concepts for the computation of Jacobsthal's function", "comments": "27 pages, 2 figures, 1 table, v2: revised description, results\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jacobsthal function has aroused interest in various contexts in the past\ndecades. We review several algorithmic ideas for the computation of\nJacobsthal's function for primorial numbers and discuss their practicability\nregarding computational effort. The respective function values were computed\nfor primes up to 251. In addition to the results including previously unknown\ndata, we provide exhaustive lists of all sequences of the appropriate maximum\nlengths in ancillary files.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 07:59:05 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:28:58 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Ziller", "Mario", ""], ["Morack", "John F.", ""]]}, {"id": "1611.03358", "submitter": "Manuel Mazzara", "authors": "Ilya Dmitrenok, Viktor Drobnyy, Leonard Johard and Manuel Mazzara", "title": "Evaluation of spatial trees for simulation of biological tissue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial organization is a core challenge for all large agent-based models\nwith local interactions. In biological tissue models, spatial search and\nreinsertion are frequently reported as the most expensive steps of the\nsimulation. One of the main methods utilized in order to maintain both\nfavorable algorithmic complexity and accuracy is spatial hierarchies. In this\npaper, we seek to clarify to which extent the choice of spatial tree affects\nperformance, and also to identify which spatial tree families are optimal for\nsuch scenarios. We make use of a prototype of the new BioDynaMo tissue\nsimulator for evaluating performances as well as for the implementation of the\ncharacteristics of several different trees.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 18:15:28 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Dmitrenok", "Ilya", ""], ["Drobnyy", "Viktor", ""], ["Johard", "Leonard", ""], ["Mazzara", "Manuel", ""]]}, {"id": "1611.03385", "submitter": "Matthew Fahrbach", "authors": "Prateek Bhakta, Ben Cousins, Matthew Fahrbach, Dana Randall", "title": "Approximately Sampling Elements with Fixed Rank in Graded Posets", "comments": "23 pages, 12 figures", "journal-ref": "Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on\n  Discrete Algorithms (SODA 2017) 1823-1838", "doi": "10.1137/1.9781611974782.119", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graded posets frequently arise throughout combinatorics, where it is natural\nto try to count the number of elements of a fixed rank. These counting problems\nare often $\\#\\textbf{P}$-complete, so we consider approximation algorithms for\ncounting and uniform sampling. We show that for certain classes of posets,\nbiased Markov chains that walk along edges of their Hasse diagrams allow us to\napproximately generate samples with any fixed rank in expected polynomial time.\nOur arguments do not rely on the typical proofs of log-concavity, which are\nused to construct a stationary distribution with a specific mode in order to\ngive a lower bound on the probability of outputting an element of the desired\nrank. Instead, we infer this directly from bounds on the mixing time of the\nchains through a method we call $\\textit{balanced bias}$.\n  A noteworthy application of our method is sampling restricted classes of\ninteger partitions of $n$. We give the first provably efficient Markov chain\nalgorithm to uniformly sample integer partitions of $n$ from general restricted\nclasses. Several observations allow us to improve the efficiency of this chain\nto require $O(n^{1/2}\\log(n))$ space, and for unrestricted integer partitions,\nexpected $O(n^{9/4})$ time. Related applications include sampling permutations\nwith a fixed number of inversions and lozenge tilings on the triangular lattice\nwith a fixed average height.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 16:27:09 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Bhakta", "Prateek", ""], ["Cousins", "Ben", ""], ["Fahrbach", "Matthew", ""], ["Randall", "Dana", ""]]}, {"id": "1611.03473", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures", "comments": "Changes from v1: Revised presentation. Added more applications of the\n  technique (SQ lower bounds for robust sparse mean estimation and robust\n  covariance estimation in spectral norm). Sharpened testing lower bound to\n  linear in the dimension (compared to nearly-linear in first version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 20:32:48 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:48:34 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1611.03579", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Themis Gouleakis, John Peebles, Eric Price", "title": "Collision-based Testers are Optimal for Uniformity and Closeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problems of (i) uniformity testing of a discrete\ndistribution, and (ii) closeness testing between two discrete distributions\nwith bounded $\\ell_2$-norm. These problems have been extensively studied in\ndistribution testing and sample-optimal estimators are known for\nthem~\\cite{Paninski:08, CDVV14, VV14, DKN:15}.\n  In this work, we show that the original collision-based testers proposed for\nthese problems ~\\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant\nfactors. Previous analyses showed sample complexity upper bounds for these\ntesters that are optimal as a function of the domain size $n$, but suboptimal\nby polynomial factors in the error parameter $\\epsilon$. Our main contribution\nis a new tight analysis establishing that these collision-based testers are\ninformation-theoretically optimal, up to constant factors, both in the\ndependence on $n$ and in the dependence on $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 03:59:24 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Peebles", "John", ""], ["Price", "Eric", ""]]}, {"id": "1611.03624", "submitter": "Charles Carlson", "authors": "Charles Carlson, Karthekeyan Chandrasekaran, Hsien-Chih Chang,\n  Alexandra Kolla", "title": "Invertibility and Largest Eigenvalue of Symmetric Matrix Signings", "comments": "24 pages; title changed, abstract updated, paper reorganized,\n  connections and motivations section revised, new results added in several\n  sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectra of signed matrices have played a fundamental role in social\nsciences, graph theory, and control theory. In this work, we investigate the\ncomputational problems of identifying symmetric signings of matrices with\nnatural spectral properties. Our results are twofold:\n  1. We show NP-completeness for the following three problems: verifying\nwhether a given matrix has a symmetric signing that is positive\nsemi-definite/singular/has bounded eigenvalues. However, we also illustrate\nthat the complexity could substantially differ for input matrices that are\nadjacency matrices of graphs.\n  2. We exhibit a stark contrast between invertibility and the above-mentioned\nspectral properties: we show a combinatorial characterization of matrices with\ninvertible symmetric signings and design an efficient algorithm using this\ncharacterization to verify whether a given matrix has an invertible symmetric\nsigning. Next, we give an efficient algorithm to solve the search problem of\nfinding an invertible symmetric signing for matrices whose support graph is\nbipartite. We also provide a lower bound on the number of invertible symmetric\nsigned adjacency matrices. Finally, we give an efficient algorithm to find a\nminimum increase in support of a given symmetric matrix so that it has an\ninvertible symmetric signing.\n  We use combinatorial and spectral techniques in addition to classic results\nfrom matching theory. Our combinatorial characterization of matrices with\ninvertible symmetric signings might be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 09:06:56 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 15:29:15 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Carlson", "Charles", ""], ["Chandrasekaran", "Karthekeyan", ""], ["Chang", "Hsien-Chih", ""], ["Kolla", "Alexandra", ""]]}, {"id": "1611.03671", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski and Vadim V. Lozin and Dani\\\"el Paulusma", "title": "Well-Quasi-Ordering versus Clique-Width: New Results on Bigenic Classes", "comments": "26 pages, 3 figures. An extended abstract of this paper appeared in\n  the proceedings of IWOCA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daligault, Rao and Thomass\\'e asked whether a hereditary class of graphs\nwell-quasi-ordered by the induced subgraph relation has bounded clique-width.\nLozin, Razgon and Zamaraev recently showed that this is not true for classes\ndefined by infinitely many forbidden induced subgraphs. However, in the case of\nfinitely many forbidden induced subgraphs the question remains open and we\nconjecture that in this case the answer is positive. The conjecture is known to\nhold for classes of graphs defined by a single forbidden induced subgraph $H$,\nas such graphs are well-quasi-ordered and are of bounded clique-width if and\nonly if $H$ is an induced subgraph of $P_4$. For bigenic classes of graphs,\ni.e. ones defined by two forbidden induced subgraphs, there are several open\ncases in both classifications. In the present paper we obtain a number of new\nresults on well-quasi-orderability of bigenic classes, each of which supports\nthe conjecture.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 12:03:23 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Lozin", "Vadim V.", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1611.03745", "submitter": "Thatchaphol Saranurak", "authors": "Danupon Nanongkai and Thatchaphol Saranurak", "title": "Dynamic Spanning Forest with Worst-Case Update Time: Adaptive, Las\n  Vegas, and $O(n^{1/2-\\epsilon})$-Time", "comments": "Submitted to STOC'17. Announced partially at China Theory Week 2016\n  (http://www.itcsc.cuhk.edu.hk/Workshops/CTW16_Workshop/chinatheoryweek.html).\n  An independent result on the dynamic MST problem can be found at\n  arXiv:1611.02864", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two algorithms for dynamically maintaining a spanning forest of a\ngraph undergoing edge insertions and deletions. Our algorithms guarantee {\\em\nworst-case update time} and work against an adaptive adversary, meaning that an\nedge update can depend on previous outputs of the algorithms. We provide the\nfirst polynomial improvement over the long-standing $O(\\sqrt{n})$ bound of\n[Frederickson STOC'83, Eppstein, Galil, Italiano and Nissenzweig FOCS'92] for\nsuch type of algorithms. The previously best improvement was $O(\\sqrt{n\n(\\log\\log n)^2/\\log n})$ [Kejlberg-Rasmussen, Kopelowitz, Pettie and Thorup\nESA'16]. We note however that these bounds were obtained by deterministic\nalgorithms while our algorithms are randomized.\n  Our first algorithm is Monte Carlo and guarantees an $O(n^{0.4+o(1)})$\nworst-case update time, where the $o(1)$ term hides the $O(\\sqrt{\\log\\log\nn/\\log n})$ factor. Our second algorithm is Las Vegas and guarantees an\n$O(n^{0.49306})$ worst-case update time with high probability. Algorithms with\nbetter update time either needed to assume that the adversary is oblivious\n(e.g. [Kapron, King and Mountjoy SODA'13]) or can only guarantee an amortized\nupdate time. Our second result answers an open problem by Kapron et al. To the\nbest of our knowledge, our algorithms are among a few non-trivial randomized\ndynamic algorithms that work against adaptive adversaries.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:33:46 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 19:27:37 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1611.03780", "submitter": "David Rolnick", "authors": "David Rolnick, Kevin Aydin, Jean Pouget-Abadie, Shahab Kamali, Vahab\n  Mirrokni, Amir Najmi", "title": "Randomized Experimental Design via Geographic Clustering", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based services often run randomized experiments to improve their\nproducts. A popular way to run these experiments is to use geographical regions\nas units of experimentation, since this does not require tracking of individual\nusers or browser cookies. Since users may issue queries from multiple\ngeographical locations, geo-regions cannot be considered independent and\ninterference may be present in the experiment. In this paper, we study this\nproblem, and first present GeoCUTS, a novel algorithm that forms geographical\nclusters to minimize interference while preserving balance in cluster size. We\nuse a random sample of anonymized traffic from Google Search to form a graph\nrepresenting user movements, then construct a geographically coherent\nclustering of the graph. Our main technical contribution is a statistical\nframework to measure the effectiveness of clusterings. Furthermore, we perform\nempirical evaluations showing that the performance of GeoCUTS is comparable to\nhand-crafted geo-regions with respect to both novel and existing metrics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 16:55:20 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 22:21:51 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Rolnick", "David", ""], ["Aydin", "Kevin", ""], ["Pouget-Abadie", "Jean", ""], ["Kamali", "Shahab", ""], ["Mirrokni", "Vahab", ""], ["Najmi", "Amir", ""]]}, {"id": "1611.03789", "submitter": "Karol W\\k{e}grzycki", "authors": "Piotr Sankowski and Karol W\\k{e}grzycki", "title": "Improved Distance Queries and Cycle Counting by Frobenius Normal Form", "comments": "Invited to the Special Issue on Theoretical Aspects of Computer\n  Science (STACS 2017)", "journal-ref": "Theory of Computing Systems 2018", "doi": "10.1007/s00224-018-9894-x", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an unweighted, directed graph $G$ with the diameter $D$. In this\npaper, we introduce the framework for counting cycles and walks of given length\nin matrix multiplication time $\\widetilde{O}(n^\\omega)$. The framework is based\non the fast decomposition into Frobenius normal form and the Hankel\nmatrix-vector multiplication. It allows us to solve the All-Nodes Shortest\nCycles, All-Pairs All Walks problems efficiently and also give some improvement\nupon distance queries in unweighted graphs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 17:31:12 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 16:08:30 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 10:38:46 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sankowski", "Piotr", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "1611.03819", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang, Andrej Risteski", "title": "Recovery Guarantee of Non-negative Matrix Factorization via Alternating\n  Updates", "comments": "To appear in NIPS 2016. 8 pages of extended abstract; 48 pages in\n  total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization is a popular tool for decomposing data into\nfeature and weight matrices under non-negativity constraints. It enjoys\npractical success but is poorly understood theoretically. This paper proposes\nan algorithm that alternates between decoding the weights and updating the\nfeatures, and shows that assuming a generative model of the data, it provably\nrecovers the ground-truth under fairly mild conditions. In particular, its only\nessential requirement on features is linear independence. Furthermore, the\nalgorithm uses ReLU to exploit the non-negativity for decoding the weights, and\nthus can tolerate adversarial noise that can potentially be as large as the\nsignal, and can tolerate unbiased noise much larger than the signal. The\nanalysis relies on a carefully designed coupling between two potential\nfunctions, which we believe is of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 19:13:37 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1611.03889", "submitter": "Baigong Zheng", "authors": "Glencora Borradaile and Baigong Zheng", "title": "A PTAS for Three-Edge Connectivity in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimum-weight subgraph that satisfies\ngiven connectivity requirements. Specifically, given a requirement $r \\in\n\\{0,1,2,3\\}$ for every vertex, we seek the minimum-weight subgraph that\ncontains, for every pair of vertices $u$ and $v$, at least $\\min\\{ r(v),\nr(u)\\}$ edge-disjoint $u$-to-$v$ paths. We give a polynomial-time approximation\nscheme (PTAS) for this problem when the input graph is planar and the subgraph\nmay use multiple copies of any given edge. This generalizes an earlier result\nfor $r \\in \\{0,1,2\\}$. In order to achieve this PTAS, we prove some properties\nof triconnected planar graphs that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 21:54:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Borradaile", "Glencora", ""], ["Zheng", "Baigong", ""]]}, {"id": "1611.04100", "submitter": "Chihao Zhang", "authors": "Pinyan Lu, Kuan Yang, Chihao Zhang and Minshen Zhu", "title": "An FPTAS for Counting Proper Four-Colorings on Cubic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph coloring is arguably the most exhaustively studied problem in the area\nof approximate counting. It is conjectured that there is a fully\npolynomial-time (randomized) approximation scheme (FPTAS/FPRAS) for counting\nthe number of proper colorings as long as $q \\geq \\Delta + 1$, where $q$ is the\nnumber of colors and $\\Delta$ is the maximum degree of the graph. The bound of\n$q = \\Delta + 1$ is the uniqueness threshold for Gibbs measure on\n$\\Delta$-regular infinite trees. However, the conjecture remained open even for\nany fixed $\\Delta\\geq 3$ (The cases of $\\Delta=1, 2$ are trivial). In this\npaper, we design an FPTAS for counting the number of proper $4$-colorings on\ngraphs with maximum degree $3$ and thus confirm the conjecture in the case of\n$\\Delta=3$. This is the first time to achieve this optimal bound of $q = \\Delta\n+ 1$. Previously, the best FPRAS requires $q > \\frac{11}{6} \\Delta$ and the\nbest deterministic FPTAS requires $q > 2.581\\Delta + 1$ for general graphs. In\nthe case of $\\Delta=3$, the best previous result is an FPRAS for counting\nproper 5-colorings. We note that there is a barrier to go beyond $q = \\Delta +\n2$ for single-site Glauber dynamics based FPRAS and we overcome this by\ncorrelation decay approach. Moreover, we develop a number of new techniques for\nthe correlation decay approach which can find applications in other approximate\ncounting problems.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 07:59:00 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Lu", "Pinyan", ""], ["Yang", "Kuan", ""], ["Zhang", "Chihao", ""], ["Zhu", "Minshen", ""]]}, {"id": "1611.04156", "submitter": "Mateo Agudelo-Toro", "authors": "Catalina Pati\\~no-Forero, Mateo Agudelo-Toro, Mauricio Toro", "title": "Planning system for deliveries in Medell\\'in", "comments": "5 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present the implementation of an application capable of planning the\nshortest delivery route in the city of Medell\\'in, Colombia. We discuss the\ndifferent approaches to this problem which is similar to the famous Traveling\nSalesman Problem (TSP), but differs in the fact that, in our problem, we can\nvisit each place (or vertex) more than once. Solving this problem is important\nsince it would help people, especially stores with delivering services, to save\ntime and money spent in fuel, because they can plan any route in an efficient\nway.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 16:51:51 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Pati\u00f1o-Forero", "Catalina", ""], ["Agudelo-Toro", "Mateo", ""], ["Toro", "Mauricio", ""]]}, {"id": "1611.04175", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Recognizing and Eliciting Weakly Single Crossing Profiles on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of single crossing preference profiles is a widely studied domain\nin social choice theory. It has been generalized to the domain of single\ncrossing preference profiles with respect to trees which inherits many\ndesirable properties from the single crossing domain, for example, transitivity\nof majority relation, existence of polynomial time algorithms for finding\nwinners of Kemeny voting rule, etc. In this paper, we consider a further\ngeneralization of the domain of single crossing profiles on trees to the domain\nconsisting of all preference profiles which can be extended to single crossing\npreference profiles with respect to some tree by adding more preferences to it.\nWe call this domain the weakly single crossing domain on trees. We present a\npolynomial time algorithm for recognizing weakly single crossing profiles on\ntrees. We then move on to develop a polynomial time algorithm with low query\ncomplexity for eliciting weakly single crossing profiles on trees even when we\ndo not know any tree with respect to which the closure of the input profile is\nsingle crossing and the preferences can be queried only sequentially; moreover,\nthe sequential order is also unknown. We complement the performance of our\npreference elicitation algorithm by proving that our algorithm makes an optimal\nnumber of queries up to constant factors when the number of preferences is\nlarge compared to the number of candidates, even if the input profile is known\nto be single crossing with respect to some given tree and the preferences can\nbe accessed randomly.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 19:35:52 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1611.04308", "submitter": "Panos Parchas Mr", "authors": "Panos Parchas, Nikolaos Papailiou, Dimitris Papadias, Francesco Bonchi", "title": "Uncertain Graph Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain graphs are prevalent in several applications including\ncommunications systems, biological databases and social networks. The ever\nincreasing size of the underlying data renders both graph storage and query\nprocessing extremely expensive. Sparsification has often been used to reduce\nthe size of deterministic graphs by maintaining only the important edges.\nHowever, adaptation of deterministic sparsification methods fails in the\nuncertain setting. To overcome this problem, we introduce the first\nsparsification techniques aimed explicitly at uncertain graphs. The proposed\nmethods reduce the number of edges and redistribute their probabilities in\norder to decrease the graph size, while preserving its underlying structure.\nThe resulting graph can be used to efficiently and accurately approximate any\nquery and mining tasks on the original graph. An extensive experimental\nevaluation with real and synthetic datasets illustrates the effectiveness of\nour techniques on several common graph tasks, including clustering coefficient,\npage rank, reliability and shortest path distance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 09:58:11 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 11:39:21 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 11:29:06 GMT"}, {"version": "v4", "created": "Wed, 24 May 2017 05:50:38 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Parchas", "Panos", ""], ["Papailiou", "Nikolaos", ""], ["Papadias", "Dimitris", ""], ["Bonchi", "Francesco", ""]]}, {"id": "1611.04535", "submitter": "Ellen Vitercik", "authors": "Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin\n  White", "title": "Learning-Theoretic Foundations of Algorithm Configuration for\n  Combinatorial Partitioning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-cut, clustering, and many other partitioning problems that are of\nsignificant importance to machine learning and other scientific fields are\nNP-hard, a reality that has motivated researchers to develop a wealth of\napproximation algorithms and heuristics. Although the best algorithm to use\ntypically depends on the specific application domain, a worst-case analysis is\noften used to compare algorithms. This may be misleading if worst-case\ninstances occur infrequently, and thus there is a demand for optimization\nmethods which return the algorithm configuration best suited for the given\napplication's typical inputs. We address this problem for clustering, max-cut,\nand other partitioning problems, such as integer quadratic programming, by\ndesigning computationally efficient and sample efficient learning algorithms\nwhich receive samples from an application-specific distribution over problem\ninstances and learn a partitioning algorithm with high expected performance.\nOur algorithms learn over common integer quadratic programming and clustering\nalgorithm families: SDP rounding algorithms and agglomerative clustering\nalgorithms with dynamic programming. For our sample complexity analysis, we\nprovide tight bounds on the pseudodimension of these algorithm classes, and\nshow that surprisingly, even for classes of algorithms parameterized by a\nsingle parameter, the pseudo-dimension is superconstant. In this way, our work\nboth contributes to the foundations of algorithm configuration and pushes the\nboundaries of learning theory, since the algorithm classes we analyze consist\nof multi-stage optimization procedures and are significantly more complex than\nclasses typically studied in learning theory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:22:21 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 23:57:09 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 10:08:24 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 16:07:08 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Nagarajan", "Vaishnavh", ""], ["Vitercik", "Ellen", ""], ["White", "Colin", ""]]}, {"id": "1611.04548", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "Real Stable Polynomials and Matroids: Optimization and Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great variety of fundamental optimization and counting problems arising in\ncomputer science, mathematics and physics can be reduced to one of the\nfollowing computational tasks involving polynomials and set systems: given an\n$m$-variate real polynomial $g$ and a family of subsets $B$ of $[m]$, (1) find\n$S\\in B$ such that the monomial in $g$ corresponding to $S$ has the largest\ncoefficient in $g$, or (2) compute the sum of coefficients of monomials in $g$\ncorresponding to all the sets in $B$. Special cases of these problems, such as\ncomputing permanents, sampling from DPPs and maximizing subdeterminants have\nbeen topics of recent interest in theoretical computer science.\n  In this paper we present a general convex programming framework geared to\nsolve both of these problems. We show that roughly, when $g$ is a real stable\npolynomial with non-negative coefficients and $B$ is a matroid, the integrality\ngap of our relaxation is finite and depends only on $m$ (and not on the\ncoefficients of g).\n  Prior to our work, such results were known only in sporadic cases that relied\non the structure of $g$ and $B$; it was not even clear if one could formulate a\nconvex relaxation that has a finite integrality gap beyond these special cases.\nTwo notable examples are a result by Gurvits on the van der Waerden conjecture\nfor real stable $g$ when $B$ is a single element and a result by Nikolov and\nSingh for multilinear real stable polynomials when $B$ is a partition matroid.\nOur work, which encapsulates most interesting cases of $g$ and $B$, benefits\nfrom both - we were inspired by the latter in deriving the right convex\nprogramming relaxation and the former in establishing the integrality gap.\nHowever, proving our results requires significant extensions of both; in that\nprocess we come up with new notions and connections between stable polynomials\nand matroids which should be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 20:11:38 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1611.04784", "submitter": "Sumit Kumar Jha", "authors": "Sumit Kumar Jha", "title": "Rate of convergence of major cost incurred in the in-situ permutation\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The in-situ permutation algorithm due to MacLeod replaces\n$(x_{1},\\cdots,x_{n})$ by $(x_{p(1)},\\cdots,x_{p(n)})$ where\n$\\pi=(p(1),\\cdots,p(n))$ is a permutation of $\\{1,2,\\cdots,n\\}$ using at most\n$O(1)$ space. Kirshenhofer, Prodinger and Tichy have shown that the major cost\nincurred in the algorithm satisfies a recurrence similar to sequence of the\nnumber of key comparisons needed by the Quicksort algorithm to sort an array of\n$n$ randomly permuted items. Further, Hwang has proved that the normalized cost\nconverges in distribution. Here, following Neininger and R\\\"uschendorf, we\nprove the that rate of convergence to be of the order $\\Theta(\\ln(n)/n)$ in the\nZolotarev metric.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 10:51:39 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Jha", "Sumit Kumar", ""]]}, {"id": "1611.04847", "submitter": "Arun Kadavankandy", "authors": "Arun Kadavankandy (MAESTRO), Konstantin Avrachenkov (MAESTRO), Laura\n  Cottatellucci, Rajesh Sundaresan (ECE)", "title": "The Power of Side-information in Subgraph Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of hidden community detection. We\nconsider Belief Propagation (BP) applied to the problem of detecting a hidden\nErd\\H{o}s-R\\'enyi (ER) graph embedded in a larger and sparser ER graph, in the\npresence of side-information. We derive two related algorithms based on BP to\nperform subgraph detection in the presence of two kinds of side-information.\nThe first variant of side-information consists of a set of nodes, called cues,\nknown to be from the subgraph. The second variant of side-information consists\nof a set of nodes that are cues with a given probability. It was shown in past\nworks that BP without side-information fails to detect the subgraph correctly\nwhen an effective signal-to-noise ratio (SNR) parameter falls below a\nthreshold. In contrast, in the presence of non-trivial side-information, we\nshow that the BP algorithm achieves asymptotically zero error for any value of\nthe SNR parameter. We validate our results through simulations on synthetic\ndatasets as well as on a few real world networks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 10:13:10 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 09:45:20 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 13:26:53 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kadavankandy", "Arun", "", "MAESTRO"], ["Avrachenkov", "Konstantin", "", "MAESTRO"], ["Cottatellucci", "Laura", "", "ECE"], ["Sundaresan", "Rajesh", "", "ECE"]]}, {"id": "1611.04942", "submitter": "Massimo Cafaro", "authors": "Italo Epicoco, Massimo Cafaro, Marco Pulimeno", "title": "Fast and Accurate Mining of Correlated Heavy Hitters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of mining Correlated Heavy Hitters (CHH) from a two-dimensional\ndata stream has been introduced recently, and a deterministic algorithm based\non the use of the Misra--Gries algorithm has been proposed by Lahiri et al. to\nsolve it. In this paper we present a new counter-based algorithm for tracking\nCHHs, formally prove its error bounds and correctness and show, through\nextensive experimental results, that our algorithm outperforms the Misra--Gries\nbased algorithm with regard to accuracy and speed whilst requiring\nasymptotically much less space.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 17:17:28 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 16:10:59 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Epicoco", "Italo", ""], ["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""]]}, {"id": "1611.04988", "submitter": "Rene Schilling", "authors": "Ren\\'e L. Schilling and Dietrich Stoyan", "title": "Continuity Assumptions in Cake-Cutting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In important papers on cake-cutting -- one of the key areas in fair division\nand resource allocation -- the measure-theoretical fundamentals are not fully\ncorrectly given. It is not clear (i) which family of sets should be taken for\nthe pieces of cake, (ii) which set-functions should be used for evaluating the\npieces, and (iii) which is the relationship between various continuity\nproperties appearing in cake-cutting.\n  We show that probably the best choice for the familiy of subsets of $[0,1]$\nis the Borel $\\sigma$-algebra and for the set-function any `sliceable' Borel\nmeasure. At least in dimension one it does not make sense to work with only\nfinitely additive contents on finite unions of intervals. For the continuity\nproperty we see two possibilities. The weaker is the traditional divisibility\nproperty, which is equivalent to being atom-free. The stronger is simply\nabsolute continuity with respect to Lebesgue measure. We also consider the case\nof a base set (cake or pie) more general than $[0,1]$.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 18:59:40 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Schilling", "Ren\u00e9 L.", ""], ["Stoyan", "Dietrich", ""]]}, {"id": "1611.04999", "submitter": "Cyrus Rashtchian", "authors": "Paul Beame and Cyrus Rashtchian", "title": "Massively-Parallel Similarity Join, Edge-Isoperimetry, and Distance\n  Correlations on the Hypercube", "comments": "23 pages, plus references and appendix. To appear in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed protocols for finding all pairs of similar vectors in a\nlarge dataset. Our results pertain to a variety of discrete metrics, and we\ngive concrete instantiations for Hamming distance. In particular, we give\nimproved upper bounds on the overhead required for similarity defined by\nHamming distance $r>1$ and prove a lower bound showing qualitative optimality\nof the overhead required for similarity over any Hamming distance $r$. Our main\nconceptual contribution is a connection between similarity search algorithms\nand certain graph-theoretic quantities. For our upper bounds, we exhibit a\ngeneral method for designing one-round protocols using edge-isoperimetric\nshapes in similarity graphs. For our lower bounds, we define a new\ncombinatorial optimization problem, which can be stated in purely\ngraph-theoretic terms yet also captures the core of the analysis in previous\ntheoretical work on distributed similarity joins. As one of our main technical\nresults, we prove new bounds on distance correlations in subsets of the Hamming\ncube.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 19:36:28 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Beame", "Paul", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1611.05247", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Guillermo de Bernardo, Ram\\'on Casares, Adri\\'an G\\'omez-Brand\\'on and\n  Jos\\'e R. Param\\'a", "title": "A new method to index and store spatio-temporal data", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proceeding of the 20th Pacific Asia Conference on Information\n  Systems (PACIS 2016). Association for Information Systems. AIS Electronic\n  Library (AISeL). Paper 93. ISBN: 9789860491029", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data structure that stores, in a compressed way, object\ntrajectories, which at the same time, allow to efficiently response queries\nwithout the need to decompress the data. We use a data structure, called\n$k^{2}$-tree, to store the full position of all objects at regular time\nintervals. For storing the positions of objects between two time instants\nrepresented with $k^{2}$-trees, we only encode the relative movements. In order\nto save space, those relative moments are encoded with only one integer,\ninstead of two (x,y)-coordinates. Moreover, the resulting integers are further\ncompressed with a technique that allows us to manipulate those movements\ndirectly in compressed form. In this paper, we show an experimental evaluation\nof this structure, which shows important savings in space and good response\ntimes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 12:19:38 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["de Bernardo", "Guillermo", ""], ["Casares", "Ram\u00f3n", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Param\u00e1", "Jos\u00e9 R.", ""]]}, {"id": "1611.05248", "submitter": "Stefan Neumann", "authors": "Monika Henzinger, Stefan Neumann", "title": "Incremental and Fully Dynamic Subgraph Connectivity For Emergency\n  Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last 10 years it has become popular to study dynamic graph\nproblems in a emergency planning or sensitivity setting: Instead of considering\nthe general fully dynamic problem, we only have to process a single batch\nupdate of size $d$; after the update we have to answer queries.\n  In this paper, we consider the dynamic subgraph connectivity problem with\nsensitivity $d$: We are given a graph of which some vertices are activated and\nsome are deactivated. After that we get a single update in which the states of\nup to d vertices are changed. Then we get a sequence of connectivity queries in\nthe subgraph of activated vertices.\n  We present the first fully dynamic algorithm for this problem which has an\nupdate and query time only slightly worse than the best decremental algorithm.\nIn addition, we present the first incremental algorithm which is tight with\nrespect to the best known conditional lower bound; moreover, the algorithm is\nsimple and we believe it is implementable and efficient in practice.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 12:21:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Henzinger", "Monika", ""], ["Neumann", "Stefan", ""]]}, {"id": "1611.05359", "submitter": "Tomohiro I", "authors": "Tomohiro I", "title": "Longest Common Extensions with Recompression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two positions $i$ and $j$ in a string $T$ of length $N$, a longest\ncommon extension (LCE) query asks for the length of the longest common prefix\nbetween suffixes beginning at $i$ and $j$. A compressed LCE data structure is a\ndata structure that stores $T$ in a compressed form while supporting fast LCE\nqueries. In this article we show that the recompression technique is a powerful\ntool for compressed LCE data structures. We present a new compressed LCE data\nstructure of size $O(z \\lg (N/z))$ that supports LCE queries in $O(\\lg N)$\ntime, where $z$ is the size of Lempel-Ziv 77 factorization without\nself-reference of $T$. Given $T$ as an uncompressed form, we show how to build\nour data structure in $O(N)$ time and space. Given $T$ as a grammar compressed\nform, i.e., an straight-line program of size n generating $T$, we show how to\nbuild our data structure in $O(n \\lg (N/n))$ time and $O(n + z \\lg (N/z))$\nspace. Our algorithms are deterministic and always return correct answers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 16:54:04 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 01:36:09 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["I", "Tomohiro", ""]]}, {"id": "1611.05372", "submitter": "Tobias Harks", "authors": "Tobias Harks and Max Klimm and Britta Peis", "title": "Sensitivity Analysis for Convex Separable Optimization over Integral\n  Polymatroids", "comments": "Succeeds arXiv paper 1407.7650. arXiv admin note: text overlap with\n  arXiv:1407.7650", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sensitivity of optimal solutions of convex separable\noptimization problems over an integral polymatroid base polytope with respect\nto parameters determining both the cost of each element and the polytope. Under\nconvexity and a regularity assumption on the functional dependency of the cost\nfunction with respect to the parameters, we show that reoptimization after a\nchange in parameters can be done by elementary local operations. Applying this\nresult, we derive that starting from any optimal solution there is a new\noptimal solution to new parameters such that the L1-norm of the difference of\nthe two solutions is at most two times the L1 norm of the difference of the\nparameters. We apply these sensitivity results to a class of non-cooperative\npolymatroid games and derive the existence of pure Nash equilibria. We\ncomplement our results by showing that polymatroids are the maximal\ncombinatorial structure enabling these results. For any non-polymatroid region,\nthere is a corresponding optimization problem for which the sensitivity results\ndo not hold. In addition, there is a game where the players strategies are\nisomorphic to the non-polymatroid region and that does not admit a pure Nash\nequilibrium.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:13:27 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 11:38:37 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Harks", "Tobias", ""], ["Klimm", "Max", ""], ["Peis", "Britta", ""]]}, {"id": "1611.05429", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy, David H. K. Kim and Rachit Nimavat", "title": "New Hardness Results for Routing on Disjoint Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Node-Disjoint Paths (NDP) problem, the input consists of an\nundirected $n$-vertex graph $G$, and a collection\n$\\mathcal{M}=\\{(s_1,t_1),\\ldots,(s_k,t_k)\\}$ of pairs of its vertices, called\nsource-destination, or demand, pairs. The goal is to route the largest possible\nnumber of the demand pairs via node-disjoint paths. The best current\napproximation for the problem is achieved by a simple greedy algorithm, whose\napproximation factor is $O(\\sqrt n)$, while the best current negative result is\nan $\\Omega(\\log^{1/2-\\delta}n)$-hardness of approximation for any constant\n$\\delta$, under standard complexity assumptions. Even seemingly simple special\ncases of the problem are still poorly understood: when the input graph is a\ngrid, the best current algorithm achieves an $\\tilde O(n^{1/4})$-approximation,\nand when it is a general planar graph, the best current approximation ratio of\nan efficient algorithm is $\\tilde O(n^{9/19})$. The best currently known lower\nbound on the approximability of both these versions of the problem is\nAPX-hardness.\n  In this paper we prove that NDP is $2^{\\Omega(\\sqrt{\\log n})}$-hard to\napproximate, unless all problems in NP have algorithms with running time\n$n^{O(\\log n)}$. Our result holds even when the underlying graph is a planar\ngraph with maximum vertex degree $3$, and all source vertices lie on the\nboundary of a single face (but the destination vertices may lie anywhere in the\ngraph). We extend this result to the closely related Edge-Disjoint Paths\nproblem, showing the same hardness of approximation ratio even for sub-cubic\nplanar graphs with all sources lying on the boundary of a single face.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:23:18 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Kim", "David H. K.", ""], ["Nimavat", "Rachit", ""]]}, {"id": "1611.05487", "submitter": "Ilya Safro", "authors": "Ehsan Sadrfaridpour, Sandeep Jeereddy, Ken Kennedy, Andre Luckow,\n  Talayeh Razzaghi, Ilya Safro", "title": "Algebraic multigrid support vector machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine is a flexible optimization-based technique widely\nused for classification problems. In practice, its training part becomes\ncomputationally expensive on large-scale data sets because of such reasons as\nthe complexity and number of iterations in parameter fitting methods,\nunderlying optimization solvers, and nonlinearity of kernels. We introduce a\nfast multilevel framework for solving support vector machine models that is\ninspired by the algebraic multigrid. Significant improvement in the running has\nbeen achieved without any loss in the quality. The proposed technique is highly\nbeneficial on imbalanced sets. We demonstrate computational results on publicly\navailable and industrial data sets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 22:32:50 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 01:10:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Sadrfaridpour", "Ehsan", ""], ["Jeereddy", "Sandeep", ""], ["Kennedy", "Ken", ""], ["Luckow", "Andre", ""], ["Razzaghi", "Talayeh", ""], ["Safro", "Ilya", ""]]}, {"id": "1611.05530", "submitter": "Pasin Manurangsi", "authors": "Haris Angelidakis, Yury Makarychev and Pasin Manurangsi", "title": "An Improved Integrality Gap for the Calinescu-Karloff-Rabani Relaxation\n  for Multiway Cut", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an improved integrality gap instance for the\nCalinescu-Karloff-Rabani LP relaxation of the Multiway Cut problem. In\nparticular, for $k \\geqslant 3$ terminals, our instance has an integrality\nratio of $6 / (5 + \\frac{1}{k - 1}) - \\varepsilon$, for every constant\n$\\varepsilon > 0$. For every $k \\geqslant 4$, our result improves upon a\nlong-standing lower bound of $8 / (7 + \\frac{1}{k - 1})$ by Freund and Karloff\n(2000). Due to Manokaran et al.'s result (2008), our integrality gap also\nimplies Unique Games hardness of approximating Multiway Cut of the same ratio.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 01:57:54 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Angelidakis", "Haris", ""], ["Makarychev", "Yury", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "1611.05561", "submitter": "Shweta Jain", "authors": "Shweta Jain, C. Seshadhri", "title": "A Fast and Provable Method for Estimating Clique Counts Using Tur\\'an's\n  Theorem", "comments": "Added a link to the code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique counts reveal important properties about the structure of massive\ngraphs, especially social networks. The simple setting of just 3-cliques\n(triangles) has received much attention from the research community. For larger\ncliques (even, say 6-cliques) the problem quickly becomes intractable because\nof combinatorial explosion. Most methods used for triangle counting do not\nscale for large cliques, and existing algorithms require massive parallelism to\nbe feasible.\n  We present a new randomized algorithm that provably approximates the number\nof k-cliques, for any constant k. The key insight is the use of (strengthenings\nof) the classic Tur\\'an's theorem: this claims that if the edge density of a\ngraph is sufficiently high, the k-clique density must be non-trivial. We define\na combinatorial structure called a Tur\\'an shadow, the construction of which\nleads to fast algorithms for clique counting.\n  We design a practical heuristic, called TUR\\'AN-SHADOW, based on this\ntheoretical algorithm, and test it on a large class of test graphs. In all\ncases,TUR\\'AN-SHADOW has less than 2% error, in a fraction of the time used by\nwell-tuned exact algorithms. We do detailed comparisons with a range of other\nsampling algorithms, and find that TUR\\'AN-SHADOW is generally much faster and\nmore accurate. For example, TUR\\'AN-SHADOW estimates all cliques numbers up to\nsize 10 in social network with over a hundred million edges. This is done in\nless than three hours on a single commodity machine.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 04:43:13 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 03:49:38 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 07:37:57 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Jain", "Shweta", ""], ["Seshadhri", "C.", ""]]}, {"id": "1611.05646", "submitter": "Ravishankar Krishnaswamy", "authors": "Anupam Gupta, Ravishankar Krishnaswamy, Amit Kumar, and Debmalya\n  Panigrahi", "title": "Online and Dynamic Algorithms for Set Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the set cover problem in the fully dynamic model. In\nthis model, the set of active elements, i.e., those that must be covered at any\ngiven time, can change due to element arrivals and departures. The goal is to\nmaintain an algorithmic solution that is competitive with respect to the\ncurrent optimal solution. This model is popular in both the dynamic algorithms\nand online algorithms communities. The difference is in the restriction placed\non the algorithm: in dynamic algorithms, the running time of the algorithm\nmaking updates (called update time) is bounded, while in online algorithms, the\nnumber of updates made to the solution (called recourse) is limited.\n  In this paper we show the following results: In the update time setting, we\nobtain O(log n)-competitiveness with O(f log n) amortized update time, and\nO(f^3)-competitiveness with O(f^2) update time. The O(log n)-competitive\nalgorithm is the first one to achieve a competitive ratio independent of f in\nthis setting. In the recourse setting, we show a competitive ratio of O(min{log\nn,f}) with constant amortized recourse. Note that this matches the best offline\nbounds with just constant recourse, something that is impossible in the\nclassical online model.\n  Our results are based on two algorithmic frameworks in the fully-dynamic\nmodel that are inspired by the classic greedy and primal-dual algorithms for\noffline set cover. We show that both frameworks can be used for obtaining both\nrecourse and update time bounds, thereby demonstrating algorithmic techniques\ncommon to these strands of research.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 12:01:43 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Gupta", "Anupam", ""], ["Krishnaswamy", "Ravishankar", ""], ["Kumar", "Amit", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1611.05660", "submitter": "Henrik Barthels", "authors": "Henrik Barthels, Paolo Bientinesi", "title": "The Matrix Chain Algorithm to Compile Linear Algebra Expressions", "comments": "DSLDI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix chain problem consists in finding the parenthesization of a matrix\nproduct $M := A_1 A_2 \\cdots A_n$ that minimizes the number of scalar\noperations. In practical applications, however, one frequently encounters more\ncomplicated scenarios, where expressions involve transposition, inversion,\nmatrices with given properties, and sequences. The computation of such\nexpressions makes use of a set of computational kernels that offer\nfunctionality well beyond the simple matrix product. The challenge then shifts\nfrom finding an optimal parenthesization to finding an optimal mapping of the\ninput expression to the available kernels. Furthermore, it is often the case\nthat a solution based on the minimization of scalar operations does not result\nin the optimal solution in terms of execution time, and/or might be numerically\nunstable. In this paper, we introduce a number of generalizations of the matrix\nchain problem--including kernels, properties, sequences, and cost\nfunctions--and present corresponding algorithmic solutions.\n  The motivation for this work comes from the fact that--despite great advances\nin the development of compilers--the task of mapping linear algebra problems to\noptimized kernels is still to be done manually. In order to relieve the user\nfrom this complex task, new techniques for the compilation of linear algebra\nexpressions have to be developed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 12:44:15 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Barthels", "Henrik", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1611.05753", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Wolfgang Dvo\\v{r}\\'ak and Monika Henzinger and David P. Williamson", "title": "Maximizing a Submodular Function with Viability Constraints", "comments": "in Algorithmica (2015)", "journal-ref": null, "doi": "10.1007/s00453-015-0066-y", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone submodular function with\nviability constraints. This problem originates from computational biology,\nwhere we are given a phylogenetic tree over a set of species and a directed\ngraph, the so-called food web, encoding viability constraints between these\nspecies. These food webs usually have constant {depth}. The goal is to select a\nsubset of $k$ species that satisfies the viability constraints and has maximal\nphylogenetic diversity. As this problem is known to be NP-hard, we investigate\napproximation algorithms. We present the first constant factor approximation\nalgorithm if the depth is constant. Its approximation ratio is\n$(1-\\frac{1}{\\sqrt{e}})$. This algorithm not only applies to phylogenetic trees\nwith viability constraints but for arbitrary monotone submodular set functions\nwith viability constraints. Second, we show that there is no\n$(1-1/e+\\epsilon)$-approximation algorithm for our problem setting (even for\nadditive functions) and that there is no approximation algorithm for a slight\nextension of this setting.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:03:36 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""], ["Williamson", "David P.", ""]]}, {"id": "1611.05907", "submitter": "Mourad El Ouali", "authors": "Mourad El Ouali, Christian Glazik, Volkmar Sauerland, Anand Srivastav", "title": "On the Query Complexity of Black-Peg AB-Mastermind", "comments": "11 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:1303.5862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastermind game is a two players zero sum game of imperfect information. The\nfirst player, called codemaker, chooses a secret code and the second player,\ncalled codebreaker, tries to break the secret code by making as few guesses as\npossible, exploiting information that is given by the codemaker after each\nguess. In this paper, we consider the so called Black-Peg variant of\nMastermind, where the only information concerning a guess is the number of\npositions in which the guess coincides with the secret code. More precisely, we\ndeal with a special version of the Black-Peg game with n holes and k<=n colors\nwhere no repetition of colors is allowed. We present upper and lower bounds on\nthe number of guesses necessary to break the secret code. We first come back to\nthe upper bound results introduced by El Ouali and Sauerland (2013). For the\ncase k=n the secret code can be algorithmically identified within less than\n(n-3)*ld(n)+5n/2 queries. That result improves the result of Ker-I Ko and\nShia-Chung Teng (1985) by almost a factor of 2. For the case k>n we prove an\nupper bound for the problem of (n-1)*ld(n)+k+1. Furthermore we prove a new\nlower bound for (a generalization of) the case k=n that improves the recent\nresult of Berger et al. (2016) from n-log(log(n)) to n. We also give a lower\nbound of k queries for the case k>n.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:42:57 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Ouali", "Mourad El", ""], ["Glazik", "Christian", ""], ["Sauerland", "Volkmar", ""], ["Srivastav", "Anand", ""]]}, {"id": "1611.05911", "submitter": "Evira Mayordomo", "authors": "Jack H. Lutz and Elvira Mayordomo", "title": "Computing Absolutely Normal Numbers in Nearly Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A real number $x$ is absolutely normal if, for every base $b\\ge 2$, every two\nequally long strings of digits appear with equal asymptotic frequency in the\nbase-$b$ expansion of $x$. This paper presents an explicit algorithm that\ngenerates the binary expansion of an absolutely normal number $x$, with the\n$n$th bit of $x$ appearing after $n$polylog$(n)$ computation steps. This speed\nis achieved by simultaneously computing and diagonalizing against a martingale\nthat incorporates Lempel-Ziv parsing algorithms in all bases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:51:13 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 13:00:23 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 08:17:41 GMT"}, {"version": "v4", "created": "Thu, 16 Jul 2020 12:42:23 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Lutz", "Jack H.", ""], ["Mayordomo", "Elvira", ""]]}, {"id": "1611.05944", "submitter": "Alex Rusciano", "authors": "James Demmel, Alex Rusciano", "title": "Parallelepipeds obtaining HBL lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the application of the discrete Holder-Brascamp-Lieb (HBL)\ninequalities to the design of communication optimal algorithms. In particular,\nit describes optimal tiling (blocking) strategies for nested loops that lack\ndata dependencies and exhibit linear memory access patterns. We attain known\nlower bounds for communication costs by unraveling the relationship between the\nHBL linear program, its dual, and tile selection. The methods used are\nconstructive and algorithmic. The case when all arrays have one index is\nexplored in depth, as a useful example in which a particularly efficient tiling\ncan be determined.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 01:23:31 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Demmel", "James", ""], ["Rusciano", "Alex", ""]]}, {"id": "1611.05998", "submitter": "Vijay Bhattiprolu", "authors": "Vijay Bhattiprolu, Mrinalkanti Ghosh, Venkatesan Guruswami, Euiwoong\n  Lee, Madhur Tulsiani", "title": "Weak Decoupling, Polynomial Folds, and Approximate Optimization over the\n  Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following basic problem: given an $n$-variate degree-$d$\nhomogeneous polynomial $f$ with real coefficients, compute a unit vector $x \\in\n\\mathbb{R}^n$ that maximizes $|f(x)|$. Besides its fundamental nature, this\nproblem arises in diverse contexts ranging from tensor and operator norms to\ngraph expansion to quantum information theory. The homogeneous degree $2$ case\nis efficiently solvable as it corresponds to computing the spectral norm of an\nassociated matrix, but the higher degree case is NP-hard.\n  We give approximation algorithms for this problem that offer a trade-off\nbetween the approximation ratio and running time: in $n^{O(q)}$ time, we get an\napproximation within factor $O_d((n/q)^{d/2-1})$ for arbitrary polynomials,\n$O_d((n/q)^{d/4-1/2})$ for polynomials with non-negative coefficients, and\n$O_d(\\sqrt{m/q})$ for sparse polynomials with $m$ monomials. The approximation\nguarantees are with respect to the optimum of the level-$q$ sum-of-squares\n(SoS) SDP relaxation of the problem. Known polynomial time algorithms for this\nproblem rely on \"decoupling lemmas.\" Such tools are not capable of offering a\ntrade-off like our results as they blow up the number of variables by a factor\nequal to the degree. We develop new decoupling tools that are more efficient in\nthe number of variables at the expense of less structure in the output\npolynomials. This enables us to harness the benefits of higher level SoS\nrelaxations.\n  We complement our algorithmic results with some polynomially large\nintegrality gaps, albeit for a slightly weaker (but still very natural)\nrelaxation. Toward this, we give a method to lift a level-$4$ solution matrix\n$M$ to a higher level solution, under a mild technical condition on $M$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 07:54:09 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 05:03:11 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bhattiprolu", "Vijay", ""], ["Ghosh", "Mrinalkanti", ""], ["Guruswami", "Venkatesan", ""], ["Lee", "Euiwoong", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1611.06189", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Query Complexity of Tournament Solutions", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph where there is exactly one edge between every pair of\nvertices is called a {\\em tournament}. Finding the \"best\" set of vertices of a\ntournament is a well studied problem in social choice theory. A {\\em tournament\nsolution} takes a tournament as input and outputs a subset of vertices of the\ninput tournament. However, in many applications, for example, choosing the best\nset of drugs from a given set of drugs, the edges of the tournament are given\nonly implicitly and knowing the orientation of an edge is costly. In such\nscenarios, we would like to know the best set of vertices (according to some\ntournament solution) by \"querying\" as few edges as possible. We, in this paper,\nprecisely study this problem for commonly used tournament solutions: given an\noracle access to the edges of a tournament T, find $f(T)$ by querying as few\nedges as possible, for a tournament solution f. We first show that the set of\nCondorcet non-losers in a tournament can be found by querying $2n-\\lfloor \\log\nn \\rfloor -2$ edges only and this is tight in the sense that every algorithm\nfor finding the set of Condorcet non-losers needs to query at least $2n-\\lfloor\n\\log n \\rfloor -2$ edges in the worst case, where $n$ is the number of vertices\nin the input tournament. We then move on to study other popular tournament\nsolutions and show that any algorithm for finding the Copeland set, the Slater\nset, the Markov set, the bipartisan set, the uncovered set, the Banks set, and\nthe top cycle must query $\\Omega(n^2)$ edges in the worst case. On the positive\nside, we are able to circumvent our strong query complexity lower bound results\nby proving that, if the size of the top cycle of the input tournament is at\nmost $k$, then we can find all the tournament solutions mentioned above by\nquerying $O(nk + \\frac{n\\log n}{\\log(1-\\frac{1}{k})})$ edges only.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:19:32 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 13:26:00 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 05:45:42 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1611.06222", "submitter": "Erik Waingarten", "authors": "Alexandr Andoni, Huy L. Nguyen, Aleksandar Nikolov, Ilya Razenshteyn,\n  Erik Waingarten", "title": "Approximate Near Neighbors for General Symmetric Norms", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every symmetric normed space admits an efficient nearest\nneighbor search data structure with doubly-logarithmic approximation.\nSpecifically, for every $n$, $d = n^{o(1)}$, and every $d$-dimensional\nsymmetric norm $\\|\\cdot\\|$, there exists a data structure for\n$\\mathrm{poly}(\\log \\log n)$-approximate nearest neighbor search over\n$\\|\\cdot\\|$ for $n$-point datasets achieving $n^{o(1)}$ query time and\n$n^{1+o(1)}$ space. The main technical ingredient of the algorithm is a\nlow-distortion embedding of a symmetric norm into a low-dimensional iterated\nproduct of top-$k$ norms.\n  We also show that our techniques cannot be extended to general norms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:56:26 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 13:21:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Andoni", "Alexandr", ""], ["Nguyen", "Huy L.", ""], ["Nikolov", "Aleksandar", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1611.06329", "submitter": "Rafal Kapelko", "authors": "Rafa{\\l} Kapelko", "title": "On the Energy Efficient Displacement of Random Sensors for Interference\n  and Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of the minimilization of energy\nconsumption in reallocation of wireless mobile sensors network (WMSN) to assure\ngood communication without interference.\n  Fix $d\\in\\mathbb{N}\\setminus\\{0\\}.$ Assume $n$ sensors are initially randomly\nplaced in the hyperoctant $[0,\\infty)^d$ according to $d$ identical and\nindependent Poisson processes each with arrival rate $\\lambda>0.$\n  Let $0< s \\le v$ be given real numbers. We are allowed to move the sensors,\nso that every two consecutive sensors are placed at distance greater than or\nequal to $s$ and less than or equal to $v.$\n  Fix $a\\ge 1.$ Assume that $i-$th sensor is displaced a distance equal to\n$m(i).$ The cost measure for the displacement of the team of sensors is the sum\n$\\sum_{i=1}^{n}d_i^a$ ($a-$total movement).\n  In this work, we discover and explain a sharp decline and a sharp increase (a\nthreshold phenomena) in the expected minimal $a-$total movement around the\ninterference-connectivity distances $s,v$ equal to $\\frac{1}{\\lambda}.$\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 09:07:49 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 19:00:37 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 07:16:53 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kapelko", "Rafa\u0142", ""]]}, {"id": "1611.06343", "submitter": "Suman Sourav", "authors": "Suman Sourav, Peter Robinson, Seth Gilbert", "title": "Slow links, fast links, and the cost of gossip", "comments": null, "journal-ref": "in IEEE Transactions on Parallel and Distributed Systems, vol. 30,\n  no. 9, pp. 2130-2147, 1 Sept. 2019", "doi": "10.1109/TPDS.2019.2905568", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the classical problem of information dissemination: one (or more)\nnodes in a network have some information that they want to distribute to the\nremainder of the network. In this paper, we study the cost of information\ndissemination in networks where edges have latencies, i.e., sending a message\nfrom one node to another takes some amount of time. We first generalize the\nidea of conductance to weighted graphs by defining $\\phi_*$ to be the \"critical\nconductance\" and $\\ell_*$ to be the \"critical latency\". One goal of this paper\nis to argue that $\\phi_*$ characterizes the connectivity of a weighted graph\nwith latencies in much the same way that conductance characterizes the\nconnectivity of unweighted graphs.\n  We give near tight lower and upper bounds on the problem of information\ndissemination, up to polylogarithmic factors. Specifically, we show that in a\ngraph with (weighted) diameter $D$ (with latencies as weights) and maximum\ndegree $\\Delta$, any information dissemination algorithm requires at least\n$\\Omega(\\min(D+\\Delta, \\ell_*/\\phi_*))$ time % in the worst case. We show\nseveral variants of the lower bound (e.g., for graphs with small diameter,\ngraphs with small max-degree, etc.) by reduction to a simple combinatorial\ngame.\n  We then give nearly matching algorithms, showing that information\ndissemination can be solved in $O(\\min((D+\\Delta)\\log^3{n}, (\\ell_*/\\phi_*)\\log\nn)$ time. This is achieved by combining two cases. We show that the classical\npush-pull algorithm is (near) optimal when the diameter or the maximum degree\nis large. For the case where the diameter and the maximum degree are small, we\ngive an alternative strategy in which we first discover the latencies and then\nuse an algorithm for known latencies based on a weighted spanner construction.\n(Our algorithms are within polylogarithmic factors of being tight both for\nknown and unknown latencies.)\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 11:35:33 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 20:15:02 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 16:41:23 GMT"}, {"version": "v4", "created": "Sat, 30 Nov 2019 13:36:05 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sourav", "Suman", ""], ["Robinson", "Peter", ""], ["Gilbert", "Seth", ""]]}, {"id": "1611.06427", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Daniel Dadush and L\\'aszl\\'o A. V\\'egh and Giacomo Zambelli", "title": "Rescaling Algorithms for Linear Conic Feasibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose simple polynomial-time algorithms for two linear conic feasibility\nproblems. For a matrix $A\\in \\mathbb{R}^{m\\times n}$, the kernel problem\nrequires a positive vector in the kernel of $A$, and the image problem requires\na positive vector in the image of $A^\\top$. Both algorithms iterate between\nsimple first order steps and rescaling steps. These rescalings improve natural\ngeometric potentials. If Goffin's condition measure $\\rho_A$ is negative, then\nthe kernel problem is feasible and the worst-case complexity of the kernel\nalgorithm is $O\\left((m^3n+mn^2)\\log{|\\rho_A|^{-1}}\\right)$; if $\\rho_A>0$,\nthen the image problem is feasible and the image algorithm runs in time\n$O\\left(m^2n^2\\log{\\rho_A^{-1}}\\right)$. We also extend the image algorithm to\nthe oracle setting.\n  We address the degenerate case $\\rho_A=0$ by extending our algorithms to find\nmaximum support nonnegative vectors in the kernel of $A$ and in the image of\n$A^\\top$. In this case the running time bounds are expressed in the bit-size\nmodel of computation: for an input matrix $A$ with integer entries and total\nencoding length $L$, the maximum support kernel algorithm runs in time\n$O\\left((m^3n+mn^2)L\\right)$, while the maximum support image algorithm runs in\ntime $O\\left(m^2n^2L\\right)$. The standard linear programming feasibility\nproblem can be easily reduced to either maximum support problems, yielding\npolynomial-time algorithms for Linear Programming.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 20:38:23 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 19:24:27 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 21:05:14 GMT"}, {"version": "v4", "created": "Sun, 17 Dec 2017 12:05:36 GMT"}, {"version": "v5", "created": "Sun, 7 Apr 2019 21:06:55 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Dadush", "Daniel", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""], ["Zambelli", "Giacomo", ""]]}, {"id": "1611.06500", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Monika Henzinger, Mikkel Thorup", "title": "Incremental Exact Min-Cut in Poly-logarithmic Amortized Update Time", "comments": "Extended abstract appeared in proceedings of ESA 2016", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2016.46", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic incremental algorithm for \\textit{exactly}\nmaintaining the size of a minimum cut with $\\widetilde{O}(1)$ amortized time\nper edge insertion and $O(1)$ query time. This result partially answers an open\nquestion posed by Thorup [Combinatorica 2007]. It also stays in sharp contrast\nto a polynomial conditional lower-bound for the fully-dynamic weighted minimum\ncut problem. Our algorithm is obtained by combining a recent sparsification\ntechnique of Kawarabayashi and Thorup [STOC 2015] and an exact incremental\nalgorithm of Henzinger [J. of Algorithm 1997]. We also study space-efficient\nincremental algorithms for the minimum cut problem. Concretely, we show that\nthere exists an ${O}(n\\log n/\\varepsilon^2)$ space Monte-Carlo algorithm that\ncan process a stream of edge insertions starting from an empty graph, and with\nhigh probability, the algorithm maintains a $(1+\\varepsilon)$-approximation to\nthe minimum cut. The algorithm has $\\widetilde{O}(1)$ amortized update-time and\nconstant query-time.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 11:08:49 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1611.06501", "submitter": "Micha{\\l} Pilipczuk", "authors": "Micha{\\l} Pilipczuk and Erik Jan van Leeuwen and Andreas Wiese", "title": "Approximation and parameterized algorithms for geometric independent set\n  with shrinking", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the Maximum Weight Independent Set problem for rectangles: given a\nfamily of weighted axis-parallel rectangles in the plane, find a maximum-weight\nsubset of non-overlapping rectangles. The problem is notoriously hard both in\nthe approximation and in the parameterized setting. The best known\npolynomial-time approximation algorithms achieve super-constant approximation\nratios [Chalermsook and Chuzhoy, SODA 2009; Chan and Har-Peled, Discrete &\nComp. Geometry 2012], even though there is a $(1+\\epsilon)$-approximation\nrunning in quasi-polynomial time [Adamaszek and Wiese, FOCS 2013; Chuzhoy and\nEne, FOCS 2016]. When parameterized by the target size of the solution, the\nproblem is $\\mathsf{W}[1]$-hard even in the unweighted setting [Marx, FOCS\n2007].\n  To achieve tractability, we study the following shrinking model: one is\nallowed to shrink each input rectangle by a multiplicative factor $1-\\delta$\nfor some fixed $\\delta>0$, but the performance is still compared against the\noptimal solution for the original, non-shrunk instance. We prove that in this\nregime, the problem admits an EPTAS with running time $f(\\epsilon,\\delta)\\cdot\nn^{\\mathcal{O}(1)}$, and an FPT algorithm with running time $f(k,\\delta)\\cdot\nn^{\\mathcal{O}(1)}$, in the setting where a maximum-weight solution of size at\nmost $k$ is to be computed. This improves and significantly simplifies a PTAS\ngiven earlier for this problem [Adamaszek et al., APPROX 2015], and provides\nthe first parameterized results for the shrinking model. Furthermore, we\nexplore kernelization in the shrinking model, by giving efficient kernelization\nprocedures for several variants of the problem when the input rectangles are\nsquares.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 11:15:41 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Pilipczuk", "Micha\u0142", ""], ["van Leeuwen", "Erik Jan", ""], ["Wiese", "Andreas", ""]]}, {"id": "1611.06529", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Pawe{\\l} Gawrychowski, Przemys{\\l}aw Uzna\\'nski", "title": "A note on distance labeling in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distance labeling scheme is an assignments of labels, that is binary\nstrings, to all nodes of a graph, so that the distance between any two nodes\ncan be computed from their labels and the labels are as short as possible. A\nmajor open problem is to determine the complexity of distance labeling in\nunweighted and undirected planar graphs. It is known that, in such a graph on\n$n$ nodes, some labels must consist of $\\Omega(n^{1/3})$ bits, but the best\nknown labeling scheme uses labels of length $O(\\sqrt{n}\\log n)$ [Gavoille,\nPeleg, P\\'erennes, and Raz, J. Algorithms, 2004]. We show that, in fact, labels\nof length $O(\\sqrt{n})$ are enough.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 15:32:27 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1611.06589", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Jon Kleinberg, David Parkes", "title": "Fair Division via Social Comparison", "comments": "18 pages, 3 figures, Proceedings of the 16th Conference on Autonomous\n  Agents and Multi-Agent Systems (AAMAS, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical cake cutting problem, a resource must be divided among\nagents with different utilities so that each agent believes they have received\na fair share of the resource relative to the other agents. We introduce a\nvariant of the problem in which we model an underlying social network on the\nagents with a graph, and agents only evaluate their shares relative to their\nneighbors' in the network. This formulation captures many situations in which\nit is unrealistic to assume a global view, and also exposes interesting\nphenomena in the original problem.\n  Specifically, we say an allocation is locally envy-free if no agent envies a\nneighbor's allocation and locally proportional if each agent values her own\nallocation as much as the average value of her neighbor's allocations, with the\nformer implying the latter. While global envy-freeness implies local\nenvy-freeness, global proportionality does not imply local proportionality, or\nvice versa. A general result is that for any two distinct graphs on the same\nset of nodes and an allocation, there exists a set of valuation functions such\nthat the allocation is locally proportional on one but not the other.\n  We fully characterize the set of graphs for which an oblivious single-cutter\nprotocol-- a protocol that uses a single agent to cut the cake into pieces\n--admits a bounded protocol with $O(n^2)$ query complexity for locally\nenvy-free allocations in the Robertson-Webb model. We also consider the price\nof envy-freeness, which compares the total utility of an optimal allocation to\nthe best utility of an allocation that is envy-free. We show that a lower bound\nof $\\Omega(\\sqrt{n})$ on the price of envy-freeness for global allocations in\nfact holds for local envy-freeness in any connected undirected graph. Thus,\nsparse graphs surprisingly do not provide more flexibility with respect to the\nquality of envy-free allocations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 20:42:07 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 19:16:36 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Abebe", "Rediet", ""], ["Kleinberg", "Jon", ""], ["Parkes", "David", ""]]}, {"id": "1611.06605", "submitter": "Haris Angelidakis", "authors": "Haris Angelidakis, Yury Makarychev and Vsevolod Oparin", "title": "Algorithmic and Hardness Results for the Hub Labeling Problem", "comments": "To appear in SODA17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant success in designing highly efficient algorithms\nfor distance and shortest-path queries in recent years; many of the\nstate-of-the-art algorithms use the hub labeling framework. In this paper, we\nstudy the approximability of the Hub Labeling problem. We prove a hardness of\n$\\Omega(\\log n)$ for Hub Labeling, matching known approximation guarantees. The\nhardness result applies to graphs that have multiple shortest paths between\nsome pairs of vertices. No hardness of approximation results were known\npreviously.\n  Then, we focus on graphs that have a unique shortest path between each pair\nof vertices. This is a very natural family of graphs, and much research on the\nHub Labeling problem has studied such graphs. We give an $O(\\log D)$\napproximation algorithm for graphs of diameter $D$ with unique shortest paths.\nIn particular, we get an $O(\\log \\log n)$ approximation for graphs of\npolylogarithmic diameter, while previously known algorithms gave an $O(\\log n)$\nproximation. Finally, we present a polynomial-time approximation scheme (PTAS)\nand quasi-polynomial time algorithms for Hub Labeling on trees; additionally,\nwe analyze a simple combinatorial heuristic for Hub Labeling on trees, proposed\nby Peleg in 2000. We show that this heuristic gives an approximation factor of\n2.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 22:44:12 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Angelidakis", "Haris", ""], ["Makarychev", "Yury", ""], ["Oparin", "Vsevolod", ""]]}, {"id": "1611.06615", "submitter": "Minsoo Jung", "authors": "Minsoo Jung, Sunmin Lee, Yongsub Lim, and U Kang", "title": "FURL: Fixed-memory and Uncertainty Reducing Local Triangle Counting for\n  Graph Streams", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we accurately estimate local triangles for all nodes in simple and\nmultigraph streams? Local triangle counting in a graph stream is one of the\nmost fundamental tasks in graph mining with important applications including\nanomaly detection and social network analysis. Although there have been several\nlocal triangle counting methods in a graph stream, their estimation has a large\nvariance which results in low accuracy, and they do not consider multigraph\nstreams which have duplicate edges. In this paper, we propose FURL, an accurate\nlocal triangle counting method for simple and multigraph streams. FURL improves\nthe accuracy by reducing a variance through biased estimation and handles\nduplicate edges for multigraph streams. Also, FURL handles a stream of any size\nby using a fixed amount of memory. Experimental results show that FURL\noutperforms the state-of-the-art method in accuracy and performs well in\nmultigraph streams. In addition, we report interesting patterns discovered from\nreal graphs by FURL, which include unusual local structures in a user\ncommunication network and a core-periphery structure in the Web.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 00:04:11 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 20:34:59 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Jung", "Minsoo", ""], ["Lee", "Sunmin", ""], ["Lim", "Yongsub", ""], ["Kang", "U", ""]]}, {"id": "1611.06729", "submitter": "Vincenzo Bonifaci", "authors": "Vincenzo Bonifaci", "title": "On the Convergence Time of a Natural Dynamics for Linear Programming", "comments": null, "journal-ref": "Algorithmica, 82(2):300-315, 2020", "doi": "10.1007/s00453-019-00615-3", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a system of nonlinear ordinary differential equations for the\nsolution of linear programming (LP) problems that was first proposed in the\nmathematical biology literature as a model for the foraging behavior of\nacellular slime mold Physarum polycephalum, and more recently considered as a\nmethod to solve LPs. We study the convergence time of the continuous Physarum\ndynamics in the context of the linear programming problem, and derive a new\ntime bound to approximate optimality that depends on the relative entropy\nbetween projected versions of the optimal point and of the initial point. The\nbound scales logarithmically with the LP cost coefficients and linearly with\nthe inverse of the relative accuracy, establishing the efficiency of the\ndynamics for arbitrary LP instances with positive costs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 11:28:55 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bonifaci", "Vincenzo", ""]]}, {"id": "1611.06795", "submitter": "Stefan Kratsch", "authors": "Stefan Kratsch", "title": "A randomized polynomial kernelization for Vertex Cover with a smaller\n  parameter", "comments": "Full version of ESA 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Vertex Cover problem we are given a graph $G=(V,E)$ and an integer $k$\nand have to determine whether there is a set $X\\subseteq V$ of size at most $k$\nsuch that each edge in $E$ has at least one endpoint in $X$. The problem can be\neasily solved in time $O^*(2^k)$, making it fixed-parameter tractable (FPT)\nwith respect to $k$. While the fastest known algorithm takes only time\n$O^*(1.2738^k)$, much stronger improvements have been obtained by studying\nparameters that are smaller than $k$. Apart from treewidth-related results, the\narguably best algorithm for Vertex Cover runs in time $O^*(2.3146^p)$, where\n$p=k-LP(G)$ is only the excess of the solution size $k$ over the best\nfractional vertex cover (Lokshtanov et al.\\ TALG 2014). Since $p\\leq k$ but $k$\ncannot be bounded in terms of $p$ alone, this strictly increases the range of\ntractable instances.\n  Recently, Garg and Philip (SODA 2016) greatly contributed to understanding\nthe parameterized complexity of the Vertex Cover problem. They prove that\n$2LP(G)-MM(G)$ is a lower bound for the vertex cover size of $G$, where $MM(G)$\nis the size of a largest matching of $G$, and proceed to study parameter\n$\\ell=k-(2LP(G)-MM(G))$. They give an algorithm of running time $O^*(3^\\ell)$,\nproving that Vertex Cover is FPT in $\\ell$. It can be easily observed that\n$\\ell\\leq p$ whereas $p$ cannot be bounded in terms of $\\ell$ alone. We\ncomplement the work of Garg and Philip by proving that Vertex Cover admits a\nrandomized polynomial kernelization in terms of $\\ell$, i.e., an efficient\npreprocessing to size polynomial in $\\ell$. This improves over parameter\n$p=k-LP(G)$ for which this was previously known (Kratsch and Wahlstr\\\"om FOCS\n2012).\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:16:41 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Kratsch", "Stefan", ""]]}, {"id": "1611.06815", "submitter": "Ignasi Sau", "authors": "Julien Baste, Dieter Rautenbach, Ignasi Sau", "title": "Uniquely restricted matchings and edge colorings", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matching in a graph is uniquely restricted if no other matching covers\nexactly the same set of vertices. This notion was defined by Golumbic, Hirst,\nand Lewenstein and studied in a number of articles. Our contribution is\ntwofold. We provide approximation algorithms for computing a uniquely\nrestricted matching of maximum size in some bipartite graphs. In particular, we\nachieve a ratio of $9/5$ for subcubic bipartite graphs, improving over a\n$2$-approximation algorithm proposed by Mishra. Furthermore, we study the\nuniquely restricted chromatic index of a graph, defined as the minimum number\nof uniquely restricted matchings into which its edge set can be partitioned. We\nprovide tight upper bounds in terms of the maximum degree and characterize all\nextremal graphs. Our constructive proofs yield efficient algorithms to\ndetermine the corresponding edge colorings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:50:57 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Baste", "Julien", ""], ["Rautenbach", "Dieter", ""], ["Sau", "Ignasi", ""]]}, {"id": "1611.06910", "submitter": "Mingfei Zhao", "authors": "Yang Cai, Mingfei Zhao", "title": "Simple Mechanisms for Subadditive Buyers via Duality", "comments": "Appeared in STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple and approximately revenue-optimal mechanisms in the\nmulti-item multi-bidder settings. We unify and improve all previous results, as\nwell as generalize the results to broader cases. In particular, we prove that\nthe better of the following two simple, deterministic and Dominant Strategy\nIncentive Compatible mechanisms, a sequential posted price mechanism or an\nanonymous sequential posted price mechanism with entry fee, achieves a constant\nfraction of the optimal revenue among all randomized, Bayesian Incentive\nCompatible mechanisms, when buyers' valuations are XOS over independent items.\nIf the buyers' valuations are subadditive over independent items, the\napproximation factor degrades to $O(\\log m)$, where $m$ is the number of items.\nWe obtain our results by first extending the Cai-Devanur-Weinberg duality\nframework to derive an effective benchmark of the optimal revenue for\nsubadditive bidders, and then analyzing this upper bound with new techniques.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 17:31:58 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 03:47:16 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 03:25:26 GMT"}, {"version": "v4", "created": "Mon, 26 Aug 2019 14:47:38 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Cai", "Yang", ""], ["Zhao", "Mingfei", ""]]}, {"id": "1611.06940", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng, Jakub Pachocki, Richard Peng, Sushant Sachdeva", "title": "A Framework for Analyzing Resparsification Algorithms", "comments": "This paper supersedes arXiv:1605.08194", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spectral sparsifier of a graph $G$ is a sparser graph $H$ that\napproximately preserves the quadratic form of $G$, i.e. for all vectors $x$,\n$x^T L_G x \\approx x^T L_H x$, where $L_G$ and $L_H$ denote the respective\ngraph Laplacians. Spectral sparsifiers generalize cut sparsifiers, and have\nfound many applications in designing graph algorithms. In recent years, there\nhas been interest in computing spectral sparsifiers in semi-streaming and\ndynamic settings. Natural algorithms in these settings often involve repeated\nsparsification of a graph, and accumulation of errors across these steps. We\npresent a framework for analyzing algorithms that perform repeated\nsparsifications that only incur error corresponding to a single sparsification\nstep, leading to better results for many resparsification-based algorithms. As\nan application, we show how to maintain a spectral sparsifier in the\nsemi-streaming setting: We present a simple algorithm that, for a graph $G$ on\n$n$ vertices and $m$ edges, computes a spectral sparsifier of $G$ with $O(n\n\\log n)$ edges in a single pass over $G$, using only $O(n \\log n)$ space, and\n$O(m \\log^2 n)$ total time. This improves on previous best semi-streaming\nalgorithms for both spectral and cut sparsifiers by a factor of $\\log{n}$ in\nboth space and runtime. The algorithm extends to semi-streaming row sampling\nfor general PSD matrices. We also use our framework to combine a spectral\nsparsification algorithm by Koutis with improved spanner constructions to give\na parallel algorithm for constructing $O(n\\log^2{n}\\log\\log{n})$ sized spectral\nsparsifiers in $O(m\\log^2{n}\\log\\log{n})$ time. This is the best known\ncombinatorial graph sparsification algorithm.The size of the sparsifiers is\nonly a factor $\\log{n}\\log\\log{n}$ more than ones produced by numerical\nroutines.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:44:25 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Kyng", "Rasmus", ""], ["Pachocki", "Jakub", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1611.07008", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "Fine-Grained Complexity and Conditional Hardness for Sparse Graphs", "comments": "abstract updated; There is no update to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fine-grained complexity of sparse graph problems that\ncurrently have $\\tilde{O}(mn)$ time algorithms, where m is the number of edges\nand n is the number of vertices in the input graph. This class includes several\nimportant path problems on both directed and undirected graphs, including APSP,\nMWC (minimum weight cycle), and Eccentricities, which is the problem of\ncomputing, for each vertex in the graph, the length of a longest shortest path\nstarting at that vertex.\n  We introduce the notion of a sparse reduction which preserves the sparsity of\ngraphs, and we present near linear-time sparse reductions between various pairs\nof graph problems in the $\\tilde{O}(mn)$ class. Surprisingly, very few of the\nknown nontrivial reductions between problems in the $\\tilde{O}(mn)$ class are\nsparse reductions. In the directed case, our results give a partial order on a\nlarge collection of problems in the $\\tilde{O}(mn)$ class (along with some\nequivalences). In the undirected case we give two nontrivial sparse reductions:\nfrom MWC to APSP, and from unweighted ANSC (all nodes shortest cycles) to APSP.\nThe latter reduction also gives an improved algorithm for ANSC (for dense\ngraphs).\n  We propose the MWC Conjecture, a new conditional hardness conjecture that the\nweight of a minimum weight cycle in a directed graph cannot be computed in time\npolynomially smaller than mn. Our sparse reductions for directed path problems\nin the $\\tilde{O}(mn)$ class establish that several problems in this class,\nincluding 2-SiSP (second simple shortest path), Radius, and Eccentricities, are\nMWCC hard. We also identify Eccentricities as a key problem in the\n$\\tilde{O}(mn)$ class which is simultaneously MWCC-hard, SETH-hard and\nk-DSH-hard, where SETH is the Strong Exponential Time Hypothesis, and k-DSH is\nthe hypothesis that a dominating set of size k cannot be computed in time\npolynomially smaller than n^k.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:53:21 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:24:17 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 01:51:38 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1611.07055", "submitter": "Harold Gabow", "authors": "Harold N. Gabow", "title": "A Data Structure for Nearest Common Ancestors with Linking", "comments": "A preliminary version of results in this paper appeared in Proc.1st\n  Annual ACM-SIAM Symp. on Disc. Algorithms (SODA), 1990", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a forest that evolves via $link$ operations that make the root of\none tree the child of a node in another tree. Intermixed with $link$ operations\nare $nca$ operations, which return the nearest common ancestor of two given\nnodes when such exists. This paper shows that a sequence of $m$ such $nca$ and\n$link$ operations on a forest of $n$ nodes can be processed on-line in time\n$O(m\\alpha(m,n)+n)$. This was previously known only for a restricted type of\n$link$ operation. The special case where a $link$ only extends a tree by adding\na new leaf occurs in Edmonds' algorithm for finding a maximum weight matching\non a general graph. Incorporating our algorithm into the implementation of\nEdmonds' algorithm in \\cite{G17} achieves time $O(n(m + n\\log n))$ for weighted\nmatching, an arguably optimum asymptotic bound ($n$ and $m$ are the number of\nvertices and edges, respectively).\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 21:10:43 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Gabow", "Harold N.", ""]]}, {"id": "1611.07138", "submitter": "Patrick Rebeschini", "authors": "Patrick Rebeschini and Sekhar Tatikonda", "title": "A New Approach to Laplacian Solvers and Flow Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the behavior of the Min-Sum message passing scheme to\nsolve systems of linear equations in the Laplacian matrices of graphs and to\ncompute electric flows. Voltage and flow problems involve the minimization of\nquadratic functions and are fundamental primitives that arise in several\ndomains. Algorithms that have been proposed are typically centralized and\ninvolve multiple graph-theoretic constructions or sampling mechanisms that make\nthem difficult to implement and analyze. On the other hand, message passing\nroutines are distributed, simple, and easy to implement. In this paper we\nestablish a framework to analyze Min-Sum to solve voltage and flow problems. We\ncharacterize the error committed by the algorithm on general weighted graphs in\nterms of hitting times of random walks defined on the computation trees that\nsupport the operations of the algorithms with time. For $d$-regular graphs with\nequal weights, we show that the convergence of the algorithms is controlled by\nthe total variation distance between the distributions of non-backtracking\nrandom walks defined on the original graph that start from neighboring nodes.\nThe framework that we introduce extends the analysis of Min-Sum to settings\nwhere the contraction arguments previously considered in the literature (based\non the assumption of walk summability or scaled diagonal dominance) can not be\nused, possibly in the presence of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:39:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 12:44:30 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Rebeschini", "Patrick", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1611.07169", "submitter": "David Kempe", "authors": "David Kempe, Leonard J. Schulman, Omer Tamuz", "title": "Quasi-regular sequences and optimal schedules for security games", "comments": "to appear in Proc. of SODA 2018", "journal-ref": null, "doi": "10.1137/1.9781611975031.106", "report-no": null, "categories": "cs.GT cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study security games in which a defender commits to a mixed strategy for\nprotecting a finite set of targets of different values. An attacker, knowing\nthe defender's strategy, chooses which target to attack and for how long. If\nthe attacker spends time $t$ at a target $i$ of value $\\alpha_i$, and if he\nleaves before the defender visits the target, his utility is $t \\cdot \\alpha_i\n$; if the defender visits before he leaves, his utility is 0. The defender's\ngoal is to minimize the attacker's utility. The defender's strategy consists of\na schedule for visiting the targets; it takes her unit time to switch between\ntargets. Such games are a simplified model of a number of real-world scenarios\nsuch as protecting computer networks from intruders, crops from thieves, etc.\n  We show that optimal defender play for this continuous time security games\nreduces to the solution of a combinatorial question regarding the existence of\ninfinite sequences over a finite alphabet, with the following properties for\neach symbol $i$: (1) $i$ constitutes a prescribed fraction $p_i$ of the\nsequence. (2) The occurrences of $i$ are spread apart close to evenly, in that\nthe ratio of the longest to shortest interval between consecutive occurrences\nis bounded by a parameter $K$. We call such sequences $K$-quasi-regular.\n  We show that, surprisingly, $2$-quasi-regular sequences suffice for optimal\ndefender play. What is more, even randomized $2$-quasi-regular sequences\nsuffice for optimality. We show that such sequences always exist, and can be\ncalculated efficiently.\n  The question of the least $K$ for which deterministic $K$-quasi-regular\nsequences exist is fascinating. Using an ergodic theoretical approach, we show\nthat deterministic $3$-quasi-regular sequences always exist. For $2 \\leq K < 3$\nwe do not know whether deterministic $K$-quasi-regular sequences always exist.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 07:16:22 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 07:43:48 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 22:11:49 GMT"}, {"version": "v4", "created": "Mon, 20 Feb 2017 03:07:34 GMT"}, {"version": "v5", "created": "Fri, 14 Jul 2017 03:35:22 GMT"}, {"version": "v6", "created": "Sun, 29 Oct 2017 00:29:29 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Kempe", "David", ""], ["Schulman", "Leonard J.", ""], ["Tamuz", "Omer", ""]]}, {"id": "1611.07191", "submitter": "Nan Hu", "authors": "Nan Hu, Qixing Huang, Boris Thibert, Leonidas Guibas", "title": "Distributable Consistent Multi-Object Matching", "comments": "Final version for CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an optimization-based framework to multiple object\nmatching. The framework takes maps computed between pairs of objects as input,\nand outputs maps that are consistent among all pairs of objects. The central\nidea of our approach is to divide the input object collection into overlapping\nsub-collections and enforce map consistency among each sub-collection. This\nleads to a distributed formulation, which is scalable to large-scale datasets.\nWe also present an equivalence condition between this decoupled scheme and the\noriginal scheme. Experiments on both synthetic and real-world datasets show\nthat our framework is competitive against state-of-the-art multi-object\nmatching techniques.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 08:21:38 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 01:41:47 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 18:57:46 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Hu", "Nan", ""], ["Huang", "Qixing", ""], ["Thibert", "Boris", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1611.07303", "submitter": "Johan von Tangen Sivertsen M.Sc", "authors": "Rasmus Pagh, Francesco Silvestri, Johan Sivertsen and Matthew Skala", "title": "Approximate Furthest Neighbor with Application to Annulus Query", "comments": null, "journal-ref": "Information Systems, Available online 22 July 2016, ISSN 0306-4379", "doi": "10.1016/j.is.2016.07.006", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Much recent work has been devoted to approximate nearest neighbor queries.\nMotivated by applications in recommender systems, we consider approximate\nfurthest neighbor (AFN) queries and present a simple, fast, and highly\npractical data structure for answering AFN queries in high- dimensional\nEuclidean space. The method builds on the technique of In- dyk (SODA 2003),\nstoring random projections to provide sublinear query time for AFN. However, we\nintroduce a different query algorithm, improving on Indyk's approximation\nfactor and reducing the running time by a logarithmic factor. We also present a\nvariation based on a query- independent ordering of the database points; while\nthis does not have the provable approximation factor of the query-dependent\ndata structure, it offers significant improvement in time and space complexity.\nWe give a theoretical analysis, and experimental results. As an application,\nthe query-dependent approach is used for deriving a data structure for the\napproximate annulus query problem, which is defined as follows: given an input\nset S and two parameters r > 0 and w >= 1, construct a data structure that\nreturns for each query point q a point p in S such that the distance between p\nand q is at least r/w and at most wr.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 14:06:14 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""], ["Sivertsen", "Johan", ""], ["Skala", "Matthew", ""]]}, {"id": "1611.07305", "submitter": "Nate Veldt", "authors": "Nate Veldt and Anthony Wirth and David F. Gleich", "title": "Correlation Clustering with Low-Rank Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation clustering is a technique for aggregating data based on\nqualitative information about which pairs of objects are labeled 'similar' or\n'dissimilar.' Because the optimization problem is NP-hard, much of the previous\nliterature focuses on finding approximation algorithms. In this paper we\nexplore how to solve the correlation clustering objective exactly when the data\nto be clustered can be represented by a low-rank matrix. We prove in particular\nthat correlation clustering can be solved in polynomial time when the\nunderlying matrix is positive semidefinite with small constant rank, but that\nthe task remains NP-hard in the presence of even one negative eigenvalue. Based\non our theoretical results, we develop an algorithm for efficiently \"solving\"\nlow-rank positive semidefinite correlation clustering by employing a procedure\nfor zonotope vertex enumeration. We demonstrate the effectiveness and speed of\nour algorithm by using it to solve several clustering problems on both\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:56:29 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 16:38:09 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Veldt", "Nate", ""], ["Wirth", "Anthony", ""], ["Gleich", "David F.", ""]]}, {"id": "1611.07336", "submitter": "Sumit Kumar Jha", "authors": "Sumit Kumar Jha", "title": "Asymptotic expansions for factorial moments of some distributions in the\n  analysis of algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish asymptotic expansions for factorial moments of following\ndistributions: number of cycles in a random permutation, number of inversions\nin a random permutation, and number of comparisons used by the randomized quick\nsort algorithm.To achieve this we use singularity analysis of certain type of\ngenerating functions due to Flajolet and Odlyzko.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 14:54:11 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Jha", "Sumit Kumar", ""]]}, {"id": "1611.07371", "submitter": "Chidambaram Annamalai", "authors": "Chidambaram Annamalai", "title": "Lazy Local Search Meets Machine Scheduling", "comments": "added 4 figures; sharpened the constant from 1+2/sqrt{5} to 17/9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the restricted case of Scheduling on Unrelated Parallel Machines. In\nthis problem, we are given a set of jobs $J$ with processing times $p_j$ and\neach job may be scheduled only on some subset of machines $S_j \\subseteq M$.\nThe goal is to find an assignment of jobs to machines to minimize the time by\nwhich all jobs can be processed. In a seminal paper, Lenstra, Shmoys, and\nTardos designed an elegant $2$-approximation for the problem in 1987. The\nquestion of whether approximation algorithms with better guarantees exist for\nthis classic scheduling problem has since remained a source of mystery.\n  In recent years, with the improvement of our understanding of Configuration\nLPs, it now appears an attainable goal to design such an algorithm. Our main\ncontribution is to make progress towards this goal. When the processing times\nof jobs are either $1$ or $\\epsilon \\in (0,1)$, we design an approximation\nalgorithm whose guarantee tends to $1 + \\sqrt{3}/2 \\approx 1.8660254,$ for the\ninteresting cases when $\\epsilon \\to 0$. This improves on the $2-\\epsilon_0$\nguarantee recently obtained by Chakrabarty, Khanna, and Li for some constant\n$\\epsilon_0 > 0$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:43:09 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 17:26:01 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Annamalai", "Chidambaram", ""]]}, {"id": "1611.07414", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty and Ravishankar Krishnaswamy and Amit Kumar", "title": "The Heterogeneous Capacitated $k$-Center Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we initiate the study of the heterogeneous capacitated\n$k$-center problem: given a metric space $X = (F \\cup C, d)$, and a collection\nof capacities. The goal is to open each capacity at a unique facility location\nin $F$, and also to assign clients to facilities so that the number of clients\nassigned to any facility is at most the capacity installed; the objective is\nthen to minimize the maximum distance between a client and its assigned\nfacility. If all the capacities $c_i$'s are identical, the problem becomes the\nwell-studied uniform capacitated $k$-center problem for which constant-factor\napproximations are known. The additional choice of determining which capacity\nshould be installed in which location makes our problem considerably different\nfrom this problem, as well the non-uniform generalizations studied thus far in\nliterature. In fact, one of our contributions is in relating the heterogeneous\nproblem to special-cases of the classical Santa Claus problem. Using this\nconnection, and by designing new algorithms for these special cases, we get the\nfollowing results: (a)A quasi-polynomial time $O(\\log\nn/\\epsilon)$-approximation where every capacity is violated by $1+\\varepsilon$,\n(b) A polynomial time $O(1)$-approximation where every capacity is violated by\nan $O(\\log n)$ factor. We get improved results for the {\\em soft-capacities}\nversion where we can place multiple facilities in the same location.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:57:49 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Krishnaswamy", "Ravishankar", ""], ["Kumar", "Amit", ""]]}, {"id": "1611.07451", "submitter": "Rasmus J Kyng", "authors": "David Durfee, Rasmus Kyng, John Peebles, Anup B. Rao, Sushant Sachdeva", "title": "Sampling Random Spanning Trees Faster than Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that, with high probability, generates a random\nspanning tree from an edge-weighted undirected graph in\n$\\tilde{O}(n^{4/3}m^{1/2}+n^{2})$ time (The $\\tilde{O}(\\cdot)$ notation hides\n$\\operatorname{polylog}(n)$ factors). The tree is sampled from a distribution\nwhere the probability of each tree is proportional to the product of its edge\nweights. This improves upon the previous best algorithm due to Colbourn et al.\nthat runs in matrix multiplication time, $O(n^\\omega)$. For the special case of\nunweighted graphs, this improves upon the best previously known running time of\n$\\tilde{O}(\\min\\{n^{\\omega},m\\sqrt{n},m^{4/3}\\})$ for $m \\gg n^{5/3}$ (Colbourn\net al. '96, Kelner-Madry '09, Madry et al. '15).\n  The effective resistance metric is essential to our algorithm, as in the work\nof Madry et al., but we eschew determinant-based and random walk-based\ntechniques used by previous algorithms. Instead, our algorithm is based on\nGaussian elimination, and the fact that effective resistance is preserved in\nthe graph resulting from eliminating a subset of vertices (called a Schur\ncomplement). As part of our algorithm, we show how to compute\n$\\epsilon$-approximate effective resistances for a set $S$ of vertex pairs via\napproximate Schur complements in $\\tilde{O}(m+(n + |S|)\\epsilon^{-2})$ time,\nwithout using the Johnson-Lindenstrauss lemma which requires $\\tilde{O}(\n\\min\\{(m + |S|)\\epsilon^{-2}, m+n\\epsilon^{-4} +|S|\\epsilon^{-2}\\})$ time. We\ncombine this approximation procedure with an error correction procedure for\nhanding edges where our estimate isn't sufficiently accurate.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 18:35:03 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 20:11:43 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Durfee", "David", ""], ["Kyng", "Rasmus", ""], ["Peebles", "John", ""], ["Rao", "Anup B.", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1611.07466", "submitter": "Laura Eslava", "authors": "Laura Eslava", "title": "Depth of vertices with high degree in random recursive trees", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T_n$ be a random recursive tree with $n$ nodes. List vertices of $T_n$\nin decreasing order of degree as $v^1,\\ldots,v^n$, and write $d^i$ and $h^i$\nfor the degree of $v^i$ and the distance of $v^i$ from the root, respectively.\nWe prove that, as $n \\to \\infty$ along suitable subsequences, \\[ \\bigg(d^i -\n\\lfloor \\log_2 n \\rfloor, \\frac{h^i - \\mu\\ln n}{\\sqrt{\\sigma^2\\ln n}}\\bigg) \\to\n((P_i,i \\ge 1),(N_i,i \\ge 1))\\, , \\] where $\\mu=1-(\\log_2 e)/2$,\n$\\sigma^2=1-(\\log_2 e)/4$, $(P_i,i \\ge 1)$ is a Poisson point process on\n$\\mathbb{Z}$ and $(N_i,i \\ge 1)$ is a vector of independent standard Gaussians.\nWe additionally establish joint normality for the depths of uniformly random\nvertices in $T_n$, which extends results for the case of a single random\nvertex. The joint limit holds even if the random vertices are conditioned to\nhave large degree, provided the normalizing constants are adjusted accordingly;\nhowever, both the mean and variance of the conditinal depths remain of orden\n$\\ln n$.\n  Our results are based on a simple relationship between random recursive trees\nand Kingman's $n$-coalescent; a utility that seems to have been largely\noverlooked.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 19:10:07 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 21:06:50 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Eslava", "Laura", ""]]}, {"id": "1611.07489", "submitter": "Shikha Singh", "authors": "Eric Angel (1), Nguyen Kim Thang (1), Shikha Singh (2) ((1) IBISC,\n  University d'Evry Val d'Essonne, France (2) Stony Brook University, Stony\n  Brook, NY, USA)", "title": "Approximating k-Forest with Resource Augmentation: A Primal-Dual\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the $k$-forest problem in the model of resource\naugmentation. In the $k$-forest problem, given an edge-weighted graph $G(V,E)$,\na parameter $k$, and a set of $m$ demand pairs $\\subseteq V \\times V$, the\nobjective is to construct a minimum-cost subgraph that connects at least $k$\ndemands. The problem is hard to approximate---the best-known approximation\nratio is $O(\\min\\{\\sqrt{n}, \\sqrt{k}\\})$. Furthermore, $k$-forest is as hard to\napproximate as the notoriously-hard densest $k$-subgraph problem.\n  While the $k$-forest problem is hard to approximate in the worst-case, we\nshow that with the use of resource augmentation, we can efficiently approximate\nit up to a constant factor.\n  First, we restate the problem in terms of the number of demands that are {\\em\nnot} connected. In particular, the objective of the $k$-forest problem can be\nviewed as to remove at most $m-k$ demands and find a minimum-cost subgraph that\nconnects the remaining demands. We use this perspective of the problem to\nexplain the performance of our algorithm (in terms of the augmentation) in a\nmore intuitive way.\n  Specifically, we present a polynomial-time algorithm for the $k$-forest\nproblem that, for every $\\epsilon>0$, removes at most $m-k$ demands and has\ncost no more than $O(1/\\epsilon^{2})$ times the cost of an optimal algorithm\nthat removes at most $(1-\\epsilon)(m-k)$ demands.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 19:54:16 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Angel", "Eric", ""], ["Thang", "Nguyen Kim", ""], ["Singh", "Shikha", ""]]}, {"id": "1611.07503", "submitter": "Michael Anastos Mr", "authors": "Michael Anastos", "title": "Purchasing a C_4 online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph with edge set $(e_1,e_2,...e_N)$. We independently\nassociate to each edge $e_i$ of $G$ a cost ${x}_i$ that is drawn from a Uniform\n[0, 1] distribution. Suppose $\\mathcal{F}$ is a set of targeted structures that\nconsists of subgraphs of $G$. We would like to buy a subset of $\\mathcal{F}$ at\nsmall cost, however we do not know a priori the values of the random variables\n${x}_1,...,{x}_N$. Instead, we inspect the random variables $x_i$ one at a\ntime. As soon as we inspect the random variable associated with the cost of an\nedge we have to decide whether we want to buy that edge or reject it for ever.\n  In the present paper we consider the case where $G$ is the complete graph on\n$n$ vertices and $\\mathcal{F}$ is the set of all $C_4$ -cycles on 4 vertices-\nout of which we want to buy one.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 20:32:18 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Anastos", "Michael", ""]]}, {"id": "1611.07541", "submitter": "Harold Gabow", "authors": "Harold N. Gabow", "title": "Data Structures for Weighted Matching and Extensions to $b$-matching and\n  $f$-factors", "comments": "This paper is a combination of two conference papers: A preliminary\n  version of the data structures part appeared in Proc. 1st Annual ACM-SIAM\n  Symp. on Disc. Algorithms (SODA), 1990. A preliminary version of the\n  extensions part, based on reduction to matching, appeared in Proc. 15th\n  Annual ACM Symp. on Theory of Comp. (STOC), 1983", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows the weighted matching problem on general graphs can be\nsolved in time $O(n(m + n\\log n))$ for $n$ and $m$ the number of vertices and\nedges, respectively. This was previously known only for bipartite graphs. The\ncrux is a data structure for blossom creation. It uses a dynamic\nnearest-common-ancestor algorithm to simplify blossom steps, so they involve\nonly back edges rather than arbitrary nontree edges.\n  The rest of the paper presents direct extensions of Edmonds' blossom\nalgorithm to weighted $b$-matching and $f$-factors. Again the time bound is the\none previously known for bipartite graphs: for $b$-matching the time is\n$O(\\min\\{b(V),n\\log n\\}(m + n\\log n))$ and for $f$-factors the time is\n$O(\\min\\{f(V),m\\log n\\}( m + n\\log n) )$, where $b(V)$ and $f(V)$ denote the\nsum of all degree constraints. Several immediate applications of the $f$-factor\nalgorithm are given: The generalized shortest path structure of \\cite{GS13},\ni.e., the analog of the shortest path tree for conservative undirected graphs,\nis shown to be a version of the blossom structure for $f$-factors. This\nstructure is found in time $O(|N|(m+n\\log n))$ for $N$ the set of negative\nedges ($0<|N|<n$). A shortest $T$-join is found in time $O(n(m+n\\log n))$, or\n$O(|T|(m+n\\log n))$ when all costs are nonnegative. These bounds are all slight\nimprovements of previously known ones, and are simply achieved by proper\ninitialization of the $f$-factor algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 21:22:33 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Gabow", "Harold N.", ""]]}, {"id": "1611.07599", "submitter": "Jia Zhang", "authors": "Jia Zhang, Zheng Wang, Qian Li, Jialin Zhang, Yanyan Lan, Qiang Li and\n  Xiaoming Sun", "title": "Efficient Delivery Policy to Minimize User Traffic Consumption in\n  Guaranteed Advertising", "comments": "Already accepted by AAAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the guaranteed delivery model which is widely used in\nonline display advertising. In the guaranteed delivery scenario, ad exposures\n(which are also called impressions in some works) to users are guaranteed by\ncontracts signed in advance between advertisers and publishers. A crucial\nproblem for the advertising platform is how to fully utilize the valuable user\ntraffic to generate as much as possible revenue.\n  Different from previous works which usually minimize the penalty of\nunsatisfied contracts and some other cost (e.g. representativeness), we propose\nthe novel consumption minimization model, in which the primary objective is to\nminimize the user traffic consumed to satisfy all contracts. Under this model,\nwe develop a near optimal method to deliver ads for users. The main advantage\nof our method lies in that it consumes nearly as least as possible user traffic\nto satisfy all contracts, therefore more contracts can be accepted to produce\nmore revenue. It also enables the publishers to estimate how much user traffic\nis redundant or short so that they can sell or buy this part of traffic in bulk\nin the exchange market. Furthermore, it is robust with regard to priori\nknowledge of user type distribution. Finally, the simulation shows that our\nmethod outperforms the traditional state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 01:55:36 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Zhang", "Jia", ""], ["Wang", "Zheng", ""], ["Li", "Qian", ""], ["Zhang", "Jialin", ""], ["Lan", "Yanyan", ""], ["Li", "Qiang", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1611.07605", "submitter": "Takanori Maehara", "authors": "Takanori Maehara, Yasushi Kawase, Hanna Sumita, Katsuya Tono, and\n  Ken-ichi Kawarabayashi", "title": "Optimal Pricing for Submodular Valuations with Bounded Curvature", "comments": "Full paper version of our AAAI'17 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal pricing problem is a fundamental problem that arises in\ncombinatorial auctions. Suppose that there is one seller who has indivisible\nitems and multiple buyers who want to purchase a combination of the items. The\nseller wants to sell his items for the highest possible prices, and each buyer\nwants to maximize his utility (i.e., valuation minus payment) as long as his\npayment does not exceed his budget. The optimal pricing problem seeks a price\nof each item and an assignment of items to buyers such that every buyer\nachieves the maximum utility under the prices. The goal of the problem is to\nmaximize the total payment from buyers. In this paper, we consider the case\nthat the valuations are submodular. We show that the problem is computationally\nhard even if there exists only one buyer. Then we propose approximation\nalgorithms for the unlimited budget case. We also extend the algorithm for the\nlimited budget case when there exists one buyer and multiple buyers collaborate\nwith each other.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 02:18:10 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Maehara", "Takanori", ""], ["Kawase", "Yasushi", ""], ["Sumita", "Hanna", ""], ["Tono", "Katsuya", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1611.07612", "submitter": "Daniel Lemire", "authors": "Wojciech Mu{\\l}a, Nathan Kurz and Daniel Lemire", "title": "Faster Population Counts Using AVX2 Instructions", "comments": "Software is at https://github.com/CountOnes/hamming_weight", "journal-ref": "Computer Journal, Volume 61, Issue 1, 1 January 2018", "doi": "10.1093/comjnl/bxx046", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counting the number of ones in a binary stream is a common operation in\ndatabase, information-retrieval, cryptographic and machine-learning\napplications. Most processors have dedicated instructions to count the number\nof ones in a word (e.g., popcnt on x64 processors). Maybe surprisingly, we show\nthat a vectorized approach using SIMD instructions can be twice as fast as\nusing the dedicated instructions on recent Intel processors. The benefits can\nbe even greater for applications such as similarity measures (e.g., the Jaccard\nindex) that require additional Boolean operations. Our approach has been\nadopted by LLVM: it is used by its popular C compiler (clang).\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 02:47:50 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 19:41:35 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 23:45:23 GMT"}, {"version": "v4", "created": "Mon, 19 Dec 2016 19:00:25 GMT"}, {"version": "v5", "created": "Mon, 20 Feb 2017 22:40:17 GMT"}, {"version": "v6", "created": "Mon, 24 Apr 2017 00:26:52 GMT"}, {"version": "v7", "created": "Fri, 26 May 2017 13:02:33 GMT"}, {"version": "v8", "created": "Fri, 13 Oct 2017 00:56:52 GMT"}, {"version": "v9", "created": "Wed, 5 Sep 2018 19:35:03 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Mu\u0142a", "Wojciech", ""], ["Kurz", "Nathan", ""], ["Lemire", "Daniel", ""]]}, {"id": "1611.07622", "submitter": "EPTCS", "authors": "Shahar Maoz (Tel Aviv University), Or Pistiner (Tel Aviv University),\n  Jan Oliver Ringert (Tel Aviv University)", "title": "Symbolic BDD and ADD Algorithms for Energy Games", "comments": "In Proceedings SYNT 2016, arXiv:1611.07178", "journal-ref": "EPTCS 229, 2016, pp. 35-54", "doi": "10.4204/EPTCS.229.5", "report-no": null, "categories": "cs.LO cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy games, which model quantitative consumption of a limited resource,\ne.g., time or energy, play a central role in quantitative models for reactive\nsystems. Reactive synthesis constructs a controller which satisfies a given\nspecification, if one exists. For energy games a synthesized controller ensures\nto satisfy not only the safety constraints of the specification but also the\nquantitative constraints expressed in the energy game. A symbolic algorithm for\nenergy games, recently presented by Chatterjee et al., is symbolic in its\nrepresentation of quantitative values but concrete in the representation of\ngame states and transitions. In this paper we present an algorithm that is\nsymbolic both in the quantitative values and in the underlying game\nrepresentation. We have implemented our algorithm using two different symbolic\nrepresentations for reactive games, Binary Decision Diagrams (BDD) and\nAlgebraic Decision Diagrams (ADD). We investigate the commonalities and\ndifferences of the two implementations and compare their running times on\nspecifications of energy games.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 03:16:27 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Maoz", "Shahar", "", "Tel Aviv University"], ["Pistiner", "Or", "", "Tel Aviv University"], ["Ringert", "Jan Oliver", "", "Tel Aviv University"]]}, {"id": "1611.07682", "submitter": "Renata Sotirov", "authors": "Hao Hu and Renata Sotirov", "title": "Special cases of the quadratic shortest path problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quadratic shortest path problem (QSPP) is \\textcolor{black}{the problem\nof finding a path with prespecified start vertex $s$ and end vertex $t$ in a\ndigraph} such that the sum of weights of arcs and the sum of interaction costs\nover all pairs of arcs on the path is minimized. We first consider a variant of\nthe QSPP known as the adjacent QSPP. It was recently proven that the adjacent\nQSPP on cyclic digraphs cannot be approximated unless P=NP. Here, we give a\nsimple proof for the same result.\n  We also show that if the quadratic cost matrix is a symmetric weak sum matrix\n\\textcolor{black}{ and all $s$-$t$ paths have the same length,} then an optimal\nsolution for the QSPP can be obtained by solving the corresponding instance of\nthe shortest path problem. Similarly, it is shown that the QSPP with a\nsymmetric product cost matrix is solvable in polynomial time.\n  Further, we provide sufficient and necessary conditions for a QSPP instance\non a complete symmetric digraph with four vertices to be linearizable. We also\ncharacterize linearizable QSPP instances on complete symmetric digraphs with\nmore than four vertices. Finally, we derive an algorithm that examines whether\na QSPP instance on the directed grid graph $G_{pq}$ ($p,q\\geq 2$) is\nlinearizable. The complexity of this algorithm is\n${\\mathcal{O}(p^{3}q^{2}+p^{2}q^{3})}$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 08:28:24 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 08:20:13 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Hu", "Hao", ""], ["Sotirov", "Renata", ""]]}, {"id": "1611.07701", "submitter": "Fahad Panolan", "authors": "Akanksha Agrawal, Fahad Panolan, Saket Saurabh, Meirav Zehavi", "title": "Simultaneous Feedback Edge Set: A Parameterized Perspective", "comments": "A preliminary version of this paper will appear in the proceedings of\n  ISAAC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider Simultaneous Feedback Edge Set (Sim-FES) problem.\nIn this problem, the input is an $n$-vertex graph $G$, an integer $k$ and a\ncoloring function ${\\sf col}: E(G) \\rightarrow 2^{[\\alpha]}$ and the objective\nis to check whether there is an edge subset $S$ of cardinality at most $k$ in\n$G$ such that for all $i \\in [\\alpha]$, $G_i - S$ is acyclic. Here, $G_i=(V(G),\n\\{e\\in E(G) \\mid i \\in {\\sf col}(e)\\})$ and $[\\alpha]=\\{1,\\ldots,\\alpha\\}$.\nWhen $\\alpha =1$, the problem is polynomial time solvable. We show that for\n$\\alpha =3$ Sim-FES is NP-hard by giving a reduction from Vertex Cover on cubic\ngraphs. The same reduction shows that the problem does not admit an algorithm\nof running time $O(2^{o(k)}n^{O(1)})$ unless ETH fails. This hardness result is\ncomplimented by an FPT algorithm for Sim-FES running in time $O(2^{\\omega\nk\\alpha+\\alpha \\log k} n^{O(1)})$, where $\\omega$ is the exponent in the\nrunning time of matrix multiplication. The same algorithm gives a polynomial\ntime algorithm for the case when $\\alpha =2$. We also give a kernel for Sim-FES\nwith $(k\\alpha)^{O(\\alpha)}$ vertices. Finally, we consider the problem Maximum\nSimultaneous Acyclic Subgraph. Here, the input is a graph $G$, an integer $q$\nand, a coloring function ${\\sf col}: E(G) \\rightarrow 2^{[\\alpha]}$. The\nquestion is whether there is a edge subset $F$ of cardinality at least $q$ in\n$G$ such that for all $i\\in [\\alpha]$, $G[F_i]$ is acyclic. Here, $F_i=\\{e \\in\nF \\mid i \\in \\textsf{col}(e)\\}$. We give an FPT algorithm for running in time\n$O(2^{\\omega q \\alpha}n^{O(1)})$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 09:32:46 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1611.07724", "submitter": "Frank Gurski", "authors": "Carolin Albrecht, Frank Gurski, Jochen Rethmann, Eda Yilmaz", "title": "Knapsack Problems: A Parameterized Point of View", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knapsack problem (KP) is a very famous NP-hard problem in combinatorial\noptimization. Also its generalization to multiple dimensions named\nd-dimensional knapsack problem (d-KP) and to multiple knapsacks named multiple\nknapsack problem (MKP) are well known problems. Since KP, d-KP, and MKP are\ninteger-valued problems defined on inputs of various informations, we study the\nfixed-parameter tractability of these problems. The idea behind fixed-parameter\ntractability is to split the complexity into two parts - one part that depends\npurely on the size of the input, and one part that depends on some parameter of\nthe problem that tends to be small in practice. Further we consider the closely\nrelated question, whether the sizes and the values can be reduced, such that\ntheir bit-length is bounded polynomially or even constantly in a given\nparameter, i.e. the existence of kernelizations is studied. We discuss the\nfollowing parameters: the number of items, the threshold value for the profit,\nthe sizes, the profits, the number d of dimensions, and the number m of\nknapsacks. We also consider the connection of parameterized knapsack problems\nto linear programming, approximation, and pseudo-polynomial algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:23:46 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Albrecht", "Carolin", ""], ["Gurski", "Frank", ""], ["Rethmann", "Jochen", ""], ["Yilmaz", "Eda", ""]]}, {"id": "1611.07745", "submitter": "Seeun William Umboh", "authors": "Shuchi Chawla, Joseph (Seffi) Naor, Debmalya Panigrahi, Mohit Singh,\n  Seeun William Umboh", "title": "Timing Matters: Online Dynamics in Broadcast Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in algorithmic game theory is to measure the inefficiency\n(ratio of costs) of Nash equilibria (NE) with respect to socially optimal\nsolutions. The two established metrics used for this purpose are price of\nanarchy (POA) and price of stability (POS), which respectively provide upper\nand lower bounds on this ratio. A deficiency of these metrics, however, is that\nthey are purely existential and shed no light on which of the equilibrium\nstates are reachable in an actual game, i.e., via natural game dynamics. This\nis particularly striking if these metrics differ significantly, such as in\nnetwork design games where the exponential gap between the best and worst NE\nstates originally prompted the notion of POS in game theory (Anshelevich et\nal., FOCS 2002). In this paper, we make progress toward bridging this gap by\nstudying network design games under natural game dynamics.\n  First we show that in a completely decentralized setting, where agents\narrive, depart, and make improving moves in an arbitrary order, the\ninefficiency of NE attained can be polynomially large. This implies that the\ngame designer must have some control over the interleaving of these events in\norder to force the game to attain efficient NE. We complement our negative\nresult by showing that if the game designer is allowed to execute a sequence of\nimproving moves to create an equilibrium state after every batch of agent\narrivals or departures, then the resulting equilibrium states attained by the\ngame are exponentially more efficient, i.e., the ratio of costs compared to the\noptimum is only logarithmic. Overall, our two results establish that in network\ngames, the efficiency of equilibrium states is dictated by whether agents are\nallowed to join or leave the game in arbitrary states, an observation that\nmight be useful in analyzing the dynamics of other classes of games with\ndivergent POS and POA bounds.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 11:18:20 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Chawla", "Shuchi", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""], ["Panigrahi", "Debmalya", ""], ["Singh", "Mohit", ""], ["Umboh", "Seeun William", ""]]}, {"id": "1611.07786", "submitter": "Avishek Anand", "authors": "Megha Khosla and Avishek Anand", "title": "A Faster Algorithm for Cuckoo Insertion and Bipartite Matching in Large\n  Graphs", "comments": "Preprint; Accepted in Algorithmica 2019", "journal-ref": "Algorithmica 2019", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are ubiquitous in computer science for efficient access to large\ndatasets. However, there is always a need for approaches that offer compact\nmemory utilisation without substantial degradation of lookup performance.\nCuckoo hashing is an efficient technique of creating hash tables with high\nspace utilisation and offer a guaranteed constant access time. We are given $n$\nlocations and $m$ items. Each item has to be placed in one of the $k\\ge2$\nlocations chosen by $k$ random hash functions. By allowing more than one choice\nfor a single item, cuckoo hashing resembles multiple choice allocations\nschemes. In addition it supports dynamically changing the location of an item\namong its possible locations. We propose and analyse an insertion algorithm for\ncuckoo hashing that runs in \\emph{linear time} with high probability and in\nexpectation. Previous work on total allocation time has analysed breadth first\nsearch, and it was shown to be linear only in \\emph{expectation}. Our algorithm\nfinds an assignment (with probability 1) whenever it exists. In contrast, the\nother known insertion method, known as \\emph{random walk insertion}, may run\nindefinitely even for a solvable instance. We also present experimental results\ncomparing the performance of our algorithm with the random walk method, also\nfor the case when each location can hold more than one item.\n  As a corollary we obtain a linear time algorithm (with high probability and\nin expectation) for finding perfect matchings in a special class of sparse\nrandom bipartite graphs. We support this by performing experiments on a real\nworld large dataset for finding maximum matchings in general large bipartite\ngraphs. We report an order of magnitude improvement in the running time as\ncompared to the \\emph{Hopkraft-Karp} matching algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 13:38:07 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 17:22:44 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Khosla", "Megha", ""], ["Anand", "Avishek", ""]]}, {"id": "1611.07866", "submitter": "Michael Dinitz", "authors": "Eden Chlamt\\'a\\v{c}, Michael Dinitz, Yury Makarychev", "title": "Minimizing the Union: Tight Approximations for Small Set Bipartite\n  Vertex Expansion", "comments": "To appear in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Minimum k-Union problem (MkU) we are given a set system with n sets\nand are asked to select k sets in order to minimize the size of their union.\nDespite being a very natural problem, it has received surprisingly little\nattention: the only known approximation algorithm is an\n$O(\\sqrt{n})$-approximation due to [Chlamt\\'a\\v{c} et al APPROX '16]. This\nproblem can also be viewed as the bipartite version of the Small Set Vertex\nExpansion problem (SSVE), which we call the Small Set Bipartite Vertex\nExpansion problem (SSBVE). SSVE, in which we are asked to find a set of k nodes\nto minimize their vertex expansion, has not been as well studied as its\nedge-based counterpart Small Set Expansion (SSE), but has recently received\nsignificant attention, e.g. [Louis-Makarychev APPROX '15]. However, due to the\nconnection to Unique Games and hardness of approximation the focus has mostly\nbeen on sets of size $k = \\Omega(n)$, while we focus on the case of general\n$k$, for which no polylogarithmic approximation is known.\n  We improve the upper bound for this problem by giving an\n$n^{1/4+\\varepsilon}$ approximation for SSBVE for any constant $\\varepsilon >\n0$. Our algorithm follows in the footsteps of Densest $k$-Subgraph (DkS) and\nrelated problems, by designing a tight algorithm for random models, and then\nextending it to give the same guarantee for arbitrary instances. Moreover, we\nshow that this is tight under plausible complexity conjectures: it cannot be\napproximated better than $O(n^{1/4})$ assuming an extension of the so-called\n\"Dense vs Random\" conjecture for DkS to hypergraphs. We show that the same\nbound is also matched by an integrality gap for a super-constant number of\nrounds of the Sherali-Adams LP hierarchy, and an even worse integrality gap for\nthe natural SDP relaxation. Finally, we design a simple bicriteria $\\tilde\nO(\\sqrt{n})$ approximation for the more general SSVE problem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:24:45 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Chlamt\u00e1\u010d", "Eden", ""], ["Dinitz", "Michael", ""], ["Makarychev", "Yury", ""]]}, {"id": "1611.07879", "submitter": "Eric Blais", "authors": "Eric Blais and Abhinav Bommireddi", "title": "Testing submodularity and other properties of valuation functions", "comments": "To appear in 8th Innovations in Theoretical Computer Science (ITCS)\n  conference, Jan. 9-11, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for any constant $\\epsilon > 0$ and $p \\ge 1$, it is possible to\ndistinguish functions $f : \\{0,1\\}^n \\to [0,1]$ that are submodular from those\nthat are $\\epsilon$-far from every submodular function in $\\ell_p$ distance\nwith a constant number of queries.\n  More generally, we extend the testing-by-implicit-learning framework of\nDiakonikolas et al. (2007) to show that every property of real-valued functions\nthat is well-approximated in $\\ell_2$ distance by a class of $k$-juntas for\nsome $k = O(1)$ can be tested in the $\\ell_p$-testing model with a constant\nnumber of queries. This result, combined with a recent junta theorem of Feldman\nand Vondrak (2016), yields the constant-query testability of submodularity. It\nalso yields constant-query testing algorithms for a variety of other natural\nproperties of valuation functions, including fractionally additive (XOS)\nfunctions, OXS functions, unit demand functions, coverage functions, and\nself-bounding functions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:51:08 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Blais", "Eric", ""], ["Bommireddi", "Abhinav", ""]]}, {"id": "1611.08060", "submitter": "T-H. Hubert Chan", "authors": "T-H. Hubert Chan and Zhihao Gavin Tang and Xiaowei Wu", "title": "On ($1$, $\\epsilon$)-Restricted Max-Min Fair Allocation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the max-min fair allocation problem in which a set of $m$\nindivisible items are to be distributed among $n$ agents such that the minimum\nutility among all agents is maximized. In the restricted setting, the utility\nof each item $j$ on agent $i$ is either $0$ or some non-negative weight $w_j$.\nFor this setting, Asadpour et al. showed that a certain configuration-LP can be\nused to estimate the optimal value within a factor of $4+\\delta$, for any\n$\\delta>0$, which was recently extended by Annamalai et al. to give a\npolynomial-time $13$-approximation algorithm for the problem. For hardness\nresults, Bezakova and Dani showed that it is \\NP-hard to approximate the\nproblem within any ratio smaller than $2$.\n  In this paper we consider the $(1,\\epsilon)$-restricted max-min fair\nallocation problem in which each item $j$ is either heavy $(w_j = 1)$ or light\n$(w_j = \\epsilon)$, for some parameter $\\epsilon \\in (0,1)$. We show that the\n$(1,\\epsilon)$-restricted case is also \\NP-hard to approximate within any ratio\nsmaller than $2$. Hence, this simple special case is still algorithmically\ninteresting.\n  Using the configuration-LP, we are able to estimate the optimal value of the\nproblem within a factor of $3+\\delta$, for any $\\delta>0$. Extending this idea,\nwe also obtain a quasi-polynomial time $(3+4\\epsilon)$-approximation algorithm\nand a polynomial time $9$-approximation algorithm. Moreover, we show that as\n$\\epsilon$ tends to $0$, the approximation ratio of our polynomial-time\nalgorithm approaches $3+2\\sqrt{2}\\approx 5.83$.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 03:41:19 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Chan", "T-H. Hubert", ""], ["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""]]}, {"id": "1611.08198", "submitter": "Travis Gagie", "authors": "Felipe A. Louza, Travis Gagie and Guilherme P. Telles", "title": "Burrows-Wheeler transform and LCP array construction in constant space", "comments": "Accepted to JDA", "journal-ref": "Journal of Discrete Algorithms, 42 (2017) 14-22", "doi": "10.1016/j.jda.2016.11.003", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we extend the elegant in-place Burrows-Wheeler transform\n(BWT) algorithm proposed by Crochemore et al. (Crochemore et al., 2015). Our\nextension is twofold: we first show how to compute simultaneously the longest\ncommon prefix (LCP) array as well as the BWT, using constant additional space;\nwe then show how to build the LCP array directly in compressed representation\nusing Elias coding, still using constant additional space and with no\nasymptotic slowdown. Furthermore, we provide a time/space tradeoff for our\nalgorithm when additional memory is allowed. Our algorithm runs in quadratic\ntime, as does Crochemore et al.'s, and is supported by interesting properties\nof the BWT and of the LCP array, contributing to our understanding of the\ntime/space tradeoff curve for building indexing structures.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 14:43:57 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Louza", "Felipe A.", ""], ["Gagie", "Travis", ""], ["Telles", "Guilherme P.", ""]]}, {"id": "1611.08209", "submitter": "Konstantinos Georgiou", "authors": "Jurek Czyzowicz, Konstantinos Georgiou, Evangelos Kranakis, Danny\n  Krizanc, Lata Narayanan, Jaroslav Opatrny, Sunil Shende", "title": "Search on a Line by Byzantine Robots", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fault-tolerant parallel search on an infinite line\nby $n$ robots. Starting from the origin, the robots are required to find a\ntarget at an unknown location. The robots can move with maximum speed $1$ and\ncan communicate in wireless mode among themselves. However, among the $n$\nrobots, there are $f$ robots that exhibit {\\em byzantine faults}. A faulty\nrobot can fail to report the target even after reaching it, or it can make\nmalicious claims about having found the target when in fact it has not. Given\nthe presence of such faulty robots, the search for the target can only be\nconcluded when the non-faulty robots have sufficient verification that the\ntarget has been found. We aim to design algorithms that minimize the value of\n$S_d(n,f)$, the time to find a target at a distance $d$ from the origin by $n$\nrobots among which $f$ are faulty. We give several different algorithms whose\nrunning time depends on the ratio $f/n$, the density of faulty robots, and also\nprove lower bounds. Our algorithms are optimal for some densities of faulty\nrobots.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:06:52 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Georgiou", "Konstantinos", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Shende", "Sunil", ""]]}, {"id": "1611.08326", "submitter": "Aviad Rubinstein", "authors": "Aviad Rubinstein", "title": "Detecting communities is hard, and counting them is even harder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the algorithmic problem of community detection in networks. Given\nan undirected friendship graph $G=\\left(V,E\\right)$, a subset $S\\subseteq V$ is\nan $\\left(\\alpha,\\beta\\right)$-community if:\n  * Every member of the community is friends with an $\\alpha$-fraction of the\ncommunity;\n  * Every non-member is friends with at most a $\\beta$-fraction of the\ncommunity.\n  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating\nall the $\\left(\\alpha,\\beta\\right)$-communities for any constants\n$\\alpha>\\beta$.\n  Here, we prove that, assuming the Exponential Time Hypothesis (ETH),\nquasi-polynomial time is in fact necessary - and even for a much weaker\napproximation desideratum. Namely, distinguishing between:\n  * $G$ contains an $\\left(1,o\\left(1\\right)\\right)$-community; and\n  * $G$ does not contain an\n$\\left(\\beta+o\\left(1\\right),\\beta\\right)$-community for any\n$\\beta\\in\\left[0,1\\right]$.\n  We also prove that counting the number of\n$\\left(1,o\\left(1\\right)\\right)$-communities requires quasi-polynomial time\nassuming the weaker #ETH.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 00:09:46 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Rubinstein", "Aviad", ""]]}, {"id": "1611.08571", "submitter": "Andr\\'as Gily\\'en", "authors": "Andr\\'as Gily\\'en and Or Sattath", "title": "On preparing ground states of gapped Hamiltonians: An efficient Quantum\n  Lov\\'asz Local Lemma", "comments": "39 pages", "journal-ref": "In 58th IEEE Symposium on Foundations of Computer Science (FOCS\n  2017), pp.439-450", "doi": "10.1109/FOCS.2017.47", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A frustration-free local Hamiltonian has the property that its ground state\nminimises the energy of all local terms simultaneously. In general, even\ndeciding whether a Hamiltonian is frustration-free is a hard task, as it is\nclosely related to the QMA1-complete quantum satisfiability problem (QSAT) --\nthe quantum analogue of SAT, which is the archetypal NP-complete problem in\nclassical computer science. This connection shows that the frustration-free\nproperty is not only relevant to physics but also to computer science. The\nQuantum Lov\\'asz Local Lemma (QLLL) provides a sufficient condition for\nfrustration-freeness. A natural question is whether there is an efficient way\nto prepare a frustration-free state under the conditions of the QLLL. Previous\nresults showed that the answer is positive if all local terms commute. In this\nwork we improve on the previous constructive results by designing an algorithm\nthat works efficiently for non-commuting terms as well, assuming that the\nsystem is \"uniformly\" gapped, by which we mean that the system and all its\nsubsystems have an inverse polynomial energy gap. Also, our analysis works\nunder the most general condition for the QLLL, known as Shearer's bound.\nSimilarly to the previous results, our algorithm has the charming feature that\nit uses only local measurement operations corresponding to the local\nHamiltonian terms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 19:59:49 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Gily\u00e9n", "Andr\u00e1s", ""], ["Sattath", "Or", ""]]}, {"id": "1611.08574", "submitter": "Abbas Bazzi", "authors": "Ashkan Norouzi-Fard, Abbas Bazzi, Marwa El Halabi, Ilija Bogunovic,\n  Ya-Ping Hsieh, Volkan Cevher", "title": "An Efficient Streaming Algorithm for the Submodular Cover Problem", "comments": "To appear in NIPS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of the classical Submodular Cover (SC) problem in the\ndata streaming model which we refer to as the Streaming Submodular Cover (SSC).\nWe show that any single pass streaming algorithm using sublinear memory in the\nsize of the stream will fail to provide any non-trivial approximation\nguarantees for SSC. Hence, we consider a relaxed version of SSC, where we only\nseek to find a partial cover.\n  We design the first Efficient bicriteria Submodular Cover Streaming\n(ESC-Streaming) algorithm for this problem, and provide theoretical guarantees\nfor its performance supported by numerical evidence. Our algorithm finds\nsolutions that are competitive with the near-optimal offline greedy algorithm\ndespite requiring only a single pass over the data stream. In our numerical\nexperiments, we evaluate the performance of ESC-Streaming on active set\nselection and large-scale graph cover problems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 20:12:24 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Norouzi-Fard", "Ashkan", ""], ["Bazzi", "Abbas", ""], ["Halabi", "Marwa El", ""], ["Bogunovic", "Ilija", ""], ["Hsieh", "Ya-Ping", ""], ["Cevher", "Volkan", ""]]}, {"id": "1611.08687", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco, Luisa Gargano, Manuel Lafond, Lata Narayanan, Adele\n  A. Rescigno, Ugo Vaccaro, Kangkang Wu", "title": "Whom to befriend to influence people", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alice wants to join a new social network, and influence its members to adopt\na new product or idea. Each person $v$ in the network has a certain threshold\n$t(v)$ for {\\em activation}, i.e adoption of the product or idea. If $v$ has at\nleast $t(v)$ activated neighbors, then $v$ will also become activated. If Alice\nwants to activate the entire social network, whom should she befriend? More\ngenerally, we study the problem of finding the minimum number of links that a\nset of external influencers should form to people in the network, in order to\nactivate the entire social network. This {\\em Minimum Links} Problem has\napplications in viral marketing and the study of epidemics. Its solution can be\nquite different from the related and widely studied Target Set Selection\nproblem. We prove that the Minimum Links problem cannot be approximated to\nwithin a ratio of $O(2^{\\log^{1-\\epsilon} n})$, for any fixed $\\epsilon>0$,\nunless $NP\\subseteq DTIME(n^{polylog(n)})$, where $n$ is the number of nodes in\nthe network. On the positive side, we give linear time algorithms to solve the\nproblem for trees, cycles, and cliques, for any given set of external\ninfluencers, and give precise bounds on the number of links needed. For general\ngraphs, we design a polynomial time algorithm to compute size-efficient link\nsets that can activate the entire graph.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 09:26:47 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 11:43:47 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Lafond", "Manuel", ""], ["Narayanan", "Lata", ""], ["Rescigno", "Adele A.", ""], ["Vaccaro", "Ugo", ""], ["Wu", "Kangkang", ""]]}, {"id": "1611.08752", "submitter": "Harishchandra Ramadas", "authors": "Avi Levy, Harishchandra Ramadas and Thomas Rothvoss", "title": "Deterministic Discrepancy Minimization via the Multiplicative Weight\n  Update Method", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known theorem of Spencer shows that any set system with $n$ sets over\n$n$ elements admits a coloring of discrepancy $O(\\sqrt{n})$. While the original\nproof was non-constructive, recent progress brought polynomial time algorithms\nby Bansal, Lovett and Meka, and Rothvoss. All those algorithms are randomized,\neven though Bansal's algorithm admitted a complicated derandomization.\n  We propose an elegant deterministic polynomial time algorithm that is\ninspired by Lovett-Meka as well as the Multiplicative Weight Update method. The\nalgorithm iteratively updates a fractional coloring while controlling the\nexponential weights that are assigned to the set constraints.\n  A conjecture by Meka suggests that Spencer's bound can be generalized to\nsymmetric matrices. We prove that $n \\times n$ matrices that are block diagonal\nwith block size $q$ admit a coloring of discrepancy $O(\\sqrt{n} \\cdot\n\\sqrt{\\log(q)})$.\n  Bansal, Dadush and Garg recently gave a randomized algorithm to find a vector\n$x$ with entries in $\\lbrace{-1,1\\rbrace}$ with $\\|Ax\\|_{\\infty} \\leq\nO(\\sqrt{\\log n})$ in polynomial time, where $A$ is any matrix whose columns\nhave length at most 1. We show that our method can be used to deterministically\nobtain such a vector.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:27:17 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 19:01:41 GMT"}, {"version": "v3", "created": "Sat, 11 Mar 2017 00:55:41 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Levy", "Avi", ""], ["Ramadas", "Harishchandra", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1611.08757", "submitter": "Rebecca Hoberg", "authors": "Rebecca Hoberg, Harishchandra Ramadas, Thomas Rothvoss and Xin Yang", "title": "Number Balancing is as hard as Minkowski's Theorem and Shortest Vector", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number balancing (NBP) problem is the following: given real numbers\n$a_1,\\ldots,a_n \\in [0,1]$, find two disjoint subsets $I_1,I_2 \\subseteq [n]$\nso that the difference $|\\sum_{i \\in I_1}a_i - \\sum_{i \\in I_2}a_i|$ of their\nsums is minimized. An application of the pigeonhole principle shows that there\nis always a solution where the difference is at most $O(\\frac{\\sqrt{n}}{2^n})$.\nFinding the minimum, however, is NP-hard. In polynomial time,the differencing\nalgorithm by Karmarkar and Karp from 1982 can produce a solution with\ndifference at most $n^{-\\Theta(\\log n)}$, but no further improvement has been\nmade since then.\n  In this paper, we show a relationship between NBP and Minkowski's Theorem.\nFirst we show that an approximate oracle for Minkowski's Theorem gives an\napproximate NBP oracle. Perhaps more surprisingly, we show that an approximate\nNBP oracle gives an approximate Minkowski oracle. In particular, we prove that\nany polynomial time algorithm that guarantees a solution of difference at most\n$2^{\\sqrt{n}} / 2^{n}$ would give a polynomial approximation for Minkowski as\nwell as a polynomial factor approximation algorithm for the Shortest Vector\nProblem.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:59:07 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 19:13:15 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Hoberg", "Rebecca", ""], ["Ramadas", "Harishchandra", ""], ["Rothvoss", "Thomas", ""], ["Yang", "Xin", ""]]}, {"id": "1611.08803", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao and Hiroshi Nagamochi", "title": "A Linear-time Algorithm for Integral Multiterminal Flows in Trees", "comments": "17 pages, 6 figures, ISAAC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of finding an integral multiflow which\nmaximizes the sum of flow values between every two terminals in an undirected\ntree with a nonnegative integer edge capacity and a set of terminals. In\ngeneral, it is known that the flow value of an integral multiflow is bounded by\nthe cut value of a cut-system which consists of disjoint subsets each of which\ncontains exactly one terminal or has an odd cut value, and there exists a pair\nof an integral multiflow and a cut-system whose flow value and cut value are\nequal; i.e., a pair of a maximum integral multiflow and a minimum cut. In this\npaper, we propose an $O(n)$-time algorithm that finds such a pair of an\nintegral multiflow and a cut-system in a given tree instance with $n$ vertices.\nThis improves the best previous results by a factor of $\\Omega (n)$. Regarding\na given tree in an instance as a rooted tree, we define $O(n)$ rooted tree\ninstances taking each vertex as a root, and establish a recursive formula on\nmaximum integral multiflow values of these instances to design a dynamic\nprogramming that computes the maximum integral multiflow values of all $O(n)$\nrooted instances in linear time. We can prove that the algorithm implicitly\nmaintains a cut-system so that not only a maximum integral multiflow but also a\nminimum cut-system can be constructed in linear time for any rooted instance\nwhenever it is necessary. The resulting algorithm is rather compact and\nsuccinct.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 07:53:27 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Xiao", "Mingyu", ""], ["Nagamochi", "Hiroshi", ""]]}, {"id": "1611.08809", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Robert Bredereck and Morgan Chopin and Sepp\n  Hartung and Falk H\\\"uffner and Andr\\'e Nichterlein and Ond\\v{r}ej Such\\'y", "title": "Fixed-Parameter Algorithms for DAG Partitioning", "comments": "A preliminary version of this article appeared at CIAC'13. Besides\n  providing full proof details, this revised and extended version improves our\n  O(2^k * n^2)-time algorithm to run in O(2^k * (n+m)) time and provides\n  linear-time executable data reduction rules. Moreover, we experimentally\n  evaluated the algorithm and compared it to known heuristics", "journal-ref": "Discrete Applied Mathematics 220:134-160, 2017", "doi": "10.1016/j.dam.2016.12.002", "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the origin of short phrases propagating through the web has been\nformalized by Leskovec et al. [ACM SIGKDD 2009] as DAG Partitioning: given an\narc-weighted directed acyclic graph on $n$ vertices and $m$ arcs, delete arcs\nwith total weight at most $k$ such that each resulting weakly-connected\ncomponent contains exactly one sink---a vertex without outgoing arcs. DAG\nPartitioning is NP-hard.\n  We show an algorithm to solve DAG Partitioning in $O(2^k \\cdot (n+m))$ time,\nthat is, in linear time for fixed $k$. We complement it with linear-time\nexecutable data reduction rules. Our experiments show that, in combination,\nthey can optimally solve DAG Partitioning on simulated citation networks within\nfive minutes for $k\\leq190$ and $m$ being $10^7$ and larger. We use our\nobtained optimal solutions to evaluate the solution quality of Leskovec et\nal.'s heuristic.\n  We show that Leskovec et al.'s heuristic works optimally on trees and\ngeneralize this result by showing that DAG Partitioning is solvable in\n$2^{O(w^2)}\\cdot n$ time if a width-$w$ tree decomposition of the input graph\nis given. Thus, we improve an algorithm and answer an open question of Alamdari\nand Mehrabian [WAW 2012].\n  We complement our algorithms by lower bounds on the running time of exact\nalgorithms and on the effectivity of data reduction.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 09:04:03 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Bredereck", "Robert", ""], ["Chopin", "Morgan", ""], ["Hartung", "Sepp", ""], ["H\u00fcffner", "Falk", ""], ["Nichterlein", "Andr\u00e9", ""], ["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "1611.08861", "submitter": "Assaf Naor", "authors": "Assaf Naor", "title": "A spectral gap precludes low-dimensional embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS math.CO math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that there is a universal constant $C>0$ with the following\nproperty. Suppose that $n\\in \\mathbb{N}$ and that $\\mathsf{A}=(a_{ij})\\in\nM_n(\\mathbb{R})$ is a symmetric stochastic matrix. Denote the second-largest\neigenvalue of $\\mathsf{A}$ by $\\lambda_2(\\mathsf{A})$. Then for $\\mathrm{\\it\nany}$ finite-dimensional normed space $(X,\\|\\cdot\\|)$ we have $$ \\forall\\,\nx_1,\\ldots,x_n\\in X,\\qquad \\mathrm{dim}(X)\\ge \\frac12\n\\exp\\left(C\\frac{1-\\lambda_2(\\mathsf{A})}{\\sqrt{n}}\\bigg(\\frac{\\sum_{i=1}^n\\sum_{j=1}^n\\|x_i-x_j\\|^2}{\\sum_{i=1}^n\\sum_{j=1}^na_{ij}\\|x_i-x_j\\|^2}\\bigg)^{\\frac12}\\right).\n$$ This implies that if an $n$-vertex $O(1)$-expander embeds with average\ndistortion $D\\ge 1$ into $X$, then necessarily $\\mathrm{dim}(X)\\gtrsim n^{c/D}$\nfor some universal constant $c>0$, thus improving over the previously\nbest-known estimate $\\mathrm{dim}(X)\\gtrsim (\\log n)^2/D^2$ of Linial, London\nand Rabinovich, strengthening a theorem of Matou\\v{s}ek, and answering a\nquestion of Andoni, Nikolov, Razenshteyn and Waingarten.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 15:07:51 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Naor", "Assaf", ""]]}, {"id": "1611.08898", "submitter": "Dominik Kempa", "authors": "Juha K\\\"arkk\\\"ainen, Dominik Kempa, Yuto Nakashima, Simon J. Puglisi,\n  Arseny M. Shur", "title": "On the Size of Lempel-Ziv and Lyndon Factorizations", "comments": "12 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2017.45", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lyndon factorization and Lempel-Ziv (LZ) factorization are both important\ntools for analysing the structure and complexity of strings, but their\ncombinatorial structure is very different. In this paper, we establish the\nfirst direct connection between the two by showing that while the Lyndon\nfactorization can be bigger than the non-overlapping LZ factorization (which we\ndemonstrate by describing a new, non-trivial family of strings) it is never\nmore than twice the size.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 19:44:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Juha", ""], ["Kempa", "Dominik", ""], ["Nakashima", "Yuto", ""], ["Puglisi", "Simon J.", ""], ["Shur", "Arseny M.", ""]]}, {"id": "1611.08954", "submitter": "Jalaj Upadhyay", "authors": "Jalaj Upadhyay", "title": "On Low-Space Differentially Private Low-rank Factorization in the\n  Spectral Norm", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank factorization is used in many areas of computer science where one\nperforms spectral analysis on large sensitive data stored in the form of\nmatrices. In this paper, we study differentially private low-rank factorization\nof a matrix with respect to the spectral norm in the turnstile update model. In\nthis problem, given an input matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\nupdated in the turnstile manner and a target rank $k$, the goal is to find two\nrank-$k$ orthogonal matrices $\\mathbf{U}_k \\in \\mathbb{R}^{m \\times k}$ and\n$\\mathbf{V}_k \\in \\mathbb{R}^{n \\times k}$, and one positive semidefinite\ndiagonal matrix $\\textbf{\\Sigma}_k \\in \\mathbb{R}^{k \\times k}$ such that\n$\\mathbf{A} \\approx \\mathbf{U}_k \\textbf{\\Sigma}_k \\mathbf{V}_k^\\mathsf{T}$\nwith respect to the spectral norm.\n  Our main contributions are two computationally efficient and sub-linear space\nalgorithms for computing a differentially private low-rank factorization. We\nconsider two levels of privacy. In the first level of privacy, we consider two\nmatrices neighboring if their difference has a Frobenius norm at most $1$. In\nthe second level of privacy, we consider two matrices as neighboring if their\ndifference can be represented as an outer product of two unit vectors. Both\nthese privacy levels are stronger than those studied in the earlier papers such\nas Dwork {\\it et al.} (STOC 2014), Hardt and Roth (STOC 2013), and Hardt and\nPrice (NIPS 2014).\n  As a corollary to our results, we get non-private algorithms that compute\nlow-rank factorization in the turnstile update model with respect to the\nspectral norm. We note that, prior to this work, no algorithm that outputs\nlow-rank factorization with respect to the spectral norm in the turnstile\nupdate model was known; i.e., our algorithm gives the first non-private\nlow-rank factorization with respect to the spectral norm in the turnstile\nupdate mode.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 01:57:53 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Upadhyay", "Jalaj", ""]]}, {"id": "1611.09012", "submitter": "Rahul Vaze", "authors": "Rahul Vaze", "title": "Online Knapsack Problem and Budgeted Truthful Bipartite Matching", "comments": "To appear in IEEE INFOCOM 2017, May 1-4, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two related online problems: knapsack and truthful bipartite matching are\nconsidered. For these two problems, the common theme is how to `match' an\narriving left vertex in an online fashion with any of the available right\nvertices, if at all, so as to maximize the sum of the value of the matched\nedges, subject to satisfying a sum-weight constraint on the matched left\nvertices. Assuming that the left vertices arrive in an uniformly random order\n(secretary model), two almost similar algorithms are proposed for the two\nproblems, that are $2e$ competitive and $24$ competitive, respectively. The\nproposed online bipartite matching algorithm is also shown to be truthful:\nthere is no incentive for any left vertex to misreport its bid/weight. Direct\napplications of these problems include job allocation with load balancing,\ngeneralized adwords, crowdsourcing auctions, and matching wireless users to\ncooperative relays in device-to-device communication enabled cellular network.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 07:48:13 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Vaze", "Rahul", ""]]}, {"id": "1611.09072", "submitter": "Le Zhang", "authors": "Ran Duan, Le Zhang", "title": "Faster Randomized Worst-Case Update Time for Dynamic Subgraph\n  Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world networks are prone to breakdowns. Typically in the underlying\ngraph $G$, besides the insertion or deletion of edges, the set of active\nvertices changes overtime. A vertex might work actively, or it might fail, and\ngets isolated temporarily. The active vertices are grouped as a set $S$. $S$ is\nsubjected to updates, i.e., a failed vertex restarts, or an active vertex\nfails, and gets deleted from $S$. Dynamic subgraph connectivity answers the\nqueries on connectivity between any two active vertices in the subgraph of $G$\ninduced by $S$. The problem is solved by a dynamic data structure, which\nsupports the updates and answers the connectivity queries. In the general\nundirected graph, the best results for it include $\\widetilde{O}(m^{2/3})$\ndeterministic amortized update time, $\\widetilde{O}(m^{4/5})$ and\n$\\widetilde{O}(\\sqrt{mn})$ deterministic worst-case update time. In the paper,\nwe propose a randomized data structure, which has $\\widetilde{O}(m^{3/4})$\nworst-case update time.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:18:27 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 16:07:34 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 12:15:08 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Duan", "Ran", ""], ["Zhang", "Le", ""]]}, {"id": "1611.09084", "submitter": "Dario Garcia-Gasulla", "authors": "Dario Garcia-Gasulla, Eduard Ayguad\\'e, Jes\\'us Labarta, Ulises\n  Cort\\'es, Toyotaro Suzumura", "title": "Hierarchical Hyperlink Prediction for the WWW", "comments": "Submitted to Transactions on Internet Technology journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hyperlink prediction task, that of proposing new links between webpages,\ncan be used to improve search engines, expand the visibility of web pages, and\nincrease the connectivity and navigability of the web. Hyperlink prediction is\ntypically performed on webgraphs composed by thousands or millions of vertices,\nwhere on average each webpage contains less than fifty links. Algorithms\nprocessing graphs so large and sparse require to be both scalable and precise,\na challenging combination. Similarity-based algorithms are among the most\nscalable solutions within the link prediction field, due to their parallel\nnature and computational simplicity. These algorithms independently explore the\nnearby topological features of every missing link from the graph in order to\ndetermine its likelihood. Unfortunately, the precision of similarity-based\nalgorithms is limited, which has prevented their broad application so far. In\nthis work we explore the performance of similarity-based algorithms for the\nparticular problem of hyperlink prediction on large webgraphs, and propose a\nnovel method which assumes the existence of hierarchical properties. We\nevaluate this new approach on several webgraphs and compare its performance\nwith that of the current best similarity-based algorithms. Its remarkable\nperformance leads us to argue on the applicability of the proposal, identifying\nseveral use cases of hyperlink prediction. We also describes the approach we\ntook for the computation of large-scale graphs from the perspective of\nhigh-performance computing, providing details on the implementation and\nparallelization of code.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:47:52 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1611.09131", "submitter": "Scott Haag", "authors": "Scott Haag, Ali Shokoufandeh", "title": "Development of a data model to facilitate rapid Watershed Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data model to store and retrieve surface watershed boundaries using graph\ntheoretic approaches is proposed. This data model integrates output from a\nstandard digital elevation models (DEM) derived stream catchment boundaries,\nand vector representation of stream centerlines then applies them to three\nnovel algorithms. The first is called Modified Nested Set (MNS), which is a\ndepth first graph traversal algorithm that searches across stream reaches\n(vertices) and stream junctions (edges) labeling vertices by their discovery\ntime, finish time, and distance from the root. The second is called Log Reduced\nGraphs (LRG), which creates a set S of logarithmically reduced graphs from the\noriginal data, to store the watershed boundaries. The final algorithm is called\nStitching Watershed, which provides a technique to merge watershed boundaries\nacross the set of graphs created in the LRG algorithm.\n  This technique was applied to the ~ 30,600 km2 Delaware River Watershed and\ncompared to hypothetical data storage models in terms of prep-processing, data\nstorage, and query complexity costs. Results show that the proposed technique\nprovides significant benefits vs. the hypothetical methods with a 99-98%\nreduction in prepossessing, 96-80% reduction in query complexity and a 76%\nreduction in storage costs. The increasing availability of high resolution\nelevation data within the United States and the internationally provides an\nopportunity to extend these results to other watersheds through the world.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:31:00 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Haag", "Scott", ""], ["Shokoufandeh", "Ali", ""]]}, {"id": "1611.09296", "submitter": "Saeed Akhoondian Amiri", "authors": "Saeed Akhoondian Amiri, Szymon Dudycz, Stefan Schmid, Sebastian\n  Wiederrecht", "title": "Congestion-Free Rerouting of Flows on DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM cs.NI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changing a given configuration in a graph into another one is known as a re-\nconfiguration problem. Such problems have recently received much interest in\nthe context of algorithmic graph theory. We initiate the theoretical study of\nthe following reconfiguration problem: How to reroute $k$ unsplittable flows of\na certain demand in a capacitated network from their current paths to their\nrespective new paths, in a congestion-free manner? This problem finds immediate\napplications, e.g., in traffic engineering in computer networks. We show that\nthe problem is generally NP-hard already for $k = 2$ flows, which motivates us\nto study rerouting on a most basic class of flow graphs, namely DAGs.\nInterestingly, we find that for general $k$, deciding whether an unsplittable\nmulti-commodity flow rerouting schedule exists, is NP-hard even on DAGs. Both\nNP-hardness proofs are non-trivial. Our main contribution is a polynomial-time\n(fixed parameter tractable) algorithm to solve the route update problem for a\nbounded number of flows on DAGs. At the heart of our algorithm lies a novel\ndecomposition of the flow network that allows us to express and resolve\nreconfiguration dependencies among flows.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 19:25:04 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 17:02:21 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 09:14:17 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Dudycz", "Szymon", ""], ["Schmid", "Stefan", ""], ["Wiederrecht", "Sebastian", ""]]}, {"id": "1611.09317", "submitter": "Karol W\\k{e}grzycki", "authors": "Andrzej Pacuk and Piotr Sankowski and Karol Wegrzycki and Piotr\n  Wygocki", "title": "Locality-Sensitive Hashing without False Negatives for l_p", "comments": "11 pages, 2 figures, COCOON 2016", "journal-ref": "Computing and Combinatorics - 22nd International Conference,\n  {COCOON} 2016, Ho Chi Minh City, Vietnam, August 2-4, 2016, Proceedings,\n  pages 105--118", "doi": "10.1007/978-3-319-42634-1_9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show a construction of locality-sensitive hash functions\nwithout false negatives, i.e., which ensure collision for every pair of points\nwithin a given radius $R$ in $d$ dimensional space equipped with $l_p$ norm\nwhen $p \\in [1,\\infty]$. Furthermore, we show how to use these hash functions\nto solve the $c$-approximate nearest neighbor search problem without false\nnegatives. Namely, if there is a point at distance $R$, we will certainly\nreport it and points at distance greater than $cR$ will not be reported for\n$c=\\Omega(\\sqrt{d},d^{1-\\frac{1}{p}})$. The constructed algorithms work: - with\npreprocessing time $\\mathcal{O}(n \\log(n))$ and sublinear expected query time,\n- with preprocessing time $\\mathcal{O}(\\mathrm{poly}(n))$ and expected query\ntime $\\mathcal{O}(\\log(n))$. Our paper reports progress on answering the open\nproblem presented by Pagh [8] who considered the nearest neighbor search\nwithout false negatives for the Hamming distance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:11:11 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Pacuk", "Andrzej", ""], ["Sankowski", "Piotr", ""], ["Wegrzycki", "Karol", ""], ["Wygocki", "Piotr", ""]]}, {"id": "1611.09474", "submitter": "Ali Khodabakhsh", "authors": "Ali Khodabakhsh, Evdokia Nikolova", "title": "Maximizing Non-Monotone DR-Submodular Functions with Cardinality\n  Constraints", "comments": "Error description: The proposed algorithms have running time issues,\n  in particular they are pseudo-polynomial and not fully polynomial-time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a non-monotone DR-submodular function\nsubject to a cardinality constraint. Diminishing returns (DR) submodularity is\na generalization of the diminishing returns property for functions defined over\nthe integer lattice. This generalization can be used to solve many machine\nlearning or combinatorial optimization problems such as optimal budget\nallocation, revenue maximization, etc. In this work we propose the first\npolynomial-time approximation algorithms for non-monotone constrained\nmaximization. We implement our algorithms for a revenue maximization problem\nwith a real-world dataset to check their efficiency and performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 03:40:36 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 15:35:48 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Khodabakhsh", "Ali", ""], ["Nikolova", "Evdokia", ""]]}, {"id": "1611.09482", "submitter": "Tom Paine", "authors": "Tom Le Paine, Pooya Khorrami, Shiyu Chang, Yang Zhang, Prajit\n  Ramachandran, Mark A. Hasegawa-Johnson, Thomas S. Huang", "title": "Fast Wavenet Generation Algorithm", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient implementation of the Wavenet generation\nprocess called Fast Wavenet. Compared to a naive implementation that has\ncomplexity O(2^L) (L denotes the number of layers in the network), our proposed\napproach removes redundant convolution operations by caching previous\ncalculations, thereby reducing the complexity to O(L) time. Timing experiments\nshow significant advantages of our fast implementation over a naive one. While\nthis method is presented for Wavenet, the same scheme can be applied anytime\none wants to perform autoregressive generation or online prediction using a\nmodel with dilated convolution layers. The code for our method is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 04:16:44 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Ramachandran", "Prajit", ""], ["Hasegawa-Johnson", "Mark A.", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1611.09590", "submitter": "Virendra Sule", "authors": "Virendra Sule", "title": "Implicant based parallel all solution solver for Boolean satisfiability", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a parallel computational solver for computing all\nsatifying assignments of a Boolean system of equations defined by Boolean\nfunctions of several variables. While there are we known solvers for\nsatisfiability of Boolean formulas in CNF form, these are designed primarily\nfor deciding satisfiability of the formula and do not address the problem of\nfinding all satisfying solutions. Moreover development of parallel solvers for\nsatisfiability problems is still an unfinished problem of Computer Science. The\nsolver proposed in this paper is aimed at representing all solutions of Boolean\nformulas even without the CNF form with a parallel algorithm. Algorithm\nproposed is applied to Boolean functions in algebraic normal form (ANF). The\nalgorithm is based on the idea to represent the satisfying assignments in terms\nof a complete set of implicants of the Boolean functions appearing as factors\nof a Boolean formula. The algorithm is effective mainly in the case when the\nfactors of the formula are sparse (i.e. have a small fraction of the total\nnumber of variables). This allows small computation of a complete set of\nimplicants of individual factors one at a time and reduce the formula at each\nstep. An algorithm is also proposed for finding a complete set of orthogonal\nimplicants of functions in ANF. An advantages of this algorithm is that all\nsolutions can be represented compactly in terms of implicants. Finally due to\nsmall and distributed computation at every step as well as computation in terms\nof independent threads, the solver proposed in this paper is expected to be\nuseful for developing heuristics for a well scalable parallel solver for large\nsize problems of Boolean satisfiability over large number of processors.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 12:27:10 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 12:42:10 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 13:27:22 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Sule", "Virendra", ""]]}, {"id": "1611.09664", "submitter": "Kaveh Geyratmand Haghighi", "authors": "Kaveh Geyratmand Haghighi, Mirkamal Mirnia, Ahmad Habibizad Navin", "title": "Optimizing run-length algorithm using octonary repetition tree", "comments": "7pages", "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS) Vol. 14, No. 8, August 2016", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is beneficial because it helps detract resource usage. It reduces\ndata storage space as well as transmission traffic and improves web pages\nloading. Run-length coding (RLC) is a lossless data compression algorithm. Data\nare stored as a data value and counts. This is useful on data that contains\nmany consecutive runs. This paper proposes a compression algorithm using\noctonary repetition tree (ORT), based on RLC. ORT is used to overcome the\nduplication problem in primary RLC algorithms, instead of using flag or\ncodeword. It's the first method of run-length encoding which has the\ncompression ratio greater than one in all tested cases. Experimental results,\nshow average improvement of roughly 3 times, 3 times and 2 times in compression\nratio field of study comparing to PRLC1, PRLC2, DF-RLC respectively. By using\nthis approach of run-length encoding we can compress wider types of data, such\nas multimedia, document, executive files, etc.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 14:55:30 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Haghighi", "Kaveh Geyratmand", ""], ["Mirnia", "Mirkamal", ""], ["Navin", "Ahmad Habibizad", ""]]}, {"id": "1611.09754", "submitter": "Marc Goerigk", "authors": "Marc Goerigk and Andr\\'e Chassein", "title": "On Scenario Aggregation to Approximate Robust Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As most robust combinatorial min-max and min-max regret problems with\ndiscrete uncertainty sets are NP-hard, research into approximation algorithm\nand approximability bounds has been a fruitful area of recent work. A simple\nand well-known approximation algorithm is the midpoint method, where one takes\nthe average over all scenarios, and solves a problem of nominal type. Despite\nits simplicity, this method still gives the best-known bound on a wide range of\nproblems, such as robust shortest path, or robust assignment problems.\n  In this paper we present a simple extension of the midpoint method based on\nscenario aggregation, which improves the current best $K$-approximation result\nto an $(\\varepsilon K)$-approximation for any desired $\\varepsilon > 0$. Our\nmethod can be applied to min-max as well as min-max regret problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 17:55:38 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Goerigk", "Marc", ""], ["Chassein", "Andr\u00e9", ""]]}, {"id": "1611.10045", "submitter": "Yasuo Tabei", "authors": "Yasuo Tabei, Simon J. Puglisi", "title": "Scalable Similarity Search for Molecular Descriptors", "comments": "To be appeared in the Proceedings of SISAP'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search over chemical compound databases is a fundamental task in\nthe discovery and design of novel drug-like molecules. Such databases often\nencode molecules as non-negative integer vectors, called molecular descriptors,\nwhich represent rich information on various molecular properties. While there\nexist efficient indexing structures for searching databases of binary vectors,\nsolutions for more general integer vectors are in their infancy. In this paper\nwe present a time- and space- efficient index for the problem that we call the\nsuccinct intervals-splitting tree algorithm for molecular descriptors (SITAd).\nOur approach extends efficient methods for binary-vector databases, and uses\nideas from succinct data structures. Our experiments, on a large database of\nover 40 million compounds, show SITAd significantly outperforms alternative\napproaches in practice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 08:27:17 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 00:30:19 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 01:45:35 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Tabei", "Yasuo", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1611.10133", "submitter": "Daniel Gerbner", "authors": "D\\'aniel Gerbner, M\\'at\\'e Vizer", "title": "Rounds in a combinatorial search problem", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following combinatorial search problem: we are given some\nexcellent elements of $[n]$ and we should find at least one, asking questions\nof the following type: \"Is there an excellent element in $A \\subset [n]$?\".\nG.O.H. Katona proved sharp results for the number of questions needed to ask in\nthe adaptive, non-adaptive and two-round versions of this problem.\n  We verify a conjecture of Katona by proving that in the $r$-round version we\nneed to ask $rn^{1/r}+O(1)$ queries for fixed $r$ and this is sharp.\n  We also prove bounds for the queries needed to ask if we want to find at\nleast $d$ excellent elements.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 13:09:41 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Gerbner", "D\u00e1niel", ""], ["Vizer", "M\u00e1t\u00e9", ""]]}, {"id": "1611.10208", "submitter": "Konstantinos Georgiou", "authors": "Konstantinos Georgiou, George Karakostas, Evangelos Kranakis", "title": "Search-and-Fetch with 2 Robots on a Disk: Wireless and Face-to-Face\n  Communication Models", "comments": "26 Pages, 6 Figures. This is the full version of the paper with the\n  same title which will appear in the proceedings of the 6th International\n  Conference on Operations Research and Enterprise Systems (ICORES), February\n  23-25, 2017, Porto, Portugal", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 21 no. 3\n  , Distributed Computing and Networking (June 13, 2019) dmtcs:5528", "doi": "10.23638/DMTCS-21-3-20", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of a new problem on searching and fetching in a\ndistributed environment concerning treasure-evacuation from a unit disk. A\ntreasure and an exit are located at unknown positions on the perimeter of a\ndisk and at known arc distance. A team of two robots start from the center of\nthe disk, and their goal is to fetch the treasure to the exit. At any time the\nrobots can move anywhere they choose on the disk, independently of each other,\nwith the same speed. A robot detects an interesting point (treasure or exit)\nonly if it passes over the exact location of that point. We are interested in\ndesigning distributed algorithms that minimize the worst-case\ntreasure-evacuation time, i.e. the time it takes for the treasure to be\ndiscovered and brought (fetched) to the exit by any of the robots.\n  The communication protocol between the robots is either wireless, where\ninformation is shared at any time, or face-to-face (i.e. non-wireless), where\ninformation can be shared only if the robots meet. For both models we obtain\nupper bounds for fetching the treasure to the exit. Our main technical\ncontribution pertains to the face-to-face model. More specifically, we\ndemonstrate how robots can exchange information without meeting, effectively\nachieving a highly efficient treasure-evacuation protocol which is minimally\naffected by the lack of distant communication. Finally, we complement our\npositive results above by providing a lower bound in the face-to-face model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 15:13:11 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 15:04:04 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 13:54:27 GMT"}, {"version": "v4", "created": "Thu, 23 May 2019 15:47:16 GMT"}, {"version": "v5", "created": "Fri, 24 May 2019 17:29:30 GMT"}, {"version": "v6", "created": "Wed, 29 May 2019 20:47:51 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Georgiou", "Konstantinos", ""], ["Karakostas", "George", ""], ["Kranakis", "Evangelos", ""]]}]