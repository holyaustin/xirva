[{"id": "1603.00091", "submitter": "Dimitri Van Assche", "authors": "Toon Calders, Dimitri Van Assche", "title": "PROMETHEE is Not Quadratic: An O(qn log(n)) Algorithm", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally believed that the preference ranking method PROMETHEE has a\nquadratic time complexity. In this paper, however, we present an exact\nalgorithm that computes PROMETHEE's net flow scores in time O(qn log(n)), where\nq represents the number of criteria and n the number of alternatives. The\nmethod is based on first sorting the alternatives after which the unicriterion\nflow scores of all alternatives can be computed in one scan over the sorted\nlist of alternatives while maintaining a sliding window. This method works with\nthe linear and level criterion preference functions. The algorithm we present\nis exact and, due to the sub-quadratic time complexity, vastly extends the\napplicability of the PROMETHEE method. Experiments show that with the new\nalgorithm, PROMETHEE can scale up to millions of tuples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 23:25:51 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Calders", "Toon", ""], ["Van Assche", "Dimitri", ""]]}, {"id": "1603.00213", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Palash Dey and David P. Woodruff", "title": "An Optimal Algorithm for l1-Heavy Hitters in Insertion Streams and\n  Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first optimal bounds for returning the $\\ell_1$-heavy hitters in\na data stream of insertions, together with their approximate frequencies,\nclosing a long line of work on this problem. For a stream of $m$ items in $\\{1,\n2, \\dots, n\\}$ and parameters $0 < \\epsilon < \\phi \\leq 1$, let $f_i$ denote\nthe frequency of item $i$, i.e., the number of times item $i$ occurs in the\nstream. With arbitrarily large constant probability, our algorithm returns all\nitems $i$ for which $f_i \\geq \\phi m$, returns no items $j$ for which $f_j \\leq\n(\\phi -\\epsilon)m$, and returns approximations $\\tilde{f}_i$ with $|\\tilde{f}_i\n- f_i| \\leq \\epsilon m$ for each item $i$ that it returns. Our algorithm uses\n$O(\\epsilon^{-1} \\log\\phi^{-1} + \\phi^{-1} \\log n + \\log \\log m)$ bits of\nspace, processes each stream update in $O(1)$ worst-case time, and can report\nits output in time linear in the output size. We also prove a lower bound,\nwhich implies that our algorithm is optimal up to a constant factor in its\nspace complexity. A modification of our algorithm can be used to estimate the\nmaximum frequency up to an additive $\\epsilon m$ error in the above amount of\nspace, resolving Question 3 in the IITK 2006 Workshop on Algorithms for Data\nStreams for the case of $\\ell_1$-heavy hitters. We also introduce several\nvariants of the heavy hitters and maximum frequency problems, inspired by rank\naggregation and voting schemes, and show how our techniques can be applied in\nsuch settings. Unlike the traditional heavy hitters problem, some of these\nvariants look at comparisons between items rather than numerical values to\ndetermine the frequency of an item.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 10:18:20 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Dey", "Palash", ""], ["Woodruff", "David P.", ""]]}, {"id": "1603.00509", "submitter": "Elham Havvaei", "authors": "Elham Havvaei, Narsingh Deo", "title": "A Game-Theoretic Approach for Detection of Overlapping Communities in\n  Dynamic Complex Networks", "comments": "The paper has been withdrawn from arxiv since it's under improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks tend to display communities which are groups of nodes\ncohesively connected among themselves in one group and sparsely connected to\nthe remainder of the network. Detecting such communities is an important\ncomputational problem, since it provides an insight into the functionality of\nnetworks. Further, investigating community structure in a dynamic network,\nwhere the network is subject to change, is even more challenging. This paper\npresents a game-theoretical technique for detecting community structures in\ndynamic as well as static complex networks. In our method, each node takes the\nrole of a player that attempts to gain a higher payoff by joining one or more\ncommunities or switching between them. The goal of the game is to reveal\ncommunity structure formed by these players by finding a Nash-equilibrium point\namong them. To the best of our knowledge, this is the first game-theoretic\nalgorithm which is able to extract overlapping communities from either static\nor dynamic networks. We present the experimental results illustrating the\neffectiveness of the proposed method on both synthetic and real-world networks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 22:16:28 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 18:09:22 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Havvaei", "Elham", ""], ["Deo", "Narsingh", ""]]}, {"id": "1603.00585", "submitter": "Gao Juntao", "authors": "Juntao Gao and Minoru Ito and Norio Shiratori", "title": "Optimal Scheduling for Incentive WiFi Offloading under Energy Constraint", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WiFi offloading, where mobile device users (e.g., smart phone users) transmit\npackets through WiFi networks rather than cellular networks, is a promising\nsolution to alleviating the heavy traffic burden of cellular networks due to\ndata explosion. However, since WiFi networks are intermittently available, a\nmobile device user in WiFi offloading usually needs to wait for WiFi connection\nand thus experiences longer delay of packet transmission. To motivate users to\nparticipate in WiFi offloading, cellular network operators give incentives\n(rewards like coupons, e-coins) to users who wait for WiFi connection and\ntransmit packets through WiFi networks.\n  In this paper, we aim at maximizing users' rewards while meeting constraints\non queue stability and energy consumption. However, we face scheduling\nchallenges from random packet arrivals, intermittent WiFi connection and time\nvarying wireless link states. To address these challenges, we first formulate\nthe problem as a stochastic optimization problem. We then propose an optimal\nscheduling policy, named Optimal scheduling Policy under Energy Constraint\n(OPEC), which makes online decisions as to when to delay packet transmission to\nwait for WiFi connection and which wireless link (WiFi link or cellular link)\nto transmit packets on. OPEC automatically adapts to random packet arrivals and\ntime varying wireless link states, not requiring a priori knowledge of packet\narrival and wireless link probabilities. As verified by simulations, OPEC\nscheduling policy can achieve the maximum rewards while keeping queue stable\nand meeting energy consumption constraint.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 06:01:47 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Gao", "Juntao", ""], ["Ito", "Minoru", ""], ["Shiratori", "Norio", ""]]}, {"id": "1603.00759", "submitter": "Jelani Nelson", "authors": "Vladimir Braverman, Stephen R. Chestnut, Nikita Ivkin, Jelani Nelson,\n  Zhengyu Wang, David P. Woodruff", "title": "BPTree: an $\\ell_2$ heavy hitters algorithm using constant memory", "comments": "v4: PODS'17 camera-ready version, includes improved space l_2\n  tracking (by log(1/epsilon) factor); v3: fixed accidental mis-sorting of\n  author last names; v2: added section explaining why pick-and-drop sampling\n  fails for l2 heavy hitters, and fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of finding heavy hitters is one of the best known and well studied\nproblems in the area of data streams. One is given a list\n$i_1,i_2,\\ldots,i_m\\in[n]$ and the goal is to identify the items among $[n]$\nthat appear frequently in the list. In sub-polynomial space, the strongest\nguarantee available is the $\\ell_2$ guarantee, which requires finding all items\nthat occur at least $\\epsilon\\|f\\|_2$ times in the stream, where the vector\n$f\\in\\mathbb{R}^n$ is the count histogram of the stream with $i$th coordinate\nequal to the number of times~$i$ appears $f_i:=\\#\\{j\\in[m]:i_j=i\\}$. The first\nalgorithm to achieve the $\\ell_2$ guarantee was the CountSketch of [CCF04],\nwhich requires $O(\\epsilon^{-2}\\log n)$ words of memory and $O(\\log n)$ update\ntime and is known to be space-optimal if the stream allows for deletions. The\nrecent work of [BCIW16] gave an improved algorithm for insertion-only streams,\nusing only $O(\\epsilon^{-2}\\log\\epsilon^{-1}\\log\\log n)$ words of memory. In\nthis work, we give an algorithm \\bptree for $\\ell_2$ heavy hitters in\ninsertion-only streams that achieves $O(\\epsilon^{-2}\\log\\epsilon^{-1})$ words\nof memory and $O(\\log\\epsilon^{-1})$ update time, which is the optimal\ndependence on $n$ and $m$. In addition, we describe an algorithm for tracking\n$\\|f\\|_2$ at all times with $O(\\epsilon^{-2})$ memory and update time. Our\nanalyses rely on bounding the expected supremum of a Bernoulli process\ninvolving Rademachers with limited independence, which we accomplish via a\nDudley-like chaining argument that may have applications elsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 15:48:36 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 02:35:37 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 17:56:02 GMT"}, {"version": "v4", "created": "Thu, 9 Nov 2017 13:49:13 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Braverman", "Vladimir", ""], ["Chestnut", "Stephen R.", ""], ["Ivkin", "Nikita", ""], ["Nelson", "Jelani", ""], ["Wang", "Zhengyu", ""], ["Woodruff", "David P.", ""]]}, {"id": "1603.00770", "submitter": "Torstein Str{\\o}mme", "authors": "Fedor V. Fomin and Torstein J. F. Str{\\o}mme", "title": "Vertex Cover Structural Parameterization Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pseudoforest is a graph whose connected components have at most one cycle.\nLet X be a pseudoforest modulator of graph G, i. e. a vertex subset of G such\nthat G-X is a pseudoforest. We show that Vertex Cover admits a polynomial\nkernel being parameterized by the size of the pseudoforest modulator. In other\nwords, we provide a polynomial time algorithm that for an input graph G and\ninteger k, outputs a graph G' and integer k', such that G' has O(|X|12)\nvertices and G has a vertex cover of size k if and only if G' has vertex cover\nof size k'. We complement our findings by proving that there is no polynomial\nkernel for Vertex Cover parameterized by the size of a modulator to a mock\nforest (a graph where no cycles share a vertex) unless NP is a subset of\ncoNP/poly. In particular, this also rules out polynomial kernels when\nparameterized by the size of a modulator to outerplanar and cactus graphs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:11:15 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Str\u00f8mme", "Torstein J. F.", ""]]}, {"id": "1603.00858", "submitter": "David Manlove", "authors": "Katarina Cechlarova and Bettina Klaus and David F. Manlove", "title": "Pareto optimal matchings of students to courses in the presence of\n  prerequisites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of allocating applicants to courses, where each\napplicant has a subset of acceptable courses that she ranks in strict order of\npreference. Each applicant and course has a capacity, indicating the maximum\nnumber of courses and applicants they can be assigned to, respectively. We thus\nessentially have a many-to-many bipartite matching problem with one-sided\npreferences, which has applications to the assignment of students to optional\ncourses at a university. We consider additive preferences and lexicographic\npreferences as two means of extending preferences over individual courses to\npreferences over bundles of courses. We additionally focus on the cases that\ncourses have prerequisite constraints and where courses may be corequisites.\n  For these extensions to the basic problem, we present the following\nalgorithmic results, which are mainly concerned with the computation of Pareto\noptimal matchings (POMs). Firstly, we consider compulsory prerequisites. For\nadditive preferences, we show that the problem of finding a POM is NP-hard. On\nthe other hand, in the case of lexicographic preferences we give a\npolynomial-time algorithm for finding a POM, based on the well-known sequential\nmechanism. However we show that the problem of deciding whether a given\nmatching is Pareto optimal is co-NP-complete. We further prove that finding a\nmaximum cardinality (Pareto optimal) matching is NP-hard. Under alternative\nprerequisites, we show that finding a POM is NP-hard for either additive or\nlexicographic preferences. Finally we consider corequisites. We prove that, as\nin the case of compulsory prerequisites, finding a POM is NP-hard for additive\npreferences, though solvable in polynomial time for lexicographic preferences.\nIn the latter case, the problem of finding a maximum cardinality POM is NP-hard\nand very difficult to approximate\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 20:34:44 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 21:28:16 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 08:11:06 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Cechlarova", "Katarina", ""], ["Klaus", "Bettina", ""], ["Manlove", "David F.", ""]]}, {"id": "1603.00963", "submitter": "Newton Campbell Jr", "authors": "Newton Campbell Jr", "title": "Using Quadrilaterals to Compute the Shortest Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new heuristic for the A* algorithm that references a data\nstructure significantly smaller than that of ALT. We characterize the behavior\nof this new heuristic based on a dual landmark configuration that leverages\nquadrilateral inequalities to identify the lower bound for shortest path. Using\nthis approach, we demonstrate both the utility and detriments of using polygon\ninequalities aside from the triangle inequality to establish lower bounds for\nshortest path queries. While this new heuristic does not dominate previous\nheuristics based on triangle inequalities, the inverse is true, as well.\nFurther, we demonstrate that an A* heuristic function does not necessarily\noutperform another heuristic that it dominates. In comparison to other landmark\nmethods, the new heuristic maintains a larger average search space while\ncommonly decreasing the number of computed arithmetic operations. The new\nheuristic can significantly outperform previous methods, particularly in graphs\nwith larger path lengths. The characterization of the use of these inequalities\nfor bounding offers insight into its applications in other theoretical spaces.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:44:45 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Campbell", "Newton", "Jr"]]}, {"id": "1603.00973", "submitter": "Zachary Friggstad", "authors": "Zachary Friggstad and Yifeng Zhang", "title": "Tight Analysis of a Multiple-Swap Heuristic for Budgeted Red-Blue Median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Budgeted Red-Blue Median is a generalization of classic $k$-Median in that\nthere are two sets of facilities, say $\\mathcal{R}$ and $\\mathcal{B}$, that can\nbe used to serve clients located in some metric space. The goal is to open\n$k_r$ facilities in $\\mathcal{R}$ and $k_b$ facilities in $\\mathcal{B}$ for\nsome given bounds $k_r, k_b$ and connect each client to their nearest open\nfacility in a way that minimizes the total connection cost.\n  We extend work by Hajiaghayi, Khandekar, and Kortsarz [2012] and show that a\nmultiple-swap local search heuristic can be used to obtain a\n$(5+\\epsilon)$-approximation for Budgeted Red-Blue Median for any constant\n$\\epsilon > 0$. This is an improvement over their single swap analysis and\nbeats the previous best approximation guarantee of 8 by Swamy [2014].\n  We also present a matching lower bound showing that for every $p \\geq 1$,\nthere are instances of Budgeted Red-Blue Median with local optimum solutions\nfor the $p$-swap heuristic whose cost is $5 + \\Omega\\left(\\frac{1}{p}\\right)$\ntimes the optimum solution cost. Thus, our analysis is tight up to the lower\norder terms. In particular, for any $\\epsilon > 0$ we show the single-swap\nheuristic admits local optima whose cost can be as bad as $7-\\epsilon$ times\nthe optimum solution cost.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 05:14:05 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Friggstad", "Zachary", ""], ["Zhang", "Yifeng", ""]]}, {"id": "1603.00977", "submitter": "EPTCS", "authors": "Mahdi Amani (Universit\\`a di Pisa, Pisa, Italy), Abbas Nowzari-Dalini\n  (UT, Tehran, Iran)", "title": "Generation, Ranking and Unranking of Ordered Trees with Degree Bounds", "comments": "In Proceedings DCM 2015, arXiv:1603.00536", "journal-ref": "EPTCS 204, 2016, pp. 31-45", "doi": "10.4204/EPTCS.204.4", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of generating, ranking and unranking of unlabeled\nordered trees whose nodes have maximum degree of $\\Delta$. This class of trees\nrepresents a generalization of chemical trees. A chemical tree is an unlabeled\ntree in which no node has degree greater than 4. By allowing up to $\\Delta$\nchildren for each node of chemical tree instead of 4, we will have a\ngeneralization of chemical trees. Here, we introduce a new encoding over an\nalphabet of size 4 for representing unlabeled ordered trees with maximum degree\nof $\\Delta$. We use this encoding for generating these trees in A-order with\nconstant average time and O(n) worst case time. Due to the given encoding, with\na precomputation of size and time O(n^2) (assuming $\\Delta$ is constant), both\nranking and unranking algorithms are also designed taking O(n) and O(nlogn)\ntime complexities.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 05:33:50 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Amani", "Mahdi", "", "Universit\u00e0 di Pisa, Pisa, Italy"], ["Nowzari-Dalini", "Abbas", "", "UT, Tehran, Iran"]]}, {"id": "1603.01060", "submitter": "Laura Carrea", "authors": "Laura Carrea and Alexei Vernitski and Martin Reed", "title": "Yes-no Bloom filter: A way of representing sets with fewer false\n  positives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bloom filter (BF) is a space efficient randomized data structure\nparticularly suitable to represent a set supporting approximate membership\nqueries. BFs have been extensively used in many applications especially in\nnetworking due to their simplicity and flexibility. The performances of BFs\nmainly depends on query overhead, space requirements and false positives. The\naim of this paper is to focus on false positives. Inspired by the recent\napplication of the BF in a novel multicast forwarding fabric for information\ncentric networks, this paper proposes the yes-no BF, a new way of representing\na set, based on the BF, but with significantly lower false positives and no\nfalse negatives. Although it requires slightly more processing at the stage of\nits formation, it offers the same processing requirements for membership\nqueries as the BF. After introducing the yes-no BF, we show through\nsimulations, that it has better false positive performance than the BF.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 11:43:05 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Carrea", "Laura", ""], ["Vernitski", "Alexei", ""], ["Reed", "Martin", ""]]}, {"id": "1603.01107", "submitter": "Daniel Tischner", "authors": "Daniel Tischner", "title": "Minimization of B\\\"uchi Automata using Fair Simulation", "comments": "Bachelor's thesis, 67 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an algorithm, which reduces the size of B\\\"uchi automata using\nfair simulation. Its time complexity is $\\mathcal{O}(|Q|^4 \\cdot |\\Delta|^2)$,\nthe space complexity is $\\mathcal{O}(|Q| \\cdot |\\Delta|)$.\n  Simulation is a common approach for minimizing $\\omega$-automata such as\nB\\\"uchi automata. Direct simulation, delayed simulation and fair simulation are\ndifferent types of simulation. As we will show, minimization based on direct or\ndelayed simulation is conceptually simple. Whereas the algorithm based on fair\nsimulation is more complex. However, fair simulation allows a stronger\nminimization of the automaton.\n  Further, we illustrate the theory behind the algorithm, cover optimizations\nuseful in practice, give experimental results and compare our technique to\nother minimization strategies.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 14:24:44 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Tischner", "Daniel", ""]]}, {"id": "1603.01181", "submitter": "David Peleg", "authors": "Neta Marcus and David Peleg", "title": "The Domination Game: Proving the 3/5 Conjecture on Isolate-Free Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the domination game, where two players, Dominator and Staller,\nconstruct together a dominating set M in a given graph, by alternately\nselecting vertices into M. Each move must increase the size of the dominated\nset. The players have opposing goals: Dominator wishes M to be as small as\npossible, and Staller has the opposite goal. Kinnersley, West and Zamani\nconjectured that when both players play optimally on an isolate-free forest,\nthere is a guaranteed upper bound for the size of the dominating set that\ndepends only on the size n of the forest. This bound is 3n/5 when the first\nplayer is Dominator, and (3n+2)/5 when the first player is Staller. The\nconjecture was proved for specific families of forests by Kinnersley et al. and\nlater extended by Bujtas. Here we prove it for all isolate-free forests, by\nsupplying an algorithm for Dominator that guarantees the desired bound.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 17:10:11 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Marcus", "Neta", ""], ["Peleg", "David", ""]]}, {"id": "1603.01191", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Artem V. Pyatkin", "title": "A fixed-parameter algorithm for a routing open shop problem: unit\n  processing times, few machines and locations", "comments": "Compared to the previous version, gives a description of the\n  algorithm in pseudocode, simplifies many proofs, corrects the incorrect Lemma\n  5.5 of the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The open shop problem is to find a minimum makespan schedule to process each\njob $J_i$ on each machine $M_q$ for $p_{iq}$ time such that, at any time, each\nmachine processes at most one job and each job is processed by at most one\nmachine. We study a problem variant in which the jobs are located in the\nvertices of an edge-weighted graph. The weights determine the time needed for\nthe machines to travel between jobs in different vertices. We show that the\nproblem with $m$ machines and $n$ unit-time jobs in $g$ vertices is solvable in\n$2^{O(gm^2\\log gm)}+O(mn\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 17:29:49 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 10:18:09 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 03:37:38 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Pyatkin", "Artem V.", ""]]}, {"id": "1603.01450", "submitter": "arXiv Admin", "authors": "Hossein Vahabi, Paul Lagr\\'ee, Claire Vernade, Olivier Capp\\'e", "title": "Sequential ranking under random semi-bandit feedback", "comments": "This submission has been withdrawn by arXiv administrators due to\n  irreconcilable authorship dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many web applications, a recommendation is not a single item suggested to\na user but a list of possibly interesting contents that may be ranked in some\ncontexts. The combinatorial bandit problem has been studied quite extensively\nthese last two years and many theoretical results now exist : lower bounds on\nthe regret or asymptotically optimal algorithms. However, because of the\nvariety of situations that can be considered, results are designed to solve the\nproblem for a specific reward structure such as the Cascade Model. The present\nwork focuses on the problem of ranking items when the user is allowed to click\non several items while scanning the list from top to bottom.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 13:25:56 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 15:36:22 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Vahabi", "Hossein", ""], ["Lagr\u00e9e", "Paul", ""], ["Vernade", "Claire", ""], ["Capp\u00e9", "Olivier", ""]]}, {"id": "1603.01486", "submitter": "David Harris", "authors": "David G. Harris, Johannes Schneider, Hsin-Hao Su", "title": "Distributed $(\\Delta+1)$-Coloring in Sublogarithmic Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new randomized distributed algorithm for $(\\Delta+1)$-coloring in\nthe LOCAL model, running in $O(\\sqrt{\\log \\Delta})+ 2^{O(\\sqrt{\\log \\log n})}$\nrounds in a graph of maximum degree~$\\Delta$. This implies that the\n$(\\Delta+1)$-coloring problem is easier than the maximal independent set\nproblem and the maximal matching problem, due to their lower bounds of $\\Omega\n\\left( \\min \\left( \\sqrt{\\frac{\\log n}{\\log \\log n}}, \\frac{\\log \\Delta}{\\log\n\\log \\Delta} \\right) \\right)$ by Kuhn, Moscibroda, and Wattenhofer [PODC'04].\nOur algorithm also extends to list-coloring where the palette of each node\ncontains $\\Delta+1$ colors. We extend the set of distributed symmetry-breaking\ntechniques by performing a decomposition of graphs into dense and sparse parts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 14:58:44 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 12:36:06 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 14:26:31 GMT"}, {"version": "v4", "created": "Wed, 17 Jan 2018 21:33:13 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Harris", "David G.", ""], ["Schneider", "Johannes", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1603.01508", "submitter": "Robert Kleinberg", "authors": "Arpita Ghosh and Robert Kleinberg", "title": "Inferential Privacy Guarantees for Differentially Private Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correlations and network structure amongst individuals in datasets\ntoday---whether explicitly articulated, or deduced from biological or\nbehavioral connections---pose new issues around privacy guarantees, because of\ninferences that can be made about one individual from another's data. This\nmotivates quantifying privacy in networked contexts in terms of \"inferential\nprivacy\"---which measures the change in beliefs about an individual's data from\nthe result of a computation---as originally proposed by Dalenius in the 1970's.\nInferential privacy is implied by differential privacy when data are\nindependent, but can be much worse when data are correlated; indeed, simple\nexamples, as well as a general impossibility theorem of Dwork and Naor,\npreclude the possibility of achieving non-trivial inferential privacy when the\nadversary can have arbitrary auxiliary information. In this paper, we ask how\ndifferential privacy guarantees translate to guarantees on inferential privacy\nin networked contexts: specifically, under what limitations on the adversary's\ninformation about correlations, modeled as a prior distribution over datasets,\ncan we deduce an inferential guarantee from a differential one?\n  We prove two main results. The first result pertains to distributions that\nsatisfy a natural positive-affiliation condition, and gives an upper bound on\nthe inferential privacy guarantee for any differentially private mechanism.\nThis upper bound is matched by a simple mechanism that adds Laplace noise to\nthe sum of the data. The second result pertains to distributions that have weak\ncorrelations, defined in terms of a suitable \"influence matrix\". The result\nprovides an upper bound for inferential privacy in terms of the differential\nprivacy parameter and the spectral norm of this matrix.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 15:50:24 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 18:52:06 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Ghosh", "Arpita", ""], ["Kleinberg", "Robert", ""]]}, {"id": "1603.01512", "submitter": "Venkatesan Guruswami", "authors": "Venkatesan Guruswami", "title": "Rapidly Mixing Markov Chains: A Comparison of Techniques (A Survey)", "comments": "Unpublished and written in 2000; posted in original, unedited form to\n  provide a permanent URL for citations; Disclaimer: This unpublished survey\n  was written in 2000, and is being posted unedited in its original form, in\n  response to requests for a permanent URL that can be cited. It is thus\n  outdated and does not reflect the state of the art in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey existing techniques to bound the mixing time of Markov chains. The\nmixing time is related to a geometric parameter called conductance which is a\nmeasure of edge-expansion. Bounds on conductance are typically obtained by a\ntechnique called \"canonical paths\" where the idea is to find a set of paths,\none between every source-destination pair, such that no edge is heavily\ncongested. However, the canonical paths approach cannot always show rapid\nmixing of a rapidly mixing chain. This drawback disappears if we allow the flow\nbetween a pair of states to be spread along multiple paths. We prove that for a\nlarge class of Markov chains canonical paths does capture rapid mixing.\nAllowing multiple paths to route the flow still does help a great deal in\nproofs, as illustrated by a result of Morris & Sinclair (FOCS'99) on the rapid\nmixing of a Markov chain for sampling 0/1 knapsack solutions.\n  A different approach to prove rapid mixing is \"Coupling\". Path Coupling is a\nvariant discovered by Bubley & Dyer (FOCS'97) that often tremendously reduces\nthe complexity of designing good Couplings. We present several applications of\nPath Coupling in proofs of rapid mixing. These invariably lead to much better\nbounds on mixing time than known using conductance, and moreover Coupling based\nproofs are typically simpler. This motivates the question of whether Coupling\ncan be made to work whenever the chain is rapidly mixing. This question was\nanswered in the negative by Kumar & Ramesh (FOCS'99), who showed that no\nCoupling strategy can prove the rapid mixing of the Jerrum-Sinclair chain for\nsampling perfect and near-perfect matchings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 16:00:13 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Guruswami", "Venkatesan", ""]]}, {"id": "1603.01583", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Pawe{\\l} Gawrychowski, Jukka Suomela, Przemys{\\l}aw Uzna\\'nski", "title": "Randomized algorithms for finding a majority element", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ colored balls, we want to detect if more than $\\lfloor n/2\\rfloor$\nof them have the same color, and if so find one ball with such majority color.\nWe are only allowed to choose two balls and compare their colors, and the goal\nis to minimize the total number of such operations. A well-known exercise is to\nshow how to find such a ball with only $2n$ comparisons while using only a\nlogarithmic number of bits for bookkeeping. The resulting algorithm is called\nthe Boyer--Moore majority vote algorithm. It is known that any deterministic\nmethod needs $\\lceil 3n/2\\rceil-2$ comparisons in the worst case, and this is\ntight. However, it is not clear what is the required number of comparisons if\nwe allow randomization. We construct a randomized algorithm which always\ncorrectly finds a ball of the majority color (or detects that there is none)\nusing, with high probability, only $7n/6+o(n)$ comparisons. We also prove that\nthe expected number of comparisons used by any such randomized method is at\nleast $1.019n$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 19:29:56 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 13:41:50 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2016 22:05:49 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Suomela", "Jukka", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1603.01607", "submitter": "Newton Campbell Jr", "authors": "Newton Campbell Jr", "title": "Computing Shortest Paths Using A*, Landmarks, and Polygon Inequalities\n  (Abstract)", "comments": "Abstract for poster presented at the SIAM 2015 Workshop on Network\n  Science in Snowbird, UT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new heuristic for the A* algorithm that references a data\nstructure much smaller than the one required by the ALT heuristic. This\nheuristic's benefits are permitted by a new approach for computing lower bounds\nusing generalized polygon inequalities, leveraging distance information from\ntwo landmarks as opposed to the common single landmark paradigm. In this paper,\nwe demonstrate that this heuristic stores a reduced amount of preprocessing\ninformation in comparison to previous landmark algorithms while performing\nfaster search queries.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 20:56:53 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Campbell", "Newton", "Jr"]]}, {"id": "1603.01733", "submitter": "David Woodruff", "authors": "David P. Woodruff", "title": "New Algorithms for Heavy Hitters in Data Streams", "comments": "A preliminary version of this paper will appear as an invited paper\n  in ICDT, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An old and fundamental problem in databases and data streams is that of\nfinding the heavy hitters, also known as the top-$k$, most popular items,\nfrequent items, elephants, or iceberg queries. There are several variants of\nthis problem, which quantify what it means for an item to be frequent,\nincluding what are known as the $\\ell_1$-heavy hitters and $\\ell_2$-heavy\nhitters. There are a number of algorithmic solutions for these problems,\nstarting with the work of Misra and Gries, as well as the CountMin and\nCountSketch data structures, among others.\n  In this survey paper, accompanying an ICDT invited talk, we cover several\nrecent results developed in this area, which improve upon the classical\nsolutions to these problems. In particular, with coauthors we develop new\nalgorithms for finding $\\ell_1$-heavy hitters and $\\ell_2$-heavy hitters, with\nsignificantly less memory required than what was known, and which are optimal\nin a number of parameter regimes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 14:38:21 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Woodruff", "David P.", ""]]}, {"id": "1603.01740", "submitter": "Krzysztof Fleszar", "authors": "Krzysztof Fleszar and Matthias Mnich and Joachim Spoerhase", "title": "New Algorithms for Maximum Disjoint Paths Based on Tree-Likeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical NP-hard problems of finding maximum-size subsets from\ngiven sets of $k$ terminal pairs that can be routed via edge-disjoint paths\n(MaxEDP) or node-disjoint paths (MaxNDP) in a given graph. The approximability\nof MaxEDP/NDP is currently not well understood; the best known lower bound is\n$\\Omega(\\log^{1/2-\\epsilon}{n})$, assuming\nNP$~\\not\\subseteq~$ZPTIME$(n^{\\mathrm{poly}\\log n})$. This constitutes a\nsignificant gap to the best known approximation upper bound of $O(\\sqrt{n})$\ndue to Chekuri et al. (2006) and closing this gap is currently one of the big\nopen problems in approximation algorithms. In their seminal paper, Raghavan and\nThompson (Combinatorica, 1987) introduce the technique of randomized rounding\nfor LPs; their technique gives an $O(1)$-approximation when edges (or nodes)\nmay be used by $O(\\frac{\\log n}{\\log\\log n})$ paths.\n  In this paper, we strengthen the above fundamental results. We provide new\nbounds formulated in terms of the feedback vertex set number $r$ of a graph,\nwhich measures its vertex deletion distance to a forest. In particular, we\nobtain the following.\n  * For MaxEDP, we give an $O(\\sqrt{r}\\cdot \\log^{1.5}{kr})$-approximation\nalgorithm. As $r\\leq n$, up to logarithmic factors, our result strengthens the\nbest known ratio $O(\\sqrt{n})$ due to Chekuri et al.\n  * Further, we show how to route $\\Omega(\\mathrm{OPT})$ pairs with congestion\n$O(\\frac{\\log{kr}}{\\log\\log{kr}})$, strengthening the bound obtained by the\nclassic approach of Raghavan and Thompson.\n  * For MaxNDP, we give an algorithm that gives the optimal answer in time\n$(k+r)^{O(r)}\\cdot n$. If $r$ is at most triple-exponential in $k$, this\nimproves the best known algorithm for MaxNDP with parameter $k$, by\nKawarabayashi and Wollan (STOC 2010).\n  We complement these positive results by proving that MaxEDP is NP-hard even\nfor $r=1$, and MaxNDP is W$[1]$-hard for parameter $r$.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 16:50:09 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 15:07:00 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Fleszar", "Krzysztof", ""], ["Mnich", "Matthias", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1603.01786", "submitter": "Sid Chi-Kin Chau", "authors": "Majid Khonji, Areg Karapetyan, Khaled Elbassioni and Sid Chi-Kin Chau", "title": "Complex-demand Scheduling Problem with Application in Smart Grid", "comments": "This paper appears in Theoretical Computer Science. A preliminary\n  version appeared in the 22nd International Conference on Computing and\n  Combinatorics, COCOON 2016, Ho Chi Minh City, Vietnam, August 2-4, 2016", "journal-ref": "Theoretical Computer Science, Volume 761, 21 February 2019, Pages\n  34-50", "doi": "10.1016/j.tcs.2018.08.023", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling complex-valued demands over a\ndiscretized time horizon. Given a set of users, each user is associated with a\nset of demands representing different power consumption preferences. A demand\nis represented by a complex number, a time interval, and a utility value\nobtained if it is satisfied. At each time slot, the magnitude of the total\nselected demands should not exceed a given generation capacity. This naturally\ncaptures the supply constraints in alternating current (AC) electric systems.\nIn this paper, we consider maximizing the aggregate user utility subject to\npower supply limits over a time horizon. We present approximation algorithms\ncharacterized by the maximum angle $\\phi$ between any two complex-valued\ndemands. More precisely, a PTAS is presented for the case $\\phi \\in\n[0,\\tfrac{\\pi}{2}]$, a bi-criteria FPTAS for $\\phi \\in [0,{\\pi} \\mbox{-}\n\\varepsilon]$ for any polynomially small $\\varepsilon$, assuming the number of\ntime slots in the discretized time horizon is a constant. Furthermore, if the\nnumber of time slots is part of the input, we present a reduction to the\nreal-valued unsplittable flow problem on a path with only a constant\napproximation ratio. Finally, we present a practical greedy algorithm for the\nsingle time slot case with an approximation ratio of $\\tfrac{1}{2}\\cos\n\\frac{\\phi}{2}$ and a running time complexity of only ${O}(N\\log N)$, $N$\nstanding for the aggregate number of user demands, which can be implemented\nefficiently in practice.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 03:56:22 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 13:21:58 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 02:46:27 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Khonji", "Majid", ""], ["Karapetyan", "Areg", ""], ["Elbassioni", "Khaled", ""], ["Chau", "Sid Chi-Kin", ""]]}, {"id": "1603.01887", "submitter": "Guy Rothblum", "authors": "Cynthia Dwork and Guy N. Rothblum", "title": "Concentrated Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Concentrated Differential Privacy, a relaxation of Differential\nPrivacy enjoying better accuracy than both pure differential privacy and its\npopular \"(epsilon,delta)\" relaxation without compromising on cumulative privacy\nloss over multiple computations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 22:41:12 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 16:29:29 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Dwork", "Cynthia", ""], ["Rothblum", "Guy N.", ""]]}, {"id": "1603.01895", "submitter": "Frederik Mallmann-Trenn", "authors": "Petra Berenbrink, George Giakkoupis, Anne-Marie Kermarrec, and\n  Frederik Mallmann-Trenn", "title": "Bounds on the Voter Model in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the voter model, each node of a graph has an opinion, and in every round\neach node chooses independently a random neighbour and adopts its opinion. We\nare interested in the consensus time, which is the first point in time where\nall nodes have the same opinion. We consider dynamic graphs in which the edges\nare rewired in every round (by an adversary) giving rise to the graph sequence\n$G_1, G_2, \\dots $, where we assume that $G_i$ has conductance at least\n$\\phi_i$. We assume that the degrees of nodes don't change over time as one can\nshow that the consensus time can become super-exponential otherwise. In the\ncase of a sequence of $d$-regular graphs, we obtain asymptotically tight\nresults. Even for some static graphs, such as the cycle, our results improve\nthe state of the art. Here we show that the expected number of rounds until all\nnodes have the same opinion is bounded by $O(m/(d_{min} \\cdot \\phi))$, for any\ngraph with $m$ edges, conductance $\\phi$, and degrees at least $d_{min}$. In\naddition, we consider a biased dynamic voter model, where each opinion $i$ is\nassociated with a probability $P_i$, and when a node chooses a neighbour with\nthat opinion, it adopts opinion $i$ with probability $P_i$ (otherwise the node\nkeeps its current opinion). We show for any regular dynamic graph, that if\nthere is an $\\epsilon>0$ difference between the highest and second highest\nopinion probabilities, and at least $\\Omega(\\log n)$ nodes have initially the\nopinion with the highest probability, then all nodes adopt w.h.p. that opinion.\nWe obtain a bound on the convergences time, which becomes $O(\\log n/\\phi)$ for\nstatic graphs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 23:14:15 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 14:14:16 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Berenbrink", "Petra", ""], ["Giakkoupis", "George", ""], ["Kermarrec", "Anne-Marie", ""], ["Mallmann-Trenn", "Frederik", ""]]}, {"id": "1603.01925", "submitter": "Longkun Guo l", "authors": "Longkun Guo, Peng Li", "title": "On the Complexity of Detecting Constrained Negative Cost Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a positive integer $k$ and a directed graph with a cost on each edge,\nthe $k$-length negative cost cycle ($k$\\emph{LNCC}) problem is to determine\nwhether there exists a negative cost cycle with at least $k$ edges, and the\nfixed-point \\emph{$k$-}length negative cost cycle \\emph{trail (FP$k$LNCCT)}\nproblem is to determine whether there exists a negative trail enrouting a given\nvertex (as the fixed point) and containing only cycles with at least $k$ edges.\nThe $k$\\emph{LNCC} problem first emerged in deadlock avoidance in synchronized\nstreaming computing network \\cite{spaa10}, generalizing two famous problems:\nnegative cycle detection and the $k$-cycle problem. As a warmup by-production,\nthe paper first shows that \\emph{FP$k$LNCCT is }${\\cal NP}$-complete in\nmultigraph\\emph{ }even for\\emph{ $k=3$} by reducing from the \\emph{3SAT}\nproblem. Then as the main result, we prove the ${\\cal NP}$-completeness of\n$k$\\emph{LNCC} by giving a sophisticated reduction from the 3 Occurrence\n3-Satisfiability (\\emph{3O3SAT}) problem, a known ${\\cal NP}$-complete special\ncase of 3SAT in which a variable occurs at most three times. The complexity\nresult is interesting, since polynomial time algorithms are known for both\n$2$\\emph{LNCC} (essentially no restriction on the value of $k$) and the\n$k$-cycle problem of fixed $k$. This paper closes the open problem proposed by\nLi et al. in \\cite{spaa10} whether $k$\\emph{LNCC} admits polynomial-time\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 03:53:17 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 17:28:39 GMT"}, {"version": "v3", "created": "Sat, 20 May 2017 03:08:53 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Guo", "Longkun", ""], ["Li", "Peng", ""]]}, {"id": "1603.01977", "submitter": "Maurice Chandoo", "authors": "Maurice Chandoo", "title": "On the Implicit Graph Conjecture", "comments": "13 pages, MFCS 2016", "journal-ref": null, "doi": "10.4230/LIPIcs.MFCS.2016.23", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implicit graph conjecture states that every sufficiently small,\nhereditary graph class has a labeling scheme with a polynomial-time computable\nlabel decoder. We approach this conjecture by investigating classes of label\ndecoders defined in terms of complexity classes such as P and EXP. For\ninstance, GP denotes the class of graph classes that have a labeling scheme\nwith a polynomial-time computable label decoder. Until now it was not even\nknown whether GP is a strict subset of GR. We show that this is indeed the case\nand reveal a strict hierarchy akin to classical complexity. We also show that\nclasses such as GP can be characterized in terms of graph parameters. This\ncould mean that certain algorithmic problems are feasible on every graph class\nin GP. Lastly, we define a more restrictive class of label decoders using\nfirst-order logic that already contains many natural graph classes such as\nforests and interval graphs. We give an alternative characterization of this\nclass in terms of directed acyclic graphs. By showing that some small,\nhereditary graph class cannot be expressed with such label decoders a weaker\nform of the implicit graph conjecture could be disproven.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 08:55:26 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 12:29:23 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 09:27:02 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Chandoo", "Maurice", ""]]}, {"id": "1603.02063", "submitter": "Diego Seco", "authors": "Nieves R. Brisaboa, Guillermo De Bernardo, Roberto Konow, Gonzalo\n  Navarro, Diego Seco", "title": "Aggregated 2D Range Queries on Clustered Points", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Information Systems, Volume 60, Pages 34-49, 2016", "doi": "10.1016/j.is.2016.03.004", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient processing of aggregated range queries on two-dimensional grids is\na common requirement in information retrieval and data mining systems, for\nexample in Geographic Information Systems and OLAP cubes. We introduce a\ntechnique to represent grids supporting aggregated range queries that requires\nlittle space when the data points in the grid are clustered, which is common in\npractice. We show how this general technique can be used to support two\nimportant types of aggregated queries, which are ranked range queries and\ncounting range queries. Our experimental evaluation shows that this technique\ncan speed up aggregated queries up to more than an order of magnitude, with a\nsmall space overhead.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 13:45:36 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 20:37:53 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["De Bernardo", "Guillermo", ""], ["Konow", "Roberto", ""], ["Navarro", "Gonzalo", ""], ["Seco", "Diego", ""]]}, {"id": "1603.02321", "submitter": "Arnold Filtser", "authors": "Michael Elkin, Arnold Filtser, Ofer Neiman", "title": "Terminal Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study {\\em terminal embeddings}, in which one is given a\nfinite metric $(X,d_X)$ (or a graph $G=(V,E)$) and a subset $K \\subseteq X$ of\nits points are designated as {\\em terminals}. The objective is to embed the\nmetric into a normed space, while approximately preserving all distances among\npairs that contain a terminal. We devise such embeddings in various settings,\nand conclude that even though we have to preserve $\\approx|K|\\cdot |X|$ pairs,\nthe distortion depends only on $|K|$, rather than on $|X|$.\n  We also strengthen this notion, and consider embeddings that approximately\npreserve the distances between {\\em all} pairs, but provide improved distortion\nfor pairs containing a terminal. Surprisingly, we show that such embeddings\nexist in many settings, and have optimal distortion bounds both with respect to\n$X \\times X$ and with respect to $K \\times X$.\n  Moreover, our embeddings have implications to the areas of Approximation and\nOnline Algorithms. In particular, [ALN08] devised an $\\tilde{O}(\\sqrt{\\log\nr})$-approximation algorithm for sparsest-cut instances with $r$ demands.\nBuilding on their framework, we provide an $\\tilde{O}(\\sqrt{\\log\n|K|})$-approximation for sparsest-cut instances in which each demand is\nincident on one of the vertices of $K$ (aka, terminals). Since $|K| \\le r$, our\nbound generalizes that of [ALN08].\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 22:26:54 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Elkin", "Michael", ""], ["Filtser", "Arnold", ""], ["Neiman", "Ofer", ""]]}, {"id": "1603.02324", "submitter": "G\\\"okalp Demirci", "authors": "G\\\"okalp Demirci and Shi Li", "title": "Constant Approximation for Capacitated $k$-Median with $(1 +\n  \\epsilon)$-Capacity Violation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Capacitated k-Median problem for which existing constant-factor\napproximation algorithms are all pseudo-approximations that violate either the\ncapacities or the upper bound k on the number of open facilities. Using the\nnatural LP relaxation for the problem, one can only hope to get the violation\nfactor down to 2. Li [SODA'16] introduced a novel LP to go beyond the limit of\n2 and gave a constant-factor approximation algorithm that opens $(1 + \\epsilon\n)k$ facilities.\n  We use the configuration LP of Li [SODA'16] to give a constant-factor\napproximation for the Capacitated k-Median problem in a seemingly harder\nconfiguration: we violate only the capacities by $1 + \\epsilon $. This result\nsettles the problem as far as pseudo-approximation algorithms are concerned.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 22:31:41 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 02:50:11 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Demirci", "G\u00f6kalp", ""], ["Li", "Shi", ""]]}, {"id": "1603.02457", "submitter": "Jhoirene Clemente", "authors": "Jeffrey Aborot, Henry Adorna, Jhoirene Clemente", "title": "On Self-Reducibility and Reoptimization of Closest Substring Problem", "comments": "8 pages", "journal-ref": "Philippine Computing Journal Vol 10 (2): 1- 7 (2015)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we define the reoptimization variant of the closest substring\nproblem (CSP) under sequence addition. We show that, even with the additional\ninformation we have about the problem instance, the problem of finding a\nclosest substring is still NP-hard. We investigate the combinatorial property\nof optimization problems called self-reducibility. We show that problems that\nare polynomial-time reducible to self-reducible problems also exhibits the same\nproperty. We illustrate this in the context of CSP. We used the property to\nshow that although we cannot improve the approximability of the problem, we can\nimprove the running time of the existing PTAS for CSP.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 10:19:06 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Aborot", "Jeffrey", ""], ["Adorna", "Henry", ""], ["Clemente", "Jhoirene", ""]]}, {"id": "1603.02525", "submitter": "Torsten M\\\"utze", "authors": "Torsten M\\\"utze, Christoph Standke, Veit Wiechert", "title": "A minimum-change version of the Chung-Feller theorem for Dyck paths", "comments": null, "journal-ref": "European Journal of Combinatorics 69:260-275, 2018", "doi": "10.1016/j.ejc.2017.11.003", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Dyck path with $2k$ steps and $e$ flaws is a path in the integer lattice\nthat starts at the origin and consists of $k$ many $\\nearrow$-steps and $k$\nmany $\\searrow$-steps that change the current coordinate by $(1,1)$ or\n$(1,-1)$, respectively, and that has exactly $e$ many $\\searrow$-steps below\nthe line $y=0$. Denoting by $D_{2k}^e$ the set of Dyck paths with $2k$ steps\nand $e$ flaws, the Chung-Feller theorem asserts that the sets\n$D_{2k}^0,D_{2k}^1,\\ldots,D_{2k}^k$ all have the same cardinality\n$\\frac{1}{k+1}\\binom{2k}{k}=C_k$, the $k$-th Catalan number. The standard\ncombinatorial proof of this classical result establishes a bijection $f'$\nbetween $D_{2k}^e$ and $D_{2k}^{e+1}$ that swaps certain parts of the given\nDyck path $x$, with the effect that $x$ and $f'(x)$ may differ in many\npositions. In this paper we strengthen the Chung-Feller theorem by presenting a\nsimple bijection $f$ between $D_{2k}^e$ and $D_{2k}^{e+1}$ which has the\nadditional feature that $x$ and $f(x)$ differ in only two positions (the least\npossible number). We also present an algorithm that allows to compute a\nsequence of applications of $f$ in constant time per generated Dyck path. As an\napplication, we use our minimum-change bijection $f$ to construct cycle-factors\nin the odd graph $O_{2k+1}$ and the middle levels graph $M_{2k+1}$ --- two\nintensively studied families of vertex-transitive graphs --- that consist of\n$C_k$ many cycles of the same length.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 14:11:54 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 15:06:37 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["M\u00fctze", "Torsten", ""], ["Standke", "Christoph", ""], ["Wiechert", "Veit", ""]]}, {"id": "1603.02611", "submitter": "Du\\v{s}an Knop", "authors": "Du\\v{s}an Knop and Martin Kouteck\\'y", "title": "Scheduling meets n-fold Integer Programming", "comments": "14 pages, 3 figures", "journal-ref": "Journal of Scheduling, 2018", "doi": "10.1007/s10951-017-0550-0", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling problems are fundamental in combinatorial optimization. Much work\nhas been done on approximation algorithms for NP-hard cases, but relatively\nlittle is known about exact solutions when some part of the input is a fixed\nparameter. In 2014, Mnich and Wiese initiated a systematic study in this\ndirection.\n  In this paper we continue this study and show that several additional cases\nof fundamental scheduling problems are fixed parameter tractable for some\nnatural parameters. Our main tool is n-fold integer programming, a recent\nvariable dimension technique which we believe to be highly relevant for the\nparameterized complexity community. This paper serves to showcase and highlight\nthis technique.\n  Specifically, we show the following four scheduling problems to be\nfixed-parameter tractable, where p max is the maximum processing time of a job\nand w max is the maximum weight of a job:\n  - Makespan minimization on uniformly related machines $(Q||C_{max} )$\nparameterized by $p_{max}$,\n  - Makespan minimization on unrelated machines $(R||C_{max} )$ parameterized\nby $p_{max}$ and the number of kinds of machines,\n  - Sum of weighted completion times minimization on unrelated machines $(R||\n\\sum w_i C_i )$ parameterized by $p_{max} + w_{max}$ and the number of kinds of\nmachines,\n  - The same problem, $(R|| \\sum w_i C_i),$ parameterized by the number of\ndistinct job times and the number of machines.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:11:14 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 11:38:03 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""]]}, {"id": "1603.02782", "submitter": "Megasthenis Asteris", "authors": "Megasthenis Asteris and Anastasios Kyrillidis and Dimitris\n  Papailiopoulos and Alexandros G. Dimakis", "title": "Bipartite Correlation Clustering -- Maximizing Agreements", "comments": "To appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bipartite Correlation Clustering (BCC) we are given a complete bipartite\ngraph $G$ with `+' and `-' edges, and we seek a vertex clustering that\nmaximizes the number of agreements: the number of all `+' edges within clusters\nplus all `-' edges cut across clusters. BCC is known to be NP-hard.\n  We present a novel approximation algorithm for $k$-BCC, a variant of BCC with\nan upper bound $k$ on the number of clusters. Our algorithm outputs a\n$k$-clustering that provably achieves a number of agreements within a\nmultiplicative ${(1-\\delta)}$-factor from the optimal, for any desired accuracy\n$\\delta$. It relies on solving a combinatorially constrained bilinear\nmaximization on the bi-adjacency matrix of $G$. It runs in time exponential in\n$k$ and $\\delta^{-1}$, but linear in the size of the input.\n  Further, we show that, in the (unconstrained) BCC setting, an\n${(1-\\delta)}$-approximation can be achieved by $O(\\delta^{-1})$ clusters\nregardless of the size of the graph. In turn, our $k$-BCC algorithm implies an\nEfficient PTAS for the BCC objective of maximizing agreements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 05:18:53 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Papailiopoulos", "Dimitris", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1603.02971", "submitter": "Diodato Ferraioli", "authors": "Vincenzo Auletta, Ioannis Caragiannis, Diodato Ferraioli, Clemente\n  Galdi, and Giuseppe Persiano", "title": "Discrete Preference Games in Heterogeneous Social Networks: Subverted\n  Majorities and the Swing Player", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrete preference games in heterogeneous social networks. These\ngames model the interplay between a player's private belief and his/her\npublicly stated opinion (which could be different from the player's belief) as\na strategic game in which the players' strategies are the opinions and the cost\nof an opinion in a state is a convex combination through a parameter\n$\\alpha\\in[0,1]$ of two factors: the disagreement between the player's opinion\nand his/her internal belief and the number of neighbors whose opinions differ\nfrom the one of the player. The parameter $\\alpha$ models how stubborn a player\nis: players with large $\\alpha$ change their opinion only if many neighbors\ndisagree with his/her belief. We consider social networks that are\nheterogeneous in the sense that the parameter $\\alpha$ can vary from player to\nplayer.\n  We ask if it is possible that the belief shared by the majority of the\nplayers does not coincide with the opinion that is publicly announced by the\nmajority of the players in an equilibrium state. Our main result is a\ncharacterization of the social networks that admit an initial belief assignment\nfor which there exists a sequence of best response moves that reach an\nequilibrium in which the initial majority is subverted. Our characterization is\neffective in the sense that can be tested efficiently and the initial belief\nassignment that can be subverted can be computed in time polynomial in the\nnumber of players. Our result is actually stronger as we show that in each\ninitial belief assignment that can be subverted, subversion is actually\nobtained in a very strong way: it only takes one move of a single player, the\nswing player, to lead the social network to a point of no return in which any\nrational move from any player leads to a subverted majority.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 17:20:43 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Auletta", "Vincenzo", ""], ["Caragiannis", "Ioannis", ""], ["Ferraioli", "Diodato", ""], ["Galdi", "Clemente", ""], ["Persiano", "Giuseppe", ""]]}, {"id": "1603.02981", "submitter": "Cameron Musco", "authors": "Cameron Musco, Hsin-Hao Su, Nancy Lynch", "title": "Ant-Inspired Density Estimation via Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many ant species employ distributed population density estimation in\napplications ranging from quorum sensing [Pra05], to task allocation [Gor99],\nto appraisal of enemy colony strength [Ada90]. It has been shown that ants\nestimate density by tracking encounter rates -- the higher the population\ndensity, the more often the ants bump into each other [Pra05,GPT93].\n  We study distributed density estimation from a theoretical perspective. We\nprove that a group of anonymous agents randomly walking on a grid are able to\nestimate their density within a small multiplicative error in few steps by\nmeasuring their rates of encounter with other agents. Despite dependencies\ninherent in the fact that nearby agents may collide repeatedly (and, worse,\ncannot recognize when this happens), our bound nearly matches what would be\nrequired to estimate density by independently sampling grid locations.\n  From a biological perspective, our work helps shed light on how ants and\nother social insects can obtain relatively accurate density estimates via\nencounter rates. From a technical perspective, our analysis provides new tools\nfor understanding complex dependencies in the collision probabilities of\nmultiple random walks. We bound the strength of these dependencies using\n$local\\ mixing\\ properties$ of the underlying graph. Our results extend beyond\nthe grid to more general graphs and we discuss applications to size estimation\nfor social networks and density estimation for robot swarms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 18:00:41 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 15:56:40 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Musco", "Cameron", ""], ["Su", "Hsin-Hao", ""], ["Lynch", "Nancy", ""]]}, {"id": "1603.03019", "submitter": "Michael Haythorpe", "authors": "Michael Haythorpe", "title": "Reducing the generalised Sudoku problem to the Hamiltonian cycle problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalised Sudoku problem with $N$ symbols is known to be NP-complete,\nand hence is equivalent to any other NP-complete problem, even for the standard\nrestricted version where $N$ is a perfect square. In particular, generalised\nSudoku is equivalent to the, classical, Hamiltonian cycle problem. A\nconstructive algorithm is given that reduces generalised Sudoku to the\nHamiltonian cycle problem, where the resultant instance of Hamiltonian cycle\nproblem is sparse, and has $O(N^3)$ vertices. The Hamiltonian cycle problem\ninstance so constructed is a directed graph, and so a (known) conversion to\nundirected Hamiltonian cycle problem is also provided so that it can be\nsubmitted to the best heuristics. A simple algorithm for obtaining the valid\nSudoku solution from the Hamiltonian cycle is provided. Techniques to reduce\nthe size of the resultant graph are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 06:34:03 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Haythorpe", "Michael", ""]]}, {"id": "1603.03024", "submitter": "Yuval Emek", "authors": "Yuval Emek, Shay Kutten, and Roger Wattenhofer", "title": "Online Matching: Haste makes Waste!", "comments": "An extended abstract will appear in Proceedings of ACM STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a new online problem, referred to as \\emph{min-cost\nperfect matching with delays (MPMD)}, defined over a finite metric space (i.e.,\na complete graph with positive edge weights obeying the triangle inequality)\n$\\mathcal{M}$ that is known to the algorithm in advance. Requests arrive in a\ncontinuous time online fashion at the points of $\\mathcal{M}$ and should be\nserved by matching them to each other. The algorithm is allowed to delay its\nrequest matching commitments, but this does not come for free: the total cost\nof the algorithm is the sum of metric distances between matched requests\n\\emph{plus} the sum of times each request waited since it arrived until it was\nmatched. A randomized online MPMD algorithm is presented whose competitive\nratio is $O (\\log^{2} n + \\log \\Delta)$, where $n$ is the number of points in\n$\\mathcal{M}$ and $\\Delta$ is its aspect ratio. The analysis is based on a\nmachinery developed in the context of a new stochastic process that can be\nviewed as two interleaved Poisson processes; surprisingly, this new process\ncaptures precisely the behavior of our algorithm. A related problem in which\nthe algorithm is allowed to clear any unmatched request at a fixed penalty is\nalso addressed. It is suggested that the MPMD problem is merely the tip of the\niceberg for a general framework of online problems with delayed service that\ncaptures many more natural problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 20:43:29 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Emek", "Yuval", ""], ["Kutten", "Shay", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1603.03158", "submitter": "Nathaniel Grammel", "authors": "Nathaniel Grammel, Lisa Hellerstein, Devorah Kletenik, Patrick Lin", "title": "Scenario Submodular Cover", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in Machine Learning can be modeled as submodular optimization\nproblems. Recent work has focused on stochastic or adaptive versions of these\nproblems. We consider the Scenario Submodular Cover problem, which is a\ncounterpart to the Stochastic Submodular Cover problem studied by Golovin and\nKrause. In Scenario Submodular Cover, the goal is to produce a cover with\nminimum expected cost, where the expectation is with respect to an empirical\njoint distribution, given as input by a weighted sample of realizations. In\ncontrast, in Stochastic Submodular Cover, the variables of the input\ndistribution are assumed to be independent, and the distribution of each\nvariable is given as input. Building on algorithms developed by Cicalese et al.\nand Golovin and Krause for related problems, we give two approximation\nalgorithms for Scenario Submodular Cover over discrete distributions. The first\nachieves an approximation factor of O(log Qm), where m is the size of the\nsample and Q is the goal utility. The second, simpler algorithm achieves an\napproximation bound of O(log QW), where Q is the goal utility and W is the sum\nof the integer weights. (Both bounds assume an integer-valued utility\nfunction.) Our results yield approximation bounds for other problems involving\nnon-independent distributions that are explicitly specified by their support.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 06:43:52 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Grammel", "Nathaniel", ""], ["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""], ["Lin", "Patrick", ""]]}, {"id": "1603.03178", "submitter": "Samet Oymak", "authors": "Samet Oymak", "title": "Near-Optimal Sample Complexity Bounds for Circulant Binary Embedding", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary embedding is the problem of mapping points from a high-dimensional\nspace to a Hamming cube in lower dimension while preserving pairwise distances.\nAn efficient way to accomplish this is to make use of fast embedding techniques\ninvolving Fourier transform e.g.~circulant matrices. While binary embedding has\nbeen studied extensively, theoretical results on fast binary embedding are\nrather limited. In this work, we build upon the recent literature to obtain\nsignificantly better dependencies on the problem parameters. A set of $N$\npoints in $\\mathbb{R}^n$ can be properly embedded into the Hamming cube $\\{\\pm\n1\\}^k$ with $\\delta$ distortion, by using $k\\sim\\delta^{-3}\\log N$ samples\nwhich is optimal in the number of points $N$ and compares well with the optimal\ndistortion dependency $\\delta^{-2}$. Our optimal embedding result applies in\nthe regime $\\log N\\lesssim n^{1/3}$. Furthermore, if the looser condition $\\log\nN\\lesssim \\sqrt{n}$ holds, we show that all but an arbitrarily small fraction\nof the points can be optimally embedded. We believe our techniques can be\nuseful to obtain improved guarantees for other nonlinear embedding problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 08:12:44 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 06:09:12 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Oymak", "Samet", ""]]}, {"id": "1603.03203", "submitter": "Vinod Prasad", "authors": "Vinodprasad P", "title": "A Novel Algorithm for String Matching with Mismatches", "comments": "6 pages, 2 figures. First published in the proceedings of the ICPRAM\n  2016, Rome 22-26 Feb 2016", "journal-ref": "ICPRAM 2016, 5th International Conference on Pattern Recognition\n  Methods and Applications, Rome 22-26 Feb 2016, pp 638-644", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online algorithm to deal with pattern matching in strings. The\nproblem we investigate is commonly known as string matching with mismatches in\nwhich the objective is to report the number of characters that match when a\npattern is aligned with every location in the text. The novel method we propose\nis based on the frequencies of individual characters in the pattern and the\ntext. The algorithm consumes minimal space in the form of simple arrays, which\nreduces the cost overhead to maintain the complex data structures such as\nsuffix trees or automaton.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 09:55:03 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["P", "Vinodprasad", ""]]}, {"id": "1603.03315", "submitter": "Dimitris Papamichail", "authors": "Dimitris Papamichail, Angela Huang, Andrew Miller, Edward Kennedy,\n  Jan-Lucas Ott, Georgios Papamichail", "title": "Most Compact Parsimonious Trees", "comments": "7 pages, 4 figures, 1 table, submitted for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Construction of phylogenetic trees has traditionally focused on binary trees\nwhere all species appear on leaves, a problem for which numerous efficient\nsolutions have been developed. Certain application domains though, such as\nviral evolution and transmission, paleontology, linguistics, and phylogenetic\nstemmatics, often require phylogeny inference that involves placing input\nspecies on ancestral tree nodes (live phylogeny), and polytomies. These\nrequirements, despite their prevalence, lead to computationally harder\nalgorithmic solutions and have been sparsely examined in the literature to\ndate. In this article we prove some unique properties of most parsimonious live\nphylogenetic trees with polytomies, and describe novel algorithms to find the\nsuch trees without resorting to exhaustive enumeration of all possible tree\ntopologies.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 16:17:33 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 15:41:52 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Papamichail", "Dimitris", ""], ["Huang", "Angela", ""], ["Miller", "Andrew", ""], ["Kennedy", "Edward", ""], ["Ott", "Jan-Lucas", ""], ["Papamichail", "Georgios", ""]]}, {"id": "1603.03490", "submitter": "Christopher Dellin", "authors": "Christopher M. Dellin and Siddhartha S. Srinivasa", "title": "A Unifying Formalism for Shortest Path Problems with Expensive Edge\n  Evaluations via Lazy Best-First Search over Paths with Edge Selectors", "comments": "Extended version of ICAPS-16 conference paper with proofs and timing\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the shortest path problem has myriad applications, the computational\nefficiency of suitable algorithms depends intimately on the underlying problem\ndomain. In this paper, we focus on domains where evaluating the edge weight\nfunction dominates algorithm running time. Inspired by approaches in robotic\nmotion planning, we define and investigate the Lazy Shortest Path class of\nalgorithms which is differentiated by the choice of an edge selector function.\nWe show that several algorithms in the literature are equivalent to this lazy\nalgorithm for appropriate choice of this selector. Further, we propose various\nnovel selectors inspired by sampling and statistical mechanics, and find that\nthese selectors outperform existing algorithms on a set of example problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:50:05 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 06:16:28 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Dellin", "Christopher M.", ""], ["Srinivasa", "Siddhartha S.", ""]]}, {"id": "1603.03505", "submitter": "Yan Gu", "authors": "Guy E. Blelloch and Jeremy T. Fineman and Phillip B. Gibbons and Yan\n  Gu and Julian Shun", "title": "Sorting with Asymmetric Read and Write Costs", "comments": null, "journal-ref": null, "doi": "10.1145/2755573.2755604", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging memory technologies have a significant gap between the cost, both in\ntime and in energy, of writing to memory versus reading from memory. In this\npaper we present models and algorithms that account for this difference, with a\nfocus on write-efficient sorting algorithms. First, we consider the PRAM model\nwith asymmetric write cost, and show that sorting can be performed in\n$O\\left(n\\right)$ writes, $O\\left(n \\log n\\right)$ reads, and logarithmic depth\n(parallel time). Next, we consider a variant of the External Memory (EM) model\nthat charges $\\omega > 1$ for writing a block of size $B$ to the secondary\nmemory, and present variants of three EM sorting algorithms (multi-way\nmergesort, sample sort, and heapsort using buffer trees) that asymptotically\nreduce the number of writes over the original algorithms, and perform roughly\n$\\omega$ block reads for every block write. Finally, we define a variant of the\nIdeal-Cache model with asymmetric write costs, and present write-efficient,\ncache-oblivious parallel algorithms for sorting, FFTs, and matrix\nmultiplication. Adapting prior bounds for work-stealing and\nparallel-depth-first schedulers to the asymmetric setting, these yield parallel\ncache complexity bounds for machines with private caches or with a shared\ncache, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 02:37:36 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Fineman", "Jeremy T.", ""], ["Gibbons", "Phillip B.", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "1603.03836", "submitter": "Amirali Aghazadeh", "authors": "Amirali Aghazadeh, Andrew Lan, Anshumali Shrivastava, Richard Baraniuk", "title": "Near-Isometric Binary Hashing for Large-scale Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scalable algorithm to learn binary hash codes for indexing\nlarge-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent\nhashing scheme that quantizes the output of a learned low-dimensional embedding\nto obtain a binary hash code. In contrast to conventional hashing schemes,\nwhich typically rely on an $\\ell_2$-norm (i.e., average distortion)\nminimization, NIBH is based on a $\\ell_{\\infty}$-norm (i.e., worst-case\ndistortion) minimization that provides several benefits, including superior\ndistance, ranking, and near-neighbor preservation performance. We develop a\npractical and efficient algorithm for NIBH based on column generation that\nscales well to large datasets. A range of experimental evaluations demonstrate\nthe superiority of NIBH over ten state-of-the-art binary hashing schemes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 01:04:50 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Aghazadeh", "Amirali", ""], ["Lan", "Andrew", ""], ["Shrivastava", "Anshumali", ""], ["Baraniuk", "Richard", ""]]}, {"id": "1603.04210", "submitter": "Aniket Basu Roy", "authors": "Aniket Basu Roy, Anil Maheshwari, Sathish Govindarajan, Neeldhara\n  Misra, Subhas C Nandy, Shreyas Shetty", "title": "The Runaway Rectangle Escape Problem", "comments": "26 pages, 7 figures, A preliminary version appeared in the\n  Proceedings of the 26th Canadian Conference on Computational Geometry, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the applications of routing in PCB buses, the Rectangle Escape\nProblem was recently introduced and studied. In this problem, we are given a\nset of rectangles $\\mathcal{S}$ in a rectangular region $R$, and we would like\nto extend these rectangles to one of the four sides of $R$. Define the density\nof a point $p$ in $R$ as the number of extended rectangles that contain $p$.\nThe question is then to find an extension with the smallest maximum density.\n  We consider the problem of maximizing the number of rectangles that can be\nextended when the maximum density allowed is at most $d$. It is known that this\nproblem is polynomially solvable for $d = 1$, and NP-hard for any $d \\geq 2$.\nWe consider approximation and exact algorithms for fixed values of $d$. We also\nshow that a very special case of this problem, when all the rectangles are unit\nsquares from a grid, continues to be NP-hard for $d = 2$.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 11:21:56 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 10:29:43 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Roy", "Aniket Basu", ""], ["Maheshwari", "Anil", ""], ["Govindarajan", "Sathish", ""], ["Misra", "Neeldhara", ""], ["Nandy", "Subhas C", ""], ["Shetty", "Shreyas", ""]]}, {"id": "1603.04330", "submitter": "Sebastiano Vigna", "authors": "Marco Genuzio, Giuseppe Ottaviano, Sebastiano Vigna", "title": "Fast Scalable Construction of (Minimal Perfect Hash) Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in random linear systems on finite fields have paved the way\nfor the construction of constant-time data structures representing static\nfunctions and minimal perfect hash functions using less space with respect to\nexisting techniques. The main obstruction for any practical application of\nthese results is the cubic-time Gaussian elimination required to solve these\nlinear systems: despite they can be made very small, the computation is still\ntoo slow to be feasible.\n  In this paper we describe in detail a number of heuristics and programming\ntechniques to speed up the resolution of these systems by several orders of\nmagnitude, making the overall construction competitive with the standard and\nwidely used MWHC technique, which is based on hypergraph peeling. In\nparticular, we introduce broadword programming techniques for fast equation\nmanipulation and a lazy Gaussian elimination algorithm. We also describe a\nnumber of technical improvements to the data structure which further reduce\nspace usage and improve lookup speed.\n  Our implementation of these techniques yields a minimal perfect hash function\ndata structure occupying 2.24 bits per element, compared to 2.68 for MWHC-based\nones, and a static function data structure which reduces the multiplicative\noverhead from 1.23 to 1.03.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 16:28:06 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 21:15:51 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Genuzio", "Marco", ""], ["Ottaviano", "Giuseppe", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1603.04350", "submitter": "Yuanzhi Li", "authors": "Elad Hazan, Yuanzhi Li", "title": "An optimal algorithm for bandit convex optimization", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online convex optimization against an arbitrary\nadversary with bandit feedback, known as bandit convex optimization. We give\nthe first $\\tilde{O}(\\sqrt{T})$-regret algorithm for this setting based on a\nnovel application of the ellipsoid method to online learning. This bound is\nknown to be tight up to logarithmic factors. Our analysis introduces new tools\nin discrete convex geometry.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 17:15:15 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 17:46:58 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Hazan", "Elad", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1603.04376", "submitter": "Andrew van der Poel", "authors": "Blair D. Sullivan and Andrew van der Poel", "title": "A Fast Parameterized Algorithm for Co-Path Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-CO-PATH SET problem asks, given a graph G and a positive integer k,\nwhether one can delete k edges from G so that the remainder is a collection of\ndisjoint paths. We give a linear-time fpt algorithm with complexity\nO^*(1.588^k) for deciding k-CO-PATH SET, significantly improving the previously\nbest known O^*(2.17^k) of Feng, Zhou, and Wang (2015). Our main tool is a new\nO^*(4^{tw(G)}) algorithm for CO-PATH SET using the Cut&Count framework, where\ntw(G) denotes treewidth. In general graphs, we combine this with a branching\nalgorithm which refines a 6k-kernel into reduced instances, which we prove have\nbounded treewidth.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 18:11:21 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 17:15:10 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 21:23:45 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Sullivan", "Blair D.", ""], ["van der Poel", "Andrew", ""]]}, {"id": "1603.04380", "submitter": "Sebastien Bougleux", "authors": "S\\'ebastien Bougleux, Luc Brun (ENSICAEN)", "title": "Linear Sum Assignment with Edition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of transforming a set of elements into another by a\nsequence of elementary edit operations, namely substitutions, removals and\ninsertions of elements. Each possible edit operation is penalized by a\nnon-negative cost and the cost of a transformation is measured by summing the\ncosts of its operations. A solution to this problem consists in defining a\ntransformation having a minimal cost, among all possible transformations. To\ncompute such a solution, the classical approach consists in representing\nremoval and insertion operations by augmenting the two sets so that they get\nthe same size. This allows to express the problem as a linear sum assignment\nproblem (LSAP), which thus finds an optimal bijection (or permutation, perfect\nmatching) between the two augmented sets. While the LSAP is known to be\nefficiently solvable in polynomial time complexity, for instance with the\nHungarian algorithm, useless time and memory are spent to treat the elements\nwhich have been added to the initial sets. In this report, we show that the\nproblem can be formalized as an extension of the LSAP which considers only one\nadditional element in each set to represent removal and insertion operations. A\nsolution to the problem is no longer represented as a bijection between the two\naugmented sets. We show that the considered problem is a binary linear program\n(BLP) very close to the LSAP. While it can be solved by any BLP solver, we\npropose an adaptation of the Hungarian algorithm which improves the time and\nmemory complexities previously obtained by the approach based on the LSAP. The\nimportance of the improvement increases as the size of the two sets and their\nabsolute difference increase. Based on the analysis of the problem presented in\nthis report, other classical algorithms can be adapted.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 18:40:44 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 13:16:04 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 15:40:08 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Bougleux", "S\u00e9bastien", "", "ENSICAEN"], ["Brun", "Luc", "", "ENSICAEN"]]}, {"id": "1603.04451", "submitter": "Ante \\'Custi\\'c", "authors": "Ante \\'Custi\\'c, Ruonan Zhang, Abraham P. Punnen", "title": "The Quadratic Minimum Spanning Tree Problem and its Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quadratic minimum spanning tree problem and its variations such as the\nquadratic bottleneck spanning tree problem, the minimum spanning tree problem\nwith conflict pair constraints, and the bottleneck spanning tree problem with\nconflict pair constraints are useful in modeling various real life\napplications. All these problems are known to be NP-hard. In this paper, we\ninvestigate these problems to obtain additional insights into the structure of\nthe problems and to identify possible demarcation between easy and hard special\ncases. New polynomially solvable cases have been identified, as well as NP-hard\ninstances on very simple graphs. As a byproduct, we have a recursive formula\nfor counting the number of spanning trees on a $(k,n)$-accordion and a\ncharacterization of matroids in the context of a quadratic objective function.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 20:04:32 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["\u0106usti\u0107", "Ante", ""], ["Zhang", "Ruonan", ""], ["Punnen", "Abraham P.", ""]]}, {"id": "1603.04549", "submitter": "Vijay Kamble", "authors": "Ramesh Johari, Vijay Kamble and Yash Kanoria", "title": "Matching while Learning", "comments": "This paper has been accepted for publication in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem faced by a service platform that needs to match\nlimited supply with demand but also to learn the attributes of new users in\norder to match them better in the future. We introduce a benchmark model with\nheterogeneous \"workers\" (demand) and a limited supply of \"jobs\" that arrive\nover time. Job types are known to the platform, but worker types are unknown\nand must be learned by observing match outcomes. Workers depart after\nperforming a certain number of jobs. The expected payoff from a match depends\non the pair of types and the goal is to maximize the steady-state rate of\naccumulation of payoff. Though we use terminology inspired by labor markets,\nour framework applies more broadly to platforms where a limited supply of\nheterogeneous products is matched to users over time.\n  Our main contribution is a complete characterization of the structure of the\noptimal policy in the limit that each worker performs many jobs. The platform\nfaces a trade-off for each worker between myopically maximizing payoffs\n(exploitation) and learning the type of the worker (exploration). This creates\na multitude of multi-armed bandit problems, one for each worker, coupled\ntogether by the constraint on availability of jobs of different types (capacity\nconstraints). We find that the platform should estimate a shadow price for each\njob type, and use the payoffs adjusted by these prices, first, to determine its\nlearning goals and then, for each worker, (i) to balance learning with payoffs\nduring the \"exploration phase,\" and (ii) to myopically match after it has\nachieved its learning goals during the \"exploitation phase.\"\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:29:31 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:11:06 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 00:39:01 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 21:36:16 GMT"}, {"version": "v5", "created": "Sat, 7 Dec 2019 18:16:30 GMT"}, {"version": "v6", "created": "Thu, 23 Apr 2020 19:49:49 GMT"}, {"version": "v7", "created": "Wed, 5 Aug 2020 22:17:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Johari", "Ramesh", ""], ["Kamble", "Vijay", ""], ["Kanoria", "Yash", ""]]}, {"id": "1603.04597", "submitter": "Mikhail Batsyn", "authors": "Irina Utkina, Mikhail Batsyn, Ekaterina Batsyna", "title": "A branch and bound algorithm for a fractional 0-1 programming problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fractional 0-1 programming problem arising in manufacturing.\nThe problem consists in clustering of machines together with parts processed on\nthese machines into manufacturing cells so that intra-cell processing of parts\nis maximized and inter-cell movement is minimized. This problem is called Cell\nFormation Problem (CFP) and it is an NP-hard optimization problem with Boolean\nvariables and constraints and with a fractional objective function. Because of\nits high computational complexity there are a lot of heuristics developed for\nit. In this paper we suggest a branch and bound algorithm which provides exact\nsolutions for the CFP with a variable number of cells and grouping efficacy\nobjective function. This algorithm finds optimal solutions for 21 of the 35\npopular benchmark instances from literature and for the remaining 14 instances\nit finds good solutions close to the best known.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 09:06:38 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Utkina", "Irina", ""], ["Batsyn", "Mikhail", ""], ["Batsyna", "Ekaterina", ""]]}, {"id": "1603.04798", "submitter": "Thibaut Lust TL", "authors": "Andrzej Jaszkiewicz and Thibaut Lust", "title": "ND-Tree-based update: a Fast Algorithm for the Dynamic Non-Dominance\n  Problem", "comments": "15 pages, 21 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method called ND-Tree-based update (or shortly\nND-Tree) for the dynamic non-dominance problem, i.e. the problem of online\nupdate of a Pareto archive composed of mutually non-dominated points. It uses a\nnew ND-Tree data structure in which each node represents a subset of points\ncontained in a hyperrectangle defined by its local approximate ideal and nadir\npoints. By building subsets containing points located close in the objective\nspace and using basic properties of the local ideal and nadir points we can\nefficiently avoid searching many branches in the tree. ND-Tree may be used in\nmultiobjective evolutionary algorithms and other multiobjective metaheuristics\nto update an archive of potentially non-dominated points. We prove that the\nproposed algorithm has sub-linear time complexity under mild assumptions. We\nexperimentally compare ND-Tree to the simple list, Quad-tree, and M-Front\nmethods using artificial and realistic benchmarks with up to 10 objectives and\nshow that with this new method substantial reduction of the number of point\ncomparisons and computational time can be obtained. Furthermore, we apply the\nmethod to the non-dominated sorting problem showing that it is highly\ncompetitive to some recently proposed algorithms dedicated to this problem.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 18:22:28 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 12:04:28 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Jaszkiewicz", "Andrzej", ""], ["Lust", "Thibaut", ""]]}, {"id": "1603.04892", "submitter": "L\\'aszl\\'o Kozma", "authors": "Parinya Chalermsook, Mayank Goswami, L\\'aszl\\'o Kozma, Kurt Mehlhorn,\n  Thatchaphol Saranurak", "title": "The landscape of bounds for binary search trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary search trees (BSTs) with rotations can adapt to various kinds of\nstructure in search sequences, achieving amortized access times substantially\nbetter than the Theta(log n) worst-case guarantee. Classical examples of\nstructural properties include static optimality, sequential access, working\nset, key-independent optimality, and dynamic finger, all of which are now known\nto be achieved by the two famous online BST algorithms (Splay and Greedy).\n(...)\n  In this paper, we introduce novel properties that explain the efficiency of\nsequences not captured by any of the previously known properties, and which\nprovide new barriers to the dynamic optimality conjecture. We also establish\nconnections between various properties, old and new. For instance, we show the\nfollowing.\n  (i) A tight bound of O(n log d) on the cost of Greedy for d-decomposable\nsequences. The result builds on the recent lazy finger result of Iacono and\nLangerman (SODA 2016). On the other hand, we show that lazy finger alone cannot\nexplain the efficiency of pattern avoiding sequences even in some of the\nsimplest cases. (ii) A hierarchy of bounds using multiple lazy fingers,\naddressing a recent question of Iacono and Langerman. (iii) The optimality of\nthe Move-to-root heuristic in the key-independent setting introduced by Iacono\n(Algorithmica 2005). (iv) A new tool that allows combining any finite number of\nsound structural properties. As an application, we show an upper bound on the\ncost of a class of sequences that all known properties fail to capture. (v) The\nequivalence between two families of BST properties. The observation on which\nthis connection is based was known before - we make it explicit, and apply it\nto classical BST properties. (...)\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 21:38:57 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Goswami", "Mayank", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Mehlhorn", "Kurt", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1603.04941", "submitter": "Jieming Mao", "authors": "Mark Braverman, Jieming Mao, S. Matthew Weinberg", "title": "Parallel Algorithms for Select and Partition with Noisy Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the $k^{th}$ highest element in a totally\nordered set of $n$ elements (select), and partitioning a totally ordered set\ninto the top $k$ and bottom $n-k$ elements (partition) using pairwise\ncomparisons. Motivated by settings like peer grading or crowdsourcing, where\nmultiple rounds of interaction are costly and queried comparisons may be\ninconsistent with the ground truth, we evaluate algorithms based both on their\ntotal runtime and the number of interactive rounds in three comparison models:\nnoiseless (where the comparisons are correct), erasure (where comparisons are\nerased with probability $1-\\gamma$), and noisy (where comparisons are correct\nwith probability $1/2+\\gamma/2$ and incorrect otherwise). We provide numerous\nmatching upper and lower bounds in all three models. Even our results in the\nnoiseless model, which is quite well-studied in the TCS literature on parallel\nalgorithms, are novel.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 02:31:09 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Braverman", "Mark", ""], ["Mao", "Jieming", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1603.04981", "submitter": "Vijay Kamble", "authors": "Vijay Kamble, Patrick Loiseau, Jean Walrand", "title": "An Approximate Dynamic Programming Approach to Adversarial Online\n  Learning", "comments": "There was an error in the statement of Proposition 4.2 in the\n  previous version that is fixed in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approximate dynamic programming (ADP) approach to compute\napproximations of the optimal strategies and of the minimal losses that can be\nguaranteed in discounted repeated games with vector-valued losses. Such games\nprominently arise in the analysis of regret in repeated decision-making in\nadversarial environments, also known as adversarial online learning. At the\ncore of our approach is a characterization of the lower Pareto frontier of the\nset of expected losses that a player can guarantee in these games as the unique\nfixed point of a set-valued dynamic programming operator. When applied to the\nproblem of regret minimization with discounted losses, our approach yields\nalgorithms that achieve markedly improved performance bounds compared to\noff-the-shelf online learning algorithms like Hedge. These results thus suggest\nthe significant potential of ADP-based approaches in adversarial online\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 07:04:24 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:08:00 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 23:42:51 GMT"}, {"version": "v4", "created": "Sun, 7 Jan 2018 23:51:48 GMT"}, {"version": "v5", "created": "Sun, 30 Sep 2018 23:51:16 GMT"}, {"version": "v6", "created": "Mon, 26 Oct 2020 16:55:34 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kamble", "Vijay", ""], ["Loiseau", "Patrick", ""], ["Walrand", "Jean", ""]]}, {"id": "1603.05047", "submitter": "Jakob Gruber", "authors": "Jakob Gruber, Jesper Larsson Tr\\\"aff, Martin Wimmer", "title": "Benchmarking Concurrent Priority Queues: Performance of k-LSM and\n  Related Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of concurrent, relaxed priority queues have recently been proposed\nand implemented. Results are commonly reported for a throughput benchmark that\nuses a uniform distribution of keys drawn from a large integer range, and\nmostly for single systems. We have conducted more extensive benchmarking of\nthree recent, relaxed priority queues on four different types of systems with\ndifferent key ranges and distributions. While we can show superior throughput\nand scalability for our own k-LSM priority queue for the uniform key\ndistribution, the picture changes drastically for other distributions, both\nwith respect to achieved throughput and relative merit of the priority queues.\nThe throughput benchmark alone is thus not sufficient to characterize the\nperformance of concurrent priority queues. Our benchmark code and k-LSM\npriority queue are publicly available to foster future comparison.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 11:46:06 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Gruber", "Jakob", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["Wimmer", "Martin", ""]]}, {"id": "1603.05183", "submitter": "Roee David", "authors": "Roee David and Uriel Feige", "title": "On the effect of randomness on planted 3-coloring models", "comments": "56 pages, one figure. To be appear in STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the hosted coloring framework for studying algorithmic and\nhardness results for the $k$-coloring problem. There is a class ${\\cal H}$ of\nhost graphs. One selects a graph $H\\in{\\cal H}$ and plants in it a balanced\n$k$-coloring (by partitioning the vertex set into $k$ roughly equal parts, and\nremoving all edges within each part). The resulting graph $G$ is given as input\nto a polynomial time algorithm that needs to $k$-color $G$ (any legal\n$k$-coloring would do -- the algorithm is not required to recover the planted\n$k$-coloring). Earlier planted models correspond to the case that ${\\cal H}$ is\nthe class of all $n$-vertex $d$-regular graphs, a member $H\\in{\\cal H}$ is\nchosen at random, and then a balanced $k$-coloring is planted at random. Blum\nand Spencer [1995] designed algorithms for this model when $d=n^{\\delta}$ (for\n$0<\\delta\\le1$), and Alon and Kahale [1997] managed to do so even when $d$ is a\nsufficiently large constant.\n  The new aspect in our framework is that it need not involve randomness. In\none model within the framework (with $k=3$) $H$ is a $d$ regular spectral\nexpander (meaning that except for the largest eigenvalue of its adjacency\nmatrix, every other eigenvalue has absolute value much smaller than $d$) chosen\nby an adversary, and the planted 3-coloring is random. We show that the\n3-coloring algorithm of Alon and Kahale [1997] can be modified to apply to this\ncase. In another model $H$ is a random $d$-regular graph but the planted\nbalanced $3$-coloring is chosen by an adversary, after seeing $H$. We show that\nfor a certain range of average degrees somewhat below $\\sqrt{n}$, finding a\n3-coloring is NP-hard. Together these results (and other results that we have)\nhelp clarify which aspects of randomness in the planted coloring model are the\nkey to successful 3-coloring algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 17:20:37 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["David", "Roee", ""], ["Feige", "Uriel", ""]]}, {"id": "1603.05346", "submitter": "Edo Liberty", "authors": "Zohar Karnin and Kevin Lang and Edo Liberty", "title": "Optimal Quantile Approximation in Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper resolves one of the longest standing basic problems in the\nstreaming computational model. Namely, optimal construction of quantile\nsketches. An $\\varepsilon$ approximate quantile sketch receives a stream of\nitems $x_1,\\ldots,x_n$ and allows one to approximate the rank of any query up\nto additive error $\\varepsilon n$ with probability at least $1-\\delta$. The\nrank of a query $x$ is the number of stream items such that $x_i \\le x$. The\nminimal sketch size required for this task is trivially at least\n$1/\\varepsilon$. Felber and Ostrovsky obtain a\n$O((1/\\varepsilon)\\log(1/\\varepsilon))$ space sketch for a fixed $\\delta$. To\ndate, no better upper or lower bounds were known even for randomly permuted\nstreams or for approximating a specific quantile, e.g.,\\ the median. This paper\nobtains an $O((1/\\varepsilon)\\log \\log (1/\\delta))$ space sketch and a matching\nlower bound. This resolves the open problem and proves a qualitative gap\nbetween randomized and deterministic quantile sketching. One of our\ncontributions is a novel representation and modification of the widely used\nmerge-and-reduce construction. This subtle modification allows for an analysis\nwhich is both tight and extremely simple. Similar techniques should be useful\nfor improving other sketching objectives and geometric coreset constructions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 03:14:24 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 20:25:36 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Karnin", "Zohar", ""], ["Lang", "Kevin", ""], ["Liberty", "Edo", ""]]}, {"id": "1603.05520", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy, David H. K. Kim, Shi Li", "title": "Improved Approximation for Node-Disjoint Paths in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical Node-Disjoint Paths (NDP) problem: given an $n$-vertex\ngraph $G$ and a collection $M=\\{(s_1,t_1),\\ldots,(s_k,t_k)\\}$ of pairs of\nvertices of $G$ called demand pairs, find a maximum-cardinality set of\nnode-disjoint paths connecting the demand pairs. NDP is one of the most basic\nrouting problems, that has been studied extensively. Despite this, there are\nstill wide gaps in our understanding of its approximability: the best currently\nknown upper bound of $O(\\sqrt n)$ on its approximation ratio is achieved via a\nsimple greedy algorithm, while the best current negative result shows that the\nproblem does not have a better than $\\Omega(\\log^{1/2-\\delta}n)$-approximation\nfor any constant $\\delta$, under standard complexity assumptions. Even for\nplanar graphs no better approximation algorithms are known, and to the best of\nour knowledge, the best negative bound is APX-hardness. Perhaps the biggest\nobstacle to obtaining better approximation algorithms for NDP is that most\ncurrently known approximation algorithms for this type of problems rely on the\nstandard multicommodity flow relaxation, whose integrality gap is $\\Omega(\\sqrt\nn)$ for NDP, even in planar graphs. In this paper, we break the barrier of\n$O(\\sqrt n)$ on the approximability of the NDP problem in planar graphs and\nobtain an $\\tilde O(n^{9/19})$-approximation. We introduce a new linear\nprogramming relaxation of the problem, and a number of new techniques, that we\nhope will be helpful in designing more powerful algorithms for this and related\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 14:57:12 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Kim", "David H. K.", ""], ["Li", "Shi", ""]]}, {"id": "1603.05609", "submitter": "Milica Bogicevic", "authors": "Milica Bogicevic, Milan Merkle", "title": "ABCDepth: efficient algorithm for Tukey depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for Tukey (halfspace) depth level sets and its\nimplementation. Given $d$-dimensional data set for any $d\\geq 2$, the algorithm\nis based on representation of level sets as intersections of balls in $R^d$,\nand can be easily adapted to related depths (Type D, Zuo and Serfling (Ann.\nStat. {\\bf 28} (2000), 461--482)). The algorithm complexity is $O(dn^2 +\nn^2\\log{n})$ where $n$ is the data set size. Examples with real and synthetic\ndata show that the algorithm is much faster than other implemented algorithms\nand that it can accept thousands of multidimensional observations, while other\nalgorithms are tested with two-dimensional data or with a couple of hundreds\nmultidimensional observations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 18:47:30 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 15:43:27 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Bogicevic", "Milica", ""], ["Merkle", "Milan", ""]]}, {"id": "1603.05614", "submitter": "Qilian Yu", "authors": "Qilian Yu, Easton Li Xu, Shuguang Cui", "title": "Streaming Algorithms for News and Scientific Literature Recommendation:\n  Submodular Maximization with a d-Knapsack Constraint", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization problems belong to the family of combinatorial\noptimization problems and enjoy wide applications. In this paper, we focus on\nthe problem of maximizing a monotone submodular function subject to a\n$d$-knapsack constraint, for which we propose a streaming algorithm that\nachieves a $\\left(\\frac{1}{1+2d}-\\epsilon\\right)$-approximation of the optimal\nvalue, while it only needs one single pass through the dataset without storing\nall the data in the memory. In our experiments, we extensively evaluate the\neffectiveness of our proposed algorithm via two applications: news\nrecommendation and scientific literature recommendation. It is observed that\nthe proposed streaming algorithm achieves both execution speedup and memory\nsaving by several orders of magnitude, compared with existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:01:12 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 16:15:56 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 00:43:45 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Yu", "Qilian", ""], ["Xu", "Easton Li", ""], ["Cui", "Shuguang", ""]]}, {"id": "1603.05642", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Optimal Black-Box Reductions Between Optimization Objectives", "comments": "new applications of our optimal reductions are obtained in this\n  version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diverse world of machine learning applications has given rise to a\nplethora of algorithms and optimization methods, finely tuned to the specific\nregression or classification task at hand. We reduce the complexity of\nalgorithm design for machine learning by reductions: we develop reductions that\ntake a method developed for one setting and apply it to the entire spectrum of\nsmoothness and strong-convexity in applications.\n  Furthermore, unlike existing results, our new reductions are OPTIMAL and more\nPRACTICAL. We show how these new reductions give rise to new and faster running\ntimes on training linear classifiers for various families of loss functions,\nand conclude with experiments showing their successes also in practice.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:51:59 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 05:11:42 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 17:03:15 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05643", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Variance Reduction for Faster Non-Convex Optimization", "comments": "polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem in non-convex optimization of efficiently\nreaching a stationary point. In contrast to the convex case, in the long\nhistory of this basic problem, the only known theoretical results on\nfirst-order non-convex optimization remain to be full gradient descent that\nconverges in $O(1/\\varepsilon)$ iterations for smooth objectives, and\nstochastic gradient descent that converges in $O(1/\\varepsilon^2)$ iterations\nfor objectives that are sum of smooth functions.\n  We provide the first improvement in this line of research. Our result is\nbased on the variance reduction trick recently introduced to convex\noptimization, as well as a brand new analysis of variance reduction that is\nsuitable for non-convex optimization. For objectives that are sum of smooth\nfunctions, our first-order minibatch stochastic method converges with an\n$O(1/\\varepsilon)$ rate, and is faster than full gradient descent by\n$\\Omega(n^{1/3})$.\n  We demonstrate the effectiveness of our methods on empirical risk\nminimizations with non-convex loss functions and training neural nets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:55:12 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 02:34:00 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05715", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna, Yang Li", "title": "Tight Bounds for Single-Pass Streaming Complexity of the Set Cover\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve the space complexity of single-pass streaming algorithms for\napproximating the classic set cover problem. For finding an\n$\\alpha$-approximate set cover (for any $\\alpha= o(\\sqrt{n})$) using a\nsingle-pass streaming algorithm, we show that $\\Theta(mn/\\alpha)$ space is both\nsufficient and necessary (up to an $O(\\log{n})$ factor); here $m$ denotes\nnumber of the sets and $n$ denotes size of the universe. This provides a strong\nnegative answer to the open question posed by Indyk et al. (2015) regarding the\npossibility of having a single-pass algorithm with a small approximation factor\nthat uses sub-linear space.\n  We further study the problem of estimating the size of a minimum set cover\n(as opposed to finding the actual sets), and establish that an additional\nfactor of $\\alpha$ saving in the space is achievable in this case and that this\nis the best possible. In other words, we show that $\\Theta(mn/\\alpha^2)$ space\nis both sufficient and necessary (up to logarithmic factors) for estimating the\nsize of a minimum set cover to within a factor of $\\alpha$. Our algorithm in\nfact works for the more general problem of estimating the optimal value of a\ncovering integer program. On the other hand, our lower bound holds even for set\ncover instances where the sets are presented in a random order.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 22:39:28 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""]]}, {"id": "1603.05922", "submitter": "Erick Elejalde", "authors": "Erick Elejalde and Jose Fuentes-Sep\\'ulveda and Leo Ferres", "title": "Empirical Evaluation of a Thread-Safe Dynamic Range Min-Max Tree using\n  HTM", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Succinct trees, such as wavelet trees and those based on, for instance, range\nMin-Max trees (RMMTs), are a family of practical data structures that store\ninformation close to their information-theoretic space lower bound. These\nstructures are often static; meaning that once they are built, nodes cannot be\nadded, deleted or modified. This read-only property simplifies concurrency.\nHowever, newer versions of these data structures allow for a fair degree of\ndynamism. Parallel programming using Hardware Transactional Memory(HTM), has\nbeen available in mainstream microprocessors since a few years ago. One\nlimitation of HTM is still on the size of each transaction. This is why HTM's\nuse, for the moment, is limited to operations that involve few memory addresses\nthat need to be updated atomically, or where the level of concurrency is low.\nWe provide the first available implementation of a concurrent, dynamic RMMT\nbased on HTM, and we compare empirically how well HTM performs compared to a\nnaive implementation using locks. We have shown that because of the formal\nproperties of RMMTs, HTM is a good fit for adding concurrency to otherwise slow\nlock-based alternatives. We have also shown that HTM performs better than locks\nwhen the number of write operations increase, making it a practical structure\nto use in several write-intensive contexts. This is, as far as we know, the\nonly practical implementation of RMMTs thoroughly tested using HTM.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 16:55:16 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Elejalde", "Erick", ""], ["Fuentes-Sep\u00falveda", "Jose", ""], ["Ferres", "Leo", ""]]}, {"id": "1603.05945", "submitter": "Nick Brettell", "authors": "\\'Edouard Bonnet and Nick Brettell and O-joung Kwon and D\\'aniel Marx", "title": "Parameterized vertex deletion problems for hereditary graph classes with\n  a block property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a class of graphs $\\mathcal{P}$, the Bounded $\\mathcal{P}$-Block Vertex\nDeletion problem asks, given a graph $G$ on $n$ vertices and positive integers\n$k$ and $d$, whether there is a set $S$ of at most $k$ vertices such that each\nblock of $G-S$ has at most $d$ vertices and is in $\\mathcal{P}$. We show that\nwhen $\\mathcal{P}$ satisfies a natural hereditary property and is recognizable\nin polynomial time, Bounded $\\mathcal{P}$-Block Vertex Deletion can be solved\nin time $2^{O(k \\log d)}n^{O(1)}$. When $\\mathcal{P}$ contains all split\ngraphs, we show that this running time is essentially optimal unless the\nExponential Time Hypothesis fails. On the other hand, if $\\mathcal{P}$ consists\nof only complete graphs, or only cycle graphs and $K_2$, then Bounded\n$\\mathcal{P}$-Block Vertex Deletion admits a $c^{k}n^{O(1)}$-time algorithm for\nsome constant $c$ independent of $d$. We also show that Bounded\n$\\mathcal{P}$-Block Vertex Deletion admits a kernel with $O(k^2 d^7)$ vertices.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 18:02:04 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Brettell", "Nick", ""], ["Kwon", "O-joung", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1603.05953", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "comments": "Version 6. Appeared in Symposium on Theory of Computing (STOC 2017)\n  and Journal of Machine Learning Research (JMLR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nesterov's momentum trick is famously known for accelerating gradient\ndescent, and has been proven useful in building fast iterative algorithms.\nHowever, in the stochastic setting, counterexamples exist and prevent\nNesterov's momentum from providing similar acceleration, even if the underlying\nproblem is convex and finite-sum.\n  We introduce $\\mathtt{Katyusha}$, a direct, primal-only stochastic gradient\nmethod to fix this issue. In convex finite-sum stochastic optimization,\n$\\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an\noptimal parallel linear speedup in the mini-batch setting.\n  The main ingredient is $\\textit{Katyusha momentum}$, a novel \"negative\nmomentum\" on top of Nesterov's momentum. It can be incorporated into a\nvariance-reduction based algorithm and speed it up, both in terms of\n$\\textit{sequential and parallel}$ performance. Since variance reduction has\nbeen successfully applied to a growing list of practical problems, our paper\nsuggests that in each of such cases, one could potentially try to give Katyusha\na hug.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 18:46:05 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 18:13:51 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 02:12:23 GMT"}, {"version": "v4", "created": "Sun, 21 Aug 2016 21:49:29 GMT"}, {"version": "v5", "created": "Tue, 2 May 2017 06:05:45 GMT"}, {"version": "v6", "created": "Mon, 24 Sep 2018 04:49:31 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1603.06002", "submitter": "Patrick Eschenfeldt", "authors": "Patrick Eschenfeldt and David Gamarnik", "title": "A Message Passing Algorithm for the Problem of Path Packing in Graphs", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of packing node-disjoint directed paths in a directed\ngraph. We consider a variant of this problem where each path starts within a\nfixed subset of root nodes, subject to a given bound on the length of paths.\nThis problem is motivated by the so-called kidney exchange problem, but has\npotential other applications and is interesting in its own right.\n  We propose a new algorithm for this problem based on the message\npassing/belief propagation technique. A priori this problem does not have an\nassociated graphical model, so in order to apply a belief propagation algorithm\nwe provide a novel representation of the problem as a graphical model. Standard\nbelief propagation on this model has poor scaling behavior, so we provide an\nefficient implementation that significantly decreases the complexity. We\nprovide numerical results comparing the performance of our algorithm on both\nartificially created graphs and real world networks to several alternative\nalgorithms, including algorithms based on integer programming (IP) techniques.\nThese comparisons show that our algorithm scales better to large instances than\nIP-based algorithms and often finds better solutions than a simple algorithm\nthat greedily selects the longest path from each root node. In some cases it\nalso finds better solutions than the ones found by IP-based algorithms even\nwhen the latter are allowed to run significantly longer than our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 21:26:59 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Eschenfeldt", "Patrick", ""], ["Gamarnik", "David", ""]]}, {"id": "1603.06109", "submitter": "Scott Roche", "authors": "Michael Mitzenmacher, Rajmohan Rajaraman, Scott Roche", "title": "Better bounds for coalescing-branching random walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coalescing-branching random walks, or {\\em cobra walks} for short, are a\nnatural variant of random walks on graphs that can model the spread of disease\nthrough contacts or the spread of information in networks. In a $k$-cobra walk,\nat each time step a subset of the vertices are active; each active vertex\nchooses $k$ random neighbors (sampled independently and uniformly with\nreplacement) that become active at the next step, and these are the only active\nvertices at the next step. A natural quantity to study for cobra walks is the\ncover time, which corresponds to the expected time when all nodes have become\ninfected or received the disseminated information.\n  In this work, we extend previous results for cobra walks in multiple ways. We\nshow that the cover time for the 2-cobra walk on $[0,n]^d$ is $O(n)$ (where the\norder notation hides constant factors that depend on $d$); previous work had\nshown the cover time was $O(n \\cdot polylog(n))$. We show that the cover time\nfor a 2-cobra walk on an $n$-vertex $d$-regular graph with conductance $\\phi_G$\nis $O(\\phi_G^{-2} \\log^2 n)$, significantly generalizing a previous result that\nheld only for expander graphs with sufficiently high expansion. And finally we\nshow that the cover time for a 2-cobra walk on a graph with $n$ vertices is\nalways $O(n^{11/4} \\log n)$; this is the first result showing that the bound of\n$\\Theta(n^3)$ for the worst-case cover time for random walks can be beaten\nusing 2-cobra walks.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 16:04:21 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Rajaraman", "Rajmohan", ""], ["Roche", "Scott", ""]]}, {"id": "1603.06217", "submitter": "Aliakbar Safilian", "authors": "Masoud Safilian and S. Mehdi Tashakkori and Sepehr Eghbali and\n  Aliakbar Safilian", "title": "An Approximation Approach for Solving the Subpath Planning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subpath planning problem is a branch of the path planning problem, which\nhas widespread applications in automated manufacturing process as well as\nvehicle and robot navigation. This problem is to find the shortest path or tour\nsubject for travelling a set of given subpaths. The current approaches for\ndealing with the subpath planning problem are all based on meta-heuristic\napproaches. It is well-known that meta-heuristic based approaches have several\ndeficiencies. To address them, we propose a novel approximation algorithm in\nthe O(n^3) time complexity class, which guarantees to solve any subpath\nplanning problem instance with the fixed ratio bound of 2. Also, the formal\nproofs of the claims, our empirical evaluation shows that our approximation\nmethod acts much better than a state-of-the-art method, both in result and\nexecution time.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 14:22:26 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Safilian", "Masoud", ""], ["Tashakkori", "S. Mehdi", ""], ["Eghbali", "Sepehr", ""], ["Safilian", "Aliakbar", ""]]}, {"id": "1603.06478", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer, Sascha Brauer, Kathrin Bujna", "title": "Hard-Clustering with Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training the parameters of statistical models to describe a given data set is\na central task in the field of data mining and machine learning. A very popular\nand powerful way of parameter estimation is the method of maximum likelihood\nestimation (MLE). Among the most widely used families of statistical models are\nmixture models, especially, mixtures of Gaussian distributions. A popular\nhard-clustering variant of the MLE problem is the so-called complete-data\nmaximum likelihood estimation (CMLE) method. The standard approach to solve the\nCMLE problem is the Classification-Expectation-Maximization (CEM) algorithm.\nUnfortunately, it is only guaranteed that the algorithm converges to some\n(possibly arbitrarily poor) stationary point of the objective function.\n  In this paper, we present two algorithms for a restricted version of the CMLE\nproblem. That is, our algorithms approximate reasonable solutions to the CMLE\nproblem which satisfy certain natural properties. Moreover, they compute\nsolutions whose cost (i.e. complete-data log-likelihood values) are at most a\nfactor $(1+\\epsilon)$ worse than the cost of the solutions that we search for.\nNote the CMLE problem in its most general, i.e. unrestricted, form is not well\ndefined and allows for trivial optimal solutions that can be thought of as\ndegenerated solutions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:02:27 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Brauer", "Sascha", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1603.06505", "submitter": "Daowen Qiu", "authors": "Daowen Qiu, Shenggen Zheng", "title": "Characterizations of symmetrically partial Boolean functions with exact\n  quantum query complexity", "comments": "33 pages, comments are welcome. Some languages are further revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give and prove an optimal exact quantum query algorithm with complexity\n$k+1$ for computing the promise problem (i.e., symmetric and partial Boolean\nfunction) $DJ_n^k$ defined as: $DJ_n^k(x)=1$ for $|x|=n/2$, $DJ_n^k(x)=0$ for\n$|x|$ in the set $\\{0, 1,\\ldots, k, n-k, n-k+1,\\ldots,n\\}$, and it is undefined\nfor the rest cases, where $n$ is even, $|x|$ is the Hamming weight of $x$. The\ncase of $k=0$ is the well-known Deutsch-Jozsa problem. We outline all symmetric\n(and partial) Boolean functions with degrees 1 and 2, and prove their exact\nquantum query complexity. Then we prove that any symmetrical (and partial)\nBoolean function $f$ has exact quantum 1-query complexity if and only if $f$\ncan be computed by the Deutsch-Jozsa algorithm. We also discover the optimal\nexact quantum 2-query complexity for distinguishing between inputs of Hamming\nweight $\\{ \\lfloor n/2\\rfloor, \\lceil n/2\\rceil \\}$ and Hamming weight in the\nset $\\{ 0, n\\}$ for all odd $n$. In addition, a method is provided to determine\nthe degree of any symmetrical (and partial) Boolean function.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 17:24:04 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 17:05:46 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 12:08:06 GMT"}, {"version": "v4", "created": "Thu, 6 Oct 2016 15:51:24 GMT"}, {"version": "v5", "created": "Mon, 10 Oct 2016 04:14:23 GMT"}, {"version": "v6", "created": "Mon, 5 Jun 2017 16:10:57 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Qiu", "Daowen", ""], ["Zheng", "Shenggen", ""]]}, {"id": "1603.06828", "submitter": "Andrei Zinovyev Dr.", "authors": "A.N. Gorban, E.M. Mirkes, A. Zinovyev", "title": "Robust principal graphs for data approximation", "comments": "A talk given at ECDA2015 (European Conference on Data Analysis,\n  September 2nd to 4th 2015, University of Essex, Colchester, UK), to be\n  published in Archives of Data Science", "journal-ref": "Archives of Data Science, Series A, Vol. 2, No. 1, 2017", "doi": "10.5445/KSP/1000058749/11", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Revealing hidden geometry and topology in noisy data sets is a challenging\ntask. Elastic principal graph is a computationally efficient and flexible data\napproximator based on embedding a graph into the data space and minimizing the\nenergy functional penalizing the deviation of graph nodes both from data points\nand from pluri-harmonic configuration (generalization of linearity). The\nstructure of principal graph is learned from data by application of a\ntopological grammar which in the simplest case leads to the construction of\nprincipal curves or trees. In order to more efficiently cope with noise and\noutliers, here we suggest using a trimmed data approximation term to increase\nthe robustness of the method. The modification of the method that we suggest\ndoes not affect either computational efficiency or general convergence\nproperties of the original elastic graph method. The trimmed elastic energy\nfunctional remains a Lyapunov function for the optimization algorithm. On\nseveral examples of complex data distributions we demonstrate how the robust\nprincipal graphs learn the global data structure and show the advantage of\nusing the trimmed data approximation term for the construction of principal\ngraphs and other popular data approximators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 15:24:41 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 11:24:10 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Gorban", "A. N.", ""], ["Mirkes", "E. M.", ""], ["Zinovyev", "A.", ""]]}, {"id": "1603.06958", "submitter": "Sebastian Deorowicz", "authors": "Sebastin Deorowicz and Agnieszka Debudaj-Grabysz and Adam Gudys", "title": "Aligning 415 519 proteins in less than two hours on PC", "comments": null, "journal-ref": "Scientific Reports, Article no. 33964 (2016)", "doi": "10.1038/srep33964", "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid development of modern sequencing platforms enabled an unprecedented\ngrowth of protein families databases. The abundance of sets composed of\nhundreds of thousands sequences is a great challenge for multiple sequence\nalignment algorithms. In the article we introduce FAMSA, a new progressive\nalgorithm designed for fast and accurate alignment of thousands of protein\nsequences. Its features include the utilisation of longest common subsequence\nmeasure for determining pairwise similarities, a novel method of gap costs\nevaluation, and a new iterative refinement scheme. Importantly, its\nimplementation is highly optimised and parallelised to make the most of modern\ncomputer platforms. Thanks to the above, quality indicators, namely\nsum-of-pairs and total-column scores, show FAMSA to be superior to competing\nalgorithms like Clustal Omega or MAFFT for datasets exceeding a few thousand of\nsequences. The quality does not compromise time and memory requirements which\nare an order of magnitude lower than that of existing solutions. For example, a\nfamily of 415 519 sequences was analysed in less than two hours and required\nonly 8GB of RAM.\n  FAMSA is freely available at http://sun.aei.polsl.pl/REFRESH/famsa.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 20:03:43 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Deorowicz", "Sebastin", ""], ["Debudaj-Grabysz", "Agnieszka", ""], ["Gudys", "Adam", ""]]}, {"id": "1603.06985", "submitter": "Shelby Kimmel", "authors": "Edward Farhi, Shelby Kimmel, and Kristan Temme", "title": "A Quantum Version of Sch\\\"oning's Algorithm Applied to Quantum 2-SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a quantum algorithm that consists of a simple quantum Markov\nprocess, and we analyze its behavior on restricted versions of Quantum 2-SAT.\nWe prove that the algorithm solves this decision problem with high probability\nfor n qubits, L clauses, and promise gap c in time O(n^2 L^2 c^{-2}). If the\nHamiltonian is additionally polynomially gapped, our algorithm efficiently\nproduces a state that has high overlap with the satisfying subspace. The Markov\nprocess we study is a quantum analogue of Sch\\\"oning's probabilistic algorithm\nfor k-SAT.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 21:14:37 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Farhi", "Edward", ""], ["Kimmel", "Shelby", ""], ["Temme", "Kristan", ""]]}, {"id": "1603.07051", "submitter": "Mohamed El Yafrani", "authors": "Mohamed El Yafrani and Bela\\\"id Ahiod", "title": "Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief\n  Problem", "comments": "12th ACS/IEEE International Conference on Computer Systems and\n  Applications (AICCSA) 2015. November 17-20, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world problems are very difficult to optimize. However, many researchers\nhave been solving benchmark problems that have been extensively investigated\nfor the last decades even if they have very few direct applications. The\nTraveling Thief Problem (TTP) is a NP-hard optimization problem that aims to\nprovide a more realistic model. TTP targets particularly routing problem under\npacking/loading constraints which can be found in supply chain management and\ntransportation. In this paper, TTP is presented and formulated mathematically.\nA combined local search algorithm is proposed and compared with Random Local\nSearch (RLS) and Evolutionary Algorithm (EA). The obtained results are quite\npromising since new better solutions were found.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 02:30:35 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Yafrani", "Mohamed El", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1603.07077", "submitter": "Sandor P. Fekete", "authors": "S\\'andor P. Fekete, Andreas Haas, Michael Hemmer, Michael Hoffmann,\n  Irina Kostitsyna, Dominik Krupke, Florian Maurer, Joseph S. B. Mitchell, Arne\n  Schmidt, Christiane Schmidt, and Julian Troegel", "title": "Computing Nonsimple Polygons of Minimum Perimeter", "comments": "24 pages, 21 figures, 1 table; full version of extended abstract that\n  is to appear in SEA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide exact and approximation methods for solving a geometric relaxation\nof the Traveling Salesman Problem (TSP) that occurs in curve reconstruction:\nfor a given set of vertices in the plane, the problem Minimum Perimeter Polygon\n(MPP) asks for a (not necessarily simply connected) polygon with shortest\npossible boundary length. Even though the closely related problem of finding a\nminimum cycle cover is polynomially solvable by matching techniques, we prove\nhow the topological structure of a polygon leads to NP-hardness of the MPP. On\nthe positive side, we show how to achieve a constant-factor approximation.\n  When trying to solve MPP instances to provable optimality by means of integer\nprogramming, an additional difficulty compared to the TSP is the fact that only\na subset of subtour constraints is valid, depending not on combinatorics, but\non geometry. We overcome this difficulty by establishing and exploiting\nadditional geometric properties. This allows us to reliably solve a wide range\nof benchmark instances with up to 600 vertices within reasonable time on a\nstandard machine. We also show that using a natural geometry-based\nsparsification yields results that are on average within 0.5% of the optimum.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 06:25:07 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Haas", "Andreas", ""], ["Hemmer", "Michael", ""], ["Hoffmann", "Michael", ""], ["Kostitsyna", "Irina", ""], ["Krupke", "Dominik", ""], ["Maurer", "Florian", ""], ["Mitchell", "Joseph S. B.", ""], ["Schmidt", "Arne", ""], ["Schmidt", "Christiane", ""], ["Troegel", "Julian", ""]]}, {"id": "1603.07210", "submitter": "Martin Hoefer", "authors": "Xiaohui Bei, Jugal Garg, Martin Hoefer, Kurt Mehlhorn", "title": "Computing Equilibria in Markets with Budget-Additive Utilities", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first analysis of Fisher markets with buyers that have\nbudget-additive utility functions. Budget-additive utilities are elementary\nconcave functions with numerous applications in online adword markets and\nrevenue optimization problems. They extend the standard case of linear\nutilities and have been studied in a variety of other market models. In\ncontrast to the frequently studied CES utilities, they have a global satiation\npoint which can imply multiple market equilibria with quite different\ncharacteristics. Our main result is an efficient combinatorial algorithm to\ncompute a market equilibrium with a Pareto-optimal allocation of goods. It\nrelies on a new descending-price approach and, as a special case, also implies\na novel combinatorial algorithm for computing a market equilibrium in linear\nFisher markets. We complement these positive results with a number of hardness\nresults for related computational questions. We prove that it is NP-hard to\ncompute a market equilibrium that maximizes social welfare, and it is PPAD-hard\nto find any market equilibrium with utility functions with separate satiation\npoints for each buyer and each good.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 14:53:44 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 08:30:23 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Bei", "Xiaohui", ""], ["Garg", "Jugal", ""], ["Hoefer", "Martin", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1603.07340", "submitter": "Tillmann Miltzow", "authors": "D\\'aniel Marx and Tillmann Miltzow", "title": "Peeling and Nibbling the Cactus: Subexponential-Time Algorithms for\n  Counting Triangulations and Related Problems", "comments": "47 pages, 23 Figures, to appear in SoCG 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points $S$ in the plane, a triangulation $T$ of $S$ is a\nmaximal set of non-crossing segments with endpoints in $S$. We present an\nalgorithm that computes the number of triangulations on a given set of $n$\npoints in time $n^{(11+ o(1))\\sqrt{n} }$, significantly improving the previous\nbest running time of $O(2^n n^2)$ by Alvarez and Seidel [SoCG 2013]. Our main\ntool is identifying separators of size $O(\\sqrt{n})$ of a triangulation in a\ncanonical way. The definition of the separators are based on the decomposition\nof the triangulation into nested layers (\"cactus graphs\"). Based on the above\nalgorithm, we develop a simple and formal framework to count other non-crossing\nstraight-line graphs in $n^{O(\\sqrt{n})}$ time. We demonstrate the usefulness\nof the framework by applying it to counting non-crossing Hamilton cycles,\nspanning trees, perfect matchings, $3$-colorable triangulations, connected\ngraphs, cycle decompositions, quadrangulations, $3$-regular graphs, and more.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 20:12:41 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "1603.07371", "submitter": "Sarma Vrudhula", "authors": "Niranjan Kulkarni and Sarma Vrudhula", "title": "Efficient Enumeration of Unidirectional Cuts for Technology Mapping of\n  Boolean Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In technology mapping, enumeration of subcircuits or cuts to be replaced by a\nstandard cell is an important step that decides both the quality of the\nsolution and execution speed. In this work, we view cuts as set of edges\ninstead of as set of nodes and based on it, provide a classification of cuts.\nIt is shown that if enumeration is restricted to a subclass of cuts called\nunidirectional cuts, the quality of solution does not degrade. We also show\nthat such cuts are equivalent to a known class of cuts called strong line cuts\nfirst proposed in [14]. We propose an efficient enumeration method based on a\nnovel graph pruning algorithm that utilizes network flow to approximate minimum\nstrong line cut. The runtimes for the proposed enumeration method are shown to\nbe quite practical for enumeration of a large number of cuts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 21:29:30 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kulkarni", "Niranjan", ""], ["Vrudhula", "Sarma", ""]]}, {"id": "1603.07376", "submitter": "Paolo Cintia", "authors": "Paolo Cintia, Mirco Nanni", "title": "An effective Time-Aware Map Matching process for low sampling GPS data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of the proliferation of Geo-Spatial Data, induced by the diffusion\nof GPS devices, the map matching problem still represents an important and\nvaluable challenge. The process of associating a segment of the underlying road\nnetwork to a GPS point gives us the chance to enrich raw data with the semantic\nlayer provided by the roadmap, with all contextual information associated to\nit, e.g. the presence of speed limits, attraction points, changes in elevation,\netc. Most state-of-art solutions for this classical problem simply look for the\nshortest or fastest path connecting any pair of consecutive points in a trip.\nWhile in some contexts that is reasonable, in this work we argue that the\nshortest/fastest path assumption can be in general erroneous. Indeed, we show\nthat such approaches can yield travel times that are significantly incoherent\nwith the real ones, and propose a Time-Aware Map matching process that tries to\nimprove the state-of-art by taking into account also such temporal aspect. Our\nalgorithm results to be very efficient, effective on low- sampling data and to\noutperform existing solutions, as proved by experiments on large datasets of\nreal GPS trajectories. Moreover, our algorithm is parameter-free and does not\ndepend on specific characteristics of the GPS localization error and of the\nroad network (e.g. density of roads, road network topology, etc.).\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 21:48:38 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Cintia", "Paolo", ""], ["Nanni", "Mirco", ""]]}, {"id": "1603.07457", "submitter": "Arnab Ganguly", "authors": "Arnab Ganguly, Rahul Shah, Sharma V. Thankachan", "title": "Parameterized Pattern Matching -- Succinctly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the $Parameterized$ $Pattern$ $Matching$ problem, where a pattern\n$P$ matches some location in a text $\\mathsf{T}$ iff there is a one-to-one\ncorrespondence between the alphabet symbols of the pattern to those of the\ntext. More specifically, assume that the text $\\mathsf{T}$ contains $n$\ncharacters from a static alphabet $\\Sigma_s$ and a parameterized alphabet\n$\\Sigma_p$, where $\\Sigma_s \\cap \\Sigma_p = \\varnothing$ and $|\\Sigma_s \\cup\n\\Sigma_p|=\\sigma$. A pattern $P$ matches a substring $S$ of $\\mathsf{T}$ iff\nthe static characters match exactly, and there exists a one-to-one function\nthat renames the parameterized characters in $S$ to that in $P$. Previous\nindexing solution [Baker, STOC 1993], known as $Parameterized$ $Suffix$ $Tree$,\nrequires $\\Theta(n\\log n)$ bits of space, and can find all $occ$ occurrences of\n$P$ in $\\mathcal{O}(|P|\\log \\sigma+ occ)$ time. In this paper, we present the\nfirst succinct index that occupies $n \\log \\sigma + \\mathcal{O}(n)$ bits and\nanswers queries in $\\mathcal{O}((|P|+ occ\\cdot \\log n) \\log\\sigma\\log \\log\n\\sigma)$ time. We also present a compact index that occupies\n$\\mathcal{O}(n\\log\\sigma)$ bits and answers queries in $\\mathcal{O}(|P|\\log\n\\sigma+ occ\\cdot \\log n)$ time. Furthermore, the techniques are extended to\nobtain the first succinct representation of the index of Shibuya for\n$Structural$ $Matching$ [SWAT, 2000], and of Idury and Sch\\\"{a}ffer for\n$Parameterized$ $Dictionary$ $Matching$ [CPM, 1994].\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 07:38:18 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 20:15:57 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ganguly", "Arnab", ""], ["Shah", "Rahul", ""], ["Thankachan", "Sharma V.", ""]]}, {"id": "1603.07504", "submitter": "Xiaowei Chen", "authors": "Xiaowei Chen, Yongkun Li, Pinghui Wang, John C.S. Lui", "title": "A General Framework for Estimating Graphlet Statistics via Random Walk", "comments": "16 pages, technical report of VLDB paper, Vol.10, No.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphlets are induced subgraph patterns and have been frequently applied to\ncharacterize the local topology structures of graphs across various domains,\ne.g., online social networks (OSNs) and biological networks. Discovering and\ncomputing graphlet statistics are highly challenging. First, the massive size\nof real-world graphs makes the exact computation of graphlets extremely\nexpensive. Secondly, the graph topology may not be readily available so one has\nto resort to web crawling using the available application programming\ninterfaces (APIs). In this work, we propose a general and novel framework to\nestimate graphlet statistics of \"any size\". Our framework is based on\ncollecting samples through consecutive steps of random walks. We derive an\nanalytical bound on the sample size (via the Chernoff-Hoeffding technique) to\nguarantee the convergence of our unbiased estimator. To further improve the\naccuracy, we introduce two novel optimization techniques to reduce the lower\nbound on the sample size. Experimental evaluations demonstrate that our methods\noutperform the state-of-the-art method up to an order of magnitude both in\nterms of accuracy and time cost.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 09:57:38 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 07:06:43 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 07:37:34 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Chen", "Xiaowei", ""], ["Li", "Yongkun", ""], ["Wang", "Pinghui", ""], ["Lui", "John C. S.", ""]]}, {"id": "1603.07768", "submitter": "Nathaniel Kell", "authors": "Nathaniel Kell, Debmalya Panigrahi", "title": "Online Budgeted Allocation with General Budgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online budgeted allocation (also called ADWORDS) problem, where\na set of impressions arriving online are allocated to a set of\nbudget-constrained advertisers to maximize revenue. Motivated by connections to\nInternet advertising, several variants of this problem have been studied since\nthe seminal work of Mehta, Saberi, Vazirani, and Vazirani (FOCS 2005). However,\nthis entire body of work focuses on a single budget for every advertising\ncampaign, whereas in order to fully represent the actual agenda of an\nadvertiser, an advertising budget should be expressible over multiple tiers of\nuser-attribute granularity. A simple example is an advertising campaign that is\nconstrained by an overall budget but is also accompanied by a set of\nsub-budgets for each target demographic. In such a contract scheme, an\nadvertiser can specify their true user-targeting goals, allowing the publisher\nto fulfill them through relevant allocations.\n  In this paper, we give a complete characterization of the ADWORDS problem for\ngeneral advertising budgets. In the most general setting, we show that, unlike\nin the single-budget ADWORDS problem, obtaining a constant competitive ratio is\nimpossible and give asymptotically tight upper and lower bounds. However for\nour main result, we observe that in many real-world scenarios (as in the above\nexample), multi-tier budgets have a laminar structure, since most relevant\nconsumer or product classifications are hierarchical. For laminar budgets, we\nobtain a competitive ratio of e/(e-1) in the small bids case, which matches the\nbest known ADWORDS result for single budgets. Our algorithm has a primal-dual\nstructure and generalizes the primal-dual analysis for single- budget ADWORDS\nfirst given by Buchbinder, Jain, and Naor (ESA 2007).\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 22:16:13 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Kell", "Nathaniel", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1603.07796", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Peter Lofgren, Ashish Goel", "title": "Approximate Personalized PageRank on Dynamic Graphs", "comments": "KDD'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze two algorithms for maintaining approximate\nPersonalized PageRank (PPR) vectors on a dynamic graph, where edges are added\nor deleted. Our algorithms are natural dynamic versions of two known local\nvariations of power iteration. One, Forward Push, propagates probability mass\nforwards along edges from a source node, while the other, Reverse Push,\npropagates local changes backwards along edges from a target. In both\nvariations, we maintain an invariant between two vectors, and when an edge is\nupdated, our algorithm first modifies the vectors to restore the invariant,\nthen performs any needed local push operations to restore accuracy.\n  For Reverse Push, we prove that for an arbitrary directed graph in a random\nedge model, or for an arbitrary undirected graph, given a uniformly random\ntarget node $t$, the cost to maintain a PPR vector to $t$ of additive error\n$\\varepsilon$ as $k$ edges are updated is $O(k + \\bar{d} / \\varepsilon)$, where\n$\\bar{d}$ is the average degree of the graph. This is $O(1)$ work per update,\nplus the cost of computing a reverse vector once on a static graph. For Forward\nPush, we show that on an arbitrary undirected graph, given a uniformly random\nstart node $s$, the cost to maintain a PPR vector from $s$ of degree-normalized\nerror $\\varepsilon$ as $k$ edges are updated is $O(k + 1 / \\varepsilon)$, which\nis again $O(1)$ per update plus the cost of computing a PPR vector once on a\nstatic graph.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 01:09:51 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 03:34:02 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Hongyang", ""], ["Lofgren", "Peter", ""], ["Goel", "Ashish", ""]]}, {"id": "1603.07947", "submitter": "Nouri Sakr", "authors": "Nourhan Sakr and Cliff Stein", "title": "An Empirical Study of Online Packet Scheduling Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies online scheduling algorithms for buffer management,\ndevelops new algorithms, and analyzes their performances. Packets arrive at a\nrelease time r, with a non-negative weight w and an integer deadline d. At each\ntime step, at most one packet is scheduled. The modified greedy (MG) algorithm\nis 1.618-competitive for the objective of maximizing the sum of weights of\npackets sent, assuming agreeable deadlines. We analyze the empirical behavior\nof MG in a situation with arbitrary deadlines and demonstrate that it is at a\ndisadvantage when frequently preferring maximum weight packets over early\ndeadline ones. We develop the MLP algorithm, which remedies this problem whilst\nmimicking the behavior of the offline algorithm. Our comparative analysis shows\nthat, although the competitive ratio of MLP is not as good as that of MG, it\nperforms better in practice. We validate this by simulating the behavior of\nboth algorithms under a spectrum of simulated parameter settings. Finally, we\npropose the design of three additional algorithms, which may help in improving\nperformance in practice.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 16:07:06 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Sakr", "Nourhan", ""], ["Stein", "Cliff", ""]]}, {"id": "1603.07981", "submitter": "Yuan Zhong", "authors": "Zhen Qiu, Cliff Stein and Yuan Zhong", "title": "Experimental Analysis of Algorithms for Coflow Scheduling", "comments": "29 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers face new scheduling challenges in optimizing job-level\nperformance objectives, where a significant challenge is the scheduling of\nhighly parallel data flows with a common performance goal (e.g., the shuffle\noperations in MapReduce applications). Chowdhury and Stoica introduced the\ncoflow abstraction to capture these parallel communication patterns, and\nChowdhury et al. proposed effective heuristics to schedule coflows efficiently.\nIn our previous paper, we considered the strongly NP-hard problem of minimizing\nthe total weighted completion time of coflows with release dates, and developed\nthe first polynomial-time scheduling algorithms with O(1)-approximation ratios.\n  In this paper, we carry out a comprehensive experimental analysis on a\nFacebook trace and extensive simulated instances to evaluate the practical\nperformance of several algorithms for coflow scheduling, including the\napproximation algorithms developed in our previous paper. Our experiments\nsuggest that simple algorithms provide effective approximations of the optimal,\nand that the performance of our approximation algorithms is relatively robust,\nnear optimal, and always among the best compared with the other algorithms, in\nboth the offline and online settings.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 18:45:51 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Qiu", "Zhen", ""], ["Stein", "Cliff", ""], ["Zhong", "Yuan", ""]]}, {"id": "1603.07991", "submitter": "Joshua Daymude", "authors": "Sarah Cannon, Joshua J. Daymude, Dana Randall, Andr\\'ea W. Richa", "title": "A Markov Chain Algorithm for Compression in Self-Organizing Particle\n  Systems", "comments": null, "journal-ref": "PODC '16: Proceedings of the 2016 ACM Symposium on Principles of\n  Distributed Computing, pp. 279-288", "doi": "10.1145/2933057.2933107", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems of programmable matter, we are given a collection of simple\ncomputation elements (or particles) with limited (constant-size) memory. We are\ninterested in when they can self-organize to solve system-wide problems of\nmovement, configuration and coordination. Here, we initiate a stochastic\napproach to developing robust distributed algorithms for programmable matter\nsystems using Markov chains. We are able to leverage the wealth of prior work\nin Markov chains and related areas to design and rigorously analyze our\ndistributed algorithms and show that they have several desirable properties.\n  We study the compression problem, in which a particle system must gather as\ntightly together as possible, as in a sphere or its equivalent in the presence\nof some underlying geometry. More specifically, we seek fully distributed,\nlocal, and asynchronous algorithms that lead the system to converge to a\nconfiguration with small boundary. We present a Markov chain-based algorithm\nthat solves the compression problem under the geometric amoebot model, for\nparticle systems that begin in a connected configuration. The algorithm takes\nas input a bias parameter $\\lambda$, where $\\lambda > 1$ corresponds to\nparticles favoring having more neighbors. We show that for all $\\lambda >\n2+\\sqrt{2}$, there is a constant $\\alpha > 1$ such that eventually with all but\nexponentially small probability the particles are $\\alpha$-compressed, meaning\nthe perimeter of the system configuration is at most $\\alpha \\cdot p_{min}$,\nwhere $p_{min}$ is the minimum possible perimeter of the particle system.\nSurprisingly, the same algorithm can also be used for expansion when $0 <\n\\lambda < 2.17$, and we prove similar results about expansion for values of\n$\\lambda$ in this range. This is counterintuitive as it shows that particles\npreferring to be next to each other ($\\lambda > 1$) is not sufficient to\nguarantee compression.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 19:36:24 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 14:30:25 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 14:07:03 GMT"}, {"version": "v4", "created": "Tue, 26 Feb 2019 00:13:06 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Cannon", "Sarah", ""], ["Daymude", "Joshua J.", ""], ["Randall", "Dana", ""], ["Richa", "Andr\u00e9a W.", ""]]}, {"id": "1603.08073", "submitter": "Hiroshi Hirai", "authors": "Hiroshi Hirai and Hiroyuki Namba", "title": "Shortest (A+B)-path packing via hafnian", "comments": "To appear in Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bj\\\"orklund and Husfeldt developed a randomized polynomial time algorithm to\nsolve the shortest two disjoint paths problem. Their algorithm is based on\ncomputation of permanents modulo 4 and the isolation lemma. In this paper, we\nconsider the following generalization of the shortest two disjoint paths\nproblem, and develop a similar algebraic algorithm. The shortest perfect\n$(A+B)$-path packing problem is: given an undirected graph $G$ and two disjoint\nnode subsets $A,B$ with even cardinalities, find a shortest $|A|/2+|B|/2$\ndisjoint paths whose ends are both in $A$ or both in $B$. Besides its\nNP-hardness, we prove that this problem can be solved in randomized polynomial\ntime if $|A|+|B|$ is fixed. Our algorithm basically follows the framework of\nBj\\\"orklund and Husfeldt but uses a new technique: computation of hafnian\nmodulo $2^k$ combined with Gallai's reduction from $T$-paths to matchings. We\nalso generalize our technique for solving other path packing problems, and\ndiscuss its limitation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 04:42:17 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 05:03:06 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Hirai", "Hiroshi", ""], ["Namba", "Hiroyuki", ""]]}, {"id": "1603.08151", "submitter": "L\\'aszl\\'o Kozma", "authors": "L\\'aszl\\'o Kozma, Thatchaphol Saranurak", "title": "Binary search trees and rectangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical problem of searching in a binary search tree (BST)\nusing rotations, and present novel connections of this problem to a number of\ngeometric and combinatorial structures. In particular, we show that the\nexecution trace of a BST that serves a sequence of queries is in close\ncorrespondence with the flip-sequence between two rectangulations.\n(Rectangulations are well-studied combinatorial objects also known as mosaic\nfloorplans.) We also reinterpret Small Manhattan Network, a problem with known\nconnections to the BST problem, in terms of flips in rectangulations. We apply\nfurther transformations to the obtained geometric model, to arrive at a\nparticularly simple view of the BST problem that resembles sequences of\nedge-relaxations in a shortest path algorithm.\n  Our connections yield new results and observations for all structures\nconcerned. In this draft we present some preliminary findings. BSTs with\nrotations are among the most fundamental and most thoroughly studied objects in\ncomputer science, nonetheless they pose long-standing open questions, such as\nthe dynamic optimality conjecture of Sleator and Tarjan (STOC 1983). Our hope\nis that the correspondences presented in this paper provide a new perspective\non this old problem and bring new tools to the study of dynamic optimality.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:49:46 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Kozma", "L\u00e1szl\u00f3", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1603.08390", "submitter": "Jingbo Zhou", "authors": "Jingbo Zhou, Qi Guo, H. V. Jagadish, Lubo\\v{s} Kr\\v{c}\\'al, Siyuan\n  Liu, Wenhao Luan, Anthony K. H. Tung, Yueji Yang, Yuxin Zheng", "title": "A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report", "comments": "18 pages, technical report for the ICDE 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 14:44:34 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 06:05:25 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 08:49:16 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhou", "Jingbo", ""], ["Guo", "Qi", ""], ["Jagadish", "H. V.", ""], ["Kr\u010d\u00e1l", "Lubo\u0161", ""], ["Liu", "Siyuan", ""], ["Luan", "Wenhao", ""], ["Tung", "Anthony K. H.", ""], ["Yang", "Yueji", ""], ["Zheng", "Yuxin", ""]]}, {"id": "1603.08432", "submitter": "Lida Kanari", "authors": "Lida Kanari, Pawe{\\l} D{\\l}otko, Martina Scolamiero, Ran Levi, Julian\n  Shillcock, Kathryn Hess, Henry Markram", "title": "Quantifying topological invariants of neuronal morphologies", "comments": "10 pages, 5 figures, conference or other essential info", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DS math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nervous systems are characterized by neurons displaying a diversity of\nmorphological shapes. Traditionally, different shapes have been qualitatively\ndescribed based on visual inspection and quantitatively described based on\nmorphometric parameters. Neither process provides a solid foundation for\ncategorizing the various morphologies, a problem that is important in many\nfields. We propose a stable topological measure as a standardized descriptor\nfor any tree-like morphology, which encodes its skeletal branching anatomy.\nMore specifically it is a barcode of the branching tree as determined by a\nspherical filtration centered at the root or neuronal soma. This Topological\nMorphology Descriptor (TMD) allows for the discrimination of groups of random\nand neuronal trees at linear computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 16:32:44 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Kanari", "Lida", ""], ["D\u0142otko", "Pawe\u0142", ""], ["Scolamiero", "Martina", ""], ["Levi", "Ran", ""], ["Shillcock", "Julian", ""], ["Hess", "Kathryn", ""], ["Markram", "Henry", ""]]}, {"id": "1603.08485", "submitter": "Sarah Allen", "authors": "Sarah R. Allen, Luis Barba, John Iacono, Stefan Langerman", "title": "Incremental Voronoi Diagrams", "comments": "19 pages, SoCG 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the amortized number of combinatorial changes (edge insertions and\nremovals) needed to update the graph structure of the Voronoi diagram\n$\\mathcal{V}(S)$ (and several variants thereof) of a set $S$ of $n$ sites in\nthe plane as sites are added.\n  We define a general update operation for planar graphs modeling the\nincremental construction of several variants of Voronoi diagrams as well as the\nincremental construction of an intersection of halfspaces in $\\mathbb{R}^3$. We\nshow that the amortized number of edge insertions and removals needed to add a\nnew site is $O(\\sqrt{n})$. A matching $\\Omega(\\sqrt{n})$ combinatorial lower\nbound is shown, even in the case where the graph of the diagram is a tree. This\ncontrasts with the $O(\\log{n})$ upper bound of Aronov et al. (2006) for\nfarthest-point Voronoi diagrams when the points are inserted in order along\ntheir convex hull.\n  We present a semi-dynamic data structure that maintains the Voronoi diagram\nof a set $S$ of $n$ sites in convex position. This structure supports the\ninsertion of a new site $p$ and finds the asymptotically minimal number $K$ of\nedge insertions and removals needed to obtain the diagram of $S \\cup \\{p\\}$\nfrom the diagram of $S$, in time $O(K\\,\\mathrm{polylog}\\ n)$ worst case, which\nis $O(\\sqrt{n}\\;\\mathrm{polylog}\\ n)$ amortized by the aforementioned result.\n  The most distinctive feature of this data structure is that the graph of the\nVoronoi diagram is maintained at all times and can be traversed in the natural\nway; this contrasts with other known data structures supporting nearest\nneighbor queries. Our data structure supports general search operations on the\ncurrent Voronoi diagram, which can, for example, be used to perform point\nlocation queries in the cells of the current Voronoi diagram in $O(\\log n)$\ntime, or to determine whether two given sites are neighbors in the Delaunay\ntriangulation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:59:36 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Allen", "Sarah R.", ""], ["Barba", "Luis", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""]]}, {"id": "1603.08616", "submitter": "Lin Chen", "authors": "Lin Chen, Forrest W Crawford, Amin Karbasi", "title": "Submodular Variational Inference for Network Reconstruction", "comments": "Accepted for UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world and online social networks, individuals receive and transmit\ninformation in real time. Cascading information transmissions (e.g. phone\ncalls, text messages, social media posts) may be understood as a realization of\na diffusion process operating on the network, and its branching path can be\nrepresented by a directed tree. The process only traverses and thus reveals a\nlimited portion of the edges. The network reconstruction/inference problem is\nto infer the unrevealed connections. Most existing approaches derive a\nlikelihood and attempt to find the network topology maximizing the likelihood,\na problem that is highly intractable. In this paper, we focus on the network\nreconstruction problem for a broad class of real-world diffusion processes,\nexemplified by a network diffusion scheme called respondent-driven sampling\n(RDS). We prove that under realistic and general models of network diffusion,\nthe posterior distribution of an observed RDS realization is a Bayesian\nlog-submodular model.We then propose VINE (Variational Inference for Network\nrEconstruction), a novel, accurate, and computationally efficient variational\ninference algorithm, for the network reconstruction problem under this model.\nCrucially, we do not assume any particular probabilistic model for the\nunderlying network. VINE recovers any connected graph with high accuracy as\nshown by our experimental results on real-life networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 02:13:17 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 06:58:29 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Lin", ""], ["Crawford", "Forrest W", ""], ["Karbasi", "Amin", ""]]}, {"id": "1603.08627", "submitter": "Matthew Williamson", "authors": "Pavlos Eirinakis, Matthew Williamson, and K. Subramani", "title": "On the Shoshan-Zwick Algorithm for the All-Pairs Shortest Path Problem", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shoshan-Zwick algorithm solves the all pairs shortest paths problem in\nundirected graphs with integer edge costs in the range $\\{1, 2, \\dots, M\\}$. It\nruns in $\\tilde{O}(M\\cdot n^{\\omega})$ time, where $n$ is the number of\nvertices, $M$ is the largest integer edge cost, and $\\omega < 2.3727$ is the\nexponent of matrix multiplication. It is the fastest known algorithm for this\nproblem. This paper points out the erroneous behavior of the Shoshan-Zwick\nalgorithm and revises the algorithm to resolve the issues that cause this\nbehavior. Moreover, it discusses implementation aspects of the Shoshan-Zwick\nalgorithm using currently-existing sub-cubic matrix multiplication algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 03:34:38 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Eirinakis", "Pavlos", ""], ["Williamson", "Matthew", ""], ["Subramani", "K.", ""]]}, {"id": "1603.08655", "submitter": "Louxin Zhang", "authors": "Andreas DM Gunawan, Bhaskar DasGupta, Louxin Zhang", "title": "Locating a Phylogenetic Tree in a Reticulation-Visible Network in\n  Quadratic Time", "comments": "The journal version of arXiv:1507.02119v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In phylogenetics, phylogenetic trees are rooted binary trees, whereas\nphylogenetic networks are rooted arbitrary acyclic digraphs. Edges are directed\naway from the root and leaves are uniquely labeled with taxa in phylogenetic\nnetworks. For the purpose of validating evolutionary models, biologists check\nwhether or not a phylogenetic tree is contained in a phylogenetic network on\nthe same taxa. This tree containment problem is known to be NP-complete. A\nphylogenetic network is reticulation-visible if every reticulation node\nseparates the root of the network from some leaves. We answer an open problem\nby proving that the problem is solvable in quadratic time for\nreticulation-visible networks. The key tool used in our answer is a powerful\ndecomposition theorem. It also allows us to design a linear-time algorithm for\nthe cluster containment problem for networks of this type and to prove that\nevery galled network with n leaves has 2(n-1) reticulation nodes at most.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 06:13:27 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Gunawan", "Andreas DM", ""], ["DasGupta", "Bhaskar", ""], ["Zhang", "Louxin", ""]]}, {"id": "1603.08675", "submitter": "Anupam Prakash", "authors": "Iordanis Kerenidis and Anupam Prakash", "title": "Quantum Recommendation Systems", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recommendation system uses the past purchases or ratings of $n$ products by\na group of $m$ users, in order to provide personalized recommendations to\nindividual users. The information is modeled as an $m \\times n$ preference\nmatrix which is assumed to have a good rank-$k$ approximation, for a small\nconstant $k$.\n  In this work, we present a quantum algorithm for recommendation systems that\nhas running time $O(\\text{poly}(k)\\text{polylog}(mn))$. All known classical\nalgorithms for recommendation systems that work through reconstructing an\napproximation of the preference matrix run in time polynomial in the matrix\ndimension. Our algorithm provides good recommendations by sampling efficiently\nfrom an approximation of the preference matrix, without reconstructing the\nentire matrix. For this, we design an efficient quantum procedure to project a\ngiven vector onto the row space of a given matrix. This is the first algorithm\nfor recommendation systems that runs in time polylogarithmic in the dimensions\nof the matrix and provides an example of a quantum machine learning algorithm\nfor a real world application.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 08:25:22 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 03:41:23 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 09:32:16 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Kerenidis", "Iordanis", ""], ["Prakash", "Anupam", ""]]}, {"id": "1603.08777", "submitter": "Wolfgang Mulzer", "authors": "Pat Morin, Wolfgang Mulzer, Tommy Reddad", "title": "Encoding Arguments", "comments": "50 pages, 7 figures", "journal-ref": "ACM Computing Surveys (CSUR), 50(3), July 2017, Article 46", "doi": "10.1145/3084288", "report-no": null, "categories": "cs.IT cs.DS math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many proofs in discrete mathematics and theoretical computer science are\nbased on the probabilistic method. To prove the existence of a good object, we\npick a random object and show that it is bad with low probability. This method\nis effective, but the underlying probabilistic machinery can be daunting.\n\"Encoding arguments\" provide an alternative presentation in which probabilistic\nreasoning is encapsulated in a \"uniform encoding lemma\". This lemma provides an\nupper bound on the probability of an event using the fact that a uniformly\nrandom choice from a set of size $n$ cannot be encoded with fewer than $\\log_2\nn$ bits on average. With the lemma, the argument reduces to devising an\nencoding where bad objects have short codewords.\n  In this expository article, we describe the basic method and provide a simple\ntutorial on how to use it. After that, we survey many applications to classic\nproblems from discrete mathematics and computer science. We also give a\ngeneralization for the case of non-uniform distributions, as well as a rigorous\njustification for the use of non-integer codeword lengths in encoding\narguments. These latter two results allow encoding arguments to be applied more\nwidely and to produce tighter results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 14:10:43 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 13:36:55 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Morin", "Pat", ""], ["Mulzer", "Wolfgang", ""], ["Reddad", "Tommy", ""]]}, {"id": "1603.08819", "submitter": "Nina Luhmann", "authors": "Nina Luhmann, Manuel Lafond, Annelyse Th\\'evenin, A\\\"ida Ouangraoua,\n  Roland Wittler and Cedric Chauve", "title": "The SCJ small parsimony problem for weighted gene adjacencies (Extended\n  version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing ancestral gene orders in a given phylogeny is a classical\nproblem in comparative genomics. Most existing methods compare conserved\nfeatures in extant genomes in the phylogeny to define potential ancestral gene\nadjacencies, and either try to reconstruct all ancestral genomes under a global\nevolutionary parsimony criterion, or, focusing on a single ancestral genome,\nuse a scaffolding approach to select a subset of ancestral gene adjacencies,\ngenerally aiming at reducing the fragmentation of the reconstructed ancestral\ngenome. In this paper, we describe an exact algorithm for the Small Parsimony\nProblem that combines both approaches. We consider that gene adjacencies at\ninternal nodes of the species phylogeny are weighted, and we introduce an\nobjective function defined as a convex combination of these weights and the\nevolutionary cost under the Single-Cut-or-Join (SCJ) model. The weights of\nancestral gene adjacencies can e.g. be obtained through the recent availability\nof ancient DNA sequencing data, which provide a direct hint at the genome\nstructure of the considered ancestor, or through probabilistic analysis of gene\nadjacencies evolution. We show the NP-hardness of our problem variant and\npropose a Fixed-Parameter Tractable algorithm based on the Sankoff-Rousseau\ndynamic programming algorithm that also allows to sample co-optimal solutions.\nWe apply our approach to mammalian and bacterial data providing different\ndegrees of complexity. We show that including adjacency weights in the\nobjective has a significant impact in reducing the fragmentation of the\nreconstructed ancestral gene orders.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:47:57 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 12:43:55 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Luhmann", "Nina", ""], ["Lafond", "Manuel", ""], ["Th\u00e9venin", "Annelyse", ""], ["Ouangraoua", "A\u00efda", ""], ["Wittler", "Roland", ""], ["Chauve", "Cedric", ""]]}, {"id": "1603.08883", "submitter": "Alfredo Braunstein", "authors": "Alfredo Braunstein, Luca Dall'Asta, Guilhem Semerjian, Lenka\n  Zdeborov\\'a", "title": "Network dismantling", "comments": "Source code and data can be found at\n  https://github.com/abraunst/decycler", "journal-ref": "Proceedings of the National Academy of Sciences 113, no. 44\n  (2016): 12368-12373", "doi": "10.1073/pnas.1605083113", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the network dismantling problem, which consists in determining a\nminimal set of vertices whose removal leaves the network broken into connected\ncomponents of sub-extensive size. For a large class of random graphs, this\nproblem is tightly connected to the decycling problem (the removal of vertices\nleaving the graph acyclic). Exploiting this connection and recent works on\nepidemic spreading we present precise predictions for the minimal size of a\ndismantling set in a large random graph with a prescribed (light-tailed) degree\ndistribution. Building on the statistical mechanics perspective we propose a\nthree-stage Min-Sum algorithm for efficiently dismantling networks, including\nheavy-tailed ones for which the dismantling and decycling problems are not\nequivalent. We also provide further insights into the dismantling problem\nconcluding that it is an intrinsically collective problem and that optimal\ndismantling sets cannot be viewed as a collection of individually well\nperforming nodes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 18:51:07 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 07:48:38 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Braunstein", "Alfredo", ""], ["Dall'Asta", "Luca", ""], ["Semerjian", "Guilhem", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1603.08976", "submitter": "Zachary Friggstad", "authors": "Zachary Friggstad, Mohsen Rezapour, and Mohammad R. Salavatipour", "title": "Local Search Yields a PTAS for k-Means in Doubling Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most well known and ubiquitous clustering problem encountered in nearly\nevery branch of science is undoubtedly $k$-means: given a set of data points\nand a parameter $k$, select $k$ centres and partition the data points into $k$\nclusters around these centres so that the sum of squares of distances of the\npoints to their cluster centre is minimized. Typically these data points lie\n$\\mathbb{R}^d$ for some $d\\geq 2$.\n  $k$-means and the first algorithms for it were introduced in the 1950's.\nSince then, hundreds of papers have studied this problem and many algorithms\nhave been proposed for it. The most commonly used algorithm is known as\nLloyd-Forgy, which is also referred to as \"the\" $k$-means algorithm, and\nvarious extensions of it often work very well in practice. However, they may\nproduce solutions whose cost is arbitrarily large compared to the optimum\nsolution. Kanungo et al. [2004] analyzed a simple local search heuristic to get\na polynomial-time algorithm with approximation ratio $9+\\epsilon$ for any fixed\n$\\epsilon>0$ for $k$-means in Euclidean space.\n  Finding an algorithm with a better approximation guarantee has remained one\nof the biggest open questions in this area, in particular whether one can get a\ntrue PTAS for fixed dimension Euclidean space. We settle this problem by\nshowing that a simple local search algorithm provides a PTAS for $k$-means in\n$\\mathbb{R}^d$ for any fixed $d$. More precisely, for any error parameter\n$\\epsilon>0$, the local search algorithm that considers swaps of up to\n$\\rho=d^{O(d)}\\cdot{\\epsilon}^{-O(d/\\epsilon)}$ centres at a time finds a\nsolution using exactly $k$ centres whose cost is at most a\n$(1+\\epsilon)$-factor greater than the optimum.\n  Finally, we provide the first demonstration that local search yields a PTAS\nfor the uncapacitated facility location problem and $k$-median with non-uniform\nopening costs in doubling metrics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:41:55 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 20:00:19 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Friggstad", "Zachary", ""], ["Rezapour", "Mohsen", ""], ["Salavatipour", "Mohammad R.", ""]]}, {"id": "1603.09009", "submitter": "Jakub Pachocki", "authors": "Alina Ene, Gary Miller, Jakub Pachocki, Aaron Sidford", "title": "Routing under Balance", "comments": "To appear in STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of balance for directed graphs: a weighted directed\ngraph is $\\alpha$-balanced if for every cut $S \\subseteq V$, the total weight\nof edges going from $S$ to $V\\setminus S$ is within factor $\\alpha$ of the\ntotal weight of edges going from $V\\setminus S$ to $S$. Several important\nfamilies of graphs are nearly balanced, in particular, Eulerian graphs (with\n$\\alpha = 1$) and residual graphs of $(1+\\epsilon)$-approximate undirected\nmaximum flows (with $\\alpha=O(1/\\epsilon)$).\n  We use the notion of balance to give a more fine-grained understanding of\nseveral well-studied routing questions that are considerably harder in directed\ngraphs. We first revisit oblivious routings in directed graphs. Our main\nalgorithmic result is an oblivious routing scheme for single-source instances\nthat achieve an $O(\\alpha \\cdot \\log^3 n / \\log \\log n)$ competitive ratio. In\nthe process, we make several technical contributions which may be of\nindependent interest. In particular, we give an efficient algorithm for\ncomputing low-radius decompositions of directed graphs parameterized by\nbalance. We also define and construct low-stretch arborescences, a\ngeneralization of low-stretch spanning trees to directed graphs.\n  On the negative side, we present new lower bounds for oblivious routing\nproblems on directed graphs. We show that the competitive ratio of oblivious\nrouting algorithms for directed graphs is $\\Omega(n)$ in general; this result\nimproves upon the long-standing best known lower bound of $\\Omega(\\sqrt{n})$\ngiven by Hajiaghayi, Kleinberg, Leighton and R\\\"acke in 2006. We also show that\nour restriction to single-source instances is necessary by showing an\n$\\Omega(\\sqrt{n})$ lower bound for multiple-source oblivious routing in\nEulerian graphs.\n  We also give a fast algorithm for the maximum flow problem in balanced\ndirected graphs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 01:09:47 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Ene", "Alina", ""], ["Miller", "Gary", ""], ["Pachocki", "Jakub", ""], ["Sidford", "Aaron", ""]]}, {"id": "1603.09205", "submitter": "Brandon Smock", "authors": "Brandon Smock and Joseph Wilson", "title": "A comprehensive theory of cascading via-paths and the reciprocal pointer\n  chain method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consolidate and expand upon the current theory and\npotential applications of the set of $k$ best \\emph{cascading via-paths} (CVPs)\nand the \\emph{reciprocal pointer chain} (RPC) method for identifying them. CVPs\nare a collection of up to $|V|$ paths between a source and a target node in a\ngraph $G = (V,E)$, computed using two shortest path trees, that have\ndistinctive properties relative to other path sets. They have been shown to be\nparticularly useful in geospatial applications, where they are an intuitive and\nefficient means for identifying a set of spatially diverse alternatives to the\nsingle shortest path between the source and target. However, spatial diversity\nis not intrinsic to paths in a graph, and little theory has been developed\noutside of application to describe the nature of these paths and the RPC method\nin general. Here we divorce the RPC method from its typical geospatial\napplications and develop a comprehensive theory of CVPs from an abstract\ngraph-theoretic perspective. Restricting ourselves to properties of the CVPs\nand of the entire set of $k$-best CVPs that can be computed in $O(|E| + |V|\n\\log |V|)$, we are able to then propose, among other things, new and efficient\napproaches to problems such as generating a diverse set of paths and to\ncomputing the $k$ shortest loopless paths between two nodes in a graph. We\nconclude by demonstrating the new theory in practice, first for a typical\napplication of finding alternative routes in road networks and then for a novel\napplication of identifying layer-boundaries in ground-penetrating radar (GPR)\ndata. It is our hope that by generalizing the RPC method, providing a sound\ntheoretical foundation, and demonstrating novel uses, we are able to broaden\nits perceived applicability and stimulate new research in this area, both\napplied and theoretical.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 14:08:11 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Smock", "Brandon", ""], ["Wilson", "Joseph", ""]]}, {"id": "1603.09320", "submitter": "Yury Malkov A", "authors": "Yu. A. Malkov, D. A. Yashunin", "title": "Efficient and robust approximate nearest neighbor search using\n  Hierarchical Navigable Small World graphs", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for the approximate K-nearest neighbor search based\non navigable small world graphs with controllable hierarchy (Hierarchical NSW,\nHNSW). The proposed solution is fully graph-based, without any need for\nadditional search structures, which are typically used at the coarse search\nstage of the most proximity graph techniques. Hierarchical NSW incrementally\nbuilds a multi-layer structure consisting from hierarchical set of proximity\ngraphs (layers) for nested subsets of the stored elements. The maximum layer in\nwhich an element is present is selected randomly with an exponentially decaying\nprobability distribution. This allows producing graphs similar to the\npreviously studied Navigable Small World (NSW) structures while additionally\nhaving the links separated by their characteristic distance scales. Starting\nsearch from the upper layer together with utilizing the scale separation boosts\nthe performance compared to NSW and allows a logarithmic complexity scaling.\nAdditional employment of a heuristic for selecting proximity graph neighbors\nsignificantly increases performance at high recall and in case of highly\nclustered data. Performance evaluation has demonstrated that the proposed\ngeneral metric space search index is able to strongly outperform previous\nopensource state-of-the-art vector-only approaches. Similarity of the algorithm\nto the skip list structure allows straightforward balanced distributed\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 19:29:44 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 07:27:25 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 12:07:54 GMT"}, {"version": "v4", "created": "Tue, 14 Aug 2018 19:29:07 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Malkov", "Yu. A.", ""], ["Yashunin", "D. A.", ""]]}, {"id": "1603.09448", "submitter": "Jianhua Tu", "authors": "Zongwen Bai, Jianhua Tu, Yongtang Shi", "title": "An improved algorithm for the vertex cover $P_3$ problem on graphs of\n  bounded treewidth", "comments": "arXiv admin note: text overlap with arXiv:1103.0534 by other authors", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 21 no.\n  4, Discrete Algorithms (November 4, 2019) dmtcs:5867", "doi": "10.23638/DMTCS-21-4-17", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ and a positive integer $t\\geq2$, the task in the\nvertex cover $P_t$ ($VCP_t$) problem is to find a minimum subset of vertices\n$F\\subseteq V$ such that every path of order $t$ in $G$ contains at least one\nvertex from $F$. The $VCP_t$ problem is NP-complete for any integer $t\\geq2$\nand has many applications in real world. Recently, the authors presented a\ndynamic programming algorithm running in time $4^p\\cdot n^{O(1)}$ for the\n$VCP_3$ problem on $n$-vertex graphs with treewidth $p$. In this paper, we\npropose an improvement of it and improved the time-complexity to $3^p\\cdot\nn^{O(1)}$.\n  The connected vertex cover $P_3$ ($CVCP_3$) problem is the connected\nvariation of the $VCP_3$ problem where $G[F]$ is required to be connected.\nUsing the Cut\\&Count technique, we give a randomized algorithm with runtime\n$4^p\\cdot n^{O(1)}$ for the $CVCP_3$ problem on $n$-vertex graphs with\ntreewidth $p$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 03:27:56 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 03:00:52 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 08:20:35 GMT"}, {"version": "v4", "created": "Tue, 25 Dec 2018 15:04:50 GMT"}, {"version": "v5", "created": "Sun, 6 Oct 2019 03:49:08 GMT"}, {"version": "v6", "created": "Thu, 24 Oct 2019 11:42:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bai", "Zongwen", ""], ["Tu", "Jianhua", ""], ["Shi", "Yongtang", ""]]}, {"id": "1603.09535", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad and Philip N. Klein and Claire Mathieu", "title": "Local search yields approximation schemes for k-means and k-median in\n  Euclidean and minor-free metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first polynomial-time approximation schemes (PTASs) for the\nfollowing problems: (1) uniform facility location in edge-weighted planar\ngraphs; (2) $k$-median and $k$-means in edge-weighted planar graphs; (3)\n$k$-means in Euclidean spaces of bounded dimension. Our first and second\nresults extend to minor-closed families of graphs. All our results extend to\ncost functions that are the $p$-th power of the shortest-path distance. The\nalgorithm is local search where the local neighborhood of a solution $S$\nconsists of all solutions obtained from $S$ by removing and adding\n$1/\\epsilon^{O(1)}$ centers.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 11:40:07 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 07:53:51 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Klein", "Philip N.", ""], ["Mathieu", "Claire", ""]]}]