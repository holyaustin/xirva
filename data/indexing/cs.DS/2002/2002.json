[{"id": "2002.00060", "submitter": "Guillaume Sagnol", "authors": "Guillaume Sagnol, Daniel Schmidt genannt Waldschmidt", "title": "Stochastic Extensible Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic extensible bin packing problem (SEBP) in which $n$\nitems of stochastic size are packed into $m$ bins of unit capacity. In contrast\nto the classical bin packing problem, the number of bins is fixed and they can\nbe extended at extra cost. This problem plays an important role in stochastic\nenvironments such as in surgery scheduling: Patients must be assigned to\noperating rooms beforehand, such that the regular capacity is fully utilized\nwhile the amount of overtime is as small as possible.\n  This paper focuses on essential ratios between different classes of policies:\nFirst, we consider the price of non-splittability, in which we compare the\noptimal non-anticipatory policy against the optimal fractional assignment\npolicy. We show that this ratio has a tight upper bound of $2$. Moreover, we\ndevelop an analysis of a fixed assignment variant of the LEPT rule yielding a\ntight approximation ratio of $(1+e^{-1}) \\approx 1.368$.\n  Furthermore, we prove that the price of fixed assignments, related to the\nbenefit of adaptivity, which describes the loss when restricting to fixed\nassignment policies, is within the same factor. This shows that in some sense,\nLEPT is the best fixed assignment policy we can hope for. We also provide a\nlower bound on the performance of this policy comparing against an optimal\nfixed assignment policy. Finally, we obtain improved bounds for the case where\nthe processing times are drawn from a particular family of distributions, with\neither a bounded Pietra index or when the familly is stochastically dominated\nat the second order.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 21:26:22 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sagnol", "Guillaume", ""], ["Waldschmidt", "Daniel Schmidt genannt", ""]]}, {"id": "2002.00071", "submitter": "Cole Franks", "authors": "Cole Franks, Ankur Moitra", "title": "Rigorous Guarantees for Tyler's M-estimator via quantum expansion", "comments": "Added Frobenius guarantees", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the shape of an elliptical distribution is a fundamental problem\nin statistics. One estimator for the shape matrix, Tyler's M-estimator, has\nbeen shown to have many appealing asymptotic properties. It performs well in\nnumerical experiments and can be quickly computed in practice by a simple\niterative procedure. Despite the many years the estimator has been studied in\nthe statistics community, there was neither a non-asymptotic bound on the rate\nof the estimator nor a proof that the iterative procedure converges in\npolynomially many steps.\n  Here we observe a surprising connection between Tyler's M-estimator and\noperator scaling, which has been intensively studied in recent years in part\nbecause of its connections to the Brascamp-Lieb inequality in analysis. We use\nthis connection, together with novel results on quantum expanders, to show that\nTyler's M-estimator has the optimal rate up to factors logarithmic in the\ndimension, and that in the generative model the iterative procedure has a\nlinear convergence rate even without regularization.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 22:01:51 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 19:32:28 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 14:09:21 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 16:37:33 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Franks", "Cole", ""], ["Moitra", "Ankur", ""]]}, {"id": "2002.00167", "submitter": "Wei Hao Khoong", "authors": "Wei Hao Khoong", "title": "Solving the Joint Order Batching and Picker Routing Problem for Large\n  Instances", "comments": "Thesis submitted to the department end April 2019. Please cite the\n  year as 2019 instead. Data and results can be found at\n  https://github.com/weihao94/Solving-the-Joint-Order-Batching-and-Picker-Routing-Problem-for-Large-Instances", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the problem of order batching and picker routing\nin warehouse storage areas. These problems are known to be capital and labour\nintensive, and often contribute to a sizable fraction of warehouse operating\ncosts. Here, we consider the case of online grocery shopping where orders may\nconsist of dozens of items.\n  We present the problem introduced and tackle the issue of solving the problem\nheuristically with proposed methods of solving that utilize batching and\nrouting heuristics. Instances with up to 50 orders were solved heuristically in\nlarge simulated warehouse instances consisting of 8 to 30 aisles, with 1 to 4\nblocks. The proposed methods were shown to have relatively short computation\ntimes as compared to optimally solving the problem in. In particular, we showed\nthat a proposed method which utilizes an optimal solver for routing yielded\npoorer results than methods that utilize routing heuristics.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 08:08:27 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Khoong", "Wei Hao", ""]]}, {"id": "2002.00253", "submitter": "Karthik Abinav Sankararaman", "authors": "Karthik Abinav Sankararaman and Aleksandrs Slivkins", "title": "Bandits with Knapsacks beyond the Worst-Case", "comments": "The initial version, titled \"Advances in Bandits with Knapsacks\", was\n  published on arxiv.org in Jan'20. The present version improves both upper and\n  lower bounds, deriving Theorem 3.2(ii) and Theorem 4.2. Moreover, it\n  simplifies the algorithm and analysis in the main result, and fixes several\n  issues in the lower bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandits with Knapsacks (BwK) is a general model for multi-armed bandits under\nsupply/budget constraints. While worst-case regret bounds for BwK are\nwell-understood, we present three results that go beyond the worst-case\nperspective. First, we provide upper and lower bounds which amount to a full\ncharacterization for logarithmic, instance-dependent regret rates. Second, we\nconsider \"simple regret\" in BwK, which tracks algorithm's performance in a\ngiven round, and prove that it is small in all but a few rounds. Third, we\nprovide a general \"reduction\" from BwK to bandits which takes advantage of some\nknown helpful structure, and apply this reduction to combinatorial\nsemi-bandits, linear contextual bandits, and multinomial-logit bandits. Our\nresults build on the BwK algorithm from \\citet{AgrawalDevanur-ec14}, providing\nnew analyses thereof.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 18:50:44 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 22:45:16 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 06:05:07 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 16:29:16 GMT"}, {"version": "v5", "created": "Mon, 31 May 2021 17:18:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sankararaman", "Karthik Abinav", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "2002.00263", "submitter": "Cun-Quan Zhang", "authors": "Scott Payne, Edgar Fuller, Cun-Quan Zhang", "title": "Edge-cuts Optimized for Average Weight: a new alternative to Ford and\n  Fulkerson", "comments": null, "journal-ref": "Asia-Pacific Journal of Operational Research, Vol. 36, No. 2\n  (2019) 1940006", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a directed graph associated with a weight $w: E(G) \\rightarrow\nR^+$. For an edge-cut $Q$ of $G$, the average weight of $Q$ is denoted and\ndefined as $w_{ave}(Q)=\\frac{\\sum_{e\\in Q}w(e)}{|Q|}$. An edge-cut of optimal\naverage weight is an edge-cut $Q$ such that $w_{ave}(Q)$ is maximum among all\nedge-cuts (or minimum, symmetrically). In this paper, a polynomial algorithm\nfor this problem is proved for finding such an optimal edge-cut in a rooted\ntree, separating the root and the set of all leafs. This algorithm enables us\nto develop an automatic clustering method with more accurate detection of\ncommunities embedded in a hierarchy tree structure.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 19:35:14 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 20:01:15 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Payne", "Scott", ""], ["Fuller", "Edgar", ""], ["Zhang", "Cun-Quan", ""]]}, {"id": "2002.00352", "submitter": "Xingwen Zhang", "authors": "Xingwen Zhang, Feng Qi, Zhigang Hua, Shuang Yang", "title": "Solving Billion-Scale Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knapsack problems (KPs) are common in industry, but solving KPs is known to\nbe NP-hard and has been tractable only at a relatively small scale. This paper\nexamines KPs in a slightly generalized form and shows that they can be solved\nnearly optimally at scale via distributed algorithms. The proposed approach can\nbe implemented fairly easily with off-the-shelf distributed computing\nframeworks (e.g. MPI, Hadoop, Spark). As an example, our implementation leads\nto one of the most efficient KP solvers known to date -- capable to solve KPs\nat an unprecedented scale (e.g., KPs with 1 billion decision variables and 1\nbillion constraints can be solved within 1 hour). The system has been deployed\nto production and called on a daily basis, yielding significant business\nimpacts at Ant Financial.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 08:51:36 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Xingwen", ""], ["Qi", "Feng", ""], ["Hua", "Zhigang", ""], ["Yang", "Shuang", ""]]}, {"id": "2002.00540", "submitter": "Albert Kim", "authors": "Albert Kim, Atalay Mert Ileri, Sam Madden", "title": "Optimizing Query Predicates with Disjunctions for Column Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, database research has given limited attention to\noptimizing predicates with disjunctions. What little past work there is has\nfocused on optimizations for traditional row-oriented databases. A key\ndifference in predicate evaluation for row stores and column stores is that\nwhile row stores apply predicates to one record at a time, column stores apply\npredicates to sets of records. Not only must the execution engine decide the\norder in which to apply the predicates, but it must also decide how many times\neach predicate should be applied and on which sets of records it should be\napplied to. In our work, we tackle exactly this problem. We formulate, analyze,\nand solve the predicate evaluation problem for column stores. Our results\ninclude proofs about various properties of the problem, and in turn, these\nproperties have allowed us to derive the first polynomial-time (i.e., O(n log\nn)) algorithm ShallowFish which evaluates predicates optimally for all\npredicate expressions with a depth of 2 or less. We capture the exact property\nwhich makes the problem more difficult for predicate expressions of depth 3 or\ngreater and propose an approximate algorithm DeepFish which outperforms\nShallowFish in these situations. Finally, we show that both ShallowFish and\nDeepFish outperform the corresponding state of the art by two orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 02:35:50 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kim", "Albert", ""], ["Ileri", "Atalay Mert", ""], ["Madden", "Sam", ""]]}, {"id": "2002.00558", "submitter": "Mark Sellke", "authors": "Mark Sellke, Aleksandrs Slivkins", "title": "The Price of Incentivizing Exploration: A Characterization via Thompson\n  Sampling and Sample Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider incentivized exploration: a version of multi-armed bandits where\nthe choice of arms is controlled by self-interested agents, and the algorithm\ncan only issue recommendations. The algorithm controls the flow of information,\nand the information asymmetry can incentivize the agents to explore. Prior work\nachieves optimal regret rates up to multiplicative factors that become\narbitrarily large depending on the Bayesian priors, and scale exponentially in\nthe number of arms. A more basic problem of sampling each arm once runs into\nsimilar factors.\n  We focus on the price of incentives: the loss in performance, broadly\nconstrued, incurred for the sake of incentive-compatibility. We prove that\nThompson Sampling, a standard bandit algorithm, is incentive-compatible if\ninitialized with sufficiently many data points. The performance loss due to\nincentives is therefore limited to the initial rounds when these data points\nare collected. The problem is largely reduced to that of sample complexity: how\nmany rounds are needed? We address this question, providing matching upper and\nlower bounds and instantiating them in various corollaries. Typically, the\noptimal sample complexity is polynomial in the number of arms and exponential\nin the \"strength of beliefs\".\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 04:58:51 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 02:59:53 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 19:43:52 GMT"}, {"version": "v4", "created": "Sat, 5 Jun 2021 04:13:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sellke", "Mark", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "2002.00813", "submitter": "Kathrin Hanauer", "authors": "Kathrin Hanauer, Monika Henzinger, Christian Schulz", "title": "Faster Fully Dynamic Transitive Closure in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully dynamic transitive closure problem asks to maintain reachability\ninformation in a directed graph between arbitrary pairs of vertices, while the\ngraph undergoes a sequence of edge insertions and deletions. The problem has\nbeen thoroughly investigated in theory and many specialized algorithms for\nsolving it have been proposed in the last decades. In two large studies\n[Frigioni ea, 2001; Krommidas and Zaroliagis, 2008], a number of these\nalgorithms have been evaluated experimentally against simple static algorithms\nfor graph traversal, showing the competitiveness and even superiority of the\nsimple algorithms in practice, except for very dense random graphs or very high\nratios of queries. A major drawback of those studies is that only small and\nmostly randomly generated graphs are considered.\n  In this paper, we engineer new algorithms to maintain all-pairs reachability\ninformation which are simple and space-efficient. Moreover, we perform an\nextensive experimental evaluation on both generated and real-world instances\nthat are several orders of magnitude larger than those in the previous studies.\nOur results indicate that our new algorithms outperform all state-of-the-art\nalgorithms on all types of input considerably in practice.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:12:12 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Hanauer", "Kathrin", ""], ["Henzinger", "Monika", ""], ["Schulz", "Christian", ""]]}, {"id": "2002.01164", "submitter": "Ali Cakmak", "authors": "Mehmet Aytimur and Ali Cakmak", "title": "Using Positional Sequence Patterns to Estimate the Selectivity of SQL\n  LIKE Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the dramatic increase in the amount of the text-based data which\ncommonly contains misspellings and other errors, querying such data with\nflexible search patterns becomes more and more commonplace. Relational\ndatabases support the LIKE operator to allow searching with a particular\nwildcard predicate (e.g., LIKE 'Sub%', which matches all strings starting with\n'Sub'). Due to the large size of text data, executing such queries in the most\noptimal way is quite critical for database performance. While building the most\nefficient execution plan for a LIKE query, the query optimizer requires the\nselectivity estimate for the flexible pattern-based query predicate. Recently,\nSPH algorithm is proposed which employs a sequence pattern-based histogram\nstructure to estimate the selectivity of LIKE queries. A drawback of the SPH\napproach is that it often overestimates the selectivity of queries. In order to\nalleviate the overestimation problem, in this paper, we propose a novel\nsequence pattern type, called positional sequence patterns. The proposed\npatterns differentiate between sequence item pairs that appear next to each\nother in all pattern occurrences from those that may have other items between\nthem. Besides, we employ redundant pattern elimination based on pattern\ninformation content during histogram construction. Finally, we propose a\npartitioning-based matching scheme during the selectivity estimation. The\nexperimental results on a real dataset from DBLP show that the proposed\napproach outperforms the state of the art by around 20% improvement in error\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 08:02:15 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Aytimur", "Mehmet", ""], ["Cakmak", "Ali", ""]]}, {"id": "2002.01178", "submitter": "Vincent Froese", "authors": "Nathan Schaar, Vincent Froese, Rolf Niedermeier", "title": "Faster Binary Mean Computation Under Dynamic Time Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many consensus string problems are based on Hamming distance. We replace\nHamming distance by the more flexible (e.g., easily coping with different input\nstring lengths) dynamic time warping distance, best known from applications in\ntime series mining. Doing so, we study the problem of finding a mean string\nthat minimizes the sum of (squared) dynamic time warping distances to a given\nset of input strings. While this problem is known to be NP-hard (even for\nstrings over a three-element alphabet), we address the binary alphabet case\nwhich is known to be polynomial-time solvable. We significantly improve on a\npreviously known algorithm in terms of worst-case running time. Moreover, we\nalso show the practical usefulness of one of our algorithms in experiments with\nreal-world and synthetic data. Finally, we identify special cases solvable in\nlinear time (e.g., finding a mean of only two binary input strings) and report\nsome empirical findings concerning combinatorial properties of optimal means.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:18:26 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Schaar", "Nathan", ""], ["Froese", "Vincent", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2002.01218", "submitter": "Eduard Eiben", "authors": "Eduard Eiben and Daniel Lokshtanov", "title": "Removing Connected Obstacles in the Plane is FPT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two points in the plane, a set of obstacles defined by closed curves,\nand an integer $k$, does there exist a path between the two designated points\nintersecting at most $k$ of the obstacles? This is a fundamental and\nwell-studied problem arising naturally in computational geometry, graph theory,\nwireless computing, and motion planning. It remains $\\textsf{NP}$-hard even\nwhen the obstacles are very simple geometric shapes (e.g., unit-length line\nsegments). In this paper, we show that the problem is fixed-parameter tractable\n($\\textsf{FPT}$) parameterized by $k$, by giving an algorithm with running time\n$k^{O(k^3)}n^{O(1)}$. Here $n$ is the number connected areas in the plane\ndrawing of all the obstacles.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 10:50:28 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Eiben", "Eduard", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "2002.01258", "submitter": "Luca Ganassali", "authors": "Luca Ganassali, Laurent Massouli\\'e", "title": "From tree matching to sparse graph alignment", "comments": "33 pages, 10 figures, accepted at COLT 2020. Typos corrected, some\n  new figures, some remarks and explanations detailed, minor changes in proof\n  of Th. 1.2", "journal-ref": "Proceedings of Thirty Third Conference on Learning Theory, PMLR\n  125:1633-1665, 2020", "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider alignment of sparse graphs, for which we introduce\nthe Neighborhood Tree Matching Algorithm (NTMA). For correlated\nErd\\H{o}s-R\\'{e}nyi random graphs, we prove that the algorithm returns -- in\npolynomial time -- a positive fraction of correctly matched vertices, and a\nvanishing fraction of mismatches. This result holds with average degree of the\ngraphs in $O(1)$ and correlation parameter $s$ that can be bounded away from 1,\nconditions under which random graph alignment is particularly challenging. As a\nbyproduct of the analysis we introduce a matching metric between trees and\ncharacterize it for several models of correlated random trees. These results\nmay be of independent interest, yielding for instance efficient tests for\ndetermining whether two random trees are correlated or independent.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 12:36:21 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 15:47:33 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ganassali", "Luca", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "2002.01289", "submitter": "Pietro  Hiram Guzzi", "authors": "Pietro Hiram Guzzi, Emanuel Salerno, Giuseppe Tradigo, and Pierangelo\n  Veltri", "title": "Extracting Dense and Connected Subgraphs in Dual Networks by Network\n  Alignment", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3020924", "report-no": null, "categories": "cs.DS cs.SI q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of network based approaches to model and analyse large datasets is\ncurrently a growing research field. For instance in biology and medicine,\nnetworks are used to model interactions among biological molecules as well as\nrelations among patients. Similarly, data coming from social networks can be\ntrivially modelled by using graphs. More recently, the use of dual networks\ngained the attention of researchers. A dual network model uses a pair of graphs\nto model a scenario in which one of the two graphs is usually unweighted (a\nnetwork representing physical associations among nodes) while the other one is\nedge-weighted (a network representing conceptual associations among nodes). In\nthis paper we focus on the problem of finding the Densest Connected sub-graph\n(DCS) having the largest density in the conceptual network which is also\nconnected in the physical network. The problem is relevant but also\ncomputationally hard, therefore the need for introducing of novel algorithms\narises. We formalise the problem and then we map DCS into a graph alignment\nproblem. Then we propose a possible solution. A set of experiments is also\npresented to support our approach.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:10:50 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Guzzi", "Pietro Hiram", ""], ["Salerno", "Emanuel", ""], ["Tradigo", "Giuseppe", ""], ["Veltri", "Pierangelo", ""]]}, {"id": "2002.01307", "submitter": "Dmitry Gribanov", "authors": "D.V. Gribanov, D.S. Malyshev, P.M. Pardalos", "title": "A note on the parametric integer programming in the average case:\n  sparsity, proximity, and FPT-algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Integer Linear Programming (ILP) problem $\\max\\{c^\\top x : A\nx \\leq b,\\, x \\in Z^n \\}$, parameterized by a right-hand side vector $b \\in\nZ^m$, where $A \\in Z^{m \\times n}$ is a matrix of the rank $n$. Let $v$ be an\noptimal vertex of the Linear Programming (LP) relaxation $\\max\\{c^\\top x : A x\n\\leq b\\}$ and $B$ be a corresponding optimal base. We show that, for almost all\n$b \\in Z^m$, an optimal point of the square ILP problem $\\max\\{c^\\top x : A_B x\n\\leq b_B,\\, x \\in Z^n \\}$ satisfies the constraints $A x \\leq b$ of the\noriginal problem. From works of R. Gomory it directly follows that the square\nILP problem $\\max\\{c^\\top x : A_B x \\leq b_B,\\, x \\in Z^n \\}$ can be solved by\nan algorithm of the arithmetic complexity $O(n \\cdot \\delta \\cdot \\log\n\\delta)$, where $\\delta = |\\det A_B|$. Consequently, it can be shown that, for\nalmost all $b \\in Z^m$, the original problem $\\max\\{c^\\top x : A x \\leq b,\\, x\n\\in Z^n \\}$ can be solved by an algorithm of the arithmetic complexity $O(n\n\\cdot \\Delta \\cdot \\log \\Delta)$, where $\\Delta$ is the maximum absolute value\nof $n \\times n$ minors of $A$. By the same technique, we give new inequalities\non the integrality gap and sparsity of a solution and slack variables.\n  Another ingredient is a known lemma that states the equality of the maximum\nabsolute values of rank minors of matrices with orthogonal columns. This lemma\ngives us a way to transform ILP problems of the type $\\max\\{c^\\top x : A x =\nb,\\, x \\in Z^n_+\\}$ to problems of the previous type, here we assume that\n$rank(A) = m$ and all the $m \\times m$ minors of $A$ are coprime. Consequently,\nit follows that, for almost all $b \\in Z^m$, there exists an algorithm with the\narithmetic complexity $O((n-m) \\cdot \\Delta \\cdot \\log \\Delta)$ to solve the\nproblem in the equality form. Sparsity and integrality gap bounds are also\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 12:40:45 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 09:34:57 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 12:09:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Gribanov", "D. V.", ""], ["Malyshev", "D. S.", ""], ["Pardalos", "P. M.", ""]]}, {"id": "2002.01610", "submitter": "Elham Havvaei", "authors": "David Eppstein, Daniel Frishberg, Elham Havvaei", "title": "Simplifying Activity-on-Edge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the simplification of activity-on-edge graphs used for\nvisualizing project schedules, where the vertices of the graphs represent\nproject milestones, and the edges represent either tasks of the project or\ntiming constraints between milestones. In this framework, a timeline of the\nproject can be constructed as a leveled drawing of the graph, where the levels\nof the vertices represent the time at which each milestone is scheduled to\nhappen. We focus on the following problem: given an activity-on-edge graph\nrepresenting a project, find an equivalent activity-on-edge graph (one with the\nsame critical paths) that has the minimum possible number of milestone vertices\namong all equivalent activity-on-edge graphs. We provide a polynomial-time\nalgorithm for solving this graph minimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 02:25:17 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Eppstein", "David", ""], ["Frishberg", "Daniel", ""], ["Havvaei", "Elham", ""]]}, {"id": "2002.01802", "submitter": "Qiankun Zhang", "authors": "Zhiyi Huang and Qiankun Zhang", "title": "Online Primal Dual Meets Online Matching with Stochastic Rewards:\n  Configuration LP to the Rescue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mehta and Panigrahi (FOCS 2012) introduce the problem of online matching with\nstochastic rewards, where edges are associated with success probabilities and a\nmatch succeeds with the probability of the corresponding edge. It is one of the\nfew online matching problems that have defied the randomized online primal dual\nframework by Devanur, Jain, and Kleinberg (SODA 2013) thus far. This paper\nunlocks the power of randomized online primal dual in online matching with\nstochastic rewards by employing the configuration linear program rather than\nthe standard matching linear program used in previous works. Our main result is\na 0.572 competitive algorithm for the case of vanishing and unequal\nprobabilities, improving the best previous bound of 0.534 by Mehta, Waggoner,\nand Zadimoghaddam (SODA 2015) and, in fact, is even better than the best\nprevious bound of 0.567 by Mehta and Panigrahi (FOCS 2012) for the more\nrestricted case of vanishing and equal probabilities. For vanishing and equal\nprobabilities, we get a better competitive ratio of 0.576. Our results further\ngeneralize to the vertex-weighted case due to the intrinsic robustness of the\nrandomized online primal dual analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 14:18:48 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Huang", "Zhiyi", ""], ["Zhang", "Qiankun", ""]]}, {"id": "2002.01919", "submitter": "Badih Ghazi", "authors": "Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh,\n  Ameya Velingker", "title": "Pure Differentially Private Summation from Anonymous Messages", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shuffled (aka anonymous) model has recently generated significant\ninterest as a candidate distributed privacy framework with trust assumptions\nbetter than the central model but with achievable errors smaller than the local\nmodel. We study pure differentially private (DP) protocols in the shuffled\nmodel for summation, a basic and widely used primitive:\n  - For binary summation where each of n users holds a bit as an input, we give\na pure $\\epsilon$-DP protocol for estimating the number of ones held by the\nusers up to an error of $O_\\epsilon(1)$, and each user sends $O_\\epsilon(\\log\nn)$ messages each of 1 bit. This is the first pure protocol in the shuffled\nmodel with error $o(\\sqrt{n})$ for constant $\\epsilon$.\n  Using this protocol, we give a pure $\\epsilon$-DP protocol that performs\nsummation of real numbers in $[0, 1]$ up to an error of $O_{\\epsilon}(1)$, and\nwhere each user sends $O_{\\epsilon}(\\log^3 n)$ messages each of $O(\\log\\log n)$\nbits.\n  - In contrast, we show that for any pure $\\epsilon$-DP protocol for binary\nsummation in the shuffled model having absolute error $n^{0.5-\\Omega(1)}$, the\nper user communication has to be at least $\\Omega_{\\epsilon}(\\sqrt{\\log n})$\nbits. This implies the first separation between the (bounded-communication)\nmulti-message shuffled model and the central model, and the first separation\nbetween pure and approximate DP protocols in the shuffled model.\n  To prove our lower bound, we consider (a generalization of) the following\nquestion: given $\\gamma$ in $(0, 1)$, what is the smallest m for which there\nare two random variables $X^0, X^1$ supported on $\\{0, \\dots ,m\\}$ such that\n(i) the total variation distance between $X^0$ and $X^1$ is at least\n$1-\\gamma$, and (ii) the moment generating functions of $X^0$ and $X^1$ are\nwithin a constant factor of each other everywhere? We show that the answer is\n$m = \\Theta(\\sqrt{\\log(1/\\gamma)})$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 18:53:21 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Ghazi", "Badih", ""], ["Golowich", "Noah", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""], ["Pagh", "Rasmus", ""], ["Velingker", "Ameya", ""]]}, {"id": "2002.02013", "submitter": "Benwei Shi", "authors": "Benwei Shi and Jeff M. Phillips", "title": "A Deterministic Streaming Sketch for Ridge Regression", "comments": "Fix a few typos. To be published in AISTATS 2021", "journal-ref": "Proceedings of The 24th International Conference on Artificial\n  Intelligence and Statistics, PMLR 130:586-594, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a deterministic space-efficient algorithm for estimating ridge\nregression. For $n$ data points with $d$ features and a large enough\nregularization parameter, we provide a solution within $\\varepsilon$ L$_2$\nerror using only $O(d/\\varepsilon)$ space. This is the first $o(d^2)$ space\ndeterministic streaming algorithm with guaranteed solution error and risk bound\nfor this classic problem. The algorithm sketches the covariance matrix by\nvariants of Frequent Directions, which implies it can operate in insertion-only\nstreams and a variety of distributed data settings. In comparisons to\nrandomized sketching algorithms on synthetic and real-world datasets, our\nalgorithm has less empirical error using less space and similar time.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 22:08:29 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 01:54:24 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 00:12:56 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 07:47:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Shi", "Benwei", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "2002.02232", "submitter": "Elizabeth Crosson", "authors": "Elizabeth Crosson, Samuel Slezak", "title": "Classical Simulation of High Temperature Quantum Ising Models", "comments": "7 pages, appendices to later be expanded", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider generalized quantum Ising models, including those which could\ndescribe disordered materials or quantum annealers, and we prove that for all\ntemperatures above a system-size independent threshold the path integral Monte\nCarlo method based on worldline heat-bath updates always mixes to stationarity\nin time $\\mathcal{O}(n \\log n)$ for an $n$ qubit system, and therefore provides\na fully polynomial-time approximation scheme for the partition function. This\nresult holds whenever the temperature is greater than four plus twice the\nmaximum interaction degree (valence) over all qubits, measured in units of the\nlocal coupling strength. For example, this implies that the classical\nsimulation of the thermal state of a superconducting device modeling a\nfrustrated quantum Ising model with maximum valence of 6 and coupling strengths\nof 1 GHz is always possible at temperatures above 800 mK. Despite the quantum\nsystem being at high temperature, the classical spin system resulting from the\nquantum-to-classical mapping contains strong couplings which cause the\nsingle-site Glauber dynamics to mix slowly, therefore this result depends on\nthe use of worldline updates (which are a form of cluster updates that can be\nimplemented efficiently). This result places definite constraints on the\ntemperatures required for a quantum advantage in analog quantum simulation with\nvarious NISQ devices based on equilibrium states of quantum Ising models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 12:56:42 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Crosson", "Elizabeth", ""], ["Slezak", "Samuel", ""]]}, {"id": "2002.02274", "submitter": "Sara Ahmadian", "authors": "Sara Ahmadian, Alessandro Epasto, Ravi Kumar, Mohammad Mahdian", "title": "Fair Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study correlation clustering under fairness constraints.\nFair variants of $k$-median and $k$-center clustering have been studied\nrecently, and approximation algorithms using a notion called fairlet\ndecomposition have been proposed. We obtain approximation algorithms for fair\ncorrelation clustering under several important types of fairness constraints.\n  Our results hinge on obtaining a fairlet decomposition for correlation\nclustering by introducing a novel combinatorial optimization problem. We define\na fairlet decomposition with cost similar to the $k$-median cost and this\nallows us to obtain approximation algorithms for a wide range of fairness\nconstraints.\n  We complement our theoretical results with an in-depth analysis of our\nalgorithms on real graphs where we show that fair solutions to correlation\nclustering can be obtained with limited increase in cost compared to the\nstate-of-the-art (unfair) algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 14:28:21 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 22:27:51 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Ahmadian", "Sara", ""], ["Epasto", "Alessandro", ""], ["Kumar", "Ravi", ""], ["Mahdian", "Mohammad", ""]]}, {"id": "2002.02304", "submitter": "Jan van den Brand", "authors": "Jan van den Brand, Yin Tat Lee, Aaron Sidford, Zhao Song", "title": "Solving Tall Dense Linear Programs in Nearly Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an $\\tilde{O}(nd+d^{3})$ time randomized algorithm\nfor solving linear programs with $d$ variables and $n$ constraints with high\nprobability. To obtain this result we provide a robust, primal-dual\n$\\tilde{O}(\\sqrt{d})$-iteration interior point method inspired by the methods\nof Lee and Sidford (2014, 2019) and show how to efficiently implement this\nmethod using new data-structures based on heavy-hitters, the\nJohnson-Lindenstrauss lemma, and inverse maintenance. Interestingly, we obtain\nthis running time without using fast matrix multiplication and consequently,\nbarring a major advance in linear system solving, our running time is near\noptimal for solving dense linear programs among algorithms that do not use fast\nmatrix multiplication.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 15:21:24 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Brand", "Jan van den", ""], ["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Song", "Zhao", ""]]}, {"id": "2002.02430", "submitter": "Rajan Udwani", "authors": "Vineet Goyal, Garud Iyengar, Rajan Udwani", "title": "Asymptotically Optimal Competitive Ratio for Online Allocation of\n  Reusable Resources", "comments": "Most recent version combines results from previous iteration\n  (arXiv:2002.02430v3) and the short note arXiv:2010.03983", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online allocation (matching, budgeted allocations,\nand assortments) of reusable resources where an adversarial sequence of\nresource requests is revealed over time and allocated resources are used/rented\nfor a stochastic duration, drawn independently from known resource usage\ndistributions. This problem is a fundamental generalization of well studied\nmodels in online matching and resource allocation. We give an algorithm that\nobtains the best possible competitive ratio of $(1-1/e)$ for general usage\ndistributions and large resource capacities.\n  At the heart of our algorithm is a new quantity that factors in the potential\nof reusability for each resource by (computationally) creating an asymmetry\nbetween identical units of the resource. In order to control the stochastic\ndependencies induced by reusability, we introduce a relaxed online algorithm\nthat is only subject to fluid approximations of the stochastic elements in the\nproblem. The output of this relaxed algorithm guides the overall algorithm.\nFinally, we establish competitive ratio guarantees by constructing a feasible\nsolution to an LP free system of constraints. More generally, these ideas lead\nto a principled approach for integrating stochastic and combinatorial elements\n(such as reusability, customer choice, and budgeted allocations) in online\nresource allocation problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:29:48 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 15:48:41 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 03:43:10 GMT"}, {"version": "v4", "created": "Sat, 13 Feb 2021 04:33:35 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Goyal", "Vineet", ""], ["Iyengar", "Garud", ""], ["Udwani", "Rajan", ""]]}, {"id": "2002.02476", "submitter": "Marco Mesiti", "authors": "Mario Pennacchioni, Emanuele Munarini, Marco Mesiti", "title": "The Sum Composition Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the \"sum composition problem\" between two lists $A$\nand $B$ of positive integers. We start by saying that $B$ is \"sum composition\"\nof $A$ when there exists an ordered $m$-partition $[A_1,\\ldots,A_m]$ of $A$\nwhere $m$ is the length of $B$ and the sum of each part $A_k$ is equal to the\ncorresponding part of $B$. Then, we consider the following two problems: $i)$\nthe \"exhaustive problem\", consisting in the generation of all partitions of $A$\nfor which $B$ is sum composition of $A$, and $ii)$ the \"existential problem\",\nconsisting in the verification of the existence of a partition of $A$ for which\n$B$ is sum composition of $A$. Starting from some general properties of the sum\ncompositions, we present a first algorithm solving the exhaustive problem and\nthen a second algorithm solving the existential problem. We also provide proofs\nof correctness and experimental analysis for assessing the quality of the\nproposed solutions along with a comparison with related works.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:17:49 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Pennacchioni", "Mario", ""], ["Munarini", "Emanuele", ""], ["Mesiti", "Marco", ""]]}, {"id": "2002.02487", "submitter": "Prathyush Sambaturu", "authors": "Prathyush Sambaturu, Aparna Gupta, Ian Davidson, S. S. Ravi, Anil\n  Vullikanti, Andrew Warren", "title": "Efficient Algorithms for Generating Provably Near-Optimal Cluster\n  Descriptors for Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the explainability of the results from machine learning methods has\nbecome an important research goal. Here, we study the problem of making\nclusters more interpretable by extending a recent approach of [Davidson et al.,\nNeurIPS 2018] for constructing succinct representations for clusters. Given a\nset of objects $S$, a partition $\\pi$ of $S$ (into clusters), and a universe\n$T$ of tags such that each element in $S$ is associated with a subset of tags,\nthe goal is to find a representative set of tags for each cluster such that\nthose sets are pairwise-disjoint and the total size of all the representatives\nis minimized. Since this problem is NP-hard in general, we develop\napproximation algorithms with provable performance guarantees for the problem.\nWe also show applications to explain clusters from datasets, including clusters\nof genomic sequences that represent different threat levels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:49:54 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Sambaturu", "Prathyush", ""], ["Gupta", "Aparna", ""], ["Davidson", "Ian", ""], ["Ravi", "S. S.", ""], ["Vullikanti", "Anil", ""], ["Warren", "Andrew", ""]]}, {"id": "2002.02641", "submitter": "Avery Miller", "authors": "Avery Miller, Andrzej Pelc, Ram Narayan Yadav", "title": "Deterministic Leader Election in Anonymous Radio Networks", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider leader election in anonymous radio networks modeled as simple\nundirected connected graphs. Nodes communicate in synchronous rounds. Nodes are\nanonymous and execute the same deterministic algorithm, so symmetry can be\nbroken only in one way: by different wake-up times of the nodes. In which\nsituations is it possible to break symmetry and elect a leader using time as\nsymmetry breaker? To answer this question, we consider configurations. A\nconfiguration is the underlying graph with nodes tagged by non-negative\nintegers with the following meaning. A node can either wake up spontaneously in\nthe round shown on its tag, according to some global clock, or can be woken up\nhearing a message sent by one of its already awoken neighbours. The local clock\nof a node starts at its wakeup and nodes do not have access to the global clock\ndetermining their tags. A configuration is feasible if there exists a\ndistributed algorithm that elects a leader for this configuration.\n  Our main result is a complete algorithmic characterization of feasible\nconfigurations: we design a centralized decision algorithm, working in\npolynomial time, whose input is a configuration and which decides if the\nconfiguration is feasible. We also provide a dedicated deterministic\ndistributed leader election algorithm for each feasible configuration that\nelects a leader for this configuration in time $O(n^2\\sigma)$, where $n$ is the\nnumber of nodes and $\\sigma$ is the difference between the largest and smallest\ntag of the configuration. We then prove that there cannot exist a universal\ndeterministic distributed algorithm electing a leader for all feasible\nconfigurations. In fact, we show that such a universal algorithm cannot exist\neven for the class of 4-node feasible configurations. We also prove that a\ndistributed version of our decision algorithm cannot exist.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:43:30 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Miller", "Avery", ""], ["Pelc", "Andrzej", ""], ["Yadav", "Ram Narayan", ""]]}, {"id": "2002.02731", "submitter": "Jeffrey Shallit", "authors": "Trevor Clokie, Thomas F. Lidbetter, Antonio Molina Lovett, Jeffrey\n  Shallit, Leon Witzman", "title": "Computational Aspects of Sturdy and Flimsy Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.FL math.CO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following Stolarsky, we say that a natural number n is flimsy in base b if\nsome positive multiple of n has smaller digit sum in base b than n does;\notherwise it is sturdy. We develop algorithmic methods for the study of sturdy\nand flimsy numbers.\n  We provide some criteria for determining whether a number is sturdy. Focusing\non the case of base b = 2, we study the computational problem of checking\nwhether a given number is sturdy, giving several algorithms for the problem. We\nfind two additional, previously unknown sturdy primes. We develop a method for\ndetermining which numbers with a fixed number of 0's in binary are flimsy.\nFinally, we develop a method that allows us to estimate the number of k-flimsy\nnumbers with n bits, and we provide explicit results for k = 3 and k = 5. Our\nresults demonstrate the utility (and fun) of creating algorithms for number\ntheory problems, based on methods of automata theory.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 12:17:41 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Clokie", "Trevor", ""], ["Lidbetter", "Thomas F.", ""], ["Lovett", "Antonio Molina", ""], ["Shallit", "Jeffrey", ""], ["Witzman", "Leon", ""]]}, {"id": "2002.02809", "submitter": "Steven Finch", "authors": "Steven Finch", "title": "Recursive PGFs for BSTs and DSTs", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.HO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review fundamentals underlying binary search trees and digital search\ntrees, with (atypical) emphasis on recursive formulas for associated\nprobability generating functions. Other topics include higher moments of BST\nsearch costs and combinatorics for a certain finite-key analog of DSTs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:33:07 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Finch", "Steven", ""]]}, {"id": "2002.02962", "submitter": "Daniel Seemaier", "authors": "Merten Popp, Sebastian Schlag, Christian Schulz, Daniel Seemaier", "title": "Multilevel Acyclic Hypergraph Partitioning", "comments": "arXiv admin note: text overlap with arXiv:1710.01968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed acyclic hypergraph is a generalized concept of a directed acyclic\ngraph, where each hyperedge can contain an arbitrary number of tails and heads.\nDirected hypergraphs can be used to model data flow and execution dependencies\nin streaming applications. Thus, hypergraph partitioning algorithms can be used\nto obtain efficient parallelizations for multiprocessor architectures. However,\nan acyclicity constraint on the partition is necessary when mapping streaming\napplications to embedded multiprocessors due to resource restrictions on this\ntype of hardware. The acyclic hypergraph partitioning problem is to partition\nthe hypernodes of a directed acyclic hypergraph into a given number of blocks\nof roughly equal size such that the corresponding quotient graph is acyclic\nwhile minimizing an objective function on the partition.\n  Here, we contribute the first n-level algorithm for the acyclic hypergraph\npartitioning problem. Our focus is on acyclic hypergraphs where hyperedges can\nhave one head and arbitrary many tails. Based on this, we engineer a memetic\nalgorithm to further reduce communication cost, as well as to improve\nscheduling makespan on embedded multiprocessor architectures. Experiments\nindicate that our algorithm outperforms previous algorithms that focus on the\ndirected acyclic graph case which have previously been employed in the\napplication domain. Moreover, our experiments indicate that using the directed\nhypergraph model for this type of application yields a significantly smaller\nmakespan.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:03:58 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 11:36:31 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Popp", "Merten", ""], ["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""], ["Seemaier", "Daniel", ""]]}, {"id": "2002.03004", "submitter": "Morris Yau", "authors": "Prasad Raghavendra, Morris Yau", "title": "List Decodable Subspace Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from data in the presence of outliers is a fundamental problem in\nstatistics. In this work, we study robust statistics in the presence of\noverwhelming outliers for the fundamental problem of subspace recovery. Given a\ndataset where an $\\alpha$ fraction (less than half) of the data is distributed\nuniformly in an unknown $k$ dimensional subspace in $d$ dimensions, and with no\nadditional assumptions on the remaining data, the goal is to recover a succinct\nlist of $O(\\frac{1}{\\alpha})$ subspaces one of which is nontrivially correlated\nwith the planted subspace. We provide the first polynomial time algorithm for\nthe 'list decodable subspace recovery' problem, and subsume it under a more\ngeneral framework of list decoding over distributions that are \"certifiably\nresilient\" capturing state of the art results for list decodable mean\nestimation and regression.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 20:18:33 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Yau", "Morris", ""]]}, {"id": "2002.03057", "submitter": "Lum Ramabaja", "authors": "Lum Ramabaja, Arber Avdullahu", "title": "The Bloom Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data structure that allows for efficient (probabilistic)\npresence proofs and non-probabilistic absence proofs in a bandwidth efficient\nand secure way. The Bloom tree combines the idea of Bloom filters with that of\nMerkle trees. Bloom filters are used to verify the presence, or absence of\nelements in a set. In the case of the Bloom tree, we are interested to verify\nand transmit the presence, or absence of an element in a secure and bandwidth\nefficient way to another party. Instead of sending the whole Bloom filter to\ncheck for the presence, or absence of an element, the Bloom tree achieves\nefficient verification by using a compact Merkle multiproof.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 00:54:19 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 12:58:19 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 11:50:25 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Ramabaja", "Lum", ""], ["Avdullahu", "Arber", ""]]}, {"id": "2002.03175", "submitter": "Matteo Ceccarello", "authors": "Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci", "title": "A General Coreset-Based Approach to Diversity Maximization under Matroid\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity maximization is a fundamental problem in web search and data\nmining. For a given dataset $S$ of $n$ elements, the problem requires to\ndetermine a subset of $S$ containing $k\\ll n$ \"representatives\" which minimize\nsome diversity function expressed in terms of pairwise distances, where\ndistance models dissimilarity. An important variant of the problem prescribes\nthat the solution satisfy an additional orthogonal requirement, which can be\nspecified as a matroid constraint (i.e., a feasible solution must be an\nindependent set of size $k$ of a given matroid). While unconstrained diversity\nmaximization admits efficient coreset-based strategies for several diversity\nfunctions, known approaches dealing with the additional matroid constraint\napply only to one diversity function (sum of distances), and are based on an\nexpensive, inherently sequential, local search over the entire input dataset.\nWe devise the first coreset-based algorithms for diversity maximization under\nmatroid constraints for various diversity functions, together with efficient\nsequential, MapReduce and Streaming implementations. Technically, our\nalgorithms rely on the construction of a small coreset, that is, a subset of\n$S$ containing a feasible solution which is no more than a factor $1-\\epsilon$\naway from the optimal solution for $S$. While our algorithms are fully general,\nfor the partition and transversal matroids, if $\\epsilon$ is a constant in\n$(0,1)$ and $S$ has bounded doubling dimension, the coreset size is independent\nof $n$ and it is small enough to afford the execution of a slow sequential\nalgorithm to extract a final, accurate, solution in reasonable time. Extensive\nexperiments show that our algorithms are accurate, fast and scalable, and\ntherefore they are capable of dealing with the large input instances typical of\nthe big data scenario.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 14:46:40 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""]]}, {"id": "2002.03213", "submitter": "Marco Molinaro", "authors": "Marco Molinaro", "title": "Curvature of Feasible Sets in Offline and Online Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the curvature of the feasible set in convex optimization\nallows for algorithms with better convergence rates, and there has been renewed\ninterest in this topic both for offline as well as online problems. In this\npaper, leveraging results on geometry and convex analysis, we further our\nunderstanding of the role of curvature in optimization:\n  - We first show the equivalence of two notions of curvature, namely strong\nconvexity and gauge bodies, proving a conjecture of Abernethy et al. As a\nconsequence, this show that the Frank-Wolfe-type method of Wang and Abernethy\nhas accelerated convergence rate $O(\\frac{1}{t^2})$ over strongly convex\nfeasible sets without additional assumptions on the (convex) objective\nfunction.\n  - In Online Linear Optimization, we identify two main properties that help\nexplaining \\emph{why/when} Follow the Leader (FTL) has only logarithmic regret\nover strongly convex sets. This allows one to directly recover a recent result\nof Huang et al., and to show that FTL has logarithmic regret over strongly\nconvex sets whenever the gain vectors are non-negative.\n  - We provide an efficient procedure for approximating convex bodies by\nstrongly convex ones while smoothly trading off approximation error and\ncurvature. This allows one to extend the improved algorithms over strongly\nconvex sets to general convex sets. As a concrete application, we extend the\nresults of Dekel et al. on Online Linear Optimization with Hints to general\nconvex sets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 18:10:20 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 02:05:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Molinaro", "Marco", ""]]}, {"id": "2002.03293", "submitter": "Khiem Pham", "authors": "Khiem Pham and Khang Le and Nhat Ho and Tung Pham and Hung Bui", "title": "On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computational complexity analysis for the Sinkhorn algorithm\nthat solves the entropic regularized Unbalanced Optimal Transport (UOT) problem\nbetween two measures of possibly different masses with at most $n$ components.\nWe show that the complexity of the Sinkhorn algorithm for finding an\n$\\varepsilon$-approximate solution to the UOT problem is of order\n$\\widetilde{\\mathcal{O}}(n^2/ \\varepsilon)$, which is near-linear time. To the\nbest of our knowledge, this complexity is better than the complexity of the\nSinkhorn algorithm for solving the Optimal Transport (OT) problem, which is of\norder $\\widetilde{\\mathcal{O}}(n^2/\\varepsilon^2)$. Our proof technique is\nbased on the geometric convergence of the Sinkhorn updates to the optimal dual\nsolution of the entropic regularized UOT problem and some properties of the\nprimal solution. It is also different from the proof for the complexity of the\nSinkhorn algorithm for approximating the OT problem since the UOT solution does\nnot have to meet the marginal constraints.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 06:03:36 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 04:02:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Pham", "Khiem", ""], ["Le", "Khang", ""], ["Ho", "Nhat", ""], ["Pham", "Tung", ""], ["Bui", "Hung", ""]]}, {"id": "2002.03352", "submitter": "Moran Feldman", "authors": "Ran Haba, Ehsan Kazemi, Moran Feldman and Amin Karbasi", "title": "Streaming Submodular Maximization under a $k$-Set System Constraint", "comments": "28 pages; 8 figures. This paper subsumes arXiv:1906.04449, which was\n  previously posted on arXiv and considered only the case of linear objective\n  functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework that converts streaming\nalgorithms for monotone submodular maximization into streaming algorithms for\nnon-monotone submodular maximization. This reduction readily leads to the\ncurrently tightest deterministic approximation ratio for submodular\nmaximization subject to a $k$-matchoid constraint. Moreover, we propose the\nfirst streaming algorithm for monotone submodular maximization subject to\n$k$-extendible and $k$-set system constraints. Together with our proposed\nreduction, we obtain $O(k\\log k)$ and $O(k^2\\log k)$ approximation ratio for\nsubmodular maximization subject to the above constraints, respectively. We\nextensively evaluate the empirical performance of our algorithm against the\nexisting work in a series of experiments including finding the maximum\nindependent set in randomly generated graphs, maximizing linear functions over\nsocial networks, movie recommendation, Yelp location summarization, and Twitter\ndata summarization.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 12:32:14 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Haba", "Ran", ""], ["Kazemi", "Ehsan", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "2002.03415", "submitter": "Arsen Vasilyan", "authors": "Ronitt Rubinfeld, Arsen Vasilyan", "title": "Monotone probability distributions over the Boolean cube can be learned\n  with sublinear samples", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2020.28", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probability distribution over the Boolean cube is monotone if flipping the\nvalue of a coordinate from zero to one can only increase the probability of an\nelement. Given samples of an unknown monotone distribution over the Boolean\ncube, we give (to our knowledge) the first algorithm that learns an\napproximation of the distribution in statistical distance using a number of\nsamples that is sublinear in the domain.\n  To do this, we develop a structural lemma describing monotone probability\ndistributions. The structural lemma has further implications to the sample\ncomplexity of basic testing tasks for analyzing monotone probability\ndistributions over the Boolean cube: We use it to give nontrivial upper bounds\non the tasks of estimating the distance of a monotone distribution to uniform\nand of estimating the support size of a monotone distribution. In the setting\nof monotone probability distributions over the Boolean cube, our algorithms are\nthe first to have sample complexity lower than known lower bounds for the same\ntesting tasks on arbitrary (not necessarily monotone) probability\ndistributions.\n  One further consequence of our learning algorithm is an improved sample\ncomplexity for the task of testing whether a distribution on the Boolean cube\nis monotone.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 18:17:08 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Rubinfeld", "Ronitt", ""], ["Vasilyan", "Arsen", ""]]}, {"id": "2002.03443", "submitter": "Michal Wlodarczyk", "authors": "Bart M.P. Jansen and Micha{\\l} W{\\l}odarczyk", "title": "Optimal polynomial-time compression for Boolean Max CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Boolean maximum constraint satisfaction problem - Max CSP$(\\Gamma)$ -\none is given a collection of weighted applications of constraints from a finite\nconstraint language $\\Gamma$, over a common set of variables, and the goal is\nto assign Boolean values to the variables so that the total weight of satisfied\nconstraints is maximized. There exists an elegant dichotomy theorem providing a\ncriterion on $\\Gamma$ for the problem to be polynomial-time solvable and\nstating that otherwise it becomes NP-hard. We study the NP hard cases through\nthe lens of kernelization and provide a complete characterization of Max\nCSP$(\\Gamma)$ with respect to the optimal compression size. Namely, we prove\nthat Max CSP$(\\Gamma)$ parameterized by the number of variables $n$ is either\npolynomial-time solvable, or there exists an integer $d \\ge 2$ depending on\n$\\Gamma$, such that\n  1. An instance of \\textsc{Max CSP$(\\Gamma)$} can be compressed into an\nequivalent instance with $O(n^d\\log n)$ bits in polynomial time,\n  2. Max CSP$(\\Gamma)$ does not admit such a compression to $O(n^{d-\\epsilon})$\nbits unless $\\text{NP} \\subseteq \\text{co-NP} / \\text{poly}$.\n  Our reductions are based on interpreting constraints as multilinear\npolynomials combined with the framework of constraint implementations. As\nanother application of our reductions, we reveal tight connections between\noptimal running times for solving Max CSP$(\\Gamma)$. More precisely, we show\nthat obtaining a running time of the form $O(2^{(1-\\epsilon)n})$ for particular\nclasses of Max CSPs is as hard as breaching this barrier for Max $d$-SAT for\nsome $d$.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 20:35:56 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "2002.03459", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Przemys{\\l}aw Uzna\\'nski", "title": "Approximating Text-to-Pattern Distance via Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-pattern distance is a fundamental problem in string matching, where\ngiven a pattern of length $m$ and a text of length $n$, over an integer\nalphabet, we are asked to compute the distance between pattern and the text at\nevery location. The distance function can be e.g. Hamming distance or $\\ell_p$\ndistance for some parameter $p > 0$. Almost all state-of-the-art exact and\napproximate algorithms developed in the past $\\sim 40$ years were using FFT as\na black-box. In this work we present $\\widetilde{O}(n/\\varepsilon^2)$ time\nalgorithms for $(1\\pm\\varepsilon)$-approximation of $\\ell_2$ distances, and\n$\\widetilde{O}(n/\\varepsilon^3)$ algorithm for approximation of Hamming and\n$\\ell_1$ distances, all without use of FFT. This is independent to the very\nrecent development by Chan et al. [STOC 2020], where $O(n/\\varepsilon^2)$\nalgorithm for Hamming distances not using FFT was presented -- although their\nalgorithm is much more \"combinatorial\", our techniques apply to other norms\nthan Hamming.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 21:58:36 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 09:00:00 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2002.03508", "submitter": "Sainyam Galhotra Mr", "authors": "Saba Ahmadi, Sainyam Galhotra, Barna Saha, Roy Schwartz", "title": "Fair Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we study the problem of correlation clustering under fairness\nconstraints. In the classic correlation clustering problem, we are given a\ncomplete graph where each edge is labeled positive or negative. The goal is to\nobtain a clustering of the vertices that minimizes disagreements -- the number\nof negative edges trapped inside a cluster plus positive edges between\ndifferent clusters.\n  We consider two variations of fairness constraint for the problem of\ncorrelation clustering where each node has a color, and the goal is to form\nclusters that do not over-represent vertices of any color.\n  The first variant aims to generate clusters with minimum disagreements, where\nthe distribution of a feature (e.g. gender) in each cluster is same as the\nglobal distribution. For the case of two colors when the desired ratio of the\nnumber of colors in each cluster is $1:p$, we get\n$\\mathcal{O}(p^2)$-approximation algorithm. Our algorithm could be extended to\nthe case of multiple colors. We prove this problem is NP-hard.\n  The second variant considers relative upper and lower bounds on the number of\nnodes of any color in a cluster. The goal is to avoid violating upper and lower\nbounds corresponding to each color in each cluster while minimizing the total\nnumber of disagreements. Along with our theoretical results, we show the\neffectiveness of our algorithm to generate fair clusters by empirical\nevaluation on real world data sets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 02:59:17 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ahmadi", "Saba", ""], ["Galhotra", "Sainyam", ""], ["Saha", "Barna", ""], ["Schwartz", "Roy", ""]]}, {"id": "2002.03580", "submitter": "Haoyu Zhao", "authors": "Wei Chen, Liwei Wang, Haoyu Zhao, Kai Zheng", "title": "Combinatorial Semi-Bandit in the Non-Stationary Environment", "comments": "Accepted to UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the non-stationary combinatorial semi-bandit\nproblem, both in the switching case and in the dynamic case. In the general\ncase where (a) the reward function is non-linear, (b) arms may be\nprobabilistically triggered, and (c) only approximate offline oracle exists\n\\cite{wang2017improving}, our algorithm achieves\n$\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{S} T})$ distribution-dependent regret in\nthe switching case, and $\\tilde{\\mathcal{O}}(\\mathcal{V}^{1/3}T^{2/3})$ in the\ndynamic case, where $\\mathcal S$ is the number of switchings and $\\mathcal V$\nis the sum of the total ``distribution changes''. The regret bounds in both\nscenarios are nearly optimal, but our algorithm needs to know the parameter\n$\\mathcal S$ or $\\mathcal V$ in advance.\n  We further show that by employing another technique, our algorithm no longer\nneeds to know the parameters $\\mathcal S$ or $\\mathcal V$ but the regret bounds\ncould become suboptimal.\n  In a special case where the reward function is linear and we have an exact\noracle, we design a parameter-free algorithm that achieves nearly optimal\nregret both in the switching case and in the dynamic case without knowing the\nparameters in advance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 07:10:56 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 06:27:35 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Wei", ""], ["Wang", "Liwei", ""], ["Zhao", "Haoyu", ""], ["Zheng", "Kai", ""]]}, {"id": "2002.03583", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Radek Hu\\v{s}ek, Du\\v{s}an Knop, Tom\\'a\\v{s} Masa\\v{r}\\'ik", "title": "Approximation Algorithms for Steiner Tree Based on Star Contractions: A\n  Unified View", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.IPEC.2020.16", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Steiner Tree problem we are given an edge weighted undirected graph $G\n= (V,E)$ and a set of terminals $R \\subseteq V$. The task is to find a\nconnected subgraph of $G$ containing $R$ and minimizing the sum of weights of\nits edges. We observe that many approximation algorithms for Steiner Tree\nfollow a similar scheme (meta-algorithm) and perform (exhaustively) a similar\nroutine which we call star contraction. Here, by a star contraction, we mean\nfinding a star-like subgraph in the input graph minimizing the ratio of its\nweight to the number of contained terminals minus one. It is not hard to see\nthat the well-known MST-approximation seeks the best star to contract among\nthose containing two terminals only. We perform an empirical study of star\ncontractions with the relaxed condition on the number of terminals in each star\ncontraction. Our experiments suggest the following: -- if the algorithm\nperforms star contractions exhaustively, the quality of the solution is usually\nslightly better than Zelikovsky's 11/6-approximation algorithm, -- on average\nthe quality of the solution returned by the MST-approximation algorithm\nimproves with every star contraction, -- the same holds for iterated MST\n(MST+), which outperforms MST in every measurement while keeping very fast\nrunning time (on average $\\sim 3\\times$ slower than MST), -- on average the\nquality of the solution obtained by exhaustively performing star contraction is\nabout 16\\% better than the initial MST-approximation, and -- we propose a more\nprecise way to find the so-called improved stars which yield a slightly better\nsolution within a comparable running time (on average $\\sim 3\\times$ slower).\nFurthermore, we propose two improvements of Zelikovsky's 11/6-approximation\nalgorithm and we empirically confirm that the quality of the solution returned\nby any of these is better than the one returned by the former algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 07:47:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hu\u0161ek", "Radek", ""], ["Knop", "Du\u0161an", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""]]}, {"id": "2002.03816", "submitter": "Sanjeev Saxena", "authors": "Akshay Singh and Sanjeev Saxena", "title": "Edge colouring Game on Trees with maximum degree $\\Delta=4$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following game. We are given a tree $T$ and two players (say)\nAlice and Bob who alternately colour an edge of a tree (using one of $k$\ncolours). If all edges of the tree get coloured, then Alice wins else Bob wins.\nGame chromatic index of trees of is the smallest index $k$ for which there is a\nwinning strategy for Alice. If the maximum degree of a node in tree is\n$\\Delta$, Erdos et.al.[6], show that the game chromatic index is at least\n$\\Delta+1$. The bound is known to be tight for all values of $\\Delta\\neq 4$.\n  In this paper we show that for $\\Delta=4$, even if Bob is allowed to skip a\nmove, Alice can always choose an edge to colour and win the game for\n$k=\\Delta+1$. Thus the game chromatic index of trees of maximum degree $4$ is\nalso $5$. Hence, game chromatic index of trees of maximum degree $\\Delta$ is\n$\\Delta+1$ for all $\\Delta\\geq 2$.\n  Moreover,the tree can be preprocessed to allow Alice to pick the next edge to\ncolour in $O(1)$ time.\n  A result of independent interest is a linear time algorithm for on-line\nedge-deletion problem on trees.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:43:26 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Singh", "Akshay", ""], ["Saxena", "Sanjeev", ""]]}, {"id": "2002.03965", "submitter": "Mikhail Rubinchik", "authors": "Mikhail Rubinchik and Arseny M. Shur", "title": "Palindromic k-Factorization in Pure Linear Time", "comments": "accepted to MFCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a string $s$ of length $n$ over a general alphabet and an integer $k$,\nthe problem is to decide whether $s$ is a concatenation of $k$ nonempty\npalindromes. Two previously known solutions for this problem work in time\n$O(kn)$ and $O(n\\log n)$ respectively. Here we settle the complexity of this\nproblem in the word-RAM model, presenting an $O(n)$-time online deciding\nalgorithm. The algorithm simultaneously finds the minimum odd number of factors\nand the minimum even number of factors in a factorization of a string into\nnonempty palindromes. We also demonstrate how to get an explicit factorization\nof $s$ into $k$ palindromes with an $O(n)$-time offline postprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:27:34 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 17:57:08 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 12:26:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Rubinchik", "Mikhail", ""], ["Shur", "Arseny M.", ""]]}, {"id": "2002.04002", "submitter": "Ralf M\\\"uller", "authors": "Ralf R. M\\\"uller, Bernhard G\\\"ade, Ali Bereyhi", "title": "Efficient Matrix Multiplication: The Sparse Power-of-2 Factorization", "comments": "Submitted to the IEEE Statistical Signal Processing Workshop 2020,\n  errors in previous version corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to reduce the computational effort for the\nmultiplication of a given matrix with an unknown column vector. The algorithm\ndecomposes the given matrix into a product of matrices whose entries are either\nzero or integer powers of two utilizing the principles of sparse recovery.\nWhile classical low resolution quantization achieves an accuracy of 6 dB per\nbit, our method can achieve many times more than that for large matrices.\nNumerical evidence suggests that the improvement actually grows unboundedly\nwith matrix size. Due to sparsity, the algorithm even allows for quantization\nlevels below 1 bit per matrix entry while achieving highly accurate\napproximations for large matrices. Applications include, but are not limited\nto, neural networks, as well as fully digital beam-forming for massive MIMO and\nmillimeter wave applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 18:16:24 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 18:24:11 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["M\u00fcller", "Ralf R.", ""], ["G\u00e4de", "Bernhard", ""], ["Bereyhi", "Ali", ""]]}, {"id": "2002.04048", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "$2$-node-connectivity network design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider network design problems in which we are given a graph $G=(V,E)$\nwith edge costs, and seek a min-cost/size $2$-node-connected subgraph\n$G'=(V',E')$ that satisfies a prescribed property. In the Block-Tree\nAugmentation problem the goal is to augment a tree $T$ by a min-size edge set\n$F \\subseteq E$ such that $G'=T \\cup F$ is $2$-node-connected. We break the\nnatural ratio of $2$ for this problem and show that it admits approximation\nratio $1.91$. This result extends to the related Crossing Family Augmentation\nproblem. In the $2$-Connected Dominating Set problem $G'$ should dominate $V$.\nWe give the first non-trivial approximation algorithm for this problem, with\nexpected ratio $\\tilde{O}(\\log^4 |V|)$. In the $2$-Connected Quota Subgraph\nproblem we are given node profits $p(v)$ and $G'$ should have profit at least a\ngiven quota $Q$. We show expected ratio $\\tilde{O}(\\log^2|V|)$, almost matching\nthe best known ratio $O(\\log^2|V|)$.\n  Our algorithms are very simple, and they combine three main ingredients. 1. A\nprobabilistic spanning tree embedding with distortion $\\tilde{O}(\\log |V|)$\nresults in a variant of the Block-Tree Augmentation problem. 2. An\napproximation ratio preserving reduction of Block-Tree Augmentation variants to\nNode Weighted Steiner Tree problems. 3. Using existing approximation algorithms\nfor variants of the Node Weighted Steiner Tree problem.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 19:02:58 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 09:41:32 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "2002.04063", "submitter": "Alfredo Torrico", "authors": "Alfredo Torrico, Mohit Singh, Sebastian Pokutta", "title": "On the Unreasonable Effectiveness of the Greedy Algorithm: Greedy Adapts\n  to Sharpness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization has been widely studied over the past decades, mostly\nbecause of its numerous applications in real-world problems. It is well known\nthat the standard greedy algorithm guarantees a worst-case approximation factor\nof 1-1/e when maximizing a monotone submodular function under a cardinality\nconstraint. However, empirical studies show that its performance is\nsubstantially better in practice. This raises a natural question of explaining\nthis improved performance of the greedy algorithm. In this work, we define\nsharpness for submodular functions as a candidate explanation for this\nphenomenon. The sharpness criterion is inspired by the concept of strong\nconvexity in convex optimization. We show that the greedy algorithm provably\nperforms better as the sharpness of the submodular function increases. This\nimprovement ties in closely with the faster convergence rates of first order\nmethods for sharp functions in convex optimization. Finally, we perform a\ncomputational study to empirically support our theoretical results and show\nthat sharpness explains the greedy performance better than other justifications\nin the literature.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 19:54:40 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Torrico", "Alfredo", ""], ["Singh", "Mohit", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2002.04121", "submitter": "Kevin Tian", "authors": "Yin Tat Lee, Ruoqi Shen, Kevin Tian", "title": "Logsmooth Gradient Concentration and Tighter Runtimes for Metropolized\n  Hamiltonian Monte Carlo", "comments": "31 pages. v2 propagates changes from COLT 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the gradient norm $\\|\\nabla f(x)\\|$ for $x \\sim \\exp(-f(x))$,\nwhere $f$ is strongly convex and smooth, concentrates tightly around its mean.\nThis removes a barrier in the prior state-of-the-art analysis for the\nwell-studied Metropolized Hamiltonian Monte Carlo (HMC) algorithm for sampling\nfrom a strongly logconcave distribution. We correspondingly demonstrate that\nMetropolized HMC mixes in $\\tilde{O}(\\kappa d)$ iterations, improving upon the\n$\\tilde{O}(\\kappa^{1.5}\\sqrt{d} + \\kappa d)$ runtime of (Dwivedi et. al. '18,\nChen et. al. '19) by a factor $(\\kappa/d)^{1/2}$ when the condition number\n$\\kappa$ is large. Our mixing time analysis introduces several techniques which\nto our knowledge have not appeared in the literature and may be of independent\ninterest, including restrictions to a nonconvex set with good conductance\nbehavior, and a new reduction technique for boosting a constant-accuracy total\nvariation guarantee under weak warmness assumptions. This is the first\nhigh-accuracy mixing time result for logconcave distributions using only\nfirst-order function information which achieves linear dependence on $\\kappa$;\nwe also give evidence that this dependence is likely to be necessary for\nstandard Metropolized first-order methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 22:44:50 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 05:24:03 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2020 02:12:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lee", "Yin Tat", ""], ["Shen", "Ruoqi", ""], ["Tian", "Kevin", ""]]}, {"id": "2002.04149", "submitter": "Chenyang Yuan", "authors": "Chenyang Yuan, Pablo A. Parrilo", "title": "Maximizing Products of Linear Forms, and The Permanent of Positive\n  Semidefinite Matrices", "comments": "12 pages, 2 figures", "journal-ref": "Math. Program. (2021)", "doi": "10.1007/s10107-021-01616-3", "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convex relaxation of a polynomial optimization problem,\nmaximizing a product of linear forms over the complex sphere. We show that this\nconvex program is also a relaxation of the permanent of Hermitian positive\nsemidefinite (HPSD) matrices. By analyzing a constructive randomized rounding\nalgorithm, we obtain an improved multiplicative approximation factor to the\npermanent of HPSD matrices, as well as computationally efficient certificates\nfor this approximation. We also propose an analog of van der Waerden's\nconjecture for HPSD matrices, where the polynomial optimization problem is\ninterpreted as a relaxation of the permanent.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 00:45:04 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 20:28:42 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yuan", "Chenyang", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "2002.04232", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Ashwin Maran,\n  N. V. Vinodchandran", "title": "Learning and Sampling of Atomic Interventions from Observations", "comments": "26 pages, 4 figures, a version appeared in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently estimating the effect of an intervention\non a single variable (atomic interventions) using observational samples in a\ncausal Bayesian network. Our goal is to give algorithms that are efficient in\nboth time and sample complexity in a non-parametric setting.\n  Tian and Pearl (AAAI `02) have exactly characterized the class of causal\ngraphs for which causal effects of atomic interventions can be identified from\nobservational data. We make their result quantitative. Suppose P is a causal\nmodel on a set $\\vec{V}$ of n observable variables with respect to a given\ncausal graph G with observable distribution $P$. Let $P_x$ denote the\ninterventional distribution over the observables with respect to an\nintervention of a designated variable X with x. Assuming that $G$ has bounded\nin-degree, bounded c-components ($k$), and that the observational distribution\nis identifiable and satisfies certain strong positivity condition, we give an\nalgorithm that takes $m=\\tilde{O}(n\\epsilon^{-2})$ samples from $P$ and $O(mn)$\ntime, and outputs with high probability a description of a distribution\n$\\hat{P}$ such that $d_{\\mathrm{TV}}(P_x, \\hat{P}) \\leq \\epsilon$, and:\n  1. [Evaluation] the description can return in $O(n)$ time the probability\n$\\hat{P}(\\vec{v})$ for any assignment $\\vec{v}$ to $\\vec{V}$\n  2. [Generation] the description can return an iid sample from $\\hat{P}$ in\n$O(n)$ time.\n  We also show lower bounds for the sample complexity showing that our sample\ncomplexity has an optimal dependence on the parameters $n$ and $\\epsilon$, as\nwell as if $k=1$ on the strong positivity parameter.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 07:15:32 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 06:11:17 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gayen", "Sutanu", ""], ["Kandasamy", "Saravanan", ""], ["Maran", "Ashwin", ""], ["Vinodchandran", "N. V.", ""]]}, {"id": "2002.04338", "submitter": "Shao-Heng Ko", "authors": "Shao-Heng Ko, Hsu-Chao Lai, Hong-Han Shuai, De-Nian Yang, Wang-Chien\n  Lee, Philip S. Yu", "title": "Optimizing Item and Subgroup Configurations for Social-Aware VR Shopping", "comments": "32 pages, 16 figures (41 subfigures). A shorter version of this paper\n  has been submitted to the 46th International Conference on Very Large\n  Databases (VLDB 2020); this is an expanded version containing supplementary\n  details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shopping in VR malls has been regarded as a paradigm shift for E-commerce,\nbut most of the conventional VR shopping platforms are designed for a single\nuser. In this paper, we envisage a scenario of VR group shopping, which brings\nmajor advantages over conventional group shopping in brick-and-mortar stores\nand Web shopping: 1) configure flexible display of items and partitioning of\nsubgroups to address individual interests in the group, and 2) support social\ninteractions in the subgroups to boost sales. Accordingly, we formulate the\nSocial-aware VR Group-Item Configuration (SVGIC) problem to configure a set of\ndisplayed items for flexibly partitioned subgroups of users in VR group\nshopping. We prove SVGIC is NP-hard to approximate within $\\frac{32}{31} -\n\\epsilon$. We design an approximation algorithm based on the idea of Co-display\nSubgroup Formation (CSF) to configure proper items for display to different\nsubgroups of friends. Experimental results on real VR datasets and a user study\nwith hTC VIVE manifest that our algorithms outperform baseline approaches by at\nleast 30.1% of solution quality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 12:12:36 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 00:40:25 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ko", "Shao-Heng", ""], ["Lai", "Hsu-Chao", ""], ["Shuai", "Hong-Han", ""], ["Yang", "De-Nian", ""], ["Lee", "Wang-Chien", ""], ["Yu", "Philip S.", ""]]}, {"id": "2002.04368", "submitter": "Karol W\\k{e}grzycki", "authors": "Jesper Nederlof, Micha{\\l} Pilipczuk, C\\'eline M. F. Swennenhuis,\n  Karol W\\k{e}grzycki", "title": "Hamiltonian Cycle Parameterized by Treedepth in Single Exponential Time\n  and Polynomial Space", "comments": "Presented at WG2020. 20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many algorithmic problems on graphs of treewidth $t$, a standard dynamic\nprogramming approach gives an algorithm with time and space complexity\n$2^{\\mathcal{O}(t)}\\cdot n^{\\mathcal{O}(1)}$. It turns out that when one\nconsiders the more restrictive parameter treedepth, it is often the case that a\nvariation of this technique can be used to reduce the space complexity to\npolynomial, while retaining time complexity of the form\n$2^{\\mathcal{O}(d)}\\cdot n^{\\mathcal{O}(1)}$, where $d$ is the treedepth. This\ntransfer of methodology is, however, far from automatic. For instance, for\nproblems with connectivity constraints, standard dynamic programming techniques\ngive algorithms with time and space complexity $2^{\\mathcal{O}(t\\log t)}\\cdot\nn^{\\mathcal{O}(1)}$ on graphs of treewidth $t$, but it is not clear how to\nconvert them into time-efficient polynomial space algorithms for graphs of low\ntreedepth.\n  Cygan et al. (FOCS'11) introduced the Cut&Count technique and showed that a\ncertain class of problems with connectivity constraints can be solved in time\nand space complexity $2^{\\mathcal{O}(t)}\\cdot n^{\\mathcal{O}(1)}$. Recently,\nHegerfeld and Kratsch (STACS'20) showed that, for some of those problems, the\nCut&Count technique can be also applied in the setting of treedepth, and it\ngives algorithms with running time $2^{\\mathcal{O}(d)}\\cdot n^{\\mathcal{O}(1)}$\nand polynomial space usage. However, a number of important problems eluded such\na treatment, with the most prominent examples being Hamiltonian Cycle and\nLongest Path.\n  In this paper we clarify the situation by showing that Hamiltonian Cycle,\nHamiltonian Path, Long Cycle, Long Path, and Min Cycle Cover all admit\n$5^d\\cdot n^{\\mathcal{O}(1)}$-time and polynomial space algorithms on graphs of\ntreedepth $d$. The algorithms are randomized Monte Carlo with only false\nnegatives.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 13:26:58 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 12:03:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Nederlof", "Jesper", ""], ["Pilipczuk", "Micha\u0142", ""], ["Swennenhuis", "C\u00e9line M. F.", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "2002.04543", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski and Maciej Pacut and Krzysztof Piecuch", "title": "An Optimal Algorithm for Online Multiple Knapsack", "comments": "To appear in the proceedings of ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online multiple knapsack problem, an algorithm faces a stream of\nitems, and each item has to be either rejected or stored irrevocably in one of\n$n$ bins (knapsacks) of equal size. The gain of an~algorithm is equal to the\nsum of sizes of accepted items and the goal is to maximize the total gain.\n  So far, for this natural problem, the best solution was the $0.5$-competitive\nalgorithm First Fit (the result holds for any $n \\geq 2$). We present the first\nalgorithm that beats this ratio, achieving the competitive ratio of\n$1/(1+\\ln(2))-O(1/n) \\approx 0.5906 - O(1/n)$. Our algorithm is deterministic\nand optimal up to lower-order terms, as the upper bound of $1/(1+\\ln(2))$ for\nrandomized solutions was given previously by Cygan et al. [TOCS 2016].\nFurthermore, we show that the lower-order term is inevitable for deterministic\nalgorithms, by improving their upper bound to $1/(1+\\ln(2))-O(1/n)$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:55:19 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 21:25:51 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Pacut", "Maciej", ""], ["Piecuch", "Krzysztof", ""]]}, {"id": "2002.04638", "submitter": "Hung Pham", "authors": "Duc Hung Pham, Krishna V. Palem, M. V. Panduranga Rao", "title": "A polynomial time parallel algorithm for graph isomorphism using a\n  quasipolynomial number of processors", "comments": "ICALP conference submission preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Graph Isomorphism (GI) problem is a theoretically interesting problem\nbecause it has not been proven to be in P nor to be NP-complete. Babai made a\nbreakthrough in 2015 when announcing a quasipolynomial time algorithm for GI\nproblem. Babai's work gives the most theoretically efficient algorithm for GI,\nas well as a strong evidence favoring the idea that class GI $\\ne$ NP and thus\nP $\\ne$ NP. Based on Babai's algorithm, we prove that GI can further be solved\nby a parallel algorithm that runs in polynomial time using a quasipolynomial\nnumber of processors. We achieve that result by identifying the bottlenecks in\nBabai's algorithms and parallelizing them. In particular, we prove that color\nrefinement can be computed in parallel logarithmic time using a polynomial\nnumber of processors, and the $k$-dimensional WL refinement can be computed in\nparallel polynomial time using a quasipolynomial number of processors. Our work\nsuggests that Graph Isomorphism and GI-complete problems can be computed\nefficiently in a parallel computer, and provides insights on speeding up\nparallel GI programs in practice.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:06:29 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Pham", "Duc Hung", ""], ["Palem", "Krishna V.", ""], ["Rao", "M. V. Panduranga", ""]]}, {"id": "2002.04727", "submitter": "Yung Tsin H", "authors": "Yung H. Tsin", "title": "A simple certifying algorithm for 3-edge-connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A linear-time certifying algorithm for 3-edge-connectivity is presented.\nGiven an undirected graph G, if G is 3-edge-connected, the algorithm generates\na construction sequence as a positive certificate for G. Otherwise, the\nalgorithm decomposes G into its 3-edge-connected components and at the same\ntime generates a construction sequence for each connected component as well as\nthe bridges and a cactus representation of the cut-pairs in G. All of these are\ndone by making only one pass over G using an innovative graph contraction\ntechnique. Moreover, the graph need not be 2-edge-connected.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 23:08:29 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tsin", "Yung H.", ""]]}, {"id": "2002.04778", "submitter": "Binhai Zhu", "authors": "Manuel Lafond and Binhai Zhu and Peng Zou", "title": "Genomic Problems Involving Copy Number Profiles: Complexity and\n  Algorithms", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, due to the genomic sequence analysis in several types of cancer,\nthe genomic data based on {\\em copy number profiles} ({\\em CNP} for short) are\ngetting more and more popular. A CNP is a vector where each component is a\nnon-negative integer representing the number of copies of a specific gene or\nsegment of interest.\n  In this paper, we present two streams of results. The first is the negative\nresults on two open problems regarding the computational complexity of the\nMinimum Copy Number Generation (MCNG) problem posed by Qingge et al. in 2018.\nIt was shown by Qingge et al. that the problem is NP-hard if the duplications\nare tandem and they left the open question of whether the problem remains\nNP-hard if arbitrary duplications are used. We answer this question\naffirmatively in this paper; in fact, we prove that it is NP-hard to even\nobtain a constant factor approximation. We also prove that the parameterized\nversion is W[1]-hard, answering another open question by Qingge et al.\n  The other result is positive and is based on a new (and more general) problem\nregarding CNP's. The \\emph{Copy Number Profile Conforming (CNPC)} problem is\nformally defined as follows: given two CNP's $C_1$ and $C_2$, compute two\nstrings $S_1$ and $S_2$ with $cnp(S_1)=C_1$ and $cnp(S_2)=C_2$ such that the\ndistance between $S_1$ and $S_2$, $d(S_1,S_2)$, is minimized. Here,\n$d(S_1,S_2)$ is a very general term, which means it could be any genome\nrearrangement distance (like reversal, transposition, and tandem duplication,\netc). We make the first step by showing that if $d(S_1,S_2)$ is measured by the\nbreakpoint distance then the problem is polynomially solvable.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 03:31:42 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Lafond", "Manuel", ""], ["Zhu", "Binhai", ""], ["Zou", "Peng", ""]]}, {"id": "2002.04783", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Nhat Ho, Xi Chen, Marco Cuturi, and Michael I. Jordan", "title": "Fixed-Support Wasserstein Barycenters: Computational Hardness and Fast\n  Algorithm", "comments": "Accepted by NeurIPS 2020; fix some confusing parts in the proof and\n  improve the empirical evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fixed-support Wasserstein barycenter problem (FS-WBP), which\nconsists in computing the Wasserstein barycenter of $m$ discrete probability\nmeasures supported on a finite metric space of size $n$. We show first that the\nconstraint matrix arising from the standard linear programming (LP)\nrepresentation of the FS-WBP is \\textit{not totally unimodular} when $m \\geq 3$\nand $n \\geq 3$. This result resolves an open question pertaining to the\nrelationship between the FS-WBP and the minimum-cost flow (MCF) problem since\nit proves that the FS-WBP in the standard LP form is not an MCF problem when $m\n\\geq 3$ and $n \\geq 3$. We also develop a provably fast \\textit{deterministic}\nvariant of the celebrated iterative Bregman projection (IBP) algorithm, named\n\\textsc{FastIBP}, with a complexity bound of\n$\\tilde{O}(mn^{7/3}\\varepsilon^{-4/3})$, where $\\varepsilon \\in (0, 1)$ is the\ndesired tolerance. This complexity bound is better than the best known\ncomplexity bound of $\\tilde{O}(mn^2\\varepsilon^{-2})$ for the IBP algorithm in\nterms of $\\varepsilon$, and that of $\\tilde{O}(mn^{5/2}\\varepsilon^{-1})$ from\naccelerated alternating minimization algorithm or accelerated primal-dual\nadaptive gradient algorithm in terms of $n$. Finally, we conduct extensive\nexperiments with both synthetic data and real images and demonstrate the\nfavorable performance of the \\textsc{FastIBP} algorithm in practice.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 03:40:52 GMT"}, {"version": "v10", "created": "Mon, 26 Jul 2021 17:26:50 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:08:33 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 20:13:25 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 11:16:44 GMT"}, {"version": "v5", "created": "Thu, 15 Oct 2020 03:38:29 GMT"}, {"version": "v6", "created": "Sat, 17 Oct 2020 02:19:25 GMT"}, {"version": "v7", "created": "Wed, 25 Nov 2020 23:08:14 GMT"}, {"version": "v8", "created": "Sun, 20 Dec 2020 11:25:02 GMT"}, {"version": "v9", "created": "Sat, 17 Jul 2021 06:25:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lin", "Tianyi", ""], ["Ho", "Nhat", ""], ["Chen", "Xi", ""], ["Cuturi", "Marco", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2002.04830", "submitter": "Kevin Tian", "authors": "Arun Jambulapati, Yin Tat Lee, Jerry Li, Swati Padmanabhan, Kevin Tian", "title": "Positive Semidefinite Programming: Mixed, Parallel, and\n  Width-Independent", "comments": "There is an error in this manuscript. This version notes the source\n  of the error on the first page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first approximation algorithm for mixed packing and covering\nsemidefinite programs (SDPs) with polylogarithmic dependence on width. Mixed\npacking and covering SDPs constitute a fundamental algorithmic primitive with\nrecent applications in combinatorial optimization, robust learning, and quantum\ncomplexity. The current approximate solvers for positive semidefinite\nprogramming can handle only pure packing instances, and technical hurdles\nprevent their generalization to a wider class of positive instances. For a\ngiven multiplicative accuracy of $\\epsilon$, our algorithm takes\n$O(\\log^3(nd\\rho) \\cdot \\epsilon^{-3})$ parallelizable iterations, where $n$,\n$d$ are dimensions of the problem and $\\rho$ is a width parameter of the\ninstance, generalizing or improving all previous parallel algorithms in the\npositive linear and semidefinite programming literature. When specialized to\npure packing SDPs, our algorithm's iteration complexity is $O(\\log^2 (nd) \\cdot\n\\epsilon^{-2})$, a slight improvement and derandomization of the\nstate-of-the-art (Allen-Zhu et. al. '16, Peng et. al. '16, Wang et. al. '15).\nFor a wide variety of structured instances commonly found in applications, the\niterations of our algorithm run in nearly-linear time.\n  In doing so, we give matrix analytic techniques for overcoming obstacles that\nhave stymied prior approaches to this open problem, as stated in past works\n(Peng et. al. '16, Mahoney et. al. '16). Crucial to our analysis are a\nsimplification of existing algorithms for mixed positive linear programs,\nachieved by removing an asymmetry caused by modifying covering constraints, and\na suite of matrix inequalities whose proofs are based on analyzing the Schur\ncomplements of matrices in a higher dimension. We hope that both our algorithm\nand techniques open the door to improved solvers for positive semidefinite\nprogramming, as well as its applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 07:47:50 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 22:48:35 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 04:07:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jambulapati", "Arun", ""], ["Lee", "Yin Tat", ""], ["Li", "Jerry", ""], ["Padmanabhan", "Swati", ""], ["Tian", "Kevin", ""]]}, {"id": "2002.04841", "submitter": "Harro Wimmel", "authors": "Uli Schlachter and Harro Wimmel", "title": "Optimal Label Splitting for Embedding an LTS into an arbitrary Petri Net\n  Reachability Graph is NP-complete", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given labelled transition system (LTS), synthesis is the task to find\nan unlabelled Petri net with an isomorphic reachability graph. Even when just\ndemanding an embedding into a reachability graph instead of an isomorphism, a\nsolution is not guaranteed. In such a case, label splitting is an option, i.e.\nrelabelling edges of the LTS such that differently labelled edges remain\ndifferent. With an appropriate label splitting, we can always obtain a solution\nfor the synthesis or embedding problem. Using the label splitting, we can\nconstruct a labelled Petri net with the intended bahaviour (e.g. embedding the\ngiven LTS in its reachability graph). As the labelled Petri net can have a\nlarge number of transitions, an optimisation may be desired, limiting the\nnumber of labels produced by the label splitting. We show that such a\nlimitation will turn the problem from being solvable in polynomial time into an\nNP-complete problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 08:28:32 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Schlachter", "Uli", ""], ["Wimmel", "Harro", ""]]}, {"id": "2002.04850", "submitter": "Luca Elias Sch\\\"afer", "authors": "Luca E. Sch\\\"afer, Tobias Dietz, Maria Barbati, Jos\\'e Rui Figueira,\n  Salvatore Greco, Stefan Ruzika", "title": "The {0,1}-knapsack problem with qualitative levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variant of the classical knapsack problem is considered in which each item\nis associated with an integer weight and a qualitative level. We define a\ndominance relation over the feasible subsets of the given item set and show\nthat this relation defines a preorder. We propose a dynamic programming\nalgorithm to compute the entire set of non-dominated rank cardinality vectors\nand we state two greedy algorithms, which efficiently compute a single\nefficient solution.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:00:29 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sch\u00e4fer", "Luca E.", ""], ["Dietz", "Tobias", ""], ["Barbati", "Maria", ""], ["Figueira", "Jos\u00e9 Rui", ""], ["Greco", "Salvatore", ""], ["Ruzika", "Stefan", ""]]}, {"id": "2002.04870", "submitter": "Rasmus Pagh", "authors": "Mayank Goswami, Riko Jacob, Rasmus Pagh", "title": "On the I/O complexity of the k-nearest neighbor problem", "comments": "Appears in proceedings of PODS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider static, external memory indexes for exact and approximate\nversions of the $k$-nearest neighbor ($k$-NN) problem, and show new lower\nbounds under a standard indivisibility assumption:\n  - Polynomial space indexing schemes for high-dimensional $k$-NN in Hamming\nspace cannot take advantage of block transfers: $\\Omega(k)$ block reads are\nneeded to to answer a query.\n  - For the $\\ell_\\infty$ metric the lower bound holds even if we allow\n$c$-appoximate nearest neighbors to be returned, for $c \\in (1, 3)$.\n  - The restriction to $c < 3$ is necessary: For every metric there exists an\nindexing scheme in the indexability model of Hellerstein et al.~using space\n$O(kn)$, where $n$ is the number of points, that can retrieve $k$ 3-approximate\nnearest neighbors using $\\lceil k/B\\rceil$ I/Os, which is optimal.\n  - For specific metrics, data structures with better approximation factors are\npossible. For $k$-NN in Hamming space and every approximation factor $c>1$\nthere exists a polynomial space data structure that returns $k$ $c$-approximate\nnearest neighbors in $\\lceil k/B\\rceil$ I/Os.\n  To show these lower bounds we develop two new techniques: First, to handle\nthat approximation algorithms have more freedom in deciding which result set to\nreturn we develop a relaxed version of the $\\lambda$-set workload technique of\nHellerstein et al. This technique allows us to show lower bounds that hold in\n$d\\geq n$ dimensions. To extend the lower bounds down to $d = O(k \\log(n/k))$\ndimensions, we develop a new deterministic dimension reduction technique that\nmay be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:47:42 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 07:06:16 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Goswami", "Mayank", ""], ["Jacob", "Riko", ""], ["Pagh", "Rasmus", ""]]}, {"id": "2002.04894", "submitter": "Stefan Engblom", "authors": "Jonathan Bull and Stefan Engblom", "title": "Distributed and Adaptive Fast Multipole Method In Three Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general distributed implementation of an adaptive fast multipole\nmethod in three space dimensions. We rely on a balanced type of adaptive space\ndiscretisation which supports a highly transparent and fully distributed\nimplementation. A complexity analysis indicates favorable scaling properties\nand numerical experiments on up to 512 cores and 1 billion source points verify\nthem. The parameters controlling the algorithm are subject to in-depth\nexperiments and the performance response to the input parameters implies that\nthe overall implementation is well-suited to automated tuning.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:06:58 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Bull", "Jonathan", ""], ["Engblom", "Stefan", ""]]}, {"id": "2002.04979", "submitter": "Jingjin Yu", "authors": "Mario Szegedy and Jingjin Yu", "title": "On Rearrangement of Items Stored in Stacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are $n \\ge 2$ stacks, each filled with $d$ items, and one empty stack.\nEvery stack has capacity $d > 0$. A robot arm, in one stack operation (step),\nmay pop one item from the top of a non-empty stack and subsequently push it\nonto a stack not at capacity. In a {\\em labeled} problem, all $nd$ items are\ndistinguishable and are initially randomly scattered in the $n$ stacks. The\nitems must be rearranged using pop-and-pushs so that in the end, the $k^{\\rm\nth}$ stack holds items $(k-1)d +1, \\ldots, kd$, in that order, from the top to\nthe bottom for all $1 \\le k \\le n$. In an {\\em unlabeled} problem, the $nd$\nitems are of $n$ types of $d$ each. The goal is to rearrange items so that\nitems of type $k$ are located in the $k^{\\rm th}$ stack for all $1 \\le k \\le\nn$. In carrying out the rearrangement, a natural question is to find the least\nnumber of required pop-and-pushes.\n  Our main contributions are: (1) an algorithm for restoring the order of $n^2$\nitems stored in an $n \\times n$ table using only $2n$ column and row\npermutations, and its generalization, and (2) an algorithm with a guaranteed\nupper bound of $O(nd)$ steps for solving both versions of the stack\nrearrangement problem when $d \\le \\lceil cn \\rceil$ for arbitrary fixed\npositive number $c$. In terms of the required number of steps, the labeled and\nunlabeled version have lower bounds $\\Omega(nd + nd{\\frac{\\log d}{\\log n}})$\nand $\\Omega(nd)$, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:37:23 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 02:42:20 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Szegedy", "Mario", ""], ["Yu", "Jingjin", ""]]}, {"id": "2002.04989", "submitter": "Shrey Dabhi", "authors": "Shrey Dabhi and Manojkumar Parmar", "title": "Eigenvector Component Calculation Speedup over NumPy for\n  High-Performance Computing", "comments": "Accepted at 8th International Conference on Recent Trends in\n  Computing (ICRTC 2020), to be published in Springer Lecture Notes in Networks\n  and Systems (LNNS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications related to artificial intelligence, machine learning, and system\nidentification simulations essentially use eigenvectors. Calculating\neigenvectors for very large matrices using conventional methods is\ncompute-intensive and renders the applications slow. Recently,\nEigenvector-Eigenvalue Identity formula promising significant speedup was\nidentified. We study the algorithmic implementation of the formula against the\nexisting state-of-the-art algorithms and their implementations to evaluate the\nperformance gains. We provide a first of its kind systematic study of the\nimplementation of the formula. We demonstrate further improvements using\nhigh-performance computing concepts over native NumPy eigenvector\nimplementation which uses LAPACK and BLAS.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:44:41 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 16:40:42 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 20:27:06 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 07:21:06 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Dabhi", "Shrey", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2002.05034", "submitter": "Yijie Han", "authors": "Yijie Han", "title": "Uniform Linked Lists Contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm (EREW PRAM algorithm) for linked lists\ncontraction. We show that when we contract a linked list from size $n$ to size\n$n/c$ for a suitable constant $c$ we can pack the linked list into an array of\nsize $n/d$ for a constant $1 < d\\leq c$ in the time of 3 coloring the list.\nThus for a set of linked lists with a total of $n$ elements and the longest\nlist has $l$ elements our algorithm contracts them in $O(n\\log\ni/p+(\\log^{(i)}n+\\log i )\\log \\log l+ \\log l)$ time, for an arbitrary\nconstructible integer $i$, with $p$ processors on the EREW PRAM, where\n$\\log^{(1)} n =\\log n$ and $\\log^{(t)}n=\\log \\log^{(t-1)} n$ and $\\log^*n=\\min\n\\{ i|\\log^{(i)} n < 10\\}$. When $i$ is a constant we get time\n$O(n/p+\\log^{(i)}n\\log \\log l+\\log l)$. Thus when $l=\\Omega (\\log^{(c)}n)$ for\nany constant $c$ we achieve $O(n/p+\\log l)$ time. The previous best\ndeterministic EREW PRAM algorithm has time $O(n/p+\\log n)$ and best CRCW PRAM\nalgorithm has time $O(n/p+\\log n/\\log \\log n+\\log l)$.\n  Keywords: Parallel algorithms, linked list, linked list contraction, uniform\nlinked list contraction, EREW PRAM.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 14:50:40 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 14:25:21 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 15:55:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Han", "Yijie", ""]]}, {"id": "2002.05068", "submitter": "Tomohiro Koana", "authors": "Tomohiro Koana, Vincent Froese, Rolf Niedermeier", "title": "Complexity of Combinatorial Matrix Completion With Diameter Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We thoroughly study a novel and still basic combinatorial matrix completion\nproblem: Given a binary incomplete matrix, fill in the missing entries so that\nthe resulting matrix has a specified maximum diameter (that is, upper-bounding\nthe maximum Hamming distance between any two rows of the completed matrix) as\nwell as a specified minimum Hamming distance between any two of the matrix\nrows. This scenario is closely related to consensus string problems as well as\nto recently studied clustering problems on incomplete data.\n  We obtain an almost complete complexity dichotomy between polynomial-time\nsolvable and NP-hard cases in terms of the minimum distance lower bound and the\nnumber of missing entries per row of the incomplete matrix. Further, we develop\npolynomial-time algorithms for maximum diameter three, which are based on\nDeza's theorem from extremal set theory. On the negative side we prove\nNP-hardness for diameter at least four. For the parameter number of missing\nentries per row, we show polynomial-time solvability when there is only one\nmissing entry and NP-hardness when there can be at least two missing entries.\nIn general, our algorithms heavily rely on Deza's theorem and the\ncorrespondingly identified sunflower structures pave the way towards solutions\nbased on computing graph factors and solving 2-SAT instances.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 16:16:20 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Koana", "Tomohiro", ""], ["Froese", "Vincent", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2002.05071", "submitter": "Subhra Mazumdar", "authors": "Subhra Mazumdar and Sushmita Ruj and Ram Govind Singh and Arindam Pal", "title": "HushRelay: A Privacy-Preserving, Efficient, and Scalable Routing\n  Algorithm for Off-Chain Payments", "comments": "9 pages, 16 figures, 1 table, accepted to the Short Paper track of\n  the 2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channel networks (PCN) are used in cryptocurrencies to enhance the\nperformance and scalability of off-chain transactions. Except for opening and\nclosing of a payment channel, no other transaction requests accepted by a PCN\nare recorded in the Blockchain. Only the parties which have opened the channel\nwill know the exact amount of fund left at a given instant. In real scenarios,\nthere might not exist a single path which can enable transfer of high value\npayments. For such cases, splitting up the transaction value across multiple\npaths is a better approach. While there exists several approaches which route\ntransactions via several paths, such techniques are quite inefficient, as the\ndecision on the number of splits must be taken at the initial phase of the\nrouting algorithm (e.g., SpeedyMurmur [42]). Algorithms which do not consider\nthe residual capacity of each channel in the network are susceptible to\nfailure. Other approaches leak sensitive information, and are quite\ncomputationally expensive [28]. To the best of our knowledge, our proposed\nscheme HushRelay is an efficient privacy preserving routing algorithm, taking\ninto account the funds left in each channel, while splitting the transaction\nvalue across several paths. Comparing the performance of our algorithm with\nexisting routing schemes on real instances (e.g., Ripple Network), we observed\nthat HushRelay attains a success ratio of 1, with an execution time of 2.4 sec.\nHowever, SpeedyMurmur [42] attains a success ratio of 0.98 and takes 4.74 sec\nwhen the number of landmarks is 6. On testing our proposed routing algorithm on\nthe Lightning Network, a success ratio of 0.99 is observed, having an execution\ntime of 0.15 sec, which is 12 times smaller than the time taken by\nSpeedyMurmur.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 16:21:36 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Mazumdar", "Subhra", ""], ["Ruj", "Sushmita", ""], ["Singh", "Ram Govind", ""], ["Pal", "Arindam", ""]]}, {"id": "2002.05121", "submitter": "Robert Meier", "authors": "Daniel Bertschinger and Johannes Lengler and Anders Martinsson and\n  Robert Meier and Angelika Steger and Milo\\v{s} Truji\\'c and Emo Welzl", "title": "An Optimal Decentralized $(\\Delta + 1)$-Coloring Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following simple coloring algorithm for a graph on $n$ vertices.\nEach vertex chooses a color from $\\{1, \\dotsc, \\Delta(G) + 1\\}$ uniformly at\nrandom. While there exists a conflicted vertex choose one such vertex uniformly\nat random and recolor it with a randomly chosen color. This algorithm was\nintroduced by Bhartia et al. [MOBIHOC'16] for channel selection in\nWIFI-networks. We show that this algorithm always converges to a proper\ncoloring in expected $O(n \\log \\Delta)$ steps, which is optimal and proves a\nconjecture of Chakrabarty and Supinski [SOSA'20].\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 17:43:54 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 15:41:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bertschinger", "Daniel", ""], ["Lengler", "Johannes", ""], ["Martinsson", "Anders", ""], ["Meier", "Robert", ""], ["Steger", "Angelika", ""], ["Truji\u0107", "Milo\u0161", ""], ["Welzl", "Emo", ""]]}, {"id": "2002.05129", "submitter": "Daniel Anderson", "authors": "Umut A. Acar, Daniel Anderson, Guy E. Blelloch, Laxman Dhulipala, Sam\n  Westrick", "title": "Parallel Batch-dynamic Trees via Change Propagation", "comments": null, "journal-ref": "Proceedings of The 28th Annual European Symposium on Algorithms\n  (ESA '20) (2020) 2:1-2:23", "doi": "10.4230/LIPIcs.ESA.2020.2", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic trees problem is to maintain a forest subject to edge insertions\nand deletions while facilitating queries such as connectivity, path weights,\nand subtree weights. Dynamic trees are a fundamental building block of a large\nnumber of graph algorithms. Although traditionally studied in the single-update\nsetting, dynamic algorithms capable of supporting batches of updates are\nincreasingly relevant today due to the emergence of rapidly evolving dynamic\ndatasets. Since processing updates on a single processor is often unrealistic\nfor large batches of updates, designing parallel batch-dynamic algorithms that\nachieve provably low span is important for many applications. In this work, we\ndesign the first work-efficient parallel batch-dynamic algorithm for dynamic\ntrees that is capable of supporting both path queries and subtree queries, as\nwell as a variety of non-local queries. To achieve this, we propose a framework\nfor algorithmically dynamizing static round-synchronous algorithms that allows\nus to obtain parallel batch-dynamic algorithms with good bounds on their work\nand span. In our framework, the algorithm designer can apply the technique to\nany suitably defined static algorithm. We then obtain theoretical guarantees\nfor algorithms in our framework by defining the notion of a computation\ndistance between two executions of the underlying algorithm.\n  Our dynamic trees algorithm is obtained by applying our dynamization\nframework to the parallel tree contraction algorithm of Miller and Reif, and\nthen performing a novel analysis of the computation distance of this algorithm\nunder batch updates. We show that $k$ updates can be performed in $O(k\n\\log(1+n/k))$ work in expectation, which matches an existing algorithm of Tseng\net al. while providing support for a substantially larger number of queries and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:20:20 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 22:56:00 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Acar", "Umut A.", ""], ["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""], ["Dhulipala", "Laxman", ""], ["Westrick", "Sam", ""]]}, {"id": "2002.05139", "submitter": "Pravesh K Kothari", "authors": "Ainesh Bakshi and Pravesh K. Kothari", "title": "List-Decodable Subspace Recovery: Dimension Independent Error in\n  Polynomial Time", "comments": "To appear in SODA 2021. This version fixes an issue in a technical\n  claim bounding the variance of degree 2 polynomials and improves exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In list-decodable subspace recovery, the input is a collection of $n$ points\n$\\alpha n$ (for some $\\alpha \\ll 1/2$) of which are drawn i.i.d. from a\ndistribution $\\mathcal{D}$ with a isotropic rank $r$ covariance $\\Pi_*$ (the\n\\emph{inliers}) and the rest are arbitrary, potential adversarial outliers. The\ngoal is to recover a $O(1/\\alpha)$ size list of candidate covariances that\ncontains a $\\hat{\\Pi}$ close to $\\Pi_*$. Two recent independent works\n(Raghavendra-Yau, Bakshi-Kothari 2020) gave the first efficient algorithm for\nthis problem. These results, however, obtain an error that grows with the\ndimension (linearly in [RY] and logarithmically in BK) at the cost of\nquasi-polynomial running time) and rely on \\emph{certifiable\nanti-concentration} - a relatively strict condition satisfied essentially only\nby the Gaussian distribution.\n  In this work, we improve on these results on all three fronts:\n\\emph{dimension-independent} error via a faster fixed-polynomial running time\nunder less restrictive distributional assumptions. Specifically, we give a\n$poly(1/\\alpha) d^{O(1)}$ time algorithm that outputs a list containing a\n$\\hat{\\Pi}$ satisfying $\\|\\hat{\\Pi} -\\Pi_*\\|_F \\leq O(1/\\alpha)$. Our result\nonly needs $\\mathcal{D}$ to have \\emph{certifiably hypercontractive} degree 2\npolynomials. As a result, in addition to Gaussians, our algorithm applies to\nthe uniform distribution on the hypercube and $q$-ary cubes and arbitrary\nproduct distributions with subgaussian marginals. Prior work (Raghavendra and\nYau, 2020) had identified such distributions as potential hard examples as such\ndistributions do not exhibit strong enough anti-concentration. When\n$\\mathcal{D}$ satisfies certifiable anti-concentration, we obtain a stronger\nerror guarantee of $\\|\\hat{\\Pi}-\\Pi_*\\|_F \\leq \\eta$ for any arbitrary $\\eta >\n0$ in $d^{O(poly(1/\\alpha) + \\log (1/\\eta))}$ time.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:30:09 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 04:53:41 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 17:54:57 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Kothari", "Pravesh K.", ""]]}, {"id": "2002.05160", "submitter": "Mathilde Fekom", "authors": "Mathilde Fekom, Nicolas Vayatis, Argyris Kalogeratos", "title": "Optimal Multiple Stopping Rule for Warm-Starting Sequential Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Warm-starting Dynamic Thresholding algorithm,\ndeveloped using dynamic programming, for a variant of the standard online\nselection problem. The problem allows job positions to be either free or\nalready occupied at the beginning of the process. Throughout the selection\nprocess, the decision maker interviews one after the other the new candidates\nand reveals a quality score for each of them. Based on that information, she\ncan (re)assign each job at most once by taking immediate and irrevocable\ndecisions. We relax the hard requirement of the class of dynamic programming\nalgorithms to perfectly know the distribution from which the scores of\ncandidates are drawn, by presenting extensions for the partial and\nno-information cases, in which the decision maker can learn the underlying\nscore distribution sequentially while interviewing candidates.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 14:04:43 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Fekom", "Mathilde", ""], ["Vayatis", "Nicolas", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "2002.05276", "submitter": "Yixin Shen", "authors": "Xavier Bonnetain, R\\'emi Bricout, Andr\\'e Schrottenloher, Yixin Shen", "title": "Improved Classical and Quantum Algorithms for Subset-Sum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new classical and quantum algorithms for solving random subset-sum\ninstances. First, we improve over the Becker-Coron-Joux algorithm (EUROCRYPT\n2011) from $\\tilde{\\mathcal{O}}(2^{0.291 n})$ downto\n$\\tilde{\\mathcal{O}}(2^{0.283 n})$, using more general representations with\nvalues in $\\{-1,0,1,2\\}$.\n  Next, we improve the state of the art of quantum algorithms for this problem\nin several directions. By combining the Howgrave-Graham-Joux algorithm\n(EUROCRYPT 2010) and quantum search, we devise an algorithm with asymptotic\ncost $\\tilde{\\mathcal{O}}(2^{0.236 n})$, lower than the cost of the quantum\nwalk based on the same classical algorithm proposed by Bernstein, Jeffery,\nLange and Meurer (PQCRYPTO 2013). This algorithm has the advantage of using\n\\emph{classical} memory with quantum random access, while the previously known\nalgorithms used the quantum walk framework, and required \\emph{quantum} memory\nwith quantum random access.\n  We also propose new quantum walks for subset-sum, performing better than the\nprevious best time complexity of $\\tilde{\\mathcal{O}}(2^{0.226 n})$ given by\nHelm and May (TQC 2018). We combine our new techniques to reach a time\n$\\tilde{\\mathcal{O}}(2^{0.216 n})$. This time is dependent on a heuristic on\nquantum walk updates, formalized by Helm and May, that is also required by the\nprevious algorithms. We show how to partially overcome this heuristic, and we\nobtain an algorithm with quantum time $\\tilde{\\mathcal{O}}(2^{0.218 n})$\nrequiring only the standard classical subset-sum heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:23:04 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 22:20:38 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 15:50:55 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2020 15:32:16 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Bonnetain", "Xavier", ""], ["Bricout", "R\u00e9mi", ""], ["Schrottenloher", "Andr\u00e9", ""], ["Shen", "Yixin", ""]]}, {"id": "2002.05321", "submitter": "Shaojie Tang", "authors": "Shaojie Tang and Jing Yuan", "title": "Assortment Optimization with Repeated Exposures and Product-dependent\n  Patience Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the assortment optimization problem faced by many\nonline retailers such as Amazon. We develop a \\emph{cascade multinomial logit\nmodel}, based on the classic multinomial logit model, to capture the consumers'\npurchasing behavior across multiple stages. Different from existing studies,\nour model allows for repeated exposures of a product, i.e., the same product\ncan be displayed multiple times across different stages. In addition, each\nconsumer has a \\emph{patience budget} that is sampled from a known distribution\nand each product is associated with a \\emph{patience cost}, which captures the\ncognitive efforts spent on browsing that product. Given an assortment of\nproducts, a consumer sequentially browses them stage by stage. After browsing\nall products in one stage, if the utility of a product exceeds the utility of\nthe outside option, the consumer proceeds to purchase the product and leave the\nplatform. Otherwise, if the patience cost of all products browsed up to that\npoint is no larger than her patience budget, she continues to view the next\nstage. We propose an approximation solution to this problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 03:12:49 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 20:32:29 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Tang", "Shaojie", ""], ["Yuan", "Jing", ""]]}, {"id": "2002.05376", "submitter": "Sumathi Sivasubramaniam", "authors": "John Augustine and Keerti Choudhary and Avi Cohen and David Peleg and\n  Sumathi Sivasubramaniam and Suman Sourav", "title": "Distributed Graph Realizations", "comments": "22 pages, 4 figures. Short version to appear in IPDPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph realization problems from a distributed perspective and we\nstudy it in the node capacitated clique (NCC) model of distributed computing,\nrecently introduced for representing peer-to-peer networks. We focus on two\ncentral variants, degree-sequence realization and minimum\nthreshold-connectivity realization both of which result in overlay network\nrealizations. Overlay network realizations can be either explicit or implicit.\nExplicit realizations require both endpoints of any edge in the realized graph\nto be aware of the edge. In implicit realizations, on the other hand, at least\none endpoint of each edge of the realized graph needs to be aware of the edge.\nThe main realization algorithms we present are the following.\n  1. An $\\tilde{O}(\\min\\{\\sqrt{m},\\Delta\\})$ time algorithm for implicit\nrealization of a degree sequence. Here, $\\Delta = \\max_v d(v)$ is the maximum\ndegree and $m = (1/2) \\sum_v d(v)$ is the number of edges in the final\nrealization. An $\\tilde{O}(\\Delta)$ time algorithm for an explicit realization\nof a degree sequence. We first compute an implicit realization and then\ntransform it into an explicit one in $\\tilde{O}(\\Delta)$ additional rounds.\n  2. An $\\tilde{O}(\\Delta)$ time algorithm for the threshold connectivity\nproblem that obtains an explicit solution and an improved $\\tilde{O}(1)$\nalgorithm for implicit realization when all nodes know each other's IDs. These\nalgorithms are 2-approximations w.r.t. the number of edges.\n  We complement our upper bounds with lower bounds to show that the above\nalgorithms are tight up to factors of $\\log n$. Additionally, we provide\nalgorithms for realizing trees and an $\\tilde{O}(1)$ round algorithm for\napproximate degree sequence realization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 07:36:34 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 17:49:31 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 15:27:25 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Augustine", "John", ""], ["Choudhary", "Keerti", ""], ["Cohen", "Avi", ""], ["Peleg", "David", ""], ["Sivasubramaniam", "Sumathi", ""], ["Sourav", "Suman", ""]]}, {"id": "2002.05378", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, N. V.\n  Vinodchandran", "title": "Efficient Distance Approximation for Structured High-Dimensional\n  Distributions via Learning", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design efficient distance approximation algorithms for several classes of\nstructured high-dimensional distributions. Specifically, we show algorithms for\nthe following problems:\n  - Given sample access to two Bayesian networks $P_1$ and $P_2$ over known\ndirected acyclic graphs $G_1$ and $G_2$ having $n$ nodes and bounded in-degree,\napproximate $d_{tv}(P_1,P_2)$ to within additive error $\\epsilon$ using\n$poly(n,\\epsilon)$ samples and time\n  - Given sample access to two ferromagnetic Ising models $P_1$ and $P_2$ on\n$n$ variables with bounded width, approximate $d_{tv}(P_1, P_2)$ to within\nadditive error $\\epsilon$ using $poly(n,\\epsilon)$ samples and time\n  - Given sample access to two $n$-dimensional Gaussians $P_1$ and $P_2$,\napproximate $d_{tv}(P_1, P_2)$ to within additive error $\\epsilon$ using\n$poly(n,\\epsilon)$ samples and time\n  - Given access to observations from two causal models $P$ and $Q$ on $n$\nvariables that are defined over known causal graphs, approximate $d_{tv}(P_a,\nQ_a)$ to within additive error $\\epsilon$ using $poly(n,\\epsilon)$ samples,\nwhere $P_a$ and $Q_a$ are the interventional distributions obtained by the\nintervention $do(A=a)$ on $P$ and $Q$ respectively for a particular variable\n$A$.\n  Our results are the first efficient distance approximation algorithms for\nthese well-studied problems. They are derived using a simple and general\nconnection to distribution learning algorithms. The distance approximation\nalgorithms imply new efficient algorithms for {\\em tolerant} testing of\ncloseness of the above-mentioned structured high-dimensional distributions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 07:42:06 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 03:03:10 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gayen", "Sutanu", ""], ["Meel", "Kuldeep S.", ""], ["Vinodchandran", "N. V.", ""]]}, {"id": "2002.05414", "submitter": "S\\'andor Kisfaludi-Bak", "authors": "S\\'andor Kisfaludi-Bak", "title": "A quasi-polynomial algorithm for well-spaced hyperbolic TSP", "comments": "SoCG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the traveling salesman problem in the hyperbolic plane of Gaussian\ncurvature $-1$. Let $\\alpha$ denote the minimum distance between any two input\npoints. Using a new separator theorem and a new rerouting argument, we give an\n$n^{O(\\log^2 n)\\max(1,1/\\alpha)}$ algorithm for Hyperbolic TSP. This is\nquasi-polynomial time if $\\alpha$ is at least some absolute constant, and it\ngrows to $n^{O(\\sqrt{n})}$ as $\\alpha$ decreases to $\\log^2 n/\\sqrt{n}$. (For\neven smaller values of $\\alpha$, we can use a planarity-based algorithm of\nHwang et al. (1993), which gives a running time of $n^{O(\\sqrt{n})}$.)\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:05:04 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Kisfaludi-Bak", "S\u00e1ndor", ""]]}, {"id": "2002.05477", "submitter": "Yuichi Yoshida", "authors": "Chien-Chung Huang and Naonori Kakimura and Simon Mauras and Yuichi\n  Yoshida", "title": "Approximability of Monotone Submodular Function Maximization under\n  Cardinality and Matroid Constraints in the Streaming Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing a monotone submodular function under various constraints is a\nclassical and intensively studied problem. However, in the single-pass\nstreaming model, where the elements arrive one by one and an algorithm can\nstore only a small fraction of input elements, there is much gap in our\nknowledge, even though several approximation algorithms have been proposed in\nthe literature.\n  In this work, we present the first lower bound on the approximation ratios\nfor cardinality and matroid constraints that beat $1-\\frac{1}{e}$ in the\nsingle-pass streaming model. Let $n$ be the number of elements in the stream.\nThen, we prove that any (randomized) streaming algorithm for a cardinality\nconstraint with approximation ratio $\\frac{2}{2+\\sqrt{2}}+\\varepsilon$ requires\n$\\Omega\\left(\\frac{n}{K^2}\\right)$ space for any $\\varepsilon>0$, where $K$ is\nthe size limit of the output set. We also prove that any (randomized) streaming\nalgorithm for a (partition) matroid constraint with approximation ratio\n$\\frac{K}{2K-1}+\\varepsilon$ requires $\\Omega\\left(\\frac{n}{K}\\right)$ space\nfor any $\\varepsilon>0$, where $K$ is the rank of the given matroid.\n  In addition, we give streaming algorithms when we only have a weak oracle\nwith which we can only evaluate function values on feasible sets. Specifically,\nwe show weak-oracle streaming algorithms for cardinality and matroid\nconstraints with approximation ratios $\\frac{K}{2K-1}$ and $\\frac{1}{2}$,\nrespectively, whose space complexity is exponential in $K$ but is independent\nof $n$. The former one exactly matches the known inapproximability result for a\ncardinality constraint in the weak oracle model.\n  The latter one almost matches our lower bound of $\\frac{K}{2K-1}$ for a\nmatroid constraint, which almost settles the approximation ratio for a matroid\nconstraint that can be obtained by a streaming algorithm whose space complexity\nis independent of $n$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 12:33:46 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Kakimura", "Naonori", ""], ["Mauras", "Simon", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2002.05576", "submitter": "Andrej Risteski", "authors": "Ankur Moitra, Andrej Risteski", "title": "Fast Convergence for Langevin Diffusion with Manifold Structure", "comments": "52 pages, in submission to NeurIPS 2020. This version: various typos\n  fixed, minor reorganization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of sampling from distributions of the\nform p(x) \\propto e^{-\\beta f(x)} for some function f whose values and\ngradients we can query. This mode of access to f is natural in the scenarios in\nwhich such problems arise, for instance sampling from posteriors in parametric\nBayesian models. Classical results show that a natural random walk, Langevin\ndiffusion, mixes rapidly when f is convex. Unfortunately, even in simple\nexamples, the applications listed above will entail working with functions f\nthat are nonconvex -- for which sampling from p may in general require an\nexponential number of queries.\n  In this paper, we focus on an aspect of nonconvexity relevant for modern\nmachine learning applications: existence of invariances (symmetries) in the\nfunction f, as a result of which the distribution p will have manifolds of\npoints with equal probability. First, we give a recipe for proving mixing time\nbounds for Langevin diffusion as a function of the geometry of these manifolds.\nSecond, we specialize our arguments to classic matrix factorization-like\nBayesian inference problems where we get noisy measurements A(XX^T), X \\in R^{d\n\\times k} of a low-rank matrix, i.e. f(X) = \\|A(XX^T) - b\\|^2_2, X \\in R^{d\n\\times k}, and \\beta the inverse of the variance of the noise. Such functions f\nare invariant under orthogonal transformations, and include problems like\nmatrix factorization, sensing, completion. Beyond sampling, Langevin dynamics\nis a popular toy model for studying stochastic gradient descent. Along these\nlines, we believe that our work is an important first step towards\nunderstanding how SGD behaves when there is a high degree of symmetry in the\nspace of parameters the produce the same output.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:49:04 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 17:48:52 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Moitra", "Ankur", ""], ["Risteski", "Andrej", ""]]}, {"id": "2002.05580", "submitter": "Fabrizio Frati", "authors": "Oswin Aichholzer, Manuel Borrazzo, Prosenjit Bose, Jean Cardinal,\n  Fabrizio Frati, Pat Morin and Birgit Vogtenhuber", "title": "Drawing Graphs as Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of embedding graphs in the plane as good geometric\nspanners. That is, for a graph $G$, the goal is to construct a straight-line\ndrawing $\\Gamma$ of $G$ in the plane such that, for any two vertices $u$ and\n$v$ of $G$, the ratio between the minimum length of any path from $u$ to $v$\nand the Euclidean distance between $u$ and $v$ is small. The maximum such\nratio, over all pairs of vertices of $G$, is the spanning ratio of $\\Gamma$.\n  First, we show that deciding whether a graph admits a straight-line drawing\nwith spanning ratio $1$, a proper straight-line drawing with spanning ratio\n$1$, and a planar straight-line drawing with spanning ratio $1$ are\nNP-complete, $\\exists \\mathbb R$-complete, and linear-time solvable problems,\nrespectively, where a drawing is proper if no two vertices overlap and no edge\noverlaps a vertex.\n  Second, we show that moving from spanning ratio $1$ to spanning ratio\n$1+\\epsilon$ allows us to draw every graph. Namely, we prove that, for every\n$\\epsilon>0$, every (planar) graph admits a proper (resp. planar) straight-line\ndrawing with spanning ratio smaller than $1+\\epsilon$.\n  Third, our drawings with spanning ratio smaller than $1+\\epsilon$ have large\nedge-length ratio, that is, the ratio between the length of the longest edge\nand the length of the shortest edge is exponential. We show that this is\nsometimes unavoidable. More generally, we identify having bounded toughness as\nthe criterion that distinguishes graphs that admit straight-line drawings with\nconstant spanning ratio and polynomial edge-length ratio from graphs that\nrequire exponential edge-length ratio in any straight-line drawing with\nconstant spanning ratio.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:52:51 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Aichholzer", "Oswin", ""], ["Borrazzo", "Manuel", ""], ["Bose", "Prosenjit", ""], ["Cardinal", "Jean", ""], ["Frati", "Fabrizio", ""], ["Morin", "Pat", ""], ["Vogtenhuber", "Birgit", ""]]}, {"id": "2002.05599", "submitter": "Timo Bingmann", "authors": "Timo Bingmann and Jasper Marianczuk and Peter Sanders", "title": "Engineering Faster Sorters for Small Sets of Items", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.08111", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting a set of items is a task that can be useful by itself or as a\nbuilding block for more complex operations. That is why a lot of effort has\nbeen put into finding sorting algorithms that sort large sets as fast as\npossible. But the more sophisticated and complex the algorithms become, the\nless efficient they are for small sets of items due to large constant factors.\nWe aim to determine if there is a faster way than insertion sort to sort small\nsets of items to provide a more efficient base case sorter. We looked at\nsorting networks, at how they can improve the speed of sorting few elements,\nand how to implement them in an efficient manner by using conditional moves.\nSince sorting networks need to be implemented explicitly for each set size,\nproviding networks for larger sizes becomes less efficient due to increased\ncode sizes. To also enable the sorting of slightly larger base cases, we\nadapted sample sort to Register Sample Sort, to break down those larger sets\ninto sizes that can in turn be sorted by sorting networks. From our experiments\nwe found that when sorting only small sets, the sorting networks outperform\ninsertion sort by a factor of at least 1.76 for any array size between six and\nsixteen, and by a factor of 2.72 on average across all machines and array\nsizes. When integrating sorting networks as a base case sorter into Quicksort,\nwe achieved far less performance improvements, which is probably due to the\nnetworks having a larger code size and cluttering the L1 instruction cache. But\nfor x86 machines with a larger L1 instruction cache of 64 KiB or more, we\nobtained speedups of 12.7% when using sorting networks as a base case sorter in\nstd::sort. In conclusion, the desired improvement in speed could only be\nachieved under special circumstances, but the results clearly show the\npotential of using conditional moves in the field of sorting algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:22:11 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 09:03:50 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 14:49:02 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Bingmann", "Timo", ""], ["Marianczuk", "Jasper", ""], ["Sanders", "Peter", ""]]}, {"id": "2002.05600", "submitter": "Giulia Bernardini", "authors": "Giulia Bernardini, Paola Bonizzoni, Pawe{\\l} Gawrychowski", "title": "On Two Measures of Distance between Fully-Labelled Trees", "comments": "17 pages, 15 figures. To be published in the proceedings of CPM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade brought a significant increase in the amount of data and a\nvariety of new inference methods for reconstructing the detailed evolutionary\nhistory of various cancers. This brings the need of designing efficient\nprocedures for comparing rooted trees representing the evolution of mutations\nin tumor phylogenies. Bernardini et al. [CPM 2019] recently introduced a notion\nof the rearrangement distance for fully-labelled trees motivated by this\nnecessity. This notion originates from two operations: one that permutes the\nlabels of the nodes, the other that affects the topology of the tree. Each\noperation alone defines a distance that can be computed in polynomial time,\nwhile the actual rearrangement distance, that combines the two, was proven to\nbe NP-hard.\n  We answer two open question left unanswered by the previous work. First, what\nis the complexity of computing the permutation distance? Second, is there a\nconstant-factor approximation algorithm for estimating the rearrangement\ndistance between two arbitrary trees? We answer the first one by showing, via a\ntwo-way reduction, that calculating the permutation distance between two trees\non $n$ nodes is equivalent, up to polylogarithmic factors, to finding the\nlargest cardinality matching in a sparse bipartite graph. In particular, by\nplugging in the algorithm of Liu and Sidford [ArXiv 2020], we obtain an\n$O(n^{4/3+o(1)})$ time algorithm for computing the permutation distance between\ntwo trees on $n$ nodes. Then we answer the second question positively, and\ndesign a linear-time constant-factor approximation algorithm that does not need\nany assumption on the trees.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:22:26 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 11:42:55 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 13:13:37 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Bernardini", "Giulia", ""], ["Bonizzoni", "Paola", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "2002.05632", "submitter": "Vasilis Kontonis", "authors": "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos\n  Zarifis", "title": "Learning Halfspaces with Massart Noise Under Structured Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning halfspaces with Massart noise in the\ndistribution-specific PAC model. We give the first computationally efficient\nalgorithm for this problem with respect to a broad family of distributions,\nincluding log-concave distributions. This resolves an open question posed in a\nnumber of prior works. Our approach is extremely simple: We identify a smooth\n{\\em non-convex} surrogate loss with the property that any approximate\nstationary point of this loss defines a halfspace that is close to the target\nhalfspace. Given this structural result, we can use SGD to solve the underlying\nlearning problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:02:37 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2002.05710", "submitter": "Daniel Anderson", "authors": "Daniel Anderson, Guy E. Blelloch, Kanat Tangwongsan", "title": "Work-efficient Batch-incremental Minimum Spanning Trees with\n  Applications to the Sliding Window Model", "comments": null, "journal-ref": "Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '20) (2020) 51-61", "doi": "10.1145/3350755.3400241", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for dynamically maintaining minimum spanning trees (MSTs) have\nreceived much attention in both the parallel and sequential settings. While\nprevious work has given optimal algorithms for dense graphs, all existing\nparallel batch-dynamic algorithms perform polynomial work per update in the\nworst case for sparse graphs. In this paper, we present the first\nwork-efficient parallel batch-dynamic algorithm for incremental MST, which can\ninsert $\\ell$ edges in $O(\\ell \\log(1+n/\\ell))$ work in expectation and\n$O(\\text{polylog}(n))$ span w.h.p. The key ingredient of our algorithm is an\nalgorithm for constructing a compressed path tree of an edge-weighted tree,\nwhich is a smaller tree that contains all pairwise heaviest edges between a\ngiven set of marked vertices. Using our batch-incremental MST algorithm, we\ndemonstrate a range of applications that become efficiently solvable in\nparallel in the sliding-window model, such as graph connectivity, approximate\nMSTs, testing bipartiteness, $k$-certificates, cycle-freeness, and maintaining\nsparsifiers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:51:07 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""], ["Tangwongsan", "Kanat", ""]]}, {"id": "2002.05808", "submitter": "Jian Li", "authors": "Shufan Wang, Jian Li, Shiqiang Wang", "title": "Online Algorithms for Multi-shop Ski Rental with Machine Learned Advice", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of augmenting online algorithms with machine learned\n(ML) advice. In particular, we consider the \\emph{multi-shop ski rental} (MSSR)\nproblem, which is a generalization of the classical ski rental problem. In\nMSSR, each shop has different prices for buying and renting a pair of skis, and\na skier has to make decisions on when and where to buy. We obtain both\ndeterministic and randomized online algorithms with provably improved\nperformance when either a single or multiple ML predictions are used to make\ndecisions. These online algorithms have no knowledge about the quality or the\nprediction error type of the ML prediction. The performance of these online\nalgorithms are robust to the poor performance of the predictors, but improve\nwith better predictions. Extensive experiments using both synthetic and real\nworld data traces verify our theoretical observations and show better\nperformance against algorithms that purely rely on online decision making.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 22:59:36 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 00:49:52 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wang", "Shufan", ""], ["Li", "Jian", ""], ["Wang", "Shiqiang", ""]]}, {"id": "2002.06005", "submitter": "Corinna Coupette", "authors": "Corinna Coupette and Christoph Lenzen", "title": "A Breezing Proof of the KMW Bound", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their seminal paper from 2004, Kuhn, Moscibroda, and Wattenhofer (KMW)\nproved a hardness result for several fundamental graph problems in the LOCAL\nmodel: For any (randomized) algorithm, there are input graphs with $n$ nodes\nand maximum degree $\\Delta$ on which $\\Omega(\\min\\{\\sqrt{\\log n/\\log \\log\nn},\\log \\Delta/\\log \\log \\Delta\\})$ (expected) communication rounds are\nrequired to obtain polylogarithmic approximations to a minimum vertex cover,\nminimum dominating set, or maximum matching. Via reduction, this hardness\nextends to symmetry breaking tasks like finding maximal independent sets or\nmaximal matchings. Today, more than $15$ years later, there is still no proof\nof this result that is easy on the reader. Setting out to change this, in this\nwork, we provide a fully self-contained and $\\mathit{simple}$ proof of the KMW\nlower bound. The key argument is algorithmic, and it relies on an invariant\nthat can be readily verified from the generation rules of the lower bound\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 12:49:15 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 06:34:38 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 07:55:10 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 20:19:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Coupette", "Corinna", ""], ["Lenzen", "Christoph", ""]]}, {"id": "2002.06037", "submitter": "Xiaowei Wu", "authors": "Zhihao Gavin Tang, Xiaowei Wu, Yuhao Zhang", "title": "A Simple 1-1/e Approximation for Oblivious Bipartite Matching", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the oblivious matching problem, which aims at finding a maximum\nmatching on a graph with unknown edge set. Any algorithm for the problem\nspecifies an ordering of the vertex pairs. The matching is then produced by\nprobing the pairs following the ordering, and including a pair if both of them\nare unmatched and there exists an edge between them. The unweighted (Chan et\nal. (SICOMP 2018)) and the vertex-weighted (Chan et al. (TALG 2018)) versions\nof the problem are well studied.\n  In this paper, we consider the edge-weighted oblivious matching problem on\nbipartite graphs, which generalizes the stochastic bipartite matching problem.\nVery recently, Gamlath et al. (SODA 2019) studied the stochastic bipartite\nmatching problem, and proposed an (1-1/e)-approximate algorithm. We give a very\nsimple algorithm adapted from the Ranking algorithm by Karp et al. (STOC 1990),\nand show that it achieves the same (1-1/e) approximation ratio for the\noblivious matching problem on bipartite graph.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 13:53:48 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""]]}, {"id": "2002.06078", "submitter": "Ignasi Sau", "authors": "R\\'emy Belmonte, Ignasi Sau", "title": "On the complexity of finding large odd induced subgraphs and odd\n  colorings", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the problems of finding, given a graph $G$, a\nlargest induced subgraph of $G$ with all degrees odd (called an odd subgraph),\nand the smallest number of odd subgraphs that partition $V(G)$. We call these\nparameters ${\\sf mos}(G)$ and $\\chi_{{\\sf odd}}(G)$, respectively. We prove\nthat deciding whether $\\chi_{{\\sf odd}}(G) \\leq q$ is polynomial-time solvable\nif $q \\leq 2$, and NP-complete otherwise. We provide algorithms in time\n$2^{O({\\sf rw})} \\cdot n^{O(1)}$ and $2^{O(q \\cdot {\\sf rw})} \\cdot n^{O(1)}$\nto compute ${\\sf mos}(G)$ and to decide whether $\\chi_{{\\sf odd}}(G) \\leq q$ on\n$n$-vertex graphs of rank-width at most ${\\sf rw}$, respectively, and we prove\nthat the dependency on rank-width is asymptotically optimal under the ETH.\nFinally, we give some tight bounds for these parameters on restricted graph\nclasses or in relation to other parameters.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:31:51 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 22:21:05 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Sau", "Ignasi", ""]]}, {"id": "2002.06265", "submitter": "Julian Pape-Lange", "authors": "Julian Pape-Lange", "title": "On Extensions of Maximal Repeats in Compressed Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an upper bound for several subsets of maximal repeats and\nmaximal pairs in compressed strings and also presents a formerly unknown\nrelationship between maximal pairs and the run-length Burrows-Wheeler\ntransform.\n  This relationship is used to obtain a different proof for the Burrows-Wheeler\nconjecture which has recently been proven by Kempa and Kociumaka in \"Resolution\nof the Burrows-Wheeler Transform Conjecture\".\n  More formally, this paper proves that a string $S$ with $z$ LZ77-factors and\nwithout $q$-th powers has at most $73(\\log_2 |S|)(z+2)^2$ runs in the\nrun-length Burrows-Wheeler transform and the number of arcs in the compacted\ndirected acyclic word graph of $S$ is bounded from above by $18q(1+\\log_q\n|S|)(z+2)^2$.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:02:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Pape-Lange", "Julian", ""]]}, {"id": "2002.06296", "submitter": "Adiel Statman", "authors": "Vladimir Braverman, Dan Feldman, Harry Lang, Daniela Rus, Adiel\n  Statman", "title": "Sparse Coresets for SVD on Infinite Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In streaming Singular Value Decomposition (SVD), $d$-dimensional rows of a\npossibly infinite matrix arrive sequentially as points in $\\mathbb{R}^d$. An\n$\\epsilon$-coreset is a (much smaller) matrix whose sum of square distances of\nthe rows to any hyperplane approximates that of the original matrix to a $1 \\pm\n\\epsilon$ factor. Our main result is that we can maintain a $\\epsilon$-coreset\nwhile storing only $O(d \\log^2 d / \\epsilon^2)$ rows. Known lower bounds of\n$\\Omega(d / \\epsilon^2)$ rows show that this is nearly optimal. Moreover, each\nrow of our coreset is a weighted subset of the input rows. This is highly\ndesirable since it: (1) preserves sparsity; (2) is easily interpretable; (3)\navoids precision errors; (4) applies to problems with constraints on the input.\nPrevious streaming results for SVD that return a subset of the input required\nstoring $\\Omega(d \\log^3 n / \\epsilon^2)$ rows where $n$ is the number of rows\nseen so far. Our algorithm, with storage independent of $n$, is the first\nresult that uses finite memory on infinite streams. We support our findings\nwith experiments on the Wikipedia dataset benchmarked against state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 01:29:40 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 17:52:23 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 18:48:58 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Braverman", "Vladimir", ""], ["Feldman", "Dan", ""], ["Lang", "Harry", ""], ["Rus", "Daniela", ""], ["Statman", "Adiel", ""]]}, {"id": "2002.06421", "submitter": "Abu Reyan Ahmed", "authors": "Reyan Ahmed, Faryad Darabi Sahneh, Keaton Hamm, Stephen Kobourov,\n  Richard Spence", "title": "Kruskal-based approximation algorithm for the multi-level Steiner tree\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-level Steiner tree problem: a generalization of the\nSteiner tree problem in graphs where terminals $T$ require varying priority,\nlevel, or quality of service. In this problem, we seek to find a minimum cost\ntree containing edges of varying rates such that any two terminals $u$, $v$\nwith priorities $P(u)$, $P(v)$ are connected using edges of rate\n$\\min\\{P(u),P(v)\\}$ or better. The case where edge costs are proportional to\ntheir rate is approximable to within a constant factor of the optimal solution.\nFor the more general case of non-proportional costs, this problem is hard to\napproximate with ratio $c \\log \\log n$, where $n$ is the number of vertices in\nthe graph. A simple greedy algorithm by Charikar et al., however, provides a\n$\\min\\{2(\\ln |T|+1), \\ell \\rho\\}$-approximation in this setting, where $\\rho$\nis an approximation ratio for a heuristic solver for the Steiner tree problem\nand $\\ell$ is the number of priorities or levels (Byrka et al. give a Steiner\ntree algorithm with $\\rho\\approx 1.39$, for example).\n  In this paper, we describe a natural generalization to the multi-level case\nof the classical (single-level) Steiner tree approximation algorithm based on\nKruskal's minimum spanning tree algorithm. We prove that this algorithm\nachieves an approximation ratio at least as good as Charikar et al., and\nexperimentally performs better with respect to the optimum solution. We develop\nan integer linear programming formulation to compute an exact solution for the\nmulti-level Steiner tree problem with non-proportional edge costs and use it to\nevaluate the performance of our algorithm on both random graphs and multi-level\ninstances derived from SteinLib.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 18:17:02 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:00:59 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Ahmed", "Reyan", ""], ["Sahneh", "Faryad Darabi", ""], ["Hamm", "Keaton", ""], ["Kobourov", "Stephen", ""], ["Spence", "Richard", ""]]}, {"id": "2002.06650", "submitter": "Alejandro Flores Velazco", "authors": "Alejandro Flores-Velazco, David M. Mount", "title": "Coresets for the Nearest-Neighbor Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a training set $P$ of labeled points, the nearest-neighbor rule\npredicts the class of an unlabeled query point as the label of its closest\npoint in the set. To improve the time and space complexity of classification, a\nnatural question is how to reduce the training set without significantly\naffecting the accuracy of the nearest-neighbor rule. Nearest-neighbor\ncondensation deals with finding a subset $R \\subseteq P$ such that for every\npoint $p \\in P$, $p$'s nearest-neighbor in $R$ has the same label as $p$. This\nrelates to the concept of coresets, which can be broadly defined as subsets of\nthe set, such that an exact result on the coreset corresponds to an approximate\nresult on the original set. However, the guarantees of a coreset hold for any\nquery point, and not only for the points of the training set.\n  This paper introduces the concept of coresets for nearest-neighbor\nclassification. We extend existing criteria used for condensation, and prove\nsufficient conditions to correctly classify any query point when using these\nsubsets. Additionally, we prove that finding such subsets of minimum\ncardinality is NP-hard, and propose quadratic-time approximation algorithms\nwith provable upper-bounds on the size of their selected subsets. Moreover, we\nshow how to improve one of these algorithms to have subquadratic runtime, being\nthe first of this kind for condensation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:00:48 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 21:07:11 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 19:14:41 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Flores-Velazco", "Alejandro", ""], ["Mount", "David M.", ""]]}, {"id": "2002.06653", "submitter": "Caleb Helbling", "authors": "Caleb Helbling", "title": "Directed Graph Hashing", "comments": "Presented at 51st Southeastern International Conference on\n  Combinatorics, Graph Theory & Computing, to be published in Congressus\n  Numerantium, December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents an algorithm for structurally hashing directed graphs.\nThe algorithm seeks to fulfill the recursive principle that a hash of a node\nshould depend only on the hash of its neighbors. The algorithm works even in\nthe presence of cycles, which prevents a naive recursive algorithm from\nfunctioning. We also discuss the implications of the recursive principle,\nlimitations of the algorithm, and potential use cases.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:08:14 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Helbling", "Caleb", ""]]}, {"id": "2002.06742", "submitter": "Ali Vakilian", "authors": "Sepideh Mahabadi and Ali Vakilian", "title": "Individual Fairness for $k$-Clustering", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a local search based algorithm for $k$-median and $k$-means (and more\ngenerally for any $k$-clustering with $\\ell_p$ norm cost function) from the\nperspective of individual fairness. More precisely, for a point $x$ in a point\nset $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of\nradius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively,\nif a set of $k$ random points are chosen from $P$ as centers, every point $x\\in\nP$ expects to have a center within radius $r(x)$. An individually fair\nclustering provides such a guarantee for every point $x\\in P$. This notion of\nfairness was introduced in [Jung et al., 2019] where they showed how to get an\napproximately feasible $k$-clustering with respect to this fairness condition.\n  In this work, we show how to get a bicriteria approximation for fair\n$k$-clustering: The $k$-median ($k$-means) cost of our solution is within a\nconstant factor of the cost of an optimal fair $k$-clustering, and our solution\napproximately satisfies the fairness condition (also within a constant factor).\nFurther, we complement our theoretical bounds with empirical evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 02:31:13 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 18:31:09 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mahabadi", "Sepideh", ""], ["Vakilian", "Ali", ""]]}, {"id": "2002.06762", "submitter": "Lawrence Li", "authors": "Seth Gilbert, Lawrence Li", "title": "How fast can you update your MST? (Dynamic algorithms for cluster\n  computing)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a large graph that is being processed by a cluster of computers,\ne.g., described by the $k$-machine model or the Massively Parallel Computation\nModel. The graph, however, is not static; instead it is receiving a constant\nstream of updates. How fast can the cluster process the stream of updates? The\nfundamental question we want to ask in this paper is whether we can update the\ngraph fast enough to keep up with the stream. We focus specifically on the\nproblem of maintaining a minimum spanning tree (MST), and we give an algorithm\nfor the $k$-machine model that can process $O(k)$ graph updates per $O(1)$\nrounds with high probability. (And these results carry over to the Massively\nParallel Computation (MPC) model.) We also show a lower bound, i.e., it is\nimpossible to process $k^{1+\\epsilon}$ updates in $O(1)$ rounds. Thus we\nprovide a nearly tight answer to the question of how fast a cluster can respond\nto a stream of graph modifications while maintaining an MST.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:08:14 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gilbert", "Seth", ""], ["Li", "Lawrence", ""]]}, {"id": "2002.06764", "submitter": "Natsumi Kikuchi", "authors": "Natsumi Kikuchi, Diptarama Hendrian, Ryo Yoshinaka, and Ayumi\n  Shinohara", "title": "Computing Covers under Substring Consistent Equivalence Relations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covers are a kind of quasiperiodicity in strings. A string $C$ is a cover of\nanother string $T$ if any position of $T$ is inside some occurrence of $C$ in\n$T$. The shortest and longest cover arrays of $T$ have the lengths of the\nshortest and longest covers of each prefix of $T$, respectively. The literature\nhas proposed linear-time algorithms computing longest and shortest cover arrays\ntaking border arrays as input. An equivalence relation $\\approx$ over strings\nis called a substring consistent equivalence relation (SCER) iff $X \\approx Y$\nimplies (1) $|X| = |Y|$ and (2) $X[i:j] \\approx Y[i:j]$ for all $1 \\le i \\le j\n\\le |X|$. In this paper, we generalize the notion of covers for SCERs and prove\nthat existing algorithms to compute the shortest cover array and the longest\ncover array of a string $T$ under the identity relation will work for any SCERs\ntaking the accordingly generalized border arrays.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:16:05 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 08:05:50 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kikuchi", "Natsumi", ""], ["Hendrian", "Diptarama", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "2002.06786", "submitter": "Katsuhito Nakashima", "authors": "Katsuhito Nakashima, Noriki Fujisato, Diptarama Hendrian, Yuto\n  Nakashima, Ryo Yoshinaka, Shunsuke Inenaga, Hideo Bannai, Ayumi Shinohara,\n  Masayuki Takeda", "title": "DAWGs for parameterized matching: online construction and related\n  indexing structures", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two strings $x$ and $y$ over $\\Sigma \\cup \\Pi$ of equal length are said to\nparameterized match (p-match) if there is a renaming bijection $f:\\Sigma \\cup\n\\Pi \\rightarrow \\Sigma \\cup \\Pi$ that is identity on $\\Sigma$ and transforms\n$x$ to $y$ (or vice versa). The p-matching problem is to look for substrings in\na text that p-match a given pattern. In this paper, we propose parameterized\nsuffix automata (p-suffix automata) and parameterized directed acyclic word\ngraphs (PDAWGs) which are the p-matching versions of suffix automata and DAWGs.\nWhile suffix automata and DAWGs are equivalent for standard strings, we show\nthat p-suffix automata can have $\\Theta(n^2)$ nodes and edges but PDAWGs have\nonly $O(n)$ nodes and edges, where $n$ is the length of an input string. We\nalso give $O(n |\\Pi| \\log (|\\Pi| + |\\Sigma|))$-time $O(n)$-space algorithm that\nbuilds the PDAWG in a left-to-right online manner. As a byproduct, it is shown\nthat the parameterized suffix tree for the reversed string can also be built in\nthe same time and space, in a right-to-left online manner. We also discuss\nparameterized compact DAWGs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:08:01 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 04:32:51 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nakashima", "Katsuhito", ""], ["Fujisato", "Noriki", ""], ["Hendrian", "Diptarama", ""], ["Nakashima", "Yuto", ""], ["Yoshinaka", "Ryo", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Shinohara", "Ayumi", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2002.06796", "submitter": "Mitsuru Funakoshi", "authors": "Mitsuru Funakoshi, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda, and Ayumi Shinohara", "title": "Detecting $k$-(Sub-)Cadences and Equidistant Subsequence Occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The equidistant subsequence pattern matching problem is considered. Given a\npattern string $P$ and a text string $T$, we say that $P$ is an\n\\emph{equidistant subsequence} of $T$ if $P$ is a subsequence of the text such\nthat consecutive symbols of $P$ in the occurrence are equally spaced. We can\nconsider the problem of equidistant subsequences as generalizations of\n(sub-)cadences. We give bit-parallel algorithms that yield $o(n^2)$ time\nalgorithms for finding $k$-(sub-)cadences and equidistant subsequences.\nFurthermore, $O(n\\log^2 n)$ and $O(n\\log n)$ time algorithms, respectively for\nequidistant and Abelian equidistant matching for the case $|P| = 3$, are shown.\nThe algorithms make use of a technique that was recently introduced which can\nefficiently compute convolutions with linear constraints.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:39:57 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Funakoshi", "Mitsuru", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "2002.06812", "submitter": "Hanlin Ren", "authors": "Ran Duan, Yong Gu and Hanlin Ren", "title": "Approximate Distance Oracles Subject to Multiple Vertex Failures", "comments": "SODA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G=(V,E)$ of $n$ vertices and $m$ edges with\nweights in $[1,W]$, we construct vertex sensitive distance oracles (VSDO),\nwhich are data structures that preprocess the graph, and answer the following\nkind of queries: Given a source vertex $u$, a target vertex $v$, and a batch of\n$d$ failed vertices $D$, output (an approximation of) the distance between $u$\nand $v$ in $G-D$ (that is, the graph $G$ with vertices in $D$ removed). An\noracle has stretch $\\alpha$ if it always holds that\n$\\delta_{G-D}(u,v)\\le\\tilde{\\delta}(u,v)\\le\\alpha\\cdot\\delta_{G-D}(u,v)$, where\n$\\delta_{G-D}(u,v)$ is the actual distance between $u$ and $v$ in $G-D$, and\n$\\tilde{\\delta}(u,v)$ is the distance reported by the oracle.\n  In this paper we construct efficient VSDOs for any number $d$ of failures.\nFor any constant $c\\geq 1$, we propose two oracles:\n  $\\bullet$ The first oracle has size $n^{2+1/c}(\\log n/\\epsilon)^{O(d)}\\cdot\n\\log W$, answers a query in ${\\rm poly}(\\log n,d^c,\\log\\log W,\\epsilon^{-1})$\ntime, and has stretch $1+\\epsilon$, for any constant $\\epsilon>0$.\n  $\\bullet$ The second oracle has size $n^{2+1/c}{\\rm poly}(\\log (nW),d)$,\nanswers a query in ${\\rm poly}(\\log n,d^c,\\log\\log W)$ time, and has stretch\n${\\rm poly}(\\log n,d)$.\n  Both of these oracles can be preprocessed in time polynomial in their space\ncomplexity. These results are the first approximate distance oracles of\npoly-logarithmic query time for any constant number of vertex failures in\ngeneral undirected graphs. Previously there are $(1+\\epsilon)$-approximate\n$d$-edge sensitive distance oracles [Chechik et al. 2017] answering distance\nqueries when $d$ edges fail, which have size $O(n^2(\\log n/\\epsilon)^d\\cdot\nd\\log W)$ and query time ${\\rm poly}(\\log n, d, \\log\\log W)$.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:40:45 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 13:02:31 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Duan", "Ran", ""], ["Gu", "Yong", ""], ["Ren", "Hanlin", ""]]}, {"id": "2002.06863", "submitter": "Ben Berger", "authors": "Ben Berger, Alon Eden and Michal Feldman", "title": "On the Power and Limits of Dynamic Pricing in Combinatorial Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power and limits of optimal dynamic pricing in combinatorial\nmarkets; i.e., dynamic pricing that leads to optimal social welfare. Previous\nwork by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal\ndynamic prices for unit-demand buyers, and showed a market with coverage\nvaluations that admits no such prices. However, finding the frontier of markets\n(i.e., valuation functions) that admit optimal dynamic prices remains an open\nproblem. In this work we establish positive and negative results that narrow\nthe existing gap.\n  On the positive side, we provide tools for handling markets beyond\nunit-demand valuations. In particular, we characterize all optimal allocations\nin multi-demand markets. This characterization allows us to partition the items\ninto equivalence classes according to the role they play in achieving\noptimality. Using these tools, we provide a poly-time optimal dynamic pricing\nalgorithm for up to $3$ multi-demand buyers.\n  On the negative side, we establish a maximal domain theorem, showing that for\nevery non-gross substitutes valuation, there exist unit-demand valuations such\nthat adding them yields a market that does not admit an optimal dynamic\npricing. This result is the dynamic pricing equivalent of the seminal maximal\ndomain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang\n[JET'17] discovered an error in their original proof, and established a\ndifferent, incomparable version of their maximal domain theorem. En route to\nour maximal domain theorem for optimal dynamic pricing, we provide the first\ncomplete proof of the original theorem by Gul and Stacchetti.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 09:49:26 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 08:33:32 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 14:31:59 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Berger", "Ben", ""], ["Eden", "Alon", ""], ["Feldman", "Michal", ""]]}, {"id": "2002.06887", "submitter": "Niklas Troost", "authors": "Markus Chimani, Niklas Troost and Tilo Wiedera", "title": "Approximating Multistage Matching Problems", "comments": "The definitive version will appear at IWOCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multistage perfect matching problems we are given a sequence of graphs on\nthe same vertex set and asked to find a sequence of perfect matchings,\ncorresponding to the sequence of graphs, such that consecutive matchings are as\nsimilar as possible. More precisely, we aim to maximize the intersections, or\nminimize the unions between consecutive matchings. We show that these problems\nare NP-hard even in very restricted scenarios. We propose new approximation\nalgorithms and present methods to transfer results between different problem\nvariants without loosing approximation guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 11:09:51 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 08:37:17 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 14:02:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chimani", "Markus", ""], ["Troost", "Niklas", ""], ["Wiedera", "Tilo", ""]]}, {"id": "2002.06912", "submitter": "R. Krithika", "authors": "Jasine Babu, Ajay Saju Jacob, R. Krithika, and Deepak Rajendraprasad", "title": "A Note on Arc-Disjoint Cycles in Bipartite Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for each non-negative integer k, every bipartite tournament\neither contains k arc-disjoint cycles or has a feedback arc set of size at most\n7(k - 1).\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 12:25:02 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Babu", "Jasine", ""], ["Jacob", "Ajay Saju", ""], ["Krithika", "R.", ""], ["Rajendraprasad", "Deepak", ""]]}, {"id": "2002.06948", "submitter": "Alexander Noe", "authors": "Monika Henzinger, Alexander Noe, Christian Schulz and Darren Strash", "title": "Finding All Global Minimum Cuts In Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practically efficient algorithm that finds all global minimum\ncuts in huge undirected graphs. Our algorithm uses a multitude of kernelization\nrules to reduce the graph to a small equivalent instance and then finds all\nminimum cuts using an optimized version of the algorithm of Nagamochi, Nakao\nand Ibaraki. In shared memory we are able to find all minimum cuts of graphs\nwith up to billions of edges and millions of minimum cuts in a few minutes. We\nalso give a new linear time algorithm to find the most balanced minimum cuts\ngiven as input the representation of all minimum cuts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:30:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "2002.06957", "submitter": "Lale Ozkahya", "authors": "Taha Sevim, Muhammet Sel\\c{c}uk G\\\"uvel, and Lale \\\"Ozkahya", "title": "A Fast Counting Method for 6-motifs with Low Connectivity", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-43120-4_25", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-motif (or graphlet) is a subgraph on $k$ nodes in a graph or network.\nCounting of motifs in complex networks has been a well-studied problem in\nnetwork analysis of various real-word graphs arising from the study of social\nnetworks and bioinformatics. In particular, the triangle counting problem has\nreceived much attention due to its significance in understanding the behavior\nof social networks. Similarly, subgraphs with more than 3 nodes have received\nmuch attention recently. While there have been successful methods developed on\nthis problem, most of the existing algorithms are not scalable to large\nnetworks with millions of nodes and edges.\n  The main contribution of this paper is a preliminary study that genaralizes\nthe exact counting algorithm provided by Pinar, Seshadhri and Vishal to a\ncollection of 6-motifs. This method uses the counts of motifs with smaller size\nto obtain the counts of 6-motifs with low connecivity, that is, containing a\ncut-vertex or a cut-edge. Therefore, it circumvents the combinatorial explosion\nthat naturally arises when counting subgraphs in large networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:51:39 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Sevim", "Taha", ""], ["G\u00fcvel", "Muhammet Sel\u00e7uk", ""], ["\u00d6zkahya", "Lale", ""]]}, {"id": "2002.06960", "submitter": "Gregorio Quintana-Ort\\'i", "authors": "Nathan Heavner, Per-Gunnar Martinsson, Gregorio Quintana-Ort\\'i", "title": "Computing rank-revealing factorizations of matrices stored out-of-core", "comments": "23 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CL cs.DC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes efficient algorithms for computing rank-revealing\nfactorizations of matrices that are too large to fit in RAM, and must instead\nbe stored on slow external memory devices such as solid-state or spinning disk\nhard drives (out-of-core or out-of-memory). Traditional algorithms for\ncomputing rank revealing factorizations, such as the column pivoted QR\nfactorization, or techniques for computing a full singular value decomposition\nof a matrix, are very communication intensive. They are naturally expressed as\na sequence of matrix-vector operations, which become prohibitively expensive\nwhen data is not available in main memory. Randomization allows these methods\nto be reformulated so that large contiguous blocks of the matrix can be\nprocessed in bulk. The paper describes two distinct methods. The first is a\nblocked version of column pivoted Householder QR, organized as a \"left-looking\"\nmethod to minimize the number of write operations (which are more expensive\nthan read operations on a spinning disk drive). The second method results in a\nso called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where\n$U$ and $V$ are unitary, and $T$ is triangular. This method is organized as an\nalgorithm-by-blocks, in which floating point operations overlap read and write\noperations. The second method incorporates power iterations, and is\nexceptionally good at revealing the numerical rank; it can often be used as a\nsubstitute for a full singular value decomposition. Numerical experiments\ndemonstrate that the new algorithms are almost as fast when processing data\nstored on a hard drive as traditional algorithms are for data stored in main\nmemory. To be precise, the computational time for fully factorizing an $n\\times\nn$ matrix scales as $cn^{3}$, with a scaling constant $c$ that is only\nmarginally larger when the matrix is stored out of core.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:58:08 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 12:18:40 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Heavner", "Nathan", ""], ["Martinsson", "Per-Gunnar", ""], ["Quintana-Ort\u00ed", "Gregorio", ""]]}, {"id": "2002.06968", "submitter": "Federico Fusco", "authors": "Shant Boodaghians, Federico Fusco, Philip Lazos, Stefano Leonardi", "title": "Pandora's Box Problem with Order Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pandora's Box Problem, originally formalized by Weitzman in 1979, models\nselection from set of random, alternative options, when evaluation is costly.\nThis includes, for example, the problem of hiring a skilled worker, where only\none hire can be made, but the evaluation of each candidate is an expensive\nprocedure. Weitzman showed that the Pandora's Box Problem admits an elegant,\nsimple solution, where the options are considered in decreasing order of\nreservation value,i.e., the value that reduces to zero the expected marginal\ngain for opening the box. We study for the first time this problem when order -\nor precedence - constraints are imposed between the boxes. We show that,\ndespite the difficulty of defining reservation values for the boxes which take\ninto account both in-depth and in-breath exploration of the various options,\ngreedy optimal strategies exist and can be efficiently computed for tree-like\norder constraints. We also prove that finding approximately optimal adaptive\nsearch strategies is NP-hard when certain matroid constraints are used to\nfurther restrict the set of boxes which may be opened, or when the order\nconstraints are given as reachability constraints on a DAG. We complement the\nabove result by giving approximate adaptive search strategies based on a\nconnection between optimal adaptive strategies and non-adaptive strategies with\nbounded adaptivity gap for a carefully relaxed version of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 14:18:56 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 11:15:38 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Boodaghians", "Shant", ""], ["Fusco", "Federico", ""], ["Lazos", "Philip", ""], ["Leonardi", "Stefano", ""]]}, {"id": "2002.06997", "submitter": "Daniel Neuen", "authors": "Daniel Neuen", "title": "Hypergraph Isomorphism for Groups with Restricted Composition Factors", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the isomorphism problem for hypergraphs taking as input two\nhypergraphs over the same set of vertices $V$ and a permutation group $\\Gamma$\nover domain $V$, and asking whether there is a permutation $\\gamma \\in \\Gamma$\nthat proves the two hypergraphs to be isomorphic. We show that for input\ngroups, all of whose composition factors are isomorphic to a subgroup of the\nsymmetric group on $d$ points, this problem can be solved in time\n$(n+m)^{O((\\log d)^{c})}$ for some absolute constant $c$ where $n$ denotes the\nnumber of vertices and $m$ the number of hyperedges. In particular, this gives\nthe currently fastest isomorphism test for hypergraphs in general. The previous\nbest algorithm for this problem due to Schweitzer and Wiebking (STOC 2019) runs\nin time $n^{O(d)}m^{O(1)}$.\n  As an application of this result, we obtain, for example, an algorithm\ntesting isomorphism of graphs excluding $K_{3,h}$ ($h \\geq 3$) as a minor in\ntime $n^{O((\\log h)^{c})}$. In particular, this gives an isomorphism test for\ngraphs of Euler genus at most $g$ running in time $n^{O((\\log g)^{c})}$.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 15:15:43 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Neuen", "Daniel", ""]]}, {"id": "2002.07012", "submitter": "Mirko H. Wagner", "authors": "Fritz B\\\"okler and Markus Chimani and Mirko H. Wagner and Tilo Wiedera", "title": "An Experimental Study of ILP Formulations for the Longest Induced Path\n  Problem", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-53262-8_8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$, the longest induced path problem asks for a maximum\ncardinality node subset $W\\subseteq V$ such that the graph induced by $W$ is a\npath. It is a long established problem with applications, e.g., in network\nanalysis. We propose novel integer linear programming (ILP) formulations for\nthe problem and discuss efficient implementations thereof. Comparing them with\nknown formulations from literature, we prove that they are beneficial in\ntheory, yielding stronger relaxations. Moreover, our experiments show their\npractical superiority.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 15:46:57 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 11:51:11 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 14:32:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["B\u00f6kler", "Fritz", ""], ["Chimani", "Markus", ""], ["Wagner", "Mirko H.", ""], ["Wiedera", "Tilo", ""]]}, {"id": "2002.07152", "submitter": "Abu Reyan Ahmed", "authors": "Reyan Ahmed, Greg Bodwin, Faryad Darabi Sahneh, Stephen Kobourov,\n  Richard Spence", "title": "Weighted Additive Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A \\emph{spanner} of a graph $G$ is a subgraph $H$ that approximately\npreserves shortest path distances in $G$. Spanners are commonly applied to\ncompress computation on metric spaces corresponding to weighted input graphs.\nClassic spanner constructions can seamlessly handle edge weights, so long as\nerror is measured \\emph{multiplicatively}. In this work, we investigate whether\none can similarly extend constructions of spanners with purely \\emph{additive}\nerror to weighted graphs. These extensions are not immediate, due to a key\nlemma about the size of shortest path neighborhoods that fails for weighted\ngraphs. Despite this, we recover a suitable amortized version, which lets us\nprove direct extensions of classic $+2$ and $+4$ unweighted spanners (both\nall-pairs and pairwise) to $+2W$ and $+4W$ weighted spanners, where $W$ is the\nmaximum edge weight. Specifically, we show that a weighted graph $G$ contains\nall-pairs (pairwise) $+2W$ and $+4W$ weighted spanners of size $O(n^{3/2})$ and\n$\\widetilde{O}(n^{7/5})$ ($O(np^{1/3})$ and $O(np^{2/7})$) respectively. For a\ntechnical reason, the $+6$ unweighted spanner becomes a $+8W$ weighted spanner;\nclosing this error gap is an interesting remaining open problem. That is, we\nshow that $G$ contains all-pairs (pairwise) $+8W$ weighted spanners of size\n$O(n^{4/3})$ ($O(np^{1/4})$).\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 17:33:05 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 01:40:29 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 23:38:45 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2020 01:29:11 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 04:12:36 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ahmed", "Reyan", ""], ["Bodwin", "Greg", ""], ["Sahneh", "Faryad Darabi", ""], ["Kobourov", "Stephen", ""], ["Spence", "Richard", ""]]}, {"id": "2002.07207", "submitter": "Asish Mukhopadhyay", "authors": "Md. Zamilur Rahman and Asish Mukhopadhyay", "title": "Semi-dynamic Algorithms for Strongly Chordal Graphs", "comments": "12 pages and 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an extensive literature on dynamic algorithms for a large number of\ngraph theoretic problems, particularly for all varieties of shortest path\nproblems. Germane to this paper are a number fully dynamic algorithms that are\nknown for chordal graphs. However, to the best of our knowledge no study has\nbeen done for the problem of dynamic algorithms for strongly chordal graphs. To\naddress this gap, in this paper, we propose a semi-dynamic algorithm for\nedge-deletions and a semi-dynamic algorithm for edge-insertions in a strongly\nchordal graph, $G = (V, E)$, on $n$ vertices and $m$ edges. The query\ncomplexity of an edge-deletion is $O(d_u^2d_v^2 (n + m))$, where $d_u$ and\n$d_v$ are the degrees of the vertices $u$ and $v$ of the candidate edge $\\{u,\nv\\}$, while the query-complexity of an edge-insertion is $O(n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 01:20:09 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Rahman", "Md. Zamilur", ""], ["Mukhopadhyay", "Asish", ""]]}, {"id": "2002.07211", "submitter": "Pedro Paredes", "authors": "Pedro Paredes", "title": "Spectrum preserving short cycle removal on regular graphs", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method to remove short cycles on regular graphs while\nmaintaining spectral bounds (the nontrivial eigenvalues of the adjacency\nmatrix), as long as the graphs have certain combinatorial properties. These\ncombinatorial properties are related to the number and distance between short\ncycles and are known to happen with high probability in uniformly random\nregular graphs.\n  Using this method we can show two results involving high girth spectral\nexpander graphs. First, we show that given $d \\geq 3$ and $n$, there exists an\nexplicit distribution of $d$-regular $\\Theta(n)$-vertex graphs where with high\nprobability its samples have girth $\\Omega(\\log_{d - 1} n)$ and are\n$\\epsilon$-near-Ramanujan; i.e., its eigenvalues are bounded in magnitude by\n$2\\sqrt{d - 1} + \\epsilon$ (excluding the single trivial eigenvalue of $d$).\nThen, for every constant $d \\geq 3$ and $\\epsilon > 0$, we give a deterministic\npoly$(n)$-time algorithm that outputs a $d$-regular graph on\n$\\Theta(n)$-vertices that is $\\epsilon$-near-Ramanujan and has girth\n$\\Omega(\\sqrt{\\log n})$, based on the work of arXiv:1909.06988 .\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 19:16:15 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Paredes", "Pedro", ""]]}, {"id": "2002.07249", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Integrating products of quadratic forms", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.MG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that if $q_1, \\ldots, q_m: {\\Bbb R}^n \\longrightarrow {\\Bbb R}$ are\nquadratic forms in variables $x_1, \\ldots, x_n$ such that each $q_k$ depends on\nat most $r$ variables and each $q_k$ has common variables with at most $r$\nother forms, then the average value of the product $\\left(1+ q_1\\right) \\cdots\n\\left(1+q_m\\right)$ with respect to the standard Gaussian measure in ${\\Bbb\nR}^n$ can be approximated within relative error $\\epsilon >0$ in\nquasi-polynomial $n^{O(1)} m^{O(\\ln m -\\ln \\epsilon)}$ time, provided $|q_k(x)|\n\\leq \\gamma \\|x\\|^2 /r$ for some absolute constant $\\gamma > 0$ and $k=1,\n\\ldots, m$. When $q_k$ are interpreted as pairwise squared distances for\nconfigurations of points in Euclidean space, the average can be interpreted as\nthe partition function of systems of particles with mollified logarithmic\npotentials. We sketch a possible application to testing the feasibility of\nsystems of real quadratic equations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:04:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "2002.07287", "submitter": "Andrej Sajenko", "authors": "Frank Kammer and Andrej Sajenko", "title": "Sorting and Ranking of Self-Delimiting Numbers with Applications to Tree\n  Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that an $N$-bit sequence $S$ of $k$ self-delimiting numbers is given\nas input. We present space-efficient algorithms for sorting, dense ranking and\n(competitive) ranking $S$ on the word RAM model with word size $\\Omega(\\log N)$\nbits. Our algorithms run in $O(k + \\frac{N}{\\log N})$ time and use $O(N)$ bits.\nThe sorting algorithm returns the given numbers in sorted order, stored within\na bit-vector of $N$ bits, whereas our ranking algorithms construct data\nstructures that allow us subsequently to return the (dense) rank of each number\n$x$ in $S$ in constant time if the position of $x$ in $S$ is given together\nwith $x$.\n  As an application of our algorithms we give an algorithm for tree\nisomorphism, which runs in $O(n)$ time and uses $O(n)$ bits on $n$-node trees.\nThe previous best linear-time algorithm for tree isomorphism uses $\\Theta(n\n\\log n)$ bits.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 22:39:00 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 10:17:08 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kammer", "Frank", ""], ["Sajenko", "Andrej", ""]]}, {"id": "2002.07336", "submitter": "Gergely Odor", "authors": "Victor Lecomte, Gergely \\'Odor, Patrick Thiran", "title": "Noisy source location on a line", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of locating the source of an epidemic diffusion process\nfrom a sparse set of sensors, under noise. In a graph $G=(V,E)$, an unknown\nsource node $v^* \\in V$ is drawn uniformly at random, and unknown edge weights\n$w(e)$ for $e\\in E$, representing the propagation delays along the edges, are\ndrawn independently from a Gaussian distribution of mean $1$ and variance\n$\\sigma^2$. An algorithm then attempts to locate $v^*$ by picking sensor (also\ncalled query) nodes $s \\in V$ and being told the length of the shortest path\nbetween $s$ and $v^*$ in graph $G$ weighted by $w$. We consider two settings:\nstatic, in which all query nodes must be decided in advance, and sequential, in\nwhich each query can depend on the results of the previous ones.\n  We characterize the query complexity when $G$ is an $n$-node path. In the\nstatic setting, $\\Theta(n\\sigma^2)$ queries are needed for $\\sigma^2 \\leq 1$,\nand $\\Theta(n)$ for $\\sigma^2 \\geq 1$. In the sequential setting, somewhat\nsurprisingly, only $\\Theta(\\log\\log_{1/\\sigma}n)$ are needed when $\\sigma^2\n\\leq 1/2$, and $\\Theta(\\log \\log n)+O_\\sigma(1)$ when $\\sigma^2 \\geq 1/2$. This\nis the first mathematical study of source location under non-trivial amounts of\nnoise.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 02:14:56 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 17:03:02 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Lecomte", "Victor", ""], ["\u00d3dor", "Gergely", ""], ["Thiran", "Patrick", ""]]}, {"id": "2002.07342", "submitter": "Sai Satwik Kuppili", "authors": "Sai Satwik Kuppili and Bhadrachalam Chitturi", "title": "An Upper Bound for Sorting $R_n$ with LRE", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A permutation $\\pi$ over alphabet $\\Sigma = {1,2,3,\\ldots,n}$, is a sequence\nwhere every element $x$ in $\\Sigma$ occurs exactly once. $S_n$ is the symmetric\ngroup consisting of all permutations of length $n$ defined over $\\Sigma$. $I_n$\n= $(1, 2, 3,\\ldots, n)$ and $R_n =(n, n-1, n-2,\\ldots, 2, 1)$ are identity\n(i.e. sorted) and reverse permutations respectively. An operation, that we call\nas an $LRE$ operation, has been defined in OEIS with identity A186752. This\noperation is constituted by three generators: left-rotation, right-rotation and\ntransposition(1,2). We call transposition(1,2) that swaps the two leftmost\nelements as $Exchange$. The minimum number of moves required to transform $R_n$\ninto $I_n$ with $LRE$ operation are known for $n \\leq 11$ as listed in OEIS\nwith sequence number A186752. For this problem no upper bound is known. OEIS\nsequence A186783 gives the conjectured diameter of the symmetric group $S_n$\nwhen generated by $LRE$ operations \\cite{oeis}. The contributions of this\narticle are: (a) The first non-trivial upper bound for the number of moves\nrequired to sort $R_n$ with $LRE$; (b) a tighter upper bound for the number of\nmoves required to sort $R_n$ with $LRE$; and (c) the minimum number of moves\nrequired to sort $R_{10}$ and $R_{11}$ have been computed. Here we are\ncomputing an upper bound of the diameter of Cayley graph generated by $LRE$\noperation. Cayley graphs are employed in computer interconnection networks to\nmodel efficient parallel architectures. The diameter of the network corresponds\nto the maximum delay in the network.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 02:44:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kuppili", "Sai Satwik", ""], ["Chitturi", "Bhadrachalam", ""]]}, {"id": "2002.07415", "submitter": "Oren Weimann", "authors": "Amir Abboud, Shon Feller, Oren Weimann", "title": "On the Fine-Grained Complexity of Parity Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parity variants of basic problems studied in fine-grained\ncomplexity. We show that finding the exact solution is just as hard as finding\nits parity (i.e. if the solution is even or odd) for a large number of\nclassical problems, including All-Pairs Shortest Paths (APSP), Diameter,\nRadius, Median, Second Shortest Path, Maximum Consecutive Subsums, Min-Plus\nConvolution, and $0/1$-Knapsack.\n  A direct reduction from a problem to its parity version is often difficult to\ndesign. Instead, we revisit the existing hardness reductions and tailor them in\na problem-specific way to the parity version. Nearly all reductions from APSP\nin the literature proceed via the (subcubic-equivalent but simpler) Negative\nWeight Triangle (NWT) problem. Our new modified reductions also start from NWT\nor a non-standard parity variant of it. We are not able to establish a\nsubcubic-equivalence with the more natural parity counting variant of NWT,\nwhere we ask if the number of negative triangles is even or odd. Perhaps\nsurprisingly, we justify this by designing a reduction from the\nseemingly-harder Zero Weight Triangle problem, showing that parity is\n(conditionally) strictly harder than decision for NWT.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 07:48:31 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 16:17:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Abboud", "Amir", ""], ["Feller", "Shon", ""], ["Weimann", "Oren", ""]]}, {"id": "2002.07448", "submitter": "Dominik Grzelak", "authors": "Dominik Grzelak (1 and 2), Barbara Priwitzer (3) and Uwe A{\\ss}mann (1\n  and 2) ((1) Software Technology Group at Technische Universit\\\"at Dresden,\n  (2) Centre for Tactile Internet with Human-in-the-Loop (CeTI) at Technische\n  Universit\\\"at Dresden, (3) Fakult\\\"at Technik at Hochschule Reutlingen)", "title": "Generating random bigraphs with preferential attachment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bigraph theory is a relatively young, yet formally rigorous, mathematical\nframework encompassing Robin Milner's previous work on process calculi, on the\none hand, and provides a generic meta-model for complex systems such as\nmulti-agent systems, on the other. A bigraph $F = \\langle F^P, F^L\\rangle$ is a\nsuperposition of two independent graph structures comprising a place graph\n$F^P$ (i.e., a forest) and a link graph $F^L$ (i.e., a hypergraph), sharing the\nsame node set, to express locality and communication of processes independently\nfrom each other.\n  In this paper, we take some preparatory steps towards an algorithm for\ngenerating random bigraphs with preferential attachment feature w.r.t. $F^P$\nand assortative (disassortative) linkage pattern w.r.t. $F^L$. We employ\nparameters allowing one to fine-tune the characteristics of the generated\nbigraph structures. To study the pattern formation properties of our\nalgorithmic model, we analyze several metrics from graph theory based on\nartificially created bigraphs under different configurations.\n  Bigraphs provide a quite useful and expressive semantic for process calculi\nfor mobile and global ubiquitous computing. So far, this subject has not\nreceived attention in the bigraph-related scientific literature. However,\nartificial models may be particularly useful for simulation and evaluation of\nreal-world applications in ubiquitous systems necessitating random structures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 09:34:59 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Grzelak", "Dominik", "", "1 and 2"], ["Priwitzer", "Barbara", "", "1\n  and 2"], ["A\u00dfmann", "Uwe", "", "1\n  and 2"]]}, {"id": "2002.07463", "submitter": "Geppino Pucci", "authors": "Andrea Pietracaprina, Geppino Pucci, Federico Sold\\`a", "title": "Coreset-based Strategies for Robust Center-type Problems", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset $V$ of points from some metric space, the popular $k$-center\nproblem requires to identify a subset of $k$ points (centers) in $V$ minimizing\nthe maximum distance of any point of $V$ from its closest center. The\n\\emph{robust} formulation of the problem features a further parameter $z$ and\nallows up to $z$ points of $V$ (outliers) to be disregarded when computing the\nmaximum distance from the centers. In this paper, we focus on two important\nconstrained variants of the robust $k$-center problem, namely, the Robust\nMatroid Center (RMC) problem, where the set of returned centers are constrained\nto be an independent set of a matroid of rank $k$ built on $V$, and the Robust\nKnapsack Center (RKC) problem, where each element $i\\in V$ is given a positive\nweight $w_i<1$ and the aggregate weight of the returned centers must be at most\n1. We devise coreset-based strategies for the two problems which yield\nefficient sequential, MapReduce, and Streaming algorithms. More specifically,\nfor any fixed $\\epsilon>0$, the algorithms return solutions featuring a\n$(3+\\epsilon)$-approximation ratio, which is a mere additive term $\\epsilon$\naway from the 3-approximations achievable by the best known polynomial-time\nsequential algorithms for the two problems. Moreover, the algorithms\nobliviously adapt to the intrinsic complexity of the dataset, captured by its\ndoubling dimension $D$. For wide ranges of the parameters $k,z,\\epsilon, D$, we\nobtain a sequential algorithm with running time linear in $|V|$, and\nMapReduce/Streaming algorithms with few rounds/passes and substantially\nsublinear local/working memory.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:04:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Sold\u00e0", "Federico", ""]]}, {"id": "2002.07553", "submitter": "Peter Sanders", "authors": "Peter Sanders", "title": "Connecting MapReduce Computations to Realistic Machine Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain how the popular, highly abstract MapReduce model of parallel\ncomputation (MRC) can be rooted in reality by explaining how it can be\nsimulated on realistic distributed-memory parallel machine models like BSP. We\nfirst refine the model (MRC$^+$) to include parameters for total work $w$,\nbottleneck work $\\hat{w}$, data volume $m$, and maximum object sizes $\\hat{m}$.\nWe then show matching upper and lower bounds for executing a MapReduce\ncalculation on the distributed-memory machine -- $\\Theta(w/p+\\hat{w}+\\log p)$\nwork and $\\Theta(m/p+\\hat{m}+\\log p)$ bottleneck communication volume using $p$\nprocessors.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:45:33 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Sanders", "Peter", ""]]}, {"id": "2002.07569", "submitter": "Philipp Zschoche", "authors": "Till Fluschnik, Rolf Niedermeier, Carsten Schubert, and Philipp\n  Zschoche", "title": "Multistage s-t Path: Confronting Similarity with Dissimilarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing a quest by Gupta et al. [ICALP'14], we provide a first,\ncomprehensive study of finding a short s-t path in the multistage graph model,\nreferred to as the Multistage s-t Path problem. Herein, given a sequence of\ngraphs over the same vertex set but changing edge sets, the task is to find\nshort s-t paths in each graph (\"snapshot\") such that in the found path sequence\nthe consecutive s-t paths are \"similar\". We measure similarity by the size of\nthe symmetric difference of either the vertex set (vertex-similarity) or the\nedge set (edge-similarity) of any two consecutive paths. We prove that these\ntwo variants of Multistage s-t Path are already NP-hard for an input sequence\nof only two graphs and maximum vertex degree four. Motivated by this fact and\nnatural applications of this scenario e.g. in traffic route planning, we\nperform a parameterized complexity analysis. Among other results, for both\nvariants, vertex- and edge-similarity, we prove parameterized hardness\n(W[1]-hardness) regarding the parameter path length (solution size) for both\nvariants, vertex- and edge-similarity. As a further conceptual study, we then\nmodify the multistage model by asking for dissimilar consecutive paths. As one\nof the main technical results (employing so-called representative sets known\nfrom non-temporal settings), we prove that dissimilarity allows for\nfixed-parameter tractability for the parameter solution size, contrasting our\nW[1]-hardness proof of the corresponding similarity case. We also provide\npartially positive results concerning efficient and effective data reduction\n(kernelization).\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:06:03 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 08:23:00 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 07:53:08 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Fluschnik", "Till", ""], ["Niedermeier", "Rolf", ""], ["Schubert", "Carsten", ""], ["Zschoche", "Philipp", ""]]}, {"id": "2002.07607", "submitter": "Kirill Simonov", "authors": "Eduard Eiben, Fedor V. Fomin, Fahad Panolan, Kirill Simonov", "title": "Manipulating Districts to Win Elections: Fine-Grained Complexity", "comments": "Presented at AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gerrymandering is a practice of manipulating district boundaries and\nlocations in order to achieve a political advantage for a particular party.\nLewenberg, Lev, and Rosenschein [AAMAS 2017] initiated the algorithmic study of\na geographically-based manipulation problem, where voters must vote at the\nballot box closest to them. In this variant of gerrymandering, for a given set\nof possible locations of ballot boxes and known political preferences of $n$\nvoters, the task is to identify locations for $k$ boxes out of $m$ possible\nlocations to guarantee victory of a certain party in at least $l$ districts.\nHere integers $k$ and $l$ are some selected parameter.\n  It is known that the problem is NP-complete already for 4 political parties\nand prior to our work only heuristic algorithms for this problem were\ndeveloped. We initiate the rigorous study of the gerrymandering problem from\nthe perspectives of parameterized and fine-grained complexity and provide\nasymptotically matching lower and upper bounds on its computational complexity.\nWe prove that the problem is W[1]-hard parameterized by $k+n$ and that it does\nnot admit an $f(n,k)\\cdot m^{o(\\sqrt{k})}$ algorithm for any function $f$ of\n$k$ and $n$ only, unless Exponential Time Hypothesis (ETH) fails. Our lower\nbounds hold already for $2$ parties. On the other hand, we give an algorithm\nthat solves the problem for a constant number of parties in time\n$(m+n)^{O(\\sqrt{k})}$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:51:19 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Eiben", "Eduard", ""], ["Fomin", "Fedor V.", ""], ["Panolan", "Fahad", ""], ["Simonov", "Kirill", ""]]}, {"id": "2002.07611", "submitter": "Guangping Li", "authors": "Sujoy Bhore, Guangping Li, Martin N\\\"ollenburg", "title": "An Algorithmic Study of Fully Dynamic Independent Sets for Map Labeling", "comments": "Full version of a paper appearing at ESA 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2020.19", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Map labeling is a classical problem in cartography and geographic information\nsystems that asks to place labels for area, line, and point features, with the\ngoal to select and place the maximum number of independent, i.e., overlap-free,\nlabels. A practically interesting case is point labeling with axis-parallel\nrectangular labels of common size. In a fully dynamic setting, at each time\nstep, either a new label appears or an existing label disappears. Then, the\nchallenge is to maintain a maximum cardinality subset of pairwise independent\nlabels with sub-linear update time. We study the maximal independent set (MIS)\nand maximum independent set (Max-IS) problems on fully dynamic\n(insertion/deletion model) sets of axis-parallel rectangles of two types---(i)\nuniform height and width and (ii) uniform height and arbitrary width; both\nsettings can be modeled as rectangle intersection graphs.\n  We present the first deterministic algorithm for maintaining a MIS (and thus\na 4-approximate Max-IS) of a dynamic set of uniform rectangles with amortized\nsub-logarithmic update time. This breaks the natural barrier of\n$\\Omega(\\Delta)$ update time (where $\\Delta$ is the maximum degree in the\ngraph) for vertex updates presented by Assadi et al. (STOC 2018). We continue\nby investigating Max-IS and provide a series of deterministic dynamic\napproximation schemes with approximation factors between 2 and 4 and\ncorresponding running-time trade-offs. We have implemented our algorithms and\nreport the results of an experimental comparison exploring the trade-off\nbetween solution quality and update time for synthetic and real-world map\nlabeling instances.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:51:48 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:31:11 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 14:37:57 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bhore", "Sujoy", ""], ["Li", "Guangping", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2002.07612", "submitter": "Danil Sagunov", "authors": "Fedor V. Fomin, Danil Sagunov, Kirill Simonov", "title": "Building large k-cores from sparse graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular model to measure network stability is the $k$-core, that is the\nmaximal induced subgraph in which every vertex has degree at least $k$. For\nexample, $k$-cores are commonly used to model the unraveling phenomena in\nsocial networks. In this model, users having less than $k$ connections within\nthe network leave it, so the remaining users form exactly the $k$-core. In this\npaper we study the question whether it is possible to make the network more\nrobust by spending only a limited amount of resources on new connections. A\nmathematical model for the $k$-core construction problem is the following Edge\n$k$-Core optimization problem. We are given a graph $G$ and integers $k$, $b$\nand $p$. The task is to ensure that the $k$-core of $G$ has at least $p$\nvertices by adding at most $b$ edges.\n  The previous studies on Edge $k$-Core demonstrate that the problem is\ncomputationally challenging. In particular, it is NP-hard when $k=3$, W[1]-hard\nbeing parameterized by $k+b+p$ (Chitnis and Talmon, 2018), and APX-hard (Zhou\net al, 2019). Nevertheless, we show that there are efficient algorithms with\nprovable guarantee when the $k$-core has to be constructed from a sparse graph\nwith some additional structural properties. Our results are 1) When the input\ngraph is a forest, Edge $k$-Core is solvable in polynomial time; 2) Edge\n$k$-Core is fixed-parameter tractable (FPT) being parameterized by the minimum\nsize of a vertex cover in the input graph. On the other hand, with such\nparameterization, the problem does not admit a polynomial kernel subject to a\nwidely-believed assumption from complexity theory; 3) Edge $k$-Core is FPT\nparameterized by $\\mathrm{tw}+k$. This improves upon a result of Chitnis and\nTalmon by not requiring $b$ to be small. Each of our algorithms is built upon a\nnew graph-theoretical result interesting in its own.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:52:07 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 12:36:19 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Sagunov", "Danil", ""], ["Simonov", "Kirill", ""]]}, {"id": "2002.07648", "submitter": "Lum Ramabaja", "authors": "Lum Ramabaja, Arber Avdullahu", "title": "Compact Merkle Multiproofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compact Merkle multiproof is a new and significantly more\nmemory-efficient way to generate and verify sparse Merkle multiproofs. A\nstandard sparse Merkle multiproof requires to store an index for every non-leaf\nhash in the multiproof. The compact Merkle multiproof on the other hand\nrequires only $k$ leaf indices, where $k$ is the number of elements used for\ncreating a multiproof. This significantly reduces the size of multirpoofs,\nespecially for larger Merke trees.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:35:47 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 18:14:29 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ramabaja", "Lum", ""], ["Avdullahu", "Arber", ""]]}, {"id": "2002.07649", "submitter": "Mohamad Ahmadi", "authors": "Mohamad Ahmadi and Fabian Kuhn", "title": "Distributed Maximum Matching Verification in CONGEST", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum cardinality matching problem in a standard distributed\nsetting, where the nodes $V$ of a given $n$-node network graph $G=(V,E)$\ncommunicate over the edges $E$ in synchronous rounds. More specifically, we\nconsider the distributed CONGEST model, where in each round, each node of $G$\ncan send an $O(\\log n)$-bit message to each of its neighbors. We show that for\nevery graph $G$ and a matching $M$ of $G$, there is a randomized CONGEST\nalgorithm to verify $M$ being a maximum matching of $G$ in time $O(|M|)$ and\ndisprove it in time $O(D + \\ell)$, where $D$ is the diameter of $G$ and $\\ell$\nis the length of a shortest augmenting path. We hope that our algorithm\nconstitutes a significant step towards developing a CONGEST algorithm to\ncompute a maximum matching in time $\\tilde{O}(s^*)$, where $s^*$ is the size of\na maximum matching.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:39:54 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ahmadi", "Mohamad", ""], ["Kuhn", "Fabian", ""]]}, {"id": "2002.07682", "submitter": "Sagar Kale", "authors": "Ashish Chiplunkar, Sagar Kale, Sivaramakrishnan Natarajan Ramamoorthy", "title": "How to Solve Fair $k$-Center in Massive Data Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fueled by massive data, important decision making is being automated with the\nhelp of algorithms, therefore, fairness in algorithms has become an especially\nimportant research topic. In this work, we design new streaming and distributed\nalgorithms for the fair $k$-center problem that models fair data summarization.\nThe streaming and distributed models of computation have an attractive feature\nof being able to handle massive data sets that do not fit into main memory. Our\nmain contributions are: (a) the first distributed algorithm; which has provably\nconstant approximation ratio and is extremely parallelizable, and (b) a\ntwo-pass streaming algorithm with a provable approximation guarantee matching\nthe best known algorithm (which is not a streaming algorithm). Our algorithms\nhave the advantages of being easy to implement in practice, being fast with\nlinear running times, having very small working memory and communication, and\noutperforming existing algorithms on several real and synthetic data sets. To\ncomplement our distributed algorithm, we also give a hardness result for\nnatural distributed algorithms, which holds for even the special case of\n$k$-center.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:11:40 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 16:55:27 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chiplunkar", "Ashish", ""], ["Kale", "Sagar", ""], ["Ramamoorthy", "Sivaramakrishnan Natarajan", ""]]}, {"id": "2002.07695", "submitter": "Riccardo Dondi", "authors": "Riccardo Dondi, Danny Hermelin", "title": "Computing the k Densest Subgraphs of a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing cohesive subgraphs is a central problem in graph theory. While many\nformulations of cohesive subgraphs lead to NP-hard problems, finding a densest\nsubgraph can be done in polynomial time. As such, the densest subgraph model\nhas emerged as the most popular notion of cohesiveness. Recently, the data\nmining community has started looking into the problem of computing k densest\nsubgraphs in a given graph, rather than one, with various restrictions on the\npossible overlap between the subgraphs. However, there seems to be very little\nknown on this important and natural generalization from a theoretical\nperspective. In this paper we hope to remedy this situation by analyzing three\nnatural variants of the k densest subgraphs problem. Each variant differs\ndepending on the amount of overlap that is allowed between the subgraphs. In\none extreme, when no overlap is allowed, we prove that the problem is NP-hard\nfor k >= 3. On the other extreme, when overlap is allowed without any\nrestrictions and the solution subgraphs only have to be distinct, we show that\nthe problem is fixed-parameter tractable with respect to k, and admits a PTAS\nfor constant k. Finally, when a limited of overlap is allowed between the\nsubgraphs, we prove that the problem is NP-hard for k = 2.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:18:54 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 15:55:35 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 13:49:47 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Dondi", "Riccardo", ""], ["Hermelin", "Danny", ""]]}, {"id": "2002.07698", "submitter": "Jens M. Schmidt", "authors": "Jan Kessler and Jens M. Schmidt", "title": "Dynamics of Cycles in Polyhedra I: The Isolation Lemma", "comments": "see also\n  http://www.tu-ilmenau.de/combinatorial-optimization/NewResult/NewResult.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cycle $C$ of a graph $G$ is \\emph{isolating} if every component of $G-V(C)$\nis a single vertex. We show that isolating cycles in polyhedral graphs can be\nextended to larger ones: every isolating cycle $C$ of length $6 \\leq |E(C)| <\n\\left \\lfloor \\frac{2}{3}(|V(G)|+4) \\right \\rfloor$ implies an isolating cycle\n$C'$ of larger length that contains $V(C)$. By \"hopping\" iteratively to such\nlarger cycles, we obtain a powerful and very general inductive motor for\nproving long cycles and computing them (we will give an algorithm with\nquadratic running time). This is the first step towards the so far elusive\nquest of finding a universal induction that captures longest cycles of\npolyhedral graph classes.\n  Our motor provides also a method to prove linear lower bounds on the length\nof Tutte cycles, as $C'$ will be a Tutte cycle of $G$ if $C$ is. We prove in\naddition that $|E(C')| \\leq |E(C)|+3$ if $G$ contains no face of size five,\nwhich gives a new tool for results about cycle spectra, and provides evidence\nthat faces of size five may obstruct long cycles in many graph classes. We test\nour motor on the following conjecture about essentially 4-connected graphs.\n  A planar graph is \\emph{essentially $4$-connected} if it is 3-connected and\nevery of its 3-separators is the neighborhood of a single vertex. Jackson and\nWormald proved that every essentially 4-connected planar graph $G$ on $n$\nvertices contains a cycle of length at least $\\frac{2}{5}(n+2)$, and this\nresult has recently been improved multiple times, culminating in the lower\nbound $\\frac{5}{8}(n+2)$. However, the currently best known upper bound is\ngiven by an infinite family of such graphs in which no graph $G$ contains a\ncycle that is longer than $\\left \\lfloor \\frac{2}{3}(n+4) \\right \\rfloor$; this\nupper bound is still unmatched.\n  Using isolating cycles, we improve the lower bound to match the upper. All\nour results are tight.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:24:29 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 09:08:53 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Kessler", "Jan", ""], ["Schmidt", "Jens M.", ""]]}, {"id": "2002.07727", "submitter": "Havana Rika", "authors": "Lee-Ad Gottlieb, Robert Krauthgamer, Havana Rika", "title": "Faster Algorithms for Orienteering and $k$-TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the rooted orienteering problem in Euclidean space: Given $n$\npoints $P$ in $\\mathbb R^d$, a root point $s\\in P$ and a budget $\\mathcal B>0$,\nfind a path that starts from $s$, has total length at most $\\mathcal B$, and\nvisits as many points of $P$ as possible. This problem is known to be NP-hard,\nhence we study $(1-\\delta)$-approximation algorithms. The previous\nPolynomial-Time Approximation Scheme (PTAS) for this problem, due to Chen and\nHar-Peled (2008), runs in time $n^{O(d\\sqrt{d}/\\delta)}(\\log\nn)^{(d/\\delta)^{O(d)}}$, and improving on this time bound was left as an open\nproblem. Our main contribution is a PTAS with a significantly improved time\ncomplexity of $n^{O(1/\\delta)}(\\log n)^{(d/\\delta)^{O(d)}}$.\n  A known technique for approximating the orienteering problem is to reduce it\nto solving $1/\\delta$ correlated instances of rooted $k$-TSP (a $k$-TSP tour is\none that visits at least $k$ points). However, the $k$-TSP tours in this\nreduction must achieve a certain excess guarantee (namely, their length can\nsurpass the optimum length only in proportion to a parameter of the optimum\ncalled excess) that is stronger than the usual $(1+\\delta)$-approximation. Our\nmain technical contribution is to improve the running time of these $k$-TSP\nvariants, particularly in its dependence on the dimension $d$. Indeed, our\nrunning time is polynomial even for a moderately large dimension, roughly up to\n$d=O(\\log\\log n)$ instead of $d=O(1)$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:53:03 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Krauthgamer", "Robert", ""], ["Rika", "Havana", ""]]}, {"id": "2002.07746", "submitter": "Max A. Deppert", "authors": "Max A. Deppert and Klaus Jansen and Kim-Manuel Klein", "title": "Fuzzy Simultaneous Congruences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a very natural generalization of the well-known problem of\nsimultaneous congruences. Instead of searching for a positive integer $s$ that\nis specified by $n$ fixed remainders modulo integer divisors $a_1,\\dots,a_n$ we\nconsider remainder intervals $R_1,\\dots,R_n$ such that $s$ is feasible if and\nonly if $s$ is congruent to $r_i$ modulo $a_i$ for some remainder $r_i$ in\ninterval $R_i$ for all $i$.\n  This problem is a special case of a 2-stage integer program with only two\nvariables per constraint which is is closely related to directed Diophantine\napproximation as well as the mixing set problem. We give a hardness result\nshowing that the problem is NP-hard in general.\n  By investigating the case of harmonic divisors, i.e. $a_{i+1}/a_i$ is an\ninteger for all $i<n$, which was heavily studied for the mixing set problem as\nwell, we also answer a recent algorithmic question from the field of real-time\nsystems. We present an algorithm to decide the feasibility of an instance in\ntime $\\mathcal{O}(n^2)$ and we show that if it exists even the smallest\nfeasible solution can be computed in strongly polynomial time\n$\\mathcal{O}(n^3)$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:26:14 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 19:51:30 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Deppert", "Max A.", ""], ["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""]]}, {"id": "2002.07761", "submitter": "Davis Issac", "authors": "Andreas Emil Feldmann and Davis Issac and Ashutosh Rai", "title": "Fixed-Parameter Tractability of the Weighted Edge Clique Partition\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an FPT algorithm and a bi-kernel for the Weighted Edge Clique\nPartition (WECP) problem, where a graph with $n$ vertices and integer edge\nweights is given together with an integer $k$, and the aim is to find $k$\ncliques, such that every edge appears in exactly as many cliques as its weight.\nThe problem has been previously only studied in the unweighted version called\nEdge Clique Partition (ECP), where the edges need to be partitioned into $k$\ncliques. It was shown that ECP admits a kernel with~$k^2$ vertices [Mujuni and\nRosamond, 2008], but this kernel does not extend to WECP. The previously\nfastest algorithm known for ECP has a runtime of $2^{\\mathcal{O}(k^2)}n^{O(1)}$\n[Issac, 2019]. For WECP we develop a bi-kernel with $4^k$ vertices, and an\nalgorithm with runtime $2^{\\mathcal{O}(k^{3/2}w^{1/2}\\log(k/w))}n^{O(1)}$,\nwhere $w$ is the maximum edge weight. The latter in particular improves the\nruntime for ECP to~$2^{\\mathcal{O}(k^{3/2}\\log k)}n^{O(1)}$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:50:26 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 13:45:21 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 13:45:58 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Feldmann", "Andreas Emil", ""], ["Issac", "Davis", ""], ["Rai", "Ashutosh", ""]]}, {"id": "2002.07782", "submitter": "Alina Ene", "authors": "Alina Ene, Sofia Maria Nikolakaki, Evimaria Terzi", "title": "Team Formation: Striking a Balance between Coverage and Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online crowdsourcing platforms as well as applications that span\nhuman-resource management in industrial and research organizations researchers\nhave studied extensively the team-formation problem. This problem has been\nprimarily formalized as an optimization problem where the goal is to optimize\nsome metric of the team performance, subject to the constraint that the team\nmembers should cover all the skills required by the task.\n  In this paper, we generalize this problem formulation and set as our\nobjective to optimize a function that maximizes the coverage of skills minus\nthe cost of the corresponding team. This formulation appears as a more natural\none, particularly in cases where one needs to strike a balance between the\ncoverage achieved and the cost being paid. To the best of our knowledge we are\nthe first to formalize the team-formation problem in this manner.\n  From the algorithmic perspective, we demonstrate that by using simple\nvariants of the standard greedy algorithm (used for submodular optimization) we\ncan design algorithms that have provable approximation guarantees, are\nextremely efficient and work very well in practice. Our experiments with real\ndata from online crowdsourcing platforms demonstrate the efficiency and the\nefficacy of our methods. Finally, we believe that our problem formulation and\nalgorithms are of independent interest and can be used in many applications\nwhere there is a submodular objective and a linear cost.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:26:47 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 17:08:38 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ene", "Alina", ""], ["Nikolakaki", "Sofia Maria", ""], ["Terzi", "Evimaria", ""]]}, {"id": "2002.07784", "submitter": "Julian Portmann", "authors": "Davin Choo, Christoph Grunau, Julian Portmann, V\\'aclav Rozho\\v{n}", "title": "k-means++: few more steps yield constant approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a\nstate-of-the-art algorithm for solving the k-means clustering problem and is\nknown to give an O(log k)-approximation in expectation. Recently, Lattanzi and\nSohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local\nsearch steps to yield a constant approximation (in expectation) to the k-means\nclustering problem. In this paper, we improve their analysis to show that, for\nany arbitrarily small constant $\\eps > 0$, with only $\\eps k$ additional local\nsearch steps, one can achieve a constant approximation guarantee (with high\nprobability in k), resolving an open problem in their paper.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:28:25 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Choo", "Davin", ""], ["Grunau", "Christoph", ""], ["Portmann", "Julian", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2002.07797", "submitter": "Konstantinos Georgiou", "authors": "Anthony Bonato, Konstantinos Georgiou, Calum MacRury, Pawel Pralat", "title": "Probabilistically Faulty Searching on a Half-Line", "comments": "This is full version of the paper with the same title which will\n  appear in the proceedings of the 14th Latin American Theoretical Informatics\n  Symposium (LATIN20), Sao Paulo, Brazil, May 25-29, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $p$-Faulty Search, a variant of the classic cow-path optimization\nproblem, where a unit speed robot searches the half-line (or $1$-ray) for a\nhidden item. The searcher is probabilistically faulty, and detection of the\nitem with each visitation is an independent Bernoulli trial whose probability\nof success $p$ is known. The objective is to minimize the worst case expected\ndetection time, relative to the distance of the hidden item to the origin. A\nvariation of the same problem was first proposed by Gal in 1980. Then in 2003,\nAlpern and Gal [The Theory of Search Games and Rendezvous] proposed a so-called\nmonotone solution for searching the line ($2$-rays); that is, a trajectory in\nwhich the newly searched space increases monotonically in each ray and in each\niteration. Moreover, they conjectured that an optimal trajectory for the\n$2$-rays problem must be monotone. We disprove this conjecture when the search\ndomain is the half-line ($1$-ray). We provide a lower bound for all monotone\nalgorithms, which we also match with an upper bound. Our main contribution is\nthe design and analysis of a sequence of refined search strategies, outside the\nfamily of monotone algorithms, which we call $t$-sub-monotone algorithms. Such\nalgorithms induce performance that is strictly decreasing with $t$, and for all\n$p \\in (0,1)$. The value of $t$ quantifies, in a certain sense, how much our\nalgorithms deviate from being monotone, demonstrating that monotone algorithms\nare sub-optimal when searching the half-line.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:52:34 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bonato", "Anthony", ""], ["Georgiou", "Konstantinos", ""], ["MacRury", "Calum", ""], ["Pralat", "Pawel", ""]]}, {"id": "2002.07799", "submitter": "Pratibha Choudhary", "authors": "Pratibha Choudhary", "title": "Polynomial Time Algorithms for Tracking Path Problems", "comments": "Submitted to IWOCA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, and terminal vertices $s$ and $t$, the TRACKING PATHS\nproblem asks to compute a minimum number of vertices to be marked as trackers,\nsuch that the sequence of trackers encountered in each s-t path is unique.\nTRACKING PATHS is NP-hard in both directed and undirected graphs in general. In\nthis paper we give a collection of polynomial time algorithms for some\nrestricted versions of TRACKING PATHS. We prove that TRACKING PATHS is\npolynomial time solvable for chordal graphs and tournament graphs. We prove\nthat TRACKING PATHS is NP-hard in graphs with bounded maximum degree\n$\\delta\\geq 6$, and give a $2(\\delta+1)$-approximate algorithm for the same. We\nalso analyze the version of tracking s-t paths where paths are tracked using\nedges instead of vertices, and we give a polynomial time algorithm for the\nsame. Finally, we show how to reconstruct an s-t path, given a sequence of\ntrackers and a tracking set for the graph in consideration.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:54:04 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Choudhary", "Pratibha", ""]]}, {"id": "2002.07800", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki, Krzysztof Onak", "title": "Dynamic Graph Algorithms with Batch Updates in the Massively Parallel\n  Computation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic graph algorithms in the Massively Parallel Computation\nmodel, which was inspired by practical data processing systems. Our goal is to\nprovide algorithms that can efficiently handle large batches of edge insertions\nand deletions.\n  We show algorithms that require fewer rounds to update a solution to problems\nsuch as Minimum Spanning Forest, 2-Edge Connected Components, and Maximal\nMatching than would be required by their static counterparts to compute it from\nscratch. They work in the most restrictive memory regime, in which local memory\nper machine is strongly sublinear in the number of graph vertices. Improving on\nthe size of the batch they can handle efficiently would improve on the round\ncomplexity of known static algorithms on sparse graphs.\n  Our algorithms can process batches of updates of size $\\Theta(S)$, for\nMinimum Spanning Forest and 2-Edge Connected Components, and\n$\\Theta(S^{1-\\varepsilon})$, for Maximal Matching, in $O(1)$ rounds, where $S$\nis the local memory of a single machine.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:54:32 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 17:52:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nowicki", "Krzysztof", ""], ["Onak", "Krzysztof", ""]]}, {"id": "2002.07892", "submitter": "Adriano Fazzone", "authors": "Matteo B\\\"ohm, Adriano Fazzone, Stefano Leonardi, Chris Schwiegelshohn", "title": "Fair Clustering with Multiple Colors", "comments": "Partially supported by the ERC Advanced Grant 788893 AMDROMA\n  \"Algorithmic and Mechanism Design Research in Online Markets\" and MIUR PRIN\n  project ALGADIMAR \"Algorithms, Games, and Digital Markets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fair clustering instance is given a data set $A$ in which every point is\nassigned some color. Colors correspond to various protected attributes such as\nsex, ethnicity, or age. A fair clustering is an instance where membership of\npoints in a cluster is uncorrelated with the coloring of the points.\n  Of particular interest is the case where all colors are equally represented.\nIf we have exactly two colors, Chierrichetti, Kumar, Lattanzi and Vassilvitskii\n(NIPS 2017) showed that various $k$-clustering objectives admit a constant\nfactor approximation. Since then, a number of follow up work has attempted to\nextend this result to a multi-color case, though so far, the only known results\neither result in no-constant factor approximation, apply only to special\nclustering objectives such as $k$-center, yield bicrititeria approximations, or\nrequire $k$ to be constant.\n  In this paper, we present a simple reduction from unconstrained\n$k$-clustering to fair $k$-clustering for a large range of clustering\nobjectives including $k$-median, $k$-means, and $k$-center. The reduction loses\nonly a constant factor in the approximation guarantee, marking the first true\nconstant factor approximation for many of these problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 21:54:01 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 23:52:05 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["B\u00f6hm", "Matteo", ""], ["Fazzone", "Adriano", ""], ["Leonardi", "Stefano", ""], ["Schwiegelshohn", "Chris", ""]]}, {"id": "2002.07912", "submitter": "Robert Vicari", "authors": "Robert Vicari", "title": "Simplex based Steiner tree instances yield large integrality gaps for\n  the bidirected cut relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The bidirected cut relaxation is the characteristic representative of the\nbidirected relaxations ($\\mathrm{\\mathcal{BCR}}$) which are a well-known class\nof equivalent LP-relaxations for the NP-hard Steiner Tree Problem in Graphs\n(STP). Although no general approximation algorithm based on\n$\\mathrm{\\mathcal{BCR}}$ with an approximation ratio better than $2$ for STP is\nknown, it is mostly preferred in integer programming as an implementation of\nSTP, since there exists a formulation of compact size, which turns out to be\nvery effective in practice.\n  It is known that the integrality gap of $\\mathrm{\\mathcal{BCR}}$ is at most\n$2$, and a long standing open question is whether the integrality gap is less\nthan $2$ or not. The best lower bound so far is $\\frac{36}{31} \\approx 1.161$\nproven by Byrka et al. [BGRS13]. Based on the work of Chakrabarty et al.\n[CDV11] about embedding STP instances into simplices by considering appropriate\ndual formulations, we improve on this result by constructing a new class of\ninstances and showing that their integrality gaps tend at least to $\\frac{6}{5}\n= 1.2$.\n  More precisely, we consider the class of equivalent LP-relaxations\n$\\mathrm{\\mathcal{BCR}}^{+}$, that can be obtained by strengthening\n$\\mathrm{\\mathcal{BCR}}$ by already known straightforward Steiner vertex degree\nconstraints, and show that the worst case ratio regarding the optimum value\nbetween $\\mathrm{\\mathcal{BCR}}$ and $\\mathrm{\\mathcal{BCR}}^{+}$ is at least\n$\\frac{6}{5}$. Since $\\mathrm{\\mathcal{BCR}}^{+}$ is a lower bound for the\nhypergraphic relaxations ($\\mathrm{\\mathcal{HYP}}$), another well-known class\nof equivalent LP-relaxations on which the current best $(\\ln(4) +\n\\varepsilon)$-approximation algorithm for STP by Byrka et al. [BGRS13] is\nbased, this worst case ratio also holds for $\\mathrm{\\mathcal{BCR}}$ and\n$\\mathrm{\\mathcal{HYP}}$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 22:45:53 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Vicari", "Robert", ""]]}, {"id": "2002.07945", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "On the Planar Two-Center Problem and Circular Hulls", "comments": "A preliminary version to appear in SoCG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $n$ points in the Euclidean plane, the two-center problem\nis to find two congruent disks of smallest radius whose union covers all points\nof $S$. Previously, Eppstein [SODA'97] gave a randomized algorithm of\n$O(n\\log^2n)$ expected time and Chan [CGTA'99] presented a deterministic\nalgorithm of $O(n\\log^2 n\\log^2\\log n)$ time. In this paper, we propose an\n$O(n\\log^2 n)$ time deterministic algorithm, which improves Chan's\ndeterministic algorithm and matches the randomized bound of Eppstein. If $S$ is\nin convex position, then we solve the problem in $O(n\\log n\\log\\log n)$\ndeterministic time. Our results rely on new techniques for dynamically\nmaintaining circular hulls under point insertions and deletions, which are of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 00:59:20 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "2002.07955", "submitter": "Rajendra Kumar", "authors": "Divesh Aggarwal, Yanlin Chen, Rajendra Kumar and Yixin Shen", "title": "Improved (Provable) Algorithms for the Shortest Vector Problem via\n  Bounded Distance Decoding", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important computational problem on lattices is the Shortest Vector\nProblem (SVP). In this paper, we present new algorithms that improve the\nstate-of-the-art for provable classical/quantum algorithms for SVP. We present\nthe following results.\n  $\\bullet$ A new algorithm for SVP that provides a smooth tradeoff between\ntime complexity and memory requirement. For any positive integer $4\\leq q\\leq\n\\sqrt{n}$, our algorithm takes $q^{13n+o(n)}$ time and requires $poly(n)\\cdot\nq^{16n/q^2}$ memory. This tradeoff which ranges from enumeration ($q=\\sqrt{n}$)\nto sieving ($q$ constant), is a consequence of a new time-memory tradeoff for\nDiscrete Gaussian sampling above the smoothing parameter.\n  $\\bullet$ A quantum algorithm that runs in time $2^{0.9535n+o(n)}$ and\nrequires $2^{0.5n+o(n)}$ classical memory and poly(n) qubits. This improves\nover the previously fastest classical (which is also the fastest quantum)\nalgorithm due to [ADRSD15] that has a time and space complexity $2^{n+o(n)}$.\n  $\\bullet$ A classical algorithm for SVP that runs in time $2^{1.741n+o(n)}$\ntime and $2^{0.5n+o(n)}$ space. This improves over an algorithm of [CCL18] that\nhas the same space complexity.\n  The time complexity of our classical and quantum algorithms are obtained\nusing a known upper bound of the kissing number which is $2^{0.402n}$. In\npractice most lattices have a much smaller kissing number which is often\n$2^{o(n)}$. In that case, our classical algorithm runs in time $2^{1.292n}$ and\nour quantum algorithm runs in time $2^{0.750n}$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:38:34 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 12:02:40 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 04:26:01 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Chen", "Yanlin", ""], ["Kumar", "Rajendra", ""], ["Shen", "Yixin", ""]]}, {"id": "2002.08004", "submitter": "Diptarama Hendrian", "authors": "Satoshi Kobayashi, Diptarama Hendrian, Ryo Yoshinaka, Ayumi Shinohara", "title": "Fast and linear-time string matching algorithms based on the distances\n  of $q$-gram occurrences", "comments": "14 pages, accepted to SEA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a text $T$ of length $n$ and a pattern $P$ of length $m$, the string\nmatching problem is a task to find all occurrences of $P$ in $T$. In this\nstudy, we propose an algorithm that solves this problem in $O((n + m)q)$ time\nconsidering the distance between two adjacent occurrences of the same $q$-gram\ncontained in $P$. We also propose a theoretical improvement of it which runs in\n$O(n + m)$ time, though it is not necessarily faster in practice. We compare\nthe execution times of our and existing algorithms on various kinds of real and\nartificial datasets such as an English text, a genome sequence and a Fibonacci\nstring. The experimental results show that our algorithm is as fast as the\nstate-of-the-art algorithms in many cases, particularly when a pattern\nfrequently appears in a text.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 04:58:41 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 00:44:56 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Kobayashi", "Satoshi", ""], ["Hendrian", "Diptarama", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "2002.08061", "submitter": "Patrick Dinklage", "authors": "Patrick Dinklage", "title": "Translating Between Wavelet Tree and Wavelet Matrix Construction", "comments": "Paper originally submitted to and presented at the Prague Stringology\n  Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet tree (Grossi et al. [SODA, 2003]) and wavelet matrix (Claude et\nal. [Inf. Syst., 2015]) are compact data structures with many applications such\nas text indexing or computational geometry. By continuing the recent research\nof Fischer et al. [ALENEX, 2018], we explore the similarities and differences\nof these heavily related data structures with focus on their construction. We\ndevelop a data structure to modify construction algorithms for either the\nwavelet tree or matrix to construct instead the other. This modification is\nefficient, in that it does not worsen the asymptotic time and space\nrequirements of any known wavelet tree or wavelet matrix construction\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 08:51:38 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Dinklage", "Patrick", ""]]}, {"id": "2002.08114", "submitter": "Arindam Pal", "authors": "Subhra Mazumdar, Arindam Pal, Francesco Parisi, V.S. Subrahmanian", "title": "BB_Evac: Fast Location-Sensitive Behavior-Based Building Evacuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past work on evacuation planning assumes that evacuees will follow\ninstructions -- however, there is ample evidence that this is not the case.\nWhile some people will follow instructions, others will follow their own\ndesires. In this paper, we present a formal definition of a behavior-based\nevacuation problem (BBEP) in which a human behavior model is taken into account\nwhen planning an evacuation. We show that a specific form of constraints can be\nused to express such behaviors. We show that BBEPs can be solved exactly via an\ninteger program called BB_IP, and inexactly by a much faster algorithm that we\ncall BB_Evac. We conducted a detailed experimental evaluation of both\nalgorithms applied to buildings (though in principle the algorithms can be\napplied to any graphs) and show that the latter is an order of magnitude faster\nthan BB_IP while producing results that are almost as good on one real-world\nbuilding graph and as well as on several synthetically generated graphs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:34:52 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Mazumdar", "Subhra", ""], ["Pal", "Arindam", ""], ["Parisi", "Francesco", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "2002.08202", "submitter": "Rajesh Jayaram", "authors": "Rajesh Jayaram, David P. Woodruff, Qiuyi Zhang", "title": "Span Recovery for Deep Neural Networks with Applications to Input\n  Obfuscation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous success of deep neural networks has motivated the need to\nbetter understand the fundamental properties of these networks, but many of the\ntheoretical results proposed have only been for shallow networks. In this\npaper, we study an important primitive for understanding the meaningful input\nspace of a deep network: span recovery. For $k<n$, let $\\mathbf{A} \\in\n\\mathbb{R}^{k \\times n}$ be the innermost weight matrix of an arbitrary feed\nforward neural network $M:\\mathbb{R}^n \\to \\mathbb{R}$, so $M(x)$ can be\nwritten as $M(x) = \\sigma(\\mathbf{A} x)$, for some network $\\sigma:\\mathbb{R}^k\n\\to \\mathbb{R}$. The goal is then to recover the row span of $\\mathbf{A}$ given\nonly oracle access to the value of $M(x)$. We show that if $M$ is a\nmulti-layered network with ReLU activation functions, then partial recovery is\npossible: namely, we can provably recover $k/2$ linearly independent vectors in\nthe row span of $\\mathbf{A}$ using poly$(n)$ non-adaptive queries to $M(x)$.\nFurthermore, if $M$ has differentiable activation functions, we demonstrate\nthat full span recovery is possible even when the output is first passed\nthrough a sign or $0/1$ thresholding function; in this case our algorithm is\nadaptive. Empirically, we confirm that full span recovery is not always\npossible, but only for unrealistically thin layers. For reasonably wide\nnetworks, we obtain full span recovery on both random networks and networks\ntrained on MNIST data. Furthermore, we demonstrate the utility of span recovery\nas an attack by inducing neural networks to misclassify data obfuscated by\ncontrolled random noise as sensical inputs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 14:17:15 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "2002.08225", "submitter": "Marco Pegoraro", "authors": "Marco Pegoraro and Merih Seran Uysal and Wil M.P. van der Aalst", "title": "Efficient Construction of Behavior Graphs for Uncertain Event Data", "comments": "12 pages, 6 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discipline of process mining deals with analyzing execution data of\noperational processes, extracting models from event data, checking the\nconformance between event data and normative models, and enhancing all aspects\nof processes. Recently, new techniques have been developed to analyze event\ndata containing uncertainty; these techniques strongly rely on representing\nuncertain event data through graph-based models capturing uncertainty. In this\npaper we present a novel approach to efficiently compute a graph representation\nof the behavior contained in an uncertain process trace. We present our new\nalgorithm, analyze its time complexity, and report experimental results showing\norder-of-magnitude performance improvements for behavior graph construction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 15:04:32 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 09:21:30 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 13:18:19 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 16:41:08 GMT"}, {"version": "v5", "created": "Mon, 9 Mar 2020 15:52:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Pegoraro", "Marco", ""], ["Uysal", "Merih Seran", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2002.08226", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin and Petr A. Golovach", "title": "Subexponential parameterized algorithms and kernelization on almost\n  chordal graphs", "comments": "This is the full version of the paper accepted for ESA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithmic properties of the graph class Chordal-ke, that is,\ngraphs that can be turned into a chordal graph by adding at most k edges or,\nequivalently, the class of graphs of fill-in at most k. We discover that a\nnumber of fundamental intractable optimization problems being parameterized by\nk admit subexponential algorithms on graphs from Chordal-ke. We identify a\nlarge class of optimization problems on Chordal-ke that admit algorithms with\nthe typical running time 2^{O(\\sqrt{k}\\log k)}\\cdot n^{O(1)}. Examples of the\nproblems from this class are finding an independent set of maximum weight,\nfinding a feedback vertex set or an odd cycle transversal of minimum weight, or\nthe problem of finding a maximum induced planar subgraph. On the other hand, we\nshow that for some fundamental optimization problems, like finding an optimal\ngraph coloring or finding a maximum clique, are FPT on Chordal-ke when\nparameterized by k but do not admit subexponential in k algorithms unless ETH\nfails. Besides subexponential time algorithms, the class of Chordal-ke graphs\nappears to be appealing from the perspective of kernelization (with parameter\nk). While it is possible to show that most of the weighted variants of\noptimization problems do not admit polynomial in k kernels on Chordal-ke\ngraphs, this does not exclude the existence of Turing kernelization and\nkernelization for unweighted graphs. In particular, we construct a polynomial\nTuring kernel for Weighted Clique on Chordal-ke graphs. For (unweighted)\nIndependent Set we design polynomial kernels on two interesting subclasses of\nChordal-ke, namely, Interval-ke and Split-ke graphs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 15:08:09 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 14:16:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""]]}, {"id": "2002.08299", "submitter": "Quanquan C. Liu", "authors": "Amartya Shankha Biswas, Talya Eden, Quanquan C. Liu, Slobodan\n  Mitrovi\\'c, Ronitt Rubinfeld", "title": "Parallel Algorithms for Small Subgraph Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting is a fundamental problem in analyzing massive graphs, often\nstudied in the context of social and complex networks. There is a rich\nliterature on designing efficient, accurate, and scalable algorithms for this\nproblem. In this work, we tackle this challenge and design several new\nalgorithms for subgraph counting in the Massively Parallel Computation (MPC)\nmodel:\n  Given a graph $G$ over $n$ vertices, $m$ edges and $T$ triangles, our first\nmain result is an algorithm that, with high probability, outputs a\n$(1+\\varepsilon)$-approximation to $T$, with optimal round and space complexity\nprovided any $S \\geq \\max{(\\sqrt m, n^2/m)}$ space per machine, assuming\n$T=\\Omega(\\sqrt{m/n})$.\n  Our second main result is an $\\tilde{O}_{\\delta}(\\log \\log n)$-rounds\nalgorithm for exactly counting the number of triangles, parametrized by the\narboricity $\\alpha$ of the input graph. The space per machine is\n$O(n^{\\delta})$ for any constant $\\delta$, and the total space is $O(m\\alpha)$,\nwhich matches the time complexity of (combinatorial) triangle counting in the\nsequential model. We also prove that this result can be extended to exactly\ncounting $k$-cliques for any constant $k$, with the same round complexity and\ntotal space $O(m\\alpha^{k-2})$. Alternatively, allowing $O(\\alpha^2)$ space per\nmachine, the total space requirement reduces to $O(n\\alpha^2)$.\n  Finally, we prove that a recent result of Bera, Pashanasangi and Seshadhri\n(ITCS 2020) for exactly counting all subgraphs of size at most $5$, can be\nimplemented in the MPC model in $\\tilde{O}_{\\delta}(\\sqrt{\\log n})$ rounds,\n$O(n^{\\delta})$ space per machine and $O(m\\alpha^3)$ total space. Therefore,\nthis result also exhibits the phenomenon that a time bound in the sequential\nmodel translates to a space bound in the MPC model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:16:41 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 17:51:56 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 03:32:48 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Eden", "Talya", ""], ["Liu", "Quanquan C.", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2002.08311", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Jan Kratochv\\'il, Tom\\'a\\v{s} Masa\\v{r}\\'ik, Jana Novotn\\'a", "title": "U-Bubble Model for Mixed Unit Interval Graphs and its Applications: The\n  MaxCut Problem Revisited", "comments": "Accepted to Mathematical Foundations of Computer Science (MFCS 2020),\n  25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval graphs, intersection graphs of segments on a real line (intervals),\nplay a key role in the study of algorithms and special structural properties.\nUnit interval graphs, their proper subclass, where each interval has a unit\nlength, has also been extensively studied. We study mixed unit interval\ngraphs---a generalization of unit interval graphs where each interval has still\na unit length, but intervals of more than one type (open, closed, semi-closed)\nare allowed. This small modification captures a much richer class of graphs. In\nparticular, mixed unit interval graphs are not claw-free, compared to unit\ninterval graphs.\n  Heggernes, Meister, and Papadopoulos defined a representation of unit\ninterval graphs called the bubble model which turned out to be useful in\nalgorithm design. We extend this model to the class of mixed unit interval\ngraphs and demonstrate the advantages of this generalized model by providing a\nsubexponential-time algorithm for solving the MaxCut problem on mixed unit\ninterval graphs. In addition, we derive a polynomial-time algorithm for certain\nsubclasses of mixed unit interval graphs. We point out a substantial mistake in\nthe proof of the polynomiality of the MaxCut problem on unit interval graphs by\nBoyaci, Ekim, and Shalom (2017). Hence, the time complexity of this problem on\nunit interval graphs remains open. We further provide a better algorithmic\nupper-bound on the clique-width of mixed unit interval graphs. Clique-width is\none of the most general structural graph parameters, where a large group of\nnatural problems is still solvable in the tractable time when an efficient\nrepresentation is given. Unfortunately, the exact computation of the\nclique-width representation is \\NP-hard. Therefore, good upper-bounds on\nclique-width are highly appreciated, in particular, when such a bound is\nalgorithmic.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:47:45 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 23:29:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Kratochv\u00edl", "Jan", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Novotn\u00e1", "Jana", ""]]}, {"id": "2002.08428", "submitter": "Shanyun Liu", "authors": "Shanyun Liu, Rui She, Zheqi Zhu, Pingyi Fan", "title": "Storage Space Allocation Strategy for Digital Data with Message\n  Importance", "comments": "34pages, 7 figures", "journal-ref": null, "doi": "10.3390/e22050591", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly focuses on the problem of lossy compression storage from\nthe perspective of message importance when the reconstructed data pursues the\nleast distortion within limited total storage size. For this purpose, we\ntransform this problem to an optimization by means of the importance-weighted\nreconstruction error in data reconstruction. Based on it, this paper puts\nforward an optimal allocation strategy in the storage of digital data by a kind\nof restrictive water-filling. That is, it is a high efficient adaptive\ncompression strategy since it can make rational use of all the storage space.\nIt also characterizes the trade-off between the relative weighted\nreconstruction error and the available storage size. Furthermore, this paper\nalso presents that both the users' preferences and the special characteristic\nof data distribution can trigger the small-probability event scenarios where\nonly a fraction of data can cover the vast majority of users' interests.\nWhether it is for one of the reasons above, the data with highly clustered\nmessage importance is beneficial to compression storage. In contrast, the data\nwith uniform information distribution is incompressible, which is consistent\nwith that in information theory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:59:35 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Liu", "Shanyun", ""], ["She", "Rui", ""], ["Zhu", "Zheqi", ""], ["Fan", "Pingyi", ""]]}, {"id": "2002.08474", "submitter": "Scott Rodilitz", "authors": "Vahideh Manshadi and Scott Rodilitz", "title": "Online Policies for Efficient Volunteer Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonprofit crowdsourcing platforms such as food recovery organizations rely on\nvolunteers to perform time-sensitive tasks. To encourage volunteers to complete\na task, platforms use nudging mechanisms to notify a subset of volunteers with\nthe hope that at least one of them responds positively. However, since\nexcessive notifications may reduce volunteer engagement, the platform faces a\ntrade-off between notifying more volunteers for the current task and saving\nthem for future ones. Motivated by these applications, we introduce the online\nvolunteer notification problem, a generalization of online stochastic bipartite\nmatching where tasks arrive following a known time-varying distribution over\ntask types. Upon arrival of a task, the platform notifies a subset of\nvolunteers with the objective of minimizing the number of missed tasks. To\ncapture each volunteer's adverse reaction to excessive notifications, we assume\nthat a notification triggers a random period of inactivity, during which she\nwill ignore all notifications. However, if a volunteer is active and notified,\nshe will perform the task with a given pair-specific match probability that\ncaptures her preference for the task. We develop two online randomized policies\nthat achieve constant-factor guarantees close to the upper bound we establish\nfor the performance of any online policy. Our policies as well as hardness\nresults are parameterized by the minimum discrete hazard rate of the\ninter-activity time distribution. The design of each policy relies on modifying\nan ex-ante feasible solution, either by properly scaling down the notification\nprobability prescribed by the ex-ante solution or by sparsifying that solution.\nFurther, in collaboration with Food Rescue U.S., a volunteer-based food\nrecovery platform, we demonstrate the effectiveness of our policies by testing\nthem on the platform's data from various locations across the U.S.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:16:37 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 00:40:21 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 03:16:47 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 17:38:27 GMT"}, {"version": "v5", "created": "Mon, 20 Jul 2020 20:16:10 GMT"}, {"version": "v6", "created": "Fri, 18 Sep 2020 17:39:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Manshadi", "Vahideh", ""], ["Rodilitz", "Scott", ""]]}, {"id": "2002.08475", "submitter": "Antti R\\\"oysk\\\"o", "authors": "Mikko Koivisto (1), Antti R\\\"oysk\\\"o (1) ((1) Department of Computer\n  Science, University of Helsinki, Finland)", "title": "Fast Multi-Subset Transform and Weighted Sums Over Acyclic Digraphs", "comments": "12 pages, 4 figures. Appeared in Scandinavian Symposium and Workshops\n  on Algorithm Theory 2020. The replacement is based on the conference version,\n  and has small changes compared to the previous version, particularly to the\n  proof of proposition 13", "journal-ref": "Leibniz International Proceedings in Informatics (LIPIcs) 162\n  (2020) 29:1-29:12", "doi": "10.4230/LIPIcs.SWAT.2020.29", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The zeta and Moebius transforms over the subset lattice of $n$ elements and\nthe so-called subset convolution are examples of unary and binary operations on\nset functions. While their direct computation requires $O(3^n)$ arithmetic\noperations, less naive algorithms only use $2^n \\mathrm{poly}(n)$ operations,\nnearly linear in the input size. Here, we investigate a related $n$-ary\noperation that takes $n$ set functions as input and maps them to a new set\nfunction. This operation, we call multi-subset transform, is the core\ningredient in the known inclusion--exclusion recurrence for weighted sums over\nacyclic digraphs, which extends Robinson's recurrence for the number of\nlabelled acyclic digraphs. Prior to this work the best known complexity bound\nwas the direct $O(3^n)$. By reducing the task to multiple instances of\nrectangular matrix multiplication, we improve the complexity to $O(2.985^n)$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:17:17 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 13:37:53 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Koivisto", "Mikko", ""], ["R\u00f6ysk\u00f6", "Antti", ""]]}, {"id": "2002.08495", "submitter": "Heather Guarnera", "authors": "Feodor F. Dragan and Heather M. Guarnera", "title": "Eccentricity terrain of $\\delta$-hyperbolic graphs", "comments": "22 pages, 4 figures", "journal-ref": "Journal of Computer and System Sciences 112 (2020) 50--65", "doi": "10.1016/j.jcss.2020.03.004", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G=(V,E)$ is $\\delta$-hyperbolic if for any four vertices $u,v,w,x$,\nthe two larger of the three distance sums $d(u,v)+d(w,x)$, $d(u,w)+d(v,x)$, and\n$d(u,x)+d(v,w)$ differ by at most $2\\delta \\geq 0$. Recent work shows that many\nreal-world graphs have small hyperbolicity $\\delta$. This paper describes the\neccentricity terrain of a $\\delta$-hyperbolic graph. The eccentricity function\n$e_G(v)=\\max\\{d(v,u) : u \\in V\\}$ partitions the vertex set of $G$ into\neccentricity layers $C_{k}(G) = \\{v \\in V : e(v)=rad(G)+k\\}$, $k \\in\n\\mathbb{N}$, where $rad(G)=\\min\\{e_G(v): v\\in V\\}$ is the radius of $G$. The\npaper studies the eccentricity layers of vertices along shortest paths,\nidentifying such terrain features as hills, plains, valleys, terraces, and\nplateaus. It introduces the notion of $\\beta$-pseudoconvexity, which implies\nGromov's $\\epsilon$-quasiconvexity, and illustrates the abundance of\npseudoconvex sets in $\\delta$-hyperbolic graphs. In particular, it shows that\nall sets $C_{\\leq k}(G)=\\{v\\in V : e_G(v) \\leq rad(G) + k\\}$, $k\\in\n\\mathbb{N}$, are $(2\\delta-1)$-pseudoconvex. Additionally, several bounds on\nthe eccentricity of a vertex are obtained which yield a few approaches to\nefficiently approximating all eccentricities. An $O(\\delta |E|)$ time\neccentricity approximation $\\hat{e}(v)$, for all $v\\in V$, is presented that\nuses distances to two mutually distant vertices and satisfies $e_G(v)-2\\delta\n\\leq \\hat{e}(v) \\leq {e_G}(v)$. It also shows existence of two eccentricity\napproximating spanning trees $T$, one constructible in $O(\\delta |E|)$ time and\nthe other in $O(|E|)$ time, which satisfy ${e}_G(v) \\leq e_T(v) \\leq\n{e}_G(v)+4\\delta+1$ and ${e}_G(v) \\leq e_T(v) \\leq {e}_G(v)+6\\delta$,\nrespectively. Thus, the eccentricity terrain of a tree gives a good\napproximation (up-to an additive error $O(\\delta))$ of the eccentricity terrain\nof a $\\delta$-hyperbolic graph.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 23:20:47 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 13:21:42 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Dragan", "Feodor F.", ""], ["Guarnera", "Heather M.", ""]]}, {"id": "2002.08498", "submitter": "Yu Zheng", "authors": "Kuan Cheng, Zhengzhong Jin, Xin Li, Yu Zheng", "title": "Space Efficient Deterministic Approximation of String Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximation algorithms for the following three string measures\nthat are widely used in practice: edit distance (ED), longest common\nsubsequence (LCS), and longest increasing sequence (LIS). All three problems\ncan be solved exactly by standard algorithms that run in polynomial time with\nroughly $\\Theta(n)$ space, where $n$ is the input length, and our goal is to\ndesign deterministic approximation algorithms that run in polynomial time with\nsignificantly smaller space.\n  Towards this, we design several algorithms that achieve $1+\\epsilon$ or\n$1-\\epsilon$ approximation for all three problems, where $\\epsilon>0$ can be\nany constant and even slightly sub constant. Our algorithms are flexible and\ncan be adjusted to achieve the following two regimes of parameters: 1) space\n$n^\\delta$ for any constant $\\delta>0$ with running time essentially the same\nas or slightly more than the standard algorithms; and 2) space\n$\\mathsf{polylog}(n)$ with (a larger) polynomial running time, which puts the\napproximation versions of the three problems in Steve's class (\\textbf{SC}).\nOur algorithms significantly improve previous results in terms of space\ncomplexity, where all known results need to use space at least\n$\\Omega(\\sqrt{n})$. Some of our algorithms can also be adapted to work in the\nasymmetric streaming model [SS13], and output the corresponding sequence.\nFurthermore, our results can be used to improve a recent result by Farhadi et.\nal. [FHRS20] about approximating ED in the asymmetric streaming model, reducing\nthe running time from being exponential in [FHRS20] to a polynomial.\n  Our algorithms are based on the idea of using recursion as in Savitch's\ntheorem [Sav70], and a careful adaption of previous techniques to make the\nrecursion work. Along the way we also give a new logspace reduction from\nlongest common subsequence to longest increasing sequence, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 23:33:39 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 06:18:37 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 13:52:20 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2020 21:58:11 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cheng", "Kuan", ""], ["Jin", "Zhengzhong", ""], ["Li", "Xin", ""], ["Zheng", "Yu", ""]]}, {"id": "2002.08659", "submitter": "Niels Gr\\\"uttemeier", "authors": "Niels Gr\\\"uttemeier, Christian Komusiewicz, Nils Morawietz", "title": "Maximum Edge-Colorable Subgraph and Strong Triadic Closure Parameterized\n  by Distance to Low-Degree Graphs", "comments": "32 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G$ and integers $c$ and $k$, the Maximum\nEdge-Colorable Subgraph problem asks whether we can delete at most $k$ edges in\n$G$ to obtain a graph that has a proper edge coloring with at most $c$ colors.\nWe show that Maximum Edge-Colorable Subgraph admits, for every fixed $c$, a\nlinear-size problem kernel when parameterized by the edge deletion distance of\n$G$ to a graph with maximum degree $c-1$. This parameterization measures the\ndistance to instances that, due to Vizing's famous theorem, are trivial\nyes-instances. For $c\\le 4$, we also provide a linear-size kernel for the same\nparameterization for Multi Strong Triadic Closure, a related edge coloring\nproblem with applications in social network analysis. We provide further\nresults for Maximum Edge-Colorable Subgraph parameterized by the vertex\ndeletion distance to graphs where every component has order at most $c$ and for\nthe list-colored versions of both problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 10:42:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""], ["Morawietz", "Nils", ""]]}, {"id": "2002.08825", "submitter": "Magnus Wahlstr\\\"om", "authors": "Magnus Wahlstr\\\"om", "title": "Quasipolynomial multicut-mimicking networks and kernelization of\n  multiway cut problems", "comments": "Updated version with simplified proof and new constructive result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the existence of an exact mimicking network of $k^{O(\\log k)}$ edges\nfor minimum multicuts over a set of terminals in an undirected graph, where $k$\nis the total capacity of the terminals, as well as a method for computing a\nmimicking network of quasipolynomial size in polynomial time. As a consequence\nof the latter, several problems are shown to have quasipolynomial kernels,\nincluding Edge Multiway Cut, Group Feedback Edge Set for an arbitrary group,\nand Edge Multicut parameterized by the solution and the number of cut requests.\nThe result combines the matroid-based irrelevant edge approach used in the\nkernel for $s$-Multiway Cut with a recursive decomposition and sparsification\nof the graph along sparse cuts. This is the first progress on the kernelization\nof Multiway Cut problems since the kernel for $s$-Multiway Cut for constant\nvalue of $s$ (Kratsch and Wahlstr\\\"om, FOCS 2012).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:03:31 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 20:51:24 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 20:42:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "2002.08872", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas", "title": "Halpern Iteration for Near-Optimal and Parameter-Free Monotone Inclusion\n  and Strong Solutions to Variational Inequalities", "comments": "23 pages; v1->v2: added acknowledgements and some more related work;\n  v2 -> v3: fixed a small typo in the proof of Lemma 2.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage the connections between nonexpansive maps, monotone Lipschitz\noperators, and proximal mappings to obtain near-optimal (i.e., optimal up to\npoly-log factors in terms of iteration complexity) and parameter-free methods\nfor solving monotone inclusion problems. These results immediately translate\ninto near-optimal guarantees for approximating strong solutions to variational\ninequality problems, approximating convex-concave min-max optimization\nproblems, and minimizing the norm of the gradient in min-max optimization\nproblems. Our analysis is based on a novel and simple potential-based proof of\nconvergence of Halpern iteration, a classical iteration for finding fixed\npoints of nonexpansive maps. Additionally, we provide a series of algorithmic\nreductions that highlight connections between different problem classes and\nlead to lower bounds that certify near-optimality of the studied methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:12:49 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 17:28:39 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 15:48:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Diakonikolas", "Jelena", ""]]}, {"id": "2002.08892", "submitter": "Ankit Singh Rawat", "authors": "Venkata Gandikota, Arya Mazumdar, Ankit Singh Rawat", "title": "Reliable Distributed Clustering with Redundant Data Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present distributed generalized clustering algorithms that\ncan handle large scale data across multiple machines in spite of straggling or\nunreliable machines. We propose a novel data assignment scheme that enables us\nto obtain global information about the entire data even when some machines fail\nto respond with the results of the assigned local computations. The assignment\nscheme leads to distributed algorithms with good approximation guarantees for a\nvariety of clustering and dimensionality reduction problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:44:37 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Gandikota", "Venkata", ""], ["Mazumdar", "Arya", ""], ["Rawat", "Ankit Singh", ""]]}, {"id": "2002.08944", "submitter": "Yassine Hamoudi", "authors": "Yassine Hamoudi and Fr\\'ed\\'eric Magniez", "title": "Quantum Time-Space Tradeoff for Finding Multiple Collision Pairs", "comments": "21 pages; v3: title and presentation changed. Previous title:\n  \"Quantum Time-Space Tradeoffs by Recording Queries\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding $K$ collision pairs in a random function $f :\n[N] \\rightarrow [N]$ by using a quantum computer. We prove that the number of\nqueries to the function in the quantum random oracle model must increase\nsignificantly when the size of the available memory is limited. Namely, we\ndemonstrate that any algorithm using $S$ qubits of memory must perform a number\n$T$ of queries that satisfies the tradeoff $T^3 S \\geq \\Omega(K^3 N)$.\nClassically, the same question has only been settled recently by Dinur\n[Eurocrypt'20], who showed that the Parallel Collision Search algorithm of van\nOorschot and Wiener achieves the optimal time-space tradeoff of $T^2 S =\n\\Theta(K^2 N)$. Our result limits the extent to which quantum computing may\ndecrease this tradeoff. We further show that any improvement to our lower bound\nwould imply a breakthrough for a related question about the Element\nDistinctness problem. Our method is based on a novel application of Zhandry's\nrecording query technique [Crypto'19] for proving lower bounds in the\nexponentially small success probability regime.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 18:48:51 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:37:10 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 10:49:29 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Hamoudi", "Yassine", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.09028", "submitter": "Felix Reidl", "authors": "Carl Einarson and Felix Reidl", "title": "A general kernelization technique for domination and independence\n  problems in sparse classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify and extend previous kernelization techniques in sparse classes\n[6,17] by defining water lilies and show how they can be used in bounded\nexpansion classes to construct linear bikernels for (r, c)-Dominating Set, (r,\nc)-Scattered Set, Total r-Domination, r-Roman Domination, and a problem we call\n(r, [{\\lambda}, {\\mu}])-Domination (implying a bikernel for r-Perfect Code). At\nthe cost of slightly changing the output graph class our bikernels can be\nturned into kernels.\n  We further demonstrate how these constructions can be combined to create\n'multikernels', meaning graphs that represent kernels for multiple problems at\nonce. Concretely, we show that r-Dominating Set, Total r-Domination, and\nr-Roman Domination admit a multikernel; as well as r-Dominating Set and\n2r-Independent Set for multiple values of r at once.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 21:56:21 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 10:54:16 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Einarson", "Carl", ""], ["Reidl", "Felix", ""]]}, {"id": "2002.09041", "submitter": "Susana Ladra", "authors": "Carlos Quijada-Fuentes and Miguel R. Penabad and Susana Ladra and\n  Gilberto Guti\\'errez", "title": "Compressed Data Structures for Binary Relations in Practice", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "IEEE Access 8, pp. 25949-25963 (2020)", "doi": "10.1109/ACCESS.2020.2970983", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary relations are commonly used in Computer Science for modeling data. In\naddition to classical representations using matrices or lists, some compressed\ndata structures have recently been proposed to represent binary relations in\ncompact space, such as the $k^2$-tree and the Binary Relation Wavelet Tree\n(BRWT). Knowing their storage needs, supported operations and time performance\nis key for enabling an appropriate choice of data representation given a domain\nor application, its data distribution and typical operations that are computed\nover the data.\n  In this work, we present an empirical comparison among several compressed\nrepresentations for binary relations. We analyze their space usage and the\nspeed of their operations using different (synthetic and real) data\ndistributions. We include both neighborhood and set operations, also proposing\nalgorithms for set operations for the BRWT, which were not presented before in\nthe literature. We conclude that there is not a clear choice that outperforms\nthe rest, but we give some recommendations of usage of each compact\nrepresentation depending on the data distribution and types of operations\nperformed over the data. We also include a scalability study of the data\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:15:41 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Quijada-Fuentes", "Carlos", ""], ["Penabad", "Miguel R.", ""], ["Ladra", "Susana", ""], ["Guti\u00e9rrez", "Gilberto", ""]]}, {"id": "2002.09067", "submitter": "Kensen Shi", "authors": "Kensen Shi, David Bieber, Charles Sutton", "title": "Incremental Sampling Without Replacement for Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is a fundamental technique, and sampling without replacement is\noften desirable when duplicate samples are not beneficial. Within machine\nlearning, sampling is useful for generating diverse outputs from a trained\nmodel. We present an elegant procedure for sampling without replacement from a\nbroad class of randomized programs, including generative neural models that\nconstruct outputs sequentially. Our procedure is efficient even for\nexponentially-large output spaces. Unlike prior work, our approach is\nincremental, i.e., samples can be drawn one at a time, allowing for increased\nflexibility. We also present a new estimator for computing expectations from\nsamples drawn without replacement. We show that incremental sampling without\nreplacement is applicable to many domains, e.g., program synthesis and\ncombinatorial optimization.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 00:12:01 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 00:09:38 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Shi", "Kensen", ""], ["Bieber", "David", ""], ["Sutton", "Charles", ""]]}, {"id": "2002.09130", "submitter": "Paul Liu", "authors": "Wenzheng Li, Paul Liu, Jan Vondrak", "title": "A polynomial lower bound on adaptive complexity of submodular\n  maximization", "comments": "To appear in STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-data applications, it is desirable to design algorithms with a high\ndegree of parallelization. In the context of submodular optimization, adaptive\ncomplexity has become a widely-used measure of an algorithm's \"sequentiality\".\nAlgorithms in the adaptive model proceed in rounds, and can issue polynomially\nmany queries to a function $f$ in each round. The queries in each round must be\nindependent, produced by a computation that depends only on query results\nobtained in previous rounds.\n  In this work, we examine two fundamental variants of submodular maximization\nin the adaptive complexity model: cardinality-constrained monotone\nmaximization, and unconstrained non-mono-tone maximization. Our main result is\nthat an $r$-round algorithm for cardinality-constrained monotone maximization\ncannot achieve an approximation factor better than $1 - 1/e - \\Omega(\\min \\{\n\\frac{1}{r}, \\frac{\\log^2 n}{r^3} \\})$, for any $r < n^c$ (where $c>0$ is some\nconstant). This is the first result showing that the number of rounds must blow\nup polynomially large as we approach the optimal factor of $1-1/e$.\n  For the unconstrained non-monotone maximization problem, we show a positive\nresult: For every instance, and every $\\delta>0$, either we obtain a\n$(1/2-\\delta)$-approximation in $1$ round, or a\n$(1/2+\\Omega(\\delta^2))$-approximation in $O(1/\\delta^2)$ rounds. In particular\n(and in contrast to the cardinality-constrained case), there cannot be an\ninstance where (i) it is impossible to achieve an approximation factor better\nthan $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to\nachieve a factor of $1/2-O(1/r)$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 04:54:45 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 23:27:14 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Wenzheng", ""], ["Liu", "Paul", ""], ["Vondrak", "Jan", ""]]}, {"id": "2002.09180", "submitter": "Jiaxin Xie", "authors": "Yuan Lei and Jiaxin Xie", "title": "A symmetric alternating minimization algorithm for total variation\n  minimization", "comments": "to appear in Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel symmetric alternating minimization\nalgorithm to solve a broad class of total variation (TV) regularization\nproblems. Unlike the usual $z^k\\to x^k$ Gauss-Seidel cycle, the proposed\nalgorithm performs the special $\\overline{x}^{k}\\to z^k\\to x^k$ cycle. The main\nidea for our setting is the recent symmetric Gauss-Seidel (sGS) technique which\nis developed for solving the multi-block convex composite problem. This idea\nalso enables us to build the equivalence between the proposed method and the\nwell-known accelerated proximal gradient (APG) method. The faster convergence\nrate of the proposed algorithm can be directly obtained from the APG framework\nand numerical results including image denoising, image deblurring, and analysis\nsparse recovery problem demonstrate the effectiveness of the new algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 08:35:08 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 02:25:14 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Lei", "Yuan", ""], ["Xie", "Jiaxin", ""]]}, {"id": "2002.09264", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Practical Estimation of Renyi Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy Estimation is an important problem with many applications in\ncryptography, statistic,machine learning. Although the estimators optimal with\nrespect to the sample complexity have beenrecently developed, there are still\nsome challenges we address in this paper.The contribution is a novel estimator\nwhich is built directly on the birthday paradox. Theanalysis turns out to be\nconsiderably simpler and offer superior confidence bounds with\nexplicitconstants. We also discuss how streaming algorithm can be used to\nmassively improve memoryconsumption. Last but not least, we study the problem\nof estimation in low or moderate regimes,adapting the estimator and proving\nrigorus bounds.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:20:21 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2002.09441", "submitter": "Nate Veldt", "authors": "Nate Veldt and Austin R. Benson and Jon Kleinberg", "title": "Minimizing Localized Ratio Cut Objectives in Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs are a useful abstraction for modeling multiway relationships in\ndata, and hypergraph clustering is the task of detecting groups of closely\nrelated nodes in such data. Graph clustering has been studied extensively, and\nthere are numerous methods for detecting small, localized clusters without\nhaving to explore an entire input graph. However, there are only a few\nspecialized approaches for localized clustering in hypergraphs. Here we present\na framework for local hypergraph clustering based on minimizing localized ratio\ncut objectives. Our framework takes an input set of reference nodes in a\nhypergraph and solves a sequence of hypergraph minimum $s$-$t$ cut problems in\norder to identify a nearby well-connected cluster of nodes that overlaps\nsubstantially with the input set.\n  Our methods extend graph-based techniques but are significantly more general\nand have new output quality guarantees. First, our methods can minimize new\ngeneralized notions of hypergraph cuts, which depend on specific configurations\nof nodes within each hyperedge, rather than just on the number of cut\nhyperedges. Second, our framework has several attractive theoretical properties\nin terms of output cluster quality. Most importantly, our algorithm is\nstrongly-local, meaning that its runtime depends only on the size of the input\nset, and does not need to explore the entire hypergraph to find good local\nclusters. We use our methodology to effectively identify clusters in\nhypergraphs of real-world data with millions of nodes, millions of hyperedges,\nand large average hyperedge size with runtimes ranging between a few seconds\nand a few minutes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 17:42:22 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 02:48:48 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Veldt", "Nate", ""], ["Benson", "Austin R.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2002.09458", "submitter": "Rad Niazadeh", "authors": "Arash Asadpour, Rad Niazadeh, Amin Saberi and Ali Shameli", "title": "Ranking an Assortment of Products via Sequential Submodular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an optimization problem capturing a core operational question for\nonline retailing platforms. Given models for the users' preferences over\nproducts as well as the number of items they are willing to observe before\nclicking on one or abandoning the search, what is the best way to rank the\nrelevant products in response to a search query?\n  In order to capture both popularity and diversity effects, we model the\nprobability that a user clicks on an element from a subset of products as a\nmonotone submodular function of this set. We also assume that the patience\nlevel of the users, or the number of items they are willing to observe before\nclicking on one or abandoning the search, is a given random variable. Under\nthose assumptions, the objective functions capturing user engagement or\nplatform revenue can be written as a new family of submodular optimization\nproblems over a sequence of elements.\n  We call this family of natural optimization problems sequential submodular\noptimization. By building on and extending the literature on submodular\nmaximization subject to matroid constraints, we derive a (1-1/e) optimal\napproximation algorithm for maximizing user engagement and a bi-criteria\napproximation algorithm for maximizing revenue subject to a lower bound on user\nengagement.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:22:40 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Asadpour", "Arash", ""], ["Niazadeh", "Rad", ""], ["Saberi", "Amin", ""], ["Shameli", "Ali", ""]]}, {"id": "2002.09460", "submitter": "Nate Veldt", "authors": "Nate Veldt and Anthony Wirth and David F. Gleich", "title": "Parameterized Correlation Clustering in Hypergraphs and Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in community detection and dense subgraph\ndiscovery, we consider new clustering objectives in hypergraphs and bipartite\ngraphs. These objectives are parameterized by one or more resolution parameters\nin order to enable diverse knowledge discovery in complex data.\n  For both hypergraph and bipartite objectives, we identify parameter regimes\nthat are equivalent to existing objectives and share their (polynomial-time)\napproximation algorithms. We first show that our parameterized hypergraph\ncorrelation clustering objective is related to higher-order notions of\nnormalized cut and modularity in hypergraphs. It is further amenable to\napproximation algorithms via hyperedge expansion techniques.\n  Our parameterized bipartite correlation clustering objective generalizes\nstandard unweighted bipartite correlation clustering, as well as bicluster\ndeletion. For a certain choice of parameters it is also related to our\nhypergraph objective. Although in general it is NP-hard, we highlight a\nparameter regime for the bipartite objective where the problem reduces to the\nbipartite matching problem and thus can be solved in polynomial time. For other\nparameter settings, we present approximation algorithms using linear program\nrounding techniques. These results allow us to introduce the first\nconstant-factor approximation for bicluster deletion, the task of removing a\nminimum number of edges to partition a bipartite graph into disjoint\nbi-cliques.\n  In several experimental results, we highlight the flexibility of our\nframework and the diversity of results that can be obtained in different\nparameter settings. This includes clustering bipartite graphs across a range of\nparameters, detecting motif-rich clusters in an email network and a food web,\nand forming clusters of retail products in a product review hypergraph, that\nare highly correlated with known product categories.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:26:53 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 15:10:01 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Veldt", "Nate", ""], ["Wirth", "Anthony", ""], ["Gleich", "David F.", ""]]}, {"id": "2002.09463", "submitter": "Huanyu Zhang", "authors": "Huanyu Zhang, Gautam Kamath, Janardhan Kulkarni, Zhiwei Steven Wu", "title": "Privately Learning Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning Markov Random Fields (including the\nprototypical example, the Ising model) under the constraint of differential\nprivacy. Our learning goals include both structure learning, where we try to\nestimate the underlying graph structure of the model, as well as the harder\ngoal of parameter learning, in which we additionally estimate the parameter on\neach edge. We provide algorithms and lower bounds for both problems under a\nvariety of privacy constraints -- namely pure, concentrated, and approximate\ndifferential privacy. While non-privately, both learning goals enjoy roughly\nthe same complexity, we show that this is not the case under differential\nprivacy. In particular, only structure learning under approximate differential\nprivacy maintains the non-private logarithmic dependence on the dimensionality\nof the data, while a change in either the learning goal or the privacy notion\nwould necessitate a polynomial dependence. As a result, we show that the\nprivacy constraint imposes a strong separation between these two learning\nproblems in the high-dimensional data regime.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:30:48 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 14:24:58 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Huanyu", ""], ["Kamath", "Gautam", ""], ["Kulkarni", "Janardhan", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2002.09464", "submitter": "Vikrant Singhal", "authors": "Gautam Kamath, Vikrant Singhal, Jonathan Ullman", "title": "Private Mean Estimation of Heavy-Tailed Distributions", "comments": "Appeared in COLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new upper and lower bounds on the minimax sample complexity of\ndifferentially private mean estimation of distributions with bounded $k$-th\nmoments. Roughly speaking, in the univariate case, we show that $n =\n\\Theta\\left(\\frac{1}{\\alpha^2} +\n\\frac{1}{\\alpha^{\\frac{k}{k-1}}\\varepsilon}\\right)$ samples are necessary and\nsufficient to estimate the mean to $\\alpha$-accuracy under\n$\\varepsilon$-differential privacy, or any of its common relaxations. This\nresult demonstrates a qualitatively different behavior compared to estimation\nabsent privacy constraints, for which the sample complexity is identical for\nall $k \\geq 2$. We also give algorithms for the multivariate setting whose\nsample complexity is a factor of $O(d)$ larger than the univariate case.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:30:48 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 22:24:31 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 17:06:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Kamath", "Gautam", ""], ["Singhal", "Vikrant", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2002.09465", "submitter": "Gautam Kamath", "authors": "Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov,\n  Zhiwei Steven Wu, Huanyu Zhang", "title": "Locally Private Hypothesis Selection", "comments": "To appear in COLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of hypothesis selection under local differential\nprivacy. Given samples from an unknown probability distribution $p$ and a set\nof $k$ probability distributions $\\mathcal{Q}$, we aim to output, under the\nconstraints of $\\varepsilon$-local differential privacy, a distribution from\n$\\mathcal{Q}$ whose total variation distance to $p$ is comparable to the best\nsuch distribution. This is a generalization of the classic problem of $k$-wise\nsimple hypothesis testing, which corresponds to when $p \\in \\mathcal{Q}$, and\nwe wish to identify $p$. Absent privacy constraints, this problem requires\n$O(\\log k)$ samples from $p$, and it was recently shown that the same\ncomplexity is achievable under (central) differential privacy. However, the\nnaive approach to this problem under local differential privacy would require\n$\\tilde O(k^2)$ samples.\n  We first show that the constraint of local differential privacy incurs an\nexponential increase in cost: any algorithm for this problem requires at least\n$\\Omega(k)$ samples. Second, for the special case of $k$-wise simple hypothesis\ntesting, we provide a non-interactive algorithm which nearly matches this\nbound, requiring $\\tilde O(k)$ samples. Finally, we provide sequentially\ninteractive algorithms for the general case, requiring $\\tilde O(k)$ samples\nand only $O(\\log \\log k)$ rounds of interactivity. Our algorithms are achieved\nthrough a reduction to maximum selection with adversarial comparators, a\nproblem of independent interest for which we initiate study in the parallel\nsetting. For this problem, we provide a family of algorithms for each number of\nallowed rounds of interaction $t$, as well as lower bounds showing that they\nare near-optimal for every $t$. Notably, our algorithms result in exponential\nimprovements on the round complexity of previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 18:30:48 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 02:58:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Gopi", "Sivakanth", ""], ["Kamath", "Gautam", ""], ["Kulkarni", "Janardhan", ""], ["Nikolov", "Aleksandar", ""], ["Wu", "Zhiwei Steven", ""], ["Zhang", "Huanyu", ""]]}, {"id": "2002.09511", "submitter": "Victor Grishchenko", "authors": "Victor Grishchenko and Mikhail Patrakeev", "title": "Chronofold: a data structure for versioned text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronofold is a replicated data structure for versioned text. It is designed\nfor use in collaborative editors and revision control systems. Past models of\nthis kind either retrofitted local linear orders to a distributed system (the\nOT approach) or employed distributed data models locally (the CRDT approach).\nThat caused either extreme fragility in a distributed setting or egregious\noverheads in local use. Overall, that local/distributed impedance mismatch is\ncognitively taxing and causes lots of complexity. We solve that by using\nsubjective linear orders locally at each replica, while inter-replica\ncommunication uses a distributed model. A separate translation layer insulates\nlocal data structures from the distributed environment. We modify the Lamport\ntimestamping scheme to make that translation as trivial as possible. We believe\nour approach has applications beyond the domain of collaborative editing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:16:07 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 07:52:43 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 11:44:07 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2020 06:34:43 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Grishchenko", "Victor", ""], ["Patrakeev", "Mikhail", ""]]}, {"id": "2002.09610", "submitter": "Ce Jin", "authors": "Mohsen Ghaffari, Christoph Grunau, Ce Jin", "title": "Improved MPC Algorithms for MIS, Matching, and Coloring on Trees and\n  Beyond", "comments": "To appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $O(\\log\\log n)$ round scalable Massively Parallel Computation\nalgorithms for maximal independent set and maximal matching, in trees and more\ngenerally graphs of bounded arboricity, as well as for constant coloring trees.\nFollowing the standards, by a scalable MPC algorithm, we mean that these\nalgorithms can work on machines that have capacity/memory as small as\n$n^{\\delta}$ for any positive constant $\\delta<1$. Our results improve over the\n$O(\\log^2\\log n)$ round algorithms of Behnezhad et al. [PODC'19]. Moreover, our\nmatching algorithm is presumably optimal as its bound matches an\n$\\Omega(\\log\\log n)$ conditional lower bound of Ghaffari, Kuhn, and Uitto\n[FOCS'19].\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 03:07:25 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 12:54:22 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Grunau", "Christoph", ""], ["Jin", "Ce", ""]]}, {"id": "2002.09707", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "Compression with wildcards: All spanning trees", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By processing all minimal cutsets of a graph G, and by using novel wildcards,\nall spanning trees of G can be compactly encoded. Thus, different from all\nprevious enumeration schemes, the spanning trees are not generated one-by-one.\nThe Mathematica implementation of one of our algorithms generated for a random\n(11,50)-graph its 819'603'181 spanning trees, in bundles of size about 400,\nwithin 52 seconds.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 14:18:53 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "2002.09722", "submitter": "David Fern\\'andez-Baca", "authors": "Ghazaleh Parvini, Katherine Braught, David Fern\\'andez-Baca", "title": "Checking Phylogenetic Decisiveness in Theory and in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have a set $X$ consisting of $n$ taxa and we are given information\nfrom $k$ loci from which to construct a phylogeny for $X$. Each locus offers\ninformation for only a fraction of the taxa. The question is whether this data\nsuffices to construct a reliable phylogeny. The decisiveness problem expresses\nthis question combinatorially. Although a precise characterization of\ndecisiveness is known, the complexity of the problem is open. Here we relate\ndecisiveness to a hypergraph coloring problem. We use this idea to (1) obtain\nlower bounds on the amount of coverage needed to achieve decisiveness, (2)\ndevise an exact algorithm for decisiveness, (3) develop problem reduction\nrules, and use them to obtain efficient algorithms for inputs with few loci,\nand (4) devise an integer linear programming formulation of the decisiveness\nproblem, which allows us to analyze data sets that arise in practice.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 15:55:46 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Parvini", "Ghazaleh", ""], ["Braught", "Katherine", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "2002.09725", "submitter": "David Fern\\'andez-Baca", "authors": "David Fern\\'andez-Baca and Lei Liu", "title": "Testing the Agreement of Trees with Internal Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the agreement problem is a collection $P = \\{T_1, T_2, \\dots ,\nT_k\\}$ of phylogenetic trees, called input trees, over partially overlapping\nsets of taxa. The question is whether there exists a tree $T$, called an\nagreement tree, whose taxon set is the union of the taxon sets of the input\ntrees, such that for each $i \\in \\{1, 2, \\dots , k\\}$, the restriction of $T$\nto the taxon set of $T_i$ is isomorphic to $T_i$. We give a $O(n k (\\sum_{i \\in\n[k]} d_i + \\log^2(nk)))$ algorithm for a generalization of the agreement\nproblem in which the input trees may have internal labels, where $n$ is the\ntotal number of distinct taxa in $P$, $k$ is the number of trees in $P$, and\n$d_i$ is the maximum number of children of a node in $T_i$.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 16:12:41 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Fern\u00e1ndez-Baca", "David", ""], ["Liu", "Lei", ""]]}, {"id": "2002.09745", "submitter": "Judy Hanwen Shen", "authors": "Sivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen,\n  Milad Shokouhi and Sergey Yekhanin", "title": "Differentially Private Set Union", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the basic operation of set union in the global model of differential\nprivacy. In this problem, we are given a universe $U$ of items, possibly of\ninfinite size, and a database $D$ of users. Each user $i$ contributes a subset\n$W_i \\subseteq U$ of items. We want an ($\\epsilon$,$\\delta$)-differentially\nprivate algorithm which outputs a subset $S \\subset \\cup_i W_i$ such that the\nsize of $S$ is as large as possible. The problem arises in countless real world\napplications; it is particularly ubiquitous in natural language processing\n(NLP) applications as vocabulary extraction. For example, discovering words,\nsentences, $n$-grams etc., from private text data belonging to users is an\ninstance of the set union problem.\n  Known algorithms for this problem proceed by collecting a subset of items\nfrom each user, taking the union of such subsets, and disclosing the items\nwhose noisy counts fall above a certain threshold. Crucially, in the above\nprocess, the contribution of each individual user is always independent of the\nitems held by other users, resulting in a wasteful aggregation process, where\nsome item counts happen to be way above the threshold. We deviate from the\nabove paradigm by allowing users to contribute their items in a\n$\\textit{dependent fashion}$, guided by a $\\textit{policy}$. In this new\nsetting ensuring privacy is significantly delicate. We prove that any policy\nwhich has certain $\\textit{contractive}$ properties would result in a\ndifferentially private algorithm. We design two new algorithms, one using\nLaplace noise and other Gaussian noise, as specific instances of policies\nsatisfying the contractive properties. Our experiments show that the new\nalgorithms significantly outperform previously known mechanisms for the\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 18:33:14 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gopi", "Sivakanth", ""], ["Gulhane", "Pankaj", ""], ["Kulkarni", "Janardhan", ""], ["Shen", "Judy Hanwen", ""], ["Shokouhi", "Milad", ""], ["Yekhanin", "Sergey", ""]]}, {"id": "2002.09807", "submitter": "Nick Gravin", "authors": "Tomer Ezra, Michal Feldman, Nick Gravin, Zhihao Gavin Tang", "title": "Online Stochastic Max-Weight Matching: prophet inequality for vertex and\n  edge arrival models", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide prophet inequality algorithms for online weighted matching in\ngeneral (non-bipartite) graphs, under two well-studied arrival models, namely\nedge arrival and vertex arrival. The weight of each edge is drawn independently\nfrom an a-priori known probability distribution. Under edge arrival, the weight\nof each edge is revealed upon arrival, and the algorithm decides whether to\ninclude it in the matching or not. Under vertex arrival, the weights of all\nedges from the newly arriving vertex to all previously arrived vertices are\nrevealed, and the algorithm decides which of these edges, if any, to include in\nthe matching. To study these settings, we introduce a novel unified framework\nof batched prophet inequalities that captures online settings where elements\narrive in batches; in particular it captures matching under the two\naforementioned arrival models. Our algorithms rely on the construction of\nsuitable online contention resolution scheme (OCRS). We first extend the\nframework of OCRS to batched-OCRS, we then establish a reduction from batched\nprophet inequality to batched OCRS, and finally we construct batched OCRSs with\nselectable ratios of 0.337 and 0.5 for edge and vertex arrival models,\nrespectively. Both results improve the state of the art for the corresponding\nsettings. For the vertex arrival, our result is tight. Interestingly, a\npricing-based prophet inequality with comparable competitive ratios is unknown.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 02:03:35 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 20:43:48 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ezra", "Tomer", ""], ["Feldman", "Michal", ""], ["Gravin", "Nick", ""], ["Tang", "Zhihao Gavin", ""]]}, {"id": "2002.09812", "submitter": "Xin Yang", "authors": "Yingyu Liang, Zhao Song, Mengdi Wang, Lin F. Yang, Xin Yang", "title": "Sketching Transformed Matrices with Applications to Natural Language\n  Processing", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are given a large matrix $A=(a_{i,j})$ that cannot be stored in\nmemory but is in a disk or is presented in a data stream. However, we need to\ncompute a matrix decomposition of the entry-wisely transformed matrix,\n$f(A):=(f(a_{i,j}))$ for some function $f$. Is it possible to do it in a space\nefficient way? Many machine learning applications indeed need to deal with such\nlarge transformed matrices, for example word embedding method in NLP needs to\nwork with the pointwise mutual information (PMI) matrix, while the entrywise\ntransformation makes it difficult to apply known linear algebraic tools.\nExisting approaches for this problem either need to store the whole matrix and\nperform the entry-wise transformation afterwards, which is space consuming or\ninfeasible, or need to redesign the learning method, which is application\nspecific and requires substantial remodeling.\n  In this paper, we first propose a space-efficient sketching algorithm for\ncomputing the product of a given small matrix with the transformed matrix. It\nworks for a general family of transformations with provable small error bounds\nand thus can be used as a primitive in downstream learning tasks. We then apply\nthis primitive to a concrete application: low-rank approximation. We show that\nour approach obtains small error and is efficient in both space and time. We\ncomplement our theoretical results with experiments on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 03:07:31 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Liang", "Yingyu", ""], ["Song", "Zhao", ""], ["Wang", "Mengdi", ""], ["Yang", "Lin F.", ""], ["Yang", "Xin", ""]]}, {"id": "2002.09880", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov, Polina Ivanova, Albina Zamaletdinova", "title": "Mixed Integer Programming for Searching Maximum Quasi-Bicliques", "comments": "This paper draft is stored here for self-archiving purposes", "journal-ref": "Springer Proceedings in Mathematics & Statistics, vol 315.\n  Springer, Cham (2020)", "doi": "10.1007/978-3-030-37157-9_2", "report-no": null, "categories": "cs.DS cs.AI cs.DM cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is related to the problem of finding the maximal quasi-bicliques\nin a bipartite graph (bigraph). A quasi-biclique in the bigraph is its \"almost\"\ncomplete subgraph. The relaxation of completeness can be understood variously;\nhere, we assume that the subgraph is a $\\gamma$-quasi-biclique if it lacks a\ncertain number of edges to form a biclique such that its density is at least\n$\\gamma \\in (0,1]$. For a bigraph and fixed $\\gamma$, the problem of searching\nfor the maximal quasi-biclique consists of finding a subset of vertices of the\nbigraph such that the induced subgraph is a quasi-biclique and its size is\nmaximal for a given graph. Several models based on Mixed Integer Programming\n(MIP) to search for a quasi-biclique are proposed and tested for working\nefficiency. An alternative model inspired by biclustering is formulated and\ntested; this model simultaneously maximizes both the size of the quasi-biclique\nand its density, using the least-square criterion similar to the one exploited\nby triclustering \\textsc{TriBox}.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 10:25:51 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Ivanova", "Polina", ""], ["Zamaletdinova", "Albina", ""]]}, {"id": "2002.09885", "submitter": "Mordecai Golin", "authors": "Mordecai Golin and Elfarouk Harb", "title": "Speeding up the AIFV-$2$ dynamic programs by two orders of magnitude\n  using Range Minimum Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AIFV-$2$ codes are a new method for constructing lossless codes for\nmemoryless sources that provide better worst-case redundancy than Huffman\ncodes. They do this by using two code trees instead of one and also allowing\nsome bounded delay in the decoding process. Known algorithms for constructing\nAIFV-code are iterative; at each step they replace the current code tree pair\nwith a \"better\" one. The current state of the art for performing this\nreplacement is a pair of Dynamic Programming (DP) algorithms that use $O(n^5)$\ntime to fill in two tables, each of size $O(n^3)$ (where $n$ is the number of\ndifferent characters in the source).\n  This paper describes how to reduce the time for filling in the DP tables by\ntwo orders of magnitude, down to $O(n^3)$. It does this by introducing a\ngrouping technique that permits separating the $\\Theta(n^3)$-space tables into\n$\\Theta(n)$ groups, each of size $O(n^2)$, and then using Two-Dimensional\nRange-Minimum Queries (RMQs) to fill in that group's table entries in $O(n^2)$\ntime. This RMQ speedup technique seems to be new and might be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 11:39:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Golin", "Mordecai", ""], ["Harb", "Elfarouk", ""]]}, {"id": "2002.09925", "submitter": "Yue Jiang", "authors": "Yue Jiang, Wolfgang Stuerzlinger, Matthias Zwicker, Christof Lutteroth", "title": "ORCSolver: An Efficient Solver for Adaptive GUI Layout with\n  OR-Constraints", "comments": "Published at CHI2020", "journal-ref": null, "doi": "10.1145/3313831.3376610", "report-no": null, "categories": "cs.HC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OR-constrained (ORC) graphical user interface layouts unify conventional\nconstraint-based layouts with flow layouts, which enables the definition of\nflexible layouts that adapt to screens with different sizes, orientations, or\naspect ratios with only a single layout specification. Unfortunately, solving\nORC layouts with current solvers is time-consuming and the needed time\nincreases exponentially with the number of widgets and constraints. To address\nthis challenge, we propose ORCSolver, a novel solving technique for adaptive\nORC layouts, based on a branch-and-bound approach with heuristic preprocessing.\nWe demonstrate that ORCSolver simplifies ORC specifications at runtime and our\napproach can solve ORC layout specifications efficiently at near-interactive\nrates.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 15:46:59 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Jiang", "Yue", ""], ["Stuerzlinger", "Wolfgang", ""], ["Zwicker", "Matthias", ""], ["Lutteroth", "Christof", ""]]}, {"id": "2002.09972", "submitter": "Ashwin Jacob", "authors": "Ashwin Jacob, Fahad Panolan, Venkatesh Raman and Vibha Sahlot", "title": "Structural Parameterizations with Modulator Oblivion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that problems like Vertex Cover, Feedback Vertex Set and Odd\nCycle Transversal are polynomial time solvable in the class of chordal graphs.\nWe consider these problems in a graph that has at most $k$ vertices whose\ndeletion results in a chordal graph, when parameterized by $k$. While this\ninvestigation fits naturally into the recent trend of what are called\n`structural parameterizations', here we assume that the deletion set is not\ngiven.\n  One method to solve them is to compute a $k$-sized or an approximate ($f(k)$\nsized, for a function $f$) chordal vertex deletion set and then use the\nstructural properties of the graph to design an algorithm. This method leads to\nat least $k^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$ running time when we use the\nknown parameterized or approximation algorithms for finding a $k$-sized chordal\ndeletion set on an $n$ vertex graph.\n  In this work, we design $2^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$ time\nalgorithms for these problems. Our algorithms do not compute a chordal vertex\ndeletion set (or even an approximate solution). Instead, we construct a tree\ndecomposition of the given graph in time $2^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$\nwhere each bag is a union of four cliques and $\\mathcal{O}(k)$ vertices. We\nthen apply standard dynamic programming algorithms over this special tree\ndecomposition. This special tree decomposition can be of independent interest.\n  Our algorithms are adaptive (robust) in the sense that given an integer $k$,\nthey detect whether the graph has a chordal vertex deletion set of size at most\n$k$ or output the special tree decomposition and solve the problem.\n  We also show lower bounds for the problems we deal with under the Strong\nExponential Time Hypothesis (SETH).\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 19:00:23 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Jacob", "Ashwin", ""], ["Panolan", "Fahad", ""], ["Raman", "Venkatesh", ""], ["Sahlot", "Vibha", ""]]}, {"id": "2002.10039", "submitter": "Karine Chubarian", "authors": "Karine Chubarian and Anastasios Sidiropoulos", "title": "Computing Bi-Lipschitz Outlier Embeddings into the Line", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing a bi-Lipschitz embedding of a graphical metric into\nthe line with minimum distortion has received a lot of attention. The\nbest-known approximation algorithm computes an embedding with distortion\n$O(c^2)$, where $c$ denotes the optimal distortion [B\\u{a}doiu \\etal~2005]. We\npresent a bi-criteria approximation algorithm that extends the above results to\nthe setting of \\emph{outliers}.\n  Specifically, we say that a metric space $(X,\\rho)$ admits a\n$(k,c)$-embedding if there exists $K\\subset X$, with $|K|=k$, such that\n$(X\\setminus K, \\rho)$ admits an embedding into the line with distortion at\nmost $c$. Given $k\\geq 0$, and a metric space that admits a $(k,c)$-embedding,\nfor some $c\\geq 1$, our algorithm computes a $({\\mathsf p}{\\mathsf o}{\\mathsf\nl}{\\mathsf y}(k, c, \\log n), {\\mathsf p}{\\mathsf o}{\\mathsf l}{\\mathsf\ny}(c))$-embedding in polynomial time. This is the first algorithmic result for\noutlier bi-Lipschitz embeddings. Prior to our work, comparable outlier\nembeddings where known only for the case of additive distortion.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:22:06 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chubarian", "Karine", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "2002.10047", "submitter": "Jessica Shi", "authors": "Jessica Shi and Laxman Dhulipala and Julian Shun", "title": "Parallel Clique Counting and Peeling Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel algorithm for $k$-clique counting/listing that has\npolylogarithmic span (parallel time) and is work-efficient (matches the work of\nthe best sequential algorithm) for sparse graphs. Our algorithm is based on\ncomputing low out-degree orientations, which we present new linear-work and\npolylogarithmic-span algorithms for computing in parallel. We also present new\nparallel algorithms for producing unbiased estimations of clique counts using\ngraph sparsification. Finally, we design two new parallel work-efficient\nalgorithms for approximating the $k$-clique densest subgraph, the first of\nwhich is a $1/k$-approximation and the second of which is a\n$1/(k(1+\\epsilon))$-approximation and has polylogarithmic span. Our first\nalgorithm does not have polylogarithmic span, but we prove that it solves a\nP-complete problem.\n  In addition to the theoretical results, we also implement the algorithms and\npropose various optimizations to improve their practical performance. On a\n30-core machine with two-way hyper-threading, our algorithms achieve\n13.23--38.99x and 1.19--13.76x self-relative parallel speedup for $k$-clique\ncounting and $k$-clique densest subgraph, respectively. Compared to the\nstate-of-the-art parallel $k$-clique counting algorithms, we achieve up to\n9.88x speedup, and compared to existing implementations of $k$-clique densest\nsubgraph, we achieve up to 11.83x speedup. We are able to compute the\n$4$-clique counts on the largest publicly-available graph with over two hundred\nbillion edges for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:48:47 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 02:28:06 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 02:13:51 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 17:30:15 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Shi", "Jessica", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2002.10142", "submitter": "Stefan Neumann", "authors": "Monika Henzinger, Stefan Neumann, Andreas Wiese", "title": "Explicit and Implicit Dynamic Coloring of Graphs with Bounded Arboricity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph coloring is a fundamental problem in computer science. We study the\nfully dynamic version of the problem in which the graph is undergoing edge\ninsertions and deletions and we wish to maintain a vertex-coloring with small\nupdate time after each insertion and deletion.\n  We show how to maintain an $O(\\alpha \\lg n)$-coloring with polylogarithmic\nupdate time, where $n$ is the number of vertices in the graph and $\\alpha$ is\nthe current arboricity of the graph. This improves upon a result by Solomon and\nWein (ESA'18) who maintained an $O(\\alpha_{\\max}\\lg^2 n)$-coloring, where\n$\\alpha_{\\max}$ is the maximum arboricity of the graph over all updates.\n  Furthermore, motivated by a lower bound by Barba et al. (Algorithmica'19), we\ninitiate the study of implicit dynamic colorings. Barba et al. showed that\ndynamic algorithms with polylogarithmic update time cannot maintain an\n$f(\\alpha)$-coloring for any function $f$ when the vertex colors are stored\nexplicitly, i.e., for each vertex the color is stored explicitly in the memory.\nPreviously, all dynamic algorithms maintained explicit colorings. Therefore, we\npropose to study implicit colorings, i.e., the data structure only needs to\noffer an efficient query procedure to return the color of a vertex (instead of\nstoring its color explicitly). We provide an algorithm which breaks the lower\nbound and maintains an implicit $2^{O(\\alpha)}$-coloring with polylogarithmic\nupdate time. In particular, this yields the first dynamic $O(1)$-coloring for\ngraphs with constant arboricity such as planar graphs or graphs with bounded\ntree-width, which is impossible using explicit colorings.\n  We also show how to dynamically maintain a partition of the graph's edges\ninto $O(\\alpha)$ forests with polylogarithmic update time. We believe this data\nstructure is of independent interest and might have more applications in the\nfuture.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:18:44 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Henzinger", "Monika", ""], ["Neumann", "Stefan", ""], ["Wiese", "Andreas", ""]]}, {"id": "2002.10435", "submitter": "Sitan Chen", "authors": "Sitan Chen, Jerry Li, Ankur Moitra", "title": "Learning Structured Distributions From Untrusted Batches: Faster and\n  Simpler", "comments": "37 pages, version 2 includes experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of learning from untrusted batches introduced by Qiao\nand Valiant [QV17]. Recently, Jain and Orlitsky [JO19] gave a simple\nsemidefinite programming approach based on the cut-norm that achieves\nessentially information-theoretically optimal error in polynomial time.\nConcurrently, Chen et al. [CLM19] considered a variant of the problem where\n$\\mu$ is assumed to be structured, e.g. log-concave, monotone hazard rate,\n$t$-modal, etc. In this case, it is possible to achieve the same error with\nsample complexity sublinear in $n$, and they exhibited a quasi-polynomial time\nalgorithm for doing so using Haar wavelets.\n  In this paper, we find an appealing way to synthesize the techniques of\n[JO19] and [CLM19] to give the best of both worlds: an algorithm which runs in\npolynomial time and can exploit structure in the underlying distribution to\nachieve sublinear sample complexity. Along the way, we simplify the approach of\n[JO19] by avoiding the need for SDP rounding and giving a more direct\ninterpretation of it through the lens of soft filtering, a powerful recent\ntechnique in high-dimensional robust estimation. We validate the usefulness of\nour algorithms in preliminary experimental evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 18:32:10 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 17:50:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Sitan", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""]]}, {"id": "2002.10499", "submitter": "Ioana O. Bercea", "authors": "Ioana O. Bercea, Guy Even", "title": "Upper Tail Analysis of Bucket Sort and Random Tries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bucket Sort is known to run in expected linear time when the input keys are\ndistributed independently and uniformly at random in the interval $[0,1)$. The\nanalysis holds even when a quadratic time algorithm is used to sort the keys in\neach bucket. We show how to obtain linear time guarantees on the running time\nof Bucket Sort that hold with very high probability. Specifically, we\ninvestigate the asymptotic behavior of the exponent in the upper tail\nprobability of the running time of Bucket Sort. We consider large additive\ndeviations from the expectation, of the form $cn$ for large enough (constant)\n$c$, where $n$ is the number of keys that are sorted.\n  Our analysis shows a profound difference between variants of Bucket Sort that\nuse a quadratic time algorithm within each bucket and variants that use a\n$\\Theta(b\\log b)$ time algorithm for sorting $b$ keys in a bucket. When a\nquadratic time algorithm is used to sort the keys in a bucket, the probability\nthat Bucket Sort takes $cn$ more time than expected is exponential in\n$\\Theta(\\sqrt{n}\\log n)$. When a $\\Theta(b\\log b)$ algorithm is used to sort\nthe keys in a bucket, the exponent becomes $\\Theta(n)$. We prove this latter\ntheorem by showing an upper bound on the tail of a random variable defined on\ntries, a result which we believe is of independent interest. This result also\nenables us to analyze the upper tail probability of a well-studied trie\nparameter, the external path length, and show that the probability that it\ndeviates from its expected value by an additive factor of $cn$ is exponential\nin $\\Theta(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 19:28:56 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Bercea", "Ioana O.", ""], ["Even", "Guy", ""]]}, {"id": "2002.10658", "submitter": "Xiangyu Guo", "authors": "Xiangyu Guo, Janardhan Kulkarni, Shi Li, Jiayi Xian", "title": "The Power of Recourse: Better Algorithms for Facility Location in Online\n  and Dynamic Models", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the facility location problem in the online with\nrecourse and dynamic algorithm models. In the online with recourse model,\nclients arrive one by one and our algorithm needs to maintain good solutions at\nall time steps with only a few changes to the previously made decisions (called\nrecourse). We show that the classic local search technique can lead to a\n$(1+\\sqrt{2}+\\epsilon)$-competitive online algorithm for facility location with\nonly $O\\left(\\frac{\\log n}{\\epsilon}\\log\\frac1\\epsilon\\right)$ amortized\nfacility and client recourse. We then turn to the dynamic algorithm model for\nthe problem, where the main goal is to design fast algorithms that maintain\ngood solutions at all time steps. We show that the result for online facility\nlocation, combined with the randomized local search technique of Charikar and\nGuha [10], leads to an $O(1+\\sqrt{2}+\\epsilon)$ approximation dynamic algorithm\nwith amortized update time of $\\tilde O(n)$ in the incremental setting. Notice\nthat the running time is almost optimal, since in general metric space it takes\n$\\Omega(n)$ time to specify a new client's position. The approximation factor\nof our algorithm also matches the best offline analysis of the classic local\nsearch algorithm. Finally, we study the fully dynamic model for facility\nlocation, where clients can both arrive and depart. Our main result is an\n$O(1)$-approximation algorithm in this model with $O(|F|)$ preprocessing time\nand $O(\\log^3 D)$ amortized update time for the HST metric spaces. Using the\nseminal results of Bartal [4] and Fakcharoenphol, Rao and Talwar [17], which\nshow that any arbitrary $N$-point metric space can be embedded into a\ndistribution over HSTs such that the expected distortion is at most $O(\\log\nN)$, we obtain a $O(\\log |F|)$ approximation with preprocessing time of\n$O(|F|^2\\log |F|)$ and $O(\\log^3 D)$ amortized update time.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 04:00:46 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Guo", "Xiangyu", ""], ["Kulkarni", "Janardhan", ""], ["Li", "Shi", ""], ["Xian", "Jiayi", ""]]}, {"id": "2002.10672", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "Algorithms for Subpath Convex Hull Queries and Ray-Shooting Among\n  Segments", "comments": "A preliminary version to appear in SoCG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first consider the subpath convex hull query problem: Given\na simple path $\\pi$ of $n$ vertices, preprocess it so that the convex hull of\nany query subpath of $\\pi$ can be quickly obtained. Previously, Guibas,\nHershberger, and Snoeyink [SODA 90'] proposed a data structure of $O(n)$ space\nand $O(\\log n\\log\\log n)$ query time; reducing the query time to $O(\\log n)$\nincreases the space to $O(n\\log\\log n)$. We present an improved result that\nuses $O(n)$ space while achieving $O(\\log n)$ query time. Like the previous\nwork, our query algorithm returns a compact interval tree representing the\nconvex hull so that standard binary-search-based queries on the hull can be\nperformed in $O(\\log n)$ time each. Our new result leads to improvements for\nseveral other problems.\n  In particular, with the help of the above result, we present new algorithms\nfor the ray-shooting problem among segments. Given a set of $n$ (possibly\nintersecting) line segments in the plane, preprocess it so that the first\nsegment hit by a query ray can be quickly found. We give a data structure of\n$O(n\\log n)$ space that can answer each query in $(\\sqrt{n}\\log n)$ time. If\nthe segments are nonintersecting or if the segments are lines, then the space\ncan be reduced to $O(n)$. All these are classical problems that have been\nstudied extensively. Previously data structures of $\\widetilde{O}(\\sqrt{n})$\nquery time (the notation $\\widetilde{O}$ suppresses a polylogarithmic factor)\nwere known in early 1990s; nearly no progress has been made for over two\ndecades. For all problems, our results provide improvements by reducing the\nspace of the data structures by at least a logarithmic factor while the\npreprocessing and query times are the same as before or even better.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 05:16:34 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "2002.10780", "submitter": "Dennis Olivetti", "authors": "Alkida Balliu, Fabian Kuhn, Dennis Olivetti", "title": "Distributed Edge Coloring in Time Quasi-Polylogarithmic in Delta", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of coloring the edges of an $n$-node graph of maximum degree\n$\\Delta$ with $2\\Delta - 1$ colors is one of the key symmetry breaking problems\nin the area of distributed graph algorithms. While there has been a lot of\nprogress towards the understanding of this problem, the dependency of the\nrunning time on $\\Delta$ has been a long-standing open question. Very recently,\nKuhn [SODA '20] showed that the problem can be solved in time\n$2^{O(\\sqrt{\\log\\Delta})}+O(\\log^* n)$.\n  In this paper, we study the edge coloring problem in the distributed LOCAL\nmodel. We show that the $(\\mathit{degree}+1)$-list edge coloring problem, and\nthus also the $(2\\Delta-1)$-edge coloring problem, can be solved\ndeterministically in time $\\log^{O(\\log\\log\\Delta)}\\Delta + O(\\log^* n)$. This\nis a significant improvement over the result of Kuhn [SODA '20].\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:23:23 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Balliu", "Alkida", ""], ["Kuhn", "Fabian", ""], ["Olivetti", "Dennis", ""]]}, {"id": "2002.10841", "submitter": "Max Willert", "authors": "Wolfgang Mulzer and Max Willert", "title": "Routing in Unit Disk Graphs without Dynamic Headers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $V\\subset\\mathbb{R}^2$ be a set of $n$ sites in the plane. The unit disk\ngraph $DG(V)$ of $V$ is the graph with vertex set $V$ in which two sites $v$\nand $w$ are adjacent if and only if their Euclidean distance is at most $1$. We\ndevelop a compact routing scheme for $DG(V)$. The routing scheme preprocesses\n$DG(V)$ by assigning a label $l(v)$ to every site $v$ in $V$. After that, for\nany two sites $s$ and $t$, the scheme must be able to route a packet from $s$\nto $t$ as follows: given the label of a current vertex $r$ (initially, $r=s$)\nand the label of the target vertex $t$, the scheme determines a neighbor $r'$\nof $r$. Then, the packet is forwarded to $r'$, and the process continues until\nthe packet reaches its desired target $t$. The resulting path between the\nsource $s$ and the target $t$ is called the routing path of $s$ and $t$. The\nstretch of the routing scheme is the maximum ratio of the total Euclidean\nlength of the routing path and of the shortest path in $DG(V)$, between any two\nsites $s, t \\in V$. We show that for any given $\\varepsilon>0$, we can\nconstruct a routing scheme for $DG(V)$ with diameter $D$ that achieves stretch\n$1+\\varepsilon$ and label size $O(\\log D\\log^3n/\\log\\log n)$ (the constant in\nthe $O$-Notation depends on $\\varepsilon$). In the past, several routing\nschemes for unit disk graphs have been proposed. Our scheme is the first one to\nachieve poly-logarithmic label size and arbitrarily small stretch without\nstoring any additional information in the packet.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 13:00:34 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mulzer", "Wolfgang", ""], ["Willert", "Max", ""]]}, {"id": "2002.10859", "submitter": "Lars Jaffke", "authors": "Jungho Ahn, Lars Jaffke, O-joung Kwon, Paloma T. Lima", "title": "Well-partitioned chordal graphs: obstruction set and disjoint paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new subclass of chordal graphs that generalizes split graphs,\nwhich we call well-partitioned chordal graphs. Split graphs are graphs that\nadmit a partition of the vertex set into cliques that can be arranged in a star\nstructure, the leaves of which are of size one. Well-partitioned chordal graphs\nare a generalization of this concept in the following two ways. First, the\ncliques in the partition can be arranged in a tree structure, and second, each\nclique is of arbitrary size. We provide a characterization of well-partitioned\nchordal graphs by forbidden induced subgraphs, and give a polynomial-time\nalgorithm that given any graph, either finds an obstruction, or outputs a\npartition of its vertex set that asserts that the graph is well-partitioned\nchordal. We demonstrate the algorithmic use of this graph class by showing that\ntwo variants of the problem of finding pairwise disjoint paths between k given\npairs of vertices is in FPT parameterized by k on well-partitioned chordal\ngraphs, while on chordal graphs, these problems are only known to be in XP.\nFrom the other end, we observe that there are problems that are polynomial-time\nsolvable on split graphs, but become NP-complete on well-partitioned chordal\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:02:11 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ahn", "Jungho", ""], ["Jaffke", "Lars", ""], ["Kwon", "O-joung", ""], ["Lima", "Paloma T.", ""]]}, {"id": "2002.10870", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Marco Valtorta, Pooyan Jamshidi", "title": "AMP Chain Graphs: Minimal Separators and Structure Learning Algorithms", "comments": "This is an arXiv version of the paper that has been accepted for\n  publication in the Journal of Artificial Intelligence Research (JAIR). arXiv\n  admin note: text overlap with arXiv:1211.3295 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding a minimal separator in an\nAndersson-Madigan-Perlman chain graph (AMP CG), namely, finding a set Z of\nnodes that separates a given nonadjacent pair of nodes such that no proper\nsubset of Z separates that pair. We analyze several versions of this problem\nand offer polynomial-time algorithms for each. These include finding a minimal\nseparator from a restricted set of nodes, finding a minimal separator for two\ngiven disjoint sets, and testing whether a given separator is minimal. To\naddress the problem of learning the structure of AMP CGs from data, we show\nthat the PC-like algorithm (Pena, 2012) is order-dependent, in the sense that\nthe output can depend on the order in which the variables are given. We propose\nseveral modifications of the PC-like algorithm that remove part or all of this\norder-dependence. We also extend the decomposition-based approach for learning\nBayesian networks (BNs) proposed by (Xie et al., 2006) to learn AMP CGs, which\ninclude BNs as a special case, under the faithfulness assumption. We prove the\ncorrectness of our extension using the minimal separator results. Using\nstandard benchmarks and synthetically generated models and data in our\nexperiments demonstrate the competitive performance of our decomposition-based\nmethod, called LCD-AMP, in comparison with the (modified versions of) PC-like\nalgorithm. The LCD-AMP algorithm usually outperforms the PC-like algorithm, and\nour modifications of the PC-like algorithm learn structures that are more\nsimilar to the underlying ground truth graphs than the original PC-like\nalgorithm, especially in high-dimensional settings. In particular, we\nempirically show that the results of both algorithms are more accurate and\nstabler when the sample size is reasonably large and the underlying graph is\nsparse.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 18:14:14 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 20:38:42 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Valtorta", "Marco", ""], ["Jamshidi", "Pooyan", ""]]}, {"id": "2002.10889", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Caleb Robelle", "title": "Efficient and Simple Algorithms for Fault Tolerant Spanners", "comments": "15 pages. Appeared at PODC 2020. This revision improves the running\n  time slightly and incorporates reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that a version of the greedy algorithm gives a\nconstruction of fault-tolerant spanners that is size-optimal, at least for\nvertex faults. However, the algorithm to construct this spanner is not\npolynomial-time, and the best-known polynomial time algorithm is significantly\nsuboptimal. Designing a polynomial-time algorithm to construct (near-)optimal\nfault-tolerant spanners was given as an explicit open problem in the two most\nrecent papers on fault-tolerant spanners ([Bodwin, Dinitz, Parter, Vassilevka\nWilliams SODA '18] and [Bodwin, Patel PODC '19]). We give a surprisingly simple\nalgorithm which runs in polynomial time and constructs fault-tolerant spanners\nthat are extremely close to optimal (off by only a linear factor in the\nstretch) by modifying the greedy algorithm to run in polynomial time. To\ncomplement this result, we also give simple distributed constructions in both\nthe LOCAL and CONGEST models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:31:02 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 14:32:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dinitz", "Michael", ""], ["Robelle", "Caleb", ""]]}, {"id": "2002.10898", "submitter": "Tesshu Hanaka", "authors": "Hans L. Bodlaender, Tesshu Hanaka, Lars Jaffke, Hirotaka Ono, Yota\n  Otachi and Tom C. van der Zanden", "title": "Hedonic Seat Arrangement Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a variant of hedonic games, called \\textsc{Seat\nArrangement}. The model is defined by a bijection from agents with preferences\nto vertices in a graph. The utility of an agent depends on the neighbors\nassigned in the graph. More precisely, it is the sum over all neighbors of the\npreferences that the agent has towards the agent assigned to the neighbor. We\nfirst consider the price of stability and fairness for different classes of\npreferences. In particular, we show that there is an instance such that the\nprice of fairness ({\\sf PoF}) is unbounded in general. Moreover, we show an\nupper bound $\\tilde{d}(G)$ and an almost tight lower bound $\\tilde{d}(G)-1/4$\nof {\\sf PoF}, where $\\tilde{d}(G)$ is the average degree of an input graph.\nThen we investigate the computational complexity of problems to find certain\n``good'' seat arrangements, say \\textsc{Maximum Welfare Arrangement},\n\\textsc{Maximin Utility Arrangement}, \\textsc{Stable Arrangement}, and\n\\textsc{Envy-free Arrangement}. We give dichotomies of computational complexity\nof four \\textsc{Seat Arrangement} problems from the perspective of the maximum\norder of connected components in an input graph. For the parameterized\ncomplexity, \\textsc{Maximum Welfare Arrangement} can be solved in time\n$n^{O(\\gamma)}$, while it cannot be solved in time $f(\\gamma)^{o(\\gamma)}$\nunder ETH, where $\\gamma$ is the vertex cover number of an input graph.\nMoreover, we show that \\textsc{Maximin Utility Arrangement} and\n\\textsc{Envy-free Arrangement} are weakly NP-hard even on graphs of bounded\nvertex cover number. Finally, we prove that determining whether a stable\narrangement can be obtained from a given arrangement by $k$ swaps is W[1]-hard\nwhen parameterized by $k+\\gamma$, whereas it can be solved in time $n^{O(k)}$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:38:14 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Hanaka", "Tesshu", ""], ["Jaffke", "Lars", ""], ["Ono", "Hirotaka", ""], ["Otachi", "Yota", ""], ["van der Zanden", "Tom C.", ""]]}, {"id": "2002.10927", "submitter": "Nikhil Kumar", "authors": "Naveen Garg, Nikhil Kumar, Andr\\'as Seb\\H{o}", "title": "Integer Plane Multiflow Maximisation : Flow-Cut Gap and\n  One-Quarter-Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we bound the integrality gap and the approximation ratio for\nmaximum plane multiflow problems and deduce bounds on the flow-cut-gap.\nPlanarity means here that the union of the supply and demand graph is planar.\nWe first prove that there exists a multiflow of value at least half of the\ncapacity of a minimum multicut. We then show how to convert any multiflow into\na half-integer one of value at least half of the original multiflow. Finally,\nwe round any half-integer multiflow into an integer multiflow, losing again at\nmost half of the value, in polynomial time, achieving a $1/4$-approximation\nalgorithm for maximum integer multiflows in the plane, and an integer-flow-cut\ngap of $8$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:02:12 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 15:37:10 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Garg", "Naveen", ""], ["Kumar", "Nikhil", ""], ["Seb\u0151", "Andr\u00e1s", ""]]}, {"id": "2002.10958", "submitter": "Alexander Birx", "authors": "Alexander Birx, Yann Disser, Alexander V. Hopp, Christina Karousatou", "title": "Improved Lower Bound for Competitive Graph Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an improved lower bound of 10/3 on the competitive ratio for the\nexploration of an undirected, edge-weighted graph with a single agent that\nneeds to return to the starting location after visiting all vertices. We assume\nthat the agent has full knowledge of all edges incident to visited vertices,\nand, in particular, vertices have unique identifiers. Our bound improves a\nlower bound of 2.5 by Dobrev et al. [SIROCCO'12] and also holds for planar\ngraphs, where it complements an upper bound of 16 by Kalyanasundaram and\nPruhs[TCS'94]. The question whether a constant competitive ratio can be\nachieved in general remains open.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:22:22 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Birx", "Alexander", ""], ["Disser", "Yann", ""], ["Hopp", "Alexander V.", ""], ["Karousatou", "Christina", ""]]}, {"id": "2002.11153", "submitter": "Viswanath Nagarajan", "authors": "Anupam Gupta and Amit Kumar and Viswanath Nagarajan and Xiangkun Shen", "title": "Stochastic Makespan Minimization in Structured Set Systems", "comments": "30 pages, 2 figures", "journal-ref": "IPCO 2020", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic combinatorial optimization problems where the objective\nis to minimize the expected maximum load (a.k.a.\\ the makespan). In this\nframework, we have a set of $n$ tasks and $m$ resources, where each task $j$\nuses some subset of the resources. Tasks have random sizes $X_j$, and our goal\nis to non-adaptively select $t$ tasks to minimize the expected maximum load\nover all resources, where the load on any resource $i$ is the total size of all\nselected tasks that use $i$. For example, when resources are points and tasks\nare intervals in a line, we obtain an $O(\\log\\log m)$-approximation algorithm.\nOur technique is also applicable to other problems with some geometric\nstructure in the relation between tasks and resources; e.g., packing paths,\nrectangles, and \"fat\" objects. Our approach uses a strong LP relaxation using\nthe cumulant generating functions of the random variables. We also show that\nthis LP has an $\\Omega(\\log^* m)$ integrality gap, even for the problem of\nselecting intervals on a line; here $\\log^* m$ is the iterated logarithm\nfunction.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 19:32:39 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 17:28:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Gupta", "Anupam", ""], ["Kumar", "Amit", ""], ["Nagarajan", "Viswanath", ""], ["Shen", "Xiangkun", ""]]}, {"id": "2002.11157", "submitter": "Dina Sokol", "authors": "Dina Sokol", "title": "2-Dimensional Palindromes with $k$ Mismatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper extends the problem of 2-dimensional palindrome search into the\narea of approximate matching. Using the Hamming distance as the measure, we\nsearch for 2D palindromes that allow up to $k$ mismatches. We consider two\ndifferent definitions of 2D palindromes and describe efficient algorithms for\nboth of them. The first definition implies a square, while the second\ndefinition (also known as a \\emph{centrosymmetric factor}), can be any\nrectangular shape. Given a text of size $n \\times m$, the time complexity of\nthe first algorithm is $O(nm (\\log m + \\log n + k))$ and for the second\nalgorithm it is $O(nm(\\log m + k) + occ)$ where $occ$ is the size of the\noutput.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 19:56:08 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Sokol", "Dina", ""]]}, {"id": "2002.11171", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Danupon Nanongkai and\n  Xiaowei Wu", "title": "Dynamic Set Cover: Improved Amortized and Worst-Case Update Time", "comments": "This new version contains an additional result on worst-case update\n  time and a revised extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the dynamic minimum set cover problem, a challenge is to minimize the\nupdate time while guaranteeing close to the optimal $\\min(O(\\log n), f)$\napproximation factor. (Throughout, $m$, $n$, $f$, and $C$ are parameters\ndenoting the maximum number of sets, number of elements, frequency, and the\ncost range.) In the high-frequency range, when $f=\\Omega(\\log n)$, this was\nachieved by a deterministic $O(\\log n)$-approximation algorithm with $O(f \\log\nn)$ amortized update time [Gupta et al. STOC'17]. In the low-frequency range,\nthe line of work by Gupta et al. [STOC'17], Abboud et al. [STOC'19], and\nBhattacharya et al. [ICALP'15, IPCO'17, FOCS'19] led to a deterministic\n$(1+\\epsilon)f$-approximation algorithm with $O(f \\log (Cn)/\\epsilon^2)$\namortized update time. In this paper we improve the latter update time and\nprovide the first bounds that subsume (and sometimes improve) the\nstate-of-the-art dynamic vertex cover algorithms. We obtain:\n  1. $(1+\\epsilon)f$-approximation ratio in $O(f\\log^2 (Cn)/\\epsilon^3)$\nworst-case update time: No non-trivial worst-case update time was previously\nknown for dynamic set cover. Our bound subsumes and improves by a logarithmic\nfactor the $O(\\log^3 n/\\text{poly}(\\epsilon))$ worst-case update time for\nunweighted dynamic vertex cover (i.e., when $f=2$ and $C=1$) by Bhattacharya et\nal. [SODA'17].\n  2. $(1+\\epsilon)f$-approximation ratio in\n$O\\left((f^2/\\epsilon^3)+(f/\\epsilon^2) \\log C\\right)$ amortized update time:\nThis result improves the previous $O(f \\log (Cn)/\\epsilon^2)$ update time bound\nfor most values of $f$ in the low-frequency range, i.e. whenever $f=o(\\log n)$.\nIt is the first that is independent of $m$ and $n$. It subsumes the constant\namortized update time of Bhattacharya and Kulkarni [SODA'19] for unweighted\ndynamic vertex cover (i.e., when $f = 2$ and $C = 1$).\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 20:54:36 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 10:29:56 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""], ["Wu", "Xiaowei", ""]]}, {"id": "2002.11237", "submitter": "Jack Murtagh", "authors": "Dean Doron, Jack Murtagh, Salil Vadhan, David Zuckerman", "title": "Spectral Sparsification via Bounded-Independence Sampling", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic, nearly logarithmic-space algorithm for mild spectral\nsparsification of undirected graphs. Given a weighted, undirected graph $G$ on\n$n$ vertices described by a binary string of length $N$, an integer $k\\leq \\log\nn$, and an error parameter $\\epsilon > 0$, our algorithm runs in space\n$\\tilde{O}(k\\log (N\\cdot w_{\\mathrm{max}}/w_{\\mathrm{min}}))$ where\n$w_{\\mathrm{max}}$ and $w_{\\mathrm{min}}$ are the maximum and minimum edge\nweights in $G$, and produces a weighted graph $H$ with\n$\\tilde{O}(n^{1+2/k}/\\epsilon^2)$ edges that spectrally approximates $G$, in\nthe sense of Spielmen and Teng [ST04], up to an error of $\\epsilon$.\n  Our algorithm is based on a new bounded-independence analysis of Spielman and\nSrivastava's effective resistance based edge sampling algorithm [SS08] and uses\nresults from recent work on space-bounded Laplacian solvers [MRSV17]. In\nparticular, we demonstrate an inherent tradeoff (via upper and lower bounds)\nbetween the amount of (bounded) independence used in the edge sampling\nalgorithm, denoted by $k$ above, and the resulting sparsity that can be\nachieved.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:49:41 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 05:17:06 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Doron", "Dean", ""], ["Murtagh", "Jack", ""], ["Vadhan", "Salil", ""], ["Zuckerman", "David", ""]]}, {"id": "2002.11271", "submitter": "Manuel Lafond", "authors": "Garance Cordonnier, Manuel Lafond", "title": "Comparing copy-number profiles under multi-copy amplifications and\n  deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During cancer progression, malignant cells accumulate somatic mutations that\ncan lead to genetic aberrations. In particular, evolutionary events akin to\nsegmental duplications or deletions can alter the copy-number profile (CNP) of\na set of genes in a genome. Our aim is to compute the evolutionary distance\nbetween two cells for which only CNPs are known. This asks for the minimum\nnumber of segmental amplifications and deletions to turn one CNP into another.\nThis was recently formalized into a model where each event is assumed to alter\na copy-number by $1$ or $-1$, even though these events can affect large\nportions of a chromosome. We propose a general cost framework where an event\ncan modify the copy-number of a gene by larger amounts. We show that any cost\nscheme that allows segmental deletions of arbitrary length makes computing the\ndistance strongly NP-hard. We then devise a factor $2$ approximation algorithm\nfor the problem when copy-numbers are non-zero and provide an implementation\ncalled \\textsf{cnp2cnp}. We evaluate our approach experimentally by\nreconstructing simulated cancer phylogenies from the pairwise distances\ninferred by \\textsf{cnp2cnp} and compare it against two other alternatives,\nnamely the \\textsf{MEDICC} distance and the Euclidean distance. The\nexperimental results show that our distance yields more accurate phylogenies on\naverage than these alternatives if the given CNPs are error-free, but that the\n\\textsf{MEDICC} distance is slightly more robust against error in the data. In\nall cases, our experiments show that either our approach or the \\textsf{MEDICC}\napproach should preferred over the Euclidean distance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:02:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Cordonnier", "Garance", ""], ["Lafond", "Manuel", ""]]}, {"id": "2002.11313", "submitter": "Stephane Breuils", "authors": "Stephane Breuils, Vincent Nozick, Akihiro Sugimoto", "title": "Computational Aspects of Geometric Algebra Products of Two Homogeneous\n  Multivectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on time and memory costs of products in geometric algebra have been\nlimited to cases where multivectors with multiple grades have only non-zero\nelements. This allows to design efficient algorithms for a generic purpose;\nhowever, it does not reflect the practical usage of geometric algebra. Indeed,\nin applications related to geometry, multivectors are likely to be full\nhomogeneous, having their non-zero elements over a single grade. In this paper,\nwe provide a complete computational study on geometric algebra products of two\nfull homogeneous multivectors, that is, the outer, inner, and geometric\nproducts of two full homogeneous multivectors. We show tight bounds on the\nnumber of the arithmetic operations required for these products. We also show\nthat algorithms exist that achieve this number of arithmetic operations.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:06:12 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Breuils", "Stephane", ""], ["Nozick", "Vincent", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "2002.11342", "submitter": "Alireza Farhadi", "authors": "Alireza Farhadi, MohammadTaghi Hajiaghayi, Aviad Rubinstein, Saeed\n  Seddighin", "title": "Asymmetric Streaming Algorithms for Edit Distance and LCS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance (ED) and longest common subsequence (LCS) are two\nfundamental problems which quantify how similar two strings are to one another.\nIn this paper, we consider these problems in the asymmetric streaming model\nintroduced by Andoni et al. (FOCS'10) and Saks and Seshadhri (SODA'13). In this\nmodel we have random access to one string and streaming access the other\nstring. Our main contribution is a constant factor approximation algorithm for\nED with the memory of $\\tilde O(n^{\\delta})$ for any constant $\\delta > 0$. In\naddition to this, we present an upper bound of $\\tilde O_\\epsilon(\\sqrt{n})$ on\nthe memory needed to approximate ED or LCS within a factor $1+\\epsilon$. All\nour algorithms are deterministic and run in a single pass.\n  For approximating ED within a constant factor, we discover yet another\napplication of triangle inequality, this time in the context of streaming\nalgorithms. Triangle inequality has been previously used to obtain subquadratic\ntime approximation algorithms for ED. Our technique is novel and elegantly\nutilizes triangle inequality to save memory at the expense of an exponential\nincrease in the runtime.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 08:04:35 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 16:31:24 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Rubinstein", "Aviad", ""], ["Seddighin", "Saeed", ""]]}, {"id": "2002.11391", "submitter": "Shivdutt Sharma", "authors": "Bireswar Das, Shivdutt Sharma, and P.R.Vaidyanathan", "title": "Space Efficient Representations of Finite Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cayley table representation of a group uses $\\mathcal{O}(n^2)$ words for\na group of order $n$ and answers multiplication queries in time\n$\\mathcal{O}(1)$. It is interesting to ask if there is a $o(n^2)$ space\nrepresentation of groups that still has $\\mathcal{O}(1)$ query-time. We show\nthat for any $\\delta$, $\\frac{1}{\\log n} \\le \\delta \\le 1$, there is an\n$\\mathcal{O}(\\frac{n^{1 +\\delta}}{\\delta})$ space representation for groups of\norder $n$ with $\\mathcal{O}(\\frac{1}{\\delta})$ query-time.\n  We also show that for Z-groups, simple groups and several group classes\ndefined in terms of semidirect product, there are linear space representations\nwith at most logarithmic query-time.\n  Farzan and Munro (ISSAC'06) defined a model for group representation and gave\na succinct data structure for abelian groups with constant query-time. They\nasked if their result can be extended to categorically larger group classes. We\nconstruct data structures in their model for Hamiltonian groups and some other\nclasses of groups with constant query-time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 10:16:44 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Das", "Bireswar", ""], ["Sharma", "Shivdutt", ""], ["Vaidyanathan", "P. R.", ""]]}, {"id": "2002.11428", "submitter": "Marcus Kaiser", "authors": "Marcus Kaiser", "title": "Computation of Dynamic Equilibria in Series-Parallel Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamic equilibria for flows over time under the fluid queuing\nmodel. In this model, queues on the links of a network take care of flow\npropagation. Flow enters the network at a single source and leaves at a single\nsink. In a dynamic equilibrium, every infinitesimally small flow particle\nreaches the sink as early as possible given the pattern of the rest of the\nflow. While this model has been examined for many decades, progress has been\nrelatively recent. In particular, the derivatives of dynamic equilibria have\nbeen characterized as thin flows with resetting, which allowed for more\nstructural results. Our two main results are based on the formulation of thin\nflows with resetting as linear complementarity problem and its analysis. We\npresent a constructive proof of existence for dynamic equilibria if the inflow\nrate is right-monotone. The complexity of computing thin flows with resetting,\nwhich occurs as a subproblem in this method, is still open. We settle it for\nthe class of two-terminal series-parallel networks by giving a recursive\nalgorithm that solves the problem for all flow values simultaneously in\npolynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:16:24 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 11:18:07 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kaiser", "Marcus", ""]]}, {"id": "2002.11541", "submitter": "Mano Vikash Janardhanan", "authors": "Mano Vikash Janardhanan, Lev Reyzin", "title": "On Learning a Hidden Directed Graph with Path Queries", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of reconstructing a directed graph\nusing path queries. In this query model of learning, a graph is hidden from the\nlearner, and the learner can access information about it with path queries. For\na source and destination node, a path query returns whether there is a directed\npath from the source to the destination node in the hidden graph. In this paper\nwe first give bounds for learning graphs on $n$ vertices and $k$ strongly\nconnected components. We then study the case of bounded degree directed trees\nand give new algorithms for learning \"almost-trees\" -- directed trees to which\nextra edges have been added. We also give some lower bound constructions\njustifying our approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 14:49:20 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 05:23:13 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Janardhanan", "Mano Vikash", ""], ["Reyzin", "Lev", ""]]}, {"id": "2002.11557", "submitter": "David Garc\\'ia-Soriano", "authors": "David Garc\\'ia-Soriano, Konstantin Kutzkov, Francesco Bonchi,\n  Charalampos Tsourakakis", "title": "Query-Efficient Correlation Clustering", "comments": "To appear in WWW 2020", "journal-ref": null, "doi": "10.1145/3366423.3380220", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation clustering is arguably the most natural formulation of\nclustering. Given n objects and a pairwise similarity measure, the goal is to\ncluster the objects so that, to the best possible extent, similar objects are\nput in the same cluster and dissimilar objects are put in different clusters.\n  A main drawback of correlation clustering is that it requires as input the\n$\\Theta(n^2)$ pairwise similarities. This is often infeasible to compute or\neven just to store. In this paper we study \\emph{query-efficient} algorithms\nfor correlation clustering. Specifically, we devise a correlation clustering\nalgorithm that, given a budget of $Q$ queries, attains a solution whose\nexpected number of disagreements is at most $3\\cdot OPT + O(\\frac{n^3}{Q})$,\nwhere $OPT$ is the optimal cost for the instance. Its running time is $O(Q)$,\nand can be easily made non-adaptive (meaning it can specify all its queries at\nthe outset and make them in parallel) with the same guarantees. Up to constant\nfactors, our algorithm yields a provably optimal trade-off between the number\nof queries $Q$ and the worst-case error attained, even for adaptive algorithms.\n  Finally, we perform an experimental study of our proposed method on both\nsynthetic and real data, showing the scalability and the accuracy of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:18:20 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Garc\u00eda-Soriano", "David", ""], ["Kutzkov", "Konstantin", ""], ["Bonchi", "Francesco", ""], ["Tsourakakis", "Charalampos", ""]]}, {"id": "2002.11593", "submitter": "Antonio Fern\\'andez Anta", "authors": "Vicent Cholvi and Antonio Fernandez Anta and Chryssis Georgiou and\n  Nicolas Nicolaou and Michel Raynal", "title": "Appending Atomically in Byzantine Distributed Ledgers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Distributed Ledger Object (DLO) is a concurrent object that maintains a\ntotally ordered sequence of records, and supports two basic operations: append,\nwhich appends a record at the end of the sequence, and get, which returns the\nsequence of records. In this work we provide a proper formalization of a\nByzantine-tolerant Distributed Ledger Object (BDLO), which is a DLO in a\ndistributed system in which processes may deviate arbitrarily from their\nindented behavior, i.e. they may be Byzantine. Our formal definition is\naccompanied by algorithms to implement BDLOs by utilizing an underlying\nByzantine Atomic Broadcast service.\n  We then utilize the BDLO implementations to solve the Atomic Appends problem\nagainst Byzantine processes. The Atomic Appends problem emerges when several\nclients have records to append, the record of each client has to be appended to\na different BDLO, and it must be guaranteed that either all records are\nappended or none. We present distributed algorithms implementing solutions for\nthe Atomic Appends problem when the clients (which are involved in the appends)\nand the servers (which maintain the BDLOs) may be Byzantine.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:22:47 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Cholvi", "Vicent", ""], ["Anta", "Antonio Fernandez", ""], ["Georgiou", "Chryssis", ""], ["Nicolaou", "Nicolas", ""], ["Raynal", "Michel", ""]]}, {"id": "2002.11617", "submitter": "Udit Agarwal", "authors": "Udit Agarwal", "title": "A Polynomial Time Algorithm for Almost Optimal Vertex Fault Tolerant\n  Spanners", "comments": "Incorrect statement of Lemma 5.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first polynomial time algorithm for the f vertex fault\ntolerant spanner problem, which achieves almost optimal spanner size. Our\nalgorithm for constructing f vertex fault tolerant spanner takes $O(k\\cdot\nn\\cdot m^2 \\cdot W)$ time, where W is the maximum edge weight, and constructs a\nspanner of size $O(n^{1+1/k}f^{1-1/k}\\cdot (\\log n)^{1-1/k})$. Our spanner has\nalmost optimal size and is at most a $\\log n$ factor away from the upper bound\non the worst-case size. Prior to this work, no other polynomial time algorithm\nwas known for constructing f vertex fault tolerant spanner with optimal size.\n  Our algorithm is based on first greedily constructing a hitting set for the\ncollection of paths of weight at most $k \\cdot w(u,v)$ between the endpoints u\nand v of an edge (u,v) and then using this set to decide whether the edge (u,v)\nneeds to be added to the growing spanner.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:01:15 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 17:39:20 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Agarwal", "Udit", ""]]}, {"id": "2002.11622", "submitter": "Guillermo de Bernardo", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Antonio\n  Fari\\~na", "title": "Revisiting compact RDF stores based on k2-trees", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new compact representation to efficiently store and query large\nRDF datasets in main memory. Our proposal, called BMatrix, is based on the\nk2-tree, a data structure devised to represent binary matrices in a compressed\nway, and aims at improving the results of previous state-of-the-art\nalternatives, especially in datasets with a relatively large number of\npredicates. We introduce our technique, together with some improvements on the\nbasic k2-tree that can be applied to our solution in order to boost\ncompression. Experimental results in the flagship RDF dataset DBPedia show that\nour proposal achieves better compression than existing alternatives, while\nyielding competitive query times, particularly in the most frequent triple\npatterns and in queries with unbound predicate, in which we outperform existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:03:28 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Fari\u00f1a", "Antonio", ""]]}, {"id": "2002.11650", "submitter": "Chara Podimata", "authors": "Akshay Krishnamurthy, Thodoris Lykouris, Chara Podimata, and Robert\n  Schapire", "title": "Contextual Search in the Presence of Irrational Agents", "comments": "Compared to the first version titled \"Corrupted Multidimensional\n  Binary Search: Learning in the Presence of Irrational Agents\", this version\n  provides a broader scope of behavioral models of irrationality, specifies how\n  the results apply to different loss functions, and discusses the power and\n  limitations of additional algorithmic approaches", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study contextual search, a generalization of binary search in higher\ndimensions, which captures settings such as feature-based dynamic pricing.\nStandard game-theoretic formulations of this problem assume that agents act in\naccordance with a specific behavioral model. In practice, however, some agents\nmay not prescribe to the dominant behavioral model or may act in ways that are\nseemingly arbitrarily irrational. Existing algorithms heavily depend on the\nbehavioral model being (approximately) accurate for all agents and have poor\nperformance in the presence of even a few such arbitrarily irrational agents.\n  We initiate the study of contextual search when some of the agents can behave\nin ways inconsistent with the underlying behavioral model. In particular, we\nprovide two algorithms, one built on robustifying multidimensional binary\nsearch methods and one on translating the setting to a proxy setting\nappropriate for gradient descent. Our techniques draw inspiration from learning\ntheory, game theory, high-dimensional geometry, and convex analysis.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:25:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 16:16:00 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 17:26:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Lykouris", "Thodoris", ""], ["Podimata", "Chara", ""], ["Schapire", "Robert", ""]]}, {"id": "2002.11661", "submitter": "Sebastian Macaluso", "authors": "Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Ji-Ah Lee,\n  Patrick Flaherty, Kyle Cranmer, Andrew McGregor, Andrew McCallum", "title": "Data Structures & Algorithms for Exact Inference in Hierarchical\n  Clustering", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a fundamental task often used to discover\nmeaningful structures in data, such as phylogenetic trees, taxonomies of\nconcepts, subtypes of cancer, and cascades of particle decays in particle\nphysics. Typically approximate algorithms are used for inference due to the\ncombinatorial number of possible hierarchical clusterings. In contrast to\nexisting methods, we present novel dynamic-programming algorithms for\n\\emph{exact} inference in hierarchical clustering based on a novel trellis data\nstructure, and we prove that we can exactly compute the partition function,\nmaximum likelihood hierarchy, and marginal probabilities of sub-hierarchies and\nclusters. Our algorithms scale in time and space proportional to the powerset\nof $N$ elements which is super-exponentially more efficient than explicitly\nconsidering each of the (2N-3)!! possible hierarchies. Also, for larger\ndatasets where our exact algorithms become infeasible, we introduce an\napproximate algorithm based on a sparse trellis that compares well to other\nbenchmarks. Exact methods are relevant to data analyses in particle physics and\nfor finding correlations among gene expression in cancer genomics, and we give\nexamples in both areas, where our algorithms outperform greedy and beam search\nbaselines. In addition, we consider Dasgupta's cost with synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:43:53 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 16:19:28 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 15:18:02 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Greenberg", "Craig S.", ""], ["Macaluso", "Sebastian", ""], ["Monath", "Nicholas", ""], ["Lee", "Ji-Ah", ""], ["Flaherty", "Patrick", ""], ["Cranmer", "Kyle", ""], ["McGregor", "Andrew", ""], ["McCallum", "Andrew", ""]]}, {"id": "2002.11679", "submitter": "Biaoshuai Tao", "authors": "Grant Schoenebeck, Biaoshuai Tao, Fang-Yi Yu", "title": "Limitations of Greed: Influence Maximization in Undirected Networks\n  Re-visited", "comments": "36 pages, 1 figure, accepted at AAMAS'20: International Conference on\n  Autonomous Agents and Multi-Agent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the influence maximization problem (selecting $k$ seeds in a\nnetwork maximizing the expected total influence) on undirected graphs under the\nlinear threshold model. On the one hand, we prove that the greedy algorithm\nalways achieves a $(1 - (1 - 1/k)^k + \\Omega(1/k^3))$-approximation, showing\nthat the greedy algorithm does slightly better on undirected graphs than the\ngeneric $(1- (1 - 1/k)^k)$ bound which also applies to directed graphs. On the\nother hand, we show that substantial improvement on this bound is impossible by\npresenting an example where the greedy algorithm can obtain at most a $(1- (1 -\n1/k)^k + O(1/k^{0.2}))$ approximation. This result stands in contrast to the\nprevious work on the independent cascade model. Like the linear threshold\nmodel, the greedy algorithm obtains a $(1-(1-1/k)^k)$-approximation on directed\ngraphs in the independent cascade model. However, Khanna and Lucier showed\nthat, in undirected graphs, the greedy algorithm performs substantially better:\na $(1-(1-1/k)^k + c)$ approximation for constant $c > 0$. Our results show\nthat, surprisingly, no such improvement occurs in the linear threshold model.\nFinally, we show that, under the linear threshold model, the approximation\nratio $(1 - (1 - 1/k)^k)$ is tight if 1) the graph is directed or 2) the\nvertices are weighted. In other words, under either of these two settings, the\ngreedy algorithm cannot achieve a $(1 - (1 - 1/k)^k + f(k))$-approximation for\nany positive function $f(k)$. The result in setting 2) is again in a sharp\ncontrast to Khanna and Lucier's $(1 - (1 - 1/k)^k + c)$-approximation result\nfor the independent cascade model, where the $(1 - (1 - 1/k)^k + c)$\napproximation guarantee can be extended to the setting where vertices are\nweighted. We also discuss extensions to more generalized settings including\nthose with edge-weighted graphs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:16:31 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Schoenebeck", "Grant", ""], ["Tao", "Biaoshuai", ""], ["Yu", "Fang-Yi", ""]]}, {"id": "2002.11691", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Adri\\'an G\\'omez-Brand\\'on", "title": "Bitvectors with runs and the successor/predecessor problem", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "2020 Data Compression Conference (DCC)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successor and predecessor problem consists of obtaining the closest value\nin a set of integers, greater/smaller than a given value. This problem has\ninteresting applications, like the intersection of inverted lists. It can be\neasily modeled by using a bitvector of size $n$ and its operations rank and\nselect. However, there is a practical approach, which keeps the best\ntheoretical bounds, and allows to solve successor and predecessor more\nefficiently. Based on that technique, we designed a novel compact data\nstructure for bitvectors with $k$ runs that achieves access, rank, and\nsuccessor/predecessor in $O(1)$ time by consuming space $O(\\sqrt{kn})$ bits. In\npractice, it obtains a compression ratio of $0.04\\%-26.33\\%$ when the runs are\nlarger than $100$, and becomes the fastest technique, which considers\ncompressibility, in successor/predecessor queries. Besides, we present a\nrecursive variant of our structure, which tends to $O(k)$ bits and takes\n$O(\\log \\frac{n}{k})$ time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:32:32 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""]]}, {"id": "2002.11818", "submitter": "Therese Biedl", "authors": "Therese Biedl and Fabian Klute", "title": "Finding large matchings in 1-planar graphs of minimum degree 3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matching is a set of edges without common endpoint. It was recently shown\nthat every 1-planar graph (i.e., a graph that can be drawn in the plane with at\nmost one crossing per edge) that has minimum degree 3 has a matching of size at\nleast $\\frac{n+12}{7}$, and this is tight for some graphs. The proof did not\ncome with an algorithm to find the matching more efficiently than a\ngeneral-purpose maximum-matching algorithm. In this paper, we give such an\nalgorithm. More generally, we show that any matching that has no augmenting\npaths of length 9 or less has size at least $\\frac{n+12}{7}$ in a 1-planar\ngraph with minimum degree 3.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 22:04:43 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 18:25:36 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Biedl", "Therese", ""], ["Klute", "Fabian", ""]]}, {"id": "2002.11830", "submitter": "Nicolas Dupin", "authors": "Nicolas Dupin", "title": "Polynomial algorithms for p-dispersion problems in a 2d Pareto Front", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Having many best compromise solutions for bi-objective optimization problems,\nthis paper studies p-dispersion problems to select $p\\geqslant 2$\nrepresentative points in the Pareto Front(PF). Four standard variants of\np-dispersion are considered. A novel variant, denoted Max-Sum-Neighbor\np-dispersion, is introduced for the specific case of a 2d PF. Firstly, it is\nproven that $2$-dispersion and $3$-dispersion problems are solvable in $O(n)$\ntime in a 2d PF. Secondly, dynamic programming algorithms are designed for\nthree p-dispersion variants, proving polynomial complexities in a 2d PF. The\nMax-Min p-dispersion problem is proven solvable in $O(pn\\log n)$ time and\n$O(n)$ memory space. The Max-Sum-Min p-dispersion problem is proven solvable in\n$O(pn^3)$ time and $O(pn^2)$ space. The Max-Sum-Neighbor p-dispersion problem\nis proven solvable in $O(pn^2)$ time and $O(pn)$ space. Complexity results and\nparallelization issues are discussed in regards to practical implementation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 22:52:58 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Dupin", "Nicolas", ""]]}, {"id": "2002.11880", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi", "title": "Stochastic Matching with Few Queries: $(1-\\varepsilon)$ Approximation", "comments": "A version of this paper is to appear at STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given an arbitrary graph $G=(V, E)$ and know that each\nedge in $E$ is going to be realized independently with some probability $p$.\nThe goal in the stochastic matching problem is to pick a sparse subgraph $Q$ of\n$G$ such that the realized edges in $Q$, in expectation, include a matching\nthat is approximately as large as the maximum matching among the realized edges\nof $G$. The maximum degree of $Q$ can depend on $p$, but not on the size of\n$G$.\n  This problem has been subject to extensive studies over the years and the\napproximation factor has been improved from $0.5$ to $0.5001$ to $0.6568$ and\neventually to $2/3$. In this work, we analyze a natural sampling-based\nalgorithm and show that it can obtain all the way up to $(1-\\epsilon)$\napproximation, for any constant $\\epsilon > 0$.\n  A key and of possible independent interest component of our analysis is an\nalgorithm that constructs a matching on a stochastic graph, which among some\nother important properties, guarantees that each vertex is matched\nindependently from the vertices that are sufficiently far. This allows us to\nbypass a previously known barrier towards achieving $(1-\\epsilon)$\napproximation based on existence of dense Ruzsa-Szemer\\'edi graphs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:33:03 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""]]}, {"id": "2002.11904", "submitter": "Hu Ding", "authors": "Hu Ding and Zixiu Wang", "title": "Layered Sampling for Robust Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world, our datasets often contain outliers. Moreover, the outliers\ncan seriously affect the final machine learning result. Most existing\nalgorithms for handling outliers take high time complexities (e.g. quadratic or\ncubic complexity). {\\em Coreset} is a popular approach for compressing data so\nas to speed up the optimization algorithms. However, the current coreset\nmethods cannot be easily extended to handle the case with outliers. In this\npaper, we propose a new variant of coreset technique, {\\em layered sampling},\nto deal with two fundamental robust optimization problems: {\\em\n$k$-median/means clustering with outliers} and {\\em linear regression with\noutliers}. This new coreset method is in particular suitable to speed up the\niterative algorithms (which often improve the solution within a local range)\nfor those robust optimization problems. Moreover, our method is easy to be\nimplemented in practice. We expect that our framework of layered sampling will\nbe applicable to other robust optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 04:01:59 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ding", "Hu", ""], ["Wang", "Zixiu", ""]]}, {"id": "2002.11933", "submitter": "Hu Ding", "authors": "Hu Ding, Fan Yang", "title": "On Metric DBSCAN with Low Doubling Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density based clustering method {\\em Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN)} is a popular method for outlier recognition\nand has received tremendous attention from many different areas. A major issue\nof the original DBSCAN is that the time complexity could be as large as\nquadratic. Most of existing DBSCAN algorithms focus on developing efficient\nindex structures to speed up the procedure in low-dimensional Euclidean space.\nHowever, the research of DBSCAN in high-dimensional Euclidean space or general\nmetric space is still quite limited, to the best of our knowledge. In this\npaper, we consider the metric DBSCAN problem under the assumption that the\ninliers (excluding the outliers) have a low doubling dimension. We apply a\nnovel randomized $k$-center clustering idea to reduce the complexity of range\nquery, which is the most time consuming step in the whole DBSCAN procedure. Our\nproposed algorithms do not need to build any complicated data structures and\nare easy to be implemented in practice. The experimental results show that our\nalgorithms can significantly outperform the existing DBSCAN algorithms in terms\nof running time.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 06:05:39 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ding", "Hu", ""], ["Yang", "Fan", ""]]}, {"id": "2002.12034", "submitter": "Inbal Talgam-Cohen", "authors": "Paul Duetting, Tim Roughgarden, Inbal Talgam-Cohen", "title": "The Complexity of Contracts", "comments": "An extended abstract appeared in Proceedings of the ACM-SIAM\n  Symposium on Discrete Algorithms, 2020 (SODA'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of computing (near-)optimal contracts in succinctly\nrepresentable principal-agent settings. Here optimality means maximizing the\nprincipal's expected payoff over all incentive-compatible contracts---known in\neconomics as \"second-best\" solutions. We also study a natural relaxation to\napproximately incentive-compatible contracts.\n  We focus on principal-agent settings with succinctly described (and\nexponentially large) outcome spaces. We show that the computational complexity\nof computing a near-optimal contract depends fundamentally on the number of\nagent actions. For settings with a constant number of actions, we present a\nfully polynomial-time approximation scheme (FPTAS) for the separation oracle of\nthe dual of the problem of minimizing the principal's payment to the agent, and\nuse this subroutine to efficiently compute a delta-incentive-compatible\n(delta-IC) contract whose expected payoff matches or surpasses that of the\noptimal IC contract.\n  With an arbitrary number of actions, we prove that the problem is hard to\napproximate within any constant c. This inapproximability result holds even for\ndelta-IC contracts where delta is a sufficiently rapidly-decaying function of\nc. On the positive side, we show that simple linear delta-IC contracts with\nconstant delta are sufficient to achieve a constant-factor approximation of the\n\"first-best\" (full-welfare-extracting) solution, and that such a contract can\nbe computed in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 11:03:34 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Duetting", "Paul", ""], ["Roughgarden", "Tim", ""], ["Talgam-Cohen", "Inbal", ""]]}, {"id": "2002.12050", "submitter": "Tirso Varela Rodeiro", "authors": "Nieves R. Brisaboa, Antonio Fari\\~na, Gonzalo Navarro and Tirso V.\n  Rodeiro", "title": "Semantrix: A Compressed Semantic Matrix", "comments": "10 pages, Data Compression Conference 2020. This research has\n  received funding from the European Union's Horizon 2020 research and\n  innovation programme under the Marie Sk{\\l}odowska-Curie Actions\n  H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a compact data structure to represent both the duration and length\nof homogeneous segments of trajectories from moving objects in a way that, as a\ndata warehouse, it allows us to efficiently answer cumulative queries. The\ndivision of trajectories into relevant segments has been studied in the\nliterature under the topic of Trajectory Segmentation. In this paper, we design\na data structure to compactly represent them and the algorithms to answer the\nmore relevant queries. We experimentally evaluate our proposal in the real\ncontext of an enterprise with mobile workers (truck drivers) where we aim at\nanalyzing the time they spend in different activities. To test our proposal\nunder higher stress conditions we generated a huge amount of synthetic\nrealistic trajectories and evaluated our system with those data to have a good\nidea about its space needs and its efficiency when answering different types of\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 11:48:33 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 13:20:14 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Fari\u00f1a", "Antonio", ""], ["Navarro", "Gonzalo", ""], ["Rodeiro", "Tirso V.", ""]]}, {"id": "2002.12159", "submitter": "Sahil Singla", "authors": "Anupam Gupta and Sahil Singla", "title": "Random-Order Models", "comments": "Preprint of Chapter 11 in \"Beyond the Worst-Case Analysis of\n  Algorithms\", Cambridge University Press, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter introduces the \\emph{random-order model} in online algorithms.\nIn this model, the input is chosen by an adversary, then randomly permuted\nbefore being presented to the algorithm. This reshuffling often weakens the\npower of the adversary and allows for improved algorithmic guarantees. We show\nsuch improvements for two broad classes of problems: packing problems where we\nmust pick a constrained set of items to maximize total value, and covering\nproblems where we must satisfy given requirements at minimum total cost. We\nalso discuss how random-order model relates to other stochastic models used for\nnon-worst-case competitive analysis.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 23:54:44 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Gupta", "Anupam", ""], ["Singla", "Sahil", ""]]}, {"id": "2002.12321", "submitter": "Wanrong Zhang", "authors": "Wanrong Zhang, Gautam Kamath, Rachel Cummings", "title": "PAPRIKA: Private Online False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hypothesis testing, a false discovery occurs when a hypothesis is\nincorrectly rejected due to noise in the sample. When adaptively testing\nmultiple hypotheses, the probability of a false discovery increases as more\ntests are performed. Thus the problem of False Discovery Rate (FDR) control is\nto find a procedure for testing multiple hypotheses that accounts for this\neffect in determining the set of hypotheses to reject. The goal is to minimize\nthe number (or fraction) of false discoveries, while maintaining a high true\npositive rate (i.e., correct discoveries).\n  In this work, we study False Discovery Rate (FDR) control in multiple\nhypothesis testing under the constraint of differential privacy for the sample.\nUnlike previous work in this direction, we focus on the online setting, meaning\nthat a decision about each hypothesis must be made immediately after the test\nis performed, rather than waiting for the output of all tests as in the offline\nsetting. We provide new private algorithms based on state-of-the-art results in\nnon-private online FDR control. Our algorithms have strong provable guarantees\nfor privacy and statistical performance as measured by FDR and power. We also\nprovide experimental results to demonstrate the efficacy of our algorithms in a\nvariety of data environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:42:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:06:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhang", "Wanrong", ""], ["Kamath", "Gautam", ""], ["Cummings", "Rachel", ""]]}, {"id": "2002.12354", "submitter": "Hu Ding", "authors": "Hu Ding, Tan Chen, Fan Yang, Mingyue Wang", "title": "A Data-Dependent Algorithm for Querying Earth Mover's Distance with Low\n  Doubling Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the following query problem: given two weighted\npoint sets $A$ and $B$ in the Euclidean space $\\mathbb{R}^d$, we want to\nquickly determine that whether their earth mover's distance (EMD) is larger or\nsmaller than a pre-specified threshold $T\\geq 0$. The problem finds a number of\nimportant applications in the fields of machine learning and data mining. In\nparticular, we assume that the dimensionality $d$ is not fixed and the sizes\n$|A|$ and $|B|$ are large. Therefore, most of existing EMD algorithms are not\nquite efficient to solve this problem due to their high complexities. Here, we\nconsider the problem under the assumption that $A$ and $B$ have low doubling\ndimensions, which is common for high-dimensional data in real world. Inspired\nby the geometric method {\\em net tree}, we propose a novel ``data-dependent''\nalgorithm to avoid directly computing the EMD between $A$ and $B$, so as to\nsolve this query problem more efficiently. We also study the performance of our\nmethod on synthetic and real datasets. The experimental results suggest that\nour method can save a large amount of running time comparing with existing EMD\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:43:47 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 02:25:56 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ding", "Hu", ""], ["Chen", "Tan", ""], ["Yang", "Fan", ""], ["Wang", "Mingyue", ""]]}, {"id": "2002.12538", "submitter": "Nave Frost", "authors": "Sanjoy Dasgupta, Nave Frost, Michal Moshkovitz, Cyrus Rashtchian", "title": "Explainable $k$-Means and $k$-Medians Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a popular form of unsupervised learning for geometric data.\nUnfortunately, many clustering algorithms lead to cluster assignments that are\nhard to explain, partially because they depend on all the features of the data\nin a complicated way. To improve interpretability, we consider using a small\ndecision tree to partition a data set into clusters, so that clusters can be\ncharacterized in a straightforward manner. We study this problem from a\ntheoretical viewpoint, measuring cluster quality by the $k$-means and\n$k$-medians objectives: Must there exist a tree-induced clustering whose cost\nis comparable to that of the best unconstrained clustering, and if so, how can\nit be found? In terms of negative results, we show, first, that popular\ntop-down decision tree algorithms may lead to clusterings with arbitrarily\nlarge cost, and second, that any tree-induced clustering must in general incur\nan $\\Omega(\\log k)$ approximation factor compared to the optimal clustering. On\nthe positive side, we design an efficient algorithm that produces explainable\nclusters using a tree with $k$ leaves. For two means/medians, we show that a\nsingle threshold cut suffices to achieve a constant factor approximation, and\nwe give nearly-matching lower bounds. For general $k \\geq 2$, our algorithm is\nan $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$\napproximation to the optimal $k$-means. Prior to our work, no algorithms were\nknown with provable guarantees independent of dimension and input size.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 04:21:53 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 00:43:14 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Dasgupta", "Sanjoy", ""], ["Frost", "Nave", ""], ["Moshkovitz", "Michal", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "2002.12662", "submitter": "Bella Zhukova", "authors": "Manuel C\\'aceres (1), Simon J. Puglisi (2), Bella Zhukova (2) ((1)\n  University of Chile, (2) University of Helsinki)", "title": "Fast Indexes for Gapped Pattern Matching", "comments": "This research is supported by Academy of Finland through grant 319454\n  and has received funding from the European Union's Horizon 2020 research and\n  innovation programme under the Marie Sklodowska-Curie Actions\n  H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "SOFSEM 2020: Theory and Practice of Computer Science", "doi": "10.1007/978-3-030-38919-2_40", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe indexes for searching large data sets for variable-length-gapped\n(VLG) patterns. VLG patterns are composed of two or more subpatterns, between\neach adjacent pair of which is a gap-constraint specifying upper and lower\nbounds on the distance allowed between subpatterns. VLG patterns have numerous\napplications in computational biology (motif search), information retrieval\n(e.g., for language models, snippet generation, machine translation) and\ncapture a useful subclass of the regular expressions commonly used in practice\nfor searching source code. Our best approach provides search speeds several\ntimes faster than prior art across a broad range of patterns and texts.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:33:30 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["C\u00e1ceres", "Manuel", ""], ["Puglisi", "Simon J.", ""], ["Zhukova", "Bella", ""]]}, {"id": "2002.12694", "submitter": "Raul Lopes", "authors": "Victor Campos, Raul Lopes, Andrea Marino, Ana Silva", "title": "Edge-Disjoint Branchings in Temporal Graphs", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal digraph ${\\cal G}$ is a triple $(G, \\gamma, \\lambda)$ where $G$ is\na digraph, $\\gamma$ is a function on $V(G)$ that tells us the timestamps when a\nvertex is active, and $\\lambda$ is a function on $E(G)$ that tells for each $uv\n\\in E(G)$ when $u$ and $v$ are linked. Given a static digraph $G$, and a subset\n$R\\subseteq V(G)$, a spanning branching with root $R$ is a subdigraph of $G$\nthat has exactly one path from $R$ to each $v\\in V(G)$. In this paper, we\nconsider the temporal version of Edmonds' classical result about the problem of\nfinding $k$ edge-disjoint spanning branchings respectively rooted at given\n$R_1,\\cdots,R_k$. We introduce and investigate different definitions of\nspanning branchings, and of edge-disjointness in the context of temporal\ngraphs. A branching ${\\cal B}$ is vertex-spanning if the root is able to reach\neach vertex $v$ of $G$ at some time where $v$ is active, while it is\ntemporal-spanning if $v$ can be reached from the root at every time where $v$\nis active. On the other hand, two branchings ${\\cal B}_1$ and ${\\cal B}_2$ are\nedge-disjoint if they do not use the same edge of $G$, and are\ntemporal-edge-disjoint if they can use the same edge of $G$ but at different\ntimes. This lead us to four definitions of disjoint spanning branchings and we\nprove that, unlike the static case, only one of these can be computed in\npolynomial time, namely the temporal-edge-disjoint temporal-spanning branchings\nproblem, while the other versions are $\\mathsf{NP}$-complete, even under very\nstrict assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 13:12:42 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Campos", "Victor", ""], ["Lopes", "Raul", ""], ["Marino", "Andrea", ""], ["Silva", "Ana", ""]]}, {"id": "2002.12706", "submitter": "Yasuaki Kobayashi", "authors": "Tesshu Hanaka and Yasuaki Kobayashi and Taiga Sone", "title": "A (probably) optimal algorithm for Bisection on bounded-treewidth graphs", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum/minimum bisection problems are, given an edge-weighted graph, to\nfind a bipartition of the vertex set into two sets whose sizes differ by at\nmost one, such that the total weight of edges between the two sets is\nmaximized/minimized. Although these two problems are known to be NP-hard, there\nis an efficient algorithm for bounded-treewidth graphs. In particular, Jansen\net al. (SIAM J. Comput. 2005) gave an $O(2^tn^3)$-time algorithm when given a\ntree decomposition of width $t$ of the input graph, where $n$ is the number of\nvertices of the input graph. Eiben et al. (ESA 2019) improved the dependency of\n$n$ in the running time by giving an $O(8^tt^5n^2\\log n)$-time algorithm.\nMoreover, they showed that there is no $O(n^{2-\\varepsilon})$-time algorithm\nfor trees under some reasonable complexity assumption.\n  In this paper, we show an $O(2^t(tn)^2)$-time algorithm for both problems,\nwhich is asymptotically tight to their conditional lower bound. We also show\nthat the exponential dependency of the treewidth is asymptotically optimal\nunder the Strong Exponential Time Hypothesis. Finally, we discuss the\n(in)tractability of both problems with respect to special graph classes.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 13:35:46 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 11:57:18 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Sone", "Taiga", ""]]}, {"id": "2002.12771", "submitter": "Sergey Dovgal", "authors": "Maciej Bendkowski, Olivier Bodini, Sergey Dovgal", "title": "Tuning as convex optimisation: a polynomial tuner for multi-parametric\n  combinatorial samplers", "comments": "44 pages, an extended version of the paper \"Polynomial tuning of\n  multiparametric combinatorial samplers\" presented at ANALCO'18. arXiv admin\n  note: text overlap with arXiv:1708.01212", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial samplers are algorithmic schemes devised for the approximate-\nand exact-size generation of large random combinatorial structures, such as\ncontext-free words, various tree-like data structures, maps, tilings, or even\nRNA sequences. In their multi-parametric variants, combinatorial samplers are\nadapted to combinatorial specifications with additional parameters, allowing\nfor a more flexible control over the output profile of parametrised\ncombinatorial patterns. One can control, for instance, the number of leaves,\nprofile of node degrees in trees or the number of certain sub-patterns in\ngenerated strings. However, such a flexible control requires an additional and\nnontrivial tuning procedure.\n  Using techniques of convex optimisation, we present an efficient polynomial\ntuning algorithm for multi-parametric combinatorial specifications. For a given\ncombinatorial system of description length $L$ with $d$ tuning parameters and\ntarget size parameter value $n$, our algorithm runs in time $O(d^{3.5} L \\log\nn)$. We demonstrate the effectiveness of our method on a series of practical\nexamples, including rational, algebraic, and so-called P\\'olya specifications.\nWe show how our method can be adapted to a broad range of less typical\ncombinatorial constructions, including symmetric polynomials, labelled sets and\ncycles with cardinality lower bounds, simple increasing trees or substitutions.\nFinally, we discuss some practical aspects of our prototype tuner\nimplementation and provide its benchmark results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:53:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Bendkowski", "Maciej", ""], ["Bodini", "Olivier", ""], ["Dovgal", "Sergey", ""]]}, {"id": "2002.12856", "submitter": "Anay Mehrotra", "authors": "Anay Mehrotra and Vibhor Porwal and Raghunath Tewari", "title": "Two Player Hidden Pointer Chasing and Multi-Pass Lower Bounds in\n  Turnstile Streams", "comments": "The authors have withdrawn this paper due to an error in the proof of\n  Lemma 3.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors have withdrawn this paper due to an error in the proof of Lemma\n3.4.\n  --------------------------------------------------------------------------------------------\n  The authors have withdrawn this paper due to an error in the proof of Lemma\n3.4z(Assadi, Chen, and Khanna, 2019) define a 4-player hidden-pointer-chasing\n($\\mathsf{HPC}^4$), and using it, give strong multi-pass lower bounds for graph\nproblems in the streaming model of computation and a lower bound on the query\ncomplexity of sub-modular minimization. We present a two-player version\n($\\mathsf{HPC}^2$) of $\\mathsf{HPC}^4$ that has matching communication\ncomplexity to $\\mathsf{HPC}^4$. Our formulation allows us to lower bound its\ncommunication complexity with a simple direct-sum argument. Using this lower\nbound on the communication complexity of $\\mathsf{HPC}^2$, we retain the\nstreaming and query complexity lower bounds by (Assadi, Chen, and Khanna,\n2019).\n  Further, by giving reductions from $\\mathsf{HPC}^2$, we prove new multi-pass\nspace lower bounds for graph problems in turnstile streams. In particular, we\nshow that any algorithm which computes the exact weight of the maximum weighted\nmatching in an $n$-vertex graph requires $\\tilde{O}(n^{2})$ space unless it\nmakes $\\omega(\\log n)$ passes over the turnstile stream, and that any algorithm\nwhich computes the minimum $s\\text{-}t$ distance in an $n$-vertex graph\nrequires $n^{2-o(1)}$ space unless it makes $n^{\\Omega(1)}$ passes over the\nturnstile stream. Our reductions can be modified to use $\\mathsf{HPC}^4$ as\nwell.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 16:33:45 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 11:05:56 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Mehrotra", "Anay", ""], ["Porwal", "Vibhor", ""], ["Tewari", "Raghunath", ""]]}, {"id": "2002.12872", "submitter": "Matteo Smerlak", "authors": "Maseim Kenmoe, Matteo Smerlak, Anton Zadorin", "title": "Dynamical perturbation theory for eigenvalue problems", "comments": "16 pages, 5 figures, 3 supplemental figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cs.DS math.MP quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in physics, chemistry and other fields are perturbative in\nnature, i.e. differ only slightly from related problems with known solutions.\nProminent among these is the eigenvalue perturbation problem, wherein one seeks\nthe eigenvectors and eigenvalues of a matrix with small off-diagonal elements.\nHere we introduce a novel iterative algorithm to compute these eigenpairs based\non fixed-point iteration for an algebraic equation in complex projective space.\nWe show from explicit and numerical examples that our algorithm outperforms the\nusual Rayleigh-Schr\\\"odinger expansion on three counts. First, since it is not\ndefined as a power series, its domain of convergence is not a priori confined\nto a disk in the complex plane; we find that it indeed usually extends beyond\nthe standard perturbative radius of convergence. Second, it converges at a\nfaster rate than the Rayleigh-Schr\\\"odinger expansion, i.e. fewer iterations\nare required to reach a given precision. Third, the (time- and space-)\nalgorithmic complexity of each iteration does not increase with the order of\nthe approximation, allowing for higher precision computations. Because this\ncomplexity is merely that of matrix multiplication, our dynamical scheme also\nscales better with the size of the matrix than general-purpose eigenvalue\nroutines such as the shifted QR or divide-and-conquer algorithms. Whether they\nare dense, sparse, symmetric or unsymmetric, we confirm that dynamical\ndiagonalization quickly outpaces LAPACK drivers as the size of matrices grows;\nfor the computation of just the dominant eigenvector, our method converges\norder of magnitudes faster than the Arnoldi algorithm implemented in ARPACK.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 17:13:22 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 08:27:49 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 15:39:46 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kenmoe", "Maseim", ""], ["Smerlak", "Matteo", ""], ["Zadorin", "Anton", ""]]}]