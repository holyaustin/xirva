[{"id": "1807.00030", "submitter": "Matthew Johnson", "authors": "Matthew P. Johnson", "title": "Deciding the Closure of Inconsistent Rooted Triples is NP-Complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting three-leaf binary trees or {\\em rooted triples} as constraints\nyields an entailment relation, whereby binary trees satisfying some rooted\ntriples must also thus satisfy others, and thence a closure operator, which is\nknown to be polynomial-time computable. This is extended to inconsistent triple\nsets by defining that a triple is entailed by such a set if it is entailed by\nany consistent subset of it.\n  Determining whether the closure of an inconsistent rooted triple set can be\ncomputed in polynomial time was posed as an open problem in the Isaac Newton\nInstitute's \"Phylogenetics\" program in 2007. It appears (as NC4) in a\ncollection of such open problems maintained by Mike Steel, and it is the last\nof that collection's five problems concerning computational complexity to have\nremained open. We resolve the complexity of computing this closure, proving\nthat its decision version is NP-Complete.\n  In the process, we also prove that detecting the existence of {\\em any}\nacyclic B-hyperpath (from specified source to destination) is NP-Complete, in a\nsignificantly narrower special case than the version whose {\\em minimization}\nproblem was recently proven NP-hard by Ritz et al. This implies it is NP-hard\nto approximate (our special case of) their minimization problem to within {\\em\nany} factor.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 18:33:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Johnson", "Matthew P.", ""]]}, {"id": "1807.00112", "submitter": "Tal Wagner", "authors": "Piotr Indyk and Tal Wagner", "title": "Approximate Nearest Neighbors in Limited Space", "comments": "COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $(1+\\epsilon)$-approximate nearest neighbor search problem:\ngiven a set $X$ of $n$ points in a $d$-dimensional space, build a data\nstructure that, given any query point $y$, finds a point $x \\in X$ whose\ndistance to $y$ is at most $(1+\\epsilon) \\min_{x \\in X} \\|x-y\\|$ for an\naccuracy parameter $\\epsilon \\in (0,1)$. Our main result is a data structure\nthat occupies only $O(\\epsilon^{-2} n \\log(n) \\log(1/\\epsilon))$ bits of space,\nassuming all point coordinates are integers in the range $\\{-n^{O(1)} \\ldots\nn^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This\nimproves over the best previously known space bound of $O(\\epsilon^{-2} n\n\\log(n)^2)$, obtained via the randomized dimensionality reduction method of\nJohnson and Lindenstrauss (1984). We also consider the more general problem of\nestimating all distances from a collection of query points to all data points\n$X$, and provide almost tight upper and lower bounds for the space complexity\nof this problem.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 02:33:15 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Indyk", "Piotr", ""], ["Wagner", "Tal", ""]]}, {"id": "1807.00121", "submitter": "Koji M. Kobayashi", "authors": "Koji M. Kobayashi", "title": "An optimal algorithm for 2-bounded delay buffer management with\n  lookahead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bounded delay buffer management problem, which was proposed by Kesselman\net~al.\\ (STOC 2001 and SIAM Journal on Computing 33(3), 2004), is an online\nproblem focusing on buffer management of a switch supporting Quality of Service\n(QoS). The problem definition is as follows: Packets arrive to a buffer over\ntime and each packet is specified by the {\\em release time}, {\\em deadline} and\n{\\em value}. An algorithm can transmit at most one packet from the buffer at\neach integer time and can gain its value as the {\\em profit} if transmitting a\npacket by its deadline after its release time. The objective of this problem is\nto maximize the gained profit. We say that an instance of the problem is\n$s$-bounded if for any packet, an algorithm has at most $s$ chances to transmit\nit. For any $s \\geq 2$, Hajek (CISS 2001) showed that the competitive ratio of\nany deterministic algorithm is at least $(1 + \\sqrt{5})/2 \\approx 1.619$. It is\nconjectured that there exists an algorithm whose competitive ratio matching\nthis lower bound for any $s$. However, it has not been shown yet. Then, when $s\n= 2$, B{\\\"o}hm et al.~(ISAAC 2016) introduced the {\\em lookahead} ability to an\nonline algorithm, that is the algorithm can gain information about future\narriving packets, and showed that the algorithm achieves the competitive ratio\nof $(-1 + \\sqrt{13})/2 \\approx 1.303$. Also, they showed that the competitive\nratio of any deterministic algorithm is at least $(1 + \\sqrt{17})/4 \\approx\n1.281$. In this paper, for the 2-bounded model with lookahead, we design an\nalgorithm with a matching competitive ratio of $(1 + \\sqrt{17})/4$.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 03:40:50 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kobayashi", "Koji M.", ""]]}, {"id": "1807.00205", "submitter": "Can Alkan", "authors": "Ibrahim Numanagi\\'c, Alim S. G\\\"okkaya, Lillian Zhang, Bonnie Berger,\n  Can Alkan, Faraz Hach", "title": "Fast Characterization of Segmental Duplications in Genome Assemblies", "comments": null, "journal-ref": "Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages\n  i706-i714", "doi": "10.1093/bioinformatics/bty586", "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA\ngreater than 1 Kbp with high sequence identity that are copied to other regions\nof the genome. SDs are among the most important sources of evolution, a common\ncause of genomic structural variation, and several are associated with diseases\nof genomic origin. Despite their functional importance, SDs present one of the\nmajor hurdles for de novo genome assembly due to the ambiguity they cause in\nbuilding and traversing both state-of-the-art overlap-layout-consensus and de\nBruijn graphs. This causes SD regions to be misassembled, collapsed into a\nunique representation, or completely missing from assembled reference genomes\nfor various organisms. In turn, this missing or incorrect information limits\nour ability to fully understand the evolution and the architecture of the\ngenomes. Despite the essential need to accurately characterize SDs in\nassemblies, there is only one tool that has been developed for this purpose,\ncalled Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several\nsteps that employ different tools and custom scripts, which makes it difficult\nand time consuming to use. Thus there is still a need for algorithms to\ncharacterize within-assembly SDs quickly, accurately, and in a user friendly\nmanner.\n  Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to\nrapidly detect SDs through sophisticated filtering strategies based on Jaccard\nsimilarity and local chaining. We show that SEDEF accurately detects SDs while\nmaintaining substantial speed up over WGAC that translates into practical run\ntimes of minutes instead of weeks. Notably, our algorithm captures up to 25%\npairwise error between segments, where previous studies focused on only 10%,\nallowing us to more deeply track the evolutionary history of the genome.\n  SEDEF is available at https://github.com/vpc-ccg/sedef\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 17:03:01 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 14:51:42 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Numanagi\u0107", "Ibrahim", ""], ["G\u00f6kkaya", "Alim S.", ""], ["Zhang", "Lillian", ""], ["Berger", "Bonnie", ""], ["Alkan", "Can", ""], ["Hach", "Faraz", ""]]}, {"id": "1807.00371", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Representation of ordered trees with a given degree distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree distribution of an ordered tree $T$ with $n$ nodes is $\\vec{n} =\n(n_0,\\ldots,n_{n-1})$, where $n_i$ is the number of nodes in $T$ with $i$\nchildren. Let $\\mathcal{N}(\\vec{n})$ be the number of trees with degree\ndistribution $\\vec{n}$. We give a data structure that stores an ordered tree\n$T$ with $n$ nodes and degree distribution $\\vec{n}$ using $\\log\n\\mathcal{N}(\\vec{n})+O(n/\\log^t n)$ bits for every constant $t$. The data\nstructure answers tree queries in constant time. This improves the current data\nstructures with lowest space for ordered trees: The structure of Jansson et\nal.\\ [JCSS 2012] that uses $\\log\\mathcal{N}(\\vec{n})+O(n\\log\\log n/\\log n)$\nbits, and the structure of Navarro and Sadakane [TALG 2014] that uses\n$2n+O(n/\\log^t n)$ bits for every constant $t$.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 19:02:15 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1807.00594", "submitter": "Immanuel Albrecht", "authors": "Immanuel Albrecht", "title": "Well-Scaling Procedure for Deciding Gammoid Class-Membership of Matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": "feU-dmo050.18", "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a procedure that solves the decision problem whether a given\nmatroid M is a gammoid. The procedure consists of three pieces: First, we\nintroduce a notion of a valid matroid tableau which captures the current state\nof knowledge regarding the properties of matroids related to the matroid under\nconsideration. Second, we give a sufficient set of rules that may be used to\ngenerate valid matroid tableaux. Third, we introduce a succession of steps that\nultimately lead to a decisive tableau starting with any valid tableau. We argue\nthat the decision problem scales well with respect to parallel computation\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 10:54:53 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Albrecht", "Immanuel", ""]]}, {"id": "1807.00736", "submitter": "Olga Ohrimenko", "authors": "Joshua Allen, Bolin Ding, Janardhan Kulkarni, Harsha Nori, Olga\n  Ohrimenko, Sergey Yekhanin", "title": "An Algorithmic Framework For Differentially Private Data Analysis on\n  Trusted Processors", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy has emerged as the main definition for private data\nanalysis and machine learning. The {\\em global} model of differential privacy,\nwhich assumes that users trust the data collector, provides strong privacy\nguarantees and introduces small errors in the output. In contrast, applications\nof differential privacy in commercial systems by Apple, Google, and Microsoft,\nuse the {\\em local model}. Here, users do not trust the data collector, and\nhence randomize their data before sending it to the data collector.\nUnfortunately, local model is too strong for several important applications and\nhence is limited in its applicability. In this work, we propose a framework\nbased on trusted processors and a new definition of differential privacy called\n{\\em Oblivious Differential Privacy}, which combines the best of both local and\nglobal models. The algorithms we design in this framework show interesting\ninterplay of ideas from the streaming algorithms, oblivious algorithms, and\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 15:14:19 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 15:37:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Allen", "Joshua", ""], ["Ding", "Bolin", ""], ["Kulkarni", "Janardhan", ""], ["Nori", "Harsha", ""], ["Ohrimenko", "Olga", ""], ["Yekhanin", "Sergey", ""]]}, {"id": "1807.00804", "submitter": "Johannes Bausch", "authors": "Johannes Bausch", "title": "Classifying Data with Local Hamiltonians", "comments": "21 pages, 8 figures, 4 tables", "journal-ref": "Int. J. Quantum Inf. 1840001 (2018)", "doi": "10.1142/S0219749918400014", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to define a notion of a quantum neural network to\nclassify data, which exploits the low energy spectrum of a local Hamiltonian.\nAs a concrete application, we build a binary classifier, train it on some\nactual data and then test its performance on a simple classification task. More\nspecifically, we use Microsoft's quantum simulator, Liquid, to construct local\nHamiltonians that can encode trained classifier functions in their ground\nspace, and which can be probed by measuring the overlap with test states\ncorresponding to the data to be classified. To obtain such a classifier\nHamiltonian, we further propose a training scheme based on quantum annealing\nwhich is completely closed-off to the environment and which does not depend on\nexternal measurements until the very end, avoiding unnecessary decoherence\nduring the annealing procedure. For a network of size n, the trained network\ncan be stored as a list of O(n) coupling strengths. We address the question of\nwhich interactions are most suitable for a given classification task, and\ndevelop a qubit-saving optimization for the training procedure on a simulated\nannealing device. Furthermore, a small neural network to classify colors into\nred vs. blue is trained and tested, and benchmarked against the annealing\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 17:58:32 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Bausch", "Johannes", ""]]}, {"id": "1807.00878", "submitter": "Qin Zhang", "authors": "David P. Woodruff and Qin Zhang", "title": "Distributed Statistical Estimation of Matrix Products with Applications", "comments": "Appeared in PODS 2018; fixed some typos of the conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical estimations of a matrix product over the integers in\na distributed setting, where we have two parties Alice and Bob; Alice holds a\nmatrix $A$ and Bob holds a matrix $B$, and they want to estimate statistics of\n$A \\cdot B$. We focus on the well-studied $\\ell_p$-norm, distinct elements ($p\n= 0$), $\\ell_0$-sampling, and heavy hitter problems. The goal is to minimize\nboth the communication cost and the number of rounds of communication.\n  This problem is closely related to the fundamental set-intersection join\nproblem in databases: when $p = 0$ the problem corresponds to the size of the\nset-intersection join. When $p = \\infty$ the output is simply the pair of sets\nwith the maximum intersection size. When $p = 1$ the problem corresponds to the\nsize of the corresponding natural join. We also consider the heavy hitters\nproblem which corresponds to finding the pairs of sets with intersection size\nabove a certain threshold, and the problem of sampling an intersecting pair of\nsets uniformly at random.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 20:28:33 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1807.00891", "submitter": "Alexander Wein", "authors": "Amelia Perry and Alexander S. Wein and Afonso S. Bandeira and Ankur\n  Moitra", "title": "Optimality and Sub-optimality of PCA I: Spiked Random Matrix Models", "comments": "67 pages, 3 figures. This is the journal version of part I of\n  arXiv:1609.05573, accepted to the Annals of Statistics. This version includes\n  the supplementary material as appendices", "journal-ref": "Ann. Statist., Volume 46, Number 5 (2018), 2416-2451", "doi": "10.1214/17-AOS1625", "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem of random matrix theory is to understand the eigenvalues of\nspiked random matrix models, introduced by Johnstone, in which a prominent\neigenvector (or \"spike\") is planted into a random matrix. These distributions\nform natural statistical models for principal component analysis (PCA) problems\nthroughout the sciences. Baik, Ben Arous and Peche showed that the spiked\nWishart ensemble exhibits a sharp phase transition asymptotically: when the\nspike strength is above a critical threshold, it is possible to detect the\npresence of a spike based on the top eigenvalue, and below the threshold the\ntop eigenvalue provides no information. Such results form the basis of our\nunderstanding of when PCA can detect a low-rank signal in the presence of\nnoise. However, under structural assumptions on the spike, not all information\nis necessarily contained in the spectrum. We study the statistical limits of\ntests for the presence of a spike, including non-spectral tests. Our results\nleverage Le Cam's notion of contiguity, and include:\n  i) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal\ndetection threshold for certain natural priors for the spike.\n  ii) For any non-Gaussian Wigner ensemble, PCA is sub-optimal for detection.\nHowever, an efficient variant of PCA achieves the optimal threshold (for\nnatural priors) by pre-transforming the matrix entries.\n  iii) For the Gaussian Wishart ensemble, the PCA threshold is optimal for\npositive spikes (for natural priors) but this is not always the case for\nnegative spikes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 21:11:57 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 03:30:03 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""], ["Moitra", "Ankur", ""]]}, {"id": "1807.00929", "submitter": "Nima Anari", "authors": "Nima Anari and Shayan Oveis Gharan and Cynthia Vinzant", "title": "Log-Concave Polynomials I: Entropy and a Deterministic Approximation\n  Algorithm for Counting Bases of Matroids", "comments": "Appeared in FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.CO math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic polynomial time $2^{O(r)}$-approximation algorithm\nfor the number of bases of a given matroid of rank $r$ and the number of common\nbases of any two matroids of rank $r$. To the best of our knowledge, this is\nthe first nontrivial deterministic approximation algorithm that works for\narbitrary matroids. Based on a lower bound of Azar, Broder, and Frieze [ABF94]\nthis is almost the best possible result assuming oracle access to independent\nsets of the matroid.\n  There are two main ingredients in our result: For the first, we build upon\nrecent results of Adiprasito, Huh, and Katz [AHK15] and Huh and Wang [HW17] on\ncombinatorial hodge theory to derive a connection between matroids and\nlog-concave polynomials. We expect that several new applications in\napproximation algorithms will be derived from this connection in future.\nFormally, we prove that the multivariate generating polynomial of the bases of\nany matroid is log-concave as a function over the positive orthant. For the\nsecond ingredient, we develop a general framework for approximate counting in\ndiscrete problems, based on convex optimization. The connection goes through\nsubadditivity of the entropy. For matroids, we prove that an approximate\nsuperadditivity of the entropy holds by relying on the log-concavity of the\ncorresponding polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 23:50:07 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 02:53:08 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""], ["Vinzant", "Cynthia", ""]]}, {"id": "1807.00936", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi", "title": "A Note on Degree vs Gap of Min-Rep Label Cover and Improved\n  Inapproximability for Connectivity Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note concerns the trade-off between the degree of the constraint graph\nand the gap in hardness of approximating the Min-Rep variant of Label Cover\n(aka Projection Game). We make a very simple observation that, for NP-hardness\nwith gap $g$, the degree can be made as small as $O(g \\log g)$, which improves\nupon the previous $\\tilde{O}(g^{1/2})$ bound from a work of Laekhanukit\n(SODA'14). Note that our bound is optimal up to a logarithmic factor since\nthere is a trivial $\\Delta$-approximation for Min-Rep where $\\Delta$ is the\nmaximum degree of the constraint graph.\n  Thanks to known reductions, this improvement implies better hardness of\napproximation results for Rooted $k$-Connectivity, Vertex-Connectivity\nSurvivable Network Design and Vertex-Connectivity $k$-Route Cut.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 00:38:03 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Manurangsi", "Pasin", ""]]}, {"id": "1807.00964", "submitter": "Pu Gao", "authors": "Pu Gao and Catherine Greenhill", "title": "Uniform generation of spanning regular subgraphs of a dense graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $H_n$ be a graph on $n$ vertices and let $\\ber{H_n}$ denote the\ncomplement of $H_n$. Suppose that $\\Delta = \\Delta(n)$ is the maximum degree of\n$\\ber{H_n}$. We analyse three algorithms for sampling $d$-regular subgraphs\n($d$-factors) of $H_n$. This is equivalent to uniformly sampling $d$-regular\ngraphs which avoid a set $E(\\ber{H_n})$ of forbidden edges. Here $d=d(n)$ is a\npositive integer which may depend on $n$.\n  Two of these algorithms produce a uniformly random $d$-factor of $H_n$ in\nexpected runtime which is linear in $n$ and low-degree polynomial in $d$ and\n$\\Delta$. The first algorithm applies when $(d+\\Delta)d\\Delta = o(n)$. This\nimproves on an earlier algorithm by the first author, which required constant\n$d$ and at most a linear number of edges in $\\ber{H_n}$. The second algorithm\napplies when $H_n$ is regular and $d^2+\\Delta^2 = o(n)$, adapting an approach\ndeveloped by the first author together with Wormald. The third algorithm is a\nsimplification of the second, and produces an approximately uniform $d$-factor\nof $H_n$ in time $O(dn)$. Here the output distribution differs from uniform by\n$o(1)$ in total variation distance, provided that $d^2+\\Delta^2 = o(n)$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 03:41:47 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 16:44:16 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gao", "Pu", ""], ["Greenhill", "Catherine", ""]]}, {"id": "1807.01070", "submitter": "Christian Konrad", "authors": "Artur Czumaj, Christian Konrad", "title": "Detecting cliques in CONGEST networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting network structures plays a central role in\ndistributed computing. One of the fundamental problems studied in this area is\nto determine whether for a given graph $H$, the input network contains a\nsubgraph isomorphic to $H$ or not. We investigate this problem for $H$ being a\nclique $K_{l}$ in the classical distributed CONGEST model, where the\ncommunication topology is the same as the topology of the underlying network,\nand with limited communication bandwidth on the links.\n  Our first and main result is a lower bound, showing that detecting $K_{l}$\nrequires $\\Omega(\\sqrt{n} / b)$ communication rounds, for every $4 \\le l \\le\n\\sqrt{n}$, and $\\Omega(n / (l b))$ rounds for every $l \\ge \\sqrt{n}$, where $b$\nis the bandwidth of the communication links. This result is obtained by using a\nreduction to the set disjointness problem in the framework of two-party\ncommunication complexity.\n  We complement our lower bound with a two-party communication protocol for\nlisting all cliques in the input graph, which up to constant factors\ncommunicates the same number of bits as our lower bound for $K_4$ detection.\nThis demonstrates that our lower bound cannot be improved using the two-party\ncommunication framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 10:28:36 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Czumaj", "Artur", ""], ["Konrad", "Christian", ""]]}, {"id": "1807.01160", "submitter": "Milana Grbic", "authors": "Milana Grbi\\'c, Aleksandar Kartelj, Savka Jankovi\\'c, Dragan Mati\\'c\n  and Vladimir Filipovi\\'c", "title": "Variable neighborhood search for partitioning sparse biological networks\n  into the maximum edge-weighted $k$-plexes", "comments": "Submitted to a scientific journal in November 2017", "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics\n  2019", "doi": "10.1109/TCBB.2019.2898189", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a network, a $k$-plex represents a subset of $n$ vertices where the degree\nof each vertex in the subnetwork induced by this subset is at least $n-k$. The\nmaximum edge-weight $k$-plex partitioning problem (Max-EkPP) is to find the\n$k$-plex partitioning in edge-weighted network, such that the sum of edge\nweights is maximal. The Max-EkPP has an important role in discovering new\ninformation in large sparse biological networks. We propose a variable\nneighborhood search (VNS) algorithm for solving Max-EkPP. The VNS implements a\nlocal search based on the 1-swap first improvement strategy and the objective\nfunction that takes into account the degree of every vertex in each partition.\nThe objective function favors feasible solutions, also enabling a gradual\nincrease in terms of objective function value when moving from slightly\ninfeasible to barely feasible solutions. A comprehensive experimental\ncomputation is performed on real metabolic networks and other benchmark\ninstances from literature. Comparing to the integer linear programming method\nfrom literature, our approach succeeds to find all known optimal solutions. For\nall other instances, the VNS either reaches previous best known solution or\nimproves it. The proposed VNS is also tested on a large-scaled dataset which\nwas not previously considered in literature.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 13:23:27 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Grbi\u0107", "Milana", ""], ["Kartelj", "Aleksandar", ""], ["Jankovi\u0107", "Savka", ""], ["Mati\u0107", "Dragan", ""], ["Filipovi\u0107", "Vladimir", ""]]}, {"id": "1807.01191", "submitter": "Kai Han", "authors": "Kai Han", "title": "Approximation Algorithms for Probabilistic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the k-median and k-center problems in probabilistic graphs. We\nanalyze the hardness of these problems, and propose several algorithms with\nimproved approximation ratios compared with the existing proposals.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 13:53:53 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 03:40:14 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Han", "Kai", ""]]}, {"id": "1807.01247", "submitter": "Fabrizio Montecchiani", "authors": "Giuseppe Liotta, Fabrizio Montecchiani, Alessandra Tappini", "title": "Ortho-polygon Visibility Representations of 3-connected 1-plane Graphs", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ortho-polygon visibility representation $\\Gamma$ of a $1$-plane graph $G$\n(OPVR of $G$) is an embedding preserving drawing that maps each vertex of $G$\nto a distinct orthogonal polygon and each edge of $G$ to a vertical or\nhorizontal visibility between its end-vertices. The representation $\\Gamma$ has\nvertex complexity $k$ if every polygon of $\\Gamma$ has at most $k$ reflex\ncorners. It is known that $3$-connected $1$-plane graphs admit an OPVR with\nvertex complexity at most twelve, while vertex complexity at least two may be\nrequired in some cases. In this paper, we reduce this gap by showing that\nvertex complexity five is always sufficient, while vertex complexity four may\nbe required in some cases. These results are based on the study of the\ncombinatorial properties of the B-, T-, and W-configurations in $3$-connected\n$1$-plane graphs. An implication of the upper bound is the existence of a\n$\\tilde{O}(n^\\frac{10}{7})$-time drawing algorithm that computes an OPVR of an\n$n$-vertex $3$-connected $1$-plane graph on an integer grid of size $O(n)\n\\times O(n)$ and with vertex complexity at most five.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:46:30 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 15:11:47 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1807.01260", "submitter": "Davide Bil\\`o", "authors": "Davide Bil\\`o and Kleitos Papadopoulos", "title": "A Novel Algorithm for the All-Best-Swap-Edge Problem on Tree Spanners", "comments": "The paper has been accepted for publication at the 29th International\n  Symposium on Algorithms and Computation (ISAAC 2018). 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a 2-edge connected, unweighted, and undirected graph $G$ with $n$\nvertices and $m$ edges, a $\\sigma$-tree spanner is a spanning tree $T$ of $G$\nin which the ratio between the distance in $T$ of any pair of vertices and the\ncorresponding distance in $G$ is upper bounded by $\\sigma$. The minimum value\nof $\\sigma$ for which $T$ is a $\\sigma$-tree spanner of $G$ is also called the\n{\\em stretch factor} of $T$. We address the fault-tolerant scenario in which\neach edge $e$ of a given tree spanner may temporarily fail and has to be\nreplaced by a {\\em best swap edge}, i.e. an edge that reconnects $T-e$ at a\nminimum stretch factor. More precisely, we design an $O(n^2)$ time and space\nalgorithm that computes a best swap edge of every tree edge. Previously, an\n$O(n^2 \\log^4 n)$ time and $O(n^2+m\\log^2n)$ space algorithm was known for\nedge-weighted graphs [Bil\\`o et al., ISAAC 2017]. Even if our improvements on\nboth the time and space complexities are of a polylogarithmic factor, we stress\nthe fact that the design of a $o(n^2)$ time and space algorithm would be\nconsidered a breakthrough.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 16:10:46 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 07:18:09 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Papadopoulos", "Kleitos", ""]]}, {"id": "1807.01341", "submitter": "Josh Borrow", "authors": "Josh Borrow, Richard G. Bower, Peter W. Draper, Pedro Gonnet, Matthieu\n  Schaller", "title": "SWIFT: Maintaining weak-scalability with a dynamic range of $10^4$ in\n  time-step size to harness extreme adaptivity", "comments": null, "journal-ref": "Proceedings of the 13th SPHERIC International Workshop, Galway,\n  Ireland, June 26-28 2018, pp. 44-51", "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmological simulations require the use of a multiple time-stepping scheme.\nWithout such a scheme, cosmological simulations would be impossible due to\ntheir high level of dynamic range; over eleven orders of magnitude in density.\nSuch a large dynamic range leads to a range of over four orders of magnitude in\ntime-step, which presents a significant load-balancing challenge. In this work,\nthe extreme adaptivity that cosmological simulations present is tackled in\nthree main ways through the use of the code SWIFT. First, an adaptive mesh is\nused to ensure that only the relevant particles are interacted in a given\ntime-step. Second, task-based parallelism is used to ensure efficient\nload-balancing within a single node, using pthreads and SIMD vectorisation.\nFinally, a domain decomposition strategy is presented, using the graph domain\ndecomposition library METIS, that bisects the work that must be performed by\nthe simulation between nodes using MPI. These three strategies are shown to\ngive SWIFT near-perfect weak-scaling characteristics, only losing 25%\nperformance when scaling from 1 to 4096 cores on a representative problem,\nwhilst being more than 30x faster than the de-facto standard Gadget-2 code.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 18:37:58 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Borrow", "Josh", ""], ["Bower", "Richard G.", ""], ["Draper", "Peter W.", ""], ["Gonnet", "Pedro", ""], ["Schaller", "Matthieu", ""]]}, {"id": "1807.01478", "submitter": "Oren Weimann", "authors": "Hsien-Chih Chang, Pawe{\\l} Gawrychowski, Shay Mozes, Oren Weimann", "title": "Near-Optimal Distance Emulator for Planar Graphs", "comments": "Preliminary version in ESA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and a set of terminals $T$, a \\emph{distance emulator} of\n$G$ is another graph $H$ (not necessarily a subgraph of $G$) containing $T$,\nsuch that all the pairwise distances in $G$ between vertices of $T$ are\npreserved in $H$. An important open question is to find the smallest possible\ndistance emulator.\n  We prove that, given any subset of $k$ terminals in an $n$-vertex undirected\nunweighted planar graph, we can construct in $\\tilde O(n)$ time a distance\nemulator of size $\\tilde O(\\min(k^2,\\sqrt{k\\cdot n}))$. This is optimal up to\nlogarithmic factors. The existence of such distance emulator provides a\nstraightforward framework to solve distance-related problems on planar graphs:\nReplace the input graph with the distance emulator, and apply whatever\nalgorithm available to the resulting emulator. In particular, our result\nimplies that, on any unweighted undirected planar graph, one can compute\nall-pairs shortest path distances among $k$ terminals in $\\tilde O(n)$ time\nwhen $k=O(n^{1/3})$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 08:26:37 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Chang", "Hsien-Chih", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1807.01680", "submitter": "Kun He", "authors": "Heng Guo, Kun He", "title": "Tight bounds for popping algorithms", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We sharpen run-time analysis for algorithms under the partial rejection\nsampling framework. Our method yields improved bounds for: the cluster-popping\nalgorithm for approximating all-terminal network reliability; the cycle-popping\nalgorithm for sampling rooted spanning trees; the sink-popping algorithm for\nsampling sink-free orientations. In all three applications, our bounds are not\nonly tight in order, but also optimal in constants.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 16:57:06 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 16:47:13 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Guo", "Heng", ""], ["He", "Kun", ""]]}, {"id": "1807.01703", "submitter": "Xin Zhang", "authors": "Xin Zhang, Hong Xiang, Tao Xiang, Li Fu, Jun Sang", "title": "An efficient quantum circuits optimizing scheme compared with QISKit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the development of quantum chips has made great progress-- the\nnumber of qubits is increasing and the fidelity is getting higher. However,\nqubits of these chips are not always fully connected, which sets additional\nbarriers for implementing quantum algorithms and programming quantum programs.\nIn this paper, we introduce a general circuit optimizing scheme, which can\nefficiently adjust and optimize quantum circuits according to arbitrary given\nqubits' layout by adding additional quantum gates, exchanging qubits and\nmerging single-qubit gates. Compared with the optimizing algorithm of IBM's\nQISKit, the quantum gates consumed by our scheme is 74.7%, and the execution\ntime is only 12.9% on average.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 08:31:08 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Zhang", "Xin", ""], ["Xiang", "Hong", ""], ["Xiang", "Tao", ""], ["Fu", "Li", ""], ["Sang", "Jun", ""]]}, {"id": "1807.01804", "submitter": "Alexander Conway", "authors": "Michael A. Bender, Jake Christensen, Alex Conway, Mart\\'in\n  Farach-Colton, Rob Johnson, Meng-Tsung Tsai", "title": "Optimal Ball Recycling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balls-and-bins games have been a wildly successful tool for modeling load\nbalancing problems. In this paper, we study a new scenario, which we call the\nball recycling game, defined as follows:\n  Throw m balls into n bins i.i.d. according to a given probability\ndistribution p. Then, at each time step, pick a non-empty bin and recycle its\nballs: take the balls from the selected bin and re-throw them according to p.\n  This balls-and-bins game closely models memory-access heuristics in\ndatabases. The goal is to have a bin-picking method that maximizes the\nrecycling rate, defined to be the expected number of balls recycled per step in\nthe stationary distribution. We study two natural strategies for ball\nrecycling: Fullest Bin, which greedily picks the bin with the maximum number of\nballs, and Random Ball, which picks a ball at random and recycles its bin. We\nshow that for general p, Random Ball is constant-optimal, whereas Fullest Bin\ncan be pessimal. However, when p = u, the uniform distribution, Fullest Bin is\noptimal to within an additive constant.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 22:49:10 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 17:37:28 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Bender", "Michael A.", ""], ["Christensen", "Jake", ""], ["Conway", "Alex", ""], ["Farach-Colton", "Mart\u00edn", ""], ["Johnson", "Rob", ""], ["Tsai", "Meng-Tsung", ""]]}, {"id": "1807.01962", "submitter": "Thomas Erlebach", "authors": "Annette M. C. Ficker, Thomas Erlebach, Matus Mihalak, Frits C. R.\n  Spieksma", "title": "Partitioning Vectors into Quadruples: Worst-Case Analysis of a\n  Matching-Based Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a problem where 4k given vectors need to be partitioned into k\nclusters of four vectors each. A cluster of four vectors is called a quad, and\nthe cost of a quad is the sum of the component-wise maxima of the four vectors\nin the quad. The problem is to partition the given 4k vectors into k quads with\nminimum total cost. We analyze a straightforward matching-based algorithm, and\nprove that this algorithm is a (3/2)-approximation algorithm for this problem.\nWe further analyze the performance of this algorithm on a hierarchy of special\ncases of the problem, and prove that, in one particular case, the algorithm is\na (5/4)-approximation algorithm. Our analysis is tight in all cases except one.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 12:34:10 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Ficker", "Annette M. C.", ""], ["Erlebach", "Thomas", ""], ["Mihalak", "Matus", ""], ["Spieksma", "Frits C. R.", ""]]}, {"id": "1807.02054", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok and Anthony Della Pella", "title": "Searching for dense subsets in a graph via the partition function", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a set $S$ of vertices of a graph $G$, we define its density $0 \\leq\n\\sigma(S) \\leq 1$ as the ratio of the number of edges of $G$ spanned by the\nvertices of $S$ to ${|S| \\choose 2}$. We show that, given a graph $G$ with $n$\nvertices and an integer $m$, the partition function $\\sum_S \\exp\\{ \\gamma m\n\\sigma(S) \\}$, where the sum is taken over all $m$-subsets $S$ of vertices and\n$0 < \\gamma <1$ is fixed in advance, can be approximated within relative error\n$0 < \\epsilon < 1$ in quasi-polynomial $n^{O(\\ln m - \\ln \\epsilon)}$ time. We\ndiscuss numerical experiments and observe that for the random graph $G(n, 1/2)$\none can afford a much larger $\\gamma$, provided the ratio $n/m$ is sufficiently\nlarge.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 15:29:18 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Barvinok", "Alexander", ""], ["Della Pella", "Anthony", ""]]}, {"id": "1807.02075", "submitter": "Zhengjun Cao", "authors": "Zhengjun Cao and Zhen Chen and Lihua Liu", "title": "Analysis of Nederlof's algorithm for subset sum", "comments": "page 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Nederlof's algorithm [Information Processing Letters, 118\n(2017), 15-16] for constructing a proof that the number of subsets summing to a\nparticular integer equals a claimed quantity is flawed because: 1) its\nconsistence is not kept; 2) the proposed recurrence formula is incorrect.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 03:41:09 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 02:50:00 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Cao", "Zhengjun", ""], ["Chen", "Zhen", ""], ["Liu", "Lihua", ""]]}, {"id": "1807.02227", "submitter": "David Goldberg", "authors": "David A. Goldberg and Yilun Chen", "title": "Beating the curse of dimensionality in options pricing and optimal\n  stopping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.OC q-fin.CP q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental problems of pricing high-dimensional path-dependent options\nand optimal stopping are central to applied probability and financial\nengineering. Modern approaches, often relying on ADP, simulation, and/or\nduality, have limited rigorous guarantees, which may scale poorly and/or\nrequire previous knowledge of basis functions. A key difficulty with many\napproaches is that to yield stronger guarantees, they would necessitate the\ncomputation of deeply nested conditional expectations, with the depth scaling\nwith the time horizon T.\n  We overcome this fundamental obstacle by providing an algorithm which can\ntrade-off between the guaranteed quality of approximation and the level of\nnesting required in a principled manner, without requiring a set of good basis\nfunctions. We develop a novel pure-dual approach, inspired by a connection to\nnetwork flows. This leads to a representation for the optimal value as an\ninfinite sum for which: 1. each term is the expectation of an elegant\nrecursively defined infimum; 2. the first k terms only require k levels of\nnesting; and 3. truncating at the first k terms yields an error of 1/k. This\nenables us to devise a simple randomized algorithm whose runtime is effectively\nindependent of the dimension, beyond the need to simulate sample paths of the\nunderlying process. Indeed, our algorithm is completely data-driven in that it\nonly needs the ability to simulate the original process, and requires no prior\nknowledge of the underlying distribution. Our method allows one to elegantly\ntrade-off between accuracy and runtime through a parameter epsilon controlling\nthe associated performance guarantee, with computational and sample complexity\nboth polynomial in T (and effectively independent of the dimension) for any\nfixed epsilon, in contrast to past methods typically requiring a complexity\nscaling exponentially in these parameters.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 02:53:45 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 18:47:02 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Goldberg", "David A.", ""], ["Chen", "Yilun", ""]]}, {"id": "1807.02290", "submitter": "Rachel Cummings", "authors": "Adrian Rivera Cardoso and Rachel Cummings", "title": "Differentially Private Online Submodular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop the first algorithms for online submodular\nminimization that preserve differential privacy under full information feedback\nand bandit feedback. A sequence of $T$ submodular functions over a collection\nof $n$ elements arrive online, and at each timestep the algorithm must choose a\nsubset of $[n]$ before seeing the function. The algorithm incurs a cost equal\nto the function evaluated on the chosen set, and seeks to choose a sequence of\nsets that achieves low expected regret.\n  Our first result is in the full information setting, where the algorithm can\nobserve the entire function after making its decision at each timestep. We give\nan algorithm in this setting that is $\\epsilon$-differentially private and\nachieves expected regret\n$\\tilde{O}\\left(\\frac{n^{3/2}\\sqrt{T}}{\\epsilon}\\right)$. This algorithm works\nby relaxing submodular function to a convex function using the Lovasz\nextension, and then simulating an algorithm for differentially private online\nconvex optimization.\n  Our second result is in the bandit setting, where the algorithm can only see\nthe cost incurred by its chosen set, and does not have access to the entire\nfunction. This setting is significantly more challenging because the algorithm\ndoes not receive enough information to compute the Lovasz extension or its\nsubgradients. Instead, we construct an unbiased estimate using a single-point\nestimation, and then simulate private online convex optimization using this\nestimate. Our algorithm using bandit feedback is $\\epsilon$-differentially\nprivate and achieves expected regret\n$\\tilde{O}\\left(\\frac{n^{3/2}T^{3/4}}{\\epsilon}\\right)$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 07:30:17 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Cardoso", "Adrian Rivera", ""], ["Cummings", "Rachel", ""]]}, {"id": "1807.02553", "submitter": "Shi Li", "authors": "Janardhan Kulkarni and Shi Li", "title": "Flow-time Optimization For Concurrent Open-Shop and Precedence\n  Constrained Scheduling Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling a set of jobs over a collection of machines is a fundamental\nproblem that needs to be solved millions of times a day in various computing\nplatforms: in operating systems, in large data clusters, and in data centers.\nAlong with makespan, flow-time, which measures the length of time a job spends\nin a system before it completes, is arguably the most important metric to\nmeasure the performance of a scheduling algorithm. In recent years, there has\nbeen a remarkable progress in understanding flow-time based objective functions\nin diverse settings such as unrelated machines scheduling, broadcast\nscheduling, multi-dimensional scheduling, to name a few.\n  Yet, our understanding of the flow-time objective is limited mostly to the\nscenarios where jobs have simple structures; in particular, each job is a\nsingle self contained entity. On the other hand, in almost all real world\napplications, think of MapReduce settings for example, jobs have more complex\nstructures. In this paper, we consider two classical scheduling models that\ncapture complex job structures: 1) concurrent open-shop scheduling and 2)\nprecedence constrained scheduling. Our main motivation to study these problems\nspecifically comes from their relevance to two scheduling problems that have\ngained importance in the context of data centers: co-flow scheduling and DAG\nscheduling. We design almost optimal approximation algorithms for open-shop\nscheduling and precedence constrained scheduling, and show hardness results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 19:46:49 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kulkarni", "Janardhan", ""], ["Li", "Shi", ""]]}, {"id": "1807.02571", "submitter": "Charlie Dickens", "authors": "Graham Cormode, Charlie Dickens, David P. Woodruff", "title": "Leveraging Well-Conditioned Bases: Streaming \\& Distributed Summaries in\n  Minkowski $p$-Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work on approximate linear algebra has led to efficient distributed and\nstreaming algorithms for problems such as approximate matrix multiplication,\nlow rank approximation, and regression, primarily for the Euclidean norm\n$\\ell_2$. We study other $\\ell_p$ norms, which are more robust for $p < 2$, and\ncan be used to find outliers for $p > 2$. Unlike previous algorithms for such\nnorms, we give algorithms that are (1) deterministic, (2) work simultaneously\nfor every $p \\geq 1$, including $p = \\infty$, and (3) can be implemented in\nboth distributed and streaming environments. We apply our results to\n$\\ell_p$-regression, entrywise $\\ell_1$-low rank approximation, and approximate\nmatrix multiplication.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 21:27:12 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Cormode", "Graham", ""], ["Dickens", "Charlie", ""], ["Woodruff", "David P.", ""]]}, {"id": "1807.02611", "submitter": "Zhengjun Cao", "authors": "Zhengjun Cao and Lihua Liu", "title": "New Algorithms for Subset Sum Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set (or multiset) S of n numbers and a target number t, the subset\nsum problem is to decide if there is a subset of S that sums up to t. There are\nseveral methods for solving this problem, including exhaustive search,\ndivide-and-conquer method, and Bellman's dynamic programming method. However,\nnone of them could generate universal and light code. In this paper, we present\na new deterministic algorithm based on a novel data arrangement, which could\ngenerate such code and return all solutions. If n is small enough, it is\nefficient for usual purpose. We also present a probabilistic version with\none-sided error and a greedy algorithm which could generate a solution with\nminimized variance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 04:40:32 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 22:02:51 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Cao", "Zhengjun", ""], ["Liu", "Lihua", ""]]}, {"id": "1807.03140", "submitter": "Victor Selivanov", "authors": "Svetlana Selivanova, Victor Selivanov", "title": "Bit Complexity of Computing Solutions for Symmetric Hyperbolic Systems\n  of PDEs with Guaranteed Precision", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish upper bounds of bit complexity of computing solution operators\nfor symmetric hyperbolic systems of PDEs. Here we continue the research started\nin in our revious publications where computability, in the rigorous sense of\ncomputable analysis, has been established for solution operators of Cauchy and\ndissipative boundary-value problems for such systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 07:59:37 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 05:49:57 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Selivanova", "Svetlana", ""], ["Selivanov", "Victor", ""]]}, {"id": "1807.03465", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee, Santosh S. Vempala", "title": "The Kannan-Lov\\'asz-Simonovits Conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kannan-Lov\\'asz-Simonovits conjecture says that the Cheeger constant of\nany logconcave density is achieved to within a universal, dimension-independent\nconstant factor by a hyperplane-induced subset. Here we survey the origin and\nconsequences of the conjecture (in geometry, probability, information theory\nand algorithms) as well as recent progress resulting in the current best\nbounds. The conjecture has lead to several techniques of general interest.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:27:24 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Lee", "Yin Tat", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1807.03479", "submitter": "Assaf Kfoury", "authors": "Assaf Kfoury and Benjamin Sisson", "title": "Efficient Reassembling of Three-Regular Planar Graphs", "comments": "49 pages, 25 figures, 15 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reassembling of a simple graph G = (V,E) is an abstraction of a problem\narising in earlier studies of network analysis. There are several equivalent\ndefinitions of graph reassembling; in this report we use a definition which\nmakes it closest to the notion of graph carving. A reassembling is a rooted\nbinary tree whose nodes are subsets of V and whose leaf nodes are singleton\nsets, with each of the latter containing a distinct vertex of G. The parent of\ntwo nodes in the reassembling is the union of the two children's vertex sets.\nThe root node of the reassembling is the full set V. The edge-boundary degree\nof a node in the reassembling is the number of edges in G that connect vertices\nin the node's set to vertices not in the node's set. A reassembling's\nalpha-measure is the largest edge-boundary degree of any node in the\nreassembling. A reassembling of G is alpha-optimal if its alpha-measure is the\nminimum among all alpha-measures of G's reassemblings.\n  The problem of finding an alpha-optimal reassembling of a simple graph in\ngeneral was already shown to be NP-hard.\n  In this report we present an algorithm which, given a 3-regular plane graph G\n= (V,E) as input, returns a reassembling of G with an alpha-measure independent\nof n (number of vertices in G) and upper-bounded by 2k, where k is the\nedge-outerplanarity of G. (Edge-outerplanarity is distinct but closely related\nto the usual notion of outerplanarity; as with outerplanarity, for a fixed\nedge-outerplanarity k, the number n of vertices can be arbitrarily large.) Our\nalgorithm runs in time linear in n. Moreover, we construct a class of\n$3$-regular plane graphs for which this alpha-measure is optimal, by proving\nthat 2k is the lower bound on the alpha-measure of any reassembling of a graph\nin that class.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 04:49:51 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Kfoury", "Assaf", ""], ["Sisson", "Benjamin", ""]]}, {"id": "1807.03611", "submitter": "Jakub T\\v{e}tek", "authors": "Martin Hora, V\\'aclav Kon\\v{c}ick\\'y, Jakub T\\v{e}tek", "title": "Theoretical Model of Computation and Algorithms for FPGA-based Hardware\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While FPGAs have been used extensively as hardware accelerators in industrial\ncomputation, no theoretical model of computation has been devised for the study\nof FPGA-based accelerators. In this paper, we present a theoretical model of\ncomputation on a system with conventional CPU and an FPGA, based on word-RAM.\nWe show several algorithms in this model which are asymptotically faster than\ntheir word-RAM counterparts. Specifically, we show an algorithm for sorting,\nevaluation of associative operation and general techniques for speeding up some\nrecursive algorithms and some dynamic programs. We also derive lower bounds on\nthe running times needed to solve some problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 13:17:37 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 10:51:21 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Hora", "Martin", ""], ["Kon\u010dick\u00fd", "V\u00e1clav", ""], ["T\u011btek", "Jakub", ""]]}, {"id": "1807.03626", "submitter": "Lars Rohwedder", "authors": "Klaus Jansen and Lars Rohwedder", "title": "A note on the integrality gap of the configuration LP for restricted\n  Santa Claus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the restricted Santa Claus problem we are given resources $\\mathcal R$ and\nplayers $\\mathcal P$. Every resource $j\\in\\mathcal R$ has a value $v_j$ and\nevery player $i$ desires a set $\\mathcal R(i)$ of resources. We are interested\nin distributing the resources to players that desire them. The quality of a\nsolution is measured by the least happy player, i.e., the lowest sum of\nresource values. This value should be maximized. The local search algorithm by\nAsadpour et al. and its connection to the configuration LP has proved itself to\nbe a very influential technique for this and related problems. In the original\nproof, a local search was used to obtain a bound of $4$ for the ratio of the\nfractional to the integral optimum of the configuration LP (integrality gap).\nThis bound is non-constructive since the local search has not been shown to\nterminate in polynomial time. On the negative side, the worst instance known\nhas an integrality gap of $2$. Although much progress was made in this area,\nneither bound has been improved since. We present a better analysis that shows\nthe integrality gap is not worse than $3 + 5/6 \\approx 3.8333$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 13:32:12 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Jansen", "Klaus", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1807.03718", "submitter": "Isaac Goldstein", "authors": "Isaac Goldstein, Moshe Lewenstein and Ely Porat", "title": "Improved Space-Time Tradeoffs for kSUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the kSUM problem we are given an array of numbers $a_1,a_2,...,a_n$ and we\nare required to determine if there are $k$ different elements in this array\nsuch that their sum is 0. This problem is a parameterized version of the\nwell-studied SUBSET-SUM problem, and a special case is the 3SUM problem that is\nextensively used for proving conditional hardness. Several works investigated\nthe interplay between time and space in the context of SUBSET-SUM. Recently,\nimproved time-space tradeoffs were proven for kSUM using both randomized and\ndeterministic algorithms.\n  In this paper we obtain an improvement over the best known results for the\ntime-space tradeoff for kSUM. A major ingredient in achieving these results is\na general self-reduction from kSUM to mSUM where $m<k$, and several useful\nobservations that enable this reduction and its implications. The main results\nwe prove in this paper include the following: (i) The best known Las Vegas\nsolution to kSUM running in approximately $O(n^{k-\\delta\\sqrt{2k}})$ time and\nusing $O(n^{\\delta})$ space, for $0 \\leq \\delta \\leq 1$. (ii) The best known\ndeterministic solution to kSUM running in approximately\n$O(n^{k-\\delta\\sqrt{k}})$ time and using $O(n^{\\delta})$ space, for $0 \\leq\n\\delta \\leq 1$. (iii) A space-time tradeoff for solving kSUM using\n$O(n^{\\delta})$ space, for $\\delta>1$. (iv) An algorithm for 6SUM running in\n$O(n^4)$ time using just $O(n^{2/3})$ space. (v) A solution to 3SUM on random\ninput using $O(n^2)$ time and $O(n^{1/3})$ space, under the assumption of a\nrandom read-only access to random bits.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 15:42:17 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Goldstein", "Isaac", ""], ["Lewenstein", "Moshe", ""], ["Porat", "Ely", ""]]}, {"id": "1807.03721", "submitter": "Maximilian Probst", "authors": "Maximilian Probst", "title": "On the complexity of the (approximate) nearest colored node problem", "comments": "Accepted to ESA'2018", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2018.68", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a graph $G=(V,E)$ where each vertex is assigned a color from the set\n$C=\\{c_1, c_2, .., c_\\sigma\\}$. In the (approximate) nearest colored node\nproblem, we want to query, given $v \\in V$ and $c \\in C$, for the (approximate)\ndistance $\\widehat{\\mathbf{dist}}(v, c)$ from $v$ to the nearest node of color\n$c$. For any integer $1 \\leq k \\leq \\log n$, we present a Color Distance Oracle\n(also often referred to as Vertex-label Distance Oracle) of stretch $4k-5$\nusing space $O(kn\\sigma^{1/k})$ and query time $O(\\log{k})$. This improves the\nquery time from $O(k)$ to $O(\\log{k})$ over the best known Color Distance\nOracle by Chechik \\cite{DBLP:journals/corr/abs-1109-3114}. We then prove a\nlower bound in the cell probe model showing that our query time is optimal in\nregard to space up to constant factors. We also investigate dynamic settings of\nthe problem and find new upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 15:46:41 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Probst", "Maximilian", ""]]}, {"id": "1807.03739", "submitter": "Naoto Shiraishi", "authors": "Naoto Shiraishi and Jun Takahashi", "title": "Constructing Concrete Hard Instances of the Maximum Independent Set\n  Problem", "comments": "9 pages, 5 figures", "journal-ref": "J. Stat. Mech. (2019) 113401", "doi": "10.1088/1742-5468/ab409d", "report-no": null, "categories": "cs.DS cond-mat.dis-nn cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a deterministic construction of hard instances for the maximum\nindependent set problem (MIS). The constructed hard instances form an infinite\ngraph sequence with increasing size, which possesses similar characteristics to\nsparse random graphs and in which MIS cannot be solved efficiently. We\nanalytically and numerically show that all algorithms employing cycle-chain\nrefutation, which is a general refutation method we introduce for capturing the\nability of many known algorithms, cannot upper bound the size of the maximum\nindependent set tightly.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 05:56:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Shiraishi", "Naoto", ""], ["Takahashi", "Jun", ""]]}, {"id": "1807.03827", "submitter": "Hicham El Zein", "authors": "Hicham El-Zein, Meng He, J. Ian Munro, Bryce Sandlund", "title": "Improved Time and Space Bounds for Dynamic Range Mode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an array A of $n$ elements, we wish to support queries for the most\nfrequent and least frequent element in a subrange $[l, r]$ of $A$. We also wish\nto support updates that change a particular element at index $i$ or insert/\ndelete an element at index $i$. For the range mode problem, our data structure\nsupports all operations in $O(n^{2/3})$ deterministic time using only $O(n)$\nspace. This improves two results by Chan et al. \\cite{C14}: a linear space data\nstructure supporting update and query operations in $\\tilde{O}(n^{3/4})$ time\nand an $O(n^{4/3})$ space data structure supporting update and query operations\nin $\\tilde{O}(n^{2/3})$ time. For the range least frequent problem, we address\ntwo variations. In the first, we are allowed to answer with an element of $A$\nthat may not appear in the query range, and in the second, the returned element\nmust be present in the query range. For the first variation, we develop a data\nstructure that supports queries in $\\tilde{O}(n^{2/3})$ time, updates in\n$O(n^{2/3})$ time, and occupies $O(n)$ space. For the second variation, we\ndevelop a Monte Carlo data structure that supports queries in $O(n^{2/3})$\ntime, updates in $\\tilde{O}(n^{2/3})$ time, and occupies $\\tilde{O}(n)$ space,\nbut requires that updates are made independently of the results of previous\nqueries. The Monte Carlo data structure is also capable of answering\n$k$-frequency queries; that is, the problem of finding an element of given\nfrequency in the specified query range. Previously, no dynamic data structures\nwere known for least frequent element or $k$-frequency queries.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 19:00:45 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["El-Zein", "Hicham", ""], ["He", "Meng", ""], ["Munro", "J. Ian", ""], ["Sandlund", "Bryce", ""]]}, {"id": "1807.03839", "submitter": "Marek Cygan", "authors": "Marek Cygan and Artur Czumaj and Marcin Mucha and Piotr Sankowski", "title": "Online Facility Location with Deletions", "comments": "full version of ESA'18 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study three previously unstudied variants of the online\nFacility Location problem, considering an intrinsic scenario when the clients\nand facilities are not only allowed to arrive to the system, but they can also\ndepart at any moment.\n  We begin with the study of a natural fully-dynamic online uncapacitated model\nwhere clients can be both added and removed. When a client arrives, then it has\nto be assigned either to an existing facility or to a new facility opened at\nthe client's location. However, when a client who has been also one of the open\nfacilities is to be removed, then our model has to allow to reconnect all\nclients that have been connected to that removed facility. In this model, we\npresent an optimal O(log n_act / log log n_act)-competitive algorithm, where\nn_act is the number of active clients at the end of the input sequence.\n  Next, we turn our attention to the capacitated Facility Location problem. We\nfirst note that if no deletions are allowed, then one can achieve an optimal\ncompetitive ratio of O(log n/ log log n), where n is the length of the\nsequence. However, when deletions are allowed, the capacitated version of the\nproblem is significantly more challenging than the uncapacitated one. We show\nthat still, using a more sophisticated algorithmic approach, one can obtain an\nonline O(log m + log c log n)-competitive algorithm for the capacitated\nFacility Location problem in the fully dynamic model, where m is number of\npoints in the input metric and c is the capacity of any open facility.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 19:41:32 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Cygan", "Marek", ""], ["Czumaj", "Artur", ""], ["Mucha", "Marcin", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1807.03847", "submitter": "Alexander Van Der Grinten", "authors": "Alexander van der Grinten and Elisabetta Bergamini and Oded Green and\n  David A. Bader and Henning Meyerhenke", "title": "Scalable Katz Ranking Computation in Large Static and Dynamic Graphs", "comments": "Published at ESA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network analysis defines a number of centrality measures to identify the most\ncentral nodes in a network. Fast computation of those measures is a major\nchallenge in algorithmic network analysis. Aside from closeness and\nbetweenness, Katz centrality is one of the established centrality measures. In\nthis paper, we consider the problem of computing rankings for Katz centrality.\nIn particular, we propose upper and lower bounds on the Katz score of a given\nnode. While previous approaches relied on numerical approximation or heuristics\nto compute Katz centrality rankings, we construct an algorithm that iteratively\nimproves those upper and lower bounds until a correct Katz ranking is obtained.\nWe extend our algorithm to dynamic graphs while maintaining its correctness\nguarantees. Experiments demonstrate that our static graph algorithm outperforms\nboth numerical approaches and heuristics with speedups between 1.5x and 3.5x,\ndepending on the desired quality guarantees. Our dynamic graph algorithm\nimproves upon the static algorithm for update batches of less than 10000 edges.\nWe provide efficient parallel CPU and GPU implementations of our algorithms\nthat enable near real-time Katz centrality computation for graphs with hundreds\nof millions of nodes in fractions of seconds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:18:40 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["van der Grinten", "Alexander", ""], ["Bergamini", "Elisabetta", ""], ["Green", "Oded", ""], ["Bader", "David A.", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1807.04152", "submitter": "Yuchen Mao", "authors": "Siu-Wing Cheng and Yuchen Mao", "title": "Integrality Gap of the Configuration LP for the Restricted Max-Min Fair\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The max-min fair allocation problem seeks an allocation of resources to\nplayers that maximizes the minimum total value obtained by any player. Each\nplayer $p$ has a non-negative value $v_{pr}$ on resource $r$. In the restricted\ncase, we have $v_{pr}\\in \\{v_r, 0\\}$. That is, a resource $r$ is worth value\n$v_r$ for the players who desire it and value 0 for the other players. In this\npaper, we consider the configuration LP, a linear programming relaxation for\nthe restricted problem. The integrality gap of the configuration LP is at least\n$2$. Asadpour, Feige, and Saberi proved an upper bound of $4$. We improve the\nupper bound to $23/6$ using the dual of the configuration LP. Since the\nconfiguration LP can be solved to any desired accuracy $\\delta$ in polynomial\ntime, our result leads to a polynomial-time algorithm which estimates the\noptimal value within a factor of $23/6+\\delta$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:20:48 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Cheng", "Siu-Wing", ""], ["Mao", "Yuchen", ""]]}, {"id": "1807.04186", "submitter": "Assaf Kfoury", "authors": "Assaf Kfoury", "title": "A Fixed-Parameter Linear-Time Algorithm for Maximum Flow in Planar Flow\n  Networks", "comments": "14 pages, 2 figures, 18 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pull together previously established graph-theoretical results to produce\nthe algorithm in the paper's title. The glue are three easy elementary lemmas.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:16:25 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kfoury", "Assaf", ""]]}, {"id": "1807.04271", "submitter": "Ewin Tang", "authors": "Ewin Tang", "title": "A quantum-inspired classical algorithm for recommendation systems", "comments": "32 pages; revised structure of document, improved runtime, simplified\n  algorithm and notation", "journal-ref": null, "doi": "10.1145/3313276.3316310", "report-no": null, "categories": "cs.IR cs.DS cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a classical analogue to Kerenidis and Prakash's quantum\nrecommendation system, previously believed to be one of the strongest\ncandidates for provably exponential speedups in quantum machine learning. Our\nmain result is an algorithm that, given an $m \\times n$ matrix in a data\nstructure supporting certain $\\ell^2$-norm sampling operations, outputs an\n$\\ell^2$-norm sample from a rank-$k$ approximation of that matrix in time\n$O(\\text{poly}(k)\\log(mn))$, only polynomially slower than the quantum\nalgorithm. As a consequence, Kerenidis and Prakash's algorithm does not in fact\ngive an exponential speedup over classical algorithms. Further, under strong\ninput assumptions, the classical recommendation system resulting from our\nalgorithm produces recommendations exponentially faster than previous classical\nsystems, which run in time linear in $m$ and $n$.\n  The main insight of this work is the use of simple routines to manipulate\n$\\ell^2$-norm sampling distributions, which play the role of quantum\nsuperpositions in the classical setting. This correspondence indicates a\npotentially fruitful framework for formally comparing quantum machine learning\nalgorithms to classical machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:57:24 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 02:49:30 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 05:03:18 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Tang", "Ewin", ""]]}, {"id": "1807.04308", "submitter": "Amariah Becker", "authors": "Amariah Becker and Alice Paul", "title": "A Framework for Vehicle Routing Approximation Schemes in Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for designing polynomial-time approximation\nschemes (PTASs) for various vehicle routing problems in trees. In these\nproblems, the goal is to optimally route a fleet of vehicles, originating at a\ndepot, to serve a set of clients, subject to various constraints. For example,\nin Minimum Makespan Vehicle Routing, the number of vehicles is fixed, and the\nobjective is to minimize the longest distance traveled by a single vehicle. Our\nmain insight is that we can often greatly restrict the set of potential\nsolutions without adding too much to the optimal solution cost. This\nsimplification relies on partitioning the tree into clusters such that there\nexists a near-optimal solution in which every vehicle that visits a given\ncluster takes on one of a few forms. In particular, only a small number of\nvehicles serve clients in any given cluster. By using these coarser building\nblocks, a dynamic programming algorithm can find a near-optimal solution in\npolynomial time. We show that the framework is flexible enough to give PTASs\nfor many problems, including Minimum Makespan Vehicle Routing,\nDistance-Constrained Vehicle Routing, Capacitated Vehicle Routing, and School\nBus Routing, and can be extended to the multiple depot setting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 18:14:33 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 21:56:47 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Becker", "Amariah", ""], ["Paul", "Alice", ""]]}, {"id": "1807.04345", "submitter": "Brenton Lessley", "authors": "Brenton Lessley", "title": "Data-Parallel Hashing Techniques for GPU Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hash tables are one of the most fundamental data structures for effectively\nstoring and accessing sparse data, with widespread usage in domains ranging\nfrom computer graphics to machine learning. This study surveys the\nstate-of-the-art research on data-parallel hashing techniques for emerging\nmassively-parallel, many-core GPU architectures. Key factors affecting the\nperformance of different hashing schemes are discovered and used to suggest\nbest practices and pinpoint areas for further research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 20:33:22 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Lessley", "Brenton", ""]]}, {"id": "1807.04400", "submitter": "Dhruv Rohatgi", "authors": "Dhruv Rohatgi", "title": "Sliding window order statistics in sublinear space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the multi-pass streaming model to sliding window problems, and\naddress the problem of computing order statistics on fixed-size sliding\nwindows, in the multi-pass streaming model as well as the closely related\ncommunication complexity model. In the $2$-pass streaming model, we show that\non input of length $N$ with values in range $[0,R]$ and a window of length $K$,\nsliding window minimums can be computed in $\\widetilde{O}(\\sqrt{N})$. We show\nthat this is nearly optimal (for any constant number of passes) when $R \\geq\nK$, but can be improved when $R = o(K)$ to $\\widetilde{O}(\\sqrt{NR/K})$.\nFurthermore, we show that there is an $(l+1)$-pass streaming algorithm which\ncomputes $l^\\text{th}$-smallest elements in $\\widetilde{O}(l^{3/2} \\sqrt{N})$\nspace. In the communication complexity model, we describe a simple\n$\\widetilde{O}(pN^{1/p})$ algorithm to compute minimums in $p$ rounds of\ncommunication for odd $p$, and a more involved algorithm which computes the\n$l^\\text{th}$-smallest elements in $\\widetilde{O}(pl^2 N^{1/(p-2l-1)})$ space.\nFinally, we prove that the majority statistic on boolean streams cannot be\ncomputed in sublinear space, implying that $l^\\text{th}$-smallest elements\ncannot be computed in space both sublinear in $N$ and independent of $l$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 02:11:34 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Rohatgi", "Dhruv", ""]]}, {"id": "1807.04404", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Michael B. Cohen and James R. Lee and Yin Tat\n  Lee", "title": "Metrical task systems on trees via mirror descent and unfair gluing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider metrical task systems on tree metrics, and present an\n$O(\\mathrm{depth} \\times \\log n)$-competitive randomized algorithm based on the\nmirror descent framework introduced in our prior work on the $k$-server\nproblem. For the special case of hierarchically separated trees (HSTs), we use\nmirror descent to refine the standard approach based on gluing unfair metrical\ntask systems. This yields an $O(\\log n)$-competitive algorithm for HSTs, thus\nremoving an extraneous $\\log\\log n$ in the bound of Fiat and Mendel (2003).\nCombined with well-known HST embedding theorems, this also gives an $O((\\log\nn)^2)$-competitive randomized algorithm for every $n$-point metric space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 02:28:32 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 21:59:20 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 22:31:23 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cohen", "Michael B.", ""], ["Lee", "James R.", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1807.04496", "submitter": "Abhranil Chatterjee", "authors": "V.Arvind, Abhranil Chatterjee, Rajit Datta, Partha Mukhopadhyay", "title": "Fast Exact Algorithms Using Hadamard Product of Polynomials", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Let $C$ be an arithmetic circuit of $poly(n)$ size given as input that\ncomputes a polynomial $f\\in\\mathbb{F}[X]$, where $X=\\{x_1,x_2,\\ldots,x_n\\}$ and\n$\\mathbb{F}$ is any field where the field arithmetic can be performed\nefficiently. We obtain new algorithms for the following two problems first\nstudied by Koutis and Williams \\cite{Kou08, Wi09, KW16}. k-MLC: Compute the sum\nof the coefficients of all degree-$k$ multilinear monomials in the polynomial\n$f$. k-MMD: Test if there is a nonzero degree-$k$ multilinear monomial in the\npolynomial $f$.\n  Our algorithms are based on the fact that the Hadamard product $f\\circ\nS_{n,k}$, is the degree-$k$ multilinear part of $f$, where $S_{n,k}$ is the\n$k^{th}$ elementary symmetric polynomial.\n  1. For k-MLC problem, we give a deterministic algorithm of run time\n$O^*(n^{k/2+c\\log k})$ (where $c$ is a constant), answering an open question of\nKoutis and Williams \\cite[ICALP'09]{KW16}. As corollaries, we show\n$O^*(\\binom{n}{\\downarrow k/2})$-time exact counting algorithms for several\ncombinatorial problems: k-Tree, t-Dominating Set, m-Dimensional k-Matching.\n  2. For k-MMD problem, we give a randomized algorithm of run time $4.32^k\\cdot\npoly(n,k)$. Our algorithm uses only $poly(n,k)$ space. This matches the run\ntime of a recent algorithm \\cite{BDH18} for $k-MMD$ which requires exponential\n(in $k$) space.\n  Other results include fast deterministic algorithms for k-MLC and k-MMD\nproblems for depth three circuits.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 09:34:29 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 19:08:13 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Arvind", "V.", ""], ["Chatterjee", "Abhranil", ""], ["Datta", "Rajit", ""], ["Mukhopadhyay", "Partha", ""]]}, {"id": "1807.04518", "submitter": "Melanie Schmidt", "authors": "Dan Feldman and Melanie Schmidt and Christian Sohler", "title": "Turning Big data into tiny data: Constant-size coresets for k-means, PCA\n  and projective clustering", "comments": "The conference version of this work appeared at SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze a method to reduce the size of a very large set of\ndata points in a high dimensional Euclidean space R d to a small set of\nweighted points such that the result of a predetermined data analysis task on\nthe reduced set is approximately the same as that for the original point set.\nFor example, computing the first k principal components of the reduced set will\nreturn approximately the first k principal components of the original set or\ncomputing the centers of a k-means clustering on the reduced set will return an\napproximation for the original set. Such a reduced set is also known as a\ncoreset. The main new feature of our construction is that the cardinality of\nthe reduced set is independent of the dimension d of the input space and that\nthe sets are mergable. The latter property means that the union of two reduced\nsets is a reduced set for the union of the two original sets (this property has\nrecently also been called composability, see Indyk et. al., PODS 2014). It\nallows us to turn our methods into streaming or distributed algorithms using\nstandard approaches. For problems such as k-means and subspace approximation\nthe coreset sizes are also independent of the number of input points. Our\nmethod is based on projecting the points on a low dimensional subspace and\nreducing the cardinality of the points inside this subspace using known\nmethods. The proposed approach works for a wide range of data analysis\ntechniques including k-means clustering, principal component analysis and\nsubspace clustering. The main conceptual contribution is a new coreset\ndefinition that allows to charge costs that appear for every solution to an\nadditive constant.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 10:25:12 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Feldman", "Dan", ""], ["Schmidt", "Melanie", ""], ["Sohler", "Christian", ""]]}, {"id": "1807.04528", "submitter": "St\\'efi Nouleho Ilemo", "authors": "St\\'efi Nouleho and Dominique Barth and Franck Quessette and\n  Marc-Antoine Weisser and Dimitri Watel and Olivier David", "title": "A new graph modelisation for molecule similarity", "comments": "32 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to define the process of restrosynthesis of a new organic molecule,\nit is often necessary to be able to draw inspiration from that of a molecule\nsimilar to the target one of which we know such a process. To compute such a\nsimilarity, an oftently used approach is to solve a Maximum Common Edge\nSubgraph (MCES) problem on molecular graphs, but such an approach is limited by\ncomputation time and pertinence of similarity measurement. In this paper, we\ndefine and analyse here a new graph representation of molecules to\nalgorithmically compare them. The purpose is to model the structure of molecule\nby a graph smaller than the molecular graph and representing the interconnexion\nof its elementary cycles. We provide an algorithm to efficiently obtain such a\ngraph of cycles from a molecular graph. Then by solving MCES problems on those\ngraphs, we evaluate the pertinence of using graphs of cycles for molecular\nsimilarity on a select set of molecules.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 10:38:47 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Nouleho", "St\u00e9fi", ""], ["Barth", "Dominique", ""], ["Quessette", "Franck", ""], ["Weisser", "Marc-Antoine", ""], ["Watel", "Dimitri", ""], ["David", "Olivier", ""]]}, {"id": "1807.04575", "submitter": "Takanori Maehara", "authors": "Masakazu Ishihata, Takanori Maehara, Tomas Rigaux", "title": "Algorithmic Meta-Theorems for Monotone Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a monotone submodular maximization problem whose constraint is\ndescribed by a logic formula on a graph. Formally, we prove the following three\n`algorithmic metatheorems.'\n  (1) If the constraint is specified by a monadic second-order logic on a graph\nof bounded treewidth, the problem is solved in $n^{O(1)}$ time with an\napproximation factor of $O(\\log n)$.\n  (2) If the constraint is specified by a first-order logic on a graph of low\ndegree, the problem is solved in $O(n^{1 + \\epsilon})$ time for any $\\epsilon >\n0$ with an approximation factor of $2$.\n  (3) If the constraint is specified by a first-order logic on a graph of\nbounded expansion, the problem is solved in $n^{O(\\log k)}$ time with an\napproximation factor of $O(\\log k)$, where $k$ is the number of variables and\n$O(\\cdot)$ suppresses only constants independent of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 12:36:27 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Ishihata", "Masakazu", ""], ["Maehara", "Takanori", ""], ["Rigaux", "Tomas", ""]]}, {"id": "1807.04599", "submitter": "Timothy Goodrich", "authors": "Eugene F. Dumitrescu, Allison L. Fisher, Timothy D. Goodrich, Travis\n  S. Humble, Blair D. Sullivan, Andrew L. Wright", "title": "Benchmarking treewidth as a practical component of tensor-network--based\n  quantum simulation", "comments": "Open source code available", "journal-ref": null, "doi": "10.1371/journal.pone.0207827", "report-no": null, "categories": "cs.DS physics.comp-ph quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor networks are powerful factorization techniques which reduce resource\nrequirements for numerically simulating principal quantum many-body systems and\nalgorithms. The computational complexity of a tensor network simulation depends\non the tensor ranks and the order in which they are contracted. Unfortunately,\ncomputing optimal contraction sequences (orderings) in general is known to be a\ncomputationally difficult (NP-complete) task. In 2005, Markov and Shi showed\nthat optimal contraction sequences correspond to optimal (minimum width) tree\ndecompositions of a tensor network's line graph, relating the contraction\nsequence problem to a rich literature in structural graph theory. While\ntreewidth-based methods have largely been ignored in favor of dataset-specific\nalgorithms in the prior tensor networks literature, we demonstrate their\npractical relevance for problems arising from two distinct methods used in\nquantum simulation: multi-scale entanglement renormalization ansatz (MERA)\ndatasets and quantum circuits generated by the quantum approximate optimization\nalgorithm (QAOA). We exhibit multiple regimes where treewidth-based algorithms\noutperform domain-specific algorithms, while demonstrating that the optimal\nchoice of algorithm has a complex dependence on the network density, expected\ncontraction complexity, and user run time requirements. We further provide an\nopen source software framework designed with an emphasis on accessibility and\nextendability, enabling replicable experimental evaluations and future\nexploration of competing methods by practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 13:32:55 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Dumitrescu", "Eugene F.", ""], ["Fisher", "Allison L.", ""], ["Goodrich", "Timothy D.", ""], ["Humble", "Travis S.", ""], ["Sullivan", "Blair D.", ""], ["Wright", "Andrew L.", ""]]}, {"id": "1807.04613", "submitter": "Kaushik Mondal", "authors": "Chen Avin, Kaushik Mondal, Stefan Schmid", "title": "Push-Down Trees: Optimal Self-Adjusting Complete Trees", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a fundamental algorithmic problem related to the design of\ndemand-aware networks: networks whose topologies adjust toward the traffic\npatterns they serve, in an online manner. The goal is to strike a tradeoff\nbetween the benefits of such adjustments (shorter routes) and their costs\n(reconfigurations). In particular, we consider the problem of designing a\nself-adjusting tree network which serves single-source, multi-destination\ncommunication. The problem has interesting connections to self-adjusting\ndatastructures. We present two constant-competitive online algorithms for this\nproblem, one randomized and one deterministic. Our approach is based on a\nnatural notion of Most Recently Used (MRU) tree, maintaining a working set. We\nprove that the working set is a cost lower bound for any online algorithm, and\nthen present a randomized algorithm RANDOM-PUSH which approximates such an MRU\ntree at low cost, by pushing less recently used communication partners down the\ntree, along a random walk. Our deterministic algorithm MOVE-HALF does not\ndirectly maintain an MRU tree, but its cost is still proportional to the cost\nof an MRU tree, and also matches the working set lower bound.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 14:00:35 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 08:50:49 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Avin", "Chen", ""], ["Mondal", "Kaushik", ""], ["Schmid", "Stefan", ""]]}, {"id": "1807.04682", "submitter": "Nicolas Schabanel", "authors": "Erik D. Demaine and Jacob Hendricks and Meagan Olsen and Matthew J.\n  Patitz and Trent A. Rogers and Nicolas Schabanel and Shinnosuke Seki and\n  Hadley Thomas", "title": "Know When to Fold 'Em: Self-Assembly of Shapes by Folding in Oritatami", "comments": null, "journal-ref": "Short version published at DNA24, 2018", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An oritatami system (OS) is a theoretical model of self-assembly via\nco-transcriptional folding. It consists of a growing chain of beads which can\nform bonds with each other as they are transcribed. During the transcription\nprocess, the $\\delta$ most recently produced beads dynamically fold so as to\nmaximize the number of bonds formed, self-assemblying into a shape\nincrementally. The parameter $\\delta$ is called the delay and is related to the\ntranscription rate in nature.\n  This article initiates the study of shape self-assembly using oritatami. A\nshape is a connected set of points in the triangular lattice. We first show\nthat oritatami systems differ fundamentally from tile-assembly systems by\nexhibiting a family of infinite shapes that can be tile-assembled but cannot be\nfolded by any OS. As it is NP-hard in general to determine whether there is an\nOS that folds into (self-assembles) a given finite shape, we explore the\nfolding of upscaled versions of finite shapes. We show that any shape can be\nfolded from a constant size seed, at any scale n >= 3, by an OS with delay 1.\nWe also show that any shape can be folded at the smaller scale 2 by an OS with\nunbounded delay. This leads us to investigate the influence of delay and to\nprove that, for all {\\delta} > 2, there are shapes that can be folded (at scale\n1) with delay {\\delta} but not with delay {\\delta}'<{\\delta}. These results\nserve as a foundation for the study of shape-building in this new model of\nself-assembly, and have the potential to provide better understanding of\ncotranscriptional folding in biology, as well as improved abilities of\nexperimentalists to design artificial systems that self-assemble via this\ncomplex dynamical process.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:47:56 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 12:28:09 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Demaine", "Erik D.", ""], ["Hendricks", "Jacob", ""], ["Olsen", "Meagan", ""], ["Patitz", "Matthew J.", ""], ["Rogers", "Trent A.", ""], ["Schabanel", "Nicolas", ""], ["Seki", "Shinnosuke", ""], ["Thomas", "Hadley", ""]]}, {"id": "1807.04802", "submitter": "Nathaniel Lahn", "authors": "Nathaniel Lahn and Sharath Raghvendra", "title": "A Faster Algorithm for Minimum-Cost Bipartite Matching in Minor-Free\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $\\tilde{O}(n^{7/5} \\log (nC))$-time algorithm to compute a\nminimum-cost maximum cardinality matching (optimal matching) in $K_h$-minor\nfree graphs with $h=O(1)$ and integer edge weights having magnitude at most\n$C$. This improves upon the $\\tilde{O}(n^{10/7}\\log{C})$ algorithm of Cohen et\nal. [SODA 2017] and the $O(n^{3/2}\\log (nC))$ algorithm of Gabow and Tarjan\n[SIAM J. Comput. 1989].\n  For a graph with $m$ edges and $n$ vertices, the well-known Hungarian\nAlgorithm computes a shortest augmenting path in each phase in $O(m)$ time,\nyielding an optimal matching in $O(mn)$ time. The Hopcroft-Karp [SIAM J.\nComput. 1973], and Gabow-Tarjan [SIAM J. Comput. 1989] algorithms compute, in\neach phase, a maximal set of vertex-disjoint shortest augmenting paths (for\nappropriately defined costs) in $O(m)$ time. This reduces the number of phases\nfrom $n$ to $O(\\sqrt{n})$ and the total execution time to $O(m\\sqrt{n})$.\n  In order to obtain our speed-up, we relax the conditions on the augmenting\npaths and iteratively compute, in each phase, a set of carefully selected\naugmenting paths that are not restricted to be shortest or vertex-disjoint. As\na result, our algorithm computes substantially more augmenting paths in each\nphase, reducing the number of phases from $O(\\sqrt{n})$ to $O(n^{2/5})$. By\nusing small vertex separators, the execution of each phase takes $\\tilde{O}(m)$\ntime on average. For planar graphs, we combine our algorithm with efficient\nshortest path data structures to obtain a minimum-cost perfect matching in\n$\\tilde{O}(n^{6/5} \\log{(nC)})$ time. This improves upon the recent\n$\\tilde{O}(n^{4/3}\\log{(nC)})$ time algorithm by Asathulla et al. [SODA 2018].\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 19:48:39 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Lahn", "Nathaniel", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "1807.04804", "submitter": "Will Perkins", "authors": "Matthew Jenssen, Peter Keevash, Will Perkins", "title": "Algorithms for #BIS-hard problems on expander graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an FPTAS and an efficient sampling algorithm for the high-fugacity\nhard-core model on bounded-degree bipartite expander graphs and the\nlow-temperature ferromagnetic Potts model on bounded-degree expander graphs.\nThe results apply, for example, to random (bipartite) $\\Delta$-regular graphs,\nfor which no efficient algorithms were known for these problems (with the\nexception of the Ising model) in the non-uniqueness regime of the infinite\n$\\Delta$-regular tree. We also find efficient counting and sampling algorithms\nfor proper $q$-colorings of random $\\Delta$-regular bipartite graphs when $q$\nis sufficiently small as a function of $\\Delta$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 20:00:05 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 11:22:51 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 20:38:10 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Jenssen", "Matthew", ""], ["Keevash", "Peter", ""], ["Perkins", "Will", ""]]}, {"id": "1807.04825", "submitter": "Kyriakos Axiotis", "authors": "Kyriakos Axiotis, Arturs Backurs, and Christos Tzamos", "title": "Fast Modular Subset Sum using Linear Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n positive integers, the Modular Subset Sum problem asks if a subset\nadds up to a given target t modulo a given integer m. This is a natural\ngeneralization of the Subset Sum problem (where m=+\\infty) with ties to\nadditive combinatorics and cryptography.\n  Recently, in [Bringmann, SODA'17] and [Koiliaris and Xu, SODA'17], efficient\nalgorithms have been developed for the non-modular case, running in near-linear\npseudo-polynomial time. For the modular case, however, the best known algorithm\nby Koiliaris and Xu [Koiliaris and Xu, SODA'17] runs in time O~(m^{5/4}).\n  In this paper, we present an algorithm running in time O~(m), which matches a\nrecent conditional lower bound of [Abboud et al.'17] based on the Strong\nExponential Time Hypothesis. Interestingly, in contrast to most previous\nresults on Subset Sum, our algorithm does not use the Fast Fourier Transform.\nInstead, it is able to simulate the \"textbook\" Dynamic Programming algorithm\nmuch faster, using ideas from linear sketching. This is one of the first\napplications of sketching-based techniques to obtain fast algorithms for\ncombinatorial problems in an offline setting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 21:18:31 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Axiotis", "Kyriakos", ""], ["Backurs", "Arturs", ""], ["Tzamos", "Christos", ""]]}, {"id": "1807.04900", "submitter": "Gregory Schwartzman", "authors": "Ran Ben-Basat, Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "Parameterized Distributed Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we initiate a thorough study of parameterized graph\noptimization problems in the distributed setting. In a parameterized problem,\nan algorithm decides whether a solution of size bounded by a \\emph{parameter}\n$k$ exists and if so, it finds one. We study fundamental problems, including\nMinimum Vertex Cover (MVC), Maximum Independent Set (MaxIS), Maximum Matching\n(MaxM), and many others, in both the LOCAL and CONGEST distributed computation\nmodels. We present lower bounds for the round complexity of solving\nparameterized problems in both models, together with optimal and near-optimal\nupper bounds.\n  Our results extend beyond the scope of parameterized problems. We show that\nany LOCAL $(1+\\epsilon)$-approximation algorithm for the above problems must\ntake $\\Omega(\\epsilon^{-1})$ rounds. Joined with the algorithm of [GKM17] and\nthe $\\Omega(\\sqrt{\\frac{\\log n}{\\log\\log n}})$ lower bound of [KMW16], this\nsettles the complexity of $(1+\\epsilon)$-approximating MVC, MaxM and MaxIS at\n$(\\epsilon^{-1}\\log n)^{\\Theta(1)}$. We also show that our parameterized\napproach reduces the runtime of exact and approximate CONGEST algorithms for\nMVC and MaxM if the optimal solution is small, without knowing its size\nbeforehand. Finally, we propose the first deterministic $o(n^2)$ rounds CONGEST\nalgorithms that approximate MVC and MaxM within a factor strictly smaller than\n$2$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 03:48:35 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 03:52:02 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1807.04910", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan", "title": "3-wise Independent Random Walks can be Slightly Unbounded", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many streaming algorithms have utilized generalizations of the fact\nthat the expected maximum distance of any $4$-wise independent random walk on a\nline over $n$ steps is $O(\\sqrt{n})$. In this paper, we show that $4$-wise\nindependence is required for all of these algorithms, by constructing a\n$3$-wise independent random walk with expected maximum distance\n$\\Omega(\\sqrt{n} \\lg n)$ from the origin. We prove that this bound is tight for\nthe first and second moment, and also extract a surprising matrix inequality\nfrom these results.\n  Next, we consider a generalization where the steps $X_i$ are $k$-wise\nindependent random variables with bounded $p$th moments. For general $k, p$, we\ndetermine the (asymptotically) maximum possible $p$th moment of the supremum of\n$X_1 + \\dots + X_i$ over $1 \\le i \\le n$. We highlight the case $k = 4, p = 2$:\nhere, we prove that the second moment of the furthest distance traveled is\n$O(\\sum X_i^2)$. For this case, we only need the $X_i$'s to have bounded second\nmoments and do not even need the $X_i$'s to be identically distributed. This\nimplies an asymptotically stronger statement than Kolmogorov's maximal\ninequality that requires only $4$-wise independent random variables, and\ngeneralizes a recent result of B{\\l}asiok.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 04:48:14 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:58:49 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Narayanan", "Shyam", ""]]}, {"id": "1807.04936", "submitter": "Abhishek Shetty", "authors": "Navin Goyal and Abhishek Shetty", "title": "Non-Gaussian Component Analysis using Entropy Methods", "comments": null, "journal-ref": null, "doi": "10.1145/3313276.3316309", "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian component analysis (NGCA) is a problem in multidimensional data\nanalysis which, since its formulation in 2006, has attracted considerable\nattention in statistics and machine learning. In this problem, we have a random\nvariable $X$ in $n$-dimensional Euclidean space. There is an unknown subspace\n$\\Gamma$ of the $n$-dimensional Euclidean space such that the orthogonal\nprojection of $X$ onto $\\Gamma$ is standard multidimensional Gaussian and the\northogonal projection of $X$ onto $\\Gamma^{\\perp}$, the orthogonal complement\nof $\\Gamma$, is non-Gaussian, in the sense that all its one-dimensional\nmarginals are different from the Gaussian in a certain metric defined in terms\nof moments. The NGCA problem is to approximate the non-Gaussian subspace\n$\\Gamma^{\\perp}$ given samples of $X$.\n  Vectors in $\\Gamma^{\\perp}$ correspond to `interesting' directions, whereas\nvectors in $\\Gamma$ correspond to the directions where data is very noisy. The\nmost interesting applications of the NGCA model is for the case when the\nmagnitude of the noise is comparable to that of the true signal, a setting in\nwhich traditional noise reduction techniques such as PCA don't apply directly.\nNGCA is also related to dimension reduction and to other data analysis problems\nsuch as ICA. NGCA-like problems have been studied in statistics for a long time\nusing techniques such as projection pursuit.\n  We give an algorithm that takes polynomial time in the dimension $n$ and has\nan inverse polynomial dependence on the error parameter measuring the angle\ndistance between the non-Gaussian subspace and the subspace output by the\nalgorithm. Our algorithm is based on relative entropy as the contrast function\nand fits under the projection pursuit framework. The techniques we develop for\nanalyzing our algorithm maybe of use for other related problems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 06:47:06 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 03:51:53 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 06:06:58 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Goyal", "Navin", ""], ["Shetty", "Abhishek", ""]]}, {"id": "1807.04942", "submitter": "Takanori Maehara", "authors": "Soh Kumabe, Takanori Maehara, Ryoma Sin'ya", "title": "Linear Pseudo-Polynomial Factor Algorithm for Automaton Constrained Tree\n  Knapsack Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automaton constrained tree knapsack problem is a variant of the knapsack\nproblem in which the items are associated with the vertices of the tree, and we\ncan select a subset of items that is accepted by a top-down tree automaton. If\nthe capacities or the profits of items are integers, the problem can be solved\nin pseudo-polynomial time using the dynamic programming algorithm. However, the\nnatural implementation of this algorithm has a quadratic pseudo-polynomial\nfactor in its complexity because of the max-plus convolution. In this study, we\npropose a new dynamic programming technique, called \\emph{heavy-light recursive\ndynamic programming}, to obtain pseudo-polynomial time algorithms having linear\npseudo-polynomial factors in the complexity. Such algorithms can be used for\nsolving the problems with polynomially small capacities/profits efficiently,\nand used for deriving efficient fully polynomial-time approximation schemes. We\nalso consider the $k$-subtree version problem that finds $k$ disjoint subtrees\nand a solution in each subtree that maximizes total profit under a budget\nconstraint. We show that this problem can be solved in almost the same order as\nthe original problem.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 07:07:21 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 07:28:08 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Kumabe", "Soh", ""], ["Maehara", "Takanori", ""], ["Sin'ya", "Ryoma", ""]]}, {"id": "1807.04965", "submitter": "Tasuku Soma", "authors": "Tasuku Soma", "title": "No-regret algorithms for online $k$-submodular maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial time algorithm for online maximization of\n$k$-submodular maximization. For online (nonmonotone) $k$-submodular\nmaximization, our algorithm achieves a tight approximate factor in an\napproximate regret. For online monotone $k$-submodular maximization, our\napproximate-regret matches to the best-known approximation ratio, which is\ntight asymptotically as $k$ tends to infinity. Our approach is based on the\nBlackwell approachability theorem and online linear optimization.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 08:19:12 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Soma", "Tasuku", ""]]}, {"id": "1807.04974", "submitter": "Yuichi Yoshida", "authors": "Tasuku Soma and Yuichi Yoshida", "title": "Spectral Sparsification of Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an undirected/directed hypergraph $G=(V,E)$, its Laplacian\n$L_G\\colon\\mathbb{R}^V\\to \\mathbb{R}^V$ is defined such that its ``quadratic\nform'' $\\boldsymbol{x}^\\top L_G(\\boldsymbol{x})$ captures the cut information\nof $G$. In particular, $\\boldsymbol{1}_S^\\top L_G(\\boldsymbol{1}_S)$ coincides\nwith the cut size of $S \\subseteq V$, where $\\boldsymbol{1}_S \\in \\mathbb{R}^V$\nis the characteristic vector of $S$. A weighted subgraph $H$ of a hypergraph\n$G$ on a vertex set $V$ is said to be an $\\epsilon$-spectral sparsifier of $G$\nif $(1-\\epsilon)\\boldsymbol{x}^\\top L_H(\\boldsymbol{x}) \\leq\n\\boldsymbol{x}^\\top L_G(\\boldsymbol{x}) \\leq (1+\\epsilon)\\boldsymbol{x}^\\top\nL_H(\\boldsymbol{x})$ holds for every $\\boldsymbol{x} \\in \\mathbb{R}^V$. In this\npaper, we present a polynomial-time algorithm that, given an\nundirected/directed hypergraph $G$ on $n$ vertices, constructs an\n$\\epsilon$-spectral sparsifier of $G$ with $O(n^3\\log n/\\epsilon^2)$\nhyperedges/hyperarcs. The proposed spectral sparsification can be used to\nimprove the time and space complexities of algorithms for solving problems that\ninvolve the quadratic form, such as computing the eigenvalues of $L_G$,\ncomputing the effective resistance between a pair of vertices in $G$,\nsemi-supervised learning based on $L_G$, and cut problems on $G$. In addition,\nour sparsification result implies that any submodular function $f\\colon 2^V \\to\n\\mathbb{R}_+$ with $f(\\emptyset)=f(V)=0$ can be concisely represented by a\ndirected hypergraph. Accordingly, we show that, for any distribution, we can\nproperly and agnostically learn submodular functions $f\\colon 2^V \\to [0,1]$\nwith $f(\\emptyset)=f(V)=0$, with $O(n^4\\log (n/\\epsilon) /\\epsilon^4)$ samples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 08:32:09 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Soma", "Tasuku", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1807.05009", "submitter": "Szabolcs Iv\\'an", "authors": "Kitti Gelle, Szabolcs Ivan", "title": "Maintaning maximal matching with lookahead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of fully dynamic maximal matching with\nlookahead. In a fully dynamic $n$-vertex graph setting, we have to handle\nupdates (insertions and removals of edges), and answer queries regarding the\ncurrent graph, preferably with a better time bound than that when running the\ntrivial deterministic algorithm with worst-case time of $O(m)$ (where $m$ is\nthe all-time maximum number of the edges) and recompute the matching from\nscratch each time a query arrives. We show that a maximal matching can be\nmaintained in an (undirected) general graph with a deterministic amortized\nupdate cost of $O(\\log m)$, provided that a lookahead of length $m$ is\navailable, i.e. we can ``take a peek'' at the next $m$ update operations in\nadvance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 11:11:34 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Gelle", "Kitti", ""], ["Ivan", "Szabolcs", ""]]}, {"id": "1807.05014", "submitter": "Ran Gelles", "authors": "Mark Braverman and Klim Efremenko and Ran Gelles and Michael A.\n  Yitayew", "title": "Optimal Short-Circuit Resilient Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fault-tolerant boolean formulas in which the output of a faulty\ngate is short-circuited to one of the gate's inputs. A recent result by Kalai\net al. (FOCS 2012) converts any boolean formula into a resilient formula of\npolynomial size that works correctly if less than a fraction $1/6$ of the gates\n(on every input-to-output path) are faulty. We improve the result of Kalai et\nal., and show how to efficiently fortify any boolean formula against a fraction\n$1/5$ of short-circuit gates per path, with only a polynomial blowup in size.\nWe additionally show that it is impossible to obtain formulas with higher\nresilience and sub-exponential growth in size.\n  Towards our results, we consider interactive coding schemes when noiseless\nfeedback is present; these produce resilient boolean formulas via a\nKarchmer-Wigderson relation. We develop a coding scheme that resists up to a\nfraction $1/5$ of corrupted transmissions in each direction of the interactive\nchannel. We further show that such a level of noise is maximal for coding\nschemes with sub-exponential blowup in communication. Our coding scheme takes a\nsurprising inspiration from Blockchain technology.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 11:29:05 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Braverman", "Mark", ""], ["Efremenko", "Klim", ""], ["Gelles", "Ran", ""], ["Yitayew", "Michael A.", ""]]}, {"id": "1807.05112", "submitter": "Jens Quedenfeld", "authors": "Susanne Albers and Jens Quedenfeld", "title": "Optimal Algorithms for Right-Sizing Data Centers - Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity cost is a dominant and rapidly growing expense in data centers.\nUnfortunately, much of the consumed energy is wasted because servers are idle\nfor extended periods of time. We study a capacity management problem that\ndynamically right-sizes a data center, matching the number of active servers\nwith the varying demand for computing capacity. We resort to a data-center\noptimization problem introduced by Lin, Wierman, Andrew and Thereska that, over\na time horizon, minimizes a combined objective function consisting of operating\ncost, modeled by a sequence of convex functions, and server switching cost. All\nprior work addresses a continuous setting in which the number of active\nservers, at any time, may take a fractional value.\n  In this paper, we investigate for the first time the discrete data-center\noptimization problem where the number of active servers, at any time, must be\ninteger valued. Thereby we seek truly feasible solutions. First, we show that\nthe offline problem can be solved in polynomial time. Our algorithm relies on a\nnew, yet intuitive graph theoretic model of the optimization problem and\nperforms binary search in a layered graph. Second, we study the online problem\nand extend the algorithm Lazy Capacity Provisioning (LCP) by Lin et al. to the\ndiscrete setting. We prove that LCP is 3-competitive. Moreover, we show that no\ndeterministic online algorithm can achieve a competitive ratio smaller than 3.\nWe develop a randomized online algorithm that is 2-competitive against an\noblivious adversary and prove that 2 is a lower bound for the competitive ratio\nof randomized online algorithms.\n  Finally, we address the continuous setting and give a lower bound of 2 on the\nbest competitiveness of online algorithms. All lower bounds mentioned above\nalso holds in a problem variant with more restricted operating cost functions,\nintroduced by Lin et al.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 14:48:57 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Albers", "Susanne", ""], ["Quedenfeld", "Jens", ""]]}, {"id": "1807.05125", "submitter": "Amir Daneshgar", "authors": "Morteza Alimi, Amir Daneshgar, Mohammad-Hadi Foroughmand-Araabi", "title": "Mean Isoperimetry with Control on Outliers: Exact and Approximation\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted graph $G=(V,E)$ with weight functions $c:E\\to \\mathbb{R}_+$\nand $\\pi:V\\to \\mathbb{R}_+$, and a subset $U\\subseteq V$, the normalized cut\nvalue for $U$ is defined as the sum of the weights of edges exiting $U$ divided\nby the weight of vertices in $U$.\n  The {\\it mean isoperimetry problem}, $\\mathsf{ISO}^1(G,k)$, for a weighted\ngraph $G$ is a generalization of the classical uniform sparsest cut problem in\nwhich, given a parameter $k$, the objective is to find $k$ disjoint nonempty\nsubsets of $V$ minimizing the average normalized cut value of the parts. The\nrobust version of the problem seeks an optimizer where the number of vertices\nthat fall out of the subpartition is bounded by some given integer $0 \\leq \\rho\n\\leq |V|$.\n  Our main result states that $\\mathsf{ISO}^1(G,k)$, as well as its robust\nversion, $\\mathsf{CRISO}^1(G,k,\\rho)$, subjected to the condition that each\npart of the subpartition induces a connected subgraph, are solvable in time\n$O(k^2 \\rho^2\\ \\pi(V(T)^3)$ on any weighted tree $T$, in which $\\pi(V(T))$ is\nthe sum of the vertex-weights.\n  This result implies that $\\mathsf{ISO}^1(G,k)$ is strongly polynomial-time\nsolvable on weighted trees when the vertex-weights are polynomially bounded and\nmay be compared to the fact that the problem is NP-Hard for weighted trees in\ngeneral.\n  Also, using this, we show that both mentioned problems, $\\mathsf{ISO}^1(G,k)$\nand $\\mathsf{CRISO}^1(G,k,\\rho)$ as well as the ordinary robust mean\nisoperimetry problem $\\mathsf{RISO}^1(G,k,\\rho)$, admit polynomial-time\n$O(\\log^{1.5}|V| \\log\\log |V|)$-approximation algorithms for weighted graphs\nwith polynomially bounded weights, using the R{\\\"a}cke-Shah tree cut\nsparsifier.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:13:23 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 08:01:37 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Alimi", "Morteza", ""], ["Daneshgar", "Amir", ""], ["Foroughmand-Araabi", "Mohammad-Hadi", ""]]}, {"id": "1807.05135", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, Huacheng Yu", "title": "Optimal Lower Bounds for Distributed and Streaming Spanning Forest\n  Computation", "comments": "v3: corrected another error in the proof of Lemma 3 and slightly\n  changed statement as well as Lemma 5 to fit new statement; again final\n  results are unchanged; v2: the proof of Lemma 3 in version 1 was incorrect,\n  and we have replaced it with a slightly different statement, as well as\n  modifying Lemma 5 to fit in with our new Lemma 3 -- our final results are\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show optimal lower bounds for spanning forest computation in two different\nmodels:\n  * One wants a data structure for fully dynamic spanning forest in which\nupdates can insert or delete edges amongst a base set of $n$ vertices. The sole\nallowed query asks for a spanning forest, which the data structure should\nsuccessfully answer with some given (potentially small) constant probability\n$\\epsilon>0$. We prove that any such data structure must use $\\Omega(n\\log^3\nn)$ bits of memory.\n  * There is a referee and $n$ vertices in a network sharing public randomness,\nand each vertex knows only its neighborhood; the referee receives no input. The\nvertices each send a message to the referee who then computes a spanning forest\nof the graph with constant probability $\\epsilon>0$. We prove the average\nmessage length must be $\\Omega(\\log^3 n)$ bits.\n  Both our lower bounds are optimal, with matching upper bounds provided by the\nAGM sketch [AGM12] (which even succeeds with probability $1 -\n1/\\mathrm{poly}(n)$). Furthermore, for the first setting we show optimal lower\nbounds even for low failure probability $\\delta$, as long as $\\delta >\n2^{-n^{1-\\epsilon}}$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:26:10 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 19:34:46 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 19:22:34 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nelson", "Jelani", ""], ["Yu", "Huacheng", ""]]}, {"id": "1807.05164", "submitter": "Rohit Gurjar", "authors": "Rohit Gurjar and Nisheeth K. Vishnoi", "title": "On the Number of Circuits in Regular Matroids (with Connections to\n  Lattices and Codes)", "comments": "to appear in SODA (Symposium on Discrete Algorithms) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for any regular matroid on $m$ elements and any $\\alpha \\geq 1$,\nthe number of $\\alpha$-minimum circuits, or circuits whose size is at most an\n$\\alpha$-multiple of the minimum size of a circuit in the matroid is bounded by\n$m^{O(\\alpha^2)}$. This generalizes a result of Karger for the number of\n$\\alpha$-minimum cuts in a graph. As a consequence, we obtain similar bounds on\nthe number of $\\alpha$-shortest vectors in \"totally unimodular\" lattices and on\nthe number of $\\alpha$-minimum weight codewords in \"regular\" codes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 16:30:10 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 13:03:26 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Gurjar", "Rohit", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1807.05194", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek and Venkatesan Guruswami", "title": "An Algorithmic Blend of LPs and Ring Equations for Promise CSPs", "comments": "41 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promise CSPs are a relaxation of constraint satisfaction problems where the\ngoal is to find an assignment satisfying a relaxed version of the constraints.\nSeveral well-known problems can be cast as promise CSPs including approximate\ngraph coloring, discrepancy minimization, and interesting variants of\nsatisfiability. Similar to CSPs, the tractability of promise CSPs can be tied\nto the structure of operations on the solution space called polymorphisms,\nthough in the promise world these operations are much less constrained. Under\nthe thesis that non-trivial polymorphisms govern tractability, promise CSPs\ntherefore provide a fertile ground for the discovery of novel algorithms.\n  In previous work, we classified Boolean promise CSPs when the constraint\npredicates are symmetric. In this work, we vastly generalize these algorithmic\nresults. Specifically, we show that promise CSPs that admit a family of\n\"regional-periodic\" polymorphisms are in P, assuming that determining which\nregion a point is in can be computed in polynomial time. Such polymorphisms are\nquite general and are obtained by gluing together several functions that are\nperiodic in the Hamming weights in different blocks of the input.\n  Our algorithm is based on a novel combination of linear programming and\nsolving linear systems over rings. We also abstract a framework based on\nreducing a promise CSP to a CSP over an infinite domain, solving it there, and\nthen rounding the solution to an assignment for the promise CSP instance. The\nrounding step is intimately tied to the family of polymorphisms and clarifies\nthe connection between polymorphisms and algorithms in this context. As a key\ningredient, we introduce the technique of finding a solution to a linear\nprogram with integer coefficients that lies in a different ring (such as\n$\\mathbb Z[\\sqrt{2}]$) to bypass ad-hoc adjustments for lying on a rounding\nboundary.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 17:29:50 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Guruswami", "Venkatesan", ""]]}, {"id": "1807.05209", "submitter": "Jevg\\=enijs Vihrovs", "authors": "Andris Ambainis, Kaspars Balodis, J\\=anis Iraids, Martins Kokainis,\n  Kri\\v{s}j\\=anis Pr\\=usis and Jevg\\=enijs Vihrovs", "title": "Quantum Speedups for Exponential-Time Dynamic Programming Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study quantum algorithms for NP-complete problems whose best\nclassical algorithm is an exponential time application of dynamic programming.\nWe introduce the path in the hypercube problem that models many of these\ndynamic programming algorithms. In this problem we are asked whether there is a\npath from $0^n$ to $1^n$ in a given subgraph of the Boolean hypercube, where\nthe edges are all directed from smaller to larger Hamming weight. We give a\nquantum algorithm that solves path in the hypercube in time $O^*(1.817^n)$. The\ntechnique combines Grover's search with computing a partial dynamic programming\ntable. We use this approach to solve a variety of vertex ordering problems on\ngraphs in the same time $O^*(1.817^n)$, and graph bandwidth in time\n$O^*(2.946^n)$. Then we use similar ideas to solve the travelling salesman\nproblem and minimum set cover in time $O^*(1.728^n)$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 17:47:00 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Ambainis", "Andris", ""], ["Balodis", "Kaspars", ""], ["Iraids", "J\u0101nis", ""], ["Kokainis", "Martins", ""], ["Pr\u016bsis", "Kri\u0161j\u0101nis", ""], ["Vihrovs", "Jevg\u0113nijs", ""]]}, {"id": "1807.05241", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese and Eduardo Laber", "title": "Approximation Algorithms for Clustering via Weighted Impurity Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An impurity measures $I:{R}^k \\to {R}^+$ maps a $k$-dimensional vector ${\\bf\nv}$ to a non-negative value $I({\\bf v})$ so that the more homogeneous ${\\bf\nv}$, the larger its impurity. We study clustering based on impurity measures:\ngiven a collection $V$ of $n$ many $k$-dimensional vectors and an impurity\nmeasure $I$, the goal is to find a partition ${\\cal P}$ of $V$ into $L$ groups\n$V_1,\\ldots,V_L$ that minimizes the total impurities of the groups in ${\\cal\nP}$, i.e., $I({\\cal P})= \\sum_{m=1}^{L} I(\\sum_{{\\bf v} \\in V_m}{\\bf v}).$\n  Impurity minimization is widely used as quality assessment measure in\nprobability distribution clustering and in categorical clustering where it is\nnot possible to rely on geometric properties of the data set. However, in\ncontrast to the case of metric based clustering, the current knowledge of\nimpurity measure based clustering in terms of approximation and\ninapproximability results is very limited.\n  Our research contributes to fill this gap. We first present a simple linear\ntime algorithm that simultaneously achieves $3$-approximation for the Gini\nimpurity measure and $O(\\log(\\sum_{{\\bf v} \\in V} \\| {\\bf v}\n\\|_1))$-approximation for the Entropy impurity measure. Then, for the Entropy\nimpurity measure---where we also show that finding the optimal clustering is\nstrongly NP-hard---we are able to design a polynomial time\n$O(\\log^2(\\min\\{k,L\\}))$-approximation algorithm. Our algorithm relies on a\nnontrivial characterization of a class of clusterings that necessarily includes\na partition achieving $O(\\log^2(\\min\\{k,L\\}))$--approximation of the impurity\nof the optimal partition. Remarkably, this is the first polynomial time\nalgorithm with approximation guarantee independent of the number of\npoints/vector and not relying on any restriction on the components of the\nvectors for producing clusterings with minimum entropy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 18:14:11 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Laber", "Eduardo", ""]]}, {"id": "1807.05307", "submitter": "Manish Raghavan", "authors": "Jon Kleinberg and Manish Raghavan", "title": "How Do Classifiers Induce Agents To Invest Effort Strategically?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are often used to produce decision-making rules that classify or\nevaluate individuals. When these individuals have incentives to be classified a\ncertain way, they may behave strategically to influence their outcomes. We\ndevelop a model for how strategic agents can invest effort in order to change\nthe outcomes they receive, and we give a tight characterization of when such\nagents can be incentivized to invest specified forms of effort into improving\ntheir outcomes as opposed to \"gaming\" the classifier. We show that whenever any\n\"reasonable\" mechanism can do so, a simple linear mechanism suffices.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 23:46:52 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:20:56 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 23:36:26 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 02:55:39 GMT"}, {"version": "v5", "created": "Thu, 1 Aug 2019 00:45:55 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Kleinberg", "Jon", ""], ["Raghavan", "Manish", ""]]}, {"id": "1807.05311", "submitter": "Zhongxiang Wang", "authors": "Ali Shafahi, Zhongxiang Wang, Ali Haghani", "title": "A matching-based heuristic algorithm for school bus routing problems", "comments": null, "journal-ref": "TRB2060", "doi": "10.1016/j.trb.2018.09.004", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  School bus planning problem (SBPP) has drawn much research attention due to\nthe huge costs of school transportation. In the literature, the SBPP is usually\ndecomposed into the routing and scheduling subproblems due to its complexity.\nBecause of the nature of the decomposition, even if all the subproblems are\nsolved to optimality, the final solution may not be as good as the solution\nfrom the integrated model. In this paper, we present a new approach that\nincorporates the scheduling information (namely the trip compatibility) into\nthe routing stage such that the interrelationship between the subproblems is\nstill considered even in the decomposed problems. A novel two-step heuristic\nadopting the trip compatibility idea is presented to solve the school bus\nrouting problem. The first step finds an initial solution using an iterative\nminimum cost matching-based insertion heuristic. Then, the initial trips are\nimproved using a Simulated Annealing and Tabu Search hybrid method. Experiments\nwere conducted on randomly generated problems and benchmark problems in the\nliterature. The result shows that our two-step heuristic improves existing\nsolutions up to 25% on the benchmark problems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 00:17:10 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Shafahi", "Ali", ""], ["Wang", "Zhongxiang", ""], ["Haghani", "Ali", ""]]}, {"id": "1807.05322", "submitter": "Yota Otachi", "authors": "R\\'emy Belmonte, Eun Jung Kim, Michael Lampis, Valia Mitsou, Yota\n  Otachi, Florian Sikora", "title": "Token Sliding on Split Graphs", "comments": "17 pages, 1 figure. STACS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of the Independent Set Reconfiguration problem\nunder the Token Sliding rule. In this problem we are given two independent sets\nof a graph and are asked if we can transform one to the other by repeatedly\nexchanging a vertex that is currently in the set with one of its neighbors,\nwhile maintaining the set independent. Our main result is to show that this\nproblem is PSPACE-complete on split graphs (and hence also on chordal graphs),\nthus resolving an open problem in this area. We then go on to consider the\n$c$-Colorable Reconfiguration problem under the same rule, where the constraint\nis now to maintain the set $c$-colorable at all times. As one may expect, a\nsimple modification of our reduction shows that this more general problem is\nPSPACE-complete for all fixed $c\\ge 1$ on chordal graphs. Somewhat\nsurprisingly, we show that the same cannot be said for split graphs: we give a\npolynomial time ($n^{O(c)}$) algorithm for all fixed values of $c$, except\n$c=1$, for which the problem is PSPACE-complete. We complement our algorithm\nwith a lower bound showing that $c$-Colorable Reconfiguration is W[2]-hard on\nsplit graphs parameterized by $c$ and the length of the solution, as well as a\ntight ETH-based lower bound for both parameters.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 01:49:50 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2019 10:55:17 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Kim", "Eun Jung", ""], ["Lampis", "Michael", ""], ["Mitsou", "Valia", ""], ["Otachi", "Yota", ""], ["Sikora", "Florian", ""]]}, {"id": "1807.05356", "submitter": "Xiaodong Wang", "authors": "Lei Wang and Xiaodong Wang", "title": "A Simple and Space Efficient Segment Tree Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segment tree is an extremely versatile data structure. In this paper, a\nnew heap based implementation of segment trees is proposed. In such an\nimplementation of segment tree, the structural information associated with the\ntree nodes can be removed completely. Some primary computational geometry\nproblems such as stabbing counting queries, measure of union of intervals, and\nmaximum clique size of Intervals are used to demonstrate the efficiency of the\nnew heap based segment tree implementation. Each interval in a set $S=\\{I_1\n,I_2 ,\\cdots,I_n\\}$ of $n$ intervals can be insert into or delete from the heap\nbased segment tree in $O(\\log n)$ time. All the primary computational geometry\nproblems can be solved efficiently.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 08:33:39 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wang", "Lei", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1807.05374", "submitter": "Manuela Fischer", "authors": "Sebastian Brandt, Manuela Fischer, Jara Uitto", "title": "Matching and MIS for Uniformly Sparse Graphs in the Low-Memory MPC Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massively Parallel Computation (MPC) model serves as a common abstraction\nof many modern large-scale parallel computation frameworks and has recently\ngained a lot of importance, especially in the context of classic graph\nproblems. Unsatisfactorily, all current $\\text{poly} (\\log \\log n)$-round MPC\nalgorithms seem to get fundamentally stuck at the linear-memory barrier: their\nefficiency crucially relies on each machine having space at least linear in the\nnumber $n$ of nodes. As this might not only be prohibitively large, but also\nallows for easy if not trivial solutions for sparse graphs, we are interested\nin the low-memory MPC model, where the space per machine is restricted to be\nstrongly sublinear, that is, $n^{\\delta}$ for any $0<\\delta<1$.\n  We devise a degree reduction technique that reduces maximal matching and\nmaximal independent set in graphs with arboricity $\\lambda$ to the\ncorresponding problems in graphs with maximum degree $\\text{poly}(\\lambda)$ in\n$O(\\log^2 \\log n)$ rounds. This gives rise to $O\\left(\\log^2\\log n +\nT(\\text{poly} \\lambda)\\right)$-round algorithms, where $T(\\Delta)$ is the\n$\\Delta$-dependency in the round complexity of maximal matching and maximal\nindependent set in graphs with maximum degree $\\Delta$. A concurrent work by\nGhaffari and Uitto shows that $T(\\Delta)=O(\\sqrt{\\log \\Delta})$.\n  For graphs with arboricity $\\lambda=\\text{poly}(\\log n)$, this almost\nexponentially improves over Luby's $O(\\log n)$-round PRAM algorithm [STOC'85,\nJALG'86], and constitutes the first $\\text{poly} (\\log \\log n)$-round maximal\nmatching algorithm in the low-memory MPC model, thus breaking the linear-memory\nbarrier. Previously, the only known subpolylogarithmic algorithm, due to\nLattanzi et al. [SPAA'11], required strongly superlinear, that is,\n$n^{1+\\Omega(1)}$, memory per machine.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 10:33:21 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 10:30:01 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Brandt", "Sebastian", ""], ["Fischer", "Manuela", ""], ["Uitto", "Jara", ""]]}, {"id": "1807.05377", "submitter": "Jos\\'e A. R. Fonollosa", "authors": "Jos\\'e A. R. Fonollosa", "title": "SAT encodings for sorting networks, single-exception sorting networks\n  and $\\epsilon-$halvers", "comments": "Software available at https://github.com/jarfo/sort", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting networks are oblivious sorting algorithms with many practical\napplications and rich theoretical properties. Propositional encodings of\nsorting networks are a key tool for proving concrete bounds on the minimum\nnumber of comparators or depth (number of parallel steps) of sorting networks.\nIn this paper, we present new SAT encodings that reduce the number of variables\nand clauses of the sorting constraint of optimality problems. Moreover, the\nproposed SAT encodings can be applied to a broader class of problems, such as\nthe search of optimal single-exception sorting networks and $\\epsilon-$halvers.\nWe obtain optimality results for single-exception sorting networks on $n \\le\n10$ inputs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 11:05:25 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1807.05443", "submitter": "Kamyar Khodamoradi", "authors": "Zachary Friggstad, Kamyar Khodamoradi, Mohammad R. Salavatipour", "title": "Exact Algorithms and Lower Bounds for Stable Instances of Euclidean\n  k-Means", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of solving stable or perturbation-resilient\ninstances of k-Means and k-Median clustering in fixed dimension Euclidean\nmetrics (or more generally doubling metrics). The notion of stable or\nperturbation resilient instances was introduced by Bilu and Linial [2010] and\nAwasthi et al. [2012]. In our context we say a k-Means instance is\n\\alpha-stable if there is a unique OPT solution which remains unchanged if\ndistances are (non-uniformly) stretched by a factor of at most \\alpha. Stable\nclustering instances have been studied to explain why heuristics such as\nLloyd's algorithm perform well in practice. In this work we show that for any\nfixed \\epsilon>0, (1+\\epsilon)-stable instances of k-Means in doubling metrics\ncan be solved in polynomial time. More precisely we show a natural multiswap\nlocal search algorithm in fact finds the OPT solution for (1+\\epsilon)-stable\ninstances of k-Means and k-Median in a polynomial number of iterations. We\ncomplement this result by showing that under a plausible PCP hypothesis this is\nessentially tight: that when the dimension d is part of the input, there is a\nfixed \\epsilon_0>0 s.t. there is not even a PTAS for (1+\\epsilon_0)-stable\nk-Means in R^d unless NP=RP. To do this, we consider a robust property of CSPs;\ncall an instance stable if there is a unique optimum solution x^* and for any\nother solution x', the number of unsatisfied clauses is proportional to the\nHamming distance between x^* and x'. Dinur et al. have already shown stable\nQSAT is hard to approximate for some constant Q, our hypothesis is simply that\nstable QSAT with bounded variable occurrence is also hard. Given this\nhypothesis, we consider \"stability-preserving\" reductions to prove our hardness\nfor stable k-Means. Such reductions seem to be more fragile than standard\nL-reductions and may be of further use to demonstrate other stable optimization\nproblems are hard.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 20:56:25 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Friggstad", "Zachary", ""], ["Khodamoradi", "Kamyar", ""], ["Salavatipour", "Mohammad R.", ""]]}, {"id": "1807.05477", "submitter": "Rad Niazadeh", "authors": "Nima Anari, Rad Niazadeh, Amin Saberi, Ali Shameli", "title": "Nearly Optimal Pricing Algorithms for Production Constrained and Laminar\n  Bayesian Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online pricing algorithms for the Bayesian selection problem with\nproduction constraints and its generalization to the laminar matroid Bayesian\nonline selection problem. Consider a firm producing (or receiving) multiple\ncopies of different product types over time. The firm can offer the products to\narriving buyers, where each buyer is interested in one product type and has a\nprivate valuation drawn independently from a possibly different but known\ndistribution.\n  Our goal is to find an adaptive pricing for serving the buyers that maximizes\nthe expected social-welfare (or revenue) subject to two constraints. First, at\nany time the total number of sold items of each type is no more than the number\nof produced items. Second, the total number of sold items does not exceed the\ntotal shipping capacity. This problem is a special case of the well-known\nmatroid Bayesian online selection problem studied in [Kleinberg and Weinberg,\n2012], when the underlying matroid is laminar.\n  We give the first Polynomial-Time Approximation Scheme (PTAS) for the above\nproblem as well as its generalization to the laminar matroid Bayesian online\nselection problem when the depth of the laminar family is bounded by a\nconstant. Our approach is based on rounding the solution of a hierarchy of\nlinear programming relaxations that systematically strengthen the commonly used\nex-ante linear programming formulation of these problems and approximate the\noptimum online solution with any degree of accuracy. Our rounding algorithm\nrespects the relaxed constraints of higher-levels of the laminar tree only in\nexpectation, and exploits the negative dependency of the selection rule of\nlower-levels to achieve the required concentration that guarantees the\nfeasibility with high probability.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 02:19:33 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Anari", "Nima", ""], ["Niazadeh", "Rad", ""], ["Saberi", "Amin", ""], ["Shameli", "Ali", ""]]}, {"id": "1807.05529", "submitter": "Mohit Garg", "authors": "Niv Buchbinder, Moran Feldman, Yuval Filmus, Mohit Garg", "title": "Online Submodular Maximization: Beating 1/2 Made Simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Submodular Welfare Maximization problem (SWM) captures an important\nsubclass of combinatorial auctions and has been studied extensively from both\ncomputational and economic perspectives. In particular, it has been studied in\na natural online setting in which items arrive one-by-one and should be\nallocated irrevocably upon arrival. In this setting, it is well known that the\ngreedy algorithm achieves a competitive ratio of 1/2, and recently Kapralov et\nal. (2013) showed that this ratio is optimal for the problem. Surprisingly,\ndespite this impossibility result, Korula et al. (2015) were able to show that\nthe same algorithm is 0.5052-competitive when the items arrive in a uniformly\nrandom order, but unfortunately, their proof is very long and involved. In this\nwork, we present an (arguably) much simpler analysis that provides a slightly\nbetter guarantee of 0.5096-competitiveness for the greedy algorithm in the\nrandom-arrival model. Moreover, this analysis applies also to a generalization\nof online SWM in which the sets defining a (simple) partition matroid arrive\nonline in a uniformly random order, and we would like to maximize a monotone\nsubmodular function subject to this matroid. Furthermore, for this more general\nproblem, we prove an upper bound of 0.576 on the competitive ratio of the\ngreedy algorithm, ruling out the possibility that the competitiveness of this\nnatural algorithm matches the optimal offline approximation ratio of 1-1/e.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 11:31:40 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 13:14:45 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""], ["Filmus", "Yuval", ""], ["Garg", "Mohit", ""]]}, {"id": "1807.05532", "submitter": "Mohit Garg", "authors": "Niv Buchbinder, Moran Feldman, Mohit Garg", "title": "Deterministic (1/2 + {\\epsilon})-Approximation for Submodular\n  Maximization over a Matroid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone submodular function subject to\na matroid constraint and present a deterministic algorithm that achieves (1/2 +\n{\\epsilon})-approximation for the problem. This algorithm is the first\ndeterministic algorithm known to improve over the 1/2-approximation ratio of\nthe classical greedy algorithm proved by Nemhauser, Wolsely and Fisher in 1978.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 11:57:00 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""], ["Garg", "Mohit", ""]]}, {"id": "1807.05554", "submitter": "Leah Epstein", "authors": "J\\'anos Balogh, J\\'ozsef B\\'ek\\'esi, Gy\\\"orgy D\\'osa, Leah Epstein,\n  Asaf Levin", "title": "A new lower bound for classic online bin packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the lower bound on the asymptotic competitive ratio of any online\nalgorithm for bin packing to above 1.54278. We demonstrate for the first time\nthe advantage of branching and the applicability of full adaptivity in the\ndesign of lower bounds for the classic online bin packing problem. We apply a\nnew method for weight based analysis, which is usually applied only in proofs\nof upper bounds. The values of previous lower bounds were approximately 1.5401\nand 1.5403.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 14:16:22 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Balogh", "J\u00e1nos", ""], ["B\u00e9k\u00e9si", "J\u00f3zsef", ""], ["D\u00f3sa", "Gy\u00f6rgy", ""], ["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "1807.05665", "submitter": "Charles Carlson", "authors": "Ali Bibak (1), Charles Carlson (2), Karthekeyan Chandrasekaran (1)\n  ((1) University of Illinois Urbana-Champaign, (2) University of Colorado\n  Boulder)", "title": "Improving the smoothed complexity of FLIP for max cut problems", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding locally optimal solutions for max-cut and max-$k$-cut are well-known\nPLS-complete problems. An instinctive approach to finding such a locally\noptimum solution is the FLIP method. Even though FLIP requires exponential time\nin worst-case instances, it tends to terminate quickly in practical instances.\nTo explain this discrepancy, the run-time of FLIP has been studied in the\nsmoothed complexity framework. Etscheid and R\\\"{o}glin showed that the smoothed\ncomplexity of FLIP for max-cut in arbitrary graphs is quasi-polynomial. Angel,\nBubeck, Peres, and Wei showed that the smoothed complexity of FLIP for max-cut\nin complete graphs is $O(\\phi^5n^{15.1})$, where $\\phi$ is an upper bound on\nthe random edge-weight density and $n$ is the number of vertices in the input\ngraph.\n  While Angel et al.'s result showed the first polynomial smoothed complexity,\nthey also conjectured that their run-time bound is far from optimal. In this\nwork, we make substantial progress towards improving the run-time bound. We\nprove that the smoothed complexity of FLIP in complete graphs is $O(\\phi\nn^{7.83})$. Our results are based on a carefully chosen matrix whose rank\ncaptures the run-time of the method along with improved rank bounds for this\nmatrix and an improved union bound based on this matrix. In addition, our\ntechniques provide a general framework for analyzing FLIP in the smoothed\nframework. We illustrate this general framework by showing that the smoothed\ncomplexity of FLIP for max-$3$-cut in complete graphs is polynomial and for\nmax-$k$-cut in arbitrary graphs is quasi-polynomial. We believe that our\ntechniques should also be of interest towards addressing the smoothed\ncomplexity of FLIP for max-$k$-cut in complete graphs for larger constants $k$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 03:05:41 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Bibak", "Ali", ""], ["Carlson", "Charles", ""], ["Chandrasekaran", "Karthekeyan", ""]]}, {"id": "1807.05803", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Amir Abboud, Loukas Georgiadis, Giuseppe F. Italiano, Robert\n  Krauthgamer, Nikos Parotsidis, Ohad Trabelsi, Przemys{\\l}aw Uzna\\'nski,\n  Daniel Wolleb-Graf", "title": "Faster Algorithms for All-Pairs Bounded Min-Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The All-Pairs Min-Cut problem (aka All-Pairs Max-Flow) asks to compute a\nminimum $s$-$t$ cut (or just its value) for all pairs of vertices $s,t$. We\nstudy this problem in directed graphs with unit edge/vertex capacities\n(corresponding to edge/vertex connectivity). Our focus is on the $k$-bounded\ncase, where the algorithm has to find all pairs with min-cut value less than\n$k$, and report only those. The most basic case $k=1$ is the Transitive Closure\n(TC) problem, which can be solved in graphs with $n$ vertices and $m$ edges in\ntime $O(mn)$ combinatorially, and in time $O(n^{\\omega})$ where $\\omega<2.38$\nis the matrix-multiplication exponent. These time bounds are conjectured to be\noptimal.\n  We present new algorithms and conditional lower bounds that advance the\nfrontier for larger $k$, as follows: (i) A randomized algorithm for vertex\ncapacities that runs in time $O((nk)^{\\omega})$. (ii) Two deterministic\nalgorithms for edge capacities (which is more general) that work in DAGs and\nfurther reports a minimum cut for each pair. The first algorithm is\ncombinatorial (does not involve matrix multiplication) and runs in time\n$O(2^{O(k^2)}\\cdot mn)$. The second algorithm can be faster on dense DAGs and\nruns in time $O((k\\log n)^{4^k+o(k)} n^{\\omega})$. (iii) The first super-cubic\nlower bound of $n^{\\omega-1-o(1)} k^2$ time under the $4$-Clique conjecture,\nwhich holds even in the simplest case of DAGs with unit vertex capacities. It\nimproves on the previous (SETH-based) lower bounds even in the unbounded\nsetting $k=n$. For combinatorial algorithms, our reduction implies an\n$n^{2-o(1)} k^2$ conditional lower bound. Thus, we identify new settings where\nthe complexity of the problem is (conditionally) higher than that of TC.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 11:45:29 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 19:04:45 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Abboud", "Amir", ""], ["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Krauthgamer", "Robert", ""], ["Parotsidis", "Nikos", ""], ["Trabelsi", "Ohad", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""], ["Wolleb-Graf", "Daniel", ""]]}, {"id": "1807.05957", "submitter": "Shantanav Chakraborty", "authors": "Shantanav Chakraborty, Leonardo Novo, J\\'er\\'emie Roland", "title": "Finding a marked node on any graph by continuous-time quantum walk", "comments": "This version deals only with new algorithms for spatial search by\n  continuous-time quantum walk (CTQW) on ergodic, reversible Markov chains.\n  Please see arXiv:2004.12686 for results on the necessary and sufficient\n  conditions for the optimality of the Childs and Goldstone algorithm for\n  spatial search by CTQW", "journal-ref": "Phys. Rev. A 102, 022227 (2020)", "doi": "10.1103/PhysRevA.102.022227", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial search by discrete-time quantum walk can find a marked node on any\nergodic, reversible Markov chain $P$ quadratically faster than its classical\ncounterpart, i.e.\\ in a time that is in the square root of the hitting time of\n$P$. However, in the framework of continuous-time quantum walks, it was\npreviously unknown whether such general speed-up is possible. In fact, in this\nframework, the widely used quantum algorithm by Childs and Goldstone fails to\nachieve such a speedup. Furthermore, it is not clear how to apply this\nalgorithm for searching any Markov chain $P$. In this article, we aim to\nreconcile the apparent differences between the running times of spatial search\nalgorithms in these two frameworks. We first present a modified version of the\nChilds and Goldstone algorithm which can search for a marked element for any\nergodic, reversible $P$ by performing a quantum walk on its edges. Although\nthis approach improves the algorithmic running time for several instances, it\ncannot provide a generic quadratic speedup for any $P$. Secondly, using the\nframework of interpolated Markov chains, we provide a new spatial search\nalgorithm by continuous-time quantum walk which can find a marked node on any\n$P$ in the square root of the classical hitting time. In the scenario where\nmultiple nodes are marked, the algorithmic running time scales as the square\nroot of a quantity known as the extended hitting time. Our results establish a\nnovel connection between discrete-time and continuous-time quantum walks and\ncan be used to develop a number of Markov chain-based quantum algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:34:44 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 16:45:52 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 06:16:10 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Chakraborty", "Shantanav", ""], ["Novo", "Leonardo", ""], ["Roland", "J\u00e9r\u00e9mie", ""]]}, {"id": "1807.05968", "submitter": "Panagiotis Charalampopoulos", "authors": "Panagiotis Charalampopoulos, Shay Mozes, Benjamin Tebeka", "title": "Exact Distance Oracles for Planar Graphs with Failing Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exact distance oracles for directed weighted planar graphs in the\npresence of failing vertices. Given a source vertex $u$, a target vertex $v$\nand a set $X$ of $k$ failed vertices, such an oracle returns the length of a\nshortest $u$-to-$v$ path that avoids all vertices in $X$. We propose oracles\nthat can handle any number $k$ of failures. More specifically, for a directed\nweighted planar graph with $n$ vertices, any constant $k$, and for any $q \\in\n[1,\\sqrt n]$, we propose an oracle of size\n$\\tilde{\\mathcal{O}}(\\frac{n^{k+3/2}}{q^{2k+1}})$ that answers queries in\n$\\tilde{\\mathcal{O}}(q)$ time. In particular, we show an\n$\\tilde{\\mathcal{O}}(n)$-size, $\\tilde{\\mathcal{O}}(\\sqrt{n})$-query-time\noracle for any constant $k$. This matches, up to polylogarithmic factors, the\nfastest failure-free distance oracles with nearly linear space. For single\nvertex failures ($k=1$), our $\\tilde{\\mathcal{O}}(\\frac{n^{5/2}}{q^3})$-size,\n$\\tilde{\\mathcal{O}}(q)$-query-time oracle improves over the previously best\nknown tradeoff of Baswana et al. [SODA 2012] by polynomial factors for $q =\n\\Omega(n^t)$, $t \\in (1/4,1/2]$. For multiple failures, no planarity exploiting\nresults were previously known.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:51:05 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 12:38:59 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Mozes", "Shay", ""], ["Tebeka", "Benjamin", ""]]}, {"id": "1807.06101", "submitter": "Frank Ban", "authors": "Frank Ban, Vijay Bhattiprolu, Karl Bringmann, Pavel Kolev, Euiwoong\n  Lee, David P. Woodruff", "title": "A PTAS for $\\ell_p$-Low Rank Approximation", "comments": "Accepted at SODA'19, 65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works have studied algorithms for entrywise $\\ell_p$-low\nrank approximation, namely, algorithms which given an $n \\times d$ matrix $A$\n(with $n \\geq d$), output a rank-$k$ matrix $B$ minimizing\n$\\|A-B\\|_p^p=\\sum_{i,j}|A_{i,j}-B_{i,j}|^p$ when $p > 0$; and\n$\\|A-B\\|_0=\\sum_{i,j}[A_{i,j}\\neq B_{i,j}]$ for $p=0$.\n  On the algorithmic side, for $p \\in (0,2)$, we give the first\n$(1+\\epsilon)$-approximation algorithm running in time\n$n^{\\text{poly}(k/\\epsilon)}$. Further, for $p = 0$, we give the first\nalmost-linear time approximation scheme for what we call the Generalized Binary\n$\\ell_0$-Rank-$k$ problem. Our algorithm computes $(1+\\epsilon)$-approximation\nin time $(1/\\epsilon)^{2^{O(k)}/\\epsilon^{2}} \\cdot nd^{1+o(1)}$.\n  On the hardness of approximation side, for $p \\in (1,2)$, assuming the Small\nSet Expansion Hypothesis and the Exponential Time Hypothesis (ETH), we show\nthat there exists $\\delta := \\delta(\\alpha) > 0$ such that the entrywise\n$\\ell_p$-Rank-$k$ problem has no $\\alpha$-approximation algorithm running in\ntime $2^{k^{\\delta}}$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 20:48:13 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 18:12:11 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 00:11:16 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ban", "Frank", ""], ["Bhattiprolu", "Vijay", ""], ["Bringmann", "Karl", ""], ["Kolev", "Pavel", ""], ["Lee", "Euiwoong", ""], ["Woodruff", "David P.", ""]]}, {"id": "1807.06168", "submitter": "Gautam Kamath", "authors": "Gautam Kamath, Christos Tzamos", "title": "Anaconda: A Non-Adaptive Conditional Sampling Algorithm for Distribution\n  Testing", "comments": "SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate distribution testing with access to non-adaptive conditional\nsamples. In the conditional sampling model, the algorithm is given the\nfollowing access to a distribution: it submits a query set $S$ to an oracle,\nwhich returns a sample from the distribution conditioned on being from $S$. In\nthe non-adaptive setting, all query sets must be specified in advance of\nviewing the outcomes.\n  Our main result is the first polylogarithmic-query algorithm for equivalence\ntesting, deciding whether two unknown distributions are equal to or far from\neach other. This is an exponential improvement over the previous best upper\nbound, and demonstrates that the complexity of the problem in this model is\nintermediate to the the complexity of the problem in the standard sampling\nmodel and the adaptive conditional sampling model. We also significantly\nimprove the sample complexity for the easier problems of uniformity and\nidentity testing. For the former, our algorithm requires only $\\tilde O(\\log\nn)$ queries, matching the information-theoretic lower bound up to a $O(\\log\n\\log n)$-factor.\n  Our algorithm works by reducing the problem from $\\ell_1$-testing to\n$\\ell_\\infty$-testing, which enjoys a much cheaper sample complexity.\nNecessitated by the limited power of the non-adaptive model, our algorithm is\nvery simple to state. However, there are significant challenges in the\nanalysis, due to the complex structure of how two arbitrary distributions may\ndiffer.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 01:12:23 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 18:47:27 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}, {"id": "1807.06194", "submitter": "Kevin Pratt", "authors": "Kevin Pratt", "title": "Waring Rank, Parameterized and Exact Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given nonnegative integers $n$ and $d$, where $n \\gg d$, what is the minimum\nnumber $r$ such that there exist linear forms $\\ell_1, \\ldots, \\ell_r \\in\n\\mathbb{C}[x_1, \\ldots, x_n]$ so that $\\ell_1^d + \\cdots + \\ell_r^d$ is\nsupported exactly on the set of all degree-$d$ multilinear monomials in $x_1,\n\\ldots, x_n$? We show that this and related questions have surprising and\nintimate connections to the areas of parameterized and exact algorithms,\ngeneralizing several well-known methods and providing a concrete approach to\nobtain faster approximate counting and deterministic decision algorithms. This\ngives a new application of Waring rank, a classical topic in algebraic geometry\nwith connections to algebraic complexity theory, to computer science.\n  To illustrate the amenability and utility of this approach, we give a\nrandomized $4.075^d \\cdot \\mathrm{poly}(n, \\varepsilon^{-1})$-time algorithm\nfor computing a $(1 + \\varepsilon)$ approximation of the sum of the\ncoefficients of the multilinear monomials in a degree-$d$ homogeneous\n$n$-variate polynomial with nonnegative coefficients. As an application of this\nwe give a faster algorithm for approximately counting subgraphs of bounded\ntreewidth, improving on earlier work of Alon et al. Along the way we give an\nexact answer to an open problem of Koutis and Williams and sharpen a lower\nbound on the size of perfectly balanced hash families given by Alon and Gutner.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 03:17:27 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 13:49:56 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 14:26:37 GMT"}, {"version": "v4", "created": "Fri, 7 Jun 2019 16:00:31 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Pratt", "Kevin", ""]]}, {"id": "1807.06251", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari and Jara Uitto", "title": "Sparsifying Distributed Algorithms with Ramifications in Massively\n  Parallel Computation and Centralized Local Computation", "comments": "This is a shortened version of the abstract. Please see the pdf for\n  the full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for sparsifying distributed algorithms and exhibit how\nit leads to improvements that go past known barriers in two algorithmic\nsettings of large-scale graph processing: Massively Parallel Computation (MPC),\nand Local Computation Algorithms (LCA).\n  - MPC with Strongly Sublinear Memory: Recently, there has been growing\ninterest in obtaining MPC algorithms that are faster than their classic $O(\\log\nn)$-round parallel counterparts for problems such as MIS, Maximal Matching,\n2-Approximation of Minimum Vertex Cover, and $(1+\\epsilon)$-Approximation of\nMaximum Matching. Currently, all such MPC algorithms require\n$\\tilde{\\Omega}(n)$ memory per machine. Czumaj et al. [STOC'18] were the first\nto handle $\\tilde{\\Omega}(n)$ memory, running in $O((\\log\\log n)^2)$ rounds. We\nobtain $\\tilde{O}(\\sqrt{\\log \\Delta})$-round MPC algorithms for all these four\nproblems that work even when each machine has memory $n^{\\alpha}$ for any\nconstant $\\alpha\\in (0, 1)$. Here, $\\Delta$ denotes the maximum degree. These\nare the first sublogarithmic-time algorithms for these problems that break the\nlinear memory barrier.\n  - LCAs with Query Complexity Below the Parnas-Ron Paradigm: Currently, the\nbest known LCA for MIS has query complexity $\\Delta^{O(\\log \\Delta)} poly(\\log\nn)$, by Ghaffari [SODA'16]. As pointed out by Rubinfeld, obtaining a query\ncomplexity of $poly(\\Delta\\log n)$ remains a central open question. Ghaffari's\nbound almost reaches a $\\Delta^{\\Omega\\left(\\frac{\\log \\Delta}{\\log\\log\n\\Delta}\\right)}$ barrier common to all known MIS LCAs, which simulate\ndistributed algorithms by learning the local topology, \\`{a} la Parnas-Ron\n[TCS'07]. This barrier follows from the $\\Omega(\\frac{\\log \\Delta}{\\log\\log\n\\Delta})$ distributed lower bound of Kuhn, et al. [JACM'16]. We break this\nbarrier and obtain an MIS LCA with query complexity $\\Delta^{O(\\log\\log\n\\Delta)} poly(\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 07:01:03 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Uitto", "Jara", ""]]}, {"id": "1807.06272", "submitter": "Gopinath Mishra", "authors": "Arijit Bishnu, Arijit Ghosh, Sudeshna Kolay, Gopinath Mishra and Saket\n  Saurabh", "title": "Parameterized Query Complexity of Hitting Set using Stability of\n  Sunflowers", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the query complexity of parameterized decision and\noptimization versions of {\\sc Hitting-Set}, {\\sc Vertex Cover}, {\\sc Packing} ,\n\\match{} and {\\sc Max-Cut}. The main focus is the query complexity of {\\sc\nHitting Set}. In doing so, we use an oracle known as \\bis{} introduced by Beame\net al.~\\cite{BeameHRRS18} and its generalizations to hypergraphs. The query\nmodels considered are the \\gpis{} and \\gpise{} oracles :\n  (i) the \\gpis{} oracle takes as input $d$ pairwise disjoint non-empty vertex\nsubsets $A_1, \\ldots, A_d$ in a hypergraph $\\cal H$ and answers whether there\nis a hyperedge with vertices in each $A_i$,\n  (ii) the \\gpise{} oracle takes the same input and returns a hyperedge that\nhas vertices in each $A_i$; NULL, otherwise.\n  The \\gpis{} and \\gpise{} oracles are used for the decision and optimization\nversions of the problems, respectively. For $d=2$, we refer \\gpis{} and\n\\gpise{} as \\bis{} and \\bise{}, respectively. We use color coding and queries\nto the oracles to generate subsamples from the hypergraph, that retain some\nstructural properties of the original hypergraph. We use the stability of the\nsunflowers in a non-trivial way to do so.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:18:49 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Bishnu", "Arijit", ""], ["Ghosh", "Arijit", ""], ["Kolay", "Sudeshna", ""], ["Mishra", "Gopinath", ""], ["Saurabh", "Saket", ""]]}, {"id": "1807.06359", "submitter": "Micha{\\l} Ga\\'nczorz", "authors": "Micha{\\l} Ga\\'nczorz", "title": "Using statistical encoding to achieve tree succinctness never seen\n  before", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new succinct representation of labeled trees which represents a\ntree T using |T|H_k(T) number of bits (plus some smaller order terms), where\n|T|H_k(T) denotes the k-th order (tree label) entropy, as defined by Ferragina\nat al. 2005. Our representation employs a new, simple method of partitioning\nthe tree, which preserves both tree shape and node degrees. Previously, the\nonly representation that used |T|H_k(T) bits was based on XBWT, a\ntransformation that linearizes tree labels into a single string, combined with\ncompression boosting. The proposed representation is much simpler than the one\nbased on XBWT, which used additional linear space (bounded by 0.01n) hidden in\nthe \"smaller order terms\" notion, as an artifact of using zeroth order entropy\ncoder; our representation uses sublinear additional space (for reasonable\nvalues of k and size of the label alphabet {\\sigma}). The proposed\nrepresentation can be naturally extended to a succinct data structure for\ntrees, which uses |T|H_k(T) plus additional O(|T|k log_{\\sigma}/ log_{\\sigma}\n|T| + |T| log log_{\\sigma} |T|/ log_{\\sigma} |T|) bits and supports all the\nusual navigational queries in constant time. At the cost of increasing the\nquery time to O(log log |T|/ log |T|) we can further reduce the space\nredundancy to O(|T| log log |T|/ log_{\\sigma} |T|) bits, assuming k <=\nlog_{\\sigma} |T|. This is a major improvement over representation based on\nXBWT: even though XBWT-based representation uses |T|H_k(T) bits, the space\nneeded for structure supporting navigational queries is much larger: (...)\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 11:44:45 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ga\u0144czorz", "Micha\u0142", ""]]}, {"id": "1807.06456", "submitter": "Yassine Hamoudi", "authors": "Yassine Hamoudi and Fr\\'ed\\'eric Magniez", "title": "Quantum Chebyshev's Inequality and Applications", "comments": "27 pages; v3: better presentation, lower bound in Theorem 4.3 is new", "journal-ref": "Proceedings of the 46th International Colloquium on Automata,\n  Languages, and Programming (ICALP), volume 132 of LIPIcs, pages 69:1-99:16,\n  2019", "doi": "10.4230/LIPIcs.ICALP.2019.69", "report-no": null, "categories": "quant-ph cs.CC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide new quantum algorithms with polynomial speed-up for\na range of problems for which no such results were known, or we improve\nprevious algorithms. First, we consider the approximation of the frequency\nmoments $F_k$ of order $k \\geq 3$ in the multi-pass streaming model with\nupdates (turnstile model). We design a $P$-pass quantum streaming algorithm\nwith memory $M$ satisfying a tradeoff of $P^2 M = \\tilde{O}(n^{1-2/k})$,\nwhereas the best classical algorithm requires $P M = \\Theta(n^{1-2/k})$. Then,\nwe study the problem of estimating the number $m$ of edges and the number $t$\nof triangles given query access to an $n$-vertex graph. We describe optimal\nquantum algorithms that perform $\\tilde{O}(\\sqrt{n}/m^{1/4})$ and\n$\\tilde{O}(\\sqrt{n}/t^{1/6} + m^{3/4}/\\sqrt{t})$ queries respectively. This is\na quadratic speed-up compared to the classical complexity of these problems.\n  For this purpose we develop a new quantum paradigm that we call Quantum\nChebyshev's inequality. Namely we demonstrate that, in a certain model of\nquantum sampling, one can approximate with relative error the mean of any\nrandom variable with a number of quantum samples that is linear in the ratio of\nthe square root of the variance to the mean. Classically the dependency is\nquadratic. Our algorithm subsumes a previous result of Montanaro [Mon15]. This\nnew paradigm is based on a refinement of the Amplitude Estimation algorithm of\nBrassard et al. [BHMT02] and of previous quantum algorithms for the mean\nestimation problem. We show that this speed-up is optimal, and we identify\nanother common model of quantum sampling where it cannot be obtained. For our\napplications, we also adapt the variable-time amplitude amplification technique\nof Ambainis [Amb10] into a variable-time amplitude estimation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:09:38 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 09:59:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 18:09:46 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Hamoudi", "Yassine", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1807.06479", "submitter": "Chi-Ning Chou", "authors": "Chi-Ning Chou, Zhixian Lei, Preetum Nakkiran", "title": "Tracking the $\\ell_2$ Norm with Constant Update Time", "comments": "To appear in APPROX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{$\\ell_2$ tracking problem} is the task of obtaining a streaming\nalgorithm that, given access to a stream of items $a_1,a_2,a_3,\\ldots$ from a\nuniverse $[n]$, outputs at each time $t$ an estimate to the $\\ell_2$ norm of\nthe \\textit{frequency vector} $f^{(t)}\\in \\mathbb{R}^n$ (where $f^{(t)}_i$ is\nthe number of occurrences of item $i$ in the stream up to time $t$). The\nprevious work [Braverman-Chestnut-Ivkin-Nelson-Wang-Woodruff, PODS 2017] gave\nan streaming algorithm with (the optimal) space using\n$O(\\epsilon^{-2}\\log(1/\\delta))$ words and $O(\\epsilon^{-2}\\log(1/\\delta))$\nupdate time to obtain an $\\epsilon$-accurate estimate with probability at least\n$1-\\delta$. We give the first algorithm that achieves update time of $O(\\log\n1/\\delta)$ which is independent of the accuracy parameter $\\epsilon$, together\nwith the nearly optimal space using $O(\\epsilon^{-2}\\log(1/\\delta))$ words. Our\nalgorithm is obtained using the \\textsf{CountSketch} of\n[Charilkar-Chen-Farach-Colton, ICALP 2002].\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:48:25 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 03:07:11 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 17:31:15 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Chou", "Chi-Ning", ""], ["Lei", "Zhixian", ""], ["Nakkiran", "Preetum", ""]]}, {"id": "1807.06481", "submitter": "Weiming Feng", "authors": "Weiming Feng, Nisheeth K. Vishnoi, Yitong Yin", "title": "Dynamic Sampling from Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of sampling from a graphical model when\nthe model itself is changing dynamically with time. This problem derives its\ninterest from a variety of inference, learning, and sampling settings in\nmachine learning, computer vision, statistical physics, and theoretical\ncomputer science. While the problem of sampling from a static graphical model\nhas received considerable attention, theoretical works for its dynamic variants\nhave been largely lacking. The main contribution of this paper is an algorithm\nthat can sample dynamically from a broad class of graphical models over\ndiscrete random variables. Our algorithm is parallel and Las Vegas: it knows\nwhen to stop and it outputs samples from the exact distribution. We also\nprovide sufficient conditions under which this algorithm runs in time\nproportional to the size of the update, on general graphical models as well as\nwell-studied specific spin systems. In particular we obtain, for the Ising\nmodel (ferromagnetic or anti-ferromagnetic) and for the hardcore model the\nfirst dynamic sampling algorithms that can handle both edge and vertex updates\n(addition, deletion, change of functions), both efficient within regimes that\nare close to the respective uniqueness regimes, beyond which, even for the\nstatic and approximate sampling, no local algorithms were known or the problem\nitself is intractable. Our dynamic sampling algorithm relies on a local\nresampling algorithm and a new \"equilibrium\" property that is shown to be\nsatisfied by our algorithm at each step, and enables us to prove its\ncorrectness. This equilibrium property is robust enough to guarantee the\ncorrectness of our algorithm, helps us improve bounds on fast convergence on\nspecific models, and should be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:54:06 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 09:18:58 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Feng", "Weiming", ""], ["Vishnoi", "Nisheeth K.", ""], ["Yin", "Yitong", ""]]}, {"id": "1807.06507", "submitter": "Alexey Poyda", "authors": "Alexey Poyda and Mikhail Zhizhin", "title": "Optimization of the n-dimensional sliding window inter-channel\n  correlation algorithm for multi-core architecture", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating the correlation in a sliding window is a common method of\nstatistical evaluation of the interconnect between two sets of data. And\nalthough the calculation of a single correlation coefficient is not\nresource-intensive and algorithmically complex, sequential computation in a\nlarge number of windows on large data sets can take quite a long time. In this\ncase, each value in the data, falling into different windows, will be processed\nmany times, increasing the complexity of the algorithm and the processing time.\nWe took this fact into account and optimized the correlation calculation in the\nsliding window, reducing the number of operations in the overlapping area of\nthe windows. In addition, we developed a parallel version of the optimized\nalgorithm for the GPU architecture. Experimental studies have shown that for a\n7x7 correlation window sliding in one pixel increments, we were able to\naccelerate the processing of an 12 MPixel image pixels on the GPU by about 60\ntimes compared to the serial version running on the CPU. The article presents\nan optimized version of the algorithm, a scheme for its parallelization, as\nwell as the results of experimental studies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 15:42:01 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Poyda", "Alexey", ""], ["Zhizhin", "Mikhail", ""]]}, {"id": "1807.06577", "submitter": "Piyush Srivastava", "authors": "Jingcheng Liu, Alistair Sinclair, Piyush Srivastava", "title": "Fisher zeros and correlation decay in the Ising model", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": "10.1063/1.5082552", "report-no": null, "categories": "math-ph cond-mat.stat-mech cs.DM cs.DS math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complex zeros of the partition function of the Ising model,\nviewed as a polynomial in the \"interaction parameter\"; these are known as\nFisher zeros in light of their introduction by Fisher in 1965. While the zeros\nof the partition function as a polynomial in the \"field\" parameter have been\nextensively studied since the classical work of Lee and Yang, comparatively\nlittle is known about Fisher zeros for general graphs. Our main result shows\nthat the zero-field Ising model has no Fisher zeros in a complex neighborhood\nof the entire region of parameters where the model exhibits correlation decay.\nIn addition to shedding light on Fisher zeros themselves, this result also\nestablishes a formal connection between two distinct notions of phase\ntransition for the Ising model: the absence of complex zeros (analyticity of\nthe free energy, or the logarithm of the partition function) and decay of\ncorrelations with distance. We also discuss the consequences of our result for\nefficient deterministic approximation of the partition function. Our proof\nrelies heavily on algorithmic techniques, notably Weitz's self-avoiding walk\ntree, and as such belongs to a growing body of work that uses algorithmic\nmethods to resolve classical questions in statistical physics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 17:35:52 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 10:41:28 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Liu", "Jingcheng", ""], ["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""]]}, {"id": "1807.06624", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Seth Pettie, Hengjie Zhang", "title": "Distributed Triangle Detection via Expander Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved distributed algorithms for triangle detection and its\nvariants in the CONGEST model. We show that Triangle Detection, Counting, and\nEnumeration can be solved in $\\tilde{O}(n^{1/2})$ rounds. In contrast, the\nprevious state-of-the-art bounds for Triangle Detection and Enumeration were\n$\\tilde{O}(n^{2/3})$ and $\\tilde{O}(n^{3/4})$, respectively, due to Izumi and\nLeGall (PODC 2017).\n  The main technical novelty in this work is a distributed graph partitioning\nalgorithm. We show that in $\\tilde{O}(n^{1-\\delta})$ rounds we can partition\nthe edge set of the network $G=(V,E)$ into three parts $E=E_m\\cup E_s\\cup E_r$\nsuch that\n  (a) Each connected component induced by $E_m$ has minimum degree\n$\\Omega(n^\\delta)$ and conductance $\\Omega(1/\\text{poly} \\log(n))$. As a\nconsequence the mixing time of a random walk within the component is\n$O(\\text{poly} \\log(n))$.\n  (b) The subgraph induced by $E_s$ has arboricity at most $n^{\\delta}$.\n  (c) $|E_r| \\leq |E|/6$.\n  All of our algorithms are based on the following generic framework, which we\nbelieve is of interest beyond this work. Roughly, we deal with the set $E_s$ by\nan algorithm that is efficient for low-arboricity graphs, and deal with the set\n$E_r$ using recursive calls. For each connected component induced by $E_m$, we\nare able to simulate congested clique algorithms with small overhead by\napplying a routing algorithm due to Ghaffari, Kuhn, and Su (PODC 2017) for high\nconductance graphs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 18:59:09 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Pettie", "Seth", ""], ["Zhang", "Hengjie", ""]]}, {"id": "1807.06645", "submitter": "Christian Coester", "authors": "Christian Coester, Elias Koutsoupias", "title": "The Online $k$-Taxi Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online $k$-taxi problem, a generalization of the $k$-server\nproblem, in which $k$ taxis serve a sequence of requests in a metric space. A\nrequest consists of two points $s$ and $t$, representing a passenger that wants\nto be carried by a taxi from $s$ to $t$. The goal is to serve all requests\nwhile minimizing the total distance traveled by all taxis. The problem comes in\ntwo flavors, called the easy and the hard $k$-taxi problem: In the easy\n$k$-taxi problem, the cost is defined as the total distance traveled by the\ntaxis; in the hard $k$-taxi problem, the cost is only the distance of empty\nruns.\n  The hard $k$-taxi problem is substantially more difficult than the easy\nversion with at least an exponential deterministic competitive ratio,\n$\\Omega(2^k)$, admitting a reduction from the layered graph traversal problem.\nIn contrast, the easy $k$-taxi problem has exactly the same competitive ratio\nas the $k$-server problem. We focus mainly on the hard version. For\nhierarchically separated trees (HSTs), we present a memoryless randomized\nalgorithm with competitive ratio $2^k-1$ against adaptive online adversaries\nand provide two matching lower bounds: for arbitrary algorithms against\nadaptive adversaries and for memoryless algorithms against oblivious\nadversaries. Due to well-known HST embedding techniques, the algorithm implies\na randomized $O(2^k\\log n)$-competitive algorithm for arbitrary $n$-point\nmetrics. This is the first competitive algorithm for the hard $k$-taxi problem\nfor general finite metric spaces and general $k$. For the special case of\n$k=2$, we obtain a precise answer of $9$ for the competitive ratio in general\nmetrics. With an algorithm based on growing, shrinking and shifting regions, we\nshow that one can achieve a constant competitive ratio also for the hard\n$3$-taxi problem on the line (abstracting the scheduling of three elevators).\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 20:04:54 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 21:05:38 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Coester", "Christian", ""], ["Koutsoupias", "Elias", ""]]}, {"id": "1807.06672", "submitter": "David Harris", "authors": "David G. Harris", "title": "Derandomizing the Lovasz Local Lemma via log-space statistical tests", "comments": "This is superseded by arXiv:1909.08065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) is a keystone principle in probability\ntheory, guaranteeing the existence of configurations which avoid a collection\n$\\mathcal B$ of \"bad\" events which are mostly independent and have low\nprobability. In its simplest form, it asserts that whenever a bad-event has\nprobability $p$ and affects at most $d$ other bad-events, and $e p (d+1) < 1$,\nthen a configuration avoiding all $\\mathcal B$ exists. A seminal algorithm of\nMoser & Tardos (2010) gives randomized algorithms for most constructions based\non the LLL. However, deterministic algorithms have lagged behind. Notably,\nprior deterministic LLL algorithms have required stringent conditions on\n$\\mathcal B$; for example, they have required that events in $\\mathcal B$ have\nlow decision-tree complexity or depend on a small number of variables. For this\nreason, they can only be applied to small fraction of the numerous LLL\napplications in practice. We describe an alternate deterministic parallel (NC)\nalgorithm for the LLL, based on a general derandomization method of Sivakumar\n(2002) using log-space statistical tests. The only requirement here is that\nbad-events should be computable via a finite automaton with $\\text{poly}(d)$\nstates. This covers most LLL applications to graph theory and combinatorics. No\nauxiliary information about the bad-events, including any conditional\nprobability calculations, are required. Additionally, the proof is a\nstraightforward combination of general derandomization results and high-level\nanalysis of the Moser-Tardos algorithm. We illustrate with applications to\ndefective vertex coloring, domatic partition, and independent transversals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 21:06:26 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 11:47:05 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1807.06686", "submitter": "Matthew Blaschko", "authors": "Maxim Berman and Matthew B. Blaschko", "title": "Supermodular Locality Sensitive Hashes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show deep connections between Locality Sensitive Hashability\nand submodular analysis. We show that the LSHablility of the most commonly\nanalyzed set similarities is in one-to-one correspondance with the\nsupermodularity of these similarities when taken with respect to the symmetric\ndifference of their arguments. We find that the supermodularity of equivalent\nLSHable similarities can be dependent on the set encoding. While monotonicity\nand supermodularity does not imply the metric condition necessary for\nsupermodularity, this condition is guaranteed for the more restricted class of\nsupermodular Hamming similarities that we introduce. We show moreover that LSH\npreserving transformations are also supermodular-preserving, yielding a way to\ngenerate families of similarities both LSHable and supermodular. Finally, we\nshow that even the more restricted family of cardinality-based supermodular\nHamming similarities presents promising aspects for the study of the link\nbetween LSHability and supermodularity. We hope that the several bridges that\nwe introduce between LSHability and supermodularity paves the way to a better\nunderstanding both of supermodular analysis and LSHability, notably in the\ncontext of large-scale supermodular optimization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 21:58:31 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Berman", "Maxim", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1807.06701", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi, Richard\n  M. Karp", "title": "Massively Parallel Symmetry Breaking on Sparse Graphs: MIS and Maximal\n  Matching", "comments": "A merger of this paper and the independent and concurrent paper\n  [arxiv:1807.05374] appeared at PODC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of modern parallel paradigms such as MapReduce, Hadoop, or Spark,\nhas attracted a significant attention to the Massively Parallel Computation\n(MPC) model over the past few years, especially on graph problems. In this\nwork, we consider symmetry breaking problems of maximal independent set (MIS)\nand maximal matching (MM), which are among the most intensively studied\nproblems in distributed/parallel computing, in MPC.\n  These problems are known to admit efficient MPC algorithms if the space per\nmachine is near-linear in $n$, the number of vertices in the graph. This space\nrequirement however, as observed in the literature, is often significantly\nlarger than we can afford; especially when the input graph is sparse. In a\nsharp contrast, in the truly sublinear regime of $n^{1-\\Omega(1)}$ space per\nmachine, all the known algorithms take $\\log^{\\Omega(1)} n$ rounds which is\nconsidered inefficient.\n  Motivated by this shortcoming, we parametrize our algorithms by the\narboricity $\\alpha$ of the input graph, which is a well-received measure of its\nsparsity. We show that both MIS and MM admit $O(\\sqrt{\\log \\alpha}\\cdot\\log\\log\n\\alpha + \\log^2\\log n)$ round algorithms using $O(n^\\epsilon)$ space per\nmachine for any constant $\\epsilon \\in (0, 1)$ and using $\\widetilde{O}(m)$\ntotal space. Therefore, for the wide range of sparse graphs with small\narboricity---such as minor-free graphs, bounded-genus graphs or bounded\ntreewidth graphs---we get an $O(\\log^2 \\log n)$ round algorithm which\nexponentially improves prior algorithms.\n  By known reductions, our results also imply a $(1+\\epsilon)$-approximation of\nmaximum cardinality matching, a $(2+\\epsilon)$-approximation of maximum\nweighted matching, and a 2-approximation of minimum vertex cover with\nessentially the same round complexity and memory requirements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:06:22 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 03:53:29 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 17:46:22 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Karp", "Richard M.", ""]]}, {"id": "1807.06719", "submitter": "Enoch Peserico", "authors": "Enoch Peserico", "title": "Deterministic oblivious distribution (and tight compaction) in linear\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an array of N elements, M positions and M elements are \"marked\". We show\nhow to permute the elements in the array so that all marked elements end in\nmarked positions, in time O(N) (in the standard word-RAM model),\ndeterministically, and obliviously - i.e. with a sequence of memory accesses\nthat depends only on N and not on which elements or positions are marked.\n  As a corollary, we answer affirmatively to an open question about the\nexistence of a deterministic oblivious algorithm with O(N) running time for\ntight compaction (move the M marked elements to the first M positions of the\narray), a building block for several cryptographic constructions. Our O(N)\nresult improves the running-time upper bounds for deterministic tight\ncompaction, for randomized tight compaction, and for the simpler problem of\nrandomized loose compaction (move the M marked elements to the first O(M)\npositions) - until now respectively O(N lg N), O(N lg lg N), and O(N lg*N).\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 00:40:26 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Peserico", "Enoch", ""]]}, {"id": "1807.06820", "submitter": "Joan Boyar", "authors": "Joan Boyar, Faith Ellen, Kim S. Larsen", "title": "The Scheduler is Very Powerful in Competitive Analysis of Distributed\n  List Accessing", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a continuation of efforts to define and understand competitive\nanalysis of algorithms in a distributed shared memory setting, which is\nsurprisingly different from the classical online setting. In fact, in a\ndistributed shared memory setting, we find a counter-example to the theorem\nconcerning classical randomized online algorithms which shows that, if there is\na $c$-competitive randomized algorithm against an adaptive offline adversary,\nthen there is a $c$-competitive deterministic algorithm [Ben-David, Borodin,\nKarp, Tardos, Wigderson, 1994]. In a distributed setting, there is additional\nlack of knowledge concerning what the other processes have done. There is also\nadditional power for the adversary, having control of the scheduler which\ndecides when each process is allowed to take steps.\n  We consider the list accessing problem, which is a benchmark problem for\nsequential online algorithms. In the distributed version of this problem, each\nprocess has its own finite sequence of requests to a shared list. The scheduler\narises as a major issue in its competitive analysis. We introduce two different\nadversaries, which differ in how they are allowed to schedule processes, and\nuse them to perform competitive analysis of distributed list accessing. We\nprove tight upper and lower bounds on combinatorial properties of merges of the\nrequest sequences, which we use in the analysis. Our analysis shows that the\neffects of the adversarial scheduler can be quite significant, dominating the\nusual quality loss due to lack of information about the future.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 08:52:06 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Boyar", "Joan", ""], ["Ellen", "Faith", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1807.06874", "submitter": "Robin Lamarche-Perrin", "authors": "Robin Lamarche-Perrin", "title": "An Information-theoretic Framework for the Lossy Compression of Link\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph compression is a data analysis technique that consists in the\nreplacement of parts of a graph by more general structural patterns in order to\nreduce its description length. It notably provides interesting exploration\ntools for the study of real, large-scale, and complex graphs which cannot be\ngrasped at first glance. This article proposes a framework for the compression\nof temporal graphs, that is for the compression of graphs that evolve with\ntime. This framework first builds on a simple and limited scheme, exploiting\nstructural equivalence for the lossless compression of static graphs, then\ngeneralises it to the lossy compression of link streams, a recent formalism for\nthe study of temporal graphs. Such generalisation relies on the natural\nextension of (bidimensional) relational data by the addition of a third\ntemporal dimension. Moreover, we introduce an information-theoretic measure to\nquantify and to control the information that is lost during compression, as\nwell as an algebraic characterisation of the space of possible compression\npatterns to enhance the expressiveness of the initial compression scheme. These\ncontributions lead to the definition of a combinatorial optimisation problem,\nthat is the Lossy Multistream Compression Problem, for which we provide an\nexact algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 11:48:37 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Lamarche-Perrin", "Robin", ""]]}, {"id": "1807.06921", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco, Luisa Gargano, Joseph Peters, Adele Anna Rescigno,\n  Ugo Vaccaro", "title": "Time-Bounded Influence Diffusion with Incentives", "comments": "An extended abstract of this paper was presented at the 25th\n  International Colloquium on Structural Information and Communication\n  Complexity (Sirocco 2018) June 18-21, 2018 Ma'ale HaHamisha, Israel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely studied model of influence diffusion in social networks represents\nthe network as a graph $G=(V,E)$ with an influence threshold $t(v)$ for each\nnode. Initially the members of an initial set $S\\subseteq V$ are influenced.\nDuring each subsequent round, the set of influenced nodes is augmented by\nincluding every node $v$ that has at least $t(v)$ previously influenced\nneighbours. The general problem is to find a small initial set that influences\nthe whole network. In this paper we extend this model by using\n\\emph{incentives} to reduce the thresholds of some nodes. The goal is to\nminimize the total of the incentives required to ensure that the process\ncompletes within a given number of rounds. The problem is hard to approximate\nin general networks. We present polynomial-time algorithms for paths, trees,\nand complete networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:36:56 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Peters", "Joseph", ""], ["Rescigno", "Adele Anna", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1807.06933", "submitter": "Sandor Kisfaludi-Bak", "authors": "Mark de Berg, Hans L. Bodlaender, S\\'andor Kisfaludi-Bak, Sudeshna\n  Kolay", "title": "An ETH-Tight Exact Algorithm for Euclidean TSP", "comments": "To appear in FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exact algorithms for {\\sc Euclidean TSP} in $\\mathbb{R}^d$. In the\nearly 1990s algorithms with $n^{O(\\sqrt{n})}$ running time were presented for\nthe planar case, and some years later an algorithm with $n^{O(n^{1-1/d})}$\nrunning time was presented for any $d\\geq 2$. Despite significant interest in\nsubexponential exact algorithms over the past decade, there has been no\nprogress on {\\sc Euclidean TSP}, except for a lower bound stating that the\nproblem admits no $2^{O(n^{1-1/d-\\epsilon})}$ algorithm unless ETH fails. Up to\nconstant factors in the exponent, we settle the complexity of {\\sc Euclidean\nTSP} by giving a $2^{O(n^{1-1/d})}$ algorithm and by showing that a\n$2^{o(n^{1-1/d})}$ algorithm does not exist unless ETH fails.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:52:28 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 09:48:44 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["de Berg", "Mark", ""], ["Bodlaender", "Hans L.", ""], ["Kisfaludi-Bak", "S\u00e1ndor", ""], ["Kolay", "Sudeshna", ""]]}, {"id": "1807.06965", "submitter": "Kitty Meeks", "authors": "Kitty Meeks and Fiona Skerman", "title": "The parameterised complexity of computing the maximum modularity of a\n  graph", "comments": "Author final version, accepted to Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum modularity of a graph is a parameter widely used to describe the\nlevel of clustering or community structure in a network. Determining the\nmaximum modularity of a graph is known to be NP-complete in general, and in\npractice a range of heuristics are used to construct partitions of the\nvertex-set which give lower bounds on the maximum modularity but without any\nguarantee on how close these bounds are to the true maximum. In this paper we\ninvestigate the parameterised complexity of determining the maximum modularity\nwith respect to various standard structural parameterisations of the input\ngraph G. We show that the problem belongs to FPT when parameterised by the size\nof a minimum vertex cover for G, and is solvable in polynomial time whenever\nthe treewidth or max leaf number of G is bounded by some fixed constant; we\nalso obtain an FPT algorithm, parameterised by treewidth, to compute any\nconstant-factor approximation to the maximum modularity. On the other hand we\nshow that the problem is W[1]-hard (and hence unlikely to admit an FPT\nalgorithm) when parameterised simultaneously by pathwidth and the size of a\nminimum feedback vertex set.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 14:25:30 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:33:18 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2018 12:43:03 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 10:58:01 GMT"}, {"version": "v5", "created": "Tue, 29 Oct 2019 15:53:26 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Meeks", "Kitty", ""], ["Skerman", "Fiona", ""]]}, {"id": "1807.07005", "submitter": "Grigoriy Bokov", "authors": "Grigoriy V. Bokov", "title": "Quantified boolean formula problem", "comments": "In Russian, 5 pages, submitted to Doklady Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the complexity of the quantified boolean formula\nproblem. We describe a simple deterministic algorithm that, for a given\nquantified boolean formula $F$, stops in time bounded by $O(|F|^4)$ and answers\nyes if $F$ is true and no otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 15:45:36 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 16:00:40 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bokov", "Grigoriy V.", ""]]}, {"id": "1807.07013", "submitter": "Phil Long", "authors": "Anindya De, Philip M. Long and Rocco A. Servedio", "title": "Learning Sums of Independent Random Variables with Sparse Collective\n  Support", "comments": "Conference version in FOCS'18; Journal version to appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the learnability of sums of independent integer random variables\ngiven a bound on the size of the union of their supports. For $\\mathcal{A}\n\\subset \\mathbf{Z}_{+}$, a sum of independent random variables with collective\nsupport $\\mathcal{A}$} (called an $\\mathcal{A}$-sum in this paper) is a\ndistribution $\\mathbf{S} = \\mathbf{X}_1 + \\cdots + \\mathbf{X}_N$ where the\n$\\mathbf{X}_i$'s are mutually independent (but not necessarily identically\ndistributed) integer random variables with $\\cup_i \\mathsf{supp}(\\mathbf{X}_i)\n\\subseteq \\mathcal{A}.$ We give two main algorithmic results for learning such\ndistributions:\n  1. For the case $| \\mathcal{A} | = 3$, we give an algorithm for learning\n$\\mathcal{A}$-sums to accuracy $\\epsilon$ that uses $\\mathsf{poly}(1/\\epsilon)$\nsamples and runs in time $\\mathsf{poly}(1/\\epsilon)$, independent of $N$ and of\nthe elements of $\\mathcal{A}$.\n  2. For an arbitrary constant $k \\geq 4$, if $\\mathcal{A} = \\{ a_1,...,a_k\\}$\nwith $0 \\leq a_1 < ... < a_k$, we give an algorithm that uses\n$\\mathsf{poly}(1/\\epsilon) \\cdot \\log \\log a_k$ samples (independent of $N$)\nand runs in time $\\mathsf{poly}(1/\\epsilon, \\log a_k).$\n  We prove an essentially matching lower bound: if $|\\mathcal{A}| = 4$, then\nany algorithm must use $\\Omega(\\log \\log a_4) $ samples even for learning to\nconstant accuracy. We also give similar-in-spirit (but quantitatively very\ndifferent) algorithmic results, and essentially matching lower bounds, for the\ncase in which $\\mathcal{A}$ is not known to the learner.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 16:02:35 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 18:14:33 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["De", "Anindya", ""], ["Long", "Philip M.", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1807.07067", "submitter": "Assaf Kfoury", "authors": "Assaf Kfoury", "title": "A Fixed-Parameter Linear-Time Algorithm to Compute Principal Typings of\n  Planar Flow Networks", "comments": "12 pages, 0 figures, 13 references. arXiv admin note: text overlap\n  with arXiv:1807.04186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative and simpler method for computing principal typings\nof flow networks. When limited to planar flow networks, the method can be made\nto run in fixed-parameter linear-time -- where the parameter not to be exceeded\nis what is called the edge-outerplanarity of the networks' underlying graphs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 16:12:41 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Kfoury", "Assaf", ""]]}, {"id": "1807.07122", "submitter": "Antoine Recanati", "authors": "Antoine Recanati, Thomas Kerdreux, Alexandre d'Aspremont", "title": "Reconstructing Latent Orderings by Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering uses a graph Laplacian spectral embedding to enhance the\ncluster structure of some data sets. When the embedding is one dimensional, it\ncan be used to sort the items (spectral ordering). A number of empirical\nresults also suggests that a multidimensional Laplacian embedding enhances the\nlatent ordering of the data, if any. This also extends to circular orderings, a\ncase where unidimensional embeddings fail. We tackle the task of retrieving\nlinear and circular orderings in a unifying framework, and show how a latent\nordering on the data translates into a filamentary structure on the Laplacian\nembedding. We propose a method to recover it, illustrated with numerical\nexperiments on synthetic data and real DNA sequencing data. The code and\nexperiments are available at https://github.com/antrec/mdso.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 19:48:52 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Recanati", "Antoine", ""], ["Kerdreux", "Thomas", ""], ["d'Aspremont", "Alexandre", ""]]}, {"id": "1807.07143", "submitter": "Kent Quanrud", "authors": "Kent Quanrud", "title": "Fast and Deterministic Approximations for $k$-Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an undirected graph, a $k$-cut is a set of edges whose removal breaks the\ngraph into at least $k$ connected components. The minimum weight $k$-cut can be\ncomputed in $O(n^{O(k)})$ time, but when $k$ is treated as part of the input,\ncomputing the minimum weight $k$-cut is NP-Hard [Holdschmidt and Hochbaum\n1994]. For $\\operatorname{poly}(m,n,k)$-time algorithms, the best possible\napproximation factor is essentially 2 under the small set expansion hypothesis\n[Manurangsi 2017]. Saran and Vazirani [1995] showed that a $(2 -\n2/k)$-approximately minimum weight $k$-cut can be computed by $O(k)$ minimum\ncuts, which implies an $\\tilde{O}(mk)$ randomized running time via the nearly\nlinear time randomized min-cut algorithm of Karger [2000]. Nagamochi and\nKamidoi [2007] showed that the minimum weight $k$-cut can be computed\ndeterministically in $O(mn + n^2 \\log n)$ time. These results prompt two basic\nquestions. The first concerns the role of randomization. Is there a\ndeterministic algorithm for 2-approximate $k$-cuts matching the randomized\nrunning time of $\\tilde{O}(mk)$? The second question qualitatively compares\nminimum cut to 2-approximate minimum $k$-cut. Can 2-approximate $k$-cuts be\ncomputed as fast as the (exact) minimum cut - in $\\tilde{O}(m)$ randomized\ntime?\n  We make progress on these questions with a deterministic approximation\nalgorithm that computes $(2 + \\epsilon)$-minimum $k$-cuts in $O(m \\log^3(n) /\n\\epsilon^2)$ time, via a $(1 + \\epsilon)$-approximate for an LP relaxation of\n$k$-cut.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 20:56:25 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 20:57:35 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Quanrud", "Kent", ""]]}, {"id": "1807.07156", "submitter": "Fahad Panolan", "authors": "Fedor V. Fomin, Petr A. Golovach, Daniel Lokshtanov, Fahad Panolan,\n  Saket Saurabh", "title": "Approximation Schemes for Low-Rank Binary Matrix Approximation Problems", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a randomized linear time approximation scheme for a generic\nproblem about clustering of binary vectors subject to additional constrains.\nThe new constrained clustering problem encompasses a number of problems and by\nsolving it, we obtain the first linear time-approximation schemes for a number\nof well-studied fundamental problems concerning clustering of binary vectors\nand low-rank approximation of binary matrices. Among the problems solvable by\nour approach are \\textsc{Low GF(2)-Rank Approximation}, \\textsc{Low\nBoolean-Rank Approximation}, and various versions of \\textsc{Binary\nClustering}. For example, for \\textsc{Low GF(2)-Rank Approximation} problem,\nwhere for an $m\\times n$ binary matrix $A$ and integer $r>0$, we seek for a\nbinary matrix $B$ of $GF_2$ rank at most $r$ such that $\\ell_0$ norm of matrix\n$A-B$ is minimum, our algorithm, for any $\\epsilon>0$ in time $\nf(r,\\epsilon)\\cdot n\\cdot m$, where $f$ is some computable function, outputs a\n$(1+\\epsilon)$-approximate solution with probability at least\n$(1-\\frac{1}{e})$. Our approximation algorithms substantially improve the\nrunning times and approximation factors of previous works. We also give\n(deterministic) PTASes for these problems running in time\n$n^{f(r)\\frac{1}{\\epsilon^2}\\log \\frac{1}{\\epsilon}}$, where $f$ is some\nfunction depending on the problem. Our algorithm for the constrained clustering\nproblem is based on a novel sampling lemma, which is interesting in its own.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 21:11:35 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""]]}, {"id": "1807.07177", "submitter": "Pavel Vesel\\'y", "authors": "Pavel Vesel\\'y, Marek Chrobak, {\\L}ukasz Je\\.z, Ji\\v{r}\\'i Sgall", "title": "A $\\phi$-Competitive Algorithm for Scheduling Packets with Deadlines", "comments": "Another major revision of the paper, with focus on presentation. Also\n  included hard examples for some simpler variants of the algorithm", "journal-ref": null, "doi": "10.1137/1.9781611975482.9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online packet scheduling problem with deadlines (PacketScheduling, for\nshort), the goal is to schedule transmissions of packets that arrive over time\nin a network switch and need to be sent across a link. Each packet has a\ndeadline, representing its urgency, and a non-negative weight, that represents\nits priority. Only one packet can be transmitted in any time slot, so, if the\nsystem is overloaded, some packets will inevitably miss their deadlines and be\ndropped. In this scenario, the natural objective is to compute a transmission\nschedule that maximizes the total weight of packets which are successfully\ntransmitted. The problem is inherently online, with the scheduling decisions\nmade without the knowledge of future packet arrivals. The central problem\nconcerning PacketScheduling, that has been a subject of intensive study since\n2001, is to determine the optimal competitive ratio of online algorithms,\nnamely the worst-case ratio between the optimum total weight of a schedule\n(computed by an offline algorithm) and the weight of a schedule computed by a\n(deterministic) online algorithm.\n  We solve this open problem by presenting a $\\phi$-competitive online\nalgorithm for PacketScheduling (where $\\phi\\approx 1.618$ is the golden ratio),\nmatching the previously established lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 22:45:19 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 22:30:39 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 15:56:09 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 14:56:07 GMT"}, {"version": "v5", "created": "Fri, 28 May 2021 12:16:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Vesel\u00fd", "Pavel", ""], ["Chrobak", "Marek", ""], ["Je\u017c", "\u0141ukasz", ""], ["Sgall", "Ji\u0159\u00ed", ""]]}, {"id": "1807.07189", "submitter": "Sami Davies", "authors": "Sami Davies, Thomas Rothvoss, Yihao Zhang", "title": "A Tale of Santa Claus, Hypergraphs and Matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known problem in scheduling and approximation algorithms is the Santa\nClaus problem. Suppose that Santa Claus has a set of gifts, and he wants to\ndistribute them among a set of children so that the least happy child is made\nas happy as possible. Here, the value that a child $i$ has for a present $j$ is\nof the form $p_{ij} \\in \\{ 0,p_j\\}$. A polynomial time algorithm by Annamalai\net al. gives a $12.33$-approximation and is based on a modification of Haxell's\nhypergraph matching argument.\n  In this paper, we introduce a matroid version of the Santa Claus problem. Our\nalgorithm is also based on Haxell's augmenting tree, but with the introduction\nof the matroid structure we solve a more general problem with cleaner methods.\nOur result can then be used as a blackbox to obtain a\n$(4+\\varepsilon)$-approximation for Santa Claus. This factor also compares\nagainst a natural, compact LP for Santa Claus.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 00:05:47 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 20:14:05 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Davies", "Sami", ""], ["Rothvoss", "Thomas", ""], ["Zhang", "Yihao", ""]]}, {"id": "1807.07483", "submitter": "Bruno Ziliotto", "authors": "Jose Correa and Raimundo Saona and Bruno Ziliotto", "title": "Prophet Secretary Through Blind Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic prophet inequality, samples from independent random variables\narrive online. A gambler that knows the distributions must decide at each point\nin time whether to stop and pick the current sample or to continue and lose\nthat sample forever. The goal of the gambler is to maximize the expected value\nof what she picks and the performance measure is the worst case ratio between\nthe expected value the gambler gets and what a prophet, that sees all the\nrealizations in advance, gets. In the late seventies, Krengel and Sucheston,\nand Gairing (1977) established that this worst case ratio is a universal\nconstant equal to 1/2. In the last decade prophet inequalities has resurged as\nan important problem due to its connections to posted price mechanisms,\nfrequently used in online sales. A very interesting variant is the Prophet\nSecretary problem, in which the only difference is that the samples arrive in a\nuniformly random order. For this variant several algorithms achieve a constant\nof 1-1/e and very recently this barrier was slightly improved. This paper\nanalyzes strategies that set a nonincreasing sequence of thresholds to be\napplied at different times. The gambler stops the first time a sample surpasses\nthe corresponding threshold. Specifically we consider a class of strategies\ncalled blind quantile strategies. They consist in fixing a function which is\nused to define a sequence of thresholds once the instance is revealed. Our main\nresult shows that they can achieve a constant of 0.665, improving upon the best\nknown result of Azar et al. (2018), and on Beyhaghi et al. (2018) (order\nselection). Our proof analyzes precisely the underlying stopping time\ndistribution, relying on Schur-convexity theory. We further prove that blind\nstrategies cannot achieve better than 0.675. Finally we prove that no algorithm\nfor the gambler can achieve better than 0.732.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 15:16:35 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 15:31:21 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Correa", "Jose", ""], ["Saona", "Raimundo", ""], ["Ziliotto", "Bruno", ""]]}, {"id": "1807.07516", "submitter": "Andr\\'e Nichterlein", "authors": "Christian Komusiewicz, Andr\\'e Nichterlein, Rolf Niedermeier, Marten\n  Picker", "title": "Exact Algorithms for Finding Well-Connected 2-Clubs in Real-World\n  Graphs: Theory and Experiments", "comments": "To appear at European Journal of Operational Research", "journal-ref": null, "doi": "10.1016/j.ejor.2018.12.006", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding large \"cliquish\" subgraphs is a central topic in graph mining and\ncommunity detection. A popular clique relaxation are 2-clubs: instead of asking\nfor subgraphs of diameter one (these are cliques), one asks for subgraphs of\ndiameter at most two (these are 2-clubs). A drawback of the 2-club model is\nthat it produces star-like hub-and-spoke structures as maximum-cardinality\nsolutions. Hence, we study 2-clubs with the additional constraint to be\nwell-connected. More specifically, we investigate the algorithmic complexity\nfor three variants of well-connected 2-clubs, all established in the\nliterature: robust, hereditary, and \"connected\" 2-clubs. Finding these more\ncohesive 2-clubs is NP-hard; nevertheless, we develop an exact combinatorial\nalgorithm, extensively using efficient data reduction rules. Besides several\ntheoretical insights we provide a number of empirical results based on an\nengineered implementation of our exact algorithm. In particular, the algorithm\nsignificantly outperforms existing algorithms on almost all (sparse) real-world\ngraphs we considered.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:14:01 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 15:17:57 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Komusiewicz", "Christian", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""], ["Picker", "Marten", ""]]}, {"id": "1807.07527", "submitter": "Alexander Wei", "authors": "Alexander Wei", "title": "Optimal Las Vegas Approximate Near Neighbors in $\\ell_p$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that approximate near neighbor search in high dimensions can be\nsolved in a Las Vegas fashion (i.e., without false negatives) for $\\ell_p$\n($1\\le p\\le 2$) while matching the performance of optimal locality-sensitive\nhashing. Specifically, we construct a data-independent Las Vegas data structure\nwith query time $O(dn^{\\rho})$ and space usage $O(dn^{1+\\rho})$ for $(r, c\nr)$-approximate near neighbors in $\\mathbb{R}^{d}$ under the $\\ell_p$ norm,\nwhere $\\rho = 1/c^p + o(1)$. Furthermore, we give a Las Vegas\nlocality-sensitive filter construction for the unit sphere that can be used\nwith the data-dependent data structure of Andoni et al. (SODA 2017) to achieve\noptimal space-time tradeoffs in the data-dependent setting. For the symmetric\ncase, this gives us a data-dependent Las Vegas data structure with query time\n$O(dn^{\\rho})$ and space usage $O(dn^{1+\\rho})$ for $(r, c r)$-approximate near\nneighbors in $\\mathbb{R}^{d}$ under the $\\ell_p$ norm, where $\\rho = 1/(2c^p -\n1) + o(1)$.\n  Our data-independent construction improves on the recent Las Vegas data\nstructure of Ahle (FOCS 2017) for $\\ell_p$ when $1 < p\\le 2$. Our\ndata-dependent construction does even better for $\\ell_p$ for all $p\\in [1, 2]$\nand is the first Las Vegas approximate near neighbors data structure to make\nuse of data-dependent approaches. We also answer open questions of Indyk (SODA\n2000), Pagh (SODA 2016), and Ahle by showing that for approximate near\nneighbors, Las Vegas data structures can match state-of-the-art Monte Carlo\ndata structures in performance for both the data-independent and data-dependent\nsettings and across space-time tradeoffs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:44:54 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Wei", "Alexander", ""]]}, {"id": "1807.07596", "submitter": "Marinella Sciortino", "authors": "F. Garofalo, G. Rosone, M. Sciortino, D. Verzotto", "title": "The colored longest common prefix array computed via sequential scans", "comments": "Preliminary version of the paper that will be included in the SPIRE\n  2018 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increased availability of large datasets of biological sequences,\nthe tools for sequence comparison are now relying on efficient alignment-free\napproaches to a greater extent. Most of the alignment-free approaches require\nthe computation of statistics of the sequences in the dataset. Such\ncomputations become impractical in internal memory when very large collections\nof long sequences are considered. In this paper, we present a new conceptual\ndata structure, the colored longest common prefix array (cLCP), that allows to\nefficiently tackle several problems with an alignment-free approach. In fact,\nwe show that such a data structure can be computed via sequential scans in\nsemi-external memory. By using cLCP, we propose an efficient lightweight\nstrategy to solve the multi-string Average Common Substring (ACS) problem, that\nconsists in the pairwise comparison of a single string against a collection of\n$m$ strings simultaneously, in order to obtain $m$ ACS induced distances.\nExperimental results confirm the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 18:33:20 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Garofalo", "F.", ""], ["Rosone", "G.", ""], ["Sciortino", "M.", ""], ["Verzotto", "D.", ""]]}, {"id": "1807.07619", "submitter": "Rishi Sonthalia", "authors": "Anna C. Gilbert and Rishi Sonthalia", "title": "Generalized Metric Repair on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data analysis algorithms either assume that or are considerably\nmore efficient if the distances between the data points satisfy a metric. These\nalgorithms include metric learning, clustering, and dimensionality reduction.\nBecause real data sets are noisy, the similarity measures often fail to satisfy\na metric. For this reason, Gilbert and Jain [11] and Fan, et al. [8] introduce\nthe closely related problems of $\\textit{sparse metric repair}$ and\n$\\textit{metric violation distance}$. The goal of each problem is to repair as\nfew distances as possible to ensure that the distances between the data points\nsatisfy a metric. We generalize these problems so as to no longer require all\nthe distances between the data points. That is, we consider a weighted graph\n$G$ with corrupted weights w and our goal is to find the smallest number of\nmodifications to the weights so that the resulting weighted graph distances\nsatisfy a metric. This problem is a natural generalization of the sparse metric\nrepair problem and is more flexible as it takes into account different\nrelationships amongst the input data points. As in previous work, we\ndistinguish amongst the types of repairs permitted (decrease, increase, and\ngeneral repairs). We focus on the increase and general versions and establish\nhardness results and show the inherent combinatorial structure of the problem.\nWe then show that if we restrict to the case when $G$ is a chordal graph, then\nthe problem is fixed parameter tractable. We also present several classes of\napproximation algorithms. These include and improve upon previous metric repair\nalgorithms for the special case when $G = K_n$\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 19:34:28 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Gilbert", "Anna C.", ""], ["Sonthalia", "Rishi", ""]]}, {"id": "1807.07626", "submitter": "Andreas Wiese", "authors": "Micha{\\l} Pilipczuk, Erik Jan van Leeuwen, and Andreas Wiese", "title": "Quasi-polynomial time approximation schemes for packing and covering\n  problems in planar graphs", "comments": "31 pages, 5 figures, accepted at ESA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two optimization problems in planar graphs. In Maximum Weight\nIndependent Set of Objects we are given a graph $G$ and a family $\\mathcal{D}$\nof objects, each being a connected subgraph of $G$ with a prescribed weight,\nand the task is to find a maximum-weight subfamily of $\\mathcal{D}$ consisting\nof pairwise disjoint objects. In Minimum Weight Distance Set Cover we are given\nan edge-weighted graph $G$, two sets $\\mathcal{D},\\mathcal{C}$ of vertices of\n$G$, where vertices of $\\mathcal{D}$ have prescribed weights, and a nonnegative\nradius $r$. The task is to find a minimum-weight subset of $\\mathcal{D}$ such\nthat every vertex of $\\mathcal{C}$ is at distance at most $r$ from some\nselected vertex. Via simple reductions, these two problems generalize a number\nof geometric optimization tasks, notably Maximum Weight Independent Set for\npolygons in the plane and Weighted Geometric Set Cover for unit disks and unit\nsquares. We present quasi-polynomial time approximation schemes (QPTASs) for\nboth of the above problems in planar graphs: given an accuracy parameter\n$\\epsilon>0$ we can compute a solution whose weight is within multiplicative\nfactor of $(1+\\epsilon)$ from the optimum in time\n$2^{\\mathrm{poly}(1/\\epsilon,\\log |\\mathcal{D}|)}\\cdot n^{\\mathcal{O}(1)}$,\nwhere $n$ is the number of vertices of the input graph. Our main technical\ncontribution is to transfer the techniques used for recursive approximation\nschemes for geometric problems due to Adamaszek, Har-Peled, and Wiese to the\nsetting of planar graphs. In particular, this yields a purely combinatorial\nviewpoint on these methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 20:00:44 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Pilipczuk", "Micha\u0142", ""], ["van Leeuwen", "Erik Jan", ""], ["Wiese", "Andreas", ""]]}, {"id": "1807.07635", "submitter": "Paris Siminelakis", "authors": "Moses Charikar, Paris Siminelakis", "title": "Multi-Resolution Hashing for Fast Pairwise Summations", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic computational primitive in the analysis of massive datasets is\nsumming simple functions over a large number of objects. Modern applications\npose an additional challenge in that such functions often depend on a parameter\nvector $y$ (query) that is unknown a priori. Given a set of points $X\\subset\n\\mathbb{R}^{d}$ and a pairwise function $w:\\mathbb{R}^{d}\\times\n\\mathbb{R}^{d}\\to [0,1]$, we study the problem of designing a data-structure\nthat enables sublinear-time approximation of the summation\n$Z_{w}(y)=\\frac{1}{|X|}\\sum_{x\\in X}w(x,y)$ for any query $y\\in\n\\mathbb{R}^{d}$. By combining ideas from Harmonic Analysis (partitions of unity\nand approximation theory) with Hashing-Based-Estimators [Charikar, Siminelakis\nFOCS'17], we provide a general framework for designing such data structures\nthrough hashing that reaches far beyond what previous techniques allowed.\n  A key design principle is a collection of $T\\geq 1$ hashing schemes with\ncollision probabilities $p_{1},\\ldots, p_{T}$ such that $\\sup_{t\\in\n[T]}\\{p_{t}(x,y)\\} = \\Theta(\\sqrt{w(x,y)})$. This leads to a data-structure\nthat approximates $Z_{w}(y)$ using a sub-linear number of samples from each\nhash family. Using this new framework along with Distance Sensitive Hashing\n[Aumuller, Christiani, Pagh, Silvestri PODS'18], we show that such a collection\ncan be constructed and evaluated efficiently for any log-convex function\n$w(x,y)=e^{\\phi(\\langle x,y\\rangle)}$ of the inner product on the unit sphere\n$x,y\\in \\mathcal{S}^{d-1}$.\n  Our method leads to data structures with sub-linear query time that\nsignificantly improve upon random sampling and can be used for Kernel Density\nor Partition Function Estimation. We provide extensions of our result from the\nsphere to $\\mathbb{R}^{d}$ and from scalar functions to vector functions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 20:50:06 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 21:53:19 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 22:23:53 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Charikar", "Moses", ""], ["Siminelakis", "Paris", ""]]}, {"id": "1807.07640", "submitter": "Suman Bera", "authors": "Suman Kalyan Bera, Prantar Ghosh", "title": "Coloring in Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate the study of the vertex coloring problem of a\ngraph in the semi streaming model. In this model, the input graph is defined by\na stream of edges, arriving in adversarial order and any algorithm must process\nthe edges in the order of arrival using space linear (up to polylogarithmic\nfactors) in the number of vertices of the graph. In the offline settings, there\nis a simple greedy algorithm for $(\\Delta+1)$-vertex coloring of a graph with\nmaximum degree $\\Delta$. We design a one pass randomized streaming algorithm\nfor $(1+\\varepsilon)\\Delta$-vertex coloring problem for any constant\n$\\varepsilon >0$ using $O(\\varepsilon^{-1} n ~\\mathrm{ poly} \\log n)$ space\nwhere $n$ is the number of vertices in the graph. Much more color efficient\nalgorithms are known for graphs with bounded arboricity in the offline\nsettings. Specifically, there is a simple $2\\alpha$-vertex coloring algorithm\nfor a graph with arboricity $\\alpha$. We present a $O(\\varepsilon^{-1}\\log n)$\npass randomized vertex coloring algorithm that requires at most\n$(2+\\varepsilon)\\alpha$ many colors for any constant $\\varepsilon>0$ for a\ngraph with arboricity $\\alpha$ in the semi streaming model.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 21:02:10 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:34:41 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Bera", "Suman Kalyan", ""], ["Ghosh", "Prantar", ""]]}, {"id": "1807.07645", "submitter": "David Harris", "authors": "David G. Harris", "title": "Distributed local approximation algorithms for maximum matching in\n  graphs and hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe approximation algorithms in Linial's classic LOCAL model of\ndistributed computing to find maximum-weight matchings in a hypergraph of rank\n$r$. Our main result is a deterministic algorithm to generate a matching which\nis an $O(r)$-approximation to the maximum weight matching, running in $\\tilde\nO(r \\log \\Delta + \\log^2 \\Delta + \\log^* n)$ rounds. (Here, the $\\tilde O()$\nnotations hides $\\text{polyloglog } \\Delta$ and $\\text{polylog } r$ factors).\nThis is based on a number of new derandomization techniques extending methods\nof Ghaffari, Harris & Kuhn (2017).\n  As a main application, we obtain nearly-optimal algorithms for the\nlong-studied problem of maximum-weight graph matching. Specifically, we get a\n$(1+\\epsilon)$ approximation algorithm using $\\tilde O(\\log \\Delta / \\epsilon^3\n+ \\text{polylog}(1/\\epsilon, \\log \\log n))$ randomized time and $\\tilde\nO(\\log^2 \\Delta / \\epsilon^4 + \\log^*n / \\epsilon)$ deterministic time.\n  The second application is a faster algorithm for hypergraph maximal matching,\na versatile subroutine introduced in Ghaffari et al. (2017) for a variety of\nlocal graph algorithms. This gives an algorithm for $(2 \\Delta - 1)$-edge-list\ncoloring in $\\tilde O(\\log^2 \\Delta \\log n)$ rounds deterministically or\n$\\tilde O( (\\log \\log n)^3 )$ rounds randomly. Another consequence (with\nadditional optimizations) is an algorithm which generates an edge-orientation\nwith out-degree at most $\\lceil (1+\\epsilon) \\lambda \\rceil$ for a graph of\narboricity $\\lambda$; for fixed $\\epsilon$ this runs in $\\tilde O(\\log^6 n)$\nrounds deterministically or $\\tilde O(\\log^3 n )$ rounds randomly.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 21:13:11 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 22:27:39 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 11:51:26 GMT"}, {"version": "v4", "created": "Fri, 14 Jun 2019 00:58:25 GMT"}, {"version": "v5", "created": "Sun, 4 Aug 2019 15:31:16 GMT"}, {"version": "v6", "created": "Fri, 3 Jan 2020 22:06:42 GMT"}, {"version": "v7", "created": "Mon, 23 Mar 2020 15:22:46 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1807.07719", "submitter": "Bryce Sandlund", "authors": "Eric Bach and Bryce Sandlund", "title": "On Euclidean Methods for Cubic and Quartic Jacobi Symbols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bit complexity of two methods, related to the Euclidean\nalgorithm, for computing cubic and quartic analogs of the Jacobi symbol. The\nmain bottleneck in such procedures is computation of a quotient for long\ndivision. We give examples to show that with standard arithmetic, if quotients\nare computed naively (by using exact norms as denominators, then rounding), the\nalgorithms have $\\Theta(n^3)$ bit complexity. It is a \"folk theorem\" that this\ncan be reduced to $O(n^2)$ by modifying the division procedure. We give a\nself-contained proof of this, and show that quadratic time is best possible for\nthese algorithms (with standard arithmetic or not).\n  We also address the relative efficiency of using reciprocity, as compared to\nEuler's criterion, for testing if a given number is a cubic or quartic residue\nmodulo an odd prime. Which is preferable depends on the number of residue tests\nto be done.\n  Finally, we discuss the cubic and quartic analogs of Eisenstein's\neven-quotient algorithm for computing Jacobi symbols in ${\\bf Z}$. Although the\nquartic algorithm was given by Smith in 1859, the version for cubic symbols\nseems to be new. As far as we know, neither was analyzed before. We show that\nboth algorithms have exponential worst-case bit complexity. The proof for the\ncubic algorithm involves a cyclic repetition of four quotients, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 07:16:22 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bach", "Eric", ""], ["Sandlund", "Bryce", ""]]}, {"id": "1807.07830", "submitter": "Briti Deb", "authors": "Briti Deb and Indrajit Mukherjee", "title": "Biclustering Using Modified Matrix Bandwidth Minimization and\n  Biogeography-based Optimization", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data matrix having different sets of entities in its rows and columns are\nknown as two mode data or affiliation data. Many practical problems require to\nfind relationships between the two modes by simultaneously clustering the rows\nand columns, a problem commonly known as biclustering. We propose a novel\nbiclustering algorithm by using matrix reordering approach introduced by\nCuthill-McKee's bandwidth minimization algorithm, and adapting it to operate on\nnon-square and non-binary matrices, without the need to know apriori the number\nof naturally occurring biclusters. This transforms a two-mode matrix into\nalmost block diagonals, where the blocks indicate the clusters between the two\nmodes of the matrix. To optimize the bandwidth minimization problem, we adapted\nthe Biogeography-based Optimization algorithm using logistic equation to model\nits migration rates. Preliminary studies indicate that this technique can\nreveal the underlying biclusters in the data and has potential of further\nresearch for two-mode data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 13:25:34 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Deb", "Briti", ""], ["Mukherjee", "Indrajit", ""]]}, {"id": "1807.07889", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Vahab Mirrokni, Morteza Zadimoghaddam", "title": "Submodular Maximization with Nearly Optimal Approximation, Adaptivity\n  and Query Complexity", "comments": "30 pages, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on\n  Discrete Algorithms (SODA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular optimization generalizes many classic problems in combinatorial\noptimization and has recently found a wide range of applications in machine\nlearning (e.g., feature engineering and active learning). For many large-scale\noptimization problems, we are often concerned with the adaptivity complexity of\nan algorithm, which quantifies the number of sequential rounds where\npolynomially-many independent function evaluations can be executed in parallel.\nWhile low adaptivity is ideal, it is not sufficient for a distributed algorithm\nto be efficient, since in many practical applications of submodular\noptimization the number of function evaluations becomes prohibitively\nexpensive. Motivated by these applications, we study the adaptivity and query\ncomplexity of adaptive submodular optimization.\n  Our main result is a distributed algorithm for maximizing a monotone\nsubmodular function with cardinality constraint $k$ that achieves a\n$(1-1/e-\\varepsilon)$-approximation in expectation. This algorithm runs in\n$O(\\log(n))$ adaptive rounds and makes $O(n)$ calls to the function evaluation\noracle in expectation. The approximation guarantee and query complexity are\noptimal, and the adaptivity is nearly optimal. Moreover, the number of queries\nis substantially less than in previous works. Last, we extend our results to\nthe submodular cover problem to demonstrate the generality of our algorithm and\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 15:19:02 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 19:25:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Mirrokni", "Vahab", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1807.07937", "submitter": "Susana Ladra", "authors": "Guillermo de Bernardo, Susana Ladra", "title": "About BIRDS project (Bioinformatics and Information Retrieval Data\n  Structures Analysis and Design)", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. CERI 2018", "journal-ref": "Proceedings of the 5th Spanish Conference on Information Retrieval\n  (CERI '18), 2018", "doi": "10.1145/3230599.3230602", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  BIRDS stands for \"Bioinformatics and Information Retrieval Data Structures\nanalysis and design\" and is a 4-year project (2016--2019) that has received\nfunding from the European Union's Horizon 2020 research and innovation\nprogramme under the Marie Sklodowska-Curie grant agreement No 690941.\n  The overall goal of BIRDS is to establish a long term international network\ninvolving leading researchers in the development of efficient data structures\nin the fields of Bioinformatics and Information Retrieval, to strengthen the\npartnership through the exchange of knowledge and expertise, and to develop\nintegrated approaches to improve current approaches in both fields. The\nresearch will address challenges in storing, processing, indexing, searching\nand navigating genome-scale data by designing new algorithms and data\nstructures for sequence analysis, networks representation or compressing and\nindexing repetitive data.\n  BIRDS project is carried out by 7 research institutions from Australia\n(University of Melbourne), Chile (University of Chile and University of\nConcepci\\'on), Finland (University of Helsinki), Japan (Kyushu University),\nPortugal (Instituto de Engenharia de Sistemas e Computadores,\nInvestiga\\c{c}\\~ao e Desenvolvimento em Lisboa, INESC-ID), and Spain\n(University of A Coru\\~na), and a Spanish SME (Enxenio S.L.). It is coordinated\nby the University of A Coru\\~na (Spain).\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 10:38:29 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["de Bernardo", "Guillermo", ""], ["Ladra", "Susana", ""]]}, {"id": "1807.08065", "submitter": "Matthew Johnson", "authors": "Matthew P. Johnson", "title": "Red-Blue-Partitioned MST, TSP, and Matching", "comments": "full version (conference version in CCCG 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arkin et al.~\\cite{ArkinBCCJKMM17} recently introduced \\textit{partitioned\npairs} network optimization problems: given a metric-weighted graph on $n$\npairs of nodes, the task is to color one node from each pair red and the other\nblue, and then to compute two separate \\textit{network structures} or disjoint\n(node-covering) subgraphs of a specified sort, one on the graph induced by the\nred nodes and the other on the blue nodes. Three structures have been\ninvestigated by \\cite{ArkinBCCJKMM17}---\\textit{spanning trees},\n\\textit{traveling salesperson tours}, and \\textit{perfect matchings}---and the\nthree objectives to optimize for when computing such pairs of structures:\n\\textit{min-sum}, \\textit{min-max}, and \\textit{bottleneck}. We provide\nimproved approximation guarantees and/or strengthened hardness results for\nthese nine NP-hard problem settings.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 01:06:43 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 04:15:43 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Johnson", "Matthew P.", ""]]}, {"id": "1807.08084", "submitter": "Renato J Cintra", "authors": "D. F. G. Coelho, R. J. Cintra, A. C. Frery, V. S. Dimitrov", "title": "Fast Matrix Inversion and Determinant Computation for Polarimetric\n  Synthetic Aperture Radar", "comments": "7 pages, 1 figure", "journal-ref": "Computers and Geosciences, no. 119 (2018), pages 109-114", "doi": "10.1016/j.cageo.2018.07.002", "report-no": null, "categories": "cs.NA cs.DS eess.SP math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast algorithm for simultaneous inversion and\ndeterminant computation of small sized matrices in the context of fully\nPolarimetric Synthetic Aperture Radar (PolSAR) image processing and analysis.\nThe proposed fast algorithm is based on the computation of the adjoint matrix\nand the symmetry of the input matrix. The algorithm is implemented in a general\npurpose graphical processing unit (GPGPU) and compared to the usual approach\nbased on Cholesky factorization. The assessment with simulated observations and\ndata from an actual PolSAR sensor show a speedup factor of about two when\ncompared to the usual Cholesky factorization. Moreover, the expressions\nprovided here can be implemented in any platform.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 04:52:06 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Coelho", "D. F. G.", ""], ["Cintra", "R. J.", ""], ["Frery", "A. C.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1807.08144", "submitter": "Jason Li", "authors": "Anupam Gupta, Euiwoong Lee, Jason Li", "title": "Faster Exact and Approximate Algorithms for $k$-Cut", "comments": "Appeared in FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-cut problem, we are given an edge-weighted graph $G$ and an\ninteger $k$, and have to remove a set of edges with minimum total weight so\nthat $G$ has at least $k$ connected components. The current best algorithms are\nan $O(n^{(2-o(1))k})$ randomized algorithm due to Karger and Stein, and an\n$\\smash{\\tilde{O}}(n^{2k})$ deterministic algorithm due to Thorup. Moreover,\nseveral $2$-approximation algorithms are known for the problem (due to Saran\nand Vazirani, Naor and Rabani, and Ravi and Sinha).\n  It has remained an open problem to (a) improve the runtime of exact\nalgorithms, and (b) to get better approximation algorithms. In this paper we\nshow an $O(k^{O(k)} \\, n^{(2\\omega/3 + o(1))k})$-time algorithm for $k$-cut.\nMoreover, we show an $(1+\\epsilon)$-approximation algorithm that runs in time\n$O((k/\\epsilon)^{O(k)} \\,n^{k + O(1)})$, and a $1.81$-approximation in\nfixed-parameter time $O(2^{O(k^2)}\\,\\text{poly}(n))$.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 13:00:22 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 03:09:33 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Gupta", "Anupam", ""], ["Lee", "Euiwoong", ""], ["Li", "Jason", ""]]}, {"id": "1807.08248", "submitter": "Konstantinos Koiliaris", "authors": "Konstantinos Koiliaris and Chao Xu", "title": "Subset Sum Made Simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset Sum is a classical optimization problem taught to undergraduates as an\nexample of an NP-hard problem, which is amenable to dynamic programming,\nyielding polynomial running time if the input numbers are relatively small.\nFormally, given a set $S$ of $n$ positive integers and a target integer $t$,\nthe Subset Sum problem is to decide if there is a subset of $S$ that sums up to\n$t$. Dynamic programming yields an algorithm with running time $O(nt)$.\nRecently, the authors [SODA '17] improved the running time to\n$\\tilde{O}\\bigl(\\sqrt{n}t\\bigr)$, and it was further improved to\n$\\tilde{O}\\bigl(n+t\\bigr)$ by a somewhat involved randomized algorithm by\nBringmann [SODA '17], where $\\tilde{O}$ hides polylogarithmic factors.\n  Here, we present a new and significantly simpler algorithm with running time\n$\\tilde{O}\\bigl(\\sqrt{n}t\\bigr)$. While not the fastest, we believe the new\nalgorithm and analysis are simple enough to be presented in an algorithms\nclass, as a striking example of a divide-and-conquer algorithm that uses FFT to\na problem that seems (at first) unrelated. In particular, the algorithm and its\nanalysis can be described in full detail in two pages (see pages 3-5).\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 06:38:50 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Koiliaris", "Konstantinos", ""], ["Xu", "Chao", ""]]}, {"id": "1807.08331", "submitter": "Christian Konrad", "authors": "Graham Cormode, Jacques Dark, Christian Konrad", "title": "Independent Sets in Vertex-Arrival Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic maximal and maximum independent set problems in three\nmodels of graph streams:\n  In the edge-arrival model we see a stream of edges which collectively define\na graph, this model has been well-studied for a variety of problems. We first\nshow that the space complexity for a one-pass streaming algorithm to find a\nmaximal independent set is quadratic (i.e. we must store all edges). We further\nshow that the problem does not become much easier if we only require\napproximate maximality.\n  In the \"explicit\" vertex stream model, the input stream is a sequence of\nvertices making up the graph, where every vertex arrives along with its\nincident edges that connect to previously arrived vertices. Various graph\nproblems require substantially less space to solve in this setting than for\nedge-arrival streams. We show that every one-pass $c$-approximation algorithm\nfor maximum independent set (MIS) on explicit vertex streams requires space\n$\\Omega(\\frac{n^2}{c^7})$, where $n$ is the number of vertices of the input\ngraph, and it is already known that space $\\tilde{\\Theta}(\\frac{n^2}{c^2})$ is\nnecessary and sufficient in the edge arrival model (Halld\\'orsson et al. 2012).\nThe MIS problem is thus not significantly easier to solve under the explicit\nvertex arrival order assumption. Our result is proved via a reduction to a new\nmulti-party communication problem closely related to pointer jumping.\n  In the \"implicit\" vertex stream model, the input stream consists of a\nsequence of objects, one per vertex. The algorithm is equipped with a function\nthat can map a pair of objects to the presence or absence of an edge, thus\ndefining the graph. This model captures, for example, geometric intersection\ngraphs such as unit disc graphs. Our final set of results consists of several\nimproved upper and lower bounds for ball intersection graphs, in both explicit\nand implicit streams.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 18:07:00 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Cormode", "Graham", ""], ["Dark", "Jacques", ""], ["Konrad", "Christian", ""]]}, {"id": "1807.08463", "submitter": "Benjamin Bergougnoux", "authors": "Thomas Bellitto and Benjamin Bergougnoux", "title": "On Minimum Connecting Transition Sets in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A forbidden transition graph is a graph defined together with a set of\npermitted transitions i.e. unordered pair of adjacent edges that one may use\nconsecutively in a walk in the graph. In this paper, we look for the smallest\nset of transitions needed to be able to go from any vertex of the given graph\nto any other. We prove that this problem is NP-hard and study approximation\nalgorithms. We develop theoretical tools that help to study this problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 07:50:32 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 14:04:14 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Bellitto", "Thomas", ""], ["Bergougnoux", "Benjamin", ""]]}, {"id": "1807.08478", "submitter": "Lior Kamma", "authors": "Alexandr Andoni, Lior Kamma, Robert Krauthgamer, Eric Price", "title": "Batch Sparse Recovery, or How to Leverage the Average Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a \\emph{batch} version of sparse recovery, where the goal is to\nreport a sequence of vectors $A_1',\\ldots,A_m' \\in \\mathbb{R}^n$ that estimate\nunknown signals $A_1,\\ldots,A_m \\in \\mathbb{R}^n$ using a few linear\nmeasurements, each involving exactly one signal vector, under an assumption of\n\\emph{average sparsity}. More precisely, we want to have \\newline\n  $(1) \\;\\;\\; \\sum_{j \\in [m]}{\\|A_j- A_j'\\|_p^p} \\le C \\cdot \\min \\Big\\{\n\\sum_{j \\in [m]}{\\|A_j - A_j^*\\|_p^p} \\Big\\}$\n  for predetermined constants $C \\ge 1$ and $p$, where the minimum is over all\n$A_1^*,\\ldots,A_m^*\\in\\mathbb{R}^n$ that are $k$-sparse on average. We assume\n$k$ is given as input, and ask for the minimal number of measurements required\nto satisfy $(1)$. The special case $m=1$ is known as stable sparse recovery and\nhas been studied extensively.\n  We resolve the question for $p =1$ up to polylogarithmic factors, by\npresenting a randomized adaptive scheme that performs $\\tilde{O}(km)$\nmeasurements and with high probability has output satisfying $(1)$, for\narbitrarily small $C > 1$. Finally, we show that adaptivity is necessary for\nevery non-trivial scheme.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:31:23 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Andoni", "Alexandr", ""], ["Kamma", "Lior", ""], ["Krauthgamer", "Robert", ""], ["Price", "Eric", ""]]}, {"id": "1807.08543", "submitter": "Noam Touitou", "authors": "Yossi Azar, Ashish Chiplunkar, Shay Kutten, Noam Touitou", "title": "Set Cover with Delay -- Clairvoyance is not Required", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most online problems with delay, clairvoyance (i.e. knowing the future\ndelay of a request upon its arrival) is required for polylogarithmic\ncompetitiveness. In this paper, we show that this is not the case for set cover\nwith delay (SCD) -- specifically, we present the first non-clairvoyant\nalgorithm, which is $O(\\log n \\log m)$-competitive, where $n$ is the number of\nelements and $m$ is the number of sets. This matches the best known result for\nthe classic online set cover (a special case of non-clairvoyant SCD). Moreover,\nclairvoyance does not allow for significant improvement - we present lower\nbounds of $\\Omega(\\sqrt{\\log n})$ and $\\Omega(\\sqrt{\\log m})$ for SCD which\napply for the clairvoyant case. In addition, the competitiveness of our\nalgorithm does not depend on the number of requests. Such a guarantee on the\nsize of the universe alone was not previously known even for the clairvoyant\ncase - the only previously-known algorithm (due to Carrasco et al.) is\nclairvoyant, with competitiveness that grows with the number of requests. For\nthe special case of vertex cover with delay, we show a simpler, deterministic\nalgorithm which is $3$-competitive (and also non-clairvoyant).\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 11:43:53 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 12:39:38 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 21:12:47 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Azar", "Yossi", ""], ["Chiplunkar", "Ashish", ""], ["Kutten", "Shay", ""], ["Touitou", "Noam", ""]]}, {"id": "1807.08579", "submitter": "Xin Han", "authors": "Xin Han, Liang Zhao, Zhishan Guo, Xingwu Liu", "title": "An Improved Speedup Factor for Sporadic Tasks with Constrained Deadlines\n  under Dynamic Priority Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schedulability is a fundamental problem in real-time scheduling, but it has\nto be approximated due to the intrinsic computational hardness. As the most\npopular algorithm for deciding schedulability on multiprocess platforms, the\nspeedup factor of partitioned-EDF is challenging to analyze and is far from\nbeen determined. Partitioned-EDF was first proposed in 2005 by Barush and\nFisher [1], and was shown to have a speedup factor at most 3-1/m, meaning that\nif the input of sporadic tasks is feasible on m processors with speed one,\npartitioned-EDF will always return succeeded on m processors with speed 3-1/m.\nIn 2011, this upper bound was improved to 2.6322-1/m by Chen and Chakraborty\n[2], and no more improvements have appeared ever since then. In this paper, we\ndevelop a novel method to discretize and regularize sporadic tasks, which\nenables us to improve, in the case of constrained deadlines, the speedup factor\nof partitioned-EDF to 2.5556-1/m, very close to the asymptotic lower bound 2.5\nin [2].\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 07:46:32 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Han", "Xin", ""], ["Zhao", "Liang", ""], ["Guo", "Zhishan", ""], ["Liu", "Xingwu", ""]]}, {"id": "1807.08678", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri and Kent Quanrud", "title": "Submodular Function Maximization in Parallel via the Multilinear\n  Relaxation", "comments": "SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balkanski and Singer [5] recently initiated the study of adaptivity (or\nparallelism) for constrained submodular function maximization, and studied the\nsetting of a cardinality constraint. Very recent improvements for this problem\nby Balkanski, Rubinstein, and Singer [6] and Ene and Nguyen [21] resulted in a\nnear-optimal $(1-1/e-\\epsilon)$-approximation in $O(\\log n/\\epsilon^2)$ rounds\nof adaptivity. Partly motivated by the goal of extending these results to more\ngeneral constraints, we describe parallel algorithms for approximately\nmaximizing the multilinear relaxation of a monotone submodular function subject\nto packing constraints. Formally our problem is to maximize $F(x)$ over $x \\in\n[0,1]^{n}$ subject to $Ax \\le 1$ where $F$ is the multilinear relaxation of a\nmonotone submodular function. Our algorithm achieves a near-optimal\n$(1-1/e-\\epsilon)$-approximation in $O(\\log^2 m \\log n/\\epsilon^4)$ rounds\nwhere $n$ is the cardinality of the ground set and $m$ is the number of packing\nconstraints. For many constraints of interest, the resulting fractional\nsolution can be rounded via known randomized rounding schemes that are\noblivious to the specific submodular function. We thus derive randomized\nalgorithms with poly-logarithmic adaptivity for a number of constraints\nincluding partition and laminar matroids, matchings, knapsack constraints, and\ntheir intersections.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 15:36:49 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 19:24:15 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""]]}, {"id": "1807.08738", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki", "title": "Random Sampling Applied to the MST Problem in the Node Congested Clique\n  Model", "comments": "simplified and corrected version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Congested Clique model proposed by Lotker et al.[SICOMP'05] was\nintroduced in order to provide a simple abstraction for overlay networks.\nCongested Clique is a model of distributed (or parallel) computing, in which\nthere are $n$ players with unique identifiers from set [n], which perform\ncomputations in synchronous rounds. Each round consists of the phase of\nunlimited local computation and the communication phase. While communicating,\neach pair of players is allowed to exchange a single message of size $O(\\log\nn)$ bits.\n  Since, in a single round, each player can communicate with even $\\Theta(n)$\nother players, the model seems to be to powerful to imitate bandwidth\nrestriction emerging from the underlying network. In this paper we study a\nrestricted version of the Congested Clique model, the Node Congested Clique\n(NCC) model, proposed by Augustine et al.[arxiv1805], in which a player is\nallowed to send/receive only $O(\\log n)$ messages per communication phase.\n  More precisely, we provide communication primitives that improve the round\ncomplexity of the MST algorithm by Augustine et al. [arxiv1805] to $O(\\log^3\nn)$ rounds, and give an $O(\\log^2 n)$ round algorithm solving the Spanning\nForest (SF) problem. Furthermore, we present an approach based on the random\nsampling technique by Karger et al.[JACM'95] that gives an $O(\\log^2 n \\log\n\\Delta / \\log \\log n)$ round algorithm for the Minimum Spanning Forest (MSF)\nproblem. Besides the faster SF/ MSF algorithms we consider the key\ncontributions to be\n  - an efficient implementation of basic protocols in the NCC model\n  - a tighter analysis of a special case of the sampling approach by Karger et\nal.[JACM'95] and related results by Pemmaraju and Sardeshmukh [FSTTCS'16]\n  - efficient k-sparse recovery data structure that requires $O((k +\\log n)\\log\nn\\log k)$ bits and provides recovery procedure that requires $O((k +\\log n)\\log\nk)$ steps\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 17:35:28 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 14:38:16 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Nowicki", "Krzysztof", ""]]}, {"id": "1807.08745", "submitter": "Krzysztof Onak", "authors": "Krzysztof Onak", "title": "Round Compression for Parallel Graph Algorithms in Strongly Sublinear\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massive Parallel Computation (MPC) model is a theoretical framework for\npopular parallel and distributed platforms such as MapReduce, Hadoop, or Spark.\nWe consider the task of computing a large matching or small vertex cover in\nthis model when the space per machine is $n^\\delta$ for $\\delta \\in (0,1)$,\nwhere $n$ is the number of vertices in the input graph. A direct simulation of\nclassic PRAM and distributed algorithms from the 1980s results in algorithms\nthat require at least a logarithmic number of MPC rounds. We give the first\nalgorithm that breaks this logarithmic barrier and runs in $\\tilde O(\\sqrt{\\log\nn})$ rounds, as long as the total space is at least slightly superlinear in the\nnumber of vertices.\n  The result is obtained by repeatedly compressing several rounds of a natural\npeeling algorithm to a logarithmically smaller number of MPC rounds. Each time\nwe show that it suffices to consider a low-degree subgraph, in which local\nneighborhoods can be explored with exponential speedup. Our techniques are\nrelatively simple and can also be used to accelerate the simulation of\ndistributed algorithms for bounded-degree graphs and finding a maximal\nindependent set in bounded-arboricity graphs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 17:49:03 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Onak", "Krzysztof", ""]]}, {"id": "1807.08777", "submitter": "Jonathan Sorenson", "authors": "Jonathan P. Sorenson and Jonathan Webster", "title": "Two Algorithms to Find Primes in Patterns", "comments": null, "journal-ref": null, "doi": "10.1090/mcom/3501", "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $k\\ge 1$ be an integer, and let $P= (f_1(x), \\ldots, f_k(x) )$ be $k$\nadmissible linear polynomials over the integers, or \\textit{the pattern}. We\npresent two algorithms that find all integers $x$ where $\\max{ \\{f_i(x) \\} }\n\\le n$ and all the $f_i(x)$ are prime.\n  Our first algorithm takes at most $O_P(n/(\\log\\log n)^k)$ arithmetic\noperations using $O(k\\sqrt{n})$ space.\n  Our second algorithm takes slightly more time, $O_P(n/(\\log \\log n)^{k-1})$\narithmetic operations, but uses only $n^{1/c}$ space for a constant $c>2$. We\nprove correctness unconditionally, but the running time relies on two unproven\nbut reasonable conjectures.\n  We are unaware of any previous complexity results for this problem beyond the\nuse of a prime sieve. We also implemented several parallel versions of our\nsecond algorithm to show it is viable in practice. In particular, we found some\nnew Cunningham chains of length 15, and we found all quadruplet primes up to\n$10^{17}$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 18:30:25 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 15:17:46 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 17:41:30 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sorenson", "Jonathan P.", ""], ["Webster", "Jonathan", ""]]}, {"id": "1807.08824", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "A Deterministic Distributed Algorithm for Weighted All Pairs Shortest\n  Paths Through Pipelining", "comments": "Minor correction in Citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new pipelined approach to compute all pairs shortest paths\n(APSP) in a directed graph with nonnegative integer edge weights (including\nzero weights) in the CONGEST model in the distributed setting. Our\ndeterministic distributed algorithm computes shortest paths of distance at most\n$\\Delta$ for all pairs of vertices in at most $2 n \\sqrt{\\Delta} + 2n$ rounds,\nand more generally, it computes h-hop shortest paths for k sources in\n$2\\sqrt{nkh} + n + k$ rounds. The algorithm is simple, and it has some novel\nfeatures and a nontrivial analysis.It uses only the directed edges in the graph\nfor communication. This algorithm can be used as a base within asymptotically\nfaster algorithms that match or improve on the current best deterministic bound\nof $\\tilde{O}(n^{3/2})$ rounds for this problem when edge weights are $O(n)$ or\nshortest path distances are $\\tilde{O}(n^{3/2})$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 20:51:42 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 15:48:52 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 20:21:14 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1807.08886", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Yu Chen, Sanjeev Khanna", "title": "Sublinear Algorithms for $(\\Delta + 1)$ Vertex Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any graph with maximum degree $\\Delta$ admits a proper vertex coloring with\n$\\Delta + 1$ colors that can be found via a simple sequential greedy algorithm\nin linear time and space. But can one find such a coloring via a sublinear\nalgorithm?\n  We answer this fundamental question in the affirmative for several canonical\nclasses of sublinear algorithms including graph streaming, sublinear time, and\nmassively parallel computation (MPC) algorithms. In particular, we design:\n  * A single-pass semi-streaming algorithm in dynamic streams using\n$\\tilde{O}(n)$ space. The only known semi-streaming algorithm prior to our work\nwas a folklore O(log n)-pass algorithm obtained by simulating classical\ndistributed algorithms in the streaming model.\n  * A sublinear-time algorithm in the standard query model that allows neighbor\nqueries and pair queries using $\\tilde{O}(n\\sqrt{n})$ time. We further show\nthat any algorithm that outputs a valid coloring with sufficiently large\nconstant probability requires $\\Omega(n\\sqrt{n})$ time. No non-trivial\nsublinear time algorithms were known prior to our work.\n  * A parallel algorithm in the massively parallel computation (MPC) model\nusing $\\tilde{O}(n)$ memory per machine and $O(1)$ MPC rounds. Our number of\nrounds significantly improves upon the recent\n$O(\\log\\log{\\Delta}\\cdot\\log^*{(n)})$-round algorithm of Parter [ICALP 2018].\n  At the core of our results is a remarkably simple meta-algorithm for the\n$(\\Delta+1)$ coloring problem: Sample $O(\\log{n})$ colors for each vertex from\nthe $\\Delta+1$ colors; find a proper coloring of the graph using only the\nsampled colors. We prove that the sampled set of colors with high probability\ncontains a proper coloring of the input graph. The sublinear algorithms are\nthen obtained by designing efficient algorithms for finding a proper coloring\nof the graph from the sampled colors in the corresponding models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 02:50:50 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 18:10:56 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Assadi", "Sepehr", ""], ["Chen", "Yu", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1807.08949", "submitter": "Jiehua Chen", "authors": "Jiehua Chen and Danny Hermelin and Manuel Sorge", "title": "A Note on Clustering Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the clustering aggregation problem in which we are given a set of\nclusterings and want to find an aggregated clustering which minimizes the sum\nof mismatches to the input clusterings. In the binary case (each clustering is\na bipartition) this problem was known to be NP-hard under Turing reduction. We\nstrengthen this result by providing a polynomial-time many-one reduction. Our\nresult also implies that no $2^{o(n)} \\cdot |I|^{O(1)}$-time algorithm exists\nfor any clustering instance $I$ with $n$ elements, unless the Exponential Time\nHypothesis fails. On the positive side, we show that the problem is\nfixed-parameter tractable with respect to the number of input clusterings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 08:15:08 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Chen", "Jiehua", ""], ["Hermelin", "Danny", ""], ["Sorge", "Manuel", ""]]}, {"id": "1807.09091", "submitter": "Raffaele Marino", "authors": "Raffaele Marino and Scott Kirkpatrick", "title": "Revisiting the Challenges of MaxClique", "comments": "17 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MaxClique problem, finding the largest complete subgraph in an\nErd{\\\"o}s-R{\\'e}nyi $G(N,p)$ random graph in the large $N$ limit, is a\nwell-known example of a simple problem for which finding any approximate\nsolution within a factor of $2$ of the known, probabilistically determined\nlimit, appears to require P$=$NP. This type of search has practical importance\nin very large graphs. Algorithmic approaches run into phase boundaries long\nbefore they reach the size of the largest likely solutions. And, most\nintriguing, there is an extensive literature of \\textit{challenges} posed for\nconcrete methods of finding maximum naturally occurring as well as artificially\nhidden cliques, with computational costs that are at most polynomial in the\nsize of the problem.\n  We use the probabilistic approach in a novel way to provide a more insightful\ntest of constructive algorithms for this problem. We show that extensions of\nexisting methods of greedy local search will be able to meet the\n\\textit{challenges} for practical problems of size $N$ as large as $10^{10}$\nand perhaps more. Experiments with spectral methods that treat a single large\nclique of size $\\alpha N^{1/2}$ \\textit{planted} in the graph as an impurity\nlevel in a tight binding energy band show that such a clique can be detected\nwhen $\\alpha \\geq \\approx1.0$. Belief propagation using a recent\n\\textit{approximate message passing} (\\textbf{AMP}) scheme of inference pushes\nthis limit down to $\\alpha \\sim \\sqrt{1/e}$. Exhaustive local search (with\nearly stopping when the planted clique is found) does even better on problems\nof practical size, and proves to be the fastest solution method for this\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 13:22:01 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 09:11:36 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 14:56:06 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 06:34:19 GMT"}, {"version": "v5", "created": "Wed, 8 May 2019 14:47:30 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Marino", "Raffaele", ""], ["Kirkpatrick", "Scott", ""]]}, {"id": "1807.09129", "submitter": "Heng Guo", "authors": "Heng Guo, Chao Liao, Pinyan Lu, Chihao Zhang", "title": "Zeros of Holant problems: locations and algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fully polynomial-time (deterministic or randomised) approximation\nschemes for Holant problems, defined by a non-negative constraint function\nsatisfying a generalised second order recurrence modulo a couple of exceptional\ncases. As a consequence, any non-negative Holant problem on cubic graphs has an\nefficient approximation algorithm unless the problem is equivalent to\napproximately counting perfect matchings, a central open problem in the area.\nThis is in sharp contrast to the computational phase transition shown by\n2-state spin systems on cubic graphs. Our main technique is the recently\nestablished connection between zeros of graph polynomials and approximate\ncounting. We also use the \"winding\" technique to deduce the second result on\ncubic graphs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:03:05 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 10:42:13 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Guo", "Heng", ""], ["Liao", "Chao", ""], ["Lu", "Pinyan", ""], ["Zhang", "Chihao", ""]]}, {"id": "1807.09302", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Michael Mitzenmacher", "title": "Metric Sublinear Algorithms via Linear Sampling", "comments": "FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide a new technique to design fast approximation\nalgorithms for graph problems where the points of the graph lie in a metric\nspace. Specifically, we present a sampling approach for such metric graphs\nthat, using a sublinear number of edge weight queries, provides a {\\em linear\nsampling}, where each edge is (roughly speaking) sampled proportionally to its\nweight.\n  For several natural problems, such as densest subgraph and max cut among\nothers, we show that by sparsifying the graph using this sampling process, we\ncan run a suitable approximation algorithm on the sparsified graph and the\nresult remains a good approximation for the original problem. Our results have\nseveral interesting implications, such as providing the first sublinear time\napproximation algorithm for densest subgraph in a metric space, and improving\nthe running time of estimating the average distance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 18:41:00 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1807.09313", "submitter": "Kevin Kremer", "authors": "Lars Nagel, Tim S\\\"u{\\ss}, Kevin Kremer, M. Umar Hameed, Lingfang\n  Zeng, Andr\\'e Brinkmann", "title": "Time-efficient Garbage Collection in SSDs", "comments": "12 pages (excluding references), 13 pages (including references), 13\n  figures, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SSDs are currently replacing magnetic disks in many application areas. A\nchallenge of the underlying flash technology is that data cannot be updated\nin-place. A block consisting of many pages must be completely erased before a\nsingle page can be rewritten. This victim block can still contain valid pages\nwhich need to be copied to other blocks before erasure. The objective of\ngarbage collection strategies is to minimize write amplification induced by\ncopying valid pages from victim blocks while minimizing the performance\noverhead of the victim selection. Victim selection strategies minimizing write\namplification, like the cost-benefit approach, have linear runtime, while the\nwrite amplifications of time-efficient strategies, like the greedy strategy,\nsignificantly reduce the lifetime of SSDs. In this paper, we propose two\nstrategies which optimize the performance of cost-benefit, while (almost)\npreserving its write amplification. Trace-driven simulations for single- and\nmulti-channel SSDs show that the optimizations help to keep the write\namplification low while improving the runtime by up to 24-times compared to the\noriginal cost-benefit strategy, so that the new strategies can be used in\nmulti-TByte SSDs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 19:22:25 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Nagel", "Lars", ""], ["S\u00fc\u00df", "Tim", ""], ["Kremer", "Kevin", ""], ["Hameed", "M. Umar", ""], ["Zeng", "Lingfang", ""], ["Brinkmann", "Andr\u00e9", ""]]}, {"id": "1807.09358", "submitter": "Lifeng Zhou", "authors": "Lifeng Zhou and Pratap Tokekar", "title": "An Approximation Algorithm for Risk-averse Submodular Optimization", "comments": "Whole version for WAFR, 2018 final submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of incorporating risk while making combinatorial\ndecisions under uncertainty. We formulate a discrete submodular maximization\nproblem for selecting a set using Conditional-Value-at-Risk (CVaR), a risk\nmetric commonly used in financial analysis. While CVaR has recently been used\nin optimization of linear cost functions in robotics, we take the first stages\ntowards extending this to discrete submodular optimization and provide several\npositive results. Specifically, we propose the Sequential Greedy Algorithm that\nprovides an approximation guarantee on finding the maxima of the CVaR cost\nfunction under a matroidal constraint. The approximation guarantee shows that\nthe solution produced by our algorithm is within a constant factor of the\noptimal and an additive term that depends on the optimal. Our analysis uses the\ncurvature of the submodular set function, and proves that the algorithm runs in\npolynomial time. This formulates a number of combinatorial optimization\nproblems that appear in robotics. We use two such problems, vehicle assignment\nunder uncertainty for mobility-on-demand and sensor selection with failures for\nenvironmental monitoring, as case studies to demonstrate the efficacy of our\nformulation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 21:10:50 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 18:40:59 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Zhou", "Lifeng", ""], ["Tokekar", "Pratap", ""]]}, {"id": "1807.09386", "submitter": "Max Simchowitz", "authors": "Max Simchowitz", "title": "On the Randomized Complexity of Minimizing a Convex Quadratic Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing a convex, quadratic objective of the form\n$f_{\\mathbf{A},\\mathbf{b}}(x) := \\frac{1}{2}x^\\top \\mathbf{A} x - \\langle\n\\mathbf{b}, x \\rangle$ for $\\mathbf{A} \\succ 0 $ is a fundamental problem in\nmachine learning and optimization. In this work, we prove gradient-query\ncomplexity lower bounds for minimizing convex quadratic functions which apply\nto both deterministic and \\emph{randomized} algorithms. Specifically, for\n$\\kappa > 1$, we exhibit a distribution over $(\\mathbf{A},\\mathbf{b})$ with\ncondition number $\\mathrm{cond}(\\mathbf{A}) \\le \\kappa$, such that any\n\\emph{randomized} algorithm requires $\\Omega(\\sqrt{\\kappa})$ gradient queries\nto find a solution $\\hat x$ for which $\\|\\hat x - \\mathbf x_\\star\\| \\le\n\\epsilon_0\\|\\mathbf{x}_{\\star}\\|$, where $\\mathbf x_{\\star} =\n\\mathbf{A}^{-1}\\mathbf{b}$ is the optimal solution, and $\\epsilon_0$ a small\nconstant. Setting $\\kappa =1/\\epsilon$, this lower bound implies the minimax\nrate of $T = \\Omega(\\lambda_1(\\mathbf{A})\\|\\mathbf\nx_\\star\\|^2/\\sqrt{\\epsilon})$ queries required to minimize an arbitrary convex\nquadratic function up to error $f(\\hat{x}) - f(\\mathbf x_\\star) \\le \\epsilon$.\n  Our lower bound holds for a distribution derived from classical ensembles in\nrandom matrix theory, and relies on a careful reduction from adaptively\nestimating a planted vector $\\mathbf u$ in a deformed Wigner model. A key step\nin deriving sharp lower bounds is demonstrating that the optimization error\n$\\mathbf x_\\star - \\hat x$ cannot align too closely with $\\mathbf{u}$. To this\nend, we prove an upper bound on the cosine between $\\mathbf x_\\star - \\hat x$\nand $\\mathbf u$ in terms of the MMSE of estimating the plant $\\mathbf u$ in a\ndeformed Wigner model. We then bound the MMSE by carefully modifying a result\ndue to Lelarge and Miolane 2016, which rigorously establishes a general\nreplica-symmetric formula for planted matrix models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 23:23:49 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 02:25:18 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 21:21:52 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 18:41:14 GMT"}, {"version": "v5", "created": "Wed, 8 Aug 2018 18:58:11 GMT"}, {"version": "v6", "created": "Sun, 23 Sep 2018 22:21:22 GMT"}, {"version": "v7", "created": "Tue, 16 Apr 2019 08:11:37 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Simchowitz", "Max", ""]]}, {"id": "1807.09389", "submitter": "Haris Angelidakis", "authors": "Haris Angelidakis", "title": "Shortest path queries, graph partitioning and covering problems in worst\n  and beyond worst case settings", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we design algorithms for several NP-hard problems in both\nworst and beyond worst case settings. In the first part of the thesis, we apply\nthe traditional worst case methodology and design approximation algorithms for\nthe Hub Labeling problem; Hub Labeling is a preprocessing technique introduced\nto speed up shortest path queries. Before this work, Hub Labeling had been\nextensively studied mainly in the beyond worst case analysis setting, and in\nparticular on graphs with low highway dimension. In this work, we significantly\nimprove our theoretical understanding of the problem and design (worst-case)\nalgorithms for various classes of graphs, such as general graphs, graphs with\nunique shortest paths and trees, as well as provide matching inapproximability\nlower bounds for the problem in its most general settings. Finally, we\ndemonstrate a connection between computing a Hub Labeling on a tree and\nsearching for a node in a tree.\n  In the second part of the thesis, we turn to beyond worst case analysis and\nextensively study the stability model introduced by Bilu and Linial in an\nattempt to describe real-life instances of graph partitioning and clustering\nproblems. Informally, an instance of a combinatorial optimization problem is\nstable if it has a unique optimal solution that remains the unique optimum\nunder small multiplicative perturbations of the parameters of the input.\nUtilizing the power of convex relaxations for stable instances, we obtain\nseveral results for problems such as Edge/Node Multiway Cut, Independent Set\n(and its equivalent, in terms of exact solvability, Vertex Cover), clustering\nproblems such as $k$-center and $k$-median and the symmetric Traveling Salesman\nproblem. We also provide strong lower bounds for certain families of algorithms\nfor covering problems, thus exhibiting potential barriers towards the design of\nimproved algorithms in this framework.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 23:29:46 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Angelidakis", "Haris", ""]]}, {"id": "1807.09405", "submitter": "Alfredo Torrico", "authors": "Sebastian Pokutta and Mohit Singh and Alfredo Torrico", "title": "Efficient algorithms for robust submodular maximization under matroid\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider robust submodular maximization with matroid\nconstraints. We give an efficient bi-criteria approximation algorithm that\noutputs a small family of feasible sets whose union has (nearly) optimal\nobjective value. This algorithm theoretically performs less function calls than\nprevious works at cost of adding more elements to the final solution. We also\nprovide significant implementation improvements showing that our algorithm\noutperforms the algorithms in the existing literature. We finally assess the\nperformance of our contributions in three real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 01:09:50 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Pokutta", "Sebastian", ""], ["Singh", "Mohit", ""], ["Torrico", "Alfredo", ""]]}, {"id": "1807.09417", "submitter": "Apurba Das", "authors": "Apurba Das and Seyed-Vahid Sanei-Mehri and Srikanta Tirthapura", "title": "Shared-Memory Parallel Maximal Clique Enumeration", "comments": "10 pages, 3 figures, proceedings of the 25th IEEE International\n  Conference on. High Performance Computing, Data, and Analytics (HiPC), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present shared-memory parallel methods for Maximal Clique Enumeration\n(MCE) from a graph. MCE is a fundamental and well-studied graph analytics task,\nand is a widely used primitive for identifying dense structures in a graph. Due\nto its computationally intensive nature, parallel methods are imperative for\ndealing with large graphs. However, surprisingly, there do not yet exist\nscalable and parallel methods for MCE on a shared-memory parallel machine. In\nthis work, we present efficient shared-memory parallel algorithms for MCE, with\nthe following properties: (1) the parallel algorithms are provably\nwork-efficient relative to a state-of-the-art sequential algorithm (2) the\nalgorithms have a provably small parallel depth, showing that they can scale to\na large number of processors, and (3) our implementations on a multicore\nmachine shows a good speedup and scaling behavior with increasing number of\ncores, and are substantially faster than prior shared-memory parallel\nalgorithms for MCE.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 02:35:48 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Das", "Apurba", ""], ["Sanei-Mehri", "Seyed-Vahid", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1807.09483", "submitter": "Marcel Radermacher", "authors": "Almut Demel, Dominik D\\\"urrschnabel, Tamara Mchedlidze, Marcel\n  Radermacher, Lasse Wulf", "title": "A Greedy Heuristic for Crossing-Angle Maximization", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crossing angle of a straight-line drawing $\\Gamma$ of a graph $G=(V, E)$\nis the smallest angle between two crossing edges in $\\Gamma$. Deciding whether\na graph $G$ has a straight-line drawing with a crossing angle of $90^\\circ$ is\n$\\mathcal NP$-hard. We propose a simple heuristic to compute a drawing with a\nlarge crossing angle. The heuristic greedily selects the best position for a\nsingle vertex in a random set of points. The algorithm is accompanied by a\nspeed-up technique to compute the crossing angle of a straight-line drawing. We\nshow the effectiveness of the heuristic in an extensive empirical evaluation.\nOur heuristic was clearly the winning algorithm (CoffeeVM) in the Graph Drawing\nChallenge 2017.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 08:43:03 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 09:42:41 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Demel", "Almut", ""], ["D\u00fcrrschnabel", "Dominik", ""], ["Mchedlidze", "Tamara", ""], ["Radermacher", "Marcel", ""], ["Wulf", "Lasse", ""]]}, {"id": "1807.09524", "submitter": "Lukas Gianinazzi", "authors": "Barbara Geissmann, Lukas Gianinazzi", "title": "Parallel Minimum Cuts in Near-linear Work and Low Depth", "comments": null, "journal-ref": "SPAA '18: 30th ACM Symposium on Parallelism in Algorithms and\n  Architectures, July 16-18, 2018, Vienna, Austria. ACM, New York, NY, USA", "doi": "10.1145/3210377.3210393", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first near-linear work and poly-logarithmic depth algorithm\nfor computing a minimum cut in a graph, while previous parallel algorithms with\npoly-logarithmic depth required at least quadratic work in the number of\nvertices. In a graph with $n$ vertices and $m$ edges, our algorithm computes\nthe correct result with high probability in $O(m {\\log}^4 n)$ work and\n$O({\\log}^3 n)$ depth. This result is obtained by parallelizing a data\nstructure that aggregates weights along paths in a tree and by exploiting the\nconnection between minimum cuts and approximate maximum packings of spanning\ntrees. In addition, our algorithm improves upon bounds on the number of cache\nmisses incurred to compute a minimum cut.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 10:48:03 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 12:51:01 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 15:29:49 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Geissmann", "Barbara", ""], ["Gianinazzi", "Lukas", ""]]}, {"id": "1807.09686", "submitter": "Tom Morgan", "authors": "Michael Mitzenmacher, Tom Morgan", "title": "Directory Reconciliation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the theoretical study of directory reconciliation, a\ngeneralization of document exchange, in which Alice and Bob each have different\nversions of a set of documents that they wish to synchronize. This problem is\ndesigned to capture the setting of synchronizing different versions of file\ndirectories, while allowing for changes of file names and locations without\nsignificant expense. We present protocols for efficiently solving directory\nreconciliation based on a reduction to document exchange under edit distance\nwith block moves, as well as protocols combining techniques for reconciling\nsets of sets with document exchange protocols. Along the way, we develop a new\nprotocol for document exchange under edit distance with block moves inspired by\nnoisy binary search in graphs, which uses only $O(k \\log n)$ bits of\ncommunication at the expense of $O(k \\log n)$ rounds of communication.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:04:01 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Morgan", "Tom", ""]]}, {"id": "1807.09694", "submitter": "Tom Morgan", "authors": "Michael Mitzenmacher, Tom Morgan", "title": "Robust Set Reconciliation via Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider variations of set reconciliation problems where two parties,\nAlice and Bob, each hold a set of points in a metric space, and the goal is for\nBob to conclude with a set of points that is close to Alice's set of points in\na well-defined way. This setting has been referred to as robust set\nreconciliation. More specifically, in one variation we examine the goal is for\nBob to end with a set of points that is close to Alice's in earth mover's\ndistance, and in another the goal is for Bob to have a point that is close to\neach of Alice's. The first problem has been studied before; our results scale\nbetter with the dimension of the space. The second problem appears new.\n  Our primary novelty is utilizing Invertible Bloom Lookup Tables in\ncombination with locality sensitive hashing. This combination allows us to cope\nwith the geometric setting in a communication-efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:13:48 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Morgan", "Tom", ""]]}, {"id": "1807.09735", "submitter": "Tam\\'as Kir\\'aly", "authors": "Krist\\'of B\\'erczi, Karthekeyan Chandrasekaran, Tam\\'as Kir\\'aly,\n  Vivek Madan", "title": "Improving the Integrality Gap for Multiway Cut", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multiway cut problem, we are given an undirected graph with\nnon-negative edge weights and a collection of $k$ terminal nodes, and the goal\nis to partition the node set of the graph into $k$ non-empty parts each\ncontaining exactly one terminal so that the total weight of the edges crossing\nthe partition is minimized. The multiway cut problem for $k\\ge 3$ is APX-hard.\nFor arbitrary $k$, the best-known approximation factor is $1.2965$ due to\n[Sharma and Vondr\\'{a}k, 2014] while the best known inapproximability factor is\n$1.2$ due to [Angelidakis, Makarychev and Manurangsi, 2017]. In this work, we\nimprove on the lower bound to $1.20016$ by constructing an integrality gap\ninstance for the CKR relaxation.\n  A technical challenge in improving the gap has been the lack of geometric\ntools to understand higher-dimensional simplices. Our instance is a non-trivial\n$3$-dimensional instance that overcomes this technical challenge. We analyze\nthe gap of the instance by viewing it as a convex combination of\n$2$-dimensional instances and a uniform 3-dimensional instance. We believe that\nthis technique could be exploited further to construct instances with larger\nintegrality gap. One of the ingredients of our proof technique is a\ngeneralization of a result on \\emph{Sperner admissible labelings} due to\n[Mirzakhani and Vondr\\'{a}k, 2015] that might be of independent combinatorial\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 17:28:44 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 17:59:52 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Chandrasekaran", "Karthekeyan", ""], ["Kir\u00e1ly", "Tam\u00e1s", ""], ["Madan", "Vivek", ""]]}, {"id": "1807.09885", "submitter": "Janardhan Kulkarni", "authors": "Uriel Feige, Janardhan Kulkarni, Shi Li", "title": "A Polynomial Time Constant Approximation For Minimizing Total Weighted\n  Flow-time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic scheduling problem of minimizing the total weighted\nflow-time on a single machine (min-WPFT), when preemption is allowed. In this\nproblem, we are given a set of $n$ jobs, each job having a release time $r_j$,\na processing time $p_j$, and a weight $w_j$. The flow-time of a job is defined\nas the amount of time the job spends in the system before it completes; that\nis, $F_j = C_j - r_j$, where $C_j$ is the completion time of job. The objective\nis to minimize the total weighted flow-time of jobs.\n  This NP-hard problem has been studied quite extensively for decades. In a\nrecent breakthrough, Batra, Garg, and Kumar presented a {\\em pseudo-polynomial}\ntime algorithm that has an $O(1)$ approximation ratio. The design of a truly\npolynomial time algorithm, however, remained an open problem. In this paper, we\nshow a transformation from pseudo-polynomial time algorithms to polynomial time\nalgorithms in the context of min-WPFT. Our result combined with the result of\nBatra, Garg, and Kumar settles the long standing conjecture that there is a\npolynomial time algorithm with $O(1)$-approximation for min-WPFT.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 22:29:51 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Feige", "Uriel", ""], ["Kulkarni", "Janardhan", ""], ["Li", "Shi", ""]]}, {"id": "1807.09898", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi and Luca Trevisan", "title": "Mildly Exponential Time Approximation Algorithms for Vertex Cover,\n  Uniform Sparsest Cut and Related Problems", "comments": "An extended abstract of this work will appear in APPROX'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the trade-off between the running time of\napproximation algorithms and their approximation guarantees. By leveraging a\nstructure of the `hard' instances of the Arora-Rao-Vazirani lemma [JACM'09], we\nshow that the Sum-of-Squares hierarchy can be adapted to provide `fast', but\nstill exponential time, approximation algorithms for several problems in the\nregime where they are believed to be NP-hard. Specifically, our framework\nyields the following algorithms; here $n$ denote the number of vertices of the\ngraph and $r$ can be any positive real number greater than 1 (possibly\ndepending on $n$).\n  (i) A $\\left(2 - \\frac{1}{O(r)}\\right)$-approximation algorithm for Vertex\nCover that runs in $\\exp\\left(\\frac{n}{2^{r^2}}\\right)n^{O(1)}$ time.\n  (ii) An $O(r)$-approximation algorithms for Uniform Sparsest Cut, Balanced\nSeparator, Minimum UnCut and Minimum 2CNF Deletion that runs in\n$\\exp\\left(\\frac{n}{2^{r^2}}\\right)n^{O(1)}$ time.\n  Our algorithm for Vertex Cover improves upon Bansal et al.'s algorithm\n[arXiv:1708.03515] which achieves $\\left(2 -\n\\frac{1}{O(r)}\\right)$-approximation in time\n$\\exp\\left(\\frac{n}{r^r}\\right)n^{O(1)}$. For the remaining problems, our\nalgorithms improve upon $O(r)$-approximation\n$\\exp\\left(\\frac{n}{2^r}\\right)n^{O(1)}$-time algorithms that follow from a\nwork of Charikar et al. [SIAM J. Comput.'10].\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 23:26:43 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Manurangsi", "Pasin", ""], ["Trevisan", "Luca", ""]]}, {"id": "1807.10147", "submitter": "Christophe Dumora", "authors": "Christophe Dumora (LaBRI, IMB), David Auber (LaBRI), J\\'er\\'emie Bigot\n  (IMB), Vincent Couallier (IMB), Cyril Leclerc (LyRE)", "title": "Data-Oriented Algorithm for Real-Time Estimation of Flow Rates and Flow\n  Directions in a Water Distribution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present how data collected from a water\ndistribution network (WDN) can be used to reconstruct flow rate and flow\ndirection all over the network to enhance knowledge and detection of unforeseen\nevents. The methodological approach consists in modeling the WDN and all\navailable sensor data related to the management of such a network in the form\nof a flow network graph G = (V, E, s, t, c), with V a set of nodes, E a set of\nedges whose elements are ordered pairs of distinct nodes, s a source node, t a\nsink node and c a capacity function on edges. Our objective is to reconstruct a\nreal-valued function f(u,v): VxV => R on all the edges E in VxV from partial\nobservations on a small number of nodes V = {1, ..., n}. This reconstruction\nmethod consists in a data-driven Ford-Fulkerson maximum-flow problem in a\nmulti-source, multi-sink context using a constrained bidirectional\nbreadth-first search based on Edmonds-Karp method. The innovative approach is\nits application in the context of smart cities to operate from sensor data,\nstructural data from a geographical information system (GIS) and consumption\nestimates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 07:10:18 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Dumora", "Christophe", "", "LaBRI, IMB"], ["Auber", "David", "", "LaBRI"], ["Bigot", "J\u00e9r\u00e9mie", "", "IMB"], ["Couallier", "Vincent", "", "IMB"], ["Leclerc", "Cyril", "", "LyRE"]]}, {"id": "1807.10262", "submitter": "Jiaming Xu", "authors": "Elchanan Mossel and Jiaming Xu", "title": "Seeded Graph Matching via Large Neighborhood Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a well known noisy model of the graph isomorphism problem. In this\nmodel, the goal is to perfectly recover the vertex correspondence between two\nedge-correlated Erd\\H{o}s-R\\'{e}nyi random graphs, with an initial seed set of\ncorrectly matched vertex pairs revealed as side information. For seeded\nproblems, our result provides a significant improvement over previously known\nresults. We show that it is possible to achieve the information-theoretic limit\nof graph sparsity in time polynomial in the number of vertices $n$. Moreover,\nwe show the number of seeds needed for exact recovery in polynomial-time can be\nas low as $n^{3\\epsilon}$ in the sparse graph regime (with the average degree\nsmaller than $n^{\\epsilon}$) and $\\Omega(\\log n)$ in the dense graph regime.\n  Our results also shed light on the unseeded problem. In particular, we give\nsub-exponential time algorithms for sparse models and an $n^{O(\\log n)}$\nalgorithm for dense models for some parameters, including some that are not\ncovered by recent results of Barak et al.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:44:00 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Mossel", "Elchanan", ""], ["Xu", "Jiaming", ""]]}, {"id": "1807.10461", "submitter": "Nicolas Gastineau", "authors": "Nicolas Gastineau (Le2i), Wahabou Abdou (UBFC), Nader Mbarek (LaBRI),\n  Olivier Togni (Le2i)", "title": "Distributed leader election and computation of local identifiers for\n  programmable matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The context of this paper is programmable matter, which consists of a set of\ncomputational elements, called particles, in an infinite graph. The considered\ninfinite graphs are the square, triangular and king grids. Each particle\noccupies one vertex, can communicate with the adjacent particles, has the same\nclockwise direction and knows the local positions of neighborhood particles.\nUnder these assumptions, we describe a new leader election algorithm affecting\na variable to the particles, called the k-local identifier, in such a way that\nparticles at close distance have each a different k-local identifier. For all\nthe presented algorithms, the particles only need a O(1)-memory space.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:20:34 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Gastineau", "Nicolas", "", "Le2i"], ["Abdou", "Wahabou", "", "UBFC"], ["Mbarek", "Nader", "", "LaBRI"], ["Togni", "Olivier", "", "Le2i"]]}, {"id": "1807.10483", "submitter": "Wiktor Zuba", "authors": "Tomasz Kociumaka, Jakub Radoszewski, Wojciech Rytter, Juliusz\n  Straszy\\'nski, Tomasz Wale\\'n and Wiktor Zuba", "title": "Faster Recovery of Approximate Periods over Edit Distance", "comments": "Accepted to SPIRE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximate period recovery problem asks to compute all\n$\\textit{approximate word-periods}$ of a given word $S$ of length $n$: all\nprimitive words $P$ ($|P|=p$) which have a periodic extension at edit distance\nsmaller than $\\tau_p$ from $S$, where $\\tau_p = \\lfloor\n\\frac{n}{(3.75+\\epsilon)\\cdot p} \\rfloor$ for some $\\epsilon>0$. Here, the set\nof periodic extensions of $P$ consists of all finite prefixes of $P^\\infty$.\n  We improve the time complexity of the fastest known algorithm for this\nproblem of Amir et al. [Theor. Comput. Sci., 2018] from $O(n^{4/3})$ to $O(n\n\\log n)$. Our tool is a fast algorithm for Approximate Pattern Matching in\nPeriodic Text. We consider only verification for the period recovery problem\nwhen the candidate approximate word-period $P$ is explicitly given up to cyclic\nrotation; the algorithm of Amir et al. reduces the general problem in $O(n)$\ntime to a logarithmic number of such more specific instances.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 08:16:29 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "1807.10531", "submitter": "Leizhen Cai", "authors": "Leizhen CAI and On Yin LEUNG", "title": "Alternating Path and Coloured Clustering", "comments": "15 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Coloured Clustering problem, we wish to colour vertices of an edge\ncoloured graph to produce as many stable edges as possible, i.e., edges with\nthe same colour as their ends. In this paper, we reveal that the problem is in\nfact a maximum subgraph problem concerning monochromatic subgraphs and\nalternating paths, and demonstrate the usefulness of such connection in\nstudying these problems.\n  We obtain a faster algorithm to solve the problem for edge-bicoloured graphs\nby reducing the problem to a minimum cut problem. On the other hand, we push\nthe NP-completeness of the problem to edge-tricoloured planar bipartite graphs\nof maximum degree four. Furthermore, we also give FPT algorithms for the\nproblem when we take the numbers of stable edges and unstable edges,\nrespectively, as parameters.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 10:54:15 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["CAI", "Leizhen", ""], ["LEUNG", "On Yin", ""]]}, {"id": "1807.10639", "submitter": "David Grimsman", "authors": "David Grimsman, Mohd. Shabbir Ali, Jo\\~ao P. Hespanha and Jason R.\n  Marden", "title": "The Impact of Information in Greedy Submodular Maximization", "comments": null, "journal-ref": "IEEE Transactions on Control of Network Systems, December 2018", "doi": "10.1109/TCNS.2018.2889005", "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximization of submodular functions is an NP-Hard problem for certain\nsubclasses of functions, for which a simple greedy algorithm has been shown to\nguarantee a solution whose quality is within 1/2 of the optimal. When this\nalgorithm is implemented in a distributed way, agents sequentially make\ndecisions based on the decisions of all previous agents. This work explores how\nlimited access to the decisions of previous agents affects the quality of the\nsolution of the greedy algorithm. Specifically, we provide tight upper and\nlower bounds on how well the algorithm performs, as a function of the\ninformation available to each agent. Intuitively, the results show that\nperformance roughly degrades proportionally to the size of the largest group of\nagents which make decisions independently. Additionally, we consider the case\nwhere a system designer is given a set of agents and a global limit on the\namount of information that can be accessed. Our results show that the best\ndesigns partition the agents into equally-sized sets and allow agents to access\nthe decisions of all previous agents within the same set.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 21:03:50 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 23:03:26 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Grimsman", "David", ""], ["Ali", "Mohd. Shabbir", ""], ["Hespanha", "Jo\u00e3o P.", ""], ["Marden", "Jason R.", ""]]}, {"id": "1807.10727", "submitter": "Michal Wlodarczyk", "authors": "Jakub {\\L}\\k{a}cki, Vahab Mirrokni, Micha{\\l} W{\\l}odarczyk", "title": "Connected Components at Scale via Local Contractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental tool in hierarchical graph clustering, computing connected\ncomponents has been a central problem in large-scale data mining. While many\nknown algorithms have been developed for this problem, they are either not\nscalable in practice or lack strong theoretical guarantees on the parallel\nrunning time, that is, the number of communication rounds. So far, the best\nproven guarantee is $\\Oh(\\log n)$, which matches the running time in the PRAM\nmodel.\n  In this paper, we aim to design a distributed algorithm for this problem that\nworks well in theory and practice. In particular, we present a simple algorithm\nbased on contractions and provide a scalable implementation of it in MapReduce.\nOn the theoretical side, in addition to showing $\\Oh(\\log n)$ convergence for\nall graphs, we prove an $\\Oh(\\log \\log n)$ parallel running time with high\nprobability for a certain class of random graphs. We work in the MPC model that\ncaptures popular parallel computing frameworks, such as MapReduce, Hadoop or\nSpark.\n  On the practical side, we show that our algorithm outperforms the\nstate-of-the-art MapReduce algorithms. To confirm its scalability, we report\nempirical results on graphs with several trillions of edges.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:53:45 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1807.10789", "submitter": "Danil Sagunov", "authors": "Ivan Bliznets and Danil Sagunov", "title": "Solving Target Set Selection with Bounded Thresholds Faster than $2^n$", "comments": "Accepted to IPEC 2018; fixed reference in preliminaries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the Target Set Selection problem. The problem\nnaturally arises in many fields like economy, sociology, medicine. In the\nTarget Set Selection problem one is given a graph $G$ with a function\n$\\operatorname{thr}: V(G) \\to \\mathbb{N} \\cup \\{0\\}$ and integers $k, \\ell$.\nThe goal of the problem is to activate at most $k$ vertices initially so that\nat the end of the activation process there is at least $\\ell$ activated\nvertices. The activation process occurs in the following way: (i) once\nactivated, a vertex stays activated forever; (ii) vertex $v$ becomes activated\nif at least $\\operatorname{thr}(v)$ of its neighbours are activated. The\nproblem and its different special cases were extensively studied from\napproximation and parameterized points of view. For example, parameterizations\nby the following parameters were studied: treewidth, feedback vertex set,\ndiameter, size of target set, vertex cover, cluster editing number and others.\n  Despite the extensive study of the problem it is still unknown whether the\nproblem can be solved in $\\mathcal{O}^*((2-\\epsilon)^n)$ time for some\n$\\epsilon >0$. We partially answer this question by presenting several\nfaster-than-trivial algorithms that work in cases of constant thresholds,\nconstant dual thresholds or when the threshold value of each vertex is bounded\nby one-third of its degree. Also, we show that the problem parameterized by\n$\\ell$ is W[1]-hard even when all thresholds are constant.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 18:37:43 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 07:41:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Bliznets", "Ivan", ""], ["Sagunov", "Danil", ""]]}, {"id": "1807.11135", "submitter": "EPTCS", "authors": "Alastair A. Abbott, Cristian S. Calude, Michael J. Dinneen, Richard\n  Hua", "title": "A Hybrid Quantum-Classical Paradigm to Mitigate Embedding Costs in\n  Quantum Annealing---Abridged Version", "comments": "In Proceedings PC 2018, arXiv:1807.10563", "journal-ref": "EPTCS 273, 2018, pp. 1-13", "doi": "10.4204/EPTCS.273.1", "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealing has shown significant potential as an approach to near-term\nquantum computing. Despite promising progress towards obtaining a quantum\nspeedup, quantum annealers are limited by the need to embed problem instances\nwithin the (often highly restricted) connectivity graph of the annealer. This\nembedding can be costly to perform and may destroy any computational speedup.\nHere we present a hybrid quantum-classical paradigm to help mitigate this\nlimitation, and show how a raw speedup that is negated by the embedding time\ncan nonetheless be exploited in certain circumstances. We illustrate this\napproach with initial results on a proof-of-concept implementation of an\nalgorithm for the dynamically weighted maximum independent set problem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 01:29:33 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Abbott", "Alastair A.", ""], ["Calude", "Cristian S.", ""], ["Dinneen", "Michael J.", ""], ["Hua", "Richard", ""]]}, {"id": "1807.11169", "submitter": "Simina Br\\^anzei", "authors": "Simina Br\\^anzei and Yuval Peres", "title": "Online Learning with an Almost Perfect Expert", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multiclass online learning problem where a forecaster makes a\nsequence of predictions using the advice of $n$ experts. Our main contribution\nis to analyze the regime where the best expert makes at most $b$ mistakes and\nto show that when $b = o(\\log_4{n})$, the expected number of mistakes made by\nthe optimal forecaster is at most $\\log_4{n} + o(\\log_4{n})$. We also describe\nan adversary strategy showing that this bound is tight and that the worst case\nis attained for binary prediction.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 04:34:59 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 15:20:35 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Br\u00e2nzei", "Simina", ""], ["Peres", "Yuval", ""]]}, {"id": "1807.11328", "submitter": "Torben Hagerup", "authors": "Torben Hagerup", "title": "Guidesort: Simpler Optimal Deterministic Sorting for the Parallel Disk\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm, Guidesort, for sorting in the uniprocessor variant of the\nparallel disk model (PDM) of Vitter and Shriver is presented. The algorithm is\ndeterministic and executes a number of (parallel) I/O operations that comes\nwithin a constant factor $C$ of the optimum. The algorithm and its analysis are\nsimpler than those proposed in previous work, and the achievable constant\nfactor $C$ of essentially 3 appears to be smaller than for all other known\ndeterministic algorithms, at least for plausible parameter values.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 13:03:27 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 08:54:03 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 14:01:33 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Hagerup", "Torben", ""]]}, {"id": "1807.11339", "submitter": "Leizhen Cai", "authors": "Leizhen Cai", "title": "Vertex Covers Revisited: Indirect Certificates and FPT Algorithms", "comments": "18 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical NP-complete problem Vertex Cover requires us to determine\nwhether a graph contains at most $k$ vertices that cover all edges. In spite of\nits intractability, the problem can be solved in FPT time for parameter $k$ by\nvarious techniques. In this paper, we present half a dozen new and simple FPT\nalgorithms for Vertex Cover with respect to parameter $k$. For this purpose, we\nexplore structural properties of vertex covers and use these properties to\nobtain FPT algorithms by iterative compression, colour coding, and indirect\ncertificating methods. In particular, we show that every graph with a\n$k$-vertex cover admits an indirect certificate with at most $k/3$ vertices,\nwhich lays the foundation of three new FPT algorithms based on random partition\nand random selection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 13:27:36 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Cai", "Leizhen", ""]]}, {"id": "1807.11419", "submitter": "Tselil Schramm", "authors": "Prasad Raghavendra, Tselil Schramm, David Steurer", "title": "High-dimensional estimation via sum-of-squares proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation is the computational task of recovering a hidden parameter $x$\nassociated with a distribution $D_x$, given a measurement $y$ sampled from the\ndistribution. High dimensional estimation problems arise naturally in\nstatistics, machine learning, and complexity theory.\n  Many high dimensional estimation problems can be formulated as systems of\npolynomial equations and inequalities, and thus give rise to natural\nprobability distributions over polynomial systems. Sum-of-squares proofs\nprovide a powerful framework to reason about polynomial systems, and further\nthere exist efficient algorithms to search for low-degree sum-of-squares\nproofs.\n  Understanding and characterizing the power of sum-of-squares proofs for\nestimation problems has been a subject of intense study in recent years. On one\nhand, there is a growing body of work utilizing sum-of-squares proofs for\nrecovering solutions to polynomial systems when the system is feasible. On the\nother hand, a general technique referred to as pseudocalibration has been\ndeveloped towards showing lower bounds on the degree of sum-of-squares proofs.\nFinally, the existence of sum-of-squares refutations of a polynomial system has\nbeen shown to be intimately connected to the existence of spectral algorithms.\nIn this article we survey these developments.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 16:13:57 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 00:56:07 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Schramm", "Tselil", ""], ["Steurer", "David", ""]]}, {"id": "1807.11462", "submitter": "Eric Balkanski", "authors": "Eric Balkanski, Adam Breuer, Yaron Singer", "title": "Non-monotone Submodular Maximization in Exponentially Fewer Iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider parallelization for applications whose objective\ncan be expressed as maximizing a non-monotone submodular function under a\ncardinality constraint. Our main result is an algorithm whose approximation is\narbitrarily close to $1/2e$ in $O(\\log^2 n)$ adaptive rounds, where $n$ is the\nsize of the ground set. This is an exponential speedup in parallel running time\nover any previously studied algorithm for constrained non-monotone submodular\nmaximization. Beyond its provable guarantees, the algorithm performs well in\npractice. Specifically, experiments on traffic monitoring and personalized data\nsummarization applications show that the algorithm finds solutions whose values\nare competitive with state-of-the-art algorithms while running in exponentially\nfewer parallel iterations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 17:44:53 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Balkanski", "Eric", ""], ["Breuer", "Adam", ""], ["Singer", "Yaron", ""]]}, {"id": "1807.11518", "submitter": "Ioannis Katsikarelis", "authors": "Tesshu Hanaka, Ioannis Katsikarelis, Michael Lampis, Yota Otachi,\n  Florian Sikora", "title": "Parameterized Orientable Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is $d$-orientable if its edges can be oriented so that the maximum\nin-degree of the resulting digraph is at most $d$. $d$-orientability is a\nwell-studied concept with close connections to fundamental graph-theoretic\nnotions and applications as a load balancing problem. In this paper we consider\nthe d-ORIENTABLE DELETION problem: given a graph $G=(V,E)$, delete the minimum\nnumber of vertices to make $G$ $d$-orientable. We contribute a number of\nresults that improve the state of the art on this problem. Specifically:\n  - We show that the problem is W[2]-hard and $\\log n$-inapproximable with\nrespect to $k$, the number of deleted vertices. This closes the gap in the\nproblem's approximability.\n  - We completely characterize the parameterized complexity of the problem on\nchordal graphs: it is FPT parameterized by $d+k$, but W-hard for each of the\nparameters $d,k$ separately.\n  - We show that, under the SETH, for all $d,\\epsilon$, the problem does not\nadmit a $(d+2-\\epsilon)^{tw}$, algorithm where $tw$ is the graph's treewidth,\nresolving as a special case an open problem on the complexity of PSEUDOFOREST\nDELETION.\n  - We show that the problem is W-hard parameterized by the input graph's\nclique-width. Complementing this, we provide an algorithm running in time\n$d^{O(d\\cdot cw)}$, showing that the problem is FPT by $d+cw$, and improving\nthe previously best known algorithm for this case.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 18:21:42 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 14:49:29 GMT"}, {"version": "v3", "created": "Sun, 26 Jan 2020 14:19:13 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Katsikarelis", "Ioannis", ""], ["Lampis", "Michael", ""], ["Otachi", "Yota", ""], ["Sikora", "Florian", ""]]}, {"id": "1807.11538", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri and Kent Quanrud", "title": "On Approximating (Sparse) Covering Integer Programs", "comments": "SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximation algorithms for covering integer programs of the\nform min $\\langle c, x \\rangle $ over $x \\in \\mathbb{N}^n $ subject to $A x\n\\geq b $ and $x \\leq d$; where $A \\in \\mathbb{R}_{\\geq 0}^{m \\times n}$, $b \\in\n\\mathbb{R}_{\\geq 0}^m$, and $c, d \\in \\mathbb{R}_{\\geq 0}^n$ all have\nnonnegative entries. We refer to this problem as $\\operatorname{CIP}$, and the\nspecial case without the multiplicity constraints $x \\le d$ as\n$\\operatorname{CIP}_{\\infty}$. These problems generalize the well-studied Set\nCover problem. We make two algorithmic contributions.\n  First, we show that a simple algorithm based on randomized rounding with\nalteration improves or matches the best known approximation algorithms for\n$\\operatorname{CIP}$ and $\\operatorname{CIP}_{\\infty}$ in a wide range of\nparameter settings, and these bounds are essentially optimal. As a byproduct of\nthe simplicity of the alteration algorithm and analysis, we can derandomize the\nalgorithm without any loss in the approximation guarantee or efficiency.\nPrevious work by Chen, Harris and Srinivasan [12] which obtained near-tight\nbounds is based on a resampling-based randomized algorithm whose analysis is\ncomplex.\n  Non-trivial approximation algorithms for $\\operatorname{CIP}$ are based on\nsolving the natural LP relaxation strengthened with knapsack cover (KC)\ninequalities [5,24,12]. Our second contribution is a fast (essentially\nnear-linear time) approximation scheme for solving the strengthened LP with a\nfactor of $n$ speed up over the previous best running time [5].\n  Together, our contributions lead to near-optimal (deterministic)\napproximation bounds with near-linear running times for $\\operatorname{CIP}$\nand $\\operatorname{CIP}_{\\infty}$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 19:28:16 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 19:13:46 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""]]}, {"id": "1807.11580", "submitter": "Ryo Yoshinaka", "authors": "Yuki Nozaki, Diptarama Hendrian, Ryo Yoshinaka, Takashi Horiyama,\n  Ayumi Shinohara", "title": "Enumerating Cryptarithms Using Deterministic Finite Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cryptarithm is a mathematical puzzle where given an arithmetic equation\nwritten with letters rather than numerals, a player must discover an assignment\nof numerals on letters that makes the equation hold true. In this paper, we\npropose a method to construct a DFA that accepts cryptarithms that admit\n(unique) solutions for each base. We implemented the method and constructed a\nDFA for bases $k \\le 7$. Those DFAs can be used as complete catalogues of\ncryptarithms,whose applications include enumeration of and counting the exact\nnumbers $G_k(n)$ of cryptarithm instances with $n$ digits that admit base-$k$\nsolutions. Moreover, explicit formulas for $G_2(n)$ and $G_3(n)$ are given.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 01:37:45 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Nozaki", "Yuki", ""], ["Hendrian", "Diptarama", ""], ["Yoshinaka", "Ryo", ""], ["Horiyama", "Takashi", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1807.11597", "submitter": "Ce Jin", "authors": "Ce Jin, Hongxun Wu", "title": "A Simple Near-Linear Pseudopolynomial Time Randomized Algorithm for\n  Subset Sum", "comments": "To appear in SOSA 2019 (fixed some typos)", "journal-ref": null, "doi": "10.4230/OASIcs.SOSA.2019.17", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a multiset $S$ of $n$ positive integers and a target integer $t$, the\nSubset Sum problem asks to determine whether there exists a subset of $S$ that\nsums up to $t$. The current best deterministic algorithm, by Koiliaris and Xu\n[SODA'17], runs in $\\tilde O(\\sqrt{n}t)$ time, where $\\tilde O$ hides\npoly-logarithm factors. Bringmann [SODA'17] later gave a randomized $\\tilde O(n\n+ t)$ time algorithm using two-stage color-coding. The $\\tilde O(n+t)$ running\ntime is believed to be near-optimal.\n  In this paper, we present a simple and elegant randomized algorithm for\nSubset Sum in $\\tilde O(n + t)$ time. Our new algorithm actually solves its\ncounting version modulo prime $p>t$, by manipulating generating functions using\nFFT.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 22:25:07 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 14:13:14 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 12:21:01 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Jin", "Ce", ""], ["Wu", "Hongxun", ""]]}, {"id": "1807.11648", "submitter": "Alireza Rezaei", "authors": "Piotr Indyk, Sepideh Mahabadi, Shayan Oveis Gharan, Alireza Rezaei", "title": "Composable Core-sets for Determinant Maximization Problems via Spectral\n  Spanners", "comments": "To appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a spectral generalization of classical combinatorial graph spanners\nto the spectral setting. Given a set of vectors $V\\subseteq \\Re^d$, we say a\nset $U\\subseteq V$ is an $\\alpha$-spectral spanner if for all $v\\in V$ there is\na probability distribution $\\mu_v$ supported on $U$ such that $$vv^\\intercal\n\\preceq \\alpha\\cdot\\mathbb{E}_{u\\sim\\mu_v} uu^\\intercal.$$ We show that any set\n$V$ has an $\\tilde{O}(d)$-spectral spanner of size $\\tilde{O}(d)$ and this\nbound is almost optimal in the worst case.\n  We use spectral spanners to study composable core-sets for spectral problems.\nWe show that for many objective functions one can use a spectral spanner,\nindependent of the underlying functions, as a core-set and obtain almost\noptimal composable core-sets. For example, for the determinant maximization\nproblem we obtain an $\\tilde{O}(k)^k$-composable core-set and we show that this\nis almost optimal in the worst case.\n  Our algorithm is a spectral analogue of the classical greedy algorithm for\nfinding (combinatorial) spanners in graphs. We expect that our spanners find\nmany other applications in distributed or parallel models of computation. Our\nproof is spectral. As a side result of our techniques, we show that the rank of\ndiagonally dominant lower-triangular matrices are robust under `small\nperturbations' which could be of independent interests.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 03:27:34 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 20:04:07 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Indyk", "Piotr", ""], ["Mahabadi", "Sepideh", ""], ["Gharan", "Shayan Oveis", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1807.11702", "submitter": "Juliusz Straszy\\'nski", "authors": "Panagiotis Charalampopoulos, Costas S. Iliopoulos, Tomasz Kociumaka,\n  Solon P. Pissis, Jakub Radoszewski, and Juliusz Straszy\\'nski", "title": "Efficient Computation of Sequence Mappability", "comments": "Accepted to SPIRE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $(k,m)$-mappability problem, for a given sequence $T$ of length $n$,\nthe goal is to compute a table whose $i$th entry is the number of indices $j\n\\ne i$ such that the length-$m$ substrings of $T$ starting at positions $i$ and\n$j$ have at most $k$ mismatches. Previous works on this problem focused on\nheuristics computing a rough approximation of the result or on the case of\n$k=1$. We present several efficient algorithms for the general case of the\nproblem. Our main result is an algorithm that, for $k=\\mathcal{O}(1)$, works in\n$\\mathcal{O}(n)$ space and, with high probability, in $\\mathcal{O}(n \\cdot\n\\min\\{m^k,\\log^k n\\})$ time. Our algorithm requires a careful adaptation of the\n$k$-errata trees of Cole et al. [STOC 2004] to avoid multiple counting of pairs\nof substrings. Our technique can also be applied to solve the all-pairs Hamming\ndistance problem introduced by Crochemore et al. [WABI 2017]. We further\ndevelop $\\mathcal{O}(n^2)$-time algorithms to compute all $(k,m)$-mappability\ntables for a fixed $m$ and all $k\\in \\{0,\\ldots,m\\}$ or a fixed $k$ and all\n$m\\in\\{k,\\ldots,n\\}$. Finally, we show that, for $k,m = \\Theta(\\log n)$, the\n$(k,m)$-mappability problem cannot be solved in strongly subquadratic time\nunless the Strong Exponential Time Hypothesis fails.\n  This is an improved and extended version of a paper that was presented at\nSPIRE 2018.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 08:49:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:40:47 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 20:03:10 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Iliopoulos", "Costas S.", ""], ["Kociumaka", "Tomasz", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Straszy\u0144ski", "Juliusz", ""]]}, {"id": "1807.11711", "submitter": "Marcel Radermacher", "authors": "Marcel Radermacher and Ignaz Rutter", "title": "Inserting an Edge into a Geometric Embedding", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithm of Gutwenger et al. to insert an edge $e$ in linear time into a\nplanar graph $G$ with a minimal number of crossings on $e$, is a helpful tool\nfor designing heuristics that minimize edge crossings in drawings of general\ngraphs. Unfortunately, some graphs do not have a geometric embedding $\\Gamma$\nsuch that $\\Gamma+e$ has the same number of crossings as the embedding $G+e$.\nThis motivates the study of the computational complexity of the following\nproblem: Given a combinatorially embedded graph $G$, compute a geometric\nembedding $\\Gamma$ that has the same combinatorial embedding as $G$ and that\nminimizes the crossings of $\\Gamma+e$. We give polynomial-time algorithms for\nspecial cases and prove that the general problem is fixed-parameter tractable\nin the number of crossings. Moreover, we show how to approximate the number of\ncrossings by a factor $(\\Delta-2)$, where $\\Delta$ is the maximum vertex degree\nof $G$.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:17:43 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Radermacher", "Marcel", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1807.11869", "submitter": "Hans Bodlaender", "authors": "Hans L. Bodlaender and Tom C. van der Zanden", "title": "On Exploring Temporal Graphs of Small Pathwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Temporal Graph Exploration Problem is NP-complete, even when\nthe underlying graph has pathwidth 2 and at each time step, the current graph\nis connected.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 15:31:37 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["van der Zanden", "Tom C.", ""]]}]